__FILENAME__ = blog
import web
from settings import render
urls = (
  '', 'reblog',
  '/', 'index',
  '/feed', 'feed',
  '/(.*)', 'post',
)

content = [
  web.storage(
    slug='fecpvs',
    title='More data!',
    author='Aaron Swartz',
    updated='2008-07-30T00:00:00Z',
    body = """
<p>
We've added even more data to the site. 
Now politician pages 
feature data from the FEC -- 
the Federal Election Commission,
which tracks all usage of money in politics.
FEC data includes things like
the amount of money raised,
who it was raised from,
and so on.
We hope to have even more
(actually, a lot more)
FEC data soon, 
but hopefully this provides an interesting start.
</p>

<p>
We've also added some more personal data
from our friends over at <a 
  href="http://votesmart.org"
>Project Vote Smart</a>.
The data includes things like
a politician's nickname
and educational history,
all of which we now provide on politician pages.
</p>

<p>
I hope you enjoy the new features 
and stay tuned for some even more exciting stuff tonight
and later this week.
</p>
    """
  ),
  web.storage(
    slug = 'alignment',
    title = 'Interest Group Alignment',
    author = 'Aaron Swartz',
    updated = '2008-06-16T00:00:00Z',
    body = """
<p>
First, 
let me say welcome aboard to our newest team member,
programmer A.S.L. Devi. 
Devi's already proved herself invaluable 
by building our latest feature:
politician&mdash;interest group alignment.
It's a terrible name
(my fault; let me know if you have a better one)
but the idea is simple:
go to a page like 
<a href="http://watchdog.net/p/mark_kirk">Mark Kirk's</a>
and scroll to the bottom.
There you'll see that Kirk is a big fan of people like
the National Association of Home Builders and
the National Association of Realtors,
but not the American Civil Liberties Union.
And for each group you can click 
and see the votes where they agree and disagree.
</p>

<p>
Furthermore, 
if you click on a bill and scroll to the bottom,
you can see all the groups that supported or opposed the bill.
</p>

<p>
It's pretty fun stuff and, 
in my opinion,
awfully exciting.
It's all made possible thanks to our partners:
<a href="http://www.govtrack.us/">GovTrack.us</a>, 
a fantastic site which provides data on bills, and
<a href="http://www.maplight.org/">MAPLight.org</a>, 
a Berkeley non-profit which each summer
(including right now)
brings interns out to search the news 
to see who is supporting and opposing 
the bills currently before Congress.
</p>

<p>
Thanks to everyone who made this happen.
I hope you enjoy it!
</p>
"""
  ),
  web.storage(
    slug = 'earmarks',
    title = 'Earmark Info',
    author = 'Aaron Swartz',
    updated = '2008-05-07T00:00:00Z',
    body = """
<p>
Thanks to the work of Alex Gourley
and data from <a href="http://taxpayer.net/">Taxpayers for Common Sense</a>,
politician pages now have basic information
about the earmarks they've requested:
the size and number requested
and the size and number eventually passed.
</p>

<p>
"Earmark" is the catch-all term for the requests
that Congresspeople attach to bills requiring Federal money
be given to particular people or places.
They've been in the news a lot lately,
criticized as a form of corruption 
in which Congresspeople hand out money to lobbyists or campaign contributors
instead of letting civil servants or the bidding process handle it.
</p>

<p>
As with other Congressional perks,
they're not exactly distributed evenly.
<a href="http://watchdog.net/p/by/amt_earmark_received">Our chart</a>
shows how House leaders like 
<a href="http://watchdog.net/p/nancy_pelosi">Nancy Pelosi</a>
come out on top,
with hundreds of millions of dollars in earmarks,
while newcomers like 
<a href="http://watchdog.net/p/laura_richardson">Laura Richardson</a>
get only hundreds of thousands.
</p>

<p>
Whatever your feelings on earmarks,
we hope this data is interesting to you.
Thanks to Alex and Taxpayers for Common Sense for making it possible.
</p>
"""
  ),
  web.storage(
    slug = 'speeches',
    title = 'Speech Data',
    author = 'Aaron Swartz',
    updated = '2008-04-21T00:00:00Z',
    body = """
<p>
I'm thrilled to say that Thursday, 
just days after we launched, 
we got our first volunteer code contribution.
Didier Deshommes created 
<a href="http://github.com/dfdeshom/watchdog/">a branch on github</a>,
added support for parsing some data about speeches politicians have made,
and I pulled it and added it to the site.
</p>

<p>
Now when you visit a page like 
<a href="http://watchdog.net/p/nancy_pelosi">Nancy Pelosi</a>
you can see how many times she's spoken this session
and the average length of her speech.
</p>

<p>
It's great to see these kinds of contributions and 
I know there are more in the pipeline!
Thanks to everyone who's been pitching in.
</p>

<p>
On a darker note, 
apologies for the outages over the weekend. 
I think I discovered the cause of the problem
and it shouldn't happen again.
</p>
"""
  ),
  web.storage(
    slug = 'momentum',
    title = 'Building Momentum',
    author = 'Aaron Swartz',
    updated = '2008-04-16T23:48:00Z',
    body = """
<p>
The response to the announcement of this little site 
has been bigger than I ever expected.
Within hours after I posted about it,
I'd received a couple dozen emails of support --
some people asking how they could help,
others sending their ideas and suggestions,
and many just saying "right on!"
</p>

<p>
I've launched dozens of sites but I've never gotten a response quite like this.
And I think it has to be chalked up to the power of this idea:
there are lots of people eager for a way to <em>get involved</em>.
If you want to do your part, 
I suggest you sign up for
<a href="http://groups.google.com/group/watchdog-volunteers">our volunteer list</a> -- 
I'll send an email out there when we need help with something.
</p>

<p>
Perhaps the most helpful -- 
and most unexpected -- 
piece has been all the Python programmers who wrote in 
asking how they could help.
The volunteers quickly ran thru everything I could think of off the top of my head
and I've had to go thru my todo list and start picking out things
I never thought I'd get to.
Of course that's a great problem to have
and <a href="http://watchdog.net/about/#feedback">we could always use more hands</a>.
</p>

<p>
And just a short while ago,
I did an interview with XM Satellite Radio about the project.
All in all, not bad for a first day.
</p>

<p>
Thanks to everyone who made it happen.
And let's make sure we don't lose this momentum -- 
together, let's build something great.
</p>
"""
  ),
  web.storage(
    slug = 'launch',
    title = 'Welcome to watchdog.net!',
    author = 'Aaron Swartz',
    updated = '2008-04-14T00:00:00Z',
    body = """
<p>
It's a big election year in the US, 
which means a lot of people have been thinking about politics lately. 
I've been far from immune, 
signing up for dozens of sites and reading bunches of blogs. 
But, despite all this, 
I feel like there's something missing: 
a way for the average person to actually <em>get involved</em> in politics.
</p>

<p>
Sure, you can be outraged over some factoid you read on a blog 
or take part in some action campaign started by a nonprofit,
but that still feels like being a spectator to me. 
Instead, I wanted to a site where you could discover the facts for yourself 
and start your own action campaigns.
</p>

<p>
Not finding one, I've decided to help build it. 
<a href="http://watchdog.net/about#people">An amazing group of people</a> 
have signed on with me 
(<a href="http://watchdog.net/about#help">although we're still looking for more</a>) 
and the Sunlight Network given us a grant to fund it.
</p>

<p>
You can read more about us and our plans on 
<a href="http://watchdog.net/about/">the about page</a> 
but for now let me just say welcome and pardon the mess. 
We're trying to develop this site fast and in public, 
so expect lots of changes. 
We'll try to keep the public brokenness to a minimum, 
but there will undoubtedly be some, 
especially these first few weeks.
</p>

<p>
And to forestall the inevitable catcalls: 
yes, there's not much here now. 
But we literally started officially working <em>today</em>. 
This is just the skeleton of the site we hope to build.
</p>

<p>
Thanks for bearing with us 
and <a href="http://watchdog.net/about/#feedback">let us know what you think</a>.
</p>
"""
  )
]
content_mapping = dict((x.slug, x) for x in content)

class index:
    def GET(self):
        return render.blog_index(content)

class feed:
    def GET(self):
        web.header('content-type', 'application/atom+xml')
        lastupdate = max(x.updated for x in content)
        return render._template('blog_feed')(content, lastupdate)

class post:
    def GET(self, name):
        if name in content_mapping:
            return render.blog_post(content_mapping[name])
        else:
            raise web.notfound()

class reblog:
    def GET(self):
        raise web.seeother('/')

app = web.application(urls, globals())

########NEW FILE########
__FILENAME__ = census
import web

from settings import db


total_pop = ['/TotalPopulation/Total' ]


edu_totals = ['/Population25YearsAndOver/Female',
        '/Population25YearsAndOver/Male']
edu_college = [ 
        '/Population25YearsAndOver/Female/AssociateDegree',
        '/Population25YearsAndOver/Female/BachelorsDegree',
        '/Population25YearsAndOver/Female/DoctorateDegree',
        '/Population25YearsAndOver/Female/MastersDegree',
        '/Population25YearsAndOver/Female/ProfessionalSchoolDegree',
        '/Population25YearsAndOver/Female/SomeCollege1OrMoreYearsNoDegree',
        '/Population25YearsAndOver/Female/SomeCollegeLessThan1Year',

        '/Population25YearsAndOver/Male/AssociateDegree',
        '/Population25YearsAndOver/Male/BachelorsDegree',
        '/Population25YearsAndOver/Male/DoctorateDegree',
        '/Population25YearsAndOver/Male/MastersDegree',
        '/Population25YearsAndOver/Male/SomeCollege1OrMoreYearsNoDegree',
        '/Population25YearsAndOver/Male/SomeCollegeLessThan1Year',
        ]
edu_prof_degree = ['/Population25YearsAndOver/Male/ProfessionalSchoolDegree']
edu_nocollege = [
        '/Population25YearsAndOver/Male/NoSchoolingCompleted',
        '/Population25YearsAndOver/Male/NurseryTo4thGrade',
        '/Population25YearsAndOver/Male/5thAnd6thGrade',
        '/Population25YearsAndOver/Male/7thAnd8thGrade',
        '/Population25YearsAndOver/Male/9thGrade',
        '/Population25YearsAndOver/Male/10thGrade',
        '/Population25YearsAndOver/Male/11thGrade',
        '/Population25YearsAndOver/Male/12thGradeNoDiploma',
        '/Population25YearsAndOver/Male/HighSchoolGraduateincludesEquivalency',

        '/Population25YearsAndOver/Female/NoSchoolingCompleted',
        '/Population25YearsAndOver/Female/NurseryTo4thGrade',
        '/Population25YearsAndOver/Female/5thAnd6thGrade',
        '/Population25YearsAndOver/Female/7thAnd8thGrade',
        '/Population25YearsAndOver/Female/9thGrade',
        '/Population25YearsAndOver/Female/10thGrade',
        '/Population25YearsAndOver/Female/11thGrade',
        '/Population25YearsAndOver/Female/12thGradeNoDiploma',
        '/Population25YearsAndOver/Female/HighSchoolGraduateincludesEquivalency',
        ]


marital_stat_totals = [ '/Population15YearsAndOver/Male',
        '/Population15YearsAndOver/Female' ]
marital_stat_never_married = [ '/Population15YearsAndOver/Male/NeverMarried',
        '/Population15YearsAndOver/Female/NeverMarried']
marital_stat_divorced = [ '/Population15YearsAndOver/Male/Divorced',
        '/Population15YearsAndOver/Female/Divorced' ]


mil_totals = ['/Population18YearsAndOver/Total']
#mil_total = ['/Population18YearsAndOver/Male','/Population18YearsAndOver/Female']
mil_cur = [ '/Population18YearsAndOver/Male/18To64Years/InArmedForces',
        '/Population18YearsAndOver/Male/65YearsAndOver/InArmedForces',
        '/Population18YearsAndOver/Female/18To64Years/InArmedForces',
        '/Population18YearsAndOver/Female/65YearsAndOver/InArmedForces' ]
mil_vet = [ '/Population18YearsAndOver/Male/18To64Years/Civilian/Veteran',
        '/Population18YearsAndOver/Male/65YearsAndOver/Civilian/Veteran',
        '/Population18YearsAndOver/Female/18To64Years/Civilian/Veteran',
        '/Population18YearsAndOver/Female/65YearsAndOver/Civilian/Veteran' ]
mil_none = [ '/Population18YearsAndOver/Male/18To64Years/Civilian/Nonveteran',
        '/Population18YearsAndOver/Male/65YearsAndOver/Civilian/Nonveteran',
        '/Population18YearsAndOver/Female/18To64Years/Civilian/Nonveteran',
        '/Population18YearsAndOver/Female/65YearsAndOver/Civilian/Nonveteran' ]


born_totals = ['/TotalPopulation/Total'] 
born_native = ['/TotalPopulation/Native']
born_foreign = ['/TotalPopulation/ForeignBorn']


def query_census(location, hr_keys):
    # Use DISTINCT since some hr_keys map to multiple internal_keys (but should
    # have same value).
    #q = db.select('census', what='SUM(DISTINCT(value))', where=web.sqlors('hr_key=', hr_keys)+' AND location='+web.sqlquote(location))
    q = db.query('SELECT SUM(value) FROM (SELECT DISTINCT value, hr_key FROM census WHERE '+web.sqlors('hr_key=', hr_keys)+' AND district_id='+web.sqlquote(location)+') AS foo;')
    if not q: return None
    return q[0].sum


# This is for the population of 18 years and older.
def mil_service(location):
    tot = query_census(location, mil_totals)
    cur = query_census(location, mil_cur)
    vet = query_census(location, mil_vet)
    none = query_census(location, mil_none)
    return {'pct_mil_cur': cur / tot,
            'pct_mil_vet': vet / tot,
            'pct_mil_none': vet / tot,
            'mil_total': tot}


# This is for the entire population 
def born_locations(location):
    tot = query_census(location, born_totals)
    native = query_census(location, born_native)
    foreign = query_census(location, born_foreign)
    return {'pct_born_foreign': foreign / tot,
            'pct_born_native': native / tot,
            'born_total': tot}


# This is for the population of 15 years and older.
def marital_stat(location):
    tot = query_census(location, marital_stat_totals)
    never_married = query_census(location, marital_stat_never_married)
    divorced = query_census(location, marital_stat_divorced)
    return {'pct_never_married': never_married / tot,
            'pct_divorced': divorced / tot,
            'marital_stat_total': tot}


# This is for the population of 25 years and older.
def education(location):
    tot = query_census(location, edu_totals)
    some_college = query_census(location, edu_college)
    professional = query_census(location, edu_prof_degree)
    no_college = query_census(location, edu_nocollege)
    return {'pct_some_college': some_college / tot,
            'pct_professional': professional / tot,
            'pct_no_college': no_college / tot,
            'edu_total': tot}


# This is for the entire population 
def get_total_pop(location):
    tot = query_census(location, total_pop)
    return {'total_pop': tot}


if __name__ == "__main__":
    from pprint import pprint
    states = ['AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 
    'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME', 'MI', 
    'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', 'NY', 
    'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VA', 'VT', 
    'WA', 'WI', 'WV', 'WY']
    for state in states:
        print state
        pprint(education(state))
        pprint(marital_stat(state))
        pprint(mil_service(state))
        pprint(get_total_pop(state))


########NEW FILE########
__FILENAME__ = config
#used to encrypt cookies etc.
try:
    secret_key = file('/home/watchdog/certs/secret_key').read().strip()
except:
    secret_key = 'UuBCsl6nlTEjZASAbqNU' #not to get error while testing on non-server env
#from address in mails sent by watchdog
from_address = '"watchdog.net" <info@watchdog.net>'
maildir_path = '/home/wathdog/Maildir'
send_errors_to = 'bugs@watchdog.net'
test_email = 'test@watchdog.net' #wyr test emails to go to this
production_test_email = 'wyr_prod@watchdog.net' #all production wyr msgs and their responses go here
cache_dir = '/home/watchdog/cache'

########NEW FILE########
__FILENAME__ = contacts
import time, hashlib, urllib, urllib2, string
from xml.dom import minidom    
import simplejson as json
from BeautifulSoup import BeautifulSoup

import web
from settings import db, render
from utils import helpers, forms

def yahooLoginURL(email, url, token=None, share_url='/', title=''):
    email = urllib.quote(email)
    lines = open('/home/watchdog/certs/yauth', 'r').readlines()
    appid = lines[0].rstrip()
    secret = lines[1].rstrip()
    ts = time.time()
    appdata = '|'.join([email, share_url, title])
    yurl = 'https://api.login.yahoo.com'
    purl = '%s?appid=%s&appdata=%s&ts=%s' % (url, appid, appdata, ts)
    surl ='%s%s' % (purl, secret)
    sig = hashlib.md5(surl).hexdigest()
    furl = '%s%s&sig=%s' % (yurl, purl, sig)
    if token: furl = '%s&token=%s' % ( furl, token)
    return  furl

def gmailLoginURL(email, share_url='/', title=''):
    gurl = 'https://www.google.com/accounts/AuthSubRequest?'
    scope = urllib2.quote('http://www.google.com/m8/feeds/')
    next = urllib2.quote(web.ctx.homedomain + '/authsub?email=%s&url=%s&title=%s' %(email, share_url, title))
    gurl += 'scope='+scope+'&session=1&secure=0&next='+ next
    return gurl
    
def msnLoginURL(email, share_url='/', title=''):
    murl = "https://consent.live.com/Delegation.aspx?RU=%s&ps=%s&pl=%s"
    appdata = '|'.join([email, share_url, title])
    return_url = urllib.quote(web.ctx.homedomain + '/auth/msn?appdata=%s' % appdata)
    permissions = 'Contacts.View'
    privacy_policy = urllib.quote(web.ctx.homedomain + '/privacy')
    murl = murl % (return_url, permissions, privacy_policy)
    #token = "appid=%s&ts=%s" % (getAppId(), time.time())
    #murl = murl + urllib.quote(token)
    return murl
    
class importcontacts:
    def GET(self):
        msg, msg_type = helpers.get_delete_msg()
        return render.import_contacts(msg)

    def POST(self):
        i = web.input()
        email, url, title = i.get('email', ''), i.get('url', '/'), i.get('title', '')
        form = forms.loadcontactsform()
        if form.validates(i):
            if i.provider == 'yahoo':
                ylogin_url = yahooLoginURL(email, '/WSLogin/V1/wslogin', share_url=url, title=title)
                raise web.seeother(ylogin_url)
            elif i.provider == 'google':
                glogin_url = gmailLoginURL(email, url, title)
                raise web.seeother(glogin_url)
            elif i.provider == 'msn':
                mlogin_url = msnLoginURL(email, url, title)
                raise web.seeother(mlogin_url)
        else:
            import petition
            share_obj = petition.share()
            emailform = forms.emailform()
            return share_obj.GET(emailform, form)
            
def save_contacts(email, contacts, provider):
    #Even if the user is not logged-in, but has an account with us, let him import contacts
    user_id = helpers.get_loggedin_userid()
    if not user_id:
        user = db.select('users', what='id', where='email=$email', vars=locals())
        if user: user_id = user[0].id
        
    if user_id:    
        for c in contacts:
            cname, cemail = c['name'], c['email']
            vars = dict(user_id=user_id, uemail=email, cemail=cemail,
                        cname=cname, provider=provider)
            e = db.select('contacts', 
                        where='user_id=$user_id and uemail=$uemail and cemail=$cemail',
                        vars=vars)
            if not e: n = db.insert('contacts', seqname=False, **vars)
            elif cname: db.update('contacts', cname=cname,
                        where='user_id=$user_id and uemail=$uemail and cemail=$cemail',
                        vars=vars)

class auth_yahoo:
    def get_contacts(self, contacts_json):
        content = json.loads(contacts_json)
        
        contacts = []
        for c in content.get('contacts'):
            fields = c['fields']
            cemail = fields[0]['data']
            cfname = ' '; clname = ' '

            if len(fields) > 1:
                cfname = fields[1].get('first', ' ')
                clname = fields[1].get('last', ' ')
        
            cname = u'%s %s' % (cfname, clname)
            cname = cname.replace('&#39;', ' ').strip()
            contacts.append(dict(email=cemail, name=cname))
        return contacts

    def GET(self):
        i = web.input()
        appid = i.get('appid').rstrip()
        email, url, title = i.get('appdata', '||').split('|')
        userhash = i.get('userhash')        
        ts = i.get('ts')        
        token = i.get('token')        
        query = urllib.urlencode(dict(url=url, title=title))
        if not token:
            raise web.seeother('/share?%s' % query)
        #XXX: security verification etc..
        url = yahooLoginURL(email, '/WSLogin/V1/wspwtoken_login', token)
        try:
            resp = urllib2.urlopen(url)
        except:
            helpers.set_msg('Authorization Failed.')
            raise web.seeother('/share?%s' % query)

        content = resp.read()
        soup = BeautifulSoup(content)        
        aurl = 'http://address.yahooapis.com/v1/searchContacts?format=json'
        wssid = soup.findAll('wssid')[0].contents[0]        
        cookie =soup.findAll('cookie')[0].contents[0]        
        cookie = cookie.strip()        

        furl = aurl + '&fields=email,name&email.present=1&appid=%s&WSSID=%s' % (appid, wssid)
        req = urllib2.Request(furl)
        req.add_header('Cookie', cookie)
        req.add_header('Content-Type', 'application/json')
        response = urllib2.urlopen(req).read()
        contacts = self.get_contacts(response)
        save_contacts(email, contacts, provider='YAHOO')
        raise web.seeother('/share?%s' % query)

def get_text(elem):
    #gets the text from XML DOM element `elem`
    text = ''
    for node in elem.childNodes:
        if node.nodeType == node.TEXT_NODE:
            text += node.data
    return text

class auth_google:
    def get_contacts(self, contacts_feed):
        ATOM_NS = 'http://www.w3.org/2005/Atom'
        doc = minidom.parse(contacts_feed)
        entries = doc.getElementsByTagNameNS(ATOM_NS, u'entry')
                    
        contacts = []
        for e in entries:
            e_title = e.getElementsByTagNameNS(ATOM_NS, u'title')[0]
            name = get_text(e_title)
            email = e.getElementsByTagName('gd:email')[0].getAttribute('address')
            contacts.append(dict(name=name, email=email))
        return contacts    

    def GET(self):
        i = web.input()
        query = urllib.urlencode(dict(url=i.get('url'), title=i.get('title')))
        authToken = i.get('token')
        if not authToken:
            raise web.seeother('/share?%s' % query)
        email = i.get('email')
        emailq = urllib2.quote(email, '')
        url = ("http://www.google.com/m8/feeds/contacts/%s/full?max-results=999" % emailq)
        headers = { 'Authorization' : 'AuthSub token="%s"' % authToken.strip() }
        request = urllib2.Request(url, None, headers)
        try:
            response = urllib2.urlopen(request)
        except:
            helpers.set_msg('Authorization Failed.')
        else:        
            contacts = self.get_contacts(response)
            save_contacts(email, contacts, provider='GOOGLE')
        raise web.seeother('/share?%s' % query)
        
class auth_msn:
    def get_consent(self, s):
        d = {}
        s = urllib.unquote(s)
        ts = s.split('&')
        for t in ts:
            k, v = t.split('=')
            d[k] = v
        return d

    def get_contacts(self, contacts_xml):
        contacts = []
        xmldoc = minidom.parse(contacts_xml)
        for c in xmldoc.getElementsByTagName('Contact'):
            name_elem = c.getElementsByTagName('SortName')[0]
            name = get_text(name_elem)
            email_elem = c.getElementsByTagName('Address')[0]
            email = get_text(email_elem)
            contacts.append(dict(name=name, email=email))
        return contacts

    def POST(self):
        i = web.input()
        appdata = i.get('appdata', '||')
        email, share_url, title = appdata.split('|')
        if i.get('ResponseCode', '') == 'RequestApproved':
            consent = self.get_consent(i.ConsentToken)
            lid = consent.get('lid')
            delegatedToken = urllib.unquote(consent.get('delt'))
            url = 'https://livecontacts.services.live.com'
            url += '/users/@L@' + lid + '/rest/LiveContacts/Contacts'
            request = urllib2.Request(url)
            request.add_header('Content-Type', 'application/xml; charset=utf-8')
            request.add_header('Authorization', 'DelegatedToken dt="%s"' % delegatedToken)
            try:
                response = urllib2.urlopen(request)
            except:
                helpers.set_msg('Authorization Failed.')
            else:        
                contacts = self.get_contacts(response)
                save_contacts(email, contacts, provider='MICROSOFT')

        query = urllib.urlencode(dict(url=share_url, title=title))
        raise web.seeother('/share?%s' % query)

########NEW FILE########
__FILENAME__ = capitolwords
#!/usr/bin/env python
"""get capitolwords jsons and store in load/capitolwords/"""

import sys
import urllib2
import datetime
from settings import db

DATA_DIR = '../data/crawl/capitolwords'

def save_capitolwords(bioguide_id, limit=5):
    """
    save the capitolwords said by politician with bioguideid `id` in last year
    """
    today = datetime.date.today()
    from_yyyy, to_yyyy = today.year -1, today.year
    from_mm = to_mm = today.month
    from_dd = to_dd = today.day
    
    url = "http://capitolwords.org/api/lawmaker/%s/%s/%s/%s/%s/%s/%s/top%s.json" % (
                bioguide_id, from_yyyy, from_mm, from_dd, to_yyyy, to_mm, to_dd, limit)
    try:
        json = urllib2.urlopen(url).read()
    except:
        pass
    else:
        fn = "%s/%s.json" % (DATA_DIR, bioguide_id)
        file(fn, 'w').write(json)

if __name__ == "__main__":
    pols = db.select('curr_politician', what='id, bioguideid', where='bioguideid is not null').list()
    for p in pols:
        print >>sys.stderr,'\rGetting capitol words for %-25s' % p.id,; sys.stderr.flush()
        save_capitolwords(p.bioguideid)

########NEW FILE########
__FILENAME__ = mortality
import os, urllib
import simplejson as json

DEBUG = True
out_dir = '../data/crawl/mortality/%s.tsv'
states = json.load(file('load/manual/states.json'))
URL = 'http://wonder.cdc.gov/controller/datarequest/D35'
# POST data for ordering by 1) Region and 2) Cause of Death, Some State, Saving as File, and (max) 10 minute timeout
data = 'javascript=on&stage=request&M_1=D35.M1&M_2=D35.M2&M_3=D35.M3&O_vintage=D35&B_1=D35.V9-level2&B_2=D35.V2-level3&B_3=*None*&B_4=*None*&B_5=*None*&O_title=&O_location=D35.V9&finder-stage-D35.V9=codeset&O_V9_fmode=freg&V_D35.V9=&F_D35.V9=%s&I_D35.V9=%s&finder-stage-D35.V10=codeset&O_V10_fmode=freg&V_D35.V10=&F_D35.V10=*All*&I_D35.V10=&O_age=D35.V5&V_D35.V5=*All*&V_D35.V6=00&V_D35.V1=*All*&V_D35.V8=*All*&V_D35.V7=*All*&V_D35.V11=*All*&O_icd=D35.V2&finder-stage-D35.V2=codeset&O_V2_fmode=freg&V_D35.V2=&F_D35.V2=*All*&I_D35.V2=&finder-stage-D35.V4=codeset&O_V4_fmode=freg&V_D35.V4=&F_D35.V4=*All*&I_D35.V4=&V_D35.V12=*All*&V_D35.V13=*All*&O_rate_per=100000&O_aar=aar_none&O_aar_pop=0000&VM_D35.M6_D35.V1=*All*&VM_D35.M6_D35.V7=*All*&VM_D35.M6_D35.V8=*All*&VM_D35.M6_D35.V10=&O_change_action=Export+Results&O_show_totals=true&O_precision=1&O_timeout=600&action=Send'

# iterate across states in /data/parse/states/index.json
for abbr, s in states.iteritems():
    if abbr == 'PW' or s['status'] != 'state' or os.path.exists(out_dir % abbr):
        continue
    fipscode, name = s['fipscode'], s['name']
    state_plus_fipscode = '%s+(%s)\r\n' % (fipscode, urllib.quote(name))
        
    if DEBUG:
        print 'logging: %s' % name
    
    d = urllib.urlopen(URL, data % (fipscode, state_plus_fipscode)).read()
    file(out_dir % abbr, 'w').write(d)

########NEW FILE########
__FILENAME__ = opensecrets
"""
Tools to crawl data from Open Secrets.
"""
import re, urllib, os

DATA_DIR = "../data/"

u_lpac = "http://www.opensecrets.org/pacs/industry.php?txt=Q03&cycle=2008"
r_lpac = re.compile(r'<tr>\s*<td><a href="lookup2\.php\?strID=(C\d+)">[^<]+</a>\s*</td><td>\s*<a href="/politicians/summary\.php\?cid=(N\d+)&cycle=2008">')

def lpacs():
    return r_lpac.findall(urllib.urlopen(u_lpac).read())

def write_lpacs():
    listing = lpacs()
    fh = file(DATA_DIR + 'crawl/opensecrets/leadership.tsv', 'w')
    fh.write('FECCommID\tCRPCandID\n')
    for item in listing:
        fh.write('%s\t%s\n' % item)
    fh.close()

u_cands = "http://www.opensecrets.org/politicians/candlist.php?congno=110"
r_candid = re.compile(r"summary\.php\?cid=([A-Z0-9]+)&cycle=\d+\" ?>([^<]+)")
u_sectors = "http://www.opensecrets.org/politicians/pop_sector.php?cycle=2008&cid=%s"

import sys
def count(x):
    lenx = len(x)
    for n, y in enumerate(x):
        sys.stderr.write('\r%s/%s = %s' % (n, lenx, float(n)/lenx))
        yield y

def sectors():
    cands = set(x[0] for x in r_candid.findall(urllib.urlopen(u_cands).read()))
    for cand in count(list(cands)):
        pth = DATA_DIR + 'crawl/opensecrets/sectors/%s.html' % cand
        if os.path.exists(pth): continue
        file(pth, 'w').write(urllib.urlopen(u_sectors % cand).read())
        time.sleep(3)
    

fhout = file('canids.tsv', 'w')
fhout.write('opensecretsid\tname\n')
for k, v in x:
    fhout.write('%s\t%s\n' % (k, v.replace('  ', '')))

fhout.close()

if __name__ == "__main__":
    write_lpacs()

########NEW FILE########
__FILENAME__ = pvsapi
import urllib, pickle, os, sys
import simplejson as json

def listify(x):
    if not isinstance(x, list):
        return [x]
    else:
        return x

def jsonify(d):
    return json.dumps(d, indent=2, sort_keys=True)

def cachejson(funct):
    name = funct.__name__
    if name.startswith('get'): name = name[len('get'):]
    content = funct()
    
    fh = file('%s.json' % name, 'w')
    fh.write(jsonify(content))
    fh.close()

def uncachejson(name):
    return json.load(file('%s.json' % name))

PVS_API_KEY = file('.pvsapikey.secret').read().strip()
PVS_URL = 'http://api.votesmart.org/%s?%s&key=%s&o=JSON'
PVS_EMPTY = [
  'No districts found fitting this criteria.',
  'No candidates found in this district.',
  'No active officials found in this district.',
  'No candidates for this ID or no additional bio data.',
  'No categories for this state and year.',
  'No Ratings fit this criteria.',
  'Campaign address no longer available or candidate does not exist.',
  'Office address no longer available or candidate does not exist.',
  'Office Web address no longer available or candidate does not exist.',
  'Campaign Web address no longer available or candidate does not exist.']

def pvs(cmd, **attrs):
    u = PVS_URL % (cmd, urllib.urlencode(attrs), PVS_API_KEY)
    d = urllib.urlopen(u)
    return json.load(d)

def pvsexists(d):
    if d.get('error') and d['error']['errorMessage'] in PVS_EMPTY:
        return False
    else:
        return d

def getstates():
    out = {}
    for state in pvs('State.getStateIDs')['stateList']['list']['state']:
        out[state.pop('stateId')] = state
    return out

# cachejson(getstates)

def getdistricts():
    out = {}
    for state in uncachejson('states').keys():
        d = pvs('District.getByOfficeState', officeId=5, stateId=state)
        if pvsexists(d):
            for x in listify(d['districtList']['district']):
                assert x.pop('officeId') == '5'
                cdist = x.pop('stateId') + '-' 
                cdist2 = x['name'].split(' ')[-1]
                if cdist2.isdigit():
                    cdist += cdist2.zfill(2)
                elif cdist2 in ['At-Large', 'Delegate', 'Commissioner']:
                    cdist += '00'
                else:
                    print d
                    raise ValueError, cdist2
                out[cdist] = x

        d = pvs('District.getByOfficeState', officeId=6, stateId=state)
        if pvsexists(d):
            for x in listify(d['districtList']['district']):
                assert x.pop('officeId') == '6'
                cdist = x.pop('stateId') + '-'
                if x['name'].startswith('Senior'):
                    cdist += 'SEN1'
                else:
                    cdist += 'SEN2'
                out[cdist] = x
    return out

def getcandidates():
    out = {}
    db = uncachejson('districts')
    for districtid, district in db.iteritems():
        sys.stderr.write('\r' + districtid.ljust(10))
        sys.stderr.flush()
        d = pvs('Officials.getByDistrict', districtId=district['districtId'])
        if pvsexists(d):
            for c in listify(d['candidateList']['candidate']):
                out.setdefault(districtid, [])
                out[districtid].append(c)

        d = pvs('Candidates.getByDistrict', districtId=district['districtId'])
        if pvsexists(d):
            for c in listify(d['candidateList']['candidate']):
                out.setdefault(districtid, [])
                out[districtid].append(c)
    return out

def getbios():
    out = {}
    for district in uncachejson('candidates').itervalues():
        for can in district:
            if can['candidateId'] in out: continue
            print can['firstName'], can['lastName']
            d = pvs('CandidateBio.getBio', candidateId=can['candidateId'])
            d2 = pvs('CandidateBio.getAddlBio', candidateId=can['candidateId'])
            if pvsexists(d2) and d2['addlbio']['additional']:
                d['bio']['additional'] = listify(d2['addlbio']['additional']['item'])
            out[can['candidateId']] = d['bio']
    return out

def getratingcategories():
    out = {}
    for state in uncachejson('states').keys():
        d = pvs('Rating.getCategories', stateId=state)
        if pvsexists(d):
            out[state] = listify(d['categories']['category'])
    return out

def getsigcategories():
    out = {}
    for state, cats in uncachejson('ratingcategories').iteritems():
        for cat in cats:
            d = pvs('Rating.getSigList', categoryId=cat['categoryId'], stateId=state)
            for sig in listify(d['sigs']['sig']):
                sig['categoryId'] = cat['categoryId']
                sig['stateId'] = state
                out[sig['sigId']] = sig
    return out

def getsigs():
    out = {}
    for sig in uncachejson('sigcategories').itervalues():
        if sig['sigId'] in out: continue
        d = pvs('Rating.getSig', sigId=sig['sigId'])
        del d['sig']['generalInfo']
        out[sig['sigId']] = d['sig']
    return out

def getratings():
    sigs = [x for x in uncachejson('sigs').values() if x['stateId'] == 'NA']
    cans = sum(uncachejson('candidates').values(), [])
    
    fh = file('ratings.json.netstrings', 'w')
    
    n = 0
    total = len(sigs) * len(cans)
    for can in cans:
        for sig in sigs:
            n += 1
            sys.stderr.write('\r%s / %s = %s' % (n, total, float(n)/total))
            sys.stderr.flush()
            d = pvs('Rating.getCandidateRating', candidateId=can['candidateId'], sigId=sig['sigId'])
            if pvsexists(d):
                out = jsonify({
                  'sigId': sig['sigId'], 
                  'candidateId': can['candidateId'],
                  'ratings': listify(d['candidateRating']['rating'])
                })
                
                fh.write('%s:%s' % (len(out), out))
        fh.flush()
    return out

def parsenetstrings(fh):
    while True:
        n = []
        n.append(fh.read(1))
        if n[-1] == '': break # we're done
        while n[-1].isdigit():
            n.append(fh.read(1))
        assert n[-1] == ':'
        yield fh.read(int(''.join(n[:-1])))

def parseratings():
    for string in parsenetstrings(file('ratings.json.netstrings')):
        yield json.loads(string)

def getoffices():
    out = {}
    cans = sum(uncachejson('candidates').values(), [])
    
    for n, can in enumerate(cans):
        d = pvs('Address.getOffice', candidateId=can['candidateId'])
        d2 = pvs('Address.getCampaign', candidateId=can['candidateId'])
        sys.stderr.write('\r%s / %s = %s' % (n, len(cans), float(n)/len(cans)))
        sys.stderr.flush()
        o = out[can['candidateId']] = {}
        if pvsexists(d):
            o['office'] = listify(d['address']['office'])
        if pvsexists(d2):
            o['campaign'] = listify(d2['address']['office'])
    return out

def getwebsites():
    out = {}
    
    cans = sum(uncachejson('candidates').values(), [])
    for n, can in enumerate(cans):
        d = pvs('Address.getOfficeWebAddress', candidateId=can['candidateId'])
        d2 = pvs('Address.getCampaignWebAddress', candidateId=can['candidateId'])
        sys.stderr.write('\r%s / %s = %s' % (n, len(cans), float(n)/len(cans)))
        sys.stderr.flush()
        o = out[can['candidateId']] = {}
        if pvsexists(d):
            o['office'] = listify(d['webaddress']['address'])
        if pvsexists(d2):
            o['campaign'] = listify(d2['webaddress']['address'])
    return out

def getnpat():
    out = {}
    cans = sum(uncachejson('candidates').values(), [])
    
    for can in cans:
        d = pvs('Npat.getNpat', candidateId=can['candidateId'])
        if d.get('error'):
            if d['error']['errorMessage'] == 'Unknown error': continue            
            print d
            continue
        out[can['candidateId']] = d
    
    return out

def webproxy():
    import web, pprint
    urls = ('/(.*)', 'main')
    class main:
        def GET(self, url):
            pprint.pprint(pvs(url, **web.input()))

    web.run(urls, locals())

if __name__ == "__main__":
    cachejson(getstates)
    cachejson(getdistricts)
    cachejson(getcandidates)
    cachejson(getbios)
    cachejson(getoffices)
    cachejson(getwebsites)
    cachejson(getnpat)

########NEW FILE########
__FILENAME__ = wyr
import sys
try:
    import json
except ImportError:
    import simplejson as json
from ClientForm import ParseResponse, ControlNotFoundError, AmbiguityError
from settings import db
from BeautifulSoup import BeautifulSoup
import urllib2
from urlparse import urljoin

manual_websites = '../import/load/manual/websites.json'
votesmart_websites = '../data/crawl/votesmart/111/websites.json'
WYR_URL = 'https://writerep.house.gov/writerep/welcome.shtml'
OLD_WYR_URL = 'https://forms.house.gov/wyr/welcome.shtml'

def is_email(s):
    return ('@' in s) and not s.startswith('http://')
    
def has_textarea(f):
    try:
        c = f.find_control(type='textarea')
    except ControlNotFoundError:
        return False
    except AmbiguityError:  #more than 1 textarea
        return True
    else:
        return True    

def urlopen(u, data=None):
    try:
        return urllib2.urlopen(u, data)
    except Exception, details:
        pass

def pol2dist(pol):
    try:
        return db.select('curr_politician', what='district_id', where='curr_politician.id=$pol', vars=locals())[0].district_id
    except KeyError:
        return

def has(f, s):
    for c in f.controls:
        if c.name and s in c.name.lower():
            return True
    return False
            
def has_zipauth(f):
    return has(f, 'zip')
    
def has_captcha(url):
    import re
    try:
        response = urlopen(url)
        soup = BeautifulSoup(response)
    except Exception, details:
        print >> sys.stderr, url, details
        return False
    else:        
        return bool(soup.findAll('img', attrs={"src": re.compile(".*[Cc]aptcha.*")}))

def any_zipauth(forms):
    return any(has_zipauth(f) for f in forms)
    
def any_ima(forms):
    return any(has_textarea(f) for f in forms)                   
    
def is_wyr(s):
    return bool(s) and ('www.house.gov/writerep' in s) or (WYR_URL == s) or (OLD_WYR_URL == s)

def getdistzipdict(zipdump):
    """returns a dict with district names as keys zipcodes falling in it as values"""
    d = {}
    for line in zipdump.strip().split('\n'):
        zip5, zip4, district = line.split('\t')
        d[district] = (zip5, zip4)
    return d

dist_zip_dict =  getdistzipdict(file('../utils/zip_per_dist.tsv').read())

def getzip(dist):
    try:
        return dist_zip_dict[dist]
    except:
        return '', ''    

def has_wyr(pol):
    dist = pol2dist(pol)
    return has_wyr_dist(dist)

def has_wyr_dist(dist):
    if len(dist) == 2: return False # senators don't have forms in WYR system

    try:
        response = urlopen(WYR_URL)
        form = ParseResponse(response, backwards_compat=False)[2] #1st form is of search, 2nd is findrep
    except:
        return False

    state = dist[:2]
    state_options = form.find_control(name='state').items
    state_l = [s.name for s in state_options if s.name[:2] == state]
    zip5, zip4 = getzip(dist)

    form['state'] = state_l
    form['zip'] = zip5
    form['zip4'] = zip4
    request = form.click()
    return request.get_full_url()
    try:
        response = urlopen(request.get_full_url(), request.get_data())
        forms = ParseResponse(response, backwards_compat=False)
    except:
        return False
    else:
        return len(forms) == 2       

def get_link_with(links, lstr):
    """
    Returns link with any of the `lstr` in the link or its text, if exists.
    """
    if not isinstance(lstr, list): lstr = [lstr]

    for s in lstr:
        for link in links:
            href = link.get('href', '') or link.get('HREF', '')
            text = link.contents and str(link.contents[0]) or ''
            if (s in href.lower()) or (s in text.lower()):
                 return href

def get_contact_page(url):
    """
    Takes home page of politician and returns the url for contact page.
        >>> get_contact_page("http://kanjorski.house.gov/")
        u'http://kanjorski.house.gov/index.php?option=com_content&task=view&id=37&Itemid=13'
        >>> get_contact_page("http://flake.house.gov/")
        u'http://flake.house.gov/Contact/'
        >>> get_contact_page("http://www.house.gov/blackburn")
        u'http://blackburn.house.gov/Contact/'
        >>> get_contact_page("http://www.house.gov/olver")
        u'http://www.house.gov/olver/contactme.html'
    """
    response = urlopen(url)

    try:
        soup = BeautifulSoup(response)
    except:
        return ''
    links = soup.findAll({'a': True, 'area':True})

    if len(links) == 1: return get_contact_page(links[0].get('href', ''))

    contact_link = get_link_with(links, ['contact', 'email'])
    if contact_link:
        return urljoin(response.geturl(), contact_link)

def get_contactform_link(url):
    _url, formtype = getformtype(url)
    if formtype: return _url, formtype

    response = urlopen(url)
    try:
        soup = BeautifulSoup(response)
    except Exception, details:
        print url, 'failed', details, 'get_contactform_link'
        return None, None

    links = soup.findAll({'a': True})
    ima_link = get_link_with(links, ['ima/', 'issues_subscribe', 'issue_subscribe'])
    url, formtype = getformtype(urljoin(response.geturl(), ima_link))
    if formtype: return url, formtype

    zipauth_link = get_link_with(links, ['zipauth', 'zip_auth'])
    url, formtype = getformtype(urljoin(response.geturl(), zipauth_link))
    if formtype: return url, formtype

    return None, None

def get_wyr_forms(pols):
    d = {}
    for pol in filter(has_wyr, pols):
        d[pol] = dict(contact=WYR_URL, contacttype='wyr', captcha=False)
    return d

def getformtype(url):
    """
    In the given url, checks for existence of 'ima' or 'zipauth' forms. If neither of them is there,
    it looks of for a one in frames, if any.
    Returns the form type, if there is a one.
    """    
    if is_wyr(url):
        return WYR_URL, 'wyr'
    
    try:
        response = urlopen(url)
        forms = ParseResponse(response, backwards_compat=False)
    except Exception, details:
        #print >> sys.stderr, url, details
        pass
    else:
        formtype = (any_ima(forms) and 'ima') or (any_zipauth(forms) and 'zipauth')
        if formtype:
            return response.geturl(), formtype 
        else:
            try:
                soup = BeautifulSoup(urlopen(url))
            except:
                pass
            else:
                frame = soup.findAll(['iframe', 'frame'])
                if frame: 
                    frameurl = frame[0].get('src')
                    return getformtype(urljoin(url, frameurl))
    return None, None
        
def get_votesmart_contacts(pols):
    d = {}
    websites = json.load(file(votesmart_websites))
    pols = tuple(pols)
    rs = db.select('curr_politician', what='id, votesmartid', where='id in $pols', vars=locals())
    
    for r in rs:
        _url, contacttype = None, None
        contact = websites.get(r.votesmartid, {})
        if 'office' in contact.keys():
            for addr in contact['office']:
                if addr['webAddressType'] == 'Email':
                    email = addr['webAddress']
                    if email.endswith('mail.house.gov') or email.endswith('mail.senate.gov'):
                        _url, contacttype = 'mailto:' + email, 'email'
                if not _url and addr['webAddressType'] == 'Webmail':
                    url = addr['webAddress']
                    _url, contacttype = getformtype(url)
                        
            if contacttype and contacttype != 'wyr':
                captcha = (contacttype == 'ima') and has_captcha(url)
                d[r.id] = dict(contact=_url, contacttype=contacttype, captcha=captcha)    
    return d

def get_from_officeurls(pols):
    # look for link to contact form on the home page, if not proceed to contact page and look there for a form
    d = {}
    rs = db.select('curr_politician', what='id, officeurl', where='id in $tuple(pols)', vars=locals())
    for r in rs:
        link, contacttype = get_contactform_link(r.officeurl)
        if not contacttype:
            contact_page = get_contact_page(r.officeurl)
            if contact_page:
                link, contacttype = get_contactform_link(contact_page)

        if contacttype and contacttype != 'wyr':
            captcha = (contacttype == 'ima') and has_captcha(link)
            d[r.id] = dict(contact=link, contacttype=contacttype, captcha=captcha)
    return d

def get_manual_contacts(pols):
    d = {}
    items = json.load(file(manual_websites))
    for pol in pols:
        url = items.get(pol, dict(contact=''))['contact']
        if url:
            if is_email(url):
                _url, contacttype = 'mailto:' + url, 'email'
            else:
                _url, contacttype = getformtype(url)
            if contacttype and contacttype != 'wyr':
                captcha = (contacttype == 'ima') and has_captcha(_url)
                d[pol] = dict(contact=_url, contacttype=contacttype, captcha=captcha)
    return d
        
def main(fname='../data/crawl/votesmart/wyr.json'):
    #@@@ PVS data has few false positives for WYR form. 
    # So, better get reps having wyr forms in house.gov and then proceed to PVS data and then to manually created json
    all_pols = [r.id for r in db.select('curr_politician', what='id')]

    print 'total pols', len(all_pols)
    d = get_wyr_forms(all_pols)
    remaining_pols = [p for p in all_pols if p not in d.keys()]
    print 'after checking wyr forms, remaining:', len(remaining_pols)

    d.update(get_votesmart_contacts(remaining_pols or ['']))
    remaining_pols = [p for p in all_pols if p not in d.keys()]
    print 'after checking votesmart contacts, remaining:', len(remaining_pols)

    d.update(get_from_officeurls(remaining_pols or ['']))
    remaining_pols = [p for p in all_pols if p not in d.keys()]
    print 'after getting contacts from officeurls, remaining:', len(remaining_pols)
    
    d.update(get_manual_contacts(all_pols))
    remaining_pols = [p for p in all_pols if p not in d.keys()]
    print 'after checking manual contacts, remaining:', len(remaining_pols)
    
    f = file(fname, 'w')
    json.dump(d, f, indent=2, sort_keys=True)                     

def main2():
    for dist in dist_zip_dict:
        print dist, has_wyr_dist(dist)

if __name__ == '__main__':
    main2()

########NEW FILE########
__FILENAME__ = almanac
#!/usr/bin/python
from __future__ import with_statement
import glob, web, os, cgitb, sys
import tools
from settings import db
from parse import almanac
sys.excepthook = cgitb.Hook(format='text', file=sys.stderr)

DATA_DIR = '../data'
ALMANAC_DIR = DATA_DIR + '/crawl/almanac/nationaljournal.com/pubs/almanac/'

def cleanint(n):
    for c in ', %$':
        n = n.replace(c, '')
    return n

def get_int(dict, key):
    return cleanint(dict.get(key, '')) or None

def coalesce_population(data, fields):
    for year, key in fields:
        pop = get_int(data, key)
        if pop is not None: return (year, pop)
    return (None, None)

def load_into_db(pname, distname, electionresults, recent_election_year):
    #load the details of the winner in recent election results into `politician` table
    #and all the details of elections in district `distname` into the `past_elections` table

    with db.transaction():
        for r in electionresults:
            candidate_id = r.candidate.split('(')[0].strip().lower()

            if r.year == recent_election_year and r.type == 'Gen' and pname in candidate_id:
                polid = db.update('politician',
                    where="district_id=$distname AND %s" % (web.sqlors('lastname ilike ', pname.split(' '))),
                    n_vote_received=r.votes,
                    pct_vote_received=r.vote_pct,
                    last_elected_year=r.year, vars=locals())

            candidate_id = candidate_id.replace(' ', '_')
            if not db.select('past_elections',
                            where='politician_id=$candidate_id and district_id=$distname '
                                    'and year=$r.year and type=$r.type',
                            vars=locals()):
                db.insert('past_elections', seqname=False, politician_id=candidate_id, district_id=distname,
                    votes_received=r.votes, pct_votes_received=r.vote_pct, type=r.type,
                    year=r.year, expenditure=r.expenditure)

def validate(d, distname):
    if 'name' not in d:
        print "No name for the congress person for: ", distname
    elif 'electionresults' not in d: 
        print "No election results for %s repsenting %s." % (d['name'], distname)
    else:
        return d

def process(d):
    election_results = []
    pname = d['name'].lower()
    for e in d['electionresults']:
        if 'candidate' in e and 'primary' not in e['election'] and \
                'prior' not in e['election'].lower() and 'prior' not in e['candidate']:
            r = web.storage(candidate=e['candidate'])
            r.year =  int(e['election'][0:4])
            r.votes = e['totalvotes'].replace(',','').lower().replace('unopposed','0')
            r.vote_pct = 100 if e['totalvotes'].lower() == 'unopposed' else e.get('percent', '0').replace('%', '')
            r.expenditure = e.get('expenditures', '0').lstrip('$').replace(',', '')
            r.type = 'SpGen' if 'special-general' in e['election'] else 'Gen'
            election_results.append(r)
    return election_results
    
def load_election_results(d, distname):    
    d = validate(d, distname)
    if not d: return
    pname = d['name'].lower()
    election_results = process(d)
    if not election_results:
        print "Didn't find a recent election for %s representing %s." %(d['name'], distname)
        return

    recent_election_year = max([e.year for e in election_results])
    load_into_db(pname, distname, election_results, recent_election_year)

def demog_to_dist(demog, district):
    if demog:
        district.cook_index = get_int(demog, 'Cook Partisan Voting Index')
        district.area_sqmi = cleanint(web.rstrips(web.rstrips(demog['Area size'], ' sq. mi.'), ' square miles'))
        district.poverty_pct = get_int(demog, 'Poverty status') or get_int(demog, 'Poverty status') 
        district.median_income = get_int(demog, 'Median income') or get_int(demog, 'Median Income') 
        (district.est_population_year,
         district.est_population) = coalesce_population(demog, [
            (2006, 'Pop. 2006 (est)'),
            (2005, 'Pop. 2005 (est)'),
            (2000, 'Pop. 2000'),
            (2006, 'Population 2006 (est)'),
            (2005, 'Population 2005 (est)'),
            (2000, 'Population 2000'),
        ])


def main():
    assert os.path.exists(ALMANAC_DIR), ALMANAC_DIR
    
    files = glob.glob(ALMANAC_DIR + '*/people/*/rep_*.htm') + \
            glob.glob(ALMANAC_DIR + '*/people/*/*s[12].htm')
    files.sort()
    for fn in files:
        district = web.storage()
        demog = None
        
        dist = web.lstrips(web.rstrips(fn.split('/')[-1], '.htm'), 'rep_')
        diststate = dist[0:2].upper()
        distnum = dist[-2:]
        distname = tools.fixdist(diststate + '-' + distnum)
        
        d = almanac.scrape_person(fn)
        load_election_results(d, distname)

        if ALMANAC_DIR + '2008' in fn:
            if 'demographics' in d:
                demog = d['demographics']
            elif distname[-2:] == '00' or '-' not in distname:   # if -00 then this district is the same as the state.
                #print "Using state file for:", distname
                statefile = ALMANAC_DIR + '2008/states/%s/index.html' % diststate.lower()
                demog = almanac.scrape_state(statefile).get('state')

            demog_to_dist(demog, district)

            district.almanac = 'http://' + d['filename'][d['filename'].find('nationaljournal.com'):]

            #print 'district:', distname, pformat(district)
            db.update('district', where='name=$distname', vars=locals(), **district)

if __name__ == '__main__': main()

########NEW FILE########
__FILENAME__ = bills
"""
load bill data

from: data/crawl/govtrack/us/*/{bills,rolls}
"""
from __future__ import with_statement
import os, sys, glob, anydbm
from psycopg2 import IntegrityError #@@level-breaker
import xmltramp
import web
from tools import govtrackp
from settings import db

DATA_DIR = '../data'
GOVTRACK_CRAWL = DATA_DIR+'/crawl/govtrack'

class NotDone(Exception): pass

def makemarkdone(done):
    def markdone(func):
        def internal(fn, *a, **kw):
            mtime = str(os.stat(fn).st_mtime)
            if not done.has_key(fn) or done[fn] != mtime:
                    try:
                        func(fn)
                        done[fn] = mtime
                    except NotDone:
                        pass
        return internal
    return markdone

def fixvote(s):
    try: return {'0': None, '+': 1, '-': -1, 'P': 0}[s]
    except: return None

def bill2dict(bill):
    d = web.storage()
    d.id = 'us/%s/%s%s' % (bill('session'), bill('type'), bill('number'))
    d.session = bill('session')
    d.type = bill('type')
    d.number = bill('number')
    d.introduced = bill.introduced('datetime')
    titles = [unicode(x) for x in bill.titles['title':] 
      if x('type') == 'short']
    if not titles:
        titles = [unicode(x) for x in bill.titles['title':]]
    d.title = titles[0]

    summaries = [unicode(x) for x in bill.titles['title':] 
      if x('type') == 'official']
    if not summaries:
        summaries = [unicode(x) for x in bill.titles['title':]]
    d.summary = summaries[0]
    
    d.sponsor_id = govtrackp(bill.sponsor().get('id'))
    return d

def loadbill(fn, maplightid=None):
    bill = xmltramp.load(fn)
    d = bill2dict(bill)
    d.maplightid = maplightid
    
    try:
        bill_id = d.id
        db.insert('bill', seqname=False, **d)
    except IntegrityError:
        bill_id = d.pop('id')
        db.update('bill', where="id=" + web.sqlquote(bill_id), **d)
    
    positions = {}
    for vote in bill.actions['vote':]:
        if not vote().get('roll'): continue
        
        rolldoc = '/us/%s/rolls/%s%s-%s.xml' % (
          d.session, vote('where'), vote('datetime')[:4], vote('roll'))
        roll = xmltramp.load(GOVTRACK_CRAWL + rolldoc)
        for voter in roll['voter':]:
            positions[govtrackp(voter('id'))] = fixvote(voter('vote'))

    if None in positions: del positions[None]
    with db.transaction():
        db.delete('position', where='bill_id=$bill_id', vars=locals())
        for p, v in positions.iteritems():
            db.insert('position', seqname=False, 
              bill_id=bill_id, politician_id=p, vote=v)
        

def loadroll(fn):
    roll = web.storage()
    roll.id = fn.split('/')[-1].split('.')[0]
    vote = xmltramp.load(fn)
    if vote['bill':]:
        b = vote.bill
        roll.bill_id = 'us/%s/%s%s' % (b('session'), b('type'), b('number'))
    else:
        roll.bill_id = None
    roll.type = str(vote.type)
    roll.question = str(vote.question)
    roll.required = str(vote.required)
    roll.result = str(vote.result)
    
    try:
        db.insert('roll', seqname=False, **roll)
    except IntegrityError:
        if not db.update('roll', where="id=" + web.sqlquote(roll.id), bill_id=roll.bill_id):
            print "\nMissing bill:", roll.bill_id
            raise NotDone
    
    with db.transaction():
        db.delete('vote', where="roll_id=$roll.id", vars=locals())
        for voter in vote['voter':]:
            rep = govtrackp(voter('id'))
            if rep:
                db.insert('vote', seqname=False, 
                  politician_id=rep, roll_id=roll.id, vote=fixvote(voter('vote')))
            else:
                pass #@@!--check again after load_everyone
                # print "\nMissing rep: %s" % voter('id')

def main(session):
    done = anydbm.open('.bills', 'c')
    markdone = makemarkdone(done)
        
    for fn in sorted(glob.glob(GOVTRACK_CRAWL+'/us/%s/bills/*.xml' % session)):
        print >>sys.stderr,'\r  %-25s' % fn,; sys.stderr.flush()
        markdone(loadbill)(fn)
    
    for fn in sorted(glob.glob(GOVTRACK_CRAWL+'/us/%s/rolls/*.xml' % session)):
        print >>sys.stderr,'\r  %-25s' % fn,; sys.stderr.flush()
        markdone(loadroll)(fn)
    print >>sys.stderr, '\r' + ' '*72

if __name__ == "__main__":
    from settings import current_session
    if current_session:
	    session = current_session # load nth session bills and rolls, specified in settings
    else:
	    session = '*' 	# load all bills and rolls sessions

    main(session)

########NEW FILE########
__FILENAME__ = bulk_loader
import os
import codecs
from types import NoneType

class bulk_loader_db:
    tables = {}
    db_name = None
    def __init__(self, db_name):
        self.db_name = db_name
    def open_table(self, table, columns, delete_first=False, filename=None):
        if table in self.tables: raise 'Table %s already open' % table
        self.tables[table] = bulk_loader(self.db_name, table, columns, delete_first, filename)
    def insert(self, table, seqname=None, _test=False, **values):
        if table not in self.tables: raise 'Table %s not open.' % table
        return self.tables[table].insert(table, seqname, _test, **values)

_text_encoding = 'latin-1'
#_text_encoding = 'utf-8'
class bulk_loader:
    """Opens a file/pipe and writes tsv."""
    hdr = "SET client_encoding = '%s';\nBEGIN;\n\n" % _text_encoding
    footer = "\.\n\nCOMMIT;\n\n"
    tsv_filename = psql_pipe = psql_out = use_file = cols = table = None
    def __init__(self, database_name, table, columns, delete_first=False, filename=None):
        """If filename is passed then write the tsv file. Otherwise pipe is opened to postgres."""
        self.use_file = self.tsv_filename = filename
        self.cols = columns
        self.table = table
        if not self.use_file:
            ## FIXME: better way to generate a unique name??
            self.tsv_filename = '/tmp/wd_loader.%s.%d.fifo' % (table, os.getpid())
            os.mkfifo(self.tsv_filename)
            # Open the psql end of pipe
            self.psql_pipe = os.popen('psql %s -f %s' % 
                    (database_name, 
                        self.tsv_filename), 'w')
        # Open our end of pipe
        self.psql_out = codecs.open(self.tsv_filename, 'w', _text_encoding)
        if delete_first:
            self.hdr += "DELETE from %s where 1=1;\n" % table
        copy_cmd = "COPY %s (%s) FROM stdin;" % (table, ', '.join(columns))
        print >>self.psql_out, self.hdr
        print >>self.psql_out, copy_cmd
    def insert(self, tablename, seqname=None, _test=False, **values): 
        assert(tablename == self.table)
        self.add_row(map(values.get, self.cols))
    def add_row(self, columns):
        def convert(x):
            if isinstance(x, NoneType):
                x = '\\N'
            if not isinstance(x, basestring):
                x = str(x)
            if not isinstance(x, unicode):
                x = unicode(x, _text_encoding)
            return x
        line = u'\t'.join([convert(x) for x in columns])
        print >>self.psql_out, line
    def __del__(self):
        if self.psql_out: 
            print >>self.psql_out, self.footer
            self.psql_out.close()
        if not self.use_file and self.tsv_filename:
            os.remove(self.tsv_filename)


########NEW FILE########
__FILENAME__ = census
from __future__ import with_statement
import os, sys
from pprint import pprint, pformat

import web

from parse import census

batch_mode = True
DATA_DIR='../data/'
tsv_file_format = DATA_DIR+'load/%s.tsv'


def load_census_meta(type):
    print >>sys.stderr, "Loading census_meta table..."
    str_cols = set(['FILEID', 'STUSAB', 'CHARITER', 'CIFSN', 'LOGRECNO',])
    all_keys = {}
    # Build list
    labelMap = {}
    for table in census.ALL_TABLES[type]:
        (_,lm,pathMap) = census.parse_sas_file(type, table, all_keys)
        labelMap.update(lm)

    # Now insert
    for hr_key, int_keys in all_keys.items():
        for k in int_keys:
            if k in str_cols: continue
            db.insert('census_meta', 
                    seqname=False,
                    internal_key=k,
                    census_type=type,
                    label=labelMap[k].replace('\t','    '),
                    hr_key=hr_key)
    print >>sys.stderr, "...Done loading census_meta table."


def load_census_population():
    print >>sys.stderr, "Loading census_population table..."
    #db.delete('census_population', where='1=1')
    geoTables = {}
    # Load the population data from SF101.
    required_geo_keys = set(['LOGRECNO', 'SUMLEV', 'STATE', 'CD110', 'COUNTY',
        'BLKGRP', 'BLOCK', 'TRACT', 'ZCTA5', 'AREALAND'])
    desired_sumlevs = set(['STATE', 'COUNTY', 'DISTS', 'ZCTA', 'TRACT',
        'BLOCK'])
    for row in census.parse_state_sum_files([1], ['P002001']):
        (layout, logrecno, fileid, stusab, chariter, cifsn, t, geo_file) = map(row.pop, 
                ['layout', 'LOGRECNO', 'FILEID', 'STUSAB', 'CHARITER', 'CIFSN', 'type', 'geo_file'])
        (geo_file, geo_args) = geo_file
        logrecno = int(logrecno)
        geo = None
        if geo_file not in geoTables:
            geoTables = dict()
            geoTables[geo_file] = dict([ (lrn, dict([(k,d[k]) for k in filter(lambda x: x in required_geo_keys, d.keys())])) for lrn,d in census.build_geo_table(geo_file, geo_args).items() ])
        geo = geoTables[geo_file]
        if logrecno not in geo: continue
        if geo[logrecno]['SUMLEV'] in desired_sumlevs and 'P002001' in row:
            if geo[logrecno]['SUMLEV'] == 'DISTS' and \
                    not geo[logrecno]['CD110']: 
                        continue
            #print "inserting", geo[logrecno]['SUMLEV']
            db.insert('census_population', 
                    state_id = geo[logrecno]['STATE'],
                    county_id = geo[logrecno]['COUNTY'],
                    blockgrp_id = geo[logrecno]['BLKGRP'],
                    block_id = geo[logrecno]['BLOCK'],
                    district_id = geo[logrecno]['CD110'],
                    zip_id = geo[logrecno]['ZCTA5'],
                    sumlev = geo[logrecno]['SUMLEV'],
                    tract_id = geo[logrecno]['TRACT'],
                    area_land = geo[logrecno]['AREALAND'],
                    population = row['P002001'])
        #else: print "oops", geo[logrecno]['SUMLEV']
    print >>sys.stderr, "...Done loading census_population table."


def load_census_data(type):
    print >>sys.stderr, "Loading census_data table..."
    geoTables = {}
    for row in census.parse_sum_files([type]): #,requesting_keys[type]):
        (layout, logrecno, fileid, stusab, chariter, cifsn, t, geo_file) = map(row.pop, \
                ['layout', 'LOGRECNO', 'FILEID', 'STUSAB', 'CHARITER', 'CIFSN', 'type', 'geo_file'])
        (geo_file, geo_args) = geo_file
        logrecno = int(logrecno)
        if geo_file not in geoTables:
            reqed_keys = set(['LOGRECNO','SUMLEV','STATE','CD110'])
            tmp = census.build_geo_table(geo_file, geo_args)
            geoTables[geo_file] = dict([ (lrn, dict([(k,d[k]) for k in filter(lambda x: x in reqed_keys, d.keys())])) for lrn,d in tmp.items()])
        geo = geoTables[geo_file]
        if logrecno in geo:
            ### Entries for states
            loc_code = ''
            if geo[logrecno]['SUMLEV'] == 'STATE':
                loc_code = geo[logrecno]['STATE']
            ### Entries for the 110th Congress.
            elif geo[logrecno]['SUMLEV'] == 'DISTS' \
                    and geo[logrecno]['CD110']:
                loc_code = '%s-%02d' % (geo[logrecno]['STATE'], int(geo[logrecno]['CD110']))
            else: continue

            # Fix some districts:
            if loc_code in ['DC','PR']: loc_code = loc_code+'-00'
            if loc_code in ['DC-98','PR-98']: continue
            for internal_key, value in row.items():
                db.insert('census_data', seqname=False, district_id=loc_code, internal_key=internal_key, census_type=type, value=value)
    print >>sys.stderr, "...Done loading census_data table."

def main():
    for type in [1, 3]:
        load_census_meta(type)
        load_census_data(type)
    load_census_population()

if __name__ == "__main__":
    if batch_mode:
        from bulk_loader import bulk_loader_db
        db = bulk_loader_db(os.environ.get('WATCHDOG_TABLE', 'watchdog_dev'))
        meta_cols = ['internal_key', 'census_type', 'hr_key', 'label']
        db.open_table('census_meta', meta_cols, filename=tsv_file_format%'census_meta')
        data_cols = ['district_id', 'internal_key', 'census_type', 'value']
        db.open_table('census_data', data_cols, filename=tsv_file_format%'census_data')
        pop_cols = ['state_id', 'county_id', 'zip_id', 'tract_id', 'blockgrp_id', 'block_id', 'district_id', 'sumlev', 'population', 'area_land']
        db.open_table('census_population', pop_cols, filename=tsv_file_format%'census_population')
        main()
    else:
        from tools import db
        with db.transaction():
            #db.delete('census_data', where='1=1')
            #db.delete('census_meta', where='1=1')
            #db.delete('census_population', where='1=1')
            main()


########NEW FILE########
__FILENAME__ = earmarks
from __future__ import with_statement

import sys
import web
import tools
from parse import earmarks
import schema
from settings import db
from pprint import pprint, pformat

reps = web.storage((x.id, x) for x in db.select(['politician','congress'], where="politician_id = id AND congress_num='110'").list())
hacks = dict()
lastname2rep = {}

# HACKs: hard-coding naming inconsistencies
hacks['jo_ann_davis'] = 'Davis, Jo Ann'
hacks['g._k._butterfield'] = 'Butterfield.'
hacks['cathy_mcmorris_rodgers'] = 'McMorris Rodger'
lastname2rep['McMorris'.lower()] = 'cathy_mcmorris_rodgers'   # Ugg, she requires multiple hacks.
# ambiguous fixes
hacks['chet_edwards'] = 'Edwards' # I think this is the correct Edwards, donna_edwards has only been a member since june 2008?
# Unusual name fixes (punctuation/spaces)
hacks['bill_young'] = 'C.W. Bill'
hacks['peter_defazio'] = 'De Fazio'
hacks['sheila_jackson-lee'] = 'Jackson Lee'
# Common name fixes
hacks['mike_thompson'] = 'Mike'
hacks['tim_f._murphy'] = 'Timothy'   #Tim is in there as BOTH Tim and Timothy
# Dups
hacks['mike_j._rogers'] = 'Mike (MI)'
hacks['mike_d._rogers'] = 'Mike (AL)'
# Spelling
hacks['corrine_brown'] = 'Corinne'
hacks['rodney_frelinghuysen'] = 'Frelinghuyson'
hacks['earl_blumenauer'] = 'Blumenaucr'
hacks['eni_faleomavaega'] = 'Faleomavaeaga'
hacks['tim_walberg'] = 'Walbergothy'

# Force a few names into ambiguous mode
ambiguous = ['neal', 'taylor', 'jones']

for repid, rep in reps.items():
    if not rep.lastname: continue
    lastname = rep.lastname.lower()
    if lastname in lastname2rep:
        ambiguous.append(lastname)
        del lastname2rep[lastname]
    else:
        lastname2rep[lastname] = repid

for repid, rep in reps.items():
    if not rep.lastname: continue
    lastname=rep.lastname.lower()
    firstname=rep.firstname.lower()
    if lastname in ambiguous:
        lastname2rep[lastname + ', ' + firstname] = repid
        if rep.nickname:
            lastname2rep[lastname + ', ' + rep.nickname.lower()] = repid
        if repid in hacks:
            lastname2rep[lastname + ', ' + hacks[repid].lower()] = repid
    if repid in hacks:
        lastname2rep[hacks[repid].lower()] = repid

def cleanrow(s):
    if isinstance(s, basestring):
        s = s.strip()
        if s == '': s = None
    return s

def load():
    outdb = {}
    done = set()
    with db.transaction():
        db.delete('earmark_sponsor', '1=1')
        db.delete('earmark', '1=1')
        for e in earmarks.parse_file(earmarks.EARMARK_FILE):
            de = dict(e)
            de['id'] = web.intget(de['id'])
            if not de['id'] or de['id'] in done: continue # missing the ID? come on!
            if isinstance(de['house_request'], basestring): continue # CLASSIFIED

            for k in de: de[k] = cleanrow(de[k])
            for x in ['house_member', 'house_state', 'house_party', 'senate_member', 'senate_state', 'senate_party', 'district']:
                de.pop(x)
            
            de['recipient_stem'] = tools.stemcorpname(de['intended_recipient'])
            try:
                db.insert('earmark', seqname=False, **de)
            except:
                pprint(de)
                raise
            done.add(de['id'])
        
    reps_not_found = set()
    for e in earmarks.parse_file(earmarks.EARMARK_FILE):
        for rawRequest, chamber in zip([e.house_request, e.senate_request],[e.house_member, e.senate_member]):
            for rep in chamber:
                if rep.lower() not in lastname2rep:
                    #@@ should work on improving quality
                    reps_not_found.add(rep)
                else:
                    rep = lastname2rep[rep.lower()]
                    if e.id in done: 
                        try:
                            db.insert('earmark_sponsor', seqname=False, earmark_id=e.id, politician_id=rep)
                        except:
                            print "Couldn't add %s as sponsor to earmark %d" %(rep, e.id)
                    outdb.setdefault(rep, {
                      'amt_earmark_requested': 0,
                      'n_earmark_requested': 0,
                      'n_earmark_received': 0,
                      'amt_earmark_received': 0
                    })
                    outdb[rep]['n_earmark_requested'] += 1
                    requested = rawRequest or e.final_amt
                    if not isinstance(requested, float):
                        requested = e.final_amt
                    if requested:
                        outdb[rep]['amt_earmark_requested'] += requested
                    if isinstance(e.final_amt, float) and e.final_amt:
                        outdb[rep]['n_earmark_received'] += 1
                        outdb[rep]['amt_earmark_received'] += e.final_amt
    
    print "Did not find",len(reps_not_found),"reps:", pformat(reps_not_found)
    for rep, d in outdb.iteritems():
        db.update('politician', where='id=$rep', vars=locals(), **d)

def calculate_per_capita():
    """ """
    print "Pre getting all populations per district..."
    pop_dist = {}
    for d in schema.District.select(where='est_population is not null'):
        pop_dist[d.name] = d.est_population

    print "Calculate the per-capita impact of each earmark..."
    pc = {}
    for e in db.select('earmark', what='final_amt, id', order='id asc'):
        done_states = set()
        amount = float(e.final_amt or 0)
        pop = 0
        sponsors = db.query("select district_id, state_id, id from politician, district, earmark_sponsor where politician.district_id = district.name and earmark_sponsor.politician_id = politician.id and earmark_id=$e.id",vars=locals()).list()
        if not sponsors: continue
        # Get the population for each district sponsoring 
        for p in sponsors:
            if p.district_id != p.state_id:
                done_states.add(p.state_id)
                pop += pop_dist.get(p.district_id, 0)
        # Get the population for state sponsoring unless a district has from
        # within state also sponsors.
        for p in sponsors:
            if p.district_id == p.state_id:
                if p.state_id in done_states: continue
                done_states.add(p.state_id)
                pop += pop_dist.get(p.district_id, 0)
        if not pop:
            pc[e.id] = 0.0
        else:
            pc[e.id] = amount / pop
        #print e.id, pc[e.id], amount, pop
    print "Aggregating per-capita impact to districts..."
    for d in schema.District.select():
        if d.name == d.state_id: continue # Don't set for states.
        congress_people = set()
        senators = db.select('curr_politician', where='district_id = $d.state.code', vars=locals())
        if senators:
            congress_people.update(p.id for p in senators)
        politician = db.select('curr_politician', where='district_id = $d.name', vars=locals())
        if politician:
            congress_people.update(p.id for p in politician)
        ems = db.select('earmark_sponsor', what='distinct(earmark_id)', where=web.sqlors('politician_id=',congress_people))
        empc = sum(map(lambda x: pc.get(x, 0.0), set(e.earmark_id for e in ems)))
        #print d.name, empc
        db.update('district',where='name=$d.name',earmark_per_capita=empc, vars=locals())

if __name__ == "__main__": 
    load()
    calculate_per_capita()


########NEW FILE########
__FILENAME__ = fec
"""
FEC data loader.
"""
from __future__ import with_statement
import itertools, datetime
import web
import tools
from tools import db
from parse import fec_cobol, fec_csv, fec_crude_csv
import psycopg2 # @@sigh
import cgitb
import glob

fec2pol = {}
def load_fec_ids():
    with db.transaction():
        db.delete('politician_fec_ids', '1=1')
        fh = iter(file('../data/crawl/opensecrets/FEC_CRP_ID.tsv'))
        header = fh.next()
        for line in fh:
            fec_id, crp_id = line.split()
            if tools.opensecretsp(crp_id):
                fec2pol[fec_id] = tools.opensecretsp(crp_id)
                db.insert('politician_fec_ids',
                  seqname=False,
                  politician_id=tools.opensecretsp(crp_id),
                  fec_id=fec_id)

def load_fec_cans():
    for can in fec_cobol.parse_cansum():
        can = web.storage(can)
        if can.candidate_id in fec2pol:
            pol_id = fec2pol[can.candidate_id]

            total = float(can.total_receipts)
            if total == 0.0:
                print "Oops:", pol_id, total, can.total_receipts, can.total_disbursements, can.contrib_from_candidate, can.total_indiv_contrib, can.contrib_from_other_pc
                continue
            db.update('politician', where='id = $pol_id', vars=locals(),
              money_raised = can.total_receipts,
              pct_spent = can.total_disbursements/total,
              pct_self = can.contrib_from_candidate/total,
              pct_indiv = can.total_indiv_contrib/total,
              pct_pac = can.contrib_from_other_pc/total
            )

def load_fec_committees():
    db.delete('contribution', '1=1')
    db.delete('committee', '1=1')
    for f in fec_cobol.parse_committees(reverse=True):
        f = web.storage(f)
        try:
            db.insert('committee', seqname=False,
              id = f.committee_id,
              name = f.committee_name,
              treasurer = f.treasurer_name,
              street1 = f.street_one,
              street2 = f.street_two,
              city = f.city,
              state = f.state,
              zip = f.zip,
              connected_org_name = f.connected_org_name,
              candidate_id = f.candidate_id,
              type = f.committee_type
            )
        except psycopg2.IntegrityError:
            pass # already imported

def load_fec_contributions():
    t = db.transaction(); n = 0
    db.delete('contribution', '1=1')
    for f in fec_cobol.parse_contributions():
        f = web.storage(f)
        f.occupation = f.occupation.replace('N/A', '')
        if '/' in f.occupation:
            employer, occupation = f.occupation.split('/', 1)
        else:
            employer = ''
            occupation = f.occupation
        
        try:
            datetime.date(*[int(x) for x in f.date.split('-')])
        except ValueError:
            f.date = None
        
        db.insert('contribution',
          fec_record_id = f.get('fec_record_id'),
          microfilm_loc = f.microfilm_loc,
          recipient_id = f.filer_id,
          name = f.name,
          street = f.get('street'),
          city = f.city,
          state = f.state,
          zip = f.zip,
          occupation = occupation,
          employer = employer,
          employer_stem = tools.stemcorpname(employer),
          committee = f.from_id or None,
          sent = f.date,
          amount = f.amount
        )
        n += 1
        if n % 10000 == 0: t.commit(); t = db.transaction(); print n
    t.commit()

def load_fec_efilings(filepattern=fec_crude_csv.DEFAULT_EFILINGS_FILEPATTERN):
    for f, schedules in fec_crude_csv.parse_efilings(glob.glob(filepattern)):
        for s in schedules:
            if s.get('type') == 'contribution':
                # XXX all this code for politician_id is currently
                # dead, does nothing useful
                politician_id = None
                if f.get('candidate_fec_id'):
                    fec_id = f['candidate_fec_id']
                    pol_fec_id = list(db.select('politician_fec_ids',
                                                where='fec_id=$fec_id',
                                                vars=locals()))
                    if pol_fec_id and len(pol_fec_id) == 1:
                        politician_id = pol_fec_id[0].politician_id
                elif not politician_id and f.get('candidate'):
                    names = f['candidate'].split(' ')
                    fn, ln = names[0], names[-1]
                    pol = list(db.select('politician',
                                        where='lastname=$ln and firstname=$fn',
                                        vars=locals()))
                    if pol and len(pol) == 1:
                        politician_id = pol[0].id
                db.insert('contribution',
                          committee=f['committee'],
                          contrib_date=s['date'],
                          contributor_org=s.get('contributor_org'),
                          contributor=s['contributor'],
                          occupation=s['occupation'],
                          employer=s['employer'],
                          employer_stem=tools.stemcorpname(s['employer']),
                          candidate_name=f.get('candidate'),
                          filer_id=f['filer_id'],
                          report_id=f['report_id'],
                          amount=s['amount'])
            elif s.get('type') == 'expenditure':
                db.insert('expenditure',
                          candidate_name=f.get('candidate'),
                          committee=f['committee'],
                          expenditure_date=s['date'],
                          recipient=s['recipient'],
                          filer_id=f['filer_id'],
                          report_id=f['report_id'],
                          amount=s['amount'])
            else:
                print "ignoring record of type %s" % \
                      s['original_data'].get('form_type')

def load_cans_fec_data():
    """Calculate percentage from business versus labor PACs
    Calculate percentage money within-state
    Calculate percentage money from small donors """
    for p in db.query("""SELECT id FROM politician"""):
        polid = p.id
        num = db.query("""SELECT count(*) 
                FROM committee cm, politician_fec_ids pfi, 
                politician p, contribution cn WHERE cn.recipient_id = cm.id 
                AND cm.candidate_id = pfi.fec_id AND pfi.politician_id = p.id 
                AND p.id = $polid""", vars=locals())[0].count
        if num:
            num_labor = db.query("""SELECT count(*) 
                FROM committee cm, politician_fec_ids pfi, politician p, contribution cn 
                WHERE cn.recipient_id = cm.id AND cm.candidate_id = pfi.fec_id 
                AND pfi.politician_id = p.id AND cm.type = 'L' 
                AND p.id = $polid""", vars=locals())[0].count
            labor_pct = num_labor/float(num)
            num_instate = db.query("""SELECT count(*) 
                FROM committee cm, politician_fec_ids pfi, politician p, 
                contribution cn, district d, state s
                WHERE cn.recipient_id = cm.id AND cm.candidate_id = pfi.fec_id 
                AND pfi.politician_id = p.id AND p.district_id = d.name 
                AND d.state_id = s.code AND lower(cn.state) = lower(s.code)
                AND p.id = $polid""", vars=locals())[0].count
            instate_pct = num_instate/float(num)
            num_smalldonor = db.query("""SELECT count(*) 
                FROM committee cm, politician_fec_ids pfi, politician p, contribution cn 
                WHERE cn.recipient_id = cm.id AND cm.candidate_id = pfi.fec_id 
                AND pfi.politician_id = p.id AND p.id = $polid 
                AND cn.amount < 250""", vars=locals())[0].count
            smalldonor_pct = num_smalldonor/float(num)
        else:
            labor_pct = 0
            instate_pct = 0
            smalldonor_pct = 0
        db.update('politician', where='id = $polid', vars=locals(),
          pct_labor = labor_pct,
          pct_instate = instate_pct,
          pct_smalldonor = smalldonor_pct
        )
        print polid, labor_pct, instate_pct, smalldonor_pct

if __name__ == "__main__":
    cgitb.enable(format='text')
    load_fec_ids()
    load_fec_cans()
    load_fec_committees()
    load_fec_contributions()
    load_cans_fec_data()

########NEW FILE########
__FILENAME__ = fecdummydb
"""Simple hack to generate a JSON 'database dump' without touching the database.
"""
import cgitb, fec, simplejson, sys

class DummyDB:
    def select(self, *args, **kwargs):
        return []
    def output(self, method, table, kwargs):
        print simplejson.dumps(dict(method=method, table=table, kwargs=kwargs))
    def update(self, table, **kwargs):
        self.output('update', table, kwargs)
    def insert(self, table, **kwargs):
        self.output('insert', table, kwargs)
    
if __name__ == '__main__':
    cgitb.enable(format='text')
    fec.db = DummyDB()
    if len(sys.argv) > 1:
        fec.load_fec_efilings(sys.argv[1])
    else:
        fec.load_fec_efilings()

########NEW FILE########
__FILENAME__ = gen_ids
"""Generating govtrack to watchdog id mapping."""

import web
import os, sys
import simplejson as json
import tools
from parse import govtrack, votesmart

ALL_PEOPLE_FILE = "../data/load/politicians/all_people.json"
MANUAL_JSON = "./load/manual/all_people.json"

def load_wd_mapping():
    print "Loading ID mapping."
    if os.path.isfile(MANUAL_JSON):
        return json.load(file(MANUAL_JSON)) # Load previous version of this generation
    return {}
def reverse_map(map, old_key, new_key):
    new_map = {}
    for id, x in map.items():
        x[old_key] = id
        if new_key in x:
            new_map[x[new_key]] = x
    return new_map

def load_gt_to_wd():
    wd_mapping = load_wd_mapping()
    return reverse_map(wd_mapping, 'watchdog_id', 'govtrack_id')

def load_vs_to_wd():
    wd_mapping = load_wd_mapping()
    return reverse_map(wd_mapping, 'watchdog_id', 'votesmart_id')

def blarb(f):
    to_wd = f()
    def get_wd_id(gt_id):
        return to_wd.get(gt_id)
    return get_wd_id
get_wd_id_GT = blarb(load_gt_to_wd)
get_wd_id_VS = blarb(load_vs_to_wd)


def gen_pol_id(pol):
    firstname = pol.get('firstName') or pol.get('firstname')
    lastname = pol.get('lastName') or pol.get('lastname')
    suffix = pol.get('suffix') or pol.get('namemod') or ''
    if suffix: suffix = '_'+suffix
    id = tools.id_ify(firstname.lower()+'_'+lastname.lower()+suffix.lower())
    return id

def generate_ids():
    wd_to_gt = load_wd_mapping()
    # Govtrack
    for pol in govtrack.parse_basics():
        current_member = False
        collision = False
        watchdog_id = tools.getWatchdogID(pol.get('represents'),pol.lastname)
        if watchdog_id:
            current_member = True
            # pol.represents should always be the same as current_member, if we
            # remove the origional politician.json file we can use that
            # instead.
            assert(pol.represents)
        else:
            try:
                assert(not pol.get('represents'))
            except:
                print "no watchdog id for", web.safestr(pol.name), web.safestr(pol.represents)
                continue
            watchdog_id = gen_pol_id(pol)
            if watchdog_id in wd_to_gt and \
                    wd_to_gt[watchdog_id]['govtrack_id'] != pol.id: 
                collision = True
        if (not collision) or current_member:
            if watchdog_id not in wd_to_gt: 
                wd_to_gt[watchdog_id] = {}
            wd_to_gt[watchdog_id]['govtrack_id'] = pol.id
        if collision: 
            wd_to_gt[watchdog_id]['collision'] = True
        if current_member: 
            wd_to_gt[watchdog_id]['current_member'] = True
    # Votesmart
    for district, cands in votesmart.candidates():
        district=tools.fix_district_name(district)
        for pol in cands:
            watchdog_id = tools.getWatchdogID(district, pol['lastName'])
            if not watchdog_id:
                watchdog_id = gen_pol_id(pol)
            vsid=pol['candidateId']
            #TODO: Could use some more checking to be sure we are 1. adding the
            #      correct votesmart id to the correct watchdog_id (eg. in the
            #      case that there was a collision in processing the govtrack
            #      data). And 2. aren't creating a new watchdog_id when there
            #      was already one for this person.
            if watchdog_id not in wd_to_gt:
                wd_to_gt[watchdog_id] = {}
            wd_to_gt[watchdog_id]['votesmart_id'] = vsid
    return wd_to_gt


if __name__ == "__main__": 
    if not os.path.isfile(ALL_PEOPLE_FILE) or os.path.getsize(ALL_PEOPLE_FILE) == 0:
        print "Generating govtrack to watchdog id mapping."
        fd = open(ALL_PEOPLE_FILE,'w')
        fd.write(json.dumps(generate_ids(), indent=2, sort_keys=True))
        fd.write('\n')
        fd.close()

########NEW FILE########
__FILENAME__ = govtrack
"""
load data from govtrack.us
"""
from __future__ import with_statement
import datetime
import simplejson as json
import web

from parse import govtrack
import schema
from settings import db

mapping = {
  'bioguideid': 'bioguideid',
  'birthday': 'birthday',
  'firstname': 'firstname',
  'gender': 'gender',
  'id': 'govtrackid',
  'lastname': 'lastname',
  'middlename': 'middlename',
  'osid': 'opensecretsid',
  'party': 'party',
  'religion': 'religion',
  'represents': 'district_id',
  'url': 'officeurl',
  'roles':'roles',
}


cong_terms = json.load(file('load/manual/congress_terms.json'))
def to_dt(s): return datetime.datetime(*(map(int,s.split('-'))))
for t in cong_terms.values():
    t['startdate'] = to_dt(t['startdate'])
    t['enddate'] = to_dt(t['enddate'])

def cong_term_lookup(dstart,dend):
    if isinstance(dstart,basestring):
        dstart = to_dt(dstart)
    if isinstance(dend,basestring):
        dend = to_dt(dend)
    ret = set()
    for n,t in cong_terms.items():
        if dend < t['startdate'] or t['enddate'] < dstart:
            continue
        #print n, t['startdate'], t['enddate'], '     ', dstart, dend
        ret.add(n)
    return ret


from gen_ids import get_wd_id_GT as get_wd_id
def combine():
    watchdog_map = {}
    govtrack_map = {}

    print "Processing govtrack.us basics."
    for pol in govtrack.parse_basics():
        wd = get_wd_id(pol.id)
        if not wd: continue
        current_member = wd.get('current_member')
        watchdog_id = wd['watchdog_id']
        govtrack_map[pol.id] = watchdog_map[watchdog_id] = newpol = web.storage()

        newpol['current_member'] = current_member

        for k, v in mapping.iteritems():
            if k in pol: newpol[v] = pol[k]
    
    print "Processing govtrack.us stats."
    for pol in govtrack.parse_stats([
      'enacted', 'introduced', 'cosponsor', 'speeches']):
        if pol.id not in govtrack_map:
            continue
        else:
            newpol = govtrack_map[pol.id]
    
        if pol.get('SponsorEnacted'):
            newpol.n_bills_introduced = int(pol.NumSponsor)
            newpol.n_bills_enacted = int(pol.SponsorEnacted)
    
        if pol.get('SponsorIntroduced'):
            newpol.n_bills_debated = int(pol.NumSponsor) - int(pol.SponsorIntroduced)
    
        if pol.get('NumCosponsor'):
            newpol.n_bills_cosponsored = int(pol.NumCosponsor)
    
        if pol.get('Speeches'):
            newpol.n_speeches = int(pol.Speeches)
            newpol.words_per_speech = int(pol.WordsPerSpeech)
    return watchdog_map


def filter_dict(f, d):
    #return dict([(x, d[x]) for x in f and d.keys()])
    return dict([(x,d[x]) for x in d.keys() if x in f])


def unidecode(d):
    newd = {}
    for k, v in d.iteritems():
        newd[k.encode('utf8')] = v
    return newd


def load_govtrack():
    fill_dicts()


def main():
    watchdog_map = combine()
    with db.transaction():
        #db.delete('congress', where='1=1')
        #db.delete('politician', where='1=1')
        for polid, pol in watchdog_map.items():
            roles = pol.pop('roles')
            # Load the Politician table
            if db.select('politician', where='id=$polid',vars=locals()): #pol.get('current_member'):
                if roles and pol.current_member:
                    pol.officeurl = roles[-1].get('url')
                    pol.party = roles[-1].get('party')
                db.update('politician', where='id=$polid', vars=locals(), 
                        **unidecode(filter_dict(schema.Politician.columns.keys(),
                            pol)))
            else:
                db.insert('politician', seqname=False, id=polid,
                        **unidecode(filter_dict(schema.Politician.columns.keys(),
                            pol)))
            # Load the Congress table
            done = set()
            for r in roles:
                repr = r.state
                if r.type == 'rep' and int(r.district) >= 0:
                    repr = '%s-%02d' % (r.state, int(r.district))
                for term in cong_term_lookup(r.startdate, r.enddate):
                    #if not db.select('congress', where='politician_id=$polid AND congress_num=$term AND district_id=$repr', vars=locals()):
                    if (polid, repr, term) not in done:
                        db.insert('congress', seqname=False, party=r.party,
                                congress_num=term, politician_id=polid,
                                current_member=pol.current_member,
                                district_id=repr)
                    done.add((polid,repr,term))


if __name__ == "__main__": 
    main()



########NEW FILE########
__FILENAME__ = govtrack_gis
"""
parse data from govtrack maps

from: data/crawl/govtrack/gis/gmapdata/
"""
import re
import simplejson as json
from settings import db
from pprint import pprint
from glob import glob

STATE_TABLE = 'load/manual/states.json'
DISTRICT_TABLE = 'load/manual/districts.json'
GMAPDATA = '../data/crawl/govtrack/gis/gmapdata'

r_center = re.compile(r'map\.setCenter\(new GLatLng\(([-0-9.]+), ([-0-9.]+)\), (\d+)\);')

def main():
    districts = json.load(file(DISTRICT_TABLE))
    states = json.load(file(STATE_TABLE))
    for table in [districts, states]:
        for dist in table.iterkeys():
            dname = GMAPDATA + '/%s[_-]marker*.js' % dist.replace('-0', '').replace('-', '')
            dname = glob(dname)
            if not dname: continue # File not found
            dname = dname[0]
            d = file(dname).read()
            x = r_center.findall(d)[0]
            db.update('district', where='name=$dist', vars=locals(), 
              center_lat = x[0], center_lng = x[1], zoom_level=x[2])
    
if __name__ == "__main__": main()

########NEW FILE########
__FILENAME__ = handshakes
from __future__ import with_statement
from settings import db
import time

def generate_handshakes():
    print 'generating handshakes ...'
    pol2corp = db.query('SELECT es.politician_id as frm, '
                            'e.recipient_stem as to, '
                            'SUM(e.final_amt) as amount, '
                            '2008 as year ' #all the data is of 2008 as of now
                        'FROM earmark e, earmark_sponsor es '
                        'WHERE es.earmark_id = e.id and '
                        'e.recipient_stem is not null and '
                        "e.recipient_stem != '' "
                        'GROUP BY es.politician_id, e.recipient_stem, year')

    corp2pol = db.query('SELECT c.employer_stem as frm, '
                                'fec.politician_id as to, '
                                'SUM(c.amount) as amount, '
                                "date_part('year', sent) as year "
                        'FROM contribution c, committee pac, politician_fec_ids fec '
                        'WHERE c.recipient_id = pac.id and '
                        'pac.candidate_id = fec.fec_id and '
                        'c.employer_stem is not null and '
                        "c.employer_stem != '' "
                        'GROUP BY c.employer_stem, fec.politician_id, year')

    pols = {}
    for r in pol2corp:
        pols[r.to, r.frm, r.year] = r.amount
    
    corps = {}
    for r in corp2pol:
        corps[r.frm, r.to, r.year] = r.amount
    
    for k in pols:
        if k in corps:
            corp, pol, year = k
            yield dict(politician_id=pol, corporation=corp, year=year,  
                        pol2corp=pols[k], corp2pol=corps[k])

def load_handshakes(handshakes):
    with db.transaction():
        db.delete('handshakes', '1=1')
        for h in handshakes:
            db.insert('handshakes', seqname=False, **h)            
    
if __name__ == '__main__':
    load_handshakes(generate_handshakes())

########NEW FILE########
__FILENAME__ = lobbyists
#TODO:
#   - Detect dups in PAC table (capitalization, 'PAC' suffix, etc.)
#   - Improve schema... need to figure out how we're to use this first.
#   - Stable IDs for tables.
#   - Link recipient to politician table
#   - Do we need to do something special for amendment filings?
from __future__ import with_statement
import re, string
from pprint import pprint, pformat

import web

from settings import db
from parse import lobbyists


lob_organization =  {
    'organizationName': 'name',
}

lob_pac = {
    'name': 'name'
}

lob_person = {
    'lobbyistPrefix': 'prefix',
    'lobbyistFirstName': 'firstname',
    'lobbyistMiddleName': 'middlename',
    'lobbyistLastName': 'lastname',
    'lobbyistSuffix': 'suffix',
    'contactName': 'contact_name',
}

lob_filing = {
    'senateRegID': 'senate_id',
    'houseRegID': 'house_id',

    'filerType': 'filer_type',

    'reportYear': 'year',
    'reportType': 'type',
    'amendment': 'amendment',
    'signedDate': 'signed_date',
    'certifiedcontent': 'certified',
    #'noContributions': None,
    'comments': 'comments',
}

lob_contribution = {
    'type': 'type',
    'contributorName': 'contributor',
    'payeeName': 'payee',
    'recipientName': 'recipient',
    'amount': 'amount',
    'date': 'date' 
}

def cleanPacName(name):
    name.replace('Cte', 'Committee').replace('Cmte.', 'Committee').replace('Corp.','Corporation').replace('Inc.', 'Inc').replace('Incorporated','Inc')
    rs = [ re.compile(x,re.IGNORECASE) for x in [r'\(.*\)', r' PAC',r' Political Action Committee', r',']]
    for r in rs:
        name = r.sub('', name)
    return name

def cleanName(name):
    rs = [ re.compile(x,re.IGNORECASE) for x in [r'^U.S. ', r'^Rep\.', r'^Sen\.', r'^Representative', r'^Senator', r'^Congressman', r'^Congresswoman', r'for Congress$', r'\(R-..\)', r'\(D-..\)', r'^Candidate', r'^Honerable']]
    for r in rs:
        name = r.sub('', name)
    return name

def findPol(raw_name):
    name = cleanName(raw_name).replace(',','').split(' ')
    name = map(string.lower,filter(lambda x: x, name))
    p = db.select('politician', where=web.sqlors('LOWER(lastname)=',name) + ' AND (' + web.sqlors('LOWER(firstname)=',name)+' OR '+web.sqlors('LOWER(nickname)=',name)+')').list()
    #print raw_name, "-->", name
    if p and len(p) == 1:
        return p[0].id

def load_house_lobbyists():
    print "Loading new lobbyist data."
    pac_id=[1]
    for i, x in enumerate(lobbyists.parse_house_lobbyists()):
        (per, org, fil) = ({}, {}, {})
        #pprint(x)
        for z, val in x.items():
            if z in lob_person: per[lob_person[z]] = val
            if z in lob_organization: org[lob_organization[z]] = val
            if z in lob_filing: fil[lob_filing[z]] = val

        # lob_person table
        if per:
            person = db.select('lob_person', where=web.db.sqlwhere(per,' AND '))
            per['id'] = i #@@ stable ids
            if not person:
                db.insert('lob_person', seqname=False, **per)
            else:
                per = person[0]

        # lob_organization table
        organization = db.select('lob_organization', where=web.db.sqlwhere(org,' AND '))
        org['id'] = i #@@ stable ids
        if not organization:
            db.insert('lob_organization', seqname=False, **org)
        else: 
            org = organization[0]

        # lob_filing table
        fil['lobbyist_id'] = per['id'] if 'id' in per else None
        fil['org_id'] = org['id']
        fil['id'] = x['file_id']
        db.insert('lob_filing', seqname=False, **fil)

        # lob_pac table
        def insert_pac(pac):
            pac_id[0] += 1
            pa = {'id':pac_id[0]}  #@@ stable ids
            for z, val in pac.items():
                if z in lob_pac: pa[lob_pac[z]] = val
            db_pac = db.select('lob_pac', where='LOWER(name)='+web.sqlquote(pa['name'].lower()))
            if not db_pac:
                db_pac = db.select('lob_pac', where='name ilike '+web.sqlquote('%'+cleanPacName(pa['name'])+'%') )
            if not db_pac:
                db.insert('lob_pac', seqname=False, **pa)
            else:
                pa = db_pac[0]
            db.insert('lob_pac_filings',seqname=False, pac_id=pa['id'], filing_id=fil['id'])
        if 'pacs' in x:
            if isinstance(x['pacs'], list):
                for pac in x['pacs']:
                    insert_pac(pac)
            else: 
                insert_pac(x['pacs'])

        # lob_contribution table
        def insert_contribution(con):
            c = {}
            for z, val in con.items():
                if z == 'amount': val = int(float(val))
                if z in lob_contribution: c[lob_contribution[z]] = val
                if z == 'recipientName': c['politician_id']=findPol(val)
            db.insert('lob_contribution', seqname=False, filing_id=x['file_id'], **c)
        if 'contributions' in x:
            if isinstance(x['contributions'], list):
                for con in x['contributions']:
                    insert_contribution(con)
            else: 
                insert_contribution(x['contributions'])


if __name__ == "__main__":
    with db.transaction():
        print "Deleting old data from lob_* tables."
        db.delete('lob_pac_filings',where='1=1')
        db.delete('lob_contribution',where='1=1')
        db.delete('lob_filing',where='1=1')
        db.delete('lob_pac',where='1=1')
        db.delete('lob_person',where='1=1')
        db.delete('lob_organization',where='1=1')
        load_house_lobbyists()


########NEW FILE########
__FILENAME__ = manual
import os, glob
import simplejson as json
import web
from settings import db
import cgitb
cgitb.enable(format='text')

DATA_DIR = 'load/manual'
current_session = 111

def unidecode(d):
    newd = {}
    for k, v in d.iteritems():
        newd[k.encode('utf8')] = v
    return newd

def items(fname):
    print 'loading', fname
    return json.load(file(DATA_DIR + '/%s.json' % fname)).iteritems()

def load_all():
    for code, state in items('states'):
        if 'aka' in state: state.pop('aka')
        db.insert('state', seqname=False, code=code, **unidecode(state))

    for name, district in items('districts'):
        db.insert('district', seqname=False, name=name, **unidecode(district))
    
    db.update('congress', where="current_member='t'", current_member=False)
    for polid, pol in items('politicians'):
        db.insert('politician', seqname=False, id=polid, **unidecode(pol))
        db.insert('congress', seqname=False, politician_id=polid, 
                    congress_num=current_session, district_id=pol['district_id'],
                    current_member=True)

if __name__ == "__main__": load_all()

########NEW FILE########
__FILENAME__ = maplight
"""
load maplight info

from: data/crawl/maplight/
"""

from __future__ import with_statement

import csv
from tools import db
import bills
    
def load_data():
    c = csv.reader(file('../data/crawl/maplight/uniq_map_export_bill_research.csv'))
    supportdict = {'0': -1, '1': 1, '2': 0 } #0: oppose ; 1: support; 2: not known (from README)
    
    with db.transaction():
        db.delete('interest_group_bill_support', '1=1')
        for line in c:
            if not line[0].startswith('#'):
                category_id, longname, maplightid, session, measure, support = line
                support = supportdict[support]
                if support == 0: continue
                typenumber = measure.lower().replace(' ', '')
                    
                r = db.select('interest_group', what="id", where="longname=$longname", vars=locals())
                if r:
                    groupid = r[0].id
                else:
                    groupid = db.insert('interest_group', longname=longname, category_id=category_id)
                    
                bill_id = 'us/%s/%s' % (session, typenumber)
                r = db.select('bill', where="id=$bill_id", vars=locals())
                if not r:
                    filename = "../data/crawl/govtrack/us/%s/bills/%s.xml" % (session, typenumber)
                    bills.loadbill(filename, maplightid=maplightid)
                else:
                    db.update('bill', maplightid=maplightid, where="id=$bill_id", vars=locals())
                    
                try:
                    #print '\r', bill_id,
                    db.insert('interest_group_bill_support', seqname=False, bill_id=bill_id, group_id=groupid, support=support)
                except:
                    print '\n Duplicate row with billid %s groupid %s support %s longname %s' % (bill_id, groupid, support, longname)
                    raise
       
def load_categories():
    c = csv.reader(file('../data/crawl/maplight/CRP_Categories.csv'))
    with db.transaction():
        db.delete('category', '1=1')
        for line in c:
            if not line[0].startswith('#'):
                cid, cname, industry, sector, empty = line
                db.insert('category', seqname=False, id=cid, name=cname, industry=industry, sector=sector)
                  
def generate_similarities():
    """
    Generate similarity information for each (interest group, politician) pair and store in DB
    """
    result = db.query('select igbp.group_id, position.politician_id, igbp.support, position.vote'
                    ' from interest_group_bill_support igbp, position'
                    ' where igbp.bill_id = position.bill_id')
    sim = {}
    total = {}
             
    for r in result:
        k = (r.group_id, r.politician_id)
        if r.support == r.vote and r.support != 0:
            sim[k] = sim.get(k, 0) + 1
        total[k] = total.get(k, 0) + 1
    
    with db.transaction():
        db.delete('group_politician_similarity', '1=1')
        for k, agreed in sim.items():
            group_id, politician_id = k
            db.insert('group_politician_similarity', seqname=False, 
                group_id=group_id, politician_id=politician_id, agreed=agreed, total=total[k])
                
                                                                        
def aggregate_similarities():
    """
    Aggregate the similarity info in group_politician_similarity table, category wise. 
    """
    result = db.query("select sum(sim.agreed) as agreed, sum(sim.total) as total,"
                      " sim.politician_id, cat.name as category_name" 
                      " from group_politician_similarity sim, interest_group grp, category cat"
                      " where sim.group_id=grp.id and grp.category_id != '' and cat.id = grp.category_id"
                      " group by sim.politician_id, cat.name")    
    #for r in result:    
    #    print r.politician_id, r.category_name, r['agreed']*100.0/r['total'], r['agreed'], r['total']
    return result
         
def main():
    load_categories()
    load_data()
    generate_similarities()
    aggregate_similarities()
                                     
if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = opensecrets
"""
Load data from opensecrets.
"""
from parse import opensecrets
import tools
from settings import db

for can in opensecrets.parse_all():
    p = tools.opensecretsp(can.get('opensecretsid'))
    if p and can:
        db.update('politician', where='id = $p', vars=locals(),
          pct_pac_business=can.business_pac/float(can.total),
          #pct_badmoney = can.badmoney/float(can.total)
        )

########NEW FILE########
__FILENAME__ = phones
"""
Loads the phone numbers of politicians into DB from votesmart offices data
"""

from __future__ import with_statement
import sys
import simplejson
from settings import db
from urllib2 import urlopen
import re

votesmart_offices = '../data/crawl/votesmart/offices.json'
    
def get_votesmart_phones(pols):
    def _get(ph, phtype):
        try:
            return ph.get(phtype, '').replace('-', '').split()[0]
        except IndexError: pass

    d = {}
    offices = simplejson.load(file(votesmart_offices))
        
    for pol in pols:
        polid = pol.id
        contact = offices.get(pol.votesmartid, {})
        offs = contact.get('office', [])
        for o in offs:
            phones, address = o.get('phone', {}), o.get('address', {})
            city = address.get('city', '')
            phone1 = _get(phones, 'phone1')
            phone2 = _get(phones, 'phone2')
            tollfree = _get(phones, 'tollFree')
            d[(polid, city)] = dict(phone1=phone1, phone2=phone2, tollfree=tollfree)
    return d

def get_all_phones():
    """return list of all phones in house from 'http://clerk.house.gov/member_info/mcapdir.html'"""
    response = urlopen('http://clerk.house.gov/member_info/mcapdir.html').read()
    r = re.compile('\d{3}-\d{4}')
    return r.findall(response)

#phones_from_house = get_all_phones()

def load_phones(phones):
    with db.transaction():
        db.delete('pol_phones', '1=1')
        for k, v in phones.items():
            polid, city = k
            db.insert('pol_phones', seqname=False, politician_id=polid, city=city, **v)
    
if __name__ == '__main__':
    pols = db.select('curr_politician', what='id, votesmartid')
    phones = get_votesmart_phones(pols)
    load_phones(phones)

########NEW FILE########
__FILENAME__ = photos
"""
find photos for politicians

from: data/crawl/house/photos/
from: data/crawl/govtrack/photos/
from: data/crawl/opensecrets/photos/
from: data/parse/politicians/govtrack.json
#from: data/crawl/votesmart/photos/ # not working yet
to:   data/parse/politicians/photos.json
"""

import os
import simplejson as json
from settings import db

def govtrack_u(govtrackid):
    return 'http://www.govtrack.us/congress/person.xpd?id=%s' % govtrackid

def bioguide_u(bioguideid):
    return 'http://bioguide.congress.gov/scripts/bibdisplay.pl?index=%s' % bioguideid

def opensecrets_u(opensecretsid):
    return 'http://opensecrets.org/politicians/summary.asp?cid=%s' % opensecretsid

def votesmart_u(votesmartid):
    return 'http://votesmart.org/bio.php?can_id=%s' % votesmartid

def govtrackcredit(govtrackid):
    """gets the credit for the GovTrack photo; credits GovTrack if we can't find it"""
    try:
        f = file('../data/crawl/govtrack/photos/%s-credit.txt')
    except IOError: # file not found
        return (govtrack_u(govtrackid), 'GovTrack')
    return f.read().split(' ', 1)

def load():
    out = {}

    pols = db.select('politician')
    #json.load(file('../data/load/politicians/govtrack.json'))

    for pol in pols:
        options = [
          (
            '../data/crawl/house/photos/%s.jpg' % pol['bioguideid'], 
            (bioguide_u(pol['bioguideid']), 'Congressional Biographical Directory')
          ),
          (
            '../data/crawl/govtrack/photos/%s.jpg' % pol['govtrackid'], 
            govtrackcredit(pol['govtrackid'])
          ),
          (
            '../data/crawl/opensecrets/photos/%s.jpg' % pol.get('opensecretsid'), 
            (opensecrets_u(pol.get('opensecretsid')), 'Center for Responsive Politics')
          ),
          (
            '../data/crawl/votesmart/photos/%s.jpg' % pol.get('votesmartid'),
            (votesmart_u(pol.get('votesmartid')), 'Project Vote Smart')
          ),
          (
            '../data/crawl/votesmart/photos/%s.JPG' % pol.get('votesmartid'),
            (votesmart_u(pol.get('votesmartid')), 'Project Vote Smart')
          )
        ]
    
        maxsize = 0
        currentfn = None
        currentcredit = None
        for fn, source in options:
            try:
                if os.stat(fn).st_size > maxsize:
                    maxsize = os.stat(fn).st_size
                    currentfn = fn
                    currentcredit = source
            except OSError: # file does not exist
                pass

        if currentfn:
            db.update('politician', where='id=$pol.id', vars=locals(), 
              photo_path=currentfn[2:],
              photo_credit_url=currentcredit[0],
              photo_credit_text=currentcredit[1]
            )

if __name__ == "__main__":
    load()

########NEW FILE########
__FILENAME__ = populations
import sys

from settings import db
from decimal import *

################################################################################
def county2dist(state, county, scale_column='population'):
    # The census provides both county and districts at the tract level
    pop_county = db.select('census_population', what='sum('+scale_column+')',
            where="sumlev='COUNTY' and state_id=$state and county_id=$county",
            vars=locals()).list()
    if pop_county and len(pop_county)==1:
        pop_county = pop_county[0].sum
    else: print "oops"; return None
    intersect_pops = db.select('census_population', 
            what='district_id, sum('+scale_column+')', 
            where="sumlev='TRACT' and district_id != '' and state_id=$state and county_id=$county", 
            group='district_id', vars=locals())

    ret = {}
    for ip in intersect_pops:
        ret['%s-%s' % (state, ip.district_id)] = Decimal(ip.sum) / pop_county if pop_county else 0.0
    return ret


def zip2dist_by_zip4(zip5):
    if db.select('state', where='code=$zip5',vars=locals()):
        return {zip5:1.0}
    dists  = db.select('zip4', 
            what='COUNT(plus4), district_id', 
            where='zip=$zip5', 
            group='district_id',
            vars=locals()).list()
    all_zip4 = sum(map(lambda d: d.count, dists))
    ret = {}
    for d in dists:
        ret[d.district_id] = float(d.count) / float(all_zip4)
    return ret

def zip2dist(zip5, scale_column='population'):
    ## ARRRG, The census provides the congressional districts down to the tract
    # level, but not to the block level. The ZCTA are provided at the block
    # level, but NOT at the tract level. 
    # This would be ok if tracts didn't overlap ZCTAs, but they do. Not sure
    # how to solve this problem.
    if scale_column=='zip4':
        return zip2dist_by_zip4(zip5)
    pop_zip = db.select('census_population', what='sum('+scale_column+')',
            where="sumlev='ZCTA' and zip_id=$zip5",
            vars=locals()).list()
    if pop_zip and len(pop_zip)==1:
        pop_zip = pop_zip[0].sum
    else: print "oops"; return None
    # Limit our search to known intersecting districts
    dists = db.select('zip4', what='district_id', 
            where="zip=$zip5", group='district_id', 
            vars=locals())

    intersect_pops = db.query("select a.district_id, b.state_id, SUM(b."+scale_column+") from (SELECT * FROM census_population WHERE sumlev='TRACT' AND district_id != '') as a INNER JOIN (SELECT * FROM census_population WHERE sumlev='BLOCK' AND zip_id=$zip5) as b ON (a.state_id=b.state_id AND a.county_id=b.county_id AND a.tract_id=b.tract_id) group by a.district_id, b.state_id", vars=locals()).list()

    # NOTE: This is not the correct behavior, but for now just adjust this to
    #       give us something that sums to 1.0.
    pop_zip2 = sum(map(lambda x: x.sum if x.sum else 0.0, intersect_pops))
    print >>sys.stderr, "Pop Zip:",pop_zip, pop_zip2
    pop_zip = pop_zip2

    ret = {}
    for ip in intersect_pops:
        print >>sys.stderr, ip.sum, pop_zip
        ret['%s-%s' % (ip.state_id, ip.district_id)] = Decimal(ip.sum) / pop_zip if pop_zip else 0.0
    return ret


counties = [('CA','067'),]
for state,county in counties:
    c = county2dist(state,county)
    print "County(%s-%s) to district by population"%(state,county), c, sum(c.values())
    c = county2dist(state, county, scale_column='area_land')
    print "County(%s-%s) to district by area"%(state,county), c, sum(c.values())

zips = ['95835', '95608', '10024', ]
for zip in zips:
    z = zip2dist(zip)
    print "Zip(%s) to district by population" % zip, z, sum(z.values())
    z = zip2dist(zip, scale_column='area_land')
    print "Zip(%s) to district by area" % zip, z, sum(z.values())
    z = zip2dist(zip, scale_column='zip4')
    print "Zip(%s) to district by zip4s" % zip, z, sum(z.values())



########NEW FILE########
__FILENAME__ = punch
from parse import punch
import tools
from settings import db

for can in punch.parse_all():
    d = tools.districtp(can.district)
    if d and can.name.split(',')[0].lower() in d:
        db.update('politician', where='id = $d', vars=locals(),
          chips2008=can.chips2008,
          progressive2008=can.progressive2008,
          progressiveall=can.progressiveall)

########NEW FILE########
__FILENAME__ = se
"""
Xapian importer.
"""
try:
    import xappy
except ImportError:
    import sys, warnings
    warnings.warn('No xappy found. Skipping search engine stuff.')
    sys.exit(0)
import schema

DBPATH = '../se'

def replacetable(s, tab):
    for k, v in tab.iteritems():
        s = s.replace(k, v or '')
    return s

def index(doc, k, v):
    return doc.fields.append(xappy.Field(k, v))

def trash(db):
    for k in db.iterids():
        db.delete(k)

nameformats = """
first middle last
first last
nickname last
last, first
last, first middle
last, nickname
""".strip()

def initdb():
    iconn = xappy.IndexerConnection(DBPATH)
    trash(iconn)
    iconn.add_field_action('name', xappy.FieldActions.INDEX_FREETEXT, spell=True)
    iconn.add_field_action('id', xappy.FieldActions.INDEX_FREETEXT)
    iconn.add_field_action('id', xappy.FieldActions.STORE_CONTENT)
    return iconn

def load_pols(iconn):
    pols = schema.Politician.select()
    for p in pols:
        doc = xappy.UnprocessedDocument()
        for format in nameformats.split('\n'):
            text = replacetable(format, 
              dict(first=p.firstname, middle=p.middlename, last=p.lastname, nickname=p.nickname))
            index(doc, 'name', text)
        index(doc, 'name', p.id.replace('_', ' '))
        index(doc, 'id', p.id)
        iconn.add(doc)

def test():
    sconn = xappy.SearchConnection(DBPATH)
    print sconn.get_doccount(), 'documents loaded.'
    def query(qtext):
        q = sconn.query_parse(sconn.spell_correct(qtext), default_op=sconn.OP_AND)
        return [x.data['id'][0] for x in sconn.search(q, 0, 10)]

    assert query('biden joe') == ['joe_biden']
    assert query('barak obma') == ['barack_obama']
    return True

if __name__ == "__main__":
    iconn = initdb()
    load_pols(iconn)
    iconn.flush()
    iconn.close()
    
    if test(): print 'Success.'
########NEW FILE########
__FILENAME__ = shapes
"""
load district shapes
"""
import simplejson as json
import web
import tools
from settings import db
from parse import shapes
from pprint import pprint

def load():
    for district in shapes.parse():
        outline = json.dumps({'type': 'MultiPolygon', 'coordinates': district['shapes']})
        if district['district']:
            district = tools.unfips(district['state_fipscode']) + '-' + district['district']
        else:
            district = tools.unfips(district['state_fipscode'])
        db.update('district', where='name=$district', outline=outline, vars=locals())

if __name__ == "__main__": load()

########NEW FILE########
__FILENAME__ = soi
from __future__ import with_statement
from pprint import pprint, pformat
import sys
from types import NoneType

import web

from parse import soi
from tools import db


# The keys from the parser are:
# {
#     'loc', 'gini', 
#     'brackets' : 
#     {
#         'agi', 'bracket_low', 
#         'n_dependents', 'n_eitc', 'n_filers', 'n_prepared', 
#         'tot_charity', 'tot_eitc', 'tot_tax', 
#         'avg_dependents', 'avg_eitc', 'avg_income', 'avg_taxburden', 
#         'pct_charity', 'pct_eitc', 'pct_prepared', 
#     }
# }

# NOTE: I am using the number of zip+4's in a zip/district to determine the
#       proportion of that zip's data to apply to a district.
def get_dist(zip5):
    # TODO: if zip5 is state (select code from state where code=$zip5) then return {zip5: 1.0}
    if db.select('state', where='code=$zip5',vars=locals()):
        return {zip5:1.0}
    dists  = db.select('zip4', 
            what='COUNT(plus4), district_id', 
            where='zip=$zip5', 
            group='district_id',
            vars=locals()).list()
    all_zip4 = sum(map(lambda d: d.count, dists))
    ret = {}
    for d in dists:
        ret[d.district_id] = float(d.count) / float(all_zip4)
    return ret


def load_soi():
    # TODO: not sure how to handle agi and gini values.
    districts = {}
    data = {}
    for z in soi.parse_soi():
        dists_for_data = get_dist(z.loc)
        if dists_for_data:
            data[z.loc] = z
            for d in dists_for_data.keys():   # for each district associated with loc
                if d not in districts:
                    districts[d] = { 'brackets': [{'n_filers':0} for x in range(len(z.brackets))] }
                for new_data,cur_data in zip(z.brackets, districts[d]['brackets']):
                    #print new_data.n_filers, new_data.n_prepared, new_data.agi, new_data.bracket_low
                    n_filers_old = cur_data['n_filers']
                    if not new_data.n_filers: new_data['n_filers'] = 0
                    n_filers_new = n_filers_old + new_data.n_filers * dists_for_data[d]
                    for k in new_data.keys():
                        if k not in cur_data:
                            cur_data[k] = 0
                        if k.startswith('n_') or k.startswith('tot_') or k == 'agi':
                            if new_data[k]:
                                cur_data[k] += new_data[k] * dists_for_data[d]
                        elif k.startswith('pct_') or k.startswith('avg_'):
                            if new_data[k]:
                                cur_data[k] = (cur_data[k] * n_filers_old + dists_for_data[d] * new_data[k] * new_data.n_filers) / n_filers_new
                        else:
                            #if k in cur_data and cur_data[k] != new_data[k]: print k, cur_data[k], new_data[k]
                            cur_data[k] = new_data[k]
    for d in districts.keys():
        for b in districts[d]['brackets']:
            if d == 'DC': d = 'DC-00' # HACK: Data has DC which should be DC-00.
            # We can't use None because bracket_low is part of the primary key.
            if isinstance(b['bracket_low'], NoneType): b['bracket_low'] = -1
            db.insert('soi', seqname=False, district_id=d, **b)
    #pprint(districts)


if __name__ == "__main__":
    with db.transaction():
        db.delete('soi',where='1=1')
        load_soi()



########NEW FILE########
__FILENAME__ = tools
#!/usr/bin/env python
"""
common tools for load scripts
"""
import os, re, string, unicodedata
import simplejson as json
import web

from settings import db

STATE_TABLE = 'load/manual/states.json'
DISTRICT_TABLE = 'load/manual/districts.json'
POLITICIAN_TABLE = 'load/manual/politicians.json'

_stripterms = ['the', 'corporation', 'corp', 'incorporated', 'inc', 'international', 'intl']
_corpmapping = {
  'none': '',
  'not employed': '',
  'self employed': 'self',
  'selfemployed': 'self',
  'n': ''
}
r_plain = re.compile(r'[a-z ]+')
def stemcorpname(name):
    """
    >>> stemcorpname('The Boeing Corp.')
    'boeing'
    >>> stemcorpname('SAIC Inc.')
    'saic'
    >>> stemcorpname('Self-Employed')
    'self'
    >>> stemcorpname('None')
    ''
    """
    if not name: return name
    name = name.lower()
    name = ''.join(r_plain.findall(name))
    name = ' '.join(x for x in name.split() if x not in _stripterms)
    if name in _corpmapping:
        name = _corpmapping[name]
    return name

_unfipscache = {}
def unfips(fipscode):
    if not _unfipscache:
        states = json.load(file(STATE_TABLE))
        for stateid, state in states.iteritems():
            _unfipscache[state['fipscode']] = stateid
        
    return _unfipscache.get(fipscode)

_districts = {}
def fixdist(dist):
    dist = dist.upper().replace('-SEN1','').replace('-SEN2','').replace('-S1','').replace('-S2','')
    if not _districts:
        districts = json.load(file(DISTRICT_TABLE))
        for k, v in districts.iteritems():
            _districts[k]=v
    if dist.endswith('-01') and dist[:-1] + '0' in _districts:
        return dist[:-1] + '0'
    else:
        return dist
    

_districtcache = {}
def districtp(district):
    """
    Return the watchdog ID for the represenative of `district`.
    """
    if not _districtcache:
        reps = json.load(file(POLITICIAN_TABLE))
        for repid, rep in reps.iteritems():
            if rep['district_id'] in _districtcache:
                _districtcache[rep['district_id']].append(repid)
            else:
                _districtcache[rep['district_id']] = [repid]
    
    return _districtcache.get(district) or []

def id_ify(text):
    """Take a string and convert it to a suitable watchdog id."""
    text = text.strip()
    # replace accented characters with non-accented ones
    text = unicodedata.normalize('NFKD',text).encode('ascii','ignore')
    P = set(string.punctuation)
    P.remove('_')
    # Remove punctuation (except '_') and replace spaces with '_' and lower case.
    text = ''.join(filter(lambda y: y not in P, text)).lower().replace(' ','_')
    return text

def getWatchdogID(district, lastname):
    # Filter out accented characters.
    #lastname = unicodedata.normalize('NFKD',lastname).encode('ascii','ignore')
    watchdog_ids = filter(lambda x: lastname.lower().replace(' ', '_') in x, districtp(district))
    if len(watchdog_ids) == 1:
        return watchdog_ids[0]
    return None


def fix_district_name(name):
    name=name.replace("-SEN1","").replace("-SEN2","").strip()
    if name in ['DC','GU','PR']:
        name += '-00'
    return name
    


_govtrackcache = {}
_opensecretscache = {}
def _fill():
    for pol in db.select('politician', what='id, govtrackid, opensecretsid'):
        _govtrackcache[pol.govtrackid] = str(pol.id)
        _opensecretscache[pol.opensecretsid] = str(pol.id)
        
def govtrackp(govtrack_id):
    """
    Return the watchdog ID for a person's `govtrack_id`.
    
        >>> govtrackp('400114')
        'michael_f._doyle'
        >>> print govtrackp('aosijdoisad') # ID we don't have
        None
    """
    if not _govtrackcache: _fill()
    return _govtrackcache.get(govtrack_id)

def opensecretsp(opensecrets_id):
    """
    Returns the watchdog ID for a person's `opensecrets_id`.
    """
    if not _opensecretscache: _fill()
    return _opensecretscache.get(opensecrets_id)

def fecp(fec_id):
    x = db.where('politician_fec_ids', fec_id=fec_id)
    if x:
        return x[0].politician_id
    else:
        return None

if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = update_pols
"""Update the DB with the new politicians of 111th Congress"""

from __future__ import with_statement
import web
import simplejson
from settings import db

new_pols = simplejson.load(open('load/manual/politicians.json'))

def unidecode(d):
    newd = {}
    for k, v in d.iteritems():
        newd[k.encode('utf8')] = v
    return newd

def create_or_update(polid, dist):
    if not db.select('district', where='name=$dist', vars=locals()):
        db.insert('district', seqname=False, name=dist, state_id=dist[:2])

    if db.select('politician', where='id=$polid', vars=locals()):
        db.update('politician', where='id=$polid', district_id=dist, last_elected_year='2008', vars=locals())
    else:
        first, last = id.split('_', 1)
        first, last = first.title(), last.title()
        db.insert('politician', seqname=False, id=polid, firstname=first, lastname=last, last_elected_year='2008', district_id=dist)

def update_congress(polid, dist):
    db.insert('congress', seqname=False, politician_id=polid, district_id=dist, congress_num=111, current_member='t')

def load_new_pols():
    with db.transaction():
        db.update('congress', where="current_member='t'", current_member=False)
        for polid, pol in new_pols.items():
            district = pol['district_id']
            create_or_update(polid, district)
            update_congress(polid, district)

if __name__ == '__main__':
    load_new_pols()

########NEW FILE########
__FILENAME__ = usps
"""
Load USPS data.
"""
from __future__ import with_statement
import gzip
from settings import db

#with db.transaction():
#db.delete('zip', '1=1')
allzips = set()
for line in file('../data/parse/all5digitzip.txt'):
    city, statezip = line.split(',')
    state, zip = statezip.split()
    allzips.add(zip)
#    db.insert('zip', seqname=False, city=city, state=state, zip=zip)

#db.delete('zip4', '1=1')
fho = file('zip4.tsv', 'w')
for line in gzip.open('../data/parse/zip4dist.txt.gz'):
    zip4, dist = line.split()
    zip5, plus4 = zip4.split('-')
    if plus4.endswith('ND'): continue
    if dist[:2] in [
      'AE', 'AA', 'AP', # Armed Forces
      'PW', # Palau
      'MP', # Marshall Islan
      'FM', # Federated States of Micronesia
      'VI', # Virgin Islands
    ]: continue
    
    if zip4 in ['22309-9501', '22309-9502', '22309-9503', '22309-9504'] and \
       dist != 'VA-11': continue
    if dist.endswith('99'):
        #@@ nebraska weirdness
        dist = dist[:-2] + '02'
    
    if zip5 not in allzips:
        print 'badzip', line
    
    fho.write('%s\t%s\t%s\n' % (zip5, plus4, dist))
    
    #try:
    #    db.insert('zip4', seqname=False, zip=zip, plus4=plus4, district=dist)
    #except Exception, e:
    #    print line
    #    print e

########NEW FILE########
__FILENAME__ = votesmart
"""
Load Project Vote Smart data.
"""
from __future__ import with_statement

from parse import votesmart
import simplejson as json
import tools
from settings import db
import schema


def unidecode(d):
    newd = {}
    for k, v in d.iteritems():
        newd[k.encode('utf8')] = v
    return newd

# Mappings from votesmart to schema
cand_mapping = {
  'candidateId':'votesmartid',
  'electionParties':'party',
  'firstName':'firstname',
  'lastName':'lastname',
  'middleName':'middlename',
  'nickName':'nickname',
  'electionStatus' : 'election_status',
}
bios_mapping = {
  'firstName':'firstname',
  'lastName':'lastname',
  'middleName':'middlename',
  'nickName':'nickname',
  'gender':'gender',
  'birthDate':'birthday',
  'birthPlace':'birthplace',
  'education':'education',
  'religion':'religion',
}

def filter_dict(f, d):
    if isinstance(f, dict):
        return dict([(i, d[k]) for k,i in f.items()])
    elif isinstance(f, list):
        return dict([(x, d[x]) for x in d.keys() if x in f])


from gen_ids import get_wd_id_VS as get_wd_id
def load_votesmart():
    # Candidates from votesmart
    for district, cands in votesmart.candidates():
        district=tools.fix_district_name(district)
        for pol in cands:
            vs_id=pol['candidateId']
            wd = get_wd_id(vs_id)
            if not wd: continue
            polid = wd['watchdog_id']

            pol_cand = filter_dict(cand_mapping, pol)
            if not db.select('politician', 
                    where='id=$polid', vars=locals()):
                db.insert('politician', 
                        seqname=False, 
                        id=polid, 
                        **unidecode(filter_dict(schema.Politician.columns.keys(),
                            pol_cand)))
            else:
                # @@ Should probably check that we really want to do this, but
                # it apears as though the data has two entries for current
                # members (the second having more info filled out).
                db.update('politician', where='id=$polid', vars=locals(),
                        **unidecode(filter_dict(schema.Politician.columns.keys(),
                            pol_cand)))

            if not db.select('congress',
                    where="politician_id=$polid AND congress_num='-1'", 
                    vars=locals()):
                db.insert('congress', seqname=False, congress_num='-1',
                        politician_id=polid, district_id=district,
                        party=pol_cand['party'])

    # Bios from votesmart
    for vs_id, p in votesmart.bios():
        pol = p['candidate']
        if pol['gender']:
            pol['gender']=pol['gender'][0]
        if pol['education']:
            pol['education'] = pol['education'].replace('\r\n', '\n')
        wd = get_wd_id(vs_id)
        if not wd: continue
        polid = wd['watchdog_id']
        pol_people = filter_dict(schema.Politician.columns.keys(),
                filter_dict(bios_mapping, pol))
        if db.select('politician', where='votesmartid=$vs_id',vars=locals()):
            db.update('politician', where='votesmartid=$vs_id', 
                    vars=locals(), **unidecode(pol_people))


def main():
    with db.transaction():
        db.update('politician', votesmartid=None, where='1=1')
        db.delete('congress', where="congress_num='-1'")
        load_votesmart()


if __name__ == "__main__": 
    main()



########NEW FILE########
__FILENAME__ = voteview
"""
import voteview
"""

import tools
from parse import voteview
from settings import db

def main():
    for pol in voteview.parse():
        if not pol.get('district_id'): continue #@@
    
        if not tools.districtp(pol.district_id) and pol.district_id.endswith('01'):
            pol.district_id = pol.district_id.split('-')[0] + '-' + '00'
    
        watchdog_id = tools.getWatchdogID(pol.district_id, pol.last_name)
        if watchdog_id:
            db.update('politician', where='id=$watchdog_id', vars=locals(),
              icpsrid = pol.icpsr_id, 
              nominate = pol.dim1, 
              predictability = 1 - (pol.n_errs / float(pol.n_votes)))

if __name__ == "__main__": main()

########NEW FILE########
__FILENAME__ = wyr
from __future__ import with_statement
import simplejson as json
from settings import db

wyr = json.load(file('../data/crawl/votesmart/wyr.json'))

types = dict(email='E', wyr='W', ima='I', zipauth='Z')

def load_wyr():
    with db.transaction():
        db.delete('pol_contacts', where='1=1')
        for pol, data in wyr.iteritems():
                if data['contacttype'] not in types.keys(): 
                    continue

                if data['contacttype'] == 'wyr':
                    contact = 'https://writerep.house.gov/writerep/welcome.shtml'
                else:
                    contact = data['contact']    
            
                d = {'politician_id':pol,
                        'contact':contact,
                        'contacttype': types[data['contacttype']],
                        'captcha': data['captcha']
                       }
                try:
                    db.insert('pol_contacts', seqname=False, **d)
                except:
                    continue
           
if __name__ == "__main__": 
    load_wyr()

########NEW FILE########
__FILENAME__ = almanac
#!/usr/bin/python
# Scrape nationaljournal.com pages.
import re, sys
from BeautifulSoup import BeautifulSoup

def scrape_photo_alt(fname, rv, alt):
    "Interpret the alt text of the portrait photos from most pages."
    # special case for
    # nationaljournal.com/pubs/almanac/2000/people/president.htm
    if alt == 'photo': return 
    photo_parts = re.match(r'((?P<title>Rep\.|Sen\.|Gov\.|Del\.) )?' +
                           '(?P<name>.*?)' +
                           r' \((?P<party>(D|R|DFL|I|IR|ID|Ind))' +
                              r'(-At Large)?\)',
                           alt)
    if photo_parts is None:
        raise Exception("couldn't understand caption " + alt + " in " + fname)
    for field in 'name party title'.split():
        rv[field] = photo_parts.group(field)

def extract_text(html):
    html = re.sub(r'(?i)<br[^>]*>', '\n', html)
    soup = BeautifulSoup(html, convertEntities='html')
    text = ''.join(soup.findAll(text=True))
    text = text.replace(u'\xa0', ' ')   # &nbsp;
    return text.strip()

def plain(fieldname):
    "A plain text field."
    def handle(rv, html):
        rv[fieldname] = extract_text(html)
    return handle

def html(fieldname):
    "A field where we get all the HTML, for debugging."
    def handle(rv, html):
        rv[fieldname] = html
    return handle

def combine(fielda, fieldb):
    "A field with more than one kind of processing, for debugging."
    def handle(rv, html):
        fielda(rv, html)
        fieldb(rv, html)
    return handle

def election_table(rv, html):
    soup = BeautifulSoup('<table><tr><td>' + html)
    headers = {0: 'election'}
    other_headers = {'Candidate': 'candidate', 'Total Votes': 'totalvotes',
                     'Percent': 'percent', 'Expenditures': 'expenditures'}
    results = []
    last_election_value = 'unknown election'
    for row in soup('tr'):
        if len(row('td')) < 2: continue
        current_row = {'election': last_election_value}
        actually_got_something = False
        for cell, col in zip(row('td'), range(100)): # won't have >100 columns!
            # underlined text in the headers
            if cell('u'): headers[col] = other_headers[cell.u.string]
            else: 
                if not headers.has_key(col):
                    # this happens in
                    # e.g. almanac/nationaljournal.com/pubs/almanac/2002/people/mn/mngv.htm
                    continue
                cell_value = extract_text(str(cell))
                if cell_value != '':
                    current_row[headers[col]] = cell_value
                    actually_got_something = True
        last_election_value = current_row['election']
        if actually_got_something: results.append(current_row)
    rv['electionresults'] = results

number = re.compile(r'\d+$')
def interest_group_ratings(rv, html):
    ratings_dict = {}
    soup = BeautifulSoup('<table>' + html)
    rows = soup('tr')
    headers = rows[0]
    datarows = rows[1:]
    # the first cell (above the years) is a td instead of a th, so
    # we're missing a column here.
    groupnames = [extract_text(str(x)) for x in headers('th')]
    for row in datarows:
        cells = row('td')
        year = extract_text(str(cells[0]))
        ratings_dict[year] = {}
        ratings = [extract_text(str(cell)) for cell in cells[1:]]
        # note that several tables either have an extra table header
        # or an extra column of '--' on the right.  So we can't do
        # this assertion:
        # assert len(ratings) == len(groupnames), (ratings, groupnames)
        for groupname, rating in zip(groupnames, ratings):
            if groupname and number.match(rating):
                ratings_dict[year][groupname] = int(rating)
    rv['interest_group_rating'] = ratings_dict

def _parse_list(html):
    soup = BeautifulSoup(html)
    hash = {}
    for li in soup('li'):
        name = li.b.string
        if name is None: continue # no bold text, prolly not a name-val pair
        name = name.strip()
        if name.endswith(':'): name = name[:-1]
        hash[name] = extract_text(str(li.b.nextSibling))
    # original version (which didn't cope well with capitalized and
    # unclosed LI tags, etc.)
    #     for mo in re.finditer(r'<li><b>(.*?): ?</b> (.*?)</li>', html):
    #         hash[mo.group(1)] = mo.group(2)
    return hash

def parse_list(name):
    def func(rv, html):
        rv[name] = _parse_list(html)
    return func

# Things we care about if they are in the left column of a 2-column
# table, or if they are headers.
person_fields = {
    'Born': plain('born'),
    'Home': plain('home'),
    'Education': plain('education'),
    'Religion': plain('religion'),
    'Marital Status': plain('maritalstatus'),
    'Elected Office': plain('office'),
    'DC Office': plain('dcoffice'),
    'State Offices': plain('stateoffice'),
    'Committees': plain('committees'),
    'District Demographics': parse_list('demographics'),
    'Election Results': election_table,
    'Group Ratings': interest_group_ratings,
}
state_fields = {
    'The State': parse_list('state'),
}
        
def scrape_by_headers(rv, fields, html):
    "Find sections of text underneath certain headers with special meanings."
    # They changed from #6666CC to #333366 after 2002, although the
    # more recent page elements ("Go Wireless") are #333366 even on old
    # pages, leading to a kind of inconsistent appearance.
    by_headers = re.split(r' color="#(?:6666CC|333366)"[^>]*>', html)
    by_headers = by_headers + re.split(r'<h3>', html)    # ugg, some use h3 headers (CO-01 for instance)

    for item in by_headers:
        for key in fields.keys():
            if item.startswith(key):
                # </UL> is a special case for "District Demographics"
                # and "The State"
                mo = re.search(r'(?is)</font>(.*?)(?:</UL>|</p>)', item)
                if mo: fields[key](rv, mo.group(1))

def scrape_table(rv, fields, html):
    tablerows = re.findall(r'(?is)<tr [^>]*>.*?</tr>', html)

    for row in tablerows:
        cells = re.findall(r'(?is)<td [^>]*>(.*?)</td>', row)
        if len(cells) != 2: continue
        name = re.sub(':$', '', extract_text(cells[0]))
        if fields.has_key(name): fields[name](rv, cells[1])
        
def scrape_person(fname):
    fo = file(fname)
    rv = {'filename': fname}
    contents = fo.read()

    photo_alt = re.search(r'<img [^>]*height="?(?:128|117)["\s][^>]*' +
                          r'alt="(?P<alt>[^"]*)',
                          contents)
    if photo_alt is not None:
        scrape_photo_alt(fname, rv, photo_alt.group('alt'))
    else: rv['no_photo_found'] = True

    scrape_by_headers(rv, person_fields, contents)
    scrape_table(rv, person_fields, contents)

    return rv

from pprint import pprint, pformat
def scrape_state_demographics(rv, html):
    median = re.search(r'Median Household Income: .*?\$([0-9,.]*)', html)
    if not median: return
    rv.setdefault('state', {})
    if median: rv['state']['Median income'] = median.group(1)
    poverty = re.search(r'([\d.]*%) are below the poverty line', html)
    if poverty: rv['state']['Poverty status'] = poverty.group(1)
    population = re.search(r'Population in (\d{4})( \(est\))?.*?([\d,.]+)', html.replace('\n',' ') )
    if population: rv['state']['Pop. %s%s'%(population.group(1),population.group(2))] = population.group(3)
    area = re.search(r'Area Size: .*? ([\d,]*) square miles', html.replace('\n',' '))
    if area: rv['state']['Area size'] = area.group(1)
    

def scrape_state(fname):
    fo = file(fname)
    rv = {'filename': fname}
    contents = fo.read()
    scrape_by_headers(rv, state_fields, contents)
    scrape_state_demographics(rv, contents)
    return rv

def main(files):
    import pprint
    if not files: raise Exception("usage: %s foo.html [bar.html ...]" % sys.argv[0])
    for fname in files:
        print "%s as person:" % fname
        print pprint.pprint(scrape_person(fname))
        print "%s as state:" % fname
        print pprint.pprint(scrape_state(fname))

def dump():
    import glob
    import tools
    
    DATA_DIR = '../data'
    ALMANAC_DIR = DATA_DIR + '/crawl/almanac/nationaljournal.com/pubs/almanac/2008/'

    for fn in glob.glob(ALMANAC_DIR + 'people/*/rep*.htm'):
        tools.export([scrape_person(fn)])
    for fn in glob.glob(ALMANAC_DIR + 'states/*/index.html'):
        tools.export([scrape_state(fn)])

if __name__ == '__main__':
    if '--dump' in sys.argv[1:]:
        dump()
    else:
        main(sys.argv[1:])

########NEW FILE########
__FILENAME__ = almanac_test
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import almanac, re, cgitb
cgitb.enable(format='text')

def ok(a, b): assert a == b, (a, b)
def ok_re(a, b): assert re.search(b, a), (a, b)

root = '../../data/crawl/almanac/nationaljournal.com/pubs/almanac/'

def test_scrape_person(scrape):
    trent_franks = scrape(root + '2004/people/az/rep_az02.htm')
    ok(trent_franks['name'], 'Trent Franks')
    ok(trent_franks['title'], 'Rep.')
    jim_bunning = scrape(root + '2008/people/ky/kys2.htm')
    ok(jim_bunning['name'], 'Jim Bunning')

    # A governor:
    janet_napolitano = scrape(root + '2004/people/az/azgv.htm')
    ok(janet_napolitano['name'], 'Janet Napolitano')
    ok(janet_napolitano['title'], 'Gov.')

    # "Representative at large", "R-At Large"
    don_young = scrape(root + '2004/people/ak/rep_ak01.htm')
    ok(don_young['name'], 'Don Young')
    ok(don_young['party'], 'R')

    # I have no idea what "DFL" means, and I'm not sure whether it's
    # an abbreviation for a political party or not.  I've also seen
    # "CFL" in Lieberman's page.
    mark_dayton = scrape(root + '2004/people/mn/mns1.htm')
    ok(mark_dayton['name'], 'Mark Dayton')
    ok(mark_dayton['party'], 'DFL')

    # This one is actually wrong; the guy's name is
    # Rubn Hinojosa, not
    # Ruben Hinojosa.  But the accent is missing in the file.
    hinojosa = scrape(root + '2004/people/tx/rep_tx15.htm')
    ok(hinojosa['name'], 'Ruben Hinojosa')

    # The junior Senator from Connecticut thinks he's an "Independent
    # Democrat".
    lieberman = scrape(root + '2008/people/ct/cts2.htm')
    ok(lieberman['name'], 'Joe Lieberman')
    ok(lieberman['party'], 'ID')

    #@@ currently can't find photo in
    # 2004/people/wi/rep_wi09.htm

    # Unusual title: "Del.", for "delegate" (from Guam)
    madeleine_bordallo = scrape(root + '2006/people/gu/rep_gu01.htm')
    ok(madeleine_bordallo['name'], 'Madeleine Bordallo')

    # Office locations.
    ok_re(trent_franks['dcoffice'], '202-225-4576')
    ok_re(jim_bunning['dcoffice'], '202-224-4343')

    # Religion
    ok(trent_franks['religion'], 'Baptist')

    # Older files.
    robert_byrd = scrape(root + '2000/people/wv/wvs1.htm')
    ok(robert_byrd['name'], 'Robert C. Byrd')

    # President
    pres = scrape(root + '2000/people/president.htm')
    # ...but there's no code to get anything useful out of that yet.

    # Jesse Ventura: Independent Party
    wrestler = scrape(root + '2002/people/mn/mngv.htm')
    ok(wrestler['name'], 'Jesse Ventura')

    # Proper handling of <br> tags.
    steve_pearce = scrape(root + '2008/people/nm/rep_nm02.htm')
    ok(steve_pearce['name'], 'Steve Pearce')
    ok_re(steve_pearce['stateoffice'], r'505-392-8325;\s+Las Cruces')

    # Quotes around magical image height of 117
    gordon_smith = scrape(root + '2000/people/or/ors2.htm')
    ok(gordon_smith['name'], 'Gordon H. Smith')
    ok_re(gordon_smith['dcoffice'], r'202-224-3753')

    # Some inconsistency.  This is the same guy in the same district.
    # "The 5th Congressional District of Wisconsin, numbered the 9th
    # before 2002 redistricting..."
    sensenbrenner_1 = scrape(root + '2006/people/wi/rep_wi05.htm')
    ok(sensenbrenner_1['name'], 'Jim Sensenbrenner')
    sensenbrenner_2 = scrape(root + '2002/people/wi/rep_wi09.htm')
    ok(sensenbrenner_2['name'], 'F. James Sensenbrenner Jr.')

    # Aaron says:
    # The data I was really hoping to get, though, were the numbers at
    # the bottom, especially the previous years' election results, and
    # district size, density, and income numbers.

    ## Election results:
    # plain HTML version
    #jim_bunning_elections = jim_bunning['electionresults_html']
    #ok_re(jim_bunning_elections, r'873,507')

    jbe2 = jim_bunning['electionresults']
    ok(jbe2[0], { 'election': '2004 general',
                  'candidate': 'Jim Bunning (R)',
                  'totalvotes': '873,507',
                  'percent': '51%',
                  'expenditures': '$6,075,399' })
    ok(jbe2[1], { 'election': '2004 general',
                  'candidate': 'Daniel Mongiardo (D)', 
                  'totalvotes': '850,855',
                  'percent': '49%',
                  'expenditures': '$3,104,981' })
    ok(jbe2[2]['election'], '2004 primary')
    ok(jbe2[2].get('expenditures'), None) # not known
    ok(jbe2[6]['candidate'], 'Other')

    ok(trent_franks['electionresults'][2], 
       {'election': '2002 general', 
        'candidate': 'Edward Carlson (Lib)',
        'totalvotes': '5,919',
        'percent': '4%' })

    ok(janet_napolitano['electionresults'][9],
       {'election': '1998 general',
        'candidate': 'Paul Johnson (D)',
        'totalvotes': '361,552',
        'percent': '36%'})

    # At present the code doesn't deal with the headerless tables from
    # the 2000 'wrestler' and 'robert_byrd' pages.  Dealing with those
    # will presumably require guessing column meanings from their
    # contents.

    sper = steve_pearce['electionresults']
    ok(sper[0], {'election': '2006 general', 
                 'candidate': 'Steve Pearce (R)', 
                 'totalvotes': '92,620',
                 'percent': '59%', 
                 'expenditures': '$1,349,394'})
    #@@ here's what it says in the table, but this is an ontological error:
    ok(sper[2]['totalvotes'], 'Unopposed')

    ## District demographics
    demog = steve_pearce['demographics']
    ok(demog['Area size'], '69,598 sq. mi.')
    ok(demog['Ancestry'], 'German: 7.4%; English: 5.9%; Irish: 5.7%;')
    ok(demog['Poverty status'], '22.4%')
    ok(demog['Median income'], '$29,269')

    ## interest group ratings
    ratings = steve_pearce['interest_group_rating']
    ok(ratings['2006']['ADA'], 5)
    ok(ratings['2005']['ADA'], 0)
    ok(ratings['2006']['ACLU'], 18)
    ok('ACLU' in ratings['2005'], False)  # value in HTML is '-'

    ok(sensenbrenner_1['interest_group_rating']['2004']['NTU'], 83)
    ok(jim_bunning['interest_group_rating']['2006']['FRC'], 87)

def test_scrape_state(scrape):
    ## State demographics
    north_dakota = scrape(root + '2008/states/nd/index.html')
    ok(north_dakota['state']['Pop. 2006 (est)'], '635,867')
    ok(north_dakota['state']['Poverty status'], '11.9%')
    ok(north_dakota['state']['Median income'], '$34,604')

if __name__ == '__main__':
    test_scrape_person(almanac.scrape_person)
    test_scrape_state(almanac.scrape_state)

########NEW FILE########
__FILENAME__ = census


## This assumes that the following files have been downloaded from the census
## bureau:
##     all_0Final_National.zip
##     all_0_National-part1.zip
##     all_0_National-part2.zip
##     sl500-in-sl010-us_h10.zip
##     sl500-in-sl010-us_s10.zip
##
## See: ../crawl/census.sh


import codecs
import csv
import fnmatch
import glob
import os
import re
import string
import sys
import time
import zipfile
from pprint import pprint, pformat

import fixed_width

DATA_DIR='../data/crawl/census/census_data'
#DATA_DIR='../../data/crawl/census/census_data/'
SAS_FORMAT='[sS][fF]%(type)d%(table)02d.[sS][aA][sS]'
ST_FORMAT='%(state)s000%(table)02d.uf%(type)d'
ST_GEO_FORMAT='%(state)sgeo.uf%(type)d'
UF_FORMAT='us000%(table)02d.uf%(type)d'
US_GEO_FORMAT='usgeo.uf%(type)d'
CONGRESS_DAT_FORMAT='sl500-in-sl040-%(state)s000%(table)02d.%(type_c)s10'
CONGRESS_GEO_FORMAT='sl500-in-sl040-%(state)sgeo.%(type_c)s10'
ST_CONGRESS_DAT_FORMAT='%(state)s000%(table)02d_%(type_c)s10'
ST_CONGRESS_GEO_FORMAT='%(state)sgeo_%(type_c)s10'
REDISTRICT_DAT_FORMAT='%(state)s000%(table)02d.upl'
REDISTRICT_GEO_FORMAT='%(state)sgeo.upl'
ALL_TABLES = { 1: range(1,40), 3: range(1,77) }
_text_encoding = 'latin-1' #'utf-8'

CENSUSSTATES = { 
        '01':'AL', '02':'AK', '04':'AZ', '05':'AR', '06':'CA', '08':'CO',
        '09':'CT', '10':'DE', '11':'DC', '12':'FL', '13':'GA', '15':'HI',
        '16':'ID', '17':'IL', '18':'IN', '19':'IA', '20':'KS', '21':'KY',
        '22':'LA', '23':'ME', '24':'MD', '25':'MA', '26':'MI', '27':'MN',
        '28':'MS', '29':'MO', '30':'MT', '31':'NE', '32':'NV', '33':'NH',
        '34':'NJ', '35':'NM', '36':'NY', '37':'NC', '38':'ND', '39':'OH',
        '40':'OK', '41':'OR', '42':'PA', '44':'RI', '45':'SC', '46':'SD',
        '47':'TN', '48':'TX', '49':'UT', '50':'VT', '51':'VA', '53':'WA',
        '54':'WV', '55':'WI', '56':'WY', '60':'AS', '66':'GU', '69':'MP',
        '72':'PR', '78':'VI', }

state_list = sorted(map(string.lower,CENSUSSTATES.values()))

SUMLEVs = { 
    '010':'US',
    '040':'STATE',
    '050':'COUNTY',
    '060':'COUNTYSUB',
    '070':'COUNTYSUBPLACE',
    '140':'TRACT',      # TRACT for countys
    '500':'DISTS',
    '511': 'TRACT',     # TRACT for districts
    '860':'ZCTA',       # National sf1 files
    '871':'ZCTA',       # State sf1 files
#    '080':'BLOCK',      # BLOCK for sf3
    '101':'BLOCK',      # BLOCK for sf1
#    '090':'BLKGRP',     # BLKGRP for sf3
    '091':'BLKGRP',     # BLKGRP for sf1
    '750':'BLOCK',      # BLOCK for Redistricting files
}

##TODO: some of these should be procesed as types other than strings.
GeoFields = {
        'D':[
            ('FILEID',    6,   fixed_width.string),
            ('STUSAB',    2,   fixed_width.string),
            ('SUMLEV',    3,   fixed_width.table_lookup(SUMLEVs)),
            #('SUMLEV',    3,   fixed_width.enum(**SUMLEVs)),
            ('GEOCOMP',   2,   fixed_width.string),
            ('CHARITER',  3,   fixed_width.string),
            ('CIFSN',     2,   fixed_width.string),
            ('LOGRECNO',  7,   fixed_width.integer),
            ('REGION',    1,   fixed_width.string),
            ('DIVISION',  1,   fixed_width.string),
            ('STATECE',   2,   fixed_width.string),
            ('STATE',     2,   fixed_width.table_lookup(CENSUSSTATES,  string.strip)),
            #('STATE',     2,   fixed_width.enum(**CENSUSSTATES)),
            ('COUNTY',    3,   fixed_width.string),
            ('COUNTYSC',  2,   fixed_width.string),
            ('COUSUB',    5,   fixed_width.string),
            ('COUSUBCC',  2,   fixed_width.string),
            ('COUSUBSC',  2,   fixed_width.string),
            ('PLACE',     5,   fixed_width.string),
            ('PLACECC',   2,   fixed_width.string),
            ('PLACEDC',   1,   fixed_width.string),
            ('PLACESC',   2,   fixed_width.string),
            ('TRACT',     6,   fixed_width.string),
            ('BLKGRP',    1,   fixed_width.string),
            ('BLOCK',     4,   fixed_width.string),
            ('IUC',       2,   fixed_width.string),
            ('CONCIT',    5,   fixed_width.string),
            ('CONCITCC',  2,   fixed_width.string),
            ('CONCITSC',  2,   fixed_width.string),
            ('AIANHH',    4,   fixed_width.string),
            ('AIANHHFP',  5,   fixed_width.string),
            ('AIANHHCC',  2,   fixed_width.string),
            ('AIHHTLI',   1,   fixed_width.string),
            ('AITSCE',    3,   fixed_width.string),
            ('AITS',      5,   fixed_width.string),
            ('AITSCC',    2,   fixed_width.string),
            ('ANRC',      5,   fixed_width.string),
            ('ANRCCC',    2,   fixed_width.string),
            ('MSACMSA',   4,   fixed_width.string),
            ('MASC',      2,   fixed_width.string),
            ('CMSA',      2,   fixed_width.string),
            ('MACCI',     1,   fixed_width.string),
            ('PMSA',      4,   fixed_width.string),
            ('NECMA',     4,   fixed_width.string),
            ('NECMACCI',  1,   fixed_width.string),
            ('NECMASC',   2,   fixed_width.string),
            ('EXI',       1,   fixed_width.string),
            ('UA',        5,   fixed_width.string),
            ('UASC',      2,   fixed_width.string),
            ('UATYPE',    1,   fixed_width.string),
            ('UR',        1,   fixed_width.string),
            ('CD106',     2,   fixed_width.string),
            ('CD108',     2,   fixed_width.string),
            ('CD109',     2,   fixed_width.string),
            ('CD110',     2,   fixed_width.string),
            ('SLDU',      3,   fixed_width.string),
            ('SLDL',      3,   fixed_width.string),
            ('VTD',       6,   fixed_width.string),
            ('VTDI',      1,   fixed_width.string),
            ('ZCTA3',     3,   fixed_width.string),
            ('ZCTA5',     5,   fixed_width.string),
            ('SUBMCD',    5,   fixed_width.string),
            ('SUBMCDCC',  2,   fixed_width.string),
            ('AREALAND',  14,  fixed_width.integer),
            ('AREAWATR',  14,  fixed_width.integer),
            ('NAME',      90,  fixed_width.string),
            ('FUNCSTAT',  1,   fixed_width.string),
            ('GCUNI',     1,   fixed_width.string),
            ('POP100',    9,   fixed_width.integer),
            ('HU100',     9,   fixed_width.integer),
            ('INTPLAT',   9,   fixed_width.integer),
            ('INTPLON',   10,  fixed_width.integer),
            ('LSADC',     2,   fixed_width.string),
            ('PARTFLAG',  1,   fixed_width.string),
            ('SDELEM',    5,   fixed_width.string),
            ('SDSEC',     5,   fixed_width.string),
            ('SDUNI',     5,   fixed_width.string),
            ('TAZ',       6,   fixed_width.string),
            ('UGA',       5,   fixed_width.string),
            ('PUMA5',     5,   fixed_width.string),
            ('PUMA1',     5,   fixed_width.string),
            ('RESERVE2',  15,  fixed_width.string),
            ('MACC',      5,   fixed_width.string),
            ('UACP',      5,   fixed_width.string),
            ('RESERVED',  7,   fixed_width.string),
        ]}


def parse_geo_file(fn, args):
    GF= {'D': list(GeoFields['D'])}
    if 'usgeo' in fn or 'by_state' in fn:
        # The geo files for usgeo.* use dos line breaks...
        GF['D'].append((None, 2, fixed_width.filler))
    else:
        # ... the congress geo files use unix line breaks.
        GF['D'].append((None, 1, fixed_width.filler))
    GF['D'].append(('geo_file', 0, lambda x: fn))
    print fn
    #file = codecs.open(fn, 'r', encoding=_text_encoding)
    file = getFile(os.path.dirname(fn), os.path.basename(fn), args)
    return fixed_width.parse_file(GF, file,lambda x:'D')


def makePath(l):
    p = ""
    for i,x in enumerate(l):
        x = x.strip()
        x[0].lower()
        x = x.replace('-','')
        x = x.split()[0] + ''.join(string.capwords(x).split()[1::])
        x = ''.join(filter(lambda c: c not in string.punctuation,list(x)))
        if i != len(l)-1 and x == "Total":
            continue
        p = "%s/%s" % (p,x)
    return p

def getIndent(s):
    return len(s) - len(s.lstrip())

def getFile(dir, fn_format, args1=None):
    path_to_zipfiles = {
            DATA_DIR + '/table_layouts': ["SF%(type)sSAS.zip", ],
            DATA_DIR + '/by_state': ["%(state)s000%(table)02d_uf%(type)d.zip", "%(state)sgeo_uf%(type)d.zip"],
            DATA_DIR + '/congress': ["%(state)s000%(table)02d_%(type_c)s10.zip", "%(state)sgeo_%(type_c)s10.zip", "sl500-in-sl040-%(state)s_%(type_c)s10.zip"],
            DATA_DIR: ["us%(table)05d_uf%(type)d.zip", "usgeo_uf%(type)d.zip"], 
            }
    args = dict(type=None, type_c=None, table=None, state=None)
    if args1: args.update(args1)
    type_c = { 1: 'h', 3:'s'}
    if not args['type_c'] and args['type']: args['type_c']=type_c[args['type']]
    #print pformat(args), dir, fn_format
    fn = glob.glob((dir+'/'+fn_format) % args)
    #if False and fn: # For now force from zipfile.
    if fn:
        if len(fn) != 1: return None # Woah, matched multiple files
        print fn[0]
        return codecs.open(fn[0], 'r', encoding=_text_encoding)
    else: # Try zip files
        r = re.compile(fnmatch.translate(fn_format%args))
        for zipfn in path_to_zipfiles[dir]:
            if not glob.glob(dir+'/'+zipfn%args): continue
            zf = zipfile.ZipFile(dir+'/'+zipfn%args)
            files = zf.namelist()
            for f in files: 
                if r.match(f):
                    print "%s/%s -> %s" % (dir, zipfn%args, f)
                    return codecs.EncodedFile(zf.open(f), _text_encoding) ## Requires zipfile from python 2.6
                    #return zf.read(f).splitlines(True)
    print "Couldn't find file %s in %s." % \
            ((dir+'/'+fn_format) % args, pformat(map(lambda x: (x % args), path_to_zipfiles[dir])))
    return []


def parse_sas_file(type, table, pathMap={}):
    #TODO: - Should be able to handle LENGTH section 
    #        and '... $ start-end' input lines.
    labelRE = re.compile(r'^\s*.*?\s*(\S+)=\'(.*)\'\s*.*$')
    inputRE = re.compile(r'^\s*([^\s;]+)\s*.*$')
    universeRE = re.compile(r'\/\*Universe: (.+)*\*\/')
    labelMap = {}    # Mapping from the census keys to the lables
    fieldList = []   # ordered list of fields
    path=[]          # 
    state = 'INIT'
    
    for line in getFile(DATA_DIR+'/table_layouts',SAS_FORMAT, {'type':type,'table':table}):
        if state == 'INIT':
            if 'LABEL' in line:
                state = 'LABEL'
        if state == 'LABEL':
            if 'INPUT' in line:
                state = 'INPUT'
                continue
            for u in universeRE.findall(line):
                path = [u]
            for l in labelRE.findall(line):
                labelMap[l[0]] = l[1]
                if path:
                    l_indent = getIndent(l[1])
                    while len(path)>1: # don't pop universe
                        curIndent=getIndent(path[-1])
                        if l_indent > curIndent: break
                        path.pop()
                    path.append(l[1])
                    path_str = makePath(path) 
                    if path_str not in pathMap:
                        pathMap[path_str] = set([ l[0] ])
                    else:  
                        pathMap[path_str].add( l[0] )
        if state == 'INPUT':
            if 'RUN' in line:
                break
            for i in inputRE.findall(line):
                fieldList.append(i)
    return (fieldList,labelMap,pathMap)


def parse_state_sum_file(type, table, state, layout):
    FIELDs = layout[0]
    args = { 'state':state, 'type':type, 'table':table }
    dat_fn = (DATA_DIR + '/by_state/' + ST_FORMAT) % args
    geo_fn = (DATA_DIR + '/by_state/' + ST_GEO_FORMAT) % args
    return _parse_sum_file(dat_fn, geo_fn, FIELDs, args)

def parse_congress_file(type, table, state, layout, use_st=True):
    FIELDs = layout[0]
    type_c = { 1: 'h', 3:'s'}
    args = {'type_c':type_c[type], 'state':state, 'table':table, 'type':type}
    if use_st:
        dat_fn = DATA_DIR + '/congress/' + ST_CONGRESS_DAT_FORMAT % args
        geo_fn = DATA_DIR + '/congress/' + ST_CONGRESS_GEO_FORMAT % args
    else:
        dat_fn = DATA_DIR + '/congress/' + CONGRESS_DAT_FORMAT % args
        geo_fn = DATA_DIR + '/congress/' + CONGRESS_GEO_FORMAT % args
    return _parse_sum_file(dat_fn, geo_fn, FIELDs, args)

def parse_redistrict_file(type, table, state, layout):
    FIELDs = layout[0]
    args = {'state':state, 'table':table, 'type':type}
    dat_fn = DATA_DIR + '/Redistrict/' + REDISTRICT_DAT_FORMAT % args
    geo_fn = DATA_DIR + '/Redistrict/' + REDISTRICT_GEO_FORMAT % args
    return _parse_sum_file(dat_fn, geo_fn, FIELDs, args)

def parse_sum_file(type, table, layout):
    FIELDs = layout[0]
    args = { 'type':type, 'table':table }
    dat_fn = (DATA_DIR + '/' + UF_FORMAT) % args
    geo_fn = (DATA_DIR + '/' + US_GEO_FORMAT) % args
    return _parse_sum_file(dat_fn, geo_fn, FIELDs, args)


def _parse_sum_file(dat_fn, geo_fn, FIELDs, args):
    file = getFile(os.path.dirname(dat_fn),os.path.basename(dat_fn), args)
    if not file: return
    c = csv.reader(file)
    for row in c:
        d = dict(zip(FIELDs,row))
        if geo_fn: d['geo_file'] = (geo_fn, args)
        yield d


### ARRG, we *MUST* be able to get blocks within districts, they even give us a
#   file that tells us when there is overlap:
#   http://www.census.gov/geo/www/cd110th/spblk110.txt
def parse_state_sum_files(types=[1,3], ReqKeyList=None):
    # About 19403085 records!
    type = 1
    table = 1
    all_keys = {}
    layout = parse_sas_file(type, table,all_keys)
    for state in state_list:
        ## Check if this file has any of the keys we want.
        if ReqKeyList :
            want = set(ReqKeyList)
            have = set(layout[0])
            if not have.intersection(want): continue
        #pprint(layout[1])
        for parser_fn in [parse_congress_file, parse_state_sum_file]:
            numRows = 0
            start_time = time.time()
            for row in parser_fn(type, table, state, layout):
                row['type']=type
                row['layout']=layout
                numRows += 1
                yield row
            print "Summary file processed with %d rows in %.4f seconds." % \
                    (numRows, time.time()-start_time)

def parse_sum_files(types=[1,3], ReqKeyList=None):
    # About 19403085 records!
    tables = ALL_TABLES
    for type in types:
        all_keys = {}
        for table in tables[type]:
            layout = parse_sas_file(type, table, all_keys)
            ## Check if this file has any of the keys we want.
            if ReqKeyList :
                want = set(ReqKeyList)
                have = set(layout[0])
                if not have.intersection(want): continue
            #pprint(layout[1])
            numRows = 0
            start_time = time.time()
            for row in parse_sum_file( type, table, layout ):
                row['type']=type
                row['layout']=layout
                numRows += 1
                yield row
            print "Summary file processed with %d rows in %.4f seconds." % \
                    (numRows, time.time()-start_time)
            numRows = 0
            start_time = time.time()
            for state in state_list:
                for row in parse_congress_file(type, table, state, layout, use_st=False):
                    row['type']=type
                    row['layout']=layout
                    numRows += 1
                    yield row
            print "Congress summary file(s) processed with %d rows in %.4f seconds." % \
                    (numRows, time.time()-start_time)

################################################################################
def print_sum_files():
    geoTables = {}
    for row in parse_sum_files():
        (geo_file, geo_args) = row['geo_file']
        if geo_file in geoTables:
            geo = geoTables[geo_file]
        else:
            geoTables[geo_file] = build_geo_table(geo_file, geo_args)
            geo = geoTables[geo_file]
        layout = row['layout']
        if row['LOGRECNO'] in geo:
            print "Found geo data for:", row['LOGRECNO'], "as", geo[row['LOGRECNO']]
        else: 
            print "Didn't find geo data for:",row['LOGRECNO']
        kk = row.keys()
        kk.remove('layout')
        kk.remove('type')
        kk.sort()
        for k in kk:
            if k in layout[1]:
                print '%-45s .......... %s'%(layout[1][k][0:45],row[k])
            else:
                print '%-45s .......... %s'%(k,row[k])
        print '='*80


def build_geo_table(fn, args):
    LOGRECNOs = {}
    numRows=0
    start_time = time.time()
    for row in parse_geo_file(fn, args):
        numRows += 1
        if row['GEOCOMP'] != '00':  ## We only care about geo code 00 for now.
            continue
        LOGRECNOs[row['LOGRECNO']] = row
    print "Geo table processed with %d rows in %.4f seconds." % \
            (numRows, time.time()-start_time)
    return LOGRECNOs


def process_all_sas():
    tables = { 1: range(1,40), 3: range(1,77) }
    for type in [1,3]:
        all_keys = {}
        for table in tables[type]:
            layout = parse_sas_file(type,table,all_keys)
        pprint(all_keys)

################################################################################
if __name__ == "__main__":
    #process_all_sas()
    print_sum_files()
    ## Just to do the following iteration through the data takes about an hour.
    #for row in parse_sum_files():
    #    pass

########NEW FILE########
__FILENAME__ = earmarks
#!/usr/bin/env python
"""
A parser for the 2008 earmarks data (XLS format) from http://taxpayer.net/

This script depends on xls2list which will convert the excel file to a 2d array. 
It then does some trivial parsing of each field and outputs the data in a few ways.
"""
__author__ = ['Alex Gourley <acgourley@gmail.com>',
              'Aaron Swartz <me@aaronsw.com>']

EARMARK_FILE = '../data/crawl/taxpayer/bigkahuna.xls'

import sys, re
import web
import xls2list

fmt = (
  'id',
  'house_request', 'senate_request', 
  'prereduction_amt', 'final_amt', 'budget_request',
  'description', 'city', 'county', 'state',
  'bill', 'bill_section', 'bill_subsection', 'project_heading',
  'house_member', 'house_party', 'house_state', 'district',
  'senate_member', 'senate_party', 'senate_state',
  'presidential', 'undisclosed', 'intended_recipient',
  'notes'
)

def parse_row(row):
    out = web.storage()
    for n, item in enumerate(fmt):
        out[item] = row[n]
    out.house_member = (out.house_member or []) and [x.strip() for x in out.house_member.split(';')]
    out.senate_member = (out.senate_member or []) and [x.strip() for x in out.senate_member.split(';')]
    #out.state = (out.state or []) and [x.strip() for x in out.state.split(';')]
    return out

def parse_file(fn):
    """Break down the xls into a 2d data array, stripping off first rows which do not have data."""
    data = xls2list.xls2list(fn)
    for n, row in enumerate(data[3:]):
        r = parse_row(row)
        # All of the earmarks have a description, stop when we finish all
        # earmarks
        if not r.description: break 
        # The id's aren't remotely uniq, map to something that is
        r.id=n+1 # Lets start at 1 instead of 0
        yield r

if __name__ == "__main__":
    import tools
    tools.export(parse_file(EARMARK_FILE))

########NEW FILE########
__FILENAME__ = fec_cobol
"""
Parser for FEC Files for files that conform to webl.txt and weball.txt
"""

__author__ = [
  "Jeremy Schwartz <jerschwartz@gmail.com>",
  "Aaron Swartz <me@aaronsw.com>",
]

import re, os, sys, gzip, glob
import web
from fixed_width import get_len, enum, filler, parse_file

COBOL_INT_TABLE = dict(']0 j1 k2 l3 m4 n5 o6 p7 q8 r9'.split())
def integer(d):
    if not d or d[0] == '?':
        return d
    elif d[-1].lower() in COBOL_INT_TABLE:
        d1 = d[:-1] + COBOL_INT_TABLE[d[-1].lower()]
        return -int(d1)
    elif d[0] == '+':
        return int(d[1:])
    elif d[0] == "-":
        return -int(d[1:])
    else:
        return int(d)

def string(d):
    # \x98 seems to be a typo
    return d.replace('\x98', '').decode('cp1251').rstrip()

def date99(d):
    """where `d` is like MMDDYY"""
    return ('19' + d[4:6] + "-" + d[0:2] + "-" + d[2:4]).replace(' ', '0')

def date(d):
    """where `d` is like MMDDYYYY"""
    return (d[4:8] + "-" + d[0:2] + "-" + d[2:4]).replace(' ', '0')

def date2(d):
    "??DDMMYYYY"
    return (d[6:10] + "-" + d[4:6] + "-" + d[2:4]).replace(' ', '0')

party = enum(**{"1": "Democratic", "2": "Republican", "3": "Other"})
ico = enum(**{" ": " ", "I": "Incumbent", "C": "Challenger", "O": "Open Seat"})
pgi = enum(**{
  'C': "CONVENTION", 
  'G': "GENERAL", 
  'P': "PRIMARY", 
  'R': "RUNOFF", 
  'S': "SPECIAL",
  '0': '0',
  '2': '2',
  '4': '4',
  '5': '5',
  '6': '6',
  '8': '8'
})
filing_freq = enum(M="MONTHLY", Q="QUARTERLY", T="TERMINATED")

amendment = enum(
  A="AMENDMENT", 
  C="CONSOLIDATED", 
  M="MULTI-CANDIDATE", 
  N="NEW", 
  S="SECONDARY", 
  T="TERMINATED"
)

cmte_type = enum(
    C="COMMUNICATION COST",
    D="DELEGATE",
    H="HOUSE",
    I="INDEPENDENT EXPENDITURE (PERSON OR GROUP, NOT A COMMITTEE)",
    N="NON-PARTY NON-QUALIFIED",
    P="PRESIDENTIAL",
    Q="QUALIFIED NON-PARTY (SEE 2 USC 441(A)(4))",
    S="SENATE",
    X="NON-QUALIFIED PARTY",
    Y="QUALIFIED PARTY (SEE 2 USC 441(A)(4))",
    Z="NATIONAL PARTY ORGANIZATION. NON FED ACCT."
)

cmte_desig = enum(
    A="AUTHORIZED BY A CANDIDATE",
    J="JOINT FUND RAISER",
    P="PRINCIPAL CAMPAIGN COMMITTEE OF A CANDIDATE",
    U="UNAUTHORIZED"
)

def_webl = [
  ('_type', 0, lambda x: 'Candidate'),
  ("candidate_id", 9, string),
  ("candidate_name", 38, string),
  ("ico", 1, ico),
  ("party", 1, party),
  ("party_desig", 3, string),
  ("total_receipts", 10, integer),
  ("auth_trans_from", 10, integer),
  ("total_disbursements", 10, integer),
  ("trans_to_auth", 10, integer),
  ("begin_cash", 10, integer),
  ("end_cash", 10, integer),
  ("contrib_from_candidate", 10, integer),
  ("loans_from_candidate", 10, integer),
  ("other_loans", 10, integer),
  ("candidate_loan_repay", 10, integer),
  ("other_loan_repay", 10, integer),
  ("debts_owed_by", 10, integer),
  ("total_indiv_contrib", 10, integer),
  ("state_code", 2, string),
  ("district", 2, string),
  ("spec_elec_status", 1, enum),
  ("primary_elec_status", 1, enum), #@@primary_general?
  ("runoff_elec_status", 1, enum),
  ("general_elec_status", 1, enum),
  ("general_elec_pct", 3, string),
  ("contrib_from_other_pc", 10, integer),
  ("contrib_from_pc", 10, integer),
  ("end_date", 8, date),
  ("refunds_to_indiv", 10, integer),
  ("refunds_to_commit", 10, integer),
  (None, 2, filler('\r\n'))
]

# Supports files for CANSUM04 CANSUM02 CANSUM00 CANSUM98 CANSUM96
def_cansum = [
  ('_type', 0, lambda x: 'Cadidate'),
  ("candidate_id", 9, string),
  ("candidate_name", 38, string),
  ("ico", 1, ico),
  ("party", 1, party),
  ("party_desig", 3, string),
  ("total_receipts", 10, integer),
  ("auth_trans_from", 10, integer),
  ("total_disbursments", 10, integer),
  ("trans_from_auth", 10, integer),
  ("begin_cash", 10, integer),
  ("end_cash", 10, integer),
  ("contrib_from_candidate", 10, integer),
  ("loans_from_candidate", 10, integer),
  ("other_loans", 10, integer),
  ("candidate_loan_repay", 10, integer),
  ("other_loan_repay", 10, integer),
  ("debts_owed_by", 10, integer),
  ("contrib_200_499", 10, integer),
  ("total_200_499", 10, integer),
  ("contrib_500_749", 10, integer),
  ("total_500_749", 10, integer),
  ("contrib_750", 10, integer),
  ("total_750", 10, integer),
  ("total_indiv_contrib", 10, integer),
  ("major_pty_contrib", 10, integer),
  ("party_indep_expend_for", 10, integer),
  ("corp_contrib", 10, integer),
  ("labor_contrib", 10, integer),
  ("non_connected_contrib", 10, integer),
  ("tmh_contrib", 10, integer),
  ("coop_contrib", 10, integer),
  ("corp_wo_stock_contrib", 10, integer),
  ("non_party_exp", 10, integer),
  ("non_party_exp_agn", 10, integer),
  ("indep_exp_for", 10, integer),
  ("indep_exp_agn", 10, integer),
  ("comm_cost_for", 10, integer),
  ("comm_cost_agn", 10, integer),
  ("state_code", 2, string),
  ("district", 2, string),
  ("spec_elec_status", 1, string),
  ("primary_elec_status", 1, string), #@@primary_general?
  ("runoff_elec_status", 1, string),
  ("gen_elec_status", 1, string),
  ("gen_elec_pct", 7, integer),
  ("spec_elec_cand", 1, string),
  ("party_coord_exp", 10, integer),
  ("party_indep_exp", 10, integer),
  (None, 2, filler('\r\n'))
]

# Supports format CANSUM94 CANSUM92
def_cansum92 = [
  ('_type', 0, lambda x: 'Candidate'),
  ("candidate_id", 9, string),
  ("candidate_name", 38, string),
  ("ico", 1, ico),
  ("party", 1, party),
  ("party_desig", 3, string),
  ("total_reciepts", 10, integer),
  ("auth_trans_from", 10, integer),
  ("total_disbursments", 10, integer),
  ("trans_to_auth", 10, integer),
  ("begin_cash", 10, integer),
  ("end_cash", 10, integer),
  ("contrib_from_cand", 10, integer),
  ("loans_from_cand", 10, integer),
  ("other_loans", 10, integer),
  ("cand_loans_repay", 10, integer),
  ("other_loan_repay", 10, integer),
  ("debts_owned_by", 10, integer),
  ("contrib_200_499", 10, integer),
  ("total_200_499", 10, integer),
  ("contrib_500_749", 10, integer),
  ("total_500_749", 10, integer),
  ("contrib_750", 10, integer),
  ("total_750", 10, integer),
  ("total_indiv_contrib", 10, integer),
  ("major_pty_contrib", 10, integer),
  ("pty_coord_expend", 10, integer),
  ("corp_contrib", 10, integer),
  ("labor_contrib", 10, integer),
  ("non_connect_cont", 10, integer),
  ("tmh_contrib", 10, integer),
  ("coop_contrib", 10, integer),
  ("corp_wo_stock", 10, integer),
  ("non_party_exp", 10, integer),
  ("non_party_exp_agn", 10, integer),
  ("indep_exp_for", 10, integer),
  ("indep_exp_agn", 10, integer),
  ("comm_cost_for", 10, integer),
  ("comm_cost_agn", 10, integer),
  ("state_code", 2, string),
  ("district", 2, string),
  ("spec_elec_status", 1, string),
  ("primary_elec_status", 1, string), #@@primary_general?
  ("runoff_elec_status", 1, string),
  ("gen_elec_status", 7, string),
  ("spec_elec_cand", 1, string),
  (None, 2, filler('\r\n'))
]

def_cansum90 = [
  ('_type', 0, lambda x: 'Candidate'),
  ("candidate_id", 9, string),
  ("candidate_name", 38, string),
  ("ico", 1, ico),
  ("fill", 10, string),
  ("party", 1, party),
  ("party_desig", 3, string),
  ("total_reciepts", 12, integer),
  ("auth_trans_from", 12, integer),
  ("total_disbursments", 12, integer),
  ("trans_to_auth", 12, integer), #9
  ("begin_cash", 12, integer),
  ("end_cash", 12, integer),
  ("contrib_from_cand", 12, integer),
  ("loans_from_cand", 12, integer),
  ("other_loans", 12, integer),
  ("cand_loans_repay", 12, integer),
  ("other_loan_repay", 12, integer),
  ("debts_owned_by", 12, integer),
  ("contrib_200_499", 8, integer),
  ("total_200_499", 12, integer), #19
  ("contrib_500_749", 8, integer),
  ("total_500_749", 12, integer),
  ("contrib_750", 8, integer),
  ("total_750", 12, integer),
  ("total_indiv_contrib", 12, integer),
  ("major_pty_contrib", 12, integer),
  ("pty_coord_expend", 12, integer),
  ("corp_contrib", 12, integer),
  ("labor_contrib", 12, integer),
  ("non_connect_cont", 12, integer), #29
  ("tmh_contrib", 12, integer), #30
  ("coop_contrib", 12, integer),
  ("corp_wo_stock", 12, integer),
  ("non_party_exp", 12, integer),
  ("non_party_exp_agn", 12, integer),
  ("indep_exp_for", 12, integer),
  ("indep_exp_agn", 12, integer),
  ("comm_cost_for", 12, integer),
  ("comm_cost_agn", 12, integer), #38
  ("state_code", 2, string),
  ("district", 2, string),
  ("spec_elec_status", 1, string),
  ("primary_elec_status", 1, string), #@@primary_general?
  ("runoff_elec_status", 1, string),
  ("gen_elec_status", 1, string),
  ("spec_elec_cand", 1, string),
  (None, 2, filler('\r\n'))
]

def_cansum88 = [
  ('_type', 0, lambda x: 'Candidate'),
  ("candidate_id", 9, string),
  ("candidate_name", 38, string),
  ("ico", 1, ico),
  ("fill", 10, string),
  ("party", 1, party),
  ("party_desig", 3, string),
  ("total_reciepts", 12, integer),
  ("auth_trans_from", 12, integer),
  ("total_disbursments", 12, integer),
  ("trans_to_auth", 12, integer), #9
  ("begin_cash", 12, integer),
  ("end_cash", 12, integer),
  ("contrib_from_cand", 12, integer),
  ("loans_from_cand", 12, integer),
  ("other_loans", 12, integer),
  ("cand_loans_repay", 12, integer),
  ("other_loan_repay", 12, integer),
  ("debts_owned_by", 12, integer),
  ("contrib_500_749", 8, integer),
  ("total_500_749", 12, integer),
  ("contrib_750", 8, integer),#20
  ("total_750", 12, integer),
  ("total_indiv_contrib", 12, integer),
  ("major_pty_contrib", 12, integer),
  ("pty_coord_expend", 12, integer),
  ("corp_contrib", 12, integer),
  ("labor_contrib", 12, integer),
  ("non_connect_cont", 12, integer),
  ("tmh_contrib", 12, integer),
  ("coop_contrib", 12, integer),
  ("corp_wo_stock", 12, integer),#30
  ("non_party_exp", 12, integer),
  ("non_party_exp_agn", 12, integer),
  ("indep_exp_for", 12, integer),
  ("indep_exp_agn", 12, integer),
  ("comm_cost_for", 12, integer),
  ("comm_cost_agn", 12, integer),
  ("state_code", 2, string),
  ("district", 2, string),
  ("spec_elec_status", 1, string),
  ("primary_elec_status", 1, string), #@@primary_general?
  ("runoff_elec_status", 1, string),
  ("gen_elec_status", 1, string),
  ("spec_elec_cand", 1, string),
  (None, 2, filler('\r\n'))
]

def_webk = [
  ('_type', 0, lambda x: 'PAC/PARTY'),
  ("id", 9, string),
  ("name", 90, string),
  ("type", 1, cmte_type),
  ("desig", 1, cmte_desig),
  ("filing_freq", 1, filing_freq),
  ("total_receipts", 10, integer),
  ("trans_from_aff", 10, string),
  ("contrib_rec_from_indiv", 10, integer),
  ("contrib_rec_from_other_pc", 10, integer),
  ("contrib_from_cand", 10, integer),
  ("cand_loans", 10, integer),
  ("total_loans_rec", 10, integer),
  ("total_disbursment", 10, integer),
  ("trans_to_aff", 10, integer),
  ("refunds_to_indiv", 10, integer),
  ("refunds_to_other_pc", 10, integer),
  ("cand_loan_repayments", 10, integer),
  ("loan_repayments", 10, integer),
  ("cash_begin", 10, integer),
  ("cash_close", 10, integer),
  ("debts_bowned_by", 10, integer),
  ("nonfederal_trans_rec", 10, integer),
  ("contrib_made_to_other", 10, integer),
  ("indep_exped_made", 10, string),
  ("party_coord_expend_made", 10, integer),
  ("nonfederal_share_of_expend", 10, integer),
  ("month", 2, integer),
  ("day", 2, integer),
  ("year", 4, integer),
  (None, 2, filler('\r\n'))
]

#Supporst PACSUM[92-04]
def_pacsum = [
  ('_type', 0, lambda x: 'PAC'),
  ("committee_id", 9, string),
  ("committee_name", 90, string),
  ("sig", 1, string),
  ("end_coverage_date", 6, date),
  ("total_receipts", 10, integer),
  ("trans_from_aff", 10, integer),
  ("contrib_from_party", 10, integer),
  ("contrib_from_non_party", 10, integer),
  ("total_indiv_contrib", 10, integer),
  ("indiv_contrib_200+", 10, integer),
  ("in_kind_contrib", 10, integer),
  ("total_disbursements", 10, integer),
  ("trans_to_aff", 10, integer),
  ("contrib_to_party", 10, integer),
  ("contrib_to_non_party", 10, integer),
  ("indiv_contrib_refund", 10, integer),
  ("begin_cash", 10, integer),
  ("end_cash", 10, integer),
  ("debts_owed_to", 10, integer),
  ("debts_owed_by", 10, integer),
  ("total_in_kind_contrib", 10, integer),
  ("total_1999_contrib", 10, integer),
  ("total_for", 10, integer),
  ("total_against", 10, integer),
  ("pres_contrib_dem", 10, integer),
  ("pres_contrib_rep", 10, integer),
  ("pres_contrib_oth", 10, integer),
  ("senate_contrib_dem", 10, integer),
  ("senate_contrib_rep", 10, integer),
  ("senate_contrib_oth", 10, integer),
  ("house_contrib_dem", 10, integer),
  ("house_contrib_rep", 10, integer),
  ("house_contrib_oth", 10, integer),
  ("senate_inc_contrib", 10, integer),
  ("senate_cha_contrib", 10, integer),
  ("senate_opn_contrib", 10, integer),
  ("house_inc_contrib", 10, integer),
  ("house_cha_contrib", 10, integer),
  ("house_opn_contrib", 10, integer),
  ("non_federal_trans", 10, integer),
  ("non_federal_expend", 10, integer),
  (None, 2, filler('\r\n'))
]

# Supports PACSUM[84-90]
def_pacsum90 = [
  ('_type', 0, lambda x: 'PAC'),
  ("committee_id", 9, string),
  ("committee_name", 90, string),
  ("sig", 1, string),
  ("end_coverage_date", 10, date2),
  ("total_receipts", 12, integer),
  ("contrib_from_aff", 12, integer),
  ("contrib_from_party", 12, integer),
  ("contrib_from_non_party", 12, integer),
  ("indiv_contrib", 12, integer),
  ("total_contrib", 12, integer),
  ("in_kind_contrib", 12, integer),
  ("total_disbursements", 12, integer),
  ("contrib_to_non_party_aff", 12, integer),
  ("contrib_to_party", 12, integer),
  ("contrib_to_non_party", 12, integer),
  ("indiv_contrib_refund", 12, integer),
  ("begin_cash_year_1", 12, integer),
  ("end_cash_year_2", 12, integer),
  ("debts_owed_to", 12, integer),
  ("debts_owed_by", 12, integer),
  ("total_in_kind_contrib", 12, integer),
  ("total_1_contrib", 12, integer),
  ("total_indep_for", 12, integer),
  ("total_indep_against", 12, integer),
  ("pres_contrib_dem", 12, integer),
  ("pres_contrib_rep", 12, integer),
  ("pres_contrib_oth", 12, integer),
  ("senate_contrib_dem", 12, integer),
  ("senate_contrib_rep", 12, integer),
  ("senate_contrib_oth", 12, integer),
  ("house_contrib_dem", 12, integer),
  ("house_contrib_rep", 12, integer),
  ("house_contrib_oth", 12, integer),
  ("senate_inc_contrib", 12, integer),
  ("senate_cha_contrib", 12, integer),
  ("senate_opn_contrib", 12, integer),
  ("house_inc_contrib", 12, integer),
  ("house_cha_contrib", 12, integer),
  ("house_opn_contrib", 12, integer),
  (None, 2, filler('\r\n'))
]


# Supports PACSUM[80-82]
def_pacsum82 = [
  ('_type', 0, lambda x: 'PAC'),
  ("committee_id", 9, string),
  ("committee_name", 90, string),
  ("committee_type", 1, string),
  ("sig", 1, string),
  ("party", 3, string),
  ("not_used", 1, string),
  ("state", 2, string),
  ("total_receipts", 10, integer),
  ("trans_in_party", 10, integer),
  ("trans_in_party_other", 10, integer),
  ("corp_contrib", 10, integer),
  ("labor_contrib", 10, integer),
  ("non_connected_contrib", 10, integer),
  ("tmh_contrib", 10, integer),
  ("coop_contrib", 10, integer),
  ("corp_wo_stock_contrib", 10, integer),
  ("num_cand_contrib_to", 10, integer),
  ("total_party_contrib", 10, integer),
  ("not_used2", 10, string),
  ("not_used3", 10, string),
  ("contrib_500+", 10, integer),
  ("total_disbursements", 10, integer),
  ("trans_out_party_same", 10, integer),
  ("trans_out_party_other", 10, integer),
  ("total_contrib", 10, integer),
  ("not_used4", 80, string),
  ("latest_cash_on_hand", 10, integer),
  ("jan_1_cash_on_hand", 10, integer),
  ("debts_owed_to", 10, integer),
  ("debts_owed_by", 10, integer),
  ("in_kind_contrib", 10, integer),
  ("contrib_refunds", 10, integer),
  ("contrib_to_dem_pres", 10, integer),
  ("contrib_to_rep_pres", 10, integer),
  ("contrib_to_other_pres", 10, integer),
  ("contrib_to_dem_senate", 10, integer),
  ("contrib_to_rep_senate", 10, integer),
  ("contrib_to_other_senate", 10, integer),
  ("contrib_to_dem_house", 10, integer),
  ("contrib_to_rep_house", 10, integer),
  ("contrib_to_other_house", 10, integer),
  ("expend_on_dem_pres", 10, integer),
  ("expend_on_rep_pres", 10, integer),
  ("expend_on_other_pres", 10, integer),
  ("expend_on_dem_senate", 10, integer),
  ("expend_on_rep_senate", 10, integer),
  ("expend_on_other_senate", 10, integer),
  ("expend_on_dem_house", 10, integer),
  ("expend_on_rep_house", 10, integer),
  ("expend_on_other_house", 10, integer),
  ("end_coverage_date", 8, date),
  ("senate_house_inc_contrib", 9, integer),
  ("senate_house_cha_contrib", 9, integer),
  ("senate_house_opn_contrib", 9, integer),
  (None, 2, filler('\r\n'))
]

def_cm = [
  ('_type', 0, lambda x: 'Committee'),
  ("committee_id", 9, string),
  ("committee_name", 90, string),
  ("treasurer_name", 38, string),
  ("street_one", 34, string),
  ("street_two", 34, string),
  ("city", 18, string),
  ("state", 2, string),
  ("zip", 5, string),
  ("committee_desig", 1, enum),
  ("committee_type", 1, enum),
  ("committee_party", 3, enum),
  ("filing_frequency", 1, enum),
  ("interest_group_category", 1, enum(
    C='CORPORATION',
    L='LABOR ORGANIZATION',
    M='MEMBERSHIP ORGANIZATION',
    T='TRADE ASSOCIATION',
    V='COOPERATIVE',
    W='CORPORATION WITHOUT CAPITAL STOCK')),
  ("connected_org_name", 38, string),
  ("candidate_id", 9, string),
  (None, 2, filler('\r\n'))
]

def_cm80 = def_cm[:-2] + [def_cm[-1]]
def_cn = [
  ('_type', 0, lambda x: 'Candidate'),
  ('candidate_id', 9, string),
  ('candidate_name', 38, string),
  ('party_desig_1', 3, string),
  ('party_desig_2', 3, string),
  ('party_desig_3', 3, string),
  ('ico', 1, ico),
  (None, 1, filler),
  ('candidate_status', 1, enum(
    C="STATUTORY CANDIDATE",
    F="STATUTORY CANDIDATE FOR A FUTURE ELECTION",
    N="NOT YET A STATUTORY CANDIDATE",
    P="STATUTORY CANDIDATE IN PRIOR CYCLE")),
  ('street_one', 34, string),
  ('street_two', 34, string),
  ('city', 18, string),
  ('state', 2, string),
  ('zip', 5, string),
  ('principal_committee_id', 9, string),
  ('election_year', 2, string),
  ('current_district', 2, string),
  (None, 2, filler('\r\n'))
]
def_pas2 = [
  ('_type', 0, lambda x: 'Transfer'),
  ("from_committee_id", 9, string),
  ("amendment_status", 1, amendment),
  ("report_type", 3, enum),
  ("primary_general", 1, pgi),
  ("microfilm_loc", 11, string),
  ("transaction_type", 3, enum),
  ("date", 8, date),
  ("amount", 7, integer),
  ("to_other_id", 9, string),
  ("to_candidate_id", 9, string),
  ("fec_record_id", 7, string),
  (None, 2, filler('\r\n'))
]

def_pas2_80 = def_pas2[:-2] + [def_pas2[-1]]
def_pas2_80[7] = ("date", 6, date99)
def_pas2_80[8] = ("amount", 6, integer)
def_pas2_90 = def_pas2_80[:]
def_pas2_90[8] = ("amount", 7, integer)
def_pas2_94 = def_pas2[:]
def_pas2_94[7] = ("date", 6, date99)
def_pas2_96 = def_pas2[:]

def_oth = [
  ('_type', 0, lambda x: 'Other Transfers'),
  ('filer_id', 9, string),
  ('amendment_status', 1, amendment),
  ('report_type', 3, enum),
  ('primary_general', 1, pgi),
  ('microfilm_loc', 11, string),
  ('transaction_type', 3, enum),
  ('name', 34, string),
  ('city', 18, string),
  ('state', 2, string),
  ('zip', 5, string),
  ('occupation', 35, string),
  ('date', 8, date),
  ('amount', 7, integer),
  ('from_id', 9, string),
  ('fec_record_id', 7, string),
  (None, 2, filler('\r\n'))
]
def_oth_86 = def_oth[:7] + [('street', 34, string)] + def_oth[7:]
def_oth_86[13] = ('date', 6, date99)
def_oth_86[14] = ('amount', 6, integer)
def_oth_86[16] = (None, 3, filler)
def_oth_90 = def_oth[:]
def_oth_90[12] = ('date', 6, date99)
def_oth_96 = def_oth[:]

def_indiv = [
  ('_type', 0, lambda x: 'Individual Contribution'),
  ('filer_id', 9, string),
  ('amendment_type', 1, amendment),
  ('report_type', 3, enum),
  ('primary_general', 1, pgi),
  ('microfilm_loc', 11, string),
  ('transaction_type', 3, enum), #@@important enumeration
  ('name', 34, string),
  ('street', 34, string),
  ('city', 18, string),
  ('state', 2, string),
  ('zip', 5, string),
  ('occupation', 35, string),
  ('date', 8, date),
  ('amount', 7, integer),
  ('from_id', 9, string),
  ('fec_record_id', 7, string),
  (None, 2, filler('\r\n'))
]

def_indiv_80 = def_indiv[:-2] + [(None, 3, filler), def_indiv[-1]]
def_indiv_80[13] = ('date', 6, date99)
def_indiv_80[14] = ('amount', 6, integer)
def_indiv_90 = def_indiv[:8] + def_indiv[9:]
def_indiv_90[12] = ('date', 6, date99)
def_indiv_96 = def_indiv[:8] + def_indiv[9:]

def fix80(line_def, fh):
    line_len = sum(x[1] for x in line_def)
    def internal():
        for line in fh:
            line_diff = line_len - len(line)
            if line_diff:
                line = line[:-2] + ' ' * line_diff + line[-2:]
            yield line
    out = web.storage()
    internal = internal()
    def read(leng):
        assert leng == line_len
        return internal.next()
    out.read = read
    return out

import sys
def parse_cansum():
    return parse_file(def_webl, file("../data/crawl/fec/2008/weball.dat"))
def parse_candidates():
    for fn in sorted(glob.glob('../data/crawl/fec/*/cn.dat')):
        print>>sys.stderr, fn
        for elt in parse_file(def_cn, file(fn)):
            yield elt
def parse_committees(latest=False, reverse=False):
    fns = sorted(glob.glob('../data/crawl/fec/*/cm.dat'))
    if latest:
        fns = [fns[-1]]
    if reverse:
        fns = reversed(fns)
    for fn in fns:
        print>>sys.stderr, fn
        fh = file(fn)
        if '1980' in fn:
            fh = fix80(def_cm, fh)
        for elt in parse_file(def_cm, fh):
            yield elt
def parse_transfers():
    cur_def = def_pas2_80
    for fn in sorted(glob.glob('../data/crawl/fec/*/pas2.dat')):
        print>>sys.stderr, fn
        fh = file(fn)
        if '1980' in fn:
            cur_def = def_pas2_80
            fh = fix80(def_pas2_80, fh)
        if '1990' in fn: cur_def = def_pas2_90
        if '1994' in fn: cur_def = def_pas2_94
        if '1996' in fn: cur_def = def_pas2_96
        for elt in parse_file(cur_def, fh):
            yield elt
def parse_contributions(latest=False):
    fns = sorted(glob.glob('../data/crawl/fec/*/indiv.dat.gz'))
    if latest:
        cur_def = def_indiv_96
        fns = [fns[-1]]
    for fn in fns:
        print>>sys.stderr, fn
        fh = gzip.open(fn)
        if '1980' in fn:
            cur_def = def_indiv_80
            fh = fix80(cur_def, fh)
        if '1990' in fn: cur_def = def_indiv_90
        if '1996' in fn: cur_def = def_indiv_96
        for elt in parse_file(cur_def, fh):
            yield elt
def parse_others():
    cur_def = def_oth_86
    for fn in sorted(glob.glob('../data/crawl/fec/*/oth.dat')):
        print>>sys.stderr, fn
        fh = file(fn)
        if '1990' in fn: cur_def = def_oth_90
        if '1996' in fn: cur_def = def_oth_96
        for elt in parse_file(cur_def, fh):
            yield elt

if __name__ == "__main__":
    import tools
    tools.export(parse_candidates())
    tools.export(parse_committees())
    tools.export(parse_transfers())
    tools.export(parse_contributions())
    tools.export(parse_others())

########NEW FILE########
__FILENAME__ = fec_crude_csv
#!/usr/bin/python
# -*- coding: utf-8 -*-
"""Import FEC data.

"""
import csv, sys, cgitb, fixed_width, zipfile, cStringIO, os, glob, time, cPickle
import codecs, re, field_mapper, simplejson, itertools, tempfile, web.utils

def strip(text):
    """
    >>> strip(' s ')
    's'
    """
    return text.strip()

def amount(text):
    """
    Decode amounts according to `FEC_v300.doc` and its kin.

    >>> map(amount, '50.00 6000 6000.00 600000'.split())
    ['50.00', '60.00', '6000.00', '6000.00']
    """
    if '.' in text: return text
    return text[:-2] + '.' + text[-2:]

def name_combo(first, middle, last):
    """
    >>> name_combo('John', '', 'Smith')
    'John Smith'
    >>> name_combo('John', 'Buckminster', 'Smith')
    'John Buckminster Smith'
    >>> name_combo('John', 'B', 'Smith')
    'John B. Smith'

    This is probably incorrectly filed, but comes from a filing on
    2006-04-12.  Should we fix it?
    >>> name_combo('Richard E', '', 'Williams')
    'Richard E Williams'

    This is from a filing from 2005-04-12.
    >>> name_combo(' JOE ', '', 'CAPPETTI ')
    ' JOE  CAPPETTI '

    """
    if len(middle) == 1: middle += '.'
    return ' '.join(filter(None, [first, middle, last]))

def parse_name(name_delim):
    """
    Examples from `FEC_v300.rtf`.
    >>> caret_separated_name = parse_name('^')
    >>> caret_separated_name('Smith^John W.^Dr.^Jr.')
    'Dr. John W. Smith, Jr.'
    >>> caret_separated_name('Smith^John W.^^Jr.')
    'John W. Smith, Jr.'
    >>> caret_separated_name('Smith Medical Corporation')
    'Smith Medical Corporation'
    >>> caret_separated_name('Smith-Reilly^Jennifer T.^Ms.')
    'Ms. Jennifer T. Smith-Reilly'

    Other examples from data:
    >>> caret_separated_name('Field^Tracy C.^^')
    'Tracy C. Field'
    >>> caret_separated_name('Thomson^Linda^Mrs.^')
    'Mrs. Linda Thomson'
    >>> caret_separated_name('Elrod^Adrienne')
    'Adrienne Elrod'

    Should we downcase/titlecase this one?
    >>> caret_separated_name('LOVE^KARA^^')
    'KARA LOVE'

    What were they smoking when they made this one up?
    >>> caret_separated_name('Everett^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^')
    'Everett'

    Some people use a non-caret:
    >>> parse_name('>')('Hagelin>John S')
    'John S Hagelin'

    """
    def rv(name):
        fields = name.split(name_delim)
        while len(fields) < 4: fields.append('')
        while len(fields) > 4:
            assert fields[-1] == ''
            fields.pop()

        lastname, firstname, prefix, suffix = fields
        if prefix: prefix += ' '
        if firstname: firstname += ' '
        if suffix: suffix = ', ' + suffix

        return ''.join([prefix, firstname, lastname, suffix])
    return rv

def schedule_type(data):
    """Is this a contribution, an expenditure, or neither?"""
    if data.get('rec_type') == 'TEXT': return # they have an unhelpful form_type
    if data['form_type'].startswith('SA'): return 'contribution'
    if data['form_type'].startswith('SB'): return 'expenditure'

def date(value):
    if value: return fixed_width.date(value)
    return None                         # to keep Postgres happy

@web.utils.memoize
def mapper_for(name_delim):
    return field_mapper.FieldMapper({
        'date': field_mapper.Reformat(format=date,
                                      source=['date',
                                              'date_received',
                                              'contribution_date',
                                              'expenditure_date',
                                              'date_(of_contribution)',
                                              # XXX this is for SC loans:
                                              'date_(incurred)',
                                              'date_of_expenditure']),
        'candidate_fec_id':
            field_mapper.Reformat(format=strip,
                                  source=['candidate_fec_id',
                                          'candidate_id_number',
                                          'fec_candidate_id_number']
                                  ),
        'tran_id': ['tran_id', 'transaction_id_number'],
        'occupation': ['occupation', 'contributor_occupation', 'indocc'],
        'contributor_org': ['contributor_org',
                            'contributor_organization_name',
                            'contrib_organization_name'],
        'contributor': [field_mapper.Reformat(format=parse_name(name_delim),
                                              source=['contributor_name']),
                        # XXX should include contributor_prefix and
                        # contributor_suffix?
                        lambda contributor_first_name,
                               contributor_middle_name,
                               contributor_last_name:
                            name_combo(contributor_first_name,
                                       contributor_middle_name,
                                       contributor_last_name),
                        ],
        # XXX recipient_name should be reformatted with parse_name(name_delim)
        # at least in 5.00
        # XXX also 'recipient_first_name' 'recipient_last_name'
        # 'recipient_middle_name' 'recipient_organization_name'
        # 'recipient_prefix' 'recipient_suffix'
        'recipient': ['payee_organization_name',
                      'recipient_name',
                      'name_(payee)'],
        'employer': ['employer', 'contributor_employer', 'indemp'],
        'amount': field_mapper.Reformat(format=amount,
                                        source=['amount',
                                                # XXX 6.x contribution_amount:
                                                # different format
                                                'contribution_amount',
                                                'amount_received',
                                                'expenditure_amount', # also 6.x
                                                'amount_of_expenditure']),
        'address': [lambda street__1, street__2, city, state, zip:
                    ' '.join([street__1, street__2, city, state, zip]),
                    lambda contributor_street__1, contributor_street__2,
                           contributor_city, contributor_state, contributor_zip:
                    ' '.join([contributor_street__1,
                              contributor_street__2,
                              contributor_city,
                              contributor_state,
                              contributor_zip])
                    ],

        'committee': ['committee_name',
                      'committee_name_______',
                      'committeename'],
        'candidate': [field_mapper.Reformat(format=parse_name(name_delim),
                                            source='candidate_name'),
                      lambda candidate_first_name,
                             candidate_middle_name,
                             candidate_last_name:
                         name_combo(candidate_first_name,
                                    candidate_middle_name,
                                    candidate_last_name),
                      ],
        'filer_id': ['filer_fec_cand_id',
                     'filer_fec_cmte_id_',
                     'filer_fec_cmte_id',
                     'filer_fec_committee_id',
                     'filer_committee_id_number',
                     'filer_candidate_id_number',
                     "filer's_fec_id_number",
                     'filer_committee_id'],

        'type': field_mapper.CatchAllField(['form_type'], schedule_type),
    })


def _regrtest_fields():
    """
    Regression tests for the `fields` table.

    >>> mapper = mapper_for('^')
    >>> mapped = mapper.map({'date_received': '20081130',
    ...                      'tran_id': '12345', 
    ...                      'weird_field': 34, 
    ...                      'amount_received': '123456'})
    >>> sorted(mapped.keys())
    ['amount', 'date', 'original_data', 'tran_id']
    >>> mapped['date']
    '2008-11-30'
    >>> mapped['amount']
    '1234.56'
    >>> mapped['original_data']['weird_field']
    34
    >>> mapped['tran_id']
    '12345'
    >>> mapper.map({'candidate_id_number': '12345'})
    ... #doctest: +ELLIPSIS
    {'candidate_fec_id': '12345', 'original_data': {...}}
    >>> mapper.map({'fec_candidate_id_number': '56789'})
    ... #doctest: +ELLIPSIS
    {'candidate_fec_id': '56789', 'original_data': {...}}
    >>> mapper.map({'transaction_id_number': '56789'})
    ... #doctest: +ELLIPSIS
    {'original_data': {...}, 'tran_id': '56789'}
    >>> mapper.map({'contributor_occupation': 'Consultant'})
    ... #doctest: +ELLIPSIS
    {'original_data': {...}, 'occupation': 'Consultant'}
    >>> mapper.map({'indocc': 'Private Investor'})
    ... #doctest: +ELLIPSIS
    {'original_data': {...}, 'occupation': 'Private Investor'}
    >>> mapper.map({'indemp': 'EEA Development'})
    ... #doctest: +ELLIPSIS
    {'original_data': {...}, 'employer': 'EEA Development'}

    >>> mapper.map({'street__1': '2531 Falcon Way',
    ...             'street__2': '#400',
    ...             'city': 'Concord',
    ...             'state': 'TX',
    ...             'zip': '20036'})
    ... #doctest: +ELLIPSIS
    {...'address': '2531 Falcon Way #400 Concord TX 20036'...}

    """

class header(csv.excel):
    delimiter = ';'
def headers(filename):
    r = csv.reader(file(filename, 'U'), dialect=header)
    rv = {}
    for line in r:
        # some of the format spec files erroneously say SchA rather than SA
        # or erroneously say SH1, SH2, etc., rather than H1, H2, etc.
        key = line[0].replace('Sch', 'S').replace('SH', 'H')
        rv[key] = [name.strip().lower().replace(' ', '_') for name in line[1:]]
    return rv
def findkey(hmap, key):
    """Find the base schedule or form number,
    given the first field from an FEC filing record.
    """
    while key:
        if key in hmap: return hmap[key]
        else: key = key[:-1]

@web.utils.memoize   # Memoizing saved about 2540% of run time when I measured
def headers_for_version(version):
    headerdir = os.path.split(__file__)[0]
    return headers(os.path.join(headerdir, 'fec_headers', '%s.csv' % version))

class ascii28separated(csv.excel):
    "The FEC moved from CSV to chr(28)-separated files in format version 6."
    delimiter = chr(28)

class translate_to_utf_8:
    """Convert a presumably Windows-1252 file to UTF-8.

    The FECs documents claim non-ASCII characters will be
    rejected, e.g. in FEC_v520.doc:

    > Generally speaking, only keyboard characters are acceptable
    > within CSV files.  Technically, any coded characters that fall
    > outside the range of ASCII characters 32 (space) through 126
    > (tilde ~) will be rejected.  Care should be taken if text is
    > cut and pasted from word processing programs, since some
    > characters such as appostrophe [sic] and smart quotes may not
    > translate into the appropriate ASCII characters.

    However, I have seen a filing (181941.fec, from 20050722.zip,
    version 5.2) in Windows-1252.  Aaron points out:

    > The `chardet` library might be useful:
    > <http://chardet.feedparser.org/>

    Right now were not using that, though.

    However, this policy changed by version 6.2, which says:

    > The following characters will be allowed in filing fields (These
    > are technically specified using the ASCII standard):
    > - Keyboard characters. These fall within the range of ASCII 32
    >   (space) through 126 (tilde ~).
    > - Some characters used in other languages. Specifically ASCII
    >   characters 128 through 156, ASCII characters 160 through 168,
    >   and ASCII character 173. This allows name and address fields
    >   to contain letters such as , , , , , , etc. Care should
    >   be taken if text is cut and pasted from word processing, or
    >   other programs, since many non-keyboard characters such as
    >   apostrophes and smart quotes (which are stored as ANSI coded
    >   characters) will not translate into the appropriate ASCII
    >   characters.

    Unfortunately this is nonsense; ASCII is and has always been a
    7-bit code, although there are many extended ASCII 8-bit
    variants.  The characters they have quoted above exist in the
    commonly-used character set ISO-8859-1, which also does not have
    smart quotes, but ISO-8859-1 have printable characters in the
    range 128 through 156 either.  The most likely character set that
    contains the characters they have quoted and also contains
    printable characters in the 128156 range is Windows-1252, which
    obsolete parts of Microsoft Windows use by default; however,
    Windows-1252 *does* contain smart quotes (characters 145148),
    contains alphabetic characters at codepoints 158 and 159 as well,
    and is missing printable characters at several codepoints inside
    the 128156 range.

    Due to the absence of any evidence that the FEC is aware that more
    than one character encoding exists, and their acceptance of the
    above-cited filing in Windows-1252 at a time when they officially
    promised not to accept such filings, I am going to assume for the
    time being that all filings are encoded in Windows-1252.

    Were not using codecs.EncodedFile because it thinks U+001C is a
    line terminator.

    """
    def __init__(self, fileobj):
        self.fileobj = fileobj
        self.encoder = codecs.getencoder('utf-8')
        self.decoder = codecs.getdecoder('windows-1252')
    def readline(self):
        line = self.fileobj.readline()
        # replace errors here for Egl\x8f,Richard,,Mr. and M RUB\x90N
        # HINOJOSA
        unicode_string, length = self.decoder(line, errors='replace')
        assert length == len(line)
        rv, length = self.encoder(unicode_string)
        assert length == len(unicode_string)
        return rv
    def __iter__(self): return iter(self.readline, '')
    def seek(self, position):
        assert position == 0
        self.fileobj.seek(0)

def decode_headerline(line):
    format_version = line[2].strip()

    if format_version < '6':
        headerheaders = 'record_type ef_type fec_ver soft_name soft_ver ' \
                        'name_delim report_id rpt_number'
    else:
        headerheaders = 'record_type ef_type fec_ver soft_name soft_ver ' \
                        'report_id rpt_number'

    headers = dict(zip(headerheaders.split(), (f.strip() for f in line)))
    assert format_version == headers['fec_ver']

    if not headers.get('name_delim'): # empty string means to use the default
        headers['name_delim'] = '^'
    if headers['name_delim'] == '0':     # special case for RNC 2002 software
        headers['name_delim'] = '^'
    assert headers['name_delim'] in '>^' # I want to know if not!

    return headers

class FilingFormatNotDocumented(Exception): pass

# Note that normally we are reading from a zipfile, and Pythons
# stupid zipfile interface doesnt AFAICT give us the option of
# streaming reads  it insists on reading the whole zipfile element at
# once.  So we dont lose much by parsing from a string rather than a
# file object here, as long as we are careful not to accidentally make
# extra copies of the string.
def readstring(astring):
    # from the Python 2.5 documentation: Note: This version of the
    # csv module doesn't support Unicode input. Also, there are
    # currently some issues regarding ASCII NUL
    # characters. Accordingly, all input should be UTF-8 or printable
    # ASCII to be safe; see the examples in section 9.1.5. These
    # restrictions will be removed in the future.
    fileobj = translate_to_utf_8(cStringIO.StringIO(astring))
    r = csv.reader(fileobj)
    headerline = r.next()
    if chr(28) in headerline[0]:
        # It must be in the new FS-separated format
        fileobj.seek(0)
        r = csv.reader(fileobj, dialect=ascii28separated)
        headerline = r.next()
    if len(headerline) == 1:
        # Its probably the old 2.x format that we dont support yet
        # because we cant find docs; return without yielding
        # anything.
        raise FilingFormatNotDocumented(astring[:20])

    headerdict = decode_headerline(headerline)

    if headerdict.get('rpt_number', '') == '':
        headerdict['rpt_number'] = None

    yield headerdict

    headermap = headers_for_version(headerdict['fec_ver'])
    mapper = mapper_for(headerdict['name_delim'])

    for line in r:
        if not line: continue         # FILPAC inserts random blank lines
        if line[0].lower().strip() in ('[begintext]', '[begin text]'):
            # see e.g. New F99 Filing Type for unstructured,
            # formatted text in FEC_v300.rtf.  Note that this data
            # may violate `csv`'s expectations, so we have to read it
            # ourselves, and rely on `csv` not doing some kind of
            # read-ahead.
            while True:
                line = fileobj.readline().lower()
                if not line: break      # robustness against premature EOF
                if line.strip() in ('[endtext]', '[end text]',
                                    # NGP Campaign Office(R) 1.0e filing 207928
                                    '[endtext]"'
                                    ):
                    break
                # XXX right now we just discard the lines
            line = r.next()
            # There can be a blank line here too, inserted by e.g. NIC
            # WebForms 6.2.1.1 in filing 353794.fec from 20080722.zip.
            if not line: continue
        fieldnames = findkey(headermap, line[0].upper())
        if not fieldnames:
            raise "could not find field defs", (line[0], headermap.keys())
        rv = mapper.map(dict(zip(fieldnames, line)))
        rv['rpt_number'] = headerdict['rpt_number']
        yield rv

candidate_name_res = [re.compile(x, re.IGNORECASE) for x in
                      [r'''(?ix)(?P<candidate>.*) \s+ for \s+ congress''',
                       r'''(?ix)friends \s+ of \s+ (?P<candidate>.*)''']]
# maybe also:
#  | committee \s+ to \s+ elect (?P<candidate>.*)
# "Alan Pedigo For US House of Rep"
# These are recipients of a certain PACs donations:
# "Citizens for Arlen Specter"
# "Ike Skelton for Congress Committee"
# "Bill Nelson for U.S. Senate Campaign"
# "Martin Frost Campaign Committee"
# "Friends of Connie Morella for Congress Committee"
# "Committee to Elect McHugh"

class WrongFirstRecordError(Exception): pass

def read_filing(astring, filename, pathname=None):
    records = readstring(astring)
    # This is where we combine the header_record and the cover_record.
    header_record = records.next()
    cover_record = records.next()
    if not cover_record['original_data']['form_type'].startswith('F'):
        raise WrongFirstRecordError(filename, cover_record)

    cover_record['pathname'] = pathname

    cover_record['this_report_id'] = filename[:-4]
    if header_record.get('report_id'):  # The field may be missing or empty.
        cover_record['report_id'] = \
            re.match('(?i)fec\s*-\s*(\d+)(\.?$|\s+|\*BD[0-9a-f]{4}$)',
                     header_record['report_id']).group(1)
    else:
        cover_record['report_id'] = cover_record['this_report_id']

    cover_record['format_version'] = header_record['fec_ver'] # for debugging
    if not cover_record.get('candidate'):
        for regex in candidate_name_res:
            mo = regex.match(cover_record.get('committee', ''))
            if mo:
                cover_record['candidate'] = mo.group('candidate')
                break
    return cover_record, records

def null_error_handler(): raise

def readfile_zip(filename, handler=null_error_handler):
    zf = zipfile.ZipFile(filename)
    for name in zf.namelist():
        try:
            yield read_filing(zf.read(name), name, pathname=filename)
        except:
            handler()

def readfile_generic(filename, handler=null_error_handler):
    try:
        if filename.endswith('.zip'):
            return readfile_zip(filename, handler)
        else:
            _, basename = os.path.split(filename)
            return [read_filing(file(filename).read(),
                                basename,
                                pathname=filename)]
    except:
        handler()
        return []

EFILINGS_PATH = '../data/crawl/fec/electronic/'
DEFAULT_EFILINGS_FILEPATTERN = EFILINGS_PATH + '*.zip'

def parse_efilings(filenames, handler=null_error_handler):
    last_time = time.time()
    for filename in filenames:
        sys.stderr.write('parsing efilings file %s\n' % filename)
        for parsed_file in readfile_generic(filename, handler):
            yield parsed_file
        now = time.time()
        sys.stderr.write('parsing took %.1f seconds\n' % (now - last_time))
        last_time = now

def atomically_commit_efiling(outfile, tempname, realname):
    outfile.flush()
    os.fsync(outfile.fileno())
    outfile.close()

    os.rename(tempname, realname)

def stash_efilings_worker(destdir, filenames, save_orig):
    cover_record = {}

    def handle_error():
        etype, evalue, etb = sys.exc_info()
        eclass = str if isinstance(etype, basestring) else etype
        if issubclass(eclass, KeyboardInterrupt): raise
        if issubclass(eclass, StopIteration): raise # let it propagate
        if issubclass(eclass, GeneratorExit): raise # let the generator die
        if issubclass(eclass, FilingFormatNotDocumented):
            # We don't bother to log these; we know they happen.
            return

        pathname = cover_record.get('pathname')
        report_id = cover_record.get('this_report_id')

        logdir = os.path.join(destdir, 'errors')
        if not os.path.exists(logdir): os.makedirs(logdir)
        fd, path = tempfile.mkstemp(dir=logdir, suffix = '.' + eclass.__name__)
        fo = os.fdopen(fd, 'w')
        lines_of_context = 5
        fo.write('Surprise; last filing successfully opened was\n%s in %s\n\n' %
                 (report_id, pathname))
        fo.write(cgitb.text((etype, evalue, etb), lines_of_context))
        fo.close()

        sys.stderr.write("logged error (in %s?) to %s, continuing\n" %
                         (report_id, path))

    for efiling in parse_efilings(filenames, handle_error):
        cover_record, records = efiling
        report_id = cover_record['report_id']
        dirpath = os.path.join(destdir, report_id[-2:], report_id)
        if not os.path.exists(dirpath): os.makedirs(dirpath)

        pathname = os.path.join(dirpath,
                                '%s.pck' % cover_record['this_report_id'])

        if os.path.exists(pathname):
            continue

        tempname = pathname + '.new'  # hoping for no concurrent writes
        outfile = file(tempname, 'w')
        if not save_orig: del cover_record['original_data']
        pickle_protocol = 2
        cPickle.dump(cover_record, outfile, pickle_protocol)

        try:
            for record in records:
                if not save_orig: del record['original_data']
                cPickle.dump(record, outfile, pickle_protocol)
        except:
            os.unlink(tempname)
            handle_error()
        else:
            atomically_commit_efiling(outfile, tempname, pathname)

def stash_efilings(destdir=None,
                   filepattern=DEFAULT_EFILINGS_FILEPATTERN,
                   save_orig=False,
                   nprocs=1):
    """Parse a bunch of electronic FEC filings, from zip files or CSV
    files, and store the parsed data in pickle files in a directory
    structure indexed by filing ID.

    * `destdir` is the directory to put the results in.
    * `filepattern` gives the filenames to find the filings in.
    * `save_orig` determines whether to include the full filing data
      in the pickled output, for debugging
    * `nprocs` specifies the number of worker processes to do the
      work.  The filenames are divided up as evenly as possible
      between them.  Because not every file takes the same time to
      process, one process may finish a few files ahead of another,
      but the loss of efficiency to that should be small.

    """
    if destdir is None: destdir = tempfile.mkdtemp()
    filenames = glob.glob(filepattern)
    children = []

    for ii in range(nprocs):
        pid = os.fork()
        if pid == 0:                    # child process
            try:
                myfiles = [filenames[jj] for jj in range(len(filenames))
                           if jj % nprocs == ii]
                stash_efilings_worker(destdir=destdir,
                                      filenames=myfiles,
                                      save_orig=save_orig)
            except:
                try:
                    cgitb.Hook(format='text').handle()
                except:
                    pass
            finally:
                os._exit(0)
        else:
            children.append(pid)

    for pid in children:
        os.waitpid(pid, 0)              # options=0
    
if __name__ == '__main__':
    cgitb.enable(format='text')
    if sys.argv[1] == '--stash-in':
        sys.argv.pop(1)
        destdir = sys.argv.pop(1)
        if len(sys.argv) > 1:
            stash_efilings(destdir=destdir, filepattern=sys.argv[1], nprocs=8)
        else:
            stash_efilings(destdir=destdir, nprocs=8)

    else:
        # pprint is unacceptable --- it made the script run 40 slower.
        for filename in sys.argv[1:]:
            for form, schedules in readfile_generic(filename):
                print simplejson.dumps(form, sort_keys=True, indent=4)
                for schedule in schedules:
                    print simplejson.dumps(schedule, sort_keys=True, indent=4)

########NEW FILE########
__FILENAME__ = fec_crude_csv_test
#!/usr/bin/python
# -*- coding: utf-8 -*-
import fec_crude_csv, doctest, cgitb

def records(inputstring):
    return list(fec_crude_csv.readstring(inputstring))

def test_endtext():
    """
    >>> records(filing_207928)
    ... #doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
    [{...'fec_ver': '5.3'...},
     {...'filer_id': 'C00410761', 'original_data': {...}...
      'committee': 'Castor for Congress'...}]
    >>> records(truncated_filing)       # same
    ... #doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
    [{...'fec_ver': '5.3'...},
     {...'filer_id': 'C00410761', 'original_data': {...}...
      'committee': 'Castor for Congress'...}]
    """

# Note the extra quote mark on the [ENDTEXT] line.
filing_207928 = '''"HDR","FEC","5.3","NGP Campaign Office(R)","1.0e","","","0",""
"F99","C00410761","Castor for Congress","P.O. Box 5419","","Tampa","FL","33675","Amy Martin","20060322","MST"
[BEGINTEXT]
March 22, 2006

Christopher A. Whyrick
Senior Campaign Finance Analyst
Federal Elections Commission
999 E Street, NW
Washington, DC 20463

Identification Number: C00410761

Reference: Year End Report (10/1/05-12/31/05)

Dear Mr. Whyrick,

	This letter is in response to the FEC request for additional information dated February 23, 2006.  As instructed in your letter, we have amended our Year End Report and corrected the discrepancy on Line 2 of FEC Form 3Z-1.

	If you need further assistance, please contact me at (813)251-5094.


Sincerely,



Amy Martin, Treasurer
Castor for Congress

[ENDTEXT]"
'''

# This is an invalid filing but it shouldnt hang the importer.  (It
# did at one point; it would loop endlessly looking for [ENDTEXT].)
truncated_filing = '''"HDR","FEC","5.3","NGP Campaign Office(R)","1.0e","","","0",""
"F99","C00410761","Castor for Congress","P.O. Box 5419","","Tampa","FL","33675","Amy Martin","20060322","MST"
[BEGINTEXT]
March 22, 2006
'''

def test_v2_02_data():
    '''We cant handle version 2.x data yet, so we should just skip
    it until we find a spec for it, by means of raising an exception.

    >>> records(filing_30458_truncated)
    Traceback (most recent call last):
    ...
    FilingFormatNotDocumented: /* Header
    FEC_Ver_# 
    '''

filing_30458_truncated = '''/* Header
FEC_Ver_# = 2.02
Soft_Name = Vocus, Inc. PACPRO
Soft_Ver# = 6.16.040
DEC/NODEC = DEC
Date_Fmat = CCYYMMDD
NameDelim = ^
Form_Name = F3XN
FEC_IDnum = C00238725
Committee = National Air Traffic Controllers Association PAC
Schedule_Counts:
SA11ai   = 00002
SB23     = 00023
/* End Header
"F3XN","C00238725","National Air Traffic Controllers Association PAC","1325 Massachusetts Ave., NW","","Washington","DC","20005","","X","M3","",,"",20020201,20020228,290018.35,45558.46,335576.81,23000,312576.81,0,0,332,45226.46,45558.46,0,0,45558.46,0,0,0,0,0,0,0,45558.46,45558.46,0,0,0,0,0,23000,0,0,0,0,0,0,0,0,0,23000,23000,45558.46,0,45558.46,0,0,0,250065.32,2002,91011.49,341076.81,28500,312576.81,534,90477.49,91011.49,0,0,91011.49,0,0,0,0,0,0,0,91011.49,91011.49,0,0,0,0,0,28500,0,0,0,0,0,0,0,0,0,28500,28500,91011.49,0,91011.49,0,0,0,"Mr. John Carr",20020323
"SA11ai","C00238725","IND","Glasserman^John^Mr.^Glasserman","4 Spruce Lane","","Essex Junction","VT","054524387","","","Federal Aviation Administration","Air Traffic Controller",404,,202,"15","","","","","","","","","","","","","","","Payroll Deduction ($101.00 Biweekly)","","10000072200300002"
'''

def test_windows_1252_characters():
    r"""The FEC claims that filings containing non-ASCII characters will be
    rejected, but here we have an example of a filing that wasnt.  It
    contains a Windows-1252 byte.  Note that it is an en dash, not an
    em dash, sigh.  I am arbitrarily deciding that the result should
    be encoded in UTF-8, in part because the Python `csv` module is
    documented to be UTF-8-safe.

    >>> records(filing_181941_truncated)[-1] #doctest: +ELLIPSIS
    {...'occupation': 'Team Ldr \xe2\x80\x93 HRIS'...}

    Not all the non-ASCII data is encoded in Windows-1252.
    >>> records(filing_221223_edited)[-1] #doctest: +ELLIPSIS
    {...'candidate': 'M RUB\xef\xbf\xbdN  HINOJOSA'...}

    """

filing_181941_truncated = u'''"HDR","FEC","5.2","Vocus PAC Management","3.00.1828","","",0,""
"F3XN","C00083857","Occidental Petroleum Corporation Political Action Committee","10889 Wilshire Blvd.","","Los Angeles","CA","90024","","X","MY","",,"",20050101,20050630,48873.88,113149.90,162023.78,97411.13,64612.65,0.00,0.00,96058.37,16874.30,112932.67,0.00,0.00,112932.67,0.00,0.00,0.00,0.00,0.00,217.23,0.00,113149.90,113149.90,0.00,0.00,0.00,0.00,0.00,97000.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,411.13,97411.13,97411.13,112932.67,0.00,112932.67,0.00,0.00,0.00,48873.88,2005,113149.90,162023.78,97411.13,64612.65,96058.37,16874.30,112932.67,0.00,0.00,112932.67,0.00,0.00,0.00,0.00,0.00,217.23,0.00,113149.90,113149.90,0.00,0.00,0.00,0.00,0.00,97000.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,411.13,97411.13,97411.13,112932.67,0.00,112932.67,0.00,0.00,0.00,"Dominick^S.P.^^",20050722,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"SA11ai","C00083857","IND","","16919 PRESTON BEND DR","","DALLAS","TX","75248","","","Occidental Chemical Corp.","GM NGO/Laurel",400.00,20050429,400.00,"15","","","","","","","","","","","","","","","","","11109795","","","","","","SEWELL","LAWRENCE","R","",""
"SA11ai","C00083857","IND","","4001 VIA SOLANA","","P VERDES ESTATES","CA","90274","","","Occidental Petroleum Corp.","Dir Internal Audit",600.00,20050429,600.00,"15","","","","","","","","","","","","","","","","","11109790","","","","","","MISTRY","MARZI","J","",""
"SA11ai","C00083857","IND","","322 FAIRHAVEN CT","","NEWBURY PARK","CA","91320","","","Occidental Petroleum Corp.","Team Ldr  HRIS",350.00,20050429,350.00,"15","","","","","","","","","","","","","","","","","11109794","","","","","","PLACANICA","VINCENT","J","",""
'''.encode('iso-8859-1')
assert chr(0x96) in filing_181941_truncated # not really an ISO-8859-1
                                            # character! Windows-1252.

filing_221223_edited = '''HDR,FEC,5.3,FECfile,5.3.1.0(f16),,FEC-206415,1
F3XA,C00105981,INVESTMENT COMPANY INSTITUTE POLITICAL ACTION COMMITTEE (ICI PAC),1401 H STREET NW SUITE 1200,,WASHINGTON,DC,20005,,X,M3,,,,20060201,20060228,243872.22,24007.99,267880.21,133054.13,134826.08,0.00,0.00,19000.00,0.00,19000.00,0.00,5000.00,24000.00,0.00,0.00,0.00,0.00,0.00,7.99,0.00,24007.99,24007.99,0.00,0.00,3756.86,3756.86,0.00,129297.27,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,133054.13,133054.13,24000.00,0.00,24000.00,3756.86,0.00,3756.86,172863.10,2006,110517.11,283380.21,148554.13,134826.08,100500.00,0.00,100500.00,0.00,10000.00,110500.00,0.00,0.00,0.00,0.00,0.00,17.11,0.00,110517.11,110517.11,0.00,0.00,3756.86,3756.86,0.00,144797.27,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,148554.13,148554.13,110500.00,0.00,110500.00,3756.86,0.00,3756.86,MAFFIA^LAWRENCE^MR.^,20060605,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
SA11A1,C00105981,IND,,1625 Broadway,Ste. 780,Denver,CO,80202,,,Oppenheimer Funds,Ind. Chairman & Trustee Oppenheimer Fd,5000.00,20060213,5000.00,15,,,,,,,,,,,,,,,,,SA11A1.6771,,,,,,ARMSTRONG,WILLIAM L,,MR.,
SB23,C00105981,CCM,,417 NEW JERSEY AVENUE SE,,WASHINGTON,DC,20003,,Contribution,P2006,,20060215,1000.00,,,HINOJOSA^RUB\x90N ^M^^,H,TX,15,,,,,,,,,,SB23.6797,,,,,,,HINOJOSA FOR CONGRESS,,,,,
'''
assert chr(0x90) in filing_221223_edited # neither ISO-8859-1 nor Windows-1252

def cover_record(data, name):
    return fec_crude_csv.read_filing(data, name)[0]

def test_candidate_name():
    """There are a variety of fields from which we can extract
    candidate names.  This tests some of them.

    This is from the committee name 'KT McFarland for Congress':
    >>> cover_record(filing_230174_truncated, '230174.fec')['candidate']
    'KT McFarland'

    This is from committee name 'Friends of Tyson Pratcher':
    >>> cover_record(filing_230176_truncated, '230176.fec')['candidate']
    'Tyson Pratcher'

    This one is from an actual `candidate_name` field in form F6:
    >>> cover_record(filing_230177_truncated, '230177.fec')['candidate']
    'Rick ODonnell'

    In this case there is a `candidate_name` field, but it is empty,
    because instead they used the `candidate_first_name`,
    `candidate_middle_name`, and `candidate_last_name` fields.  The
    `committee_name_(pcc)` field would give us 'Sue Kelly'.

    >>> cover_record(filing_230179, '230179.fec')['candidate']
    'Sue W. Kelly'

    In this case, `candidate_name` contains a ^-separated name, which
    needs to be properly reordered.
    >>> cover_record(filing_230185, '230179.fec')['candidate']
    'HOWARD KALOOGIAN'

    In this case, there is a more specific candidate name 'JOHN
    T. DOOLITTLE' to be extracted from the committee name, but we
    dont yet do it.
    >>> cover_record(filing_181904_truncated, '181904.fec')['candidate']
    'JOHN DOOLITTLE'

    """

def test_format_6():
    r"""Format 6.x is a little tricky to decode.

    The header line has changed, the separator is now `\x1c`, presumably
    there are no quotes any more, and the official rule on amounts now
    treats '500' as meaning '$500' and not '$5.00' (which XXX is still
    not supported in the code!)

    This is a very minimal test ensuring that we dont completely
    break the ability to read version 6.x files again.  (Apparently
    Unicode thinks that `\x1c` is a kind of paragraph separator, so if
    youre doing a `.readline()` on an instance of the `streamreader`
    of the Windows-1252 codec, it will give you the bytes up to the
    next `\x1c`, and so I accidentally broke reading 6.x files by
    introducing a `streamreader` into the pipeline.)

    >>> cover_record(filing_333594_truncated, '333594.fec')['committee']
    'Amerigroup Corporation Political Action Committee (Amerigroup PAC)'
    >>> cover_record(filing_333600_truncated, '333600.fec')['committee']
    'Dan Grant for Congress'

    """

def test_report_id():
    """The report ID ties together the original filing and its amendments.

    >>> import operator
    >>> report_ids = operator.itemgetter('report_id', 'this_report_id')
    >>> r = lambda data, name: report_ids(cover_record(data, name))

    Filing 230176 is a new filing.
    >>> r(filing_230176_truncated, '230176.fec')
    ('230176', '230176')

    But filing 230174 is an amendment of filing 211016.
    >>> r(filing_230174_truncated, '230174.fec')
    ('211016', '230174')

    The original report ID is in a different position in the 6.x header.
    >>> r(filing_333600_truncated, '333600.fec')
    ('306890', '333600')

    """

def test_strange_headers():
    """Filing 184656 doesnt include all of the header fields.

    >>> cover_record(filing_184656, '184656.fec')['committee']
    'Hewlett Packard Company PAC'

    Filing 184693s report_id has an extraneous space character at the
    end, which should *not* be considered part of the report ID:
    >>> cover_record(filing_184693_truncated, '184693.fec')['report_id']
    '180927'

    Filing 19538 specifies a Name Delim character of  , but it
    doesnt really mean it, because it delimits its names with ^
    just like all the other filings.  So I'm going to assume that
    blanks in this field are extraneous.  To verify that the delimiter
    is being properly parsed, Im going to verify that names are
    parsed correctly.
    >>> [rec.get('contributor') for rec in
    ...       fec_crude_csv.read_filing(filing_19538_truncated, '19538.fec')[1]]
    [None, None, 'Mrs. Jean Abernathy', 'Mrs. Renee Abraham']

    Filing 22784 has a space in the FEC ID of the filing its supposed
    to amend.
    >>> cover_record(filing_22784_truncated, '22784.fec')['report_id']
    '17081'

    Filing 39775 says its report ID is FEC-23808 F3XN, which
    includes the form type of the filing its amending.
    >>> cover_record(filing_39775_truncated, '39775.fec')['report_id']
    '23808'

    Filing 48608 appends a period to the end of the FEC ID of the
    filing it amends.
    >>> cover_record(filing_48608_truncated, '48608.fec')['report_id']
    '33531'

    Filing 353121 has a space on the end of the version number.
    >>> cover_record(filing_353121_truncated, '353121.fec')['format_version']
    '6.2'

    Filing 83615 (and about 100 other amendment filings produced with
    Vocus PAC Management) have random hexadecimal crap stuck onto the
    end of the FEC filing ID, preceded with '*BD'.  Maybe this is a
    checksum of the original filing or something.  (Ive verified that
    in fact filing 63727 is a filing from the same people of the same
    type covering the same period of time.)
    >>> cover_record(filing_83615_truncated, '83615.fec')['report_id']
    '63727'

    Filing 217921 (and a few other filing amendments from Public
    Affairs Support Services Inc. software) puts a space before and
    after the dash in its original report ID.
    >>> cover_record(filing_217921_truncated, '217921.fec')['report_id']
    '213367'

    """

def test_rpt_number():
    r"""Filing 83615, an amended filing, has a `rpt_number` of 2.

    >>> r, d = fec_crude_csv.read_filing(filing_83615_truncated, '83615.fec')
    >>> r['rpt_number']
    '2'

    XXX what about '9' < '10'?  Intify?

    The `rpt_number` should be propagated to all subsequent records
    read from the file.

    >>> d.next()['rpt_number']
    '2'

    Filings that have no report number in the header should be treated
    as having a `rpt_number` of None, which sorts before numbers and
    strings.
    >>> None < '2'
    True
    >>> None < 2
    True
    >>> r, d = fec_crude_csv.read_filing(filing_31454_faked, '31454.fec')
    >>> r['rpt_number']
    >>> d.next()['rpt_number']

    """

def test_lowercase_form_type():
    """Filing 22795 uses lowercase for some of its form type fields.

    I swear I am not making this data up.

    >>> [rec['contributor'] for rec in
    ...      fec_crude_csv.read_filing(filing_22795_truncated, '22795.fec')[1]]
    ... #doctest: +NORMALIZE_WHITESPACE
    ['Ballard Spahr Andrews & Ingersoll LLP',
     'Ballard Spahr Andrews & Ingersoll LLP']

    """

def test_extra_carets():
    """Filing 23422 has a name field with a large number of carets in it.

    I swear I am not making this data up.  This filing was ACTUALLY
    ACCEPTED BY THE FEC.

    >>> [rec['candidate'] for rec in
    ...     fec_crude_csv.read_filing(filing_23422_abbreviated, '23422.fec')[1]]
    ['', 'Everett']

    """

def test_noncarets():
    """Filing 31454 uses the > character instead of ^ to separate names.

    Unfortunately it doesnt have any records in it that we do
    anything useful with yet, although some of them clearly have
    >-separated names.  So I have faked up a record from another
    filing and stuck it in there instead.
    
    >>> [rec['contributor'] for rec in
    ...      fec_crude_csv.read_filing(filing_31454_faked, '31454.fec')[1]]
    ['Melinda S Ackerman']

    However, the Republican National Committees 2002 proprietary
    software claims to use 0 for its name delimiter, even though it
    actually uses ^ like normal software.  Since I dont think
    anyone will really use 0 as a name delimiter, I am going to
    special-case it.
    >>> [rec.get('contributor') for rec in
    ...      fec_crude_csv.read_filing(filing_33818_truncated, '33818.fec')[1]]
    ... #doctest: +NORMALIZE_WHITESPACE
    [None, None, 'Mrs. MarciA. Abel', 'Ms. Margaret Allyn',
     'Ms. Helen Andalla', 'Mr. RalphJ. Anderson']

    """

filing_230174_truncated = '''HDR,FEC,5.3,CMDI FEC FILER,5.3.0,,FEC-211016,1,
F3A,C00415620,"KT McFarland for Congress","954 Lexington Avenue","Box 135","New York",NY,10021,,NY,14,Q1,P2006,20061101,NY,X,,,,20060101,20060331,172080.00,1000.00,171080.00,135651.09,0.00,135651.09,24293.78,0.00,0.00,168150.00,3930.00,172080.00,0.00,0.00,0.00,172080.00,0.00,0.00,0.00,0.00,0.00,0.00,172080.00,135651.09,400000.00,0.00,0.00,0.00,1000.00,0.00,0.00,1000.00,2000.00,538651.09,390864.87,172080.00,562944.87,538651.09,24293.78,602005.00,1000.00,601005.00,174691.47,0.00,174691.47,572075.00,3930.00,576005.00,0.00,1000.00,25000.00,602005.00,0.00,0.00,0.00,0.00,0.00,0.00,602005.00,174691.47,400000.00,0.00,0.00,0.00,1000.00,0.00,0.00,1000.00,2000.00,577691.47,"Alan McFarland",20060721,,,,,,,,,
SA11A1,C00415620,IND,,"219 East 69th Street","Apt 5-D","New York",NY,10021,P2006,,"Auda Private Equity LLC","Investment Manager",1000.00,20060201,1000.00,15,"Receipt",,,,,,,,,,,,,,,,60314.C590,,,,,,"Andryc","David",,,
SA11A1,C00415620,IND,,"219 East 69th Street","Apt 5-D","New York",NY,10021,P2006,,"Auda Private Equity LLC","Investment Manager",500.00,20060201,-500.00,15,"Reattribution Memo",,,,,,,,,,,,,X,"REATTRIBUTION TO SPOUSE",,60314.C591,60314.C590,SA11A1,,,,"Andryc","David",,,
'''

filing_230176_truncated = '''"HDR","FEC","5.3","NGP Campaign Office(R)","1.0e","","","0",""
"F3N","C00421578","Friends of Tyson Pratcher","454 North Willett","","Memphis","TN","38112","","TN","09","12P","P2006","20060803","TN","X","","","","20060401","20060714","3955.00","4200.00","-245.00","57578.74","171.00","57407.74","15078.20","0","0","2750.00","1205.00","3955.00","0","0","0","3955.00","0","0","0","0","171.00","0","4126.00","57578.74","0","0","0","0","0","0","4200.00","4200.00","5000.00","66778.74","77730.94","4126.00","81856.94","66778.74","15078.20","88064.44","4200.00","83864.44","63957.24","171.00","63786.24","76810.00","6054.44","82864.44","0","5200.00","0","88064.44","0","0","0","0","171.00","0","88235.44","63957.24","0","0","0","0","0","0","4200.00","4200.00","5000.00","73157.24","Horne^Allison","20060721","","","","","","","","",""
"SA11AI","C00421578","IND","","341 West 11th Street #8E","","New York","NY","10014","P2006","","Self-Employed","Attorney","500.00","20060519","500.00","","","","","","","","","","","","","","","","","","C350641","","","","","","Jones","Gregory","Davis","",""
'''

filing_230177_truncated = '''"HDR","FEC","5.3","Aristotle International CM4 PM4","Version 4.2.1","^","",""
"F6","C00374777","Coloradans for Rick ODonnell","PO Box 260693","","Lakewood","CO","80226   ","H2CO07055","Rick ODonnell","H","CO","07",20060721
"F65","C00374777","IND","ONeill^Timothy","2191 Baldy Lane","","Evergreen","CO","80439","Snell & Wilmer","Attorney",20060720,1000.00,"","","","","",,"","","","","","","","60721.C2980"
"F65","C00374777","IND","Kauffman^Kevin","K.P. Kauffman Company, In","1675 Broadway, Suite 28 ","Denver","CO","80202","K.P. Kauffman Company, In","Chairman, CEO",20060720,1600.00,"","","","","",,"","","","","","","","60721.C2977"
'''

filing_230179 = '''HDR,FEC,5.3,FILPAC,4.23,"^",,
F2N,H4NY19073,,187 Jay Street,,Katonah,NY,10536,REP,,NY,19,2006,C00294900,Sue Kelly for Congress,PO Box 599,,Katonah,NY,105360599,,ROMP IV 2006,"228 S. Washington St., Ste. 115",,Alexandria,VA,22314,Sue Kelly,20060721,0.00,0.00,Kelly,Sue,W.,,
'''

filing_230185 = '''HDR,FEC,5.3,FECfile,5.3.1.0(f16),,,
F1MN,C00400937,VETERANS FOR VICTORY PAC,"2245 148TH AVENUE, NE",,BELLEVUE,WA,98007,N,,,-,H4ND00038,SAND^DUANE^^,H,ND,00,20040810,H2CA39102,ESCOBAR^TIM^^,H,CA,39,20040817,H2KY04071,DAVIS^GEOFFREY C^^,H,KY,04,20040824,S6VT00111,PARKE^GREGORY TARL^^,S,VT,00,20041012,S4CA00282,KALOOGIAN^HOWARD^^,S,CA,00,20060329,20040617,20040510,20060329,MCCURDY^RUSSELL^^,20060722
'''

filing_181904_truncated = '''HDR,FEC,5.2,NetFile,1967,^,FEC-180228,001,Report Generated: 07/22/2005 11:18:27
F3A,C00242768,JOHN T. DOOLITTLE FOR CONGRESS,2150 RIVER PLAZA DR. #150,,SACRAMENTO,CA,95833,,CA,4,Q2,P2006,20060606,CA,,,,,20050401,20050630,151947.33,0.00,151947.33,99667.66,0.00,99667.66,215344.09,0.00,15468.80,105249.60,20172.73,125422.33,0.00,26525.00,0.00,151947.33,0.00,0.00,0.00,0.00,0.00,0.00,151947.33,99667.66,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,99667.66,163064.42,151947.33,315011.75,99667.66,215344.09,280515.83,0.00,280515.83,197866.25,3745.00,194121.25,182756.60,40384.23,223140.83,0.00,57375.00,0.00,280515.83,0.00,0.00,0.00,0.00,3745.00,0.00,284260.83,197866.25,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,27499.86,225366.11,David Bauer,20050722,H0CA14042,DOOLITTLE^JOHN,D31,259532.33,0.00,259532.33,5875.00,0.00,5875.00
SA11AI,C00242768,IND,,748 E. HILLCREST AVE.,,Yuba City,CA,95991,P2006,,,NONE,1200.00,20050417,1000.00,15,,,,,,,,,,,,,,,,,INC:A:63552,,,,,,BOYER,KARNA J.,,,
SA11AI,C00242768,IND,,2150 PROFESSIONAL DRIVE,,ROSEVILLE,CA,95661,P2006,,self,Property Management,350.00,20050417,350.00,15,,,,,,,,,,,,,,,,,INC:A:63536,,,,,,BRYANT,ERIC,,MR.,
'''

filing_333594_truncated = '''HDR\x1cFEC\x1c6.1\x1cAristotle International CM5 PM5\x1cVersion 5.2\x1c\x1c0\x1c\x1c
F3XN\x1cC00428102\x1cAmerigroup Corporation Political Action Committee (Amerigroup PAC)\x1c\x1c4425 Corporation Lane\x1c\x1cVirginia Beach\x1cVA\x1c23462   \x1cQ1\x1c\x1c\x1c\x1c20080101\x1c20080331\x1cX\x1cLittel\x1cJohn\x1cE.\x1c\x1c\x1c20080415\x1c29749.49\x1c34461.13\x1c64210.62\x1c9039.97\x1c55170.65\x1c0.00\x1c0.00\x1c31467.51\x1c2993.62\x1c34461.13\x1c0.00\x1c0.00\x1c34461.13\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c34461.13\x1c34461.13\x1c0.00\x1c0.00\x1c39.97\x1c39.97\x1c0.00\x1c6500.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c2500.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c9039.97\x1c9039.97\x1c34461.13\x1c0.00\x1c34461.13\x1c39.97\x1c0.00\x1c39.97\x1c29749.49\x1c2008\x1c34461.13\x1c64210.62\x1c9039.97\x1c55170.65\x1c31467.51\x1c2993.62\x1c34461.13\x1c0.00\x1c0.00\x1c34461.13\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c34461.13\x1c34461.13\x1c0.00\x1c0.00\x1c39.97\x1c39.97\x1c0.00\x1c6500.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c2500.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c9039.97\x1c9039.97\x1c34461.13\x1c0.00\x1c34461.13\x1c39.97\x1c0.00\x1c39.97
SA11AI\x1cC00428102\x1c80413.C183\x1c\x1c\x1cIND\x1c\x1cAncona\x1cVincent\x1c\x1c\x1c\x1c6640 Towering Oak Path\x1c\x1cColumbia\x1cMD\x1c21044\x1c\x1c\x1c20080111\x1c38.50\x1c38.50\x1c15\x1cReceipt\x1c\x1cAMERIGROUP Maryland  Inc.\x1cCOO - Health Plan\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1cPayroll Deduction: (38.50/Pay Period          )\x1c\x1c
SA11AI\x1cC00428102\x1c80413.C223\x1c\x1c\x1cIND\x1c\x1cAncona\x1cVincent\x1c\x1c\x1c\x1c6640 Towering Oak Path\x1c\x1cColumbia\x1cMD\x1c21044\x1c\x1c\x1c20080125\x1c288.45\x1c326.95\x1c15\x1cReceipt\x1c\x1cAMERIGROUP Maryland  Inc.\x1cCOO - Health Plan\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1cPayroll Deduction: (57.69/Pay Period          )\x1c\x1c
'''

filing_333600_truncated = '''HDR\x1cFEC\x1c6.1\x1cNGP Campaign Office(R)\x1c3.0\x1cFEC-306890\x1c1\x1c
F3A\x1cC00434621\x1cDan Grant for Congress\x1c\x1c6109 Rickey Drive\x1c\x1cAustin\x1cTX\x1c78757\x1cTX\x1c10\x1cQ3\x1c\x1c\x1c\x1c20070701\x1c20070930\x1cGrant\x1cBarbara\x1c\x1c\x1c\x1c20080415\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c45247.00\x1c250.00\x1c44997.00\x1c38096.51\x1c0.00\x1c38096.51\x1c72247.01\x1c0.00\x1c4128.92\x1c41677.00\x1c1070.00\x1c42747.00\x1c0.00\x1c2500.00\x1c0.00\x1c45247.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c45247.00\x1c38096.51\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c250.00\x1c0.00\x1c0.00\x1c250.00\x1c0.00\x1c38346.51\x1c65346.52\x1c45247.00\x1c110593.52\x1c38346.51\x1c72247.01\x1c118112.08\x1c250.00\x1c117862.08\x1c45615.07\x1c0.00\x1c45615.07\x1c114162.08\x1c1350.00\x1c115512.08\x1c0.00\x1c2500.00\x1c100.00\x1c118112.08\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c118112.08\x1c45615.07\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c250.00\x1c0.00\x1c0.00\x1c250.00\x1c0.00\x1c45865.07\x1c\x1c\x1c\x1c\x1c\x1c
SA11AI\x1cC00434621\x1cC4068349\x1c\x1c\x1cIND\x1c\x1cAndries\x1cLarry\x1c\x1c\x1c\x1c1140 San Ysidro Dr\x1c\x1cBeverly Hills\x1cCA\x1c902102103\x1cP2008\x1c\x1c20070923\x1c50.00\x1c50.00\x1c\x1c\x1c\x1c20th Century Fox\x1cWriter\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c
SA11AI\x1cC00434621\x1cC4068348\x1c\x1c\x1cIND\x1c\x1cAtchity\x1cKenneth\x1c\x1c\x1c\x1c400 S Burnside No. 11B\x1c\x1cLos Angeles\x1cCA\x1c90036\x1cP2008\x1c\x1c20070923\x1c100.00\x1c100.00\x1c\x1c\x1c\x1cSelf\x1cProducer\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c
'''

filing_184656 = '''HDR,FEC,5.2,DemocracyDirect,Ver 6.5
F99,C00196725,Hewlett Packard Company PAC,3000 Hanover Street,20BX,Palo Alto,CA,94304,Ann Baskins,20050818,
[BEGINTEXT]
This amendment, filed in response to the letter from the FEC dated August 12, 2005, corrects the figures on Lines 11(a)(i) and 11(a)(ii), Column B of the Detailed Summary Page.
[ENDTEXT]
'''

filing_184693_truncated = '''HDR,FEC,5.2,"FECfile4",5.2,,FEC-180927 ,1
F3XA,C00093054,"Wal-Mart Stores Inc. PAC For Responsible Government","702 S.W. 8th Street","","Bentonville","AR","727160150","","X","M7","","","","20050601","20050630",513583.82,164630.83,678214.65,185500.00,492714.65,0.00,0.00,16783.90,146603.16,163387.06,0.00,0.00,163387.06,0.00,0.00,0.00,0.00,0.00,1243.77,0.00,164630.83,164630.83,0.00,0.00,0.00,0.00,0.00,125000.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,60500.00,185500.00,185500.00,163387.06,0.00,163387.06,0.00,0.00,0.00,301464.31,2005,719467.34,1020931.65,528217.00,492714.65,36939.60,674942.03,711881.63,0.00,0.00,711881.63,0.00,0.00,0.00,0.00,2500.00,5085.71,0.00,719467.34,719467.34,0.00,0.00,917.00,917.00,0.00,329500.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,197800.00,528217.00,528217.00,711881.63,0.00,711881.63,917.00,0.00,917.00,"Raymond W Bracy","20050818",0,0,0,0,0,0,0,0,0,0,0,0
SA11A1,C00093054,"IND",,"6 Canterbury Park","","Bentonville","AR","727124088","","","WAL-Mart Stores Inc.","Vice President Dmm",260,"20050616",20,"","","","","","","","","","","","","","","","","","WP2011866162005","","",,,,"Alderson",Art ,,,
SA11A1,C00093054,"IND",,"6 Canterbury Park","","Bentonville","AR","727124088","","","WAL-Mart Stores Inc.","Vice President Dmm",260,"20050602",20,"","","","","","","","","","","","","","","","","","WP188779622005","","",,,,"Alderson",Art ,,,
'''

filing_19538_truncated = '''HDR,"FEC","3.00","Proprietary","1.00"," ","FEC-12344",2,""
F3XA,"C00003418","REPUBLICAN NATIONAL COMMITTEE","310 FIRST STREET SE","","WASHINGTON","DC","20003"," ","X","M2","",20010101,"",20010101,20010131,24061917.05,10607884.29,34669801.34,9549466.96,25120334.38,1446.75,0.00,2056600.10,6193833.84,8250433.94,0.00,32500.00,8282933.94,0.00,0.00,0.00,37805.46,20360.00,94077.72,2172707.17,10607884.29,8435177.12,3515525.78,3698621.93,2313778.25,9527925.96,2876.00,0.00,0.00,0.00,0.00,0.00,18665.00,0.00,0.00,18665.00,0.00,9549466.96,5850845.03,8282933.94,18665.00,8264268.94,5829304.03,37805.46,5791498.57,24061917.05,2001,10607884.29,34669801.34,9549466.96,25120334.38,2056600.10,6193833.84,8250433.94,0.00,32500.00,8282933.94,0.00,0.00,0.00,37805.46,20360.00,94077.72,2172707.17,10607884.29,8435177.12,3515525.78,3698621.93,2313778.25,9527925.96,2876.00,0.00,0.00,0.00,0.00,0.00,18665.00,0.00,0.00,18665.00,0.00,9549466.96,5850845.03,8282933.94,18665.00,8264268.94,5829304.03,37805.46,5791498.57,"JAY C. BANNING Asstant Treasurer","20010905"
SD9,"C00003418","CAN","FRIENDS OF SENATOR DAVID KARNES","626 109TH PLAZA","","OMAHA","NE","68154","NEWS RELEASES",612.00,0.00,0.00,612.00,"","",""," ","","","","","","","",""," ","D09-01"
SD9,"C00003418","CAN","SENATOR KARNES CAMPAIGN","626 109TH PLAZA","","OMAHA","NE","68154","NEWS RELEASES",834.75,0.00,0.00,834.75,"","",""," ","","","","","","","",""," ","D09-02"
SA11A1,"C00003418","IND","Abernathy^Jean^Mrs.","50 Broad Cove Lane","","Montgomery","TX","77356","P2002","","","Retired",0.00,20001012,300.00,"15","","","",""," ","","","","","","","","","X","","C","31912284","","",""
SA11A1,"C00003418","IND","Abraham^Renee^Mrs.","1317 University Avenue","","Lubbock","TX","79401","P2002","","Self-employed","Office Manager",0.00,20001205,55.00,"15","","","",""," ","","","","","","","","","X","","C","32331751","","",""
'''

filing_22784_truncated = '''"HDR","FEC","3.00","THE FEC ELECTRONIC FILER","1.01a2zac..01C","^","FEC- 17081",1,""
"F3A","C00282970","FRIENDS OF BLAGOJEVICH","PO BOX 18415","","CHICAGO","IL","60618","","IL",5,"MY","",20021105,"IL","X","","","",20010101,20010630,137992.12,0.00,137992.12,50032.56,0.00,50032.56,1023657.79,0.00,27954.00,56945.00,21495.00,78440.00,802.12,58750.00,0.00,137992.12,0.00,0.00,0.00,0.00,0.00,11115.14,149107.26,50032.56,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,50032.56,924583.09,149107.26,1073690.35,50032.56,1023657.79,137992.12,0.00,137992.12,50032.56,0.00,50032.56,56945.00,21495.00,78440.00,802.12,58750.00,0.00,137992.12,0.00,0.00,0.00,0.00,0.00,11115.14,149107.26,50032.56,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,50032.56,"MICHAEL ASCARIDIS","20011226"
"SA11C","C00282970","PAC","AFGE Political Action Committee","80 F Street, NW","","Washington","DC","20001","P2002","","","",500.00,20010420,500.00,"15","","","","","","",,"","","","","","","","","","C00bE06","                    "
'''

filing_22795_truncated = '''HDR,FEC,3.00,Campaign Central,,,FEC-20671,1,
F4A,C00342519,Philadelphia 2000,1735 Market Street,51st Floor,Philadelphia,PA,19103,A,,Q3,20010701,20010930,468626.72,16173.15,484799.87,114174.12,370625.75,359667.00,0.00,110236.62,0.00,110236.62,0.00,110236.62,0.00,14713.80,0.00,14713.80,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,1459.35,0.00,1459.35,16173.15,110236.62,0.00,110236.62,0.00,0.00,0.00,0.00,3937.50,0.00,3937.50,114174.12,4404651.52,2001,828116.74,5232768.26,4862142.51,370625.75,4437134.58,141496.91,4295637.67,0.00,4295637.67,0.00,335366.65,0.00,0.00,141496.91,0.00,351253.18,828116.74,4437134.58,0.00,0.00,425007.93,4862142.51,Karen Dougherty Bucholz,20011227
SA14A,C00342519,ORG,Ballard Spahr Andrews & Ingersoll LLP,1735 Market Street,51st Floor,Philadelphia,PA,19103,C2000,,,,74929.53,20010701,4963.72,,In-Kind Contribution,,,,,,,,,,,,,,,,41101,,,
sA14A,C00342519,ORG,Ballard Spahr Andrews & Ingersoll LLP,1735 Market Street,51st Floor,Philadelphia,PA,19103,C2000,,,,81018.5,20010731,6088.97,,In-Kind Contribution,,,,,,,,,,,,,,,,41102,,,
'''

filing_23422_abbreviated = '''HDR,FEC,3.00,"FECfile4",4.00,,FEC-15985,1
F3XA,C00096842,"The American Electric Power Committee For Responsible Government","1 Riverside Plaza","P.O. Box 16631","Coumbus","OH","43216","","X","MY","","","","20010101","20010630",58496.65,231937.92,290434.57,206900.00,83534.57,0.00,0.00,34341.83,134678.00,169019.83,0.00,62918.09,231937.92,0.00,0.00,0.00,0.00,0.00,0.00,0.00,231937.92,231937.92,0.00,0.00,0.00,0.00,0.00,147500.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,59400.00,206900.00,206900.00,231937.92,0.00,231937.92,0.00,0.00,0.00,58496.65,2001,231937.92,290434.57,206900.00,83534.57,169019.83,0.00,169019.83,0.00,62918.09,231937.92,0.00,0.00,0.00,0.00,0.00,0.00,0.00,231937.92,231937.92,0.00,0.00,0.00,0.00,0.00,147500.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,59400.00,206900.00,206900.00,231937.92,0.00,231937.92,0.00,0.00,0.00,"Doreen W. Hohl","20020115"
SA11A1,C00096842,"IND","Ackerman^Melinda S","4033 Silver Springs Lane","","Columbus","OH","432309874","","","AEP Services Corporation","Svp-Human Resources",420,"20010331",70,"","PayDed Id - 655","","","","","","","","","","","","","",,"","1177784688","",""
SB23,C00096842,"CCM","Hall for Congress","2833 Northeast Weidler Street","","Portland","OR","97232","","2002 Primary","P2002","","20010410",1000,"C00288357","H4OR03044","Everett^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","H","OR","03","","","","","","","",,"","23188910104105668548","",""
'''

filing_31454_faked = '''HDR,FEC,3.00,we did it,0203HGA,>,,,
F3PN,C00361790,Hagelin/Goldhaber,PO Box 1900,,Fairfield,IA,52556,,,,Q1,G2000,20001107,,20020101,20020331,74.21,,74.21,15.90,58.31,,55578.23,,,15.90,,,,,,,,,,,,,,,,,15.90,,,,,,,,,,,,15.90,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3021.00,,,,3021.00,,,,,525.00,,,525.00,,3546.00,969.91,,1862.44,,,,,150.00,,,150.00,,2982.35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Blanche Woodward,20020409
SA11A1,C00361790,"IND","Ackerman>Melinda S","4033 Silver Springs Lane","","Columbus","OH","432309874","","","AEP Services Corporation","Svp-Human Resources",420,"20010331",70,"","PayDed Id - 655","","","","","","","","","","","","","",,"","1177784688","",""
'''

filing_33818_truncated = '''"HDR","FEC","3.00","Proprietary",2002,"0","",0,"M4 2002"
"F3XN","C00003418","REPUBLICAN NATIONAL COMMITTEE","310 FIRST STREET SE","","WASHINGTON","DC","20003"," ","X","M4","",20020420,"",20020301,20020331,44472804.65,8906846.93,53379651.58,7533865.45,45845786.13,1446.75,0.00,1859262.91,5082449.96,6941712.87,0.00,21500.00,6963212.87,0.00,0.00,0.00,154880.12,0.00,56421.87,1732332.07,8906846.93,7174514.86,2335814.04,1748981.24,2888242.17,6973037.45,533976.00,25000.00,0.00,0.00,0.00,0.00,1852.00,0.00,0.00,1852.00,0.00,7533865.45,5784884.21,6963212.87,1852.00,6961360.87,5224056.21,154880.12,5069176.09,34420547.12,2002,32088616.47,66509163.59,20663377.46,45845786.13,6122494.10,20356875.00,26479369.10,0.00,94500.00,26573869.10,2327.00,0.00,0.00,551329.16,0.00,160956.48,4800134.73,32088616.47,27288481.74,6780967.14,4634383.55,8598758.14,20014108.83,600311.90,25000.00,0.00,0.00,0.00,0.00,23956.73,0.00,0.00,23956.73,0.00,20663377.46,16028993.91,26573869.10,23956.73,26549912.37,15379725.28,551329.16,14828396.12,"JAY C. BANNING Asstant Treasurer",20020420
"SD9","C00003418","","FRIENDS OF SENATOR DAVID KARNES","626 109TH PLAZA","","OMAHA","NE","68154","NEWS RELEASES",612.00,0.00,0.00,612.00,"","",""," ","","","","","","","",""," ","D09-01"
"SD9","C00003418","","SENATOR KARNES CAMPAIGN","626 109TH PLAZA","","OMAHA","NE","68154","NEWS RELEASES",834.75,0.00,0.00,834.75,"","",""," ","","","","","","","",""," ","D09-02"
"SA11A1","C00003418","IND","Abel^MarciA.^Mrs.","2342 E. Riverdale Circle","","Mesa","AZ","85213"," "," ","Alaska Airlines","Reservations Sales Coord.",500.00,20020117,500.00,"15",""," "," "," "," "," "," "," "," "," "," "," "," ","X"," ","C","33999581"," "," ",""
"SA11A1","C00003418","IND","Allyn^Margaret^Ms.","1420 Sheridan Road","","Wilmette","IL","60091"," "," ","","Housewife",500.00,20020115,500.00,"15",""," "," "," "," "," "," "," "," "," "," "," "," ","X"," ","C","33981112"," "," ",""
"SA11A1","C00003418","IND","Andalla^Helen^Ms.","7237 Wapello Drive","","Rockville","MD","20855"," "," ","Facility Service Co., Inc.","Landscaper",800.00,20020222,400.00,"15",""," "," "," "," "," "," "," "," "," "," "," "," ","X"," ","C","34279570"," "," ",""
"SA11A1","C00003418","IND","Anderson^RalphJ.^Mr.","4 Orchard Hill Drive","","Moline","IL","61265"," "," ","","Retired",250.00,20020226,250.00,"15",""," "," "," "," "," "," "," "," "," "," "," "," ","X"," ","C","34304612"," "," ",""
'''

filing_39775_truncated = '''HDR,FEC,3.00,Region's Own Software,2.00,,FEC-23808 F3XN,1,
F3XA,C00179473,Regions Financial Corporation Political Action Committee,417 20th Street North,,Birmingham,AL,35203,,,YE,,,AL,20010701,20011231,290160.83,155186.18,445347.01,44500.00,400847.01,,,69942.59,85243.59,155186.18,.00,.00,155186.18,.00,.00,.00,.00,.00,.00,.00,155186.18,155186.18,.00,.00,.00,.00,29500.00,15000.00,.00,.00,.00,.00,.00,.00,.00,.00,.00,44500.00,44500.00,155186.18,.00,155186.18,.00,.00,.00,214934.87,2001,278880.62,493815.49,92968.48,400847.01,126271.96,152608.66,278880.62,.00,.00,278880.62,.00,.00,.00,.00,.00,.00,.00,278880.62,278880.62,.00,.00,.00,.00,53768.48,39200.00,.00,.00,.00,.00,.00,.00,.00,.00,.00,92968.48,92968.48,278880.62,.00,278880.62,.00,.00,.00,John Hixon,20020620
SA11ai,C00179473,IND,"West^Neil^","6712 Hollytree Circle","",Tyler,TX,75703,,,"Regions Bank","President",500.00,20010709,500.00,,,,,,,,,,,,,,,,,N,YEC011
SA11ai,C00179473,IND,"Weaver^Michael^D.^","40 Cheet Road","",Oneonta,AL,35121,,,"Otelco Tel.","President",200.00,20010809,200.00,,,,,,,,,,,,,,,,,N,YEC012
'''
# Emacs isn't smart enough to parse the triple-quoted string.

filing_48608_truncated = '''"HDR","FEC","3.00","Aristotle International CM4 PM4","Version 4.1.1","^","FEC-33531.","1"
"F3A","C00367854","Ballenger for Congress","PO Box 2009","","Council Bluffs","IA","51502   ","X","IA",5,"Q1","P2002",20021105,"","X","","","",20020101,20020331,45823.00,0.00,45823.00,82360.05,0.00,82360.05,205835.18,0.00,250000.00,35315.00,9508.00,44823.00,0.00,1000.00,0.00,45823.00,0.00,150000.00,0.00,150000.00,0.00,0.00,195823.00,82360.05,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,82360.05,92372.23,195823.00,288195.23,82360.05,205835.18,105938.23,2000.00,103938.23,148123.05,20.00,148103.05,,,92677.23,98.00,7000.00,6163.00,105938.23,0.00,150000.00,100000.00,250000.00,20.00,0.00,355958.23,148123.05,0.00,0.00,0.00,0.00,2000.00,0.00,0.00,2000.00,0.00,150123.05,"M Eastman Chance",20020829
"TEXT","F3A","C00367854",""
"SA11A1","C00367854","IND","Mark E Abel","424 Oakland Ave","","Council Bluffs","IA","51503    ","P2002","","Smith Davis & Abel","Insurance",600.00,20020209,100.00,"15","Receipt","","","","","","","","","","","",,"","","","0407200255C2471"
'''

filing_353121_truncated = '''HDR\x1cFEC\x1c6.2 \x1cMeadWestvaco Corp. Political Action Committee\x1c3.1\x1c\x1c0\x1c
F3XN\x1cC00065987\x1cMeadWestvaco Corp. Political Action Committee\x1c\x1c11013 West Broad Street\x1c\x1cGlen Allen\x1cVA\x1c23060\x1cM7\x1c\x1c\x1c\x1c20080601\x1c20080630\x1cX\x1cStoddard\x1cAlexander\x1cH.\x1c\x1c\x1c20080718\x1c112749.30\x1c7348.18\x1c120097.48\x1c7000.00\x1c113097.48\x1c0.00\x1c0.00\x1c1840.49\x1c4434.97\x1c6275.46\x1c0.00\x1c0.00\x1c6275.46\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c1000.00\x1c72.72\x1c0.00\x1c0.00\x1c0.00\x1c7348.18\x1c7348.18\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c7000.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c7000.00\x1c7000.00\x1c6275.46\x1c0.00\x1c6275.46\x1c0.00\x1c0.00\x1c0.00\x1c103358.03\x1c2008\x1c37188.45\x1c140546.48\x1c27449.00\x1c113097.48\x1c6369.49\x1c29377.75\x1c35747.24\x1c0.00\x1c0.00\x1c35747.24\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c1000.00\x1c441.21\x1c0.00\x1c0.00\x1c0.00\x1c37188.45\x1c37188.45\x1c0.00\x1c0.00\x1c349.00\x1c349.00\x1c0.00\x1c21000.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c6100.00\x1c0.00\x1c0.00\x1c0.00\x1c0.00\x1c27449.00\x1c27449.00\x1c35747.24\x1c0.00\x1c35747.24\x1c349.00\x1c0.00\x1c349.00
SA11AI\x1cC00065987\x1cA2008-1323375\x1c\x1c\x1cIND\x1c\x1cBuzzard\x1cJames\x1cA\x1c\x1c\x1c584 Manakin Towne Place\x1c\x1cManakin Sabot\x1cVA\x1c23103\x1c\x1c\x1c20080630\x1c100.00\x1c600.00\x1c15\x1c\x1c\x1cMEADWESTVACO CORP\x1cPresident\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c\x1c
'''

filing_83615_truncated = '''"HDR","FEC","5.00","Vocus PAC Management","3.00.214","","FEC-63727*BDfbd0",2,""
"F3XA","C00107771","Xcel Energy Employee Political Action Committee","1225 17th Street, Suite 900","","Denver","CO","80202","","X","12G","G2002",20021105,"",20021001,20021016,1660.56,8523.69,10184.25,10000.00,184.25,0.00,0.00,5581.22,1942.47,7523.69,0.00,0.00,7523.69,0.00,500.00,0.00,0.00,500.00,0.00,0.00,8523.69,8523.69,0.00,0.00,0.00,0.00,0.00,10000.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,10000.00,10000.00,7523.69,0.00,7523.69,0.00,0.00,0.00,21777.45,2002,86122.97,107900.42,107716.17,184.25,76166.54,7402.39,83568.93,0.00,0.00,83568.93,0.00,500.00,0.00,0.00,2000.00,54.04,0.00,86122.97,86122.97,0.00,0.00,16.17,16.17,0.00,104950.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,2750.00,107716.17,107716.17,83568.93,0.00,83568.93,16.17,0.00,16.17,"Johnston^Christine^Ms.^",20030429,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"SA11ai","C00107771","IND","Evans^Cynthia A^^","432 Fillmore St","","Denver","CO","802064324","","","XS","State VP Co/Wy/Az",1125.00,,112.50,"15","","","","","","","","","","","","","","","P/R Deduction ($112.50 Monthly)","","PR720968310179","","","",""
'''

filing_217921_truncated = '''HDR,FEC,5.30,Public Affairs Support Services Inc.,3.1,,FEC - 213367,1,,
F3XA,C00144345,sanofi-aventis group Employees PAC,300 Somerset Corp.  Blvd. MS: SC3-,,Bridgewater,NJ,08807,,X,M4,,,,20060301,20060331,99806.65,4809.28,104615.93,4005.00,100610.93,,,1920.36,2888.92,4809.28,,,4809.28,,,,,,,,4809.28,4809.28,,,5.00,5.00,,4000.00,,,,,,,,,,4005.00,4005.00,4809.28,,4809.28,5.00,,5.00,91184.25,2006,14491.68,105675.93,5065.00,100610.93,3021.68,11470.00,14491.68,,,14491.68,,,,,,,,14491.68,14491.68,,,65.00,65.00,,5000.00,,,,,,,,,,5065.00,5065.00,14491.68,,14491.68,65.00,,65.00,Timothy  Clark,20060517,,,,,,,,,,,,,
TEXT,SA11A1,,"Please note that the PAC is aware that we follow an alternate method of itemizing payroll receipts rather than the suggested manner of disclosing a single total for the reporting period along with the amount deducted per pay period.  Because the amounts collected per pay period may change often during the time covered by a single report, we find that reporting individual deductions separately more accurately discloses how the receipts are collected.",
'''

if __name__ == "__main__":
    cgitb.enable(format='text')
    doctest.testmod()

########NEW FILE########
__FILENAME__ = fec_csv
"""
Parser for FEC electronic filings.
"""
__author__ = ["Simon Carstensen <me@simonbc.com>"]

import glob, zipfile, re, tools, sys, os

HEADERS_PATH = '../data/crawl/fec/electronic/headers/'
EFILINGS_PATH = '../data/crawl/fec/electronic/'

def parse_headers():
    """Parse and load the specifications of the FEC electronic filing formats."""

    out = dict()
    for f in glob.glob(HEADERS_PATH + '*.csv'):
        headers = file(f).read().strip().split('\r')
        headers = filter(lambda x: not x.startswith('TEXT'), headers) # remove comments
        ver = f.split('/')[-1][:-4]
        out[ver] = dict()
        for h in headers:
            cols = [x.strip() for x in h.split(';') if x != '']
            form_type = cols[0].replace(' ', '')
            out[ver][form_type] = cols[1:]
    return out

def value_separator(header):
    """Determine whether the value separator is "," or FS"""
    if header.startswith('/* Header'):
        # we don't know how to parse format verison 2.0
        return None
    comma_separated = header.startswith('HDR,') or header.startswith('"HDR",')
    return comma_separated and ',' or chr(28)

def fixquotes(val):
    """Sometimes values are put inside quotes, remove these"""
    if val.startswith('"') and val.endswith('"'):
        val = val[1:-1]
    return val

VERSIONS = ['3.00', '5.00', '5.1', '5.2', '6.1', '6.2']
def get_format_ver(hdr, sep):
    """Determines the format version of a given FEC file"""
    ver = hdr.split(sep)[2]
    ver = fixquotes(ver)
    ver = ver.strip()
    if ver == '5.3': ver = '5.2'
    return (ver in VERSIONS and ver) or None

def get_form_type(report, sep):
    ftype = report.split(sep)[0]
    ftype = fixquotes(ftype)
    return ftype

def get_orig_report_id(hdr, sep, ver):
    i = (ver in ['6.1', '6.2'] and 5) or 6
    out = fixquotes(hdr.split(sep)[i])[4:]
    return out

def get_report_no(hdr, sep, ver):
    i = (ver in ['6.1', '6.2'] and 6) or 7
    out = fixquotes(hdr.split(sep)[i])
    return out

SPLIT_RE = re.compile('(,"[^,"]+),([^,"])')
def rsplit(filing, sep):
    """split for FEC records"""
    if sep == ',':
        # make sure we don't split inside quotes
        n = 1
        while n:
            filing, n = SPLIT_RE.subn('\g<1>\x1c\g<2>', filing)
        out = filing.split(',')
        out = [o.replace(chr(28), ',') for o in out]
    else:
        out = filing.split(sep)
        if not out[-1]:
            out = out[:-1]
    out = [fixquotes(o) for o in out]
    return out

def amendment_sort(x, y):
    return cmp(x['report_no'], y['report_no'])

def file_index(filepattern):
    reports = list()
    amendments = dict()

    files = glob.glob(filepattern)
    files.sort()
    for f in files:
        print >> sys.stderr, '\r', f,
        sys.stderr.flush()

        if not os.stat(f).st_size: continue
        try: zf = zipfile.ZipFile(f)
        except: continue
        filenames = zf.namelist()
        for fn in filenames:
            try:
                d = read_report(f, fn, zf.read(fn))
                if not d: continue
            except:
                continue
            if d['form_type'].endswith('A'):
                # amendment
                orig_report_id = get_orig_report_id(d['hdr'], d['sep'], d['ver'])
                d['report_no'] = get_report_no(d['hdr'], d['sep'], d['ver'])
                if not amendments.has_key(orig_report_id):
                    amendments[orig_report_id] = list()
                amendments[orig_report_id].append(d)
            else:
                # new report or termination
                reports.append(d)
        zf.close()
    for k, v in amendments.items():
        amendments[k] = sorted(v, amendment_sort)
    return (reports, amendments)

def get_form_id(report, sep):
    return rsplit(report, sep)[1]

def get_committee(report, sep):
    return rsplit(report, sep)[2]

def get_candidate_fec(header, report, form_type, ver):
    if form_type.startswith('F3X') or ver == '3.00':
        return None
    if ver in ['6.1', '6.2']:
        i = header.index('CANDIDATE ID NUMBER')
    else:
        i = header.index('FEC CANDIDATE ID NUMBER')
    if len(report) <= i: return None
    out = report[i].strip() or None
    return out

def get_contrib_candidate_fec(header, schedule, ver):
    if ver in ['6.1', '6.2']:
        i = header.index('DONOR CANDIDATE FEC ID')
    else:
        i = header.index('FEC CANDIDATE ID NUMBER')
    if len(schedule) <= i: return None
    out = schedule[i] or None
    return out

def get_expend_candidate_fec(header, schedule, ver):
    if ver == '6.1':
        i = header.index('PAYEE CANDIDATE FEC ID')
    elif ver == '6.2':
        i = header.index('BENEFICIARY CANDIDATE FEC ID')
    else:
        i = header.index('FEC CANDIDATE ID NUMBER')
    if len(schedule) <= i: return None
    out = schedule[i] or None
    return out

RE_COMMITTEE = re.compile('([^ ]+ .+) for congress', re.IGNORECASE)
def get_candidate(header, report, form_type, sep, ver):
    fields = rsplit(report, sep)
    if form_type.startswith('F3X'):
        return None
    if ver in ['6.1', '6.2']:
        i_first = header.index('CANDIDATE FIRST NAME')
        i_middle = header.index('CANDIDATE MIDDLE NAME')
        i_last = header.index('CANDIDATE LAST NAME')
        if len(fields) <= i_middle: return None
        middle = fields[i_middle]
        middle = middle + (len(middle) is 1 and '.' or '')
        out = ' '.join(filter(lambda x: x, [fields[i_first], middle, fields[i_last]]))
    else:
        i = header.index('11(d) The Candidate')
        if len(fields) <= i: return None
        out = fields[i]
    if not out:
        #committee name is sometimes 'x for Congress', get candidate name here
        committee = get_committee(report, sep)
        m =  RE_COMMITTEE.match(committee)
        if m:
            out = m.groups()[0]
    return out or None

def get_tran_id(header, schedule, ver):
    if ver in ['6.1', '6.2']:
        i = header.index('TRANSACTION ID NUMBER')
    else:
        i = header.index('TRAN ID')
    if len(schedule) <= i: return None
    out = schedule[i] or None
    return out

def get_occupation(header, schedule, ver):
    if ver in ['6.1', '6.2']:
        i = header.index('CONTRIBUTOR OCCUPATION')
        if len(schedule) <= i: return None
        return schedule[i] or None
    else:
        return None

def get_contributor_org(header, schedule, ver):
    if ver in ['3.00', '5.00']:
        return None
    if ver in ['6.1', '6.2']:
        i = header.index('CONTRIBUTOR ORGANIZATION NAME')
    else:
        i = header.index('CONTRIB ORGANIZATION NAME')

    if len(schedule) <= i: return None
    return schedule[i] or None

def get_contributor(header, sch, ver):
    if ver in ['6.1', '6.2']:
        i_first = header.index('CONTRIBUTOR FIRST NAME')
        i_middle = header.index('CONTRIBUTOR MIDDLE NAME')
        i_last = header.index('CONTRIBUTOR LAST NAME')
        if len(sch) <= i_middle: return None
        middle = sch[i_middle]
        middle = middle + (len(middle) is 1 and '.' or '')
        out = ' '.join(filter(lambda x: x, [sch[i_first], middle, sch[i_last]]))
    else:
        i = header.index('CONTRIBUTOR NAME')
        if len(sch) <= i: return None
        out = sch[i]
    return out or None

def get_employer(header, schedule, ver):
    if ver in ['6.1', '6.2']:
        i = header.index('CONTRIBUTOR EMPLOYER')
        if len(schedule) <= i: return None
        return schedule[i] or None
    else:
        return None

def get_contribution_amount(header, schedule, ver):
    if ver in ['6.1', '6.2']:
        i = header.index('CONTRIBUTION AMOUNT')
    else:
        i = header.index('AMOUNT RECEIVED')
    if len(schedule) <= i: return None
    return schedule[i] or None

def get_expenditure_amount(header, schedule, ver):
    if ver in ['6.1', '6.2']:
        i = header.index('EXPENDITURE AMOUNT')
    else:
        i = header.index('AMOUNT OF EXPENDITURE')
    if len(schedule) <= i: return None
    return schedule[i] or None

def get_recipient(header, sch, ver):
    if ver in ['6.1', '6.2']:
        i = header.index('PAYEE ORGANIZATION NAME')
    else:
        i = header.index('RECIPIENT NAME')
    if len(sch) <= i: return None
    out = sch[i]
    return out.strip() or None

def get_contribution_date(header, sch, ver):
    "YYYYMMDD"
    if ver in ['6.1', '6.2']:
        i = header.index('CONTRIBUTION DATE')
    else:
        i = header.index('DATE RECEIVED')
    if len(sch) <= i: return None
    date = sch[i]
    if not date:
        return None
    year = date[0:4]
    month = date[4:6]
    day = date[6:8]
    return '%s-%s-%s' % (year, month, day)

def get_expenditure_date(header, sch, ver):
    "YYYYMMDD"
    if ver in ['6.1', '6.2']:
        i = header.index('EXPENDITURE DATE')
    else:
        i = header.index('DATE OF EXPENDITURE')
    if len(sch) <= i: return None
    date = sch[i]
    if not date:
        return None
    year = date[0:4]
    month = date[4:6]
    day = date[6:8]
    return '%s-%s-%s' % (year, month, day)

def get_header(headers, form_type, ver):
    if not headers.has_key(ver): return None
    headers = headers[ver]
    if form_type.startswith('F'):
        form_type = form_type[:-1] # remove 'N', 'T' or 'A'
    elif form_type.startswith('S'):
        if ver in ['5.2', '6.1', '6.2']:
            form_type = 'Sch'+form_type[1]
        else:
            form_type = 'S'+form_type[1]
    return headers.has_key(form_type) and headers[form_type] or None

SCH_RE = re.compile('^"?(Sch|S)(A|B)')
SCHA_RE = re.compile('^(Sch|S)A')
def get_schedules(headers, schedules, sep, ver):
    out = dict()
    for s in schedules:
        if not SCH_RE.match(s):
            continue
        sch = dict()
        sch['form_type'] = get_form_type(s, sep)
        header = get_header(headers, sch['form_type'], ver)
        if not header:
            continue
        fields = rsplit(s, sep)
        sch['tran_id'] = get_tran_id(header, fields, ver)
        if SCHA_RE.match(sch['form_type']):
            sch['type'] = 'contribution'
            sch['date'] = get_contribution_date(header, fields, ver)
            sch['contributor_org'] = get_contributor_org(header, fields, ver)
            sch['contributor'] = get_contributor(header, fields, ver)
            sch['occupation'] = get_occupation(header, fields, ver)
            sch['employer'] = get_employer(header, fields, ver)
            sch['amount'] = get_contribution_amount(header, fields, ver)
        else:
            sch['type'] = 'expenditure'
            sch['date'] = get_expenditure_date(header, fields, ver)
            sch['recipient'] = get_recipient(header, fields, ver)
            sch['amount'] = get_expenditure_amount(header, fields, ver)
        out[sch['tran_id']] = sch
    return out

def get_records(data):
    zf = zipfile.ZipFile(data['zfn'])
    filing = zf.read(data['report_id']+'.fec')
    lines = filing.split('\n')
    lines = filter(lambda x: x != '', map(lambda x: x.strip(), lines))
    lines = map(lambda x: x.decode('latin1'), lines)
    cover, schedules = lines[1], lines[2:]
    return cover, schedules

def parse_report(headers, data):
    cover, schedules = get_records(data)
    form_type, ver, sep = data['form_type'], data['ver'], data['sep']
    header = get_header(headers, form_type, ver)
    out = dict()
    out['filer_id'] = get_form_id(cover, sep)
    out['form_type'] = form_type
    out['report_id'] = data['report_id']
    out['committee'] = get_committee(cover, sep)
    out['candidate'] = get_candidate(header, cover, form_type, sep, ver)
    out['candidate_fec_id'] = get_candidate_fec(header, cover, form_type, ver)
    out['schedules'] = get_schedules(headers, schedules, sep, ver)
    return out

FORM_TYPES = ['F3', 'F3N', 'F3A', 'F3T', 'F3X', 'F3XN', 'F3XA', 'F3T']
def read_report(zfilename, filename, data):
    report_id = filename[:-4]
    lines = data.split('\n')
    lines = filter(lambda x: x != '', map(lambda x: x.strip(), lines))
    lines = map(lambda x: x.decode('latin1'), lines)
    hdr, records = lines[0], lines[1:]
    sep = value_separator(hdr)
    if not sep:
        return None
    ver = get_format_ver(hdr, sep)
    if not ver:
        return None
    form_type = get_form_type(records[0], sep)
    if form_type not in FORM_TYPES:
        return None
    out = dict(zfn=zfilename, report_id=report_id, hdr=hdr, sep=sep, ver=ver, form_type=form_type)
    return out

def apply_amendment(headers, report, amendment):
    cover, schedules = get_records(amendment)
    sep, ver = amendment['sep'], amendment['ver']
    amendment = parse_report(headers, amendment)
    report['committee'] = amendment['committee']
    report['candidate'] = amendment['candidate']
    for k, v in amendment['schedules'].items():
        report['schedules'][k] = v

def apply_amendments(headers, report, amendments):
    report_id = report['report_id']
    if amendments.has_key(report_id):
        amendments = amendments[report_id]
        for a in amendments:
            apply_amendment(headers, report, a)
    report['schedules'] = report['schedules'].values()
    return report

def parse_filings(headers, reports, amendments):
    for r in reports:
        print r['zfn']
        r = parse_report(headers, r)
        r = apply_amendments(headers, r, amendments)
        yield r

def parse_efilings(filepattern = EFILINGS_PATH + '*.zip'):
    headers = parse_headers()
    reports, amendments = file_index(filepattern)
    return parse_filings(headers, reports, amendments)

if __name__ == "__main__":
    tools.export(parse_efilings())


########NEW FILE########
__FILENAME__ = field_mapper
#!/usr/bin/python
# -*- coding: utf-8; -*-
"""Field mapping
================

When you have a big pile of dicts in slightly different formats, and
you want to transform them to dicts in a consistent format, this
module lets you write the field mappings in a small amount of code and
execute them reasonably quickly.

Quick start
-----------

    >>> m = FieldMapper({'age': ['Age', 'age'], 'lastname': 'sname'})
    >>> [m.map(x) for x in [dict(Age=30, sname='Smith'),
    ...                     dict(age=49, sname='Wilson'),
    ...                     dict(x=3, y=4, lastname='Jones')]]
    ... #doctest: +NORMALIZE_WHITESPACE
    [{'lastname': 'Smith', 'age': 30,
      'original_data': {'Age': 30, 'sname': 'Smith'}},
     {'lastname': 'Wilson', 'age': 49,
      'original_data': {'age': 49, 'sname': 'Wilson'}},
     {'original_data': {'y': 4, 'lastname': 'Jones', 'x': 3}}]

You can include functions as mappings; it looks for their argument
names in the input data.

    >>> m = FieldMapper({'age': ['age', lambda birthyear: 2008 - birthyear]})
    >>> m.map({'age': 27})
    {'age': 27, 'original_data': {'age': 27}}
    >>> m.map({'birthyear': 1970})
    {'age': 38, 'original_data': {'birthyear': 1970}}
    >>> m = FieldMapper({'date': lambda year, month, day:
    ...                          '%s-%s-%s' % (year, month, day)})
    >>> m.map(dict(year=2008, month=11, day=19))['date']
    '2008-11-19'

You can construct (slightly) more complicated pipelines with `Reformat`:

    >>> def invert_name(n):
    ...     last, first = n.split(',')
    ...     return '%s %s' % (first.strip(), last.strip())
    >>> m = FieldMapper({'name': Reformat(format=invert_name,
    ...                                   source=['name', 'fullname'])})
    >>> m.map({'name': 'Smith, John'})['name']
    'John Smith'
    >>> m.map({'fullname': 'Smith,John'})['name']
    'John Smith'

Finally, you can use `CatchAllField` (q.v.) to handle cases where these
tools dont cut it.

BUGS
----

-  Any particular input field can be used reliably by at most one
   output field.  If you try to use the same input field for more than
   one thing, you may get an `AssertionError`.  (You can often work
   around this with `CatchAllField`  just set its `inputs` to
   non-conflicting input field names.)

        >>> FieldMapper({'name': lambda firstname, lastname: firstname + ' ' + lastname,
        ...              'firstname': 'firstname', 'lastname': 'lastname'})
        ... #doctest: +ELLIPSIS
        Traceback (most recent call last):
          ...
        AssertionError: ...

-   If more than one of the possible sources for a single output field
    are present in the input data, one will overwrite the others
    arbitrarily.  In this example, we only successfully got a nonempty
    'x' in one of the two cases:

        >>> m = FieldMapper({'x': ['a', 'b']})
        >>> datum1 = {'a': '', 'b': 'hi'}
        >>> datum2 = {'a': 'hi', 'b': ''}
        >>> m.map(datum1)['x'] and m.map(datum2)['x']
        ''

- You cant construct *arbitrarily* more complicated pipelines with
  `Reformat`.
- Its still way too slow.

"""

import types

class Field:
    """Represents a field in the output data, and knows how to compute it.

    This is an abstract base class; most concrete subclasses can be
    constructed most conveniently with the factory function
    `as_field`, which creates a sort of tiny DSEL for describing field
    mappings.

    Two of the concrete subclasses can contain Fields themselves;
    CompositeField contains a list of Fields any of which can supply
    its value, and Transform contains a single Field whose value it
    transforms.

    """
    def get_from(self, data):
        "Simple method for testing."
        for key, func in self.inverteds().items():
            if key in data: return func(data)
    def inverteds(self):
        """Return a dictionary of ways to get this fields value.

        The keys of the dictionary are the names of original source
        fields that must be present for the function to be applicable;
        the values are functions that take the original source data
        dictionary.  Those functions are entitled to access other
        fields in the dictionary, and to throw a KeyError if they want
        to fail.

        This is an efficiency hack; the objective is that we can avoid
        trying to look at fields that arent present at all in the
        source data.  It saved only about 14% of user CPU time when I
        tested it.

        """

class InputField(Field):
    """
    >>> f = InputField('bob')
    >>> f.inverteds().keys()
    ['bob']
    >>> f.inverteds()['bob']({'bob': 4, 'mel': 5})
    4
    """
    def __init__(self, name): self.name = name
    def inverteds(self):
        name = self.name
        return {name: lambda data: data[name]}


class Reformat(Field):
    """Changes the format of data in a field.

    >>> import fixed_width
    >>> Reformat(format=fixed_width.date,
    ...          source=['bob']).get_from({'bob': '20080930'})
    '2008-09-30'
    >>> Reformat(format=fixed_width.date,
    ...          source=['bob']).get_from({'dan': '20080830'})

    Note that the above test failed to return anything.

    >>> f = Reformat(format=lambda x: x, source=['bob', 'fred'])
    >>> sorted(f.inverteds().keys())
    ['bob', 'fred']
    >>> f.inverteds()['bob']({'bob': 39})
    39
    """
    def __init__(self, source, format):
        self._source = as_field(source)
        self._format = format
    def inverteds(self):
        rv = {}
        format = self._format
        for k, v in self._source.inverteds().items():
            # v=v so each lambda has its own v instead of all sharing
            # the same v; it's not intended that callers will override
            # v!
            rv[k] = lambda data, v=v: format(v(data))
        return rv

class MultiInputField(Field):
    """A field whose value is computed from more than one input field.

    Its `inverteds()` includes only one of the input fields, currently
    the shortest one.  Thats because theres no reason to call it
    repeatedly; if one of the other input fields is missing, it will
    fail harmlessly with a KeyError.

    >>> f = MultiInputField(('a', 'b'), lambda a, b: a + ': ' + b)
    >>> f.inverteds().keys()
    ['a']

    >>> ffunc = f.inverteds().values()[0]
    >>> ffunc({'a': 'foo', 'b': 'bar'})
    'foo: bar'
    """
    def __init__(self, names, function):
        "`names` are the field names from which to get `function`'s args."
        self.names, self.function = names, function
    def inverteds(self):
        names = self.names
        def getter(data):
            return self.function(*[data[k] for k in names])
        return {self.names[0]: getter}

class CompositeField(Field):
    """A field with more than one possible source for its data.

    >>> f = CompositeField(['a', Reformat(source=['b'], format=len)])
    >>> sorted(f.inverteds().keys())
    ['a', 'b']
    >>> f.inverteds()['a']({'a': '90210'})
    '90210'
    >>> f.inverteds()['b']({'b': '90210'})
    5
    """
    def __init__(self, fields):
        self.fields = fields
    def inverteds(self):
        rv = {}
        for field in self.fields: rv.update(as_field(field).inverteds())
        return rv

class CatchAllField(Field):
    """A field for special cases.

    The `inputs` argument specifies a list of input field names that
    should cause this one to fire.  If any of those fields is present,
    `function` is called with the entire input record as an argument.

    >>> f = CatchAllField(['x', 'y'], lambda data: data.get('z'))
    >>> m = FieldMapper({'a': f})
    >>> [m.map(x) for x in [{'z': 2}, {'x': 4, 'z': 3}, {'y': 5, 'z': 6},
    ...                     {'x': 7}]]
    ... #doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
    [{'original_data': {...}}, {...'a': 3...}, {...'a': 6...}, 
     {...'a': None...}]

    """
    def __init__(self, inputs, function):
        self.inputs, self.function = inputs, function
    def inverteds(self):
        return dict((input, self.function) for input in self.inputs)

def argnames(func):
    "Compute a list of the names of the arguments of a Python function."
    return func.func_code.co_varnames[:func.func_code.co_argcount]

def as_field(obj):
    """Coerce obj to some kind of field."""
    if hasattr(obj, 'inverteds'):
        return obj
    elif isinstance(obj, basestring):
        return InputField(obj)
    elif isinstance(obj, types.ListType):
        return CompositeField(obj)
    elif isinstance(obj, types.FunctionType):
        return MultiInputField(argnames(obj), obj)
    raise "can't coerce to a field", obj

class FieldMapper:
    """Maps fields according to a field-mapping specification.

    Takes and returns a dict. The original dict comes out as a
    member named 'original_data'; otherwise its members are only
    copied across according to applicable field specs.

    If the fields output name is not specifically mentioned among a
    fields aliases, it isnt included in the fields to copy from.

    """
    def __init__(self, fields):
        self.inverteds = {}
        for name, field in fields.items():
            field = as_field(field)
            for k, v in field.inverteds().items():
                assert k not in self.inverteds, (name, k, field,
                                                 self.inverteds[k])
                self.inverteds[k] = (name, v)
        self.inverted_keys = set(self.inverteds.keys())
    def map(self, data):
        rv = {'original_data': data}
        # Here we intersect the keys in the data with the keys were
        # interested in, in order to avoid doing unnecessary work in
        # Python.
        for fieldname in set(data.keys()) & self.inverted_keys:
            name, func = self.inverteds[fieldname]
            try:
                v = func(data)
            except KeyError:
                pass
            else:
                rv[name] = v
        return rv

if __name__ == '__main__':
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = fixed_width
"""
Library for processing fixed-width files.
"""

import time, warnings
try:
    from web import storage
except ImportError:
    storage = dict

## Types used in definitions

def date(s):
    """where `s` is YYYYMMDD"""
    out = s[0:4] + '-' + s[4:6]
    if s[6:8]:
        out += '-' + s[6:8]
        assert time.strptime(out, '%Y-%m-%d')
    elif out == '0000-00':
        out = None
    else:
        assert time.strptime(out, '%Y-%m')
    return out

def year(s):
    return '20' + s

def string(s):
    return s.strip()

def state(s):
    s = string(s)
    if s == '.': return None
    assert s == s.upper()
    assert s.isalpha()
    return s

def digits(s):
    s = string(s)
    assert s.isdigit()
    return s

def boolean(s):
    return {'Y': True, 'N': False, ' ': None}[s]
    
def filler(required=None):
    def filler_internal(s):
        if required:
            assert s == required, repr(s)
    return filler_internal

def enum(s=None, **db):
    if isinstance(s, basestring):
        return string(s)
    else:
        if ' ' not in db: db[' '] = None
        def enum_lookup(s):
            if s in db:
                return db[s]
            else:
                warnings.warn('Expected one of %s in enumeration, but got %s' % 
                  (list(db), repr(s)))
                return None
        return enum_lookup

def table_lookup(table, preProcess=None):
    def get_from_table(d):
        if preProcess:
            d=preProcess(d)
        if d in table:
            return table[d]
        else:
            return string(d)
    return get_from_table

def integer(s):
    s = s.strip()
    if s:
        try:
            return int(s)
        except ValueError:
            return s
    else: return None

## Format of the definitions

FIELD_KEY = 0
FIELD_LEN = 1
FIELD_TYP = 2

## The functions you might want to call

def parse_line(linedef, line, debug=False):
    out = storage()
    n = 0
    for (k, l, t) in linedef:
        if l < 0 : # go back
            out[k] = t(line[n+l:n])
        if k is None:
            t(line[n:n+l])
        else:
            if debug: print k, repr(line[n:n+l]),
            out[k] = t(line[n:n+l])
            if debug: print repr(out[k])
        if l > 0: n += l
    return out

def get_len(filedef):
    if isinstance(filedef, dict):
        linelen = set(sum(line[FIELD_LEN] for line in kind) for kind in filedef.itervalues())
        assert len(linelen) == 1, [(kind_name, sum(line[FIELD_LEN] for line in kind)) for kind_name, kind in filedef.iteritems()]
        linelen = list(linelen)[0]
        return linelen
    else:
        return sum(line[FIELD_LEN] for line in filedef)

def parse_file(filedef, fh, f_whichdef=None, debug=False):
    linelen = get_len(filedef)
    if isinstance(filedef, dict):
        if not f_whichdef: f_whichdef = lambda x: x[0]
    else:
        f_whichdef = lambda x: slice(None, None)
    for line in iter(lambda: fh.read(linelen), ''):
        if line.replace('\x00', '').strip():
            yield parse_line(filedef[f_whichdef(line)], line, debug=debug)

########NEW FILE########
__FILENAME__ = generate_fec_csv_field_definitions
# -*- coding: utf-8; -*-
"""Generates CSV file of FEC field definitions.

This regenerates e.g. the `watchdog/import/parse/fec_headers/5.1.csv`
file from the `Fec_v510.xls` spreadsheet provided by the FEC.  It
doesnt completely work yet, because Im not quite sure what to do
about form 12; also, it omits form 3Z-1 entirely because the
spreadsheet doesnt contain any field definitions for it.

Instructions for use:

    python2.5 generate_fec_csv_field_definitions.py \
        ~/docs/FEC_v5.x/Fec_v510_revised_by_kragen.xls > fec_headers/5.1.csv

I really thought this would be about 5 lines of code.

"""

import pyExcelerator, sys

def numeric_cells_in_column_a(sheet):
    """Generates ((row, col), value) tuples for numeric cells in column A.

    Not in order.

    This is useful because column A contains the sequence number for
    the (filing) columns were looking for.
    """
    return (((row, col), value) for (row, col), value in sheet[1].items()
            if col == 0 and type(value) is not unicode)
 
def get_field_names(sheet):
    """Outputs the field names for a particular sheet.
    """
    sheetname, cells = sheet

    current_field_number = 0
    field_names = []
    for (row, col), value in sorted(numeric_cells_in_column_a(sheet)):
        if value != current_field_number + 1:
            # This still fails on F12, because a bunch of fields are omitted!
            # It found an error in Sch I in the FECs version of
            # Fec_v510.xls and Fec_v500.xls; Im running from a
            # corrected one.
            # XXX maybe do something better than just omit it?
            sys.stderr.write(("in sheet %s, field %d " +
                              "follows field %d; omitting this sheet\n") % (
                sheetname, value, current_field_number))
            return
        current_field_number = value
        field_names.append(cells[row, 1])
    print ';'.join([sheetname.replace('Sch ', 'S').upper()] + field_names)

def sheets_having_col_seq(sheets):
    """Sheets describing a record format say COL SEQ in the first column,
    in the first few rows.  This generator finds those sheets.

    """
    for name, cells in sheets:
        for row in range(20):
            if cells.get((row, 0)) == 'COL':
                assert cells.get((row+1, 0)) == 'SEQ'
                yield name, cells
                break
        else:
            sys.stderr.write("sheet %s has no COL SEQ; omitting this sheet\n"
                             % name)

def get_all_field_names(sheets):
    """Outputs the field names for all record types."""
    for sheet in sheets_having_col_seq(sheets): get_field_names(sheet)

if __name__ == '__main__':
    get_all_field_names(pyExcelerator.parse_xls(sys.argv[1]))

########NEW FILE########
__FILENAME__ = govtrack
"""
parse data from govtrack.us
"""

from settings import current_session

STATS_XML = '../data/crawl/govtrack/us/%s/repstats/' % current_session + '%s.xml'
FEC_XML = '../data/crawl/govtrack/us/fec/campaigns-2008.xml'
METRICS = ['enacted', 'novote', 'verbosity', 'speeches', 
  'spectrum', 'introduced', 'cosponsor']

from xml.dom import pulldom
import web
import tools
import glob

def parse_basics():
    for fn in glob.glob(STATS_XML % 'people'):
        dom = pulldom.parse(fn)
        for event, node in dom:
            if event == "START_ELEMENT" and node.tagName == "person":
                out = web.storage(node.attributes.items())
                dom.expandNode(node)
                out.roles = map(lambda r: web.storage(r.attributes.items()), node.getElementsByTagName('role'))
                
                if out.get('district'):
                    out.represents = out.state + '-' + out.district.zfill(2)
                else:
                    if out.get('state'):
                        out.represents = out.state
                        assert out.title == 'Sen.'
                
                if 'current-committee-assignment' in [
                  hasattr(x, 'tagName') and x.tagName for x in node.childNodes
                ]:
                    out.active = True

                yield out

def parse_stats(metrics=METRICS):
    for metric in metrics:
        for fn in glob.glob(STATS_XML % metric):
            try:
                dom = pulldom.parse(fn)
            except IOError:
                continue
            for event, node in dom:
                if event == "START_ELEMENT" and node.tagName == 'representative':
                    yield web.storage(node.attributes.items())

def parse_fec():
    dom = pulldom.parse(FEC_XML)
    for event, node in dom:
        if event == "START_ELEMENT" and node.tagName == 'candidate':
            dom.expandNode(node)
            fec_id = node.getElementsByTagName('id')[0].firstChild.nodeValue
            uri = node.getElementsByTagName('uri')[0].firstChild.nodeValue
            if fec_id in uri: continue
            bioguide_id = uri.split('/')[-1]
            yield {'fecid': fec_id, 'bioguideid': bioguide_id}

if __name__ == "__main__":
    tools.export(parse_basics())
    tools.export(parse_stats())
    if current_session == 110: tools.export(parse_fec())

########NEW FILE########
__FILENAME__ = irs_5500
from fixed_width import parse_file, string, date, integer, filler, state, digits

def_5500 = [
  ('unk1_digits', 26, string),
  ('unk2', 8, date),
  ('unk3', 8, date),
  ('unk4', 1, integer),
  ('unk4', 1, integer),
  ('unk4', 1, integer),
  ('unk4', 1, integer),
  ('unk4', 1, integer),
  ('unk4', 1, integer),
  ('unk4', 1, integer),
  ('unk4', 1, integer),
  ('plan_name', 140, string),
  ('unk5', 8, date),
  ('corp_name', 141, string),
  ('street1', 35, string),
  ('street2', 108, string),
  ('city', 22, string),
  ('state', 2, state),
  ('zip', 5, digits),
  ('zip4', 4, digits),
  ('unk6', 3, string),
  (None, 792, filler), # unparsed
  (None, 2, filler('\r\n'))
]

if __name__ == "__main__":
    import tools
    tools.export(parse_file(def_5500, file('../data/crawl/irs/5500/F_5500_2006.txt')))
########NEW FILE########
__FILENAME__ = irs_eo
import glob, itertools
from fixed_width import integer, string, date, filler, parse_file, enum, state, digits

def integer2(s): return integer(s[-1] + s[:-1])

def_eo = [
#  ('_type', 0, lambda s: 'Exempt Organization'),
  ('ein', 9-0, digits),
  ('primary_name', 79-9, string),
  ('careof_name', 114-79, string),
  ('street', 149-114, string),
  ('city', 171-149, string),
  ('state', 173-171, state),
  ('zip', 183-173, string),
  ('group_exemption_num', 187-183, string),
  ('subsection_code', 189-187, string),
  ('affiliation', 1, enum),
  ('classification_code', 194-190, string),
  ('ruling_date', 200-194, string),
  ('deductibility_code', 1, string),  
  ('foundation_code', 2, string),  
  ('activity_code', 212-203, string),  
  ('organization_code', 1, string),  
  ('exempt_org_status_code', 2, string),  
  ('advance_ruling_expiration', 221-215, string),
  ('tax_period', 227-221, string),
  ('asset_code', 1, string),  
  ('income_code', 1, string),  
  ('filing_requirement_code', 3, string),  
  (None, 3, filler),  
  ('accounting_period', 2, string),  
  ('asset_amt', 250-237, integer),  
  ('income_amt', 264-250, integer2),  
  ('form_990_revenue_amt', 278-264, integer2),  
  ('ntee_code', 282-278, string),  
  ('sort_name', 318-282, string),
  (None, 2, filler('\r\n'))
]

def parse():
    return itertools.chain(*[parse_file(def_eo, file(fn)) for fn in glob.glob('../data/crawl/irs/eo/*.LST')])
    
if __name__ == "__main__":
    import sys
    if 'load' in sys.argv:
        from settings import db
        db.multiple_insert("exempt_org", parse(), seqname=False)
    else:
        import tools
        tools.export(parse())

########NEW FILE########
__FILENAME__ = irs_pol
"""
Parse IRS' political organizations' form download.
"""
import warnings
import web
from fixed_width import string, integer, date, filler, enum, state, digits

olddate = date
def date(s):
    out = olddate(s)
    if out == '--': return None
    else: return out

boolean = enum(**{'1': True, '0': False, '': None})

pipe = (None, 1, filler('|'))

def def_address(name): 
    return [
      (name + '_address_1', string),
      (name + '_address_2', string),
      (name + '_address_city', string),
      (name + '_address_state', state),
      (name + '_address_zip', digits),
      (name + '_address_zip4', digits)
    ]


def_entity = [
  ('form_id', integer),
  ('entity_id', integer),
  ('org_name', string),
  ('ein', digits),
  ('entity_name', string),
  ('entity_tile', string),
] + def_address('entity')

def_polorgs = {
  'H': [
    ('_type', lambda s: 'Header'),
    ('transmission_date', date),
    ('transmission_time', string), #@@
    ('file_type', enum)
  ],
  '1': [
    ('_type', lambda s: '8871 Record'),
    ('form_type', integer), # == 8871
    ('form_id', integer),
    ('initial_report', boolean),
    ('amended_report', boolean),
    ('final_report', boolean),
    ('ein', digits),
    ('org_name', string),
  ] + def_address('mailing') + [
    ('email', string),
    ('established_date', date),
    ('custodian_name', string),
    ] + def_address('custodian') + [
    ('contact_name', string),
    ] + def_address('contact') 
      + def_address('business') + [
    ('exempt_8872', boolean),
    ('exempt_state', string),
    ('exempt_990', boolean),
    ('purpose', string),
    ('material_change_date', date),
    ('insert_datetime', string), #@@
    ('related_entity_bypass', boolean),
    ('eain_bypass', boolean)
  ],
  'D': [('_type', lambda s: 'Director')] + def_entity,
  'R': [('_type', lambda s: 'Related Entity')] + def_entity,
  'E': [
    ('_type', lambda s: 'Election Authority Identification Number (EAIN)'),
    ('form_id', integer),
    ('eain_id', integer),
    ('eain', string),
    ('state_issued', string)
  ],
  '2': [
    ('_type', lambda s: '8872 Record'),
    ('form_type', integer), # == 8872
    ('form_id', integer),
    ('period_begin_date', date),
    ('period_end_date', date),
    ('initial_report', boolean),
    ('amended_report', boolean),
    ('final_report', boolean),
    ('change_of_address', boolean),
    ('org_name', string),
    ('ein', digits),
    ] + def_address('mailing') + [
    ('email', string),
    ('established_date', date),
    ('custodian_name', string),
    ] + def_address('custodian') + [
    ('contact_name', string),
    ] + def_address('contact') 
      + def_address('business') + [
    ('quarter', enum), #?!
    #enum(
    #  a='First Quarterly', 
    #  b='Second Quarterly', 
    #  c='Third Quarterly',
    #  d='Year-End', 
    #  e='Mid-Year', 
    #  f='Monthly', 
    #  g='Pre-election', 
    #  h='Post-election')
    ('monthly_report_month', string),
    ('pre_election_type', string),
    ('pre_or_post_election_date', date),
    ('pre_or_post_election_state', string),
    ('sched_a', boolean),
    ('total_sched_a', integer),
    ('sched_b', boolean),
    ('total_sched_b', integer),
    ('insert_datetime', string), #@@
  ],
  'A': [
    ('_type', lambda s: 'Schedule A'),
    ('form_id', integer),
    ('schedule_a_id', integer),
    ('org_name', string),
    ('ein', digits),
    ('contributor_name', string),
  ] + def_address('contributor') + [
    ('contributor_employer', string),
    ('contribution_amount', string),
    ('contributor_occupation', string),
    ('agg_contribution_ytd', string), #@@float?
    ('contribution_date', date)
  ],
  'B': [
    ('_type', lambda s: 'Schedule B'),
    ('form_id', integer),
    ('schedule_b_id', integer),
    ('org_name', string),
    ('ein', digits),
    ('recipient_name', string),
  ] + def_address('recipient') + [
    ('recipient_employer', string),
    ('expenditure_amount', string),
    ('recipient_occupation', string),
    ('expenditure_date', date),
    ('expenditure_purpose', string)
  ],
  'F': [
    ('_type', 'Footer'),
    ('transmission_date', date),
    ('transmission_time', string), #@@
    ('record_count', integer)
  ]
}

def parse_line(s):
    if s[0] in def_polorgs:
        return parse_line_type(s.strip(), def_polorgs[s[0]])
    else:
        warnings.warn("Don't recognize: " + s[0])

def parse_line_type(line, def4type):
    out = web.storage()
    for (name, kind), val in zip(def4type, line.split('|')):
        out[name] = kind(val)
        print name, val, kind(val)
    return out

def parse_doc(doc):
    for line in doc: yield parse_line(line)

if __name__ == "__main__":
    import tools
    tools.export(parse_doc(file('../data/crawl/irs/pol/FullDataFile.txt')))
########NEW FILE########
__FILENAME__ = lobbyists
import glob, re, sys
from xml.dom import minidom
from pprint import pprint, pformat

import web

ZIPFILE='../data/crawl/house/lobby/2008_MidYear_XML.zip'
HOUSE_FILES='../data/crawl/house/lobby/*.xml'

def cleanint(n):
    for c in ', %$':
        n = n.replace(c, '')
    return n

def fixbool(x):
    if x == 'true': x = True
    elif x == 'false': x = False
    return x

# HACK: There is only one invalid date (02/31/2008) in the current data set.
#       Lets just set it to Null for now. A beter solution might be to parse
#       the dates here and check that set ones that don't parse to None.
def fixdate(x):
    if '02/31/2008' in x:
        return None
    return x

xml_schema = {
    'filerType': None,
    'organizationName': None,
    'lobbyistPrefix': None,
    'lobbyistFirstName': None,
    'lobbyistMiddleName': None,
    'lobbyistLastName': None,
    'lobbyistSuffix': None,
    'contactName': None,
    'senateRegID': None,
    'houseRegID': None,
    'reportYear': None,
    'reportType': None,
    'amendment': fixbool,
    'signedDate': fixdate,
    'certifiedcontent': fixbool,
    'noContributions': fixbool,
    'comments': None,
    'pacs': { 
        'pac': {
            'name': None
     } },
    'contributions': { 
        'contribution': {
            'type': None, 
            'contributorName': None, 
            'payeeName': None, 
            'recipientName': None, 
            'amount': cleanint, 
            'date': fixdate
    } },
}


def _parse_house_lobbyist(node, sch):
    #out = web.storage()
    out = {}
    for t, s in sch.items():
        elms = node.getElementsByTagName(t)
        for n in elms:
            if hasattr(n, 'firstChild') and n.firstChild:
                if t in out:
                    if not isinstance(out[t], list):
                        out[t] = [out[t]]
                    add_item = lambda x: (out[t].append(x) if x else None)
                else:
                    add_item = lambda x: (out.setdefault(t,x) if x else None)
                if not s:
                    add_item(n.firstChild.data.strip())
                elif callable(s):
                    add_item(s(n.firstChild.data.strip()))
                else:
                    add_item(_parse_house_lobbyist(n, s))
    if 'contribution' in out: return out['contribution']
    if 'pac' in out: return out['pac']
    return out


fileid_regex = re.compile(r'.*?([0-9]*)\.xml')
def parse_house_lobbyists():
    files = glob.glob(HOUSE_FILES)
    parse = lambda f: minidom.parse(f)
    if not files:
        import zipfile
        zf = zipfile.ZipFile(ZIPFILE)
        files = zf.namelist()
        parse = lambda f: minidom.parseString(zf.read(f))
    for f in files:
        print f
        dom = parse(f)
        out = _parse_house_lobbyist(dom, xml_schema)
        out['file_id'] = fileid_regex.match(f).group(1)
        yield out


if __name__ == "__main__":
    for x in parse_house_lobbyists():
        pprint(x)



########NEW FILE########
__FILENAME__ = mortality
"""
Imports NCHS (National Center for Health Statistics) 
mortality data scraped from http://wonder.cdc.gov
"""

__author__ = 'garryj'

import re, csv
from glob import glob
import web

from fips import CENSUSSTATES

DATA_DIR='../data'


FILES=[DATA_DIR+'/crawl/mortality/%s.tsv' % x for x in CENSUSSTATES.values()]


def parse_line(x):
    out = web.storage()
    out.county_name = x['County']
    out.state=CENSUSSTATES[x['County Code'][:2]]
    out.county_fips = x['County Code'][2::]
    out.cause_name = x['Cause of death']
    out.cause_code = x['Cause of death Code']
    if 'Suppressed' in x['Count']:
        out.deaths_suppressed = True
    else:
        out.deaths_suppressed = False
        out.deaths = x['Count']
    out.population = x['Population']
    if 'Suppressed'  in x['Crude Rate']:
        out.crude_rate_suppressed = True
    else:
        out.crude_rate_suppressed = False
        crude_rate = re.search(r'(?P<rate>\d*(\.\d*))', x['Crude Rate'])
        if crude_rate:
            out.crude_rate = crude_rate.group('rate')
        unreliable = 'Unreliable' in x['Crude Rate']
        out.crude_rate_reliable = not unreliable
    return out

def parse_file(fn):
    if not glob(fn):
        return 
    parser = csv.DictReader(file(fn), delimiter="\t")
    for x in parser:
        # IgnoreTotal and footer lines
        if x['Notes'] == "Total" or x['Count'] == None:
            continue
        else:
            yield parse_line(x)

if __name__=="__main__":
    import sys
    import tools
    for i in sys.argv[1:]:
        tools.export(parse_file(i))

########NEW FILE########
__FILENAME__ = opensecrets
import glob, re
import web
import xmltramp

CANSUM = '../data/crawl/opensecrets/cansum/%s/%s.xml'

class ParseError(Exception): pass

def parse_can(opensecretsid, year=2008):
    out = web.storage()
    out.opensecretsid = opensecretsid

    d = xmltramp.load(CANSUM % (year, opensecretsid))
    out.total = int(d.candidate.totals('total_receipts'))

    for source in d.candidate.totals.sources:
        if source('type') == 'PAC':
            out.business_pac = 0 # in case it doesn't appear
            for sd in source:
                if sd('type') == "Business":
                    out.business_pac = int(sd('total_receipts'))

    bad = 0
    for sector in d.candidate.totals.sectors:
        if sector('name') not in ['Labor']:
            bad += int(sector('pac'))
    out.badmoney = bad

    return out

r_row = re.compile(r"<tr><td>([A-Za-z ]+)</td><td class='number'>\$([\d,]+)</td><td class='number'>\$([\d,]+)</td><td class='number'>\$([\d,]+)</td></tr>")

def parse_sector():
    fhout = file('sectors.tsv', 'w')
    fhout.write('opensecretsid\tsector\ttotal\tpacs\tindividuals\n')
    for fn in glob.glob('*.html'):
        opensecretsid = fn.split('.')[0]
        fh = file(fn).read()
        for (sector, total, pacs, indivs) in r_row.findall(fh):
            total = total.replace(',', '')
            pacs = pacs.replace(',', '')
            indivs = indivs.replace(',', '')
            fhout.write('\t'.join([opensecretsid, sector, total, pacs, indivs]) + '\n')

    fhout.close()

def parse_all():
    for fn in glob.glob(CANSUM % (2008, '*')):
        opensecretsid = fn.split('/')[-1].split('.')[0]
        try:
            s8 = parse_can(opensecretsid, 2008)
            try:
                s6 = parse_can(opensecretsid, 2006)
            except:
                yield s8
            else:
                s = web.storage()
                s.badmoney = s8.badmoney + s6.badmoney
                s.total = s8.total + s6.total
                s.business_pac = s8.business_pac + s6.business_pac
                yield s
        except:
            print "Could not read", opensecretsid

if __name__ == "__main__":
    import tools
    tools.export(parse_all())

########NEW FILE########
__FILENAME__ = punch
from decimal import Decimal
import re
import web

r_row = re.compile(r'<tr>(.*?)</tr>', re.S)
r_td = re.compile(r'<td v[^>]+>([^<]*)</td>')
r_member = re.compile(r'member=([^"]+)">([^<]+)<')

def fixdec(d):
    d = d.strip()
    return Decimal(d) and Decimal(d)/100

def parse_doc(d):
    for row in r_row.findall(d):
        out = r_td.findall(row)
        if out:
            dist, membername = r_member.findall(row)[0]
            dist = dist.replace('At Large', '00')
            dist = dist[:2] + '-' + dist[2:].zfill(2)
            
            s = web.storage()
            s.district = dist
            s.progressive2008 = fixdec(out[0])
            s.chips2008 = fixdec(out[1])
            s.progressiveall = fixdec(out[3])
            s.name = membername.decode('iso-8859-1')
            
            yield s

def parse_all():
    d = file('../data/crawl/punch/house.html').read()
    for x in parse_doc(d): yield x
    d = file('../data/crawl/punch/senate.html').read()
    for x in parse_doc(d): yield x

if __name__ == "__main__":
    import tools
    tools.export(parse_all())
########NEW FILE########
__FILENAME__ = rvdb
# historical_voting.py - Parse and import historical voting by county
# fors years 1964 - 2004
# Copyright (C) 2008 didier deshommes <dfdeshom@gmail.com>

STATE_CODES = '../data/crawl/manual/rvdb/state_codes'
DATA_PATH = '../data/crawl/manual/rvdb/allYears/'

import glob

def read_state_codes(fname=STATE_CODES):
    """Turn `fname` into a dict."""
    state_codes = {}
    for line in file(fname).readlines():
        line = line.split(' ',1)
        state_codes[line[0]] = line[1].strip().title()
    return state_codes

def parse_historical_voting():
    """
    Parse county-level data. The data is in the format:
    STATE_CODE  COUNTY_NAME DEMOCRAT_COUNT REPUBLICAN_COUNT OTHER_COUNT
    """
    state_codes = read_state_codes()
    files = glob.glob(DATA_PATH + '*')
    
    for fname in files[:-1]: # skip junk file
        for line in file(fname).readlines():
            code, county_name, numbers = line.split('"')
            dem_count, rep_count, other_count = numbers.split()
            state = state_codes[code.strip()]
        
            yield {
              'n_democrats': dem_count,
              'n_republicans': rep_count,
              'n_other': other_count,
              'state_name': state,
              'state_fips': code.strip(),
              'county_name': county_name,
              'year': fname.split('/')[-1]
            }

if __name__ == "__main__":
    import tools
    tools.export(parse_historical_voting())

########NEW FILE########
__FILENAME__ = shapes
"""
parse district shapes

from: data/crawl/census/cd99_110*.dat
"""
import web
import tools

DATA_DIR = '../data/crawl/census'

def parse():
    shapeid2district = {}
    for filename,grp in [('cd99_110',7), ('st99_d00',6)]:
        for lines in web.group(file(DATA_DIR + '/'+filename+'a.dat'), grp):
            if filename[0:2] == 'cd':
                num, fipscode, distnum, distname, distid, distdesc, ignore = \
                        [x.strip().strip('"') for x in lines]
            elif filename[0:2] == 'st':
                num, fipscode, distname, distdesc, ignore, ignore2 = \
                        [x.strip().strip('"') for x in lines]
                distnum = None
            if not fipscode.strip(): continue
            shapeid2district[num] = (fipscode, distnum)
        
        out = {}
        for line in file(DATA_DIR + '/'+filename+'.dat'):
            nums = line.strip().split()
            if len(nums) == 3:
                shapeid = nums[0] # other points are the center
                if shapeid in shapeid2district:
                    SKIPME = False
                    district = shapeid2district[shapeid]
                    out.setdefault(district, [])
                    out[district].append([])
                else:
                    SKIPME = True
            elif len(nums) == 2 and not SKIPME:
                out[district][-1].append((float(nums[0]), float(nums[1])))
        
        for (fipscode, distnum), shapes in out.iteritems():
            yield {
              '_type': 'district', 
              'state_fipscode': fipscode, 
              'district': distnum,
              'shapes': shapes
            }

if __name__ == "__main__": tools.export(parse())

########NEW FILE########
__FILENAME__ = soi
"""
Parse IRS SOI statistics.
"""

import web
import xls2list

SOI_PATH = "../data/crawl/irs/soi/2005/ZIP Code 2005 %s.xls"

class MissingData(Exception): pass
def gini_est(data):
    """
    Estimates upper and lower bounds for Gini coefficient based on
    equations 1.1 thru 1.3 from Mehran 1975 as summarized in
    Gastwirth 1972 at <http://www.jstor.org/stable/2285377?seq=1>.
    
    `data` is a list of dictionaries with the keys
    `lower_bound`, `n_filers`, and `adjusted_gross_income`.
    The first dictionary should have `lower_bound = None`
    and contain the totals for the set.
    """
    def fraction(i, col):
        out = 0.
        for n, x in enumerate(data):
            if x[col] is None:
                raise MissingData, col
            if n == 0:
                total = x[col]
            else:
                out += x[col]
            if n == i: break
        return out/total

    people_f = lambda i: fraction(i, 'n_filers')
    income_f = lambda i: fraction(i, 'agi')

    def mean_r(i):
        if data[i].agi == 0: return 0
        return float(data[i].agi)/data[i].n_filers

    def topof(i):
        if i < len(data)-1:
            return data[i+1].bracket_low
        else:
            return data[-1].agi

    ksum = 0
    for i in range(1, len(data)):
    	ksum += (people_f(i) - people_f(i-1)) * \
    	        (income_f(i) + income_f(i-1))
    lower_bound = 1 - ksum

    dsum = 0
    for i in range(1, len(data)):
    	dsum += (people_f(i) - people_f(i-1))**2 * \
    	        (topof(i) - mean_r(i)) * \
    	        (mean_r(i) - topof(i-1)) * \
    	        (topof(i) - topof(i-1))**-1
    grouping = mean_r(0)**-1 * dsum
    upper_bound = lower_bound + grouping

    return lower_bound, upper_bound

def parse_state(state):
    def fixnum(x, multiply=1):
        if isinstance(x, unicode) and '*' in x:
            return None
        else:
            return x * multiply
    
    stats = xls2list.xls2list(SOI_PATH % state)
    
    loc = 11 # rest is all headers
    while loc+7 < len(stats):
        out = web.storage()
        bundle = stats[loc:loc+7]
        if bundle[0][0] == None:
            break

        out.loc = bundle[0][0]
        if isinstance(out.loc, float):
            out.loc = str(int(out.loc)).zfill(5)
        
        if out.loc.strip() == "MISSOURI":
            loc += 8 # duped data
            continue

        out.brackets = []
        
        for line in bundle:
            if (isinstance(line[0], unicode) and line[0].strip() == 'Total'
               ) or isinstance(line[0], float):
                line[0] = None
            elif line[0].strip() == "Under $10,000":
                line[0] = 0
            else:
                line[0] = int(''.join([x for x in line[0].split()[0] if x.isdigit()]))
            out.brackets.append(web.storage(
              bracket_low=line[0], 
              n_filers=fixnum(line[1]), 
              agi=fixnum(line[4], 1000),
              tot_tax=fixnum(line[35], 1000),
              n_dependents=fixnum(line[3]),
              n_eitc=fixnum(line[36]),
              tot_eitc=fixnum(line[37], 1000),
              tot_charity=fixnum(line[26], 1000),
              n_prepared=fixnum(line[38])
            ))
            
            br = out.brackets[-1]            
            err = (TypeError, ZeroDivisionError)
            
            try: br.pct_prepared = float(br.n_prepared)/br.n_filers
            except err: pass

            try: br.pct_charity = float(br.tot_charity)/br.agi
            except err: pass

            try:
                br.avg_eitc = float(br.tot_eitc)/br.n_eitc
            except TypeError:
                pass
            except ZeroDivisionError:
                br.avg_eitc = 0

            try: br.pct_eitc = float(br.n_eitc)/br.n_filers
            except err: pass

            try: br.avg_dependents = float(br.n_dependents)/br.n_filers
            except err: pass

            try: br.avg_taxburden = float(br.tot_tax)/br.agi
            except err: pass

            try: br.avg_income = float(br.agi)/br.n_filers
            except err: pass
                
        try: out.gini = gini_est(out.brackets)
        except MissingData: pass
        
        yield out
        
        loc += 8

def parse_soi(verbose=False):
    import sys

    states = ['AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 
    'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME', 'MI', 
    'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', 'NY', 
    'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VA', 'VT', 
    'WA', 'WI', 'WV', 'WY']

    if verbose: print>>sys.stderr
    for state in states:
        if verbose: print>>sys.stderr, "\rParsing", state + '...',
        for x in parse_state(state):
            if x.loc.strip() == 'Total':
                x.loc = state
            yield x
    if verbose: print >>sys.stderr, '\r                     '

if __name__ == "__main__":
    import tools
    tools.export(parse_soi(verbose=True))

########NEW FILE########
__FILENAME__ = tools
#!/usr/bin/env python
"""
common tools for parsers
"""

import sys
import simplejson as json

def netstring(x):
    """
        >>> netstring('banana')
        '6:banana,'
    """
    return str(len(x)) + ':' + x + ','

def jsonify(d):
    return json.dumps(d, indent=2, sort_keys=True)

def export(generator):
    for item in generator:
        sys.stdout.write(netstring(jsonify(item)))

def unexport(fh):
    n = '0'
    while n:
        n = int(n + ''.join(c for c in iter(lambda: fh.read(1), ':')))
        yield json.loads(fh.read(n))
        assert fh.read(1) == ','
        n = fh.read(1)

if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = usps
"""
Parser for USPS AIS records.
"""

from fixed_width import date, year, string, boolean, filler, enum, integer, \
  FIELD_KEY, FIELD_LEN, FIELD_TYP, get_len, parse_line, parse_file

## Types used in definitions

def halfbool1(s):
    return dict(A=True, B=True, C=False, D=False)

def halfbool2(s):
    return dict(A=True, B=False, C=True, D=False)

oddeven = enum(O='ODD', E='EVEN', B='BOTH')

## The definitions

def def_copyright(n):
    return [
      ('_type', 1, lambda s: 'File Header'),
      (None, 5, filler),
      ('copyright_statement', 12, string),
      (None, 1, filler),
      ('month', 2, integer),
      (None, 1, filler),
      ('year', 2, year),
      (None, 1, filler),
      ('copyright_owner', 4, string),
      (None, 1, filler),
      ('volume_seq', 3, integer),
      (None, n, filler)
    ]

def_ctystate = {
  'C': def_copyright(96),
  'S': [
    ('_type', 1, lambda s: 'ZIP Scheme Combination'),
    ('label_zip', 5, string),
    ('combined_zip', 5, string),
    (None, 118, filler)
  ],
  'A': [
    ('_type', 1, lambda s: 'Street Alias'),
    ('zip', 5, string),
    ('alias_street_pre_direction', 2, string),
    ('alias_street_name', 28, string),
    ('alias_street_suffix', 4, string),
    ('alias_street_post_suffix', 2, string),
    ('street_pre_direction', 2, string),
    ('street_name', 28, string),
    ('street_suffix', 4, string),
    ('street_post_suffix', 2, string),
    ('alias_type', 1, enum(
      A='ABBREVIATED', 
      C='STREET NAME CHANGED', 
      O='NICKNAME/OTHER', 
      P='PREFERRED STREET NAME'
    )),
    ('alias_date', 8, date),
    ('delivery_low', 10, string),
    ('delivery_high', 10, string),
    ('odd_even', 1, oddeven),
    (None, 21, filler)
  ],
  'Z': [
    ('_type', 1, lambda s: 'Zone Split'),
    ('old_zip', 5, string),
    ('old_route', 4, string),
    ('new_zip', 5, string),
    ('new_route', 4, string),
    ('date', 8, date),
    (None, 102, filler)
  ],
  'D': [
    ('_type', 1, lambda s: 'ZIP City State'),
    ('zip', 5, string),
    ('city_state_key', 6, string),
    ('zip_class_code', 1, enum(**{
      ' ': 'NON-UNIQUE',
      'M': 'AFO/FPO/DPO MILITARY',
      'P': 'PO BOX ZIP',
      'U': 'UNIQUE ZIP'
    })),
    ('city_state_name', 28, string),
    ('city_state_abbrev', 13, string),
    ('facility_code', 1, enum(
      B='BRANCH',
      C='COMMUNITY POST OFFICE',
      N='NON-POSTAL COMMUNITY NAME, FORMER POSTAL FACILITY, OR PLACE NAME',
      P='POST OFFICE',
      S='STATION',
      U='URBANIZATION'
    )),
    ('mailing_name', 1, boolean),
    ('preferred_last_line_key', 6, string),
    ('preferred_last_line_name', 28, string),
    ('city_delivery', 1, boolean),
    ('carrier_route_sort_rate', 1, halfbool1),
    ('merging_permitted', -1, halfbool2),
    ('unique_zip_name', 1, boolean),
    ('finance_no', 6, string),
    ('state_abbrev', 2, string),
    ('county_no', 3, string),
    ('county_name', 25, string)
  ],
  'N': [
    ('_type', 1, lambda s: 'Seasonal'),
    ('zip', 5, string),
    ('jan', 1, boolean),
    ('feb', 1, boolean),
    ('mar', 1, boolean),
    ('apr', 1, boolean),
    ('may', 1, boolean),
    ('jun', 1, boolean),
    ('jul', 1, boolean),
    ('aug', 1, boolean),
    ('sep', 1, boolean),
    ('oct', 1, boolean),
    ('nov', 1, boolean),
    ('dec', 1, boolean),
    (None, 111, filler)
  ]
}

def_5digit = {
  'C': def_copyright(62),
  'D': [
    ('_type', 1, lambda s: "Five-Digit ZIP Detail"),
    ('zip', 5, string),
    ('update_key', 10, string),
    ('action_code', 1, enum(A="ADD", D="DELETE")),
    ('record_type', 1, enum(
      G='GENERAL DELIVERY',
      P='PO BOX',
      R='RURAL ROUTE/HIGHWAY CONTRACT',
      S='STREET'
    )),
    ('street_pre_dir', 2, string),
    ('street_name', 28, string),
    ('street_suffix', 4, string),
    ('street_post_dir', 2, string),
    ('addr_primary_lo', 10, string),
    ('addr_primary_hi', 10, string),
    ('addr_primary_odd_even', 1, oddeven),
    ('finance_no', 6, string),
    ('state_abbrev', 2, string),
    ('urbanization_ctyst_key', 6, string),
    ('prefd_lastline_ctyst_key', 6, string)
  ]
}

def_zip4 = {
  'C': def_copyright(149),
  'D': [
    ('_type', 1, lambda s: "ZIP+4 Detail"),
    ('zip', 5, string),
    ('update_key', 10, integer), #?
    ('action', 1, enum(A='ADD', D='DELETE')),
    ('record_type', 1, enum(
      F='FIRM',
      G='GENERAL DELIVERY',
      H='HIGH-RISE',
      P='PO BOX',
      R='RURAL ROUTE/HIGHWAY CONTRACT',
      S='STREET'
    )),
    ('carrier_route_id', 4, string),
    ('street_pre_dir', 2, string),
    ('street_name', 28, string),
    ('street_suffix', 4, string),
    ('street_post_dir', 2, string),
    ('addr_primary_lo', 10, string),
    ('addr_primary_hi', 10, string),
    ('addr_primary_odd_even', 1, oddeven),
    ('building_or_firm_name', 40, string),
    ('addr_secondary_abbrev', 4, string),
    ('addr_secondary_low', 8, string),
    ('addr_secondary_high', 8, string),
    ('addr_secondary_odd_even', 1, oddeven),
    ('zip4_lo', 4, string),
    ('zip4_hi', 4, string),
    ('base_alt', 1, enum(B='BASE', A='ALTERNATIVE')),
    ('lacs_status', 1, enum(L='LACS CONVERTED')),
    ('govt_bldg', 1, enum(
      A='CITY GOV BLDG',
      B='FEDERAL GOV BLDG',
      C='STATE GOV BLDG',
      D='FIRM ONLY',
      E='CITY GOV BLDG AND FIRM ONLY',
      F='FED GOV BLDG',
      G='STATE GOV BLDG AND FIRM ONLY'
    )),
    ('finance_no', 6, string),
    ('state_abbrev', 2, string),
    ('county_no', 3, string),
    ('congress_dist', 2, string),
    ('municipality_ctyst_key', 6, string),
    ('urbanization_ctyst_key', 6, string),
    ('prefd_lastline_ctyst_key', 6, string)
  ]
}

def_delstat = {
  'C': def_copyright(276),
  'D': [
    ('_type', 1, lambda s: "Delivery Statistics"),
    ('zip', 5, string),
    ('update_id', 10, string),
    ('action', 1, enum(A="ADD", D="DELETE")),
    ('carrier_route_id', 4, string),
    ('active_business_centralized', 5, integer),
    ('active_business_curb', 5, integer),
    ('active_business_ndcbu', 5, integer),
    ('active_business_other', 5, integer),
    ('active_business_facility_box', 5, integer),
    ('active_business_contract_box', 5, integer),
    ('active_business_detached_box', 5, integer),
    ('active_business_npu', 5, integer),
    ('active_business_caller_service_box', 5, integer),
    ('active_business_remittance_box', 5, integer),
    ('active_business_contest_box', 5, integer),
    ('active_business_other_box', 5, integer),
    ('active_residential_centralized', 5, integer),
    ('active_residential_curb', 5, integer),
    ('active_residential_ndcbu', 5, integer),
    ('active_residential_other', 5, integer),
    ('active_residential_facility_box', 5, integer),
    ('active_residential_contract_box', 5, integer),
    ('active_residential_detached_box', 5, integer),
    ('active_residential_npu', 5, integer),
    ('active_residential_caller_service_box', 5, integer),
    ('active_residential_remittance_box', 5, integer),
    ('active_residential_contest_box', 5, integer),
    ('active_residential_other_box', 5, integer),
    ('active_general', 5, integer),
    ('possible_business_centralized', 5, integer),
    ('possible_business_curb', 5, integer),
    ('possible_business_ndcbu', 5, integer),
    ('possible_business_other', 5, integer),
    ('possible_business_facility_box', 5, integer),
    ('possible_business_contract_box', 5, integer),
    ('possible_business_detached_box', 5, integer),
    ('possible_business_npu', 5, integer),
    ('possible_business_caller_service_box', 5, integer),
    ('possible_business_remittance_box', 5, integer),
    ('possible_business_contest_box', 5, integer),
    ('possible_business_other_box', 5, integer),
    ('possible_residential_centralized', 5, integer),
    ('possible_residential_curb', 5, integer),
    ('possible_residential_ndcbu', 5, integer),
    ('possible_residential_other', 5, integer),
    ('possible_residential_facility_box', 5, integer),
    ('possible_residential_contract_box', 5, integer),
    ('possible_residential_detached_box', 5, integer),
    ('possible_residential_npu', 5, integer),
    ('possible_residential_caller_service_box', 5, integer),
    ('possible_residential_remittance_box', 5, integer),
    ('possible_residential_contest_box', 5, integer),
    ('possible_residential_other_box', 5, integer),
    ('possible_general', 5, integer),
    ('drop_business_and_families_served', 5, integer),
    ('active_business_residential_mixed', 5, integer),
    ('active_residential_business_mixed', 5, integer),
    ('finance_no', 6, string),
    ('state', 2, string),
    ('county_code', 3, string),
    ('city_state_key', 6, string),
    ('preferred_last_line_key', 6, string)
  ]
}

def_tigerdat = [
  ('_type', 0, lambda s: "Census/USPS County Map"),
  ('state_code', 2, string),
  ('state_abbrev', 2, string),
  ('county_code', 3, string),
  ('county_name', 25, string),
  (None, 2, filler)
]

def_tigerzip = {
  'C': [
    ('_type', 0, lambda s: 'File Header'),
    (None, 5, filler),
    ('year', 4, integer),
    ('month', 2, integer),
    ('copyright_statement', 16, string),
    (None, 59, filler), # not really there, it seems
    (None, 2, filler) # CRLF
  ],
  'D': [
    ('_type', 0, lambda s: 'TIGER/ZIP Data'),
    ('zip', 5, string),
    ('zip4', 4, string),
    ('tlid', 10, string),
    ('carrier_route', 4, string),
    ('state_code', 2, string),
    ('county_code', 3, string),
    ('right_left', 1, enum(R='RIGHT', L='LEFT', B='BOTH')),
    ('census_tract', 6, string),
    ('census_block', 4, string),
    ('from_lat', 9, string),
    ('from_long', 10, string),
    ('to_lat', 9, string),
    ('to_long', 10, string),
    ('pmsa', 4, string),
    ('cmsa', 4, string),
    ('multimatch', 1, boolean),
    (None, 2, filler)
  ]
}

## The functions you might want to call

def parse_tigerzip(fh):
    linelen = get_len(def_tigerzip)
    for line in fh:
        if line.startswith(' ' * 5):
            t = 'C'
        else:
            t = 'D'

        yield parse_line(def_tigerzip[t], line)

def parse_tigerdat(fh):
    for line in fh:
        yield parse_line(def_tigerdat, line)

def parse_zip2dist(fh):
    for row in parse_file(def_zip4, fh):
        if row['_type'] != 'ZIP+4 Detail': continue
        if row['congress_dist'] == 'AL':
            row['congress_dist'] = '00'
        if row['zip4_lo'] == row['zip4_hi']:
            zip4s = [row['zip4_lo']]
        else:
            zip4s = [str(x).zfill(4) for x in xrange(int(row['zip4_lo']), int(row['zip4_hi']) + 1)]
        for zip4 in zip4s:
            yield row['zip'] + '-' + zip4, row['state_abbrev'] + '-' + row['congress_dist']

if __name__ == "__main__":
    import sys, glob, tools
    
    def_map = {
      '--ctystate': def_ctystate, 
      '--5digit': def_5digit, 
      '--zip4': def_zip4, 
      '--delstat': def_delstat
    }
    
    if sys.argv[1] in def_map:
        for fn in glob.glob(sys.argv[2] + '*.txt'):
            tools.export(parse_file(def_map[sys.argv[1]], file(fn)))
    elif sys.argv[1] == '--tiger':
        for fn in glob.glob(sys.argv[2] + '*/*.txt'):
            tools.export(parse_tigerzip(file(fn)))
    elif sys.argv[1] == '--tigerdat':
        for fn in glob.glob(sys.argv[2] + '*/TIGER.DAT'):
            tools.export(parse_tigerdat(file(fn)))

########NEW FILE########
__FILENAME__ = votesmart
import simplejson as json

DATA_DIR='../data/crawl/votesmart/111'

def items(fname):
    print 'loading', fname
    return json.load(file(DATA_DIR + '/%s.json' % fname)).iteritems()

def candidates():
    return items('candidates')

def bios():
    return items('bios')

def websites():
    return items('websites')

def districts():
    return items('districts')

########NEW FILE########
__FILENAME__ = voteview
"""
parse voteview partisanship data
"""

HOUSE_DAT = "../data/crawl/voteview/HL01110C21_PRES_BSSE.DAT"
SENATE_DAT = "../data/crawl/voteview/SL01110C21_BSSE.dat"

state_map = { #@@ import to state json as icpsr
  41: 'AL', 81: 'AK', 61: 'AZ', 42: 'AR', 71: 'CA', 62: 'CO', 1: 'CT',
  11: 'DE', 43: 'FL', 44: 'GA', 82: 'HI', 63: 'ID', 21: 'IL', 22: 'IN',
  31: 'IA', 32: 'KS', 51: 'KY', 45: 'LA', 2: 'ME', 52: 'MD', 3: 'MA',
  23: 'MI', 33: 'MN', 46: 'MS', 34: 'MO', 64: 'MT', 35: 'NE', 65: 'NV',
   4: 'NH', 12: 'NJ', 66: 'NM', 13: 'NY', 47: 'NC', 36: 'ND', 24: 'OH',
  53: 'OK', 72: 'OR', 14: 'PA',  5: 'RI', 48: 'SC', 37: 'SD', 54: 'TN',
  49: 'TX', 67: 'UT', 6: 'VT', 40: 'VA', 73: 'WA', 56: 'WV', 25: 'WI',
  68: 'WY', 55: 'DC'
}

import web
import tools

def parse():
   for fn in [HOUSE_DAT, SENATE_DAT]:
       for line in file(fn):
           out = web.storage()
           out.congress = int(line[0:4])
           out.icpsr_id = int(line[4:10])
           out.icpsr_state = int(line[10:13])
           out.district = int(line[13:15])        
           out.state_name = line[15:23].strip()
           out.party_code = int(line[23:28])
           out.last_name = line[28:41].strip()
           out.dim1 = float(line[41:47])
           out.dim2 = float(line[47:54])
           out.std1 = float(line[54:61])
           out.std2 = float(line[61:68])
           out.corr = float(line[68:75])
           out.loglike = float(line[75:87])
           out.n_votes = int(line[87:92])
           out.n_errs = int(line[92:97])
           out.n_geomeanprob = float(line[97:104])
           
           if out.icpsr_state in state_map:
               out.state_code = state_map[out.icpsr_state]
               if out.district:
                   out.district_id = out.state_code + '-' + str(out.district).zfill(2)
               else:
                   out.district_id = out.state_code 
           
           yield out

if __name__ == "__main__":
    tools.export(parse())

########NEW FILE########
__FILENAME__ = xls2list
# MS Excel to Python list of list converter 
# 
# Provides the functions: 
# xls2list(fname, worksheet=1,encoding='cp1251') 
#  
# requires: 
# pyExcelerator 0.6.3a or later  
#  
 
from pyExcelerator import * 
from datetime import datetime 
 
def xls2list(fname, worksheet=1,encoding='cp1251'): 
    """  
    Return a Python list of lists that contains all cell-values  
    in the specified worksheet (default is the first)  
    of Excel spreadsheet with name=fname. 
     
    worksheet: an integer; worksheet=1 corresponds to the first  
    worksheet, etc. 
    encoding: use this unicode encoding, default is 'cp1251' 
     
    Interior blank cells are assigned the value None. 
     
    Example:  
    If the file "sales_beer.xls" consists of one worksheet with cells 
     
    "Year" "Sales" "Profit" 
    2001 1,000 300  
    2002 1,501 410 
    2003 1,900 520 
     
    Then the Python statement 
    sales=xls_to_list("sales_beer.xls) 
    will return the Python list object 
    sales=[["Year","Sales","Profit"], 
    [ 2001, 1000, 300], 
    [ 2002, 1501, 410], 
    [ 2003, 1900, 520]] 
     
    We assume a rectangular layout, and replace blank cells in the 
    spreadsheet with None values in the list.  
    The array dimensions is determined by the non-blank cells.  
     
    For example,  
    If the file "sales_cheap_beer.xls" contains one worksheet with cells 
     
    "Year" "Sales" "Profit" 
    2001 1,000 30  
    2002 1,501 41 
    2003 1,900 -52 blank "WHAT! NO PROFIT!!" 
     
    Then the Python statement 
    sales=xls_to_list("sales_cheap_beer.xls) 
    will return the Python rectangular list-of-lists object 
    sales=[["Year","Sales","Profit", None, None], 
    [ 2001, 1000, 30, None, None], 
    [ 2002, 1501, 41, None, None], 
    [ 2003, 1900, -52, None, "WHAT! NO PROFIT!!"]] 
     
     
    """ 
    data = parse_xls(fname,encoding) 
    # parse_xls returns a list of 2-tuples. One tuple for each worksheet. 
    # each tuple = (worksheet_name, worksheet_data) 
    # worksheet_data is a dict mapping each non-blank cell to its cell value. 
    # Each cell has a key of the form 
    # (i,j) with i=0,1,...,no_columns, j=0,1,...,no_rows 
    # to the associated cell entry. Blank cells are omitted from 
    # the dict. 
    sheet_name=data[worksheet-1][0]
    values = data[worksheet-1][1] 
     
    # first stuff all the nonempty cells into a dict keyed by tuples 
    # e.g. vdict[(0,0)] holds the contents of the top-left cell (if non-empty) 
    # else, (0,0) key will not exist in vdict. 
    # and vdict[(0,3)] corresponds to cell D1  
    # 0 -> row 1 (Excel row 1) and 3 -> column 4 (Excel column D) 
    vdict = {} 
    row_idx_max = 0 
    col_idx_max = 0 
    for row_idx, col_idx in sorted(values.keys()): 
        row_idx_max = max(row_idx,row_idx_max) 
        col_idx_max = max(col_idx,col_idx_max) 
        v = values[(row_idx, col_idx)] 
        vdict[(row_idx,col_idx)] = v 
     
    # Convert vdict (dict of cell values) to vlist (a 2D list). 
    # Blank cells are indicated by None. 
    vlist = [] 
    for row in range(row_idx_max+1): 
        vlist.append([]) 
        for col in range(col_idx_max+1): 
            if (row,col) not in vdict: 
                vdict[(row,col)]=None 
            vlist[row].append(vdict[(row,col)]) 
    return vlist 


########NEW FILE########
__FILENAME__ = petition

from __future__ import with_statement

import web
from utils import forms, helpers, auth, wyrapp
from settings import db, render, render_plain
from utils.auth import require_login
from utils.users import fill_user_details, update_user_details
from utils.writerep import require_captcha, send_msgs
import config
import simplejson

from datetime import datetime
import urllib

urls = (
  '', 'redir',
  '/', 'index',
  '/new', 'new',
  '/login', 'login',
  '/signup', 'signup',
  '/verify', 'checkID',
  '/(.*)/signatories', 'signatories',
  '/(.*)', 'petition'
)

class redir:
    def GET(self): raise web.seeother('/')

class checkID:
    def POST(self):
        "Return True if petition with id `pid` does not exist"
        pid = web.input().pid
        exists = bool(db.select('petition', where='id=$pid', vars=locals()))
        return pid != 'new' and not(exists)

class index:
    def index(self):
        pids = db.select('petition', what='id', where='petition.deleted is null and petition.published is not null')
        return (('/c/%s' % p.id,  '/c/%s/signatories' % p.id) for p in pids)

    def GET(self):
        petitions = db.select(['petition', 'signatory'],
                    what='petition.id, petition.title, count(signatory.user_id) as signature_count',
                    where='petition.deleted is null and petition.published is not null '
                            'and petition.id = signatory.petition_id  and signatory.deleted is null',
                    group='petition.id, petition.title',
                    order='count(signatory.user_id) desc'
                    )

        msg, msg_type = helpers.get_delete_msg()
        return render.petition_list(petitions, msg)

def send_to_congress(uid, i, signid):
    #@@@ compose here too
    env = simplejson.loads(i.get('captcha_env', '{}'))
    i.msg = i.msg + '\n' + i.get('comment', '')
    send_msgs(uid, i, source_id='s%s' % signid, env=env)

def create_petition(i, email):
    tocongress = i.get('tocongress', 'off') == 'on'
    i.pid = i.pid.replace(' ', '-')
    u = helpers.get_user_by_email(email)
    is_draft = 'save' in i
    published = None if is_draft else datetime.now()
    try:
        db.insert('petition', seqname=False, id=i.pid, title=i.ptitle, 
                    created=datetime.now(), published=published,
                    description=i.msg, owner_id=u.id, to_congress=tocongress)
    except: return
    
    if is_draft:
        msg = """Petition saved for publishing later."""
        helpers.set_msg(msg)
    else:
        create_first_signature(i, u.email)

def create_first_signature(i, email):
    tocongress = i.get('tocongress', 'off') == 'on'
    i.pid = i.pid.replace(' ', '-')
    u = helpers.get_user_by_email(email)    
    signid = save_signature(i, i.pid, u.id)
    if tocongress: send_to_congress(u.id, i, signid)
    sendmail_to_signatory(u, i.pid)
    msg = """Congratulations, you've created your petition.
             Now share it with all your friends."""
    helpers.set_msg(msg)
    
class new:
    def GET(self, pf=None, wf=None):
        pf = pf or forms.petitionform()
        if not wf:
            #create a new form and initialize with current user details
            wf = forms.wyrform()
            u = helpers.get_user()
            u and fill_user_details(wf, u)
        captcha_html = wyrapp.prepare_for_captcha(wf)
        msg, msg_type = helpers.get_delete_msg()
        return render.petitionform(pf, wf, captchas=captcha_html, msg=msg)

    def POST(self):
        i = web.input()
        tocongress = i.get('tocongress', 'off') == 'on'
        pf, wf = forms.petitionform(), forms.wyrform()
        i.email = 'one@valid.mail' # to make wf valid, find a better work around
        wyr_valid = (not(tocongress) or wf.validates(i))
        captcha_needed = require_captcha(i)
        wyr_valid = wyr_valid and not captcha_needed

        if not pf.validates(i) or not wyr_valid:
            if captcha_needed: wf.valid, wf.note = False, 'Please fill the captcha below'
            pf.fill(i), wf.fill(i)
            return self.GET(pf, wf)

        email = helpers.get_loggedin_email()
        if not email:
            return login().GET(i)

        create_petition(i, email)
        raise web.seeother('/%s' % i.pid)

class login:
    def GET(self, i=None, wf=None):
        i = i or web.input()
        lf, sf = forms.loginform(), forms.signupform()
        pf, wf = forms.petitionform(), (wf or forms.wyrform())
        pf.fill(i), wf.fill(i)
        is_draft = 'save' in i
        return render.petitionlogin(lf, sf, pf, wf, is_draft=is_draft)

    def POST(self):
        i = web.input()
        lf, wf =  forms.loginform(), forms.wyrform()
        if not lf.validates(i):
            pf, sf = forms.petitionform(), forms.signupform()
            lf.fill(i), pf.fill(i), wf.fill(i)
            is_draft = 'save' in i
            return render.petitionlogin(lf, sf, pf, wf, is_draft=is_draft)
        create_petition(i, i.useremail)
        raise web.seeother('/%s' % i.pid)

class signup:
    def POST(self):
        i = web.input()
        sf, wf = forms.signupform(), forms.wyrform()
        if not sf.validates(i):
            lf, pf = forms.loginform(), forms.petitionform()
            sf.fill(i), pf.fill(i), wf.fill(i)
            return render.petitionlogin(lf, sf, pf, wf)
        user = auth.new_user(i.email, i.password)
        helpers.set_login_cookie(i.email)
        create_petition(i, i.email)    
        raise web.seeother('/%s' % i.pid)

def save_signature(i, pid, uid):
    where = 'petition_id=$pid AND user_id=$uid'
    signed = db.select('signatory', where=where, vars=locals())
    share_with = (i.get('share_with', 'off') == 'on' and 'N') or 'A'
    update_user_details(i)
    if not signed:
        referrer = get_referrer(pid, uid)
        signid = db.insert('signatory', user_id=uid, share_with=share_with,
                petition_id=pid, comment=i.get('comment'), referrer=referrer)
        helpers.set_msg("Thanks for your signing! Why don't you tell your friends about it now?")
        return signid
    else:
        db.update('signatory', where='user_id=$uid and petition_id=$pid', 
                    comment=i.get('comment'), deleted=None, vars=locals())
        helpers.set_msg("Your signature has been changed. Why don't you tell your friends about it now?")
        return 'old_%s' % signed[0].id
  
def sendmail_to_signatory(user, pid):
    """sends a thanks mail to the user, with request to share the petition with friends.
    """
    p = get_petition_by_id(pid)
    p.url = 'http://watchdog.net/c/%s' % (pid)
    token = auth.get_secret_token(user.email)
    msg = render_plain.signatory_mailer(user, p, token)
    web.sendmail(config.from_address, user.email, msg.subject.strip(), str(msg))

def is_author(email, pid):
    user = email and helpers.get_user_by_email(email)
    where = 'id=$pid and owner_id=$user.id and deleted is null'
    return user and bool(db.select('petition', where=where, vars=locals()))

def is_signatory(email, pid):
    user = email and helpers.get_user_by_email(email)
    where = 'petition_id=$pid and user_id=$user.id and deleted is null'
    return user and bool(db.select('signatory', where=where, vars=locals()))
    
def get_signs(pid):
    where = "petition_id=$pid AND users.id=user_id AND deleted is null" 
    return db.select(['signatory', 'users'],
                        what='users.fname, users.lname, users.email, '
                              'signatory.share_with, signatory.comment, '
                              'signatory.signed',
                        where=where,
                        order='signed desc',
                        vars=locals())

def to_congress(pid):
    return bool(db.select("petition", where="id=$pid AND to_congress='t'", vars=locals()))

def get_num_signs(pid):
    where = "petition_id=$pid AND deleted is null"
    r = db.query("select count(*) from signatory where " + where, vars=locals())
    return r[0].count
                       
def get_petition_by_id(pid):
    try:
        return db.select('petition', where='id=$pid and deleted is null', vars=locals())[0]
    except IndexError:
        return                            

class signatories:
    def GET(self, pid):
        user_email = helpers.get_loggedin_email()
        p = get_petition_by_id(pid)
        if not p: raise web.notfound()
        ptitle = p.title
        signs = get_signs(pid).list()
        return render.signature_list(pid, ptitle, signs, is_author(user_email, pid))
                        
def set_referrer_cookie(tid, pid):
    if helpers.check_trackid(tid, pid):
        helpers.setcookie('tid', tid)

def get_referrer(pid, uid):
    tid = helpers.getcookie('tid')
    referrer = helpers.check_trackid(tid, pid)
    if referrer != uid:
        return referrer

def is_draft(p):
    return not bool(p.published)

class petition:
    def GET(self, pid, sf=None, wf=None):
        i = web.input()
        pid = pid.rstrip('/')
        p = get_petition_by_id(pid)
        if not p: raise web.notfound()
        
        options = ['unsign', 'edit', 'delete']
        if i.get('m', None) in options:
            handler = getattr(self, 'GET_'+i.m)
            return handler(pid)

        if not sf:
            sf = forms.signform()
            fill_user_details(sf)

        captcha_html = ''
        if to_congress(pid):
            if not wf:
                wf = forms.wyrform()
                fill_user_details(wf)
            captcha_html = wyrapp.prepare_for_captcha(wf)    

        if 'tid' in i: 
            set_referrer_cookie(i.tid, pid)
            raise web.seeother('/%s' % pid)
        
        u = web.storage()    
        u.email = helpers.get_loggedin_email() or helpers.get_unverified_email()
        u.isauthor = is_author(u.email, pid)
        u.issignatory = is_signatory(u.email, pid)
        p.isdraft = is_draft(p)
        p.signatory_count = get_num_signs(pid)
        msg, msg_type = helpers.get_delete_msg()
        return render.petition(p, u, sf, wf, captcha_html, msg)

    @auth.require_login
    def GET_edit(self, pid):
        user_email = helpers.get_loggedin_email()
        if is_author(user_email, pid):
            p = get_petition_by_id(pid)
            u = helpers.get_user_by_email(user_email)
            pf = forms.petitionform()
            pf.fill(userid=u.id, email=user_email, pid=p.id, ptitle=p.title, msg=p.description, tocongress=p.to_congress)
            wf = forms.wyrform()
            fill_user_details(wf)
            isdraft = is_draft(p)
            return render.petitionform(pf, wf, is_new=False, is_draft=isdraft)
        elif user_email:
            msg = "You don't have permissions to edit this petition."
        else:    
            login_link = '<a href="/u/login">Login</a>'
            msg = 'Only author of this petition can edit it. %s if you are.' % login_link
        helpers.set_msg(msg)
        raise web.seeother('/%s' % pid)


    def GET_unsign(self, pid):
        i = web.input()
        user = helpers.get_user_by_email(i.email)

        if user:
            where = 'petition_id=$pid and user_id=$user.id and deleted is null'
            signatory = db.select('signatory', where=where, vars=locals())

        valid_token = auth.check_secret_token(i.get('email', ''), i.get('token', '@'))
        if not (user and signatory and valid_token):
            msg = "Invalid token or there is no signature for this petition with this email."
            msg_type = 'error'
        else:
            msg = str(render_plain.confirm_unsign(pid, user.id))
            msg_type = ''

        helpers.set_msg(msg, msg_type)
        raise web.seeother('/%s' % pid)

    def GET_delete(self, pid):
        user_email = helpers.get_loggedin_email()
        if is_author(user_email, pid):
            msg = str(render_plain.confirm_deletion(pid))
        elif user_email:
            msg = "You don't have permissions to delete this petition."
        else:    
            login_link = '<a href="/u/login">Login</a>'
            msg = 'Only author of this petition can delete it. %s if you are.' % login_link
        helpers.set_msg(msg)
        raise web.seeother('/%s' % pid)

    def POST(self, pid):
        i = web.input('m', _method='GET')
        options = ['sign', 'unsign', 'edit', 'delete']
        if i.m in options:
            handler = getattr(self, 'POST_'+i.m)
            return handler(pid)
        else:
            raise ValueError

    def POST_sign(self, pid):
        i = web.input()
        sf = forms.signform()
        tocongress = to_congress(pid)
        p = get_petition_by_id(pid)
        
        is_new = lambda sid: not isinstance(sid, str)
        get_new = lambda sid: int(web.lstrips(sid, 'old_'))
        if tocongress:
            i.pid, i.ptitle, i.msg = pid, p.title, p.description
            wf = forms.wyrform()
            captcha_needed = require_captcha(i)
            wyr_valid =  wf.validates(i) and not captcha_needed
            if captcha_needed: wf.valid, wf.note = False, 'Please fill the captcha below'
        else:
            wf, wyr_valid = None, True

        if sf.validates(i) and wyr_valid:
            uid = auth.assert_login(i)
            signid = save_signature(i, pid, uid)
            if is_new(signid):
                user = helpers.get_user_by_id(uid)
                sendmail_to_signatory(user, pid)
            else:
                signid = get_new(signid)
            if tocongress: send_to_congress(uid, i, signid)
            query = urllib.urlencode(dict(url='/c/%s' % pid, title=p.title))
            raise web.seeother('/share?%s' % query, absolute=True)
        else:
            return self.GET(pid, sf=sf, wf=wf)

    @auth.require_login
    def POST_edit(self, pid):
        i = web.input()
        tocongress = i.get('tocongress', 'off') == 'on'
        pf = forms.petitionform()
        pf.inputs = filter(lambda i: i.name != 'pid', pf.inputs)
        wf = forms.wyrform()
        i.email = helpers.get_loggedin_email()
        wyr_valid = (not(tocongress) or wf.validates(i))
        if not pf.validates(i) or not wyr_valid:
            return render.petitionform(pf, wf, is_new=False, is_draft=is_draft)
        
        p = dict(title=i.ptitle, description=i.msg, to_congress=tocongress)
        if 'publish' in i: p['published'] = datetime.now()
        db.update('petition', where='id=$pid', vars=locals(), **p)
        if 'publish' in i:
            create_first_signature(i, i.email)
        update_user_details(i)
        raise web.seeother('/%s' % pid)

    def POST_unsign(self, pid):
        i = web.input()
        now = datetime.now()
        db.update('signatory',
                        deleted=now,
                        where='petition_id=$pid and user_id=$i.user_id',
                        vars=locals())
        msg = 'Your signature has been removed for this petition.'
        helpers.set_msg(msg)
        raise web.seeother('/%s' % pid)

    def POST_delete(self, pid):
        now = datetime.now()
        title = db.select('petition', what='title', where='id=$pid', vars=locals())[0].title
        db.update('petition', where='id=$pid', deleted=now, vars=locals())
        helpers.set_msg('Petition "%s" deleted' % (title))
        raise web.seeother('/')

def get_contacts(user, by='id'):
    if by == 'email':
        where = 'uemail=$user'
    else:
        where = 'user_id=$user'

    contacts = db.select('contacts',
                    what='cname as name, cemail as email, provider',
                    where=where,
                    vars=locals()).list()

    if by == 'id':
        #remove repeated emails due to multiple providers; prefer the one which has name
        cdict = {}
        for c in contacts:
            if c.email not in cdict.keys():
                cdict[c.email] = c
            elif c.name:
                cdict[c.email] = c
        contacts = cdict.values()

    for c in contacts:
        c.name = c.name or c.email.split('@')[0]

    contacts.sort(key=lambda x: x.name.lower())
    return contacts

class share:
    def GET(self, emailform=None, loadcontactsform=None):
        i = web.input()
        url = i.get('url', '/')
        title = i.get('title', 'The good government site with teeth')

        user_id = helpers.get_loggedin_userid()
        contacts = get_contacts(user_id)
        sender = helpers.get_user_by_email(helpers.get_loggedin_email() or helpers.get_unverified_email())

        page_or_petition = 'page'
        isdraft = False
        if not emailform:
            emailform = forms.emailform()
            track_id, description = None, None
            if url.startswith('/c/') and url != '/c/':
                url = url.rstrip('/')
                pid = web.lstrips(url, '/c/')
                p = get_petition_by_id(pid)
                isdraft = is_draft(p)
                description = p and p.description
                track_id = helpers.get_trackid(user_id, pid) if not isdraft else None
                contacts = filter(lambda c: not is_signatory(c.email, pid), contacts)
                page_or_petition = 'petition'
            msg = render_plain.share_mail(title, url, sender, description, isdraft, track_id)
            emailform.fill(subject=title, body=msg)

        loadcontactsform = loadcontactsform or forms.loadcontactsform()

        msg, msg_type = helpers.get_delete_msg()
        return render.share(title, url, emailform,
                            contacts, loadcontactsform, page_or_petition, msg)

    def POST(self):
        i = web.input()
        emailform, loadcontactsform = forms.emailform(), forms.loadcontactsform()
        if emailform.validates(i):
            url, msg, subject = i.url, i.body, i.subject
            emails = [e.strip() for e in i.emails.strip(', ').split(',')]
            u = helpers.get_user_by_email(helpers.get_loggedin_email() or helpers.get_unverified_email())
            from_address = u and "%s %s <%s>" % (u.fname, u.lname, u.email) or config.from_address
            for email in emails:
                web.sendmail(from_address, email, subject, msg)
            page_or_petition = url.startswith('/c/') and 'petition' or 'page'
            helpers.set_msg('Thanks for sharing this %s with your friends!' % page_or_petition)
            raise web.seeother(url)
        else:
            return self.GET(emailform=emailform, loadcontactsform=loadcontactsform)

app = web.application(urls, globals())

if __name__ == '__main__':
    app.run()

########NEW FILE########
__FILENAME__ = schema
import simplejson as json
import web
import smartersql as sql

from settings import db
sql.Table.db = db

class State(sql.Table):
    @property
    def _uri_(self):
        return 'http://watchdog.net/us/%s#it' % self.code.lower()

    # states.json
    code = sql.String(2, primary=True)
    name = sql.String(256)
    status = sql.String(256)
    wikipedia = sql.URL()
    fipscode = sql.String(2)

    #senators = sql.Backreference('Politician', 'district')
    districts = sql.Backreference('District', 'state', order='name asc')

class District(sql.Table):
    @property
    def _uri_(self):
        return 'http://watchdog.net/us/%s#it' % self.name.lower()

    # districts.json
    name = sql.String(10, primary=True)
    district = sql.Integer()
    state = sql.Reference(State) #@@renames to state_id
    voting = sql.Boolean()
    wikipedia = sql.URL()

    #politician = sql.Backreference('Politician', 'district')

    @property
    def districtth(self):
        if self.district == 0:
            return 'at-large'
        else:
            return web.nthstr(self.district)
    
    @property 
    def representatives(self):
        ids = [x.id for x in db.select(
          'curr_politician', where="district_id = $self.name", vars=locals())]
        return Politician.select(where=web.sqlors('id=', ids))

    # almanac.json
    almanac = sql.URL()
    area_sqmi = sql.Number() #@@Square Miles
    cook_index = sql.String() #@@integer
    poverty_pct = sql.Percentage()
    median_income = sql.Dollars()
    est_population = sql.Number()      # most recent population estimate
    est_population_year = sql.Year()   # year of the estimate

    # shapes.json
    outline = sql.String(export=False) #@@geojson

    # centers.json
    center_lat = sql.Float() #@@ latlong type
    center_lng = sql.Float()
    zoom_level = sql.Integer()

    earmark_per_capita = sql.Float()

class Zip(sql.Table):
    zip = sql.String(5, primary=True)
    city = sql.String()
    state = sql.String(2) #@@ reference state...(@@no AE=Armed Forces Europe)
    gini = sql.Float()
    # @@other IRS stuff

class Zip4(sql.Table):
    zip = sql.String(5, primary=True) #@@references zip, (seems some ZIP+4s aren't in ctyst?)
    plus4 = sql.String(4, primary=True)
    district = sql.Reference(District)

#--alter table zip4 drop constraint zip4_pkey;
#--COPY zip4 FROM  '/home/watchdog/web/data/load/zip4.tsv';
#alter table zip4 add primary key (zip, plus4);
#--alter table zip4 add constraint "zip4_district_fkey" FOREIGN KEY (district) REFERENCES district(name) #@@
#--GRANT ALL ON zip4 TO watchdog;

class GovtrackID(sql.URL):
    towhatever = lambda f: (lambda self, x, *a: f(self,
      'http://www.govtrack.us/congress/person.xpd?id=' + x, *a))
    toxml = towhatever(sql.URL.toxml)
    ton3 = towhatever(sql.URL.ton3)

class Politician(sql.Table):
    @property
    def _uri_(self): return 'http://watchdog.net/p/%s#it' % self.id

    # politicians.json
    id = sql.String(256, primary=True)
    district = sql.Reference(District) # Moved to Congress table
    wikipedia = sql.URL()

    # govtrack.json --@@get from votesmart?
    bioguideid = sql.String()
    opensecretsid = sql.String()
    govtrackid = GovtrackID()
    gender = sql.String(1)
    birthday = sql.String() #@@date
    firstname = sql.String()
    middlename = sql.String()
    lastname = sql.String()
    election_status = sql.String()

    def xmllines(self):
        sameas = lambda x: '  <owl:sameAs xmlns:owl="http://www.w3.org/2002/07/owl#" \
rdf:resource="%s" />' % x
        return [sameas(x) for x in self.akas()]

    def n3lines(self, indent):
        sameas = lambda x: indent + '<http://www.w3.org/2002/07/owl#sameAs> <%s>;' % x
        return [sameas(x) for x in self.akas()]

    def akas(self):
        if self.bioguideid:
            yield 'http://www.rdfabout.com/rdf/usgov/congress/people/' + self.bioguideid
        if self.wikipedia:
            yield 'http://dbpedia.org/resource/' + self.wikipedia.split('/')[-1]

    @property
    def name(self):
        if hasattr(self, 'nickname') and self.nickname:
            return self.nickname + ' ' + self.lastname
        else:
            return self.fullname

    @property
    def fullname(self):
        return (self.firstname or '') + ' ' + (self.middlename or '') + ' ' + (self.lastname or '')

    @property
    def title(self):
        dist = self.district_id
        if self.is_current:
            return 'Sen.' if State.where(code=dist) else 'Rep.'
        else:
            return ''

    @property
    def handshakes(self):
        return db.query('select * from handshakes '
                    'where politician_id=$self.id '
                    'order by year desc, pol2corp+corp2pol desc',
                    vars=locals()).list()

    @property
    def opponents(self):
        return db.select('past_elections', 
                        where='district_id=$self.district_id and politician_id != $self.id',
                        order='year desc', vars=locals()).list()
    
    @property
    def past_votes(self):
        return db.select('past_elections', 
                    where='district_id=$self.district_id and politician_id = $self.id',
                    order='year desc', vars=locals()).list()

    @property
    def is_current(self):
        latest_congress = 111
        return latest_congress in self.congresses

    #@@ cache query results?
    @property
    def congresses(self):
        return [r.congress_num for r in
                db.select('congress',
                          where='politician_id = $self.id',
                          order='congress_num',
                          vars=locals())]

    officeurl = sql.URL()
    party = sql.String() # Moved to Congress table
    religion = sql.String()

    n_bills_introduced = sql.Number()
    n_bills_enacted = sql.Number()
    n_bills_debated = sql.Number()
    n_bills_cosponsored = sql.Number()
    n_speeches = sql.Number()
    words_per_speech = sql.Number()

    # voteview.json
    icpsrid = sql.Integer()
    nominate = sql.Float() #@@
    predictability = sql.Percentage()

    # earmarks.json
    amt_earmark_requested = sql.Dollars(default=0)
    n_earmark_requested = sql.Number(default=0)
    n_earmark_received = sql.Number(default=0)
    amt_earmark_received = sql.Dollars(default=0)

    # photos.json
    photo_path = sql.URL()
    photo_credit_url = sql.URL()
    photo_credit_text = sql.String()

    # fec
    money_raised = sql.Dollars()
    pct_spent = sql.Percentage()
    pct_self = sql.Percentage()
    pct_indiv = sql.Percentage()
    pct_pac = sql.Percentage()
    pct_labor = sql.Percentage()
    pct_instate = sql.Percentage()
    pct_smalldonor = sql.Percentage()

    # votesmart
    nickname = sql.String()
    votesmartid = sql.String()
    birthplace = sql.String()
    education = sql.String()

    # punch
    chips2008 = sql.Percentage()
    progressive2008 = sql.Percentage()
    progressiveall = sql.Percentage()

    # opensecrets
    pct_pac_business = sql.Percentage()

    # almanac.json
    n_vote_received = sql.Number()
    pct_vote_received = sql.Percentage()
    last_elected_year = sql.Number()

    bills_sponsored = sql.Backreference('Bill', 'sponsor')
    earmarks_sponsored = sql.Backreference('Earmark_sponsor', 'politician')

class Congress(sql.Table):
    politician = sql.Reference(Politician, primary=True)
    congress_num = sql.Integer(primary=True)
    #district = sql.Reference(District) #@@ renames to district_id
    district_id = sql.String(10, primary=True) # Can't make this a reference to district. District table only has CURRENT districts in it.
    party = sql.String()
    current_member = sql.Boolean()

class Politician_FEC_IDs(sql.Table):
    politician = sql.Reference(Politician, primary=True)
    fec_id = sql.String(primary=True)
    # cycle = sql.Integer()

class Interest_Group(sql.Table): # @@capitalization looks dorky..
    id = sql.Serial(primary=True)
    groupname = sql.String(10)
    category_id = sql.String(10) # references category,
    longname  = sql.String(unique=True)

class Interest_group_rating(sql.Table):
    """interest group scores for politicians"""
    politician = sql.Reference(Politician)

    # almanac.json
    year = sql.Year()
    group = sql.Reference(Interest_Group)
    rating = sql.Integer() # typically 0-100

class Bill(sql.Table):
    @property
    def _uri_(self):
        return 'http://watchdog.net/b/%s#it' % self.id

    id = sql.String(primary=True)
    session = sql.Integer()
    type = sql.String(5)
    number = sql.Integer()
    introduced = sql.Date()
    title = sql.String()
    sponsor = sql.Reference(Politician) #@@rename to sponsor_id
    summary = sql.String()
    maplightid = sql.String(10)

    interest_group_support = sql.Backreference('Interest_group_bill_support', 'bill', order='support desc')

    @property
    def name(self):
        typemap = {
          'h': 'H.R.',
          's': 'S.',
          'hj': 'H.J.Res.',
          'sj': 'S.J.Res.',
          'hc': 'H.Con.Res.',
          'sc': 'S.Con.Res.',
          'hr': 'H.Res.',
          'sr': 'S.Res.'
        }

        return typemap[self.type] + ' ' + str(self.number)

    @property
    def votes_by_party(self):
        """Get the votes of the political parties for a bill."""
        result = db.select(['politician p, position v'],
                what="v.vote, count(v.vote), p.party",
                where="v.politician_id = p.id and v.bill_id = $self.id "
                        "AND v.vote is not null",
                group="p.party, v.vote",
                vars = locals()
                ).list()

        d = {}
        for r in result:
            d.setdefault(r.party, {})
            d[r.party][r.vote] = r.count
        return d

    @property
    def votes_by_caucus(self):
        caucuses = json.load(file('import/load/manual/caucuses.json'))
        members = sum([x['members'] for x in caucuses], [])
        result = db.select(['position'],
            where=web.sqlors('politician_id=', members) +
              'AND bill_id=' + web.sqlquote(self.id),
            vars=locals()
            ).list()

        if not result: return None

        votemap = dict((r.politician_id, r.vote) for r in result)
        d = {}
        for c in caucuses:
            cdict = d[c['name']] = {}
            for m in c['members']:
                v = votemap.get(m)
                cdict.setdefault(v, 0)
                cdict[v] += 1
        return d

class Roll(sql.Table):
    id = sql.String(primary=True)
    type = sql.String()
    question = sql.String()
    required = sql.String()
    result = sql.String()
    bill = sql.Reference(Bill)

    @property
    def _uri_(self):
        return 'http://watchdog.net/r/us/%s#it' % self.id
    #@@@@@ DON'T REPEAT YOURSELF
    @property
    def votes_by_party(self):
        """Get the votes of the political parties for a bill."""
        result = db.select(['politician p, vote v'],
                what="v.vote, count(v.vote), p.party",
                where="v.politician_id = p.id and v.roll_id = $self.id "
                        "AND v.vote is not null",
                group="p.party, v.vote",
                vars = locals()
                ).list()

        d = {}
        for r in result:
            d.setdefault(r.party, {})
            d[r.party][r.vote] = r.count
        return d

    @property
    def votes_by_caucus(self):
        caucuses = json.load(file('import/load/manual/caucuses.json'))
        members = sum([x['members'] for x in caucuses], [])
        result = db.select(['vote'],
            where=web.sqlors('politician_id=', members) +
              'AND roll_id=' + web.sqlquote(self.id),
            vars=locals()
            ).list()

        if not result: return None

        votemap = dict((r.politician_id, r.vote) for r in result)
        d = {}
        for c in caucuses:
            cdict = d[c['name']] = {}
            for m in c['members']:
                v = votemap.get(m)
                cdict.setdefault(v, 0)
                cdict[v] += 1
        return d

class Vote(sql.Table):
    roll = sql.Reference(Roll, primary=True)
    politician = sql.Reference(Politician, primary=True)
    vote = sql.Int2()

class Position(sql.Table):
    bill = sql.Reference(Bill, primary=True)
    politician = sql.Reference(Politician, primary=True)
    vote = sql.Int2()

class Interest_group_bill_support(sql.Table):
    bill = sql.Reference(Bill, primary=True)
    group = sql.Reference(Interest_Group, primary=True)
    support = sql.Int2()

class Group_politician_similarity(sql.Table):
    group = sql.Reference(Interest_Group, primary=True)
    politician = sql.Reference(Politician, primary=True)
    agreed = sql.Integer()
    total = sql.Integer()

class Category (sql.Table):
    id = sql.String(10, primary=True)
    name = sql.String()
    industry = sql.String()
    sector = sql.String()

class Committee (sql.Table):
    id = sql.String(primary=True) # FEC ID
    name = sql.String()
    treasurer = sql.String()
    street1 = sql.String()
    street2 = sql.String()
    city = sql.String()
    state = sql.String()
    zip = sql.String()
    connected_org_name = sql.String()
    candidate_id = sql.String()
    type = sql.String()

class Contribution (sql.Table):
    id = sql.Serial(primary=True)
    fec_record_id = sql.String()
    microfilm_loc = sql.String()
    report_id = sql.String()
    recipient = sql.Reference(Committee)
    # contributor
    name = sql.String()
    street = sql.String()
    city = sql.String()
    state = sql.String()
    zip = sql.String()
    occupation = sql.String()
    employer = sql.String()
    employer_stem = sql.String()
    committee = sql.String() # sigh: sometimes committee, sometimes candidate

    sent = sql.Date()
    amount = sql.Float()

#@@INDEX by employer_stem

class Earmark(sql.Table):
    id = sql.Integer(primary=True)
    house_request = sql.Dollars()
    senate_request = sql.Dollars()
    final_amt = sql.Dollars()
    budget_request = sql.Dollars()
    prereduction_amt = sql.Dollars()
    description = sql.String()
    city = sql.String() # eventually a ref, we hope
    county = sql.String()
    state = sql.String() #@@ref?
    bill = sql.String() #@@ref
    bill_section = sql.String()
    bill_subsection = sql.String()
    project_heading = sql.String()
    district = sql.Integer()
    presidential = sql.String()
    undisclosed = sql.String()
    intended_recipient = sql.String()
    recipient_stem = sql.String()
    notes = sql.String()
    sponsors = sql.Backreference('Earmark_sponsor', 'earmark')

class Earmark_sponsor(sql.Table):
    earmark = sql.Reference(Earmark, primary=True)
    politician = sql.Reference(Politician, primary=True)


## Lobbyiest stuff:
class lob_organization(sql.Table):
    id = sql.Number(primary=True)  #@@TODO: design stable ids
    name = sql.String()
    filings = sql.Backreference('lob_filing', 'org')

class lob_person(sql.Table):
    id = sql.Number(primary=True)  #@@TODO: design stable ids
    prefix = sql.String()
    firstname = sql.String()
    middlename = sql.String()
    lastname = sql.String()
    suffix = sql.String()
    contact_name = sql.String()
    filings = sql.Backreference('lob_filing', 'lobbyist')

class lob_pac(sql.Table):
    id = sql.Number(primary=True)  #@@TODO: design stable ids
    name = sql.String()
    filings = sql.Backreference('lob_pac_filings', 'pac')

class lob_filing(sql.Table):
    id = sql.Number(primary=True)  #Uses xml file number as a stable id.
    year = sql.Year()
    type = sql.String()
    signed_date = sql.Date()
    amendment = sql.Boolean()
    certified = sql.Boolean()
    comments = sql.String()

    senate_id = sql.Number()
    house_id = sql.Number()
    filer_type = sql.String(1)

    lobbyist = sql.Reference(lob_person)
    org = sql.Reference(lob_organization)
    pacs = sql.Backreference('lob_pac_filings', 'filing')
    contributions = sql.Backreference('lob_contribution', 'filing', order='amount asc')
    @property
    def house_url(self):
        return "http://disclosures.house.gov/lc/xmlform.aspx?id=%d" % self.id

class lob_contribution(sql.Table):
    filing = sql.Reference(lob_filing)
    date = sql.Date()
    type = sql.String()
    contributor = sql.String()
    payee = sql.String()
    recipient = sql.String()
    amount = sql.Dollars()
    politician = sql.Reference(Politician)

class lob_pac_filings(sql.Table):
    pac = sql.Reference(lob_pac)
    filing = sql.Reference(lob_filing)


class Expenditure (sql.Table):
    id = sql.Serial(primary=True)
    candidate_name = sql.String()
    committee = sql.String()
    expenditure_date = sql.Date()
    recipient = sql.String()
    filer_id = sql.String(10)
    report_id = sql.Integer()
    amount = sql.String(20)

class SOI(sql.Table):
    #district_id = sql.String(10, primary=True)
    district = sql.Reference(District, primary=True)
    # irs/soi
    bracket_low = sql.Integer(primary=True)
    agi = sql.Float()
    n_dependents = sql.Float()
    n_eitc = sql.Float()
    n_filers = sql.Float()
    n_prepared = sql.Float()
    tot_charity = sql.Float()
    tot_eitc = sql.Float()
    tot_tax = sql.Float()
    avg_dependents = sql.Float()
    avg_eitc = sql.Float()
    avg_income = sql.Float()
    avg_taxburden = sql.Float()
    pct_charity = sql.Percentage()
    pct_eitc = sql.Percentage()
    pct_prepared = sql.Percentage()

class Census_meta(sql.Table):
    internal_key = sql.String(10, primary=True)
    census_type = sql.Integer(primary=True)
    hr_key = sql.String(512)
    label = sql.String()

class Census_data(sql.Table):
    #district_id = sql.String(10, primary=True)
    district = sql.Reference(District, primary=True)
    internal_key = sql.String(10, primary=True)
    census_type = sql.Integer(primary=True)
    value = sql.Float()

class Pol_contacts(sql.Table):
    politician = sql.Reference(Politician, primary=True)
    contact = sql.String()
    contacttype = sql.String(1)     # E=email, W=wyr, I=ima, Z=zipauth
    captcha = sql.Boolean()

class Pol_phones(sql.Table):
    politician = sql.Reference(Politician, primary=True)
    city = sql.String(primary=True)
    phone1 = sql.String(15)
    phone2 = sql.String(15)
    tollfree = sql.String(15)

class Census_Population(sql.Table):
    state_id = sql.String(2)     # STATE
    county_id = sql.String(3)    # COUNTY
    blockgrp_id = sql.String(1)  # BLOCKGRP
    block_id = sql.String(4)     # BLOCK
    district_id = sql.String(2)  # CD110
    tract_id = sql.String(6)     # TRACT
    zip_id = sql.String(5)       # ZCTA
    sumlev = sql.String(16)      # SUMLEV
    area_land = sql.Integer()    # AREALAND
    area_land.sql_type = 'bigint'
    population = sql.Integer()

class Handshakes(sql.Table):
    politician = sql.Reference(Politician, primary=True)
    corporation = sql.String(primary=True)
    pol2corp = sql.Dollars()
    corp2pol = sql.Dollars()
    year = sql.Integer()

class Past_Elections(sql.Table):
    politician_id = sql.String(256, primary=True) # don't refer `politician` table as lost candidates might not be there
    district = sql.Reference(District, primary=True)
    year = sql.Year(primary=True)
    type = sql.String(10, primary=True) #Gen for general, SpGen for special-general elections
    votes_received = sql.Integer()
    pct_votes_received = sql.Percentage()
    expenditure = sql.Dollars()

class Exempt_Org(sql.Table):
    ein = sql.Integer(primary=True)
    primary_name = sql.String()
    careof_name = sql.String()
    street = sql.String()
    city = sql.String()
    state = sql.String(2)
    zip = sql.String()
    group_exemption_num = sql.String()
    subsection_code = sql.String()
    affiliation = sql.String()
    classification_code = sql.String()
    ruling_date = sql.String()
    deductibility_code = sql.String()
    foundation_code = sql.String()
    activity_code = sql.String()
    organization_code = sql.String()
    exempt_org_status_code = sql.String()
    advance_ruling_expiration = sql.String()
    tax_period = sql.String()
    asset_code = sql.String()
    income_code = sql.String()
    filing_requirement_code = sql.String()
    accounting_period = sql.String()
    asset_amt = sql.BigInteger()
    income_amt = sql.BigInteger()
    form_990_revenue_amt = sql.BigInteger()
    ntee_code = sql.String()
    sort_name = sql.String()

def init():
    db.query("CREATE VIEW census AS select * from census_meta NATURAL JOIN census_data")
    db.query("CREATE INDEX contribution_recipient_id_idx ON contribution (recipient_id)")
    db.query("CREATE INDEX contribution_zip_idx ON contribution (zip)")
    db.query("CREATE INDEX contribution_empl_stem_idx ON contribution (LOWER(employer_stem))")
    db.query("CREATE INDEX contribution_occupation_idx ON contribution (LOWER(occupation));")
    db.query("CREATE INDEX contribution_lower_name_zip_idx ON contribution (LOWER(name), zip);")
    db.query("ANALYZE contribution;")
    db.query("CREATE VIEW curr_politician AS SELECT politician.* FROM politician, congress WHERE politician.id = politician_id AND congress_num='111' AND current_member = 't' ;")
    db.query("GRANT ALL on curr_politician TO watchdog;")

    try:
        db.query("GRANT ALL on census TO watchdog")
    except:
        pass # group doesn't exist


########NEW FILE########
__FILENAME__ = settings
import os
import tempfile
import web

production_mode = os.environ.get('PRODUCTION_MODE', False)
render = web.template.render('templates/', base='base', cache=production_mode)
render_plain = web.template.render('templates/', cache=production_mode) #without base, useful for sending mails
if os.environ.get('DATABASE_URL'):
    url = os.environ['DATABASE_URL']
    db = web.database(dbn=url.split(':')[0], user=url.split('//')[1].split(':')[0],
      pw=url.split('@')[0].split(':')[2],
      host=url.split('@')[1].split('/')[0],
      db=url.split('/')[3]
    )
else:
    db = web.database(dbn=os.environ.get('DATABASE_ENGINE', 'postgres'),
                  db=os.environ.get('WATCHDOG_TABLE', 'watchdog_dev'))
db.printing = False
current_session = '*' # * to load data from all sessions; 'xxx' to load data of that session

########NEW FILE########
__FILENAME__ = loaddb
from web.browser import AppBrowser
from webtest import app

test_email = 'testemail@example.com'
test_passwd = 'secret4test'
test_pid = 'test_petition'
test_pid_to_cong = 'test_petition_to_congress'

def loaddb(db):
    create_test_user(db)
    create_test_petition(db)
    create_test_petition_to_congress(db)
    
def create_test_user(db):
    userexists = db.select('users', where='email=$test_email', vars=dict(test_email=test_email))
    if not userexists:
        b = AppBrowser(app)
        b.open('/u/login')
        b.select_form(name='signup')
        b['email'] = test_email
        b['password'] = b['password_again'] = test_passwd
        b.submit()
        assert b.path ==  '/', b.path
        b.open('/c/new')
        assert 'Hi,' in b.data, 'creating test user failed'

def create_test_petition(db):
    petition_exists = db.select('petition', where='id=$test_pid', vars=dict(test_pid=test_pid))
    if not petition_exists:
        b = AppBrowser(app)
        b.open('/c/new')
        b.select_form()
        b['ptitle'] = test_pid.replace('_', ' ')
        b['pid'] = test_pid
        b['msg'] = 'Here is a test petition with a not-so-worthy description, just for testing purpose'
        b.submit()
        assert b.path == '/c/new'
        form = b.select_form(name='login')
        b['useremail'] = test_email
        b['password'] = test_passwd
        b.submit()
        assert 'Congratulations' in b.data, 'creating test petition failed'

def create_test_petition_to_congress(db):
    petition_exists = db.select('petition', where='id=$test_pid_to_cong', vars=dict(test_pid_to_cong=test_pid_to_cong))
    if not petition_exists:
        b = AppBrowser(app)
        b.open('/c/new')
        f = b.select_form()
        f['ptitle'] = test_pid_to_cong.replace('_', ' ')
        f['pid'] = test_pid_to_cong
        f['msg'] = 'All the signatures of it are claimed to be sent to reps'
        f['tocongress'] = ['on']
        f['prefix'] = ['Ms.']
        f['lname'] = 'tester'
        f['fname'] = 'here'
        f['addr1'] = '103, Reach St.'
        f['city'] = 'City of Flowers'
        f['phone'] = '999-100-9999'
        f['zip5'], f['zip4'], f['state'] = '80132', '0001', ['CO'] #CO-05
        b.submit()
        assert b.path == '/c/new'
        form = b.select_form(name='login')
        b['useremail'] = test_email
        b['password'] = test_passwd
        b.submit()
        assert 'Congratulations' in b.data, 'creating test petition failed'

if __name__ == '__main__':
    loaddb()

########NEW FILE########
__FILENAME__ = test_all
import webtest

def suite():
    modules = ["test_login", "test_petitions", "test_wyrapi"]
    return webtest.suite(modules)

if __name__ == "__main__":
    webtest.main()

########NEW FILE########
__FILENAME__ = test_login
import webtest
from loaddb import test_email, test_passwd

class LoginTest(webtest.TestCase):
    def testRegister(self):
        b = self.browser()
        b.open('/u/login')
        b.select_form(name='signup')
        b['email'] = 'user@example.com'
        b['password'] = b['password_again'] = 'secret'
        res = b.submit()
        self.assertEquals(b.path, '/')

    def testRegister_diff_passwds(self):
        b = self.browser()
        b.open('/u/login')
        b.select_form(name='signup')
        b['email'] = 'user@example.com'
        b['password'] = 'secret'
        b['password_again'] = 'different'
        res = b.submit()
        self.assertEquals(b.path, '/u/signup')
        assert "Oops, passwords don&#39;t match" in b.data

    def testRegister_user_exists(self):
        b = self.browser()
        b.open('/u/login')
        b.select_form(name='signup')
        b['email'] = test_email
        b['password'] = 'anything'
        b['password_again'] = 'anything'
        b.submit()
        self.assertEquals(b.path, '/u/signup')
        self.assertTrue("An account with that email already exists" in b.data)

    def testLogin(self):
        b = self.browser()
        b.open('/u/login')
        b.select_form(name='login')
        b['useremail'] = test_email
        b['password'] = test_passwd
        b.submit()
        self.assertEquals(b.path, '/')

    def testLogin_wrong_passwd(self):
        b = self.browser()
        b.open('/u/login')
        b.select_form(name='login')
        b['useremail'] = test_email
        b['password'] = test_passwd + '@@@@'
        b.submit()
        self.assertEquals(b.path, '/u/login')
        self.assertTrue('Oops, wrong email or password' in b.data)

    def testLogin_no_user_exist(self):
        b = self.browser()
        b.open('/u/login')
        b.select_form(name='login')
        b['useremail'] = test_email + 'extra'
        b['password'] = test_passwd
        b.submit()
        self.assertEquals(b.path, '/u/login')
        self.assertTrue('No account exists with this email' in b.data)

    def testLogout(self):
        b = self.browser()
        self.login()
        b.open('/c/new')
        self.assertTrue('Hi, testemail.' in b.get_text())
        self.logout()
        b.open('/c/new')
        self.assertTrue('Hi, testemail.' not in b.get_text())
        
if __name__ == "__main__":
    webtest.main()

########NEW FILE########
__FILENAME__ = test_petitions
import webtest
from loaddb import test_email, test_passwd, test_pid, test_pid_to_cong
from utils import messages

def fill_captcha(f):
    for c in f.controls:
        cname = c.name
        if cname and cname.startswith('captcha_') and cname != 'captcha_env':
            c.value = 'xxxxx'

def fill_zip(f, captcha=False):
    if captcha:
        f['zip5'], f['zip4'], f['state'] = '27532', '0001', ['NC'] # NC-03
        #f['zip5'], f['zip4'], f['state'] = '92003', '0001', ['CA'] # CA-49
        #f['zip5'], f['zip4'], f['state'] = '75101', '0001', ['TX'] # TX-06 for 2 captchas
    else:
        f['zip5'], f['zip4'], f['state'] = '54101', '0011', ['WI'] # WI-08
        
def fill_user_details(f, to_congress=False, captcha=False):
    f['fname'] = 'Cool'
    f['lname'] = 'Fellow'
    try:
        e = f.find_control(name='email')
    except:
        pass
    else:
        if e.type != 'hidden': f['email'] = 'cool@fellow.com'
    if to_congress:
        f['prefix'] = ['Mr.']
        f['addr1'] = '10, Ed St.'
        f['city'] = 'Garden City'
        f['phone'] = '101-100-9999'
        fill_zip(f, captcha=captcha)
    
class PetitionTest(webtest.TestCase):
    def fill_petition_form(self, f, title, desc, to_congress=False, captcha=False):
        f['ptitle'] = title
        f['pid'] = title.replace(' ', '-')
        f['msg'] = desc
        if to_congress:
            f['tocongress'] = ['on']
            fill_user_details(f, to_congress, captcha=captcha)

    def _test_create(self, to_congress=False, captcha=False):
        self.b.open('/c/new')
        form = self.b.select_form()
        ptitle = 'save the world'
        self.fill_petition_form(form, ptitle,
            'Make the world better place to live!', to_congress=to_congress, captcha=captcha)
        return ptitle

    def _test_L(self, to_congress=False):
        """for a logged-in-user"""
        b = self.browser()
        self.login()
        self._test_create(to_congress=to_congress)
        b.submit()
        self.assertEquals(b.path, '/c/save-the-world')
        self.assertTrue('Congratulations' in b.data)
        return b.data
        
    def _test_NL(self, to_congress=False):
        """for a NON-logged-in-user"""
        b = self.browser()
        self._test_create(to_congress=to_congress)
        b.submit()
        self.assertEquals(b.path, '/c/new')
        form = b.select_form(name='login')
        b['useremail'] = test_email
        b['password'] = test_passwd
        b.submit()
        self.assertEquals(b.path, '/c/save-the-world')
        self.assertTrue('Congratulations' in b.data)
        return b.data

    def test_create_L_NTC(self):
        """for a logged-in-user, NOT to congress"""
        data = self._test_L(to_congress=False)
        self.assertTrue('your comments will be sent to your Representative too.' not in data)

    def test_create_L_TC(self):
        """for a logged-in-user, to congress"""
        data = self._test_L(to_congress=True)
        self.assertTrue('your comments will be sent to your Representative too.' in data)

    def test_create_NL_NTC(self):
        """for a NON-logged-in-user, NOT to congress"""
        data = self._test_NL(to_congress=False)
        self.assertTrue('your comments will be sent to your Representative too.' not in data)

    def test_create_NL_TC(self):
        """for a NON-logged-in-user, to congress"""
        data = self._test_NL(to_congress=True)
        self.assertTrue('your comments will be sent to your Representative too.' in data)

    def test_create_L_TC_captcha(self):
        """for a logged-in-user, to congress with a captcha"""
        b = self.browser()
        self.login()
        self._test_create(to_congress=True, captcha=True)
        b.submit()
        self.assertEquals(b.path, '/c/new')
        self.assertTrue('Please fill the captcha' in b.data)
        f = b.select_form()
        fill_captcha(f)
        b.submit()
        self.assertEquals(b.path, '/c/save-the-world')
        self.assertTrue('Congratulations' in b.data)

    def test_petition_list(self):
        b = self.browser()
        b.open('/c/')
        self.assertTrue(test_pid in b.data)
        self.assertTrue(test_pid_to_cong in b.data)

    def _test_save_draft(self, b, ptitle):    
        # make sure that it doesn't come into list in /c/ and share is okay 
        self.assertTrue('Petition saved for publishing later' in b.data)
        self.assertTrue('Share Draft' in b.data)
        b.follow_link(text='Share Draft')
        self.assertTrue('Could you please review the draft of the petition' in b.data)
        b.open('/c/')
        self.assertTrue(ptitle not in b.data)
    
    def _test_publish_draft(self, b, ptitle):
        #now open it back, publish it and make sure that it comes in list in /c/
        pid = ptitle.replace(' ', '-')
        b.open('/c/%s?m=edit' % pid)
        b.select_form(name='petition')
        b.submit(name='publish')
        self.assertTrue('Congratulations' in b.data)
        self.assertTrue('1 person has signed this petition')
        b.follow_link(text='Share')
        self.assertTrue('join me in signing the petition' in b.data)
        b.open('/c')
        self.assertTrue(ptitle in b.data)

    def test_draft_L(self):
        b = self.browser()
        self.login()
        b.open('/c/new')
        ptitle = self._test_create()
        b.submit(name='save')
        self._test_save_draft(b, ptitle)
        self._test_publish_draft(b, ptitle)
        
    def test_draft_NL(self):
        b = self.browser()
        b.open('/c/new')
        ptitle = self._test_create()
        b.submit(name='save')
        self.assertEquals(b.path, '/c/new')
        form = b.select_form(name='login')
        b['useremail'] = test_email
        b['password'] = test_passwd
        b.submit()
        self._test_save_draft(b, ptitle)
        self._test_publish_draft(b, ptitle)
        
        
    def test_edit_by_owner(self):
        b = self.browser()
        self.login()
        b.open('/c/%s/' % test_pid)
        b.follow_link(text='Edit')
        f = b.select_form(name='petition')
        f['msg'] = 'changing the content of the petition' + f['msg']
        b.submit()
        self.assertEquals(b.path, '/c/'+ test_pid)
        self.assertTrue('changing the content of the petition' in b.data)

    def test_edit_by_non_owner(self):
        b = self.browser()
        b.open('/c/%s/?m=edit' % test_pid)
        self.assertTrue(b.path.startswith('/u/login?redirect='))

    def test_delete_by_owner(self):
        b = self.browser()
        self.login()
        self._test_create()
        b.submit()
        b.open('/c/%s/' % 'save-the-world')
        b.follow_link(text='Delete')
        self.assertTrue('Are you sure you want to delete your petition' in b.data)
        b.select_form(name='delete')
        b.submit()
        b.open('/c/save-the-world')
        self.assertEquals(b.status, 404)        

class SignTest(webtest.TestCase):
    def make_sign_NL(self, to_congress):
        b = self.browser()
        b.open('/c/%s' % (test_pid_to_cong if to_congress else test_pid))
        f = b.select_form()
        fill_user_details(f, to_congress=to_congress)
        comment = 'a comment here'
        b['comment'] = comment
        b.submit()
        self.assertTrue(b.path.startswith('/share'))
        self.assertTrue('Thanks for your signing' in b.data)
        b.open('/c/%s/signatories' % (test_pid_to_cong if to_congress else test_pid))
        self.assertTrue(comment in b.data)

    def test_sign_NL_NTC(self):
        self.make_sign_NL(to_congress=False)

    def test_sign_NL_TC(self):
        before = len(messages.query().list())
        self.make_sign_NL(to_congress=True)
        after = len(messages.query().list())
        self.assertEquals(before+3, after) # one msg gets added for each rep/sen

    def test_edit_sign(self):
        b = self.browser()
        self.login()
        b.open('/c/%s' % test_pid)
        self.assertTrue('Change your signature' in b.data)
        f = b.select_form(name="sign")
        comment = "I'm the creator of it"
        f['fname'], f['lname'] = 'fname', 'lname'
        f['comment'] = comment 
        b.submit()
        self.assertTrue(b.path.startswith('/share'))
        self.assertTrue('Your signature has been changed' in b.data)
        b.open('/c/%s/signatories' % test_pid)
        self.assertTrue(comment in b.data)

if __name__ == '__main__':
    webtest.main()

########NEW FILE########
__FILENAME__ = test_wyr
import webtest
from loaddb import test_email, test_passwd
from test_petitions import fill_user_details, fill_captcha

class WYRTest(webtest.TestCase):
    def _fill_wyr_form(self, f, subject, msg, captcha=False):
        fill_user_details(f, to_congress=True, captcha=captcha)
        f['ptitle'] = subject
        f['msg'] = msg
            
    def _create_msg(self, captcha=False):
        subject = 'Test subject'
        msg = 'Test msg'
        b = self.browser()
        b.open('/writerep/')
        f = b.select_form(name='writerep')
        self._fill_wyr_form(f, subject, msg, captcha=captcha)
        return b
                
    def test_no_captcha(self):
        b = self._create_msg()
        b.submit()
        self.assertEquals(b.path, '/writerep/')
        self.assertTrue('Your message has been sent to ' in b.data)
        
    def test_captcha(self):
        b = self._create_msg(captcha=True)
        b.submit()
        self.assertTrue('Please fill the captcha' in b.data)
        f = b.select_form(name='writerep')
        fill_captcha(f)
        b.submit()
        self.assertEquals(b.path, '/writerep/')
        self.assertTrue('Your message has been sent to ' in b.data)
        
    def test_incomplete_details(self):
        b = self._create_msg()
        f = b.select_form()
        f['zip5'] = '' #take off zip
        b.submit()
        self.assertTrue('Please try again after fixing the errors highlighted below')
        f = b.select_form(name='writerep')
        fill_user_details(f, to_congress=True)
        b.submit()
        self.assertEquals(b.path, '/writerep/')
        self.assertTrue('Your message has been sent to' in b.data)

if __name__ == "__main__":
    webtest.main()

########NEW FILE########
__FILENAME__ = test_wyrapi
import webtest
import simplejson
import urllib
from utils.api import app as apiapp

user_data = dict(prefix='Mr.', lname='lastname', fname='firstname', district='WI-01', 
            address='10, North Road', phone='123456789',
            zip5='53101', zip4='0001', city='Garden city',
            subject='hello', msg='just to say hello', env={})

class WYRAPI_TEST(webtest.TestCase):
    def request(self, path, data=None):
        b = self.browser()
        response = b.open(path, data)
        self.assertEquals(response.headers['Content-Type'], 'application/json')
        return simplejson.loads(response.read())

    def test_prepare(self):
        data = self.request('/api/wyr.prepare?district=WI-01')
        for pol in data:
            self.assertEquals(data[pol]['captcha_src'], None)

    def test_prepare_captcha(self):
        data = self.request('/api/wyr.prepare?district=CA-49')
        captcha_src = None
        for pol in data:
            captcha_src = captcha_src or data[pol]['captcha_src']
        self.assertTrue(captcha_src)

    def test_prepare_wrong_input(self):
        data = self.request('/api/wyr.prepare?zip5=00000&zip4=1111')
        self.assertEquals(data.get('err_msg'), 'Invalid zip and/or address' )

        data = self.request('/api/wyr.prepare?district=AA-01')
        self.assertEquals(data.get('err_msg'), 'Invalid district')

    def test_send(self):
        udata_dict = user_data.copy()
        udata = urllib.urlencode(udata_dict)
        prepared_data = self.request('/api/wyr.prepare?%s' % udata)
        udata_dict['env'] = simplejson.dumps(prepared_data)
        data = self.request('/api/wyr.send', udata)
        for pol in data:
            self.assertEquals(data[pol]['status'], 'SENT')
            self.assertTrue(data[pol]['msgid'])

    def test_send_wrong_input(self):
        udata = user_data.copy()
        del udata['lname']
        udata = urllib.urlencode(udata)
        data = self.request('/api/wyr.send', udata)
        self.assertTrue('err_msg' in data)
        self.assertEquals(data['err_msg'], 'Invalid Last Name')
            
    def test_send_captcha(self):
        udata_dict = user_data.copy()
        udata_dict['district'] = 'CA-49'
        udata_dict['zip5'], udata_dict['zip4'] = '92003', '0001'
        udata = urllib.urlencode(udata_dict)
        prepared_data = self.request('/api/wyr.prepare?%s' % udata)

        #send without filling captcha value and get error
        udata_dict['env'] = simplejson.dumps(prepared_data)
        send_data = urllib.urlencode(udata_dict)
        data = self.request('/api/wyr.send', send_data)
        self.assertEquals(data['err_msg'], 'Invalid captcha value')
        
        #fill captcha value and succeed in sending
        for pol in prepared_data:
            prepared_data[pol]['captcha_value'] = 'something'

        udata_dict['env'] = simplejson.dumps(prepared_data)
        send_data = urllib.urlencode(udata_dict)
        data = self.request('/api/wyr.send', send_data)
        
        for pol in data:
            self.assertEquals(data[pol]['status'], 'SENT')
            self.assertTrue(data[pol]['msgid'])

if __name__ == '__main__':
    webtest.main()    

########NEW FILE########
__FILENAME__ = webtest
import sys
sys.path.insert(0, '.')
sys.path.append('vendor')

import web
from web.browser import AppBrowser
import web.test as webtest
from web.test import *  #including main

import settings
settings.db = web.database(dbn=os.environ.get('DATABASE_ENGINE', 'postgres'), db='watchdog_test')
db = settings.db
db.printing = False

from webapp import app

import loaddb
loaddb.loaddb(db)

debug = web.debug

class TestCase(webtest.TestCase):
    def setUp(self):
        self.t = db.transaction()
        self.ctx = dict(self.t.ctx)

    def tearDown(self):
        self.t.ctx = web.storage(self.ctx)
        self.t.rollback()

    def browser(self):
        self.b = AppBrowser(app)
        return self.b

    def login(self, uemail=None, password=None):
        self.b.open('/u/login')
        self.b.select_form(name='login')
        self.b['useremail'] = uemail or loaddb.test_email
        self.b['password'] = password or loaddb.test_passwd
        self.b.submit()

    def logout(self):
        self.b.open('/u/logout')
        self.b.select_form()
        self.b.submit()

    def get_errors(self):
        soup = self.b.get_soup()
        errs = soup.findAll(attrs={'class': ['error', 'wrong']})
        return [self.b.get_text(e) for e in errs]

########NEW FILE########
__FILENAME__ = api
import web
from wyrutils import getdist, dist2pols, has_captcha
import writerep, wyrapp, messages
from auth import new_user
import simplejson
from settings import db

urls = (
    '/wyr\.prepare', 'wyr_prepare',
    '/wyr\.send', 'wyr_send',
    '/wyr\.getresponses', 'wyr_getresponses'
)

def api_processor(handler):
    try:
        result = handler()
    except WYR_Error, e:
        result = dict(err_code=e.code, err_msg=e.msg)

    web.header('Content-Type', 'application/json')
    return simplejson.dumps(result)

class WYR_Error(Exception):
    def __init__(self, code_msg):
        code, msg = code_msg
        Exception.__init__(self, msg)
        self.code = code
        self.msg = msg

INVALID_DISTRICT = 1, "Invalid district"
INVALID_ZIP_OR_ADDRESS = 2, "Invalid zip and/or address"
INVALID_CAPTCHA_VALUE = 3, "Invalid captcha value"
NUM_ERR_CODES = 3

def get_pols(i):
    district = i.get('district', '')
    dist = getdist(i.get('zip5', ''), i.get('zip4', ''), i.get('address', ''))
    dist = dist and dist[0]
    if district and dist:
        if district != dist:
            raise WYR_Error(INVALID_ZIP_OR_ADDRESS)
    else:
        district = district or dist
    i['district'] = district    
    pols = dist2pols(district)
    if not pols:
        if i.get('district'):
            raise WYR_Error(INVALID_DISTRICT)
        else:
            raise WYR_Error(INVALID_ZIP_OR_ADDRESS)
    return pols

def validate(i):
    reqs = ['prefix', 'fname', 'lname', 'address', 'city', 'zip5', 'subject', 'msg']
    d = dict((r, r.title()) for r in reqs)
    d.update(fname='First Name', lname='Last Name', msg='Message')

    for r in reqs:
        if not i.get(r):
            err_code_msg = reqs.index(r)+NUM_ERR_CODES+1, 'Invalid %s' % d[r]
            raise WYR_Error(err_code_msg)

    if 'env' in i:
        env = simplejson.loads(i.env)
        for pol in env:
            if env[pol]['captcha_src'] and not env[pol].get('captcha_value'):
                raise WYR_Error(INVALID_CAPTCHA_VALUE)
            elif env[pol]['captcha_src']:
                i['captcha_%s' % pol] = env[pol]['captcha_value']

    return get_pols(i)
    
class wyr_prepare:
    def GET(self):
        i = web.input()
        pols = get_pols(i)
        e = {}
        for pol in pols:
    	    e[pol] = has_captcha(pol) and writerep.prepare(pol) or dict(captcha_src=None)
        return e

def get_userid():
    useremail = 'apiuser@opencongress.com' # change it later
    uid = db.select('users', where="email=$useremail", vars=locals())
    return uid and uid[0].id or new_user(useremail, 'apiuser').id

class wyr_send:
    def POST(self):
        i = web.input()
        pols = validate(i)
        uid = get_userid()
        i['ptitle'] = i.get('subject')
        i['addr1'], i['addr2'] = i.get('address'), ''
        i['state'] = i.district[:2]
        env = simplejson.loads(i.get('env', '{}'))
        msgids = writerep.send_msgs(uid, i, source_id='wyr', pols=pols, env=env)
        ret = {}
        for pol in msgids:
            ret[pol] = dict(msgid=msgids[pol], status='SENT')
        return ret

class wyr_getresponses:
    def GET(self):
        i = web.input()
        msgid = i.get('msgid')
        responses = messages.get_responses(msgid)
        ret = []
        for r in responses:
            ret.append(dict(id=r.id, msgid=r.msgid, response=r.response, timestamp=r.received))        
        return ret

app = web.application(urls, globals())
app.add_processor(api_processor)

if __name__ == "__main__":
    app.run()

########NEW FILE########
__FILENAME__ = apipublish
"""
publish Python objects as various API formats
"""

import datetime
import simplejson as json
import web

API_PREFIX = "http://watchdog.net/about/api#"

def _listify(v):
    if v is None:
        return []
    elif not isinstance(v, list):
        return [v]
    else:
        return v

def _getitems(obj, listify=True):
    out = []
    for k in obj.columns:
        c = obj.columns[k]
        if hasattr(c, 'export') and not c.export:
            pass
        else:
            v = getattr(obj, k)
            if listify: v = _listify(v)
            out.append((k, c, v))
    out.sort(lambda x, y: cmp(x[0], y[0]))
    return out

class SmartJSONEncoder(json.JSONEncoder):
    DATE_FORMAT = "%Y-%m-%d"
    TIME_FORMAT = "%H:%M:%S"
    def _default(self, obj):
        if isinstance(obj, datetime.datetime):
            return obj.strftime("%sT%sZ" % (self.DATE_FORMAT, self.TIME_FORMAT))
        elif isinstance(obj, datetime.date):
            return obj.strftime(self.DATE_FORMAT)
        elif isinstance(obj, datetime.time):
            return obj.strftime(self.TIME_FORMAT)
        else:
            try:
                return super(SmartJSONEncoder, self).default(obj)
            except TypeError:
                try:
                    return json.dumps(obj)
                except:
                    try:
                        return obj._uri_
                    except:
                        return None
    
    def default(self, obj):
        return getattr(obj, 'tojson', lambda: self._default(obj))()

def tojson(x):
    return json.dumps(x, cls=SmartJSONEncoder)

def publishjson(lst):
    out = ['[']
    first = True
    for obj in lst:
        if not first:
            out.append('  ,')
        first = False
        out.append('  {')
        out.append('    "_type": "%s",' % obj.__class__.__name__)
        out.append('    "uri": "%s",' % obj._uri_)
        for k, c, v in _getitems(obj, listify=False):
            out.append('    "%s": %s,' % (k, tojson(v)))
        out[-1] = out[-1][:-1]
        out.append('  }')
    out.append(']\n')
    return '\n'.join(out)

n3_basic_indent = '  '

def _n3ify(obj, indent):
    if isinstance(obj, dict):
        return n3ify_dict(obj, indent)
    elif isinstance(obj, list):
        return ', '.join([n3ify(item, indent + n3_basic_indent)
                          for item in obj])
    elif isinstance(obj, bool):
        return str(obj).lower()
    elif isinstance(obj, (int, float)):
        return obj
    else:
        #@@ strings.  Improve this code to cover the cases in
        # http://www.w3.org/TR/rdf-testcases/#ntrip_strings
        return '"%s"' % (unicode(obj)
                         .replace('\\', r'\\')
                         .replace('\n', r'\n')
                         .replace('"', r'\"'))

def n3ify(obj, indent, c=None):
    if hasattr(obj, 'ton3'):
        return obj.ton3(indent)
    elif hasattr(c, 'ton3'):
        return c.ton3(obj, indent)
    else:
        return _n3ify(obj, indent)

def publishn3(lst):
    indent = n3_basic_indent
    out = ['@prefix : <%s> .' % API_PREFIX, '']
    for obj in lst:
        out.append('<%s> a :%s;' % (obj._uri_, obj.__class__.__name__))
        for k, c, v in _getitems(obj):
            for item in v:
                n3v = n3ify(item, indent, c)
                if not n3v: continue
                out.append(indent + 
                  ':%s %s;' % (k, n3v)
                )
        if hasattr(obj, 'n3lines'): out.extend(obj.n3lines(indent))
        out.append('.\n')
    return '\n'.join(out)

def publishxml(lst):
    out = [
      '<?xml version="1.0"?>',
      '<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"',
      '  xmlns="%s"' % API_PREFIX,
      '  xmlns:x="%s">' % API_PREFIX]
    for obj in lst:
        objtype = obj.__class__.__name__
        out.append('<%s rdf:about="%s">' % (objtype, obj._uri_))
                
        for k, c, v in _getitems(obj):
            for item in v:
                xmlv = c.toxml(item)
                if not xmlv: continue
                outline = '  <%s%s</%s>' % (k, xmlv, k)
                outline = outline.replace('></%s>' % k, ' />') # clean up empty values
                out.append(outline)
        if hasattr(obj, 'xmllines'): out.extend(obj.xmllines())
        out.append('</%s>' % objtype)
    out.append('</rdf:RDF>')
    return '\n'.join(out) + '\n'

def _findq(x):
    """
    Find the associated quality level with a media type instance:
    
        >>> _findq('text/xml')
        (1, 'text/xml')
        >>> _findq('text/xml;level=1')
        (1, 'text/xml')
        >>> _findq('text/xml;level=1;q=.5')
        (0.5, 'text/xml')
        >>> _findq('text/xml; level=2; q=.5')
        (0.5, 'text/xml')
        >>> _findq('text/xml;q=.5;level=1')
        (0.5, 'text/xml')
    """
    options = x.split(';')
    mediatype = options[0].strip()
    for option in options[1:]:
        option = option.strip()
        if option.startswith('q='):
            q = float(option[2:])
            break
    else:
        q = 1
    return (q, mediatype)

def bestaccepted(options, source=None):
    """
    @@move to web.py
    
    Find the media type in `options` that best matches the 
    acceptable options in `source` (or the `Accept:` header if
    `source` is None).
    
        >>> bestaccepted(
        ...  ['application/xml+xhtml', 'text/html', 'text/plain'], 
        ...  'text/plain; q=0.5, text/html, text/x-dvi; q=0.8, text/x-c')
        'text/html'
        >>> bestaccepted(
        ...  ['application/pdf', 'text/html'],
        ...  'text/*;q=.4, application/pdf;q=.2')
        'text/html'
        >>> bestaccepted(
        ...  ['application/pdf', 'text/html'],
        ...  '*/*;q=1, text/html; q=.2')
        'application/pdf'
    
    todo: support quality factors on input
    todo: send a 406 if no match
    """
    if source is None:
        source = web.ctx.env.get('HTTP_ACCEPT', '')
    
    accepts = [_findq(x) for x in source.split(',')]
    accepts.sort(reverse=True)
    for q, mtype in accepts:
        if mtype in options:
            return mtype
        elif mtype == '*/*':
            return options[0]
        elif mtype.endswith('/*'):
            for option in options:
                if option.startswith(mtype[:-1]):
                    return option
    return options[0] #@@ should probably send a 406

def publish(lst, ftype=None):
    preferences = ['html', 'n3', 'json', 'xml']
    ftype_map = {
      'html': 'text/html',
      'n3': 'text/rdf+n3',
      'json': 'application/json',
      'xml': 'application/rdf+xml'
    }
    if ftype:
        format = ftype_map[ftype]
    else:
        format = bestaccepted([ftype_map[x] for x in preferences])
    if format == 'text/html':
        return False
    elif format == 'text/rdf+n3':
        web.header('Content-Type', 'text/rdf+n3')
        return publishn3(lst)
    elif format == 'application/rdf+xml':
        web.header('Content-Type', 'application/rdf+xml')
        return publishxml(lst)
    elif format == 'application/json':
        web.header('Content-Type', 'application/json')
        return publishjson(lst)
    else:
        raise ValueError, 'unknown format'

if __name__ == "__main__":
    print publishjson(exampleobj)
    print
    print publishn3(exampleobj)
    print
    print publishxml(exampleobj)

########NEW FILE########
__FILENAME__ = auth
import urllib, random, hmac, datetime
import simplejson as json
from hashlib import sha1

import web
import helpers, forms, config
from settings import db, render

def get_hexdigest(key, s):
    return hmac.new(key, s, sha1).hexdigest()

def encrypt_password(password):
    key = str(random.random())[2:]
    return '@'.join([key, get_hexdigest(key, password)])

def check_password(user, password):
    key, enc_password = user.password.split('@')
    return enc_password == get_hexdigest(key, password)

def loginuser(useremail, password):
    user = helpers.get_user_by_email(useremail)
    if user and check_password(user, password):
        helpers.set_login_cookie(useremail)
        return user
    else:
        return None

def new_user(email, password):
    password = encrypt_password(password)
    exists = db.select('users', where='email=$email', vars=locals())
    if exists:
        return None

    user_id = db.insert('users', email=email, password=password, verified=True)
    user = web.storage(id=user_id, email=email, password=password, verified=True)
    return user
        
class signup:
    def POST(self):
        i = web.input(redirect='/')
        sf = forms.signupform()
        if not sf.validates(i):
            lf = forms.loginform()
            lf['redirect'].value = sf['redirect'].value = i.redirect
            sf.fill(i)
            return render.login(lf, sf, redirect=i.redirect)
        user = new_user(i.email, i.password)
        helpers.set_login_cookie(i.email)
        raise web.seeother(i.redirect, absolute=True)
      
def internal_redirect(path, method, query, data):
    # does an internal redirect within the application
    from webapp import app
    env = web.ctx.env
    env['REQUEST_METHOD'] = method
    env['PATH_INFO'] = path
    env['QUERY_STRING'] = web.utf8(query)

    cookie_headers = [(k, v) for k, v in web.ctx.headers if k == 'Set-Cookie'] 
    app.load(env)

    env['HTTP_COOKIE'] = env.get('HTTP_COOKIE', '') + ';' + ";".join([v for (k, v) in cookie_headers])
    web.ctx.headers = cookie_headers

    if method == 'POST':
        web.ctx.data = web.utf8(data)
    return app.handle()
         
class login:
    def GET(self):
        referer = web.ctx.env.get('HTTP_REFERER', '/')
        i = web.input(redirect=referer)
        lf, sf= forms.loginform(), forms.signupform()
        lf.fill(i)
        sf['redirect'].value = sf['redirect'].value = i.redirect
        msg, msg_type = helpers.get_delete_msg()
        return render.login(lf, sf, msg, i.redirect)

    def POST(self):
        i = web.input(redirect='/')
        lf = forms.loginform()
        if not lf.validates(i):
            sf = forms.signupform()
            lf['redirect'].value = sf['redirect'].value = i.redirect
            lf.fill(i)
            return render.login(lf, sf, redirect=i.redirect)
        else:
            state = i.get('state')
            if state:
                state = json.loads(state)
                return internal_redirect(state['redirect'], state['method'], state['query'], state['data'])
            else:    
                raise web.seeother(i.redirect, absolute=True)

class logout:
    def GET(self):
        return render.logout()

    def POST(self):
        helpers.del_login_cookie()
        helpers.del_unverified_cookie()
        referer = web.ctx.env.get('HTTP_REFERER', '/')
        raise web.seeother(referer)

def get_secret_token(email, validity=7):
    valid_till = (datetime.date.today() + datetime.timedelta(validity)).isoformat()
    return '@'.join([valid_till, helpers.encrypt(email + valid_till)])

def check_secret_token(email, token):
    valid_till, enc_email_ts = token.split('@')
    tampered = helpers.encrypt(email + valid_till) != enc_email_ts
    def expired():
        today = datetime.date.today()
        valid_date = datetime.date(*[int(t) for t in valid_till.split('-')])
        return today > valid_date

    return not(tampered or expired())

def set_password_url(email, token):
    query = urllib.urlencode(dict(email=email, token=token))
    url = 'http://watchdog.net/u/set_password?%s' % (query)
    return url

class forgot_password:
    def GET(self, form=None):
        form = form or forms.forgot_password()
        msg, msg_type = helpers.get_delete_msg()
        return render.forgot_password(form, msg)

    def POST(self):
        i = web.input()
        form = forms.forgot_password()
        if form.validates(i):
            token = get_secret_token(i.email)
            reset_url = set_password_url(i.email, token)
            subject = 'Reset your watchdog.net password'
            msg = """\
You asked to reset your password on watchdog.net.
You can do so at:

%s

but you have to do it within the next 7 days.

Thanks,
watchdog.net
""" % (reset_url)
            web.sendmail(config.from_address, i.email, subject, msg )
            helpers.set_msg('Check your email to reset your password.')
            raise web.seeother('/u/forgot_password', absolute=True)
        else:
            return self.GET(form)

class set_password:
    def GET(self, form=None):
        i = web.input()
        email = i.get('email', '')
        email_exists = bool(helpers.get_user_by_email(email))
        if email_exists and check_secret_token(email, i.get('token', '@')):
            form = form or forms.passwordform()
            return render.set_password(form, i.email)
        else:
            if email_exists: msg = 'Invalid token'
            else: msg = 'No user account exists with this email'
            helpers.set_msg(msg, msg_type='error')
            raise web.seeother('/u/forgot_password', absolute=True)

    def POST(self):
        i = web.input()
        form = forms.passwordform()
        if form.validates(i):
            password = encrypt_password(i.password)
            db.update('users', password=password, verified=True, where='email=$i.email', vars=locals())
            helpers.set_login_cookie(i.email)
            helpers.set_msg('Password stored')
            raise web.seeother('/c/', absolute=True)
        else:
            return self.GET(form)

def send_mail_to_set_password(email):
    token = get_secret_token(email, validity=365)
    url = set_password_url(email, token)
    subject = 'Set your watchdog.net password'
    msg = """\
Thanks for using watchdog.net. We've created an account
for you with this email address -- but we don't have
a password for it. So that you can log in later, please
set your password at:

%s

If you've already set a password, then don't worry about
it and sorry for the interruption. If you think you received
this email in error, please hit reply and let us know.

Thanks,
watchdog.net
""" % (url)
    web.sendmail(config.from_address, email, subject, msg)

def assert_login(i=None):
    # let unlogged in users also do actions like signing, wyr
    # if the email has verified account with us but not logged-in, redirect to login form
    # if the email has unverified account, make them login #and send set password email
    # if the email has no account, set an unverified account #and send set password email
    # and return user id
    i = i or web.input()
    email = i.email
    if helpers.get_loggedin_email():
        uid = helpers.get_loggedin_userid()
    elif helpers.is_verified(email):
        login_page = do_login(email, set_state())
        raise web.webapi.HTTPError('200 OK', {}, data=str(login_page))
    else:
        #send_mail_to_set_password(email)
        uid = helpers.unverified_login(email, i.get('fname'), i.get('lname'))
    return uid

def set_state():
    if web.ctx.method == 'POST':
        data = web.data()
    else:
        data = None
    query = web.ctx.env['QUERY_STRING']
    redirect = web.ctx.homepath + web.ctx.path
    method = web.ctx.method
    return dict(redirect=redirect, query=query, method=method, data=data)
    
def do_login(email, state):
    lf, sf = forms.loginform(), forms.signupform()
    lf.fill(useremail=email, redirect=state['redirect'], state=json.dumps(state))
    sf.fill(redirect=state['redirect'], state=state)
    return render.login(lf, sf)
    
def require_login(f):
    def g(*a, **kw):
        if not helpers.get_loggedin_email():
            query = urllib.urlencode(dict(redirect=web.ctx.homepath + web.ctx.fullpath))
            raise web.seeother("/u/login?%s" % (query), absolute=True)
        return f(*a, **kw)
    return g

########NEW FILE########
__FILENAME__ = browser
"""
Browser: maintains state across multiple urlopens
"""

import urllib2, cookielib
from BeautifulSoup import BeautifulSoup
from ClientForm import ParseFile, ParseError, XHTMLCompatibleFormParser
from StringIO import StringIO

class Browser:
    def __init__(self, state=None):
        self.cp = urllib2.HTTPCookieProcessor()
        self.page = None
        self.url = None
        if state: self.set_state(state)
        
    def get_state(self):
        return [self._dump_cookie(c) for c in self._get_cookies(self.cp.cookiejar)]
      
    def set_state(self, state):
        cookies = state
        self._set_cookies(self.cp.cookiejar, [self._load_cookie(d) for d in cookies])
        
    def open(self, request, data=None):
        """opens the url or processes the request and returns the response"""
        response = urllib2.build_opener(self.cp).open(request, data)
        self.page = response.read()
        self.url = response.geturl()
        return self.page
        
    def get_forms(self, predicate=None):
        """Returns all the forms satisfying predicate."""
        try:
            forms = ParseFile(StringIO(self.page), self.url, backwards_compat=False)
        except ParseError:
            forms = ParseFile(StringIO(self.page), self.url, backwards_compat=False, \
                    form_parser_class=XHTMLCompatibleFormParser)
        return (f for f in forms if predicate is None or predicate(f))
    
    def get_form(self, predicate):
        try:
            return self.get_forms(predicate).next()
        except StopIteration:
            pass    
    
    def get_text(self):
        soup = BeautifulSoup(self.page)
        return ''.join(e.strip() for e in soup.recursiveChildGenerator() if isinstance(e, unicode))
    
    def has_text(self, msg):
        text = self.get_text()
        return msg.lower() in text.lower()
        
    def find_nodes(self, tags, predicate=None, attrs={}):
        """Finds matching nodes from the current page"""
        soup = BeautifulSoup(self.page)
        return [n for n in soup.findAll(tags, attrs) if predicate is None or predicate(n)]
        
    def _get_cookies(self, cookiejar):
        """returns all cookies in the cookiejar."""
        for domain, domain_cookies in cookiejar._cookies.items():
            for path, path_cookies in domain_cookies.items():
                for name, cookie in path_cookies.items():
                    yield cookie   

    def _dump_cookie(self, cookie):
        """convert a cookie to a dictionary."""
        d = dict(cookie.__dict__)
        d['rest'] = d.pop('_rest')
        return d

    def _set_cookies(self, cookiejar, cookies):
        """adds the given cookies to the cookie jar."""
        for cookie in cookies:
            cookiejar.set_cookie(cookie)

    def _load_cookie(self, data):
        """Creates a cookie from the dumped dict."""
        d = dict( [(str(k), v) for (k, v) in data.items()]) #keys are getting unicode values somewhere
        return cookielib.Cookie(**d)


########NEW FILE########
__FILENAME__ = captchasolver
#!/usr/bin/env python
# encoding: utf-8
"""
captchasolver.py

This module provides mathematic captcha solvers. 
Created by Pradeep Gowda on 2008-04-22.
Copyright (c) 2008 Watchdog.net. All rights reserved.
"""

import sys
import os
import operator 

def toint(num):
    '''
    >>> toint('Two')
    2
    '''
    nums = ['zero','one', 'two', 'three', 'four','five', 'six', 'seven', 'eight', 'nine', 'ten']
    try:
        return int(num)
    except:
        num = num.lower()
        try:
            return nums.index(num)
        except:
            pass

def toop(op):
    '''
    >>> toop('X')
    <built-in function mul>
    '''
    op= op.lower()
    if op == 'x' or op == '*':
        return operator.mul
    if op == '+': return operator.add
    if op == '-': return operator.sub
    if op == '/': return operator.div
        
def sumof(captcha):
    '''
    >>> sumof('What is the sum of 1 plus 1')
    2
    '''
    captcha = captcha.replace('?','')
    st = captcha.find('sum of') + len('sum of')+1
    vars = captcha[st:].split(' ')
    total = 0
    for i in vars:
        num = toint(i)
        if num: total += num
    return total

def mathprob(captcha):
    '''
    >>> mathprob("Please solve the following math problem : two x 1")
    2
    >>> mathprob("Please solve the following math problem : two + three")
    5
    >>> mathprob("Please solve the following math problem: three x one?")
    3
    ''' 
    captcha = captcha.rstrip('?')
    vars = captcha.split(':')[1].split(' ')
    vars = [v for v in vars if v]
    op = [v for v in vars if v in('+','-','/', 'X','x', '*')][0]
    vars.remove(op)
    vars = [toint(v) for v in vars]
    return reduce(toop(op), vars)
    

def beginning(captcha):
    '''
    >>> beginning("01 : What number appears at the beginning of this question?")
    '01'
    '''
    return captcha.split(':')[0].strip()
    
def largest(captcha):
    '''
    >>> largest("Which of the numbers is largest: 1,3,7,19,2 ?")
    19
    '''
    foo = captcha.split(':')[1]
    foo.replace('?', '')
    foo = foo.split(',')
    nums = []
    for i in foo:
        try:
            nums.append(int(i))
        except:
            pass
    return max(nums)
    
def nextnum(captcha):
    '''
    >>> nextnum('Please provide the next number in this sequence: 2, 3, 4, 5:')
    6
    '''
    seq = captcha.split(':')[1]
    nums = [int(i.strip()) for i in seq.split(',') if i]
    diff = nums[1] - nums[0]
    next = nums[-1]+ diff
    return next
    

def minus(captcha):
    '''
    >>> minus ("what is 1 minus 1?")
    0
    >>> solve("what is ten minus one?")
    9
    '''
    captcha = captcha.rstrip('?')
    pos = captcha.find('what is')
    vars = captcha[pos+len('what is'):].split(' ')
    vars = [toint(v) for v in vars if toint(v)]
    return reduce(operator.sub, vars)

def solve(captcha):
    '''
    >>> solve("what is 4 minus 1?")
    3
    >>> solve("what is 1 minus 3?")
    -2
    >>> largest("Which of the numbers is largest: 1,3,7,19,2 ?")
    19
    >>> sumof('What is the sum of 21 plus 23')
    44
    >>> solve("What is ten minus one?")
    9
    >>> solve("Please solve the following math problem: three x one?    ")
    3
    '''
    captcha = captcha.strip().lower()
    result = None

    for s,f in [('sum of', sumof), ('math problem', mathprob),\
                    ('largest', largest), ('beginning', beginning), \
                    ('next number', nextnum), ('minus', minus)]:
        if captcha.find(s) > -1:  result =  f(captcha)
    return result

def _test():
    import doctest
    doctest.testmod()

if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = doctester
#!/usr/bin/python
import doctest, sys
if __name__ == '__main__':
    sys.path.insert(0, '.')
    doctest.testmod(__import__(sys.argv[1], fromlist=True))

########NEW FILE########
__FILENAME__ = forms
import web
from web import form
from settings import db
from wyrutils import getdist
from auth import loginuser
import re

email_regex = r'[\w\.-]+@[\w\.-]+\.[a-zA-Z]{1,4}'
email_list_regex = r'^%s$|^(%s *, *)*(%s)?$' % (email_regex, email_regex, email_regex)
html_link_regex = r'.*<\s*a\s*href\s*=.*'

def petitionnotexists(pid):
    "Return True if petition with id `pid` does not exist"
    exists = bool(db.select('petition', where='id=$pid', vars=locals()))
    return pid != 'new' and not(exists)

def getstates():
    return [(s.code, s.name) for s in db.select('state', what='code, name', order='name')]

def no_html_link(comment):
    """don't allow HTML links in comments"""
    regx = re.compile(html_link_regex)
    return not bool(regx.match(comment))

class ZipValidator:
    def valid(self, i):
        dists = getdist(i.zip5, i.zip4, i.addr1+i.addr2)
        msg = ''
        if len(dists) > 1 and not i.zip4:
            msg = "Zipcode is shared between two districts. Enter zip4 too."
        elif len(dists) != 1:
            msg = "Couldn't find district for this address and zip"
        elif not dists[0].startswith(i.state):
            msg = "Zipcode and address doesn't fall in the selected state"
        web.ctx.zip_validator_msg = msg
        return not bool(msg)
    
    def get_msg(self):
        return web.ctx.get('zip_validator_msg', '')
    msg = property(get_msg)

def emailnotexists(email):
    "Return True if account with email `email` does not exist"
    exists = bool(db.select('users', where='email=$email', vars=locals()))
    return not(exists)
    
def check_len(phone):
    return len(web.numify(phone)) <= 15

petitionform = form.Form(
      form.Textbox('ptitle', form.Validator("Title can't be blank", bool), description="Title:", size='80'),
      form.Textbox('pid', form.Validator("Address can't be blank", bool), form.Validator('ID already exists, Choose a different one.', petitionnotexists),
                    pre='http://watchdog.net/c/', description='URL:', size='30'),
      form.Textarea('msg', form.Validator("Description can't be blank", bool), description="Description:", rows='15', cols='80'),
      form.Checkbox('tocongress', value='', description="Petition to Congress?"),
      form.Hidden('userid')
      )

wyrform = form.Form(
      form.Dropdown('prefix', ['Mr.', 'Mrs.', 'Dr.', 'Ms.', 'Miss'],  description='Prefix'),
      form.Textbox('lname', form.Validator("Last name can't be blank", bool), size='16', description='Last Name'),
      form.Textbox('fname', form.Validator("First name can't be blank", bool),  size='16', description='First Name'),
      form.Textbox('email', form.notnull, form.regexp(email_regex, 'Please enter a valid email'), description='Email', size='30'),
      form.Textbox('addr1', form.Validator("Address can't be blank", bool), description='Address', size='20'),
      form.Textbox('addr2', description='Address', size='20'),
      form.Textbox('city', form.Validator("City can't be blank", bool), description='City'),
      form.Dropdown('state', [(None, 'Select state')] + getstates(), form.Validator("State can't be blank", bool), description='State'),
      form.Textbox('zip5', form.Validator("Zip code can't be blank", bool), form.regexp(r'^[0-9]{5}$', 'Please enter a valid zip'),
                    size='5', maxlength='5', description='Zip'),
      form.Textbox('zip4', form.regexp(r'^$|[0-9]{4}', 'Please Enter a valid zip'),
                    size='4', maxlength='4',description=''),
      form.Textbox('phone', form.Validator("Phone can't be blank", bool), form.regexp(r'^[0-9-. ]*$', 'Please enter a valid phone number'), 
                    form.Validator('Please enter a valid phone number', check_len), size='15', description='Phone'),
      form.Textbox('ptitle', form.Validator("Title can't be blank", bool), description="Title:", size='80'),
      form.Textarea('msg', form.Validator("Description can't be blank", bool), description="Description:", rows='15', cols='80'),
      form.Hidden('captcha_env'),
      validators = [ZipValidator()]
      )

signform = form.Form(
    form.Textbox('fname', form.notnull, description='First Name:', post=' *', size='17'),
    form.Textbox('lname', form.notnull, description='Last Name:', post=' *', size='17'),
    form.Textbox('email',
            form.notnull,
            form.regexp(email_regex, 'Please enter a valid email'),
            description='Email:',
            post=' *',
            size='30'),
    form.Checkbox('share_with', value='off', description="Share my email with the author of this petition"),
    form.Textarea('comment', form.Validator("Comment cannot contain HTML links", no_html_link),
            description='Personal comment (explain how this affects you):', cols=70, rows=4)
    )

passwordform = form.Form(
    form.Password('password', form.notnull, description="Password:", size='10'),
    form.Password('password_again', form.notnull, description="Repeat:", size='10'),
    validators = [form.Validator("Passwords do not match.",
                    lambda i: i.password == i.password_again)]
    )

emailform = form.Form(
    form.Textarea('emails',
                form.notnull,
                form.regexp(email_list_regex, 'One or more emails are not valid'),
                description="To:",
                cols=70,
                rows=3,
                tabindex=1),
    form.Textbox('subject', form.notnull, description="Subject:", size='50', tabindex=2),
    form.Textarea('body', form.notnull, description="", cols=70, rows=12, tabindex=3)
    )

loadcontactsform = form.Form(
    form.Textbox('email',
            form.notnull,
            form.regexp(email_regex, 'Please enter a valid email'),
            description='Email:',
            size='20'),
    form.Dropdown('provider',
            [(None, 'Select Provider'),
            ('google', 'Google'),
            ('yahoo', 'Yahoo'),
            ('msn', 'MSN/Hotmail')],
            form.notnull,
            description='')
    )

signupform = form.Form(
    form.Textbox('email',
            form.notnull,
            form.regexp(email_regex, 'Please enter a valid email'),
            form.Validator('An account with that email already exists', emailnotexists),
            description='Email'),
    form.Password('password', form.notnull, description='Password'),
    form.Password('password_again', form.notnull, description='Password again'),
    form.Hidden('redirect'),
    form.Hidden('state'),
    validators = [form.Validator("Oops, passwords don't match", lambda i: i.password == i.password_again)]
    )

loginform = form.Form(
    form.Textbox('useremail',
            form.notnull,
            form.regexp(email_regex, 'Please enter a valid email'),
            form.Validator('No account exists with this email', lambda e: not emailnotexists(e)),
            description='Email'),
    form.Password('password', form.notnull, description='Password'),
    form.Hidden('redirect'),
    form.Hidden('state'),
    validators = [form.Validator('Oops, wrong email or password', lambda i: bool(loginuser(i.useremail, i.password)))]
    )

forgot_password = form.Form(
    form.Textbox('email',
            form.notnull,
            form.regexp(email_regex, 'Please enter a valid email'),
            description='Email:'
            ),
    validators = [form.Validator("No account exists with this email", lambda i: not emailnotexists(i.email))]
    )

userinfo = form.Form(
        form.Dropdown('prefix', [(None, 'Select'), 'Mr.', 'Mrs.', 'Dr.', 'Ms.', 'Miss'], description='Prefix'),
        form.Textbox('fname', description='First Name'),
        form.Textbox('lname', description='Last Name'),
        form.Textbox('addr1', description='Address Line1', size='20'),
        form.Textbox('addr2', description='Address Line2', size='20'),
        form.Textbox('city', description='City'),
        form.Dropdown('state', [(None, 'Select state')] + getstates(), description='State'),
        form.Textbox('zip5', form.regexp(r'^$|[0-9]{5}', 'Please enter a valid zip'),
                         size='5', maxlength='5', description='Zip'),
        form.Textbox('zip4', form.regexp(r'^$|[0-9]{4}', 'Please Enter a valid zip'),
                         size='4', maxlength='4', description='Zip4'),
        form.Textbox('phone', form.regexp(r'^[0-9-. ]*$', 'Please enter a valid phone number'),
                    form.Validator('Please enter a valid phone number', check_len), maxlength='15', description='Phone')
        )

change_password = form.Form(
        form.Password('password', form.notnull, description="New Password:", size='10'),
        form.Password('password_again', form.notnull, description="Confirm Password:", size='10'),
        validators = [form.Validator("Passwords do not match.", lambda f: f.password == f.password_again)]
    )

curr_password = form.Password('curr_password',
            form.notnull,
            description='Current Password:',
            size='10'
            )

########NEW FILE########
__FILENAME__ = helpers
import os
import hmac
import base64

import web
from config import secret_key
from settings import db

def urlify(s):
    """
    >>> urlify("What the !@#$%^ is going on here!?")
    'what-the--is-going-on-here'
    """
    s = s.lower()
    out = []
    for k in s:
        if k == " ": out.append('-')
        elif k.isalpha() or k.isdigit(): out.append(k)
    return ''.join(out)

def encrypt(msg, key=None):
    return hmac.new(key or secret_key, msg).hexdigest()

def setcookie(name, value, expires=''):
    encoded = value + '#@#' + encrypt(value)
    web.setcookie(name, encoded, expires)

def getcookie(name):
    encoded = web.cookies().get(name, '#@#')
    value, encrypt_value = encoded.split('#@#')
    if encrypt(value) == encrypt_value:
        return value

def deletecookie(name):
    web.setcookie(name, expires=-1)

def get_trackid(uid, pid):
    if not uid: return
    uid = str(uid)
    uid_pid = base64.urlsafe_b64encode(uid+pid[:10])
    return ':'.join([uid, uid_pid])

def check_trackid(tid, pid):
    try:
        uid, uid_pid = tid.split(':')
        uid_pid = base64.urlsafe_b64decode(str(uid_pid))
    except:
        return
    if uid_pid == uid + pid[:10]:
        return uid

def set_msg(msg, msg_type=None):
    if msg_type == 'error':
        msg += '$ERR$'
    elif msg_type == 'note':
        msg += '$NOTE$'    
    web.setcookie('wd_msg', msg)

def get_delete_msg():
    msg = web.cookies().get('wd_msg', None)
    web.setcookie('wd_msg', '', expires=-1)

    msg_type = None
    if msg:
        if msg.endswith('$ERR$'):
            msg_type = 'error'
            msg = msg[:-5]
        elif msg.endswith('$NOTE$'):
            msg_type = 'note'    
            msg = msg[:-6]
    return msg, msg_type

def get_loggedin_email():
    return getcookie('wd_login')

def get_unverified_email():
    return getcookie('wd_email')

def get_loggedin_userid():
    email = get_loggedin_email()
    user = get_user_by_email(email)
    return user and user.id
    
def get_unverified_userid():
    email = get_unverified_email()
    user = get_user_by_email(email)
    return user and user.id

def get_user_by_email(email):
    try:
        return db.select('users', where='email=$email', vars=locals())[0]
    except IndexError:
        return

def get_user_by_id(uid):
    try:
        return db.select('users', where='id=$uid', vars=locals())[0]
    except IndexError:
        return
    
def set_login_cookie(email):
    setcookie('wd_login', email)

def del_login_cookie():
    web.setcookie("wd_login", "", expires=-1)

def del_unverified_cookie():
    web.setcookie("wd_email", "", expires=-1)
    
def unverified_login(email, fname, lname):
    setcookie('wd_email', email)
    user = get_user_by_email(email)
    if user:
        return user.id
    return db.insert('users', fname=fname, lname=lname, email=email)

def is_verified(email):
    verified = db.select('users', where='email=$email and (verified=True or password is not null)', vars=locals())
    return bool(verified)

def query_param(param, default_value):
    d = {param:default_value}
    i = web.input(**d)
    return i.get(param)

def get_user():
    email = get_loggedin_email() or get_unverified_email()
    user = get_user_by_email(email)
    return user
    
def get_user_name():
    user = get_user()
    return (user.fname or user.email[:user.email.index('@')]) if user else ''

def format_name(name):
    if name.find(',') != -1:
      i = name.index(',')
      name = name[i+2:]+' '+name[0:i]
    return name

def date_range(dfrom, dto):
    if dfrom and dto and dfrom != dto:
        return 'from %s to %s' % (web.datestr(dfrom), web.datestr(dto))
    elif ((dfrom == dto) and dfrom) or dfrom or dto:
        return 'on %s' % web.datestr(dfrom or dto)
    else:
        return ''

eo_codes = web.storage(ded = {"1":"Deductible",
                    "2":"Not deductible",
                    "3":"Deductible by treaty"},

             foundation = {"00":"All organizations except 501(c)(3)",
                           "02":"Private operating foundation exempt from paying excise taxes on investment income",
                           "03":"Private operating foundation (other)",
                           "04":"Private non-operating foundation",
                           "09":"Suspense",
                           "10":"Church 170(b)(1)(A)(i)",
                           "11":"School 170(b)(1)(A)(ii)",
                           "12":"Hospital or medical research organization 170(b)(1)(A)(iii)",
                           "13":"Organization which operates for benefit of college or university and is owned or operated by a governmental unit 170(b)(1)(A)(iv)",
                           "14":"Governmental unit 170(b)(1)(A)(v)",
                           "15":"Organization which receives a substantial part of its support from a governmental unit or the general public 170(b)(1)(A)(vi)",
                           "16":"Organization that normally receives no more than one third of its support from gross investment income and unrelated business income and at the same time more than one third of its support from contributions, fees, and gross receipts related to exempt purposes. 509(a)(2)",
                           "17":"Organizations operated solely for the benefit of and in conjunction with organizations described in 10 through 16 above. 509(a)(3)",
                           "18":"Organization organized and operated to test for public safety509(a)(4)",
                           },

             category = {("01","1"):"Government Instrumentality",
                         ("02","1"):"Title Holding Corporation",
                         ("03","1"):"Charitable Organization",
                         ("03","2"):"Educational Organization",
                         ("03","3"):"Literary Organization",
                         ("03","4"):"Organization to Prevent Cruelty to Animals",
                         ("03","5"):"Organization to Prevent Cruelty to Children",
                         ("03","6"):"Organization for Public Safety Testing",
                         ("03","7"):"Religious Organization",
                         ("03","8"):"Scientific Organization",
                         ("04","1"):"Civic League",
                         ("04","2"):"Local Association of Employees",
                         ("04","3"):"Social Welfare Organization",
                         ("05","1"):"Agricultural Organization",
                         ("05","2"):"Horticultural Organization",
                         ("05","3"):"Labor Organization",
                         ("06","1"):"Board of Trade",
                         ("06","2"):"Business League",
                         ("06","3"):"Chamber of Commerce",
                         ("06","4"):"Real Estate Board",
                         ("07","1"):"Pleasure, Recreational, or Social Club",
                         ("08","1"):"Fraternal Beneficiary Society, Order or Association",
                         ("09","1"):"Voluntary Employees' Beneficiary Association (Non-Govt. Emps.)",
                         ("09","2"):"Voluntary Employees' Beneficiary Association (Govt. Emps.)",
                         ("10","1"):"Domestic Fraternal Societies and Associations",
                         ("11","1"):"Teachers Retirement Fund Assoc.",
                         ("12","1"):"Benevolent Life Insurance Assoc.",
                         ("12","2"):"Mutual Ditch or Irrigation Co.",
                         ("12","3"):"Mutual Cooperative Telephone Co.",
                         ("12","4"):"Organization Like Those on Three Preceding Lines",
                         ("13","1"):"Burial Association",
                         ("13","2"):"Cemetery Company",
                         ("14","1"):"Credit Union",
                         ("14","2"):"Other Mutual Corp. or Assoc.",
                         ("15","1"):"Mutual Insurance Company or Assoc. Other Than Life or Marine",
                         ("16","1"):"Corp. Financing Crop Operations",
                         ("17","1"):"Supplemental Unemployment Compensation Trust or Plan",
                         ("18","1"):"Employee Funded Pension Trust (Created Before 6/25/59)",
                         ("19","1"):"Post or Organization of War Veterans",
                         ("20","1"):"Legal Service Organization",
                         ("21","1"):"Black Lung Trust",
                         ("22","1"):"Multiemployer Pension Plan",
                         ("23","1"):"Veterans Assoc. Formed Prior to 1880",
                         ("24","1"):"Trust Described in Sect. 4049 of ERISA",
                         ("25","1"):"Title Holding Co. for Pensions, etc.",
                         ("26","1"):"State-Sponsored High Risk Health Insurance Organizations",
                         ("27","1"):"State-Sponsored Workers' Compensation Reinsurance",
                         ("40","1"):"Apostolic and Religious Org. (501(d))",
                         ("50","1"):"Cooperative Hospital Service Organization (501(e))",
                         ("60","1"):"Cooperative Service Organization of Operating Educational Organization (501(f))",
                         ("70","1"):"Child Care Organization (501(k))",
                         ("71","1"):"Charitable Risk Pool",
                         ("81","1"):"Qualified State-Sponsored Tuition Program ",
                         ("92","1"):"4947(a)(1) - Private Foundation (Form 990PF Filer)"},
             affiliation = {"1":"Central",
                            "2":"Intermediate",
                            "3":"Independent",
                            "6":"Central",
                            "7":"Intermediate",
                            "8":"Central",
                            "9":"Subordinate"},
             activity = {"000":"", #000 is NOT an officially defined code but is used in the records to indicate a NULL value
                         "001":"Church, synagogue, etc",
                         "002":"Association or convention of  churches",
                         "003":"Religious order",
                         "004":"Church auxiliary",
                         "005":"Mission",
                         "006":"Missionary activities",
                         "007":"Evangelism",
                         "008":"Religious publishing activities",
                         "029":"Other religious activities",
                         "030":"School, college, trade school, etc.",
                         "031":"Special school for the blind, handicapped, etc",
                         "032":"Nursery school",
                         "033":"Faculty group",
                         "034":"Alumni association or group",
                         "035":"Parent or parent teachers association",
                         "036":"Fraternity or sorority",
                         "037":"Other student society or group",
                         "038":"School or college athletic association",
                         "039":"Scholarships for children of employees",
                         "040":"Scholarships (other)",
                         "041":"Student loans",
                         "042":"Student housing activities",
                         "043":"Other student aid",
                         "044":"Student exchange with foreign country",
                         "045":"Student operated business",
                         "046":"Private school",
                         "059":"Other school related activities",
                         "060":"Museum, zoo, planetarium, etc.",
                         "061":"Library",
                         "062":"Historical site, records or reenactment",
                         "063":"Monument",
                         "064":"Commemorative event (centennial, festival, pageant, etc.)",
                         "065":"Fair",
                         "088":"Community theatrical group",
                         "089":"Singing society or group",
                         "090":"Cultural performances",
                         "091":"Art exhibit",
                         "092":"Literary activities",
                         "093":"Cultural exchanges with foreign country",
                         "094":"Genealogical activities",
                         "119":"Other cultural or historical activities",
                         "120":"Publishing activities",
                         "121":"Radio or television broadcasting",
                         "122":"Producing films",
                         "123":"Discussion groups, forums, panels lectures, etc.",
                         "124":"Study and research (nonscientific)",
                         "125":"Giving information or opinion (see also Advocacy)",
                         "126":"Apprentice training",
                         "149":"Other instruction and training",
                         "150":"Hospital",
                         "151":"Hospital auxiliary",
                         "152":"Nursing or convalescent home",
                         "153":"Care and housing for the aged (see also 382)",
                         "154":"Health clinic",
                         "155":"Rural medical facility",
                         "156":"Blood bank",
                         "157":"Cooperative hospital service organization",
                         "158":"Rescue and emergency service",
                         "159":"Nurses register or bureau",
                         "160":"Aid to the handicapped (see also 031)",
                         "161":"Scientific research (diseases)",
                         "162":"Other medical research",
                         "163":"Health insurance (medical, dental, optical, etc.)",
                         "164":"Prepared group health plan",
                         "165":"Community health planning",
                         "166":"Mental health care",
                         "167":"Group medical practice association",
                         "168":"Infaculty group practice association",
                         "169":"Hospital pharmacy, parking facility, food services, etc.",
                         "179":"Other health services",
                         "180":"Contact or sponsored scientific research for industry",
                         "181":"Scientific research for government",
                         "199":"Other scientific research activities",
                         "200":"Business promotion (chamber of commerce, business league, etc.	",
                         "201":"Real estate association",
                         "202":"Board of trade",
                         "203":"Regulating business",
                         "204":"Promotion of fair business practices",
                         "205":"Professional association",
                         "206":"Professional association auxiliary",
                         "207":"Industry trade shows",
                         "208":"Convention displays",
                         "209":"Research, development and testing",
                         "210":"Professional athletic league",
                         "211":"Underwriting municipal insurance",
                         "212":"Assigned risk insurance activities",
                         "213":"Tourist bureau",
                         "229":"Other business or professional group",
                         "230":"Farming",
                         "231":"Farm bureau",
                         "232":"Agricultural group",
                         "233":"Horticultural group",
                         "234":"Farmers cooperative marketing or purchasing",
                         "235":"Farmers cooperative marketing or purchasing",
                         "236":"Dairy herd improvement association",
                         "237":"Breeders association",
                         "249":"Other farming and related activities",
                         "250":"Mutual ditch, irrigation, telephone, electric company or like organization",
                         "251":"Credit union",
                         "252":"Reserve funds or insurance for domestic building and loan association, cooperative bank, or mutual savings bank",
                         "253":"Mutual insurance company",
                         "254":"Corporation organized under an Act of Congress (see also use (904)",
                         "259":"Other mutual organization",
                         "260":"Fraternal Beneficiary society, order, or association",
                         "261":"Improvement of conditions of workers",
                         "262":"Association of municipal employees",
                         "263":"Association of employees",
                         "264":"Employee or member welfare association",
                         "265":"Sick, accident, death, or similar benefits",
                         "266":"Strike benefits",
                         "267":"Unemployment benefits",
                         "268":"Pension or retirement benefits",
                         "269":"Vacation benefits",
                         "279":"Other services or benefits to members or employees",
                         "280":"Country club",
                         "281":"Hobby club",
                         "282":"Dinner club",
                         "283":"Variety club",
                         "284":"Dog club",
                         "285":"Women's club",
                         "286":"Hunting or fishing club",
                         "287":"Swimming or tennis club",
                         "288":"Other sports club",
                         "296":"Community center",
                         "297":"Community recreational facilities (park, playground, etc)",
                         "298":"Training in sports",
                         "299":"Travel tours",
                         "300":"Amateur athletic association",
                         "301":"Fundraising athletic or sports event",
                         "317":"Other sports or athletic activities",
                         "318":"Other recreational activities",
                         "319":"Other social activities",
                         "320":"Boy Scouts, Girl Scouts, etc.",
                         "321":"Boys Club, Little League, etc.",
                         "322":"FFA, FHA, 4H club, etc.",
                         "323":"Key club",
                         "324":"YMCA, YWCA, YMCA, etc.",
                         "325":"Camp",
                         "326":"Care and housing of children (orphanage, etc)",
                         "327":"Prevention of cruelty to children",
                         "328":"Combat juvenile delinquency",
                         "349":"Other youth organization or activities",
                         "350":"Preservation of natural resources (conservation)",
                         "351":"Combating or preventing pollution (air, water, etc)",
                         "352":"Land acquisition for preservation",
                         "353":"Soil or water conservation",
                         "354":"Preservation of scenic beauty",
                         "355":"Wildlife sanctuary or refuge",
                         "356":"Garden club",
                         "379":"Other conservation, environmental or beautification activities",
                         "380":"Lowincome housing",
                         "381":"Low and moderate income housing",
                         "382":"Housing for the aged (see also 153)",
                         "398":"Instruction and guidance on housing",
                         "399":"Other housing activities",
                         "400":"Area development, redevelopment of renewal",
                         "401":"Homeowners association",
                         "402":"Other activity aimed t combating community deterioration",
                         "403":"Attracting new industry or retaining industry in an area",
                         "404":"Community promotion",
                         "405":"Loans or grants for minority businesses",
                         "406":"Crime prevention",
                         "407":"Voluntary firemen's organization or auxiliary",
                         "408":"Community service organization",
                         "429":"Other inner city or community benefit activities",
                         "430":"Defense of human and civil rights",
                         "431":"Elimination of prejudice and discrimination (race, religion, sex, national origin, etc)",
                         "432":"Lessen neighborhood tensions",
                         "449":"Other civil rights activities",
                         "460":"Public interest litigation activities",
                         "461":"Other litigation or support of litigation",
                         "462":"Legal aid to indigents",
                         "463":"Providing bail",
                         "465":"Plan under IRC section 120",
                         "480":"Propose, support, or oppose legislation",
                         "481":"Voter information on issues or candidates",
                         "482":"Voter education (mechanics of registering, voting etc.)",
                         "483":"Support, oppose, or rate political candidates",
                         "484":"Provide facilities or services for political campaign activities",
                         "509":"Other legislative and political activities",
                         "510":"Firearms control",
                         "511":"Selective Service System",
                         "512":"National defense policy",
                         "513":"Weapons systems",
                         "514":"Government spending",
                         "515":"Taxes or tax exemption",
                         "516":"Separation of church and state",
                         "517":"Government aid to parochial schools",
                         "518":"U.S. foreign policy",
                         "519":"U.S. military involvement",
                         "520":"Pacifism and peace",
                         "521":"Economicpolitical system of U.S.",
                         "522":"Anticommunism",
                         "523":"Right to work",
                         "524":"Zoning or rezoning",
                         "525":"Location of highway or transportation system",
                         "526":"Rights of criminal defendants",
                         "527":"Capital punishment",
                         "528":"Stricter law enforcement",
                         "529":"Ecology or conservation",
                         "530":"Protection of consumer interests",
                         "531":"Medical care service",
                         "532":"Welfare systems",
                         "533":"Urban renewal",
                         "534":"Busing student to achieve racial balance",
                         "535":"Racial integration",
                         "536":"Use of intoxicating beverage",
                         "537":"Use of drugs or narcotics",
                         "538":"Use of tobacco",
                         "539":"Prohibition of erotica",
                         "540":"Sex education in public schools",
                         "541":"Population control",
                         "542":"Birth control methods",
                         "543":"Legalized abortion",
                         "559":"Other matters",
                         "560":"Supplying money, goods or services to the poor",
                         "561":"Gifts or grants to individuals (other than scholarships)",
                         "562":"Other loans to individuals",
                         "563":"Marriage counseling",
                         "564":"Family planning",
                         "565":"Credit counseling an assistance",
                         "566":"Job training, counseling, or assistance",
                         "567":"Draft counseling",
                         "568":"Vocational counseling",
                         "569":"Referral service (social agencies)",
                         "572":"Rehabilitating convicts or exconvicts",
                         "573":"Rehabilitating alcoholics, drug abusers, compulsive gamblers, etc.",
                         "574":"Day care center",
                         "575":"Services for the aged (see also 153 ad 382)",
                         "600":"Community Chest, United Way, etc.",
                         "601":"Booster club",
                         "602":"Gifts, grants, or loans to other organizations",
                         "603":"Nonfinancial services of facilities to other organizations",
                         "900":"Cemetery or burial activities",
                         "901":"Perpetual (care fund (cemetery, columbarium, etc)",
                         "902":"Emergency or disaster aid fund",
                         "903":"Community trust or component",
                         "904":"Government instrumentality or agency (see also 254)",
                         "905":"Testing products for public safety",
                         "906":"Consumer interest group",
                         "907":"Veterans activities",
                         "908":"Patriotic activities",
                         "909":"Non-exempt charitable trust described in section 4947(a)(1) of the Code",
                         "910":"Domestic organization with activities outside U.S.",
                         "911":"Foreign organization",
                         "912":"Title holding corporation",
                         "913":"Prevention of cruelty to animals",
                         "914":"Achievement pries of awards",
                         "915":"Erection or maintenance of public building or works",
                         "916":"Cafeteria, restaurant, snack bar, food services, etc.",
                         "917":"Thrift ship, retail outlet, etc.",
                         "918":"Book, gift  or supply store",
                         "919":"Advertising",
                         "920":"Association of employees",
                         "921":"Loans or credit reporting",
                         "922":"Endowment fund or financial services",
                         "923":"Indians (tribes, cultures, etc.)",
                         "924":"Traffic or tariff bureau",
                         "925":"Section 501(c)(1) with 50% deductibility",
                         "926":"Government instrumentality other than section 501(c)",
                         "927":"Fundraising",
                         "928":"4947(a)(2) trust",
                         "930":"Prepaid legal services pan exempt under IRC section 501(c)(20)",
                         "931":"Withdrawal liability payment fund",
                         "990":"Section 501(k) child care organization",
                         "994":"Described in section 170(b)1)(a)(vi) of the Code",
                         "995":"Described in section 509(a)(2) of the Code",
                         "998":"Denied or failed to establish it's exempt status.  Will be updated when status is granted"},
             org = {"1":"Corporation",
                    "2":"Trust",
                    "3":"Cooperative",
                    "4":"Partnership",
                    "5":"Association"},
             exempt = { "01":"Unconditional Exemption",
                        "02":"Conditional Exemption",
                        "12":"Trust described in section 4947(a)(2) of the IR Code",
                        "25":"Organization terminating its private foundation status under section507(b)(1)(B) of the Code",
                        "32":"Organization that did not respond to an IRS CP 140 notice requesting information on its continued exempt status",
                        },
             income = {"0":"0",
                       "1":"1   to   9,999",
                       "2":"10,000   to   24,999",
                       "3":"25,000   to   99,999",
                       "4":"100,000   to   499,999",
                       "5":"500,000   to   999,999",
                       "6":"1,000,000   to   4,999,999",
                       "7":"5,000,000   to   9,999,999",
                       "8":"10,000,000   to   49,999,999",
                       "9":"50,000,000   and   greater"
                       },

             fr1 = {"03":"990 - Group return",
                    "07":"990 - Government 501(c)(1)",
                    "01":"990 (all other) or 990EZ return",
                    "02":"990 - Not required to file Form 990 (income less than $25,000)",
                    "06":"990 - Not required to file (church) ",
                    "13":"990 - Not required to file (religious organization)",
                    "14":"990 - Not required to file (instrumentalities of states or political subdivisions)",
                    "00":"990 - Not required to file(all other)"
                    },

             fr2 = {"1":"990-PF return",
                    "0":"No 990-PF return"
                    },
            ntee = {"A01":"Alliance/Advocacy Organizations",
                    "A02":"Management & Technical Assistance",
                    "A03":"Professional Societies, Associations ",
                    "A05":"Research Institutes and/or Public Policy Analysis",
                    "A11":"Single Organization Support",
                    "A12":"Fund Raising and/or Fund Distribution",
                    "A19":"Nonmonetary Support N.E.C.*",
                    "A20":"Arts, Cultural Organizations - Multipurpose",
                    "A23":"Cultural, Ethnic Awareness",
                    "A25":"Arts Education",
                    "A26":"Arts Council/Agency",
                    "A30":"Media, Communications Organizations",
                    "A31":"Film, Video",
                    "A32":"Television",
                    "A33":"Printing, Publishing",
                    "A34":"Radio",
                    "A40":"Visual Arts Organizations",
                    "A50":"Museum, Museum Activities",
                    "A51":"Art Museums",
                    "A52":"Children's Museums",
                    "A54":"History Museums",
                    "A56":"Natural History, Natural Science Museums",
                    "A57":"Science and Technology Museums",
                    "A60":"Performing Arts Organizations",
                    "A61":"Performing Arts Centers",
                    "A62":"Dance",
                    "A63":"Ballet",
                    "A65":"Theater",
                    "A68":"Music",
                    "A69":"Symphony Orchestras",
                    "A6A":"Opera",
                    "A6B":"Singing, Choral",
                    "A6C":"Music Groups, Bands, Ensembles",
                    "A6E":"Performing Arts Schools",
                    "A70":"Humanities Organizations	 ",
                    "A80":"Historical Societies, Related Historical Activities",
                    "A84":"Commemorative Events",
                    "A90":"Arts Service Organizations and Activities",
                    "A99":"Arts, Culture, and Humanities N.E.C.",
                    "B01":"Alliance/Advocacy Organizations ",
                    "B02":"Management & Technical Assistance ",
                    "B03":"Professional Societies, Associations ",
                    "B05":"Research Institutes and/or Public Policy Analysis ",
                    "B11":"Single Organization Support ",
                    "B12":"Fund Raising and/or Fund Distribution ",
                    "B19":"Nonmonetary Support N.E.C.",
                    "B20":"Elementary, Secondary Education, K - 12",
                    "B21":"Kindergarten, Preschool, Nursery School, Early Admissions",
                    "B24":"Primary, Elementary Schools",
                    "B25":"Secondary, High School",
                    "B28":"Specialized Education Institutions",
                    "B30":"Vocational, Technical Schools",
                    "B40":"Higher Education Institutions",
                    "B41":"Community or Junior Colleges",
                    "B42":"Undergraduate College (4-year)",
                    "B43":"University or Technological Institute",
                    "B50":"Graduate, Professional Schools (Separate Entities)",
                    "B60":"Adult, Continuing Education",
                    "B70":"Libraries",
                    "B80":"Student Services, Organizations of Students",
                    "B82":"Scholarships, Student Financial Aid Services, Awards",
                    "B83":"Student Sororities, Fraternities",
                    "B84":"Alumni Associations",
                    "B90":"Educational Services and Schools - Other",
                    "B92":"Remedial Reading, Reading Encouragement",
                    "B94":"Parent/Teacher Group",
                    "B99":"Education N.E.C.",
                    "C01":"Alliance/Advocacy Organizations ",
                    "C02":"Management & Technical Assistance ",
                    "C03":"Professional Societies, Associations ",
                    "C05":"Research Institutes and/or Public Policy Analysis ",
                    "C11":"Single Organization Support ",
                    "C12":"Fund Raising and/or Fund Distribution ",
                    "C19":"Nonmonetary Support N.E.C.",
                    "C20":"Pollution Abatement and Control Services",
                    "C27":"Recycling Programs",
                    "C30":"Natural Resources Conservation and Protection",
                    "C32":"Water Resource, Wetlands Conservation and Management",
                    "C34":"Land Resources Conservation",
                    "C35":"Energy Resources Conservation and Development",
                    "C36":"Forest Conservation",
                    "C40":"Botanical, Horticultural, and Landscape Services",
                    "C41":"Botanical Gardens, Arboreta and Botanical Organizations",
                    "C42":"Garden Club, Horticultural Program",
                    "C50":"Environmental Beautification and Aesthetics",
                    "C60":"Environmental Education and Outdoor Survival Programs",
                    "C99":"Environmental Quality, Protection, and Beautification N.E.C.",
                    "D01":"Alliance/Advocacy Organizations ",
                    "D02":"Management & Technical Assistance ",
                    "D03":"Professional Societies, Associations ",
                    "D05":"Research Institutes and/or Public Policy Analysis ",
                    "D11":"Single Organization Support ",
                    "D12":"Fund Raising and/or Fund Distribution ",
                    "D19":"Nonmonetary Support N.E.C.",
                    "D20":"Animal Protection and Welfare",
                    "D30":"Wildlife Preservation, Protection",
                    "D31":"Protection of Endangered Species",
                    "D32":"Bird Sanctuary, Preserve",
                    "D33":"Fisheries Resources",
                    "D34":"Wildlife Sanctuary, Refuge",
                    "D40":"Veterinary Services",
                    "D50":"Zoo, Zoological Society",
                    "D60":"Other Services - Specialty Animals",
                    "D61":"Animal Training, Behavior",
                    "D99":"Animal-Related N.E.C.",
                    "E01":"Alliance/Advocacy Organizations ",
                    "E02":"Management & Technical Assistance ",
                    "E03":"Professional Societies, Associations ",
                    "E05":"Research Institutes and/or Public Policy Analysis ",
                    "E11":"Single Organization Support ",
                    "E12":"Fund Raising and/or Fund Distribution ",
                    "E19":"Nonmonetary Support N.E.C.",
                    "E20":"Hospitals and Related Primary Medical Care Facilities",
                    "E21":"Community Health Systems",
                    "E22":"Hospital, General",
                    "E24":"Hospital, Specialty",
                    "E30":"Health Treatment Facilities, Primarily Outpatient",
                    "E31":"Group Health Practice (Health Maintenance Organizations)",
                    "E32":"Ambulatory Health Center, Community Clinic",
                    "E40":"Reproductive Health Care Facilities and Allied Services",
                    "E42":"Family Planning Centers",
                    "E50":"Rehabilitative Medical Services",
                    "E60":"Health Support Services",
                    "E61":"Blood Supply Related",
                    "E62":"Ambulance, Emergency Medical Transport Services",
                    "E65":"Organ and Tissue Banks",
                    "E70":"Public Health Program (Includes General Health and Wellness PromotionServices)",
                    "E80":"Health, General and Financing",
                    "E86":"Patient Services - Entertainment, Recreation",
                    "E90":"Nursing Services (General)",
                    "E91":"Nursing, Convalescent Facilities",
                    "E92":"Home Health Care",
                    "E99":"Health - General and Rehabilitative N.E.C.",
                    "F01":"Alliance/Advocacy Organizations ",
                    "F02":"Management & Technical Assistance ",
                    "F03":"Professional Societies, Associations ",
                    "F05":"Research Institutes and/or Public Policy Analysis ",
                    "F11":"Single Organization Support ",
                    "F12":"Fund Raising and/or Fund Distribution ",
                    "F19":"Nonmonetary Support N.E.C.",
                    "F20":"Alcohol, Drug and Substance Abuse, Dependency Prevention and Treatment",
                    "F21":"Alcohol, Drug Abuse, Prevention Only",
                    "F22":"Alcohol, Drug Abuse, Treatment Only",
                    "F30":"Mental Health Treatment - Multipurpose and N.E.C.",
                    "F31":"Psychiatric, Mental Health Hospital",
                    "F32":"Community Mental Health Center",
                    "F33":"Group Home, Residential Treatment Facility - Mental Health Related",
                    "F40":"Hot Line, Crisis Intervention Services",
                    "F42":"Rape Victim Services",
                    "F50":"Addictive Disorders N.E.C.",
                    "F52":"Smoking Addiction",
                    "F53":"Eating Disorder, Addiction",
                    "F54":"Gambling Addiction",
                    "F60":"Counseling, Support Groups",
                    "F70":"Mental Health Disorders",
                    "F80":"Mental Health Association, Multipurpose",
                    "F99":"Mental Health, Crisis Intervention N.E.C.",
                    "G01":"Alliance/Advocacy Organizations ",
                    "G02":"Management & Technical Assistance ",
                    "G03":"Professional Societies, Associations ",
                    "G05":"Research Institutes and/or Public Policy Analysis ",
                    "G11":"Single Organization Support ",
                    "G12":"Fund Raising and/or Fund Distribution ",
                    "G19":"Nonmonetary Support N.E.C.",
                    "G20":"Birth Defects and Genetic Diseases",
                    "G25":"Down Syndrome",
                    "G30":"Cancer",
                    "G40":"Diseases of Specific Organs",
                    "G41":"Eye Diseases, Blindness and Vision Impairments",
                    "G42":"Ear and Throat Diseases",
                    "G43":"Heart and Circulatory System Diseases, Disorders",
                    "G44":"Kidney Disease",
                    "G45":"Lung Disease",
                    "G48":"Brain Disorders",
                    "G50":"Nerve, Muscle and Bone Diseases",
                    "G51":"Arthritis",
                    "G54":"Epilepsy",
                    "G60":"Allergy Related Diseases",
                    "G61":"Asthma",
                    "G70":"Digestive Diseases, Disorders",
                    "G80":"Specifically Named Diseases",
                    "G81":"AIDS",
                    "G83":"Alzheimer's Disease",
                    "G84":"Autism",
                    "G90":"Medical Disciplines",
                    "G92":"Biomedicine, Bioengineering ",
                    "G94":"Geriatrics",
                    "G96":"Neurology, Neuroscience",
                    "G98":"Pediatrics",
                    "G9B":"Surgery",
                    "G99":"Diseases, Disorders, Medical Disciplines N.E.C.",
                    "H01":"Alliance/Advocacy Organizations ",
                    "H02":"Management & Technical Assistance ",
                    "H03":"Professional Societies, Associations ",
                    "H05":"Research Institutes and/or Public Policy Analysis ",
                    "H11":"Single Organization Support ",
                    "H12":"Fund Raising and/or Fund Distribution ",
                    "H19":"Nonmonetary Support N.E.C.",
                    "H20":"Birth Defects, Genetic Diseases Research",
                    "H25":"Down Syndrome Research",
                    "H30":"Cancer Research",
                    "H40":"Specific Organ Research",
                    "H41":"Eye Research",
                    "H42":"Ear and Throat Research",
                    "H43":"Heart, Circulatory Research",
                    "H44":"Kidney Research",
                    "H45":"Lung Research",
                    "H48":"Brain Disorders Research",
                    "H50":"Nerve, Muscle, Bone Research",
                    "H51":"Arthritis Research",
                    "H54":"Epilepsy Research",
                    "H60":"Allergy Related Disease Research",
                    "H61":"Asthma Research",
                    "H70":"Digestive Disease, Disorder Research",
                    "H80":"Specifically Named Diseases Research",
                    "H81":"AIDS Research",
                    "H83":"Alzheimer's Disease Research",
                    "H84":"Autism Research",
                    "H90":"Medical Specialty Research",
                    "H92":"Biomedicine, Bioengineering Research",
                    "H94":"Geriatrics Research",
                    "H96":"Neurology, Neuroscience Research",
                    "H98":"Pediatrics Research",
                    "H9B":"Surgery Research",
                    "H99":"Medical Research N.E.C.",
                    "I01":"Alliance/Advocacy Organizations ",
                    "I02":"Management & Technical Assistance ",
                    "I03":"Professional Societies, Associations ",
                    "I05":"Research Institutes and/or Public Policy Analysis ",
                    "I11":"Single Organization Support ",
                    "I12":"Fund Raising and/or Fund Distribution ",
                    "I19":"Nonmonetary Support N.E.C.",
                    "I20":"Crime Prevention N.E.C.",
                    "I21":"Delinquency Prevention",
                    "I23":"Drunk Driving Related",
                    "I30":"Correctional Facilities N.E.C.",
                    "I31":"Transitional Care, Half-Way House for Offenders, Ex-Offenders",
                    "I40":"Rehabilitation Services for Offenders",
                    "I43":"Services to Prisoners and Families - Multipurpose",
                    "I44":"Prison Alternatives",
                    "I50":"Administration of Justice, Courts",
                    "I51":"Dispute Resolution, Mediation Services",
                    "I60":"Law Enforcement Agencies (Police Departments)",
                    "I70":"Protection Against, Prevention of Neglect, Abuse, Exploitation",
                    "I71":"Spouse Abuse, Prevention of",
                    "I72":"Child Abuse, Prevention of",
                    "I73":"Sexual Abuse, Prevention of",
                    "I80":"Legal Services",
                    "I83":"Public Interest Law, Litigation",
                    "I99":"Crime, Legal Related N.E.C.",
                    "J01":"Alliance/Advocacy Organizations ",
                    "J02":"Management & Technical Assistance ",
                    "J03":"Professional Societies, Associations ",
                    "J05":"Research Institutes and/or Public Policy Analysis ",
                    "J11":"Single Organization Support ",
                    "J12":"Fund Raising and/or Fund Distribution ",
                    "J19":"Nonmonetary Support N.E.C.",
                    "J20":"Employment Procurement Assistance, Job Training",
                    "J21":"Vocational Counseling, Guidance and Testing",
                    "J22":"Vocational Training",
                    "J30":"Vocational Rehabilitation",
                    "J32":"Goodwill Industries",
                    "J33":"Sheltered Remunerative Employment, Work Activity Center N.E.C.",
                    "J40":"Labor Unions, Organizations",
                    "J99":"Employment, Job Related N.E.C.",
                    "K01":"Alliance/Advocacy Organizations ",
                    "K02":"Management & Technical Assistance ",
                    "K03":"Professional Societies, Associations ",
                    "K05":"Research Institutes and/or Public Policy Analysis ",
                    "K11":"Single Organization Support ",
                    "K12":"Fund Raising and/or Fund Distribution ",
                    "K19":"Nonmonetary Support N.E.C.",
                    "K20":"Agricultural Programs",
                    "K25":"Farmland Preservation",
                    "K26":"Livestock Breeding, Development, Management",
                    "K28":"Farm Bureau, Grange",
                    "K30":"Food Service, Free Food Distribution Programs",
                    "K31":"Food Banks, Food Pantries",
                    "K34":"Congregate Meals",
                    "K35":"Eatery, Agency, Organization Sponsored",
                    "K36":"Meals on Wheels",
                    "K40":"Nutrition Programs",
                    "K50":"Home Economics",
                    "K99":"Food, Agriculture, and Nutrition N.E.C.",
                    "L01":"Alliance/Advocacy Organizations ",
                    "L02":"Management & Technical Assistance ",
                    "L03":"Professional Societies, Associations ",
                    "L05":"Research Institutes and/or Public Policy Analysis ",
                    "L11":"Single Organization Support ",
                    "L12":"Fund Raising and/or Fund Distribution ",
                    "L19":"Nonmonetary Support N.E.C.",
                    "L20":"Housing Development, Construction, Management",
                    "L21":"Public Housing Facilities",
                    "L22":"Senior Citizens' Housing/Retirement Communities",
                    "L25":"Housing Rehabilitation",
                    "L30":"Housing Search Assistance",
                    "L40":"Low-Cost Temporary Housing",
                    "L41":"Homeless, Temporary Shelter For",
                    "L50":"Housing Owners, Renters Organizations",
                    "L80":"Housing Support Services -- Other",
                    "L81":"Home Improvement and Repairs",
                    "L82":"Housing Expense Reduction Support",
                    "L99":"Housing, Shelter N.E.C.",
                    "M01":"Alliance/Advocacy Organizations ",
                    "M02":"Management & Technical Assistance ",
                    "M03":"Professional Societies, Associations ",
                    "M05":"Research Institutes and/or Public Policy Analysis ",
                    "M11":"Single Organization Support ",
                    "M12":"Fund Raising and/or Fund Distribution ",
                    "M19":"Nonmonetary Support N.E.C.",
                    "M20":"Disaster Preparedness and Relief Services",
                    "M23":"Search and Rescue Squads, Services",
                    "M24":"Fire Prevention, Protection, Control",
                    "M40":"Safety Education",
                    "M41":"First Aid Training, Services",
                    "M42":"Automotive Safety",
                    "M99":"Public Safety, Disaster Preparedness, and Relief N.E.C.",
                    "N01":"Alliance/Advocacy Organizations ",
                    "N02":"Management & Technical Assistance ",
                    "N03":"Professional Societies, Associations ",
                    "N05":"Research Institutes and/or Public Policy Analysis ",
                    "N11":"Single Organization Support ",
                    "N12":"Fund Raising and/or Fund Distribution ",
                    "N19":"Nonmonetary Support N.E.C.",
                    "N20":"Recreational and Sporting Camps",
                    "N30":"Physical Fitness and Community Recreational Facilities",
                    "N31":"Community Recreational Centers",
                    "N32":"Parks and Playgrounds",
                    "N40":"Sports Training Facilities, Agencies",
                    "N50":"Recreational, Pleasure, or Social Club",
                    "N52":"Fairs, County and Other",
                    "N60":"Amateur Sports Clubs, Leagues, N.E.C.",
                    "N61":"Fishing, Hunting Clubs",
                    "N62":"Basketball",
                    "N63":"Baseball, Softball",
                    "N64":"Soccer Clubs, Leagues",
                    "N65":"Football Clubs, Leagues",
                    "N66":"Tennis, Racquet Sports Clubs, Leagues",
                    "N67":"Swimming, Water Recreation",
                    "N68":"Winter Sports (Snow and Ice)",
                    "N69":"Equestrian, Riding",
                    "N6A":"Golf",
                    "N70":"Amateur Sports Competitions",
                    "N71":"Olympics Committees and Related International Competitions",
                    "N72":"Special Olympics",
                    "N80":"Professional Athletic Leagues",
                    "N99":"Recreation, Sports, Leisure, Athletics N.E.C.",
                    "O01":"Alliance/Advocacy Organizations ",
                    "O02":"Management & Technical Assistance ",
                    "O03":"Professional Societies, Associations ",
                    "O05":"Research Institutes and/or Public Policy Analysis ",
                    "O11":"Single Organization Support ",
                    "O12":"Fund Raising and/or Fund Distribution ",
                    "O19":"Nonmonetary Support N.E.C.",
                    "O20":"Youth Centers, Clubs, Multipurpose",
                    "O21":"Boys Clubs",
                    "O22":"Girls Clubs",
                    "O23":"Boys and Girls Clubs (Combined)",
                    "O30":"Adult, Child Matching Programs",
                    "O31":"Big Brothers, Big Sisters",
                    "O40":"Scouting Organizations",
                    "O41":"Boy Scouts of America",
                    "O42":"Girl Scouts of the U.S.A.",
                    "O43":"Camp Fire",
                    "O50":"Youth Development Programs, Other",
                    "O51":"Youth Community Service Clubs",
                    "O52":"Youth Development - Agricultural",
                    "O53":"Youth Development - Business",
                    "O54":"Youth Development - Citizenship Programs",
                    "O55":"Youth Development - Religious Leadership",
                    "O99":"Youth Development N.E.C.",
                    "P01":"Alliance/Advocacy Organizations ",
                    "P02":"Management & Technical Assistance ",
                    "P03":"Professional Societies, Associations ",
                    "P05":"Research Institutes and/or Public Policy Analysis ",
                    "P11":"Single Organization Support ",
                    "P12":"Fund Raising and/or Fund Distribution ",
                    "P19":"Nonmonetary Support N.E.C.",
                    "P20":"Human Service Organizations - Multipurpose",
                    "P21":"American Red Cross",
                    "P22":"Urban League",
                    "P24":"Salvation Army",
                    "P26":"Volunteers of America",
                    "P27":"Young Men's or Women's Associations (YMCA, YWCA, YWHA, YMHA)",
                    "P28":"Neighborhood Centers, Settlement Houses",
                    "P29":"Thrift Shops",
                    "P30":"Children's, Youth Services",
                    "P31":"Adoption",
                    "P32":"Foster Care",
                    "P33":"Child Day Care",
                    "P40":"Family Services",
                    "P42":"Single Parent Agencies, Services",
                    "P43":"Family Violence Shelters, Services",
                    "P44":"Homemaker, Home Health Aide",
                    "P45":"Family Services, Adolescent Parents",
                    "P46":"Family Counseling",
                    "P50":"Personal Social Services",
                    "P51":"Financial Counseling, Money Management",
                    "P52":"Transportation, Free or Subsidized",
                    "P58":"Gift Distribution",
                    "P60":"Emergency Assistance (Food, Clothing, Cash)",
                    "P61":"Travelers' Aid",
                    "P62":"Victims' Services",
                    "P70":"Residential, Custodial Care",
                    "P72":"Half-Way House (Short-Term Residential Care)",
                    "P73":"Group Home (Long Term)",
                    "P74":"Hospice",
                    "P75":"Senior Continuing Care Communities",
                    "P80":"Services to Promote the Independence of Specific Populations",
                    "P81":"Senior Centers, Services",
                    "P82":"Developmentally Disabled Centers, Services",
                    "P84":"Ethnic, Immigrant Centers, Services",
                    "P85":"Homeless Persons Centers, Services",
                    "P86":"Blind/Visually Impaired Centers, Services ",
                    "P87":"Deaf/Hearing Impaired Centers, Services ",
                    "P99":"Human Services - Multipurpose and Other N.E.C.",
                    "Q01":"Alliance/Advocacy Organizations ",
                    "Q02":"Management & Technical Assistance ",
                    "Q03":"Professional Societies, Associations ",
                    "Q05":"Research Institutes and/or Public Policy Analysis ",
                    "Q11":"Single Organization Support ",
                    "Q12":"Fund Raising and/or Fund Distribution ",
                    "Q19":"Nonmonetary Support N.E.C.",
                    "Q20":"Promotion of International Understanding",
                    "Q21":"International Cultural Exchange",
                    "Q22":"International Student Exchange and Aid",
                    "Q23":"International Exchanges, N.E.C.",
                    "Q30":"International Development, Relief Services",
                    "Q31":"International Agricultural Development",
                    "Q32":"International Economic Development",
                    "Q33":"International Relief",
                    "Q40":"International Peace and Security",
                    "Q41":"Arms Control, Peace Organizations",
                    "Q42":"United Nations Association",
                    "Q43":"National Security, Domestic",
                    "Q70":"International Human Rights",
                    "Q71":"International Migration, Refugee Issues",
                    "Q99":"International, Foreign Affairs, and National Security N.E.C.",
                    "R01":"Alliance/Advocacy Organizations ",
                    "R02":"Management & Technical Assistance ",
                    "R03":"Professional Societies, Associations ",
                    "R05":"Research Institutes and/or Public Policy Analysis ",
                    "R11":"Single Organization Support ",
                    "R12":"Fund Raising and/or Fund Distribution ",
                    "R19":"Nonmonetary Support N.E.C.",
                    "R20":"Civil Rights, Advocacy for Specific Groups",
                    "R22":"Minority Rights",
                    "R23":"Disabled Persons' Rights",
                    "R24":"Women's Rights",
                    "R25":"Seniors' Rights",
                    "R26":"Lesbian, Gay Rights",
                    "R30":"Intergroup, Race Relations",
                    "R40":"Voter Education, Registration",
                    "R60":"Civil Liberties Advocacy",
                    "R61":"Reproductive Rights",
                    "R62":"Right to Life",
                    "R63":"Censorship, Freedom of Speech and Press Issues",
                    "R67":"Right to Die, Euthanasia Issues",
                    "R99":"Civil Rights, Social Action, Advocacy N.E.C.",
                    "S01":"Alliance/Advocacy Organizations ",
                    "S02":"Management & Technical Assistance ",
                    "S03":"Professional Societies, Associations ",
                    "S05":"Research Institutes and/or Public Policy Analysis ",
                    "S11":"Single Organization Support ",
                    "S12":"Fund Raising and/or Fund Distribution ",
                    "S19":"Nonmonetary Support N.E.C.",
                    "S20":"Community, Neighborhood Development, Improvement (General)",
                    "S21":"Community Coalitions",
                    "S22":"Neighborhood, Block Associations",
                    "S30":"Economic Development",
                    "S31":"Urban, Community Economic Development",
                    "S32":"Rural Development",
                    "S40":"Business and Industry",
                    "S41":"Promotion of Business",
                    "S43":"Management Services for Small Business, Entrepreneurs",
                    "S46":"Boards of Trade",
                    "S47":"Real Estate Organizations",
                    "S50":"Nonprofit Management",
                    "S80":"Community Service Clubs",
                    "S81":"Women's Service Clubs",
                    "S82":"Men's Service Clubs",
                    "S99":"Community Improvement, Capacity Building N.E.C.",
                    "T01":"Alliance/Advocacy Organizations ",
                    "T02":"Management & Technical Assistance ",
                    "T03":"Professional Societies, Associations ",
                    "T05":"Research Institutes and/or Public Policy Analysis ",
                    "T11":"Single Organization Support ",
                    "T12":"Fund Raising and/or Fund Distribution ",
                    "T19":"Nonmonetary Support N.E.C.",
                    "T20":"Private Grantmaking Foundations",
                    "T21":"Corporate Foundations",
                    "T22":"Private Independent Foundations",
                    "T23":"Private Operating Foundations",
                    "T30":"Public Foundations",
                    "T31":"Community Foundations",
                    "T40":"Voluntarism Promotion",
                    "T50":"Philanthropy, Charity, Voluntarism Promotion, General",
                    "T70":"Fund Raising Organizations That Cross Categories",
                    "T90":"Named Trusts/Foundations N.E.C. ",
                    "T99":"Philanthropy, Voluntarism, and Grantmaking Foundations N.E.C.",
                    "U01":"Alliance/Advocacy Organizations ",
                    "U02":"Management & Technical Assistance ",
                    "U03":"Professional Societies, Associations ",
                    "U05":"Research Institutes and/or Public Policy Analysis ",
                    "U11":"Single Organization Support ",
                    "U12":"Fund Raising and/or Fund Distribution ",
                    "U19":"Nonmonetary Support N.E.C.",
                    "U20":"Science, General",
                    "U21":"Marine Science and Oceanography",
                    "U30":"Physical Sciences, Earth Sciences Research and Promotion",
                    "U31":"Astronomy",
                    "U33":"Chemistry, Chemical Engineering",
                    "U34":"Mathematics",
                    "U36":"Geology",
                    "U40":"Engineering and Technology Research, Services",
                    "U41":"Computer Science",
                    "U42":"Engineering",
                    "U50":"Biological, Life Science Research",
                    "U99":"Science and Technology Research Institutes, Services N.E.C.",
                    "V01":"Alliance/Advocacy Organizations ",
                    "V02":"Management & Technical Assistance ",
                    "V03":"Professional Societies, Associations ",
                    "V05":"Research Institutes and/or Public Policy Analysis ",
                    "V11":"Single Organization Support ",
                    "V12":"Fund Raising and/or Fund Distribution ",
                    "V19":"Nonmonetary Support N.E.C.",
                    "V20":"Social Science Institutes, Services",
                    "V21":"Anthropology, Sociology",
                    "V22":"Economics (as a social science)",
                    "V23":"Behavioral Science",
                    "V24":"Political Science",
                    "V25":"Population Studies",
                    "V26":"Law, International Law, Jurisprudence",
                    "V30":"Interdisciplinary Research",
                    "V31":"Black Studies",
                    "V32":"Women's Studies",
                    "V33":"Ethnic Studies",
                    "V34":"Urban Studies",
                    "V35":"International Studies",
                    "V36":"Gerontology (as a social science)",
                    "V37":"Labor Studies",
                    "V99":"Social Science Research Institutes, Services N.E.C.",
                    "W01":"Alliance/Advocacy Organizations ",
                    "W02":"Management & Technical Assistance ",
                    "W03":"Professional Societies, Associations ",
                    "W05":"Research Institutes and/or Public Policy Analysis ",
                    "W11":"Single Organization Support ",
                    "W12":"Fund Raising and/or Fund Distribution ",
                    "W19":"Nonmonetary Support N.E.C.",
                    "W20":"Government and Public Administration",
                    "W22":"Public Finance, Taxation, Monetary Policy",
                    "W24":"Citizen Participation",
                    "W30":"Military, Veterans' Organizations",
                    "W40":"Public Transportation Systems, Services",
                    "W50":"Telephone, Telegraph and Telecommunication Services",
                    "W60":"Financial Institutions, Services (Non-Government Related)",
                    "W61":"Credit Unions",
                    "W70":"Leadership Development",
                    "W80":"Public Utilities",
                    "W90":"Consumer Protection, Safety",
                    "W99":"Public, Society Benefit - Multipurpose and Other N.E.C.",
                    "X01":"Alliance/Advocacy Organizations ",
                    "X02":"Management & Technical Assistance ",
                    "X03":"Professional Societies, Associations ",
                    "X05":"Research Institutes and/or Public Policy Analysis ",
                    "X11":"Single Organization Support ",
                    "X12":"Fund Raising and/or Fund Distribution ",
                    "X19":"Nonmonetary Support N.E.C.",
                    "X20":"Christian",
                    "X21":"Protestant",
                    "X22":"Roman Catholic",
                    "X30":"Jewish",
                    "X40":"Islamic",
                    "X50":"Buddhist",
                    "X70":"Hindu",
                    "X80":"Religious Media, Communications Organizations ",
                    "X81":"Religious Film, Video",
                    "X82":"Religious Television",
                    "X83":"Religious Printing, Publishing",
                    "X84":"Religious Radio ",
                    "X90":"Interfaith Issues",
                    "X99":"Religion Related, Spiritual Development N.E.C.",
                    "Y01":"Alliance/Advocacy Organizations ",
                    "Y02":"Management & Technical Assistance ",
                    "Y03":"Professional Societies, Associations ",
                    "Y05":"Research Institutes and/or Public Policy Analysis ",
                    "Y11":"Single Organization Support ",
                    "Y12":"Fund Raising and/or Fund Distribution ",
                    "Y19":"Nonmonetary Support N.E.C.",
                    "Y20":"Insurance Providers, Services",
                    "Y22":"Local Benevolent Life Insurance Associations, Mutual Irrigation and Telephone Companies, and Like Organizations",
                    "Y23":"Mutual Insurance Company or Association",
                    "Y24":"Supplemental Unemployment Compensation",
                    "Y25":"State-Sponsored Worker's Compensation Reinsurance Organizations",
                    "Y30":"Pension and Retirement Funds",
                    "Y33":"Teachers Retirement Fund Association",
                    "Y34":"Employee Funded Pension Trust",
                    "Y35":"Multi-Employer Pension Plans",
                    "Y40":"Fraternal Beneficiary Societies",
                    "Y42":"Domestic Fraternal Societies",
                    "Y43":"Voluntary Employees Beneficiary Associations (Non-Government)",
                    "Y44":"Voluntary Employees Beneficiary Associations (Government) ",
                    "Y50":"Cemeteries, Burial Services",
                    "Y99":"Mutual/Membership Benefit Organizations, Other N.E.C.",
                    "Z99":"Unknown"
                    }

             )


g = web.template.Template.globals
g['slice'] = slice
g['commify'] = web.commify
g['int'] = int
g['round'] = round
g['abs'] = abs
g['len'] = len
g['changequery'] = web.changequery
g['enumerate'] = enumerate
g['datestr'] = web.datestr
g['urlquote'] = web.urlquote
g['format_name'] = format_name
g['date_range'] = date_range
g['getattr'] = getattr

g['query_param'] = query_param
g['is_logged_in'] = lambda: bool(get_loggedin_email() or get_unverified_email())

import markdown
g['format'] = markdown.markdown

import blog
g['blog_content'] = blog.content

import re
r_html = re.compile(r'<[^>]+?>')
def striphtml(x):
    return r_html.sub('', x).replace('\n', ' ')
g['striphtml'] = striphtml
g['getpath'] = lambda : web.ctx.homepath + web.ctx.path
g['getfullpath'] = lambda : web.ctx.homepath + web.ctx.fullpath

g['cookies_on'] = lambda : True #bool(web.cookies().get('webpy_session_id')) #@@@ fix this
g['get_user_id'] = lambda: get_loggedin_userid() or get_unverified_userid()
g['get_user_name'] = get_user_name

########NEW FILE########
__FILENAME__ = hotshot_profile_by_line
#!/usr/bin/python
# -*- coding: utf-8 -*-
"""Display per-line profile information from `hotshot`.

`hotshot` can record per-line profiling statistics, but the standard
`hotshot.stats` module only shows you per-function statistics, not
per-line statistics.

This program is likely to produce voluminous output, on the order of
the size of the program being profiled (minus comments, blank lines,
and lines that werent covered in the profiler run).

The output is formatted with several objectives in mind:

- its in a readable tabular format
- it has the most time-consuming lines at the top, so that you can
  pipe this programs output to head(1) and usefully reduce the
  time it takes to run
- it shows the lines of source directly, so you dont have to
  constantly refer back and forth between the profiler output and your
  source code
- its hypertext if you load it into Emacs, so that if you want to see
  your expensive source code lines in context, you can do it just by
  clicking the mouse on the line
- if you pipe it to sort(1) you get back a version of your program,
  annotated with execution counts and times in the left margin,
  omitting parts that didnt run.  This works best if sort(1) is
  configured to sort things by ASCII code (e.g. `| LANG=C sort`)
  instead of e.g. English sort order.

These are in conflict to some extent.

BUGS
----

Because `hotshot` doesnt record full paths to files, let alone their
contents, this program relies on `sys.path` (via `linecache`) to find
the contents of source files.  This means that if you run it on a
different machine or in a different version of Python than you ran the
profile in, or if you have edited the program since you ran the
profiler, the source code shown will be wonky.

`hotshot` has the option to record per-line profiling statistics, but
doesnt do so by default.  You have to request it using the
`lineevents` flag when you instantiate a `hotshot.Profile`:

    >>> import hotshot
    >>> prof = hotshot.Profile('parse.fec_csv.prof', lineevents=True)
    >>> x = prof.runcall(whatever)
    >>> prof.close()

Reading `hotshot` profile log files is kind of slow.  On my machine it
takes about 4.5 as long to generate this profile data as to run the
original code to be profiled.

The output (and the source, and the documentation) is in UTF-8-encoded
Unicode.  Not everything defaults to UTF-8 yet; in particular `pydoc
-w` seems to still default to ISO-8859-1.

The Emacs hypertext works better with stuff in your current directory
than with stuff pulled in from Python libraries.

The output doesnt sort correctly with sort(1) if you have modules
over 9999 lines.
"""

import sys, hotshot.log, cgitb, linecache, os

def display_profiling_information(filename):
    """Display `hotshot` per-line profiling information.

    We treat ENTER, EXIT, and LINE events as mostly equivalent; each
    just provides a time delta which gets charged to the specified
    line number.  I dont know if that is correct with regard to
    `hotshot` output.

    Arguments:
    - `filename`: the filename to read the log from.
    """
    
    log = hotshot.log.LogReader(filename)
    time = {}
    n = {}

    for what, where, tdelta in log:
        time.setdefault(where, 0)
        time[where] += tdelta

        n.setdefault(where, 0)
        if what != hotshot.log.EXIT: n[where] += 1

    total = sum(time.values())

    here = os.getcwd()
    # leading space to get `LANG=C sort` to leave these at the top
    print '  -*- mode: grep; default-directory: "%s"; coding: utf-8 -*-' % here
    print " (output of proflines.py %s, formatted for emacs; %.3gs total)" % (
        filename, total * 1e-6)
    for where in sorted(time.keys(), key=time.get, reverse=True):
        filename, lineno, funcname = where
        line = linecache.getline(filename, lineno)
        header = '%s:%04d: (%s)' % (filename, lineno, funcname)
        seconds = time[where] * 1e-6
        try:
            percall = seconds / n[where]
        except ZeroDivisionError:
            percall = 0
        print "%s%s %8.3gs (%4d  %7.2g) %s"  % (header,
                                                 ' ' * max(0, 40 - len(header)),
                                                 seconds,
                                                 n[where],
                                                 percall,
                                                 line.rstrip())

def main(argv):
    """Display `hotshot` profiling information per line.
    
    Arguments:
    - `argv`: [inputfilename]
    """
    display_profiling_information(argv[1])

if __name__ == '__main__':
    cgitb.enable(format='text')
    main(sys.argv)

########NEW FILE########
__FILENAME__ = index
import web
import webapp
import petition
import inspect, types, itertools, gzip, os
from web.browser import AppBrowser

single_pages = ('/', '/about', '/about/team', '/about/api', '/about/feedback', '/about/help',
                '/contribute/', '/blog/',
                '/lob/c/', '/lob/f/', '/lob/o/', '/lob/pa/', '/lob/pe/',
                '/b/', '/e/', '/p/', '/c/', '/writerep/')

b = AppBrowser(webapp.app)

def test(klass):
    index = klass().index()
    for path in do_flatten(take(2, iter(index))):
        try:
            b.open(path)
            assert(b.status == 200)
        except:
            print b.status, path, klass

def get_class_index(klass, _test=False):
    try:
        if _test:
            return test(klass)  
        return klass().index()
    except AttributeError:
        return []

def do_flatten(d):
    """
    >>> list(do_flatten([1, 2, 3]))
    [1, 2, 3]
    >>> list(do_flatten([1, [2, [3]], [4, [5]]]))
    [1, 2, 3, 4, 5]
    """
    for x in d:
        if type(x) in [types.ListType, types.GeneratorType, types.TupleType]:
            for y in do_flatten(x):
                yield y
        else:
            yield x

def flatten(f):
    def g(*args, **kw):
        return do_flatten(f(*args, **kw))
    return g

@flatten
def getindex(app, _test=False):
    for page in single_pages:
        yield page
    kns = list(app.fvars['urls'])[1::2]
    for kn in kns:
        if isinstance(kn, types.StringType):
            if '.' in kn:
                modname, kls = kn.split('.')
                mod = __import__(modname)
                kls = getattr(mod, kls, None)
            else:    
                kls = app.fvars[kn]
            yield get_class_index(kls, _test)
        elif isinstance(kn, web.application):
            yield getindex(kn, _test)

def take(n, seq):
    for i in xrange(n):
        yield seq.next()

def group(seq, maxsize):
    def limit(seq, maxsize, itemlen):
        size = 0
        while size < maxsize:
            x = seq.next()
            if not x: break
            size += itemlen(x)
            yield x
    
    overhead = len('<a href=""></a>\n')
    itemlen = lambda x: overhead+2*len(x)
    x = 1
    while x:
        x = list(limit(seq, maxsize, itemlen))
        yield x

t_sitemap = """$def with (title, items)
<style>a{display:block;}</style>
<h1>Index</h1>
<a href="index.html">Back</a> | <a href="../index.html">Back to index</a></br>
$for item in items:
    <a href="$item">$item</a>
<a href="index.html">Back</a> | <a href="../index.html">Back to index</a>
"""

t_index = """$def with (title, items)
<style>a{display:block;}</style>
<h1>$title</h1>

$if title != "index": <a href="../index.html">Back to index</a>
$for item in items:
    <a href="$item">$item</a>
$if title != "index": <a href="../index.html">Back to index</a>
"""

make_sitemap = web.template.Template(t_sitemap)
make_index = web.template.Template(t_index)
pagesize = 99*1024 #1K for overheads like <h1> and back links
entries_per_page = pagesize/50  #len('<a href="index_xxxxx.html">index_xxxxx.html</a>') = 47

def write(filename, text):
    f = open(filename, 'w')
    f.write(text)
    print filename, len(text)/1024
    f.close()

def write_sitemap(i, seq, index_dir):
    dir = index_dir + '/%02d' % (i/entries_per_page)
    filename = "%s/index_%05d.html" % (dir, i)
    if not os.path.exists(dir):
        os.mkdir(dir)
    write(filename, str(make_sitemap(filename, seq)))

def write_sitemaps(data, index_dir, offset=0):
    for i, x in enumerate(group(data, pagesize)):
        write_sitemap(i+offset, x, index_dir)

def update_indexes(index_dir):
    dirs = [d for d in os.listdir(index_dir) if os.path.isdir(os.path.join(index_dir, d))]
    write(index_dir + '/index.html', str(make_index(index_dir, sorted(d+'/index.html' for d in dirs))))

    for d in dirs:
        d = os.path.join(index_dir, d)
        write(d + '/index.html', str(make_index('index %s' % (d), sorted(os.listdir(d)))))

def create_index(index_dir, _test=False):
    if not os.path.exists(index_dir):
        os.mkdir(index_dir)
    
    data = getindex(webapp.app, _test)
    write_sitemaps(data, index_dir)
    update_indexes(index_dir)

if __name__ == "__main__":
    create_index('static/index', _test=False)

########NEW FILE########
__FILENAME__ = load_responses
import sys
import mailbox
import email
from datetime import datetime

import web
import config
from utils import helpers, messages

MAILDIR_PATH = config.maildir_path

def getid(msg):
    to = msg.get('To')
    if not to.startswith('p-'): return
    id = email[email.index('p-')+2 : email.index('@')]
    return int(id, 36)
    
def format_date(date):
    """
        >>> date = 'Fri, 22 Aug 2008 11:38:05 +0530 (IST)'
        >>> format_date(date)
        datetime.datetime(2008, 8, 22, 11, 38, 5)
    """
    date = msg.get('Date')
    date = date[0:date.index(' +')] #@@ FIX IT - loosing timezone info
    return datetime.strptime(date, '%a, %d %b %Y %H:%M:%S')
    
def get_msg_body(msg):    
    if msg.is_multipart():
        msgbody = "\n".join(m.get_payload().strip() for m in msg)
    else:
        msgbody = msg.get_payload().strip()
    return msgbody
    
def process(msg):
    #store the msg in db and send followup msg to the sender
    msgid = getid(msg)
    msgbody = get_msg_body(msg)
    received = format_date(msg.get('Date')) #msg.get_date() doesn't work!!
    messages.save_response(msgid, msgbody, received)
    send_followup(msgid, msgbody)

def get_sender_email(msgid):
    uid = messages.get_sender_id(msgid)
    if uid: 
        user = helpers.get_user_by_id(uid)
        if user: return user.email

def send_followup(msgid, response_body):
    from_addr = config.from_address
    to_addr = get_sender_email(msgid)
    if not to_addr: return
    subject = 'FILL IN HERE'
    body = response_body +  'FILL IN HERE'                   
    web.sendmail(from_addr, to_addr, subject, body)
    
if __name__ == '__main__':
    inbox = mailbox.Maildir(MAILDIR_PATH, factory=mailbox.MaildirMessage, create=False)

    for msg in inbox.itervalues():
        process(msg)

########NEW FILE########
__FILENAME__ = messages
"""
Library for managing messages sent to reps and tracking their responses.
"""
import web
from settings import db

def save_msg(frm, to, subj, msg, source_id=None, status=None):
    """saves the given msg and return back msg id. 
    It's assumed that the msg is already sent. 
    msg is always sent from a user to a politician. 
    Optionally the source of the message can be specified  by `source_id`.
    """
    return db.insert('messages', from_id=frm, to_id=to, subject=subj,
		 body=msg, source_id=source_id, sent=status)

def get_msg(msgid):
    """returns the msg with the given `msgid`
    """
    msg = db.select('messages', where='id=$msgid', vars=locals())    
    if msg: return msg[0]

def update_msg_status(msgid, status):
    """updates status of the message with id `msgid` with status.
    """
    db.update('messages', sent=status, where='id=$msgid', vars=locals())

def save_response(msgid, response, timestamp):
    """saves the `response` to msg with `msgid` and returns the response id
    """
    try:
	    return db.insert('responses', msg_id=msgid, response=response, received=timestamp)
    except:
	    pass

def get_responses(msgid):
    """returns all the responses to the msg with the given `msgid`
    """
    return db.select('responses', where='msg_id=$msgid', vars=locals())

def query(frm=None, to=None, source_id=None, limit=None, offset=None, order=None):
    """queries for matching messsages and returns their ids
    """
    where = ''
    if frm: where += 'from_id = $frm and '
    if to:  where += 'to_id = $to and '
    if source_id: where += 'source_id = $source_id and '
    web.rstrips(where, 'and ')
    
    try:
	    return db.select('messages', where=where or None, limit=limit, offset=offset, order=order)
    except Exception, details:
	    print where, details

def get_sender_id(msgid):
    """return the id of the sender of the message with id `msgid`.
    """
    u = db.select('messages', what='sender', where='id=$msgid', vars=locals())
    if u: return u[0].sender

########NEW FILE########
__FILENAME__ = pckcat
#!/usr/bin/python
"""Print out the contents of a pickle file.

Since I started writing pickle files from `fec_crude_csv.py`, I
thought I should write a convenient way to see their contents.  This
program converts such files to JSON.

"""

import sys, cPickle, simplejson

def main(filenames):
    for filename in filenames:
        fo = file(filename)
        while True:
            try:
                record = cPickle.load(fo)
            except EOFError:
                break
            print simplejson.dumps(record, sort_keys=True, indent=4)

if __name__ == '__main__':
    main(sys.argv[1:])

########NEW FILE########
__FILENAME__ = rdftramp
#!/usr/bin/env python
"""rdftramp: Makes RDF look like Python data structures."""
__version__ = 1.1
__copyright__ = "(C) 2008 Aaron Swartz <http://www.aaronsw.com/>. GNU GPL 3."

"""
2008-04-24: 1.1. rename rdftramp, add support for ints and floats
2002-11-17: 1.0. first release
"""

from rdflib import URIRef as URI, Literal
from rdflib.Graph import Graph

class Namespace:
        """A class so namespaced URIs can be abbreviated (like dc.subject).
        label provides the abbreviation that should be used on output)"""

        def __init__(self, prefix, label=''): 
                self.prefix = prefix; self.label = label
        def __getattr__(self, name): return URI(self.prefix + name)
        def __getitem__(self, name): return URI(self.prefix + name)
        
rdf = Namespace('http://www.w3.org/1999/02/22-rdf-syntax-ns#', 'rdf')
rdfs = Namespace('http://www.w3.org/2000/01/rdf-schema#', 'rdfs')
xsd = Namespace('http://www.w3.org/2001/XMLSchema#', 'xsd')
rss = Namespace('http://purl.org/rss/1.0/', 'rss')
daml = Namespace('http://www.daml.org/2001/03/daml+oil#', 'daml')
log = Namespace('http://www.w3.org/2000/10/swap/log#', 'log')
dc = Namespace('http://purl.org/dc/elements/1.1/', 'dc')
foaf = Namespace('http://xmlns.com/foaf/0.1/', 'foaf')
doc = Namespace('http://www.w3.org/2000/10/swap/pim/doc#', 'doc')
cc = Namespace('http://web.resource.org/cc/', 'cc')
content = Namespace('http://purl.org/rss/1.0/modules/content/', 'content')

class Thing:
    """Takes an RDF object and makes it look like a dictionary."""
    def __init__(self, name, store): self.name, self.store = name, store
    
    def __getitem__(self, v):
        out = list(self.store.objects(self.name, v))
        out = [thing(x, self, v) for x in out]
        
        if len(out) == 1:
            return out[0]
        else:
            return PseudoList(out, self, v)
    
    def __setitem__(self, k, v):
        if isinstance(v, Thing): v = v.name
        for triple in self.store.triples((self.name, k, None)):
            self.store.remove((triple[0], triple[1], triple[2]))
                    
        if not isinstance(v, list): v = [v]
        for val in v:
            if not isinstance(val, URI) and isinstance(val, (unicode, str, int, float)):
                val = Literal(val)
            self.store.add((self.name, k, val))
    
    def __iter__(self):
        #@@ add support for new-style lists?
        i = 1
        while rdf["_" + `i`] in self: 
            yield self[rdf["_" + `i`]]
            i += 1

    def __contains__(self, val): return not not self[val]

    def __repr__(self): return repr(self.name)
    def __str__(self): return str(self.name)

    def __eq__(self, other): 
        if isinstance(other, Thing): return self.name == other.name
        else: return self.name == other

def thing(x, store, prop):
    if isinstance(x, URI): return Thing(x, store.store)
    if isinstance(x, (Thing, PseudoBase)): return x
    if isinstance(x, Literal):
        if x.datatype in [xsd.integer, xsd.int]:
            return PseudoInteger(x, store, prop)
        elif x.datatype == xsd.float:
            return PseudoFloat(x, store, prop)
        else:
            return PseudoString(x, store, prop)
    raise ValueError, "couldn't thingify %s (a %s)" % (x, x.__class__)

class PseudoBase(object):        
    def __new__(self, name, thing, item):
        self = super(PseudoBase, self).__new__(self, name)
        self._thing, self._item = thing, item
        return self

    def append(self, x):
        self._thing[self._item] = [self, x]

class PseudoString(PseudoBase, unicode):
    def __eq__(self, other):
        return unicode(self) == other

class PseudoInteger(PseudoBase, int): pass
class PseudoFloat(PseudoBase, float): pass
class PseudoList(PseudoBase, list):
    def __init__(self, name, thing, item):
        list.__init__(self, name)
        self._thing, self._item = thing, item
    
    def append(self, x):
        self._thing[self._item] = list(self) + [x]



if __name__ == "__main__":
    # Unit tests, baby!
    
    store = Graph(); ex = Namespace("http://example.org/")
    Aaron = Thing(URI("http://me.aaronsw.com/"), store)
    assert Aaron == Thing(URI("http://me.aaronsw.com/"), store)
    
    # URIs do not equal pseudostrings
    a_uri_string = "http://me.aaronsw.com/"
    a_uri = URI(a_uri_string)
    pseudo = PseudoString(a_uri_string, None, None)
    assert str(pseudo) == a_uri_string
    assert pseudo == pseudo
    assert a_uri == a_uri
    assert not (pseudo == a_uri)
    assert not (a_uri == pseudo)

    Aaron[ex.name] = "Aaron Swartz"
    assert Aaron[ex.name] == "Aaron Swartz"
    Aaron[ex.homepage] = URI("http://www.aaronsw.com/")
    assert Aaron[ex.homepage] == URI("http://www.aaronsw.com/")
    
    Aaron[ex.machine] = ["vorpal", "slithy"]
    assert sorted(Aaron[ex.machine]) == ["slithy", "vorpal"]
    # (we sort because order isn't necessarily preserved)
    Aaron[ex.machine] = ["vorpal"]
    # (this replaces old triples)
    assert Aaron[ex.machine] == "vorpal"
    # (if there's only one, it's returned as itself)
    Aaron[ex.machine].append("slithy")
    # (this adds a triple)
    assert Aaron[ex.machine].sort() == ["vorpal", "slithy"].sort()
    Aaron[ex.machine].append('tumtum')
    assert Aaron[ex.machine].sort() == ["vorpal", "slithy", "tumtum"].sort()
    
    # let's do numbers!
    Aaron[ex.age] = 14
    assert Aaron[ex.age] == 14
    Aaron[ex.age] = 14.1
    assert Aaron[ex.age] == 14.1
    
    
    # Lists are hard to make because you shouldn't be making them
    r = rdf
    f = Aaron[ex.topFiveFrobs] = Thing(ex.frobList9028292, store)
    f[r.type] = r.Seq
    f[r._1], f[r._2], f[r._3], f[r._4], f[r._5] = \
        "John", "Jacob", "Jingle", "Heimer", "Schmidt"
    # but since other people did, we still parse them
    n = 0; frobs = ["John", "Jacob", "Jingle", "Heimer", "Schmidt"]
    for frob in Aaron[ex.topFiveFrobs]:
        assert frob == frobs[n]
        n += 1
    assert n == 5
    
    # "pred in subj" == bool(subj[pred])
    assert ex.children not in Aaron
    # you probably don't need it since subj[pred] returns [], not error
    assert not Aaron[ex.children]

    assert str(Aaron) == "http://me.aaronsw.com/"
    assert str(Aaron[ex.name]) == "Aaron Swartz"
    
# Mark Nottingham's Sparta <http://www.mnot.net/sw/sparta/> inspired TRAMP.
# I am open to an LGPL license if you have a convincing reason.

########NEW FILE########
__FILENAME__ = se
"""
Utils for xapian.
"""
try:
    import xappy
    DBPATH = 'se'

    def query(s):
        sconn = xappy.SearchConnection(DBPATH)
        q = sconn.query_parse(sconn.spell_correct(s), default_op=sconn.OP_AND)
        return [x.data['id'][0] for x in sconn.search(q, 0, 10)]
    
except ImportError:
    import warnings
    warnings.warn('No xappy found. Skipping search engine stuff.')
    def query(s):
        return []

########NEW FILE########
__FILENAME__ = simplegraphs
import Image, ImageDraw, StringIO

def sparkline(points, point=None, height=15*2, width=40*2, bubble=2*2, linewidth=1.5*2, margin=5*2, scalefactor=5):
    margin *= scalefactor
    height *= scalefactor
    width *= scalefactor
    bubble *= scalefactor
    
    im = Image.new("RGBA", (width, height), (0,0,0,0))
    draw = ImageDraw.Draw(im)
    height -= margin
    width -= margin
    
    maxpoint = max(points)
    minpoint = min(points)
    if maxpoint == minpoint: maxpoint += 0.001 # avoid Divide by zero error
    
    mypoints = [(
      int(margin/2. + (width*(n/float(len(points))))),
      int((height+margin/2.) - ((height*((float(i) - minpoint)/(maxpoint-minpoint)))))
    ) for (n, i) in enumerate(points)]
    draw.line(mypoints, fill='#888888', width=int(linewidth*scalefactor))
    
    if float(point) in points:
        x, y = mypoints[points.index(float(point))]
        draw.ellipse((x-bubble, y-bubble, x+bubble, y+bubble), fill='#f55')        
    
    height += margin
    width += margin
    
    im.thumbnail((width/scalefactor, height/scalefactor), Image.ANTIALIAS)
    f = StringIO.StringIO()
    im.save(f, 'PNG')
    return f.getvalue()

########NEW FILE########
__FILENAME__ = sitemap
"""Script to generate XML sitemap of openlibrary.org website.
"""

import web
import os
import itertools
import datetime
import urllib

import webapp
from index import getindex

def uniq(iterator):
    seen = set()
    for item in iterator:
        if item in seen: continue
        seen.add(item)
        yield item

t_sitemap = """$def with (items)
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
    $for i in items:
        <url><loc>http://watchdog.net$i</loc></url>
</urlset>
"""

t_siteindex = """$def with (names, timestamp)
<?xml version="1.0" encoding="UTF-8"?>
<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
    $for x in names:
        <sitemap>
            <loc>http://static.watchdog.net/sitemaps/sitemap_${x}.xml.gz</loc>
            <lastmod>$timestamp</lastmod>
        </sitemap>
</sitemapindex>
"""

sitemap = web.template.Template(t_sitemap, filter=web.websafe)
siteindex = web.template.Template(t_siteindex, filter=web.websafe)

def write(path, text):
    from gzip import open as gzopen
    print 'writing', path, text.count('\n')
    f = gzopen(path, 'w')
    f.write(text)
    f.close()

def make_siteindex(urls):
    groups = web.group(urls, 50000)
    
    if not os.path.exists('sitemaps'):
        os.mkdir('sitemaps')
    
    for i, x in enumerate(groups):
        sitemap_lines = [
          '<?xml version="1.0" encoding="UTF-8"?>', 
          '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">']
        for item in x:
            sitemap_lines.append('<url><loc>http://watchdog.net%s</loc></url>' % item)
        sitemap_lines.append('</urlset>')
        write("sitemaps/sitemap_%04d.xml.gz" % i, '\n'.join(sitemap_lines))
    
    names = ["%04d" % j for j in range(i)]
    timestamp = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S') + 'Z'
    index = siteindex(names, timestamp)
    write("sitemaps/siteindex.xml.gz", str(index))

def write_urls():
    fh = file('urls.txt', 'w')
    for line in getindex(webapp.app):
        fh.write(line + '\n')
    
    fh.close()

if __name__ == "__main__":
    #write_urls()
    # sort -u urls.txt > urls.uniq.txt
    make_siteindex(x.strip() for x in file('urls.uniq.txt'))

########NEW FILE########
__FILENAME__ = userinfo
import web
from settings import db, render
import forms, helpers, auth

urls = (
    '/(.*)', 'userinfo'
    )

def get_password_form(user):
    #if the user has already set a password before, add the current password field to the form.
    form = forms.change_password()
    if user.password:
        curr_password = forms.curr_password
        form.inputs = (curr_password, ) + form.inputs
    return form    

class userinfo():
    def GET(self, uid, info_form=None, password_form=None):
        try:
            user = db.select('users', where='id=$uid', vars=locals())[0]
        except IndexError:     
            raise web.notfound()
        
        info_form = info_form or forms.userinfo()
        if not password_form:
            password_form = get_password_form(user)
            
        info_form.fill(**user)
        msg, msg_type = helpers.get_delete_msg()
        return render.userpage(uid, info_form, password_form, msg)

    def POST(self, uid):
        if web.input('m', _method='GET'):
            return self.POST_password(uid)
        
        form = forms.userinfo()
        i = web.input()
        if form.validates(i):
            i.pop('submit')
            db.update('users', where='id=$uid', vars=locals(), **i)
            helpers.set_msg('User information updated.')
            raise web.seeother('/%s' % uid)
        else:
            return self.GET(uid, info_form=form)
    
    def POST_password(self, uid):
        user = db.select('users', what='password', where='id=$uid', vars=locals())[0]
        form = get_password_form(user)
        i = web.input()
        if form.validates(i):
            if ('curr_password' not in form) or auth.check_password(user, i.curr_password):
                enc_password = auth.encrypt_password(i.password)
                db.update('users', password=enc_password, verified=True, where='id=$uid', vars=locals())
                helpers.set_msg('Password saved.')
            else:
                helpers.set_msg('Invalid Password', 'error')    
            raise web.seeother('/%s' % uid)
        else:
             return self.GET(uid, password_form=form)   
            
app = web.application(urls, globals())
if __name__ == '__main__':
    app.run()            
########NEW FILE########
__FILENAME__ = users
import web
from settings import db, render
import forms, helpers, auth
from contacts import importcontacts
from auth import login, signup, logout, forgot_password, set_password
import urllib

urls = (
    '/login', 'login',
    '/logout', 'logout',
    '/signup', 'signup',
    '/set_password', 'set_password',
    '/forgot_password', 'forgot_password',
    '/import_contacts', 'importcontacts',
    r'/(\d+)', 'petitions',
    r'/(\d+)/preferences', 'userinfo',
    )
    
def fill_user_details(form, user=None):
    user = user or helpers.get_user()
    if user:
        form.fill(userid=user.id)
        form.fill(user)

def update_user_details(i, uid=None):
    if not uid:
        user = helpers.get_user_by_email(i.get('email'))
        uid = user and user.id
    i['phone'] = web.numify(i.get('phone'))
    details = ['prefix', 'lname', 'fname', 'addr1', 'addr2', 'city', 'zip5', 'zip4', 'phone', 'state']
    
    d = {}
    for (k, v) in i.items():
        if v and (k in details):
            d[k] = v
    db.update('users', where='id=$uid', vars=locals(), **d)
    
def get_password_form(user):
    #if the user has already set a password before, add the current password field to the form.
    form = forms.change_password()
    if user.password:
        curr_password = forms.curr_password
        form.inputs = (curr_password, ) + form.inputs
    return form    


def check_permission(uid):
    current_uid = helpers.get_loggedin_userid()
    if current_uid != int(uid):
        query = urllib.urlencode(dict(redirect=web.ctx.homepath + web.ctx.fullpath))
        raise web.seeother("/u/login?%s" % (query), absolute=True)

def created_by(uid):
    created = db.select('petition',
            where='owner_id=$uid and petition.deleted is null',
            order='created desc',
            vars=locals())
    return created

def signed_by(uid):
    signed = db.select('signatory, petition', what='petition.id, title, signed, comment',
            where='user_id=$uid and \
                   petition.id = signatory.petition_id and \
                   petition.deleted is null',
            order='signed desc',
            vars=locals())
    return signed

class petitions():
    def GET(self, uid):
        from petition import get_num_signs
        user = db.select('users', what='id, lname, fname', where='id=$uid', vars=locals())      
        if not user: raise web.notfound()
        created, signed = created_by(uid).list(), signed_by(uid).list()
        for p in created + signed: p.signcount = get_num_signs(p.id)
        logged_in = (helpers.get_loggedin_userid() == int(uid))
        return render.user_petitions(user[0], created, signed, logged_in)

class userinfo():
    def GET(self, uid, info_form=None, password_form=None):
        check_permission(uid)
        try:
            user = db.select('users', where='id=$uid', vars=locals())[0]
        except IndexError:     
            raise web.notfound()
        
        info_form = info_form or forms.userinfo()
        if not password_form:
            password_form = get_password_form(user)
            
        info_form.fill(**user)
        msg, msg_type = helpers.get_delete_msg()
        return render.userpage(uid, info_form, password_form, msg)

    def POST(self, uid):
        i = web.input('m', _method='GET')
        if i.m == 'password':
            return self.POST_password(uid)
        
        form = forms.userinfo()
        i = web.input(_method='POST')
        if form.validates(i):
            if 'submit' in i: i.pop('submit')
            db.update('users', where='id=$uid', vars=locals(), **i)
            helpers.set_msg('User information updated.')
            raise web.seeother('/%s/preferences' % uid)
        else:
            return self.GET(uid, info_form=form)
    
    def POST_password(self, uid):
        user = db.select('users', what='password', where='id=$uid', vars=locals())[0]
        form = get_password_form(user)
        set_passwd_form = 'curr_password' not in [inp.name for inp in list(form.inputs)]
        i = web.input()
        if form.validates(i):
            if set_passwd_form or auth.check_password(user, i.curr_password):
                enc_password = auth.encrypt_password(i.password)
                db.update('users', password=enc_password, verified=True, where='id=$uid', vars=locals())
                helpers.set_msg('Password %s.' % ('saved' if set_passwd_form else 'changed'))
                raise web.seeother('/%s/preferences' % uid)
            else:
                helpers.set_msg('Invalid Password', 'error')    
                form.note = 'Current Password invalid.'
                form.valid = False
        return self.GET(uid, password_form=form)   
            
app = web.application(urls, globals())
if __name__ == '__main__':
    app.run()            
########NEW FILE########
__FILENAME__ = writerep
"""
Functions to send msgs to reps and deal with captchas
"""

import sys
import re, urllib2
from urlparse import urljoin

import web
from settings import db
from wyrutils import *
from utils import captchasolver, messages, helpers, browser
from config import production_test_email, from_address, test_email
from settings import production_mode

__all__ = ["prepare", "send_msgs", "send", "writerep"]

test_mode = (not production_mode)
DEBUG = False

def prepare(pol):
    """
    Checks if the `pol`'s contact url has captcha, and if so - takes care of
    with cookies, return env with captcha url, cookies and form having captcha.
    """
    env = {}
    url = getcontact(pol).get('contact')
    b = browser.Browser()
    b.open(url)
    captchas = b.find_nodes('img', attrs={'src': re.compile('.*[Cc]aptcha.*')})
    captcha_src, form = get_src_and_form(url, captchas)
    if captcha_src:
	    env['captcha_src'], env['form'], env['cookies'] = captcha_src, form, b.get_state()
    return env

def send(frm, to, subj, msg, user_details, source_id=None, env={}):
    """
    Sends the given `msg` to `to`, with the `user_details` and saves it in DB. 
    uses `env` if `to` has captcha.
    """
    msgid = messages.save_msg(frm, to, subj, msg, source_id)
    user_details.email = 'p-%s@watchdog.net' % web.to36(msgid)
    user_details.full_msg = compose_msg(to, msg)
    user_details.subject = user_details.ptitle
    status = writerep(to, user_details, env)
    if status: messages.update_msg_status(msgid, status)
    return msgid

def send_msgs(uid, i, source_id, pols=[], env={}):
    """
    Sends msgs to ALL the politcians who are senators/reps of the 
    district defined by zip5, zip4, address in i
    """
    pols = pols or getpols(i.zip5,  i.zip4, i.addr1+i.addr2)
    msgids = {}
    for pol in pols:
	    msgids[pol] = send(uid, pol, i.ptitle, i.msg, i, source_id, env.get(pol, {}))
    return msgids
        
def compose_msg(polid, msg):
    #@@ compose msg here
    p = db.select('politician', where='id=$polid', 
            what='firstname, middlename, lastname, district_id', vars=locals())[0]
    pol_name = "%s %s %s" % (p.firstname or '', p.middlename or '', p.lastname or '') 
    rep_or_sen = 'Sen.' if len(p.district_id) == 2 else 'Rep.'
    full_msg = 'Dear %s %s,\n%s' % (rep_or_sen, pol_name, msg)
    return full_msg

def writerep(pol, i, env={}):
    """
    Checks for the contact type for the `pol` and calls one of writerep_{wyr, ima, email} 
    with the contact url appropriately and returns the status.
    """
    i.prefix = i.get('prefix', '.').rstrip('.') #few forms take only Mr, Ms etc.
    c = getcontact(pol)
    if not c: return False
    contact, contacttype = c.contact, c.contacttype
    handlers = dict(E=writerep_email, W=writerep_wyr, I=writerep_ima, Z=writerep_zipauth)
    try:
        handler = handlers[contacttype]
        if DEBUG: print handler.__name__,
        if contacttype == 'I':
            msg_sent = handler(pol, contact, i, env)
        else:
            msg_sent = handler(pol, contact, i)    
    except Exception, details:
        print >> sys.stderr, 'Error in writerep:', details
        msg_sent = False
    if not msg_sent: send_failure_report(pol, i)
    if DEBUG: print msg_sent and 'Success' or 'Failure'
    return msg_sent

def writerep_wyr(pol, wyr_link, i):
    """Sends the msg along with the sender details from `i` through the WYR system.
    The WYR system has 3 forms typically (and a captcha form for few reps in between 1st and 2nd forms).
    Form 1 asks for state and zipcode
    Form 2 asks for sender's details such as prefix, name, city, address, email, phone etc
    Form 3 asks for the msg to send.
    """
    b = browser.Browser()

    def wyr_step1(url):
        b.open(url)
        form = get_form(b, not_signup_or_search)
        # state names are in form: "PRPuerto Rico"
        state_options = form.find_control_by_name('state').items
        state_l = [s.name for s in state_options if s.name[:2] == i.state]
        form.fill_all(state=state_l[0], zipcode=i.zip5, zip4=i.zip4)
        if DEBUG: print 'step1 done',
        return form.click()
            
    def get_challenge():
        labels = b.find_nodes('label', lambda x: x.get('for') == 'HIP_response')
        if labels: return labels[0].string

    def get_wyr_form2(request):
        b.open(request)
        form = get_form(b, not_signup_or_search)
        if not form:
            if b.has_text("is shared by more than one"): raise ZipShared
            elif b.has_text("not correct for the selected State"): raise ZipIncorrect
            elif b.has_text("was not found in our database."): raise ZipNotFound
            elif b.has_text("Use your web browser's <b>BACK</b> capability "): raise WyrError
            else: raise NoForm
        else:
            challenge = get_challenge()
            if challenge:
                try:
                    solution = captchasolver.solve(challenge)
                except Exception, detail:
                    print >> sys.stderr, 'Exception in CaptchaSolve', detail
                    print >> sys.stderr, 'Could not solve:"%s"' % challenge,
                else:        
                    form.f['HIP_response'] = str(solution)
                    request = form.click()
                    form = get_wyr_form2(request)
                    return form
            else:
                return form
        
    def wyr_step2(request):
        form = get_wyr_form2(request)
        if form and form.fill_name(i.prefix, i.fname, i.lname):
            form.fill_address(i.addr1, i.addr2)
            form.fill_all(city=i.city, phone=i.phone, email=i.email)
            request = form.click()
            if DEBUG: print 'step2 done',
            return request
            
    def wyr_step3(request):
        b.open(request)
        form = get_form(b, lambda f: f.find_control_by_type('textarea'))
        if form and form.fill(i.full_msg, type='textarea'):
            if DEBUG: print 'step3 done',
            return submit_form(b, form, i)

    return wyr_step3(wyr_step2(wyr_step1(wyr_link)))

def writerep_ima(pol, ima_link, i, env={}):
    """Sends the msg along with the sender details from `i` through the form @ ima_link.
        The web page at `ima_link` typically has a single form, with the sender's details
        and subject and msg (and with a captcha img for few reps/senators).
        If it has a captcha img, the form to fill captcha is taken from env.
    """
    b = browser.Browser(env.get('cookies', []))
    b.url, b.page = ima_link, env.get('form')
    f = get_form(b, lambda f: f.find_control_by_type('textarea'))
    if not f:
        b.open(ima_link)
        f = get_form(b, lambda f: f.find_control_by_type('textarea'))

    if f:
        f.fill_name(i.prefix, i.fname, i.lname)
        f.fill_address(i.addr1, i.addr2)
        f.fill_phone(i.phone)
        f.fill(type='textarea', value=i.full_msg)
        captcha_val = i.get('captcha_%s' % pol, '')
        f.fill_all(city=i.city, state=i.state.upper(), zipcode=i.zip5, zip4=i.zip4, email=i.email,\
                    issue=['GEN', 'OTH'], subject=i.subject, captcha=captcha_val, reply='yes')
        return submit_form(b, f, i)
    else:
        print >> sys.stderr, 'Error: No IMA form in', ima_link,

def writerep_zipauth(pol, zipauth_link, i):
    """Sends the msg along with the sender details from `i` through the WYR system.
      This has 2 forms typically.
      Form 1 asks for zipcode and few user details 
      Form 2 asks for the subject and msg to send and other sender's details.
    """
    def zipauth_step1(f):
        f.fill_name(i.prefix, i.fname, i.lname)
        f.fill_address(i.addr1, i.addr2)
        f.fill_phone(i.phone)
        f.fill_all(email=i.email, zipcode=i.zip5, zip4=i.zip4, city=i.city)
        if 'lamborn.house.gov' in zipauth_link:
            f.f.action = urljoin(zipauth_link, '/Contact/ContactForm.htm') #@@ they do it in ajax
        if DEBUG: print 'step1 done',
        return f.click()
        
    def zipauth_step2(request):
        request.add_header('Cookie', 'District=%s' % i.zip5)  #@@ done in ajax :(
        response = b.open(request)
        f = get_form(b, lambda f: f.find_control_by_type('textarea'))
        if f:
            f.fill_name(i.prefix, i.fname, i.lname)
            f.fill_address(i.addr1, i.addr2)
            f.fill_phone(i.phone)
            f.fill(type='textarea', value=i.full_msg)
            f.fill_all(city=i.city, zipcode=i.zip5, zip4=i.zip4, state=i.state.upper(),
                    email=i.email, issue=['GEN', 'OTH'], subject=i.subject, reply='yes')
            if DEBUG: print 'step2 done',
            return submit_form(b, f, i)
        else:
            print >> sys.stderr, 'no form with text area'
            if b.has_text('zip code is split between more'): raise ZipShared
            if b.has_text('Access to the requested form is denied'): raise ZipIncorrect
            if b.has_text('you are outside'): raise ZipIncorrect 
            
    b = browser.Browser()
    b.open(zipauth_link)
    form = get_form(b, lambda f: f.has(name='zip'))
    if form:
        return zipauth_step2(zipauth_step1(form))
    else:
        print >> sys.stderr, 'Error: No zipauth form in', zipauth_link

def writerep_email(pol, pol_email, i):
    name = '%s. %s %s' % (i.prefix, i.fname, i.lname)
    from_addr = '%s <%s>' % (name, i.email)

    if production_mode:
        to_addr = web.lstrips(pol_email, 'mailto:')
    elif test_mode:
        to_addr = test_email
    web.sendmail(from_addr, to_addr, i.subject, i.full_msg)
    return True

def submit_form(browser, f, i):
    """clicks the form `f` and opens the request in browser `b` and sends response."""
    if production_mode:
        request = f.click()
        response = browser.open(request)
        send_response(production_test_email, i, f.controls, response)
    elif test_mode:
        send_response(test_email, i, f.controls, response='')
    return True

def send_response(to, i, form_controls, response):
    """sends a mail to `to` to check if the form is submitted properly.
    """
    inputs ='\n'.join(['%s: %s' % (k, v) for k, v in i.items()])
    msg = 'Filled at watchdog.net:\n\n%s' % inputs

    form_values = "\n".join(["%s: %s" % (c.name, c.value) for c in form_controls])
    msg += '\n\nFilled in the last form:\n\n%s' % form_values

    if response: 
        msg +=  '\n\nResponse: \n\n' + response
    else:
        msg += '\n\n(Not Production click, no response)'

    subject = 'wyr mail'
    web.sendmail(from_address, to, subject, msg)

def send_failure_report(pol, i):
    inputs ='\n'.join(['%s: %s' % (k, v) for k, v in i.items()])
    msg = 'Filled at watchdog.net:\n\n%s' % inputs
    subject = 'Writerep to %s failed' % pol
    to = production_test_email if production_mode else test_email
    web.sendmail(from_address, to, subject, msg)    
    
def not_signup_or_search(form):
    has_textarea = form.find_control_by_type('textarea')
    if has_textarea:
        return True
    else:    
        action = form.action
        signup = 'signup' in action and 'email' in action
        search = 'search' in action or 'thomas.loc.gov' in action
        return not(search or signup)

def get_form(browser, predicate=None):
    """wrapper for browser.get_form method to return Form ojects instead of ClientForm objects"""
    f = browser.get_form(lambda f: predicate is None or predicate(Form(f)))
    if f: return Form(f)

def get_src_and_form(url, imgs):
    """Returns the source of captcha img(if any) and form containing it.
    """
    try:
        img = imgs[0]
        img_src = img.get('src', '')
        form = repr(img.findParent('form')) or ''
        captcha_src = urljoin(url, img_src)
    except Exception, details:
        print >> sys.stderr, details
        return None, None
    else:
        return captcha_src, form

########NEW FILE########
__FILENAME__ = wyrapp
"""
web interface to Write Your Rep 
"""
import web
import forms, helpers, auth
from users import fill_user_details, update_user_details
from wyrutils import *
from settings import db, render
import writerep
import simplejson

urls = (
    '/', 'write_rep',
    '/getcaptcha', 'get_captchas',
    '/verifyzip', 'verify_zip'
)

def captcha_box(pol, img_src):
    name = 'captcha_%s' % pol
    pre = "<img src='%s'/>" % img_src
    return web.form.Textbox('captcha_%s' % pol, web.form.notnull,
	        web.form.Validator("Enter the letters as they are shown in the image", bool), 
	        size='10', pre=pre, description='Validation')
    
def render_captcha(c):
    return """<tr><td colspan=3>
                <label for='%s'>Verification</label> %s %s 
                </td></tr>""" % (c.name, c.pre, c.render())

def add_captcha(form, img_src, pol):
    c = captcha_box(pol, img_src)   
    form.inputs = list(form.inputs) + [c]
    return render_captcha(c)

def prepare_for_captcha(wf, pols=None):
    env = {}
    captcha_html = ''
    if not pols:
        address = (wf.addr1.value or '') + (wf.addr2.value or '')
    	pols = getpols(wf.zip5.value, wf.zip4.value, address)
    for pol in pols:
    	if has_captcha(pol):
    	    e = writerep.prepare(pol)
    	    if e:
                captcha_html += add_captcha(wf, e['captcha_src'], pol)
                env[pol] = e
    if env:
        wf.captcha_env.value = simplejson.dumps(env)
    return captcha_html

class write_rep:
    def GET(self, wf=None):
        u = helpers.get_user()
        uemail = u and u.email
        if not wf:
    	    #create a new form and initialize with current user details
            wf = forms.wyrform()
    	    u and fill_user_details(wf, u)
    	captcha_html = prepare_for_captcha(wf)
    	msg, msg_type = helpers.get_delete_msg()
    	return render.writerep(wf, useremail=uemail, captchas=captcha_html, msg=msg)

    def POST(self):
        def pol_link(polid):
            p = db.select('politician', what='firstname, middlename, lastname',
                            where='id=$polid', vars=locals())[0]
            return '<a href="/p/%s">%s %s %s</a>' % (polid, p.firstname or '',
                            p.middlename or '', p.lastname or '')
                
    	i = web.input()
    	wf = forms.wyrform()
    	pols = getpols(i.zip5, i.zip4, i.addr1+i.addr2)
    	captcha_needed = require_captcha(i, pols)
    	if not wf.validates(i) or captcha_needed:
            if captcha_needed: wf.valid, wf.note = False, 'Please fill the captcha below'
    	    wf.fill(i)
    	    return self.GET(wf)
    	else:
    	    uid = auth.assert_login(i)
    	    update_user_details(i, uid)
    	    env = simplejson.loads(i.get('captcha_env', '{}'))
    	    status = writerep.send_msgs(uid, i, source_id='wyr', pols=pols, env=env)
    	    pol_str = ", ".join([pol_link(p) for p in pols])
    	    helpers.set_msg('Your message has been sent to %s' % pol_str)
    	    raise web.seeother('/')

class get_captchas:
    def GET(self):
        i = web.input()
        pols = dist2pols(i.get('dist'))
        wf = forms.wyrform()
        captcha_html = prepare_for_captcha(wf, pols)
        return captcha_html

class verify_zip:
    def GET(self):
        i = web.input()
        dists = getdist(i.zip5, i.zip4, i.address)
        if len(dists) == 1:
            return dists[0]
        else:
            return len(dists)

app = web.application(urls, globals())
if __name__ == "__main__":
    app.run()
########NEW FILE########
__FILENAME__ = wyrtest
import web
from settings import db
from writerep import writerep, compose_msg
from wyrutils import pol2dist

def test(formtype=None):
    def getdistzipdict(zipdump):
        """returns a dict with district names as keys zipcodes falling in it as values"""
        d = {}
        for line in zipdump.strip().split('\n'):
            zip5, zip4, dist = line.split('\t')
            d[dist] = (zip5, zip4)
        return d

    try:        
       dist_zip_dict =  getdistzipdict(file('zip_per_dist.tsv').read())
    except:
       import os, sys
       path = os.path.dirname(sys.modules[__name__].__file__)
       dist_zip_dict =  getdistzipdict(file(path + '/zip_per_dist.tsv').read())

    def getzip(dist):
        try:
            return dist_zip_dict[dist]
        except KeyError:
            for d in dist_zip_dict.keys():
                if d.startswith(dist+'-'):
                    return dist_zip_dict[d]
        return '', ''
          
          
    query = "select politician_id from pol_contacts where contacttype='%s'" % formtype[0].upper()
    pols = [r.politician_id for r in db.query(query)]
    for pol in pols:
        print pol,
        dist = pol2dist(pol)
        zip5, zip4 = getzip(dist)
        print zip5, zip4,
        msg = compose_msg(pol, 'testing...')
        u = web.Storage(zip5=zip5, zip4=zip4, prefix='Mr.', state=dist[:2],
                    fname='test', lname ='Tester', addr1='111 av', addr2='addr extn', city='test city', 
                    phone='0010010010', email='test@tryitout.net', subject='general', full_msg=msg)
        msg_sent = writerep(pol, u)
        print msg_sent and 'Success' or 'Failure'
    
if __name__ == '__main__':
    test('email')
    test('wyr')
    test('ima')
    test('zipauth')

########NEW FILE########
__FILENAME__ = wyrutils
import web
from zip2rep import zip2dist
from settings import db

import sys
from ClientForm import ControlNotFoundError, AmbiguityError

__all__ = ['ZipShared', 'ZipIncorrect', 'ZipNotFound', 'NoForm', 'WyrError', #all exceptions
            'numdists', 'getdist', 'getcontact', 'getpols', 'has_captcha', 'dist2pols',
            'Form', 'require_captcha']
            
class ZipShared(Exception): pass
class ZipIncorrect(Exception): pass
class ZipNotFound(Exception): pass
class WyrError(Exception): pass
class NoForm(Exception): pass

name_options = dict(prefix=['pre', 'salut', 'title'],
                    lname=['lname', 'last'],
                    fname=['fname', 'first', 'name'],
                    zipcode=['zip', 'zipcode'],
                    zip4=['zip4', 'four', 'plus'],
                    address=['addr1', 'address1', 'add1', 'address', 'add'],
                    addr2=['addr2', 'add2', 'address2'],
                    city=['city'],
                    state=['state'],
                    email=['email', 'e-mail'],
                    phone=['phone'],
                    issue=['issue', 'subject', 'topic', 'title'],
                    subject=['subject', 'topic'],
                    message=['message', 'msg', 'comment', 'text'],
                    captcha=['captcha', 'validat'],
                    reply=['reply', 'response', 'answer']
                )

def numdists(zip5, zip4=None, address=None):
    return len(get_dist(zip5, zip4, address))

def getdist(zip5, zip4=None, address=None):
    query = 'select distinct district_id from zip4 where zip=$zip5'
    if zip4: query += ' and plus4=$zip4'
    query += ' limit 2'     #just to see uniqness of districts
    dist = [x.district_id for x in db.query(query, vars=locals())]
    if len(dist) != 1:
        try:
            dist = zip2dist(zip5, address and address.strip())
        except Exception, details:
            pass
    return dist

def getcontact(pol):
    p = db.select('pol_contacts', where='politician_id=$pol', vars=locals())
    return p[0] if p else {}

def getpols(zip5, zip4=None, address=None):
    dist = getdist(zip5, zip4, address)
    dist = dist[0] if dist else ''
    return dist2pols(dist)

def has_captcha(pol):
    r = db.select('pol_contacts', what='contact', where="politician_id=$pol and captcha='t'", vars=locals())
    return bool(r)

def pol2dist(pol):
    try:
        return db.select('curr_politician', what='district_id', where='curr_politician.id=$pol', vars=locals())[0].district_id
    except KeyError:
        return

def dist2pols(dist):
    if not dist: return []
    where = 'curr_politician.district_id=$dist or curr_politician.district_id=$dist[:2]'
    try:
        return [p.id for p in db.select('curr_politician', what='id', where=where, vars=locals())]
    except KeyError:
        return []

def first(seq):
    """returns first True element"""    
    if not seq: return False
    for s in seq:
        if s: return s

def require_captcha(i, pols=None):
    """returns if the residents of the district defined by zip5, zip4, address in `i`
    have to fill captcha in the pol contact form.
    """
    captcha_fields = [c for c in i if c.startswith('captcha_')]
    captchas_filled = captcha_fields and all([i.get(c) for c in captcha_fields])
    pols = pols or getpols(i.get('zip5'), i.get('zip4'), i.get('addr1', '')+i.get('addr2', ''))
    have_captcha = any(has_captcha(p) for p in pols)
    return (not captchas_filled) and have_captcha

def matches(a, b):
    """`a` matches `b` if any name_options of `a` is a part of `b` in lower case
    >>> matches('name', 'required-fname')
    True
    >>> matches('addr2', 'address2')
    True
    >>> matches('captcha', 'validation')
    True
    >>> matches('lname', 'required-name')
    False
    """
    if not a in name_options.keys():
        return False
    for n in name_options[a]:
        if n in b.lower():
            return True
    return False 

class Form(object):
    def __init__(self, f):
        self.f = f
        self.action = self.f.action
        self.controls = filter(lambda c: not (c.readonly or c.type == 'hidden') and c.name, f.controls)

    def __repr__(self):
        return repr(self.f)

    def __str__(self):
        return str(self.f)

    def  __getattr__(self, x): 
        return getattr(self.f, x)

    def click(self):
        try:
            return self.f.click()
        except Exception, detail:
            print >> sys.stderr, detail

    def fill_name(self, prefix, fname, lname):
        self.fill(prefix, 'prefix')
        if self.fill(lname, 'lname'):
            return self.fill(fname, 'fname')
        else:
            name = "%s %s %s" % (prefix, lname, fname)
            return self.fill(fname, 'fname')

    def fill_address(self, addr1, addr2):    
        if self.fill(addr2, 'addr2'):
            return self.fill(addr1, 'address')
        else:
            address = "%s %s" % (addr1, addr2)
            return self.fill(address, 'address')

    def fill_phone(self, phone):
        phone = phone + ' '* (10 - len(phone)) # make phone length 10
        ph_ctrls = [c.name for c in self.controls if 'phone' in c.name.lower() and c.type == 'text']
        num_ph = len(ph_ctrls)
        if num_ph == 1:
            return self.f.set_value(phone, ph_ctrls[0], type='text', nr=0)
        elif num_ph == 2:
            self.f.set_value(phone[:3], name=ph_ctrls[0], type='text', nr=0)
            self.f.set_value(phone[3:], name=ph_ctrls[1], type='text', nr=0)
        elif num_ph == 3:
            self.f.set_value(phone[:3], name=ph_ctrls[0], type='text', nr=0)
            self.f.set_value(phone[3:6], name=ph_ctrls[1], type='text', nr=0)
            self.f.set_value(phone[6:], name=ph_ctrls[2], type='text', nr=0)

    def select_value(self, control, options):
        if not isinstance(options, list): options = [options]
        items = [str(item).lstrip('*') for item in control.items]
        for option in options:
            for item in items:
                if option.lower() in item.lower():
                    return [item]
        return [item]

    def fill(self, value, name=None, type=None, control=None):
        c = control or self.find_control(name=name, type=type)
        if c:
            if c.type in ['select', 'radio', 'checkbox']: 
                value = self.select_value(c, value)
            elif isinstance(value, list):
                value = value[0]
            self.f.set_value(value, name=c.name, type=c.type, nr=0)
            return True 
        return False

    def fill_all(self, **d):
        #fill all the fields of the form with the values from `d`
        for c in self.controls:
            filled = False
            if c.name in d.keys():
                filled = self.fill(d[c.name], control=c)
            else:
                for k in d.keys():
                    if matches(k, c.name):
                        filled = self.fill(d[k], control=c)
            #if not filled and not c.value: print "couldn't fill %s" % (c.name),

    def has(self, name=None, type=None):
        return bool(self.find_control(name=name, type=type))

    def find_control(self, name=None, type=None):
        """return the form control of type `type` or matching `name_options` of `name`"""
        if not (name or type): return

        try:
            names = name_options[name]
        except KeyError:
            names = name and [name]
        c = None
        if type: c = self.find_control_by_type(type)
        if not c and names: c = first(self.find_control_by_name(name) for name in names)
        if not c and names: c = first(self.find_control_by_id(name) for name in names)

        return c     

    def find_control_by_name(self, name):
        name = name.lower()
        return first(c for c in self.controls if c.name and name in c.name.lower())

    def find_control_by_id(self, id):
        id = id.lower()
        return first(c for c in self.controls if c.id and id in c.id.lower())

    def find_control_by_type(self, type):
        try:
            return self.f.find_control(type=type)
        except ControlNotFoundError:
            return None
        except AmbiguityError:  #@@  TO BE FIXED
            return self.f.find_control(type=type, nr=1)


########NEW FILE########
__FILENAME__ = zip2rep
"""
zip2rep: convert locations into congressional districts
http://www.aaronsw.com/2002/zip2rep/

The function you probably want is `zip2dist` (the last one).
If you use this software, please send me an email letting me know.
If you like this software, you should support gerrymandering reform:
http://fairvote.org/?page=564
"""

__author__ = "Aaron Swartz <me@aaronsw.com>"
__version__ = "0.3"
__license__ = "Public domain"

"""
Revision history:

2008-04-11: 0.3  - fix bug causing us to lose Indiana (sorry, Indiana!)
2008-04-04: 0.21 - zip2rep now returns [] on invalid zipcodes
2008-03-25: 0.2  - fix bad bug with last district getting cut off (tx Jordan)
2008-03-23: 0.1  - initial version
"""
import urllib, re
u = "http://frwebgate.access.gpo.gov/cgi-bin/getdoc.cgi?dbname=%s_congressional_directory&docid=%sth_txt-"

def count(lst):
    """
    Takes a list of items and counts the values, returning 
    a sorted list of (value, count) pairs.
    
        >>> count(['a', 'a', 'a', 'c', 'a'])
        [(4, 'a'), (1, 'c')]
    """
    d = {}
    for item in lst:
        if item in d:
            d[item] += 1
        else:
            d[item] = 1
    out = [(v, k) for k, v in d.iteritems()]
    out.sort(reverse=True)
    return out

def cleanzips(ziplisting):
    def tozip(n):
        return str(n).zfill(5)
    s = ziplisting
    s = s.replace('\n', '').replace(' ', '') # remove whitespace
    s = s.split(',')
    ziplisting = s
    out = []
    for zcode in ziplisting:
        if '-' in zcode: # e.g. "20210-13"
            head, tail = zcode.split('-')
            out.append(head)
            head = int(head)
            assert tail.isdigit() # sanity check
            while not tozip(head).endswith(tail):
                head += 1
                out.append(tozip(head))
        else:
            out.append(zcode) # e.g. "20080"
    return out

r_pagenum = re.compile('\[\[Page \d+\]\]')
r_strong = re.compile('\</?strong\>')
r_zip = re.compile(r'ZIP Codes: (.*?)(?:\*  \*  \*|</pre>)', re.DOTALL)
r_state = re.compile('([A-Z][A-Z]) \d\d\d\d\d')

def parseone(u):
    t = urllib.urlopen(u).read()
    t = r_pagenum.sub('', t)
    t = r_strong.sub('', t)
    
    states = [s for c, s in count(r_state.findall(t)) if s != 'DC']
    if states:
        state = states[0]
    else: # must be DC
        state = 'DC'
    
    ziplist = [cleanzips(x) for x in r_zip.findall(t)]
    if len(ziplist) == 1: # at-large district
        yield (state + '-00', ziplist[0])
    else:
        for n, zips in enumerate(ziplist):
            yield (state + '-' + str(n+1).zfill(2), zips)

def parseall(session=110):
    """
    Parses the entire Congressional Directory into an iterator of
    2-tuples consisting of district code and a list of zip codes, e.g.:
    
        ('PR-00', ['00601', '00602', '00603', ...])
    """
    # workaround for missing data
    missing = {
      110: {
        'TX-25': ['78701'],
        'TX-21': ['78701']
      }
    }
    
    for i in range(2, 2 + 55):
        for item in parseone(u % (session, session) + str(i)):
            if item[0] in missing.get(session, {}):
                yield item[0], item[1] + missing[session][item[0]]
            else:
                yield item
    
def zipdict(session=110):
    """
    Returns a dictionary mapping zip codes to congressional districts:
    
    {
      '00601': ['PR-00'],
      '20231': ['DC-00', 'VA-08'],
      ...
    }
    """
    
    d = {}
    for district, zipcodes in parseall():
        for zipcode in zipcodes:
            d.setdefault(zipcode, [])
            d[zipcode].append(district)
    return d

def dumpzipdict(zipd):
    """
    Serializes the output of `zipdict` to a format like:
    
        00601: PR-00
        20231: DC-00 VA-08
        ...
    """
    out = []
    for k, v in zipd.iteritems():
        out.append(k + ': ')
        out.append(' '.join(v))
        out.append('\n')
    return ''.join(out)

def parsezipdict(zipdump):
    """
    Reparses the output of `dumpzipdict`.
    """
    d = {}
    for line in zipdump.strip().split('\n'):
        zipcode, districts = line.split(': ', 1)
        districts = districts.split(' ')
        d[zipcode] = districts
    return d

try:
    # file('zipdict.txt', 'w').write(dumpzipdict(zipdict()))
    myzipdict = parsezipdict(file('zipdict.txt', 'r').read())
except IOError:
    try:
        import os, sys
        myzipdict = parsezipdict(file(
          os.path.abspath(os.path.dirname(sys.modules[__name__].__file__)) +
          '/zipdict.txt', 'r').read())
    except IOError:
        pass

class BadAddress(Exception): pass
geocoder_u = "http://rpc.geocoder.us/service/csv?address="
def geocoder(addr):
    """
    Runs an address thru geocoder.us, returning a list like:
    
        ['38.898748', '-77.037684', '1600 Pennsylvania Ave NW',
         'Washington', 'DC', '20006']
    
    TODO: handle more errors
    """
    u = geocoder_u + urllib.quote(addr)
    t = urllib.urlopen(u).read()
    if t.startswith('2: '):
        raise BadAddress, t
    return t.split(',')

govtrack_u = 'http://www.govtrack.us/perl/wms/get-region.cgi?layer=cc-pac&lat=%s&long=%s&format=text'
def govtrack(lat, lng):
    """
    Runs a (lat, lng) pair thru govtrack.us, returning a 
    Congressional District (e.g. 'VA-01').
    
    TODO: handle errors
    """
    
    u = govtrack_u % (lat, lng)
    t = urllib.urlopen(u).read()
    distparts = t.split('\t')[0].split('/')
    state = distparts[-4]
    distnum = distparts[-1]
    return state.upper() + '-' + distnum.zfill(2)
    
def zip2dist(zipcode, addr=None):
    """
    Takes a zip code and an optional address and returns a list of 
    matching congressional districts.
        
        >>> zip2dist('12345')
        ['NY-21']
        >>> zip2dist('90210')
        ['CA-30']
        >>> zip2dist('12010')
        ['NY-20', 'NY-21', 'NY-23']
        >>> zip2dist('12010', '10 E Main St')
        ['NY-21']

    TODO: handle errors
    """
    try:
        dists = myzipdict[zipcode]
    except KeyError:
        return []
    if len(dists) == 1 or addr is None:
        return dists
    else:
        lat, lng = geocoder(addr + ', ' + zipcode)[:2]
        return [govtrack(lat, lng)]
        

if __name__ == "__main__":
    #print "Generating the zipdict (this will take some time and bandwidth)..."
    #file('zipdict.txt', 'w').write(dumpzipdict(zipdict()))
    print "Get the latest zipdict.txt file at http://watchdog.net/about/api#zip2rep"
########NEW FILE########
__FILENAME__ = BeautifulSoup
"""Beautiful Soup
Elixir and Tonic
"The Screen-Scraper's Friend"
http://www.crummy.com/software/BeautifulSoup/

Beautiful Soup parses a (possibly invalid) XML or HTML document into a
tree representation. It provides methods and Pythonic idioms that make
it easy to navigate, search, and modify the tree.

A well-formed XML/HTML document yields a well-formed data
structure. An ill-formed XML/HTML document yields a correspondingly
ill-formed data structure. If your document is only locally
well-formed, you can use this library to find and process the
well-formed part of it.

Beautiful Soup works with Python 2.2 and up. It has no external
dependencies, but you'll have more success at converting data to UTF-8
if you also install these three packages:

* chardet, for auto-detecting character encodings
  http://chardet.feedparser.org/
* cjkcodecs and iconv_codec, which add more encodings to the ones supported
  by stock Python.
  http://cjkpython.i18n.org/

Beautiful Soup defines classes for two main parsing strategies:

 * BeautifulStoneSoup, for parsing XML, SGML, or your domain-specific
   language that kind of looks like XML.

 * BeautifulSoup, for parsing run-of-the-mill HTML code, be it valid
   or invalid. This class has web browser-like heuristics for
   obtaining a sensible parse tree in the face of common HTML errors.

Beautiful Soup also defines a class (UnicodeDammit) for autodetecting
the encoding of an HTML or XML document, and converting it to
Unicode. Much of this code is taken from Mark Pilgrim's Universal Feed Parser.

For more than you ever wanted to know about Beautiful Soup, see the
documentation:
http://www.crummy.com/software/BeautifulSoup/documentation.html

Here, have some legalese:

Copyright (c) 2004-2008, Leonard Richardson

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

  * Redistributions of source code must retain the above copyright
    notice, this list of conditions and the following disclaimer.

  * Redistributions in binary form must reproduce the above
    copyright notice, this list of conditions and the following
    disclaimer in the documentation and/or other materials provided
    with the distribution.

  * Neither the name of the the Beautiful Soup Consortium and All
    Night Kosher Bakery nor the names of its contributors may be
    used to endorse or promote products derived from this software
    without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE, DAMMIT.

"""
from __future__ import generators

__author__ = "Leonard Richardson (leonardr@segfault.org)"
__version__ = "3.0.7a"
__copyright__ = "Copyright (c) 2004-2008 Leonard Richardson"
__license__ = "New-style BSD"

from sgmllib import SGMLParser, SGMLParseError
import codecs
import markupbase
import types
import re
import sgmllib
try:
  from htmlentitydefs import name2codepoint
except ImportError:
  name2codepoint = {}
try:
    set
except NameError:
    from sets import Set as set

#These hacks make Beautiful Soup able to parse XML with namespaces
sgmllib.tagfind = re.compile('[a-zA-Z][-_.:a-zA-Z0-9]*')
markupbase._declname_match = re.compile(r'[a-zA-Z][-_.:a-zA-Z0-9]*\s*').match

DEFAULT_OUTPUT_ENCODING = "utf-8"

# First, the classes that represent markup elements.

class PageElement:
    """Contains the navigational information for some part of the page
    (either a tag or a piece of text)"""

    def setup(self, parent=None, previous=None):
        """Sets up the initial relations between this element and
        other elements."""
        self.parent = parent
        self.previous = previous
        self.next = None
        self.previousSibling = None
        self.nextSibling = None
        if self.parent and self.parent.contents:
            self.previousSibling = self.parent.contents[-1]
            self.previousSibling.nextSibling = self

    def replaceWith(self, replaceWith):
        oldParent = self.parent
        myIndex = self.parent.contents.index(self)
        if hasattr(replaceWith, 'parent') and replaceWith.parent == self.parent:
            # We're replacing this element with one of its siblings.
            index = self.parent.contents.index(replaceWith)
            if index and index < myIndex:
                # Furthermore, it comes before this element. That
                # means that when we extract it, the index of this
                # element will change.
                myIndex = myIndex - 1
        self.extract()
        oldParent.insert(myIndex, replaceWith)

    def extract(self):
        """Destructively rips this element out of the tree."""
        if self.parent:
            try:
                self.parent.contents.remove(self)
            except ValueError:
                pass

        #Find the two elements that would be next to each other if
        #this element (and any children) hadn't been parsed. Connect
        #the two.
        lastChild = self._lastRecursiveChild()
        nextElement = lastChild.next

        if self.previous:
            self.previous.next = nextElement
        if nextElement:
            nextElement.previous = self.previous
        self.previous = None
        lastChild.next = None

        self.parent = None
        if self.previousSibling:
            self.previousSibling.nextSibling = self.nextSibling
        if self.nextSibling:
            self.nextSibling.previousSibling = self.previousSibling
        self.previousSibling = self.nextSibling = None
        return self

    def _lastRecursiveChild(self):
        "Finds the last element beneath this object to be parsed."
        lastChild = self
        while hasattr(lastChild, 'contents') and lastChild.contents:
            lastChild = lastChild.contents[-1]
        return lastChild

    def insert(self, position, newChild):
        if (isinstance(newChild, basestring)
            or isinstance(newChild, unicode)) \
            and not isinstance(newChild, NavigableString):
            newChild = NavigableString(newChild)

        position =  min(position, len(self.contents))
        if hasattr(newChild, 'parent') and newChild.parent != None:
            # We're 'inserting' an element that's already one
            # of this object's children.
            if newChild.parent == self:
                index = self.find(newChild)
                if index and index < position:
                    # Furthermore we're moving it further down the
                    # list of this object's children. That means that
                    # when we extract this element, our target index
                    # will jump down one.
                    position = position - 1
            newChild.extract()

        newChild.parent = self
        previousChild = None
        if position == 0:
            newChild.previousSibling = None
            newChild.previous = self
        else:
            previousChild = self.contents[position-1]
            newChild.previousSibling = previousChild
            newChild.previousSibling.nextSibling = newChild
            newChild.previous = previousChild._lastRecursiveChild()
        if newChild.previous:
            newChild.previous.next = newChild

        newChildsLastElement = newChild._lastRecursiveChild()

        if position >= len(self.contents):
            newChild.nextSibling = None

            parent = self
            parentsNextSibling = None
            while not parentsNextSibling:
                parentsNextSibling = parent.nextSibling
                parent = parent.parent
                if not parent: # This is the last element in the document.
                    break
            if parentsNextSibling:
                newChildsLastElement.next = parentsNextSibling
            else:
                newChildsLastElement.next = None
        else:
            nextChild = self.contents[position]
            newChild.nextSibling = nextChild
            if newChild.nextSibling:
                newChild.nextSibling.previousSibling = newChild
            newChildsLastElement.next = nextChild

        if newChildsLastElement.next:
            newChildsLastElement.next.previous = newChildsLastElement
        self.contents.insert(position, newChild)

    def append(self, tag):
        """Appends the given tag to the contents of this tag."""
        self.insert(len(self.contents), tag)

    def findNext(self, name=None, attrs={}, text=None, **kwargs):
        """Returns the first item that matches the given criteria and
        appears after this Tag in the document."""
        return self._findOne(self.findAllNext, name, attrs, text, **kwargs)

    def findAllNext(self, name=None, attrs={}, text=None, limit=None,
                    **kwargs):
        """Returns all items that match the given criteria and appear
        after this Tag in the document."""
        return self._findAll(name, attrs, text, limit, self.nextGenerator,
                             **kwargs)

    def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):
        """Returns the closest sibling to this Tag that matches the
        given criteria and appears after this Tag in the document."""
        return self._findOne(self.findNextSiblings, name, attrs, text,
                             **kwargs)

    def findNextSiblings(self, name=None, attrs={}, text=None, limit=None,
                         **kwargs):
        """Returns the siblings of this Tag that match the given
        criteria and appear after this Tag in the document."""
        return self._findAll(name, attrs, text, limit,
                             self.nextSiblingGenerator, **kwargs)
    fetchNextSiblings = findNextSiblings # Compatibility with pre-3.x

    def findPrevious(self, name=None, attrs={}, text=None, **kwargs):
        """Returns the first item that matches the given criteria and
        appears before this Tag in the document."""
        return self._findOne(self.findAllPrevious, name, attrs, text, **kwargs)

    def findAllPrevious(self, name=None, attrs={}, text=None, limit=None,
                        **kwargs):
        """Returns all items that match the given criteria and appear
        before this Tag in the document."""
        return self._findAll(name, attrs, text, limit, self.previousGenerator,
                           **kwargs)
    fetchPrevious = findAllPrevious # Compatibility with pre-3.x

    def findPreviousSibling(self, name=None, attrs={}, text=None, **kwargs):
        """Returns the closest sibling to this Tag that matches the
        given criteria and appears before this Tag in the document."""
        return self._findOne(self.findPreviousSiblings, name, attrs, text,
                             **kwargs)

    def findPreviousSiblings(self, name=None, attrs={}, text=None,
                             limit=None, **kwargs):
        """Returns the siblings of this Tag that match the given
        criteria and appear before this Tag in the document."""
        return self._findAll(name, attrs, text, limit,
                             self.previousSiblingGenerator, **kwargs)
    fetchPreviousSiblings = findPreviousSiblings # Compatibility with pre-3.x

    def findParent(self, name=None, attrs={}, **kwargs):
        """Returns the closest parent of this Tag that matches the given
        criteria."""
        # NOTE: We can't use _findOne because findParents takes a different
        # set of arguments.
        r = None
        l = self.findParents(name, attrs, 1)
        if l:
            r = l[0]
        return r

    def findParents(self, name=None, attrs={}, limit=None, **kwargs):
        """Returns the parents of this Tag that match the given
        criteria."""

        return self._findAll(name, attrs, None, limit, self.parentGenerator,
                             **kwargs)
    fetchParents = findParents # Compatibility with pre-3.x

    #These methods do the real heavy lifting.

    def _findOne(self, method, name, attrs, text, **kwargs):
        r = None
        l = method(name, attrs, text, 1, **kwargs)
        if l:
            r = l[0]
        return r

    def _findAll(self, name, attrs, text, limit, generator, **kwargs):
        "Iterates over a generator looking for things that match."

        if isinstance(name, SoupStrainer):
            strainer = name
        else:
            # Build a SoupStrainer
            strainer = SoupStrainer(name, attrs, text, **kwargs)
        results = ResultSet(strainer)
        g = generator()
        while True:
            try:
                i = g.next()
            except StopIteration:
                break
            if i:
                found = strainer.search(i)
                if found:
                    results.append(found)
                    if limit and len(results) >= limit:
                        break
        return results

    #These Generators can be used to navigate starting from both
    #NavigableStrings and Tags.
    def nextGenerator(self):
        i = self
        while i:
            i = i.next
            yield i

    def nextSiblingGenerator(self):
        i = self
        while i:
            i = i.nextSibling
            yield i

    def previousGenerator(self):
        i = self
        while i:
            i = i.previous
            yield i

    def previousSiblingGenerator(self):
        i = self
        while i:
            i = i.previousSibling
            yield i

    def parentGenerator(self):
        i = self
        while i:
            i = i.parent
            yield i

    # Utility methods
    def substituteEncoding(self, str, encoding=None):
        encoding = encoding or "utf-8"
        return str.replace("%SOUP-ENCODING%", encoding)

    def toEncoding(self, s, encoding=None):
        """Encodes an object to a string in some encoding, or to Unicode.
        ."""
        if isinstance(s, unicode):
            if encoding:
                s = s.encode(encoding)
        elif isinstance(s, str):
            if encoding:
                s = s.encode(encoding)
            else:
                s = unicode(s)
        else:
            if encoding:
                s  = self.toEncoding(str(s), encoding)
            else:
                s = unicode(s)
        return s

class NavigableString(unicode, PageElement):

    def __new__(cls, value):
        """Create a new NavigableString.

        When unpickling a NavigableString, this method is called with
        the string in DEFAULT_OUTPUT_ENCODING. That encoding needs to be
        passed in to the superclass's __new__ or the superclass won't know
        how to handle non-ASCII characters.
        """
        if isinstance(value, unicode):
            return unicode.__new__(cls, value)
        return unicode.__new__(cls, value, DEFAULT_OUTPUT_ENCODING)

    def __getnewargs__(self):
        return (NavigableString.__str__(self),)

    def __getattr__(self, attr):
        """text.string gives you text. This is for backwards
        compatibility for Navigable*String, but for CData* it lets you
        get the string without the CData wrapper."""
        if attr == 'string':
            return self
        else:
            raise AttributeError, "'%s' object has no attribute '%s'" % (self.__class__.__name__, attr)

    def __unicode__(self):
        return str(self).decode(DEFAULT_OUTPUT_ENCODING)

    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
        if encoding:
            return self.encode(encoding)
        else:
            return self

class CData(NavigableString):

    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
        return "<![CDATA[%s]]>" % NavigableString.__str__(self, encoding)

class ProcessingInstruction(NavigableString):
    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
        output = self
        if "%SOUP-ENCODING%" in output:
            output = self.substituteEncoding(output, encoding)
        return "<?%s?>" % self.toEncoding(output, encoding)

class Comment(NavigableString):
    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
        return "<!--%s-->" % NavigableString.__str__(self, encoding)

class Declaration(NavigableString):
    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
        return "<!%s>" % NavigableString.__str__(self, encoding)

class Tag(PageElement):

    """Represents a found HTML tag with its attributes and contents."""

    def _invert(h):
        "Cheap function to invert a hash."
        i = {}
        for k,v in h.items():
            i[v] = k
        return i

    XML_ENTITIES_TO_SPECIAL_CHARS = { "apos" : "'",
                                      "quot" : '"',
                                      "amp" : "&",
                                      "lt" : "<",
                                      "gt" : ">" }

    XML_SPECIAL_CHARS_TO_ENTITIES = _invert(XML_ENTITIES_TO_SPECIAL_CHARS)

    def _convertEntities(self, match):
        """Used in a call to re.sub to replace HTML, XML, and numeric
        entities with the appropriate Unicode characters. If HTML
        entities are being converted, any unrecognized entities are
        escaped."""
        x = match.group(1)
        if self.convertHTMLEntities and x in name2codepoint:
            return unichr(name2codepoint[x])
        elif x in self.XML_ENTITIES_TO_SPECIAL_CHARS:
            if self.convertXMLEntities:
                return self.XML_ENTITIES_TO_SPECIAL_CHARS[x]
            else:
                return u'&%s;' % x
        elif len(x) > 0 and x[0] == '#':
            # Handle numeric entities
            if len(x) > 1 and x[1] == 'x':
                return unichr(int(x[2:], 16))
            else:
                return unichr(int(x[1:]))

        elif self.escapeUnrecognizedEntities:
            return u'&amp;%s;' % x
        else:
            return u'&%s;' % x

    def __init__(self, parser, name, attrs=None, parent=None,
                 previous=None):
        "Basic constructor."

        # We don't actually store the parser object: that lets extracted
        # chunks be garbage-collected
        self.parserClass = parser.__class__
        self.isSelfClosing = parser.isSelfClosingTag(name)
        self.name = name
        if attrs == None:
            attrs = []
        self.attrs = attrs
        self.contents = []
        self.setup(parent, previous)
        self.hidden = False
        self.containsSubstitutions = False
        self.convertHTMLEntities = parser.convertHTMLEntities
        self.convertXMLEntities = parser.convertXMLEntities
        self.escapeUnrecognizedEntities = parser.escapeUnrecognizedEntities

        # Convert any HTML, XML, or numeric entities in the attribute values.
        convert = lambda(k, val): (k,
                                   re.sub("&(#\d+|#x[0-9a-fA-F]+|\w+);",
                                          self._convertEntities,
                                          val))
        self.attrs = map(convert, self.attrs)

    def get(self, key, default=None):
        """Returns the value of the 'key' attribute for the tag, or
        the value given for 'default' if it doesn't have that
        attribute."""
        return self._getAttrMap().get(key, default)

    def has_key(self, key):
        return self._getAttrMap().has_key(key)

    def __getitem__(self, key):
        """tag[key] returns the value of the 'key' attribute for the tag,
        and throws an exception if it's not there."""
        return self._getAttrMap()[key]

    def __iter__(self):
        "Iterating over a tag iterates over its contents."
        return iter(self.contents)

    def __len__(self):
        "The length of a tag is the length of its list of contents."
        return len(self.contents)

    def __contains__(self, x):
        return x in self.contents

    def __nonzero__(self):
        "A tag is non-None even if it has no contents."
        return True

    def __setitem__(self, key, value):
        """Setting tag[key] sets the value of the 'key' attribute for the
        tag."""
        self._getAttrMap()
        self.attrMap[key] = value
        found = False
        for i in range(0, len(self.attrs)):
            if self.attrs[i][0] == key:
                self.attrs[i] = (key, value)
                found = True
        if not found:
            self.attrs.append((key, value))
        self._getAttrMap()[key] = value

    def __delitem__(self, key):
        "Deleting tag[key] deletes all 'key' attributes for the tag."
        for item in self.attrs:
            if item[0] == key:
                self.attrs.remove(item)
                #We don't break because bad HTML can define the same
                #attribute multiple times.
            self._getAttrMap()
            if self.attrMap.has_key(key):
                del self.attrMap[key]

    def __call__(self, *args, **kwargs):
        """Calling a tag like a function is the same as calling its
        findAll() method. Eg. tag('a') returns a list of all the A tags
        found within this tag."""
        return apply(self.findAll, args, kwargs)

    def __getattr__(self, tag):
        #print "Getattr %s.%s" % (self.__class__, tag)
        if len(tag) > 3 and tag.rfind('Tag') == len(tag)-3:
            return self.find(tag[:-3])
        elif tag.find('__') != 0:
            return self.find(tag)
        raise AttributeError, "'%s' object has no attribute '%s'" % (self.__class__, tag)

    def __eq__(self, other):
        """Returns true iff this tag has the same name, the same attributes,
        and the same contents (recursively) as the given tag.

        NOTE: right now this will return false if two tags have the
        same attributes in a different order. Should this be fixed?"""
        if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):
            return False
        for i in range(0, len(self.contents)):
            if self.contents[i] != other.contents[i]:
                return False
        return True

    def __ne__(self, other):
        """Returns true iff this tag is not identical to the other tag,
        as defined in __eq__."""
        return not self == other

    def __repr__(self, encoding=DEFAULT_OUTPUT_ENCODING):
        """Renders this tag as a string."""
        return self.__str__(encoding)

    def __unicode__(self):
        return self.__str__(None)

    BARE_AMPERSAND_OR_BRACKET = re.compile("([<>]|"
                                           + "&(?!#\d+;|#x[0-9a-fA-F]+;|\w+;)"
                                           + ")")

    def _sub_entity(self, x):
        """Used with a regular expression to substitute the
        appropriate XML entity for an XML special character."""
        return "&" + self.XML_SPECIAL_CHARS_TO_ENTITIES[x.group(0)[0]] + ";"

    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING,
                prettyPrint=False, indentLevel=0):
        """Returns a string or Unicode representation of this tag and
        its contents. To get Unicode, pass None for encoding.

        NOTE: since Python's HTML parser consumes whitespace, this
        method is not certain to reproduce the whitespace present in
        the original string."""

        encodedName = self.toEncoding(self.name, encoding)

        attrs = []
        if self.attrs:
            for key, val in self.attrs:
                fmt = '%s="%s"'
                if isString(val):
                    if self.containsSubstitutions and '%SOUP-ENCODING%' in val:
                        val = self.substituteEncoding(val, encoding)

                    # The attribute value either:
                    #
                    # * Contains no embedded double quotes or single quotes.
                    #   No problem: we enclose it in double quotes.
                    # * Contains embedded single quotes. No problem:
                    #   double quotes work here too.
                    # * Contains embedded double quotes. No problem:
                    #   we enclose it in single quotes.
                    # * Embeds both single _and_ double quotes. This
                    #   can't happen naturally, but it can happen if
                    #   you modify an attribute value after parsing
                    #   the document. Now we have a bit of a
                    #   problem. We solve it by enclosing the
                    #   attribute in single quotes, and escaping any
                    #   embedded single quotes to XML entities.
                    if '"' in val:
                        fmt = "%s='%s'"
                        if "'" in val:
                            # TODO: replace with apos when
                            # appropriate.
                            val = val.replace("'", "&squot;")

                    # Now we're okay w/r/t quotes. But the attribute
                    # value might also contain angle brackets, or
                    # ampersands that aren't part of entities. We need
                    # to escape those to XML entities too.
                    val = self.BARE_AMPERSAND_OR_BRACKET.sub(self._sub_entity, val)

                attrs.append(fmt % (self.toEncoding(key, encoding),
                                    self.toEncoding(val, encoding)))
        close = ''
        closeTag = ''
        if self.isSelfClosing:
            close = ' /'
        else:
            closeTag = '</%s>' % encodedName

        indentTag, indentContents = 0, 0
        if prettyPrint:
            indentTag = indentLevel
            space = (' ' * (indentTag-1))
            indentContents = indentTag + 1
        contents = self.renderContents(encoding, prettyPrint, indentContents)
        if self.hidden:
            s = contents
        else:
            s = []
            attributeString = ''
            if attrs:
                attributeString = ' ' + ' '.join(attrs)
            if prettyPrint:
                s.append(space)
            s.append('<%s%s%s>' % (encodedName, attributeString, close))
            if prettyPrint:
                s.append("\n")
            s.append(contents)
            if prettyPrint and contents and contents[-1] != "\n":
                s.append("\n")
            if prettyPrint and closeTag:
                s.append(space)
            s.append(closeTag)
            if prettyPrint and closeTag and self.nextSibling:
                s.append("\n")
            s = ''.join(s)
        return s

    def decompose(self):
        """Recursively destroys the contents of this tree."""
        contents = [i for i in self.contents]
        for i in contents:
            if isinstance(i, Tag):
                i.decompose()
            else:
                i.extract()
        self.extract()

    def prettify(self, encoding=DEFAULT_OUTPUT_ENCODING):
        return self.__str__(encoding, True)

    def renderContents(self, encoding=DEFAULT_OUTPUT_ENCODING,
                       prettyPrint=False, indentLevel=0):
        """Renders the contents of this tag as a string in the given
        encoding. If encoding is None, returns a Unicode string.."""
        s=[]
        for c in self:
            text = None
            if isinstance(c, NavigableString):
                text = c.__str__(encoding)
            elif isinstance(c, Tag):
                s.append(c.__str__(encoding, prettyPrint, indentLevel))
            if text and prettyPrint:
                text = text.strip()
            if text:
                if prettyPrint:
                    s.append(" " * (indentLevel-1))
                s.append(text)
                if prettyPrint:
                    s.append("\n")
        return ''.join(s)

    #Soup methods

    def find(self, name=None, attrs={}, recursive=True, text=None,
             **kwargs):
        """Return only the first child of this Tag matching the given
        criteria."""
        r = None
        l = self.findAll(name, attrs, recursive, text, 1, **kwargs)
        if l:
            r = l[0]
        return r
    findChild = find

    def findAll(self, name=None, attrs={}, recursive=True, text=None,
                limit=None, **kwargs):
        """Extracts a list of Tag objects that match the given
        criteria.  You can specify the name of the Tag and any
        attributes you want the Tag to have.

        The value of a key-value pair in the 'attrs' map can be a
        string, a list of strings, a regular expression object, or a
        callable that takes a string and returns whether or not the
        string matches for some custom definition of 'matches'. The
        same is true of the tag name."""
        generator = self.recursiveChildGenerator
        if not recursive:
            generator = self.childGenerator
        return self._findAll(name, attrs, text, limit, generator, **kwargs)
    findChildren = findAll

    # Pre-3.x compatibility methods
    first = find
    fetch = findAll

    def fetchText(self, text=None, recursive=True, limit=None):
        return self.findAll(text=text, recursive=recursive, limit=limit)

    def firstText(self, text=None, recursive=True):
        return self.find(text=text, recursive=recursive)

    #Private methods

    def _getAttrMap(self):
        """Initializes a map representation of this tag's attributes,
        if not already initialized."""
        if not getattr(self, 'attrMap'):
            self.attrMap = {}
            for (key, value) in self.attrs:
                self.attrMap[key] = value
        return self.attrMap

    #Generator methods
    def childGenerator(self):
        for i in range(0, len(self.contents)):
            yield self.contents[i]
        raise StopIteration

    def recursiveChildGenerator(self):
        stack = [(self, 0)]
        while stack:
            tag, start = stack.pop()
            if isinstance(tag, Tag):
                for i in range(start, len(tag.contents)):
                    a = tag.contents[i]
                    yield a
                    if isinstance(a, Tag) and tag.contents:
                        if i < len(tag.contents) - 1:
                            stack.append((tag, i+1))
                        stack.append((a, 0))
                        break
        raise StopIteration

# Next, a couple classes to represent queries and their results.
class SoupStrainer:
    """Encapsulates a number of ways of matching a markup element (tag or
    text)."""

    def __init__(self, name=None, attrs={}, text=None, **kwargs):
        self.name = name
        if isString(attrs):
            kwargs['class'] = attrs
            attrs = None
        if kwargs:
            if attrs:
                attrs = attrs.copy()
                attrs.update(kwargs)
            else:
                attrs = kwargs
        self.attrs = attrs
        self.text = text

    def __str__(self):
        if self.text:
            return self.text
        else:
            return "%s|%s" % (self.name, self.attrs)

    def searchTag(self, markupName=None, markupAttrs={}):
        found = None
        markup = None
        if isinstance(markupName, Tag):
            markup = markupName
            markupAttrs = markup
        callFunctionWithTagData = callable(self.name) \
                                and not isinstance(markupName, Tag)

        if (not self.name) \
               or callFunctionWithTagData \
               or (markup and self._matches(markup, self.name)) \
               or (not markup and self._matches(markupName, self.name)):
            if callFunctionWithTagData:
                match = self.name(markupName, markupAttrs)
            else:
                match = True
                markupAttrMap = None
                for attr, matchAgainst in self.attrs.items():
                    if not markupAttrMap:
                         if hasattr(markupAttrs, 'get'):
                            markupAttrMap = markupAttrs
                         else:
                            markupAttrMap = {}
                            for k,v in markupAttrs:
                                markupAttrMap[k] = v
                    attrValue = markupAttrMap.get(attr)
                    if not self._matches(attrValue, matchAgainst):
                        match = False
                        break
            if match:
                if markup:
                    found = markup
                else:
                    found = markupName
        return found

    def search(self, markup):
        #print 'looking for %s in %s' % (self, markup)
        found = None
        # If given a list of items, scan it for a text element that
        # matches.
        if isList(markup) and not isinstance(markup, Tag):
            for element in markup:
                if isinstance(element, NavigableString) \
                       and self.search(element):
                    found = element
                    break
        # If it's a Tag, make sure its name or attributes match.
        # Don't bother with Tags if we're searching for text.
        elif isinstance(markup, Tag):
            if not self.text:
                found = self.searchTag(markup)
        # If it's text, make sure the text matches.
        elif isinstance(markup, NavigableString) or \
                 isString(markup):
            if self._matches(markup, self.text):
                found = markup
        else:
            raise Exception, "I don't know how to match against a %s" \
                  % markup.__class__
        return found

    def _matches(self, markup, matchAgainst):
        #print "Matching %s against %s" % (markup, matchAgainst)
        result = False
        if matchAgainst == True and type(matchAgainst) == types.BooleanType:
            result = markup != None
        elif callable(matchAgainst):
            result = matchAgainst(markup)
        else:
            #Custom match methods take the tag as an argument, but all
            #other ways of matching match the tag name as a string.
            if isinstance(markup, Tag):
                markup = markup.name
            if markup and not isString(markup):
                markup = unicode(markup)
            #Now we know that chunk is either a string, or None.
            if hasattr(matchAgainst, 'match'):
                # It's a regexp object.
                result = markup and matchAgainst.search(markup)
            elif isList(matchAgainst):
                result = markup in matchAgainst
            elif hasattr(matchAgainst, 'items'):
                result = markup.has_key(matchAgainst)
            elif matchAgainst and isString(markup):
                if isinstance(markup, unicode):
                    matchAgainst = unicode(matchAgainst)
                else:
                    matchAgainst = str(matchAgainst)

            if not result:
                result = matchAgainst == markup
        return result

class ResultSet(list):
    """A ResultSet is just a list that keeps track of the SoupStrainer
    that created it."""
    def __init__(self, source):
        list.__init__([])
        self.source = source

# Now, some helper functions.

def isList(l):
    """Convenience method that works with all 2.x versions of Python
    to determine whether or not something is listlike."""
    return hasattr(l, '__iter__') \
           or (type(l) in (types.ListType, types.TupleType))

def isString(s):
    """Convenience method that works with all 2.x versions of Python
    to determine whether or not something is stringlike."""
    try:
        return isinstance(s, unicode) or isinstance(s, basestring)
    except NameError:
        return isinstance(s, str)

def buildTagMap(default, *args):
    """Turns a list of maps, lists, or scalars into a single map.
    Used to build the SELF_CLOSING_TAGS, NESTABLE_TAGS, and
    NESTING_RESET_TAGS maps out of lists and partial maps."""
    built = {}
    for portion in args:
        if hasattr(portion, 'items'):
            #It's a map. Merge it.
            for k,v in portion.items():
                built[k] = v
        elif isList(portion):
            #It's a list. Map each item to the default.
            for k in portion:
                built[k] = default
        else:
            #It's a scalar. Map it to the default.
            built[portion] = default
    return built

# Now, the parser classes.

class BeautifulStoneSoup(Tag, SGMLParser):

    """This class contains the basic parser and search code. It defines
    a parser that knows nothing about tag behavior except for the
    following:

      You can't close a tag without closing all the tags it encloses.
      That is, "<foo><bar></foo>" actually means
      "<foo><bar></bar></foo>".

    [Another possible explanation is "<foo><bar /></foo>", but since
    this class defines no SELF_CLOSING_TAGS, it will never use that
    explanation.]

    This class is useful for parsing XML or made-up markup languages,
    or when BeautifulSoup makes an assumption counter to what you were
    expecting."""

    SELF_CLOSING_TAGS = {}
    NESTABLE_TAGS = {}
    RESET_NESTING_TAGS = {}
    QUOTE_TAGS = {}
    PRESERVE_WHITESPACE_TAGS = []

    MARKUP_MASSAGE = [(re.compile('(<[^<>]*)/>'),
                       lambda x: x.group(1) + ' />'),
                      (re.compile('<!\s+([^<>]*)>'),
                       lambda x: '<!' + x.group(1) + '>')
                      ]

    ROOT_TAG_NAME = u'[document]'

    HTML_ENTITIES = "html"
    XML_ENTITIES = "xml"
    XHTML_ENTITIES = "xhtml"
    # TODO: This only exists for backwards-compatibility
    ALL_ENTITIES = XHTML_ENTITIES

    # Used when determining whether a text node is all whitespace and
    # can be replaced with a single space. A text node that contains
    # fancy Unicode spaces (usually non-breaking) should be left
    # alone.
    STRIP_ASCII_SPACES = { 9: None, 10: None, 12: None, 13: None, 32: None, }

    def __init__(self, markup="", parseOnlyThese=None, fromEncoding=None,
                 markupMassage=True, smartQuotesTo=XML_ENTITIES,
                 convertEntities=None, selfClosingTags=None, isHTML=False):
        """The Soup object is initialized as the 'root tag', and the
        provided markup (which can be a string or a file-like object)
        is fed into the underlying parser.

        sgmllib will process most bad HTML, and the BeautifulSoup
        class has some tricks for dealing with some HTML that kills
        sgmllib, but Beautiful Soup can nonetheless choke or lose data
        if your data uses self-closing tags or declarations
        incorrectly.

        By default, Beautiful Soup uses regexes to sanitize input,
        avoiding the vast majority of these problems. If the problems
        don't apply to you, pass in False for markupMassage, and
        you'll get better performance.

        The default parser massage techniques fix the two most common
        instances of invalid HTML that choke sgmllib:

         <br/> (No space between name of closing tag and tag close)
         <! --Comment--> (Extraneous whitespace in declaration)

        You can pass in a custom list of (RE object, replace method)
        tuples to get Beautiful Soup to scrub your input the way you
        want."""

        self.parseOnlyThese = parseOnlyThese
        self.fromEncoding = fromEncoding
        self.smartQuotesTo = smartQuotesTo
        self.convertEntities = convertEntities
        # Set the rules for how we'll deal with the entities we
        # encounter
        if self.convertEntities:
            # It doesn't make sense to convert encoded characters to
            # entities even while you're converting entities to Unicode.
            # Just convert it all to Unicode.
            self.smartQuotesTo = None
            if convertEntities == self.HTML_ENTITIES:
                self.convertXMLEntities = False
                self.convertHTMLEntities = True
                self.escapeUnrecognizedEntities = True
            elif convertEntities == self.XHTML_ENTITIES:
                self.convertXMLEntities = True
                self.convertHTMLEntities = True
                self.escapeUnrecognizedEntities = False
            elif convertEntities == self.XML_ENTITIES:
                self.convertXMLEntities = True
                self.convertHTMLEntities = False
                self.escapeUnrecognizedEntities = False
        else:
            self.convertXMLEntities = False
            self.convertHTMLEntities = False
            self.escapeUnrecognizedEntities = False

        self.instanceSelfClosingTags = buildTagMap(None, selfClosingTags)
        SGMLParser.__init__(self)

        if hasattr(markup, 'read'):        # It's a file-type object.
            markup = markup.read()
        self.markup = markup
        self.markupMassage = markupMassage
        try:
            self._feed(isHTML=isHTML)
        except StopParsing:
            pass
        self.markup = None                 # The markup can now be GCed

    def convert_charref(self, name):
        """This method fixes a bug in Python's SGMLParser."""
        try:
            n = int(name)
        except ValueError:
            return
        if not 0 <= n <= 127 : # ASCII ends at 127, not 255
            return
        return self.convert_codepoint(n)

    def _feed(self, inDocumentEncoding=None, isHTML=False):
        # Convert the document to Unicode.
        markup = self.markup
        if isinstance(markup, unicode):
            if not hasattr(self, 'originalEncoding'):
                self.originalEncoding = None
        else:
            dammit = UnicodeDammit\
                     (markup, [self.fromEncoding, inDocumentEncoding],
                      smartQuotesTo=self.smartQuotesTo, isHTML=isHTML)
            markup = dammit.unicode
            self.originalEncoding = dammit.originalEncoding
            self.declaredHTMLEncoding = dammit.declaredHTMLEncoding
        if markup:
            if self.markupMassage:
                if not isList(self.markupMassage):
                    self.markupMassage = self.MARKUP_MASSAGE
                for fix, m in self.markupMassage:
                    markup = fix.sub(m, markup)
                # TODO: We get rid of markupMassage so that the
                # soup object can be deepcopied later on. Some
                # Python installations can't copy regexes. If anyone
                # was relying on the existence of markupMassage, this
                # might cause problems.
                del(self.markupMassage)
        self.reset()

        SGMLParser.feed(self, markup)
        # Close out any unfinished strings and close all the open tags.
        self.endData()
        while self.currentTag.name != self.ROOT_TAG_NAME:
            self.popTag()

    def __getattr__(self, methodName):
        """This method routes method call requests to either the SGMLParser
        superclass or the Tag superclass, depending on the method name."""
        #print "__getattr__ called on %s.%s" % (self.__class__, methodName)

        if methodName.find('start_') == 0 or methodName.find('end_') == 0 \
               or methodName.find('do_') == 0:
            return SGMLParser.__getattr__(self, methodName)
        elif methodName.find('__') != 0:
            return Tag.__getattr__(self, methodName)
        else:
            raise AttributeError

    def isSelfClosingTag(self, name):
        """Returns true iff the given string is the name of a
        self-closing tag according to this parser."""
        return self.SELF_CLOSING_TAGS.has_key(name) \
               or self.instanceSelfClosingTags.has_key(name)

    def reset(self):
        Tag.__init__(self, self, self.ROOT_TAG_NAME)
        self.hidden = 1
        SGMLParser.reset(self)
        self.currentData = []
        self.currentTag = None
        self.tagStack = []
        self.quoteStack = []
        self.pushTag(self)

    def popTag(self):
        tag = self.tagStack.pop()
        # Tags with just one string-owning child get the child as a
        # 'string' property, so that soup.tag.string is shorthand for
        # soup.tag.contents[0]
        if len(self.currentTag.contents) == 1 and \
           isinstance(self.currentTag.contents[0], NavigableString):
            self.currentTag.string = self.currentTag.contents[0]

        #print "Pop", tag.name
        if self.tagStack:
            self.currentTag = self.tagStack[-1]
        return self.currentTag

    def pushTag(self, tag):
        #print "Push", tag.name
        if self.currentTag:
            self.currentTag.contents.append(tag)
        self.tagStack.append(tag)
        self.currentTag = self.tagStack[-1]

    def endData(self, containerClass=NavigableString):
        if self.currentData:
            currentData = u''.join(self.currentData)
            if (currentData.translate(self.STRIP_ASCII_SPACES) == '' and
                not set([tag.name for tag in self.tagStack]).intersection(
                    self.PRESERVE_WHITESPACE_TAGS)):
                if '\n' in currentData:
                    currentData = '\n'
                else:
                    currentData = ' '
            self.currentData = []
            if self.parseOnlyThese and len(self.tagStack) <= 1 and \
                   (not self.parseOnlyThese.text or \
                    not self.parseOnlyThese.search(currentData)):
                return
            o = containerClass(currentData)
            o.setup(self.currentTag, self.previous)
            if self.previous:
                self.previous.next = o
            self.previous = o
            self.currentTag.contents.append(o)


    def _popToTag(self, name, inclusivePop=True):
        """Pops the tag stack up to and including the most recent
        instance of the given tag. If inclusivePop is false, pops the tag
        stack up to but *not* including the most recent instqance of
        the given tag."""
        #print "Popping to %s" % name
        if name == self.ROOT_TAG_NAME:
            return

        numPops = 0
        mostRecentTag = None
        for i in range(len(self.tagStack)-1, 0, -1):
            if name == self.tagStack[i].name:
                numPops = len(self.tagStack)-i
                break
        if not inclusivePop:
            numPops = numPops - 1

        for i in range(0, numPops):
            mostRecentTag = self.popTag()
        return mostRecentTag

    def _smartPop(self, name):

        """We need to pop up to the previous tag of this type, unless
        one of this tag's nesting reset triggers comes between this
        tag and the previous tag of this type, OR unless this tag is a
        generic nesting trigger and another generic nesting trigger
        comes between this tag and the previous tag of this type.

        Examples:
         <p>Foo<b>Bar *<p>* should pop to 'p', not 'b'.
         <p>Foo<table>Bar *<p>* should pop to 'table', not 'p'.
         <p>Foo<table><tr>Bar *<p>* should pop to 'tr', not 'p'.

         <li><ul><li> *<li>* should pop to 'ul', not the first 'li'.
         <tr><table><tr> *<tr>* should pop to 'table', not the first 'tr'
         <td><tr><td> *<td>* should pop to 'tr', not the first 'td'
        """

        nestingResetTriggers = self.NESTABLE_TAGS.get(name)
        isNestable = nestingResetTriggers != None
        isResetNesting = self.RESET_NESTING_TAGS.has_key(name)
        popTo = None
        inclusive = True
        for i in range(len(self.tagStack)-1, 0, -1):
            p = self.tagStack[i]
            if (not p or p.name == name) and not isNestable:
                #Non-nestable tags get popped to the top or to their
                #last occurance.
                popTo = name
                break
            if (nestingResetTriggers != None
                and p.name in nestingResetTriggers) \
                or (nestingResetTriggers == None and isResetNesting
                    and self.RESET_NESTING_TAGS.has_key(p.name)):

                #If we encounter one of the nesting reset triggers
                #peculiar to this tag, or we encounter another tag
                #that causes nesting to reset, pop up to but not
                #including that tag.
                popTo = p.name
                inclusive = False
                break
            p = p.parent
        if popTo:
            self._popToTag(popTo, inclusive)

    def unknown_starttag(self, name, attrs, selfClosing=0):
        #print "Start tag %s: %s" % (name, attrs)
        if self.quoteStack:
            #This is not a real tag.
            #print "<%s> is not real!" % name
            attrs = ''.join(map(lambda(x, y): ' %s="%s"' % (x, y), attrs))
            self.handle_data('<%s%s>' % (name, attrs))
            return
        self.endData()

        if not self.isSelfClosingTag(name) and not selfClosing:
            self._smartPop(name)

        if self.parseOnlyThese and len(self.tagStack) <= 1 \
               and (self.parseOnlyThese.text or not self.parseOnlyThese.searchTag(name, attrs)):
            return

        tag = Tag(self, name, attrs, self.currentTag, self.previous)
        if self.previous:
            self.previous.next = tag
        self.previous = tag
        self.pushTag(tag)
        if selfClosing or self.isSelfClosingTag(name):
            self.popTag()
        if name in self.QUOTE_TAGS:
            #print "Beginning quote (%s)" % name
            self.quoteStack.append(name)
            self.literal = 1
        return tag

    def unknown_endtag(self, name):
        #print "End tag %s" % name
        if self.quoteStack and self.quoteStack[-1] != name:
            #This is not a real end tag.
            #print "</%s> is not real!" % name
            self.handle_data('</%s>' % name)
            return
        self.endData()
        self._popToTag(name)
        if self.quoteStack and self.quoteStack[-1] == name:
            self.quoteStack.pop()
            self.literal = (len(self.quoteStack) > 0)

    def handle_data(self, data):
        self.currentData.append(data)

    def _toStringSubclass(self, text, subclass):
        """Adds a certain piece of text to the tree as a NavigableString
        subclass."""
        self.endData()
        self.handle_data(text)
        self.endData(subclass)

    def handle_pi(self, text):
        """Handle a processing instruction as a ProcessingInstruction
        object, possibly one with a %SOUP-ENCODING% slot into which an
        encoding will be plugged later."""
        if text[:3] == "xml":
            text = u"xml version='1.0' encoding='%SOUP-ENCODING%'"
        self._toStringSubclass(text, ProcessingInstruction)

    def handle_comment(self, text):
        "Handle comments as Comment objects."
        self._toStringSubclass(text, Comment)

    def handle_charref(self, ref):
        "Handle character references as data."
        if self.convertEntities:
            data = unichr(int(ref))
        else:
            data = '&#%s;' % ref
        self.handle_data(data)

    def handle_entityref(self, ref):
        """Handle entity references as data, possibly converting known
        HTML and/or XML entity references to the corresponding Unicode
        characters."""
        data = None
        if self.convertHTMLEntities:
            try:
                data = unichr(name2codepoint[ref])
            except KeyError:
                pass

        if not data and self.convertXMLEntities:
                data = self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref)

        if not data and self.convertHTMLEntities and \
            not self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref):
                # TODO: We've got a problem here. We're told this is
                # an entity reference, but it's not an XML entity
                # reference or an HTML entity reference. Nonetheless,
                # the logical thing to do is to pass it through as an
                # unrecognized entity reference.
                #
                # Except: when the input is "&carol;" this function
                # will be called with input "carol". When the input is
                # "AT&T", this function will be called with input
                # "T". We have no way of knowing whether a semicolon
                # was present originally, so we don't know whether
                # this is an unknown entity or just a misplaced
                # ampersand.
                #
                # The more common case is a misplaced ampersand, so I
                # escape the ampersand and omit the trailing semicolon.
                data = "&amp;%s" % ref
        if not data:
            # This case is different from the one above, because we
            # haven't already gone through a supposedly comprehensive
            # mapping of entities to Unicode characters. We might not
            # have gone through any mapping at all. So the chances are
            # very high that this is a real entity, and not a
            # misplaced ampersand.
            data = "&%s;" % ref
        self.handle_data(data)

    def handle_decl(self, data):
        "Handle DOCTYPEs and the like as Declaration objects."
        self._toStringSubclass(data, Declaration)

    def parse_declaration(self, i):
        """Treat a bogus SGML declaration as raw data. Treat a CDATA
        declaration as a CData object."""
        j = None
        if self.rawdata[i:i+9] == '<![CDATA[':
             k = self.rawdata.find(']]>', i)
             if k == -1:
                 k = len(self.rawdata)
             data = self.rawdata[i+9:k]
             j = k+3
             self._toStringSubclass(data, CData)
        else:
            try:
                j = SGMLParser.parse_declaration(self, i)
            except SGMLParseError:
                toHandle = self.rawdata[i:]
                self.handle_data(toHandle)
                j = i + len(toHandle)
        return j

class BeautifulSoup(BeautifulStoneSoup):

    """This parser knows the following facts about HTML:

    * Some tags have no closing tag and should be interpreted as being
      closed as soon as they are encountered.

    * The text inside some tags (ie. 'script') may contain tags which
      are not really part of the document and which should be parsed
      as text, not tags. If you want to parse the text as tags, you can
      always fetch it and parse it explicitly.

    * Tag nesting rules:

      Most tags can't be nested at all. For instance, the occurance of
      a <p> tag should implicitly close the previous <p> tag.

       <p>Para1<p>Para2
        should be transformed into:
       <p>Para1</p><p>Para2

      Some tags can be nested arbitrarily. For instance, the occurance
      of a <blockquote> tag should _not_ implicitly close the previous
      <blockquote> tag.

       Alice said: <blockquote>Bob said: <blockquote>Blah
        should NOT be transformed into:
       Alice said: <blockquote>Bob said: </blockquote><blockquote>Blah

      Some tags can be nested, but the nesting is reset by the
      interposition of other tags. For instance, a <tr> tag should
      implicitly close the previous <tr> tag within the same <table>,
      but not close a <tr> tag in another table.

       <table><tr>Blah<tr>Blah
        should be transformed into:
       <table><tr>Blah</tr><tr>Blah
        but,
       <tr>Blah<table><tr>Blah
        should NOT be transformed into
       <tr>Blah<table></tr><tr>Blah

    Differing assumptions about tag nesting rules are a major source
    of problems with the BeautifulSoup class. If BeautifulSoup is not
    treating as nestable a tag your page author treats as nestable,
    try ICantBelieveItsBeautifulSoup, MinimalSoup, or
    BeautifulStoneSoup before writing your own subclass."""

    def __init__(self, *args, **kwargs):
        if not kwargs.has_key('smartQuotesTo'):
            kwargs['smartQuotesTo'] = self.HTML_ENTITIES
        kwargs['isHTML'] = True
        BeautifulStoneSoup.__init__(self, *args, **kwargs)

    SELF_CLOSING_TAGS = buildTagMap(None,
                                    ['br' , 'hr', 'input', 'img', 'meta',
                                    'spacer', 'link', 'frame', 'base'])

    PRESERVE_WHITESPACE_TAGS = set(['pre', 'textarea'])

    QUOTE_TAGS = {'script' : None, 'textarea' : None}

    #According to the HTML standard, each of these inline tags can
    #contain another tag of the same type. Furthermore, it's common
    #to actually use these tags this way.
    NESTABLE_INLINE_TAGS = ['span', 'font', 'q', 'object', 'bdo', 'sub', 'sup',
                            'center']

    #According to the HTML standard, these block tags can contain
    #another tag of the same type. Furthermore, it's common
    #to actually use these tags this way.
    NESTABLE_BLOCK_TAGS = ['blockquote', 'div', 'fieldset', 'ins', 'del']

    #Lists can contain other lists, but there are restrictions.
    NESTABLE_LIST_TAGS = { 'ol' : [],
                           'ul' : [],
                           'li' : ['ul', 'ol'],
                           'dl' : [],
                           'dd' : ['dl'],
                           'dt' : ['dl'] }

    #Tables can contain other tables, but there are restrictions.
    NESTABLE_TABLE_TAGS = {'table' : [],
                           'tr' : ['table', 'tbody', 'tfoot', 'thead'],
                           'td' : ['tr'],
                           'th' : ['tr'],
                           'thead' : ['table'],
                           'tbody' : ['table'],
                           'tfoot' : ['table'],
                           }

    NON_NESTABLE_BLOCK_TAGS = ['address', 'form', 'p', 'pre']

    #If one of these tags is encountered, all tags up to the next tag of
    #this type are popped.
    RESET_NESTING_TAGS = buildTagMap(None, NESTABLE_BLOCK_TAGS, 'noscript',
                                     NON_NESTABLE_BLOCK_TAGS,
                                     NESTABLE_LIST_TAGS,
                                     NESTABLE_TABLE_TAGS)

    NESTABLE_TAGS = buildTagMap([], NESTABLE_INLINE_TAGS, NESTABLE_BLOCK_TAGS,
                                NESTABLE_LIST_TAGS, NESTABLE_TABLE_TAGS)

    # Used to detect the charset in a META tag; see start_meta
    CHARSET_RE = re.compile("((^|;)\s*charset=)([^;]*)", re.M)

    def start_meta(self, attrs):
        """Beautiful Soup can detect a charset included in a META tag,
        try to convert the document to that charset, and re-parse the
        document from the beginning."""
        httpEquiv = None
        contentType = None
        contentTypeIndex = None
        tagNeedsEncodingSubstitution = False

        for i in range(0, len(attrs)):
            key, value = attrs[i]
            key = key.lower()
            if key == 'http-equiv':
                httpEquiv = value
            elif key == 'content':
                contentType = value
                contentTypeIndex = i

        if httpEquiv and contentType: # It's an interesting meta tag.
            match = self.CHARSET_RE.search(contentType)
            if match:
                if (self.declaredHTMLEncoding is not None or
                    self.originalEncoding == self.fromEncoding):
                    # An HTML encoding was sniffed while converting
                    # the document to Unicode, or an HTML encoding was
                    # sniffed during a previous pass through the
                    # document, or an encoding was specified
                    # explicitly and it worked. Rewrite the meta tag.
                    def rewrite(match):
                        return match.group(1) + "%SOUP-ENCODING%"
                    newAttr = self.CHARSET_RE.sub(rewrite, contentType)
                    attrs[contentTypeIndex] = (attrs[contentTypeIndex][0],
                                               newAttr)
                    tagNeedsEncodingSubstitution = True
                else:
                    # This is our first pass through the document.
                    # Go through it again with the encoding information.
                    newCharset = match.group(3)
                    if newCharset and newCharset != self.originalEncoding:
                        self.declaredHTMLEncoding = newCharset
                        self._feed(self.declaredHTMLEncoding)
                        raise StopParsing
                    pass
        tag = self.unknown_starttag("meta", attrs)
        if tag and tagNeedsEncodingSubstitution:
            tag.containsSubstitutions = True

class StopParsing(Exception):
    pass

class ICantBelieveItsBeautifulSoup(BeautifulSoup):

    """The BeautifulSoup class is oriented towards skipping over
    common HTML errors like unclosed tags. However, sometimes it makes
    errors of its own. For instance, consider this fragment:

     <b>Foo<b>Bar</b></b>

    This is perfectly valid (if bizarre) HTML. However, the
    BeautifulSoup class will implicitly close the first b tag when it
    encounters the second 'b'. It will think the author wrote
    "<b>Foo<b>Bar", and didn't close the first 'b' tag, because
    there's no real-world reason to bold something that's already
    bold. When it encounters '</b></b>' it will close two more 'b'
    tags, for a grand total of three tags closed instead of two. This
    can throw off the rest of your document structure. The same is
    true of a number of other tags, listed below.

    It's much more common for someone to forget to close a 'b' tag
    than to actually use nested 'b' tags, and the BeautifulSoup class
    handles the common case. This class handles the not-co-common
    case: where you can't believe someone wrote what they did, but
    it's valid HTML and BeautifulSoup screwed up by assuming it
    wouldn't be."""

    I_CANT_BELIEVE_THEYRE_NESTABLE_INLINE_TAGS = \
     ['em', 'big', 'i', 'small', 'tt', 'abbr', 'acronym', 'strong',
      'cite', 'code', 'dfn', 'kbd', 'samp', 'strong', 'var', 'b',
      'big']

    I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS = ['noscript']

    NESTABLE_TAGS = buildTagMap([], BeautifulSoup.NESTABLE_TAGS,
                                I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS,
                                I_CANT_BELIEVE_THEYRE_NESTABLE_INLINE_TAGS)

class MinimalSoup(BeautifulSoup):
    """The MinimalSoup class is for parsing HTML that contains
    pathologically bad markup. It makes no assumptions about tag
    nesting, but it does know which tags are self-closing, that
    <script> tags contain Javascript and should not be parsed, that
    META tags may contain encoding information, and so on.

    This also makes it better for subclassing than BeautifulStoneSoup
    or BeautifulSoup."""

    RESET_NESTING_TAGS = buildTagMap('noscript')
    NESTABLE_TAGS = {}

class BeautifulSOAP(BeautifulStoneSoup):
    """This class will push a tag with only a single string child into
    the tag's parent as an attribute. The attribute's name is the tag
    name, and the value is the string child. An example should give
    the flavor of the change:

    <foo><bar>baz</bar></foo>
     =>
    <foo bar="baz"><bar>baz</bar></foo>

    You can then access fooTag['bar'] instead of fooTag.barTag.string.

    This is, of course, useful for scraping structures that tend to
    use subelements instead of attributes, such as SOAP messages. Note
    that it modifies its input, so don't print the modified version
    out.

    I'm not sure how many people really want to use this class; let me
    know if you do. Mainly I like the name."""

    def popTag(self):
        if len(self.tagStack) > 1:
            tag = self.tagStack[-1]
            parent = self.tagStack[-2]
            parent._getAttrMap()
            if (isinstance(tag, Tag) and len(tag.contents) == 1 and
                isinstance(tag.contents[0], NavigableString) and
                not parent.attrMap.has_key(tag.name)):
                parent[tag.name] = tag.contents[0]
        BeautifulStoneSoup.popTag(self)

#Enterprise class names! It has come to our attention that some people
#think the names of the Beautiful Soup parser classes are too silly
#and "unprofessional" for use in enterprise screen-scraping. We feel
#your pain! For such-minded folk, the Beautiful Soup Consortium And
#All-Night Kosher Bakery recommends renaming this file to
#"RobustParser.py" (or, in cases of extreme enterprisiness,
#"RobustParserBeanInterface.class") and using the following
#enterprise-friendly class aliases:
class RobustXMLParser(BeautifulStoneSoup):
    pass
class RobustHTMLParser(BeautifulSoup):
    pass
class RobustWackAssHTMLParser(ICantBelieveItsBeautifulSoup):
    pass
class RobustInsanelyWackAssHTMLParser(MinimalSoup):
    pass
class SimplifyingSOAPParser(BeautifulSOAP):
    pass

######################################################
#
# Bonus library: Unicode, Dammit
#
# This class forces XML data into a standard format (usually to UTF-8
# or Unicode).  It is heavily based on code from Mark Pilgrim's
# Universal Feed Parser. It does not rewrite the XML or HTML to
# reflect a new encoding: that happens in BeautifulStoneSoup.handle_pi
# (XML) and BeautifulSoup.start_meta (HTML).

# Autodetects character encodings.
# Download from http://chardet.feedparser.org/
try:
    import chardet
#    import chardet.constants
#    chardet.constants._debug = 1
except ImportError:
    chardet = None

# cjkcodecs and iconv_codec make Python know about more character encodings.
# Both are available from http://cjkpython.i18n.org/
# They're built in if you use Python 2.4.
try:
    import cjkcodecs.aliases
except ImportError:
    pass
try:
    import iconv_codec
except ImportError:
    pass

class UnicodeDammit:
    """A class for detecting the encoding of a *ML document and
    converting it to a Unicode string. If the source encoding is
    windows-1252, can replace MS smart quotes with their HTML or XML
    equivalents."""

    # This dictionary maps commonly seen values for "charset" in HTML
    # meta tags to the corresponding Python codec names. It only covers
    # values that aren't in Python's aliases and can't be determined
    # by the heuristics in find_codec.
    CHARSET_ALIASES = { "macintosh" : "mac-roman",
                        "x-sjis" : "shift-jis" }

    def __init__(self, markup, overrideEncodings=[],
                 smartQuotesTo='xml', isHTML=False):
        self.declaredHTMLEncoding = None
        self.markup, documentEncoding, sniffedEncoding = \
                     self._detectEncoding(markup, isHTML)
        self.smartQuotesTo = smartQuotesTo
        self.triedEncodings = []
        if markup == '' or isinstance(markup, unicode):
            self.originalEncoding = None
            self.unicode = unicode(markup)
            return

        u = None
        for proposedEncoding in overrideEncodings:
            u = self._convertFrom(proposedEncoding)
            if u: break
        if not u:
            for proposedEncoding in (documentEncoding, sniffedEncoding):
                u = self._convertFrom(proposedEncoding)
                if u: break

        # If no luck and we have auto-detection library, try that:
        if not u and chardet and not isinstance(self.markup, unicode):
            u = self._convertFrom(chardet.detect(self.markup)['encoding'])

        # As a last resort, try utf-8 and windows-1252:
        if not u:
            for proposed_encoding in ("utf-8", "windows-1252"):
                u = self._convertFrom(proposed_encoding)
                if u: break

        self.unicode = u
        if not u: self.originalEncoding = None

    def _subMSChar(self, orig):
        """Changes a MS smart quote character to an XML or HTML
        entity."""
        sub = self.MS_CHARS.get(orig)
        if type(sub) == types.TupleType:
            if self.smartQuotesTo == 'xml':
                sub = '&#x%s;' % sub[1]
            else:
                sub = '&%s;' % sub[0]
        return sub

    def _convertFrom(self, proposed):
        proposed = self.find_codec(proposed)
        if not proposed or proposed in self.triedEncodings:
            return None
        self.triedEncodings.append(proposed)
        markup = self.markup

        # Convert smart quotes to HTML if coming from an encoding
        # that might have them.
        if self.smartQuotesTo and proposed.lower() in("windows-1252",
                                                      "iso-8859-1",
                                                      "iso-8859-2"):
            markup = re.compile("([\x80-\x9f])").sub \
                     (lambda(x): self._subMSChar(x.group(1)),
                      markup)

        try:
            # print "Trying to convert document to %s" % proposed
            u = self._toUnicode(markup, proposed)
            self.markup = u
            self.originalEncoding = proposed
        except Exception, e:
            # print "That didn't work!"
            # print e
            return None
        #print "Correct encoding: %s" % proposed
        return self.markup

    def _toUnicode(self, data, encoding):
        '''Given a string and its encoding, decodes the string into Unicode.
        %encoding is a string recognized by encodings.aliases'''

        # strip Byte Order Mark (if present)
        if (len(data) >= 4) and (data[:2] == '\xfe\xff') \
               and (data[2:4] != '\x00\x00'):
            encoding = 'utf-16be'
            data = data[2:]
        elif (len(data) >= 4) and (data[:2] == '\xff\xfe') \
                 and (data[2:4] != '\x00\x00'):
            encoding = 'utf-16le'
            data = data[2:]
        elif data[:3] == '\xef\xbb\xbf':
            encoding = 'utf-8'
            data = data[3:]
        elif data[:4] == '\x00\x00\xfe\xff':
            encoding = 'utf-32be'
            data = data[4:]
        elif data[:4] == '\xff\xfe\x00\x00':
            encoding = 'utf-32le'
            data = data[4:]
        newdata = unicode(data, encoding)
        return newdata

    def _detectEncoding(self, xml_data, isHTML=False):
        """Given a document, tries to detect its XML encoding."""
        xml_encoding = sniffed_xml_encoding = None
        try:
            if xml_data[:4] == '\x4c\x6f\xa7\x94':
                # EBCDIC
                xml_data = self._ebcdic_to_ascii(xml_data)
            elif xml_data[:4] == '\x00\x3c\x00\x3f':
                # UTF-16BE
                sniffed_xml_encoding = 'utf-16be'
                xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')
            elif (len(xml_data) >= 4) and (xml_data[:2] == '\xfe\xff') \
                     and (xml_data[2:4] != '\x00\x00'):
                # UTF-16BE with BOM
                sniffed_xml_encoding = 'utf-16be'
                xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')
            elif xml_data[:4] == '\x3c\x00\x3f\x00':
                # UTF-16LE
                sniffed_xml_encoding = 'utf-16le'
                xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')
            elif (len(xml_data) >= 4) and (xml_data[:2] == '\xff\xfe') and \
                     (xml_data[2:4] != '\x00\x00'):
                # UTF-16LE with BOM
                sniffed_xml_encoding = 'utf-16le'
                xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')
            elif xml_data[:4] == '\x00\x00\x00\x3c':
                # UTF-32BE
                sniffed_xml_encoding = 'utf-32be'
                xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')
            elif xml_data[:4] == '\x3c\x00\x00\x00':
                # UTF-32LE
                sniffed_xml_encoding = 'utf-32le'
                xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')
            elif xml_data[:4] == '\x00\x00\xfe\xff':
                # UTF-32BE with BOM
                sniffed_xml_encoding = 'utf-32be'
                xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')
            elif xml_data[:4] == '\xff\xfe\x00\x00':
                # UTF-32LE with BOM
                sniffed_xml_encoding = 'utf-32le'
                xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')
            elif xml_data[:3] == '\xef\xbb\xbf':
                # UTF-8 with BOM
                sniffed_xml_encoding = 'utf-8'
                xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')
            else:
                sniffed_xml_encoding = 'ascii'
                pass
        except:
            xml_encoding_match = None
        xml_encoding_match = re.compile(
            '^<\?.*encoding=[\'"](.*?)[\'"].*\?>').match(xml_data)
        if not xml_encoding_match and isHTML:
            regexp = re.compile('<\s*meta[^>]+charset=([^>]*?)[;\'">]', re.I)
            xml_encoding_match = regexp.search(xml_data)
        if xml_encoding_match is not None:
            xml_encoding = xml_encoding_match.groups()[0].lower()
            if isHTML:
                self.declaredHTMLEncoding = xml_encoding
            if sniffed_xml_encoding and \
               (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',
                                 'iso-10646-ucs-4', 'ucs-4', 'csucs4',
                                 'utf-16', 'utf-32', 'utf_16', 'utf_32',
                                 'utf16', 'u16')):
                xml_encoding = sniffed_xml_encoding
        return xml_data, xml_encoding, sniffed_xml_encoding


    def find_codec(self, charset):
        return self._codec(self.CHARSET_ALIASES.get(charset, charset)) \
               or (charset and self._codec(charset.replace("-", ""))) \
               or (charset and self._codec(charset.replace("-", "_"))) \
               or charset

    def _codec(self, charset):
        if not charset: return charset
        codec = None
        try:
            codecs.lookup(charset)
            codec = charset
        except (LookupError, ValueError):
            pass
        return codec

    EBCDIC_TO_ASCII_MAP = None
    def _ebcdic_to_ascii(self, s):
        c = self.__class__
        if not c.EBCDIC_TO_ASCII_MAP:
            emap = (0,1,2,3,156,9,134,127,151,141,142,11,12,13,14,15,
                    16,17,18,19,157,133,8,135,24,25,146,143,28,29,30,31,
                    128,129,130,131,132,10,23,27,136,137,138,139,140,5,6,7,
                    144,145,22,147,148,149,150,4,152,153,154,155,20,21,158,26,
                    32,160,161,162,163,164,165,166,167,168,91,46,60,40,43,33,
                    38,169,170,171,172,173,174,175,176,177,93,36,42,41,59,94,
                    45,47,178,179,180,181,182,183,184,185,124,44,37,95,62,63,
                    186,187,188,189,190,191,192,193,194,96,58,35,64,39,61,34,
                    195,97,98,99,100,101,102,103,104,105,196,197,198,199,200,
                    201,202,106,107,108,109,110,111,112,113,114,203,204,205,
                    206,207,208,209,126,115,116,117,118,119,120,121,122,210,
                    211,212,213,214,215,216,217,218,219,220,221,222,223,224,
                    225,226,227,228,229,230,231,123,65,66,67,68,69,70,71,72,
                    73,232,233,234,235,236,237,125,74,75,76,77,78,79,80,81,
                    82,238,239,240,241,242,243,92,159,83,84,85,86,87,88,89,
                    90,244,245,246,247,248,249,48,49,50,51,52,53,54,55,56,57,
                    250,251,252,253,254,255)
            import string
            c.EBCDIC_TO_ASCII_MAP = string.maketrans( \
            ''.join(map(chr, range(256))), ''.join(map(chr, emap)))
        return s.translate(c.EBCDIC_TO_ASCII_MAP)

    MS_CHARS = { '\x80' : ('euro', '20AC'),
                 '\x81' : ' ',
                 '\x82' : ('sbquo', '201A'),
                 '\x83' : ('fnof', '192'),
                 '\x84' : ('bdquo', '201E'),
                 '\x85' : ('hellip', '2026'),
                 '\x86' : ('dagger', '2020'),
                 '\x87' : ('Dagger', '2021'),
                 '\x88' : ('circ', '2C6'),
                 '\x89' : ('permil', '2030'),
                 '\x8A' : ('Scaron', '160'),
                 '\x8B' : ('lsaquo', '2039'),
                 '\x8C' : ('OElig', '152'),
                 '\x8D' : '?',
                 '\x8E' : ('#x17D', '17D'),
                 '\x8F' : '?',
                 '\x90' : '?',
                 '\x91' : ('lsquo', '2018'),
                 '\x92' : ('rsquo', '2019'),
                 '\x93' : ('ldquo', '201C'),
                 '\x94' : ('rdquo', '201D'),
                 '\x95' : ('bull', '2022'),
                 '\x96' : ('ndash', '2013'),
                 '\x97' : ('mdash', '2014'),
                 '\x98' : ('tilde', '2DC'),
                 '\x99' : ('trade', '2122'),
                 '\x9a' : ('scaron', '161'),
                 '\x9b' : ('rsaquo', '203A'),
                 '\x9c' : ('oelig', '153'),
                 '\x9d' : '?',
                 '\x9e' : ('#x17E', '17E'),
                 '\x9f' : ('Yuml', ''),}

#######################################################################


#By default, act as an HTML pretty-printer.
if __name__ == '__main__':
    import sys
    soup = BeautifulSoup(sys.stdin)
    print soup.prettify()

########NEW FILE########
__FILENAME__ = capitolwords
python-capitolwords-0.3.0/capitolwords.py
########NEW FILE########
__FILENAME__ = ClientForm
python-clientform/ClientForm.py
########NEW FILE########
__FILENAME__ = markdown
#!/usr/bin/env python

version = "1.7"
version_info = (1,7,0,"rc-2")
__revision__ = "$Rev: 72 $"

"""
Python-Markdown
===============

Converts Markdown to HTML.  Basic usage as a module:

    import markdown
    md = Markdown()
    html = md.convert(your_text_string)

See http://www.freewisdom.org/projects/python-markdown/ for more
information and instructions on how to extend the functionality of the
script.  (You might want to read that before you try modifying this
file.)

Started by [Manfred Stienstra](http://www.dwerg.net/).  Continued and
maintained  by [Yuri Takhteyev](http://www.freewisdom.org) and [Waylan
Limberg](http://achinghead.com/).

Contact: yuri [at] freewisdom.org
         waylan [at] gmail.com

License: GPL 2 (http://www.gnu.org/copyleft/gpl.html) or BSD

"""


import re, sys, codecs

from logging import getLogger, StreamHandler, Formatter, \
                    DEBUG, INFO, WARN, ERROR, CRITICAL


MESSAGE_THRESHOLD = CRITICAL


# Configure debug message logger (the hard way - to support python 2.3)
logger = getLogger('MARKDOWN')
logger.setLevel(DEBUG) # This is restricted by handlers later
console_hndlr = StreamHandler()
formatter = Formatter('%(name)s-%(levelname)s: "%(message)s"')
console_hndlr.setFormatter(formatter)
console_hndlr.setLevel(MESSAGE_THRESHOLD)
logger.addHandler(console_hndlr)


def message(level, text):
    ''' A wrapper method for logging debug messages. '''
    logger.log(level, text)


# --------------- CONSTANTS YOU MIGHT WANT TO MODIFY -----------------

TAB_LENGTH = 4            # expand tabs to this many spaces
ENABLE_ATTRIBUTES = True  # @id = xyz -> <... id="xyz">
SMART_EMPHASIS = 1        # this_or_that does not become this<i>or</i>that
HTML_REMOVED_TEXT = "[HTML_REMOVED]" # text used instead of HTML in safe mode

RTL_BIDI_RANGES = ( (u'\u0590', u'\u07FF'),
                    # from Hebrew to Nko (includes Arabic, Syriac and Thaana)
                    (u'\u2D30', u'\u2D7F'),
                    # Tifinagh
                    )

# Unicode Reference Table:
# 0590-05FF - Hebrew
# 0600-06FF - Arabic
# 0700-074F - Syriac
# 0750-077F - Arabic Supplement
# 0780-07BF - Thaana
# 07C0-07FF - Nko

BOMS = { 'utf-8': (codecs.BOM_UTF8, ),
         'utf-16': (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE),
         #'utf-32': (codecs.BOM_UTF32_LE, codecs.BOM_UTF32_BE)
         }

def removeBOM(text, encoding):
    convert = isinstance(text, unicode)
    for bom in BOMS[encoding]:
        bom = convert and bom.decode(encoding) or bom
        if text.startswith(bom):
            return text.lstrip(bom)
    return text

# The following constant specifies the name used in the usage
# statement displayed for python versions lower than 2.3.  (With
# python2.3 and higher the usage statement is generated by optparse
# and uses the actual name of the executable called.)

EXECUTABLE_NAME_FOR_USAGE = "python markdown.py"
                    

# --------------- CONSTANTS YOU _SHOULD NOT_ HAVE TO CHANGE ----------

# a template for html placeholders
HTML_PLACEHOLDER_PREFIX = "qaodmasdkwaspemas"
HTML_PLACEHOLDER = HTML_PLACEHOLDER_PREFIX + "%dajkqlsmdqpakldnzsdfls"

BLOCK_LEVEL_ELEMENTS = ['p', 'div', 'blockquote', 'pre', 'table',
                        'dl', 'ol', 'ul', 'script', 'noscript',
                        'form', 'fieldset', 'iframe', 'math', 'ins',
                        'del', 'hr', 'hr/', 'style']

def isBlockLevel (tag):
    return ( (tag in BLOCK_LEVEL_ELEMENTS) or
             (tag[0] == 'h' and tag[1] in "0123456789") )

"""
======================================================================
========================== NANODOM ===================================
======================================================================

The three classes below implement some of the most basic DOM
methods.  I use this instead of minidom because I need a simpler
functionality and do not want to require additional libraries.

Importantly, NanoDom does not do normalization, which is what we
want. It also adds extra white space when converting DOM to string
"""

ENTITY_NORMALIZATION_EXPRESSIONS = [ (re.compile("&"), "&amp;"),
                                     (re.compile("<"), "&lt;"),
                                     (re.compile(">"), "&gt;")]

ENTITY_NORMALIZATION_EXPRESSIONS_SOFT = [ (re.compile("&(?!\#)"), "&amp;"),
                                     (re.compile("<"), "&lt;"),
                                     (re.compile(">"), "&gt;"),
                                     (re.compile("\""), "&quot;")]


def getBidiType(text):

    if not text: return None

    ch = text[0]

    if not isinstance(ch, unicode) or not ch.isalpha():
        return None

    else:

        for min, max in RTL_BIDI_RANGES:
            if ( ch >= min and ch <= max ):
                return "rtl"
        else:
            return "ltr"


class Document:

    def __init__ (self):
        self.bidi = "ltr"

    def appendChild(self, child):
        self.documentElement = child
        child.isDocumentElement = True
        child.parent = self
        self.entities = {}

    def setBidi(self, bidi):
        if bidi:
            self.bidi = bidi

    def createElement(self, tag, textNode=None):
        el = Element(tag)
        el.doc = self
        if textNode:
            el.appendChild(self.createTextNode(textNode))
        return el

    def createTextNode(self, text):
        node = TextNode(text)
        node.doc = self
        return node

    def createEntityReference(self, entity):
        if entity not in self.entities:
            self.entities[entity] = EntityReference(entity)
        return self.entities[entity]

    def createCDATA(self, text):
        node = CDATA(text)
        node.doc = self
        return node

    def toxml (self):
        return self.documentElement.toxml()

    def normalizeEntities(self, text, avoidDoubleNormalizing=False):

        if avoidDoubleNormalizing:
            regexps = ENTITY_NORMALIZATION_EXPRESSIONS_SOFT
        else:
            regexps = ENTITY_NORMALIZATION_EXPRESSIONS

        for regexp, substitution in regexps:
            text = regexp.sub(substitution, text)
        return text

    def find(self, test):
        return self.documentElement.find(test)

    def unlink(self):
        self.documentElement.unlink()
        self.documentElement = None


class CDATA:

    type = "cdata"

    def __init__ (self, text):
        self.text = text

    def handleAttributes(self):
        pass

    def toxml (self):
        return "<![CDATA[" + self.text + "]]>"

class Element:

    type = "element"

    def __init__ (self, tag):

        self.nodeName = tag
        self.attributes = []
        self.attribute_values = {}
        self.childNodes = []
        self.bidi = None
        self.isDocumentElement = False

    def setBidi(self, bidi):

        if bidi:

            orig_bidi = self.bidi

            if not self.bidi or self.isDocumentElement:
                # Once the bidi is set don't change it (except for doc element)
                self.bidi = bidi
                self.parent.setBidi(bidi)


    def unlink(self):
        for child in self.childNodes:
            if child.type == "element":
                child.unlink()
        self.childNodes = None

    def setAttribute(self, attr, value):
        if not attr in self.attributes:
            self.attributes.append(attr)

        self.attribute_values[attr] = value

    def insertChild(self, position, child):
        self.childNodes.insert(position, child)
        child.parent = self

    def removeChild(self, child):
        self.childNodes.remove(child)

    def replaceChild(self, oldChild, newChild):
        position = self.childNodes.index(oldChild)
        self.removeChild(oldChild)
        self.insertChild(position, newChild)

    def appendChild(self, child):
        self.childNodes.append(child)
        child.parent = self

    def handleAttributes(self):
        pass

    def find(self, test, depth=0):
        """ Returns a list of descendants that pass the test function """
        matched_nodes = []
        for child in self.childNodes:
            if test(child):
                matched_nodes.append(child)
            if child.type == "element":
                matched_nodes += child.find(test, depth+1)
        return matched_nodes

    def toxml(self):
        if ENABLE_ATTRIBUTES:
            for child in self.childNodes:
                child.handleAttributes()

        buffer = ""
        if self.nodeName in ['h1', 'h2', 'h3', 'h4']:
            buffer += "\n"
        elif self.nodeName in ['li']:
            buffer += "\n "

        # Process children FIRST, then do the attributes

        childBuffer = ""

        if self.childNodes or self.nodeName in ['blockquote']:
            childBuffer += ">"
            for child in self.childNodes:
                childBuffer += child.toxml()
            if self.nodeName == 'p':
                childBuffer += "\n"
            elif self.nodeName == 'li':
                childBuffer += "\n "
            childBuffer += "</%s>" % self.nodeName
        else:
            childBuffer += "/>"


            
        buffer += "<" + self.nodeName

        if self.nodeName in ['p', 'li', 'ul', 'ol',
                             'h1', 'h2', 'h3', 'h4', 'h5', 'h6']:

            if not self.attribute_values.has_key("dir"):
                if self.bidi:
                    bidi = self.bidi
                else:
                    bidi = self.doc.bidi
                    
                if bidi=="rtl":
                    self.setAttribute("dir", "rtl")
        
        for attr in self.attributes:
            value = self.attribute_values[attr]
            value = self.doc.normalizeEntities(value,
                                               avoidDoubleNormalizing=True)
            buffer += ' %s="%s"' % (attr, value)


        # Now let's actually append the children

        buffer += childBuffer

        if self.nodeName in ['p', 'br ', 'li', 'ul', 'ol',
                             'h1', 'h2', 'h3', 'h4'] :
            buffer += "\n"

        return buffer


class TextNode:

    type = "text"
    attrRegExp = re.compile(r'\{@([^\}]*)=([^\}]*)}') # {@id=123}

    def __init__ (self, text):
        self.value = text        

    def attributeCallback(self, match):

        self.parent.setAttribute(match.group(1), match.group(2))

    def handleAttributes(self):
        self.value = self.attrRegExp.sub(self.attributeCallback, self.value)

    def toxml(self):

        text = self.value

        self.parent.setBidi(getBidiType(text))
        
        if not text.startswith(HTML_PLACEHOLDER_PREFIX):
            if self.parent.nodeName == "p":
                text = text.replace("\n", "\n   ")
            elif (self.parent.nodeName == "li"
                  and self.parent.childNodes[0]==self):
                text = "\n     " + text.replace("\n", "\n     ")
        text = self.doc.normalizeEntities(text)
        return text


class EntityReference:

    type = "entity_ref"

    def __init__(self, entity):
        self.entity = entity

    def handleAttributes(self):
        pass

    def toxml(self):
        return "&" + self.entity + ";"


"""
======================================================================
========================== PRE-PROCESSORS ============================
======================================================================

Preprocessors munge source text before we start doing anything too
complicated.

There are two types of preprocessors: TextPreprocessor and Preprocessor.

"""


class TextPreprocessor:
    '''
    TextPreprocessors are run before the text is broken into lines.
    
    Each TextPreprocessor implements a "run" method that takes a pointer to a
    text string of the document, modifies it as necessary and returns
    either the same pointer or a pointer to a new string.  
    
    TextPreprocessors must extend markdown.TextPreprocessor.
    '''

    def run(self, text):
        pass


class Preprocessor:
    '''
    Preprocessors are run after the text is broken into lines.

    Each preprocessor implements a "run" method that takes a pointer to a
    list of lines of the document, modifies it as necessary and returns
    either the same pointer or a pointer to a new list.  
    
    Preprocessors must extend markdown.Preprocessor.
    '''

    def run(self, lines):
        pass
 

class HtmlBlockPreprocessor(TextPreprocessor):
    """Removes html blocks from the source text and stores it."""
    
    def _get_left_tag(self, block):
        return block[1:].replace(">", " ", 1).split()[0].lower()


    def _get_right_tag(self, left_tag, block):
        return block.rstrip()[-len(left_tag)-2:-1].lower()

    def _equal_tags(self, left_tag, right_tag):
        
        if left_tag == 'div' or left_tag[0] in ['?', '@', '%']: # handle PHP, etc.
            return True
        if ("/" + left_tag) == right_tag:
            return True
        if (right_tag == "--" and left_tag == "--"):
            return True
        elif left_tag == right_tag[1:] \
            and right_tag[0] != "<":
            return True
        else:
            return False

    def _is_oneliner(self, tag):
        return (tag in ['hr', 'hr/'])

    
    def run(self, text):

        new_blocks = []
        text = text.split("\n\n")
        
        items = []
        left_tag = ''
        right_tag = ''
        in_tag = False # flag
        
        for block in text:
            if block.startswith("\n"):
                block = block[1:]

            if not in_tag:

                if block.startswith("<"):
                    
                    left_tag = self._get_left_tag(block)
                    right_tag = self._get_right_tag(left_tag, block)

                    if not (isBlockLevel(left_tag) \
                        or block[1] in ["!", "?", "@", "%"]):
                        new_blocks.append(block)
                        continue

                    if self._is_oneliner(left_tag):
                        new_blocks.append(block.strip())
                        continue
                        
                    if block[1] == "!":
                        # is a comment block
                        left_tag = "--"
                        right_tag = self._get_right_tag(left_tag, block)
                        # keep checking conditions below and maybe just append
                        
                    if block.rstrip().endswith(">") \
                        and self._equal_tags(left_tag, right_tag):
                        new_blocks.append(
                            self.stash.store(block.strip()))
                        continue
                    else: #if not block[1] == "!":
                        # if is block level tag and is not complete
                        items.append(block.strip())
                        in_tag = True
                        continue

                new_blocks.append(block)

            else:
                items.append(block.strip())
                
                right_tag = self._get_right_tag(left_tag, block)
                
                if self._equal_tags(left_tag, right_tag):
                    # if find closing tag
                    in_tag = False
                    new_blocks.append(
                        self.stash.store('\n\n'.join(items)))
                    items = []

        if items:
            new_blocks.append(self.stash.store('\n\n'.join(items)))
            new_blocks.append('\n')
            
        return "\n\n".join(new_blocks)

HTML_BLOCK_PREPROCESSOR = HtmlBlockPreprocessor()


class HeaderPreprocessor(Preprocessor):

    """
       Replaces underlined headers with hashed headers to avoid
       the nead for lookahead later.
    """

    def run (self, lines):

        i = -1
        while i+1 < len(lines):
            i = i+1
            if not lines[i].strip():
                continue

            if lines[i].startswith("#"):
                lines.insert(i+1, "\n")

            if (i+1 <= len(lines)
                  and lines[i+1]
                  and lines[i+1][0] in ['-', '=']):

                underline = lines[i+1].strip()

                if underline == "="*len(underline):
                    lines[i] = "# " + lines[i].strip()
                    lines[i+1] = ""
                elif underline == "-"*len(underline):
                    lines[i] = "## " + lines[i].strip()
                    lines[i+1] = ""

        return lines

HEADER_PREPROCESSOR = HeaderPreprocessor()


class LinePreprocessor(Preprocessor):
    """Deals with HR lines (needs to be done before processing lists)"""

    blockquote_re = re.compile(r'^(> )+')

    def run (self, lines):
        for i in range(len(lines)):
            prefix = ''
            m = self.blockquote_re.search(lines[i])
            if m : prefix = m.group(0)
            if self._isLine(lines[i][len(prefix):]):
                lines[i] = prefix + self.stash.store("<hr />", safe=True)
        return lines

    def _isLine(self, block):
        """Determines if a block should be replaced with an <HR>"""
        if block.startswith("    "): return 0  # a code block
        text = "".join([x for x in block if not x.isspace()])
        if len(text) <= 2:
            return 0
        for pattern in ['isline1', 'isline2', 'isline3']:
            m = RE.regExp[pattern].match(text)
            if (m and m.group(1)):
                return 1
        else:
            return 0

LINE_PREPROCESSOR = LinePreprocessor()


class ReferencePreprocessor(Preprocessor):
    ''' 
    Removes reference definitions from the text and stores them for later use.
    '''

    def run (self, lines):

        new_text = [];
        for line in lines:
            m = RE.regExp['reference-def'].match(line)
            if m:
                id = m.group(2).strip().lower()
                t = m.group(4).strip()  # potential title
                if not t:
                    self.references[id] = (m.group(3), t)
                elif (len(t) >= 2
                      and (t[0] == t[-1] == "\""
                           or t[0] == t[-1] == "\'"
                           or (t[0] == "(" and t[-1] == ")") ) ):
                    self.references[id] = (m.group(3), t[1:-1])
                else:
                    new_text.append(line)
            else:
                new_text.append(line)

        return new_text #+ "\n"

REFERENCE_PREPROCESSOR = ReferencePreprocessor()

"""
======================================================================
========================== INLINE PATTERNS ===========================
======================================================================

Inline patterns such as *emphasis* are handled by means of auxiliary
objects, one per pattern.  Pattern objects must be instances of classes
that extend markdown.Pattern.  Each pattern object uses a single regular
expression and needs support the following methods:

  pattern.getCompiledRegExp() - returns a regular expression

  pattern.handleMatch(m, doc) - takes a match object and returns
                                a NanoDom node (as a part of the provided
                                doc) or None

All of python markdown's built-in patterns subclass from Patter,
but you can add additional patterns that don't.

Also note that all the regular expressions used by inline must
capture the whole block.  For this reason, they all start with
'^(.*)' and end with '(.*)!'.  In case with built-in expression
Pattern takes care of adding the "^(.*)" and "(.*)!".

Finally, the order in which regular expressions are applied is very
important - e.g. if we first replace http://.../ links with <a> tags
and _then_ try to replace inline html, we would end up with a mess.
So, we apply the expressions in the following order:

       * escape and backticks have to go before everything else, so
         that we can preempt any markdown patterns by escaping them.

       * then we handle auto-links (must be done before inline html)

       * then we handle inline HTML.  At this point we will simply
         replace all inline HTML strings with a placeholder and add
         the actual HTML to a hash.

       * then inline images (must be done before links)

       * then bracketed links, first regular then reference-style

       * finally we apply strong and emphasis
"""

NOBRACKET = r'[^\]\[]*'
BRK = ( r'\[('
        + (NOBRACKET + r'(\[')*6
        + (NOBRACKET+ r'\])*')*6
        + NOBRACKET + r')\]' )
NOIMG = r'(?<!\!)'

BACKTICK_RE = r'\`([^\`]*)\`'                    # `e= m*c^2`
DOUBLE_BACKTICK_RE =  r'\`\`(.*)\`\`'            # ``e=f("`")``
ESCAPE_RE = r'\\(.)'                             # \<
EMPHASIS_RE = r'\*([^\*]*)\*'                    # *emphasis*
STRONG_RE = r'\*\*(.*)\*\*'                      # **strong**
STRONG_EM_RE = r'\*\*\*([^_]*)\*\*\*'            # ***strong***

if SMART_EMPHASIS:
    EMPHASIS_2_RE = r'(?<!\S)_(\S[^_]*)_'        # _emphasis_
else:
    EMPHASIS_2_RE = r'_([^_]*)_'                 # _emphasis_

STRONG_2_RE = r'__([^_]*)__'                     # __strong__
STRONG_EM_2_RE = r'___([^_]*)___'                # ___strong___

LINK_RE = NOIMG + BRK + r'\s*\(([^\)]*)\)'               # [text](url)
LINK_ANGLED_RE = NOIMG + BRK + r'\s*\(<([^\)]*)>\)'      # [text](<url>)
IMAGE_LINK_RE = r'\!' + BRK + r'\s*\(([^\)]*)\)' # ![alttxt](http://x.com/)
REFERENCE_RE = NOIMG + BRK+ r'\s*\[([^\]]*)\]'           # [Google][3]
IMAGE_REFERENCE_RE = r'\!' + BRK + '\s*\[([^\]]*)\]' # ![alt text][2]
NOT_STRONG_RE = r'( \* )'                        # stand-alone * or _
AUTOLINK_RE = r'<(http://[^>]*)>'                # <http://www.123.com>
AUTOMAIL_RE = r'<([^> \!]*@[^> ]*)>'               # <me@example.com>
#HTML_RE = r'(\<[^\>]*\>)'                        # <...>
HTML_RE = r'(\<[a-zA-Z/][^\>]*\>)'               # <...>
ENTITY_RE = r'(&[\#a-zA-Z0-9]*;)'                # &amp;
LINE_BREAK_RE = r'  \n'                     # two spaces at end of line
LINE_BREAK_2_RE = r'  $'                    # two spaces at end of text

class Pattern:

    def __init__ (self, pattern):
        self.pattern = pattern
        self.compiled_re = re.compile("^(.*)%s(.*)$" % pattern, re.DOTALL)

    def getCompiledRegExp (self):
        return self.compiled_re

BasePattern = Pattern # for backward compatibility

class SimpleTextPattern (Pattern):

    def handleMatch(self, m, doc):
        return doc.createTextNode(m.group(2))

class SimpleTagPattern (Pattern):

    def __init__ (self, pattern, tag):
        Pattern.__init__(self, pattern)
        self.tag = tag

    def handleMatch(self, m, doc):
        el = doc.createElement(self.tag)
        el.appendChild(doc.createTextNode(m.group(2)))
        return el

class SubstituteTagPattern (SimpleTagPattern):

    def handleMatch (self, m, doc):
        return doc.createElement(self.tag)

class BacktickPattern (Pattern):

    def __init__ (self, pattern):
        Pattern.__init__(self, pattern)
        self.tag = "code"

    def handleMatch(self, m, doc):
        el = doc.createElement(self.tag)
        text = m.group(2).strip()
        #text = text.replace("&", "&amp;")
        el.appendChild(doc.createTextNode(text))
        return el


class DoubleTagPattern (SimpleTagPattern): 

    def handleMatch(self, m, doc):
        tag1, tag2 = self.tag.split(",")
        el1 = doc.createElement(tag1)
        el2 = doc.createElement(tag2)
        el1.appendChild(el2)
        el2.appendChild(doc.createTextNode(m.group(2)))
        return el1


class HtmlPattern (Pattern):

    def handleMatch (self, m, doc):
        rawhtml = m.group(2)
        inline = True
        place_holder = self.stash.store(rawhtml)
        return doc.createTextNode(place_holder)


class LinkPattern (Pattern):

    def handleMatch(self, m, doc):
        el = doc.createElement('a')
        el.appendChild(doc.createTextNode(m.group(2)))
        parts = m.group(9).split('"')
        # We should now have [], [href], or [href, title]
        if parts:
            el.setAttribute('href', parts[0].strip())
        else:
            el.setAttribute('href', "")
        if len(parts) > 1:
            # we also got a title
            title = '"' + '"'.join(parts[1:]).strip()
            title = dequote(title) #.replace('"', "&quot;")
            el.setAttribute('title', title)
        return el


class ImagePattern (Pattern):

    def handleMatch(self, m, doc):
        el = doc.createElement('img')
        src_parts = m.group(9).split()
        if src_parts:
            el.setAttribute('src', src_parts[0])
        else:
            el.setAttribute('src', "")
        if len(src_parts) > 1:
            el.setAttribute('title', dequote(" ".join(src_parts[1:])))
        if ENABLE_ATTRIBUTES:
            text = doc.createTextNode(m.group(2))
            el.appendChild(text)
            text.handleAttributes()
            truealt = text.value
            el.childNodes.remove(text)
        else:
            truealt = m.group(2)
        el.setAttribute('alt', truealt)
        return el

class ReferencePattern (Pattern):

    def handleMatch(self, m, doc):

        if m.group(9):
            id = m.group(9).lower()
        else:
            # if we got something like "[Google][]"
            # we'll use "google" as the id
            id = m.group(2).lower()

        if not self.references.has_key(id): # ignore undefined refs
            return None
        href, title = self.references[id]
        text = m.group(2)
        return self.makeTag(href, title, text, doc)

    def makeTag(self, href, title, text, doc):
        el = doc.createElement('a')
        el.setAttribute('href', href)
        if title:
            el.setAttribute('title', title)
        el.appendChild(doc.createTextNode(text))
        return el


class ImageReferencePattern (ReferencePattern):

    def makeTag(self, href, title, text, doc):
        el = doc.createElement('img')
        el.setAttribute('src', href)
        if title:
            el.setAttribute('title', title)
        el.setAttribute('alt', text)
        return el


class AutolinkPattern (Pattern):

    def handleMatch(self, m, doc):
        el = doc.createElement('a')
        el.setAttribute('href', m.group(2))
        el.appendChild(doc.createTextNode(m.group(2)))
        return el

class AutomailPattern (Pattern):

    def handleMatch(self, m, doc):
        el = doc.createElement('a')
        email = m.group(2)
        if email.startswith("mailto:"):
            email = email[len("mailto:"):]
        for letter in email:
            entity = doc.createEntityReference("#%d" % ord(letter))
            el.appendChild(entity)
        mailto = "mailto:" + email
        mailto = "".join(['&#%d;' % ord(letter) for letter in mailto])
        el.setAttribute('href', mailto)
        return el

ESCAPE_PATTERN          = SimpleTextPattern(ESCAPE_RE)
NOT_STRONG_PATTERN      = SimpleTextPattern(NOT_STRONG_RE)

BACKTICK_PATTERN        = BacktickPattern(BACKTICK_RE)
DOUBLE_BACKTICK_PATTERN = BacktickPattern(DOUBLE_BACKTICK_RE)
STRONG_PATTERN          = SimpleTagPattern(STRONG_RE, 'strong')
STRONG_PATTERN_2        = SimpleTagPattern(STRONG_2_RE, 'strong')
EMPHASIS_PATTERN        = SimpleTagPattern(EMPHASIS_RE, 'em')
EMPHASIS_PATTERN_2      = SimpleTagPattern(EMPHASIS_2_RE, 'em')

STRONG_EM_PATTERN       = DoubleTagPattern(STRONG_EM_RE, 'strong,em')
STRONG_EM_PATTERN_2     = DoubleTagPattern(STRONG_EM_2_RE, 'strong,em')

LINE_BREAK_PATTERN      = SubstituteTagPattern(LINE_BREAK_RE, 'br ')
LINE_BREAK_PATTERN_2    = SubstituteTagPattern(LINE_BREAK_2_RE, 'br ')

LINK_PATTERN            = LinkPattern(LINK_RE)
LINK_ANGLED_PATTERN     = LinkPattern(LINK_ANGLED_RE)
IMAGE_LINK_PATTERN      = ImagePattern(IMAGE_LINK_RE)
IMAGE_REFERENCE_PATTERN = ImageReferencePattern(IMAGE_REFERENCE_RE)
REFERENCE_PATTERN       = ReferencePattern(REFERENCE_RE)

HTML_PATTERN            = HtmlPattern(HTML_RE)
ENTITY_PATTERN          = HtmlPattern(ENTITY_RE)

AUTOLINK_PATTERN        = AutolinkPattern(AUTOLINK_RE)
AUTOMAIL_PATTERN        = AutomailPattern(AUTOMAIL_RE)


"""
======================================================================
========================== POST-PROCESSORS ===========================
======================================================================

Markdown also allows post-processors, which are similar to
preprocessors in that they need to implement a "run" method. However,
they are run after core processing.

There are two types of post-processors: Postprocessor and TextPostprocessor
"""


class Postprocessor:
    '''
    Postprocessors are run before the dom it converted back into text.
    
    Each Postprocessor implements a "run" method that takes a pointer to a
    NanoDom document, modifies it as necessary and returns a NanoDom 
    document.
    
    Postprocessors must extend markdown.Postprocessor.

    There are currently no standard post-processors, but the footnote
    extension uses one.
    '''

    def run(self, dom):
        pass



class TextPostprocessor:
    '''
    TextPostprocessors are run after the dom it converted back into text.
    
    Each TextPostprocessor implements a "run" method that takes a pointer to a
    text string, modifies it as necessary and returns a text string.
    
    TextPostprocessors must extend markdown.TextPostprocessor.
    '''

    def run(self, text):
        pass


class RawHtmlTextPostprocessor(TextPostprocessor):

    def __init__(self):
        pass

    def run(self, text):
        for i in range(self.stash.html_counter):
            html, safe  = self.stash.rawHtmlBlocks[i]
            if self.safeMode and not safe:
                if str(self.safeMode).lower() == 'escape':
                    html = self.escape(html)
                elif str(self.safeMode).lower() == 'remove':
                    html = ''
                else:
                    html = HTML_REMOVED_TEXT
                                   
            text = text.replace("<p>%s\n</p>" % (HTML_PLACEHOLDER % i),
                              html + "\n")
            text =  text.replace(HTML_PLACEHOLDER % i, html)
        return text

    def escape(self, html):
        ''' Basic html escaping '''
        html = html.replace('&', '&amp;')
        html = html.replace('<', '&lt;')
        html = html.replace('>', '&gt;')
        return html.replace('"', '&quot;')

RAWHTMLTEXTPOSTPROCESSOR = RawHtmlTextPostprocessor()

"""
======================================================================
========================== MISC AUXILIARY CLASSES ====================
======================================================================
"""

class HtmlStash:
    """This class is used for stashing HTML objects that we extract
        in the beginning and replace with place-holders."""

    def __init__ (self):
        self.html_counter = 0 # for counting inline html segments
        self.rawHtmlBlocks=[]

    def store(self, html, safe=False):
        """Saves an HTML segment for later reinsertion.  Returns a
           placeholder string that needs to be inserted into the
           document.

           @param html: an html segment
           @param safe: label an html segment as safe for safemode
           @param inline: label a segmant as inline html
           @returns : a placeholder string """
        self.rawHtmlBlocks.append((html, safe))
        placeholder = HTML_PLACEHOLDER % self.html_counter
        self.html_counter += 1
        return placeholder


class BlockGuru:

    def _findHead(self, lines, fn, allowBlank=0):

        """Functional magic to help determine boundaries of indented
           blocks.

           @param lines: an array of strings
           @param fn: a function that returns a substring of a string
                      if the string matches the necessary criteria
           @param allowBlank: specifies whether it's ok to have blank
                      lines between matching functions
           @returns: a list of post processes items and the unused
                      remainder of the original list"""

        items = []
        item = -1

        i = 0 # to keep track of where we are

        for line in lines:

            if not line.strip() and not allowBlank:
                return items, lines[i:]

            if not line.strip() and allowBlank:
                # If we see a blank line, this _might_ be the end
                i += 1

                # Find the next non-blank line
                for j in range(i, len(lines)):
                    if lines[j].strip():
                        next = lines[j]
                        break
                else:
                    # There is no more text => this is the end
                    break

                # Check if the next non-blank line is still a part of the list

                part = fn(next)

                if part:
                    items.append("")
                    continue
                else:
                    break # found end of the list

            part = fn(line)

            if part:
                items.append(part)
                i += 1
                continue
            else:
                return items, lines[i:]
        else:
            i += 1

        return items, lines[i:]


    def detabbed_fn(self, line):
        """ An auxiliary method to be passed to _findHead """
        m = RE.regExp['tabbed'].match(line)
        if m:
            return m.group(4)
        else:
            return None


    def detectTabbed(self, lines):

        return self._findHead(lines, self.detabbed_fn,
                              allowBlank = 1)


def print_error(string):
    """Print an error string to stderr"""
    sys.stderr.write(string +'\n')


def dequote(string):
    """ Removes quotes from around a string """
    if ( ( string.startswith('"') and string.endswith('"'))
         or (string.startswith("'") and string.endswith("'")) ):
        return string[1:-1]
    else:
        return string

"""
======================================================================
========================== CORE MARKDOWN =============================
======================================================================

This stuff is ugly, so if you are thinking of extending the syntax,
see first if you can do it via pre-processors, post-processors,
inline patterns or a combination of the three.
"""

class CorePatterns:
    """This class is scheduled for removal as part of a refactoring
        effort."""

    patterns = {
        'header':          r'(#*)([^#]*)(#*)', # # A title
        'reference-def':   r'(\ ?\ ?\ ?)\[([^\]]*)\]:\s*([^ ]*)(.*)',
                           # [Google]: http://www.google.com/
        'containsline':    r'([-]*)$|^([=]*)', # -----, =====, etc.
        'ol':              r'[ ]{0,3}[\d]*\.\s+(.*)', # 1. text
        'ul':              r'[ ]{0,3}[*+-]\s+(.*)', # "* text"
        'isline1':         r'(\**)', # ***
        'isline2':         r'(\-*)', # ---
        'isline3':         r'(\_*)', # ___
        'tabbed':          r'((\t)|(    ))(.*)', # an indented line
        'quoted':          r'> ?(.*)', # a quoted block ("> ...")
    }

    def __init__ (self):

        self.regExp = {}
        for key in self.patterns.keys():
            self.regExp[key] = re.compile("^%s$" % self.patterns[key],
                                          re.DOTALL)

        self.regExp['containsline'] = re.compile(r'^([-]*)$|^([=]*)$', re.M)

RE = CorePatterns()


class Markdown:
    """ Markdown formatter class for creating an html document from
        Markdown text """


    def __init__(self, source=None,  # depreciated
                 extensions=[],
                 extension_configs=None,
                 safe_mode = False):
        """Creates a new Markdown instance.

           @param source: The text in Markdown format. Depreciated!
           @param extensions: A list if extensions.
           @param extension-configs: Configuration setting for extensions.
           @param safe_mode: Disallow raw html. """

        self.source = source
        if source is not None:
            message(WARN, "The `source` arg of Markdown.__init__() is depreciated and will be removed in the future. Use `instance.convert(source)` instead.")
        self.safeMode = safe_mode
        self.blockGuru = BlockGuru()
        self.registeredExtensions = []
        self.stripTopLevelTags = 1
        self.docType = ""

        self.textPreprocessors = [HTML_BLOCK_PREPROCESSOR]

        self.preprocessors = [HEADER_PREPROCESSOR,
                              LINE_PREPROCESSOR,
                              # A footnote preprocessor will
                              # get inserted here
                              REFERENCE_PREPROCESSOR]


        self.postprocessors = [] # a footnote postprocessor will get
                                 # inserted later

        self.textPostprocessors = [# a footnote postprocessor will get
                                   # inserted here
                                   RAWHTMLTEXTPOSTPROCESSOR]

        self.prePatterns = []
        

        self.inlinePatterns = [DOUBLE_BACKTICK_PATTERN,
                               BACKTICK_PATTERN,
                               ESCAPE_PATTERN,
                               REFERENCE_PATTERN,
                               LINK_ANGLED_PATTERN,
                               LINK_PATTERN,
                               IMAGE_LINK_PATTERN,
			                   IMAGE_REFERENCE_PATTERN,
			                   AUTOLINK_PATTERN,
                               AUTOMAIL_PATTERN,
                               LINE_BREAK_PATTERN_2,
                               LINE_BREAK_PATTERN,
                               HTML_PATTERN,
                               ENTITY_PATTERN,
                               NOT_STRONG_PATTERN,
                               STRONG_EM_PATTERN,
                               STRONG_EM_PATTERN_2,
                               STRONG_PATTERN,
                               STRONG_PATTERN_2,
                               EMPHASIS_PATTERN,
                               EMPHASIS_PATTERN_2
                               # The order of the handlers matters!!!
                               ]

        self.registerExtensions(extensions = extensions,
                                configs = extension_configs)

        self.reset()


    def registerExtensions(self, extensions, configs):

        if not configs:
            configs = {}

        for ext in extensions:

            extension_module_name = "mdx_" + ext

            try:
                module = __import__(extension_module_name)

            except:
                message(CRITICAL,
                        "couldn't load extension %s (looking for %s module)"
                        % (ext, extension_module_name) )
            else:

                if configs.has_key(ext):
                    configs_for_ext = configs[ext]
                else:
                    configs_for_ext = []
                extension = module.makeExtension(configs_for_ext)    
                extension.extendMarkdown(self, globals())




    def registerExtension(self, extension):
        """ This gets called by the extension """
        self.registeredExtensions.append(extension)

    def reset(self):
        """Resets all state variables so that we can start
            with a new text."""
        self.references={}
        self.htmlStash = HtmlStash()

        HTML_BLOCK_PREPROCESSOR.stash = self.htmlStash
        LINE_PREPROCESSOR.stash = self.htmlStash
        REFERENCE_PREPROCESSOR.references = self.references
        HTML_PATTERN.stash = self.htmlStash
        ENTITY_PATTERN.stash = self.htmlStash
        REFERENCE_PATTERN.references = self.references
        IMAGE_REFERENCE_PATTERN.references = self.references
        RAWHTMLTEXTPOSTPROCESSOR.stash = self.htmlStash
        RAWHTMLTEXTPOSTPROCESSOR.safeMode = self.safeMode

        for extension in self.registeredExtensions:
            extension.reset()


    def _transform(self):
        """Transforms the Markdown text into a XHTML body document

           @returns: A NanoDom Document """

        # Setup the document

        self.doc = Document()
        self.top_element = self.doc.createElement("span")
        self.top_element.appendChild(self.doc.createTextNode('\n'))
        self.top_element.setAttribute('class', 'markdown')
        self.doc.appendChild(self.top_element)

        # Fixup the source text
        text = self.source
        text = text.replace("\r\n", "\n").replace("\r", "\n")
        text += "\n\n"
        text = text.expandtabs(TAB_LENGTH)

        # Split into lines and run the preprocessors that will work with
        # self.lines

        self.lines = text.split("\n")

        # Run the pre-processors on the lines
        for prep in self.preprocessors :
            self.lines = prep.run(self.lines)

        # Create a NanoDom tree from the lines and attach it to Document


        buffer = []
        for line in self.lines:
            if line.startswith("#"):
                self._processSection(self.top_element, buffer)
                buffer = [line]
            else:
                buffer.append(line)
        self._processSection(self.top_element, buffer)
        
        #self._processSection(self.top_element, self.lines)

        # Not sure why I put this in but let's leave it for now.
        self.top_element.appendChild(self.doc.createTextNode('\n'))

        # Run the post-processors
        for postprocessor in self.postprocessors:
            postprocessor.run(self.doc)

        return self.doc


    def _processSection(self, parent_elem, lines,
                        inList = 0, looseList = 0):

        """Process a section of a source document, looking for high
           level structural elements like lists, block quotes, code
           segments, html blocks, etc.  Some those then get stripped
           of their high level markup (e.g. get unindented) and the
           lower-level markup is processed recursively.

           @param parent_elem: A NanoDom element to which the content
                               will be added
           @param lines: a list of lines
           @param inList: a level
           @returns: None"""

        # Loop through lines until none left.
        while lines:

            # Check if this section starts with a list, a blockquote or
            # a code block

            processFn = { 'ul':     self._processUList,
                          'ol':     self._processOList,
                          'quoted': self._processQuote,
                          'tabbed': self._processCodeBlock}

            for regexp in ['ul', 'ol', 'quoted', 'tabbed']:
                m = RE.regExp[regexp].match(lines[0])
                if m:
                    processFn[regexp](parent_elem, lines, inList)
                    return

            # We are NOT looking at one of the high-level structures like
            # lists or blockquotes.  So, it's just a regular paragraph
            # (though perhaps nested inside a list or something else).  If
            # we are NOT inside a list, we just need to look for a blank
            # line to find the end of the block.  If we ARE inside a
            # list, however, we need to consider that a sublist does not
            # need to be separated by a blank line.  Rather, the following
            # markup is legal:
            #
            # * The top level list item
            #
            #     Another paragraph of the list.  This is where we are now.
            #     * Underneath we might have a sublist.
            #

            if inList:

                start, lines  = self._linesUntil(lines, (lambda line:
                                 RE.regExp['ul'].match(line)
                                 or RE.regExp['ol'].match(line)
                                                  or not line.strip()))

                self._processSection(parent_elem, start,
                                     inList - 1, looseList = looseList)
                inList = inList-1

            else: # Ok, so it's just a simple block

                paragraph, lines = self._linesUntil(lines, lambda line:
                                                     not line.strip())

                if len(paragraph) and paragraph[0].startswith('#'):
                    self._processHeader(parent_elem, paragraph)

                elif paragraph:
                    self._processParagraph(parent_elem, paragraph,
                                          inList, looseList)

            if lines and not lines[0].strip():
                lines = lines[1:]  # skip the first (blank) line


    def _processHeader(self, parent_elem, paragraph):
        m = RE.regExp['header'].match(paragraph[0])
        if m:
            level = len(m.group(1))
            h = self.doc.createElement("h%d" % level)
            parent_elem.appendChild(h)
            for item in self._handleInline(m.group(2).strip()):
                h.appendChild(item)
        else:
            message(CRITICAL, "We've got a problem header!")


    def _processParagraph(self, parent_elem, paragraph, inList, looseList):
        list = self._handleInline("\n".join(paragraph))

        if ( parent_elem.nodeName == 'li'
                and not (looseList or parent_elem.childNodes)):

            # If this is the first paragraph inside "li", don't
            # put <p> around it - append the paragraph bits directly
            # onto parent_elem
            el = parent_elem
        else:
            # Otherwise make a "p" element
            el = self.doc.createElement("p")
            parent_elem.appendChild(el)

        for item in list:
            el.appendChild(item)
 

    def _processUList(self, parent_elem, lines, inList):
        self._processList(parent_elem, lines, inList,
                         listexpr='ul', tag = 'ul')

    def _processOList(self, parent_elem, lines, inList):
        self._processList(parent_elem, lines, inList,
                         listexpr='ol', tag = 'ol')


    def _processList(self, parent_elem, lines, inList, listexpr, tag):
        """Given a list of document lines starting with a list item,
           finds the end of the list, breaks it up, and recursively
           processes each list item and the remainder of the text file.

           @param parent_elem: A dom element to which the content will be added
           @param lines: a list of lines
           @param inList: a level
           @returns: None"""

        ul = self.doc.createElement(tag)  # ul might actually be '<ol>'
        parent_elem.appendChild(ul)

        looseList = 0

        # Make a list of list items
        items = []
        item = -1

        i = 0  # a counter to keep track of where we are

        for line in lines: 

            loose = 0
            if not line.strip():
                # If we see a blank line, this _might_ be the end of the list
                i += 1
                loose = 1

                # Find the next non-blank line
                for j in range(i, len(lines)):
                    if lines[j].strip():
                        next = lines[j]
                        break
                else:
                    # There is no more text => end of the list
                    break

                # Check if the next non-blank line is still a part of the list
                if ( RE.regExp['ul'].match(next) or
                     RE.regExp['ol'].match(next) or 
                     RE.regExp['tabbed'].match(next) ):
                    # get rid of any white space in the line
                    items[item].append(line.strip())
                    looseList = loose or looseList
                    continue
                else:
                    break # found end of the list

            # Now we need to detect list items (at the current level)
            # while also detabing child elements if necessary

            for expr in ['ul', 'ol', 'tabbed']:

                m = RE.regExp[expr].match(line)
                if m:
                    if expr in ['ul', 'ol']:  # We are looking at a new item
                        #if m.group(1) :
                        # Removed the check to allow for a blank line
                        # at the beginning of the list item
                        items.append([m.group(1)])
                        item += 1
                    elif expr == 'tabbed':  # This line needs to be detabbed
                        items[item].append(m.group(4)) #after the 'tab'

                    i += 1
                    break
            else:
                items[item].append(line)  # Just regular continuation
                i += 1 # added on 2006.02.25
        else:
            i += 1

        # Add the dom elements
        for item in items:
            li = self.doc.createElement("li")
            ul.appendChild(li)

            self._processSection(li, item, inList + 1, looseList = looseList)

        # Process the remaining part of the section

        self._processSection(parent_elem, lines[i:], inList)


    def _linesUntil(self, lines, condition):
        """ A utility function to break a list of lines upon the
            first line that satisfied a condition.  The condition
            argument should be a predicate function.
            """

        i = -1
        for line in lines:
            i += 1
            if condition(line): break
        else:
            i += 1
        return lines[:i], lines[i:]

    def _processQuote(self, parent_elem, lines, inList):
        """Given a list of document lines starting with a quote finds
           the end of the quote, unindents it and recursively
           processes the body of the quote and the remainder of the
           text file.

           @param parent_elem: DOM element to which the content will be added
           @param lines: a list of lines
           @param inList: a level
           @returns: None """

        dequoted = []
        i = 0
        blank_line = False # allow one blank line between paragraphs
        for line in lines:
            m = RE.regExp['quoted'].match(line)
            if m:
                dequoted.append(m.group(1))
                i += 1
                blank_line = False
            elif not blank_line and line.strip() != '':
                dequoted.append(line)
                i += 1
            elif not blank_line and line.strip() == '':
                dequoted.append(line)
                i += 1
                blank_line = True
            else:
                break

        blockquote = self.doc.createElement('blockquote')
        parent_elem.appendChild(blockquote)

        self._processSection(blockquote, dequoted, inList)
        self._processSection(parent_elem, lines[i:], inList)




    def _processCodeBlock(self, parent_elem, lines, inList):
        """Given a list of document lines starting with a code block
           finds the end of the block, puts it into the dom verbatim
           wrapped in ("<pre><code>") and recursively processes the
           the remainder of the text file.

           @param parent_elem: DOM element to which the content will be added
           @param lines: a list of lines
           @param inList: a level
           @returns: None"""

        detabbed, theRest = self.blockGuru.detectTabbed(lines)

        pre = self.doc.createElement('pre')
        code = self.doc.createElement('code')
        parent_elem.appendChild(pre)
        pre.appendChild(code)
        text = "\n".join(detabbed).rstrip()+"\n"
        #text = text.replace("&", "&amp;")
        code.appendChild(self.doc.createTextNode(text))
        self._processSection(parent_elem, theRest, inList)



    def _handleInline (self, line, patternIndex=0):
        """Transform a Markdown line with inline elements to an XHTML
        fragment.

        This function uses auxiliary objects called inline patterns.
        See notes on inline patterns above.

        @param line: A line of Markdown text
        @param patternIndex: The index of the inlinePattern to start with
        @return: A list of NanoDom nodes """


        parts = [line]

        while patternIndex < len(self.inlinePatterns):

            i = 0

            while i < len(parts):
                
                x = parts[i]

                if isinstance(x, (str, unicode)):
                    result = self._applyPattern(x, \
                                self.inlinePatterns[patternIndex], \
                                patternIndex)

                    if result:
                        i -= 1
                        parts.remove(x)
                        for y in result:
                            parts.insert(i+1,y)

                i += 1
            patternIndex += 1

        for i in range(len(parts)):
            x = parts[i]
            if isinstance(x, (str, unicode)):
                parts[i] = self.doc.createTextNode(x)

        return parts
        

    def _applyPattern(self, line, pattern, patternIndex):

        """ Given a pattern name, this function checks if the line
        fits the pattern, creates the necessary elements, and returns
        back a list consisting of NanoDom elements and/or strings.
        
        @param line: the text to be processed
        @param pattern: the pattern to be checked

        @returns: the appropriate newly created NanoDom element if the
                  pattern matches, None otherwise.
        """

        # match the line to pattern's pre-compiled reg exp.
        # if no match, move on.



        m = pattern.getCompiledRegExp().match(line)
        if not m:
            return None

        # if we got a match let the pattern make us a NanoDom node
        # if it doesn't, move on
        node = pattern.handleMatch(m, self.doc)

        # check if any of this nodes have children that need processing

        if isinstance(node, Element):

            if not node.nodeName in ["code", "pre"]:
                for child in node.childNodes:
                    if isinstance(child, TextNode):
                        
                        result = self._handleInline(child.value, patternIndex+1)
                        
                        if result:

                            if result == [child]:
                                continue
                                
                            result.reverse()
                            #to make insertion easier

                            position = node.childNodes.index(child)
                            
                            node.removeChild(child)

                            for item in result:

                                if isinstance(item, (str, unicode)):
                                    if len(item) > 0:
                                        node.insertChild(position,
                                             self.doc.createTextNode(item))
                                else:
                                    node.insertChild(position, item)
                



        if node:
            # Those are in the reverse order!
            return ( m.groups()[-1], # the string to the left
                     node,           # the new node
                     m.group(1))     # the string to the right of the match

        else:
            return None

    def convert (self, source = None):
        """Return the document in XHTML format.

        @returns: A serialized XHTML body."""

        if source is not None: #Allow blank string
            self.source = source

        if not self.source:
            return u""

        try:
            self.source = unicode(self.source)
        except UnicodeDecodeError:
            message(CRITICAL, 'UnicodeDecodeError: Markdown only accepts unicode or ascii  input.')
            return u""

        for pp in self.textPreprocessors:
            self.source = pp.run(self.source)

        doc = self._transform()
        xml = doc.toxml()


        # Return everything but the top level tag

        if self.stripTopLevelTags:
            xml = xml.strip()[23:-7] + "\n"

        for pp in self.textPostprocessors:
            xml = pp.run(xml)

        return (self.docType + xml).strip()


    def __str__(self):
        ''' Report info about instance. Markdown always returns unicode. '''
        if self.source is None:
            status = 'in which no source text has been assinged.'
        else:
            status = 'which contains %d chars and %d line(s) of source.'%\
                     (len(self.source), self.source.count('\n')+1)
        return 'An instance of "%s" %s'% (self.__class__, status)

    __unicode__ = convert # markdown should always return a unicode string





# ====================================================================

def markdownFromFile(input = None,
                     output = None,
                     extensions = [],
                     encoding = None,
                     message_threshold = CRITICAL,
                     safe = False):

    global console_hndlr
    console_hndlr.setLevel(message_threshold)

    message(DEBUG, "input file: %s" % input)

    if not encoding:
        encoding = "utf-8"

    input_file = codecs.open(input, mode="r", encoding=encoding)
    text = input_file.read()
    input_file.close()

    text = removeBOM(text, encoding)

    new_text = markdown(text, extensions, safe_mode = safe)

    if output:
        output_file = codecs.open(output, "w", encoding=encoding)
        output_file.write(new_text)
        output_file.close()

    else:
        sys.stdout.write(new_text.encode(encoding))

def markdown(text,
             extensions = [],
             safe_mode = False):
    
    message(DEBUG, "in markdown.markdown(), received text:\n%s" % text)

    extension_names = []
    extension_configs = {}
    
    for ext in extensions:
        pos = ext.find("(") 
        if pos == -1:
            extension_names.append(ext)
        else:
            name = ext[:pos]
            extension_names.append(name)
            pairs = [x.split("=") for x in ext[pos+1:-1].split(",")]
            configs = [(x.strip(), y.strip()) for (x, y) in pairs]
            extension_configs[name] = configs

    md = Markdown(extensions=extension_names,
                  extension_configs=extension_configs,
                  safe_mode = safe_mode)

    return md.convert(text)
        

class Extension:

    def __init__(self, configs = {}):
        self.config = configs

    def getConfig(self, key):
        if self.config.has_key(key):
            return self.config[key][0]
        else:
            return ""

    def getConfigInfo(self):
        return [(key, self.config[key][1]) for key in self.config.keys()]

    def setConfig(self, key, value):
        self.config[key][0] = value


OPTPARSE_WARNING = """
Python 2.3 or higher required for advanced command line options.
For lower versions of Python use:

      %s INPUT_FILE > OUTPUT_FILE
    
""" % EXECUTABLE_NAME_FOR_USAGE

def parse_options():

    try:
        optparse = __import__("optparse")
    except:
        if len(sys.argv) == 2:
            return {'input': sys.argv[1],
                    'output': None,
                    'message_threshold': CRITICAL,
                    'safe': False,
                    'extensions': [],
                    'encoding': None }

        else:
            print OPTPARSE_WARNING
            return None

    parser = optparse.OptionParser(usage="%prog INPUTFILE [options]")

    parser.add_option("-f", "--file", dest="filename",
                      help="write output to OUTPUT_FILE",
                      metavar="OUTPUT_FILE")
    parser.add_option("-e", "--encoding", dest="encoding",
                      help="encoding for input and output files",)
    parser.add_option("-q", "--quiet", default = CRITICAL,
                      action="store_const", const=60, dest="verbose",
                      help="suppress all messages")
    parser.add_option("-v", "--verbose",
                      action="store_const", const=INFO, dest="verbose",
                      help="print info messages")
    parser.add_option("-s", "--safe", dest="safe", default=False,
                      metavar="SAFE_MODE",
                      help="same mode ('replace', 'remove' or 'escape'  user's HTML tag)")
    
    parser.add_option("--noisy",
                      action="store_const", const=DEBUG, dest="verbose",
                      help="print debug messages")
    parser.add_option("-x", "--extension", action="append", dest="extensions",
                      help = "load extension EXTENSION", metavar="EXTENSION")

    (options, args) = parser.parse_args()

    if not len(args) == 1:
        parser.print_help()
        return None
    else:
        input_file = args[0]

    if not options.extensions:
        options.extensions = []

    return {'input': input_file,
            'output': options.filename,
            'message_threshold': options.verbose,
            'safe': options.safe,
            'extensions': options.extensions,
            'encoding': options.encoding }

if __name__ == '__main__':
    """ Run Markdown from the command line. """

    options = parse_options()

    #if os.access(inFile, os.R_OK):

    if not options:
        sys.exit(0)
    
    markdownFromFile(**options)











########NEW FILE########
__FILENAME__ = antlr
## This file is part of PyANTLR. See LICENSE.txt for license
## details..........Copyright (C) Wolfgang Haefelinger, 2004.

## get sys module
import sys

version = sys.version.split()[0]
if version < '2.2.1':
    False = 0
if version < '2.3':
    True = not False

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                     global symbols                             ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

### ANTLR Standard Tokens
SKIP                = -1
INVALID_TYPE        = 0
EOF_TYPE            = 1
EOF                 = 1
NULL_TREE_LOOKAHEAD = 3
MIN_USER_TYPE       = 4

### ANTLR's EOF Symbol
EOF_CHAR            = ''

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                    general functions                           ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

def error(fmt,*args):
    if fmt:
        print "error: ", fmt % tuple(args)

def ifelse(cond,_then,_else):
    if cond :
        r = _then
    else:
        r = _else
    return r

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                     ANTLR Exceptions                           ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class ANTLRException(Exception):

    def __init__(self, *args):
        Exception.__init__(self, *args)


class RecognitionException(ANTLRException):

    def __init__(self, *args):
        ANTLRException.__init__(self, *args)
        self.fileName = None
        self.line = -1
        self.column = -1
        if len(args) >= 2:
            self.fileName = args[1]
        if len(args) >= 3:
            self.line = args[2]
        if len(args) >= 4:
            self.column = args[3]

    def __str__(self):
        buf = ['']
        if self.fileName:
            buf.append(self.fileName + ":")
        if self.line != -1:
            if not self.fileName:
                buf.append("line ")
            buf.append(str(self.line))
            if self.column != -1:
                buf.append(":" + str(self.column))
            buf.append(":")
        buf.append(" ")
        return str('').join(buf)

    __repr__ = __str__


class NoViableAltException(RecognitionException):

    def __init__(self, *args):
        RecognitionException.__init__(self, *args)
        self.token = None
        self.node  = None
        if isinstance(args[0],AST):
            self.node = args[0]
        elif isinstance(args[0],Token):
            self.token = args[0]
        else:
            raise TypeError("NoViableAltException requires Token or AST argument")

    def __str__(self):
        if self.token:
            line = self.token.getLine()
            col  = self.token.getColumn()
            text = self.token.getText()
            return "unexpected symbol at line %s (column %s): \"%s\"" % (line,col,text)
        if self.node == ASTNULL:
            return "unexpected end of subtree"
        assert self.node
        ### hackish, we assume that an AST contains method getText
        return "unexpected node: %s" % (self.node.getText())

    __repr__ = __str__


class NoViableAltForCharException(RecognitionException):

    def __init__(self, *args):
        self.foundChar = None
        if len(args) == 2:
            self.foundChar = args[0]
            scanner = args[1]
            RecognitionException.__init__(self, "NoViableAlt",
                                          scanner.getFilename(),
                                          scanner.getLine(),
                                          scanner.getColumn())
        elif len(args) == 4:
            self.foundChar = args[0]
            fileName = args[1]
            line = args[2]
            column = args[3]
            RecognitionException.__init__(self, "NoViableAlt",
                                          fileName, line, column)
        else:
            RecognitionException.__init__(self, "NoViableAlt",
                                          '', -1, -1)

    def __str__(self):
        mesg = "unexpected char: "
        if self.foundChar >= ' ' and self.foundChar <= '~':
            mesg += "'" + self.foundChar + "'"
        elif self.foundChar:
            mesg += "0x" + hex(ord(self.foundChar)).upper()[2:]
        else:
            mesg += "<None>"
        return mesg

    __repr__ = __str__


class SemanticException(RecognitionException):

    def __init__(self, *args):
        RecognitionException.__init__(self, *args)


class MismatchedCharException(RecognitionException):

    NONE = 0
    CHAR = 1
    NOT_CHAR = 2
    RANGE = 3
    NOT_RANGE = 4
    SET = 5
    NOT_SET = 6

    def __init__(self, *args):
        self.args = args
        if len(args) == 5:
            # Expected range / not range
            if args[3]:
                self.mismatchType = MismatchedCharException.NOT_RANGE
            else:
                self.mismatchType = MismatchedCharException.RANGE
            self.foundChar = args[0]
            self.expecting = args[1]
            self.upper = args[2]
            self.scanner = args[4]
            RecognitionException.__init__(self, "Mismatched char range",
                                          self.scanner.getFilename(),
                                          self.scanner.getLine(),
                                          self.scanner.getColumn())
        elif len(args) == 4 and isinstance(args[1], str):
            # Expected char / not char
            if args[2]:
                self.mismatchType = MismatchedCharException.NOT_CHAR
            else:
                self.mismatchType = MismatchedCharException.CHAR
            self.foundChar = args[0]
            self.expecting = args[1]
            self.scanner = args[3]
            RecognitionException.__init__(self, "Mismatched char",
                                          self.scanner.getFilename(),
                                          self.scanner.getLine(),
                                          self.scanner.getColumn())
        elif len(args) == 4 and isinstance(args[1], BitSet):
            # Expected BitSet / not BitSet
            if args[2]:
                self.mismatchType = MismatchedCharException.NOT_SET
            else:
                self.mismatchType = MismatchedCharException.SET
            self.foundChar = args[0]
            self.set = args[1]
            self.scanner = args[3]
            RecognitionException.__init__(self, "Mismatched char set",
                                          self.scanner.getFilename(),
                                          self.scanner.getLine(),
                                          self.scanner.getColumn())
        else:
            self.mismatchType = MismatchedCharException.NONE
            RecognitionException.__init__(self, "Mismatched char")

    ## Append a char to the msg buffer.  If special,
    #  then show escaped version
    #
    def appendCharName(self, sb, c):
        if not c or c == 65535:
            # 65535 = (char) -1 = EOF
            sb.append("'<EOF>'")
        elif c == '\n':
            sb.append("'\\n'")
        elif c == '\r':
            sb.append("'\\r'");
        elif c == '\t':
            sb.append("'\\t'")
        else:
            sb.append('\'' + c + '\'')

    ##
    # Returns an error message with line number/column information
    #
    def __str__(self):
        sb = ['']
        sb.append(RecognitionException.__str__(self))

        if self.mismatchType == MismatchedCharException.CHAR:
            sb.append("expecting ")
            self.appendCharName(sb, self.expecting)
            sb.append(", found ")
            self.appendCharName(sb, self.foundChar)
        elif self.mismatchType == MismatchedCharException.NOT_CHAR:
            sb.append("expecting anything but '")
            self.appendCharName(sb, self.expecting)
            sb.append("'; got it anyway")
        elif self.mismatchType in [MismatchedCharException.RANGE, MismatchedCharException.NOT_RANGE]:
            sb.append("expecting char ")
            if self.mismatchType == MismatchedCharException.NOT_RANGE:
                sb.append("NOT ")
            sb.append("in range: ")
            appendCharName(sb, self.expecting)
            sb.append("..")
            appendCharName(sb, self.upper)
            sb.append(", found ")
            appendCharName(sb, self.foundChar)
        elif self.mismatchType in [MismatchedCharException.SET, MismatchedCharException.NOT_SET]:
            sb.append("expecting ")
            if self.mismatchType == MismatchedCharException.NOT_SET:
                sb.append("NOT ")
            sb.append("one of (")
            for i in range(len(self.set)):
                self.appendCharName(sb, self.set[i])
            sb.append("), found ")
            self.appendCharName(sb, self.foundChar)

        return str().join(sb).strip()

    __repr__ = __str__


class MismatchedTokenException(RecognitionException):

    NONE = 0
    TOKEN = 1
    NOT_TOKEN = 2
    RANGE = 3
    NOT_RANGE = 4
    SET = 5
    NOT_SET = 6

    def __init__(self, *args):
        self.args =  args
        self.tokenNames = []
        self.token = None
        self.tokenText = ''
        self.node =  None
        if len(args) == 6:
            # Expected range / not range
            if args[3]:
                self.mismatchType = MismatchedTokenException.NOT_RANGE
            else:
                self.mismatchType = MismatchedTokenException.RANGE
            self.tokenNames = args[0]
            self.expecting = args[2]
            self.upper = args[3]
            self.fileName = args[5]

        elif len(args) == 4 and isinstance(args[2], int):
            # Expected token / not token
            if args[3]:
                self.mismatchType = MismatchedTokenException.NOT_TOKEN
            else:
                self.mismatchType = MismatchedTokenException.TOKEN
            self.tokenNames = args[0]
            self.expecting = args[2]

        elif len(args) == 4 and isinstance(args[2], BitSet):
            # Expected BitSet / not BitSet
            if args[3]:
                self.mismatchType = MismatchedTokenException.NOT_SET
            else:
                self.mismatchType = MismatchedTokenException.SET
            self.tokenNames = args[0]
            self.set = args[2]

        else:
            self.mismatchType = MismatchedTokenException.NONE
            RecognitionException.__init__(self, "Mismatched Token: expecting any AST node", "<AST>", -1, -1)

        if len(args) >= 2:
            if isinstance(args[1],Token):
                self.token = args[1]
                self.tokenText = self.token.getText()
                RecognitionException.__init__(self, "Mismatched Token",
                                              self.fileName,
                                              self.token.getLine(),
                                              self.token.getColumn())
            elif isinstance(args[1],AST):
                self.node = args[1]
                self.tokenText = str(self.node)
                RecognitionException.__init__(self, "Mismatched Token",
                                              "<AST>",
                                              self.node.getLine(),
                                              self.node.getColumn())
            else:
                self.tokenText = "<empty tree>"
                RecognitionException.__init__(self, "Mismatched Token",
                                              "<AST>", -1, -1)

    def appendTokenName(self, sb, tokenType):
        if tokenType == INVALID_TYPE:
            sb.append("<Set of tokens>")
        elif tokenType < 0 or tokenType >= len(self.tokenNames):
            sb.append("<" + str(tokenType) + ">")
        else:
            sb.append(self.tokenNames[tokenType])

    ##
    # Returns an error message with line number/column information
    #
    def __str__(self):
        sb = ['']
        sb.append(RecognitionException.__str__(self))

        if self.mismatchType == MismatchedTokenException.TOKEN:
            sb.append("expecting ")
            self.appendTokenName(sb, self.expecting)
            sb.append(", found " + self.tokenText)
        elif self.mismatchType == MismatchedTokenException.NOT_TOKEN:
            sb.append("expecting anything but '")
            self.appendTokenName(sb, self.expecting)
            sb.append("'; got it anyway")
        elif self.mismatchType in [MismatchedTokenException.RANGE, MismatchedTokenException.NOT_RANGE]:
            sb.append("expecting token ")
            if self.mismatchType == MismatchedTokenException.NOT_RANGE:
                sb.append("NOT ")
            sb.append("in range: ")
            appendTokenName(sb, self.expecting)
            sb.append("..")
            appendTokenName(sb, self.upper)
            sb.append(", found " + self.tokenText)
        elif self.mismatchType in [MismatchedTokenException.SET, MismatchedTokenException.NOT_SET]:
            sb.append("expecting ")
            if self.mismatchType == MismatchedTokenException.NOT_SET:
                sb.append("NOT ")
            sb.append("one of (")
            for i in range(len(self.set)):
                self.appendTokenName(sb, self.set[i])
            sb.append("), found " + self.tokenText)

        return str().join(sb).strip()

    __repr__ = __str__


class TokenStreamException(ANTLRException):

    def __init__(self, *args):
        ANTLRException.__init__(self, *args)


# Wraps an Exception in a TokenStreamException
class TokenStreamIOException(TokenStreamException):

    def __init__(self, *args):
        if args and isinstance(args[0], Exception):
            io = args[0]
            TokenStreamException.__init__(self, str(io))
            self.io = io
        else:
            TokenStreamException.__init__(self, *args)
            self.io = self


# Wraps a RecognitionException in a TokenStreamException
class TokenStreamRecognitionException(TokenStreamException):

    def __init__(self, *args):
        if args and isinstance(args[0], RecognitionException):
            recog = args[0]
            TokenStreamException.__init__(self, str(recog))
            self.recog = recog
        else:
            raise TypeError("TokenStreamRecognitionException requires RecognitionException argument")

    def __str__(self):
        return str(self.recog)

    __repr__ = __str__


class TokenStreamRetryException(TokenStreamException):

    def __init__(self, *args):
        TokenStreamException.__init__(self, *args)


class CharStreamException(ANTLRException):

    def __init__(self, *args):
        ANTLRException.__init__(self, *args)


# Wraps an Exception in a CharStreamException
class CharStreamIOException(CharStreamException):

    def __init__(self, *args):
        if args and isinstance(args[0], Exception):
            io = args[0]
            CharStreamException.__init__(self, str(io))
            self.io = io
        else:
            CharStreamException.__init__(self, *args)
            self.io = self


class TryAgain(Exception):
    pass


###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       Token                                    ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class Token(object):
    SKIP                = -1
    INVALID_TYPE        = 0
    EOF_TYPE            = 1
    EOF                 = 1
    NULL_TREE_LOOKAHEAD = 3
    MIN_USER_TYPE       = 4

    def __init__(self,**argv):
        try:
            self.type = argv['type']
        except:
            self.type = INVALID_TYPE
        try:
            self.text = argv['text']
        except:
            self.text = "<no text>"

    def isEOF(self):
        return (self.type == EOF_TYPE)

    def getColumn(self):
        return 0

    def getLine(self):
        return 0

    def getFilename(self):
        return None

    def setFilename(self,name):
        return self

    def getText(self):
        return "<no text>"

    def setText(self,text):
        if isinstance(text,str):
            pass
        else:
            raise TypeError("Token.setText requires string argument")
        return self

    def setColumn(self,column):
        return self

    def setLine(self,line):
        return self

    def getType(self):
        return self.type

    def setType(self,type):
        if isinstance(type,int):
            self.type = type
        else:
            raise TypeError("Token.setType requires integer argument")
        return self

    def toString(self):
        ## not optimal
        type_ = self.type
        if type_ == 3:
            tval = 'NULL_TREE_LOOKAHEAD'
        elif type_ == 1:
            tval = 'EOF_TYPE'
        elif type_ == 0:
            tval = 'INVALID_TYPE'
        elif type_ == -1:
            tval = 'SKIP'
        else:
            tval = type_
        return '["%s",<%s>]' % (self.getText(),tval)

    __str__ = toString
    __repr__ = toString

### static attribute ..
Token.badToken = Token( type=INVALID_TYPE, text="<no text>")

if __name__ == "__main__":
    print "testing .."
    T = Token.badToken
    print T

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       CommonToken                              ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class CommonToken(Token):

    def __init__(self,**argv):
        Token.__init__(self,**argv)
        self.line = 0
        self.col  = 0
        try:
            self.line = argv['line']
        except:
            pass
        try:
            self.col = argv['col']
        except:
            pass

    def getLine(self):
        return self.line

    def getText(self):
        return self.text

    def getColumn(self):
        return self.col

    def setLine(self,line):
        self.line = line
        return self

    def setText(self,text):
        self.text = text
        return self

    def setColumn(self,col):
        self.col = col
        return self

    def toString(self):
        ## not optimal
        type_ = self.type
        if type_ == 3:
            tval = 'NULL_TREE_LOOKAHEAD'
        elif type_ == 1:
            tval = 'EOF_TYPE'
        elif type_ == 0:
            tval = 'INVALID_TYPE'
        elif type_ == -1:
            tval = 'SKIP'
        else:
            tval = type_
        d = {
           'text' : self.text,
           'type' : tval,
           'line' : self.line,
           'colm' : self.col
           }

        fmt = '["%(text)s",<%(type)s>,line=%(line)s,col=%(colm)s]'
        return fmt % d

    __str__ = toString
    __repr__ = toString


if __name__ == '__main__' :
    T = CommonToken()
    print T
    T = CommonToken(col=15,line=1,text="some text", type=5)
    print T
    T = CommonToken()
    T.setLine(1).setColumn(15).setText("some text").setType(5)
    print T
    print T.getLine()
    print T.getColumn()
    print T.getText()
    print T.getType()

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                    CommonHiddenStreamToken                     ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class CommonHiddenStreamToken(CommonToken):
    def __init__(self,*args):
        CommonToken.__init__(self,*args)
        self.hiddenBefore = None
        self.hiddenAfter  = None

    def getHiddenAfter(self):
        return self.hiddenAfter

    def getHiddenBefore(self):
        return self.hiddenBefore

    def setHiddenAfter(self,t):
        self.hiddenAfter = t

    def setHiddenBefore(self, t):
        self.hiddenBefore = t

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       Queue                                    ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

## Shall be a circular buffer on tokens ..
class Queue(object):

    def __init__(self):
        self.buffer = [] # empty list

    def append(self,item):
        self.buffer.append(item)

    def elementAt(self,index):
        return self.buffer[index]

    def reset(self):
        self.buffer = []

    def removeFirst(self):
        self.buffer.pop(0)

    def length(self):
        return len(self.buffer)

    def __str__(self):
        return str(self.buffer)

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       InputBuffer                              ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class InputBuffer(object):
    def __init__(self):
        self.nMarkers = 0
        self.markerOffset = 0
        self.numToConsume = 0
        self.queue = Queue()

    def __str__(self):
        return "(%s,%s,%s,%s)" % (
           self.nMarkers,
           self.markerOffset,
           self.numToConsume,
           self.queue)

    def __repr__(self):
        return str(self)

    def commit(self):
        self.nMarkers -= 1

    def consume(self) :
        self.numToConsume += 1

    ## probably better to return a list of items
    ## because of unicode. Or return a unicode
    ## string ..
    def getLAChars(self) :
        i = self.markerOffset
        n = self.queue.length()
        s = ''
        while i<n:
            s += self.queue.elementAt(i)
        return s

    ## probably better to return a list of items
    ## because of unicode chars
    def getMarkedChars(self) :
        s = ''
        i = 0
        n = self.markerOffset
        while i<n:
            s += self.queue.elementAt(i)
        return s

    def isMarked(self) :
        return self.nMarkers != 0

    def fill(self,k):
        ### abstract method
        raise NotImplementedError()

    def LA(self,k) :
        self.fill(k)
        return self.queue.elementAt(self.markerOffset + k - 1)

    def mark(self) :
        self.syncConsume()
        self.nMarkers += 1
        return self.markerOffset

    def rewind(self,mark) :
        self.syncConsume()
        self.markerOffset = mark
        self.nMarkers -= 1

    def reset(self) :
        self.nMarkers = 0
        self.markerOffset = 0
        self.numToConsume = 0
        self.queue.reset()

    def syncConsume(self) :
        while self.numToConsume > 0:
            if self.nMarkers > 0:
                # guess mode -- leave leading characters and bump offset.
                self.markerOffset += 1
            else:
                # normal mode -- remove first character
                self.queue.removeFirst()
            self.numToConsume -= 1

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       CharBuffer                               ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class CharBuffer(InputBuffer):
    def __init__(self,reader):
        ##assert isinstance(reader,file)
        super(CharBuffer,self).__init__()
        ## a reader is supposed to be anything that has
        ## a method 'read(int)'.
        self.input = reader

    def __str__(self):
        base = super(CharBuffer,self).__str__()
        return "CharBuffer{%s,%s" % (base,str(input))

    def fill(self,amount):
        try:
            self.syncConsume()
            while self.queue.length() < (amount + self.markerOffset) :
                ## retrieve just one char - what happend at end
                ## of input?
                c = self.input.read(1)
                ### python's behaviour is to return the empty string  on
                ### EOF, ie. no exception whatsoever is thrown. An empty
                ### python  string  has  the  nice feature that it is of
                ### type 'str' and  "not ''" would return true. Contrary,
                ### one can't  do  this: '' in 'abc'. This should return
                ### false,  but all we  get  is  then  a TypeError as an
                ### empty string is not a character.

                ### Let's assure then that we have either seen a
                ### character or an empty string (EOF).
                assert len(c) == 0 or len(c) == 1

                ### And it shall be of type string (ASCII or UNICODE).
                assert isinstance(c,str) or isinstance(c,unicode)

                ### Just append EOF char to buffer. Note that buffer may
                ### contain then just more than one EOF char ..

                ### use unicode chars instead of ASCII ..
                self.queue.append(c)
        except Exception,e:
            raise CharStreamIOException(e)
        ##except: # (mk) Cannot happen ...
            ##error ("unexpected exception caught ..")
            ##assert 0

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       LexerSharedInputState                    ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class LexerSharedInputState(object):
    def __init__(self,ibuf):
        assert isinstance(ibuf,InputBuffer)
        self.input = ibuf
        self.column = 1
        self.line = 1
        self.tokenStartColumn = 1
        self.tokenStartLine = 1
        self.guessing = 0
        self.filename = None

    def reset(self):
        self.column = 1
        self.line = 1
        self.tokenStartColumn = 1
        self.tokenStartLine = 1
        self.guessing = 0
        self.filename = None
        self.input.reset()

    def LA(self,k):
        return self.input.LA(k)

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                    TokenStream                                 ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TokenStream(object):
    def nextToken(self):
        pass

    def __iter__(self):
        return TokenStreamIterator(self)

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                    TokenStreamIterator                                 ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TokenStreamIterator(object):
    def __init__(self,inst):
        if isinstance(inst,TokenStream):
            self.inst = inst
            return
        raise TypeError("TokenStreamIterator requires TokenStream object")

    def next(self):
        assert self.inst
        item = self.inst.nextToken()
        if not item or item.isEOF():
            raise StopIteration()
        return item

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                    TokenStreamSelector                        ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TokenStreamSelector(TokenStream):

    def __init__(self):
        self._input = None
        self._stmap = {}
        self._stack = []

    def addInputStream(self,stream,key):
        self._stmap[key] = stream

    def getCurrentStream(self):
        return self._input

    def getStream(self,sname):
        try:
            stream = self._stmap[sname]
        except:
            raise ValueError("TokenStream " + sname + " not found");
        return stream;

    def nextToken(self):
        while 1:
            try:
                return self._input.nextToken()
            except TokenStreamRetryException,r:
                ### just retry "forever"
                pass

    def pop(self):
        stream = self._stack.pop();
        self.select(stream);
        return stream;

    def push(self,arg):
        self._stack.append(self._input);
        self.select(arg)

    def retry(self):
        raise TokenStreamRetryException()

    def select(self,arg):
        if isinstance(arg,TokenStream):
            self._input = arg
            return
        if isinstance(arg,str):
            self._input = self.getStream(arg)
            return
        raise TypeError("TokenStreamSelector.select requires " +
                        "TokenStream or string argument")

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                      TokenStreamBasicFilter                    ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TokenStreamBasicFilter(TokenStream):

    def __init__(self,input):

        self.input = input;
        self.discardMask = BitSet()

    def discard(self,arg):
        if isinstance(arg,int):
            self.discardMask.add(arg)
            return
        if isinstance(arg,BitSet):
            self.discardMark = arg
            return
        raise TypeError("TokenStreamBasicFilter.discard requires" +
                        "integer or BitSet argument")

    def nextToken(self):
        tok = self.input.nextToken()
        while tok and self.discardMask.member(tok.getType()):
            tok = self.input.nextToken()
        return tok

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                      TokenStreamHiddenTokenFilter              ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TokenStreamHiddenTokenFilter(TokenStreamBasicFilter):

    def __init__(self,input):
        TokenStreamBasicFilter.__init__(self,input)
        self.hideMask = BitSet()
        self.nextMonitoredToken = None
        self.lastHiddenToken = None
        self.firstHidden = None

    def consume(self):
        self.nextMonitoredToken = self.input.nextToken()

    def consumeFirst(self):
        self.consume()

        p = None;
        while self.hideMask.member(self.LA(1).getType()) or \
              self.discardMask.member(self.LA(1).getType()):
            if self.hideMask.member(self.LA(1).getType()):
                if not p:
                    p = self.LA(1)
                else:
                    p.setHiddenAfter(self.LA(1))
                    self.LA(1).setHiddenBefore(p)
                    p = self.LA(1)
                self.lastHiddenToken = p
                if not self.firstHidden:
                    self.firstHidden = p
            self.consume()

    def getDiscardMask(self):
        return self.discardMask

    def getHiddenAfter(self,t):
        return t.getHiddenAfter()

    def getHiddenBefore(self,t):
        return t.getHiddenBefore()

    def getHideMask(self):
        return self.hideMask

    def getInitialHiddenToken(self):
        return self.firstHidden

    def hide(self,m):
        if isinstance(m,int):
            self.hideMask.add(m)
            return
        if isinstance(m.BitMask):
            self.hideMask = m
            return

    def LA(self,i):
        return self.nextMonitoredToken

    def nextToken(self):
        if not self.LA(1):
            self.consumeFirst()

        monitored = self.LA(1)

        monitored.setHiddenBefore(self.lastHiddenToken)
        self.lastHiddenToken = None

        self.consume()
        p = monitored

        while self.hideMask.member(self.LA(1).getType()) or \
              self.discardMask.member(self.LA(1).getType()):
            if self.hideMask.member(self.LA(1).getType()):
                p.setHiddenAfter(self.LA(1))
                if p != monitored:
                    self.LA(1).setHiddenBefore(p)
                p = self.lastHiddenToken = self.LA(1)
            self.consume()
        return monitored

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       StringBuffer                             ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class StringBuffer:
    def __init__(self,string=None):
        if string:
            self.text = list(string)
        else:
            self.text = []

    def setLength(self,sz):
        if not sz :
            self.text = []
            return
        assert sz>0
        if sz >= self.length():
            return
        ### just reset to empty buffer
        self.text = self.text[0:sz]

    def length(self):
        return len(self.text)

    def append(self,c):
        self.text.append(c)

    ### return buffer as string. Arg 'a' is  used  as index
    ## into the buffer and 2nd argument shall be the length.
    ## If 2nd args is absent, we return chars till end of
    ## buffer starting with 'a'.
    def getString(self,a=None,length=None):
        if not a :
            a = 0
        assert a>=0
        if a>= len(self.text) :
            return ""

        if not length:
            ## no second argument
            L = self.text[a:]
        else:
            assert (a+length) <= len(self.text)
            b = a + length
            L = self.text[a:b]
        s = ""
        for x in L : s += x
        return s

    toString = getString ## alias

    def __str__(self):
        return str(self.text)

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       Reader                                   ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

## When reading Japanese chars, it happens that a stream returns a
## 'char' of length 2. This looks like  a  bug  in the appropriate
## codecs - but I'm  rather  unsure about this. Anyway, if this is
## the case, I'm going to  split  this string into a list of chars
## and put them  on  hold, ie. on a  buffer. Next time when called
## we read from buffer until buffer is empty.
## wh: nov, 25th -> problem does not appear in Python 2.4.0.c1.

class Reader(object):
    def __init__(self,stream):
        self.cin = stream
        self.buf = []

    def read(self,num):
        assert num==1

        if len(self.buf):
            return self.buf.pop()

        ## Read a char - this may return a string.
        ## Is this a bug in codecs/Python?
        c = self.cin.read(1)

        if not c or len(c)==1:
            return c

        L = list(c)
        L.reverse()
        for x in L:
            self.buf.append(x)

        ## read one char ..
        return self.read(1)

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       CharScanner                              ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class CharScanner(TokenStream):
    ## class members
    NO_CHAR = 0
    EOF_CHAR = ''  ### EOF shall be the empty string.

    def __init__(self, *argv, **kwargs):
        super(CharScanner, self).__init__()
        self.saveConsumedInput = True
        self.tokenClass = None
        self.caseSensitive = True
        self.caseSensitiveLiterals = True
        self.literals = None
        self.tabsize = 8
        self._returnToken = None
        self.commitToPath = False
        self.traceDepth = 0
        self.text = StringBuffer()
        self.hashString = hash(self)
        self.setTokenObjectClass(CommonToken)
        self.setInput(*argv)

    def __iter__(self):
        return CharScannerIterator(self)

    def setInput(self,*argv):
        ## case 1:
        ## if there's no arg we default to read from
        ## standard input
        if not argv:
            import sys
            self.setInput(sys.stdin)
            return

        ## get 1st argument
        arg1 = argv[0]

        ## case 2:
        ## if arg1 is a string,  we assume it's a file name
        ## and  open  a  stream  using 2nd argument as open
        ## mode. If there's no 2nd argument we fall back to
        ## mode '+rb'.
        if isinstance(arg1,str):
            f = open(arg1,"rb")
            self.setInput(f)
            self.setFilename(arg1)
            return

        ## case 3:
        ## if arg1 is a file we wrap it by a char buffer (
        ## some additional checks?? No, can't do this in
        ## general).
        if isinstance(arg1,file):
            self.setInput(CharBuffer(arg1))
            return

        ## case 4:
        ## if arg1 is of type SharedLexerInputState we use
        ## argument as is.
        if isinstance(arg1,LexerSharedInputState):
            self.inputState = arg1
            return

        ## case 5:
        ## check whether argument type is of type input
        ## buffer. If so create a SharedLexerInputState and
        ## go ahead.
        if isinstance(arg1,InputBuffer):
            self.setInput(LexerSharedInputState(arg1))
            return

        ## case 6:
        ## check whether argument type has a method read(int)
        ## If so create CharBuffer ...
        try:
            if arg1.read:
                rd = Reader(arg1)
                cb = CharBuffer(rd)
                ss = LexerSharedInputState(cb)
                self.inputState = ss
            return
        except:
            pass

        ## case 7:
        ## raise wrong argument exception
        raise TypeError(argv)

    def setTabSize(self,size) :
        self.tabsize = size

    def getTabSize(self) :
        return self.tabsize

    def setCaseSensitive(self,t) :
        self.caseSensitive = t

    def setCommitToPath(self,commit) :
        self.commitToPath = commit

    def setFilename(self,f) :
        self.inputState.filename = f

    def setLine(self,line) :
        self.inputState.line = line

    def setText(self,s) :
        self.resetText()
        self.text.append(s)

    def getCaseSensitive(self) :
        return self.caseSensitive

    def getCaseSensitiveLiterals(self) :
        return self.caseSensitiveLiterals

    def getColumn(self) :
        return self.inputState.column

    def setColumn(self,c) :
        self.inputState.column = c

    def getCommitToPath(self) :
        return self.commitToPath

    def getFilename(self) :
        return self.inputState.filename

    def getInputBuffer(self) :
        return self.inputState.input

    def getInputState(self) :
        return self.inputState

    def setInputState(self,state) :
        assert isinstance(state,LexerSharedInputState)
        self.inputState = state

    def getLine(self) :
        return self.inputState.line

    def getText(self) :
        return str(self.text)

    def getTokenObject(self) :
        return self._returnToken

    def LA(self,i) :
        c = self.inputState.input.LA(i)
        if not self.caseSensitive:
            ### E0006
            c = c.__class__.lower(c)
        return c

    def makeToken(self,type) :
        try:
            ## dynamically load a class
            assert self.tokenClass
            tok = self.tokenClass()
            tok.setType(type)
            tok.setColumn(self.inputState.tokenStartColumn)
            tok.setLine(self.inputState.tokenStartLine)
            return tok
        except:
            self.panic("unable to create new token")
        return Token.badToken

    def mark(self) :
        return self.inputState.input.mark()

    def _match_bitset(self,b) :
        if b.member(self.LA(1)):
            self.consume()
        else:
            raise MismatchedCharException(self.LA(1), b, False, self)

    def _match_string(self,s) :
        for c in s:
            if self.LA(1) == c:
                self.consume()
            else:
                raise MismatchedCharException(self.LA(1), c, False, self)

    def match(self,item):
        if isinstance(item,str) or isinstance(item,unicode):
            return self._match_string(item)
        else:
            return self._match_bitset(item)

    def matchNot(self,c) :
        if self.LA(1) != c:
            self.consume()
        else:
            raise MismatchedCharException(self.LA(1), c, True, self)

    def matchRange(self,c1,c2) :
        if self.LA(1) < c1 or self.LA(1) > c2 :
            raise MismatchedCharException(self.LA(1), c1, c2, False, self)
        else:
            self.consume()

    def newline(self) :
        self.inputState.line += 1
        self.inputState.column = 1

    def tab(self) :
        c = self.getColumn()
        nc = ( ((c-1)/self.tabsize) + 1) * self.tabsize + 1
        self.setColumn(nc)

    def panic(self,s='') :
        print "CharScanner: panic: " + s
        sys.exit(1)

    def reportError(self,ex) :
        print ex

    def reportError(self,s) :
        if not self.getFilename():
            print "error: " + str(s)
        else:
            print self.getFilename() + ": error: " + str(s)

    def reportWarning(self,s) :
        if not self.getFilename():
            print "warning: " + str(s)
        else:
            print self.getFilename() + ": warning: " + str(s)

    def resetText(self) :
        self.text.setLength(0)
        self.inputState.tokenStartColumn = self.inputState.column
        self.inputState.tokenStartLine = self.inputState.line

    def rewind(self,pos) :
        self.inputState.input.rewind(pos)

    def setTokenObjectClass(self,cl):
        self.tokenClass = cl

    def testForLiteral(self,token):
        if not token:
            return
        assert isinstance(token,Token)

        _type = token.getType()

        ## special tokens can't be literals
        if _type in [SKIP,INVALID_TYPE,EOF_TYPE,NULL_TREE_LOOKAHEAD] :
            return

        _text = token.getText()
        if not _text:
            return

        assert isinstance(_text,str) or isinstance(_text,unicode)
        _type = self.testLiteralsTable(_text,_type)
        token.setType(_type)
        return _type

    def testLiteralsTable(self,*args):
        if isinstance(args[0],str) or isinstance(args[0],unicode):
            s = args[0]
            i = args[1]
        else:
            s = self.text.getString()
            i = args[0]

        ## check whether integer has been given
        if not isinstance(i,int):
            assert isinstance(i,int)

        ## check whether we have a dict
        assert isinstance(self.literals,dict)
        try:
            ## E0010
            if not self.caseSensitiveLiterals:
                s = s.__class__.lower(s)
            i = self.literals[s]
        except:
            pass
        return i

    def toLower(self,c):
        return c.__class__.lower()

    def traceIndent(self):
        print ' ' * self.traceDepth

    def traceIn(self,rname):
        self.traceDepth += 1
        self.traceIndent()
        print "> lexer %s c== %s" % (rname,self.LA(1))

    def traceOut(self,rname):
        self.traceIndent()
        print "< lexer %s c== %s" % (rname,self.LA(1))
        self.traceDepth -= 1

    def uponEOF(self):
        pass

    def append(self,c):
        if self.saveConsumedInput :
            self.text.append(c)

    def commit(self):
        self.inputState.input.commit()

    def consume(self):
        if not self.inputState.guessing:
            c = self.LA(1)
            if self.caseSensitive:
                self.append(c)
            else:
                # use input.LA(), not LA(), to get original case
                # CharScanner.LA() would toLower it.
                c =  self.inputState.input.LA(1)
                self.append(c)

            if c and c in "\t":
                self.tab()
            else:
                self.inputState.column += 1
        self.inputState.input.consume()

    ## Consume chars until one matches the given char
    def consumeUntil_char(self,c):
        while self.LA(1) != EOF_CHAR and self.LA(1) != c:
            self.consume()

    ## Consume chars until one matches the given set
    def consumeUntil_bitset(self,bitset):
        while self.LA(1) != EOF_CHAR and not self.set.member(self.LA(1)):
            self.consume()

    ### If symbol seen is EOF then generate and set token, otherwise
    ### throw exception.
    def default(self,la1):
        if not la1 :
            self.uponEOF()
            self._returnToken = self.makeToken(EOF_TYPE)
        else:
            self.raise_NoViableAlt(la1)

    def filterdefault(self,la1,*args):
        if not la1:
            self.uponEOF()
            self._returnToken = self.makeToken(EOF_TYPE)
            return

        if not args:
            self.consume()
            raise TryAgain()
        else:
            ### apply filter object
            self.commit();
            try:
                func=args[0]
                args=args[1:]
                apply(func,args)
            except RecognitionException, e:
                ## catastrophic failure
                self.reportError(e);
                self.consume();
            raise TryAgain()

    def raise_NoViableAlt(self,la1=None):
        if not la1: la1 = self.LA(1)
        fname = self.getFilename()
        line  = self.getLine()
        col   = self.getColumn()
        raise NoViableAltForCharException(la1,fname,line,col)

    def set_return_token(self,_create,_token,_ttype,_offset):
        if _create and not _token and (not _ttype == SKIP):
            string = self.text.getString(_offset)
            _token = self.makeToken(_ttype)
            _token.setText(string)
        self._returnToken = _token
        return _token

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                   CharScannerIterator                          ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class CharScannerIterator:

    def __init__(self,inst):
        if isinstance(inst,CharScanner):
            self.inst = inst
            return
        raise TypeError("CharScannerIterator requires CharScanner object")

    def next(self):
        assert self.inst
        item = self.inst.nextToken()
        if not item or item.isEOF():
            raise StopIteration()
        return item

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       BitSet                                   ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

### I'm assuming here that a long is 64bits. It appears however, that
### a long is of any size. That means we can use a single long as the
### bitset (!), ie. Python would do almost all the work (TBD).

class BitSet(object):
    BITS     = 64
    NIBBLE   = 4
    LOG_BITS = 6
    MOD_MASK = BITS -1

    def __init__(self,data=None):
        if not data:
            BitSet.__init__(self,[long(0)])
            return
        if isinstance(data,int):
            BitSet.__init__(self,[long(data)])
            return
        if isinstance(data,long):
            BitSet.__init__(self,[data])
            return
        if not isinstance(data,list):
            raise TypeError("BitSet requires integer, long, or " +
                            "list argument")
        for x in data:
            if not isinstance(x,long):
                raise TypeError(self,"List argument item is " +
                                "not a long: %s" % (x))
        self.data = data

    def __str__(self):
        bits = len(self.data) * BitSet.BITS
        s = ""
        for i in xrange(0,bits):
            if self.at(i):
                s += "1"
            else:
                s += "o"
            if not ((i+1) % 10):
                s += '|%s|' % (i+1)
        return s

    def __repr__(self):
        return str(self)

    def member(self,item):
        if not item:
            return False

        if isinstance(item,int):
            return self.at(item)

        if not (isinstance(item,str) or isinstance(item,unicode)):
            raise TypeError(self,"char or unichar expected: %s" % (item))

        ## char is a (unicode) string with at most lenght 1, ie.
        ## a char.

        if len(item) != 1:
            raise TypeError(self,"char expected: %s" % (item))

        ### handle ASCII/UNICODE char
        num = ord(item)

        ### check whether position num is in bitset
        return self.at(num)

    def wordNumber(self,bit):
        return bit >> BitSet.LOG_BITS

    def bitMask(self,bit):
        pos = bit & BitSet.MOD_MASK  ## bit mod BITS
        return (1L << pos)

    def set(self,bit,on=True):
        # grow bitset as required (use with care!)
        i = self.wordNumber(bit)
        mask = self.bitMask(bit)
        if i>=len(self.data):
            d = i - len(self.data) + 1
            for x in xrange(0,d):
                self.data.append(0L)
            assert len(self.data) == i+1
        if on:
            self.data[i] |=  mask
        else:
            self.data[i] &= (~mask)

    ### make add an alias for set
    add = set

    def off(self,bit,off=True):
        self.set(bit,not off)

    def at(self,bit):
        i = self.wordNumber(bit)
        v = self.data[i]
        m = self.bitMask(bit)
        return v & m


###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                      some further funcs                        ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

def illegalarg_ex(func):
    raise ValueError(
       "%s is only valid if parser is built for debugging" %
       (func.func_name))

def runtime_ex(func):
    raise RuntimeException(
       "%s is only valid if parser is built for debugging" %
       (func.func_name))

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       TokenBuffer                              ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TokenBuffer(object):
    def __init__(self,stream):
        self.input = stream
        self.nMarkers = 0
        self.markerOffset = 0
        self.numToConsume = 0
        self.queue = Queue()

    def reset(self) :
        self.nMarkers = 0
        self.markerOffset = 0
        self.numToConsume = 0
        self.queue.reset()

    def consume(self) :
        self.numToConsume += 1

    def fill(self, amount):
        self.syncConsume()
        while self.queue.length() < (amount + self.markerOffset):
            self.queue.append(self.input.nextToken())

    def getInput(self):
        return self.input

    def LA(self,k) :
        self.fill(k)
        return self.queue.elementAt(self.markerOffset + k - 1).type

    def LT(self,k) :
        self.fill(k)
        return self.queue.elementAt(self.markerOffset + k - 1)

    def mark(self) :
        self.syncConsume()
        self.nMarkers += 1
        return self.markerOffset

    def rewind(self,mark) :
        self.syncConsume()
        self.markerOffset = mark
        self.nMarkers -= 1

    def syncConsume(self) :
        while self.numToConsume > 0:
            if self.nMarkers > 0:
                # guess mode -- leave leading characters and bump offset.
                self.markerOffset += 1
            else:
                # normal mode -- remove first character
                self.queue.removeFirst()
            self.numToConsume -= 1

    def __str__(self):
        return "(%s,%s,%s,%s,%s)" % (
           self.input,
           self.nMarkers,
           self.markerOffset,
           self.numToConsume,
           self.queue)

    def __repr__(self):
        return str(self)

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       ParserSharedInputState                   ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class ParserSharedInputState(object):

    def __init__(self):
        self.input = None
        self.reset()

    def reset(self):
        self.guessing = 0
        self.filename = None
        if self.input:
            self.input.reset()

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       Parser                                   ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class Parser(object):

    def __init__(self, *args, **kwargs):
        self.tokenNames = None
        self.returnAST  = None
        self.astFactory = None
        self.tokenTypeToASTClassMap = {}
        self.ignoreInvalidDebugCalls = False
        self.traceDepth = 0
        if not args:
            self.inputState = ParserSharedInputState()
            return
        arg0 = args[0]
        assert isinstance(arg0,ParserSharedInputState)
        self.inputState = arg0
        return

    def getTokenTypeToASTClassMap(self):
        return self.tokenTypeToASTClassMap


    def addMessageListener(self, l):
        if not self.ignoreInvalidDebugCalls:
            illegalarg_ex(addMessageListener)

    def addParserListener(self,l) :
        if (not self.ignoreInvalidDebugCalls) :
            illegalarg_ex(addParserListener)

    def addParserMatchListener(self, l) :
        if (not self.ignoreInvalidDebugCalls) :
            illegalarg_ex(addParserMatchListener)

    def addParserTokenListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            illegalarg_ex(addParserTokenListener)

    def addSemanticPredicateListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            illegalarg_ex(addSemanticPredicateListener)

    def addSyntacticPredicateListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            illegalarg_ex(addSyntacticPredicateListener)

    def addTraceListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            illegalarg_ex(addTraceListener)

    def consume(self):
        raise NotImplementedError()

    def _consumeUntil_type(self,tokenType):
        while self.LA(1) != EOF_TYPE and self.LA(1) != tokenType:
            self.consume()

    def _consumeUntil_bitset(self, set):
        while self.LA(1) != EOF_TYPE and not set.member(self.LA(1)):
            self.consume()

    def consumeUntil(self,arg):
        if isinstance(arg,int):
            self._consumeUntil_type(arg)
        else:
            self._consumeUntil_bitset(arg)

    def defaultDebuggingSetup(self):
        pass

    def getAST(self) :
        return self.returnAST

    def getASTFactory(self) :
        return self.astFactory

    def getFilename(self) :
        return self.inputState.filename

    def getInputState(self) :
        return self.inputState

    def setInputState(self, state) :
        self.inputState = state

    def getTokenName(self,num) :
        return self.tokenNames[num]

    def getTokenNames(self) :
        return self.tokenNames

    def isDebugMode(self) :
        return self.false

    def LA(self, i):
        raise NotImplementedError()

    def LT(self, i):
        raise NotImplementedError()

    def mark(self):
        return self.inputState.input.mark()

    def _match_int(self,t):
        if (self.LA(1) != t):
            raise MismatchedTokenException(
               self.tokenNames, self.LT(1), t, False, self.getFilename())
        else:
            self.consume()

    def _match_set(self, b):
        if (not b.member(self.LA(1))):
            raise MismatchedTokenException(
               self.tokenNames,self.LT(1), b, False, self.getFilename())
        else:
            self.consume()

    def match(self,set) :
        if isinstance(set,int):
            self._match_int(set)
            return
        if isinstance(set,BitSet):
            self._match_set(set)
            return
        raise TypeError("Parser.match requires integer ot BitSet argument")

    def matchNot(self,t):
        if self.LA(1) == t:
            raise MismatchedTokenException(
               tokenNames, self.LT(1), t, True, self.getFilename())
        else:
            self.consume()

    def removeMessageListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeMessageListener)

    def removeParserListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeParserListener)

    def removeParserMatchListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeParserMatchListener)

    def removeParserTokenListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeParserTokenListener)

    def removeSemanticPredicateListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeSemanticPredicateListener)

    def removeSyntacticPredicateListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeSyntacticPredicateListener)

    def removeTraceListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeTraceListener)

    def reportError(self,x) :
        fmt = "syntax error:"
        f = self.getFilename()
        if f:
            fmt = ("%s:" % f) + fmt
        if isinstance(x,Token):
            line = x.getColumn()
            col  = x.getLine()
            text = x.getText()
            fmt  = fmt + 'unexpected symbol at line %s (column %s) : "%s"'
            print >>sys.stderr, fmt % (line,col,text)
        else:
            print >>sys.stderr, fmt,str(x)

    def reportWarning(self,s):
        f = self.getFilename()
        if f:
            print "%s:warning: %s" % (f,str(x))
        else:
            print "warning: %s" % (str(x))

    def rewind(self, pos) :
        self.inputState.input.rewind(pos)

    def setASTFactory(self, f) :
        self.astFactory = f

    def setASTNodeClass(self, cl) :
        self.astFactory.setASTNodeType(cl)

    def setASTNodeType(self, nodeType) :
        self.setASTNodeClass(nodeType)

    def setDebugMode(self, debugMode) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(setDebugMode)

    def setFilename(self, f) :
        self.inputState.filename = f

    def setIgnoreInvalidDebugCalls(self, value) :
        self.ignoreInvalidDebugCalls = value

    def setTokenBuffer(self, t) :
        self.inputState.input = t

    def traceIndent(self):
        print " " * self.traceDepth

    def traceIn(self,rname):
        self.traceDepth += 1
        self.trace("> ", rname)

    def traceOut(self,rname):
        self.trace("< ", rname)
        self.traceDepth -= 1

    ### wh: moved from ASTFactory to Parser
    def addASTChild(self,currentAST, child):
        if not child:
            return
        if not currentAST.root:
            currentAST.root = child
        elif not currentAST.child:
            currentAST.root.setFirstChild(child)
        else:
            currentAST.child.setNextSibling(child)
        currentAST.child = child
        currentAST.advanceChildToEnd()

    ### wh: moved from ASTFactory to Parser
    def makeASTRoot(self,currentAST,root) :
        if root:
            ### Add the current root as a child of new root
            root.addChild(currentAST.root)
            ### The new current child is the last sibling of the old root
            currentAST.child = currentAST.root
            currentAST.advanceChildToEnd()
            ### Set the new root
            currentAST.root = root

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       LLkParser                                ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class LLkParser(Parser):

    def __init__(self, *args, **kwargs):
        try:
            arg1 = args[0]
        except:
            arg1 = 1

        if isinstance(arg1,int):
            super(LLkParser,self).__init__()
            self.k = arg1
            return

        if isinstance(arg1,ParserSharedInputState):
            super(LLkParser,self).__init__(arg1)
            self.set_k(1,*args)
            return

        if isinstance(arg1,TokenBuffer):
            super(LLkParser,self).__init__()
            self.setTokenBuffer(arg1)
            self.set_k(1,*args)
            return

        if isinstance(arg1,TokenStream):
            super(LLkParser,self).__init__()
            tokenBuf = TokenBuffer(arg1)
            self.setTokenBuffer(tokenBuf)
            self.set_k(1,*args)
            return

        ### unknown argument
        raise TypeError("LLkParser requires integer, " +
                        "ParserSharedInputStream or TokenStream argument")

    def consume(self):
        self.inputState.input.consume()

    def LA(self,i):
        return self.inputState.input.LA(i)

    def LT(self,i):
        return self.inputState.input.LT(i)

    def set_k(self,index,*args):
        try:
            self.k = args[index]
        except:
            self.k = 1

    def trace(self,ee,rname):
        print type(self)
        self.traceIndent()
        guess = ""
        if self.inputState.guessing > 0:
            guess = " [guessing]"
        print(ee + rname + guess)
        for i in xrange(1,self.k+1):
            if i != 1:
                print(", ")
            if self.LT(i) :
                v = self.LT(i).getText()
            else:
                v = "null"
            print "LA(%s) == %s" % (i,v)
        print("\n")

    def traceIn(self,rname):
        self.traceDepth += 1;
        self.trace("> ", rname);

    def traceOut(self,rname):
        self.trace("< ", rname);
        self.traceDepth -= 1;

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                    TreeParserSharedInputState                  ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TreeParserSharedInputState(object):
    def __init__(self):
        self.guessing = 0

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       TreeParser                               ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TreeParser(object):

    def __init__(self, *args, **kwargs):
        self.inputState = TreeParserSharedInputState()
        self._retTree   = None
        self.tokenNames = []
        self.returnAST  = None
        self.astFactory = ASTFactory()
        self.traceDepth = 0

    def getAST(self):
        return self.returnAST

    def getASTFactory(self):
        return self.astFactory

    def getTokenName(self,num) :
        return self.tokenNames[num]

    def getTokenNames(self):
        return self.tokenNames

    def match(self,t,set) :
        assert isinstance(set,int) or isinstance(set,BitSet)
        if not t or t == ASTNULL:
            raise MismatchedTokenException(self.getTokenNames(), t,set, False)

        if isinstance(set,int) and t.getType() != set:
            raise MismatchedTokenException(self.getTokenNames(), t,set, False)

        if isinstance(set,BitSet) and not set.member(t.getType):
            raise MismatchedTokenException(self.getTokenNames(), t,set, False)

    def matchNot(self,t, ttype) :
        if not t or (t == ASTNULL) or (t.getType() == ttype):
            raise MismatchedTokenException(getTokenNames(), t, ttype, True)

    def reportError(self,ex):
        print >>sys.stderr,"error:",ex

    def  reportWarning(self, s):
        print "warning:",s

    def setASTFactory(self,f):
        self.astFactory = f

    def setASTNodeType(self,nodeType):
        self.setASTNodeClass(nodeType)

    def setASTNodeClass(self,nodeType):
        self.astFactory.setASTNodeType(nodeType)

    def traceIndent(self):
        print " " * self.traceDepth

    def traceIn(self,rname,t):
        self.traceDepth += 1
        self.traceIndent()
        print("> " + rname + "(" +
              ifelse(t,str(t),"null") + ")" +
              ifelse(self.inputState.guessing>0,"[guessing]",""))

    def traceOut(self,rname,t):
        self.traceIndent()
        print("< " + rname + "(" +
              ifelse(t,str(t),"null") + ")" +
              ifelse(self.inputState.guessing>0,"[guessing]",""))
        self.traceDepth -= 1

    ### wh: moved from ASTFactory to TreeParser
    def addASTChild(self,currentAST, child):
        if not child:
            return
        if not currentAST.root:
            currentAST.root = child
        elif not currentAST.child:
            currentAST.root.setFirstChild(child)
        else:
            currentAST.child.setNextSibling(child)
        currentAST.child = child
        currentAST.advanceChildToEnd()

    ### wh: moved from ASTFactory to TreeParser
    def makeASTRoot(self,currentAST,root):
        if root:
            ### Add the current root as a child of new root
            root.addChild(currentAST.root)
            ### The new current child is the last sibling of the old root
            currentAST.child = currentAST.root
            currentAST.advanceChildToEnd()
            ### Set the new root
            currentAST.root = root

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###               funcs to work on trees                           ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

def rightmost(ast):
    if ast:
        while(ast.right):
            ast = ast.right
    return ast

def cmptree(s,t,partial):
    while(s and t):
        ### as a quick optimization, check roots first.
        if not s.equals(t):
            return False

        ### if roots match, do full list match test on children.
        if not cmptree(s.getFirstChild(),t.getFirstChild(),partial):
            return False

        s = s.getNextSibling()
        t = t.getNextSibling()

    r = ifelse(partial,not t,not s and not t)
    return r

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                          AST                                   ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class AST(object):
    def __init__(self):
        pass

    def addChild(self, c):
        pass

    def equals(self, t):
        return False

    def equalsList(self, t):
        return False

    def equalsListPartial(self, t):
        return False

    def equalsTree(self, t):
        return False

    def equalsTreePartial(self, t):
        return False

    def findAll(self, tree):
        return None

    def findAllPartial(self, subtree):
        return None

    def getFirstChild(self):
        return self

    def getNextSibling(self):
        return self

    def getText(self):
        return ""

    def getType(self):
        return INVALID_TYPE

    def getLine(self):
        return 0

    def getColumn(self):
        return 0

    def getNumberOfChildren(self):
        return 0

    def initialize(self, t, txt):
        pass

    def initialize(self, t):
        pass

    def setFirstChild(self, c):
        pass

    def setNextSibling(self, n):
        pass

    def setText(self, text):
        pass

    def setType(self, ttype):
        pass

    def toString(self):
        self.getText()

    __str__ = toString

    def toStringList(self):
        return self.getText()

    def toStringTree(self):
        return self.getText()

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       ASTNULLType                              ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

### There is only one instance of this class **/
class ASTNULLType(AST):
    def __init__(self):
        AST.__init__(self)
        pass

    def getText(self):
        return "<ASTNULL>"

    def getType(self):
        return NULL_TREE_LOOKAHEAD


###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       BaseAST                                  ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class BaseAST(AST):

    verboseStringConversion = False
    tokenNames = None

    def __init__(self):
        self.down  = None ## kid
        self.right = None ## sibling

    def addChild(self,node):
        if node:
            t = rightmost(self.down)
            if t:
                t.right = node
            else:
                assert not self.down
                self.down = node

    def getNumberOfChildren(self):
        t = self.down
        n = 0
        while t:
            n += 1
            t = t.right
        return n

    def doWorkForFindAll(self,v,target,partialMatch):
        sibling = self

        while sibling:
            c1 = partialMatch and sibling.equalsTreePartial(target)
            if c1:
                v.append(sibling)
            else:
                c2 = not partialMatch and sibling.equalsTree(target)
                if c2:
                    v.append(sibling)

            ### regardless of match or not, check any children for matches
            if sibling.getFirstChild():
                sibling.getFirstChild().doWorkForFindAll(v,target,partialMatch)

            sibling = sibling.getNextSibling()

    ### Is node t equal to 'self' in terms of token type and text?
    def equals(self,t):
        if not t:
            return False
        return self.getText() == t.getText() and self.getType() == t.getType()

    ### Is t an exact structural and equals() match of this tree.  The
    ### 'self' reference is considered the start of a sibling list.
    ###
    def equalsList(self, t):
        return cmptree(self, t, partial=False)

    ### Is 't' a subtree of this list?
    ### The siblings of the root are NOT ignored.
    ###
    def equalsListPartial(self,t):
        return cmptree(self,t,partial=True)

    ### Is tree rooted at 'self' equal to 't'?  The siblings
    ### of 'self' are ignored.
    ###
    def equalsTree(self, t):
        return self.equals(t) and \
               cmptree(self.getFirstChild(), t.getFirstChild(), partial=False)

    ### Is 't' a subtree of the tree rooted at 'self'?  The siblings
    ### of 'self' are ignored.
    ###
    def equalsTreePartial(self, t):
        if not t:
            return True
        return self.equals(t) and cmptree(
           self.getFirstChild(), t.getFirstChild(), partial=True)

    ### Walk the tree looking for all exact subtree matches.  Return
    ### an ASTEnumerator that lets the caller walk the list
    ### of subtree roots found herein.
    def findAll(self,target):
        roots = []

        ### the empty tree cannot result in an enumeration
        if not target:
            return None
        # find all matches recursively
        self.doWorkForFindAll(roots, target, False)
        return roots

    ### Walk the tree looking for all subtrees.  Return
    ###  an ASTEnumerator that lets the caller walk the list
    ###  of subtree roots found herein.
    def findAllPartial(self,sub):
        roots = []

        ### the empty tree cannot result in an enumeration
        if not sub:
            return None

        self.doWorkForFindAll(roots, sub, True)  ### find all matches recursively
        return roots

    ### Get the first child of this node None if not children
    def getFirstChild(self):
        return self.down

    ### Get the next sibling in line after this one
    def getNextSibling(self):
        return self.right

    ### Get the token text for this node
    def getText(self):
        return ""

    ### Get the token type for this node
    def getType(self):
        return 0

    def getLine(self):
        return 0

    def getColumn(self):
        return 0

    ### Remove all children */
    def removeChildren(self):
        self.down = None

    def setFirstChild(self,c):
        self.down = c

    def setNextSibling(self, n):
        self.right = n

    ### Set the token text for this node
    def setText(self, text):
        pass

    ### Set the token type for this node
    def setType(self, ttype):
        pass

    ### static
    def setVerboseStringConversion(verbose,names):
        verboseStringConversion = verbose
        tokenNames = names
    setVerboseStringConversion = staticmethod(setVerboseStringConversion)

    ### Return an array of strings that maps token ID to it's text.
    ##  @since 2.7.3
    def getTokenNames():
        return tokenNames

    def toString(self):
        return self.getText()

    ### return tree as lisp string - sibling included
    def toStringList(self):
        ts = self.toStringTree()
        sib = self.getNextSibling()
        if sib:
            ts += sib.toStringList()
        return ts

    __str__ = toStringList

    ### return tree as string - siblings ignored
    def toStringTree(self):
        ts = ""
        kid = self.getFirstChild()
        if kid:
            ts += " ("
        ts += " " + self.toString()
        if kid:
            ts += kid.toStringList()
            ts += " )"
        return ts

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       CommonAST                                ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

### Common AST node implementation
class CommonAST(BaseAST):
    def __init__(self,token=None):
        super(CommonAST,self).__init__()
        self.ttype = INVALID_TYPE
        self.text  = "<no text>"
        self.initialize(token)
        #assert self.text

    ### Get the token text for this node
    def getText(self):
        return self.text

    ### Get the token type for this node
    def getType(self):
        return self.ttype

    def initialize(self,*args):
        if not args:
            return

        arg0 = args[0]

        if isinstance(arg0,int):
            arg1 = args[1]
            self.setType(arg0)
            self.setText(arg1)
            return

        if isinstance(arg0,AST) or isinstance(arg0,Token):
            self.setText(arg0.getText())
            self.setType(arg0.getType())
            return

    ### Set the token text for this node
    def setText(self,text_):
        assert isinstance(text_,str)
        self.text = text_

    ### Set the token type for this node
    def setType(self,ttype_):
        assert isinstance(ttype_,int)
        self.ttype = ttype_

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                     CommonASTWithHiddenTokens                  ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class CommonASTWithHiddenTokens(CommonAST):

    def __init__(self,*args):
        CommonAST.__init__(self,*args)
        self.hiddenBefore = None
        self.hiddenAfter  = None

    def getHiddenAfter(self):
        return self.hiddenAfter

    def getHiddenBefore(self):
        return self.hiddenBefore

    def initialize(self,*args):
        CommonAST.initialize(self,*args)
        if args and isinstance(args[0],Token):
            assert isinstance(args[0],CommonHiddenStreamToken)
            self.hideenBefore = args[0].getHiddenBefore()
            self.hiddenAfter  = args[0].getHiddenAfter()

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       ASTPair                                  ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class ASTPair(object):
    def __init__(self):
        self.root = None          ### current root of tree
        self.child = None         ### current child to which siblings are added

    ### Make sure that child is the last sibling */
    def advanceChildToEnd(self):
        if self.child:
            while self.child.getNextSibling():
                self.child = self.child.getNextSibling()

    ### Copy an ASTPair.  Don't call it clone() because we want type-safety */
    def copy(self):
        tmp = ASTPair()
        tmp.root = self.root
        tmp.child = self.child
        return tmp

    def toString(self):
        r = ifelse(not root,"null",self.root.getText())
        c = ifelse(not child,"null",self.child.getText())
        return "[%s,%s]" % (r,c)

    __str__ = toString
    __repr__ = toString


###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       ASTFactory                               ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class ASTFactory(object):
    def __init__(self,table=None):
        self._class = None
        self._classmap = ifelse(table,table,None)

    def create(self,*args):
        if not args:
            return self.create(INVALID_TYPE)

        arg0 = args[0]
        arg1 = None
        arg2 = None

        try:
            arg1 = args[1]
            arg2 = args[2]
        except:
            pass

        # ctor(int)
        if isinstance(arg0,int) and not arg2:
            ### get class for 'self' type
            c = self.getASTNodeType(arg0)
            t = self.create(c)
            if t:
                t.initialize(arg0, ifelse(arg1,arg1,""))
            return t

        # ctor(int,something)
        if isinstance(arg0,int) and arg2:
            t = self.create(arg2)
            if t:
                t.initialize(arg0,arg1)
            return t

        # ctor(AST)
        if isinstance(arg0,AST):
            t = self.create(arg0.getType())
            if t:
                t.initialize(arg0)
            return t

        # ctor(token)
        if isinstance(arg0,Token) and not arg1:
            ttype = arg0.getType()
            assert isinstance(ttype,int)
            t = self.create(ttype)
            if t:
                t.initialize(arg0)
            return t

        # ctor(token,class)
        if isinstance(arg0,Token) and arg1:
            assert isinstance(arg1,type)
            assert issubclass(arg1,AST)
            # this creates instance of 'arg1' using 'arg0' as
            # argument. Wow, that's magic!
            t = arg1(arg0)
            assert t and isinstance(t,AST)
            return t

        # ctor(class)
        if isinstance(arg0,type):
            ### next statement creates instance of type (!)
            t = arg0()
            assert isinstance(t,AST)
            return t


    def setASTNodeClass(self,className=None):
        if not className:
            return
        assert isinstance(className,type)
        assert issubclass(className,AST)
        self._class = className

    ### kind of misnomer - use setASTNodeClass instead.
    setASTNodeType = setASTNodeClass

    def getASTNodeClass(self):
        return self._class



    def getTokenTypeToASTClassMap(self):
        return self._classmap

    def setTokenTypeToASTClassMap(self,amap):
        self._classmap = amap

    def error(self, e):
        import sys
        print >> sys.stderr, e

    def setTokenTypeASTNodeType(self, tokenType, className):
        """
        Specify a mapping between a token type and a (AST) class.
        """
        if not self._classmap:
            self._classmap = {}

        if not className:
            try:
                del self._classmap[tokenType]
            except:
                pass
        else:
            ### here we should also perform actions to ensure that
            ### a. class can be loaded
            ### b. class is a subclass of AST
            ###
            assert isinstance(className,type)
            assert issubclass(className,AST)  ## a & b
            ### enter the class
            self._classmap[tokenType] = className

    def getASTNodeType(self,tokenType):
        """
        For a given token type return the AST node type. First we
        lookup a mapping table, second we try _class
        and finally we resolve to "antlr.CommonAST".
        """

        # first
        if self._classmap:
            try:
                c = self._classmap[tokenType]
                if c:
                    return c
            except:
                pass
        # second
        if self._class:
            return self._class

        # default
        return CommonAST

    ### methods that have been moved to file scope - just listed
    ### here to be somewhat consistent with original API
    def dup(self,t):
        return antlr.dup(t,self)

    def dupList(self,t):
        return antlr.dupList(t,self)

    def dupTree(self,t):
        return antlr.dupTree(t,self)

    ### methods moved to other classes
    ### 1. makeASTRoot  -> Parser
    ### 2. addASTChild  -> Parser

    ### non-standard: create alias for longish method name
    maptype = setTokenTypeASTNodeType

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       ASTVisitor                               ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class ASTVisitor(object):
    def __init__(self,*args):
        pass

    def visit(self,ast):
        pass

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###               static methods and variables                     ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

ASTNULL = ASTNULLType()

### wh: moved from ASTFactory as there's nothing ASTFactory-specific
### in this method.
def make(*nodes):
    if not nodes:
        return None

    for i in xrange(0,len(nodes)):
        node = nodes[i]
        if node:
            assert isinstance(node,AST)

    root = nodes[0]
    tail = None
    if root:
        root.setFirstChild(None)

    for i in xrange(1,len(nodes)):
        if not nodes[i]:
            continue
        if not root:
            root = tail = nodes[i]
        elif not tail:
            root.setFirstChild(nodes[i])
            tail = root.getFirstChild()
        else:
            tail.setNextSibling(nodes[i])
            tail = tail.getNextSibling()

        ### Chase tail to last sibling
        while tail.getNextSibling():
            tail = tail.getNextSibling()
    return root

def dup(t,factory):
    if not t:
        return None

    if factory:
        dup_t = factory.create(t.__class__)
    else:
        raise TypeError("dup function requires ASTFactory argument")
    dup_t.initialize(t)
    return dup_t

def dupList(t,factory):
    result = dupTree(t,factory)
    nt = result
    while t:
        ## for each sibling of the root
        t = t.getNextSibling()
        nt.setNextSibling(dupTree(t,factory))
        nt = nt.getNextSibling()
    return result

def dupTree(t,factory):
    result = dup(t,factory)
    if t:
        result.setFirstChild(dupList(t.getFirstChild(),factory))
    return result

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
### $Id: antlr.py,v 1.1 2005/07/20 07:24:12 rvk Exp $

# Local Variables:    ***
# mode: python        ***
# py-indent-offset: 4 ***
# End:                ***

########NEW FILE########
__FILENAME__ = BIFFRecords
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
#
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
#
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
#
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
#
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
#
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.

__rev_id__ = """$Id: BIFFRecords.py,v 1.6 2005/08/11 08:53:48 rvk Exp $"""


from struct import pack
from UnicodeUtils import *
import sys

class SharedStringTable(object):
    _SST_ID = 0x00FC
    _CONTINUE_ID = 0x003C

    def __init__(self):
        self._sst_record = ''
        self._continues = []
        self._current_piece = pack('<II', 0, 0)
        self._pos = len(self._current_piece)

        self._str_indexes = {}
        self._add_calls = 0

    def add_str(self, s):
        self._add_calls += 1
        if s not in self._str_indexes:
            self._str_indexes[s] = len(self._str_indexes)
            self._add_to_sst(s)
        return self._str_indexes[s]

    def str_index(self, s):
        return self._str_indexes[s]

    def get_biff_record(self):
        self._new_piece()
        result = pack('<2HII', self._SST_ID, len(self._sst_record), self._add_calls, len(self._str_indexes))
        result += self._sst_record[8:]
        result += ''.join(self._continues)
        return result

    def _add_to_sst(self, s):
        u_str = upack2(s)
        if len(u_str) > 0xFFFF:
            raise Exception('error: very long string.')

        is_unicode_str = u_str[2] == '\x01'
        if is_unicode_str:
            atom_len = 5 # 2 byte -- len,
                         # 1 byte -- options,
                         # 2 byte -- 1st sym
        else:
            atom_len = 4 # 2 byte -- len,
                         # 1 byte -- options,
                         # 1 byte -- 1st sym

        self._save_atom(u_str[0:atom_len])
        self._save_splitted(u_str[atom_len:], is_unicode_str)

    def _new_piece(self):
        if self._sst_record == '':
            self._sst_record = self._current_piece
        else:
            curr_piece_len = len(self._current_piece)
            self._continues.append(pack('<2H%ds'%curr_piece_len, self._CONTINUE_ID, curr_piece_len, self._current_piece))
        self._current_piece = ''
        self._pos = len(self._current_piece)

    def _save_atom(self, s):
        atom_len = len(s)
        free_space = 0x2020 - len(self._current_piece)
        if free_space < atom_len:
            self._new_piece()
        self._current_piece += s

    def _save_splitted(self, s, is_unicode_str):
        i = 0
        str_len = len(s)
        while i < str_len:
            piece_len = len(self._current_piece)
            free_space = 0x2020 - piece_len
            tail_len = str_len - i
            need_more_space = free_space < tail_len

            if not need_more_space:
                atom_len = tail_len
            else:
                if is_unicode_str:
                    atom_len = free_space & 0xFFFE
                else:
                    atom_len = free_space

            self._current_piece += s[i:i+atom_len]

            if need_more_space:
                self._new_piece()
                if is_unicode_str:
                    self._current_piece += '\x01'
                else:
                    self._current_piece += '\x00'

            i += atom_len


class BiffRecord(object):
    def __init__(self):
        self._rec_data = ''

    def get_rec_id(self):
        return _REC_ID

    def get_rec_header(self):
        return pack('<2H', self._REC_ID, len(self._rec_data))

    def get_rec_data(self):
        return self._rec_data

    def get(self):
        data = self.get_rec_data()

        if len(data) > 0x2020: # limit for BIFF7/8
            chunks = []
            pos = 0
            while pos < len(data):
                chunk_pos = pos + 0x2020
                chunk = data[pos:chunk_pos]
                chunks.append(chunk)
                pos = chunk_pos
            continues = pack('<2H', self._REC_ID, len(chunks[0])) + chunks[0]
            for chunk in chunks[1:]:
                continues += pack('<2H%ds'%len(chunk), 0x003C, len(chunk), chunk)
                # 0x003C -- CONTINUE record id
            return continues
        else:
            return self.get_rec_header() + data


class Biff8BOFRecord(BiffRecord):
    """
    Offset Size Contents
    0      2    Version, contains 0600H for BIFF8 and BIFF8X
    2      2    Type of the following data:
                  0005H = Workbook globals
                  0006H = Visual Basic module
                  0010H = Worksheet
                  0020H = Chart
                  0040H = Macro sheet
                  0100H = Workspace file
    4      2    Build identifier
    6      2    Build year
    8      4    File history flags
    12     4    Lowest Excel version that can read all records in this file
    """
    _REC_ID      = 0x0809
    # stream types
    BOOK_GLOBAL = 0x0005
    VB_MODULE   = 0x0006
    WORKSHEET   = 0x0010
    CHART       = 0x0020
    MACROSHEET  = 0x0040
    WORKSPACE   = 0x0100

    def __init__(self, rec_type):
        BiffRecord.__init__(self)

        version  = 0x0600
        build    = 0x0DBB
        year     = 0x07CC
        file_hist_flags = 0x00L
        ver_can_read    = 0x06L

        self._rec_data = pack('<4H2I', version, rec_type, build, year, file_hist_flags, ver_can_read)


class InteraceHdrRecord(BiffRecord):
    _REC_ID = 0x00E1

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('BB', 0xB0, 0x04)


class InteraceEndRecord(BiffRecord):
    _REC_ID = 0x00E2

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = ''


class MMSRecord(BiffRecord):
    _REC_ID = 0x00C1

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', 0x00)


class WriteAccessRecord(BiffRecord):
    """
    This record is part of the file protection. It contains the name of the
    user  that  has  saved  the  file. The user name is always stored as an
    equal-sized  string.  All  unused  characters after the name are filled
    with space characters. It is not required to write the mentioned string
    length. Every other length will be accepted too.
    """
    _REC_ID = 0x005C

    def __init__(self, owner):
        BiffRecord.__init__(self)

        uowner = owner[0:0x30]
        uowner_len = len(uowner)
        self._rec_data = pack('%ds%ds' % (uowner_len, 0x70 - uowner_len), uowner, ' '*(0x70 - uowner_len))


class DSFRecord(BiffRecord):
    """
    This  record  specifies  if the file contains an additional BIFF5/BIFF7
    workbook stream.
    Record DSF, BIFF8:
    Offset Size Contents
    0        2     0 = Only the BIFF8 Workbook stream is present
                   1 = Additional BIFF5/BIFF7 Book stream is in the file
    A  double  stream file can be read by Excel 5.0 and Excel 95, and still
    contains  all  new  features  added to BIFF8 (which are left out in the
    BIFF5/BIFF7 Book stream).
    """
    _REC_ID = 0x0161

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', 0x00)


class TabIDRecord(BiffRecord):
    _REC_ID = 0x013D

    def __init__(self, sheetcount):
        BiffRecord.__init__(self)

        for i in range(sheetcount):
            self._rec_data += pack('<H', i+1)


class FnGroupCountRecord(BiffRecord):
    _REC_ID = 0x009C

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('BB', 0x0E, 0x00)


class WindowProtectRecord(BiffRecord):
    """
    This record is part of the worksheet/workbook protection. It determines
    whether  the window configuration of this document is protected. Window
    protection is not active, if this record is omitted.
    """
    _REC_ID = 0x0019

    def __init__(self, wndprotect):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', wndprotect)


class ObjectProtectRecord(BiffRecord):
    """
    This record is part of the worksheet/workbook protection.
    It determines whether the objects of the current sheet are protected.
    Object protection is not active, if this record is omitted.
    """
    _REC_ID = 0x0063


    def __init__(self, objprotect):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', objprotect)


class ScenProtectRecord(BiffRecord):
    """
    This record is part of the worksheet/workbook protection. It
    determines whether the scenarios of the current sheet are protected.
    Scenario protection is not active, if this record is omitted.
    """
    _REC_ID = 0x00DD


    def __init__(self, scenprotect):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', scenprotect)


class ProtectRecord(BiffRecord):
    """
    This  record is part of the worksheet/workbook protection. It specifies
    whether  a  worksheet  or a workbook is protected against modification.
    Protection is not active, if this record is omitted.
    """

    _REC_ID = 0x0012

    def __init__(self, protect):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', protect)


class PasswordRecord(BiffRecord):
    """
    This record is part of the worksheet/workbook protection. It
    stores a 16-bit hash value, calculated from the worksheet or workbook
    protection password.
    """
    _REC_ID = 0x0013
    def passwd_hash(self, plaintext):
        """
        Based on the algorithm provided by Daniel Rentz of OpenOffice.
        """
        if plaintext == "":
            return 0

        passwd_hash = 0x0000
        for i, char in enumerate(plaintext):
            c = ord(char) << (i + 1)
            low_15 = c & 0x7fff
            high_15 = c & 0x7fff << 15
            high_15 = high_15 >> 15
            c = low_15 | high_15
            passwd_hash ^= c
        passwd_hash ^= len(plaintext)
        passwd_hash ^= 0xCE4B
        return passwd_hash

    def __init__(self, passwd = ""):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', self.passwd_hash(passwd))


class Prot4RevRecord(BiffRecord):
    _REC_ID = 0x01AF

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', 0x00)


class Prot4RevPassRecord(BiffRecord):
    _REC_ID = 0x01BC

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', 0x00)


class BackupRecord(BiffRecord):
    """
    This  record  contains  a Boolean value determining whether Excel makes
    a backup of the file while saving.
    """
    _REC_ID = 0x0040

    def __init__(self, backup):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', backup)

class HideObjRecord(BiffRecord):
    """
    This record specifies whether and how to show objects in the workbook.

    Record HIDEOBJ, BIFF3-BIFF8:
    Offset  Size    Contents
    0       2       Viewing mode for objects:
                        0 = Show all objects
                        1 = Show placeholders
                        2 = Do not show objects
    """
    _REC_ID = 0x008D

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', 0x00)



class RefreshAllRecord(BiffRecord):
    """
    """

    _REC_ID = 0x01B7

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', 0x00)


class BookBoolRecord(BiffRecord):
    """
    This record contains a Boolean value determining whether to save values
    linked  from external workbooks (CRN records and XCT records). In BIFF3
    and BIFF4 this option is stored in the WSBOOL record.

    Record BOOKBOOL, BIFF5-BIFF8:

    Offset  Size    Contents
    0       2       0 = Save external linked values;
                    1 = Do not save external linked values
    """

    _REC_ID = 0x00DA

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', 0x00)


class CountryRecord(BiffRecord):
    """
    This   record   stores  two  Windows  country  identifiers.  The  first
    represents  the  user  interface language of the Excel version that has
    saved  the file, and the second represents the system regional settings
    at the time the file was saved.

    Record COUNTRY, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       Windows country identifier of the user interface language of Excel
    2       2       Windows country identifier of the system regional settings

    The  following  table  shows most of the used country identifiers. Most
    of  these  identifiers  are  equal to the international country calling
    codes.

    1   USA
    2   Canada
    7   Russia
    """

    _REC_ID = 0x00DA

    def __init__(self, ui_id, sys_settings_id):
        BiffRecord.__init__(self)

        self._rec_data = pack('<2H', ui_id, sys_settings_id)


class UseSelfsRecord(BiffRecord):
    """
    This  record  specifies if the formulas in the workbook can use natural
    language  formulas.  This  type  of  formula can refer to cells by its
    content or the content of the column or row header cell.

    Record USESELFS, BIFF8:

    Offset  Size    Contents
    0       2       0 = Do not use natural language formulas
                    1 = Use natural language formulas

    """

    _REC_ID = 0x0160

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', 0x01)


class EOFRecord(BiffRecord):
    _REC_ID = 0x000A

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = ''


class DateModeRecord(BiffRecord):
    """
    This  record  specifies  the  base date for displaying date values. All
    dates  are  stored as count of days past this base date. In BIFF2-BIFF4
    this   record  is  part  of  the  Calculation  Settings  Block.
    In BIFF5-BIFF8 it is stored in the Workbook Globals Substream.

    Record DATEMODE, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       0 = Base is 1899-Dec-31 (the cell = 1 represents 1900-Jan-01)
                    1 = Base is 1904-Jan-01 (the cell = 1 represents 1904-Jan-02)
    """
    _REC_ID = 0x0022

    def __init__(self, from1904):
        BiffRecord.__init__(self)

        if from1904:
            self._rec_data = pack('<H', 1)
        else:
            self._rec_data = pack('<H', 0)


class PrecisionRecord(BiffRecord):
    """
    This record stores if formulas use the real cell values for calculation
    or  the  values  displayed  on  the screen. In BIFF2- BIFF4 this record
    is  part of the Calculation Settings Block. In BIFF5-BIFF8 it is stored
    in the Workbook Globals Substream.

    Record PRECISION, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       0 = Use displayed values;
                    1 = Use real cell values
    """
    _REC_ID = 0x000E

    def __init__(self, use_real_values):
        BiffRecord.__init__(self)

        if use_real_values:
            self._rec_data = pack('<H', 1)
        else:
            self._rec_data = pack('<H', 0)


class CodepageBiff8Record(BiffRecord):
    """
    This record stores the text encoding used to write byte strings, stored
    as MS Windows code page identifier. The CODEPAGE record in BIFF8 always
    contains  the  code  page  1200  (UTF-16).  Therefore  it is not
    possible  to  obtain the encoding used for a protection password (it is
    not UTF-16).

    Record CODEPAGE, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       Code page identifier used for byte string text encoding:
                      016FH = 367 = ASCII
                      01B5H = 437 = IBM PC CP-437 (US)
                      02D0H = 720 = IBM PC CP-720 (OEM Arabic)
                      02E1H = 737 = IBM PC CP-737 (Greek)
                      0307H = 775 = IBM PC CP-775 (Baltic)
                      0352H = 850 = IBM PC CP-850 (Latin I)
                      0354H = 852 = IBM PC CP-852 (Latin II (Central European))
                      0357H = 855 = IBM PC CP-855 (Cyrillic)
                      0359H = 857 = IBM PC CP-857 (Turkish)
                      035AH = 858 = IBM PC CP-858 (Multilingual Latin I with Euro)
                      035CH = 860 = IBM PC CP-860 (Portuguese)
                      035DH = 861 = IBM PC CP-861 (Icelandic)
                      035EH = 862 = IBM PC CP-862 (Hebrew)
                      035FH = 863 = IBM PC CP-863 (Canadian (French))
                      0360H = 864 = IBM PC CP-864 (Arabic)
                      0361H = 865 = IBM PC CP-865 (Nordic)
                      0362H = 866 = IBM PC CP-866 (Cyrillic (Russian))
                      0365H = 869 = IBM PC CP-869 (Greek (Modern))
                      036AH = 874 = Windows CP-874 (Thai)
                      03A4H = 932 = Windows CP-932 (Japanese Shift-JIS)
                      03A8H = 936 = Windows CP-936 (Chinese Simplified GBK)
                      03B5H = 949 = Windows CP-949 (Korean (Wansung))
                      03B6H = 950 = Windows CP-950 (Chinese Traditional BIG5)
                      04B0H = 1200 = UTF-16 (BIFF8)
                      04E2H = 1250 = Windows CP-1250 (Latin II) (Central European)
                      04E3H = 1251 = Windows CP-1251 (Cyrillic)
                      04E4H = 1252 = Windows CP-1252 (Latin I) (BIFF4-BIFF7)
                      04E5H = 1253 = Windows CP-1253 (Greek)
                      04E6H = 1254 = Windows CP-1254 (Turkish)
                      04E7H = 1255 = Windows CP-1255 (Hebrew)
                      04E8H = 1256 = Windows CP-1256 (Arabic)
                      04E9H = 1257 = Windows CP-1257 (Baltic)
                      04EAH = 1258 = Windows CP-1258 (Vietnamese)
                      0551H = 1361 = Windows CP-1361 (Korean (Johab))
                      2710H = 10000 = Apple Roman
                      8000H = 32768 = Apple Roman
                      8001H = 32769 = Windows CP-1252 (Latin I) (BIFF2-BIFF3)
    """
    _REC_ID = 0x0042
    UTF_16 = 0x04B0

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', self.UTF_16)

class Window1Record(BiffRecord):
    """
    Offset Size Contents
    0      2    Horizontal position of the document window (in twips = 1/20 of a point)
    2      2    Vertical position of the document window (in twips = 1/20 of a point)
    4      2    Width of the document window (in twips = 1/20 of a point)
    6      2    Height of the document window (in twips = 1/20 of a point)
    8      2    Option flags:
                  Bits  Mask  Contents
                  0     0001H 0 = Window is visible 1 = Window is hidden
                  1     0002H 0 = Window is open 1 = Window is minimised
                  3     0008H 0 = Horizontal scroll bar hidden 1 = Horizontal scroll bar visible
                  4     0010H 0 = Vertical scroll bar hidden 1 = Vertical scroll bar visible
                  5     0020H 0 = Worksheet tab bar hidden 1 = Worksheet tab bar visible
    10     2    Index to active (displayed) worksheet
    12     2    Index of first visible tab in the worksheet tab bar
    14     2    Number of selected worksheets (highlighted in the worksheet tab bar)
    16     2    Width of worksheet tab bar (in 1/1000 of window width). The remaining space is used by the
                horizontal scrollbar.
    """
    _REC_ID = 0x003D
    # flags

    def __init__(self,
                 hpos_twips, vpos_twips,
                 width_twips, height_twips,
                 flags,
                 active_sheet,
                 first_tab_index, selected_tabs, tab_width):
        BiffRecord.__init__(self)

        self._rec_data = pack('<9H', hpos_twips, vpos_twips,
                                      width_twips, height_twips,
                                      flags,
                                      active_sheet,
                                      first_tab_index, selected_tabs, tab_width)

class FontRecord(BiffRecord):
    """
    WARNING
        The font with index 4 is omitted in all BIFF versions.
        This means the first four fonts have zero-based indexes, and
        the fifth font and all following fonts are referenced with one-based
        indexes.

    Offset Size Contents
    0      2    Height of the font (in twips = 1/20 of a point)
    2      2    Option flags:
                  Bit Mask    Contents
                  0   0001H   1 = Characters are bold (redundant, see below)
                  1   0002H   1 = Characters are italic
                  2   0004H   1 = Characters are underlined (redundant, see below)
                  3   0008H   1 = Characters are struck out
                        0010H 1 = Outline
                        0020H  1 = Shadow
    4     2     Colour index
    6     2     Font weight (100-1000).
                Standard values are 0190H (400) for normal text and 02BCH
                (700) for bold text.
    8     2     Escapement type:
                  0000H = None
                  0001H = Superscript
                  0002H = Subscript
    10    1     Underline type:
                  00H = None
                  01H = Single
                  21H = Single accounting
                  02H = Double
                  22H = Double accounting
    11    1     Font family:
                  00H = None (unknown or don't care)
                  01H = Roman (variable width, serifed)
                  02H = Swiss (variable width, sans-serifed)
                  03H = Modern (fixed width, serifed or sans-serifed)
                  04H = Script (cursive)
                  05H = Decorative (specialised, i.e. Old English, Fraktur)
    12    1     Character set:
                  00H = 0 = ANSI Latin
                  01H = 1 = System default
                  02H = 2 = Symbol
                  4DH = 77 = Apple Roman
                  80H = 128 = ANSI Japanese Shift-JIS
                  81H = 129 = ANSI Korean (Hangul)
                  82H = 130 = ANSI Korean (Johab)
                  86H = 134 = ANSI Chinese Simplified GBK
                  88H = 136 = ANSI Chinese Traditional BIG5
                  A1H = 161 = ANSI Greek
                  A2H = 162 = ANSI Turkish
                  A3H = 163 = ANSI Vietnamese
                  B1H = 177 = ANSI Hebrew
                  B2H = 178 = ANSI Arabic
                  BAH = 186 = ANSI Baltic
                  CCH = 204 = ANSI Cyrillic
                  DEH = 222 = ANSI Thai
                  EEH = 238 = ANSI Latin II (Central European)
                  FFH = 255 = OEM Latin I
    13    1     Not used
    14    var.  Font name:
                  BIFF5/BIFF7: Byte string, 8-bit string length
                  BIFF8: Unicode string, 8-bit string length
    The boldness and underline flags are still set in the options field,
    but not used on reading the font. Font weight and underline type
    are specified in separate fields instead.
    """
    _REC_ID = 0x0031

    def __init__(self,
                    height, options, colour_index, weight, escapement,
                    underline, family, charset,
                    name):
        BiffRecord.__init__(self)

        uname = upack1(name)
        uname_len = len(uname)

        self._rec_data = pack('<5H4B%ds' % uname_len, height, options, colour_index, weight, escapement,
                                                underline, family, charset, 0x00,
                                                uname)

class NumberFormatRecord(BiffRecord):
    """
    Record FORMAT, BIFF8:
    Offset  Size    Contents
    0       2       Format index used in other records
    2       var.    Number format string (Unicode string, 16-bit string length)

    From  BIFF5  on,  the built-in number formats will be omitted. The built-in
    formats  are  dependent  on  the current regional settings of the operating
    system.  The following table shows which number formats are used by default
    in  a  US-English  environment.  All indexes from 0 to 163 are reserved for
    built-in formats. The first user-defined format starts at 164.

    The built-in number formats, BIFF5-BIFF8

    Index   Type        Format string
    0       General     General
    1       Decimal     0
    2       Decimal     0.00
    3       Decimal     #,##0
    4       Decimal     #,##0.00
    5       Currency    "$"#,##0_);("$"#,##
    6       Currency    "$"#,##0_);[Red]("$"#,##
    7       Currency    "$"#,##0.00_);("$"#,##
    8       Currency    "$"#,##0.00_);[Red]("$"#,##
    9       Percent     0%
    10      Percent     0.00%
    11      Scientific  0.00E+00
    12      Fraction    # ?/?
    13      Fraction    # ??/??
    14      Date        M/D/YY
    15      Date        D-MMM-YY
    16      Date        D-MMM
    17      Date        MMM-YY
    18      Time        h:mm AM/PM
    19      Time        h:mm:ss AM/PM
    20      Time        h:mm
    21      Time        h:mm:ss
    22      Date/Time   M/D/YY h:mm
    37      Account     _(#,##0_);(#,##0)
    38      Account     _(#,##0_);[Red](#,##0)
    39      Account     _(#,##0.00_);(#,##0.00)
    40      Account     _(#,##0.00_);[Red](#,##0.00)
    41      Currency    _("$"* #,##0_);_("$"* (#,##0);_("$"* "-"_);_(@_)
    42      Currency    _(* #,##0_);_(* (#,##0);_(* "-"_);_(@_)
    43      Currency    _("$"* #,##0.00_);_("$"* (#,##0.00);_("$"* "-"??_);_(@_)
    44      Currency    _(* #,##0.00_);_(* (#,##0.00);_(* "-"??_);_(@_)
    45      Time        mm:ss
    46      Time        [h]:mm:ss
    47      Time        mm:ss.0
    48      Scientific  ##0.0E+0
    49      Text        @
    """
    _REC_ID = 0x041E

    def __init__(self, idx, fmtstr):
        BiffRecord.__init__(self)

        ufmtstr = upack2(fmtstr)
        ufmtstr_len = len(ufmtstr)

        self._rec_data = pack('<H%ds' % ufmtstr_len, idx, ufmtstr)


class XFRecord(BiffRecord):
    """
    XF Substructures
    -------------------------------------------------------------------------
    XF_TYPE_PROT  XF Type and Cell Protection (3 Bits), BIFF3-BIFF8
    These 3 bits are part of a specific data byte.
    Bit Mask    Contents
    0   01H     1 = Cell is locked
    1   02H     1 = Formula is hidden
    2   04H     0 = Cell XF; 1 = Style XF

    XF_USED_ATTRIB   Attributes   Used  from  Parent  Style  XF  (6  Bits),
    BIFF3-BIFF8  Each  bit  describes  the  validity  of  a  specific group
    of  attributes.  In  cell XFs a cleared bit means the attributes of the
    parent  style XF are used (but only if the attributes are valid there),
    a  set  bit  means  the  attributes  of  this XF are used. In style XFs
    a cleared bit means the attribute setting is valid, a set bit means the
    attribute should be ignored.
    Bit Mask    Contents
    0   01H     Flag for number format
    1   02H     Flag for font
    2   04H     Flag for horizontal and vertical alignment, text wrap, indentation, orientation, rotation, and
                text direction
    3   08H     Flag for border lines
    4   10H     Flag for background area style
    5   20H     Flag for cell protection (cell locked and formula hidden)

    XF_HOR_ALIGN  Horizontal Alignment (3 Bits), BIFF2-BIFF8 The horizontal
    alignment consists of 3 bits and is part of a specific data byte.
    Value   Horizontal alignment
    00H     General
    01H     Left
    02H     Centred
    03H     Right
    04H     Filled
    05H     Justified (BIFF4-BIFF8X)
    06H     Centred across selection (BIFF4-BIFF8X)
    07H     Distributed (BIFF8X)

    XF_VERT_ALIGN Vertical Alignment (2 or 3 Bits), BIFF4-BIFF8
    The vertical alignment consists of 2 bits (BIFF4) or 3 bits (BIFF5-BIFF8)
    and is part of a specific data byte. Vertical alignment is not available
    in BIFF2 and BIFF3.
    Value   Vertical alignment
    00H     Top
    01H     Centred
    02H     Bottom
    03H     Justified (BIFF5-BIFF8X)
    04H     Distributed (BIFF8X)

    XF_ORIENTATION  Text  Orientation  (2  Bits),  BIFF4-BIFF7  In the BIFF
    versions  BIFF4-BIFF7,  text  can  be  rotated  in  steps of 90 degrees
    or  stacked.  The  orientation  mode  consists of 2 bits and is part of
    a specific data byte. In BIFF8 a rotation angle occurs instead of these
    flags.
    Value   Text orientation
    00H     Not rotated
    01H     Letters are stacked top-to-bottom, but not rotated
    02H     Text is rotated 90 degrees counterclockwise
    03H     Text is rotated 90 degrees clockwise

    XF_ROTATION Text Rotation Angle (1 Byte), BIFF8
    Value   Text rotation
    0       Not rotated
    1-90    1 to 90 degrees counterclockwise
    91-180  1 to 90 degrees clockwise
    255     Letters are stacked top-to-bottom, but not rotated

    XF_BORDER_34  Cell  Border  Style  (4  Bytes), BIFF3-BIFF4 Cell borders
    contain a line style and a line colour for each line of the border.
    Bit     Mask        Contents
    2-0     00000007H   Top line style
    7-3     000000F8H   Colour index for top line colour
    10-8    00000700H   Left line style
    15-11   0000F800H   Colour index for left line colour
    18-16   00070000H   Bottom line style
    23-19   00F80000H   Colour index for bottom line colour
    26-24   07000000H   Right line style
    31-27   F8000000H   Colour index for right line colour

    XF_AREA_34  Cell  Background  Area  Style (2 Bytes), BIFF3-BIFF4 A cell
    background  area  style  contains  an area pattern and a foreground and
    background colour.
    Bit     Mask    Contents
    5-0     003FH   Fill pattern
    10-6    07C0H   Colour index for pattern colour
    15-11   F800H   Colour index for pattern background
 ---------------------------------------------------------------------------------------------
    Record XF, BIFF8:
    Offset      Size    Contents
    0           2       Index to FONT record
    2           2       Index to FORMAT record
    4           2       Bit     Mask    Contents
                        2-0     0007H   XF_TYPE_PROT . XF type, cell protection (see above)
                        15-4    FFF0H   Index to parent style XF (always FFFH in style XFs)
    6           1       Bit     Mask    Contents
                        2-0     07H     XF_HOR_ALIGN . Horizontal alignment (see above)
                        3       08H     1 = Text is wrapped at right border
                        6-4     70H     XF_VERT_ALIGN . Vertical alignment (see above)
    7           1       XF_ROTATION: Text rotation angle (see above)
    8           1       Bit     Mask    Contents
                        3-0     0FH     Indent level
                        4       10H     1 = Shrink content to fit into cell
                        5               merge
                        7-6     C0H     Text direction (BIFF8X only)
                                        00b = According to context
                                        01b = Left-to-right
                                        10b = Right-to-left
    9           1       Bit     Mask    Contents
                        7-2     FCH     XF_USED_ATTRIB . Used attributes (see above)
    10          4       Cell border lines and background area:
                        Bit     Mask      Contents
                        3-0     0000000FH Left line style
                        7-4     000000F0H Right line style
                        11-8    00000F00H Top line style
                        15-12   0000F000H Bottom line style
                        22-16   007F0000H Colour index for left line colour
                        29-23   3F800000H Colour index for right line colour
                        30      40000000H 1 = Diagonal line from top left to right bottom
                        31      80000000H 1 = Diagonal line from bottom left to right top
    14          4       Bit     Mask      Contents
                        6-0     0000007FH Colour index for top line colour
                        13-7    00003F80H Colour index for bottom line colour
                        20-14   001FC000H Colour index for diagonal line colour
                        24-21   01E00000H Diagonal line style
                        31-26   FC000000H Fill pattern
    18          2       Bit     Mask    Contents
                        6-0     007FH   Colour index for pattern colour
                        13-7    3F80H   Colour index for pattern background

    """
    _REC_ID = 0x00E0

    def __init__(self, xf, xftype='cell'):
        BiffRecord.__init__(self)

        font_xf_idx, fmt_str_xf_idx, alignment, borders, pattern, protection = xf
        fnt = struct.pack('<H', font_xf_idx)
        fmt = struct.pack('<H', fmt_str_xf_idx)
        if xftype == 'cell':
            prt = struct.pack('<H',
                ((protection.cell_locked    & 0x01) << 0) |
                ((protection.formula_hidden & 0x01) << 1)
            )
        else:
            prt = struct.pack('<H', 0xFFF5)
        aln = struct.pack('B',
            ((alignment.horz & 0x07) << 0) |
            ((alignment.wrap & 0x01) << 3) |
            ((alignment.vert & 0x07) << 4)
        )
        rot = struct.pack('B', alignment.rota)
        txt = struct.pack('B',
            ((alignment.inde & 0x0F) << 0) |
            ((alignment.shri & 0x01) << 4) |
            ((alignment.merg & 0x01) << 5) |
            ((alignment.dire & 0x03) << 6)
        )
        if xftype == 'cell':
            used_attr = struct.pack('B', 0xF8)
        else:
            used_attr = struct.pack('B', 0xF4)

        if borders.left == borders.NO_LINE:
            borders.left_colour = 0x00
        if borders.right == borders.NO_LINE:
            borders.right_colour = 0x00
        if borders.top == borders.NO_LINE:
            borders.top_colour = 0x00
        if borders.bottom == borders.NO_LINE:
            borders.bottom_colour = 0x00
        if borders.diag == borders.NO_LINE:
            borders.diag_colour = 0x00
        brd1 = struct.pack('<L',
            ((borders.left          & 0x0F) << 0 ) |
            ((borders.right         & 0x0F) << 4 ) |
            ((borders.top           & 0x0F) << 8 ) |
            ((borders.bottom        & 0x0F) << 12) |
            ((borders.left_colour   & 0x7F) << 16) |
            ((borders.right_colour  & 0x7F) << 23) |
            ((borders.need_diag1    & 0x01) << 30) |
            ((borders.need_diag2    & 0x01) << 31)
        )
        brd2 = struct.pack('<L',
            ((borders.top_colour    & 0x7F) << 0 ) |
            ((borders.bottom_colour & 0x7F) << 7 ) |
            ((borders.diag_colour   & 0x7F) << 14) |
            ((borders.diag          & 0x0F) << 21) |
            ((pattern.pattern       & 0x0F) << 26)
        )
        pat = struct.pack('<H',
            ((pattern.pattern_fore_colour & 0x7F) << 0 ) |
            ((pattern.pattern_back_colour & 0x7F) << 7 )
        )
        self._rec_data = fnt + fmt + prt + \
                        aln + rot + txt + used_attr + \
                        brd1 + brd2 + \
                        pat

class StyleRecord(BiffRecord):
    """
    STYLE record for user-defined cell styles, BIFF3-BIFF8:
    Offset  Size    Contents
    0       2       Bit     Mask    Contents
                    11-0    0FFFH   Index to style XF record
                    15      8000H   Always 0 for user-defined styles
    2       var.    BIFF2-BIFF7: Non-empty byte string, 8-bit string length
                    BIFF8: Non-empty Unicode string, 16-bit string length
    STYLE record for built-in cell styles, BIFF3-BIFF8:
    Offset  Size    Contents
    0       2       Bit     Mask    Contents
                    11-0    0FFFH   Index to style XF record
                    15      8000H   Always 1 for built-in styles
    2       1       Identifier of the built-in cell style:
                        00H = Normal
                        01H = RowLevel_lv (see next field)
                        02H = ColLevel_lv (see next field)
                        03H = Comma
                        04H = Currency
                        05H = Percent
                        06H = Comma [0] (BIFF4-BIFF8)
                        07H = Currency [0] (BIFF4-BIFF8)
                        08H = Hyperlink (BIFF8)
                        09H = Followed Hyperlink (BIFF8)
    3       1       Level for RowLevel or ColLevel style
                    (zero-based, lv), FFH otherwise
    The  RowLevel  and  ColLevel  styles specify the formatting of subtotal
    cells  in  a specific outline level. The level is specified by the last
    field  in the STYLE record. Valid values are 0-6 for the outline levels
    1-7.
    """
    _REC_ID = 0x0293

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = struct.pack('<HBB', 0x8000, 0x00, 0xFF)
        # TODO: implement user-defined styles???


class PaletteRecord(BiffRecord):
    """
    This  record  contains  the  definition  of  all  user-defined  colours
    available for cell and object formatting.

    Record PALETTE, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       Number of following colours (nm). Contains 16 in BIFF3-BIFF4 and 56 in BIFF5-BIFF8.
    2       4*nm    List of nm RGB colours

    The following table shows how colour indexes are used in other records:

    Colour index    Resulting colour or internal list index
    00H             Built-in Black (R = 00H, G = 00H, B = 00H)
    01H             Built-in White (R = FFH, G = FFH, B = FFH)
    02H             Built-in Red (R = FFH, G = 00H, B = 00H)
    03H             Built-in Green (R = 00H, G = FFH, B = 00H)
    04H             Built-in Blue (R = 00H, G = 00H, B = FFH)
    05H             Built-in Yellow (R = FFH, G = FFH, B = 00H)
    06H             Built-in Magenta (R = FFH, G = 00H, B = FFH)
    07H             Built-in Cyan (R = 00H, G = FFH, B = FFH)
    08H             First user-defined colour from the PALETTE record (entry 0 from record colour list)
    .........................

    17H (BIFF3-BIFF4) Last user-defined colour from the PALETTE record (entry 15 or 55 from record colour list)
    3FH (BIFF5-BIFF8)

    18H (BIFF3-BIFF4) System window text colour for border lines (used in records XF, CF, and
    40H (BIFF5-BIFF8) WINDOW2 (BIFF8 only))

    19H (BIFF3-BIFF4) System window background colour for pattern background (used in records XF, and CF)
    41H (BIFF5-BIFF8)

    43H             System face colour (dialogue background colour)
    4DH             System window text colour for chart border lines
    4EH             System window background colour for chart areas
    4FH             Automatic colour for chart border lines (seems to be always Black)
    50H             System ToolTip background colour (used in note objects)
    51H             System ToolTip text colour (used in note objects)
    7FFFH           System window text colour for fonts (used in records FONT, EFONT, and CF)

    """
    _REC_ID = 0x0092


class BoundSheetRecord(BiffRecord):
    """
    This  record  is  located  in  the workbook globals area and represents
    a  sheet  inside  of  the  workbook. For each sheet a BOUNDSHEET record
    is  written.  It  stores  the sheet name and a stream offset to the BOF
    record    within   the   workbook   stream.  The  record  is also known
    as BUNDLESHEET.

    Record BOUNDSHEET, BIFF5-BIFF8:
    Offset  Size    Contents
    0       4       Absolute stream position of the BOF record of the sheet represented by this record. This
                    field is never encrypted in protected files.
    4       1       Visibility:
                        00H = Visible
                        01H = Hidden
                        02H = Strong hidden
    5       1       Sheet type:
                        00H = Worksheet
                        02H = Chart
                        06H = Visual Basic module
    6       var.    Sheet name:
                        BIFF5/BIFF7: Byte string, 8-bit string length
                        BIFF8: Unicode string, 8-bit string length
    """
    _REC_ID = 0x0085

    def __init__(self, stream_pos, visibility, sheetname):
        BiffRecord.__init__(self)

        usheetname = upack1(sheetname)
        uusheetname_len = len(usheetname)

        self._rec_data = pack('<LBB%ds' % uusheetname_len, stream_pos, visibility, 0x00, usheetname)


class ContinueRecord(BiffRecord):
    """
    Whenever  the content of a record exceeds the given limits (see table),
    the  record  must  be  split.  Several  CONTINUE records containing the
    additional data are added after the parent record.

    BIFF version    Maximum data size of a record
    BIFF2-BIFF7     2080 bytes (2084 bytes including record header)
    BIFF8           8224 bytes (8228 bytes including record header) (0x2020)

    Record CONTINUE, BIFF2-BIFF8:
    Offset  Size    Contents
    0       var.    Data continuation of the previous record

    Unicode  strings  are  split in a special way. At the beginning of each
    CONTINUE  record  the option flags byte is repeated. Only the character
    size  flag  will  be set in this flags byte, the Rich-Text flag and the
    Far-East  flag  are set to zero. In each CONTINUE record it is possible
    that  the  character  size  changes  from  8-bit  characters  to 16-bit
    characters and vice versa.

    Never  a  Unicode  string  is  split  until  and  including  the  first
    character.  That means, all header fields (string length, option flags,
    optional Rich-Text size, and optional Far-East data size) and the first
    character  of  the string have to occur together in the leading record,
    or  have  to  be  moved completely into the CONTINUE record. Formatting
    runs cannot be split between their components (character index and FONT
    record  index).  If  a string is split between two formatting runs, the
    option flags field will not be repeated in the CONTINUE record.
    """
    _REC_ID = 0x003C


class SSTRecord(BiffRecord):
    """
    This  record  contains  a  list  of  all  strings  used anywhere in the
    workbook.  Each string occurs only once. The workbook uses indexes into
    the list to reference the strings.

    Record SST, BIFF8:
    Offset  Size    Contents
    0       4       Total number of strings in the workbook (see below)
    4       4       Number of following strings (nm)
    8       var.    List of nm Unicode strings, 16-bit string length

    The  first  field  of  the  SST  record  counts  the  total  occurrence
    of  strings  in  the  workbook.  For  instance,  the string AAA is used
    3  times  and  the string BBB is used 2 times. The first field contains
    5 and the second field contains 2, followed by the two strings.
    """
    _REC_ID = 0x00FC


class ExtSSTRecord(BiffRecord):
    """
    This  record  occurs  in  conjunction  with  the SST record. It is used
    by  Excel  to create a hash table with stream offsets to the SST record
    to optimise string search operations. Excel may not shorten this record
    if  strings  are deleted from the shared string table, so the last part
    might  contain  invalid  data. The stream indexes in this record divide
    the SST into portions containing a constant number of strings.

    Record EXTSST, BIFF8:

    Offset  Size    Contents
    0       2       Number of strings in a portion, this number is >=8
    2       var.    List of OFFSET structures for all portions. Each OFFSET contains the following data:
                        Offset Size Contents
                        0       4   Absolute stream position of first string of the portion
                        4       2   Position of first string of the portion inside of current record,
                                    including record header. This counter restarts at zero, if the SST
                                    record is continued with a CONTINUE record.
                        6       2   Not used
    """
    _REC_ID = 0x00FF

    def __init__(self, sst_stream_pos, str_placement, portions_len):
        BiffRecord.__init__(self)

        extsst = {}
        abs_stream_pos = sst_stream_pos
        str_counter = 0
        portion_counter = 0
        while str_counter < len(str_placement):
            str_chunk_num, pos_in_chunk = str_placement[str_counter]
            if str_chunk_num <> portion_counter:
                portion_counter = str_chunk_num
                abs_stream_pos += portions_len[portion_counter-1]
                #print hex(abs_stream_pos)
            str_stream_pos = abs_stream_pos + pos_in_chunk + 4 # header
            extsst[str_counter] = (pos_in_chunk, str_stream_pos)
            str_counter += 1

        exsst_str_count_delta = max(8, len(str_placement)*8/0x2000) # maybe smth else?
        self._rec_data = pack('<H', exsst_str_count_delta)
        str_counter = 0
        while str_counter < len(str_placement):
            self._rec_data += pack('<IHH', extsst[str_counter][1], extsst[str_counter][0], 0)
            str_counter += exsst_str_count_delta

class DimensionsRecord(BiffRecord):
    """
    Record DIMENSIONS, BIFF8:

    Offset  Size    Contents
    0       4       Index to first used row
    4       4       Index to last used row, increased by 1
    8       2       Index to first used column
    10      2       Index to last used column, increased by 1
    12      2       Not used
    """
    _REC_ID = 0x0200
    def __init__(self, first_used_row, last_used_row, first_used_col, last_used_col):
        BiffRecord.__init__(self)

        self._rec_data = struct.pack('<2L3H',
                                            first_used_row, last_used_row + 1,
                                            first_used_col, last_used_col + 1,
                                            0x00)


class Window2Record(BiffRecord):
    """
    Record WINDOW2, BIFF8:

    Offset  Size Contents
    0       2 Option flags (see below)
    2       2 Index to first visible row
    4       2 Index to first visible column
    6       2 Colour index of grid line colour. Note that in BIFF2-BIFF7 an RGB colour is
                written instead.
    8       2 Not used
    10      2 Cached magnification factor in page break preview (in percent); 0 = Default (60%)
    12      2 Cached magnification factor in normal view (in percent); 0 = Default (100%)
    14      4 Not used

    In  BIFF8  this record stores used magnification factors for page break
    preview  and  normal  view.  These  values  are  used  to  restore  the
    magnification,  when the view is changed. The real magnification of the
    currently  active  view  is  stored  in the SCL record. The type of the
    active view is stored in the option flags field (see below).

     0 0001H 0 = Show formula results 1 = Show formulas
     1 0002H 0 = Do not show grid lines 1 = Show grid lines
     2 0004H 0 = Do not show sheet headers 1 = Show sheet headers
     3 0008H 0 = Panes are not frozen 1 = Panes are frozen (freeze)
     4 0010H 0 = Show zero values as empty cells 1 = Show zero values
     5 0020H 0 = Manual grid line colour 1 = Automatic grid line colour
     6 0040H 0 = Columns from left to right 1 = Columns from right to left
     7 0080H 0 = Do not show outline symbols 1 = Show outline symbols
     8 0100H 0 = Keep splits if pane freeze is removed 1 = Remove splits if pane freeze is removed
     9 0200H 0 = Sheet not selected 1 = Sheet selected (BIFF5-BIFF8)
    10 0400H 0 = Sheet not visible 1 = Sheet visible (BIFF5-BIFF8)
    11 0800H 0 = Show in normal view 1 = Show in page break preview (BIFF8)

    The freeze flag specifies, if a following PANE record describes unfrozen or frozen panes.
    """
    _REC_ID = 0x023E

    def __init__(self, options, first_visible_row, first_visible_col, grid_colour, preview_magn, normal_magn):
        BiffRecord.__init__(self)

        self._rec_data = struct.pack('<7HL', options,
                                    first_visible_row, first_visible_col,
                                    grid_colour,
                                    0x00,
                                    preview_magn, normal_magn,
                                    0x00L)


class PanesRecord(BiffRecord):
    """
    This record stores the position of window panes. It is part of the Sheet
    View Settings Block. If the sheet does not contain any splits, this
    record will not occur.
    A sheet can be split in two different ways, with unfrozen panes or with
    frozen panes. A flag in the WINDOW2 record specifies, if the panes are
    frozen, which affects the contents of this record.

    Record PANE, BIFF2-BIFF8:
    Offset      Size        Contents
    0           2           Position of the vertical split
                            (px, 0 = No vertical split):
                            Unfrozen pane: Width of the left pane(s)
                            (in twips = 1/20 of a point)
                            Frozen pane: Number of visible
                            columns in left pane(s)
    2           2           Position of the horizontal split
                            (py, 0 = No horizontal split):
                            Unfrozen pane: Height of the top pane(s)
                            (in twips = 1/20 of a point)
                            Frozen pane: Number of visible
                            rows in top pane(s)
    4           2           Index to first visible row
                            in bottom pane(s)
    6           2           Index to first visible column
                            in right pane(s)
    8           1           Identifier of pane with active
                            cell cursor
    [9]         1           Not used (BIFF5-BIFF8 only, not written
                            in BIFF2-BIFF4)

    If the panes are frozen, pane0 is always active, regardless
    of the cursor position. The correct identifiers for all possible
    combinations of visible panes are shown in the following pictures.

    px = 0, py = 0                  px = 0, py > 0
    --------------------------      ------------|-------------
    |                        |      |                        |
    |                        |      |           3            |
    |                        |      |                        |
    -           3            -      --------------------------
    |                        |      |                        |
    |                        |      |           2            |
    |                        |      |                        |
    --------------------------      ------------|-------------

    px > 0, py = 0                  px > 0, py > 0
    ------------|-------------      ------------|-------------
    |           |            |      |           |            |
    |           |            |      |     3     |      2     |
    |           |            |      |           |            |
    -     3     |      1     -      --------------------------
    |           |            |      |           |            |
    |           |            |      |     1     |      0     |
    |           |            |      |           |            |
    ------------|-------------      ------------|-------------
    """
    _REC_ID = 0x0041
    def __init__(self, px, py, first_row_bottom, first_col_right, active_pane):
        BiffRecord.__init__(self)

        self._rec_data = struct.pack('<5H',
                                            px, py,
                                            first_row_bottom, first_col_right,
                                            active_pane)


class RowRecord(BiffRecord):
    """
    This  record  contains  the properties of a single row in a sheet. Rows
    and cells in a sheet are divided into blocks of 32 rows.

    Record ROW, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       Index of this row
    2       2       Index to column of the first cell which is described by a cell record
    4       2       Index to column of the last cell which is described by a cell record,
                    increased by 1
    6       2       Bit     Mask    Contents
                    14-0    7FFFH   Height of the row, in twips = 1/20 of a point
                    15      8000H   0 = Row has custom height; 1 = Row has default height
    8       2       Not used
    10      2       In BIFF3-BIFF4 this field contains a relative offset
                    to calculate stream position of the first cell record
                    for this row. In BIFF5-BIFF8 this field is not used
                    anymore, but the DBCELL record instead.
    12      4       Option flags and default row formatting:
                    Bit     Mask        Contents
                    2-0     00000007H   Outline level of the row
                    4       00000010H   1 = Outline group starts or ends here (depending
                                        on where the outline buttons are located,
                                        see WSBOOL record), and is collapsed
                    5       00000020H   1 = Row is hidden (manually, or by a filter or outline group)
                    6       00000040H   1 = Row height and default font height do not match
                    7       00000080H   1 = Row has explicit default format (fl)
                    8       00000100H   Always 1
                    27-16   0FFF0000H   If fl=1: Index to default XF record
                    28      10000000H   1 = Additional space above the row. This flag is set,
                                        if the upper border of at least one cell in this row
                                        or if the lower border of at least one cell in the row
                                        above is formatted with a thick line style.
                                        Thin and medium line styles are not taken into account.
                    29      20000000H   1 = Additional space below the row. This flag is set,
                                        if the lower border of at least one cell in this row
                                        or if the upper border of at least one cell in the row
                                        below is formatted with a medium or thick line style.
                                        Thin line styles are not taken into account.
    """

    _REC_ID = 0x0208

    def __init__(self, index, first_col, last_col, height_options, options):
        BiffRecord.__init__(self)

        self._rec_data = struct.pack('<6HL', index, first_col, last_col + 1,
                                        height_options,
                                        0x00, 0x00,
                                        options)

class LabelSSTRecord(BiffRecord):
    """
    This record represents a cell that contains a string. It replaces the
    LABEL record and RSTRING record used in BIFF2-BIFF7.
    """
    _REC_ID = 0x00FD

    def __init__(self, row, col, xf_idx, sst_idx):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<3HL', row, col, xf_idx, sst_idx)


class MergedCellsRecord(BiffRecord):
    """
    This record contains all merged cell ranges of the current sheet.

    Record MERGEDCELLS, BIFF8:

    Offset  Size    Contents
    0       var.    Cell range address list with all merged ranges

    ------------------------------------------------------------------

    A cell range address list consists of a field with the number of ranges
    and the list of the range addresses.

    Cell range address list, BIFF8:

    Offset  Size            Contents
    0       2               Number of following cell range addresses (nm)
    2       8*nm            List of nm cell range addresses

    ---------------------------------------------------------------------
    Cell range address, BIFF8:

    Offset  Size    Contents
    0       2       Index to first row
    2       2       Index to last row
    4       2       Index to first column
    6       2       Index to last column

    """
    _REC_ID = 0x00E5

    def __init__(self, merged_list):
        BiffRecord.__init__(self)

        i = len(merged_list) - 1
        while i >= 0:
            j = 0
            merged = ''
            while (i >= 0) and (j < 0x403):
                r1, r2, c1, c2 = merged_list[i]
                merged += struct.pack('<4H', r1, r2, c1, c2)
                i -= 1
                j += 1
            self._rec_data += struct.pack('<3H', self._REC_ID, len(merged) + 2, j) + \
                                    merged

    # for some reason Excel doesn't use CONTINUE
    def get(self):
        return self._rec_data

class MulBlankRecord(BiffRecord):
    """
    This  record  represents  a  cell  range  of empty cells. All cells are
    located in the same row.

    Record MULBLANK, BIFF5-BIFF8:

    Offset  Size    Contents
    0       2       Index to row
    2       2       Index to first column (fc)
    4       2*nc    List of nc=lc-fc+1 16-bit indexes to XF records
    4+2*nc  2       Index to last column (lc)
    """
    _REC_ID = 0x00BE

    def __init__(self, row, first_col, last_col, xf_index):
        BiffRecord.__init__(self)
        blanks_count = last_col-first_col+1
        self._rec_data = struct.pack('%dH' % blanks_count, *([xf_index]*blanks_count))
        self._rec_data = struct.pack('<2H', row, first_col) +  self._rec_data + struct.pack('<H',  last_col)


class BlankRecord(BiffRecord):
    """
    This  record  represents  an empty cell.

    Record BLANK, BIFF5-BIFF8:

    Offset  Size    Contents
    0       2       Index to row
    2       2       Index to first column (fc)
    4       2       indexes to XF record
    """
    _REC_ID = 0x0201

    def __init__(self, row, col, xf_index):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<3H', row, col, xf_index)


class RKRecord(BiffRecord):
    """
    This record represents a cell that contains an RK value (encoded integer or
    floating-point value). If a floating-point value cannot be encoded to an RK value,
    a NUMBER record will be written.
    """
    _REC_ID = 0x027E

    def __init__(self, row, col, xf_index, rk_encoded):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<3HI', row, col, xf_index, rk_encoded)


class NumberRecord(BiffRecord):
    """
    This record represents a cell that contains an IEEE-754 floating-point value.
    """
    _REC_ID = 0x0203

    def __init__(self, row, col, xf_index, number):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<3Hd', row, col, xf_index, number)


class FormulaRecord(BiffRecord):
    """
    Offset Size Contents
    0      2    Index to row
    2      2    Index to column
    4      2    Index to XF record
    6      8    Result of the formula
    14     2    Option flags:
                Bit Mask    Contents
                0   0001H   1 = Recalculate always
                1   0002H   1 = Calculate on open
                3   0008H   1 = Part of a shared formula
    16     4    Not used
    20     var. Formula data (RPN token array)

    """
    _REC_ID = 0x0006

    def __init__(self, row, col, xf_index, rpn):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<3HQHL', row, col, xf_index, 0xFFFF000000000003, 0, 0) + rpn


class GutsRecord(BiffRecord):
    """
    This record contains information about the layout of outline symbols.

    Record GUTS, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       Width of the area to display row outlines (left of the sheet), in pixel
    2       2       Height of the area to display column outlines (above the sheet), in pixel
    4       2       Number of visible row outline levels (used row levels + 1; or 0, if not used)
    6       2       Number of visible column outline levels (used column levels + 1; or 0, if not used)

    """

    _REC_ID = 0x0080

    def __init__(self, row_gut_width, col_gut_height, row_visible_levels, col_visible_levels):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<4H', row_gut_width, col_gut_height, row_visible_levels, col_visible_levels)

class WSBoolRecord(BiffRecord):
    """
    This  record stores a 16 bit value with Boolean options for the current
    sheet.  From BIFF5 on the "Save external linked values" option is moved
    to the record BOOKBOOL.

    Option flags of record WSBOOL, BIFF3-BIFF8:

    Bit     Mask    Contents
    0       0001H   0 = Do not show automatic page breaks
                    1 = Show automatic page breaks
    4       0010H   0 = Standard sheet
                    1 = Dialogue sheet (BIFF5-BIFF8)
    5       0020H   0 = No automatic styles in outlines
                    1 = Apply automatic styles to outlines
    6       0040H   0 = Outline buttons above outline group
                    1 = Outline buttons below outline group
    7       0080H   0 = Outline buttons left of outline group
                    1 = Outline buttons right of outline group
    8       0100H   0 = Scale printout in percent
                    1 = Fit printout to number of pages
    9       0200H   0 = Save external linked values (BIFF3?BIFF4 only)
                    1 = Do not save external linked values (BIFF3?BIFF4 only)
    10      0400H   0 = Do not show row outline symbols
                    1 = Show row outline symbols
    11      0800H   0 = Do not show column outline symbols
                    1 = Show column outline symbols
    13-12   3000H   These flags specify the arrangement of windows.
                    They are stored in BIFF4 only.
                    00 = Arrange windows tiled
                    01 = Arrange windows horizontal
                    10 = Arrange windows vertical112 = Arrange windows cascaded
    The following flags are valid for BIFF4-BIFF8 only:
    14      4000H   0 = Standard expression evaluation
                    1 = Alternative expression evaluation
    15      8000H   0 = Standard formula entries
                    1 = Alternative formula entries

    """
    _REC_ID = 0x0081

    def __init__(self, options):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', options)

class ColInfoRecord(BiffRecord):
    """
    This record specifies the width for a given range of columns.
    If a column does not have a corresponding COLINFO record,
    the width specified in the record STANDARDWIDTH is used. If
    this record is also not present, the contents of the record
    DEFCOLWIDTH is used instead.
    This record also specifies a default XF record to use for
    cells in the columns that are not described by any cell record
    (which contain the XF index for that cell). Additionally,
    the option flags field contains hidden, outline, and collapsed
    options applied at the columns.

    Record COLINFO, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       Index to first column in the range
    2       2       Index to last column in the range
    4       2       Width of the columns in 1/256 of the width of the zero character, using default font
                    (first FONT record in the file)
    6       2       Index to XF record for default column formatting
    8       2       Option flags:
                    Bits    Mask    Contents
                    0       0001H   1 = Columns are hidden
                    10-8    0700H   Outline level of the columns (0 = no outline)
                    12      1000H   1 = Columns are collapsed
    10      2       Not used

    """
    _REC_ID = 0x007D

    def __init__(self, first_col, last_col, width, xf_index, options):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<6H', first_col, last_col, width, xf_index, options, 0)

class CalcModeRecord(BiffRecord):
    """
    This record is part of the Calculation Settings Block.
    It specifies whether to calculate formulas manually,
    automatically or automatically except for multiple table operations.

    Record CALCMODE, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       FFFFH = automatic except for multiple table operations
                    0000H = manually
                    0001H = automatically (default)
    """
    _REC_ID = 0x000D

    def __init__(self, calc_mode):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<h', calc_mode)


class CalcCountRecord(BiffRecord):
    """
    This record is part of the Calculation Settings Block. It specifies the maximum
    number of times the formulas should be iteratively calculated. This is a fail-safe
    against mutually recursive formulas locking up a spreadsheet application.

    Record CALCCOUNT, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       Maximum number of iterations allowed in circular references
    """

    _REC_ID = 0x000C

    def __init__(self, calc_count):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', calc_count)

class RefModeRecord(BiffRecord):
    """
    This record is part of the Calculation Settings Block.
    It stores which method is used to show cell addresses in formulas.
    The RC mode uses numeric indexes for rows and columns,
    i.e. R(1)C(-1), or R1C1:R2C2.
    The A1 mode uses characters for columns and numbers for rows,
    i.e. B1, or $A$1:$B$2.

    Record REFMODE, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       0 = RC mode; 1 = A1 mode

    """
    _REC_ID = 0x00F

    def __init__(self, ref_mode):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', ref_mode)

class IterationRecord(BiffRecord):
    """
    This record is part of the Calculation Settings Block.
    It stores if iterations are allowed while calculating recursive formulas.

    Record ITERATION, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       0 = Iterations off; 1 = Iterations on
    """
    _REC_ID = 0x011

    def __init__(self, iterations_on):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', iterations_on)

class DeltaRecord(BiffRecord):
    """
    This record is part of the Calculation Settings Block.
    It stores the maximum change of the result to exit an iteration.

    Record DELTA, BIFF2-BIFF8:

    Offset  Size    Contents
    0       8       Maximum change in iteration
                    (IEEE 754 floating-point value,
                     64bit double precision)
    """
    _REC_ID = 0x010

    def __init__(self, delta):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<d', delta)

class SaveRecalcRecord(BiffRecord):
    """
    This record is part of the Calculation Settings Block.
    It contains the Recalculate before save option in
    Excel's calculation settings dialogue.

    Record SAVERECALC, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       0 = Do not recalculate;
                    1 = Recalculate before saving the document

    """
    _REC_ID = 0x05F

    def __init__(self, recalc):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', recalc)

class PrintHeadersRecord(BiffRecord):
    """
    This record stores if the row and column headers
    (the areas with row numbers and column letters) will be printed.

    Record PRINTHEADERS, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       0 = Do not print row/column headers;
                    1 = Print row/column headers
    """
    _REC_ID = 0x02A

    def __init__(self, print_headers):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', print_headers)


class PrintGridLinesRecord(BiffRecord):
    """
    This record stores if sheet grid lines will be printed.

    Record PRINTGRIDLINES, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       0 = Do not print sheet grid lines;
                    1 = Print sheet grid lines

    """
    _REC_ID = 0x02B

    def __init__(self, print_grid):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', print_grid)


class GridSetRecord(BiffRecord):
    """
    This record specifies if the option to print sheet grid lines
    (record PRINTGRIDLINES) has ever been changed.

    Record GRIDSET, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       0 = Print grid lines option never changed
                    1 = Print grid lines option changed
    """
    _REC_ID = 0x082

    def __init__(self, print_grid_changed):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', print_grid_changed)


class DefaultRowHeight(BiffRecord):
    """
    This record specifies the default height and default flags
    for rows that do not have a corresponding ROW record.

    Record DEFAULTROWHEIGHT, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       Option flags:
                    Bit Mask    Contents
                    0   0001H   1 = Row height and default font height do not match
                    1   0002H   1 = Row is hidden
                    2   0004H   1 = Additional space above the row
                    3   0008H   1 = Additional space below the row
    2       2       Default height for unused rows, in twips = 1/20 of a point

    """
    _REC_ID = 0x0225

    def __init__(self, options, def_height):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<2H', options, def_height)


class DefColWidthRecord(BiffRecord):
    """
    This record specifies the default column width for columns that
    do not have a specific width set using the record COLINFO or COLWIDTH.
    This record has no effect, if a STANDARDWIDTH record is present in the file.

    Record DEFCOLWIDTH, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       Column width in characters, using the width of the zero
                    character from default font (first FONT record in the file)
    """
    _REC_ID = 0x0055

    def __init__(self, def_width):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', options, def_width)

class HorizontalPageBreaksRecord(BiffRecord):
    """
    This  record  is  part  of  the  Page  Settings  Block. It contains all
    horizontal manual page breaks.

    Record HORIZONTALPAGEBREAKS, BIFF8:
    Offset  Size  Contents
    0       2     Number of following row index structures (nm)
    2       6nm   List of nm row index structures. Each row index
                  structure contains:
                    Offset  Size    Contents
                    0       2       Index to first row below the page break
                    2       2       Index to first column of this page break
                    4       2       Index to last column of this page break

    The row indexes in the lists must be ordered ascending.
    If in BIFF8 a row contains several page breaks, they must be ordered
    ascending by start column index.
    """
    _REC_ID = 0x001B

    def __init__(self, breaks_list):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', len(breaks_list))
        for r, c1, c2 in breaks_list:
            self._rec_data += struct.pack('<3H', r, c1, c2)

class VerticalPageBreaksRecord(BiffRecord):
    """
    This  record  is  part  of  the  Page  Settings  Block. It contains all
    vertical manual page breaks.

    Record VERTICALPAGEBREAKS, BIFF8:
    Offset  Size  Contents
    0       2     Number of following column index structures (nm)
    2       6nm   List of nm column index structures. Each column index
                  structure contains:
                    Offset  Size    Contents
                    0       2       Index to first column following the page
                                    break
                    2       2       Index to first row of this page break
                    4       2       Index to last row of this page break

    The column indexes in the lists must be ordered ascending.
    If in BIFF8 a column contains several page breaks, they must be ordered
    ascending by start row index.
    """
    _REC_ID = 0x001A

    def __init__(self, breaks_list):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', len(breaks_list))
        for r, c1, c2 in breaks_list:
            self._rec_data += struct.pack('<3H', r, c1, c2)

class HeaderRecord(BiffRecord):
    """
    This record is part of the Page Settings Block. It specifies the
    page  header  string  for  the current worksheet. If this record is not
    present  or  completely  empty  (record  size is 0), the sheet does not
    contain a page header.

    Record HEADER for non-empty page header, BIFF2-BIFF8:
    Offset      Size    Contents
    0           var.    Page header string
                        BIFF2-BIFF7:    Non-empty byte string, 8bit string
                        length
                        BIFF8: Non-empty Unicode string, 16bit string length
    The  header  string may contain special commands, i.e. placeholders for
    the  page  number,  current  date, or text formatting attributes. These
    fields  are  represented  by  single  letters (exception: font name and
    size,  see  below)  with  a  leading  ampersand ("&"). If the ampersand
    is  part  of the regular header text, it will be duplicated ("&&"). The
    page  header is divided into 3 sections: the left, the centred, and the
    right  section.  Each  section  is introduced by a special command. All
    text  and all commands following are part of the selected section. Each
    section  starts  with the text formatting specified in the default font
    (first  FONT  record  in  the  file). Active formatting attributes from
    a previous section do not go into the next section.

    The following table shows all available commands:

    Command         Contents
    &&              The "&" character itself
    &L              Start of the left section
    &C              Start of the centred section
    &R              Start of the right section
    &P              Current page number
    &N              Page count
    &D              Current date
    &T              Current time
    &A              Sheet name (BIFF5-BIFF8)
    &F              File name without path
    &Z              File path without file name (BIFF8X)
    &G              Picture (BIFF8X)
    &B              Bold on/off (BIFF2-BIFF4)
    &I              Italic on/off (BIFF2-BIFF4)
    &U              Underlining on/off
    &E              Double underlining on/off (BIFF5-BIFF8)
    &S              Strikeout on/off
    &X              Superscript on/off (BIFF5-BIFF8)
    &Y              Subscript on/off (BIFF5-BIFF8)
    &"<fontname>"   Set new font <fontname>
    &"<fontname>,<fontstyle>"
                    Set new font with specified style <fontstyle>.
                    The style <fontstyle> is in most cases one of
                    "Regular", "Bold", "Italic", or "Bold Italic".
                    But this setting is dependent on the used font,
                    it may differ (localised style names, or "Standard",
                    "Oblique", ...). (BIFF5-BIFF8)
    &<fontheight>   Set font height in points (<fontheight> is a decimal value).
                    If this command is followed by a plain number to be printed
                    in the header, it will be separated from the font height
                    with a space character.

    """
    _REC_ID = 0x0014

    def __init__(self, header_str):
        BiffRecord.__init__(self)
        self._rec_data = upack2(header_str)

class FooterRecord(BiffRecord):
    """
    Semantic is equal to HEADER record
    """
    _REC_ID = 0x0015

    def __init__(self, footer_str):
        BiffRecord.__init__(self)
        self._rec_data = upack2(footer_str)


class HCenterRecord(BiffRecord):
    """
    This  record  is  part  of the Page Settings Block. It specifies if the
    sheet is centred horizontally when printed.

    Record HCENTER, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       0 = Print sheet left aligned
                    1 = Print sheet centred horizontally

    """
    _REC_ID = 0x0083

    def __init__(self, is_horz_center):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', is_horz_center)


class VCenterRecord(BiffRecord):
    """
    This  record  is  part  of the Page Settings Block. It specifies if the
    sheet is centred vertically when printed.

    Record VCENTER, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       0 = Print sheet aligned at top page border
                    1 = Print sheet vertically centred

    """
    _REC_ID = 0x0084

    def __init__(self, is_vert_center):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', is_vert_center)


class LeftMarginRecord(BiffRecord):
    """
    This  record  is  part of the Page Settings Block. It contains the left
    page margin of the current worksheet.

    Record LEFTMARGIN, BIFF2-BIFF8:

    Offset  Size    Contents
    0       8       Left page margin in inches
                    (IEEE 754 floating-point value, 64bit double precision)

    """
    _REC_ID = 0x0026

    def __init__(self, margin):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<d', margin)


class RightMarginRecord(BiffRecord):
    """
    This  record  is  part of the Page Settings Block. It contains the right
    page margin of the current worksheet.

    Offset  Size    Contents
    0       8       Right page margin in inches
                    (IEEE 754 floating-point value, 64?bit double precision)

    """
    _REC_ID = 0x0027

    def __init__(self, margin):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<d', margin)

class TopMarginRecord(BiffRecord):
    """
    This  record  is  part of the Page Settings Block. It contains the top
    page margin of the current worksheet.

    Offset  Size    Contents
    0       8       Top page margin in inches
                    (IEEE 754 floating-point value, 64?bit double precision)

    """
    _REC_ID = 0x0028

    def __init__(self, margin):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<d', margin)


class BottomMarginRecord(BiffRecord):
    """
    This  record  is  part of the Page Settings Block. It contains the bottom
    page margin of the current worksheet.

    Offset  Size    Contents
    0       8       Bottom page margin in inches
                    (IEEE 754 floating-point value, 64?bit double precision)

    """
    _REC_ID = 0x0029

    def __init__(self, margin):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<d', margin)

class SetupPageRecord(BiffRecord):
    """
    This   record   is  part of the Page Settings Block. It stores the page
    format   settings   of   the  current sheet. The pages may be scaled in
    percent   or  by  using  an  absolute  number of pages. This setting is
    located   in  the  WSBOOL  record.  If  pages  are  scaled in  percent,
    the   scaling  factor  in  this  record is used, otherwise the "Fit  to
    pages"  values. One of the "Fit to pages" values may be 0. In this case
    the sheet is scaled to fit only to the other value.

    Record SETUP, BIFF5-BIFF8:

    Offset      Size    Contents
    0           2       Paper size (see below)
    2           2       Scaling factor in percent
    4           2       Start page number
    6           2       Fit worksheet width to this number of pages
                        (0 = use as many as needed)
    8           2       Fit worksheet height to this number of pages
                        (0 = use as many as needed)
    10          2       Option flags:
                        Bit     Mask        Contents
                        0       0001H       0 = Print pages in columns
                                            1 = Print pages in rows
                        1       0002H       0 = Landscape
                                            1 = Portrait
                        2       0004H       1 = Paper size, scaling factor,
                                            paper orientation (portrait/landscape),
                                            print resolution and number of copies
                                            are not initialised
                        3       0008H       0 = Print coloured
                                            1 = Print black and white
                        4       0010H       0 = Default print quality
                                            1 = Draft quality
                        5       0020H       0 = Do not print cell notes
                                            1 = Print cell notes
                        6       0040H       0 = Paper orientation setting is valid
                                            1 = Paper orientation setting not
                                            initialised
                        7       0080H       0 = Automatic page numbers
                                            1 = Use start page number
                        The following flags are valid for BIFF8 only:
                        9       0200H       0 = Print notes as displayed
                                            1 = Print notes at end of sheet
                        11-10   0C00H       00 = Print errors as displayed
                                            01 = Do not print errors
                                            10 = Print errors as "--"
                                            11 = Print errors as "#N/A!"
    12          2       Print resolution in dpi
    14          2       Vertical print resolution in dpi
    16          8       Header margin (IEEE 754 floating-point value,
                        64bit double precision)
    24          8       Footer margin (IEEE 754 floating-point value,
                        64bit double precision)
    32          2       Number of copies to print


    PAPER TYPES:

    Index   Paper type              Paper size
    0       Undefined
    1       Letter                  8 1/2" x 11"
    2       Letter small            8 1/2" x 11"
    3       Tabloid                 11" x 17"
    4       Ledger                  17" x 11"
    5       Legal                   8 1/2" x 14"
    6       Statement               5 1/2" x 8 1/2"
    7       Executive               7 1/4" x 10 1/2"
    8       A3                      297mm x 420mm
    9       A4                      210mm x 297mm
    10      A4 small                210mm x 297mm
    11      A5                      148mm x 210mm
    12      B4 (JIS)                257mm x 364mm
    13      B5 (JIS)                182mm x 257mm
    14      Folio                   8 1/2" x 13"
    15      Quarto                  215mm x 275mm
    16      10x14                   10" x 14"
    17      11x17                   11" x 17"
    18      Note                    8 1/2" x 11"
    19      Envelope #9             3 7/8" x 8 7/8"
    20      Envelope #10            4 1/8" x 9 1/2"
    21      Envelope #11            4 1/2" x 10 3/8"
    22      Envelope #12            4 3/4" x 11"
    23      Envelope #14            5" x 11 1/2"
    24      C                       17" x 22"
    25      D                       22" x 34"
    26      E                       34" x 44"
    27      Envelope DL             110mm x 220mm
    28      Envelope C5             162mm x 229mm
    29      Envelope C3             324mm x 458mm
    30      Envelope C4             229mm x 324mm
    31      Envelope C6             114mm x 162mm
    32      Envelope C6/C5          114mm x 229mm
    33      B4 (ISO)                250mm x 353mm
    34      B5 (ISO)                176mm x 250mm
    35      B6 (ISO)                125mm x 176mm
    36      Envelope Italy          110mm x 230mm
    37      Envelope Monarch        3 7/8" x 7 1/2"
    38      63/4 Envelope           3 5/8" x 6 1/2"
    39      US Standard Fanfold     14 7/8" x 11"
    40      German Std. Fanfold     8 1/2" x 12"
    41      German Legal Fanfold    8 1/2" x 13"
    42      B4 (ISO)                250mm x 353mm
    43      Japanese Postcard       100mm x 148mm
    44      9x11                    9" x 11"
    45      10x11                   10" x 11"
    46      15x11                   15" x 11"
    47      Envelope Invite         220mm x 220mm
    48      Undefined
    49      Undefined
    50      Letter Extra            9 1/2" x 12"
    51      Legal Extra             9 1/2" x 15"
    52      Tabloid Extra           11 11/16" x 18"
    53      A4 Extra                235mm x 322mm
    54      Letter Transverse       8 1/2" x 11"
    55      A4 Transverse           210mm x 297mm
    56      Letter Extra Transv.    9 1/2" x 12"
    57      Super A/A4              227mm x 356mm
    58      Super B/A3              305mm x 487mm
    59      Letter Plus             8 1/2" x 12 11/16"
    60      A4 Plus                 210mm x 330mm
    61      A5 Transverse           148mm x 210mm
    62      B5 (JIS) Transverse     182mm x 257mm
    63      A3 Extra                322mm x 445mm
    64      A5 Extra                174mm x 235mm
    65      B5 (ISO) Extra          201mm x 276mm
    66      A2                      420mm x 594mm
    67      A3 Transverse           297mm x 420mm
    68      A3 Extra Transverse     322mm x 445mm
    69      Dbl. Japanese Postcard  200mm x 148mm
    70      A6                      105mm x 148mm
    71
    72
    73
    74
    75      Letter Rotated          11" x 8 1/2"
    76      A3 Rotated              420mm x 297mm
    77      A4 Rotated              297mm x 210mm
    78      A5 Rotated              210mm x 148mm
    79      B4 (JIS) Rotated        364mm x 257mm
    80      B5 (JIS) Rotated        257mm x 182mm
    81      Japanese Postcard Rot.  148mm x 100mm
    82      Dbl. Jap. Postcard Rot. 148mm x 200mm
    83      A6 Rotated              148mm x 105mm
    84
    85
    86
    87
    88      B6 (JIS)                128mm x 182mm
    89      B6 (JIS) Rotated        182mm x 128mm
    90      12x11                   12" x 11"

    """
    _REC_ID = 0x00A1
    def __init__(self, paper, scaling, start_num, fit_width_to, fit_height_to,
                    options,
                    hres, vres,
                    header_margin, footer_margin,
                    num_copies):
        BiffRecord.__init__(self)

        self._rec_data = struct.pack('<8H2dH', paper, scaling, start_num,
                                        fit_width_to, fit_height_to, \
                                        options,
                                        hres, vres,
                                        header_margin, footer_margin,
                                        num_copies)


########NEW FILE########
__FILENAME__ = Bitmap
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov

#  Portions are Copyright (c) 2004 Evgeny Filatov <fufff@users.sourceforge.net>

#  Portions are Copyright (c) 2002-2004 John McNamara (Perl Spreadsheet::WriteExcel)

#  All rights reserved.

# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


__rev_id__ = """$Id: Bitmap.py,v 1.4 2005/07/20 07:24:11 rvk Exp $"""


from BIFFRecords import BiffRecord
from struct import *


def _size_col(sheet, col):
    return sheet.col_width(col)


def _size_row(sheet, row):
    return sheet.row_height(row)     


def _position_image(sheet, row_start, col_start, x1, y1, width, height):
    """Calculate the vertices that define the position of the image as required by
    the OBJ record.

             +------------+------------+
             |     A      |      B     |
       +-----+------------+------------+
       |     |(x1,y1)     |            |
       |  1  |(A1)._______|______      |
       |     |    |              |     |
       |     |    |              |     |
       +-----+----|    BITMAP    |-----+
       |     |    |              |     |
       |  2  |    |______________.     |
       |     |            |        (B2)|
       |     |            |     (x2,y2)|
       +---- +------------+------------+

    Example of a bitmap that covers some of the area from cell A1 to cell B2.

    Based on the width and height of the bitmap we need to calculate 8 vars:
        col_start, row_start, col_end, row_end, x1, y1, x2, y2.
    The width and height of the cells are also variable and have to be taken into
    account.
    The values of col_start and row_start are passed in from the calling
    function. The values of col_end and row_end are calculated by subtracting
    the width and height of the bitmap from the width and height of the
    underlying cells.
    The vertices are expressed as a percentage of the underlying cell width as
    follows (rhs values are in pixels):

           x1 = X / W *1024
           y1 = Y / H *256
           x2 = (X-1) / W *1024
           y2 = (Y-1) / H *256

           Where:  X is distance from the left side of the underlying cell
                   Y is distance from the top of the underlying cell
                   W is the width of the cell
                   H is the height of the cell

    Note: the SDK incorrectly states that the height should be expressed as a
    percentage of 1024.

    col_start  - Col containing upper left corner of object
    row_start  - Row containing top left corner of object
    x1  - Distance to left side of object
    y1  - Distance to top of object
    width  - Width of image frame
    height  - Height of image frame
    
    """
    # Adjust start column for offsets that are greater than the col width
    while x1 >= _size_col(sheet, col_start):
        x1 -= _size_col(sheet, col_start)
        col_start += 1
    # Adjust start row for offsets that are greater than the row height
    while y1 >= _size_row(sheet, row_start):
        y1 -= _size_row(sheet, row_start)
        row_start += 1
    # Initialise end cell to the same as the start cell
    row_end = row_start   # Row containing bottom right corner of object
    col_end = col_start   # Col containing lower right corner of object
    width = width + x1 - 1
    height = height + y1 - 1
    # Subtract the underlying cell widths to find the end cell of the image
    while (width >= _size_col(sheet, col_end)):
        width -= _size_col(sheet, col_end)
        col_end += 1
    # Subtract the underlying cell heights to find the end cell of the image
    while (height >= _size_row(sheet, row_end)):
        height -= _size_row(sheet, row_end)
        row_end += 1
    # Bitmap isn't allowed to start or finish in a hidden cell, i.e. a cell
    # with zero height or width.
    if ((_size_col(sheet, col_start) == 0) or (_size_col(sheet, col_end) == 0)
            or (_size_row(sheet, row_start) == 0) or (_size_row(sheet, row_end) == 0)):
        return
    # Convert the pixel values to the percentage value expected by Excel
    x1 = float(x1) / _size_col(sheet, col_start) * 1024
    y1 = float(y1) / _size_row(sheet, row_start) * 256
    # Distance to right side of object
    x2 = float(width) / _size_col(sheet, col_end) * 1024
    # Distance to bottom of object
    y2 = float(height) / _size_row(sheet, row_end) * 256
    return (col_start, x1, row_start, y1, col_end, x2, row_end, y2)


class ObjBmpRecord(BiffRecord):
    _REC_ID = 0x005D    # Record identifier

    def __init__(self, row, col, sheet, im_data_bmp, x, y, scale_x, scale_y):
        # Scale the frame of the image.
        width = im_data_bmp.width * scale_x
        height = im_data_bmp.height * scale_y

        # Calculate the vertices of the image and write the OBJ record
        col_start, x1, row_start, y1, col_end, x2, row_end, y2 = _position_image(sheet, row, col, x, y, width, height)

        """Store the OBJ record that precedes an IMDATA record. This could be generalise
        to support other Excel objects.

        """
        cObj = 0x0001      # Count of objects in file (set to 1)
        OT = 0x0008        # Object type. 8 = Picture
        id = 0x0001        # Object ID
        grbit = 0x0614     # Option flags
        colL = col_start    # Col containing upper left corner of object
        dxL = x1            # Distance from left side of cell
        rwT = row_start     # Row containing top left corner of object
        dyT = y1            # Distance from top of cell
        colR = col_end      # Col containing lower right corner of object
        dxR = x2            # Distance from right of cell
        rwB = row_end       # Row containing bottom right corner of object
        dyB = y2            # Distance from bottom of cell
        cbMacro = 0x0000    # Length of FMLA structure
        Reserved1 = 0x0000  # Reserved
        Reserved2 = 0x0000  # Reserved
        icvBack = 0x09      # Background colour
        icvFore = 0x09      # Foreground colour
        fls = 0x00          # Fill pattern
        fAuto = 0x00        # Automatic fill
        icv = 0x08          # Line colour
        lns = 0xff          # Line style
        lnw = 0x01          # Line weight
        fAutoB = 0x00       # Automatic border
        frs = 0x0000        # Frame style
        cf = 0x0009         # Image format, 9 = bitmap
        Reserved3 = 0x0000  # Reserved
        cbPictFmla = 0x0000 # Length of FMLA structure
        Reserved4 = 0x0000  # Reserved
        grbit2 = 0x0001     # Option flags
        Reserved5 = 0x0000  # Reserved
        
        data = pack("<L", cObj)
        data += pack("<H", OT)
        data += pack("<H", id)
        data += pack("<H", grbit)
        data += pack("<H", colL)
        data += pack("<H", dxL)
        data += pack("<H", rwT)
        data += pack("<H", dyT)
        data += pack("<H", colR)
        data += pack("<H", dxR)
        data += pack("<H", rwB)
        data += pack("<H", dyB)
        data += pack("<H", cbMacro)
        data += pack("<L", Reserved1)
        data += pack("<H", Reserved2)
        data += pack("<B", icvBack)
        data += pack("<B", icvFore)
        data += pack("<B", fls)
        data += pack("<B", fAuto)
        data += pack("<B", icv)
        data += pack("<B", lns)
        data += pack("<B", lnw)
        data += pack("<B", fAutoB)
        data += pack("<H", frs)
        data += pack("<L", cf)
        data += pack("<H", Reserved3)
        data += pack("<H", cbPictFmla)
        data += pack("<H", Reserved4)
        data += pack("<H", grbit2)
        data += pack("<L", Reserved5)

        self._rec_data = data

def _process_bitmap(bitmap):
    """Convert a 24 bit bitmap into the modified internal format used by Windows.
    This is described in BITMAPCOREHEADER and BITMAPCOREINFO structures in the
    MSDN library.

    """
    # Open file and binmode the data in case the platform needs it.
    fh = file(bitmap, "rb")
    try:
        # Slurp the file into a string.
        data = fh.read()
    finally:
        fh.close()
    # Check that the file is big enough to be a bitmap.
    if len(data) <= 0x36:
        raise Exception("bitmap doesn't contain enough data.")
    # The first 2 bytes are used to identify the bitmap.
    if (data[:2] != "BM"):
        raise Exception("bitmap doesn't appear to to be a valid bitmap image.")
    # Remove bitmap data: ID.
    data = data[2:]
    # Read and remove the bitmap size. This is more reliable than reading
    # the data size at offset 0x22.
    #
    size = unpack("<L", data[:4])[0]
    size -=  0x36   # Subtract size of bitmap header.
    size +=  0x0C   # Add size of BIFF header.
    data = data[4:]
    # Remove bitmap data: reserved, offset, header length.
    data = data[12:]
    # Read and remove the bitmap width and height. Verify the sizes.
    width, height = unpack("<LL", data[:8])
    data = data[8:]
    if (width > 0xFFFF):
        raise Exception("bitmap: largest image width supported is 65k.")
    if (height > 0xFFFF):
        raise Exception("bitmap: largest image height supported is 65k.")
    # Read and remove the bitmap planes and bpp data. Verify them.
    planes, bitcount = unpack("<HH", data[:4])
    data = data[4:]
    if (bitcount != 24):
        raise Exception("bitmap isn't a 24bit true color bitmap.")
    if (planes != 1):
        raise Exception("bitmap: only 1 plane supported in bitmap image.")
    # Read and remove the bitmap compression. Verify compression.
    compression = unpack("<L", data[:4])[0]
    data = data[4:]
    if (compression != 0):
        raise Exception("bitmap: compression not supported in bitmap image.")
    # Remove bitmap data: data size, hres, vres, colours, imp. colours.
    data = data[20:]
    # Add the BITMAPCOREHEADER data
    header = pack("<LHHHH", 0x000c, width, height, 0x01, 0x18)
    data = header + data
    return (width, height, size, data)


class ImDataBmpRecord(BiffRecord):
    _REC_ID = 0x007F

    def __init__(self, filename):
        """Insert a 24bit bitmap image in a worksheet. The main record required is
        IMDATA but it must be proceeded by a OBJ record to define its position.

        """
        BiffRecord.__init__(self)

        self.width, self.height, self.size, data = _process_bitmap(filename)
        # Write the IMDATA record to store the bitmap data
        cf = 0x09
        env = 0x01
        lcb = self.size
        self._rec_data = pack("<HHL", cf, env, lcb) + data



########NEW FILE########
__FILENAME__ = Cell
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


__rev_id__ = """$Id: Cell.py,v 1.3 2005/08/11 08:53:48 rvk Exp $"""


import struct
import BIFFRecords


class StrCell(object):
    __slots__ = ["__init__", "get_biff_data",
                "__parent", "__idx", "__xf_idx", "__sst_idx"]

    def __init__(self, parent, idx, xf_idx, sst_idx):
        self.__parent = parent
        self.__idx = idx
        self.__xf_idx = xf_idx
        self.__sst_idx = sst_idx

    def get_biff_data(self):
        return BIFFRecords.LabelSSTRecord(self.__parent.get_index(), self.__idx, self.__xf_idx, self.__sst_idx).get()


class BlankCell(object):
    __slots__ = ["__init__", "get_biff_data",
                "__parent", "__idx", "__xf_idx"]

    def __init__(self, parent, idx, xf_idx):
        self.__parent = parent
        self.__idx = idx
        self.__xf_idx = xf_idx

    def get_biff_data(self):
        return BIFFRecords.BlankRecord(self.__parent.get_index(), self.__idx, self.__xf_idx).get()


class MulBlankCell(object):
    __slots__ = ["__init__", "get_biff_data",
                "__parent", "__col1", "__col2", "__xf_idx"]

    def __init__(self, parent, col1, col2, xf_idx):
        self.__parent = parent
        self.__col1 = col1
        self.__col2 = col2
        self.__xf_idx = xf_idx

    def get_biff_data(self):
        return BIFFRecords.MulBlankRecord(self.__parent.get_index(), self.__col1, self.__col2, self.__xf_idx).get()


class NumberCell(object):
    __slots__ = ["__init__", "get_biff_data",
                "__parent", "__idx", "__xf_idx", "__number"]


    def __init__(self, parent, idx, xf_idx, number):
        self.__parent = parent
        self.__idx = idx
        self.__xf_idx = xf_idx
        self.__number = float(number)


    def get_biff_data(self):
        rk_encoded = 0

        packed = struct.pack('<d', self.__number)

        #print self.__number
        w0, w1, w2, w3 = struct.unpack('<4H', packed)
        if w0 == 0 and w1 == 0 and w2 & 0xFFFC == w2:
            # 34 lsb are 0
            #print "float RK"
            rk_encoded = (w3 << 16) | w2
            return BIFFRecords.RKRecord(self.__parent.get_index(), self.__idx, self.__xf_idx, rk_encoded).get()

        if abs(self.__number) < 0x40000000 and int(self.__number) == self.__number:
            #print "30-bit integer RK"
            rk_encoded = 2 | (int(self.__number) << 2)
            return BIFFRecords.RKRecord(self.__parent.get_index(), self.__idx, self.__xf_idx, rk_encoded).get()

        temp = self.__number*100
        packed100 = struct.pack('<d', temp)
        w0, w1, w2, w3 = struct.unpack('<4H', packed100)
        if w0 == 0 and w1 == 0 and w2 & 0xFFFC == w2:
            # 34 lsb are 0
            #print "float RK*100"
            rk_encoded = 1 | (w3 << 16) | w2
            return BIFFRecords.RKRecord(self.__parent.get_index(), self.__idx, self.__xf_idx, rk_encoded).get()

        if abs(temp) < 0x40000000 and int(temp) == temp:
            #print "30-bit integer RK*100"
            rk_encoded = 3 | (int(temp) << 2)
            return BIFFRecords.RKRecord(self.__parent.get_index(), self.__idx, self.__xf_idx, rk_encoded).get()

        #print "Number" 
        #print
        return BIFFRecords.NumberRecord(self.__parent.get_index(), self.__idx, self.__xf_idx, self.__number).get()


class MulNumberCell(object):
    __slots__ = ["__init__", "get_biff_data"]

    def __init__(self, parent, idx, xf_idx, sst_idx):
        self.__parent = parent
        self.__idx = idx
        self.__xf_idx = xf_idx
        self.__sst_idx = sst_idx


    def get_biff_data(self):
        raise Exception


class FormulaCell(object):
    __slots__ = ["__init__", "get_biff_data",
                "__parent", "__idx", "__xf_idx", "__frmla"]

    def __init__(self, parent, idx, xf_idx, frmla):
        self.__parent = parent
        self.__idx = idx
        self.__xf_idx = xf_idx
        self.__frmla = frmla


    def get_biff_data(self):
        return BIFFRecords.FormulaRecord(self.__parent.get_index(), self.__idx, self.__xf_idx, self.__frmla.rpn()).get()




########NEW FILE########
__FILENAME__ = Column
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


__rev_id__ = """$Id: Column.py,v 1.4 2005/07/20 07:24:11 rvk Exp $"""


from BIFFRecords import ColInfoRecord
from Deco import *
from Worksheet import Worksheet


class Column(object):
    @accepts(object, int, Worksheet)
    def __init__(self, indx, parent_sheet):
        self._index = indx
        self._parent = parent_sheet
        self._parent_wb = parent_sheet.get_parent()
        self._xf_index = 0x0F
        
        self.width = 0x0B92
        self.hidden = 0
        self.level = 0
        self.collapse = 0


    def get_biff_record(self):
        options =  (self.hidden & 0x01) << 0
        options |= (self.level & 0x07) << 8
        options |= (self.collapse & 0x01) << 12
        
        return ColInfoRecord(self._index, self._index, self.width, self._xf_index, options).get()
        
        
        

########NEW FILE########
__FILENAME__ = CompoundDoc
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


import sys
import struct



__rev_id__ = """$Id: CompoundDoc.py,v 1.6 2005/09/30 09:12:23 rvk Exp $"""

        
class Reader:
    def __init__(self, filename, dump = False):
        self.dump = dump
        self.STREAMS = {}

        doc = file(filename, 'rb').read()
        self.header, self.data = doc[0:512], doc[512:]
        del doc

        self.__build_header()
        self.__build_MSAT()
        self.__build_SAT()
        self.__build_directory()
        self.__build_short_sectors_data()
        
        if len(self.short_sectors_data) > 0:
            self.__build_SSAT()
        else:
            if self.dump and (self.total_ssat_sectors != 0 or self.ssat_start_sid != -2):
                print 'NOTE: header says that must be', self.total_ssat_sectors, 'short sectors'
                print 'NOTE: starting at', self.ssat_start_sid, 'sector'
                print 'NOTE: but file does not contains data in short sectors'
            self.ssat_start_sid = -2
            self.total_ssat_sectors = 0
            self.SSAT = [-2]

        for dentry in self.dir_entry_list[1:]:
            (did, 
             sz, name, 
             t, c, 
             did_left, did_right, did_root, 
             dentry_start_sid, 
             stream_size
            ) = dentry
            stream_data = ''
            if stream_size > 0:
                if stream_size >= self.min_stream_size:
                    args = (self.data, self.SAT, dentry_start_sid, self.sect_size)
                else:
                    args = (self.short_sectors_data, self.SSAT, dentry_start_sid, self.short_sect_size)
                stream_data = self.get_stream_data(*args)

            if name != '':
                # BAD IDEA: names may be equal. NEED use full paths...
                self.STREAMS[name] = stream_data

    
    def __build_header(self):
        self.doc_magic             = self.header[0:8]

        if self.doc_magic != '\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1':
            raise Exception, 'Not an OLE file.'

        self.file_uid              = self.header[8:24]
        self.rev_num               = self.header[24:26]
        self.ver_num               = self.header[26:28]
        self.byte_order            = self.header[28:30]
        self.log2_sect_size,       = struct.unpack('<H', self.header[30:32])
        self.log2_short_sect_size, = struct.unpack('<H', self.header[32:34])
        self.total_sat_sectors,    = struct.unpack('<L', self.header[44:48])
        self.dir_start_sid,        = struct.unpack('<l', self.header[48:52])
        self.min_stream_size,      = struct.unpack('<L', self.header[56:60])
        self.ssat_start_sid,       = struct.unpack('<l', self.header[60:64])
        self.total_ssat_sectors,   = struct.unpack('<L', self.header[64:68])
        self.msat_start_sid,       = struct.unpack('<l', self.header[68:72])
        self.total_msat_sectors,   = struct.unpack('<L', self.header[72:76])
         
        self.sect_size        = 1 << self.log2_sect_size
        self.short_sect_size  = 1 << self.log2_short_sect_size

        if self.dump:
            print 'file magic: '
            print_bin_data(self.doc_magic)

            print 'file uid: '
            print_bin_data(self.file_uid)

            print 'revision number: '
            print_bin_data(self.rev_num)
         
            print 'version number: '
            print_bin_data(self.ver_num)
            
            print 'byte order: '
            print_bin_data(self.byte_order)
            
            print 'sector size                                :', hex(self.sect_size), self.sect_size
            #print 'total sectors in file                      :', hex(self.total_sectors), self.total_sectors
            print 'short sector size                          :', hex(self.short_sect_size), self.short_sect_size
            print 'Total number of sectors used for the SAT   :', hex(self.total_sat_sectors), self.total_sat_sectors
            print 'SID of first sector of the directory stream:', hex(self.dir_start_sid), self.dir_start_sid
            print 'Minimum size of a standard stream          :', hex(self.min_stream_size), self.min_stream_size
            print 'SID of first sector of the SSAT            :', hex(self.ssat_start_sid), self.ssat_start_sid
            print 'Total number of sectors used for the SSAT  :', hex(self.total_ssat_sectors), self.total_ssat_sectors
            print 'SID of first additional sector of the MSAT :', hex(self.msat_start_sid), self.msat_start_sid
            print 'Total number of sectors used for the MSAT  :', hex(self.total_msat_sectors), self.total_msat_sectors


    def __build_MSAT(self):
        self.MSAT = list(struct.unpack('<109l', self.header[76:]))
        
        next = self.msat_start_sid
        while next > 0:
           msat_sector = struct.unpack('<128l', self.data[next*self.sect_size:(next+1)*self.sect_size])
           self.MSAT.extend(msat_sector[:127])
           next = msat_sector[-1]

        if self.dump:
            print 'MSAT (header part): \n', self.MSAT[:109]
            print 'additional MSAT sectors: \n', self.MSAT[109:]


    def __build_SAT(self):
        sat_stream = ''.join([self.data[i*self.sect_size:(i+1)*self.sect_size] for i in self.MSAT if i >= 0])

        sat_sids_count = len(sat_stream) >> 2
        self.SAT = struct.unpack('<%dl' % sat_sids_count, sat_stream) # SIDs tuple

        if self.dump:
            print 'SAT sid count:\n', sat_sids_count
            print 'SAT content:\n', self.SAT


    def __build_SSAT(self):
        ssat_stream = self.get_stream_data(self.data, self.SAT, self.ssat_start_sid, self.sect_size)

        ssids_count = len(ssat_stream) >> 2
        self.SSAT = struct.unpack('<%dl' % ssids_count, ssat_stream)

        if self.dump:
            print 'SSID count:', ssids_count
            print 'SSAT content:\n', self.SSAT


    def __build_directory(self):
        dir_stream = self.get_stream_data(self.data, self.SAT, self.dir_start_sid, self.sect_size)

        self.dir_entry_list = []

        i = 0
        while i < len(dir_stream):
            dentry = dir_stream[i:i+128] # 128 -- dir entry size
            i += 128
            
            did = len(self.dir_entry_list)
            sz, = struct.unpack('<H', dentry[64:66])
            if sz > 0 :
                name = dentry[0:sz-2].decode('utf_16_le', 'replace')
            else:
                name = u''
            t,  = struct.unpack('B', dentry[66])
            c,  = struct.unpack('B', dentry[67])
            did_left ,  = struct.unpack('<l', dentry[68:72])
            did_right ,  = struct.unpack('<l', dentry[72:76])
            did_root ,  = struct.unpack('<l', dentry[76:80])
            dentry_start_sid ,  = struct.unpack('<l', dentry[116:120])
            stream_size ,  = struct.unpack('<L', dentry[120:124])

            self.dir_entry_list.extend([(did, sz, name, t, c, 
                                            did_left, did_right, did_root, 
                                            dentry_start_sid, stream_size)]) 

        if self.dump:
            dentry_types = {
                0x00: 'Empty',
                0x01: 'User storage',
                0x02: 'User stream',
                0x03: 'LockBytes',
                0x04: 'Property',
                0x05: 'Root storage'
            }
            node_colours = {
                0x00: 'Red',
                0x01: 'Black'
            }
            print 'total directory entries:', len(self.dir_entry_list)

            for dentry in self.dir_entry_list:
                (did, sz, name, t, c, 
                 did_left, did_right, did_root, 
                 dentry_start_sid, stream_size) = dentry
                print 'DID', did
                print 'Size of the used area of the character buffer of the name:', sz
                print 'dir entry name:', repr(name)
                print 'type of entry:', t, dentry_types[t]
                print 'entry colour:', c, node_colours[c]
                print 'left child DID :', did_left
                print 'right child DID:', did_right
                print 'root DID       :', did_root
                print 'start SID       :', dentry_start_sid
                print 'stream size     :', stream_size
                if stream_size == 0:
                    print 'stream is empty'
                elif stream_size >= self.min_stream_size:
                    print 'stream stored as normal stream'
                else:
                    print 'stream stored as short-stream'

    
    def __build_short_sectors_data(self):
        (did, sz, name, t, c, 
         did_left, did_right, did_root, 
         dentry_start_sid, stream_size) = self.dir_entry_list[0]
        assert t == 0x05 # Short-Stream Container Stream (SSCS) resides in Root Storage
        if stream_size == 0:
            self.short_sectors_data = ''
        else:
            self.short_sectors_data = self.get_stream_data(self.data, self.SAT, dentry_start_sid, self.sect_size)


    def get_stream_data(self, data, SAT, start_sid, sect_size):
        sid = start_sid
        chunks = [(sid, sid)]
        stream_data = ''

        while SAT[sid] >= 0:
            next_in_chain = SAT[sid]
            last_chunk_start, last_chunk_finish = chunks[-1]
            if next_in_chain - last_chunk_finish <= 1:
                chunks[-1] = last_chunk_start, next_in_chain
            else:
                chunks.extend([(next_in_chain, next_in_chain)]) 
            sid = next_in_chain
        for s, f in chunks:
            stream_data += data[s*sect_size:(f+1)*sect_size]
        #print chunks
        return stream_data

        
def print_bin_data(data):
    i = 0
    while i < len(data):
        j = 0
        while (i < len(data)) and (j < 16):
            c = '0x%02X' % ord(data[i])
            sys.stdout.write(c)
            sys.stdout.write(' ')
            i += 1
            j += 1
        print
    if i == 0:
        print '<NO DATA>'



# This implementation writes only 'Root Entry', 'Workbook' streams
# and 2 empty streams for aligning directory stream on sector boundary
# 
# LAYOUT:
# 0         header
# 76                MSAT (1st part: 109 SID)
# 512       workbook stream
# ...       additional MSAT sectors if streams' size > about 7 Mb == (109*512 * 128)
# ...       SAT
# ...       directory stream
#
# NOTE: this layout is "ad hoc". It can be more general. RTFM

class XlsDoc:
    SECTOR_SIZE = 0x0200
    MIN_LIMIT   = 0x1000

    SID_FREE_SECTOR  = -1
    SID_END_OF_CHAIN = -2
    SID_USED_BY_SAT  = -3
    SID_USED_BY_MSAT = -4

    def __init__(self):
        #self.book_stream = ''                # padded
        self.book_stream_sect = []

        self.dir_stream = ''
        self.dir_stream_sect = []

        self.packed_SAT = ''
        self.SAT_sect = []

        self.packed_MSAT_1st = ''
        self.packed_MSAT_2nd = ''
        self.MSAT_sect_2nd = []

        self.header = ''

    def __build_directory(self): # align on sector boundary
        self.dir_stream = ''

        dentry_name      = '\x00'.join('Root Entry\x00') + '\x00'
        dentry_name_sz   = len(dentry_name)
        dentry_name_pad  = '\x00'*(64 - dentry_name_sz)
        dentry_type      = 0x05 # root storage
        dentry_colour    = 0x01 # black
        dentry_did_left  = -1
        dentry_did_right = -1
        dentry_did_root  = 1
        dentry_start_sid = -2
        dentry_stream_sz = 0

        self.dir_stream += struct.pack('<64s H 2B 3l 9L l L L',
           dentry_name + dentry_name_pad,
           dentry_name_sz,
           dentry_type,
           dentry_colour,
           dentry_did_left, 
           dentry_did_right,
           dentry_did_root,
           0, 0, 0, 0, 0, 0, 0, 0, 0,
           dentry_start_sid,
           dentry_stream_sz,
           0
        )

        dentry_name      = '\x00'.join('Workbook\x00') + '\x00'
        dentry_name_sz   = len(dentry_name)
        dentry_name_pad  = '\x00'*(64 - dentry_name_sz)
        dentry_type      = 0x02 # user stream
        dentry_colour    = 0x01 # black
        dentry_did_left  = -1
        dentry_did_right = -1
        dentry_did_root  = -1
        dentry_start_sid = 0     
        dentry_stream_sz = self.book_stream_len

        self.dir_stream += struct.pack('<64s H 2B 3l 9L l L L',
           dentry_name + dentry_name_pad,
           dentry_name_sz,
           dentry_type,
           dentry_colour,
           dentry_did_left, 
           dentry_did_right,
           dentry_did_root,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 
           dentry_start_sid,
           dentry_stream_sz,
           0
        )
        
        # padding
        dentry_name      = ''
        dentry_name_sz   = len(dentry_name)
        dentry_name_pad  = '\x00'*(64 - dentry_name_sz)
        dentry_type      = 0x00 # empty
        dentry_colour    = 0x01 # black
        dentry_did_left  = -1
        dentry_did_right = -1
        dentry_did_root  = -1
        dentry_start_sid = -2
        dentry_stream_sz = 0

        self.dir_stream += struct.pack('<64s H 2B 3l 9L l L L',
           dentry_name + dentry_name_pad,
           dentry_name_sz,
           dentry_type,
           dentry_colour,
           dentry_did_left, 
           dentry_did_right,
           dentry_did_root,
           0, 0, 0, 0, 0, 0, 0, 0, 0,
           dentry_start_sid,
           dentry_stream_sz,
           0
        ) * 2
    
    def __build_sat(self):
        # Build SAT
        book_sect_count = self.book_stream_len >> 9
        dir_sect_count  = len(self.dir_stream) >> 9
        
        total_sect_count     = book_sect_count + dir_sect_count
        SAT_sect_count       = 0
        MSAT_sect_count      = 0
        SAT_sect_count_limit = 109
        while total_sect_count > 128*SAT_sect_count or SAT_sect_count > SAT_sect_count_limit:
            SAT_sect_count   += 1
            total_sect_count += 1
            if SAT_sect_count > SAT_sect_count_limit:
                MSAT_sect_count      += 1
                total_sect_count     += 1
                SAT_sect_count_limit += 127


        SAT = [self.SID_FREE_SECTOR]*128*SAT_sect_count

        sect = 0
        while sect < book_sect_count - 1:
            self.book_stream_sect.append(sect)
            SAT[sect] = sect + 1
            sect += 1
        self.book_stream_sect.append(sect)
        SAT[sect] = self.SID_END_OF_CHAIN
        sect += 1

        while sect < book_sect_count + MSAT_sect_count:
            self.MSAT_sect_2nd.append(sect)
            SAT[sect] = self.SID_USED_BY_MSAT
            sect += 1

        while sect < book_sect_count + MSAT_sect_count + SAT_sect_count:
            self.SAT_sect.append(sect)            
            SAT[sect] = self.SID_USED_BY_SAT
            sect += 1

        while sect < book_sect_count + MSAT_sect_count + SAT_sect_count + dir_sect_count - 1:
            self.dir_stream_sect.append(sect)
            SAT[sect] = sect + 1
            sect += 1
        self.dir_stream_sect.append(sect)
        SAT[sect] = self.SID_END_OF_CHAIN
        sect += 1

        self.packed_SAT = struct.pack('<%dl' % (SAT_sect_count*128), *SAT)

        MSAT_1st = [self.SID_FREE_SECTOR]*109
        for i, SAT_sect_num in zip(range(0, 109), self.SAT_sect):
            MSAT_1st[i] = SAT_sect_num
        self.packed_MSAT_1st = struct.pack('<109l', *MSAT_1st)

        MSAT_2nd = [self.SID_FREE_SECTOR]*128*MSAT_sect_count
        if MSAT_sect_count > 0:
            MSAT_2nd[- 1] = self.SID_END_OF_CHAIN

        i = 109
        msat_sect = 0
        sid_num = 0
        while i < SAT_sect_count:
            if (sid_num + 1) % 128 == 0:
                #print 'link: ',
                msat_sect += 1
                if msat_sect < len(self.MSAT_sect_2nd):
                    MSAT_2nd[sid_num] = self.MSAT_sect_2nd[msat_sect]
            else:
                #print 'sid: ',
                MSAT_2nd[sid_num] = self.SAT_sect[i]
                i += 1
            #print sid_num, MSAT_2nd[sid_num]
            sid_num += 1

        self.packed_MSAT_2nd = struct.pack('<%dl' % (MSAT_sect_count*128), *MSAT_2nd)

        #print vars()
        #print zip(range(0, sect), SAT)
        #print self.book_stream_sect
        #print self.MSAT_sect_2nd
        #print MSAT_2nd
        #print self.SAT_sect
        #print self.dir_stream_sect


    def __build_header(self):
        doc_magic             = '\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1'
        file_uid              = '\x00'*16
        rev_num               = '\x3E\x00'
        ver_num               = '\x03\x00'
        byte_order            = '\xFE\xFF'
        log_sect_size         = struct.pack('<H', 9)
        log_short_sect_size   = struct.pack('<H', 6)
        not_used0             = '\x00'*10
        total_sat_sectors     = struct.pack('<L', len(self.SAT_sect))
        dir_start_sid         = struct.pack('<l', self.dir_stream_sect[0])
        not_used1             = '\x00'*4        
        min_stream_size       = struct.pack('<L', 0x1000)
        ssat_start_sid        = struct.pack('<l', -2)
        total_ssat_sectors    = struct.pack('<L', 0)

        if len(self.MSAT_sect_2nd) == 0:
            msat_start_sid        = struct.pack('<l', -2)
        else:
            msat_start_sid        = struct.pack('<l', self.MSAT_sect_2nd[0])

        total_msat_sectors    = struct.pack('<L', len(self.MSAT_sect_2nd))

        self.header =       ''.join([  doc_magic,
                                        file_uid,
                                        rev_num,
                                        ver_num,
                                        byte_order,
                                        log_sect_size,
                                        log_short_sect_size,
                                        not_used0,
                                        total_sat_sectors,
                                        dir_start_sid,
                                        not_used1,
                                        min_stream_size,
                                        ssat_start_sid,
                                        total_ssat_sectors,
                                        msat_start_sid,
                                        total_msat_sectors
                                    ])
                                        

    def save(self, filename, stream):
        # 1. Align stream on 0x1000 boundary (and therefore on sector boundary)
        padding = '\x00' * (0x1000 - (len(stream) % 0x1000))
        self.book_stream_len = len(stream) + len(padding)

        self.__build_directory()
        self.__build_sat()
        self.__build_header()
        
        f = file(filename, 'wb')
        f.write(self.header)
        f.write(self.packed_MSAT_1st)
        f.write(stream)
        f.write(padding)
        f.write(self.packed_MSAT_2nd)
        f.write(self.packed_SAT)
        f.write(self.dir_stream)
        f.close()


if __name__ == '__main__':
    d = XlsDoc()
    d.save('a.aaa', 'b'*17000)






########NEW FILE########
__FILENAME__ = Deco
# This code from 

# PEP: 318 
# Title: Decorators for Functions and Methods 
# Version: 1.35 
# Last-Modified: 2004/09/14 07:34:23 
# Author: Kevin D. Smith, Jim Jewett, Skip Montanaro, Anthony Baxter 

# This code fixes error in example 4: 
# authors' code contains
#       @accepts(int, (int,float))
#       @returns((int,float))
#       def func(arg1, arg2):
#           return arg1 * arg2
# 
# It should be:
#       @returns((int,float))
#       @accepts(int, (int,float))
#       def func(arg1, arg2):
#           return arg1 * arg2
# because of decorators are applied from bottom to up.


__rev_id__ = """$Id: Deco.py,v 1.4 2005/07/20 07:24:11 rvk Exp $"""


def accepts(*types):
    #print types
    def check_accepts(f):
        assert len(types) == f.func_code.co_argcount
        def new_f(*args, **kwds):
            for (a, t) in zip(args, types):
                assert isinstance(a, t), \
                       "arg %r does not match %s" % (a,t)
            return f(*args, **kwds)
        new_f.func_name = f.func_name
        return new_f
    return check_accepts

def returns(rtype):
    def check_returns(f):
        def new_f(*args, **kwds):
            result = f(*args, **kwds)
            assert isinstance(result, rtype), \
                   "return value %r does not match %s" % (result,rtype)
            return result
        new_f.func_name = f.func_name
        return new_f
    return check_returns


if __name__ == '__main__':
    import types 

    @returns(types.NoneType)
    @accepts(int, (int,float))
    def func(arg1, arg2):
        #return str(arg1 * arg2)
        pass

    func(1, 2)      

########NEW FILE########
__FILENAME__ = ExcelFormula
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


__rev_id__ = """$Id: ExcelFormula.py,v 1.3 2005/08/11 08:53:48 rvk Exp $"""


import ExcelFormulaParser, ExcelFormulaLexer
import struct
from antlr import ANTLRException


class Formula(object):
    __slots__ = ["__init__", "text", "rpn", "__s", "__parser"]


    def __init__(self, s):
        try:
            self.__s = s
            lexer = ExcelFormulaLexer.Lexer(s)
            self.__parser = ExcelFormulaParser.Parser(lexer)
            self.__parser.formula()
        except ANTLRException:
            raise Exception, "can't parse formula " + s

    def text(self):
        return self.__s

    def rpn(self):
        '''
        Offset    Size    Contents
        0         2       Size of the following formula data (sz)
        2         sz      Formula data (RPN token array)
        [2+sz]    var.    (optional) Additional data for specific tokens

        '''
        return struct.pack("<H", len(self.__parser.rpn)) + self.__parser.rpn


########NEW FILE########
__FILENAME__ = ExcelFormulaLexer
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
#
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
#
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
#
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
#
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
#
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


__rev_id__ = """$Id: ExcelFormulaLexer.py,v 1.4 2005/08/14 06:40:23 rvk Exp $"""


import sys
from antlr import EOF, CommonToken as Tok, TokenStream, TokenStreamException
import struct
import ExcelFormulaParser
from re import compile as recompile, match, LOCALE, UNICODE, IGNORECASE


int_const_pattern = recompile(r"\d+")
flt_const_pattern = recompile(r"\d*\.\d+(?:[Ee][+-]?\d+)?")
str_const_pattern = recompile(r'["][^"]*["]')
#range2d_pattern   = recompile(r"\$?[A-I]?[A-Z]\$?\d+:\$?[A-I]?[A-Z]\$?\d+")
ref2d_pattern     = recompile(r"\$?[A-I]?[A-Z]\$?\d+")
true_pattern      = recompile(r"TRUE", IGNORECASE)
false_pattern     = recompile(r"FALSE", IGNORECASE)
name_pattern      = recompile(r"[\.\w]+", LOCALE)

pattern_type_tuples = (
    (flt_const_pattern, ExcelFormulaParser.NUM_CONST),
    (int_const_pattern, ExcelFormulaParser.INT_CONST),
    (str_const_pattern, ExcelFormulaParser.STR_CONST),
#    (range2d_pattern  , ExcelFormulaParser.RANGE2D),
    (ref2d_pattern    , ExcelFormulaParser.REF2D),
    (true_pattern     , ExcelFormulaParser.TRUE_CONST),
    (false_pattern    , ExcelFormulaParser.FALSE_CONST),
    (name_pattern     , ExcelFormulaParser.NAME)
)


type_text_tuples = (
    (ExcelFormulaParser.NE, '<>'),
    (ExcelFormulaParser.LE, '<='),
    (ExcelFormulaParser.GE, '>='),
    (ExcelFormulaParser.EQ, '='),
    (ExcelFormulaParser.LT, '<'),
    (ExcelFormulaParser.GT, '>'),
    (ExcelFormulaParser.ADD, '+'),
    (ExcelFormulaParser.SUB, '-'),
    (ExcelFormulaParser.MUL, '*'),
    (ExcelFormulaParser.DIV, '/'),
    (ExcelFormulaParser.COLON, ':'),
    (ExcelFormulaParser.SEMICOLON, ';'),
    (ExcelFormulaParser.COMMA, ','),
    (ExcelFormulaParser.LP, '('),
    (ExcelFormulaParser.RP, ')'),
    (ExcelFormulaParser.CONCAT, '&'),
    (ExcelFormulaParser.PERCENT, '%'),
    (ExcelFormulaParser.POWER, '^')
)


class Lexer(TokenStream):
    def __init__(self, text):
        self._text = text[:]
        self._pos = 0
        self._line = 0


    def isEOF(self):
        return len(self._text) <= self._pos


    def curr_ch(self):
        return self._text[self._pos]


    def next_ch(self, n = 1):
        self._pos += n


    def is_whitespace(self):
        return self.curr_ch() in " \t\n\r\f\v"


    def match_pattern(self, pattern, toktype):
        m = pattern.match(self._text[self._pos:])
        if m:
            start_pos = self._pos + m.start(0)
            end_pos = self._pos + m.end(0)
            tt = self._text[start_pos:end_pos]
            self._pos = end_pos
            return Tok(type = toktype, text = tt, col = start_pos + 1)
        else:
            return None


    def nextToken(self):
        # skip whitespace
        while not self.isEOF() and self.is_whitespace():
            self.next_ch()
        if self.isEOF():
            return Tok(type = EOF)
        # first, try to match token with more chars
        for ptt in pattern_type_tuples:
            t = self.match_pattern(*ptt);
            if t:
                return t
        # second, we want find short tokens
        for ty, te in type_text_tuples:
            if self.curr_ch() == te:
                self.next_ch()
                return Tok(type = ty, text = te, col = self._pos)
        # at this point, smth strange is happened
        raise TokenStreamException("Unknown char %s at %u col." % (self.curr_ch(), self._pos))


if __name__ == '__main__' :
    import locale
    locale.setlocale(locale.LC_ALL, 'russian')
    try:
        for t in Lexer('1+2+3+67.8678 + " @##$$$ klhkh kljhklhkl " + .58e-678*A1:B4 - 1lkjljlkjl3535'):
            print t
    except TokenStreamException, e:
        print "error:", e

########NEW FILE########
__FILENAME__ = ExcelFormulaParser
### $ANTLR 2.7.5 (20050128): "excel-formula.g" -> "ExcelFormulaParser.py"$
### import antlr and other modules ..
import sys
import antlr

version = sys.version.split()[0]
if version < '2.2.1':
    False = 0
if version < '2.3':
    True = not False
### header action >>> 
__rev_id__ = """$Id: ExcelFormulaParser.py,v 1.4 2005/08/14 06:40:23 rvk Exp $"""

import struct
import Utils
from UnicodeUtils import upack1
from ExcelMagic import *
### header action <<< 
### preamble action>>>

### preamble action <<<

### import antlr.Token 
from antlr import Token
### >>>The Known Token Types <<<
SKIP                = antlr.SKIP
INVALID_TYPE        = antlr.INVALID_TYPE
EOF_TYPE            = antlr.EOF_TYPE
EOF                 = antlr.EOF
NULL_TREE_LOOKAHEAD = antlr.NULL_TREE_LOOKAHEAD
MIN_USER_TYPE       = antlr.MIN_USER_TYPE
TRUE_CONST = 4
FALSE_CONST = 5
STR_CONST = 6
NUM_CONST = 7
INT_CONST = 8
NAME = 9
EQ = 10
NE = 11
GT = 12
LT = 13
GE = 14
LE = 15
ADD = 16
SUB = 17
MUL = 18
DIV = 19
POWER = 20
PERCENT = 21
LP = 22
RP = 23
LB = 24
RB = 25
COLON = 26
COMMA = 27
SEMICOLON = 28
CONCAT = 29
REF2D = 30

class Parser(antlr.LLkParser):
    ### user action >>>
    ### user action <<<
    
    def __init__(self, *args, **kwargs):
        antlr.LLkParser.__init__(self, *args, **kwargs)
        self.tokenNames = _tokenNames
        ### __init__ header action >>> 
        self.rpn = ""
        ### __init__ header action <<< 
        
    def formula(self):    
        
        pass
        self.expr("V")
    
    def expr(self,
        arg_type
    ):    
        
        pass
        self.prec0_expr(arg_type)
        while True:
            if ((self.LA(1) >= EQ and self.LA(1) <= LE)):
                pass
                la1 = self.LA(1)
                if False:
                    pass
                elif la1 and la1 in [EQ]:
                    pass
                    self.match(EQ)
                    op = struct.pack('B', ptgEQ)
                elif la1 and la1 in [NE]:
                    pass
                    self.match(NE)
                    op = struct.pack('B', ptgNE)
                elif la1 and la1 in [GT]:
                    pass
                    self.match(GT)
                    op = struct.pack('B', ptgGE)
                elif la1 and la1 in [LT]:
                    pass
                    self.match(LT)
                    op = struct.pack('B', ptgLT)
                elif la1 and la1 in [GE]:
                    pass
                    self.match(GE)
                    op = struct.pack('B', ptgGE)
                elif la1 and la1 in [LE]:
                    pass
                    self.match(LE)
                    op = struct.pack('B', ptgLE)
                else:
                        raise antlr.NoViableAltException(self.LT(1), self.getFilename())
                    
                self.prec0_expr(arg_type)
                self.rpn += op
            else:
                break
            
    
    def prec0_expr(self,
        arg_type
    ):    
        
        pass
        self.prec1_expr(arg_type)
        while True:
            if (self.LA(1)==CONCAT):
                pass
                pass
                self.match(CONCAT)
                op = struct.pack('B', ptgConcat)
                self.prec1_expr(arg_type)
                self.rpn += op
            else:
                break
            
    
    def prec1_expr(self,
        arg_type
    ):    
        
        pass
        self.prec2_expr(arg_type)
        while True:
            if (self.LA(1)==ADD or self.LA(1)==SUB):
                pass
                la1 = self.LA(1)
                if False:
                    pass
                elif la1 and la1 in [ADD]:
                    pass
                    self.match(ADD)
                    op = struct.pack('B', ptgAdd)
                elif la1 and la1 in [SUB]:
                    pass
                    self.match(SUB)
                    op = struct.pack('B', ptgSub)
                else:
                        raise antlr.NoViableAltException(self.LT(1), self.getFilename())
                    
                self.prec2_expr(arg_type)
                self.rpn += op
            else:
                break
            
    
    def prec2_expr(self,
        arg_type
    ):    
        
        pass
        self.prec3_expr(arg_type)
        while True:
            if (self.LA(1)==MUL or self.LA(1)==DIV):
                pass
                la1 = self.LA(1)
                if False:
                    pass
                elif la1 and la1 in [MUL]:
                    pass
                    self.match(MUL)
                    op = struct.pack('B', ptgMul)
                elif la1 and la1 in [DIV]:
                    pass
                    self.match(DIV)
                    op = struct.pack('B', ptgDiv)
                else:
                        raise antlr.NoViableAltException(self.LT(1), self.getFilename())
                    
                self.prec3_expr(arg_type)
                self.rpn += op
            else:
                break
            
    
    def prec3_expr(self,
        arg_type
    ):    
        
        pass
        self.prec4_expr(arg_type)
        while True:
            if (self.LA(1)==POWER):
                pass
                pass
                self.match(POWER)
                op = struct.pack('B', ptgPower)
                self.prec4_expr(arg_type)
                self.rpn += op
            else:
                break
            
    
    def prec4_expr(self,
        arg_type
    ):    
        
        pass
        self.prec5_expr(arg_type)
        la1 = self.LA(1)
        if False:
            pass
        elif la1 and la1 in [PERCENT]:
            pass
            self.match(PERCENT)
            self.rpn += struct.pack('B', ptgPercent)
        elif la1 and la1 in [EOF,EQ,NE,GT,LT,GE,LE,ADD,SUB,MUL,DIV,POWER,RP,SEMICOLON,CONCAT]:
            pass
        else:
                raise antlr.NoViableAltException(self.LT(1), self.getFilename())
            
    
    def prec5_expr(self,
        arg_type
    ):    
        
        la1 = self.LA(1)
        if False:
            pass
        elif la1 and la1 in [TRUE_CONST,FALSE_CONST,STR_CONST,NUM_CONST,INT_CONST,NAME,LP,REF2D]:
            pass
            self.primary(arg_type)
        elif la1 and la1 in [SUB]:
            pass
            self.match(SUB)
            self.primary(arg_type)
            self.rpn += struct.pack('B', ptgUminus)
        else:
                raise antlr.NoViableAltException(self.LT(1), self.getFilename())
            
    
    def primary(self,
        arg_type
    ):    
        
        str_tok = None
        int_tok = None
        num_tok = None
        ref2d_tok = None
        ref2d1_tok = None
        ref2d2_tok = None
        name_tok = None
        func_tok = None
        la1 = self.LA(1)
        if False:
            pass
        elif la1 and la1 in [TRUE_CONST]:
            pass
            self.match(TRUE_CONST)
            self.rpn += struct.pack("2B", ptgBool, 1)
        elif la1 and la1 in [FALSE_CONST]:
            pass
            self.match(FALSE_CONST)
            self.rpn += struct.pack("2B", ptgBool, 0)
        elif la1 and la1 in [STR_CONST]:
            pass
            str_tok = self.LT(1)
            self.match(STR_CONST)
            self.rpn += struct.pack("B", ptgStr) + upack1(str_tok.text[1:-1])
        elif la1 and la1 in [INT_CONST]:
            pass
            int_tok = self.LT(1)
            self.match(INT_CONST)
            self.rpn += struct.pack("<BH", ptgInt, int(int_tok.text))
        elif la1 and la1 in [NUM_CONST]:
            pass
            num_tok = self.LT(1)
            self.match(NUM_CONST)
            self.rpn += struct.pack("<Bd", ptgNum, float(num_tok.text))
        elif la1 and la1 in [LP]:
            pass
            self.match(LP)
            self.expr(arg_type)
            self.match(RP)
            self.rpn += struct.pack("B", ptgParen)
        else:
            if (self.LA(1)==REF2D) and (_tokenSet_0.member(self.LA(2))):
                pass
                ref2d_tok = self.LT(1)
                self.match(REF2D)
                r, c = Utils.cell_to_packed_rowcol(ref2d_tok.text)
                if arg_type == "R":
                   self.rpn += struct.pack("<B2H", ptgRefR, r, c)
                else:
                   self.rpn += struct.pack("<B2H", ptgRefV, r, c)
            elif (self.LA(1)==REF2D) and (self.LA(2)==COLON):
                pass
                ref2d1_tok = self.LT(1)
                self.match(REF2D)
                self.match(COLON)
                ref2d2_tok = self.LT(1)
                self.match(REF2D)
                r1, c1 = Utils.cell_to_packed_rowcol(ref2d1_tok.text)
                r2, c2 = Utils.cell_to_packed_rowcol(ref2d2_tok.text)
                if arg_type == "R":
                   self.rpn += struct.pack("<B4H", ptgAreaR, r1, r2, c1, c2)
                else:
                   self.rpn += struct.pack("<B4H", ptgAreaV, r1, r2, c1, c2)
            elif (self.LA(1)==NAME) and (_tokenSet_0.member(self.LA(2))):
                pass
                name_tok = self.LT(1)
                self.match(NAME)
                self.rpn += ""
            elif (self.LA(1)==NAME) and (self.LA(2)==LP):
                pass
                func_tok = self.LT(1)
                self.match(NAME)
                if func_tok.text.upper() in std_func_by_name:
                   (opcode,
                   min_argc,
                   max_argc,
                   func_type,
                   arg_type_list,
                   volatile_func) = std_func_by_name[func_tok.text.upper()]
                else:
                   raise Exception, "unknown function: %s" % func_tok.text
                self.match(LP)
                arg_count=self.expr_list(arg_type_list, min_argc, max_argc)
                self.match(RP)
                if arg_count > max_argc or arg_count < min_argc:
                   raise Exception, "%d parameters for function: %s" % (arg_count, func_tok.text)
                if min_argc == max_argc:
                   if func_type == "V":
                       func_ptg = ptgFuncV
                   elif func_type == "R":
                       func_ptg = ptgFuncR
                   elif func_type == "A":
                       func_ptg = ptgFuncA
                   else:
                       raise Exception, "wrong function type"
                   self.rpn += struct.pack("<BH", func_ptg, opcode)
                else:
                   if func_type == "V":
                       func_ptg = ptgFuncVarV
                   elif func_type == "R":
                       func_ptg = ptgFuncVarR
                   elif func_type == "A":
                       func_ptg = ptgFuncVarA
                   else:
                       raise Exception, "wrong function type"
                   self.rpn += struct.pack("<2BH", func_ptg, arg_count, opcode)
            else:
                raise antlr.NoViableAltException(self.LT(1), self.getFilename())
            
    
    def expr_list(self,
        arg_type_list, min_argc, max_argc
    ):    
        arg_cnt = None
        
        arg_cnt = 0
        arg_type_list = arg_type_list.split()
        arg_type = arg_type_list[arg_cnt]
        la1 = self.LA(1)
        if False:
            pass
        elif la1 and la1 in [TRUE_CONST,FALSE_CONST,STR_CONST,NUM_CONST,INT_CONST,NAME,SUB,LP,REF2D]:
            pass
            self.expr(arg_type)
            arg_cnt += 1
            while True:
                if (self.LA(1)==SEMICOLON):
                    pass
                    if arg_cnt < len(arg_type_list):
                       arg_type = arg_type_list[arg_cnt]
                    else:
                       arg_type = arg_type_list[-1]
                    if arg_type == "...":
                       arg_type = arg_type_list[-2]
                    self.match(SEMICOLON)
                    la1 = self.LA(1)
                    if False:
                        pass
                    elif la1 and la1 in [TRUE_CONST,FALSE_CONST,STR_CONST,NUM_CONST,INT_CONST,NAME,SUB,LP,REF2D]:
                        pass
                        self.expr(arg_type)
                    elif la1 and la1 in [RP,SEMICOLON]:
                        pass
                        self.rpn += struct.pack("B", ptgMissArg)
                    else:
                            raise antlr.NoViableAltException(self.LT(1), self.getFilename())
                        
                    arg_cnt += 1
                else:
                    break
                
        elif la1 and la1 in [RP]:
            pass
        else:
                raise antlr.NoViableAltException(self.LT(1), self.getFilename())
            
        return arg_cnt
    

_tokenNames = [
    "<0>", 
    "EOF", 
    "<2>", 
    "NULL_TREE_LOOKAHEAD", 
    "TRUE_CONST", 
    "FALSE_CONST", 
    "STR_CONST", 
    "NUM_CONST", 
    "INT_CONST", 
    "NAME", 
    "EQ", 
    "NE", 
    "GT", 
    "LT", 
    "GE", 
    "LE", 
    "ADD", 
    "SUB", 
    "MUL", 
    "DIV", 
    "POWER", 
    "PERCENT", 
    "LP", 
    "RP", 
    "LB", 
    "RB", 
    "COLON", 
    "COMMA", 
    "SEMICOLON", 
    "CONCAT", 
    "REF2D"
]
    

### generate bit set
def mk_tokenSet_0(): 
    ### var1
    data = [ 817888258L, 0L]
    return data
_tokenSet_0 = antlr.BitSet(mk_tokenSet_0())
    

########NEW FILE########
__FILENAME__ = ExcelMagic
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
#
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
#
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
#
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
#
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
#
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


__rev_id__ = """$Id: ExcelMagic.py,v 1.1 2005/08/11 08:53:48 rvk Exp $"""


"""
lots of Excel Magic Numbers
"""

# Boundaries BIFF8+

MAX_ROW = 65536
MAX_COL = 256


biff_records = {
    0x0000: "DIMENSIONS",
    0x0001: "BLANK",
    0x0002: "INTEGER",
    0x0003: "NUMBER",
    0x0004: "LABEL",
    0x0005: "BOOLERR",
    0x0006: "FORMULA",
    0x0007: "STRING",
    0x0008: "ROW",
    0x0009: "BOF",
    0x000A: "EOF",
    0x000B: "INDEX",
    0x000C: "CALCCOUNT",
    0x000D: "CALCMODE",
    0x000E: "PRECISION",
    0x000F: "REFMODE",
    0x0010: "DELTA",
    0x0011: "ITERATION",
    0x0012: "PROTECT",
    0x0013: "PASSWORD",
    0x0014: "HEADER",
    0x0015: "FOOTER",
    0x0016: "EXTERNCOUNT",
    0x0017: "EXTERNSHEET",
    0x0018: "NAME",
    0x0019: "WINDOWPROTECT",
    0x001A: "VERTICALPAGEBREAKS",
    0x001B: "HORIZONTALPAGEBREAKS",
    0x001C: "NOTE",
    0x001D: "SELECTION",
    0x001E: "FORMAT",
    0x001F: "FORMATCOUNT",
    0x0020: "COLUMNDEFAULT",
    0x0021: "ARRAY",
    0x0022: "1904",
    0x0023: "EXTERNNAME",
    0x0024: "COLWIDTH",
    0x0025: "DEFAULTROWHEIGHT",
    0x0026: "LEFTMARGIN",
    0x0027: "RIGHTMARGIN",
    0x0028: "TOPMARGIN",
    0x0029: "BOTTOMMARGIN",
    0x002A: "PRINTHEADERS",
    0x002B: "PRINTGRIDLINES",
    0x002F: "FILEPASS",
    0x0031: "FONT",
    0x0036: "TABLE",
    0x003C: "CONTINUE",
    0x003D: "WINDOW1",
    0x003E: "WINDOW2",
    0x0040: "BACKUP",
    0x0041: "PANE",
    0x0042: "CODEPAGE",
    0x0043: "XF",
    0x0044: "IXFE",
    0x0045: "EFONT",
    0x004D: "PLS",
    0x0050: "DCON",
    0x0051: "DCONREF",
    0x0053: "DCONNAME",
    0x0055: "DEFCOLWIDTH",
    0x0056: "BUILTINFMTCNT",
    0x0059: "XCT",
    0x005A: "CRN",
    0x005B: "FILESHARING",
    0x005C: "WRITEACCESS",
    0x005D: "OBJ",
    0x005E: "UNCALCED",
    0x005F: "SAFERECALC",
    0x0060: "TEMPLATE",
    0x0063: "OBJPROTECT",
    0x007D: "COLINFO",
    0x007E: "RK",
    0x007F: "IMDATA",
    0x0080: "GUTS",
    0x0081: "WSBOOL",
    0x0082: "GRIDSET",
    0x0083: "HCENTER",
    0x0084: "VCENTER",
    0x0085: "BOUNDSHEET",
    0x0086: "WRITEPROT",
    0x0087: "ADDIN",
    0x0088: "EDG",
    0x0089: "PUB",
    0x008C: "COUNTRY",
    0x008D: "HIDEOBJ",
    0x008E: "BUNDLESOFFSET",
    0x008F: "BUNDLEHEADER",
    0x0090: "SORT",
    0x0091: "SUB",
    0x0092: "PALETTE",
    0x0093: "STYLE",
    0x0094: "LHRECORD",
    0x0095: "LHNGRAPH",
    0x0096: "SOUND",
    0x0098: "LPR",
    0x0099: "STANDARDWIDTH",
    0x009A: "FNGROUPNAME",
    0x009B: "FILTERMODE",
    0x009C: "FNGROUPCOUNT",
    0x009D: "AUTOFILTERINFO",
    0x009E: "AUTOFILTER",
    0x00A0: "SCL",
    0x00A1: "SETUP",
    0x00A9: "COORDLIST",
    0x00AB: "GCW",
    0x00AE: "SCENMAN",
    0x00AF: "SCENARIO",
    0x00B0: "SXVIEW",
    0x00B1: "SXVD",
    0x00B2: "SXVI",
    0x00B4: "SXIVD",
    0x00B5: "SXLI",
    0x00B6: "SXPI",
    0x00B8: "DOCROUTE",
    0x00B9: "RECIPNAME",
    0x00BC: "SHRFMLA",
    0x00BD: "MULRK",
    0x00BE: "MULBLANK",
    0x00C1: "MMS",
    0x00C2: "ADDMENU",
    0x00C3: "DELMENU",
    0x00C5: "SXDI",
    0x00C6: "SXDB",
    0x00C7: "SXFIELD",
    0x00C8: "SXINDEXLIST",
    0x00C9: "SXDOUBLE",
    0x00CD: "SXSTRING",
    0x00CE: "SXDATETIME",
    0x00D0: "SXTBL",
    0x00D1: "SXTBRGITEM",
    0x00D2: "SXTBPG",
    0x00D3: "OBPROJ",
    0x00D5: "SXIDSTM",
    0x00D6: "RSTRING",
    0x00D7: "DBCELL",
    0x00DA: "BOOKBOOL",
    0x00DC: "SXEXT|PARAMQRY",
    0x00DD: "SCENPROTECT",
    0x00DE: "OLESIZE",
    0x00DF: "UDDESC",
    0x00E0: "XF",
    0x00E1: "INTERFACEHDR",
    0x00E2: "INTERFACEEND",
    0x00E3: "SXVS",
    0x00E5: "MERGEDCELLS",
    0x00E9: "BITMAP",
    0x00EB: "MSODRAWINGGROUP",
    0x00EC: "MSODRAWING",
    0x00ED: "MSODRAWINGSELECTION",
    0x00F0: "SXRULE",
    0x00F1: "SXEX",
    0x00F2: "SXFILT",
    0x00F6: "SXNAME",
    0x00F7: "SXSELECT",
    0x00F8: "SXPAIR",
    0x00F9: "SXFMLA",
    0x00FB: "SXFORMAT",
    0x00FC: "SST",
    0x00FD: "LABELSST",
    0x00FF: "EXTSST",
    0x0100: "SXVDEX",
    0x0103: "SXFORMULA",
    0x0122: "SXDBEX",
    0x0137: "CHTRINSERT",
    0x0138: "CHTRINFO",
    0x013B: "CHTRCELLCONTENT",
    0x013D: "TABID",
    0x0140: "CHTRMOVERANGE",
    0x014D: "CHTRINSERTTAB",
    0x015F: "LABELRANGES",
    0x0160: "USESELFS",
    0x0161: "DSF",
    0x0162: "XL5MODIFY",
    0x0196: "CHTRHEADER",
    0x01A9: "USERBVIEW",
    0x01AA: "USERSVIEWBEGIN",
    0x01AB: "USERSVIEWEND",
    0x01AD: "QSI",
    0x01AE: "SUPBOOK",
    0x01AF: "PROT4REV",
    0x01B0: "CONDFMT",
    0x01B1: "CF",
    0x01B2: "DVAL",
    0x01B5: "DCONBIN",
    0x01B6: "TXO",
    0x01B7: "REFRESHALL",
    0x01B8: "HLINK",
    0x01BA: "CODENAME",
    0x01BB: "SXFDBTYPE",
    0x01BC: "PROT4REVPASS",
    0x01BE: "DV",
    0x01C0: "XL9FILE",
    0x01C1: "RECALCID",
    0x0200: "DIMENSIONS",
    0x0201: "BLANK",
    0x0203: "NUMBER",
    0x0204: "LABEL",
    0x0205: "BOOLERR",
    0x0206: "FORMULA",
    0x0207: "STRING",
    0x0208: "ROW",
    0x0209: "BOF",
    0x020B: "INDEX",
    0x0218: "NAME",
    0x0221: "ARRAY",
    0x0223: "EXTERNNAME",
    0x0225: "DEFAULTROWHEIGHT",
    0x0231: "FONT",
    0x0236: "TABLE",
    0x023E: "WINDOW2",
    0x0243: "XF",
    0x027E: "RK",
    0x0293: "STYLE",
    0x0406: "FORMULA",
    0x0409: "BOF",
    0x041E: "FORMAT",
    0x0443: "XF",
    0x04BC: "SHRFMLA",
    0x0800: "SCREENTIP",
    0x0803: "WEBQRYSETTINGS",
    0x0804: "WEBQRYTABLES",
    0x0809: "BOF",
    0x0862: "SHEETLAYOUT",
    0x0867: "SHEETPROTECTION",
    0x1001: "UNITS",
    0x1002: "ChartChart",
    0x1003: "ChartSeries",
    0x1006: "ChartDataformat",
    0x1007: "ChartLineformat",
    0x1009: "ChartMarkerformat",
    0x100A: "ChartAreaformat",
    0x100B: "ChartPieformat",
    0x100C: "ChartAttachedlabel",
    0x100D: "ChartSeriestext",
    0x1014: "ChartChartformat",
    0x1015: "ChartLegend",
    0x1016: "ChartSerieslist",
    0x1017: "ChartBar",
    0x1018: "ChartLine",
    0x1019: "ChartPie",
    0x101A: "ChartArea",
    0x101B: "ChartScatter",
    0x101C: "ChartChartline",
    0x101D: "ChartAxis",
    0x101E: "ChartTick",
    0x101F: "ChartValuerange",
    0x1020: "ChartCatserrange",
    0x1021: "ChartAxislineformat",
    0x1022: "ChartFormatlink",
    0x1024: "ChartDefaulttext",
    0x1025: "ChartText",
    0x1026: "ChartFontx",
    0x1027: "ChartObjectLink",
    0x1032: "ChartFrame",
    0x1033: "BEGIN",
    0x1034: "END",
    0x1035: "ChartPlotarea",
    0x103A: "Chart3D",
    0x103C: "ChartPicf",
    0x103D: "ChartDropbar",
    0x103E: "ChartRadar",
    0x103F: "ChartSurface",
    0x1040: "ChartRadararea",
    0x1041: "ChartAxisparent",
    0x1043: "ChartLegendxn",
    0x1044: "ChartShtprops",
    0x1045: "ChartSertocrt",
    0x1046: "ChartAxesused",
    0x1048: "ChartSbaseref",
    0x104A: "ChartSerparent",
    0x104B: "ChartSerauxtrend",
    0x104E: "ChartIfmt",
    0x104F: "ChartPos",
    0x1050: "ChartAlruns",
    0x1051: "ChartAI",
    0x105B: "ChartSerauxerrbar",
    0x105D: "ChartSerfmt",
    0x105F: "Chart3DDataFormat",
    0x1060: "ChartFbi",
    0x1061: "ChartBoppop",
    0x1062: "ChartAxcext",
    0x1063: "ChartDat",
    0x1064: "ChartPlotgrowth",
    0x1065: "ChartSiindex",
    0x1066: "ChartGelframe",
    0x1067: "ChartBoppcustom",
    0xFFFF: ""
}


std_func_by_name = {
             "ABS": (0x018,  1,  1,   "V",             "V", False), # 1
            "ACOS": (0x063,  1,  1,   "V",             "V", False), # 2
           "ACOSH": (0x0e9,  1,  1,   "V",             "V", False), # 3
         "ADDRESS": (0x0db,  2,  5,   "V",     "V V V V V", False), # 4
             "AND": (0x024,  1, 30,   "V",         "R ...", False), # 5
          "ARCTAN": (0x012,  1,  1,   "V",             "V", False), # 6
           "AREAS": (0x04b,  1,  1,   "V",             "R", False), # 7
             "ASC": (0x0d6,  1,  1,   "V",             "V", False), # 8
            "ASIN": (0x062,  1,  1,   "V",             "V", False), # 9
           "ASINH": (0x0e8,  1,  1,   "V",             "V", False), # 10
           "ATAN2": (0x061,  2,  2,   "V",           "V V", False), # 11
           "ATANH": (0x0ea,  1,  1,   "V",             "V", False), # 12
          "AVEDEV": (0x10d,  1, 30,   "V",         "R ...", False), # 13
         "AVERAGE": (0x005,  1, 30,   "V",         "R ...", False), # 14
        "AVERAGEA": (0x169,  1, 30,   "V",         "R ...", False), # 15
        "BETADIST": (0x10e,  3,  5,   "V",     "V V V V V", False), # 16
         "BETAINV": (0x110,  3,  5,   "V",     "V V V V V", False), # 17
       "BINOMDIST": (0x111,  4,  4,   "V",       "V V V V", False), # 18
         "CEILING": (0x120,  2,  2,   "V",           "V V", False), # 19
            "CELL": (0x07d,  1,  2,   "V",           "V R",  True), # 20
            "CHAR": (0x06f,  1,  1,   "V",             "V", False), # 21
         "CHIDIST": (0x112,  2,  2,   "V",           "V V", False), # 22
          "CHIINV": (0x113,  2,  2,   "V",           "V V", False), # 23
         "CHITEST": (0x132,  2,  2,   "V",           "A A", False), # 24
          "CHOOSE": (0x064,  2, 30,   "R",       "V R ...", False), # 25
           "CLEAN": (0x0a2,  1,  1,   "V",             "V", False), # 26
            "CODE": (0x079,  1,  1,   "V",             "V", False), # 27
          "COLUMN": (0x009,  0,  1,   "V",             "R", False), # 28
         "COLUMNS": (0x04d,  1,  1,   "V",             "R", False), # 29
          "COMBIN": (0x114,  2,  2,   "V",           "V V", False), # 30
     "CONCATENATE": (0x150,  0, 30,   "V",         "V ...", False), # 31
      "CONFIDENCE": (0x115,  3,  3,   "V",         "V V V", False), # 32
          "CORREL": (0x133,  2,  2,   "V",           "A A", False), # 33
             "COS": (0x010,  1,  1,   "V",             "V", False), # 34
            "COSH": (0x0e6,  1,  1,   "V",             "V", False), # 35
           "COUNT": (0x000,  0, 30,   "V",         "R ...", False), # 36
          "COUNTA": (0x0a9,  0, 30,   "V",         "R ...", False), # 37
      "COUNTBLANK": (0x15b,  1,  1,   "V",             "R", False), # 38
         "COUNTIF": (0x15a,  2,  2,   "V",           "R V", False), # 39
           "COVAR": (0x134,  2,  2,   "V",           "A A", False), # 40
       "CRITBINOM": (0x116,  3,  3,   "V",         "V V V", False), # 41
            "DATE": (0x041,  3,  3,   "V",         "V V V", False), # 42
         "DATEDIF": (0x15f,  3,  3,   "V",         "V V V", False), # 43
      "DATESTRING": (0x160,  1,  1,   "V",             "V", False), # 44
       "DATEVALUE": (0x08c,  1,  1,   "V",             "V", False), # 45
        "DAVERAGE": (0x02a,  3,  3,   "V",         "R R R", False), # 46
             "DAY": (0x043,  1,  1,   "V",             "V", False), # 47
         "DAYS360": (0x0dc,  2,  3,   "V",         "V V V", False), # 48
              "DB": (0x0f7,  4,  5,   "V",     "V V V V V", False), # 49
            "DBSC": (0x0d7,  1,  1,   "V",             "V", False), # 50
          "DCOUNT": (0x028,  3,  3,   "V",         "R R R", False), # 51
         "DCOUNTA": (0x0c7,  3,  3,   "V",         "R R R", False), # 52
             "DDB": (0x090,  4,  5,   "V",     "V V V V V", False), # 53
         "DEGREES": (0x157,  1,  1,   "V",             "V", False), # 54
           "DEVSQ": (0x13e,  1, 30,   "V",         "R ...", False), # 55
            "DGET": (0x0eb,  3,  3,   "V",         "R R R", False), # 56
            "DMAX": (0x02c,  3,  3,   "V",         "R R R", False), # 57
            "DMIN": (0x02b,  3,  3,   "V",         "R R R", False), # 58
          "DOLLAR": (0x00d,  1,  2,   "V",           "V V", False), # 59
        "DPRODUCT": (0x0bf,  3,  3,   "V",         "R R R", False), # 60
          "DSTDEV": (0x02d,  3,  3,   "V",         "R R R", False), # 61
         "DSTDEVP": (0x0c3,  3,  3,   "V",         "R R R", False), # 62
            "DSUM": (0x029,  3,  3,   "V",         "R R R", False), # 63
            "DVAR": (0x02f,  3,  3,   "V",         "R R R", False), # 64
           "DVARP": (0x0c4,  3,  3,   "V",         "R R R", False), # 65
      "ERROR.TYPE": (0x105,  1,  1,   "V",             "V", False), # 66
            "EVEN": (0x117,  1,  1,   "V",             "V", False), # 67
           "EXACT": (0x075,  2,  2,   "V",           "V V", False), # 68
             "EXP": (0x015,  1,  1,   "V",             "V", False), # 69
       "EXPONDIST": (0x118,  3,  3,   "V",         "V V V", False), # 70
            "FACT": (0x0b8,  1,  1,   "V",             "V", False), # 71
           "FALSE": (0x023,  0,  0,   "V",             "-", False), # 72
           "FDIST": (0x119,  3,  3,   "V",         "V V V", False), # 73
            "FIND": (0x07c,  2,  3,   "V",         "V V V", False), # 74
           "FINDB": (0x0cd,  2,  3,   "V",         "V V V", False), # 75
            "FINV": (0x11a,  3,  3,   "V",         "V V V", False), # 76
          "FISHER": (0x11b,  1,  1,   "V",             "V", False), # 77
       "FISHERINV": (0x11c,  1,  1,   "V",             "V", False), # 78
           "FIXED": (0x00e,  2,  3,   "V",         "V V V", False), # 79
           "FLOOR": (0x11d,  2,  2,   "V",           "V V", False), # 80
        "FORECAST": (0x135,  3,  3,   "V",         "V A A", False), # 81
       "FREQUENCY": (0x0fc,  2,  2,   "A",           "R R", False), # 82
           "FTEST": (0x136,  2,  2,   "V",           "A A", False), # 83
              "FV": (0x039,  3,  5,   "V",     "V V V V V", False), # 84
       "GAMMADIST": (0x11e,  4,  4,   "V",       "V V V V", False), # 85
        "GAMMAINV": (0x11f,  3,  3,   "V",         "V V V", False), # 86
         "GAMMALN": (0x10f,  1,  1,   "V",             "V", False), # 87
         "GEOMEAN": (0x13f,  1, 30,   "V",         "R ...", False), # 88
    "GETPIVOTDATA": (0x166,  2, 30,   "A",             "-", False), # 89
          "GROWTH": (0x034,  1,  4,   "A",       "R R R V", False), # 90
         "HARMEAN": (0x140,  1, 30,   "V",         "R ...", False), # 91
         "HLOOKUP": (0x065,  3,  4,   "V",       "V R R V", False), # 92
            "HOUR": (0x047,  1,  1,   "V",             "V", False), # 93
       "HYPERLINK": (0x167,  1,  2,   "V",           "V V", False), # 94
     "HYPGEOMVERT": (0x121,  4,  4,   "V",       "V V V V", False), # 95
              "IF": (0x001,  2,  3,   "R",         "V R R", False), # 96
           "INDEX": (0x01d,  2,  4,   "R",       "R V V V", False), # 97
        "INDIRECT": (0x094,  1,  2,   "R",           "V V",  True), # 98
            "INFO": (0x0f4,  1,  1,   "V",             "V", False), # 99
             "INT": (0x019,  1,  1,   "V",             "V", False), # 100
       "INTERCEPT": (0x137,  2,  2,   "V",           "A A", False), # 101
            "IPMT": (0x0a7,  4,  6,   "V",   "V V V V V V", False), # 102
             "IRR": (0x03e,  1,  2,   "V",           "R V", False), # 103
         "ISBLANK": (0x081,  1,  1,   "V",             "V", False), # 104
           "ISERR": (0x07e,  1,  1,   "V",             "V", False), # 105
         "ISERROR": (0x003,  1,  1,   "V",             "V", False), # 106
       "ISLOGICAL": (0x0c6,  1,  1,   "V",             "V", False), # 107
            "ISNA": (0x002,  1,  1,   "V",             "V", False), # 108
       "ISNONTEXT": (0x0c0,  1,  1,   "V",             "V", False), # 109
        "ISNUMBER": (0x080,  1,  1,   "V",             "V", False), # 110
           "ISPMT": (0x15e,  4,  4,   "V",       "V V V V", False), # 111
           "ISREF": (0x069,  1,  1,   "V",             "R", False), # 112
          "ISTEXT": (0x07f,  1,  1,   "V",             "V", False), # 113
            "KURT": (0x142,  1, 30,   "V",         "R ...", False), # 114
           "LARGE": (0x145,  2,  2,   "V",           "R V", False), # 115
            "LEFT": (0x073,  1,  2,   "V",           "V V", False), # 116
           "LEFTB": (0x0d0,  1,  2,   "V",           "V V", False), # 117
             "LEN": (0x020,  1,  1,   "V",             "V", False), # 118
            "LENB": (0x0d3,  1,  1,   "V",             "V", False), # 119
          "LINEST": (0x031,  1,  4,   "A",       "R R V V", False), # 120
              "LN": (0x016,  1,  1,   "V",             "V", False), # 121
             "LOG": (0x06d,  1,  2,   "V",           "V V", False), # 122
           "LOG10": (0x017,  1,  1,   "V",             "V", False), # 123
          "LOGEST": (0x033,  1,  4,   "A",       "R R V V", False), # 124
          "LOGINV": (0x123,  3,  3,   "V",         "V V V", False), # 125
     "LOGNORMDIST": (0x122,  3,  3,   "V",         "V V V", False), # 126
          "LOOKUP": (0x01c,  2,  3,   "V",         "V R R", False), # 127
           "LOWER": (0x070,  1,  1,   "V",             "V", False), # 128
           "MATCH": (0x040,  2,  3,   "V",         "V R R", False), # 129
             "MAX": (0x007,  1, 30,   "V",         "R ...", False), # 130
            "MAXA": (0x16a,  1, 30,   "V",         "R ...", False), # 131
         "MDETERM": (0x0a3,  1,  1,   "V",             "A", False), # 132
          "MEDIAN": (0x0e3,  1, 30,   "V",         "R ...", False), # 133
             "MID": (0x01f,  3,  3,   "V",         "V V V", False), # 134
            "MIDB": (0x0d2,  3,  3,   "V",         "V V V", False), # 135
             "MIN": (0x006,  1, 30,   "V",         "R ...", False), # 136
            "MINA": (0x16b,  1, 30,   "V",         "R ...", False), # 137
          "MINUTE": (0x048,  1,  1,   "V",             "V", False), # 138
        "MINVERSE": (0x0a4,  1,  1,   "A",             "A", False), # 139
            "MIRR": (0x03d,  3,  3,   "V",         "R V V", False), # 140
           "MMULT": (0x0a5,  2,  2,   "A",           "A A", False), # 141
       "MNORMSINV": (0x128,  1,  1,   "V",             "V", False), # 142
             "MOD": (0x027,  2,  2,   "V",           "V V", False), # 143
            "MODE": (0x14a,  1, 30,   "V",         "A ...", False), # 144
           "MONTH": (0x044,  1,  1,   "V",             "V", False), # 145
               "N": (0x083,  1,  1,   "V",             "R", False), # 146
              "NA": (0x00a,  0,  0,   "V",             "-", False), # 147
    "NEGBINOMDIST": (0x124,  3,  3,   "V",         "V V V", False), # 148
        "NORMDIST": (0x125,  4,  4,   "V",       "V V V V", False), # 149
         "NORMINV": (0x127,  3,  3,   "V",         "V V V", False), # 150
       "NORMSDIST": (0x126,  1,  1,   "V",             "V", False), # 151
             "NOT": (0x026,  1,  1,   "V",             "V", False), # 152
             "NOW": (0x04a,  0,  0,   "V",             "-",  True), # 153
            "NPER": (0x03a,  3,  5,   "V",     "V V V V V", False), # 154
             "NPV": (0x00b,  2, 30,   "V",       "V R ...", False), # 155
    "NUMBERSTRING": (0x161,  2,  2,   "V",           "V V", False), # 156
             "ODD": (0x12a,  1,  1,   "V",             "V", False), # 157
          "OFFSET": (0x04e,  3,  5,   "R",     "R V V V V",  True), # 158
              "OR": (0x025,  1, 30,   "V",         "R ...", False), # 159
         "PEARSON": (0x138,  2,  2,   "V",           "A A", False), # 160
      "PERCENTILE": (0x148,  2,  2,   "V",           "R V", False), # 161
     "PERCENTRANK": (0x149,  2,  3,   "V",         "R V V", False), # 162
          "PERMUT": (0x12b,  2,  2,   "V",           "V V", False), # 163
        "PHONETIC": (0x168,  1,  1,   "V",             "R", False), # 164
              "PI": (0x013,  0,  0,   "V",             "-", False), # 165
             "PMT": (0x03b,  3,  5,   "V",     "V V V V V", False), # 166
         "POISSON": (0x12c,  3,  3,   "V",         "V V V", False), # 167
           "POWER": (0x151,  2,  2,   "V",           "V V", False), # 168
            "PPMT": (0x0a8,  4,  6,   "V",   "V V V V V V", False), # 169
            "PROB": (0x13d,  3,  4,   "V",       "A A V V", False), # 170
         "PRODUCT": (0x0b7,  0, 30,   "V",         "R ...", False), # 171
          "PROPER": (0x072,  1,  1,   "V",             "V", False), # 172
              "PV": (0x038,  3,  5,   "V",     "V V V V V", False), # 173
        "QUARTILE": (0x147,  2,  2,   "V",           "R V", False), # 174
         "RADIANS": (0x156,  1,  1,   "V",             "V", False), # 175
            "RAND": (0x03f,  0,  0,   "V",             "-",  True), # 176
            "RANK": (0x0d8,  2,  3,   "V",         "V R V", False), # 177
            "RATE": (0x03c,  3,  6,   "V",   "V V V V V V", False), # 178
         "REPLACE": (0x077,  4,  4,   "V",       "V V V V", False), # 179
        "REPLACEB": (0x0cf,  4,  4,   "V",       "V V V V", False), # 180
            "REPT": (0x01e,  2,  2,   "V",           "V V", False), # 181
           "RIGHT": (0x074,  1,  2,   "V",           "V V", False), # 182
          "RIGHTB": (0x0d1,  1,  2,   "V",           "V V", False), # 183
           "ROMAN": (0x162,  1,  2,   "V",           "V V", False), # 184
           "ROUND": (0x01b,  2,  2,   "V",           "V V", False), # 185
       "ROUNDDOWN": (0x0d5,  2,  2,   "V",           "V V", False), # 186
         "ROUNDUP": (0x0d4,  2,  2,   "V",           "V V", False), # 187
             "ROW": (0x008,  0,  1,   "V",             "R", False), # 188
            "ROWS": (0x04c,  1,  1,   "V",             "R", False), # 189
             "RSQ": (0x139,  2,  2,   "V",           "A A", False), # 190
          "SEARCH": (0x052,  2,  3,   "V",         "V V V", False), # 191
         "SEARCHB": (0x0ce,  2,  3,   "V",         "V V V", False), # 192
          "SECOND": (0x049,  1,  1,   "V",             "V", False), # 193
            "SIGN": (0x01a,  1,  1,   "V",             "V", False), # 194
             "SIN": (0x00f,  1,  1,   "V",             "V", False), # 195
            "SINH": (0x0e5,  1,  1,   "V",             "V", False), # 196
            "SKEW": (0x143,  1, 30,   "V",         "R ...", False), # 197
             "SLN": (0x08e,  3,  3,   "V",         "V V V", False), # 198
           "SLOPE": (0x13b,  2,  2,   "V",           "A A", False), # 199
           "SMALL": (0x146,  2,  2,   "V",           "R V", False), # 200
            "SQRT": (0x014,  1,  1,   "V",             "V", False), # 201
     "STANDARDIZE": (0x129,  3,  3,   "V",         "V V V", False), # 202
           "STDEV": (0x00c,  1, 30,   "V",         "R ...", False), # 203
          "STDEVA": (0x16e,  1, 30,   "V",         "R ...", False), # 204
          "STDEVP": (0x0c1,  1, 30,   "V",         "R ...", False), # 205
         "STDEVPA": (0x16c,  1, 30,   "V",         "R ...", False), # 206
           "STEYX": (0x13a,  2,  2,   "V",           "A A", False), # 207
      "SUBSTITUTE": (0x078,  3,  4,   "V",       "V V V V", False), # 208
        "SUBTOTAL": (0x158,  2, 30,   "V",       "V R ...", False), # 209
             "SUM": (0x004,  0, 30,   "V",         "R ...", False), # 210
           "SUMIF": (0x159,  2,  3,   "V",         "R V R", False), # 211
      "SUMPRODUCT": (0x0e4,  1, 30,   "V",         "A ...", False), # 212
           "SUMSQ": (0x141,  0, 30,   "V",         "R ...", False), # 213
        "SUMX2MY2": (0x130,  2,  2,   "V",           "A A", False), # 214
        "SUMX2PY2": (0x131,  2,  2,   "V",           "A A", False), # 215
         "SUMXMY2": (0x12f,  2,  2,   "V",           "A A", False), # 216
             "SYD": (0x08f,  4,  4,   "V",       "V V V V", False), # 217
               "T": (0x082,  1,  1,   "V",             "R", False), # 218
             "TAN": (0x011,  1,  1,   "V",             "V", False), # 219
            "TANH": (0x0e7,  1,  1,   "V",             "V", False), # 220
           "TDIST": (0x12d,  3,  3,   "V",         "V V V", False), # 221
            "TEXT": (0x030,  2,  2,   "V",           "V V", False), # 222
            "TIME": (0x042,  3,  3,   "V",         "V V V", False), # 223
       "TIMEVALUE": (0x08d,  1,  1,   "V",             "V", False), # 224
            "TINV": (0x14c,  2,  2,   "V",           "V V", False), # 225
           "TODAY": (0x0dd,  0,  0,   "V",             "-",  True), # 226
       "TRANSPOSE": (0x053,  1,  1,   "A",             "A", False), # 227
           "TREND": (0x032,  1,  4,   "A",       "R R R V", False), # 228
            "TRIM": (0x076,  1,  1,   "V",             "V", False), # 229
        "TRIMMEAN": (0x14b,  2,  2,   "V",           "R V", False), # 230
            "TRUE": (0x022,  0,  0,   "V",             "-", False), # 231
           "TRUNC": (0x0c5,  1,  2,   "V",           "V V", False), # 232
           "TTEST": (0x13c,  4,  4,   "V",       "A A V V", False), # 233
            "TYPE": (0x056,  1,  1,   "V",             "V", False), # 234
           "UPPER": (0x071,  1,  1,   "V",             "V", False), # 235
        "USDOLLAR": (0x0cc,  1,  2,   "V",           "V V", False), # 236
           "VALUE": (0x021,  1,  1,   "V",             "V", False), # 237
             "VAR": (0x02e,  1, 30,   "V",         "R ...", False), # 238
            "VARA": (0x16f,  1, 30,   "V",         "R ...", False), # 239
            "VARP": (0x0c2,  1, 30,   "V",         "R ...", False), # 240
           "VARPA": (0x16d,  1, 30,   "V",         "R ...", False), # 241
             "VDB": (0x0de,  5,  7,   "V", "V V V V V V V", False), # 242
         "VLOOKUP": (0x066,  3,  4,   "V",       "V R R V", False), # 243
         "WEEKDAY": (0x046,  1,  2,   "V",           "V V", False), # 244
         "WEIBULL": (0x12e,  4,  4,   "V",       "V V V V", False), # 245
            "YEAR": (0x045,  1,  1,   "V",             "V", False), # 246
           "ZTEST": (0x144,  2,  3,   "V",         "R V V", False)  # 247
}


std_func_by_num = {
    0x000: (       "COUNT",  0, 30,   "V",         "R ...", False), # 1
    0x001: (          "IF",  2,  3,   "R",         "V R R", False), # 2
    0x002: (        "ISNA",  1,  1,   "V",             "V", False), # 3
    0x003: (     "ISERROR",  1,  1,   "V",             "V", False), # 4
    0x004: (         "SUM",  0, 30,   "V",         "R ...", False), # 5
    0x005: (     "AVERAGE",  1, 30,   "V",         "R ...", False), # 6
    0x006: (         "MIN",  1, 30,   "V",         "R ...", False), # 7
    0x007: (         "MAX",  1, 30,   "V",         "R ...", False), # 8
    0x008: (         "ROW",  0,  1,   "V",             "R", False), # 9
    0x009: (      "COLUMN",  0,  1,   "V",             "R", False), # 10
    0x00a: (          "NA",  0,  0,   "V",             "-", False), # 11
    0x00b: (         "NPV",  2, 30,   "V",       "V R ...", False), # 12
    0x00c: (       "STDEV",  1, 30,   "V",         "R ...", False), # 13
    0x00d: (      "DOLLAR",  1,  2,   "V",           "V V", False), # 14
    0x00e: (       "FIXED",  2,  3,   "V",         "V V V", False), # 15
    0x00f: (         "SIN",  1,  1,   "V",             "V", False), # 16
    0x010: (         "COS",  1,  1,   "V",             "V", False), # 17
    0x011: (         "TAN",  1,  1,   "V",             "V", False), # 18
    0x012: (      "ARCTAN",  1,  1,   "V",             "V", False), # 19
    0x013: (          "PI",  0,  0,   "V",             "-", False), # 20
    0x014: (        "SQRT",  1,  1,   "V",             "V", False), # 21
    0x015: (         "EXP",  1,  1,   "V",             "V", False), # 22
    0x016: (          "LN",  1,  1,   "V",             "V", False), # 23
    0x017: (       "LOG10",  1,  1,   "V",             "V", False), # 24
    0x018: (         "ABS",  1,  1,   "V",             "V", False), # 25
    0x019: (         "INT",  1,  1,   "V",             "V", False), # 26
    0x01a: (        "SIGN",  1,  1,   "V",             "V", False), # 27
    0x01b: (       "ROUND",  2,  2,   "V",           "V V", False), # 28
    0x01c: (      "LOOKUP",  2,  3,   "V",         "V R R", False), # 29
    0x01d: (       "INDEX",  2,  4,   "R",       "R V V V", False), # 30
    0x01e: (        "REPT",  2,  2,   "V",           "V V", False), # 31
    0x01f: (         "MID",  3,  3,   "V",         "V V V", False), # 32
    0x020: (         "LEN",  1,  1,   "V",             "V", False), # 33
    0x021: (       "VALUE",  1,  1,   "V",             "V", False), # 34
    0x022: (        "TRUE",  0,  0,   "V",             "-", False), # 35
    0x023: (       "FALSE",  0,  0,   "V",             "-", False), # 36
    0x024: (         "AND",  1, 30,   "V",         "R ...", False), # 37
    0x025: (          "OR",  1, 30,   "V",         "R ...", False), # 38
    0x026: (         "NOT",  1,  1,   "V",             "V", False), # 39
    0x027: (         "MOD",  2,  2,   "V",           "V V", False), # 40
    0x028: (      "DCOUNT",  3,  3,   "V",         "R R R", False), # 41
    0x029: (        "DSUM",  3,  3,   "V",         "R R R", False), # 42
    0x02a: (    "DAVERAGE",  3,  3,   "V",         "R R R", False), # 43
    0x02b: (        "DMIN",  3,  3,   "V",         "R R R", False), # 44
    0x02c: (        "DMAX",  3,  3,   "V",         "R R R", False), # 45
    0x02d: (      "DSTDEV",  3,  3,   "V",         "R R R", False), # 46
    0x02e: (         "VAR",  1, 30,   "V",         "R ...", False), # 47
    0x02f: (        "DVAR",  3,  3,   "V",         "R R R", False), # 48
    0x030: (        "TEXT",  2,  2,   "V",           "V V", False), # 49
    0x031: (      "LINEST",  1,  4,   "A",       "R R V V", False), # 50
    0x032: (       "TREND",  1,  4,   "A",       "R R R V", False), # 51
    0x033: (      "LOGEST",  1,  4,   "A",       "R R V V", False), # 52
    0x034: (      "GROWTH",  1,  4,   "A",       "R R R V", False), # 53
    0x038: (          "PV",  3,  5,   "V",     "V V V V V", False), # 54
    0x039: (          "FV",  3,  5,   "V",     "V V V V V", False), # 55
    0x03a: (        "NPER",  3,  5,   "V",     "V V V V V", False), # 56
    0x03b: (         "PMT",  3,  5,   "V",     "V V V V V", False), # 57
    0x03c: (        "RATE",  3,  6,   "V",   "V V V V V V", False), # 58
    0x03d: (        "MIRR",  3,  3,   "V",         "R V V", False), # 59
    0x03e: (         "IRR",  1,  2,   "V",           "R V", False), # 60
    0x03f: (        "RAND",  0,  0,   "V",             "-",  True), # 61
    0x040: (       "MATCH",  2,  3,   "V",         "V R R", False), # 62
    0x041: (        "DATE",  3,  3,   "V",         "V V V", False), # 63
    0x042: (        "TIME",  3,  3,   "V",         "V V V", False), # 64
    0x043: (         "DAY",  1,  1,   "V",             "V", False), # 65
    0x044: (       "MONTH",  1,  1,   "V",             "V", False), # 66
    0x045: (        "YEAR",  1,  1,   "V",             "V", False), # 67
    0x046: (     "WEEKDAY",  1,  2,   "V",           "V V", False), # 68
    0x047: (        "HOUR",  1,  1,   "V",             "V", False), # 69
    0x048: (      "MINUTE",  1,  1,   "V",             "V", False), # 70
    0x049: (      "SECOND",  1,  1,   "V",             "V", False), # 71
    0x04a: (         "NOW",  0,  0,   "V",             "-",  True), # 72
    0x04b: (       "AREAS",  1,  1,   "V",             "R", False), # 73
    0x04c: (        "ROWS",  1,  1,   "V",             "R", False), # 74
    0x04d: (     "COLUMNS",  1,  1,   "V",             "R", False), # 75
    0x04e: (      "OFFSET",  3,  5,   "R",     "R V V V V",  True), # 76
    0x052: (      "SEARCH",  2,  3,   "V",         "V V V", False), # 77
    0x053: (   "TRANSPOSE",  1,  1,   "A",             "A", False), # 78
    0x056: (        "TYPE",  1,  1,   "V",             "V", False), # 79
    0x061: (       "ATAN2",  2,  2,   "V",           "V V", False), # 80
    0x062: (        "ASIN",  1,  1,   "V",             "V", False), # 81
    0x063: (        "ACOS",  1,  1,   "V",             "V", False), # 82
    0x064: (      "CHOOSE",  2, 30,   "R",       "V R ...", False), # 83
    0x065: (     "HLOOKUP",  3,  4,   "V",       "V R R V", False), # 84
    0x066: (     "VLOOKUP",  3,  4,   "V",       "V R R V", False), # 85
    0x069: (       "ISREF",  1,  1,   "V",             "R", False), # 86
    0x06d: (         "LOG",  1,  2,   "V",           "V V", False), # 87
    0x06f: (        "CHAR",  1,  1,   "V",             "V", False), # 88
    0x070: (       "LOWER",  1,  1,   "V",             "V", False), # 89
    0x071: (       "UPPER",  1,  1,   "V",             "V", False), # 90
    0x072: (      "PROPER",  1,  1,   "V",             "V", False), # 91
    0x073: (        "LEFT",  1,  2,   "V",           "V V", False), # 92
    0x074: (       "RIGHT",  1,  2,   "V",           "V V", False), # 93
    0x075: (       "EXACT",  2,  2,   "V",           "V V", False), # 94
    0x076: (        "TRIM",  1,  1,   "V",             "V", False), # 95
    0x077: (     "REPLACE",  4,  4,   "V",       "V V V V", False), # 96
    0x078: (  "SUBSTITUTE",  3,  4,   "V",       "V V V V", False), # 97
    0x079: (        "CODE",  1,  1,   "V",             "V", False), # 98
    0x07c: (        "FIND",  2,  3,   "V",         "V V V", False), # 99
    0x07d: (        "CELL",  1,  2,   "V",           "V R",  True), # 100
    0x07e: (       "ISERR",  1,  1,   "V",             "V", False), # 101
    0x07f: (      "ISTEXT",  1,  1,   "V",             "V", False), # 102
    0x080: (    "ISNUMBER",  1,  1,   "V",             "V", False), # 103
    0x081: (     "ISBLANK",  1,  1,   "V",             "V", False), # 104
    0x082: (           "T",  1,  1,   "V",             "R", False), # 105
    0x083: (           "N",  1,  1,   "V",             "R", False), # 106
    0x08c: (   "DATEVALUE",  1,  1,   "V",             "V", False), # 107
    0x08d: (   "TIMEVALUE",  1,  1,   "V",             "V", False), # 108
    0x08e: (         "SLN",  3,  3,   "V",         "V V V", False), # 109
    0x08f: (         "SYD",  4,  4,   "V",       "V V V V", False), # 110
    0x090: (         "DDB",  4,  5,   "V",     "V V V V V", False), # 111
    0x094: (    "INDIRECT",  1,  2,   "R",           "V V",  True), # 112
    0x0a2: (       "CLEAN",  1,  1,   "V",             "V", False), # 113
    0x0a3: (     "MDETERM",  1,  1,   "V",             "A", False), # 114
    0x0a4: (    "MINVERSE",  1,  1,   "A",             "A", False), # 115
    0x0a5: (       "MMULT",  2,  2,   "A",           "A A", False), # 116
    0x0a7: (        "IPMT",  4,  6,   "V",   "V V V V V V", False), # 117
    0x0a8: (        "PPMT",  4,  6,   "V",   "V V V V V V", False), # 118
    0x0a9: (      "COUNTA",  0, 30,   "V",         "R ...", False), # 119
    0x0b7: (     "PRODUCT",  0, 30,   "V",         "R ...", False), # 120
    0x0b8: (        "FACT",  1,  1,   "V",             "V", False), # 121
    0x0bf: (    "DPRODUCT",  3,  3,   "V",         "R R R", False), # 122
    0x0c0: (   "ISNONTEXT",  1,  1,   "V",             "V", False), # 123
    0x0c1: (      "STDEVP",  1, 30,   "V",         "R ...", False), # 124
    0x0c2: (        "VARP",  1, 30,   "V",         "R ...", False), # 125
    0x0c3: (     "DSTDEVP",  3,  3,   "V",         "R R R", False), # 126
    0x0c4: (       "DVARP",  3,  3,   "V",         "R R R", False), # 127
    0x0c5: (       "TRUNC",  1,  2,   "V",           "V V", False), # 128
    0x0c6: (   "ISLOGICAL",  1,  1,   "V",             "V", False), # 129
    0x0c7: (     "DCOUNTA",  3,  3,   "V",         "R R R", False), # 130
    0x0cc: (    "USDOLLAR",  1,  2,   "V",           "V V", False), # 131
    0x0cd: (       "FINDB",  2,  3,   "V",         "V V V", False), # 132
    0x0ce: (     "SEARCHB",  2,  3,   "V",         "V V V", False), # 133
    0x0cf: (    "REPLACEB",  4,  4,   "V",       "V V V V", False), # 134
    0x0d0: (       "LEFTB",  1,  2,   "V",           "V V", False), # 135
    0x0d1: (      "RIGHTB",  1,  2,   "V",           "V V", False), # 136
    0x0d2: (        "MIDB",  3,  3,   "V",         "V V V", False), # 137
    0x0d3: (        "LENB",  1,  1,   "V",             "V", False), # 138
    0x0d4: (     "ROUNDUP",  2,  2,   "V",           "V V", False), # 139
    0x0d5: (   "ROUNDDOWN",  2,  2,   "V",           "V V", False), # 140
    0x0d6: (         "ASC",  1,  1,   "V",             "V", False), # 141
    0x0d7: (        "DBSC",  1,  1,   "V",             "V", False), # 142
    0x0d8: (        "RANK",  2,  3,   "V",         "V R V", False), # 143
    0x0db: (     "ADDRESS",  2,  5,   "V",     "V V V V V", False), # 144
    0x0dc: (     "DAYS360",  2,  3,   "V",         "V V V", False), # 145
    0x0dd: (       "TODAY",  0,  0,   "V",             "-",  True), # 146
    0x0de: (         "VDB",  5,  7,   "V", "V V V V V V V", False), # 147
    0x0e3: (      "MEDIAN",  1, 30,   "V",         "R ...", False), # 148
    0x0e4: (  "SUMPRODUCT",  1, 30,   "V",         "A ...", False), # 149
    0x0e5: (        "SINH",  1,  1,   "V",             "V", False), # 150
    0x0e6: (        "COSH",  1,  1,   "V",             "V", False), # 151
    0x0e7: (        "TANH",  1,  1,   "V",             "V", False), # 152
    0x0e8: (       "ASINH",  1,  1,   "V",             "V", False), # 153
    0x0e9: (       "ACOSH",  1,  1,   "V",             "V", False), # 154
    0x0ea: (       "ATANH",  1,  1,   "V",             "V", False), # 155
    0x0eb: (        "DGET",  3,  3,   "V",         "R R R", False), # 156
    0x0f4: (        "INFO",  1,  1,   "V",             "V", False), # 157
    0x0f7: (          "DB",  4,  5,   "V",     "V V V V V", False), # 158
    0x0fc: (   "FREQUENCY",  2,  2,   "A",           "R R", False), # 159
    0x105: (  "ERROR.TYPE",  1,  1,   "V",             "V", False), # 160
    0x10d: (      "AVEDEV",  1, 30,   "V",         "R ...", False), # 161
    0x10e: (    "BETADIST",  3,  5,   "V",     "V V V V V", False), # 162
    0x10f: (     "GAMMALN",  1,  1,   "V",             "V", False), # 163
    0x110: (     "BETAINV",  3,  5,   "V",     "V V V V V", False), # 164
    0x111: (   "BINOMDIST",  4,  4,   "V",       "V V V V", False), # 165
    0x112: (     "CHIDIST",  2,  2,   "V",           "V V", False), # 166
    0x113: (      "CHIINV",  2,  2,   "V",           "V V", False), # 167
    0x114: (      "COMBIN",  2,  2,   "V",           "V V", False), # 168
    0x115: (  "CONFIDENCE",  3,  3,   "V",         "V V V", False), # 169
    0x116: (   "CRITBINOM",  3,  3,   "V",         "V V V", False), # 170
    0x117: (        "EVEN",  1,  1,   "V",             "V", False), # 171
    0x118: (   "EXPONDIST",  3,  3,   "V",         "V V V", False), # 172
    0x119: (       "FDIST",  3,  3,   "V",         "V V V", False), # 173
    0x11a: (        "FINV",  3,  3,   "V",         "V V V", False), # 174
    0x11b: (      "FISHER",  1,  1,   "V",             "V", False), # 175
    0x11c: (   "FISHERINV",  1,  1,   "V",             "V", False), # 176
    0x11d: (       "FLOOR",  2,  2,   "V",           "V V", False), # 177
    0x11e: (   "GAMMADIST",  4,  4,   "V",       "V V V V", False), # 178
    0x11f: (    "GAMMAINV",  3,  3,   "V",         "V V V", False), # 179
    0x120: (     "CEILING",  2,  2,   "V",           "V V", False), # 180
    0x121: ( "HYPGEOMVERT",  4,  4,   "V",       "V V V V", False), # 181
    0x122: ( "LOGNORMDIST",  3,  3,   "V",         "V V V", False), # 182
    0x123: (      "LOGINV",  3,  3,   "V",         "V V V", False), # 183
    0x124: ("NEGBINOMDIST",  3,  3,   "V",         "V V V", False), # 184
    0x125: (    "NORMDIST",  4,  4,   "V",       "V V V V", False), # 185
    0x126: (   "NORMSDIST",  1,  1,   "V",             "V", False), # 186
    0x127: (     "NORMINV",  3,  3,   "V",         "V V V", False), # 187
    0x128: (   "MNORMSINV",  1,  1,   "V",             "V", False), # 188
    0x129: ( "STANDARDIZE",  3,  3,   "V",         "V V V", False), # 189
    0x12a: (         "ODD",  1,  1,   "V",             "V", False), # 190
    0x12b: (      "PERMUT",  2,  2,   "V",           "V V", False), # 191
    0x12c: (     "POISSON",  3,  3,   "V",         "V V V", False), # 192
    0x12d: (       "TDIST",  3,  3,   "V",         "V V V", False), # 193
    0x12e: (     "WEIBULL",  4,  4,   "V",       "V V V V", False), # 194
    0x12f: (     "SUMXMY2",  2,  2,   "V",           "A A", False), # 195
    0x130: (    "SUMX2MY2",  2,  2,   "V",           "A A", False), # 196
    0x131: (    "SUMX2PY2",  2,  2,   "V",           "A A", False), # 197
    0x132: (     "CHITEST",  2,  2,   "V",           "A A", False), # 198
    0x133: (      "CORREL",  2,  2,   "V",           "A A", False), # 199
    0x134: (       "COVAR",  2,  2,   "V",           "A A", False), # 200
    0x135: (    "FORECAST",  3,  3,   "V",         "V A A", False), # 201
    0x136: (       "FTEST",  2,  2,   "V",           "A A", False), # 202
    0x137: (   "INTERCEPT",  2,  2,   "V",           "A A", False), # 203
    0x138: (     "PEARSON",  2,  2,   "V",           "A A", False), # 204
    0x139: (         "RSQ",  2,  2,   "V",           "A A", False), # 205
    0x13a: (       "STEYX",  2,  2,   "V",           "A A", False), # 206
    0x13b: (       "SLOPE",  2,  2,   "V",           "A A", False), # 207
    0x13c: (       "TTEST",  4,  4,   "V",       "A A V V", False), # 208
    0x13d: (        "PROB",  3,  4,   "V",       "A A V V", False), # 209
    0x13e: (       "DEVSQ",  1, 30,   "V",         "R ...", False), # 210
    0x13f: (     "GEOMEAN",  1, 30,   "V",         "R ...", False), # 211
    0x140: (     "HARMEAN",  1, 30,   "V",         "R ...", False), # 212
    0x141: (       "SUMSQ",  0, 30,   "V",         "R ...", False), # 213
    0x142: (        "KURT",  1, 30,   "V",         "R ...", False), # 214
    0x143: (        "SKEW",  1, 30,   "V",         "R ...", False), # 215
    0x144: (       "ZTEST",  2,  3,   "V",         "R V V", False), # 216
    0x145: (       "LARGE",  2,  2,   "V",           "R V", False), # 217
    0x146: (       "SMALL",  2,  2,   "V",           "R V", False), # 218
    0x147: (    "QUARTILE",  2,  2,   "V",           "R V", False), # 219
    0x148: (  "PERCENTILE",  2,  2,   "V",           "R V", False), # 220
    0x149: ( "PERCENTRANK",  2,  3,   "V",         "R V V", False), # 221
    0x14a: (        "MODE",  1, 30,   "V",         "A ...", False), # 222
    0x14b: (    "TRIMMEAN",  2,  2,   "V",           "R V", False), # 223
    0x14c: (        "TINV",  2,  2,   "V",           "V V", False), # 224
    0x150: ( "CONCATENATE",  0, 30,   "V",         "V ...", False), # 225
    0x151: (       "POWER",  2,  2,   "V",           "V V", False), # 226
    0x156: (     "RADIANS",  1,  1,   "V",             "V", False), # 227
    0x157: (     "DEGREES",  1,  1,   "V",             "V", False), # 228
    0x158: (    "SUBTOTAL",  2, 30,   "V",       "V R ...", False), # 229
    0x159: (       "SUMIF",  2,  3,   "V",         "R V R", False), # 230
    0x15a: (     "COUNTIF",  2,  2,   "V",           "R V", False), # 231
    0x15b: (  "COUNTBLANK",  1,  1,   "V",             "R", False), # 232
    0x15e: (       "ISPMT",  4,  4,   "V",       "V V V V", False), # 233
    0x15f: (     "DATEDIF",  3,  3,   "V",         "V V V", False), # 234
    0x160: (  "DATESTRING",  1,  1,   "V",             "V", False), # 235
    0x161: ("NUMBERSTRING",  2,  2,   "V",           "V V", False), # 236
    0x162: (       "ROMAN",  1,  2,   "V",           "V V", False), # 237
    0x166: ("GETPIVOTDATA",  2, 30,   "A",             "-", False), # 238
    0x167: (   "HYPERLINK",  1,  2,   "V",           "V V", False), # 239
    0x168: (    "PHONETIC",  1,  1,   "V",             "R", False), # 240
    0x169: (    "AVERAGEA",  1, 30,   "V",         "R ...", False), # 241
    0x16a: (        "MAXA",  1, 30,   "V",         "R ...", False), # 242
    0x16b: (        "MINA",  1, 30,   "V",         "R ...", False), # 243
    0x16c: (     "STDEVPA",  1, 30,   "V",         "R ...", False), # 244
    0x16d: (       "VARPA",  1, 30,   "V",         "R ...", False), # 245
    0x16e: (      "STDEVA",  1, 30,   "V",         "R ...", False), # 246
    0x16f: (        "VARA",  1, 30,   "V",         "R ...", False)  # 247
}


# Formulas Parse things

ptgExp          = 0x01
ptgTbl          = 0x02
ptgAdd          = 0x03
ptgSub          = 0x04
ptgMul          = 0x05
ptgDiv          = 0x06
ptgPower        = 0x07
ptgConcat       = 0x08
ptgLT           = 0x09
ptgLE           = 0x0a
ptgEQ           = 0x0b
ptgGE           = 0x0c
ptgGT           = 0x0d
ptgNE           = 0x0e
ptgIsect        = 0x0f
ptgUnion        = 0x10
ptgRange        = 0x11
ptgUplus        = 0x12
ptgUminus       = 0x13
ptgPercent      = 0x14
ptgParen        = 0x15
ptgMissArg      = 0x16
ptgStr          = 0x17
ptgExtend       = 0x18
ptgAttr         = 0x19
ptgSheet        = 0x1a
ptgEndSheet     = 0x1b
ptgErr          = 0x1c
ptgBool         = 0x1d
ptgInt          = 0x1e
ptgNum          = 0x1f

ptgArrayR       = 0x20
ptgFuncR        = 0x21
ptgFuncVarR     = 0x22
ptgNameR        = 0x23
ptgRefR         = 0x24
ptgAreaR        = 0x25
ptgMemAreaR     = 0x26
ptgMemErrR      = 0x27
ptgMemNoMemR    = 0x28
ptgMemFuncR     = 0x29
ptgRefErrR      = 0x2a
ptgAreaErrR     = 0x2b
ptgRefNR        = 0x2c
ptgAreaNR       = 0x2d
ptgMemAreaNR    = 0x2e
ptgMemNoMemNR   = 0x2f
ptgNameXR       = 0x39
ptgRef3dR       = 0x3a
ptgArea3dR      = 0x3b
ptgRefErr3dR    = 0x3c
ptgAreaErr3dR   = 0x3d

ptgArrayV       = 0x40
ptgFuncV        = 0x41
ptgFuncVarV     = 0x42
ptgNameV        = 0x43
ptgRefV         = 0x44
ptgAreaV        = 0x45
ptgMemAreaV     = 0x46
ptgMemErrV      = 0x47
ptgMemNoMemV    = 0x48
ptgMemFuncV     = 0x49
ptgRefErrV      = 0x4a
ptgAreaErrV     = 0x4b
ptgRefNV        = 0x4c
ptgAreaNV       = 0x4d
ptgMemAreaNV    = 0x4e
ptgMemNoMemNV   = 0x4f
ptgFuncCEV      = 0x58
ptgNameXV       = 0x59
ptgRef3dV       = 0x5a
ptgArea3dV      = 0x5b
ptgRefErr3dV    = 0x5c
ptgAreaErr3dV   = 0x5d

ptgArrayA       = 0x60
ptgFuncA        = 0x61
ptgFuncVarA     = 0x62
ptgNameA        = 0x63
ptgRefA         = 0x64
ptgAreaA        = 0x65
ptgMemAreaA     = 0x66
ptgMemErrA      = 0x67
ptgMemNoMemA    = 0x68
ptgMemFuncA     = 0x69
ptgRefErrA      = 0x6a
ptgAreaErrA     = 0x6b
ptgRefNA        = 0x6c
ptgAreaNA       = 0x6d
ptgMemAreaNA    = 0x6e
ptgMemNoMemNA   = 0x6f
ptgFuncCEA      = 0x78
ptgNameXA       = 0x79
ptgRef3dA       = 0x7a
ptgArea3dA      = 0x7b
ptgRefErr3dA    = 0x7c
ptgAreaErr3dA   = 0x7d


PtgNames = {
    ptgExp         : "ptgExp",
    ptgTbl         : "ptgTbl",
    ptgAdd         : "ptgAdd",
    ptgSub         : "ptgSub",
    ptgMul         : "ptgMul",
    ptgDiv         : "ptgDiv",
    ptgPower       : "ptgPower",
    ptgConcat      : "ptgConcat",
    ptgLT          : "ptgLT",
    ptgLE          : "ptgLE",
    ptgEQ          : "ptgEQ",
    ptgGE          : "ptgGE",
    ptgGT          : "ptgGT",
    ptgNE          : "ptgNE",
    ptgIsect       : "ptgIsect",
    ptgUnion       : "ptgUnion",
    ptgRange       : "ptgRange",
    ptgUplus       : "ptgUplus",
    ptgUminus      : "ptgUminus",
    ptgPercent     : "ptgPercent",
    ptgParen       : "ptgParen",
    ptgMissArg     : "ptgMissArg",
    ptgStr         : "ptgStr",
    ptgExtend      : "ptgExtend",
    ptgAttr        : "ptgAttr",
    ptgSheet       : "ptgSheet",
    ptgEndSheet    : "ptgEndSheet",
    ptgErr         : "ptgErr",
    ptgBool        : "ptgBool",
    ptgInt         : "ptgInt",
    ptgNum         : "ptgNum",
    ptgArrayR      : "ptgArrayR",
    ptgFuncR       : "ptgFuncR",
    ptgFuncVarR    : "ptgFuncVarR",
    ptgNameR       : "ptgNameR",
    ptgRefR        : "ptgRefR",
    ptgAreaR       : "ptgAreaR",
    ptgMemAreaR    : "ptgMemAreaR",
    ptgMemErrR     : "ptgMemErrR",
    ptgMemNoMemR   : "ptgMemNoMemR",
    ptgMemFuncR    : "ptgMemFuncR",
    ptgRefErrR     : "ptgRefErrR",
    ptgAreaErrR    : "ptgAreaErrR",
    ptgRefNR       : "ptgRefNR",
    ptgAreaNR      : "ptgAreaNR",
    ptgMemAreaNR   : "ptgMemAreaNR",
    ptgMemNoMemNR  : "ptgMemNoMemNR",
    ptgNameXR      : "ptgNameXR",
    ptgRef3dR      : "ptgRef3dR",
    ptgArea3dR     : "ptgArea3dR",
    ptgRefErr3dR   : "ptgRefErr3dR",
    ptgAreaErr3dR  : "ptgAreaErr3dR",
    ptgArrayV      : "ptgArrayV",
    ptgFuncV       : "ptgFuncV",
    ptgFuncVarV    : "ptgFuncVarV",
    ptgNameV       : "ptgNameV",
    ptgRefV        : "ptgRefV",
    ptgAreaV       : "ptgAreaV",
    ptgMemAreaV    : "ptgMemAreaV",
    ptgMemErrV     : "ptgMemErrV",
    ptgMemNoMemV   : "ptgMemNoMemV",
    ptgMemFuncV    : "ptgMemFuncV",
    ptgRefErrV     : "ptgRefErrV",
    ptgAreaErrV    : "ptgAreaErrV",
    ptgRefNV       : "ptgRefNV",
    ptgAreaNV      : "ptgAreaNV",
    ptgMemAreaNV   : "ptgMemAreaNV",
    ptgMemNoMemNV  : "ptgMemNoMemNV",
    ptgFuncCEV     : "ptgFuncCEV",
    ptgNameXV      : "ptgNameXV",
    ptgRef3dV      : "ptgRef3dV",
    ptgArea3dV     : "ptgArea3dV",
    ptgRefErr3dV   : "ptgRefErr3dV",
    ptgAreaErr3dV  : "ptgAreaErr3dV",
    ptgArrayA      : "ptgArrayA",
    ptgFuncA       : "ptgFuncA",
    ptgFuncVarA    : "ptgFuncVarA",
    ptgNameA       : "ptgNameA",
    ptgRefA        : "ptgRefA",
    ptgAreaA       : "ptgAreaA",
    ptgMemAreaA    : "ptgMemAreaA",
    ptgMemErrA     : "ptgMemErrA",
    ptgMemNoMemA   : "ptgMemNoMemA",
    ptgMemFuncA    : "ptgMemFuncA",
    ptgRefErrA     : "ptgRefErrA",
    ptgAreaErrA    : "ptgAreaErrA",
    ptgRefNA       : "ptgRefNA",
    ptgAreaNA      : "ptgAreaNA",
    ptgMemAreaNA   : "ptgMemAreaNA",
    ptgMemNoMemNA  : "ptgMemNoMemNA",
    ptgFuncCEA     : "ptgFuncCEA",
    ptgNameXA      : "ptgNameXA",
    ptgRef3dA      : "ptgRef3dA",
    ptgArea3dA     : "ptgArea3dA",
    ptgRefErr3dA   : "ptgRefErr3dA",
    ptgAreaErr3dA  : "ptgAreaErr3dA"
}


error_msg_by_code = {
    0x00: u"#NULL!",  # intersection of two cell ranges is empty
    0x07: u"#DIV/0!", # division by zero
    0x0F: u"#VALUE!", # wrong type of operand
    0x17: u"#REF!",   # illegal or deleted cell reference
    0x1D: u"#NAME?",  # wrong function or range name
    0x24: u"#NUM!",   # value range overflow
    0x2A: u"#N/A!"    # argument or function not available
}

########NEW FILE########
__FILENAME__ = Formatting
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.



'''
The  XF  record is able to store explicit cell formatting attributes or the
attributes  of  a cell style. Explicit formatting includes the reference to
a  cell  style  XF  record. This allows to extend a defined cell style with
some  explicit  attributes.  The  formatting  attributes  are  divided into
6 groups:

Group           Attributes
-------------------------------------
Number format   Number format index (index to FORMAT record)
Font            Font index (index to FONT record)
Alignment       Horizontal and vertical alignment, text wrap, indentation, 
                orientation/rotation, text direction
Border          Border line styles and colours
Background      Background area style and colours
Protection      Cell locked, formula hidden

For  each  group  a flag in the cell XF record specifies whether to use the
attributes  contained  in  that  XF  record  or  in  the  referenced  style
XF  record. In style XF records, these flags specify whether the attributes
will  overwrite  explicit  cell  formatting  when  the  style is applied to
a  cell. Changing a cell style (without applying this style to a cell) will
change  all  cells which already use that style and do not contain explicit
cell  attributes for the changed style attributes. If a cell XF record does
not  contain  explicit  attributes  in a group (if the attribute group flag
is not set), it repeats the attributes of its style XF record.

'''

__rev_id__ = """$Id: Formatting.py,v 1.4 2005/07/20 07:24:11 rvk Exp $"""


import BIFFRecords

class Font(object):
    ESCAPEMENT_NONE         = 0x00
    ESCAPEMENT_SUPERSCRIPT  = 0x01
    ESCAPEMENT_SUBSCRIPT    = 0x02
    
    UNDERLINE_NONE          = 0x00
    UNDERLINE_SINGLE        = 0x01
    UNDERLINE_SINGLE_ACC    = 0x21
    UNDERLINE_DOUBLE        = 0x02
    UNDERLINE_DOUBLE_ACC    = 0x22
    
    FAMILY_NONE         = 0x00
    FAMILY_ROMAN        = 0x01
    FAMILY_SWISS        = 0x02
    FAMILY_MODERN       = 0x03
    FAMILY_SCRIPT       = 0x04
    FAMILY_DECORARTIVE  = 0x05
    
    CHARSET_ANSI_LATIN          = 0x00
    CHARSET_SYS_DEFAULT         = 0x01
    CHARSET_SYMBOL              = 0x02
    CHARSET_APPLE_ROMAN         = 0x4D
    CHARSET_ANSI_JAP_SHIFT_JIS  = 0x80
    CHARSET_ANSI_KOR_HANGUL     = 0x81
    CHARSET_ANSI_KOR_JOHAB      = 0x82
    CHARSET_ANSI_CHINESE_GBK    = 0x86
    CHARSET_ANSI_CHINESE_BIG5   = 0x88
    CHARSET_ANSI_GREEK          = 0xA1
    CHARSET_ANSI_TURKISH        = 0xA2
    CHARSET_ANSI_VIETNAMESE     = 0xA3
    CHARSET_ANSI_HEBREW         = 0xB1
    CHARSET_ANSI_ARABIC         = 0xB2
    CHARSET_ANSI_BALTIC         = 0xBA
    CHARSET_ANSI_CYRILLIC       = 0xCC
    CHARSET_ANSI_THAI           = 0xDE
    CHARSET_ANSI_LATIN_II       = 0xEE
    CHARSET_OEM_LATIN_I         = 0xFF   
    
    def __init__(self):
        # twip = 1/20 of a point = 1/1440 of a inch
        # usually resolution == 96 pixels per 1 inch 
        # (rarely 120 pixels per 1 inch or another one)
        
        self.height = 0x00C8 # 200: this is font with height 10 points
        self.italic = False
        self.struck_out = False
        self.outline = False
        self.shadow = False
        self.colour_index = 0x7FFF
        self.bold = False
        self._weight = 0x0190 # 0x02BC gives bold font
        self.escapement = self.ESCAPEMENT_NONE
        self.underline = self.UNDERLINE_NONE
        self.family = self.FAMILY_NONE
        self.charset = self.CHARSET_ANSI_CYRILLIC       
        self.name = 'Arial'
                
    def get_biff_record(self):
        height = self.height
        
        options = 0x00
        if self.bold:
            options |= 0x01
            self._weight = 0x02BC
        if self.italic:
            options |= 0x02
        if self.underline != self.UNDERLINE_NONE:
            options |= 0x04
        if self.struck_out:
            options |= 0x08
        if self.outline:
            options |= 0x010
        if self.shadow:
            options |= 0x020
            
        colour_index = self.colour_index 
        weight = self._weight
        escapement = self.escapement
        underline = self.underline 
        family = self.family 
        charset = self.charset
        name = self.name
        
        return BIFFRecords.FontRecord(height, options, colour_index, weight, escapement, 
                    underline, family, charset, 
                    name)

class Alignment(object):
    HORZ_GENERAL                = 0x00
    HORZ_LEFT                   = 0x01
    HORZ_CENTER                 = 0x02
    HORZ_RIGHT                  = 0x03
    HORZ_FILLED                 = 0x04
    HORZ_JUSTIFIED              = 0x05 # BIFF4-BIFF8X
    HORZ_CENTER_ACROSS_SEL      = 0x06 # Centred across selection (BIFF4-BIFF8X)
    HORZ_DISTRIBUTED            = 0x07 # Distributed (BIFF8X)
    
    VERT_TOP                    = 0x00 
    VERT_CENTER                 = 0x01
    VERT_BOTTOM                 = 0x02
    VERT_JUSTIFIED              = 0x03 # Justified (BIFF5-BIFF8X)
    VERT_DISIRIBUTED            = 0x04 # Distributed (BIFF8X)

    DIRECTION_GENERAL           = 0x00 # BIFF8X
    DIRECTION_LR                = 0x01
    DIRECTION_RL                = 0x02

    ORIENTATION_NOT_ROTATED     = 0x00
    ORIENTATION_STACKED         = 0x01
    ORIENTATION_90_CC           = 0x02
    ORIENTATION_90_CW           = 0x03

    ROTATION_0_ANGLE            = 0x00
    ROTATION_STACKED            = 0xFF
    
    WRAP_AT_RIGHT               = 0x01
    NOT_WRAP_AT_RIGHT           = 0x00
    
    SHRINK_TO_FIT               = 0x01
    NOT_SHRINK_TO_FIT           = 0x00       

    def __init__(self):
        self.horz = self.HORZ_GENERAL
        self.vert = self.VERT_BOTTOM
        self.dire = self.DIRECTION_GENERAL
        self.orie = self.ORIENTATION_NOT_ROTATED
        self.rota = self.ROTATION_0_ANGLE
        self.wrap = self.NOT_WRAP_AT_RIGHT
        self.shri = self.NOT_SHRINK_TO_FIT
        self.inde = 0
        self.merg = 0

class Borders(object):
    NO_LINE = 0x00
    THIN    = 0x01
    MEDIUM  = 0x02
    DASHED  = 0x03
    DOTTED  = 0x04
    THICK   = 0x05
    DOUBLE  = 0x06
    HAIR    = 0x07
    #The following for BIFF8
    MEDIUM_DASHED               = 0x08
    THIN_DASH_DOTTED            = 0x09
    MEDIUM_DASH_DOTTED          = 0x0A
    THIN_DASH_DOT_DOTTED        = 0x0B
    MEDIUM_DASH_DOT_DOTTED      = 0x0C
    SLANTED_MEDIUM_DASH_DOTTED  = 0x0D
    
    NEED_DIAG1      = 0x01
    NEED_DIAG2      = 0x01
    NO_NEED_DIAG1   = 0x00
    NO_NEED_DIAG2   = 0x00

    def __init__(self):
        self.left   = self.NO_LINE
        self.right  = self.NO_LINE
        self.top    = self.NO_LINE
        self.bottom = self.NO_LINE
        self.diag   = self.NO_LINE

        self.left_colour   = 0x40
        self.right_colour  = 0x40
        self.top_colour    = 0x40
        self.bottom_colour = 0x40
        self.diag_colour   = 0x40
        
        self.need_diag1 = self.NO_NEED_DIAG1
        self.need_diag2 = self.NO_NEED_DIAG2

class Pattern(object):
    # patterns 0x00 - 0x12
    NO_PATTERN      = 0x00 
    SOLID_PATTERN   = 0x01 
    
    def __init__(self):
        self.pattern = self.NO_PATTERN
        self.pattern_fore_colour = 0x40
        self.pattern_back_colour = 0x41
        
class Protection(object):
    def __init__(self):
        self.cell_locked = 1
        self.formula_hidden = 0
        
        
if __name__ == '__main__':
    font0 = Font()
    font0.name = 'Arial'
    font1 = Font()
    font1.name = 'Arial Cyr'
    font2 = Font()
    font2.name = 'Times New Roman'
    font3 = Font()
    font3.name = 'Courier New Cyr'
    
    for font, filename in [(font0, 'font0.bin'), (font1, 'font1.bin'), (font2, 'font2.bin'), (font3, 'font3.bin')]:
        f = file(filename, 'wb')
        f.write(font.get_biff_record().get_data())
        f.close
        
########NEW FILE########
__FILENAME__ = ImportXLS
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


__rev_id__ = """$Id: ImportXLS.py,v 1.5 2005/07/20 07:24:11 rvk Exp $"""


import UnicodeUtils
import CompoundDoc
import ExcelMagic
from struct import pack, unpack


def parse_xls(filename, encoding = None):
    
    ##########################################################################

    def process_BOUNDSHEET(biff8, rec_data):
        sheet_stream_pos, visibility, sheet_type = unpack('<I2B', rec_data[:6])
        sheet_name = rec_data[6:]

        if biff8:
            chars_num, options = unpack('2B', sheet_name[:2])
            
            chars_start = 2
            runs_num = 0
            asian_phonetic_size = 0

            result = ''

            compressed = (options & 0x01) == 0
            has_asian_phonetic = (options & 0x04) != 0
            has_format_runs = (options & 0x08) != 0

            if has_format_runs:
                runs_num , = unpack('<H', sheet_name[chars_start:chars_start+2])
                chars_start += 2
            if has_asian_phonetic:
                asian_phonetic_size , = unpack('<I', sheet_name[chars_start:chars_start+4])
                chars_start += 4

            if compressed:
                chars_end = chars_start + chars_num
                result = sheet_name[chars_start:chars_end].decode('latin_1', 'replace')
            else:
                chars_end = chars_start + 2*chars_num
                result = sheet_name[chars_start:chars_end].decode('utf_16_le', 'replace')
            
            tail_size = 4*runs_num + asian_phonetic_size
        else:
            result = sheet_name[1:].decode(encoding, 'replace')
        
        return result


    def unpack2str(biff8, label_name): # 2 bytes length str
        if biff8:
            chars_num, options = unpack('<HB', label_name[:3])
            
            chars_start = 3
            runs_num = 0
            asian_phonetic_size = 0

            result = ''

            compressed = (options & 0x01) == 0
            has_asian_phonetic = (options & 0x04) != 0
            has_format_runs = (options & 0x08) != 0

            if has_format_runs:
                runs_num , = unpack('<H', label_name[chars_start:chars_start+2])
                chars_start += 2
            if has_asian_phonetic:
                asian_phonetic_size , = unpack('<I', label_name[chars_start:chars_start+4])
                chars_start += 4

            if compressed:
                chars_end = chars_start + chars_num
                result = label_name[chars_start:chars_end].decode('latin_1', 'replace')
            else:
                chars_end = chars_start + 2*chars_num
                result = label_name[chars_start:chars_end].decode('utf_16_le', 'replace')
            
            tail_size = 4*runs_num + asian_phonetic_size
        else:
            result = label_name[2:].decode(encoding, 'replace')

        return result


    def process_LABEL(biff8, rec_data):
        row_idx, col_idx, xf_idx = unpack('<3H', rec_data[:6])
        label_name = rec_data[6:]
        result = unpack2str(biff8, label_name)
        return (row_idx, col_idx, result)


    def process_LABELSST(rec_data):
        row_idx, col_idx, xf_idx, sst_idx = unpack('<3HI', rec_data)
        return (row_idx, col_idx, sst_idx)


    def process_RSTRING(biff8, rec_data):
        if biff8:
            return process_LABEL(biff8, rec_data)
        else:
            row_idx, col_idx, xf_idx, length = unpack('<4H', rec_data[:8])
            result = rec_data[8:8+length].decode(encoding, 'replace')

        return (row_idx, col_idx, result)
        

    def decode_rk(encoded):
        b0, b1, b2, b3 = unpack('4B', encoded)
        is_multed_100 = (b0 & 0x01) != 0
        is_integer = (b0 & 0x02) != 0

        if is_integer:
            result , = unpack('<i', encoded)
            result >>= 2
        else:
            ieee754 = struct.pack('8B', 0, 0, 0, 0, b0 & 0xFC, b1, b2, b3)
            result , = unpack('<d', ieee754)
        if is_multed_100:
            result /= 100.0
        
        return result


    def process_RK(rec_data):
        row_idx, col_idx, xf_idx, encoded = unpack('<3H4s', rec_data)
        result = decode_rk(encoded)
        return (row_idx, col_idx, result)


    def process_MULRK(rec_data):
        row_idx, first_col_idx = unpack('<2H', rec_data[:4])
        last_col_idx , = unpack('<H', rec_data[-2:])
        xf_rk_num = last_col_idx - first_col_idx + 1

        results = []
        for i in range(xf_rk_num):
            xf_idx, encoded = unpack('<H4s', rec_data[4+6*i : 4+6*(i+1)])
            results.append(decode_rk(encoded))

        return zip([row_idx]*xf_rk_num, range(first_col_idx, last_col_idx+1), results)


    def process_NUMBER(rec_data):
        row_idx, col_idx, xf_idx, result = unpack('<3Hd', rec_data)
        return (row_idx, col_idx, result)

    
    def process_SST(rec_data, sst_continues):
        # 0x00FC
        total_refs, total_str = unpack('<2I', rec_data[:8])
        #print total_refs, str_num

        pos = 8
        curr_block = rec_data
        curr_block_num = -1
        curr_str_num = 0
        SST = {}

        while curr_str_num < total_str:
            if pos >= len(curr_block):
                curr_block_num += 1
                curr_block = sst_continues[curr_block_num]
                pos = 0

            chars_num, options = unpack('<HB', curr_block[pos:pos+3])
            #print chars_num, options
            pos += 3

            asian_phonetic_size = 0
            runs_num = 0
            has_asian_phonetic = (options & 0x04) != 0
            has_format_runs = (options & 0x08) != 0
            if has_format_runs:
                runs_num , = unpack('<H', curr_block[pos:pos+2])
                pos += 2
            if has_asian_phonetic:
                asian_phonetic_size , = unpack('<I', curr_block[pos:pos+4])
                pos += 4

            curr_char = 0
            result = ''
            while curr_char < chars_num:
                if pos >= len(curr_block):
                    curr_block_num += 1
                    curr_block = sst_continues[curr_block_num]
                    options = ord(curr_block[0])
                    pos = 1
                #print curr_block_num

                compressed = (options & 0x01) == 0
                if compressed:
                    chars_end = pos + chars_num - curr_char
                else:
                    chars_end = pos + 2*(chars_num - curr_char)
                #print compressed, has_asian_phonetic, has_format_runs

                splitted = chars_end > len(curr_block)
                if splitted:
                    chars_end = len(curr_block)
                #print splitted, curr_char, pos, chars_end, repr(curr_block[pos:chars_end])

                if compressed:
                    result += curr_block[pos:chars_end].decode('latin_1', 'replace')
                else:
                    result += curr_block[pos:chars_end].decode('utf_16_le', 'replace')

                pos = chars_end
                curr_char = len(result)
            # end while

            # TODO: handle spanning format runs over CONTINUE blocks ???
            tail_size = 4*runs_num + asian_phonetic_size
            if len(curr_block) < pos + tail_size:
                pos = pos + tail_size - len(curr_block)
                curr_block_num += 1
                curr_block = sst_continues[curr_block_num]
            else:
                pos += tail_size

            #print result.encode('cp866')

            SST[curr_str_num] = result
            curr_str_num += 1

        return SST


    #####################################################################################
    
    import struct

    encodings = {
        0x016F: 'ascii',     #ASCII
        0x01B5: 'cp437',     #IBM PC CP-437 (US)
        0x02D0: 'cp720',     #IBM PC CP-720 (OEM Arabic)
        0x02E1: 'cp737',     #IBM PC CP-737 (Greek)
        0x0307: 'cp775',     #IBM PC CP-775 (Baltic)
        0x0352: 'cp850',     #IBM PC CP-850 (Latin I)
        0x0354: 'cp852',     #IBM PC CP-852 (Latin II (Central European))
        0x0357: 'cp855',     #IBM PC CP-855 (Cyrillic)
        0x0359: 'cp857',     #IBM PC CP-857 (Turkish)
        0x035A: 'cp858',     #IBM PC CP-858 (Multilingual Latin I with Euro)
        0x035C: 'cp860',     #IBM PC CP-860 (Portuguese)
        0x035D: 'cp861',     #IBM PC CP-861 (Icelandic)
        0x035E: 'cp862',     #IBM PC CP-862 (Hebrew)
        0x035F: 'cp863',     #IBM PC CP-863 (Canadian (French))
        0x0360: 'cp864',     #IBM PC CP-864 (Arabic)
        0x0361: 'cp865',     #IBM PC CP-865 (Nordic)
        0x0362: 'cp866',     #IBM PC CP-866 (Cyrillic (Russian))
        0x0365: 'cp869',     #IBM PC CP-869 (Greek (Modern))
        0x036A: 'cp874',     #Windows CP-874 (Thai)
        0x03A4: 'cp932',     #Windows CP-932 (Japanese Shift-JIS)
        0x03A8: 'cp936',     #Windows CP-936 (Chinese Simplified GBK)
        0x03B5: 'cp949',     #Windows CP-949 (Korean (Wansung))
        0x03B6: 'cp950',     #Windows CP-950 (Chinese Traditional BIG5)
        0x04B0: 'utf_16_le', #UTF-16 (BIFF8)
        0x04E2: 'cp1250',    #Windows CP-1250 (Latin II) (Central European)
        0x04E3: 'cp1251',    #Windows CP-1251 (Cyrillic)
        0x04E4: 'cp1252',    #Windows CP-1252 (Latin I) (BIFF4-BIFF7)
        0x04E5: 'cp1253',    #Windows CP-1253 (Greek)
        0x04E6: 'cp1254',    #Windows CP-1254 (Turkish)
        0x04E7: 'cp1255',    #Windows CP-1255 (Hebrew)
        0x04E8: 'cp1256',    #Windows CP-1256 (Arabic)
        0x04E9: 'cp1257',    #Windows CP-1257 (Baltic)
        0x04EA: 'cp1258',    #Windows CP-1258 (Vietnamese)
        0x0551: 'cp1361',    #Windows CP-1361 (Korean (Johab))
        0x2710: 'mac_roman', #Apple Roman
        0x8000: 'mac_roman', #Apple Roman
        0x8001: 'cp1252'     #Windows CP-1252 (Latin I) (BIFF2-BIFF3)
    }

    biff8 = True
    SST = {}
    sheets = []
    sheet_names = []
    values = {}
    ws_num = 0
    BOFs = 0
    EOFs = 0

    # Inside MS Office document looks like filesystem
    # We need extract stream named 'Workbook' or 'Book'
    ole_streams = CompoundDoc.Reader(filename).STREAMS

    if 'Workbook' in ole_streams:
        workbook_stream = ole_streams['Workbook']
    elif 'Book' in ole_streams:
        workbook_stream = ole_streams['Book']
    else:
        raise Exception, 'No workbook stream in file.'

    workbook_stream_len = len(workbook_stream)
    stream_pos = 0
    
    # Excel's method of data storing is based on 
    # ancient technology "TLV" (Type, Length, Value).
    # In addition, if record size grows to some limit
    # Excel writes CONTINUE records
    while stream_pos < workbook_stream_len and EOFs <= ws_num:
        rec_id, data_size = unpack('<2H', workbook_stream[stream_pos:stream_pos+4])
        stream_pos += 4
        
        rec_data = workbook_stream[stream_pos:stream_pos+data_size]
        stream_pos += data_size

        if rec_id == 0x0809: # BOF
            #print 'BOF', 
            BOFs += 1
            ver, substream_type = unpack('<2H', rec_data[:4])
            if substream_type == 0x0005:
                # workbook global substream
                biff8 = ver >= 0x0600
            elif substream_type == 0x0010:
                # worksheet substream
                pass
            else: # skip chart stream or unknown stream
            # stream offsets may be used from BOUNDSHEET record
                rec_id, data_size = unpack('<2H', workbook_stream[stream_pos:stream_pos+4])
                while rec_id != 0x000A: # EOF
                    #print 'SST CONTINUE'
                    stream_pos += 4
                    stream_pos += data_size
                    rec_id, data_size = unpack('<2H', workbook_stream[stream_pos:stream_pos+4])
            #print 'BIFF8 == ', biff8
        elif rec_id == 0x000A: # EOF
            #print 'EOF'
            if BOFs > 1:
                sheets.extend([values])
                values = {}
            EOFs += 1
        elif rec_id == 0x0042: # CODEPAGE
            cp ,  = unpack('<H', rec_data)
            #print 'CODEPAGE', hex(cp)
            if not encoding:
                encoding = encodings[cp]
            #print encoding
        elif rec_id == 0x0085: # BOUNDSHEET
            #print 'BOUNDSHEET',
            ws_num += 1
            b = process_BOUNDSHEET(biff8, rec_data)
            sheet_names.extend([b])
            #print b.encode('cp866')
        elif rec_id == 0x00FC: # SST
            #print 'SST'
            sst_data = rec_data
            sst_continues = []
            rec_id, data_size = unpack('<2H', workbook_stream[stream_pos:stream_pos+4])
            while rec_id == 0x003C: # CONTINUE
                #print 'SST CONTINUE'
                stream_pos += 4
                rec_data = workbook_stream[stream_pos:stream_pos+data_size]
                sst_continues.extend([rec_data])
                stream_pos += data_size
                rec_id, data_size = unpack('<2H', workbook_stream[stream_pos:stream_pos+4])
            SST = process_SST(sst_data, sst_continues)
        elif rec_id == 0x00FD: # LABELSST
            #print 'LABELSST',
            r, c, i = process_LABELSST(rec_data)
            values[(r, c)] = SST[i]
            #print r, c, SST[i].encode('cp866')
        elif rec_id == 0x0204: # LABEL
            #print 'LABEL',
            r, c, b = process_LABEL(biff8, rec_data)
            values[(r, c)] = b
            #print r, c, b.encode('cp866')
        elif rec_id == 0x00D6: # RSTRING
            #print 'RSTRING',
            r, c, b = process_RSTRING(biff8, rec_data)
            values[(r, c)] = b
            #print r, c, b.encode('cp866')
        elif rec_id == 0x027E: # RK
            #print 'RK',
            r, c, b = process_RK(rec_data)
            values[(r, c)] = b
            #print r, c, b
        elif rec_id == 0x00BD: # MULRK
            #print 'MULRK',
            for r, c, b in process_MULRK(rec_data):
                values[(r, c)] = b
            #print r, c, b
        elif rec_id == 0x0203: # NUMBER
            #print 'NUMBER',
            r, c, b = process_NUMBER(rec_data)
            values[(r, c)] = b
            #print r, c, b
        elif rec_id == 0x0006: # FORMULA
            #print 'FORMULA',
            r, c, x = unpack('<3H', rec_data[0:6])
            if rec_data[12] == '\xFF' and rec_data[13] == '\xFF':
                if rec_data[6] == '\x00':
                    got_str = False
                    if ord(rec_data[14]) & 8:
                        # part of shared formula
                        rec_id, data_size = unpack('<2H', workbook_stream[stream_pos:stream_pos+4])
                        stream_pos += 4                      
                        rec_data = workbook_stream[stream_pos:stream_pos+data_size]
                        stream_pos += data_size
                        if rec_id == 0x0207: # STRING
                            got_str = True
                        elif rec_id not in (0x0221, 0x04BC, 0x0236, 0x0037, 0x0036):
                            raise Exception("Expected ARRAY, SHRFMLA, TABLEOP* or STRING record")
                    if not got_str:
                        rec_id, data_size = unpack('<2H', workbook_stream[stream_pos:stream_pos+4])
                        stream_pos += 4                      
                        rec_data = workbook_stream[stream_pos:stream_pos+data_size]
                        stream_pos += data_size
                        if rec_id != 0x0207: # STRING
                            raise Exception("Expected STRING record")
                    values[(r, c)] = unpack2str(biff8, rec_data)
                elif rec_data[6] == '\x01':
                    # boolean 
                    v = ord(rec_data[8])
                    values[(r, c)] = bool(v)
                elif rec_data[6] == '\x02':
                    # error
                    v = ord(rec_data[8])
                    if v in ExcelMagic.error_msg_by_code:
                        values[(r, c)] = ExcelMagic.error_msg_by_code[v]
                    else:
                        values[(r, c)] = u'#UNKNOWN ERROR!'
                elif rec_data[6] == '\x03':
                    # empty
                    values[(r, c)] = u''
                else:
                    raise Exception("Unknown value for formula result")
            else:
                # 64-bit float
                d, = unpack("<d", rec_data[6:14])
                values[(r, c)] = d

    encoding = None
    return zip(sheet_names, sheets)

########NEW FILE########
__FILENAME__ = Row
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


__rev_id__ = """$Id: Row.py,v 1.6 2005/08/11 08:53:48 rvk Exp $"""


import BIFFRecords
from Deco import *
from Worksheet import Worksheet
import Style
import Cell
import ExcelFormula
import datetime as dt


class Row(object):
    __slots__ = ["__init__", 
                 "__adjust_height",
                 "__adjust_bound_col_idx",
                 "__excel_date_dt",
                 "get_height_in_pixels",
                 "set_style",
                 "get_xf_index",
                 "get_cells_count",
                 "get_min_col",
                 "get_max_col",
                 "get_str_count",
                 "get_row_biff_data",
                 "get_cells_biff_data",
                 "get_index",
                 "write",
                 "write_blanks",
                 # private variables
                 "__idx",
                 "__parent",
                 "__parent_wb",
                 "__cells",
                 "__min_col_idx",
                 "__max_col_idx",
                 "__total_str",
                 "__xf_index",
                 "__has_default_format",
                 "__height_in_pixels",
                 # public variables
                 "height",
                 "has_default_height",
                 "level",
                 "collapse",
                 "hidden",
                 "space_above",
                 "space_below"]

    #################################################################
    ## Constructor
    #################################################################
    def __init__(self, index, parent_sheet):
        self.__idx = index
        self.__parent = parent_sheet
        self.__parent_wb = parent_sheet.get_parent()
        self.__cells = []
        self.__min_col_idx = 0
        self.__max_col_idx = 0
        self.__total_str = 0
        self.__xf_index = 0x0F
        self.__has_default_format = 0
        self.__height_in_pixels = 0x11
        
        self.height = 0x00FF
        self.has_default_height = 0x00
        self.level = 0
        self.collapse = 0
        self.hidden = 0
        self.space_above = 0
        self.space_below = 0


    def __adjust_height(self, style):
        twips = style.font.height
        points = float(twips)/20.0
        # Cell height in pixels can be calcuted by following approx. formula:
        # cell height in pixels = font height in points * 83/50 + 2/5
        # It works when screen resolution is 96 dpi 
        pix = int(round(points*83.0/50.0 + 2.0/5.0))
        if pix > self.__height_in_pixels:
            self.__height_in_pixels = pix


    def __adjust_bound_col_idx(self, *args):
        for arg in args:
            if arg < self.__min_col_idx:
                self.__min_col_idx = arg
            elif arg > self.__max_col_idx:
                self.__max_col_idx = arg

    def __excel_date_dt(self, date):
        if isinstance(date, dt.date) and (not isinstance(date, dt.datetime)):
            epoch = dt.date(1899, 12, 31)
        elif isinstance(date, dt.time):
            date = dt.datetime.combine(dt.datetime(1900, 1, 1), date)
            epoch = dt.datetime(1900, 1, 1, 0, 0, 0)
        else:
            epoch = dt.datetime(1899, 12, 31, 0, 0, 0)
        delta = date - epoch
        xldate = delta.days + float(delta.seconds) / (24*60*60)
        # Add a day for Excel's missing leap day in 1900
        if xldate > 59:
            xldate += 1
        return xldate

    def get_height_in_pixels(self):
        return self.__height_in_pixels


    @accepts(object, Style.XFStyle)
    def set_style(self, style):
        self.__adjust_height(style)
        self.__xf_index = self.__parent_wb.add_style(style)

            
    def get_xf_index(self):
        return self.__xf_index

    
    def get_cells_count(self):
        return len(self.__cells)

    
    def get_min_col(self):
        return self.__min_col_idx

        
    def get_max_col(self):
        return self.__min_col_idx

        
    def get_str_count(self):
        return self.__total_str


    def get_row_biff_data(self):
        height_options = (self.height & 0x07FFF) 
        height_options |= (self.has_default_height & 0x01) << 15

        options =  (self.level & 0x07) << 0
        options |= (self.collapse & 0x01) << 4
        options |= (self.hidden & 0x01) << 5
        options |= (0x00 & 0x01) << 6
        options |= (0x01 & 0x01) << 8
        if self.__xf_index != 0x0F:
            options |= (0x01 & 0x01) << 7
        else:
            options |= (0x00 & 0x01) << 7
        options |= (self.__xf_index & 0x0FFF) << 16 
        options |= (0x00 & self.space_above) << 28
        options |= (0x00 & self.space_below) << 29
        
        return BIFFRecords.RowRecord(self.__idx, self.__min_col_idx, self.__max_col_idx, height_options, options).get()                                              
                        

    def get_cells_biff_data(self):
        return ''.join([ cell.get_biff_data() for cell in self.__cells ])


    def get_index(self):
        return self.__idx


    @accepts(object, int, (str, unicode, int, float, dt.datetime, dt.time, dt.date, ExcelFormula.Formula), Style.XFStyle)
    def write(self, col, label, style):
        self.__adjust_height(style)
        self.__adjust_bound_col_idx(col)
        if isinstance(label, (str, unicode)):
            if len(label) > 0:
                self.__cells.extend([ Cell.StrCell(self, col, self.__parent_wb.add_style(style), self.__parent_wb.add_str(label)) ])
                self.__total_str += 1
            else:
                self.__cells.extend([ Cell.BlankCell(self, col, self.__parent_wb.add_style(style)) ])
        elif isinstance(label, (int, float)):
            self.__cells.extend([ Cell.NumberCell(self, col, self.__parent_wb.add_style(style), label) ])            
        elif isinstance(label, (dt.datetime, dt.time)):
            self.__cells.extend([ Cell.NumberCell(self, col, self.__parent_wb.add_style(style), self.__excel_date_dt(label)) ])
        else:
            self.__cells.extend([ Cell.FormulaCell(self, col, self.__parent_wb.add_style(style), label) ])

    @accepts(object, int, int, Style.XFStyle)                        
    def write_blanks(self, c1, c2, style):
        self.__adjust_height(style)
        self.__adjust_bound_col_idx(c1, c2)
        self.__cells.extend([ Cell.MulBlankCell(self, c1, c2, self.__parent_wb.add_style(style)) ])

        
        
########NEW FILE########
__FILENAME__ = Style
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


__rev_id__ = """$Id: Style.py,v 1.4 2005/07/20 07:24:11 rvk Exp $"""


import Formatting
from BIFFRecords import *


_default_num_format = 'general'
_default_font = Formatting.Font()
_default_alignment = Formatting.Alignment()
_default_borders = Formatting.Borders()
_default_pattern = Formatting.Pattern()
_default_protection = Formatting.Protection()

class XFStyle(object):
    
    def __init__(self):
        self.num_format_str  = _default_num_format
        self.font            = _default_font 
        self.alignment       = _default_alignment
        self.borders         = _default_borders
        self.pattern         = _default_pattern 
        self.protection      = _default_protection

class StyleCollection(object):
    _std_num_fmt_list = [
            'general',
            '0',
            '0.00',
            '#,##0',
            '#,##0.00',
            '"$"#,##0_);("$"#,##',
            '"$"#,##0_);[Red]("$"#,##',
            '"$"#,##0.00_);("$"#,##',
            '"$"#,##0.00_);[Red]("$"#,##',
            '0%',
            '0.00%',
            '0.00E+00',
            '# ?/?',
            '# ??/??',
            'M/D/YY',
            'D-MMM-YY',
            'D-MMM',
            'MMM-YY',
            'h:mm AM/PM',
            'h:mm:ss AM/PM',
            'h:mm',
            'h:mm:ss',
            'M/D/YY h:mm',
            '_(#,##0_);(#,##0)',
            '_(#,##0_);[Red](#,##0)',
            '_(#,##0.00_);(#,##0.00)',
            '_(#,##0.00_);[Red](#,##0.00)',
            '_("$"* #,##0_);_("$"* (#,##0);_("$"* "-"_);_(@_)',
            '_(* #,##0_);_(* (#,##0);_(* "-"_);_(@_)',
            '_("$"* #,##0.00_);_("$"* (#,##0.00);_("$"* "-"??_);_(@_)',
            '_(* #,##0.00_);_(* (#,##0.00);_(* "-"??_);_(@_)',
            'mm:ss',
            '[h]:mm:ss',
            'mm:ss.0',
            '##0.0E+0',
            '@'   
    ]

    def __init__(self):
        self._fonts = {}
        self._fonts[Formatting.Font()] = 0
        self._fonts[Formatting.Font()] = 1
        self._fonts[Formatting.Font()] = 2
        self._fonts[Formatting.Font()] = 3
        # The font with index 4 is omitted in all BIFF versions
        self._fonts[Formatting.Font()] = 5
        
        self._num_formats = {}
        for fmtidx, fmtstr in zip(range(0, 23), StyleCollection._std_num_fmt_list[0:23]):
            self._num_formats[fmtstr] = fmtidx 
        for fmtidx, fmtstr in zip(range(37, 50), StyleCollection._std_num_fmt_list[23:]):
            self._num_formats[fmtstr] = fmtidx 

        self._xf = {}
        self.default_style = XFStyle()
        self._default_xf = self._add_style(self.default_style)[0]
        
    def add(self, style): 
        if style == None:
            return 0x10
        return self._add_style(style)[1]
    
    def _add_style(self, style):
        num_format_str = style.num_format_str
        if num_format_str in self._num_formats:
            num_format_idx = self._num_formats[num_format_str]
        else:
            num_format_idx = 163 + len(self._num_formats) - len(StyleCollection._std_num_fmt_list)
            self._num_formats[num_format_str] = num_format_idx
            
        font = style.font
        if font in self._fonts:
            font_idx = self._fonts[font]
        else:
            font_idx = len(self._fonts) + 1
            self._fonts[font] = font_idx
            
        xf = (font_idx, num_format_idx, style.alignment, style.borders, style.pattern, style.protection)
        
        if xf in self._xf:
            xf_index = self._xf[xf]
        else:
            xf_index = 0x10 + len(self._xf)
            self._xf[xf] = xf_index
            
        return xf, xf_index
        
    def get_biff_data(self):
        result = ''
        result += self._all_fonts()
        result += self._all_num_formats()
        result += self._all_cell_styles()
        result += self._all_styles()
        return result 
            
    def _all_fonts(self):
        result = ''
        i = sorted([(v, k) for k, v in self._fonts.items()])
        for font_idx, font in i:
            result += font.get_biff_record().get()
        return result
    
    def _all_num_formats(self):
        result = ''
        i = sorted([(v, k) for k, v in self._num_formats.items() if v>=163])
        for fmtidx, fmtstr in i:
            result += NumberFormatRecord(fmtidx, fmtstr).get()
        return result
    
    def _all_cell_styles(self):        
        result = ''
        for i in range(0, 16):
            result += XFRecord(self._default_xf, 'style').get()
            
        i = sorted([(v, k) for k, v in self._xf.items()])                            
        for xf_idx, xf in i:
            result += XFRecord(xf).get()
        return result
        
    def _all_styles(self):
        return StyleRecord().get()
        
if __name__ == '__main__':
    sc = StyleCollection()
    f = file('styles.bin', 'wb')
    f.write(sc.get_biff_data())
    f.close()

            
########NEW FILE########
__FILENAME__ = UnicodeUtils
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


'''
From BIFF8 on, strings are always stored using UTF-16LE  text encoding. The
character  array  is  a  sequence  of  16-bit  values4.  Additionally it is
possible  to  use  a  compressed  format, which omits the high bytes of all
characters, if they are all zero.

The following tables describe the standard format of the entire string, but
in many records the strings differ from this format. This will be mentioned
separately. It is possible (but not required) to store Rich-Text formatting
information  and  Asian  phonetic information inside a Unicode string. This
results  in  four  different  ways  to  store a string. The character array
is not zero-terminated.

The  string  consists  of  the  character count (as usual an 8-bit value or
a  16-bit value), option flags, the character array and optional formatting
information.  If the string is empty, sometimes the option flags field will
not occur. This is mentioned at the respective place.

Offset  Size    Contents
0       1 or 2  Length of the string (character count, ln)
1 or 2  1       Option flags:
                  Bit   Mask Contents
                  0     01H  Character compression (ccompr):
                               0 = Compressed (8-bit characters)
                               1 = Uncompressed (16-bit characters)
                  2     04H  Asian phonetic settings (phonetic):
                               0 = Does not contain Asian phonetic settings
                               1 = Contains Asian phonetic settings
                  3     08H  Rich-Text settings (richtext):
                               0 = Does not contain Rich-Text settings
                               1 = Contains Rich-Text settings
[2 or 3] 2      (optional, only if richtext=1) Number of Rich-Text formatting runs (rt)
[var.]   4      (optional, only if phonetic=1) Size of Asian phonetic settings block (in bytes, sz)
var.     ln or 
         2ln   Character array (8-bit characters or 16-bit characters, dependent on ccompr)
[var.]   4rt   (optional, only if richtext=1) List of rt formatting runs 
[var.]   sz     (optional, only if phonetic=1) Asian Phonetic Settings Block 
'''


__rev_id__ = """$Id: UnicodeUtils.py,v 1.4 2005/07/20 07:24:11 rvk Exp $"""


import struct


DEFAULT_ENCODING = 'cp1251'

def u2ints(ustr):
    ints = [ord(uchr) for uchr in ustr]
    return ints

def u2bytes(ustr):
    ints = u2ints(ustr)
    return struct.pack('<' + 'H'*len(ints), *ints)

def upack2(_str):
    try:
        ustr = u2bytes(unicode(_str, 'ascii'))
        return struct.pack('<HB', len(_str), 0) + _str    
    except:
        if isinstance(_str, unicode):
            ustr = u2bytes(_str)
        else:
            ustr = u2bytes(unicode(_str, DEFAULT_ENCODING))
        return struct.pack('<HB', len(_str), 1) + ustr

def upack1(_str):
    try:
        ustr = u2bytes(unicode(_str, 'ascii'))
        return struct.pack('BB', len(_str), 0) + _str    
    except:
        if isinstance(_str, unicode):
            ustr = u2bytes(_str)
        else:
            ustr = u2bytes(unicode(_str, DEFAULT_ENCODING))
        return struct.pack('BB', len(_str), 1) + ustr

if __name__ == '__main__':   
    f = file('out0.bin', 'wb')
    f.write(u2bytes(': unicode'))
    f.close()

    f = file('out1.bin', 'wb')
    f.write(upack1(': unicode'))
    f.close()

    f = file('out2.bin', 'wb')
    f.write(upack2(': unicode'))
    f.close()




########NEW FILE########
__FILENAME__ = Utils
# pyXLWriter: A library for generating Excel Spreadsheets
# Copyright (c) 2004 Evgeny Filatov <fufff@users.sourceforge.net>
# Copyright (c) 2002-2004 John McNamara (Perl Spreadsheet::WriteExcel)
#
# This library is free software; you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published by
# the Free Software Foundation; either version 2.1 of the License, or
# (at your option) any later version.
#
# This library is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser
# General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with this library; if not, write to the Free Software Foundation,
# Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
#----------------------------------------------------------------------------
# This module was written/ported from PERL Spreadsheet::WriteExcel module
# The author of the PERL Spreadsheet::WriteExcel module is John McNamara
# <jmcnamara@cpan.org>
#----------------------------------------------------------------------------
# See the README.txt distributed with pyXLWriter for more details.

# Portions are (C) Roman V. Kiseliov, 2005

"""pyXLWriter.utilites

Utilities for work with reference to cells

"""
__rev_id__ = """$Id: Utils.py,v 1.1 2005/08/11 08:53:48 rvk Exp $"""

import re
from struct import pack
from ExcelMagic import MAX_ROW, MAX_COL


_re_cell_ex = re.compile(r"(\$?)([A-I]?[A-Z])(\$?)(\d+)")
_re_row_range = re.compile(r"\$?(\d+):\$?(\d+)")
_re_col_range = re.compile(r"\$?([A-I]?[A-Z]):\$?([A-I]?[A-Z])")
_re_cell_range = re.compile(r"\$?([A-I]?[A-Z]\$?\d+):\$?([A-I]?[A-Z]\$?\d+)")
_re_cell_ref = re.compile(r"\$?([A-I]?[A-Z]\$?\d+)")


def col_by_name(colname):
    """
    """
    col = 0
    pow = 1
    for i in xrange(len(colname)-1, -1, -1):
        ch = colname[i]
        col += (ord(ch) - ord('A') + 1) * pow
        pow *= 26
    return col - 1


def cell_to_rowcol(cell):
    """Convert an Excel cell reference string in A1 notation
    to numeric row/col notation.
  
    Returns: row, col, row_abs, col_abs
 
    """
    m = _re_cell_ex.match(cell)
    if not m:
        raise Exception("Error in cell format")
    col_abs, col, row_abs, row = m.groups()
    row_abs = bool(row_abs)
    col_abs = bool(col_abs)
    row = int(row) - 1
    col = col_by_name(col)
    return row, col, row_abs, col_abs


def cell_to_rowcol2(cell):
    """Convert an Excel cell reference string in A1 notation
    to numeric row/col notation.
  
    Returns: row, col
    
    """
    m = _re_cell_ex.match(cell)
    if not m:
        raise Exception("Error in cell format")
    col_abs, col, row_abs, row = m.groups()
    # Convert base26 column string to number
    # All your Base are belong to us.
    row = int(row) - 1
    col = col_by_name(col)
    return row, col
    
    
def rowcol_to_cell(row, col, row_abs=False, col_abs=False):
    """Convert numeric row/col notation to an Excel cell reference string in
    A1 notation.
 
    """
    d = col // 26
    m = col % 26
    chr1 = ""    # Most significant character in AA1
    if row_abs:
        row_abs = '$'
    else:
        row_abs = ''
    if col_abs:
        col_abs = '$'
    else:
        col_abs = ''
    if d > 0:
        chr1 = chr(ord('A') + d  - 1)
    chr2 = chr(ord('A') + m)
    # Zero index to 1-index
    return col_abs + chr1 + chr2 + row_abs + str(row + 1)


def cellrange_to_rowcol_pair(cellrange):
    """Convert cell range string in A1 notation to numeric row/col 
    pair.

    Returns: row1, col1, row2, col2
    
    """
    cellrange = cellrange.upper()
    # Convert a row range: '1:3'
    res = _re_row_range.match(cellrange)
    if res:
        row1 = int(res.group(1)) - 1
        col1 = 0
        row2 = int(res.group(2)) - 1
        col2 = -1
        return row1, col1, row2, col2
    # Convert a column range: 'A:A' or 'B:G'.
    # A range such as A:A is equivalent to A1:A16384, so add rows as required
    res = _re_col_range.match(cellrange)
    if res:
        col1 = col_by_name(res.group(1))
        row1 = 0
        col2 = col_by_name(res.group(2))
        row2 = -1
        return row1, col1, row2, col2
    # Convert a cell range: 'A1:B7'
    res = _re_cell_range.match(cellrange)
    if res:
        row1, col1 = cell_to_rowcol2(res.group(1))
        row2, col2 = cell_to_rowcol2(res.group(2))
        return row1, col1, row2, col2
    # Convert a cell reference: 'A1' or 'AD2000'
    res = _re_cell_ref.match(cellrange)
    if res:
        row1, col1 = cell_to_rowcol2(res.group(1))
        return row1, col1, row1, col1
    raise Exception("Unknown cell reference %s" % (cell))


def cell_to_packed_rowcol(cell):
    """ pack row and column into the required 4 byte format """
    row, col, row_abs, col_abs = cell_to_rowcol(cell)
    if col >= MAX_COL:
        raise Exception("Column %s greater than IV in formula" % cell)
    if row >= MAX_ROW: # this for BIFF8. for BIFF7 available 2^14
        raise Exception("Row %s greater than %d in formula" % (cell, MAX_ROW))
    col |= int(not row_abs) << 15
    col |= int(not col_abs) << 14
    return row, col

########NEW FILE########
__FILENAME__ = Workbook
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


'''
Record Order in BIFF8
  Workbook Globals Substream
      BOF Type = workbook globals
      Interface Header
      MMS
      Interface End
      WRITEACCESS
      CODEPAGE
      DSF
      TABID
      FNGROUPCOUNT
      Workbook Protection Block
            WINDOWPROTECT
            PROTECT
            PASSWORD
            PROT4REV
            PROT4REVPASS
      BACKUP
      HIDEOBJ 
      WINDOW1 
      DATEMODE 
      PRECISION
      REFRESHALL
      BOOKBOOL 
      FONT +
      FORMAT *
      XF +
      STYLE +
    ? PALETTE
      USESELFS
    
      BOUNDSHEET +
    
      COUNTRY 
    ? Link Table 
      SST 
      ExtSST
      EOF
'''


__rev_id__ = """$Id: Workbook.py,v 1.4 2005/07/20 07:24:11 rvk Exp $"""


import BIFFRecords
import Style
from Deco import accepts, returns


class Workbook(object):

    #################################################################
    ## Constructor
    #################################################################
    @accepts(object)
    def __init__(self):
        self.__owner = 'None'       
        self.__country_code = 0x07 
        self.__wnd_protect = 0
        self.__obj_protect = 0
        self.__protect = 0        
        self.__backup_on_save = 0
        # for WINDOW1 record
        self.__hpos_twips = 0x01E0
        self.__vpos_twips = 0x005A
        self.__width_twips = 0x3FCF
        self.__height_twips = 0x2A4E
        
        self.__active_sheet = 0
        self.__first_tab_index = 0
        self.__selected_tabs = 0x01
        self.__tab_width_twips = 0x0258
        
        self.__wnd_hidden = 0
        self.__wnd_mini = 0
        self.__hscroll_visible = 1
        self.__vscroll_visible = 1
        self.__tabs_visible = 1

        self.__styles = Style.StyleCollection()
         
        self.__dates_1904 = 0
        self.__use_cell_values = 1
        
        self.__sst = BIFFRecords.SharedStringTable()
        
        self.__worksheets = []

    #################################################################
    ## Properties, "getters", "setters"
    #################################################################

    @accepts(object, str)
    def set_owner(self, value):
        self.__owner = value

    def get_owner(self):
        return self.__owner

    owner = property(get_owner, set_owner)

    #################################################################

    @accepts(object, int)
    def set_country_code(self, value):
        self.__country_code = value

    def get_country_code(self):
        return self.__country_code

    country_code = property(get_country_code, set_country_code)

    #################################################################

    @accepts(object, bool)
    def set_wnd_protect(self, value):
        self.__wnd_protect = int(value)

    def get_wnd_protect(self):
        return bool(self.__wnd_protect)

    wnd_protect = property(get_wnd_protect, set_wnd_protect)

    #################################################################

    @accepts(object, bool)
    def set_obj_protect(self, value):
        self.__obj_protect = int(value)

    def get_obj_protect(self):
        return bool(self.__obj_protect)

    obj_protect = property(get_obj_protect, set_obj_protect)

    #################################################################

    @accepts(object, bool)
    def set_protect(self, value):
        self.__protect = int(value)

    def get_protect(self):
        return bool(self.__protect)

    protect = property(get_protect, set_protect)
    
    #################################################################

    @accepts(object, bool)
    def set_backup_on_save(self, value):
        self.__backup_on_save = int(value)

    def get_backup_on_save(self):
        return bool(self.__backup_on_save)

    backup_on_save = property(get_backup_on_save, set_backup_on_save)

    #################################################################

    @accepts(object, int)
    def set_hpos(self, value):
        self.__hpos_twips = value & 0xFFFF

    def get_hpos(self):
        return self.__hpos_twips

    hpos = property(get_hpos, set_hpos)

    #################################################################

    @accepts(object, int)
    def set_vpos(self, value):
        self.__vpos_twips = value & 0xFFFF

    def get_vpos(self):
        return self.__vpos_twips

    vpos = property(get_vpos, set_vpos)

    #################################################################

    @accepts(object, int)
    def set_width(self, value):
        self.__width_twips = value & 0xFFFF

    def get_width(self):
        return self.__width_twips

    width = property(get_width, set_width)

    #################################################################

    @accepts(object, int)
    def set_height(self, value):
        self.__height_twips = value & 0xFFFF

    def get_height(self):
        return self.__height_twips

    height = property(get_height, set_height)

    #################################################################

    @accepts(object, int)
    def set_active_sheet(self, value):
        self.__active_sheet = value & 0xFFFF
        self.__first_tab_index = self.__active_sheet

    def get_active_sheet(self):
        return self.__active_sheet

    active_sheet = property(get_active_sheet, set_active_sheet)

    #################################################################

    @accepts(object, int)
    def set_tab_width(self, value):
        self.__tab_width_twips = value & 0xFFFF

    def get_tab_width(self):
        return self.__tab_width_twips

    tab_width = property(get_tab_width, set_tab_width)

    #################################################################

    @accepts(object, bool)
    def set_wnd_visible(self, value):
        self.__wnd_hidden = int(not value)

    def get_wnd_visible(self):
        return not bool(self.__wnd_hidden)

    wnd_visible = property(get_wnd_visible, set_wnd_visible)

    #################################################################

    @accepts(object, bool)
    def set_wnd_mini(self, value):
        self.__wnd_mini = int(value)

    def get_wnd_mini(self):
        return bool(self.__wnd_mini)

    wnd_mini = property(get_wnd_mini, set_wnd_mini)

    #################################################################

    @accepts(object, bool)
    def set_hscroll_visible(self, value):
        self.__hscroll_visible = int(value)

    def get_hscroll_visible(self):
        return bool(self.__hscroll_visible)

    hscroll_visible = property(get_hscroll_visible, set_hscroll_visible)

    #################################################################

    @accepts(object, bool)
    def set_vscroll_visible(self, value):
        self.__vscroll_visible = int(value)

    def get_vscroll_visible(self):
        return bool(self.__vscroll_visible)

    vscroll_visible = property(get_vscroll_visible, set_vscroll_visible)

    #################################################################

    @accepts(object, bool)
    def set_tabs_visible(self, value):
        self.__tabs_visible = int(value)

    def get_tabs_visible(self):
        return bool(self.__tabs_visible)

    tabs_visible = property(get_tabs_visible, set_tabs_visible)

    #################################################################

    @accepts(object, bool)
    def set_dates_1904(self, value):
        self.__dates_1904 = int(value)

    def get_dates_1904(self):
        return bool(self.__dates_1904)

    dates_1904 = property(get_dates_1904, set_dates_1904)

    #################################################################

    @accepts(object, bool)
    def set_use_cell_values(self, value):
        self.__use_cell_values = int(value)

    def get_use_cell_values(self):
        return bool(self.__use_cell_values)

    use_cell_values = property(get_use_cell_values, set_use_cell_values)

    #################################################################

    def get_default_style(self):
        return self.__styles.default_style

    default_style = property(get_default_style)

    ##################################################################
    ## Methods
    ##################################################################

    @accepts(object, Style.XFStyle)
    def add_style(self, style):
        return self.__styles.add(style)

    @accepts(object, (str, unicode))    
    def add_str(self, s):
        return self.__sst.add_str(s)
        
    @accepts(object, str)    
    def str_index(self, s):
        return self.__sst.str_index(s)
        
    @accepts(object, (str, unicode))    
    def add_sheet(self, sheetname):
        import Worksheet
        self.__worksheets.append(Worksheet.Worksheet(sheetname, self))
        return self.__worksheets[-1]

    @accepts(object, int)    
    def get_sheet(self, sheetnum):
        return self.__worksheets[sheetnum]
        
    ##################################################################
    ## BIFF records generation
    ##################################################################

    def __bof_rec(self):
        return BIFFRecords.Biff8BOFRecord(BIFFRecords.Biff8BOFRecord.BOOK_GLOBAL).get()

    def __eof_rec(self):
        return BIFFRecords.EOFRecord().get()
        
    def __intf_hdr_rec(self):
        return BIFFRecords.InteraceHdrRecord().get()

    def __intf_end_rec(self):
        return BIFFRecords.InteraceEndRecord().get()

    def __intf_mms_rec(self):
        return BIFFRecords.MMSRecord().get()

    def __write_access_rec(self):
        return BIFFRecords.WriteAccessRecord(self.__owner).get()

    def __wnd_protect_rec(self):
        return BIFFRecords.WindowProtectRecord(self.__wnd_protect).get()

    def __obj_protect_rec(self):
        return BIFFRecords.ObjectProtectRecord(self.__obj_protect).get()

    def __protect_rec(self):
        return BIFFRecords.ProtectRecord(self.__protect).get()

    def __password_rec(self):
        return BIFFRecords.PasswordRecord().get()

    def __prot4rev_rec(self):
        return BIFFRecords.Prot4RevRecord().get()

    def __prot4rev_pass_rec(self):
        return BIFFRecords.Prot4RevPassRecord().get()

    def __backup_rec(self):
        return BIFFRecords.BackupRecord(self.__backup_on_save).get()
        
    def __hide_obj_rec(self):
        return BIFFRecords.HideObjRecord().get()
        
    def __window1_rec(self):
        flags = 0
        flags |= (self.__wnd_hidden) << 0
        flags |= (self.__wnd_mini) << 1
        flags |= (self.__hscroll_visible) << 3
        flags |= (self.__vscroll_visible) << 4
        flags |= (self.__tabs_visible) << 5
        
        return BIFFRecords.Window1Record(self.__hpos_twips, self.__vpos_twips, 
                                self.__width_twips, self.__height_twips, 
                                flags,
                                self.__active_sheet, self.__first_tab_index, 
                                self.__selected_tabs, self.__tab_width_twips).get()
        
    def __codepage_rec(self):
        return BIFFRecords.CodepageBiff8Record().get()
        
    def __country_rec(self):
        return BIFFRecords.CountryRecord(self.__country_code, self.__country_code).get()
        
    def __dsf_rec(self):
        return BIFFRecords.DSFRecord().get()
        
    def __tabid_rec(self):
        return BIFFRecords.TabIDRecord(len(self.__worksheets)).get()
        
    def __fngroupcount_rec(self):
        return BIFFRecords.FnGroupCountRecord().get()
        
    def __datemode_rec(self):
        return BIFFRecords.DateModeRecord(self.__dates_1904).get()        

    def __precision_rec(self):
        return BIFFRecords.PrecisionRecord(self.__use_cell_values).get()         

    def __refresh_all_rec(self):
        return BIFFRecords.RefreshAllRecord().get()        

    def __bookbool_rec(self):
        return BIFFRecords.BookBoolRecord().get()         

    def __all_fonts_num_formats_xf_styles_rec(self):
        return self.__styles.get_biff_data()

    def __palette_rec(self):
        result = ''
        return result
        
    def __useselfs_rec(self):
        return BIFFRecords.UseSelfsRecord().get()
        
    def __boundsheets_rec(self, data_len_before, data_len_after, sheet_biff_lens):
        #  .................................  
        # BOUNDSEHEET0
        # BOUNDSEHEET1
        # BOUNDSEHEET2
        # ..................................
        # WORKSHEET0
        # WORKSHEET1
        # WORKSHEET2
        boundsheets_len = 0
        for sheet in self.__worksheets:
            boundsheets_len += len(BIFFRecords.BoundSheetRecord(0x00L, sheet.hidden, sheet.name).get())
        
        start = data_len_before + boundsheets_len + data_len_after
        
        result = ''
        for sheet_biff_len,  sheet in zip(sheet_biff_lens, self.__worksheets):
            result += BIFFRecords.BoundSheetRecord(start, sheet.hidden, sheet.name).get()
            start += sheet_biff_len            
        return result

    def __all_links_rec(self):
        result = ''
        return result
        
    def __sst_rec(self):
        return self.__sst.get_biff_record()
        
    def __ext_sst_rec(self, abs_stream_pos):
        return ''
        #return BIFFRecords.ExtSSTRecord(abs_stream_pos, self.sst_record.str_placement,
        #self.sst_record.portions_len).get()

    def get_biff_data(self):
        before = ''
        before += self.__bof_rec()
        before += self.__intf_hdr_rec()
        before += self.__intf_mms_rec()
        before += self.__intf_end_rec()
        before += self.__write_access_rec()
        before += self.__codepage_rec()
        before += self.__dsf_rec() 
        before += self.__tabid_rec() 
        before += self.__fngroupcount_rec()
        before += self.__wnd_protect_rec()
        before += self.__protect_rec()
        before += self.__obj_protect_rec()
        before += self.__password_rec()
        before += self.__prot4rev_rec()
        before += self.__prot4rev_pass_rec()
        before += self.__backup_rec()        
        before += self.__hide_obj_rec()        
        before += self.__window1_rec()
        before += self.__datemode_rec()
        before += self.__precision_rec()
        before += self.__refresh_all_rec()
        before += self.__bookbool_rec()
        before += self.__all_fonts_num_formats_xf_styles_rec()
        before += self.__palette_rec()
        before += self.__useselfs_rec()
        
        country            = self.__country_rec()
        all_links          = self.__all_links_rec()
        
        shared_str_table   = self.__sst_rec()        
        after = country + all_links + shared_str_table
        
        ext_sst = self.__ext_sst_rec(0) # need fake cause we need calc stream pos
        eof = self.__eof_rec()

        self.__worksheets[self.__active_sheet].selected = True
        sheets = ''
        sheet_biff_lens = []
        for sheet in self.__worksheets:
            data = sheet.get_biff_data()
            sheets += data
            sheet_biff_lens.append(len(data))
            
        bundlesheets = self.__boundsheets_rec(len(before), len(after)+len(ext_sst)+len(eof), sheet_biff_lens)       
       
        sst_stream_pos = len(before) + len(bundlesheets) + len(country)  + len(all_links)
        ext_sst = self.__ext_sst_rec(sst_stream_pos)           
        
        return before + bundlesheets + after + ext_sst + eof + sheets

    def save(self, filename):
        import CompoundDoc

        doc = CompoundDoc.XlsDoc()
        doc.save(filename, self.get_biff_data())

if __name__ == '__main__':
    wb = Workbook()
    f = file('workbook.bin', 'wb')
    f.write(wb.get_biff_data())
    f.close()
    
########NEW FILE########
__FILENAME__ = Worksheet
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
#
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
#
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
#
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
#
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
#
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


'''
            BOF
            UNCALCED
            INDEX
            Calculation Settings Block
            PRINTHEADERS
            PRINTGRIDLINES
            GRIDSET
            GUTS
            DEFAULTROWHEIGHT
            WSBOOL
            Page Settings Block
            Worksheet Protection Block
            DEFCOLWIDTH
            COLINFO
            SORT
            DIMENSIONS
            Row Blocks
            WINDOW2
            SCL
            PANE
            SELECTION
            STANDARDWIDTH
            MERGEDCELLS
            LABELRANGES
            PHONETIC
            Conditional Formatting Table
            Hyperlink Table
            Data Validity Table
            SHEETLAYOUT (BIFF8X only)
            SHEETPROTECTION (BIFF8X only)
            RANGEPROTECTION (BIFF8X only)
            EOF
'''

__rev_id__ = """$Id: Worksheet.py,v 1.7 2005/08/11 08:53:48 rvk Exp $"""


import BIFFRecords
import Bitmap
import Formatting
import Style
from Deco import *


class Worksheet(object):
    from Workbook import Workbook

    #################################################################
    ## Constructor
    #################################################################
    @accepts(object, (str, unicode), Workbook)
    def __init__(self, sheetname, parent_book):
        import Row
        self.Row = Row.Row

        import Column
        self.Column = Column.Column

        self.__name = sheetname
        self.__parent = parent_book

        self.__rows = {}
        self.__cols = {}
        self.__merged_ranges = []
        self.__bmp_rec = ''

        self.__show_formulas = 0
        self.__show_grid = 1
        self.__show_headers = 1
        self.__panes_frozen = 0
        self.__show_empty_as_zero = 1
        self.__auto_colour_grid = 1
        self.__cols_right_to_left = 0
        self.__show_outline = 1
        self.__remove_splits = 0
        self.__selected = 0
        self.__hidden = 0
        self.__page_preview = 0

        self.__first_visible_row = 0
        self.__first_visible_col = 0
        self.__grid_colour = 0x40
        self.__preview_magn = 0
        self.__normal_magn = 0

        self.__vert_split_pos = None
        self.__horz_split_pos = None
        self.__vert_split_first_visible = None
        self.__horz_split_first_visible = None
        self.__split_active_pane = None

        self.__row_gut_width = 0
        self.__col_gut_height = 0

        self.__show_auto_page_breaks = 1
        self.__dialogue_sheet = 0
        self.__auto_style_outline = 0
        self.__outline_below = 0
        self.__outline_right = 0
        self.__fit_num_pages = 0
        self.__show_row_outline = 1
        self.__show_col_outline = 1
        self.__alt_expr_eval = 0
        self.__alt_formula_entries = 0

        self.__row_default_height = 0x00FF
        self.__col_default_width = 0x0008

        self.__calc_mode = 1
        self.__calc_count = 0x0064
        self.__RC_ref_mode = 1
        self.__iterations_on = 0
        self.__delta = 0.001
        self.__save_recalc = 0

        self.__print_headers = 0
        self.__print_grid = 0
        self.__grid_set = 1
        self.__vert_page_breaks = []
        self.__horz_page_breaks = []
        self.__header_str = '&P'
        self.__footer_str = '&F'
        self.__print_centered_vert = 0
        self.__print_centered_horz = 1
        self.__left_margin = 0.3 #0.5
        self.__right_margin = 0.3 #0.5
        self.__top_margin = 0.61 #1.0
        self.__bottom_margin = 0.37 #1.0
        self.__paper_size_code = 9 # A4
        self.__print_scaling = 100
        self.__start_page_number = 1
        self.__fit_width_to_pages = 1
        self.__fit_height_to_pages = 1
        self.__print_in_rows = 1
        self.__portrait = 1
        self.__print_not_colour = 0
        self.__print_draft = 0
        self.__print_notes = 0
        self.__print_notes_at_end = 0
        self.__print_omit_errors = 0
        self.__print_hres = 0x012C # 300 dpi
        self.__print_vres = 0x012C # 300 dpi
        self.__header_margin = 0.1
        self.__footer_margin = 0.1
        self.__copies_num = 1

        self.__wnd_protect = 0
        self.__obj_protect = 0
        self.__protect = 0
        self.__scen_protect = 0
        self.__password = ''

    #################################################################
    ## Properties, "getters", "setters"
    #################################################################

    @accepts(object, (str, unicode))
    def set_name(self, value):
        self.__name = value

    def get_name(self):
        return self.__name

    name = property(get_name, set_name)

    #################################################################

    def get_parent(self):
        return self.__parent

    parent = property(get_parent)

    #################################################################

    def get_rows(self):
        return self.__rows

    rows = property(get_rows)

    #################################################################

    def get_cols(self):
        return self.__cols

    cols = property(get_cols)

    #################################################################

    def get_merged_ranges(self):
        return self.__merged_ranges

    merged_ranges = property(get_merged_ranges)

    #################################################################

    def get_bmp_rec(self):
        return self.__bmp_rec

    bmp_rec = property(get_bmp_rec)

    #################################################################

    @accepts(object, bool)
    def set_show_formulas(self, value):
        self.__show_formulas = int(value)

    def get_show_formulas(self):
        return bool(self.__show_formulas)

    show_formulas = property(get_show_formulas, set_show_formulas)

    #################################################################

    @accepts(object, bool)
    def set_show_grid(self, value):
        self.__show_grid = int(value)

    def get_show_grid(self):
        return bool(self.__show_grid)

    show_grid = property(get_show_grid, set_show_grid)

    #################################################################

    @accepts(object, bool)
    def set_show_headers(self, value):
        self.__show_headers = int(value)

    def get_show_headers(self):
        return bool(self.__show_headers)

    show_headers = property(get_show_headers, set_show_headers)

    #################################################################

    @accepts(object, bool)
    def set_panes_frozen(self, value):
        self.__panes_frozen = int(value)

    def get_panes_frozen(self):
        return bool(self.__panes_frozen)

    panes_frozen = property(get_panes_frozen, set_panes_frozen)

    #################################################################

    @accepts(object, bool)
    def set_show_empty_as_zero(self, value):
        self.__show_empty_as_zero = int(value)

    def get_show_empty_as_zero(self):
        return bool(self.__show_empty_as_zero)

    show_empty_as_zero = property(get_show_empty_as_zero, set_show_empty_as_zero)

    #################################################################

    @accepts(object, bool)
    def set_auto_colour_grid(self, value):
        self.__auto_colour_grid = int(value)

    def get_auto_colour_grid(self):
        return bool(self.__auto_colour_grid)

    auto_colour_grid = property(get_auto_colour_grid, set_auto_colour_grid)

    #################################################################

    @accepts(object, bool)
    def set_cols_right_to_left(self, value):
        self.__cols_right_to_left = int(value)

    def get_cols_right_to_left(self):
        return bool(self.__cols_right_to_left)

    cols_right_to_left = property(get_cols_right_to_left, set_cols_right_to_left)

    #################################################################

    @accepts(object, bool)
    def set_show_outline(self, value):
        self.__show_outline = int(value)

    def get_show_outline(self):
        return bool(self.__show_outline)

    show_outline = property(get_show_outline, set_show_outline)

    #################################################################

    @accepts(object, bool)
    def set_remove_splits(self, value):
        self.__remove_splits = int(value)

    def get_remove_splits(self):
        return bool(self.__remove_splits)

    remove_splits = property(get_remove_splits, set_remove_splits)

    #################################################################

    @accepts(object, bool)
    def set_selected(self, value):
        self.__selected = int(value)

    def get_selected(self):
        return bool(self.__selected)

    selected = property(get_selected, set_selected)

    #################################################################

    @accepts(object, bool)
    def set_hidden(self, value):
        self.__hidden = int(value)

    def get_hidden(self):
        return bool(self.__hidden)

    hidden = property(get_hidden, set_hidden)

    #################################################################

    @accepts(object, bool)
    def set_page_preview(self, value):
        self.__page_preview = int(value)

    def get_page_preview(self):
        return bool(self.__page_preview)

    page_preview = property(get_page_preview, set_page_preview)

    #################################################################

    @accepts(object, int)
    def set_first_visible_row(self, value):
        self.__first_visible_row = value

    def get_first_visible_row(self):
        return self.__first_visible_row

    first_visible_row = property(get_first_visible_row, set_first_visible_row)

    #################################################################

    @accepts(object, int)
    def set_first_visible_col(self, value):
        self.__first_visible_col = value

    def get_first_visible_col(self):
        return self.__first_visible_col

    first_visible_col = property(get_first_visible_col, set_first_visible_col)

    #################################################################

    @accepts(object, int)
    def set_grid_colour(self, value):
        self.__grid_colour = value

    def get_grid_colour(self):
        return self.__grid_colour

    grid_colour = property(get_grid_colour, set_grid_colour)

    #################################################################

    @accepts(object, int)
    def set_preview_magn(self, value):
        self.__preview_magn = value

    def get_preview_magn(self):
        return self.__preview_magn

    preview_magn = property(get_preview_magn, set_preview_magn)

    #################################################################

    @accepts(object, int)
    def set_normal_magn(self, value):
        self.__normal_magn = value

    def get_normal_magn(self):
        return self.__normal_magn

    normal_magn = property(get_normal_magn, set_normal_magn)

    #################################################################

    @accepts(object, int)
    def set_vert_split_pos(self, value):
        self.__vert_split_pos = abs(value)

    def get_vert_split_pos(self):
        return self.__vert_split_pos

    vert_split_pos = property(get_vert_split_pos, set_vert_split_pos)

    #################################################################

    @accepts(object, int)
    def set_horz_split_pos(self, value):
        self.__horz_split_pos = abs(value)

    def get_horz_split_pos(self):
        return self.__horz_split_pos

    horz_split_pos = property(get_horz_split_pos, set_horz_split_pos)

    #################################################################

    @accepts(object, int)
    def set_vert_split_first_visible(self, value):
        self.__vert_split_first_visible = abs(value)

    def get_vert_split_first_visible(self):
        return self.__vert_split_first_visible

    vert_split_first_visible = property(get_vert_split_first_visible, set_vert_split_first_visible)

    #################################################################

    @accepts(object, int)
    def set_horz_split_first_visible(self, value):
        self.__horz_split_first_visible = abs(value)

    def get_horz_split_first_visible(self):
        return self.__horz_split_first_visible

    horz_split_first_visible = property(get_horz_split_first_visible, set_horz_split_first_visible)

    #################################################################

    #@accepts(object, int)
    #def set_split_active_pane(self, value):
    #    self.__split_active_pane = abs(value) & 0x03
    #
    #def get_split_active_pane(self):
    #    return self.__split_active_pane
    #
    #split_active_pane = property(get_split_active_pane, set_split_active_pane)

    #################################################################

    #@accepts(object, int)
    #def set_row_gut_width(self, value):
    #    self.__row_gut_width = value
    #
    #def get_row_gut_width(self):
    #    return self.__row_gut_width
    #
    #row_gut_width = property(get_row_gut_width, set_row_gut_width)
    #
    #################################################################
    #
    #@accepts(object, int)
    #def set_col_gut_height(self, value):
    #    self.__col_gut_height = value
    #
    #def get_col_gut_height(self):
    #    return self.__col_gut_height
    #
    #col_gut_height = property(get_col_gut_height, set_col_gut_height)
    #
    #################################################################

    @accepts(object, bool)
    def set_show_auto_page_breaks(self, value):
        self.__show_auto_page_breaks = int(value)

    def get_show_auto_page_breaks(self):
        return bool(self.__show_auto_page_breaks)

    show_auto_page_breaks = property(get_show_auto_page_breaks, set_show_auto_page_breaks)

    #################################################################

    @accepts(object, bool)
    def set_dialogue_sheet(self, value):
        self.__dialogue_sheet = int(value)

    def get_dialogue_sheet(self):
        return bool(self.__dialogue_sheet)

    dialogue_sheet = property(get_dialogue_sheet, set_dialogue_sheet)

    #################################################################

    @accepts(object, bool)
    def set_auto_style_outline(self, value):
        self.__auto_style_outline = int(value)

    def get_auto_style_outline(self):
        return bool(self.__auto_style_outline)

    auto_style_outline = property(get_auto_style_outline, set_auto_style_outline)

    #################################################################

    @accepts(object, bool)
    def set_outline_below(self, value):
        self.__outline_below = int(value)

    def get_outline_below(self):
        return bool(self.__outline_below)

    outline_below = property(get_outline_below, set_outline_below)

    #################################################################

    @accepts(object, bool)
    def set_outline_right(self, value):
        self.__outline_right = int(value)

    def get_outline_right(self):
        return bool(self.__outline_right)

    outline_right = property(get_outline_right, set_outline_right)

    #################################################################

    @accepts(object, int)
    def set_fit_num_pages(self, value):
        self.__fit_num_pages = value

    def get_fit_num_pages(self):
        return self.__fit_num_pages

    fit_num_pages = property(get_fit_num_pages, set_fit_num_pages)

    #################################################################

    @accepts(object, bool)
    def set_show_row_outline(self, value):
        self.__show_row_outline = int(value)

    def get_show_row_outline(self):
        return bool(self.__show_row_outline)

    show_row_outline = property(get_show_row_outline, set_show_row_outline)

    #################################################################

    @accepts(object, bool)
    def set_show_col_outline(self, value):
        self.__show_col_outline = int(value)

    def get_show_col_outline(self):
        return bool(self.__show_col_outline)

    show_col_outline = property(get_show_col_outline, set_show_col_outline)

    #################################################################

    @accepts(object, bool)
    def set_alt_expr_eval(self, value):
        self.__alt_expr_eval = int(value)

    def get_alt_expr_eval(self):
        return bool(self.__alt_expr_eval)

    alt_expr_eval = property(get_alt_expr_eval, set_alt_expr_eval)

    #################################################################

    @accepts(object, bool)
    def set_alt_formula_entries(self, value):
        self.__alt_formula_entries = int(value)

    def get_alt_formula_entries(self):
        return bool(self.__alt_formula_entries)

    alt_formula_entries = property(get_alt_formula_entries, set_alt_formula_entries)

    #################################################################

    @accepts(object, int)
    def set_row_default_height(self, value):
        self.__row_default_height = value

    def get_row_default_height(self):
        return self.__row_default_height

    row_default_height = property(get_row_default_height, set_row_default_height)

    #################################################################

    @accepts(object, int)
    def set_col_default_width(self, value):
        self.__col_default_width = value

    def get_col_default_width(self):
        return self.__col_default_width

    col_default_width = property(get_col_default_width, set_col_default_width)

    #################################################################

    @accepts(object, int)
    def set_calc_mode(self, value):
        self.__calc_mode = value & 0x03

    def get_calc_mode(self):
        return self.__calc_mode

    calc_mode = property(get_calc_mode, set_calc_mode)

    #################################################################

    @accepts(object, int)
    def set_calc_count(self, value):
        self.__calc_count = value

    def get_calc_count(self):
        return self.__calc_count

    calc_count = property(get_calc_count, set_calc_count)

    #################################################################

    @accepts(object, bool)
    def set_RC_ref_mode(self, value):
        self.__RC_ref_mode = int(value)

    def get_RC_ref_mode(self):
        return bool(self.__RC_ref_mode)

    RC_ref_mode = property(get_RC_ref_mode, set_RC_ref_mode)

    #################################################################

    @accepts(object, bool)
    def set_iterations_on(self, value):
        self.__iterations_on = int(value)

    def get_iterations_on(self):
        return bool(self.__iterations_on)

    iterations_on = property(get_iterations_on, set_iterations_on)

    #################################################################

    @accepts(object, float)
    def set_delta(self, value):
        self.__delta = value

    def get_delta(self):
        return self.__delta

    delta = property(get_delta, set_delta)

    #################################################################

    @accepts(object, bool)
    def set_save_recalc(self, value):
        self.__save_recalc = int(value)

    def get_save_recalc(self):
        return bool(self.__save_recalc)

    save_recalc = property(get_save_recalc, set_save_recalc)

    #################################################################

    @accepts(object, bool)
    def set_print_headers(self, value):
        self.__print_headers = int(value)

    def get_print_headers(self):
        return bool(self.__print_headers)

    print_headers = property(get_print_headers, set_print_headers)

    #################################################################

    @accepts(object, bool)
    def set_print_grid(self, value):
        self.__print_grid = int(value)

    def get_print_grid(self):
        return bool(self.__print_grid)

    print_grid = property(get_print_grid, set_print_grid)

    #################################################################
    #
    #@accepts(object, bool)
    #def set_grid_set(self, value):
    #    self.__grid_set = int(value)
    #
    #def get_grid_set(self):
    #    return bool(self.__grid_set)
    #
    #grid_set = property(get_grid_set, set_grid_set)
    #
    #################################################################

    @accepts(object, list)
    def set_vert_page_breaks(self, value):
        self.__vert_page_breaks = value

    def get_vert_page_breaks(self):
        return self.__vert_page_breaks

    vert_page_breaks = property(get_vert_page_breaks, set_vert_page_breaks)

    #################################################################

    @accepts(object, list)
    def set_horz_page_breaks(self, value):
        self.__horz_page_breaks = value

    def get_horz_page_breaks(self):
        return self.__horz_page_breaks

    horz_page_breaks = property(get_horz_page_breaks, set_horz_page_breaks)

    #################################################################

    @accepts(object, (str, unicode))
    def set_header_str(self, value):
        self.__header_str = value

    def get_header_str(self):
        return self.__header_str

    header_str = property(get_header_str, set_header_str)

    #################################################################

    @accepts(object, (str, unicode))
    def set_footer_str(self, value):
        self.__footer_str = value

    def get_footer_str(self):
        return self.__footer_str

    footer_str = property(get_footer_str, set_footer_str)

    #################################################################

    @accepts(object, bool)
    def set_print_centered_vert(self, value):
        self.__print_centered_vert = int(value)

    def get_print_centered_vert(self):
        return bool(self.__print_centered_vert)

    print_centered_vert = property(get_print_centered_vert, set_print_centered_vert)

    #################################################################

    @accepts(object, bool)
    def set_print_centered_horz(self, value):
        self.__print_centered_horz = int(value)

    def get_print_centered_horz(self):
        return bool(self.__print_centered_horz)

    print_centered_horz = property(get_print_centered_horz, set_print_centered_horz)

    #################################################################

    @accepts(object, float)
    def set_left_margin(self, value):
        self.__left_margin = value

    def get_left_margin(self):
        return self.__left_margin

    left_margin = property(get_left_margin, set_left_margin)

    #################################################################

    @accepts(object, float)
    def set_right_margin(self, value):
        self.__right_margin = value

    def get_right_margin(self):
        return self.__right_margin

    right_margin = property(get_right_margin, set_right_margin)

    #################################################################

    @accepts(object, float)
    def set_top_margin(self, value):
        self.__top_margin = value

    def get_top_margin(self):
        return self.__top_margin

    top_margin = property(get_top_margin, set_top_margin)

    #################################################################

    @accepts(object, float)
    def set_bottom_margin(self, value):
        self.__bottom_margin = value

    def get_bottom_margin(self):
        return self.__bottom_margin

    bottom_margin = property(get_bottom_margin, set_bottom_margin)

    #################################################################

    @accepts(object, int)
    def set_paper_size_code(self, value):
        self.__paper_size_code = value

    def get_paper_size_code(self):
        return self.__paper_size_code

    paper_size_code = property(get_paper_size_code, set_paper_size_code)

    #################################################################

    @accepts(object, int)
    def set_print_scaling(self, value):
        self.__print_scaling = value

    def get_print_scaling(self):
        return self.__print_scaling

    print_scaling = property(get_print_scaling, set_print_scaling)

    #################################################################

    @accepts(object, int)
    def set_start_page_number(self, value):
        self.__start_page_number = value

    def get_start_page_number(self):
        return self.__start_page_number

    start_page_number = property(get_start_page_number, set_start_page_number)

    #################################################################

    @accepts(object, int)
    def set_fit_width_to_pages(self, value):
        self.__fit_width_to_pages = value

    def get_fit_width_to_pages(self):
        return self.__fit_width_to_pages

    fit_width_to_pages = property(get_fit_width_to_pages, set_fit_width_to_pages)

    #################################################################

    @accepts(object, int)
    def set_fit_height_to_pages(self, value):
        self.__fit_height_to_pages = value

    def get_fit_height_to_pages(self):
        return self.__fit_height_to_pages

    fit_height_to_pages = property(get_fit_height_to_pages, set_fit_height_to_pages)

    #################################################################

    @accepts(object, bool)
    def set_print_in_rows(self, value):
        self.__print_in_rows = int(value)

    def get_print_in_rows(self):
        return bool(self.__print_in_rows)

    print_in_rows = property(get_print_in_rows, set_print_in_rows)

    #################################################################

    @accepts(object, bool)
    def set_portrait(self, value):
        self.__portrait = int(value)

    def get_portrait(self):
        return bool(self.__portrait)

    portrait = property(get_portrait, set_portrait)

    #################################################################

    @accepts(object, bool)
    def set_print_colour(self, value):
        self.__print_not_colour = int(not value)

    def get_print_colour(self):
        return not bool(self.__print_not_colour)

    print_colour = property(get_print_colour, set_print_colour)

    #################################################################

    @accepts(object, bool)
    def set_print_draft(self, value):
        self.__print_draft = int(value)

    def get_print_draft(self):
        return bool(self.__print_draft)

    print_draft = property(get_print_draft, set_print_draft)

    #################################################################

    @accepts(object, bool)
    def set_print_notes(self, value):
        self.__print_notes = int(value)

    def get_print_notes(self):
        return bool(self.__print_notes)

    print_notes = property(get_print_notes, set_print_notes)

    #################################################################

    @accepts(object, bool)
    def set_print_notes_at_end(self, value):
        self.__print_notes_at_end = int(value)

    def get_print_notes_at_end(self):
        return bool(self.__print_notes_at_end)

    print_notes_at_end = property(get_print_notes_at_end, set_print_notes_at_end)

    #################################################################

    @accepts(object, bool)
    def set_print_omit_errors(self, value):
        self.__print_omit_errors = int(value)

    def get_print_omit_errors(self):
        return bool(self.__print_omit_errors)

    print_omit_errors = property(get_print_omit_errors, set_print_omit_errors)

    #################################################################

    @accepts(object, int)
    def set_print_hres(self, value):
        self.__print_hres = value

    def get_print_hres(self):
        return self.__print_hres

    print_hres = property(get_print_hres, set_print_hres)

    #################################################################

    @accepts(object, int)
    def set_print_vres(self, value):
        self.__print_vres = value

    def get_print_vres(self):
        return self.__print_vres

    print_vres = property(get_print_vres, set_print_vres)

    #################################################################

    @accepts(object, float)
    def set_header_margin(self, value):
        self.__header_margin = value

    def get_header_margin(self):
        return self.__header_margin

    header_margin = property(get_header_margin, set_header_margin)

    #################################################################

    @accepts(object, float)
    def set_footer_margin(self, value):
        self.__footer_margin = value

    def get_footer_margin(self):
        return self.__footer_margin

    footer_margin = property(get_footer_margin, set_footer_margin)

    #################################################################

    @accepts(object, int)
    def set_copies_num(self, value):
        self.__copies_num = value

    def get_copies_num(self):
        return self.__copies_num

    copies_num = property(get_copies_num, set_copies_num)

    ##################################################################

    @accepts(object, bool)
    def set_wnd_protect(self, value):
        self.__wnd_protect = int(value)

    def get_wnd_protect(self):
        return bool(self.__wnd_protect)

    wnd_protect = property(get_wnd_protect, set_wnd_protect)

    #################################################################

    @accepts(object, bool)
    def set_obj_protect(self, value):
        self.__obj_protect = int(value)

    def get_obj_protect(self):
        return bool(self.__obj_protect)

    obj_protect = property(get_obj_protect, set_obj_protect)

    #################################################################

    @accepts(object, bool)
    def set_protect(self, value):
        self.__protect = int(value)

    def get_protect(self):
        return bool(self.__protect)

    protect = property(get_protect, set_protect)

    #################################################################

    @accepts(object, bool)
    def set_scen_protect(self, value):
        self.__scen_protect = int(value)

    def get_scen_protect(self):
        return bool(self.__scen_protect)

    scen_protect = property(get_scen_protect, set_scen_protect)

    #################################################################

    @accepts(object, str)
    def set_password(self, value):
        self.__password = value

    def get_password(self):
        return self.__password

    password = property(get_password, set_password)

    ##################################################################
    ## Methods
    ##################################################################

    def get_parent(self):
        return self.__parent

    def write(self, r, c, label="", style=Style.XFStyle()):
        self.row(r).write(c, label, style)

    def merge(self, r1, r2, c1, c2, style=Style.XFStyle()):
        self.row(r1).write_blanks(c1, c2,  style)
        for r in range(r1+1, r2+1):
            self.row(r).write_blanks(c1, c2,  style)
        self.__merged_ranges.append((r1, r2, c1, c2))

    def write_merge(self, r1, r2, c1, c2, label="", style=Style.XFStyle()):
        self.merge(r1, r2, c1, c2, style)
        self.write(r1, c1,  label, style)

    def insert_bitmap(self, filename, row, col, x = 0, y = 0, scale_x = 1, scale_y = 1):
        bmp = Bitmap.ImDataBmpRecord(filename)
        obj = Bitmap.ObjBmpRecord(row, col, self, bmp, x, y, scale_x, scale_y)

        self.__bmp_rec += obj.get() + bmp.get()

    def col(self, indx):
        if indx not in self.__cols:
            self.__cols[indx] = self.Column(indx, self)
        return self.__cols[indx]

    def row(self, indx):
        if indx not in self.__rows:
            self.__rows[indx] = self.Row(indx, self)
        return self.__rows[indx]

    def row_height(self, row): # in pixels
        if row in self.__rows:
            return self.__rows[row].get_height_in_pixels()
        else:
            return 17

    def col_width(self, col): # in pixels
        #if col in self.__cols:
        #    return self.__cols[col].width_in_pixels()
        #else:
            return 64

    def get_labels_count(self):
        result = 0
        for r in self.__rows:
            result += self.__rows[r].get_str_count()
        return result

    ##################################################################
    ## BIFF records generation
    ##################################################################

    def __bof_rec(self):
        return BIFFRecords.Biff8BOFRecord(BIFFRecords.Biff8BOFRecord.WORKSHEET).get()

    def __guts_rec(self):
        row_visible_levels = 0
        if len(self.__rows) != 0:
            row_visible_levels = max([self.__rows[r].level for r in self.__rows]) + 1

        col_visible_levels = 0
        if len(self.__cols) != 0:
            col_visible_levels = max([self.__cols[c].level for c in self.__cols]) + 1

        return BIFFRecords.GutsRecord(self.__row_gut_width, self.__col_gut_height, row_visible_levels, col_visible_levels).get()

    def __wsbool_rec(self):
        options = 0x00
        options |= (self.__show_auto_page_breaks & 0x01) << 0
        options |= (self.__dialogue_sheet & 0x01) << 4
        options |= (self.__auto_style_outline & 0x01) << 5
        options |= (self.__outline_below & 0x01) << 6
        options |= (self.__outline_right & 0x01) << 7
        options |= (self.__fit_num_pages & 0x01) << 8
        options |= (self.__show_row_outline & 0x01) << 10
        options |= (self.__show_col_outline & 0x01) << 11
        options |= (self.__alt_expr_eval & 0x01) << 14
        options |= (self.__alt_formula_entries & 0x01) << 15

        return BIFFRecords.WSBoolRecord(options).get()

    def __eof_rec(self):
        return BIFFRecords.EOFRecord().get()

    def __colinfo_rec(self):
        result = ''
        for col in self.__cols:
            result += self.__cols[col].get_biff_record()
        return result

    def __dimensions_rec(self):
        first_used_row = 0
        last_used_row = 0
        first_used_col = 0
        last_used_col = 0
        if len(self.__rows) > 0:
            first_used_row = min(self.__rows)
            last_used_row = max(self.__rows)
            first_used_col = 0xFFFFFFFF
            last_used_col = 0
            for r in self.__rows:
                _min = self.__rows[r].get_min_col()
                _max = self.__rows[r].get_max_col()
                if _min < first_used_col:
                    first_used_col = _min
                if _max > last_used_col:
                    last_used_col = _max

        return BIFFRecords.DimensionsRecord(first_used_row, last_used_row, first_used_col, last_used_col).get()

    def __window2_rec(self):
        options = 0
        options |= (self.__show_formulas        & 0x01) << 0
        options |= (self.__show_grid            & 0x01) << 1
        options |= (self.__show_headers         & 0x01) << 2
        options |= (self.__panes_frozen         & 0x01) << 3
        options |= (self.__show_empty_as_zero   & 0x01) << 4
        options |= (self.__auto_colour_grid     & 0x01) << 5
        options |= (self.__cols_right_to_left   & 0x01) << 6
        options |= (self.__show_outline         & 0x01) << 7
        options |= (self.__remove_splits        & 0x01) << 8
        options |= (self.__selected             & 0x01) << 9
        options |= (self.__hidden               & 0x01) << 10
        options |= (self.__page_preview         & 0x01) << 11

        return BIFFRecords.Window2Record(options, self.__first_visible_row, self.__first_visible_col,
                                        self.__grid_colour,
                                        self.__preview_magn, self.__normal_magn).get()

    def __panes_rec(self):
        if self.__vert_split_pos is None and self.__horz_split_pos is None:
            return ""

        if self.__vert_split_pos is None:
            self.__vert_split_pos = 0
        if self.__horz_split_pos is None:
            self.__horz_split_pos = 0

        if self.__panes_frozen:
            if self.__vert_split_first_visible is None:
                self.__vert_split_first_visible = self.__vert_split_pos
            if self.__horz_split_first_visible is None:
                self.__horz_split_first_visible = self.__horz_split_pos
        else:
            if self.__vert_split_first_visible is None:
                self.__vert_split_first_visible = 0
            if self.__horz_split_first_visible is None:
                self.__horz_split_first_visible = 0
            # inspired by pyXLWriter
            self.__horz_split_pos = 20*self.__horz_split_pos + 255
            self.__vert_split_pos = 113.879*self.__vert_split_pos + 390

        if self.__vert_split_pos > 0 and self.__horz_split_pos > 0:
            self.__split_active_pane = 0
        elif self.__vert_split_pos > 0 and self.__horz_split_pos == 0:
            self.__split_active_pane = 1
        elif self.__vert_split_pos == 0 and self.__horz_split_pos > 0:
            self.__split_active_pane = 2
        else:
            self.__split_active_pane = 3

        result = BIFFRecords.PanesRecord(self.__vert_split_pos,
                                         self.__horz_split_pos,
                                         self.__horz_split_first_visible,
                                         self.__vert_split_first_visible,
                                         self.__split_active_pane).get()
        return result

    def __row_blocks_rec(self):
        # this function takes almost 99% of overall execution time 
        # when file is saved
        # return '' 
        result = []
        i = 0
        used_rows = self.__rows.keys()
        while i < len(used_rows):
            j = 0
            while i < len(used_rows) and (j < 32):
                result.append(self.__rows[used_rows[i]].get_row_biff_data())
                result.append(self.__rows[used_rows[i]].get_cells_biff_data())
                j += 1
                i += 1

        return ''.join(result)

    def __merged_rec(self):
        return BIFFRecords.MergedCellsRecord(self.__merged_ranges).get()

    def __bitmaps_rec(self):
        return self.__bmp_rec

    def __calc_settings_rec(self):
        result = ''
        result += BIFFRecords.CalcModeRecord(self.__calc_mode & 0x01).get()
        result += BIFFRecords.CalcCountRecord(self.__calc_count & 0xFFFF).get()
        result += BIFFRecords.RefModeRecord(self.__RC_ref_mode & 0x01).get()
        result += BIFFRecords.IterationRecord(self.__iterations_on & 0x01).get()
        result += BIFFRecords.DeltaRecord(self.__delta).get()
        result += BIFFRecords.SaveRecalcRecord(self.__save_recalc & 0x01).get()
        return result

    def __print_settings_rec(self):
        result = ''
        result += BIFFRecords.PrintHeadersRecord(self.__print_headers).get()
        result += BIFFRecords.PrintGridLinesRecord(self.__print_grid).get()
        result += BIFFRecords.GridSetRecord(self.__grid_set).get()
        result += BIFFRecords.HorizontalPageBreaksRecord(self.__horz_page_breaks).get()
        result += BIFFRecords.VerticalPageBreaksRecord(self.__vert_page_breaks).get()
        result += BIFFRecords.HeaderRecord(self.__header_str).get()
        result += BIFFRecords.FooterRecord(self.__footer_str).get()
        result += BIFFRecords.HCenterRecord(self.__print_centered_horz).get()
        result += BIFFRecords.VCenterRecord(self.__print_centered_vert).get()
        result += BIFFRecords.LeftMarginRecord(self.__left_margin).get()
        result += BIFFRecords.RightMarginRecord(self.__right_margin).get()
        result += BIFFRecords.TopMarginRecord(self.__top_margin).get()
        result += BIFFRecords.BottomMarginRecord(self.__bottom_margin).get()

        setup_page_options =  (self.__print_in_rows & 0x01) << 0
        setup_page_options |=  (self.__portrait & 0x01) << 1
        setup_page_options |=  (0x00 & 0x01) << 2
        setup_page_options |=  (self.__print_not_colour & 0x01) << 3
        setup_page_options |=  (self.__print_draft & 0x01) << 4
        setup_page_options |=  (self.__print_notes & 0x01) << 5
        setup_page_options |=  (0x00 & 0x01) << 6
        setup_page_options |=  (0x01 & 0x01) << 7
        setup_page_options |=  (self.__print_notes_at_end & 0x01) << 9
        setup_page_options |=  (self.__print_omit_errors & 0x03) << 10

        result += BIFFRecords.SetupPageRecord(self.__paper_size_code,
                                self.__print_scaling,
                                self.__start_page_number,
                                self.__fit_width_to_pages,
                                self.__fit_height_to_pages,
                                setup_page_options,
                                self.__print_hres,
                                self.__print_vres,
                                self.__header_margin,
                                self.__footer_margin,
                                self.__copies_num).get()
        return result

    def __protection_rec(self):
        result = ''
        result += BIFFRecords.ProtectRecord(self.__protect).get()
        result += BIFFRecords.ScenProtectRecord(self.__scen_protect).get()
        result += BIFFRecords.WindowProtectRecord(self.__wnd_protect).get()
        result += BIFFRecords.ObjectProtectRecord(self.__obj_protect).get()
        result += BIFFRecords.PasswordRecord(self.__password).get()
        return result

    def get_biff_data(self):
        result = ''
        result += self.__bof_rec()
        result += self.__calc_settings_rec()
        result += self.__guts_rec()
        result += self.__wsbool_rec()
        result += self.__colinfo_rec()
        result += self.__dimensions_rec()
        result += self.__print_settings_rec()
        result += self.__protection_rec()
        result += self.__row_blocks_rec()
        result += self.__merged_rec()
        result += self.__bitmaps_rec()
        result += self.__window2_rec()
        result += self.__panes_rec()
        result += self.__eof_rec()

        return result




########NEW FILE########
__FILENAME__ = big-16Mb
#!/usr/bin/env python
# tries stress SST, SAT and MSAT
__rev_id__ = """$Id: big-16Mb.py,v 1.3 2005/03/27 12:47:06 rvk Exp $"""


from time import *
from pyExcelerator.Workbook import *
from pyExcelerator.Style import *

style = XFStyle()

wb = Workbook()
ws0 = wb.add_sheet('0')

colcount = 200 + 1
rowcount = 6000 + 1

t0 = time()
print "\nstart: %s" % ctime(t0)

print "Filling..."
for col in xrange(colcount):
    print "[%d]" % col, 
    for row in xrange(rowcount):
        #ws0.write(row, col, "BIG(%d, %d)" % (row, col))
        ws0.write(row, col, "BIG")

t1 = time() - t0
print "\nsince starting elapsed %.2f s" % (t1)

print "Storing..."
wb.save('big-16Mb.xls')

t2 = time() - t0
print "since starting elapsed %.2f s" % (t2)



########NEW FILE########
__FILENAME__ = big-35Mb
#!/usr/bin/env python
# tries stress SST, SAT and MSAT
__rev_id__ = """$Id: big-35Mb.py,v 1.3 2005/03/27 12:47:06 rvk Exp $"""


from time import *
from pyExcelerator import *

style = XFStyle()

wb = Workbook()
ws0 = wb.add_sheet('0')

colcount = 200 + 1
rowcount = 6000 + 1

t0 = time()
print "\nstart: %s" % ctime(t0)

print "Filling..."
for col in xrange(colcount):
    print "[%d]" % col, 
    for row in xrange(rowcount):
        ws0.write(row, col, "BIG(%d, %d)" % (row, col))
        #ws0.write(row, col, "BIG")

t1 = time() - t0
print "\nsince starting elapsed %.2f s" % (t1)

print "Storing..."
wb.save('big-35Mb.xls')

t2 = time() - t0
print "since starting elapsed %.2f s" % (t2)



########NEW FILE########
__FILENAME__ = blanks
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman

__rev_id__ = """$Id: blanks.py,v 1.2 2005/07/22 08:22:26 rvk Exp $"""


from pyExcelerator import *

font0 = Font()
font0.name = 'Times New Roman'
font0.struck_out = True
font0.bold = True

style0 = XFStyle()
style0.font = font0


wb = Workbook()
ws0 = wb.add_sheet('0')

ws0.write(1, 1, 'Test', style0)

for i in range(0, 0x53):
    borders = Borders()
    borders.left = i
    borders.right = i
    borders.top = i
    borders.bottom = i

    style = XFStyle()
    style.borders = borders

    ws0.write(i, 2, '', style)
    ws0.write(i, 3, hex(i), style0)

ws0.write_merge(5, 8, 6, 10, "")

wb.save('blanks.xls')
########NEW FILE########
__FILENAME__ = col_width
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman
__rev_id__ = """$Id: col_width.py,v 1.3 2005/03/27 12:47:06 rvk Exp $"""


from pyExcelerator import *

w = Workbook()
ws = w.add_sheet('Hey, Dude')

for i in range(6, 80):
    fnt = Font()
    fnt.height = i*20
    style = XFStyle()
    style.font = fnt
    ws.write(1, i, 'Test')
    ws.col(i).width = 0x0d00 + i
w.save('col_width.xls')

########NEW FILE########
__FILENAME__ = dates
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman
__rev_id__ = """$Id: dates.py,v 1.1 2005/07/20 07:24:11 rvk Exp $"""


from pyExcelerator import *
from datetime import datetime

w = Workbook()
ws = w.add_sheet('Hey, Dude')

fmts = [
    'M/D/YY',
    'D-MMM-YY',
    'D-MMM',
    'MMM-YY',
    'h:mm AM/PM',
    'h:mm:ss AM/PM',
    'h:mm',
    'h:mm:ss',
    'M/D/YY h:mm',
    'mm:ss',
    '[h]:mm:ss',
    'mm:ss.0',
]

i = 0
for fmt in fmts:
    ws.write(i, 0, fmt)

    style = XFStyle()
    style.num_format_str = fmt

    ws.write(i, 4, datetime.now(), style)

    i += 1

w.save('dates.xls')

########NEW FILE########
__FILENAME__ = format
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman

__rev_id__ = """$Id: format.py,v 1.3 2005/03/27 12:47:06 rvk Exp $"""


from pyExcelerator import *

font0 = Font()
font0.name = 'Times New Roman'
font0.struck_out = True
font0.bold = True

style0 = XFStyle()
style0.font = font0


wb = Workbook()
ws0 = wb.add_sheet('0')

ws0.write(1, 1, 'Test', style0)

for i in range(0, 0x53):
    fnt = Font()
    fnt.name = 'Arial'
    fnt.colour_index = i
    fnt.outline = True

    borders = Borders()
    borders.left = i

    style = XFStyle()
    style.font = fnt
    style.borders = borders

    ws0.write(i, 2, 'colour', style)
    ws0.write(i, 3, hex(i), style0)


wb.save('format.xls')
########NEW FILE########
__FILENAME__ = formulas
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman
__rev_id__ = """$Id: formulas.py,v 1.4 2005/10/26 07:44:24 rvk Exp $"""


from pyExcelerator import *

w = Workbook()
ws = w.add_sheet('F')

ws.write(0, 0, Formula("-(1+1)"))
ws.write(1, 0, Formula("-(1+1)/(-2-2)"))
ws.write(2, 0, Formula("-(134.8780789+1)"))
ws.write(3, 0, Formula("-(134.8780789e-10+1)"))
ws.write(4, 0, Formula("-1/(1+1)+9344"))

ws.write(0, 1, Formula("-(1+1)"))
ws.write(1, 1, Formula("-(1+1)/(-2-2)"))
ws.write(2, 1, Formula("-(134.8780789+1)"))
ws.write(3, 1, Formula("-(134.8780789e-10+1)"))
ws.write(4, 1, Formula("-1/(1+1)+9344"))

ws.write(0, 2, Formula("A1*B1"))
ws.write(1, 2, Formula("A2*B2"))
ws.write(2, 2, Formula("A3*B3"))
ws.write(3, 2, Formula("A4*B4*sin(pi()/4)"))
ws.write(4, 2, Formula("A5%*B5*pi()/1000"))

##############
## NOTE: parameters are separated by semicolon!!!
##############


ws.write(5, 2, Formula("C1+C2+C3+C4+C5/(C1+C2+C3+C4/(C1+C2+C3+C4/(C1+C2+C3+C4)+C5)+C5)-20.3e-2"))
ws.write(5, 3, Formula("C1^2"))
ws.write(6, 2, Formula("SUM(C1;C2;;;;;C3;;;C4)"))
ws.write(6, 3, Formula("SUM($A$1:$C$5)"))

ws.write(7, 0, Formula('"lkjljllkllkl"'))
ws.write(7, 1, Formula('"yuyiyiyiyi"'))
ws.write(7, 2, Formula('A8 & B8 & A8'))
ws.write(8, 2, Formula('now()'))

ws.write(10, 2, Formula('TRUE'))
ws.write(11, 2, Formula('FALSE'))
ws.write(12, 3, Formula('IF(A1>A2;3;"hkjhjkhk")'))

w.save('formulas.xls')

########NEW FILE########
__FILENAME__ = formula_names
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman
__rev_id__ = """$Id: formula_names.py,v 1.1 2005/08/11 08:53:48 rvk Exp $"""


from pyExcelerator import *

w = Workbook()
ws = w.add_sheet('F')

i = 0
for n in sorted(ExcelMagic.std_func_by_name):
    ws.write(i, 0, n)
    ws.write(i, 3, Formula(n + "($A$1)"))
    i += 1

w.save('formula_names.xls')
########NEW FILE########
__FILENAME__ = hyperlinks
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman
__rev_id__ = """$Id: hyperlinks.py,v 1.1 2005/10/26 07:44:24 rvk Exp $"""


from pyExcelerator import *

f = Font()
f.height = 20*72
f.name = 'Verdana'
f.bold = True
f.underline = Font.UNDERLINE_DOUBLE
f.colour_index = 4

h_style = XFStyle()
h_style.font = f

w = Workbook()
ws = w.add_sheet('F')

##############
## NOTE: parameters are separated by semicolon!!!
##############

n = "HYPERLINK"
ws.write_merge(1, 1, 1, 10, Formula(n + '("http://www.irs.gov/pub/irs-pdf/f1000.pdf";"f1000.pdf")'), h_style)
ws.write_merge(2, 2, 2, 25, Formula(n + '("mailto:roman.kiseliov@gmail.com?subject=pyExcelerator-feedback&Body=Hello,%20Roman!";"pyExcelerator-feedback")'), h_style)

w.save("hyperlinks.xls")
########NEW FILE########
__FILENAME__ = image
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman

__rev_id__ = """$Id: image.py,v 1.3 2005/03/27 12:47:06 rvk Exp $"""


from pyExcelerator import *

w = Workbook()
ws = w.add_sheet('Image')
ws.insert_bitmap('python.bmp', 2, 2)
ws.insert_bitmap('python.bmp', 10, 2)

w.save('image.xls')

########NEW FILE########
__FILENAME__ = merged
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman

__rev_id__ = """$Id: merged.py,v 1.3 2005/03/27 12:47:06 rvk Exp $"""


from pyExcelerator import *

fnt = Font()
fnt.name = 'Arial'
fnt.colour_index = 4
fnt.bold = True

borders = Borders()
borders.left = 6
borders.right = 6
borders.top = 6
borders.bottom = 6

al = Alignment()
al.horz = Alignment.HORZ_CENTER
al.vert = Alignment.VERT_CENTER

style = XFStyle()
style.font = fnt
style.borders = borders
style.alignment = al


wb = Workbook()
ws0 = wb.add_sheet('sheet0')
ws1 = wb.add_sheet('sheet1')
ws2 = wb.add_sheet('sheet2')

for i in range(0, 0x200, 2):
    ws0.write_merge(i, i+1, 1, 5, 'test %d' % i, style)
    ws1.write_merge(i, i, 1, 7, 'test %d' % i, style)
    ws2.write_merge(i, i+1, 1, 7 + (i%10), 'test %d' % i, style)


wb.save('merged.xls')

########NEW FILE########
__FILENAME__ = merged0
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman
__rev_id__ = """$Id: merged0.py,v 1.3 2005/03/27 12:47:06 rvk Exp $"""


from pyExcelerator import *

wb = Workbook()
ws0 = wb.add_sheet('sheet0')


fnt = Font()
fnt.name = 'Arial'
fnt.colour_index = 4
fnt.bold = True

borders = Borders()
borders.left = 6
borders.right = 6
borders.top = 6
borders.bottom = 6

style = XFStyle()
style.font = fnt
style.borders = borders

ws0.write_merge(3, 3, 1, 5, 'test1', style)
ws0.write_merge(4, 10, 1, 5, 'test2', style)
ws0.col(1).width = 0x0d00

wb.save('merged0.xls')

########NEW FILE########
__FILENAME__ = merged1
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman
__rev_id__ = """$Id: merged1.py,v 1.1 2005/07/22 08:22:26 rvk Exp $"""


from pyExcelerator import *

wb = Workbook()
ws0 = wb.add_sheet('sheet0')

fnt1 = Font()
fnt1.name = 'Verdana'
fnt1.bold = True
fnt1.height = 18*0x14

pat1 = Pattern()
pat1.pattern = Pattern.SOLID_PATTERN
pat1.pattern_fore_colour = 0x16

brd1 = Borders()
brd1.left = 0x06
brd1.right = 0x06
brd1.top = 0x06
brd1.bottom = 0x06

fnt2 = Font()
fnt2.name = 'Verdana'
fnt2.bold = True
fnt2.height = 14*0x14

brd2 = Borders()
brd2.left = 0x01
brd2.right = 0x01
brd2.top = 0x01
brd2.bottom = 0x01

pat2 = Pattern()
pat2.pattern = Pattern.SOLID_PATTERN
pat2.pattern_fore_colour = 0x01F

fnt3 = Font()
fnt3.name = 'Verdana'
fnt3.bold = True
fnt3.italic = True
fnt3.height = 12*0x14

brd3 = Borders()
brd3.left = 0x07
brd3.right = 0x07
brd3.top = 0x07
brd3.bottom = 0x07

fnt4 = Font()

al1 = Alignment()
al1.horz = Alignment.HORZ_CENTER
al1.vert = Alignment.VERT_CENTER

al2 = Alignment()
al2.horz = Alignment.HORZ_RIGHT
al2.vert = Alignment.VERT_CENTER

al3 = Alignment()
al3.horz = Alignment.HORZ_LEFT
al3.vert = Alignment.VERT_CENTER

style1 = XFStyle()
style1.font = fnt1
style1.alignment = al1
style1.pattern = pat1
style1.borders = brd1

style2 = XFStyle()
style2.font = fnt2
style2.alignment = al1
style2.pattern = pat2
style2.borders = brd2

style3 = XFStyle()
style3.font = fnt3
style3.alignment = al1
style3.pattern = pat2
style3.borders = brd3

price_style = XFStyle()
price_style.font = fnt4
price_style.alignment = al2
price_style.borders = brd3
price_style.num_format_str = '_(#,##0.00_) "money"'

ware_style = XFStyle()
ware_style.font = fnt4
ware_style.alignment = al3
ware_style.borders = brd3


ws0.merge(3, 3, 1, 5, style1)
ws0.merge(4, 10, 1, 6, style2)
ws0.merge(14, 16, 1, 7, style3)
ws0.col(1).width = 0x0d00


wb.save('merged1.xls')

########NEW FILE########
__FILENAME__ = mini
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman
__rev_id__ = """$Id: mini.py,v 1.3 2005/03/27 12:47:06 rvk Exp $"""


from pyExcelerator import *

w = Workbook()
ws = w.add_sheet('Hey, Dude')
w.save('mini.xls')

########NEW FILE########
__FILENAME__ = numbers
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman
__rev_id__ = """$Id: numbers.py,v 1.1 2005/07/20 07:24:11 rvk Exp $"""


from pyExcelerator import *

w = Workbook()
ws = w.add_sheet('Hey, Dude')

ws.write(0, 0, 1)
ws.write(1, 0, 1.23)
ws.write(2, 0, 12345678)
ws.write(3, 0, 123456.78)

ws.write(0, 1, -1)
ws.write(1, 1, -1.23)
ws.write(2, 1, -12345678)
ws.write(3, 1, -123456.78)

ws.write(0, 2, -17867868678687.0)
ws.write(1, 2, -1.23e-5)
ws.write(2, 2, -12345678.90780980)
ws.write(3, 2, -123456.78)

w.save('numbers.xls')

########NEW FILE########
__FILENAME__ = num_formats
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman
__rev_id__ = """$Id: num_formats.py,v 1.1 2005/07/20 07:24:11 rvk Exp $"""


from pyExcelerator import *

w = Workbook()
ws = w.add_sheet('Hey, Dude')

fmts = [
    'general',
    '0',
    '0.00',
    '#,##0',
    '#,##0.00',
    '"$"#,##0_);("$"#,##',
    '"$"#,##0_);[Red]("$"#,##',
    '"$"#,##0.00_);("$"#,##',
    '"$"#,##0.00_);[Red]("$"#,##',
    '0%',
    '0.00%',
    '0.00E+00',
    '# ?/?',
    '# ??/??',
    'M/D/YY',
    'D-MMM-YY',
    'D-MMM',
    'MMM-YY',
    'h:mm AM/PM',
    'h:mm:ss AM/PM',
    'h:mm',
    'h:mm:ss',
    'M/D/YY h:mm',
    '_(#,##0_);(#,##0)',
    '_(#,##0_);[Red](#,##0)',
    '_(#,##0.00_);(#,##0.00)',
    '_(#,##0.00_);[Red](#,##0.00)',
    '_("$"* #,##0_);_("$"* (#,##0);_("$"* "-"_);_(@_)',
    '_(* #,##0_);_(* (#,##0);_(* "-"_);_(@_)',
    '_("$"* #,##0.00_);_("$"* (#,##0.00);_("$"* "-"??_);_(@_)',
    '_(* #,##0.00_);_(* (#,##0.00);_(* "-"??_);_(@_)',
    'mm:ss',
    '[h]:mm:ss',
    'mm:ss.0',
    '##0.0E+0',
    '@'   
]

i = 0
for fmt in fmts:
    ws.write(i, 0, fmt)

    style = XFStyle()
    style.num_format_str = fmt

    ws.write(i, 4, -1278.9078, style)

    i += 1

w.save('num_formats.xls')

########NEW FILE########
__FILENAME__ = outline
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman
__rev_id__ = """$Id: outline.py,v 1.3 2005/03/27 12:47:06 rvk Exp $"""


from pyExcelerator import *

fnt = Font()
fnt.name = 'Arial'
fnt.colour_index = 4
fnt.bold = True

borders = Borders()
borders.left = 6
borders.right = 6
borders.top = 6
borders.bottom = 6

style = XFStyle()
style.font = fnt
style.borders = borders

wb = Workbook()

ws0 = wb.add_sheet('Rows Outline')

ws0.write_merge(1, 1, 1, 5, 'test 1', style)
ws0.write_merge(2, 2, 1, 4, 'test 1', style)
ws0.write_merge(3, 3, 1, 3, 'test 2', style)
ws0.write_merge(4, 4, 1, 4, 'test 1', style)
ws0.write_merge(5, 5, 1, 4, 'test 3', style)
ws0.write_merge(6, 6, 1, 5, 'test 1', style)
ws0.write_merge(7, 7, 1, 5, 'test 4', style)
ws0.write_merge(8, 8, 1, 4, 'test 1', style)
ws0.write_merge(9, 9, 1, 3, 'test 5', style)

ws0.row(1).level = 1
ws0.row(2).level = 1
ws0.row(3).level = 2
ws0.row(4).level = 2
ws0.row(5).level = 2
ws0.row(6).level = 2
ws0.row(7).level = 2
ws0.row(8).level = 1
ws0.row(9).level = 1


ws1 = wb.add_sheet('Columns Outline')

ws1.write_merge(1, 1, 1, 5, 'test 1', style)
ws1.write_merge(2, 2, 1, 4, 'test 1', style)
ws1.write_merge(3, 3, 1, 3, 'test 2', style)
ws1.write_merge(4, 4, 1, 4, 'test 1', style)
ws1.write_merge(5, 5, 1, 4, 'test 3', style)
ws1.write_merge(6, 6, 1, 5, 'test 1', style)
ws1.write_merge(7, 7, 1, 5, 'test 4', style)
ws1.write_merge(8, 8, 1, 4, 'test 1', style)
ws1.write_merge(9, 9, 1, 3, 'test 5', style)

ws1.col(1).level = 1
ws1.col(2).level = 1
ws1.col(3).level = 2
ws1.col(4).level = 2
ws1.col(5).level = 2
ws1.col(6).level = 2
ws1.col(7).level = 2
ws1.col(8).level = 1
ws1.col(9).level = 1


ws2 = wb.add_sheet('Rows and Columns Outline')

ws2.write_merge(1, 1, 1, 5, 'test 1', style)
ws2.write_merge(2, 2, 1, 4, 'test 1', style)
ws2.write_merge(3, 3, 1, 3, 'test 2', style)
ws2.write_merge(4, 4, 1, 4, 'test 1', style)
ws2.write_merge(5, 5, 1, 4, 'test 3', style)
ws2.write_merge(6, 6, 1, 5, 'test 1', style)
ws2.write_merge(7, 7, 1, 5, 'test 4', style)
ws2.write_merge(8, 8, 1, 4, 'test 1', style)
ws2.write_merge(9, 9, 1, 3, 'test 5', style)

ws2.row(1).level = 1
ws2.row(2).level = 1
ws2.row(3).level = 2
ws2.row(4).level = 2
ws2.row(5).level = 2
ws2.row(6).level = 2
ws2.row(7).level = 2
ws2.row(8).level = 1
ws2.row(9).level = 1

ws2.write_merge(1, 1, 1, 5, 'test 1', style)
ws2.write_merge(2, 2, 1, 4, 'test 1', style)
ws2.write_merge(3, 3, 1, 3, 'test 2', style)
ws2.write_merge(4, 4, 1, 4, 'test 1', style)
ws2.write_merge(5, 5, 1, 4, 'test 3', style)
ws2.write_merge(6, 6, 1, 5, 'test 1', style)
ws2.write_merge(7, 7, 1, 5, 'test 4', style)
ws2.write_merge(8, 8, 1, 4, 'test 1', style)
ws2.write_merge(9, 9, 1, 3, 'test 5', style)

ws2.col(1).level = 1
ws2.col(2).level = 1
ws2.col(3).level = 2
ws2.col(4).level = 2
ws2.col(5).level = 2
ws2.col(6).level = 2
ws2.col(7).level = 2
ws2.col(8).level = 1
ws2.col(9).level = 1


wb.save('outline.xls')

########NEW FILE########
__FILENAME__ = panes
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman
__rev_id__ = """$Id: panes.py,v 1.1 2005/10/26 07:44:24 rvk Exp $"""


from pyExcelerator import *

w = Workbook()
ws1 = w.add_sheet('sheet 1')
ws2 = w.add_sheet('sheet 2')
ws3 = w.add_sheet('sheet 3')
ws4 = w.add_sheet('sheet 4')
ws5 = w.add_sheet('sheet 5')
ws6 = w.add_sheet('sheet 6')

for i in range(0x100):
    ws1.write(i/0x10, i%0x10, i)

for i in range(0x100):
    ws2.write(i/0x10, i%0x10, i)

for i in range(0x100):
    ws3.write(i/0x10, i%0x10, i)

for i in range(0x100):
    ws4.write(i/0x10, i%0x10, i)

for i in range(0x100):
    ws5.write(i/0x10, i%0x10, i)

for i in range(0x100):
    ws6.write(i/0x10, i%0x10, i)

ws1.panes_frozen = True
ws1.horz_split_pos = 2

ws2.panes_frozen = True
ws2.vert_split_pos = 2

ws3.panes_frozen = True
ws3.horz_split_pos = 1
ws3.vert_split_pos = 1

ws4.panes_frozen = False
ws4.horz_split_pos = 12
ws4.horz_split_first_visible = 2

ws5.panes_frozen = False
ws5.vert_split_pos = 40
ws4.vert_split_first_visible = 2

ws6.panes_frozen = False
ws6.horz_split_pos = 12
ws4.horz_split_first_visible = 2
ws6.vert_split_pos = 40
ws4.vert_split_first_visible = 2

w.save('panes.xls')


########NEW FILE########
__FILENAME__ = parse-fmla
from pyExcelerator import ExcelFormulaParser, ExcelFormula
import sys

f = ExcelFormula.Formula(
""" -((1.80 + 2.898 * 1)/(1.80 + 2.898))*
AVERAGE((1.80 + 2.898 * 1)/(1.80 + 2.898); 
        (1.80 + 2.898 * 1)/(1.80 + 2.898); 
        (1.80 + 2.898 * 1)/(1.80 + 2.898)) + 
SIN(PI()/4)""")

#for t in f.rpn():
#    print "%15s %15s" % (ExcelFormulaParser.PtgNames[t[0]], t[1])

########NEW FILE########
__FILENAME__ = protection
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman
__rev_id__ = """$Id: protection.py,v 1.1 2005/10/26 07:44:24 rvk Exp $"""


from pyExcelerator import *

fnt = Font()
fnt.name = 'Arial'
fnt.colour_index = 4
fnt.bold = True

borders = Borders()
borders.left = 6
borders.right = 6
borders.top = 6
borders.bottom = 6

style = XFStyle()
style.font = fnt
style.borders = borders

wb = Workbook()

ws0 = wb.add_sheet('Rows Outline')

ws0.write_merge(1, 1, 1, 5, 'test 1', style)
ws0.write_merge(2, 2, 1, 4, 'test 1', style)
ws0.write_merge(3, 3, 1, 3, 'test 2', style)
ws0.write_merge(4, 4, 1, 4, 'test 1', style)
ws0.write_merge(5, 5, 1, 4, 'test 3', style)
ws0.write_merge(6, 6, 1, 5, 'test 1', style)
ws0.write_merge(7, 7, 1, 5, 'test 4', style)
ws0.write_merge(8, 8, 1, 4, 'test 1', style)
ws0.write_merge(9, 9, 1, 3, 'test 5', style)

ws0.row(1).level = 1
ws0.row(2).level = 1
ws0.row(3).level = 2
ws0.row(4).level = 2
ws0.row(5).level = 2
ws0.row(6).level = 2
ws0.row(7).level = 2
ws0.row(8).level = 1
ws0.row(9).level = 1


ws1 = wb.add_sheet('Columns Outline')

ws1.write_merge(1, 1, 1, 5, 'test 1', style)
ws1.write_merge(2, 2, 1, 4, 'test 1', style)
ws1.write_merge(3, 3, 1, 3, 'test 2', style)
ws1.write_merge(4, 4, 1, 4, 'test 1', style)
ws1.write_merge(5, 5, 1, 4, 'test 3', style)
ws1.write_merge(6, 6, 1, 5, 'test 1', style)
ws1.write_merge(7, 7, 1, 5, 'test 4', style)
ws1.write_merge(8, 8, 1, 4, 'test 1', style)
ws1.write_merge(9, 9, 1, 3, 'test 5', style)

ws1.col(1).level = 1
ws1.col(2).level = 1
ws1.col(3).level = 2
ws1.col(4).level = 2
ws1.col(5).level = 2
ws1.col(6).level = 2
ws1.col(7).level = 2
ws1.col(8).level = 1
ws1.col(9).level = 1


ws2 = wb.add_sheet('Rows and Columns Outline')

ws2.write_merge(1, 1, 1, 5, 'test 1', style)
ws2.write_merge(2, 2, 1, 4, 'test 1', style)
ws2.write_merge(3, 3, 1, 3, 'test 2', style)
ws2.write_merge(4, 4, 1, 4, 'test 1', style)
ws2.write_merge(5, 5, 1, 4, 'test 3', style)
ws2.write_merge(6, 6, 1, 5, 'test 1', style)
ws2.write_merge(7, 7, 1, 5, 'test 4', style)
ws2.write_merge(8, 8, 1, 4, 'test 1', style)
ws2.write_merge(9, 9, 1, 3, 'test 5', style)

ws2.row(1).level = 1
ws2.row(2).level = 1
ws2.row(3).level = 2
ws2.row(4).level = 2
ws2.row(5).level = 2
ws2.row(6).level = 2
ws2.row(7).level = 2
ws2.row(8).level = 1
ws2.row(9).level = 1

ws2.write_merge(1, 1, 1, 5, 'test 1', style)
ws2.write_merge(2, 2, 1, 4, 'test 1', style)
ws2.write_merge(3, 3, 1, 3, 'test 2', style)
ws2.write_merge(4, 4, 1, 4, 'test 1', style)
ws2.write_merge(5, 5, 1, 4, 'test 3', style)
ws2.write_merge(6, 6, 1, 5, 'test 1', style)
ws2.write_merge(7, 7, 1, 5, 'test 4', style)
ws2.write_merge(8, 8, 1, 4, 'test 1', style)
ws2.write_merge(9, 9, 1, 3, 'test 5', style)

ws2.col(1).level = 1
ws2.col(2).level = 1
ws2.col(3).level = 2
ws2.col(4).level = 2
ws2.col(5).level = 2
ws2.col(6).level = 2
ws2.col(7).level = 2
ws2.col(8).level = 1
ws2.col(9).level = 1


ws0.protect = True
ws0.wnd_protect = True
ws0.obj_protect = True
ws0.scen_protect = True
ws0.password = "123456"

ws1.protect = True
ws1.wnd_protect = True
ws1.obj_protect = True
ws1.scen_protect = True
ws1.password = "abcdefghij"

ws2.protect = True
ws2.wnd_protect = True
ws2.obj_protect = True
ws2.scen_protect = True
ws2.password = "ok"

wb.protect = True
wb.wnd_protect = True
wb.obj_protect = True
wb.save('protection.xls')

########NEW FILE########
__FILENAME__ = row_styles
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman
__rev_id__ = """$Id: row_styles.py,v 1.3 2005/03/27 12:47:06 rvk Exp $"""


from pyExcelerator import *

w = Workbook()
ws = w.add_sheet('Hey, Dude')

for i in range(6, 80):
    fnt = Font()
    fnt.height = i*20
    style = XFStyle()
    style.font = fnt
    ws.write(i, 1, 'Test')
    ws.row(i).set_style(style)
w.save('row_styles.xls')

########NEW FILE########
__FILENAME__ = row_styles_empty
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman
__rev_id__ = """$Id: row_styles_empty.py,v 1.3 2005/03/27 12:47:06 rvk Exp $"""


from pyExcelerator import *

w = Workbook()
ws = w.add_sheet('Hey, Dude')

for i in range(6, 80):
    fnt = Font()
    fnt.height = i*20
    style = XFStyle()
    style.font = fnt
    ws.row(i).set_style(style)
w.save('row_styles_empty.xls')

########NEW FILE########
__FILENAME__ = sst
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman
__rev_id__ = """$Id: sst.py,v 1.2 2005/03/25 07:31:12 rvk Exp $"""


from pyExcelerator import *

font0 = Formatting.Font()
font0.name = 'Arial'
font1 = Formatting.Font()
font1.name = 'Arial Cyr'
font2 = Formatting.Font()
font2.name = 'Times New Roman'
font3 = Formatting.Font()
font3.name = 'Courier New Cyr'

num_format0 = '0.00000'
num_format1 = '0.000000'
num_format2 = '0.0000000'
num_format3 = '0.00000000'

st0 = XFStyle()
st1 = XFStyle()
st2 = XFStyle()
st3 = XFStyle()
st4 = XFStyle()

st0.font = font0
st0.num_format = num_format0

st1.font = font1
st1.num_format = num_format1

st2.font = font2
st2.num_format = num_format2

st3.font = font3
st3.num_format = num_format3

wb = Workbook()

wb.add_style(st0)
wb.add_style(st1)
wb.add_style(st2)
wb.add_style(st3)

ws0 = wb.add_sheet('0')
ws0.write(0, 0, 'Olya'*0x4000, st0)

#for i in range(0, 0x10):
#    ws0.write(i, 2, ('%d'%i)*0x4000, st1)
    
wb.save('sst.xls')

########NEW FILE########
__FILENAME__ = unicode0
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman
__rev_id__ = """$Id: unicode0.py,v 1.1 2005/03/27 12:47:06 rvk Exp $"""


from pyExcelerator import *

w = Workbook()
ws1 = w.add_sheet('cp1251')

UnicodeUtils.DEFAULT_ENCODING = 'cp1251'
ws1.write(0, 0, '')

w.save('unicode0.xls')


########NEW FILE########
__FILENAME__ = unicode1
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman
__rev_id__ = """$Id: unicode1.py,v 1.1 2005/03/27 12:47:06 rvk Exp $"""


from pyExcelerator import *

w = Workbook()
ws1 = w.add_sheet(u'\N{GREEK SMALL LETTER ALPHA}\N{GREEK SMALL LETTER BETA}\N{GREEK SMALL LETTER GAMMA}')

ws1.write(0, 0, u'\N{GREEK SMALL LETTER ALPHA}\N{GREEK SMALL LETTER BETA}\N{GREEK SMALL LETTER GAMMA}')
ws1.write(1, 1, u'\N{GREEK SMALL LETTER DELTA}x = 1 + \N{GREEK SMALL LETTER DELTA}')

ws1.write(2,0, u'A\u2262\u0391.')     # RFC2152 example
ws1.write(3,0, u'Hi Mom -\u263a-!')   # RFC2152 example
ws1.write(4,0, u'\u65E5\u672C\u8A9E') # RFC2152 example
ws1.write(5,0, u'Item 3 is \u00a31.') # RFC2152 example
ws1.write(8,0, u'\N{INTEGRAL}')       # RFC2152 example

w.add_sheet(u'A\u2262\u0391.')     # RFC2152 example
w.add_sheet(u'Hi Mom -\u263a-!')   # RFC2152 example
one_more_ws = w.add_sheet(u'\u65E5\u672C\u8A9E') # RFC2152 example
w.add_sheet(u'Item 3 is \u00a31.') # RFC2152 example

one_more_ws.write(0, 0, u'\u2665\u2665')

w.add_sheet(u'\N{GREEK SMALL LETTER ETA WITH TONOS}')
w.save('unicode1.xls')


########NEW FILE########
__FILENAME__ = unicode2
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman
__rev_id__ = """$Id: unicode2.py,v 1.1 2005/03/27 12:47:06 rvk Exp $"""


from pyExcelerator import *

w = Workbook()
ws1 = w.add_sheet(u'\N{GREEK SMALL LETTER ALPHA}\N{GREEK SMALL LETTER BETA}\N{GREEK SMALL LETTER GAMMA}\u2665\u041e\u041b\u042f\u2665')

fnt = Font()
fnt.height = 26*20
style = XFStyle()
style.font = fnt

for i in range(0x10000):
    ws1.write(i/0x10, i%0x10, unichr(i), style)

w.save('unicode2.xls')


########NEW FILE########
__FILENAME__ = wsprops
__rev_id__ = """$Id: wsprops.py,v 1.3 2005/03/27 12:47:06 rvk Exp $"""

props = \
[
        'name',
        'parent',
        'rows',
        'cols',
        'merged_ranges',
        'bmp_rec',
        'show_formulas',
        'show_grid',
        'show_headers',
        'panes_frozen',
        'show_empty_as_zero',
        'auto_colour_grid',
        'cols_right_to_left',
        'show_outline',
        'remove_splits',
        'selected',
        'hidden',
        'page_preview',
        'first_visible_row',
        'first_visible_col',
        'grid_colour',
        'preview_magn',
        'normal_magn',
        'row_gut_width',
        'col_gut_height',
        'show_auto_page_breaks',
        'dialogue_sheet',
        'auto_style_outline',
        'outline_below',
        'outline_right',
        'fit_num_pages',
        'show_row_outline',
        'show_col_outline',
        'alt_expr_eval',
        'alt_formula_entries',
        'row_default_height',
        'col_default_width',
        'calc_mode',
        'calc_count',
        'RC_ref_mode',
        'iterations_on',
        'delta',
        'save_recalc',
        'print_headers',
        'print_grid',
        'grid_set',
        'vert_page_breaks',
        'horz_page_breaks',
        'header_str',
        'footer_str',
        'print_centered_vert',
        'print_centered_horz',
        'left_margin',
        'right_margin',
        'top_margin',
        'bottom_margin',
        'paper_size_code',
        'print_scaling',
        'start_page_number',
        'fit_width_to_pages',
        'fit_height_to_pages',
        'print_in_rows',
        'portrait',
        'print_not_colour',
        'print_draft',
        'print_notes',
        'print_notes_at_end',
        'print_omit_errors',
        'print_hres',
        'print_vres',
        'header_margin',
        'footer_margin',
        'copies_num',
]

from pyExcelerator import *

wb = Workbook()
ws = wb.add_sheet('sheet')

print ws.name
print ws.parent
print ws.rows
print ws.cols
print ws.merged_ranges
print ws.bmp_rec
print ws.show_formulas
print ws.show_grid
print ws.show_headers
print ws.panes_frozen
print ws.show_empty_as_zero
print ws.auto_colour_grid
print ws.cols_right_to_left
print ws.show_outline
print ws.remove_splits
print ws.selected
print ws.hidden
print ws.page_preview
print ws.first_visible_row
print ws.first_visible_col
print ws.grid_colour
print ws.preview_magn
print ws.normal_magn
#print ws.row_gut_width
#print ws.col_gut_height
print ws.show_auto_page_breaks
print ws.dialogue_sheet
print ws.auto_style_outline
print ws.outline_below
print ws.outline_right
print ws.fit_num_pages
print ws.show_row_outline
print ws.show_col_outline
print ws.alt_expr_eval
print ws.alt_formula_entries
print ws.row_default_height
print ws.col_default_width
print ws.calc_mode
print ws.calc_count
print ws.RC_ref_mode
print ws.iterations_on
print ws.delta
print ws.save_recalc
print ws.print_headers
print ws.print_grid
#print ws.grid_set
print ws.vert_page_breaks
print ws.horz_page_breaks
print ws.header_str
print ws.footer_str
print ws.print_centered_vert
print ws.print_centered_horz
print ws.left_margin
print ws.right_margin
print ws.top_margin
print ws.bottom_margin
print ws.paper_size_code
print ws.print_scaling
print ws.start_page_number
print ws.fit_width_to_pages
print ws.fit_height_to_pages
print ws.print_in_rows
print ws.portrait
print ws.print_colour
print ws.print_draft
print ws.print_notes
print ws.print_notes_at_end
print ws.print_omit_errors
print ws.print_hres
print ws.print_vres
print ws.header_margin
print ws.footer_margin
print ws.copies_num

########NEW FILE########
__FILENAME__ = py-2.4-hrc
specials = [
    "Ellipsis", 
    "False", 
    "None", 
    "NotImplemented", 
    "True", 
    "__abs__", 
    "__add__", 
    "__and__", 
    "__base__", 
    "__bases__", 
    "__basicsize__", 
    "__builtins__", 
    "__call__", 
    "__class__", 
    "__cmp__", 
    "__coerce__", 
    "__contains__", 
    "__debug__", 
    "__del__", 
    "__delattr__", 
    "__delete__", 
    "__delitem__", 
    "__delslice__", 
    "__dict__", 
    "__dictoffset__", 
    "__div__", 
    "__divmod__", 
    "__doc__", 
    "__eq__", 
    "__flags__", 
    "__float__", 
    "__floordiv__", 
    "__ge__", 
    "__get__", 
    "__getattr__", 
    "__getattribute__", 
    "__getitem__", 
    "__getnewargs__", 
    "__getslice__", 
    "__gt__", 
    "__hash__", 
    "__hex__", 
    "__iadd__", 
    "__iand__", 
    "__idiv__", 
    "__ifloordiv__", 
    "__ilshift__", 
    "__imod__", 
    "__import__", 
    "__imul__", 
    "__init__", 
    "__int__", 
    "__invert__", 
    "__ior__", 
    "__ipow__", 
    "__irshift__", 
    "__isub__", 
    "__itemsize__", 
    "__iter__", 
    "__itruediv__", 
    "__ixor__", 
    "__le__", 
    "__len__", 
    "__long__", 
    "__lshift__", 
    "__lt__", 
    "__main__", 
    "__mod__", 
    "__module__", 
    "__mro__", 
    "__mul__", 
    "__name__", 
    "__ne__", 
    "__neg__", 
    "__new__", 
    "__nonzero__", 
    "__oct__", 
    "__or__", 
    "__pos__", 
    "__pow__", 
    "__radd__", 
    "__rand__", 
    "__rdiv__", 
    "__rdivmod__", 
    "__reduce__", 
    "__reduce_ex__", 
    "__repr__", 
    "__rfloordiv__", 
    "__rlshift__", 
    "__rmod__", 
    "__rmul__", 
    "__ror__", 
    "__rpow__", 
    "__rrshift__", 
    "__rshift__", 
    "__rsub__", 
    "__rtruediv__", 
    "__rxor__", 
    "__set__", 
    "__setattr__", 
    "__setitem__", 
    "__setslice__", 
    "__str__", 
    "__sub__", 
    "__truediv__", 
    "__weakrefoffset__", 
    "__xor__", 
    "abs", 
    "apply", 
    "basestring", 
    "bool", 
    "buffer", 
    "callable", 
    "capitalize", 
    "center", 
    "chr", 
    "classmethod", 
    "cmp", 
    "coerce", 
    "compile", 
    "complex", 
    "count", 
    "decode", 
    "delattr", 
    "dict", 
    "dir", 
    "divmod", 
    "encode", 
    "endswith", 
    "enumerate", 
    "eval", 
    "execfile", 
    "exit", 
    "expandtabs", 
    "file", 
    "filter", 
    "find", 
    "float", 
    "frozenset", 
    "getattr", 
    "globals", 
    "hasattr", 
    "hash", 
    "hex", 
    "id", 
    "index", 
    "input", 
    "int", 
    "intern", 
    "isalnum", 
    "isalpha", 
    "isdigit", 
    "isinstance", 
    "islower", 
    "isspace", 
    "issubclass", 
    "istitle", 
    "isupper", 
    "iter", 
    "join", 
    "len", 
    "list", 
    "ljust", 
    "locals", 
    "long", 
    "lower", 
    "lstrip", 
    "map", 
    "max", 
    "min", 
    "next", 
    "object", 
    "oct", 
    "open", 
    "ord", 
    "pow", 
    "property", 
    "quit", 
    "range", 
    "raw_input", 
    "reduce", 
    "reload", 
    "replace", 
    "repr", 
    "reversed", 
    "rfind", 
    "rindex", 
    "rjust", 
    "round", 
    "rsplit", 
    "rstrip", 
    "set", 
    "setattr", 
    "slice", 
    "sorted", 
    "split", 
    "splitlines", 
    "startswith", 
    "staticmethod", 
    "str", 
    "strip", 
    "sum", 
    "super", 
    "swapcase", 
    "title", 
    "translate", 
    "tuple", 
    "type", 
    "unichr", 
    "unicode", 
    "upper", 
    "vars", 
    "xrange", 
    "zfill", 
    "zip"
]

exceptions = [
    "Exception",
    "SystemExit",
    "StopIteration",
    "StandardError",
    "KeyboardInterrupt",
    "ImportError",
    "EnvironmentError",
    "IOError",
    "OSError",
    "WindowsError",
    "EOFError",
    "RuntimeError",
    "NotImplementedError",
    "NameError",
    "UnboundLocalError",
    "AttributeError",
    "SyntaxError",
    "IndentationError",
    "TabError",
    "TypeError",
    "AssertionError",
    "LookupError",
    "IndexError",
    "KeyError",
    "ArithmeticError",
    "OverflowError",
    "ZeroDivisionError",
    "FloatingPointError",
    "ValueError",
    "UnicodeError",
    "UnicodeEncodeError",
    "UnicodeDecodeError",
    "UnicodeTranslateError",
    "ReferenceError",
    "SystemError",
    "MemoryError",
    "Warning",
    "UserWarning",
    "DeprecationWarning",
    "PendingDeprecationWarning",
    "SyntaxWarning",
    "OverflowWarning", # not generated in 2.4; won't exist in 2.5
    "RuntimeWarning",
    "FutureWarning"
]

reserved_words = [
    "and",
    "del",       
    "for",       
    "is",       
    "raise",
    "assert",    
    "elif",
    "from" ,     
    "lambda",    
    "return",   
    "break",     
    "else",      
    "global",    
    "not",
    "try",     
    "class",     
    "except",    
    "if",   
    "or",        
    "while",    
    "continue",  
    "exec",      
    "import",    
    "pass" ,     
    "yield" ,   
    "def",       
    "finally",   
    "in",        
    "print"
]

print '<keywords region="pyWord">'
for word in sorted(reserved_words):
    print '    <word name="%s"/>' % word
print '</keywords>'

print '<keywords region="pyException">'
for exc in sorted(exceptions):
    print '    <word name="%s"/>' % exc
print '</keywords>'

print '<keywords region="pyBuiltins">'
for special in sorted(specials):
    print '    <word name="%s"/>' % special
print '</keywords>'


########NEW FILE########
__FILENAME__ = antlr
## This file is part of PyANTLR. See LICENSE.txt for license
## details..........Copyright (C) Wolfgang Haefelinger, 2004.

## get sys module
import sys

version = sys.version.split()[0]
if version < '2.2.1':
    False = 0
if version < '2.3':
    True = not False

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                     global symbols                             ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

### ANTLR Standard Tokens
SKIP                = -1
INVALID_TYPE        = 0
EOF_TYPE            = 1
EOF                 = 1
NULL_TREE_LOOKAHEAD = 3
MIN_USER_TYPE       = 4

### ANTLR's EOF Symbol
EOF_CHAR            = ''

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                    general functions                           ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

def error(fmt,*args):
    if fmt:
        print "error: ", fmt % tuple(args)

def ifelse(cond,_then,_else):
    if cond :
        r = _then
    else:
        r = _else
    return r

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                     ANTLR Exceptions                           ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class ANTLRException(Exception):

    def __init__(self, *args):
        Exception.__init__(self, *args)


class RecognitionException(ANTLRException):

    def __init__(self, *args):
        ANTLRException.__init__(self, *args)
        self.fileName = None
        self.line = -1
        self.column = -1
        if len(args) >= 2:
            self.fileName = args[1]
        if len(args) >= 3:
            self.line = args[2]
        if len(args) >= 4:
            self.column = args[3]

    def __str__(self):
        buf = ['']
        if self.fileName:
            buf.append(self.fileName + ":")
        if self.line != -1:
            if not self.fileName:
                buf.append("line ")
            buf.append(str(self.line))
            if self.column != -1:
                buf.append(":" + str(self.column))
            buf.append(":")
        buf.append(" ")
        return str('').join(buf)

    __repr__ = __str__


class NoViableAltException(RecognitionException):

    def __init__(self, *args):
        RecognitionException.__init__(self, *args)
        self.token = None
        self.node  = None
        if isinstance(args[0],AST):
            self.node = args[0]
        elif isinstance(args[0],Token):
            self.token = args[0]
        else:
            raise TypeError("NoViableAltException requires Token or AST argument")

    def __str__(self):
        if self.token:
            line = self.token.getLine()
            col  = self.token.getColumn()
            text = self.token.getText()
            return "unexpected symbol at line %s (column %s): \"%s\"" % (line,col,text)
        if self.node == ASTNULL:
            return "unexpected end of subtree"
        assert self.node
        ### hackish, we assume that an AST contains method getText
        return "unexpected node: %s" % (self.node.getText())

    __repr__ = __str__


class NoViableAltForCharException(RecognitionException):

    def __init__(self, *args):
        self.foundChar = None
        if len(args) == 2:
            self.foundChar = args[0]
            scanner = args[1]
            RecognitionException.__init__(self, "NoViableAlt",
                                          scanner.getFilename(),
                                          scanner.getLine(),
                                          scanner.getColumn())
        elif len(args) == 4:
            self.foundChar = args[0]
            fileName = args[1]
            line = args[2]
            column = args[3]
            RecognitionException.__init__(self, "NoViableAlt",
                                          fileName, line, column)
        else:
            RecognitionException.__init__(self, "NoViableAlt",
                                          '', -1, -1)

    def __str__(self):
        mesg = "unexpected char: "
        if self.foundChar >= ' ' and self.foundChar <= '~':
            mesg += "'" + self.foundChar + "'"
        elif self.foundChar:
            mesg += "0x" + hex(ord(self.foundChar)).upper()[2:]
        else:
            mesg += "<None>"
        return mesg

    __repr__ = __str__


class SemanticException(RecognitionException):

    def __init__(self, *args):
        RecognitionException.__init__(self, *args)


class MismatchedCharException(RecognitionException):

    NONE = 0
    CHAR = 1
    NOT_CHAR = 2
    RANGE = 3
    NOT_RANGE = 4
    SET = 5
    NOT_SET = 6

    def __init__(self, *args):
        self.args = args
        if len(args) == 5:
            # Expected range / not range
            if args[3]:
                self.mismatchType = MismatchedCharException.NOT_RANGE
            else:
                self.mismatchType = MismatchedCharException.RANGE
            self.foundChar = args[0]
            self.expecting = args[1]
            self.upper = args[2]
            self.scanner = args[4]
            RecognitionException.__init__(self, "Mismatched char range",
                                          self.scanner.getFilename(),
                                          self.scanner.getLine(),
                                          self.scanner.getColumn())
        elif len(args) == 4 and isinstance(args[1], str):
            # Expected char / not char
            if args[2]:
                self.mismatchType = MismatchedCharException.NOT_CHAR
            else:
                self.mismatchType = MismatchedCharException.CHAR
            self.foundChar = args[0]
            self.expecting = args[1]
            self.scanner = args[3]
            RecognitionException.__init__(self, "Mismatched char",
                                          self.scanner.getFilename(),
                                          self.scanner.getLine(),
                                          self.scanner.getColumn())
        elif len(args) == 4 and isinstance(args[1], BitSet):
            # Expected BitSet / not BitSet
            if args[2]:
                self.mismatchType = MismatchedCharException.NOT_SET
            else:
                self.mismatchType = MismatchedCharException.SET
            self.foundChar = args[0]
            self.set = args[1]
            self.scanner = args[3]
            RecognitionException.__init__(self, "Mismatched char set",
                                          self.scanner.getFilename(),
                                          self.scanner.getLine(),
                                          self.scanner.getColumn())
        else:
            self.mismatchType = MismatchedCharException.NONE
            RecognitionException.__init__(self, "Mismatched char")

    ## Append a char to the msg buffer.  If special,
    #  then show escaped version
    #
    def appendCharName(self, sb, c):
        if not c or c == 65535:
            # 65535 = (char) -1 = EOF
            sb.append("'<EOF>'")
        elif c == '\n':
            sb.append("'\\n'")
        elif c == '\r':
            sb.append("'\\r'");
        elif c == '\t':
            sb.append("'\\t'")
        else:
            sb.append('\'' + c + '\'')

    ##
    # Returns an error message with line number/column information
    #
    def __str__(self):
        sb = ['']
        sb.append(RecognitionException.__str__(self))

        if self.mismatchType == MismatchedCharException.CHAR:
            sb.append("expecting ")
            self.appendCharName(sb, self.expecting)
            sb.append(", found ")
            self.appendCharName(sb, self.foundChar)
        elif self.mismatchType == MismatchedCharException.NOT_CHAR:
            sb.append("expecting anything but '")
            self.appendCharName(sb, self.expecting)
            sb.append("'; got it anyway")
        elif self.mismatchType in [MismatchedCharException.RANGE, MismatchedCharException.NOT_RANGE]:
            sb.append("expecting char ")
            if self.mismatchType == MismatchedCharException.NOT_RANGE:
                sb.append("NOT ")
            sb.append("in range: ")
            appendCharName(sb, self.expecting)
            sb.append("..")
            appendCharName(sb, self.upper)
            sb.append(", found ")
            appendCharName(sb, self.foundChar)
        elif self.mismatchType in [MismatchedCharException.SET, MismatchedCharException.NOT_SET]:
            sb.append("expecting ")
            if self.mismatchType == MismatchedCharException.NOT_SET:
                sb.append("NOT ")
            sb.append("one of (")
            for i in range(len(self.set)):
                self.appendCharName(sb, self.set[i])
            sb.append("), found ")
            self.appendCharName(sb, self.foundChar)

        return str().join(sb).strip()

    __repr__ = __str__


class MismatchedTokenException(RecognitionException):

    NONE = 0
    TOKEN = 1
    NOT_TOKEN = 2
    RANGE = 3
    NOT_RANGE = 4
    SET = 5
    NOT_SET = 6

    def __init__(self, *args):
        self.args =  args
        self.tokenNames = []
        self.token = None
        self.tokenText = ''
        self.node =  None
        if len(args) == 6:
            # Expected range / not range
            if args[3]:
                self.mismatchType = MismatchedTokenException.NOT_RANGE
            else:
                self.mismatchType = MismatchedTokenException.RANGE
            self.tokenNames = args[0]
            self.expecting = args[2]
            self.upper = args[3]
            self.fileName = args[5]

        elif len(args) == 4 and isinstance(args[2], int):
            # Expected token / not token
            if args[3]:
                self.mismatchType = MismatchedTokenException.NOT_TOKEN
            else:
                self.mismatchType = MismatchedTokenException.TOKEN
            self.tokenNames = args[0]
            self.expecting = args[2]

        elif len(args) == 4 and isinstance(args[2], BitSet):
            # Expected BitSet / not BitSet
            if args[3]:
                self.mismatchType = MismatchedTokenException.NOT_SET
            else:
                self.mismatchType = MismatchedTokenException.SET
            self.tokenNames = args[0]
            self.set = args[2]

        else:
            self.mismatchType = MismatchedTokenException.NONE
            RecognitionException.__init__(self, "Mismatched Token: expecting any AST node", "<AST>", -1, -1)

        if len(args) >= 2:
            if isinstance(args[1],Token):
                self.token = args[1]
                self.tokenText = self.token.getText()
                RecognitionException.__init__(self, "Mismatched Token",
                                              self.fileName,
                                              self.token.getLine(),
                                              self.token.getColumn())
            elif isinstance(args[1],AST):
                self.node = args[1]
                self.tokenText = str(self.node)
                RecognitionException.__init__(self, "Mismatched Token",
                                              "<AST>",
                                              self.node.getLine(),
                                              self.node.getColumn())
            else:
                self.tokenText = "<empty tree>"
                RecognitionException.__init__(self, "Mismatched Token",
                                              "<AST>", -1, -1)

    def appendTokenName(self, sb, tokenType):
        if tokenType == INVALID_TYPE:
            sb.append("<Set of tokens>")
        elif tokenType < 0 or tokenType >= len(self.tokenNames):
            sb.append("<" + str(tokenType) + ">")
        else:
            sb.append(self.tokenNames[tokenType])

    ##
    # Returns an error message with line number/column information
    #
    def __str__(self):
        sb = ['']
        sb.append(RecognitionException.__str__(self))

        if self.mismatchType == MismatchedTokenException.TOKEN:
            sb.append("expecting ")
            self.appendTokenName(sb, self.expecting)
            sb.append(", found " + self.tokenText)
        elif self.mismatchType == MismatchedTokenException.NOT_TOKEN:
            sb.append("expecting anything but '")
            self.appendTokenName(sb, self.expecting)
            sb.append("'; got it anyway")
        elif self.mismatchType in [MismatchedTokenException.RANGE, MismatchedTokenException.NOT_RANGE]:
            sb.append("expecting token ")
            if self.mismatchType == MismatchedTokenException.NOT_RANGE:
                sb.append("NOT ")
            sb.append("in range: ")
            appendTokenName(sb, self.expecting)
            sb.append("..")
            appendTokenName(sb, self.upper)
            sb.append(", found " + self.tokenText)
        elif self.mismatchType in [MismatchedTokenException.SET, MismatchedTokenException.NOT_SET]:
            sb.append("expecting ")
            if self.mismatchType == MismatchedTokenException.NOT_SET:
                sb.append("NOT ")
            sb.append("one of (")
            for i in range(len(self.set)):
                self.appendTokenName(sb, self.set[i])
            sb.append("), found " + self.tokenText)

        return str().join(sb).strip()

    __repr__ = __str__


class TokenStreamException(ANTLRException):

    def __init__(self, *args):
        ANTLRException.__init__(self, *args)


# Wraps an Exception in a TokenStreamException
class TokenStreamIOException(TokenStreamException):

    def __init__(self, *args):
        if args and isinstance(args[0], Exception):
            io = args[0]
            TokenStreamException.__init__(self, str(io))
            self.io = io
        else:
            TokenStreamException.__init__(self, *args)
            self.io = self


# Wraps a RecognitionException in a TokenStreamException
class TokenStreamRecognitionException(TokenStreamException):

    def __init__(self, *args):
        if args and isinstance(args[0], RecognitionException):
            recog = args[0]
            TokenStreamException.__init__(self, str(recog))
            self.recog = recog
        else:
            raise TypeError("TokenStreamRecognitionException requires RecognitionException argument")

    def __str__(self):
        return str(self.recog)

    __repr__ = __str__


class TokenStreamRetryException(TokenStreamException):

    def __init__(self, *args):
        TokenStreamException.__init__(self, *args)


class CharStreamException(ANTLRException):

    def __init__(self, *args):
        ANTLRException.__init__(self, *args)


# Wraps an Exception in a CharStreamException
class CharStreamIOException(CharStreamException):

    def __init__(self, *args):
        if args and isinstance(args[0], Exception):
            io = args[0]
            CharStreamException.__init__(self, str(io))
            self.io = io
        else:
            CharStreamException.__init__(self, *args)
            self.io = self


class TryAgain(Exception):
    pass


###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       Token                                    ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class Token(object):
    SKIP                = -1
    INVALID_TYPE        = 0
    EOF_TYPE            = 1
    EOF                 = 1
    NULL_TREE_LOOKAHEAD = 3
    MIN_USER_TYPE       = 4

    def __init__(self,**argv):
        try:
            self.type = argv['type']
        except:
            self.type = INVALID_TYPE
        try:
            self.text = argv['text']
        except:
            self.text = "<no text>"

    def isEOF(self):
        return (self.type == EOF_TYPE)

    def getColumn(self):
        return 0

    def getLine(self):
        return 0

    def getFilename(self):
        return None

    def setFilename(self,name):
        return self

    def getText(self):
        return "<no text>"

    def setText(self,text):
        if isinstance(text,str):
            pass
        else:
            raise TypeError("Token.setText requires string argument")
        return self

    def setColumn(self,column):
        return self

    def setLine(self,line):
        return self

    def getType(self):
        return self.type

    def setType(self,type):
        if isinstance(type,int):
            self.type = type
        else:
            raise TypeError("Token.setType requires integer argument")
        return self

    def toString(self):
        ## not optimal
        type_ = self.type
        if type_ == 3:
            tval = 'NULL_TREE_LOOKAHEAD'
        elif type_ == 1:
            tval = 'EOF_TYPE'
        elif type_ == 0:
            tval = 'INVALID_TYPE'
        elif type_ == -1:
            tval = 'SKIP'
        else:
            tval = type_
        return '["%s",<%s>]' % (self.getText(),tval)

    __str__ = toString
    __repr__ = toString

### static attribute ..
Token.badToken = Token( type=INVALID_TYPE, text="<no text>")

if __name__ == "__main__":
    print "testing .."
    T = Token.badToken
    print T

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       CommonToken                              ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class CommonToken(Token):

    def __init__(self,**argv):
        Token.__init__(self,**argv)
        self.line = 0
        self.col  = 0
        try:
            self.line = argv['line']
        except:
            pass
        try:
            self.col = argv['col']
        except:
            pass

    def getLine(self):
        return self.line

    def getText(self):
        return self.text

    def getColumn(self):
        return self.col

    def setLine(self,line):
        self.line = line
        return self

    def setText(self,text):
        self.text = text
        return self

    def setColumn(self,col):
        self.col = col
        return self

    def toString(self):
        ## not optimal
        type_ = self.type
        if type_ == 3:
            tval = 'NULL_TREE_LOOKAHEAD'
        elif type_ == 1:
            tval = 'EOF_TYPE'
        elif type_ == 0:
            tval = 'INVALID_TYPE'
        elif type_ == -1:
            tval = 'SKIP'
        else:
            tval = type_
        d = {
           'text' : self.text,
           'type' : tval,
           'line' : self.line,
           'colm' : self.col
           }

        fmt = '["%(text)s",<%(type)s>,line=%(line)s,col=%(colm)s]'
        return fmt % d

    __str__ = toString
    __repr__ = toString


if __name__ == '__main__' :
    T = CommonToken()
    print T
    T = CommonToken(col=15,line=1,text="some text", type=5)
    print T
    T = CommonToken()
    T.setLine(1).setColumn(15).setText("some text").setType(5)
    print T
    print T.getLine()
    print T.getColumn()
    print T.getText()
    print T.getType()

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                    CommonHiddenStreamToken                     ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class CommonHiddenStreamToken(CommonToken):
    def __init__(self,*args):
        CommonToken.__init__(self,*args)
        self.hiddenBefore = None
        self.hiddenAfter  = None

    def getHiddenAfter(self):
        return self.hiddenAfter

    def getHiddenBefore(self):
        return self.hiddenBefore

    def setHiddenAfter(self,t):
        self.hiddenAfter = t

    def setHiddenBefore(self, t):
        self.hiddenBefore = t

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       Queue                                    ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

## Shall be a circular buffer on tokens ..
class Queue(object):

    def __init__(self):
        self.buffer = [] # empty list

    def append(self,item):
        self.buffer.append(item)

    def elementAt(self,index):
        return self.buffer[index]

    def reset(self):
        self.buffer = []

    def removeFirst(self):
        self.buffer.pop(0)

    def length(self):
        return len(self.buffer)

    def __str__(self):
        return str(self.buffer)

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       InputBuffer                              ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class InputBuffer(object):
    def __init__(self):
        self.nMarkers = 0
        self.markerOffset = 0
        self.numToConsume = 0
        self.queue = Queue()

    def __str__(self):
        return "(%s,%s,%s,%s)" % (
           self.nMarkers,
           self.markerOffset,
           self.numToConsume,
           self.queue)

    def __repr__(self):
        return str(self)

    def commit(self):
        self.nMarkers -= 1

    def consume(self) :
        self.numToConsume += 1

    ## probably better to return a list of items
    ## because of unicode. Or return a unicode
    ## string ..
    def getLAChars(self) :
        i = self.markerOffset
        n = self.queue.length()
        s = ''
        while i<n:
            s += self.queue.elementAt(i)
        return s

    ## probably better to return a list of items
    ## because of unicode chars
    def getMarkedChars(self) :
        s = ''
        i = 0
        n = self.markerOffset
        while i<n:
            s += self.queue.elementAt(i)
        return s

    def isMarked(self) :
        return self.nMarkers != 0

    def fill(self,k):
        ### abstract method
        raise NotImplementedError()

    def LA(self,k) :
        self.fill(k)
        return self.queue.elementAt(self.markerOffset + k - 1)

    def mark(self) :
        self.syncConsume()
        self.nMarkers += 1
        return self.markerOffset

    def rewind(self,mark) :
        self.syncConsume()
        self.markerOffset = mark
        self.nMarkers -= 1

    def reset(self) :
        self.nMarkers = 0
        self.markerOffset = 0
        self.numToConsume = 0
        self.queue.reset()

    def syncConsume(self) :
        while self.numToConsume > 0:
            if self.nMarkers > 0:
                # guess mode -- leave leading characters and bump offset.
                self.markerOffset += 1
            else:
                # normal mode -- remove first character
                self.queue.removeFirst()
            self.numToConsume -= 1

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       CharBuffer                               ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class CharBuffer(InputBuffer):
    def __init__(self,reader):
        ##assert isinstance(reader,file)
        super(CharBuffer,self).__init__()
        ## a reader is supposed to be anything that has
        ## a method 'read(int)'.
        self.input = reader

    def __str__(self):
        base = super(CharBuffer,self).__str__()
        return "CharBuffer{%s,%s" % (base,str(input))

    def fill(self,amount):
        try:
            self.syncConsume()
            while self.queue.length() < (amount + self.markerOffset) :
                ## retrieve just one char - what happend at end
                ## of input?
                c = self.input.read(1)
                ### python's behaviour is to return the empty string  on
                ### EOF, ie. no exception whatsoever is thrown. An empty
                ### python  string  has  the  nice feature that it is of
                ### type 'str' and  "not ''" would return true. Contrary,
                ### one can't  do  this: '' in 'abc'. This should return
                ### false,  but all we  get  is  then  a TypeError as an
                ### empty string is not a character.

                ### Let's assure then that we have either seen a
                ### character or an empty string (EOF).
                assert len(c) == 0 or len(c) == 1

                ### And it shall be of type string (ASCII or UNICODE).
                assert isinstance(c,str) or isinstance(c,unicode)

                ### Just append EOF char to buffer. Note that buffer may
                ### contain then just more than one EOF char ..

                ### use unicode chars instead of ASCII ..
                self.queue.append(c)
        except Exception,e:
            raise CharStreamIOException(e)
        ##except: # (mk) Cannot happen ...
            ##error ("unexpected exception caught ..")
            ##assert 0

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       LexerSharedInputState                    ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class LexerSharedInputState(object):
    def __init__(self,ibuf):
        assert isinstance(ibuf,InputBuffer)
        self.input = ibuf
        self.column = 1
        self.line = 1
        self.tokenStartColumn = 1
        self.tokenStartLine = 1
        self.guessing = 0
        self.filename = None

    def reset(self):
        self.column = 1
        self.line = 1
        self.tokenStartColumn = 1
        self.tokenStartLine = 1
        self.guessing = 0
        self.filename = None
        self.input.reset()

    def LA(self,k):
        return self.input.LA(k)

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                    TokenStream                                 ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TokenStream(object):
    def nextToken(self):
        pass

    def __iter__(self):
        return TokenStreamIterator(self)

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                    TokenStreamIterator                                 ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TokenStreamIterator(object):
    def __init__(self,inst):
        if isinstance(inst,TokenStream):
            self.inst = inst
            return
        raise TypeError("TokenStreamIterator requires TokenStream object")

    def next(self):
        assert self.inst
        item = self.inst.nextToken()
        if not item or item.isEOF():
            raise StopIteration()
        return item

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                    TokenStreamSelector                        ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TokenStreamSelector(TokenStream):

    def __init__(self):
        self._input = None
        self._stmap = {}
        self._stack = []

    def addInputStream(self,stream,key):
        self._stmap[key] = stream

    def getCurrentStream(self):
        return self._input

    def getStream(self,sname):
        try:
            stream = self._stmap[sname]
        except:
            raise ValueError("TokenStream " + sname + " not found");
        return stream;

    def nextToken(self):
        while 1:
            try:
                return self._input.nextToken()
            except TokenStreamRetryException,r:
                ### just retry "forever"
                pass

    def pop(self):
        stream = self._stack.pop();
        self.select(stream);
        return stream;

    def push(self,arg):
        self._stack.append(self._input);
        self.select(arg)

    def retry(self):
        raise TokenStreamRetryException()

    def select(self,arg):
        if isinstance(arg,TokenStream):
            self._input = arg
            return
        if isinstance(arg,str):
            self._input = self.getStream(arg)
            return
        raise TypeError("TokenStreamSelector.select requires " +
                        "TokenStream or string argument")

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                      TokenStreamBasicFilter                    ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TokenStreamBasicFilter(TokenStream):

    def __init__(self,input):

        self.input = input;
        self.discardMask = BitSet()

    def discard(self,arg):
        if isinstance(arg,int):
            self.discardMask.add(arg)
            return
        if isinstance(arg,BitSet):
            self.discardMark = arg
            return
        raise TypeError("TokenStreamBasicFilter.discard requires" +
                        "integer or BitSet argument")

    def nextToken(self):
        tok = self.input.nextToken()
        while tok and self.discardMask.member(tok.getType()):
            tok = self.input.nextToken()
        return tok

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                      TokenStreamHiddenTokenFilter              ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TokenStreamHiddenTokenFilter(TokenStreamBasicFilter):

    def __init__(self,input):
        TokenStreamBasicFilter.__init__(self,input)
        self.hideMask = BitSet()
        self.nextMonitoredToken = None
        self.lastHiddenToken = None
        self.firstHidden = None

    def consume(self):
        self.nextMonitoredToken = self.input.nextToken()

    def consumeFirst(self):
        self.consume()

        p = None;
        while self.hideMask.member(self.LA(1).getType()) or \
              self.discardMask.member(self.LA(1).getType()):
            if self.hideMask.member(self.LA(1).getType()):
                if not p:
                    p = self.LA(1)
                else:
                    p.setHiddenAfter(self.LA(1))
                    self.LA(1).setHiddenBefore(p)
                    p = self.LA(1)
                self.lastHiddenToken = p
                if not self.firstHidden:
                    self.firstHidden = p
            self.consume()

    def getDiscardMask(self):
        return self.discardMask

    def getHiddenAfter(self,t):
        return t.getHiddenAfter()

    def getHiddenBefore(self,t):
        return t.getHiddenBefore()

    def getHideMask(self):
        return self.hideMask

    def getInitialHiddenToken(self):
        return self.firstHidden

    def hide(self,m):
        if isinstance(m,int):
            self.hideMask.add(m)
            return
        if isinstance(m.BitMask):
            self.hideMask = m
            return

    def LA(self,i):
        return self.nextMonitoredToken

    def nextToken(self):
        if not self.LA(1):
            self.consumeFirst()

        monitored = self.LA(1)

        monitored.setHiddenBefore(self.lastHiddenToken)
        self.lastHiddenToken = None

        self.consume()
        p = monitored

        while self.hideMask.member(self.LA(1).getType()) or \
              self.discardMask.member(self.LA(1).getType()):
            if self.hideMask.member(self.LA(1).getType()):
                p.setHiddenAfter(self.LA(1))
                if p != monitored:
                    self.LA(1).setHiddenBefore(p)
                p = self.lastHiddenToken = self.LA(1)
            self.consume()
        return monitored

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       StringBuffer                             ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class StringBuffer:
    def __init__(self,string=None):
        if string:
            self.text = list(string)
        else:
            self.text = []

    def setLength(self,sz):
        if not sz :
            self.text = []
            return
        assert sz>0
        if sz >= self.length():
            return
        ### just reset to empty buffer
        self.text = self.text[0:sz]

    def length(self):
        return len(self.text)

    def append(self,c):
        self.text.append(c)

    ### return buffer as string. Arg 'a' is  used  as index
    ## into the buffer and 2nd argument shall be the length.
    ## If 2nd args is absent, we return chars till end of
    ## buffer starting with 'a'.
    def getString(self,a=None,length=None):
        if not a :
            a = 0
        assert a>=0
        if a>= len(self.text) :
            return ""

        if not length:
            ## no second argument
            L = self.text[a:]
        else:
            assert (a+length) <= len(self.text)
            b = a + length
            L = self.text[a:b]
        s = ""
        for x in L : s += x
        return s

    toString = getString ## alias

    def __str__(self):
        return str(self.text)

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       Reader                                   ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

## When reading Japanese chars, it happens that a stream returns a
## 'char' of length 2. This looks like  a  bug  in the appropriate
## codecs - but I'm  rather  unsure about this. Anyway, if this is
## the case, I'm going to  split  this string into a list of chars
## and put them  on  hold, ie. on a  buffer. Next time when called
## we read from buffer until buffer is empty.
## wh: nov, 25th -> problem does not appear in Python 2.4.0.c1.

class Reader(object):
    def __init__(self,stream):
        self.cin = stream
        self.buf = []

    def read(self,num):
        assert num==1

        if len(self.buf):
            return self.buf.pop()

        ## Read a char - this may return a string.
        ## Is this a bug in codecs/Python?
        c = self.cin.read(1)

        if not c or len(c)==1:
            return c

        L = list(c)
        L.reverse()
        for x in L:
            self.buf.append(x)

        ## read one char ..
        return self.read(1)

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       CharScanner                              ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class CharScanner(TokenStream):
    ## class members
    NO_CHAR = 0
    EOF_CHAR = ''  ### EOF shall be the empty string.

    def __init__(self, *argv, **kwargs):
        super(CharScanner, self).__init__()
        self.saveConsumedInput = True
        self.tokenClass = None
        self.caseSensitive = True
        self.caseSensitiveLiterals = True
        self.literals = None
        self.tabsize = 8
        self._returnToken = None
        self.commitToPath = False
        self.traceDepth = 0
        self.text = StringBuffer()
        self.hashString = hash(self)
        self.setTokenObjectClass(CommonToken)
        self.setInput(*argv)

    def __iter__(self):
        return CharScannerIterator(self)

    def setInput(self,*argv):
        ## case 1:
        ## if there's no arg we default to read from
        ## standard input
        if not argv:
            import sys
            self.setInput(sys.stdin)
            return

        ## get 1st argument
        arg1 = argv[0]

        ## case 2:
        ## if arg1 is a string,  we assume it's a file name
        ## and  open  a  stream  using 2nd argument as open
        ## mode. If there's no 2nd argument we fall back to
        ## mode '+rb'.
        if isinstance(arg1,str):
            f = open(arg1,"rb")
            self.setInput(f)
            self.setFilename(arg1)
            return

        ## case 3:
        ## if arg1 is a file we wrap it by a char buffer (
        ## some additional checks?? No, can't do this in
        ## general).
        if isinstance(arg1,file):
            self.setInput(CharBuffer(arg1))
            return

        ## case 4:
        ## if arg1 is of type SharedLexerInputState we use
        ## argument as is.
        if isinstance(arg1,LexerSharedInputState):
            self.inputState = arg1
            return

        ## case 5:
        ## check whether argument type is of type input
        ## buffer. If so create a SharedLexerInputState and
        ## go ahead.
        if isinstance(arg1,InputBuffer):
            self.setInput(LexerSharedInputState(arg1))
            return

        ## case 6:
        ## check whether argument type has a method read(int)
        ## If so create CharBuffer ...
        try:
            if arg1.read:
                rd = Reader(arg1)
                cb = CharBuffer(rd)
                ss = LexerSharedInputState(cb)
                self.inputState = ss
            return
        except:
            pass

        ## case 7:
        ## raise wrong argument exception
        raise TypeError(argv)

    def setTabSize(self,size) :
        self.tabsize = size

    def getTabSize(self) :
        return self.tabsize

    def setCaseSensitive(self,t) :
        self.caseSensitive = t

    def setCommitToPath(self,commit) :
        self.commitToPath = commit

    def setFilename(self,f) :
        self.inputState.filename = f

    def setLine(self,line) :
        self.inputState.line = line

    def setText(self,s) :
        self.resetText()
        self.text.append(s)

    def getCaseSensitive(self) :
        return self.caseSensitive

    def getCaseSensitiveLiterals(self) :
        return self.caseSensitiveLiterals

    def getColumn(self) :
        return self.inputState.column

    def setColumn(self,c) :
        self.inputState.column = c

    def getCommitToPath(self) :
        return self.commitToPath

    def getFilename(self) :
        return self.inputState.filename

    def getInputBuffer(self) :
        return self.inputState.input

    def getInputState(self) :
        return self.inputState

    def setInputState(self,state) :
        assert isinstance(state,LexerSharedInputState)
        self.inputState = state

    def getLine(self) :
        return self.inputState.line

    def getText(self) :
        return str(self.text)

    def getTokenObject(self) :
        return self._returnToken

    def LA(self,i) :
        c = self.inputState.input.LA(i)
        if not self.caseSensitive:
            ### E0006
            c = c.__class__.lower(c)
        return c

    def makeToken(self,type) :
        try:
            ## dynamically load a class
            assert self.tokenClass
            tok = self.tokenClass()
            tok.setType(type)
            tok.setColumn(self.inputState.tokenStartColumn)
            tok.setLine(self.inputState.tokenStartLine)
            return tok
        except:
            self.panic("unable to create new token")
        return Token.badToken

    def mark(self) :
        return self.inputState.input.mark()

    def _match_bitset(self,b) :
        if b.member(self.LA(1)):
            self.consume()
        else:
            raise MismatchedCharException(self.LA(1), b, False, self)

    def _match_string(self,s) :
        for c in s:
            if self.LA(1) == c:
                self.consume()
            else:
                raise MismatchedCharException(self.LA(1), c, False, self)

    def match(self,item):
        if isinstance(item,str) or isinstance(item,unicode):
            return self._match_string(item)
        else:
            return self._match_bitset(item)

    def matchNot(self,c) :
        if self.LA(1) != c:
            self.consume()
        else:
            raise MismatchedCharException(self.LA(1), c, True, self)

    def matchRange(self,c1,c2) :
        if self.LA(1) < c1 or self.LA(1) > c2 :
            raise MismatchedCharException(self.LA(1), c1, c2, False, self)
        else:
            self.consume()

    def newline(self) :
        self.inputState.line += 1
        self.inputState.column = 1

    def tab(self) :
        c = self.getColumn()
        nc = ( ((c-1)/self.tabsize) + 1) * self.tabsize + 1
        self.setColumn(nc)

    def panic(self,s='') :
        print "CharScanner: panic: " + s
        sys.exit(1)

    def reportError(self,ex) :
        print ex

    def reportError(self,s) :
        if not self.getFilename():
            print "error: " + str(s)
        else:
            print self.getFilename() + ": error: " + str(s)

    def reportWarning(self,s) :
        if not self.getFilename():
            print "warning: " + str(s)
        else:
            print self.getFilename() + ": warning: " + str(s)

    def resetText(self) :
        self.text.setLength(0)
        self.inputState.tokenStartColumn = self.inputState.column
        self.inputState.tokenStartLine = self.inputState.line

    def rewind(self,pos) :
        self.inputState.input.rewind(pos)

    def setTokenObjectClass(self,cl):
        self.tokenClass = cl

    def testForLiteral(self,token):
        if not token:
            return
        assert isinstance(token,Token)

        _type = token.getType()

        ## special tokens can't be literals
        if _type in [SKIP,INVALID_TYPE,EOF_TYPE,NULL_TREE_LOOKAHEAD] :
            return

        _text = token.getText()
        if not _text:
            return

        assert isinstance(_text,str) or isinstance(_text,unicode)
        _type = self.testLiteralsTable(_text,_type)
        token.setType(_type)
        return _type

    def testLiteralsTable(self,*args):
        if isinstance(args[0],str) or isinstance(args[0],unicode):
            s = args[0]
            i = args[1]
        else:
            s = self.text.getString()
            i = args[0]

        ## check whether integer has been given
        if not isinstance(i,int):
            assert isinstance(i,int)

        ## check whether we have a dict
        assert isinstance(self.literals,dict)
        try:
            ## E0010
            if not self.caseSensitiveLiterals:
                s = s.__class__.lower(s)
            i = self.literals[s]
        except:
            pass
        return i

    def toLower(self,c):
        return c.__class__.lower()

    def traceIndent(self):
        print ' ' * self.traceDepth

    def traceIn(self,rname):
        self.traceDepth += 1
        self.traceIndent()
        print "> lexer %s c== %s" % (rname,self.LA(1))

    def traceOut(self,rname):
        self.traceIndent()
        print "< lexer %s c== %s" % (rname,self.LA(1))
        self.traceDepth -= 1

    def uponEOF(self):
        pass

    def append(self,c):
        if self.saveConsumedInput :
            self.text.append(c)

    def commit(self):
        self.inputState.input.commit()

    def consume(self):
        if not self.inputState.guessing:
            c = self.LA(1)
            if self.caseSensitive:
                self.append(c)
            else:
                # use input.LA(), not LA(), to get original case
                # CharScanner.LA() would toLower it.
                c =  self.inputState.input.LA(1)
                self.append(c)

            if c and c in "\t":
                self.tab()
            else:
                self.inputState.column += 1
        self.inputState.input.consume()

    ## Consume chars until one matches the given char
    def consumeUntil_char(self,c):
        while self.LA(1) != EOF_CHAR and self.LA(1) != c:
            self.consume()

    ## Consume chars until one matches the given set
    def consumeUntil_bitset(self,bitset):
        while self.LA(1) != EOF_CHAR and not self.set.member(self.LA(1)):
            self.consume()

    ### If symbol seen is EOF then generate and set token, otherwise
    ### throw exception.
    def default(self,la1):
        if not la1 :
            self.uponEOF()
            self._returnToken = self.makeToken(EOF_TYPE)
        else:
            self.raise_NoViableAlt(la1)

    def filterdefault(self,la1,*args):
        if not la1:
            self.uponEOF()
            self._returnToken = self.makeToken(EOF_TYPE)
            return

        if not args:
            self.consume()
            raise TryAgain()
        else:
            ### apply filter object
            self.commit();
            try:
                func=args[0]
                args=args[1:]
                apply(func,args)
            except RecognitionException, e:
                ## catastrophic failure
                self.reportError(e);
                self.consume();
            raise TryAgain()

    def raise_NoViableAlt(self,la1=None):
        if not la1: la1 = self.LA(1)
        fname = self.getFilename()
        line  = self.getLine()
        col   = self.getColumn()
        raise NoViableAltForCharException(la1,fname,line,col)

    def set_return_token(self,_create,_token,_ttype,_offset):
        if _create and not _token and (not _ttype == SKIP):
            string = self.text.getString(_offset)
            _token = self.makeToken(_ttype)
            _token.setText(string)
        self._returnToken = _token
        return _token

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                   CharScannerIterator                          ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class CharScannerIterator:

    def __init__(self,inst):
        if isinstance(inst,CharScanner):
            self.inst = inst
            return
        raise TypeError("CharScannerIterator requires CharScanner object")

    def next(self):
        assert self.inst
        item = self.inst.nextToken()
        if not item or item.isEOF():
            raise StopIteration()
        return item

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       BitSet                                   ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

### I'm assuming here that a long is 64bits. It appears however, that
### a long is of any size. That means we can use a single long as the
### bitset (!), ie. Python would do almost all the work (TBD).

class BitSet(object):
    BITS     = 64
    NIBBLE   = 4
    LOG_BITS = 6
    MOD_MASK = BITS -1

    def __init__(self,data=None):
        if not data:
            BitSet.__init__(self,[long(0)])
            return
        if isinstance(data,int):
            BitSet.__init__(self,[long(data)])
            return
        if isinstance(data,long):
            BitSet.__init__(self,[data])
            return
        if not isinstance(data,list):
            raise TypeError("BitSet requires integer, long, or " +
                            "list argument")
        for x in data:
            if not isinstance(x,long):
                raise TypeError(self,"List argument item is " +
                                "not a long: %s" % (x))
        self.data = data

    def __str__(self):
        bits = len(self.data) * BitSet.BITS
        s = ""
        for i in xrange(0,bits):
            if self.at(i):
                s += "1"
            else:
                s += "o"
            if not ((i+1) % 10):
                s += '|%s|' % (i+1)
        return s

    def __repr__(self):
        return str(self)

    def member(self,item):
        if not item:
            return False

        if isinstance(item,int):
            return self.at(item)

        if not (isinstance(item,str) or isinstance(item,unicode)):
            raise TypeError(self,"char or unichar expected: %s" % (item))

        ## char is a (unicode) string with at most lenght 1, ie.
        ## a char.

        if len(item) != 1:
            raise TypeError(self,"char expected: %s" % (item))

        ### handle ASCII/UNICODE char
        num = ord(item)

        ### check whether position num is in bitset
        return self.at(num)

    def wordNumber(self,bit):
        return bit >> BitSet.LOG_BITS

    def bitMask(self,bit):
        pos = bit & BitSet.MOD_MASK  ## bit mod BITS
        return (1L << pos)

    def set(self,bit,on=True):
        # grow bitset as required (use with care!)
        i = self.wordNumber(bit)
        mask = self.bitMask(bit)
        if i>=len(self.data):
            d = i - len(self.data) + 1
            for x in xrange(0,d):
                self.data.append(0L)
            assert len(self.data) == i+1
        if on:
            self.data[i] |=  mask
        else:
            self.data[i] &= (~mask)

    ### make add an alias for set
    add = set

    def off(self,bit,off=True):
        self.set(bit,not off)

    def at(self,bit):
        i = self.wordNumber(bit)
        v = self.data[i]
        m = self.bitMask(bit)
        return v & m


###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                      some further funcs                        ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

def illegalarg_ex(func):
    raise ValueError(
       "%s is only valid if parser is built for debugging" %
       (func.func_name))

def runtime_ex(func):
    raise RuntimeException(
       "%s is only valid if parser is built for debugging" %
       (func.func_name))

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       TokenBuffer                              ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TokenBuffer(object):
    def __init__(self,stream):
        self.input = stream
        self.nMarkers = 0
        self.markerOffset = 0
        self.numToConsume = 0
        self.queue = Queue()

    def reset(self) :
        self.nMarkers = 0
        self.markerOffset = 0
        self.numToConsume = 0
        self.queue.reset()

    def consume(self) :
        self.numToConsume += 1

    def fill(self, amount):
        self.syncConsume()
        while self.queue.length() < (amount + self.markerOffset):
            self.queue.append(self.input.nextToken())

    def getInput(self):
        return self.input

    def LA(self,k) :
        self.fill(k)
        return self.queue.elementAt(self.markerOffset + k - 1).type

    def LT(self,k) :
        self.fill(k)
        return self.queue.elementAt(self.markerOffset + k - 1)

    def mark(self) :
        self.syncConsume()
        self.nMarkers += 1
        return self.markerOffset

    def rewind(self,mark) :
        self.syncConsume()
        self.markerOffset = mark
        self.nMarkers -= 1

    def syncConsume(self) :
        while self.numToConsume > 0:
            if self.nMarkers > 0:
                # guess mode -- leave leading characters and bump offset.
                self.markerOffset += 1
            else:
                # normal mode -- remove first character
                self.queue.removeFirst()
            self.numToConsume -= 1

    def __str__(self):
        return "(%s,%s,%s,%s,%s)" % (
           self.input,
           self.nMarkers,
           self.markerOffset,
           self.numToConsume,
           self.queue)

    def __repr__(self):
        return str(self)

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       ParserSharedInputState                   ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class ParserSharedInputState(object):

    def __init__(self):
        self.input = None
        self.reset()

    def reset(self):
        self.guessing = 0
        self.filename = None
        if self.input:
            self.input.reset()

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       Parser                                   ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class Parser(object):

    def __init__(self, *args, **kwargs):
        self.tokenNames = None
        self.returnAST  = None
        self.astFactory = None
        self.tokenTypeToASTClassMap = {}
        self.ignoreInvalidDebugCalls = False
        self.traceDepth = 0
        if not args:
            self.inputState = ParserSharedInputState()
            return
        arg0 = args[0]
        assert isinstance(arg0,ParserSharedInputState)
        self.inputState = arg0
        return

    def getTokenTypeToASTClassMap(self):
        return self.tokenTypeToASTClassMap


    def addMessageListener(self, l):
        if not self.ignoreInvalidDebugCalls:
            illegalarg_ex(addMessageListener)

    def addParserListener(self,l) :
        if (not self.ignoreInvalidDebugCalls) :
            illegalarg_ex(addParserListener)

    def addParserMatchListener(self, l) :
        if (not self.ignoreInvalidDebugCalls) :
            illegalarg_ex(addParserMatchListener)

    def addParserTokenListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            illegalarg_ex(addParserTokenListener)

    def addSemanticPredicateListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            illegalarg_ex(addSemanticPredicateListener)

    def addSyntacticPredicateListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            illegalarg_ex(addSyntacticPredicateListener)

    def addTraceListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            illegalarg_ex(addTraceListener)

    def consume(self):
        raise NotImplementedError()

    def _consumeUntil_type(self,tokenType):
        while self.LA(1) != EOF_TYPE and self.LA(1) != tokenType:
            self.consume()

    def _consumeUntil_bitset(self, set):
        while self.LA(1) != EOF_TYPE and not set.member(self.LA(1)):
            self.consume()

    def consumeUntil(self,arg):
        if isinstance(arg,int):
            self._consumeUntil_type(arg)
        else:
            self._consumeUntil_bitset(arg)

    def defaultDebuggingSetup(self):
        pass

    def getAST(self) :
        return self.returnAST

    def getASTFactory(self) :
        return self.astFactory

    def getFilename(self) :
        return self.inputState.filename

    def getInputState(self) :
        return self.inputState

    def setInputState(self, state) :
        self.inputState = state

    def getTokenName(self,num) :
        return self.tokenNames[num]

    def getTokenNames(self) :
        return self.tokenNames

    def isDebugMode(self) :
        return self.false

    def LA(self, i):
        raise NotImplementedError()

    def LT(self, i):
        raise NotImplementedError()

    def mark(self):
        return self.inputState.input.mark()

    def _match_int(self,t):
        if (self.LA(1) != t):
            raise MismatchedTokenException(
               self.tokenNames, self.LT(1), t, False, self.getFilename())
        else:
            self.consume()

    def _match_set(self, b):
        if (not b.member(self.LA(1))):
            raise MismatchedTokenException(
               self.tokenNames,self.LT(1), b, False, self.getFilename())
        else:
            self.consume()

    def match(self,set) :
        if isinstance(set,int):
            self._match_int(set)
            return
        if isinstance(set,BitSet):
            self._match_set(set)
            return
        raise TypeError("Parser.match requires integer ot BitSet argument")

    def matchNot(self,t):
        if self.LA(1) == t:
            raise MismatchedTokenException(
               tokenNames, self.LT(1), t, True, self.getFilename())
        else:
            self.consume()

    def removeMessageListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeMessageListener)

    def removeParserListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeParserListener)

    def removeParserMatchListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeParserMatchListener)

    def removeParserTokenListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeParserTokenListener)

    def removeSemanticPredicateListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeSemanticPredicateListener)

    def removeSyntacticPredicateListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeSyntacticPredicateListener)

    def removeTraceListener(self, l) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(removeTraceListener)

    def reportError(self,x) :
        fmt = "syntax error:"
        f = self.getFilename()
        if f:
            fmt = ("%s:" % f) + fmt
        if isinstance(x,Token):
            line = x.getColumn()
            col  = x.getLine()
            text = x.getText()
            fmt  = fmt + 'unexpected symbol at line %s (column %s) : "%s"'
            print >>sys.stderr, fmt % (line,col,text)
        else:
            print >>sys.stderr, fmt,str(x)

    def reportWarning(self,s):
        f = self.getFilename()
        if f:
            print "%s:warning: %s" % (f,str(x))
        else:
            print "warning: %s" % (str(x))

    def rewind(self, pos) :
        self.inputState.input.rewind(pos)

    def setASTFactory(self, f) :
        self.astFactory = f

    def setASTNodeClass(self, cl) :
        self.astFactory.setASTNodeType(cl)

    def setASTNodeType(self, nodeType) :
        self.setASTNodeClass(nodeType)

    def setDebugMode(self, debugMode) :
        if (not self.ignoreInvalidDebugCalls):
            runtime_ex(setDebugMode)

    def setFilename(self, f) :
        self.inputState.filename = f

    def setIgnoreInvalidDebugCalls(self, value) :
        self.ignoreInvalidDebugCalls = value

    def setTokenBuffer(self, t) :
        self.inputState.input = t

    def traceIndent(self):
        print " " * self.traceDepth

    def traceIn(self,rname):
        self.traceDepth += 1
        self.trace("> ", rname)

    def traceOut(self,rname):
        self.trace("< ", rname)
        self.traceDepth -= 1

    ### wh: moved from ASTFactory to Parser
    def addASTChild(self,currentAST, child):
        if not child:
            return
        if not currentAST.root:
            currentAST.root = child
        elif not currentAST.child:
            currentAST.root.setFirstChild(child)
        else:
            currentAST.child.setNextSibling(child)
        currentAST.child = child
        currentAST.advanceChildToEnd()

    ### wh: moved from ASTFactory to Parser
    def makeASTRoot(self,currentAST,root) :
        if root:
            ### Add the current root as a child of new root
            root.addChild(currentAST.root)
            ### The new current child is the last sibling of the old root
            currentAST.child = currentAST.root
            currentAST.advanceChildToEnd()
            ### Set the new root
            currentAST.root = root

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       LLkParser                                ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class LLkParser(Parser):

    def __init__(self, *args, **kwargs):
        try:
            arg1 = args[0]
        except:
            arg1 = 1

        if isinstance(arg1,int):
            super(LLkParser,self).__init__()
            self.k = arg1
            return

        if isinstance(arg1,ParserSharedInputState):
            super(LLkParser,self).__init__(arg1)
            self.set_k(1,*args)
            return

        if isinstance(arg1,TokenBuffer):
            super(LLkParser,self).__init__()
            self.setTokenBuffer(arg1)
            self.set_k(1,*args)
            return

        if isinstance(arg1,TokenStream):
            super(LLkParser,self).__init__()
            tokenBuf = TokenBuffer(arg1)
            self.setTokenBuffer(tokenBuf)
            self.set_k(1,*args)
            return

        ### unknown argument
        raise TypeError("LLkParser requires integer, " +
                        "ParserSharedInputStream or TokenStream argument")

    def consume(self):
        self.inputState.input.consume()

    def LA(self,i):
        return self.inputState.input.LA(i)

    def LT(self,i):
        return self.inputState.input.LT(i)

    def set_k(self,index,*args):
        try:
            self.k = args[index]
        except:
            self.k = 1

    def trace(self,ee,rname):
        print type(self)
        self.traceIndent()
        guess = ""
        if self.inputState.guessing > 0:
            guess = " [guessing]"
        print(ee + rname + guess)
        for i in xrange(1,self.k+1):
            if i != 1:
                print(", ")
            if self.LT(i) :
                v = self.LT(i).getText()
            else:
                v = "null"
            print "LA(%s) == %s" % (i,v)
        print("\n")

    def traceIn(self,rname):
        self.traceDepth += 1;
        self.trace("> ", rname);

    def traceOut(self,rname):
        self.trace("< ", rname);
        self.traceDepth -= 1;

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                    TreeParserSharedInputState                  ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TreeParserSharedInputState(object):
    def __init__(self):
        self.guessing = 0

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       TreeParser                               ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class TreeParser(object):

    def __init__(self, *args, **kwargs):
        self.inputState = TreeParserSharedInputState()
        self._retTree   = None
        self.tokenNames = []
        self.returnAST  = None
        self.astFactory = ASTFactory()
        self.traceDepth = 0

    def getAST(self):
        return self.returnAST

    def getASTFactory(self):
        return self.astFactory

    def getTokenName(self,num) :
        return self.tokenNames[num]

    def getTokenNames(self):
        return self.tokenNames

    def match(self,t,set) :
        assert isinstance(set,int) or isinstance(set,BitSet)
        if not t or t == ASTNULL:
            raise MismatchedTokenException(self.getTokenNames(), t,set, False)

        if isinstance(set,int) and t.getType() != set:
            raise MismatchedTokenException(self.getTokenNames(), t,set, False)

        if isinstance(set,BitSet) and not set.member(t.getType):
            raise MismatchedTokenException(self.getTokenNames(), t,set, False)

    def matchNot(self,t, ttype) :
        if not t or (t == ASTNULL) or (t.getType() == ttype):
            raise MismatchedTokenException(getTokenNames(), t, ttype, True)

    def reportError(self,ex):
        print >>sys.stderr,"error:",ex

    def  reportWarning(self, s):
        print "warning:",s

    def setASTFactory(self,f):
        self.astFactory = f

    def setASTNodeType(self,nodeType):
        self.setASTNodeClass(nodeType)

    def setASTNodeClass(self,nodeType):
        self.astFactory.setASTNodeType(nodeType)

    def traceIndent(self):
        print " " * self.traceDepth

    def traceIn(self,rname,t):
        self.traceDepth += 1
        self.traceIndent()
        print("> " + rname + "(" +
              ifelse(t,str(t),"null") + ")" +
              ifelse(self.inputState.guessing>0,"[guessing]",""))

    def traceOut(self,rname,t):
        self.traceIndent()
        print("< " + rname + "(" +
              ifelse(t,str(t),"null") + ")" +
              ifelse(self.inputState.guessing>0,"[guessing]",""))
        self.traceDepth -= 1

    ### wh: moved from ASTFactory to TreeParser
    def addASTChild(self,currentAST, child):
        if not child:
            return
        if not currentAST.root:
            currentAST.root = child
        elif not currentAST.child:
            currentAST.root.setFirstChild(child)
        else:
            currentAST.child.setNextSibling(child)
        currentAST.child = child
        currentAST.advanceChildToEnd()

    ### wh: moved from ASTFactory to TreeParser
    def makeASTRoot(self,currentAST,root):
        if root:
            ### Add the current root as a child of new root
            root.addChild(currentAST.root)
            ### The new current child is the last sibling of the old root
            currentAST.child = currentAST.root
            currentAST.advanceChildToEnd()
            ### Set the new root
            currentAST.root = root

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###               funcs to work on trees                           ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

def rightmost(ast):
    if ast:
        while(ast.right):
            ast = ast.right
    return ast

def cmptree(s,t,partial):
    while(s and t):
        ### as a quick optimization, check roots first.
        if not s.equals(t):
            return False

        ### if roots match, do full list match test on children.
        if not cmptree(s.getFirstChild(),t.getFirstChild(),partial):
            return False

        s = s.getNextSibling()
        t = t.getNextSibling()

    r = ifelse(partial,not t,not s and not t)
    return r

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                          AST                                   ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class AST(object):
    def __init__(self):
        pass

    def addChild(self, c):
        pass

    def equals(self, t):
        return False

    def equalsList(self, t):
        return False

    def equalsListPartial(self, t):
        return False

    def equalsTree(self, t):
        return False

    def equalsTreePartial(self, t):
        return False

    def findAll(self, tree):
        return None

    def findAllPartial(self, subtree):
        return None

    def getFirstChild(self):
        return self

    def getNextSibling(self):
        return self

    def getText(self):
        return ""

    def getType(self):
        return INVALID_TYPE

    def getLine(self):
        return 0

    def getColumn(self):
        return 0

    def getNumberOfChildren(self):
        return 0

    def initialize(self, t, txt):
        pass

    def initialize(self, t):
        pass

    def setFirstChild(self, c):
        pass

    def setNextSibling(self, n):
        pass

    def setText(self, text):
        pass

    def setType(self, ttype):
        pass

    def toString(self):
        self.getText()

    __str__ = toString

    def toStringList(self):
        return self.getText()

    def toStringTree(self):
        return self.getText()

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       ASTNULLType                              ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

### There is only one instance of this class **/
class ASTNULLType(AST):
    def __init__(self):
        AST.__init__(self)
        pass

    def getText(self):
        return "<ASTNULL>"

    def getType(self):
        return NULL_TREE_LOOKAHEAD


###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       BaseAST                                  ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class BaseAST(AST):

    verboseStringConversion = False
    tokenNames = None

    def __init__(self):
        self.down  = None ## kid
        self.right = None ## sibling

    def addChild(self,node):
        if node:
            t = rightmost(self.down)
            if t:
                t.right = node
            else:
                assert not self.down
                self.down = node

    def getNumberOfChildren(self):
        t = self.down
        n = 0
        while t:
            n += 1
            t = t.right
        return n

    def doWorkForFindAll(self,v,target,partialMatch):
        sibling = self

        while sibling:
            c1 = partialMatch and sibling.equalsTreePartial(target)
            if c1:
                v.append(sibling)
            else:
                c2 = not partialMatch and sibling.equalsTree(target)
                if c2:
                    v.append(sibling)

            ### regardless of match or not, check any children for matches
            if sibling.getFirstChild():
                sibling.getFirstChild().doWorkForFindAll(v,target,partialMatch)

            sibling = sibling.getNextSibling()

    ### Is node t equal to 'self' in terms of token type and text?
    def equals(self,t):
        if not t:
            return False
        return self.getText() == t.getText() and self.getType() == t.getType()

    ### Is t an exact structural and equals() match of this tree.  The
    ### 'self' reference is considered the start of a sibling list.
    ###
    def equalsList(self, t):
        return cmptree(self, t, partial=False)

    ### Is 't' a subtree of this list?
    ### The siblings of the root are NOT ignored.
    ###
    def equalsListPartial(self,t):
        return cmptree(self,t,partial=True)

    ### Is tree rooted at 'self' equal to 't'?  The siblings
    ### of 'self' are ignored.
    ###
    def equalsTree(self, t):
        return self.equals(t) and \
               cmptree(self.getFirstChild(), t.getFirstChild(), partial=False)

    ### Is 't' a subtree of the tree rooted at 'self'?  The siblings
    ### of 'self' are ignored.
    ###
    def equalsTreePartial(self, t):
        if not t:
            return True
        return self.equals(t) and cmptree(
           self.getFirstChild(), t.getFirstChild(), partial=True)

    ### Walk the tree looking for all exact subtree matches.  Return
    ### an ASTEnumerator that lets the caller walk the list
    ### of subtree roots found herein.
    def findAll(self,target):
        roots = []

        ### the empty tree cannot result in an enumeration
        if not target:
            return None
        # find all matches recursively
        self.doWorkForFindAll(roots, target, False)
        return roots

    ### Walk the tree looking for all subtrees.  Return
    ###  an ASTEnumerator that lets the caller walk the list
    ###  of subtree roots found herein.
    def findAllPartial(self,sub):
        roots = []

        ### the empty tree cannot result in an enumeration
        if not sub:
            return None

        self.doWorkForFindAll(roots, sub, True)  ### find all matches recursively
        return roots

    ### Get the first child of this node None if not children
    def getFirstChild(self):
        return self.down

    ### Get the next sibling in line after this one
    def getNextSibling(self):
        return self.right

    ### Get the token text for this node
    def getText(self):
        return ""

    ### Get the token type for this node
    def getType(self):
        return 0

    def getLine(self):
        return 0

    def getColumn(self):
        return 0

    ### Remove all children */
    def removeChildren(self):
        self.down = None

    def setFirstChild(self,c):
        self.down = c

    def setNextSibling(self, n):
        self.right = n

    ### Set the token text for this node
    def setText(self, text):
        pass

    ### Set the token type for this node
    def setType(self, ttype):
        pass

    ### static
    def setVerboseStringConversion(verbose,names):
        verboseStringConversion = verbose
        tokenNames = names
    setVerboseStringConversion = staticmethod(setVerboseStringConversion)

    ### Return an array of strings that maps token ID to it's text.
    ##  @since 2.7.3
    def getTokenNames():
        return tokenNames

    def toString(self):
        return self.getText()

    ### return tree as lisp string - sibling included
    def toStringList(self):
        ts = self.toStringTree()
        sib = self.getNextSibling()
        if sib:
            ts += sib.toStringList()
        return ts

    __str__ = toStringList

    ### return tree as string - siblings ignored
    def toStringTree(self):
        ts = ""
        kid = self.getFirstChild()
        if kid:
            ts += " ("
        ts += " " + self.toString()
        if kid:
            ts += kid.toStringList()
            ts += " )"
        return ts

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       CommonAST                                ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

### Common AST node implementation
class CommonAST(BaseAST):
    def __init__(self,token=None):
        super(CommonAST,self).__init__()
        self.ttype = INVALID_TYPE
        self.text  = "<no text>"
        self.initialize(token)
        #assert self.text

    ### Get the token text for this node
    def getText(self):
        return self.text

    ### Get the token type for this node
    def getType(self):
        return self.ttype

    def initialize(self,*args):
        if not args:
            return

        arg0 = args[0]

        if isinstance(arg0,int):
            arg1 = args[1]
            self.setType(arg0)
            self.setText(arg1)
            return

        if isinstance(arg0,AST) or isinstance(arg0,Token):
            self.setText(arg0.getText())
            self.setType(arg0.getType())
            return

    ### Set the token text for this node
    def setText(self,text_):
        assert isinstance(text_,str)
        self.text = text_

    ### Set the token type for this node
    def setType(self,ttype_):
        assert isinstance(ttype_,int)
        self.ttype = ttype_

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                     CommonASTWithHiddenTokens                  ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class CommonASTWithHiddenTokens(CommonAST):

    def __init__(self,*args):
        CommonAST.__init__(self,*args)
        self.hiddenBefore = None
        self.hiddenAfter  = None

    def getHiddenAfter(self):
        return self.hiddenAfter

    def getHiddenBefore(self):
        return self.hiddenBefore

    def initialize(self,*args):
        CommonAST.initialize(self,*args)
        if args and isinstance(args[0],Token):
            assert isinstance(args[0],CommonHiddenStreamToken)
            self.hideenBefore = args[0].getHiddenBefore()
            self.hiddenAfter  = args[0].getHiddenAfter()

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       ASTPair                                  ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class ASTPair(object):
    def __init__(self):
        self.root = None          ### current root of tree
        self.child = None         ### current child to which siblings are added

    ### Make sure that child is the last sibling */
    def advanceChildToEnd(self):
        if self.child:
            while self.child.getNextSibling():
                self.child = self.child.getNextSibling()

    ### Copy an ASTPair.  Don't call it clone() because we want type-safety */
    def copy(self):
        tmp = ASTPair()
        tmp.root = self.root
        tmp.child = self.child
        return tmp

    def toString(self):
        r = ifelse(not root,"null",self.root.getText())
        c = ifelse(not child,"null",self.child.getText())
        return "[%s,%s]" % (r,c)

    __str__ = toString
    __repr__ = toString


###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       ASTFactory                               ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class ASTFactory(object):
    def __init__(self,table=None):
        self._class = None
        self._classmap = ifelse(table,table,None)

    def create(self,*args):
        if not args:
            return self.create(INVALID_TYPE)

        arg0 = args[0]
        arg1 = None
        arg2 = None

        try:
            arg1 = args[1]
            arg2 = args[2]
        except:
            pass

        # ctor(int)
        if isinstance(arg0,int) and not arg2:
            ### get class for 'self' type
            c = self.getASTNodeType(arg0)
            t = self.create(c)
            if t:
                t.initialize(arg0, ifelse(arg1,arg1,""))
            return t

        # ctor(int,something)
        if isinstance(arg0,int) and arg2:
            t = self.create(arg2)
            if t:
                t.initialize(arg0,arg1)
            return t

        # ctor(AST)
        if isinstance(arg0,AST):
            t = self.create(arg0.getType())
            if t:
                t.initialize(arg0)
            return t

        # ctor(token)
        if isinstance(arg0,Token) and not arg1:
            ttype = arg0.getType()
            assert isinstance(ttype,int)
            t = self.create(ttype)
            if t:
                t.initialize(arg0)
            return t

        # ctor(token,class)
        if isinstance(arg0,Token) and arg1:
            assert isinstance(arg1,type)
            assert issubclass(arg1,AST)
            # this creates instance of 'arg1' using 'arg0' as
            # argument. Wow, that's magic!
            t = arg1(arg0)
            assert t and isinstance(t,AST)
            return t

        # ctor(class)
        if isinstance(arg0,type):
            ### next statement creates instance of type (!)
            t = arg0()
            assert isinstance(t,AST)
            return t


    def setASTNodeClass(self,className=None):
        if not className:
            return
        assert isinstance(className,type)
        assert issubclass(className,AST)
        self._class = className

    ### kind of misnomer - use setASTNodeClass instead.
    setASTNodeType = setASTNodeClass

    def getASTNodeClass(self):
        return self._class



    def getTokenTypeToASTClassMap(self):
        return self._classmap

    def setTokenTypeToASTClassMap(self,amap):
        self._classmap = amap

    def error(self, e):
        import sys
        print >> sys.stderr, e

    def setTokenTypeASTNodeType(self, tokenType, className):
        """
        Specify a mapping between a token type and a (AST) class.
        """
        if not self._classmap:
            self._classmap = {}

        if not className:
            try:
                del self._classmap[tokenType]
            except:
                pass
        else:
            ### here we should also perform actions to ensure that
            ### a. class can be loaded
            ### b. class is a subclass of AST
            ###
            assert isinstance(className,type)
            assert issubclass(className,AST)  ## a & b
            ### enter the class
            self._classmap[tokenType] = className

    def getASTNodeType(self,tokenType):
        """
        For a given token type return the AST node type. First we
        lookup a mapping table, second we try _class
        and finally we resolve to "antlr.CommonAST".
        """

        # first
        if self._classmap:
            try:
                c = self._classmap[tokenType]
                if c:
                    return c
            except:
                pass
        # second
        if self._class:
            return self._class

        # default
        return CommonAST

    ### methods that have been moved to file scope - just listed
    ### here to be somewhat consistent with original API
    def dup(self,t):
        return antlr.dup(t,self)

    def dupList(self,t):
        return antlr.dupList(t,self)

    def dupTree(self,t):
        return antlr.dupTree(t,self)

    ### methods moved to other classes
    ### 1. makeASTRoot  -> Parser
    ### 2. addASTChild  -> Parser

    ### non-standard: create alias for longish method name
    maptype = setTokenTypeASTNodeType

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###                       ASTVisitor                               ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

class ASTVisitor(object):
    def __init__(self,*args):
        pass

    def visit(self,ast):
        pass

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###
###               static methods and variables                     ###
###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx###

ASTNULL = ASTNULLType()

### wh: moved from ASTFactory as there's nothing ASTFactory-specific
### in this method.
def make(*nodes):
    if not nodes:
        return None

    for i in xrange(0,len(nodes)):
        node = nodes[i]
        if node:
            assert isinstance(node,AST)

    root = nodes[0]
    tail = None
    if root:
        root.setFirstChild(None)

    for i in xrange(1,len(nodes)):
        if not nodes[i]:
            continue
        if not root:
            root = tail = nodes[i]
        elif not tail:
            root.setFirstChild(nodes[i])
            tail = root.getFirstChild()
        else:
            tail.setNextSibling(nodes[i])
            tail = tail.getNextSibling()

        ### Chase tail to last sibling
        while tail.getNextSibling():
            tail = tail.getNextSibling()
    return root

def dup(t,factory):
    if not t:
        return None

    if factory:
        dup_t = factory.create(t.__class__)
    else:
        raise TypeError("dup function requires ASTFactory argument")
    dup_t.initialize(t)
    return dup_t

def dupList(t,factory):
    result = dupTree(t,factory)
    nt = result
    while t:
        ## for each sibling of the root
        t = t.getNextSibling()
        nt.setNextSibling(dupTree(t,factory))
        nt = nt.getNextSibling()
    return result

def dupTree(t,factory):
    result = dup(t,factory)
    if t:
        result.setFirstChild(dupList(t.getFirstChild(),factory))
    return result

###xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
### $Id: antlr.py,v 1.2 2005/10/26 07:44:24 rvk Exp $

# Local Variables:    ***
# mode: python        ***
# py-indent-offset: 4 ***
# End:                ***

########NEW FILE########
__FILENAME__ = BIFFRecords
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
#
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
#
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
#
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
#
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
#
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.

__rev_id__ = """$Id: BIFFRecords.py,v 1.7 2005/10/26 07:44:24 rvk Exp $"""


from struct import pack
from UnicodeUtils import *
import sys

class SharedStringTable(object):
    _SST_ID = 0x00FC
    _CONTINUE_ID = 0x003C

    def __init__(self):
        self._sst_record = ''
        self._continues = []
        self._current_piece = pack('<II', 0, 0)
        self._pos = len(self._current_piece)

        self._str_indexes = {}
        self._add_calls = 0

    def add_str(self, s):
        self._add_calls += 1
        if s not in self._str_indexes:
            self._str_indexes[s] = len(self._str_indexes)
            self._add_to_sst(s)
        return self._str_indexes[s]

    def str_index(self, s):
        return self._str_indexes[s]

    def get_biff_record(self):
        self._new_piece()
        result = pack('<2HII', self._SST_ID, len(self._sst_record), self._add_calls, len(self._str_indexes))
        result += self._sst_record[8:]
        result += ''.join(self._continues)
        return result

    def _add_to_sst(self, s):
        u_str = upack2(s)
        if len(u_str) > 0xFFFF:
            raise Exception('error: very long string.')

        is_unicode_str = u_str[2] == '\x01'
        if is_unicode_str:
            atom_len = 5 # 2 byte -- len,
                         # 1 byte -- options,
                         # 2 byte -- 1st sym
        else:
            atom_len = 4 # 2 byte -- len,
                         # 1 byte -- options,
                         # 1 byte -- 1st sym

        self._save_atom(u_str[0:atom_len])
        self._save_splitted(u_str[atom_len:], is_unicode_str)

    def _new_piece(self):
        if self._sst_record == '':
            self._sst_record = self._current_piece
        else:
            curr_piece_len = len(self._current_piece)
            self._continues.append(pack('<2H%ds'%curr_piece_len, self._CONTINUE_ID, curr_piece_len, self._current_piece))
        self._current_piece = ''
        self._pos = len(self._current_piece)

    def _save_atom(self, s):
        atom_len = len(s)
        free_space = 0x2020 - len(self._current_piece)
        if free_space < atom_len:
            self._new_piece()
        self._current_piece += s

    def _save_splitted(self, s, is_unicode_str):
        i = 0
        str_len = len(s)
        while i < str_len:
            piece_len = len(self._current_piece)
            free_space = 0x2020 - piece_len
            tail_len = str_len - i
            need_more_space = free_space < tail_len

            if not need_more_space:
                atom_len = tail_len
            else:
                if is_unicode_str:
                    atom_len = free_space & 0xFFFE
                else:
                    atom_len = free_space

            self._current_piece += s[i:i+atom_len]

            if need_more_space:
                self._new_piece()
                if is_unicode_str:
                    self._current_piece += '\x01'
                else:
                    self._current_piece += '\x00'

            i += atom_len


class BiffRecord(object):
    def __init__(self):
        self._rec_data = ''

    def get_rec_id(self):
        return _REC_ID

    def get_rec_header(self):
        return pack('<2H', self._REC_ID, len(self._rec_data))

    def get_rec_data(self):
        return self._rec_data

    def get(self):
        data = self.get_rec_data()

        if len(data) > 0x2020: # limit for BIFF7/8
            chunks = []
            pos = 0
            while pos < len(data):
                chunk_pos = pos + 0x2020
                chunk = data[pos:chunk_pos]
                chunks.append(chunk)
                pos = chunk_pos
            continues = pack('<2H', self._REC_ID, len(chunks[0])) + chunks[0]
            for chunk in chunks[1:]:
                continues += pack('<2H%ds'%len(chunk), 0x003C, len(chunk), chunk)
                # 0x003C -- CONTINUE record id
            return continues
        else:
            return self.get_rec_header() + data


class Biff8BOFRecord(BiffRecord):
    """
    Offset Size Contents
    0      2    Version, contains 0600H for BIFF8 and BIFF8X
    2      2    Type of the following data:
                  0005H = Workbook globals
                  0006H = Visual Basic module
                  0010H = Worksheet
                  0020H = Chart
                  0040H = Macro sheet
                  0100H = Workspace file
    4      2    Build identifier
    6      2    Build year
    8      4    File history flags
    12     4    Lowest Excel version that can read all records in this file
    """
    _REC_ID      = 0x0809
    # stream types
    BOOK_GLOBAL = 0x0005
    VB_MODULE   = 0x0006
    WORKSHEET   = 0x0010
    CHART       = 0x0020
    MACROSHEET  = 0x0040
    WORKSPACE   = 0x0100

    def __init__(self, rec_type):
        BiffRecord.__init__(self)

        version  = 0x0600
        build    = 0x0DBB
        year     = 0x07CC
        file_hist_flags = 0x00L
        ver_can_read    = 0x06L

        self._rec_data = pack('<4H2I', version, rec_type, build, year, file_hist_flags, ver_can_read)


class InteraceHdrRecord(BiffRecord):
    _REC_ID = 0x00E1

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('BB', 0xB0, 0x04)


class InteraceEndRecord(BiffRecord):
    _REC_ID = 0x00E2

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = ''


class MMSRecord(BiffRecord):
    _REC_ID = 0x00C1

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', 0x00)


class WriteAccessRecord(BiffRecord):
    """
    This record is part of the file protection. It contains the name of the
    user  that  has  saved  the  file. The user name is always stored as an
    equal-sized  string.  All  unused  characters after the name are filled
    with space characters. It is not required to write the mentioned string
    length. Every other length will be accepted too.
    """
    _REC_ID = 0x005C

    def __init__(self, owner):
        BiffRecord.__init__(self)

        uowner = owner[0:0x30]
        uowner_len = len(uowner)
        self._rec_data = pack('%ds%ds' % (uowner_len, 0x70 - uowner_len), uowner, ' '*(0x70 - uowner_len))


class DSFRecord(BiffRecord):
    """
    This  record  specifies  if the file contains an additional BIFF5/BIFF7
    workbook stream.
    Record DSF, BIFF8:
    Offset Size Contents
    0        2     0 = Only the BIFF8 Workbook stream is present
                   1 = Additional BIFF5/BIFF7 Book stream is in the file
    A  double  stream file can be read by Excel 5.0 and Excel 95, and still
    contains  all  new  features  added to BIFF8 (which are left out in the
    BIFF5/BIFF7 Book stream).
    """
    _REC_ID = 0x0161

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', 0x00)


class TabIDRecord(BiffRecord):
    _REC_ID = 0x013D

    def __init__(self, sheetcount):
        BiffRecord.__init__(self)

        for i in range(sheetcount):
            self._rec_data += pack('<H', i+1)


class FnGroupCountRecord(BiffRecord):
    _REC_ID = 0x009C

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('BB', 0x0E, 0x00)


class WindowProtectRecord(BiffRecord):
    """
    This record is part of the worksheet/workbook protection. It determines
    whether  the window configuration of this document is protected. Window
    protection is not active, if this record is omitted.
    """
    _REC_ID = 0x0019

    def __init__(self, wndprotect):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', wndprotect)


class ObjectProtectRecord(BiffRecord):
    """
    This record is part of the worksheet/workbook protection.
    It determines whether the objects of the current sheet are protected.
    Object protection is not active, if this record is omitted.
    """
    _REC_ID = 0x0063


    def __init__(self, objprotect):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', objprotect)


class ScenProtectRecord(BiffRecord):
    """
    This record is part of the worksheet/workbook protection. It
    determines whether the scenarios of the current sheet are protected.
    Scenario protection is not active, if this record is omitted.
    """
    _REC_ID = 0x00DD


    def __init__(self, scenprotect):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', scenprotect)


class ProtectRecord(BiffRecord):
    """
    This  record is part of the worksheet/workbook protection. It specifies
    whether  a  worksheet  or a workbook is protected against modification.
    Protection is not active, if this record is omitted.
    """

    _REC_ID = 0x0012

    def __init__(self, protect):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', protect)


class PasswordRecord(BiffRecord):
    """
    This record is part of the worksheet/workbook protection. It
    stores a 16-bit hash value, calculated from the worksheet or workbook
    protection password.
    """
    _REC_ID = 0x0013
    def passwd_hash(self, plaintext):
        """
        Based on the algorithm provided by Daniel Rentz of OpenOffice.
        """
        if plaintext == "":
            return 0

        passwd_hash = 0x0000
        for i, char in enumerate(plaintext):
            c = ord(char) << (i + 1)
            low_15 = c & 0x7fff
            high_15 = c & 0x7fff << 15
            high_15 = high_15 >> 15
            c = low_15 | high_15
            passwd_hash ^= c
        passwd_hash ^= len(plaintext)
        passwd_hash ^= 0xCE4B
        return passwd_hash

    def __init__(self, passwd = ""):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', self.passwd_hash(passwd))


class Prot4RevRecord(BiffRecord):
    _REC_ID = 0x01AF

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', 0x00)


class Prot4RevPassRecord(BiffRecord):
    _REC_ID = 0x01BC

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', 0x00)


class BackupRecord(BiffRecord):
    """
    This  record  contains  a Boolean value determining whether Excel makes
    a backup of the file while saving.
    """
    _REC_ID = 0x0040

    def __init__(self, backup):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', backup)

class HideObjRecord(BiffRecord):
    """
    This record specifies whether and how to show objects in the workbook.

    Record HIDEOBJ, BIFF3-BIFF8:
    Offset  Size    Contents
    0       2       Viewing mode for objects:
                        0 = Show all objects
                        1 = Show placeholders
                        2 = Do not show objects
    """
    _REC_ID = 0x008D

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', 0x00)



class RefreshAllRecord(BiffRecord):
    """
    """

    _REC_ID = 0x01B7

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', 0x00)


class BookBoolRecord(BiffRecord):
    """
    This record contains a Boolean value determining whether to save values
    linked  from external workbooks (CRN records and XCT records). In BIFF3
    and BIFF4 this option is stored in the WSBOOL record.

    Record BOOKBOOL, BIFF5-BIFF8:

    Offset  Size    Contents
    0       2       0 = Save external linked values;
                    1 = Do not save external linked values
    """

    _REC_ID = 0x00DA

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', 0x00)


class CountryRecord(BiffRecord):
    """
    This   record   stores  two  Windows  country  identifiers.  The  first
    represents  the  user  interface language of the Excel version that has
    saved  the file, and the second represents the system regional settings
    at the time the file was saved.

    Record COUNTRY, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       Windows country identifier of the user interface language of Excel
    2       2       Windows country identifier of the system regional settings

    The  following  table  shows most of the used country identifiers. Most
    of  these  identifiers  are  equal to the international country calling
    codes.

    1   USA
    2   Canada
    7   Russia
    """

    _REC_ID = 0x00DA

    def __init__(self, ui_id, sys_settings_id):
        BiffRecord.__init__(self)

        self._rec_data = pack('<2H', ui_id, sys_settings_id)


class UseSelfsRecord(BiffRecord):
    """
    This  record  specifies if the formulas in the workbook can use natural
    language  formulas.  This  type  of  formula can refer to cells by its
    content or the content of the column or row header cell.

    Record USESELFS, BIFF8:

    Offset  Size    Contents
    0       2       0 = Do not use natural language formulas
                    1 = Use natural language formulas

    """

    _REC_ID = 0x0160

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', 0x01)


class EOFRecord(BiffRecord):
    _REC_ID = 0x000A

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = ''


class DateModeRecord(BiffRecord):
    """
    This  record  specifies  the  base date for displaying date values. All
    dates  are  stored as count of days past this base date. In BIFF2-BIFF4
    this   record  is  part  of  the  Calculation  Settings  Block.
    In BIFF5-BIFF8 it is stored in the Workbook Globals Substream.

    Record DATEMODE, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       0 = Base is 1899-Dec-31 (the cell = 1 represents 1900-Jan-01)
                    1 = Base is 1904-Jan-01 (the cell = 1 represents 1904-Jan-02)
    """
    _REC_ID = 0x0022

    def __init__(self, from1904):
        BiffRecord.__init__(self)

        if from1904:
            self._rec_data = pack('<H', 1)
        else:
            self._rec_data = pack('<H', 0)


class PrecisionRecord(BiffRecord):
    """
    This record stores if formulas use the real cell values for calculation
    or  the  values  displayed  on  the screen. In BIFF2- BIFF4 this record
    is  part of the Calculation Settings Block. In BIFF5-BIFF8 it is stored
    in the Workbook Globals Substream.

    Record PRECISION, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       0 = Use displayed values;
                    1 = Use real cell values
    """
    _REC_ID = 0x000E

    def __init__(self, use_real_values):
        BiffRecord.__init__(self)

        if use_real_values:
            self._rec_data = pack('<H', 1)
        else:
            self._rec_data = pack('<H', 0)


class CodepageBiff8Record(BiffRecord):
    """
    This record stores the text encoding used to write byte strings, stored
    as MS Windows code page identifier. The CODEPAGE record in BIFF8 always
    contains  the  code  page  1200  (UTF-16).  Therefore  it is not
    possible  to  obtain the encoding used for a protection password (it is
    not UTF-16).

    Record CODEPAGE, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       Code page identifier used for byte string text encoding:
                      016FH = 367 = ASCII
                      01B5H = 437 = IBM PC CP-437 (US)
                      02D0H = 720 = IBM PC CP-720 (OEM Arabic)
                      02E1H = 737 = IBM PC CP-737 (Greek)
                      0307H = 775 = IBM PC CP-775 (Baltic)
                      0352H = 850 = IBM PC CP-850 (Latin I)
                      0354H = 852 = IBM PC CP-852 (Latin II (Central European))
                      0357H = 855 = IBM PC CP-855 (Cyrillic)
                      0359H = 857 = IBM PC CP-857 (Turkish)
                      035AH = 858 = IBM PC CP-858 (Multilingual Latin I with Euro)
                      035CH = 860 = IBM PC CP-860 (Portuguese)
                      035DH = 861 = IBM PC CP-861 (Icelandic)
                      035EH = 862 = IBM PC CP-862 (Hebrew)
                      035FH = 863 = IBM PC CP-863 (Canadian (French))
                      0360H = 864 = IBM PC CP-864 (Arabic)
                      0361H = 865 = IBM PC CP-865 (Nordic)
                      0362H = 866 = IBM PC CP-866 (Cyrillic (Russian))
                      0365H = 869 = IBM PC CP-869 (Greek (Modern))
                      036AH = 874 = Windows CP-874 (Thai)
                      03A4H = 932 = Windows CP-932 (Japanese Shift-JIS)
                      03A8H = 936 = Windows CP-936 (Chinese Simplified GBK)
                      03B5H = 949 = Windows CP-949 (Korean (Wansung))
                      03B6H = 950 = Windows CP-950 (Chinese Traditional BIG5)
                      04B0H = 1200 = UTF-16 (BIFF8)
                      04E2H = 1250 = Windows CP-1250 (Latin II) (Central European)
                      04E3H = 1251 = Windows CP-1251 (Cyrillic)
                      04E4H = 1252 = Windows CP-1252 (Latin I) (BIFF4-BIFF7)
                      04E5H = 1253 = Windows CP-1253 (Greek)
                      04E6H = 1254 = Windows CP-1254 (Turkish)
                      04E7H = 1255 = Windows CP-1255 (Hebrew)
                      04E8H = 1256 = Windows CP-1256 (Arabic)
                      04E9H = 1257 = Windows CP-1257 (Baltic)
                      04EAH = 1258 = Windows CP-1258 (Vietnamese)
                      0551H = 1361 = Windows CP-1361 (Korean (Johab))
                      2710H = 10000 = Apple Roman
                      8000H = 32768 = Apple Roman
                      8001H = 32769 = Windows CP-1252 (Latin I) (BIFF2-BIFF3)
    """
    _REC_ID = 0x0042
    UTF_16 = 0x04B0

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = pack('<H', self.UTF_16)

class Window1Record(BiffRecord):
    """
    Offset Size Contents
    0      2    Horizontal position of the document window (in twips = 1/20 of a point)
    2      2    Vertical position of the document window (in twips = 1/20 of a point)
    4      2    Width of the document window (in twips = 1/20 of a point)
    6      2    Height of the document window (in twips = 1/20 of a point)
    8      2    Option flags:
                  Bits  Mask  Contents
                  0     0001H 0 = Window is visible 1 = Window is hidden
                  1     0002H 0 = Window is open 1 = Window is minimised
                  3     0008H 0 = Horizontal scroll bar hidden 1 = Horizontal scroll bar visible
                  4     0010H 0 = Vertical scroll bar hidden 1 = Vertical scroll bar visible
                  5     0020H 0 = Worksheet tab bar hidden 1 = Worksheet tab bar visible
    10     2    Index to active (displayed) worksheet
    12     2    Index of first visible tab in the worksheet tab bar
    14     2    Number of selected worksheets (highlighted in the worksheet tab bar)
    16     2    Width of worksheet tab bar (in 1/1000 of window width). The remaining space is used by the
                horizontal scrollbar.
    """
    _REC_ID = 0x003D
    # flags

    def __init__(self,
                 hpos_twips, vpos_twips,
                 width_twips, height_twips,
                 flags,
                 active_sheet,
                 first_tab_index, selected_tabs, tab_width):
        BiffRecord.__init__(self)

        self._rec_data = pack('<9H', hpos_twips, vpos_twips,
                                      width_twips, height_twips,
                                      flags,
                                      active_sheet,
                                      first_tab_index, selected_tabs, tab_width)

class FontRecord(BiffRecord):
    """
    WARNING
        The font with index 4 is omitted in all BIFF versions.
        This means the first four fonts have zero-based indexes, and
        the fifth font and all following fonts are referenced with one-based
        indexes.

    Offset Size Contents
    0      2    Height of the font (in twips = 1/20 of a point)
    2      2    Option flags:
                  Bit Mask    Contents
                  0   0001H   1 = Characters are bold (redundant, see below)
                  1   0002H   1 = Characters are italic
                  2   0004H   1 = Characters are underlined (redundant, see below)
                  3   0008H   1 = Characters are struck out
                        0010H 1 = Outline
                        0020H  1 = Shadow
    4     2     Colour index
    6     2     Font weight (100-1000).
                Standard values are 0190H (400) for normal text and 02BCH
                (700) for bold text.
    8     2     Escapement type:
                  0000H = None
                  0001H = Superscript
                  0002H = Subscript
    10    1     Underline type:
                  00H = None
                  01H = Single
                  21H = Single accounting
                  02H = Double
                  22H = Double accounting
    11    1     Font family:
                  00H = None (unknown or don't care)
                  01H = Roman (variable width, serifed)
                  02H = Swiss (variable width, sans-serifed)
                  03H = Modern (fixed width, serifed or sans-serifed)
                  04H = Script (cursive)
                  05H = Decorative (specialised, i.e. Old English, Fraktur)
    12    1     Character set:
                  00H = 0 = ANSI Latin
                  01H = 1 = System default
                  02H = 2 = Symbol
                  4DH = 77 = Apple Roman
                  80H = 128 = ANSI Japanese Shift-JIS
                  81H = 129 = ANSI Korean (Hangul)
                  82H = 130 = ANSI Korean (Johab)
                  86H = 134 = ANSI Chinese Simplified GBK
                  88H = 136 = ANSI Chinese Traditional BIG5
                  A1H = 161 = ANSI Greek
                  A2H = 162 = ANSI Turkish
                  A3H = 163 = ANSI Vietnamese
                  B1H = 177 = ANSI Hebrew
                  B2H = 178 = ANSI Arabic
                  BAH = 186 = ANSI Baltic
                  CCH = 204 = ANSI Cyrillic
                  DEH = 222 = ANSI Thai
                  EEH = 238 = ANSI Latin II (Central European)
                  FFH = 255 = OEM Latin I
    13    1     Not used
    14    var.  Font name:
                  BIFF5/BIFF7: Byte string, 8-bit string length
                  BIFF8: Unicode string, 8-bit string length
    The boldness and underline flags are still set in the options field,
    but not used on reading the font. Font weight and underline type
    are specified in separate fields instead.
    """
    _REC_ID = 0x0031

    def __init__(self,
                    height, options, colour_index, weight, escapement,
                    underline, family, charset,
                    name):
        BiffRecord.__init__(self)

        uname = upack1(name)
        uname_len = len(uname)

        self._rec_data = pack('<5H4B%ds' % uname_len, height, options, colour_index, weight, escapement,
                                                underline, family, charset, 0x00,
                                                uname)

class NumberFormatRecord(BiffRecord):
    """
    Record FORMAT, BIFF8:
    Offset  Size    Contents
    0       2       Format index used in other records
    2       var.    Number format string (Unicode string, 16-bit string length)

    From  BIFF5  on,  the built-in number formats will be omitted. The built-in
    formats  are  dependent  on  the current regional settings of the operating
    system.  The following table shows which number formats are used by default
    in  a  US-English  environment.  All indexes from 0 to 163 are reserved for
    built-in formats. The first user-defined format starts at 164.

    The built-in number formats, BIFF5-BIFF8

    Index   Type        Format string
    0       General     General
    1       Decimal     0
    2       Decimal     0.00
    3       Decimal     #,##0
    4       Decimal     #,##0.00
    5       Currency    "$"#,##0_);("$"#,##
    6       Currency    "$"#,##0_);[Red]("$"#,##
    7       Currency    "$"#,##0.00_);("$"#,##
    8       Currency    "$"#,##0.00_);[Red]("$"#,##
    9       Percent     0%
    10      Percent     0.00%
    11      Scientific  0.00E+00
    12      Fraction    # ?/?
    13      Fraction    # ??/??
    14      Date        M/D/YY
    15      Date        D-MMM-YY
    16      Date        D-MMM
    17      Date        MMM-YY
    18      Time        h:mm AM/PM
    19      Time        h:mm:ss AM/PM
    20      Time        h:mm
    21      Time        h:mm:ss
    22      Date/Time   M/D/YY h:mm
    37      Account     _(#,##0_);(#,##0)
    38      Account     _(#,##0_);[Red](#,##0)
    39      Account     _(#,##0.00_);(#,##0.00)
    40      Account     _(#,##0.00_);[Red](#,##0.00)
    41      Currency    _("$"* #,##0_);_("$"* (#,##0);_("$"* "-"_);_(@_)
    42      Currency    _(* #,##0_);_(* (#,##0);_(* "-"_);_(@_)
    43      Currency    _("$"* #,##0.00_);_("$"* (#,##0.00);_("$"* "-"??_);_(@_)
    44      Currency    _(* #,##0.00_);_(* (#,##0.00);_(* "-"??_);_(@_)
    45      Time        mm:ss
    46      Time        [h]:mm:ss
    47      Time        mm:ss.0
    48      Scientific  ##0.0E+0
    49      Text        @
    """
    _REC_ID = 0x041E

    def __init__(self, idx, fmtstr):
        BiffRecord.__init__(self)

        ufmtstr = upack2(fmtstr)
        ufmtstr_len = len(ufmtstr)

        self._rec_data = pack('<H%ds' % ufmtstr_len, idx, ufmtstr)


class XFRecord(BiffRecord):
    """
    XF Substructures
    -------------------------------------------------------------------------
    XF_TYPE_PROT  XF Type and Cell Protection (3 Bits), BIFF3-BIFF8
    These 3 bits are part of a specific data byte.
    Bit Mask    Contents
    0   01H     1 = Cell is locked
    1   02H     1 = Formula is hidden
    2   04H     0 = Cell XF; 1 = Style XF

    XF_USED_ATTRIB   Attributes   Used  from  Parent  Style  XF  (6  Bits),
    BIFF3-BIFF8  Each  bit  describes  the  validity  of  a  specific group
    of  attributes.  In  cell XFs a cleared bit means the attributes of the
    parent  style XF are used (but only if the attributes are valid there),
    a  set  bit  means  the  attributes  of  this XF are used. In style XFs
    a cleared bit means the attribute setting is valid, a set bit means the
    attribute should be ignored.
    Bit Mask    Contents
    0   01H     Flag for number format
    1   02H     Flag for font
    2   04H     Flag for horizontal and vertical alignment, text wrap, indentation, orientation, rotation, and
                text direction
    3   08H     Flag for border lines
    4   10H     Flag for background area style
    5   20H     Flag for cell protection (cell locked and formula hidden)

    XF_HOR_ALIGN  Horizontal Alignment (3 Bits), BIFF2-BIFF8 The horizontal
    alignment consists of 3 bits and is part of a specific data byte.
    Value   Horizontal alignment
    00H     General
    01H     Left
    02H     Centred
    03H     Right
    04H     Filled
    05H     Justified (BIFF4-BIFF8X)
    06H     Centred across selection (BIFF4-BIFF8X)
    07H     Distributed (BIFF8X)

    XF_VERT_ALIGN Vertical Alignment (2 or 3 Bits), BIFF4-BIFF8
    The vertical alignment consists of 2 bits (BIFF4) or 3 bits (BIFF5-BIFF8)
    and is part of a specific data byte. Vertical alignment is not available
    in BIFF2 and BIFF3.
    Value   Vertical alignment
    00H     Top
    01H     Centred
    02H     Bottom
    03H     Justified (BIFF5-BIFF8X)
    04H     Distributed (BIFF8X)

    XF_ORIENTATION  Text  Orientation  (2  Bits),  BIFF4-BIFF7  In the BIFF
    versions  BIFF4-BIFF7,  text  can  be  rotated  in  steps of 90 degrees
    or  stacked.  The  orientation  mode  consists of 2 bits and is part of
    a specific data byte. In BIFF8 a rotation angle occurs instead of these
    flags.
    Value   Text orientation
    00H     Not rotated
    01H     Letters are stacked top-to-bottom, but not rotated
    02H     Text is rotated 90 degrees counterclockwise
    03H     Text is rotated 90 degrees clockwise

    XF_ROTATION Text Rotation Angle (1 Byte), BIFF8
    Value   Text rotation
    0       Not rotated
    1-90    1 to 90 degrees counterclockwise
    91-180  1 to 90 degrees clockwise
    255     Letters are stacked top-to-bottom, but not rotated

    XF_BORDER_34  Cell  Border  Style  (4  Bytes), BIFF3-BIFF4 Cell borders
    contain a line style and a line colour for each line of the border.
    Bit     Mask        Contents
    2-0     00000007H   Top line style
    7-3     000000F8H   Colour index for top line colour
    10-8    00000700H   Left line style
    15-11   0000F800H   Colour index for left line colour
    18-16   00070000H   Bottom line style
    23-19   00F80000H   Colour index for bottom line colour
    26-24   07000000H   Right line style
    31-27   F8000000H   Colour index for right line colour

    XF_AREA_34  Cell  Background  Area  Style (2 Bytes), BIFF3-BIFF4 A cell
    background  area  style  contains  an area pattern and a foreground and
    background colour.
    Bit     Mask    Contents
    5-0     003FH   Fill pattern
    10-6    07C0H   Colour index for pattern colour
    15-11   F800H   Colour index for pattern background
 ---------------------------------------------------------------------------------------------
    Record XF, BIFF8:
    Offset      Size    Contents
    0           2       Index to FONT record
    2           2       Index to FORMAT record
    4           2       Bit     Mask    Contents
                        2-0     0007H   XF_TYPE_PROT . XF type, cell protection (see above)
                        15-4    FFF0H   Index to parent style XF (always FFFH in style XFs)
    6           1       Bit     Mask    Contents
                        2-0     07H     XF_HOR_ALIGN . Horizontal alignment (see above)
                        3       08H     1 = Text is wrapped at right border
                        6-4     70H     XF_VERT_ALIGN . Vertical alignment (see above)
    7           1       XF_ROTATION: Text rotation angle (see above)
    8           1       Bit     Mask    Contents
                        3-0     0FH     Indent level
                        4       10H     1 = Shrink content to fit into cell
                        5               merge
                        7-6     C0H     Text direction (BIFF8X only)
                                        00b = According to context
                                        01b = Left-to-right
                                        10b = Right-to-left
    9           1       Bit     Mask    Contents
                        7-2     FCH     XF_USED_ATTRIB . Used attributes (see above)
    10          4       Cell border lines and background area:
                        Bit     Mask      Contents
                        3-0     0000000FH Left line style
                        7-4     000000F0H Right line style
                        11-8    00000F00H Top line style
                        15-12   0000F000H Bottom line style
                        22-16   007F0000H Colour index for left line colour
                        29-23   3F800000H Colour index for right line colour
                        30      40000000H 1 = Diagonal line from top left to right bottom
                        31      80000000H 1 = Diagonal line from bottom left to right top
    14          4       Bit     Mask      Contents
                        6-0     0000007FH Colour index for top line colour
                        13-7    00003F80H Colour index for bottom line colour
                        20-14   001FC000H Colour index for diagonal line colour
                        24-21   01E00000H Diagonal line style
                        31-26   FC000000H Fill pattern
    18          2       Bit     Mask    Contents
                        6-0     007FH   Colour index for pattern colour
                        13-7    3F80H   Colour index for pattern background

    """
    _REC_ID = 0x00E0

    def __init__(self, xf, xftype='cell'):
        BiffRecord.__init__(self)

        font_xf_idx, fmt_str_xf_idx, alignment, borders, pattern, protection = xf
        fnt = struct.pack('<H', font_xf_idx)
        fmt = struct.pack('<H', fmt_str_xf_idx)
        if xftype == 'cell':
            prt = struct.pack('<H',
                ((protection.cell_locked    & 0x01) << 0) |
                ((protection.formula_hidden & 0x01) << 1)
            )
        else:
            prt = struct.pack('<H', 0xFFF5)
        aln = struct.pack('B',
            ((alignment.horz & 0x07) << 0) |
            ((alignment.wrap & 0x01) << 3) |
            ((alignment.vert & 0x07) << 4)
        )
        rot = struct.pack('B', alignment.rota)
        txt = struct.pack('B',
            ((alignment.inde & 0x0F) << 0) |
            ((alignment.shri & 0x01) << 4) |
            ((alignment.merg & 0x01) << 5) |
            ((alignment.dire & 0x03) << 6)
        )
        if xftype == 'cell':
            used_attr = struct.pack('B', 0xF8)
        else:
            used_attr = struct.pack('B', 0xF4)

        if borders.left == borders.NO_LINE:
            borders.left_colour = 0x00
        if borders.right == borders.NO_LINE:
            borders.right_colour = 0x00
        if borders.top == borders.NO_LINE:
            borders.top_colour = 0x00
        if borders.bottom == borders.NO_LINE:
            borders.bottom_colour = 0x00
        if borders.diag == borders.NO_LINE:
            borders.diag_colour = 0x00
        brd1 = struct.pack('<L',
            ((borders.left          & 0x0F) << 0 ) |
            ((borders.right         & 0x0F) << 4 ) |
            ((borders.top           & 0x0F) << 8 ) |
            ((borders.bottom        & 0x0F) << 12) |
            ((borders.left_colour   & 0x7F) << 16) |
            ((borders.right_colour  & 0x7F) << 23) |
            ((borders.need_diag1    & 0x01) << 30) |
            ((borders.need_diag2    & 0x01) << 31)
        )
        brd2 = struct.pack('<L',
            ((borders.top_colour    & 0x7F) << 0 ) |
            ((borders.bottom_colour & 0x7F) << 7 ) |
            ((borders.diag_colour   & 0x7F) << 14) |
            ((borders.diag          & 0x0F) << 21) |
            ((pattern.pattern       & 0x0F) << 26)
        )
        pat = struct.pack('<H',
            ((pattern.pattern_fore_colour & 0x7F) << 0 ) |
            ((pattern.pattern_back_colour & 0x7F) << 7 )
        )
        self._rec_data = fnt + fmt + prt + \
                        aln + rot + txt + used_attr + \
                        brd1 + brd2 + \
                        pat

class StyleRecord(BiffRecord):
    """
    STYLE record for user-defined cell styles, BIFF3-BIFF8:
    Offset  Size    Contents
    0       2       Bit     Mask    Contents
                    11-0    0FFFH   Index to style XF record
                    15      8000H   Always 0 for user-defined styles
    2       var.    BIFF2-BIFF7: Non-empty byte string, 8-bit string length
                    BIFF8: Non-empty Unicode string, 16-bit string length
    STYLE record for built-in cell styles, BIFF3-BIFF8:
    Offset  Size    Contents
    0       2       Bit     Mask    Contents
                    11-0    0FFFH   Index to style XF record
                    15      8000H   Always 1 for built-in styles
    2       1       Identifier of the built-in cell style:
                        00H = Normal
                        01H = RowLevel_lv (see next field)
                        02H = ColLevel_lv (see next field)
                        03H = Comma
                        04H = Currency
                        05H = Percent
                        06H = Comma [0] (BIFF4-BIFF8)
                        07H = Currency [0] (BIFF4-BIFF8)
                        08H = Hyperlink (BIFF8)
                        09H = Followed Hyperlink (BIFF8)
    3       1       Level for RowLevel or ColLevel style
                    (zero-based, lv), FFH otherwise
    The  RowLevel  and  ColLevel  styles specify the formatting of subtotal
    cells  in  a specific outline level. The level is specified by the last
    field  in the STYLE record. Valid values are 0-6 for the outline levels
    1-7.
    """
    _REC_ID = 0x0293

    def __init__(self):
        BiffRecord.__init__(self)

        self._rec_data = struct.pack('<HBB', 0x8000, 0x00, 0xFF)
        # TODO: implement user-defined styles???


class PaletteRecord(BiffRecord):
    """
    This  record  contains  the  definition  of  all  user-defined  colours
    available for cell and object formatting.

    Record PALETTE, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       Number of following colours (nm). Contains 16 in BIFF3-BIFF4 and 56 in BIFF5-BIFF8.
    2       4*nm    List of nm RGB colours

    The following table shows how colour indexes are used in other records:

    Colour index    Resulting colour or internal list index
    00H             Built-in Black (R = 00H, G = 00H, B = 00H)
    01H             Built-in White (R = FFH, G = FFH, B = FFH)
    02H             Built-in Red (R = FFH, G = 00H, B = 00H)
    03H             Built-in Green (R = 00H, G = FFH, B = 00H)
    04H             Built-in Blue (R = 00H, G = 00H, B = FFH)
    05H             Built-in Yellow (R = FFH, G = FFH, B = 00H)
    06H             Built-in Magenta (R = FFH, G = 00H, B = FFH)
    07H             Built-in Cyan (R = 00H, G = FFH, B = FFH)
    08H             First user-defined colour from the PALETTE record (entry 0 from record colour list)
    .........................

    17H (BIFF3-BIFF4) Last user-defined colour from the PALETTE record (entry 15 or 55 from record colour list)
    3FH (BIFF5-BIFF8)

    18H (BIFF3-BIFF4) System window text colour for border lines (used in records XF, CF, and
    40H (BIFF5-BIFF8) WINDOW2 (BIFF8 only))

    19H (BIFF3-BIFF4) System window background colour for pattern background (used in records XF, and CF)
    41H (BIFF5-BIFF8)

    43H             System face colour (dialogue background colour)
    4DH             System window text colour for chart border lines
    4EH             System window background colour for chart areas
    4FH             Automatic colour for chart border lines (seems to be always Black)
    50H             System ToolTip background colour (used in note objects)
    51H             System ToolTip text colour (used in note objects)
    7FFFH           System window text colour for fonts (used in records FONT, EFONT, and CF)

    """
    _REC_ID = 0x0092


class BoundSheetRecord(BiffRecord):
    """
    This  record  is  located  in  the workbook globals area and represents
    a  sheet  inside  of  the  workbook. For each sheet a BOUNDSHEET record
    is  written.  It  stores  the sheet name and a stream offset to the BOF
    record    within   the   workbook   stream.  The  record  is also known
    as BUNDLESHEET.

    Record BOUNDSHEET, BIFF5-BIFF8:
    Offset  Size    Contents
    0       4       Absolute stream position of the BOF record of the sheet represented by this record. This
                    field is never encrypted in protected files.
    4       1       Visibility:
                        00H = Visible
                        01H = Hidden
                        02H = Strong hidden
    5       1       Sheet type:
                        00H = Worksheet
                        02H = Chart
                        06H = Visual Basic module
    6       var.    Sheet name:
                        BIFF5/BIFF7: Byte string, 8-bit string length
                        BIFF8: Unicode string, 8-bit string length
    """
    _REC_ID = 0x0085

    def __init__(self, stream_pos, visibility, sheetname):
        BiffRecord.__init__(self)

        usheetname = upack1(sheetname)
        uusheetname_len = len(usheetname)

        self._rec_data = pack('<LBB%ds' % uusheetname_len, stream_pos, visibility, 0x00, usheetname)


class ContinueRecord(BiffRecord):
    """
    Whenever  the content of a record exceeds the given limits (see table),
    the  record  must  be  split.  Several  CONTINUE records containing the
    additional data are added after the parent record.

    BIFF version    Maximum data size of a record
    BIFF2-BIFF7     2080 bytes (2084 bytes including record header)
    BIFF8           8224 bytes (8228 bytes including record header) (0x2020)

    Record CONTINUE, BIFF2-BIFF8:
    Offset  Size    Contents
    0       var.    Data continuation of the previous record

    Unicode  strings  are  split in a special way. At the beginning of each
    CONTINUE  record  the option flags byte is repeated. Only the character
    size  flag  will  be set in this flags byte, the Rich-Text flag and the
    Far-East  flag  are set to zero. In each CONTINUE record it is possible
    that  the  character  size  changes  from  8-bit  characters  to 16-bit
    characters and vice versa.

    Never  a  Unicode  string  is  split  until  and  including  the  first
    character.  That means, all header fields (string length, option flags,
    optional Rich-Text size, and optional Far-East data size) and the first
    character  of  the string have to occur together in the leading record,
    or  have  to  be  moved completely into the CONTINUE record. Formatting
    runs cannot be split between their components (character index and FONT
    record  index).  If  a string is split between two formatting runs, the
    option flags field will not be repeated in the CONTINUE record.
    """
    _REC_ID = 0x003C


class SSTRecord(BiffRecord):
    """
    This  record  contains  a  list  of  all  strings  used anywhere in the
    workbook.  Each string occurs only once. The workbook uses indexes into
    the list to reference the strings.

    Record SST, BIFF8:
    Offset  Size    Contents
    0       4       Total number of strings in the workbook (see below)
    4       4       Number of following strings (nm)
    8       var.    List of nm Unicode strings, 16-bit string length

    The  first  field  of  the  SST  record  counts  the  total  occurrence
    of  strings  in  the  workbook.  For  instance,  the string AAA is used
    3  times  and  the string BBB is used 2 times. The first field contains
    5 and the second field contains 2, followed by the two strings.
    """
    _REC_ID = 0x00FC


class ExtSSTRecord(BiffRecord):
    """
    This  record  occurs  in  conjunction  with  the SST record. It is used
    by  Excel  to create a hash table with stream offsets to the SST record
    to optimise string search operations. Excel may not shorten this record
    if  strings  are deleted from the shared string table, so the last part
    might  contain  invalid  data. The stream indexes in this record divide
    the SST into portions containing a constant number of strings.

    Record EXTSST, BIFF8:

    Offset  Size    Contents
    0       2       Number of strings in a portion, this number is >=8
    2       var.    List of OFFSET structures for all portions. Each OFFSET contains the following data:
                        Offset Size Contents
                        0       4   Absolute stream position of first string of the portion
                        4       2   Position of first string of the portion inside of current record,
                                    including record header. This counter restarts at zero, if the SST
                                    record is continued with a CONTINUE record.
                        6       2   Not used
    """
    _REC_ID = 0x00FF

    def __init__(self, sst_stream_pos, str_placement, portions_len):
        BiffRecord.__init__(self)

        extsst = {}
        abs_stream_pos = sst_stream_pos
        str_counter = 0
        portion_counter = 0
        while str_counter < len(str_placement):
            str_chunk_num, pos_in_chunk = str_placement[str_counter]
            if str_chunk_num <> portion_counter:
                portion_counter = str_chunk_num
                abs_stream_pos += portions_len[portion_counter-1]
                #print hex(abs_stream_pos)
            str_stream_pos = abs_stream_pos + pos_in_chunk + 4 # header
            extsst[str_counter] = (pos_in_chunk, str_stream_pos)
            str_counter += 1

        exsst_str_count_delta = max(8, len(str_placement)*8/0x2000) # maybe smth else?
        self._rec_data = pack('<H', exsst_str_count_delta)
        str_counter = 0
        while str_counter < len(str_placement):
            self._rec_data += pack('<IHH', extsst[str_counter][1], extsst[str_counter][0], 0)
            str_counter += exsst_str_count_delta

class DimensionsRecord(BiffRecord):
    """
    Record DIMENSIONS, BIFF8:

    Offset  Size    Contents
    0       4       Index to first used row
    4       4       Index to last used row, increased by 1
    8       2       Index to first used column
    10      2       Index to last used column, increased by 1
    12      2       Not used
    """
    _REC_ID = 0x0200
    def __init__(self, first_used_row, last_used_row, first_used_col, last_used_col):
        BiffRecord.__init__(self)

        self._rec_data = struct.pack('<2L3H',
                                            first_used_row, last_used_row + 1,
                                            first_used_col, last_used_col + 1,
                                            0x00)


class Window2Record(BiffRecord):
    """
    Record WINDOW2, BIFF8:

    Offset  Size Contents
    0       2 Option flags (see below)
    2       2 Index to first visible row
    4       2 Index to first visible column
    6       2 Colour index of grid line colour. Note that in BIFF2-BIFF7 an RGB colour is
                written instead.
    8       2 Not used
    10      2 Cached magnification factor in page break preview (in percent); 0 = Default (60%)
    12      2 Cached magnification factor in normal view (in percent); 0 = Default (100%)
    14      4 Not used

    In  BIFF8  this record stores used magnification factors for page break
    preview  and  normal  view.  These  values  are  used  to  restore  the
    magnification,  when the view is changed. The real magnification of the
    currently  active  view  is  stored  in the SCL record. The type of the
    active view is stored in the option flags field (see below).

     0 0001H 0 = Show formula results 1 = Show formulas
     1 0002H 0 = Do not show grid lines 1 = Show grid lines
     2 0004H 0 = Do not show sheet headers 1 = Show sheet headers
     3 0008H 0 = Panes are not frozen 1 = Panes are frozen (freeze)
     4 0010H 0 = Show zero values as empty cells 1 = Show zero values
     5 0020H 0 = Manual grid line colour 1 = Automatic grid line colour
     6 0040H 0 = Columns from left to right 1 = Columns from right to left
     7 0080H 0 = Do not show outline symbols 1 = Show outline symbols
     8 0100H 0 = Keep splits if pane freeze is removed 1 = Remove splits if pane freeze is removed
     9 0200H 0 = Sheet not selected 1 = Sheet selected (BIFF5-BIFF8)
    10 0400H 0 = Sheet not visible 1 = Sheet visible (BIFF5-BIFF8)
    11 0800H 0 = Show in normal view 1 = Show in page break preview (BIFF8)

    The freeze flag specifies, if a following PANE record describes unfrozen or frozen panes.
    """
    _REC_ID = 0x023E

    def __init__(self, options, first_visible_row, first_visible_col, grid_colour, preview_magn, normal_magn):
        BiffRecord.__init__(self)

        self._rec_data = struct.pack('<7HL', options,
                                    first_visible_row, first_visible_col,
                                    grid_colour,
                                    0x00,
                                    preview_magn, normal_magn,
                                    0x00L)


class PanesRecord(BiffRecord):
    """
    This record stores the position of window panes. It is part of the Sheet
    View Settings Block. If the sheet does not contain any splits, this
    record will not occur.
    A sheet can be split in two different ways, with unfrozen panes or with
    frozen panes. A flag in the WINDOW2 record specifies, if the panes are
    frozen, which affects the contents of this record.

    Record PANE, BIFF2-BIFF8:
    Offset      Size        Contents
    0           2           Position of the vertical split
                            (px, 0 = No vertical split):
                            Unfrozen pane: Width of the left pane(s)
                            (in twips = 1/20 of a point)
                            Frozen pane: Number of visible
                            columns in left pane(s)
    2           2           Position of the horizontal split
                            (py, 0 = No horizontal split):
                            Unfrozen pane: Height of the top pane(s)
                            (in twips = 1/20 of a point)
                            Frozen pane: Number of visible
                            rows in top pane(s)
    4           2           Index to first visible row
                            in bottom pane(s)
    6           2           Index to first visible column
                            in right pane(s)
    8           1           Identifier of pane with active
                            cell cursor
    [9]         1           Not used (BIFF5-BIFF8 only, not written
                            in BIFF2-BIFF4)

    If the panes are frozen, pane0 is always active, regardless
    of the cursor position. The correct identifiers for all possible
    combinations of visible panes are shown in the following pictures.

    px = 0, py = 0                  px = 0, py > 0
    --------------------------      ------------|-------------
    |                        |      |                        |
    |                        |      |           3            |
    |                        |      |                        |
    -           3            -      --------------------------
    |                        |      |                        |
    |                        |      |           2            |
    |                        |      |                        |
    --------------------------      ------------|-------------

    px > 0, py = 0                  px > 0, py > 0
    ------------|-------------      ------------|-------------
    |           |            |      |           |            |
    |           |            |      |     3     |      2     |
    |           |            |      |           |            |
    -     3     |      1     -      --------------------------
    |           |            |      |           |            |
    |           |            |      |     1     |      0     |
    |           |            |      |           |            |
    ------------|-------------      ------------|-------------
    """
    _REC_ID = 0x0041
    def __init__(self, px, py, first_row_bottom, first_col_right, active_pane):
        BiffRecord.__init__(self)

        self._rec_data = struct.pack('<5H',
                                            px, py,
                                            first_row_bottom, first_col_right,
                                            active_pane)


class RowRecord(BiffRecord):
    """
    This  record  contains  the properties of a single row in a sheet. Rows
    and cells in a sheet are divided into blocks of 32 rows.

    Record ROW, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       Index of this row
    2       2       Index to column of the first cell which is described by a cell record
    4       2       Index to column of the last cell which is described by a cell record,
                    increased by 1
    6       2       Bit     Mask    Contents
                    14-0    7FFFH   Height of the row, in twips = 1/20 of a point
                    15      8000H   0 = Row has custom height; 1 = Row has default height
    8       2       Not used
    10      2       In BIFF3-BIFF4 this field contains a relative offset
                    to calculate stream position of the first cell record
                    for this row. In BIFF5-BIFF8 this field is not used
                    anymore, but the DBCELL record instead.
    12      4       Option flags and default row formatting:
                    Bit     Mask        Contents
                    2-0     00000007H   Outline level of the row
                    4       00000010H   1 = Outline group starts or ends here (depending
                                        on where the outline buttons are located,
                                        see WSBOOL record), and is collapsed
                    5       00000020H   1 = Row is hidden (manually, or by a filter or outline group)
                    6       00000040H   1 = Row height and default font height do not match
                    7       00000080H   1 = Row has explicit default format (fl)
                    8       00000100H   Always 1
                    27-16   0FFF0000H   If fl=1: Index to default XF record
                    28      10000000H   1 = Additional space above the row. This flag is set,
                                        if the upper border of at least one cell in this row
                                        or if the lower border of at least one cell in the row
                                        above is formatted with a thick line style.
                                        Thin and medium line styles are not taken into account.
                    29      20000000H   1 = Additional space below the row. This flag is set,
                                        if the lower border of at least one cell in this row
                                        or if the upper border of at least one cell in the row
                                        below is formatted with a medium or thick line style.
                                        Thin line styles are not taken into account.
    """

    _REC_ID = 0x0208

    def __init__(self, index, first_col, last_col, height_options, options):
        BiffRecord.__init__(self)

        self._rec_data = struct.pack('<6HL', index, first_col, last_col + 1,
                                        height_options,
                                        0x00, 0x00,
                                        options)

class LabelSSTRecord(BiffRecord):
    """
    This record represents a cell that contains a string. It replaces the
    LABEL record and RSTRING record used in BIFF2-BIFF7.
    """
    _REC_ID = 0x00FD

    def __init__(self, row, col, xf_idx, sst_idx):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<3HL', row, col, xf_idx, sst_idx)


class MergedCellsRecord(BiffRecord):
    """
    This record contains all merged cell ranges of the current sheet.

    Record MERGEDCELLS, BIFF8:

    Offset  Size    Contents
    0       var.    Cell range address list with all merged ranges

    ------------------------------------------------------------------

    A cell range address list consists of a field with the number of ranges
    and the list of the range addresses.

    Cell range address list, BIFF8:

    Offset  Size            Contents
    0       2               Number of following cell range addresses (nm)
    2       8*nm            List of nm cell range addresses

    ---------------------------------------------------------------------
    Cell range address, BIFF8:

    Offset  Size    Contents
    0       2       Index to first row
    2       2       Index to last row
    4       2       Index to first column
    6       2       Index to last column

    """
    _REC_ID = 0x00E5

    def __init__(self, merged_list):
        BiffRecord.__init__(self)

        i = len(merged_list) - 1
        while i >= 0:
            j = 0
            merged = ''
            while (i >= 0) and (j < 0x403):
                r1, r2, c1, c2 = merged_list[i]
                merged += struct.pack('<4H', r1, r2, c1, c2)
                i -= 1
                j += 1
            self._rec_data += struct.pack('<3H', self._REC_ID, len(merged) + 2, j) + \
                                    merged

    # for some reason Excel doesn't use CONTINUE
    def get(self):
        return self._rec_data

class MulBlankRecord(BiffRecord):
    """
    This  record  represents  a  cell  range  of empty cells. All cells are
    located in the same row.

    Record MULBLANK, BIFF5-BIFF8:

    Offset  Size    Contents
    0       2       Index to row
    2       2       Index to first column (fc)
    4       2*nc    List of nc=lc-fc+1 16-bit indexes to XF records
    4+2*nc  2       Index to last column (lc)
    """
    _REC_ID = 0x00BE

    def __init__(self, row, first_col, last_col, xf_index):
        BiffRecord.__init__(self)
        blanks_count = last_col-first_col+1
        self._rec_data = struct.pack('%dH' % blanks_count, *([xf_index]*blanks_count))
        self._rec_data = struct.pack('<2H', row, first_col) +  self._rec_data + struct.pack('<H',  last_col)


class BlankRecord(BiffRecord):
    """
    This  record  represents  an empty cell.

    Record BLANK, BIFF5-BIFF8:

    Offset  Size    Contents
    0       2       Index to row
    2       2       Index to first column (fc)
    4       2       indexes to XF record
    """
    _REC_ID = 0x0201

    def __init__(self, row, col, xf_index):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<3H', row, col, xf_index)


class RKRecord(BiffRecord):
    """
    This record represents a cell that contains an RK value (encoded integer or
    floating-point value). If a floating-point value cannot be encoded to an RK value,
    a NUMBER record will be written.
    """
    _REC_ID = 0x027E

    def __init__(self, row, col, xf_index, rk_encoded):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<3HI', row, col, xf_index, rk_encoded)


class NumberRecord(BiffRecord):
    """
    This record represents a cell that contains an IEEE-754 floating-point value.
    """
    _REC_ID = 0x0203

    def __init__(self, row, col, xf_index, number):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<3Hd', row, col, xf_index, number)


class FormulaRecord(BiffRecord):
    """
    Offset Size Contents
    0      2    Index to row
    2      2    Index to column
    4      2    Index to XF record
    6      8    Result of the formula
    14     2    Option flags:
                Bit Mask    Contents
                0   0001H   1 = Recalculate always
                1   0002H   1 = Calculate on open
                3   0008H   1 = Part of a shared formula
    16     4    Not used
    20     var. Formula data (RPN token array)

    """
    _REC_ID = 0x0006

    def __init__(self, row, col, xf_index, rpn):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<3HQHL', row, col, xf_index, 0xFFFF000000000003, 0, 0) + rpn


class GutsRecord(BiffRecord):
    """
    This record contains information about the layout of outline symbols.

    Record GUTS, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       Width of the area to display row outlines (left of the sheet), in pixel
    2       2       Height of the area to display column outlines (above the sheet), in pixel
    4       2       Number of visible row outline levels (used row levels + 1; or 0, if not used)
    6       2       Number of visible column outline levels (used column levels + 1; or 0, if not used)

    """

    _REC_ID = 0x0080

    def __init__(self, row_gut_width, col_gut_height, row_visible_levels, col_visible_levels):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<4H', row_gut_width, col_gut_height, row_visible_levels, col_visible_levels)

class WSBoolRecord(BiffRecord):
    """
    This  record stores a 16 bit value with Boolean options for the current
    sheet.  From BIFF5 on the "Save external linked values" option is moved
    to the record BOOKBOOL.

    Option flags of record WSBOOL, BIFF3-BIFF8:

    Bit     Mask    Contents
    0       0001H   0 = Do not show automatic page breaks
                    1 = Show automatic page breaks
    4       0010H   0 = Standard sheet
                    1 = Dialogue sheet (BIFF5-BIFF8)
    5       0020H   0 = No automatic styles in outlines
                    1 = Apply automatic styles to outlines
    6       0040H   0 = Outline buttons above outline group
                    1 = Outline buttons below outline group
    7       0080H   0 = Outline buttons left of outline group
                    1 = Outline buttons right of outline group
    8       0100H   0 = Scale printout in percent
                    1 = Fit printout to number of pages
    9       0200H   0 = Save external linked values (BIFF3?BIFF4 only)
                    1 = Do not save external linked values (BIFF3?BIFF4 only)
    10      0400H   0 = Do not show row outline symbols
                    1 = Show row outline symbols
    11      0800H   0 = Do not show column outline symbols
                    1 = Show column outline symbols
    13-12   3000H   These flags specify the arrangement of windows.
                    They are stored in BIFF4 only.
                    00 = Arrange windows tiled
                    01 = Arrange windows horizontal
                    10 = Arrange windows vertical112 = Arrange windows cascaded
    The following flags are valid for BIFF4-BIFF8 only:
    14      4000H   0 = Standard expression evaluation
                    1 = Alternative expression evaluation
    15      8000H   0 = Standard formula entries
                    1 = Alternative formula entries

    """
    _REC_ID = 0x0081

    def __init__(self, options):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', options)

class ColInfoRecord(BiffRecord):
    """
    This record specifies the width for a given range of columns.
    If a column does not have a corresponding COLINFO record,
    the width specified in the record STANDARDWIDTH is used. If
    this record is also not present, the contents of the record
    DEFCOLWIDTH is used instead.
    This record also specifies a default XF record to use for
    cells in the columns that are not described by any cell record
    (which contain the XF index for that cell). Additionally,
    the option flags field contains hidden, outline, and collapsed
    options applied at the columns.

    Record COLINFO, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       Index to first column in the range
    2       2       Index to last column in the range
    4       2       Width of the columns in 1/256 of the width of the zero character, using default font
                    (first FONT record in the file)
    6       2       Index to XF record for default column formatting
    8       2       Option flags:
                    Bits    Mask    Contents
                    0       0001H   1 = Columns are hidden
                    10-8    0700H   Outline level of the columns (0 = no outline)
                    12      1000H   1 = Columns are collapsed
    10      2       Not used

    """
    _REC_ID = 0x007D

    def __init__(self, first_col, last_col, width, xf_index, options):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<6H', first_col, last_col, width, xf_index, options, 0)

class CalcModeRecord(BiffRecord):
    """
    This record is part of the Calculation Settings Block.
    It specifies whether to calculate formulas manually,
    automatically or automatically except for multiple table operations.

    Record CALCMODE, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       FFFFH = automatic except for multiple table operations
                    0000H = manually
                    0001H = automatically (default)
    """
    _REC_ID = 0x000D

    def __init__(self, calc_mode):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<h', calc_mode)


class CalcCountRecord(BiffRecord):
    """
    This record is part of the Calculation Settings Block. It specifies the maximum
    number of times the formulas should be iteratively calculated. This is a fail-safe
    against mutually recursive formulas locking up a spreadsheet application.

    Record CALCCOUNT, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       Maximum number of iterations allowed in circular references
    """

    _REC_ID = 0x000C

    def __init__(self, calc_count):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', calc_count)

class RefModeRecord(BiffRecord):
    """
    This record is part of the Calculation Settings Block.
    It stores which method is used to show cell addresses in formulas.
    The RC mode uses numeric indexes for rows and columns,
    i.e. R(1)C(-1), or R1C1:R2C2.
    The A1 mode uses characters for columns and numbers for rows,
    i.e. B1, or $A$1:$B$2.

    Record REFMODE, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       0 = RC mode; 1 = A1 mode

    """
    _REC_ID = 0x00F

    def __init__(self, ref_mode):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', ref_mode)

class IterationRecord(BiffRecord):
    """
    This record is part of the Calculation Settings Block.
    It stores if iterations are allowed while calculating recursive formulas.

    Record ITERATION, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       0 = Iterations off; 1 = Iterations on
    """
    _REC_ID = 0x011

    def __init__(self, iterations_on):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', iterations_on)

class DeltaRecord(BiffRecord):
    """
    This record is part of the Calculation Settings Block.
    It stores the maximum change of the result to exit an iteration.

    Record DELTA, BIFF2-BIFF8:

    Offset  Size    Contents
    0       8       Maximum change in iteration
                    (IEEE 754 floating-point value,
                     64bit double precision)
    """
    _REC_ID = 0x010

    def __init__(self, delta):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<d', delta)

class SaveRecalcRecord(BiffRecord):
    """
    This record is part of the Calculation Settings Block.
    It contains the Recalculate before save option in
    Excel's calculation settings dialogue.

    Record SAVERECALC, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       0 = Do not recalculate;
                    1 = Recalculate before saving the document

    """
    _REC_ID = 0x05F

    def __init__(self, recalc):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', recalc)

class PrintHeadersRecord(BiffRecord):
    """
    This record stores if the row and column headers
    (the areas with row numbers and column letters) will be printed.

    Record PRINTHEADERS, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       0 = Do not print row/column headers;
                    1 = Print row/column headers
    """
    _REC_ID = 0x02A

    def __init__(self, print_headers):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', print_headers)


class PrintGridLinesRecord(BiffRecord):
    """
    This record stores if sheet grid lines will be printed.

    Record PRINTGRIDLINES, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       0 = Do not print sheet grid lines;
                    1 = Print sheet grid lines

    """
    _REC_ID = 0x02B

    def __init__(self, print_grid):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', print_grid)


class GridSetRecord(BiffRecord):
    """
    This record specifies if the option to print sheet grid lines
    (record PRINTGRIDLINES) has ever been changed.

    Record GRIDSET, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       0 = Print grid lines option never changed
                    1 = Print grid lines option changed
    """
    _REC_ID = 0x082

    def __init__(self, print_grid_changed):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', print_grid_changed)


class DefaultRowHeight(BiffRecord):
    """
    This record specifies the default height and default flags
    for rows that do not have a corresponding ROW record.

    Record DEFAULTROWHEIGHT, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       Option flags:
                    Bit Mask    Contents
                    0   0001H   1 = Row height and default font height do not match
                    1   0002H   1 = Row is hidden
                    2   0004H   1 = Additional space above the row
                    3   0008H   1 = Additional space below the row
    2       2       Default height for unused rows, in twips = 1/20 of a point

    """
    _REC_ID = 0x0225

    def __init__(self, options, def_height):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<2H', options, def_height)


class DefColWidthRecord(BiffRecord):
    """
    This record specifies the default column width for columns that
    do not have a specific width set using the record COLINFO or COLWIDTH.
    This record has no effect, if a STANDARDWIDTH record is present in the file.

    Record DEFCOLWIDTH, BIFF2-BIFF8:

    Offset  Size    Contents
    0       2       Column width in characters, using the width of the zero
                    character from default font (first FONT record in the file)
    """
    _REC_ID = 0x0055

    def __init__(self, def_width):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', options, def_width)

class HorizontalPageBreaksRecord(BiffRecord):
    """
    This  record  is  part  of  the  Page  Settings  Block. It contains all
    horizontal manual page breaks.

    Record HORIZONTALPAGEBREAKS, BIFF8:
    Offset  Size  Contents
    0       2     Number of following row index structures (nm)
    2       6nm   List of nm row index structures. Each row index
                  structure contains:
                    Offset  Size    Contents
                    0       2       Index to first row below the page break
                    2       2       Index to first column of this page break
                    4       2       Index to last column of this page break

    The row indexes in the lists must be ordered ascending.
    If in BIFF8 a row contains several page breaks, they must be ordered
    ascending by start column index.
    """
    _REC_ID = 0x001B

    def __init__(self, breaks_list):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', len(breaks_list))
        for r, c1, c2 in breaks_list:
            self._rec_data += struct.pack('<3H', r, c1, c2)

class VerticalPageBreaksRecord(BiffRecord):
    """
    This  record  is  part  of  the  Page  Settings  Block. It contains all
    vertical manual page breaks.

    Record VERTICALPAGEBREAKS, BIFF8:
    Offset  Size  Contents
    0       2     Number of following column index structures (nm)
    2       6nm   List of nm column index structures. Each column index
                  structure contains:
                    Offset  Size    Contents
                    0       2       Index to first column following the page
                                    break
                    2       2       Index to first row of this page break
                    4       2       Index to last row of this page break

    The column indexes in the lists must be ordered ascending.
    If in BIFF8 a column contains several page breaks, they must be ordered
    ascending by start row index.
    """
    _REC_ID = 0x001A

    def __init__(self, breaks_list):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', len(breaks_list))
        for r, c1, c2 in breaks_list:
            self._rec_data += struct.pack('<3H', r, c1, c2)

class HeaderRecord(BiffRecord):
    """
    This record is part of the Page Settings Block. It specifies the
    page  header  string  for  the current worksheet. If this record is not
    present  or  completely  empty  (record  size is 0), the sheet does not
    contain a page header.

    Record HEADER for non-empty page header, BIFF2-BIFF8:
    Offset      Size    Contents
    0           var.    Page header string
                        BIFF2-BIFF7:    Non-empty byte string, 8bit string
                        length
                        BIFF8: Non-empty Unicode string, 16bit string length
    The  header  string may contain special commands, i.e. placeholders for
    the  page  number,  current  date, or text formatting attributes. These
    fields  are  represented  by  single  letters (exception: font name and
    size,  see  below)  with  a  leading  ampersand ("&"). If the ampersand
    is  part  of the regular header text, it will be duplicated ("&&"). The
    page  header is divided into 3 sections: the left, the centred, and the
    right  section.  Each  section  is introduced by a special command. All
    text  and all commands following are part of the selected section. Each
    section  starts  with the text formatting specified in the default font
    (first  FONT  record  in  the  file). Active formatting attributes from
    a previous section do not go into the next section.

    The following table shows all available commands:

    Command         Contents
    &&              The "&" character itself
    &L              Start of the left section
    &C              Start of the centred section
    &R              Start of the right section
    &P              Current page number
    &N              Page count
    &D              Current date
    &T              Current time
    &A              Sheet name (BIFF5-BIFF8)
    &F              File name without path
    &Z              File path without file name (BIFF8X)
    &G              Picture (BIFF8X)
    &B              Bold on/off (BIFF2-BIFF4)
    &I              Italic on/off (BIFF2-BIFF4)
    &U              Underlining on/off
    &E              Double underlining on/off (BIFF5-BIFF8)
    &S              Strikeout on/off
    &X              Superscript on/off (BIFF5-BIFF8)
    &Y              Subscript on/off (BIFF5-BIFF8)
    &"<fontname>"   Set new font <fontname>
    &"<fontname>,<fontstyle>"
                    Set new font with specified style <fontstyle>.
                    The style <fontstyle> is in most cases one of
                    "Regular", "Bold", "Italic", or "Bold Italic".
                    But this setting is dependent on the used font,
                    it may differ (localised style names, or "Standard",
                    "Oblique", ...). (BIFF5-BIFF8)
    &<fontheight>   Set font height in points (<fontheight> is a decimal value).
                    If this command is followed by a plain number to be printed
                    in the header, it will be separated from the font height
                    with a space character.

    """
    _REC_ID = 0x0014

    def __init__(self, header_str):
        BiffRecord.__init__(self)
        self._rec_data = upack2(header_str)

class FooterRecord(BiffRecord):
    """
    Semantic is equal to HEADER record
    """
    _REC_ID = 0x0015

    def __init__(self, footer_str):
        BiffRecord.__init__(self)
        self._rec_data = upack2(footer_str)


class HCenterRecord(BiffRecord):
    """
    This  record  is  part  of the Page Settings Block. It specifies if the
    sheet is centred horizontally when printed.

    Record HCENTER, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       0 = Print sheet left aligned
                    1 = Print sheet centred horizontally

    """
    _REC_ID = 0x0083

    def __init__(self, is_horz_center):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', is_horz_center)


class VCenterRecord(BiffRecord):
    """
    This  record  is  part  of the Page Settings Block. It specifies if the
    sheet is centred vertically when printed.

    Record VCENTER, BIFF3-BIFF8:

    Offset  Size    Contents
    0       2       0 = Print sheet aligned at top page border
                    1 = Print sheet vertically centred

    """
    _REC_ID = 0x0084

    def __init__(self, is_vert_center):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<H', is_vert_center)


class LeftMarginRecord(BiffRecord):
    """
    This  record  is  part of the Page Settings Block. It contains the left
    page margin of the current worksheet.

    Record LEFTMARGIN, BIFF2-BIFF8:

    Offset  Size    Contents
    0       8       Left page margin in inches
                    (IEEE 754 floating-point value, 64bit double precision)

    """
    _REC_ID = 0x0026

    def __init__(self, margin):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<d', margin)


class RightMarginRecord(BiffRecord):
    """
    This  record  is  part of the Page Settings Block. It contains the right
    page margin of the current worksheet.

    Offset  Size    Contents
    0       8       Right page margin in inches
                    (IEEE 754 floating-point value, 64?bit double precision)

    """
    _REC_ID = 0x0027

    def __init__(self, margin):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<d', margin)

class TopMarginRecord(BiffRecord):
    """
    This  record  is  part of the Page Settings Block. It contains the top
    page margin of the current worksheet.

    Offset  Size    Contents
    0       8       Top page margin in inches
                    (IEEE 754 floating-point value, 64?bit double precision)

    """
    _REC_ID = 0x0028

    def __init__(self, margin):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<d', margin)


class BottomMarginRecord(BiffRecord):
    """
    This  record  is  part of the Page Settings Block. It contains the bottom
    page margin of the current worksheet.

    Offset  Size    Contents
    0       8       Bottom page margin in inches
                    (IEEE 754 floating-point value, 64?bit double precision)

    """
    _REC_ID = 0x0029

    def __init__(self, margin):
        BiffRecord.__init__(self)
        self._rec_data = struct.pack('<d', margin)

class SetupPageRecord(BiffRecord):
    """
    This   record   is  part of the Page Settings Block. It stores the page
    format   settings   of   the  current sheet. The pages may be scaled in
    percent   or  by  using  an  absolute  number of pages. This setting is
    located   in  the  WSBOOL  record.  If  pages  are  scaled in  percent,
    the   scaling  factor  in  this  record is used, otherwise the "Fit  to
    pages"  values. One of the "Fit to pages" values may be 0. In this case
    the sheet is scaled to fit only to the other value.

    Record SETUP, BIFF5-BIFF8:

    Offset      Size    Contents
    0           2       Paper size (see below)
    2           2       Scaling factor in percent
    4           2       Start page number
    6           2       Fit worksheet width to this number of pages
                        (0 = use as many as needed)
    8           2       Fit worksheet height to this number of pages
                        (0 = use as many as needed)
    10          2       Option flags:
                        Bit     Mask        Contents
                        0       0001H       0 = Print pages in columns
                                            1 = Print pages in rows
                        1       0002H       0 = Landscape
                                            1 = Portrait
                        2       0004H       1 = Paper size, scaling factor,
                                            paper orientation (portrait/landscape),
                                            print resolution and number of copies
                                            are not initialised
                        3       0008H       0 = Print coloured
                                            1 = Print black and white
                        4       0010H       0 = Default print quality
                                            1 = Draft quality
                        5       0020H       0 = Do not print cell notes
                                            1 = Print cell notes
                        6       0040H       0 = Paper orientation setting is valid
                                            1 = Paper orientation setting not
                                            initialised
                        7       0080H       0 = Automatic page numbers
                                            1 = Use start page number
                        The following flags are valid for BIFF8 only:
                        9       0200H       0 = Print notes as displayed
                                            1 = Print notes at end of sheet
                        11-10   0C00H       00 = Print errors as displayed
                                            01 = Do not print errors
                                            10 = Print errors as "--"
                                            11 = Print errors as "#N/A!"
    12          2       Print resolution in dpi
    14          2       Vertical print resolution in dpi
    16          8       Header margin (IEEE 754 floating-point value,
                        64bit double precision)
    24          8       Footer margin (IEEE 754 floating-point value,
                        64bit double precision)
    32          2       Number of copies to print


    PAPER TYPES:

    Index   Paper type              Paper size
    0       Undefined
    1       Letter                  8 1/2" x 11"
    2       Letter small            8 1/2" x 11"
    3       Tabloid                 11" x 17"
    4       Ledger                  17" x 11"
    5       Legal                   8 1/2" x 14"
    6       Statement               5 1/2" x 8 1/2"
    7       Executive               7 1/4" x 10 1/2"
    8       A3                      297mm x 420mm
    9       A4                      210mm x 297mm
    10      A4 small                210mm x 297mm
    11      A5                      148mm x 210mm
    12      B4 (JIS)                257mm x 364mm
    13      B5 (JIS)                182mm x 257mm
    14      Folio                   8 1/2" x 13"
    15      Quarto                  215mm x 275mm
    16      10x14                   10" x 14"
    17      11x17                   11" x 17"
    18      Note                    8 1/2" x 11"
    19      Envelope #9             3 7/8" x 8 7/8"
    20      Envelope #10            4 1/8" x 9 1/2"
    21      Envelope #11            4 1/2" x 10 3/8"
    22      Envelope #12            4 3/4" x 11"
    23      Envelope #14            5" x 11 1/2"
    24      C                       17" x 22"
    25      D                       22" x 34"
    26      E                       34" x 44"
    27      Envelope DL             110mm x 220mm
    28      Envelope C5             162mm x 229mm
    29      Envelope C3             324mm x 458mm
    30      Envelope C4             229mm x 324mm
    31      Envelope C6             114mm x 162mm
    32      Envelope C6/C5          114mm x 229mm
    33      B4 (ISO)                250mm x 353mm
    34      B5 (ISO)                176mm x 250mm
    35      B6 (ISO)                125mm x 176mm
    36      Envelope Italy          110mm x 230mm
    37      Envelope Monarch        3 7/8" x 7 1/2"
    38      63/4 Envelope           3 5/8" x 6 1/2"
    39      US Standard Fanfold     14 7/8" x 11"
    40      German Std. Fanfold     8 1/2" x 12"
    41      German Legal Fanfold    8 1/2" x 13"
    42      B4 (ISO)                250mm x 353mm
    43      Japanese Postcard       100mm x 148mm
    44      9x11                    9" x 11"
    45      10x11                   10" x 11"
    46      15x11                   15" x 11"
    47      Envelope Invite         220mm x 220mm
    48      Undefined
    49      Undefined
    50      Letter Extra            9 1/2" x 12"
    51      Legal Extra             9 1/2" x 15"
    52      Tabloid Extra           11 11/16" x 18"
    53      A4 Extra                235mm x 322mm
    54      Letter Transverse       8 1/2" x 11"
    55      A4 Transverse           210mm x 297mm
    56      Letter Extra Transv.    9 1/2" x 12"
    57      Super A/A4              227mm x 356mm
    58      Super B/A3              305mm x 487mm
    59      Letter Plus             8 1/2" x 12 11/16"
    60      A4 Plus                 210mm x 330mm
    61      A5 Transverse           148mm x 210mm
    62      B5 (JIS) Transverse     182mm x 257mm
    63      A3 Extra                322mm x 445mm
    64      A5 Extra                174mm x 235mm
    65      B5 (ISO) Extra          201mm x 276mm
    66      A2                      420mm x 594mm
    67      A3 Transverse           297mm x 420mm
    68      A3 Extra Transverse     322mm x 445mm
    69      Dbl. Japanese Postcard  200mm x 148mm
    70      A6                      105mm x 148mm
    71
    72
    73
    74
    75      Letter Rotated          11" x 8 1/2"
    76      A3 Rotated              420mm x 297mm
    77      A4 Rotated              297mm x 210mm
    78      A5 Rotated              210mm x 148mm
    79      B4 (JIS) Rotated        364mm x 257mm
    80      B5 (JIS) Rotated        257mm x 182mm
    81      Japanese Postcard Rot.  148mm x 100mm
    82      Dbl. Jap. Postcard Rot. 148mm x 200mm
    83      A6 Rotated              148mm x 105mm
    84
    85
    86
    87
    88      B6 (JIS)                128mm x 182mm
    89      B6 (JIS) Rotated        182mm x 128mm
    90      12x11                   12" x 11"

    """
    _REC_ID = 0x00A1
    def __init__(self, paper, scaling, start_num, fit_width_to, fit_height_to,
                    options,
                    hres, vres,
                    header_margin, footer_margin,
                    num_copies):
        BiffRecord.__init__(self)

        self._rec_data = struct.pack('<8H2dH', paper, scaling, start_num,
                                        fit_width_to, fit_height_to, \
                                        options,
                                        hres, vres,
                                        header_margin, footer_margin,
                                        num_copies)


########NEW FILE########
__FILENAME__ = Bitmap
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov

#  Portions are Copyright (c) 2004 Evgeny Filatov <fufff@users.sourceforge.net>

#  Portions are Copyright (c) 2002-2004 John McNamara (Perl Spreadsheet::WriteExcel)

#  All rights reserved.

# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


__rev_id__ = """$Id: Bitmap.py,v 1.4 2005/07/20 07:24:11 rvk Exp $"""


from BIFFRecords import BiffRecord
from struct import *


def _size_col(sheet, col):
    return sheet.col_width(col)


def _size_row(sheet, row):
    return sheet.row_height(row)     


def _position_image(sheet, row_start, col_start, x1, y1, width, height):
    """Calculate the vertices that define the position of the image as required by
    the OBJ record.

             +------------+------------+
             |     A      |      B     |
       +-----+------------+------------+
       |     |(x1,y1)     |            |
       |  1  |(A1)._______|______      |
       |     |    |              |     |
       |     |    |              |     |
       +-----+----|    BITMAP    |-----+
       |     |    |              |     |
       |  2  |    |______________.     |
       |     |            |        (B2)|
       |     |            |     (x2,y2)|
       +---- +------------+------------+

    Example of a bitmap that covers some of the area from cell A1 to cell B2.

    Based on the width and height of the bitmap we need to calculate 8 vars:
        col_start, row_start, col_end, row_end, x1, y1, x2, y2.
    The width and height of the cells are also variable and have to be taken into
    account.
    The values of col_start and row_start are passed in from the calling
    function. The values of col_end and row_end are calculated by subtracting
    the width and height of the bitmap from the width and height of the
    underlying cells.
    The vertices are expressed as a percentage of the underlying cell width as
    follows (rhs values are in pixels):

           x1 = X / W *1024
           y1 = Y / H *256
           x2 = (X-1) / W *1024
           y2 = (Y-1) / H *256

           Where:  X is distance from the left side of the underlying cell
                   Y is distance from the top of the underlying cell
                   W is the width of the cell
                   H is the height of the cell

    Note: the SDK incorrectly states that the height should be expressed as a
    percentage of 1024.

    col_start  - Col containing upper left corner of object
    row_start  - Row containing top left corner of object
    x1  - Distance to left side of object
    y1  - Distance to top of object
    width  - Width of image frame
    height  - Height of image frame
    
    """
    # Adjust start column for offsets that are greater than the col width
    while x1 >= _size_col(sheet, col_start):
        x1 -= _size_col(sheet, col_start)
        col_start += 1
    # Adjust start row for offsets that are greater than the row height
    while y1 >= _size_row(sheet, row_start):
        y1 -= _size_row(sheet, row_start)
        row_start += 1
    # Initialise end cell to the same as the start cell
    row_end = row_start   # Row containing bottom right corner of object
    col_end = col_start   # Col containing lower right corner of object
    width = width + x1 - 1
    height = height + y1 - 1
    # Subtract the underlying cell widths to find the end cell of the image
    while (width >= _size_col(sheet, col_end)):
        width -= _size_col(sheet, col_end)
        col_end += 1
    # Subtract the underlying cell heights to find the end cell of the image
    while (height >= _size_row(sheet, row_end)):
        height -= _size_row(sheet, row_end)
        row_end += 1
    # Bitmap isn't allowed to start or finish in a hidden cell, i.e. a cell
    # with zero height or width.
    if ((_size_col(sheet, col_start) == 0) or (_size_col(sheet, col_end) == 0)
            or (_size_row(sheet, row_start) == 0) or (_size_row(sheet, row_end) == 0)):
        return
    # Convert the pixel values to the percentage value expected by Excel
    x1 = float(x1) / _size_col(sheet, col_start) * 1024
    y1 = float(y1) / _size_row(sheet, row_start) * 256
    # Distance to right side of object
    x2 = float(width) / _size_col(sheet, col_end) * 1024
    # Distance to bottom of object
    y2 = float(height) / _size_row(sheet, row_end) * 256
    return (col_start, x1, row_start, y1, col_end, x2, row_end, y2)


class ObjBmpRecord(BiffRecord):
    _REC_ID = 0x005D    # Record identifier

    def __init__(self, row, col, sheet, im_data_bmp, x, y, scale_x, scale_y):
        # Scale the frame of the image.
        width = im_data_bmp.width * scale_x
        height = im_data_bmp.height * scale_y

        # Calculate the vertices of the image and write the OBJ record
        col_start, x1, row_start, y1, col_end, x2, row_end, y2 = _position_image(sheet, row, col, x, y, width, height)

        """Store the OBJ record that precedes an IMDATA record. This could be generalise
        to support other Excel objects.

        """
        cObj = 0x0001      # Count of objects in file (set to 1)
        OT = 0x0008        # Object type. 8 = Picture
        id = 0x0001        # Object ID
        grbit = 0x0614     # Option flags
        colL = col_start    # Col containing upper left corner of object
        dxL = x1            # Distance from left side of cell
        rwT = row_start     # Row containing top left corner of object
        dyT = y1            # Distance from top of cell
        colR = col_end      # Col containing lower right corner of object
        dxR = x2            # Distance from right of cell
        rwB = row_end       # Row containing bottom right corner of object
        dyB = y2            # Distance from bottom of cell
        cbMacro = 0x0000    # Length of FMLA structure
        Reserved1 = 0x0000  # Reserved
        Reserved2 = 0x0000  # Reserved
        icvBack = 0x09      # Background colour
        icvFore = 0x09      # Foreground colour
        fls = 0x00          # Fill pattern
        fAuto = 0x00        # Automatic fill
        icv = 0x08          # Line colour
        lns = 0xff          # Line style
        lnw = 0x01          # Line weight
        fAutoB = 0x00       # Automatic border
        frs = 0x0000        # Frame style
        cf = 0x0009         # Image format, 9 = bitmap
        Reserved3 = 0x0000  # Reserved
        cbPictFmla = 0x0000 # Length of FMLA structure
        Reserved4 = 0x0000  # Reserved
        grbit2 = 0x0001     # Option flags
        Reserved5 = 0x0000  # Reserved
        
        data = pack("<L", cObj)
        data += pack("<H", OT)
        data += pack("<H", id)
        data += pack("<H", grbit)
        data += pack("<H", colL)
        data += pack("<H", dxL)
        data += pack("<H", rwT)
        data += pack("<H", dyT)
        data += pack("<H", colR)
        data += pack("<H", dxR)
        data += pack("<H", rwB)
        data += pack("<H", dyB)
        data += pack("<H", cbMacro)
        data += pack("<L", Reserved1)
        data += pack("<H", Reserved2)
        data += pack("<B", icvBack)
        data += pack("<B", icvFore)
        data += pack("<B", fls)
        data += pack("<B", fAuto)
        data += pack("<B", icv)
        data += pack("<B", lns)
        data += pack("<B", lnw)
        data += pack("<B", fAutoB)
        data += pack("<H", frs)
        data += pack("<L", cf)
        data += pack("<H", Reserved3)
        data += pack("<H", cbPictFmla)
        data += pack("<H", Reserved4)
        data += pack("<H", grbit2)
        data += pack("<L", Reserved5)

        self._rec_data = data

def _process_bitmap(bitmap):
    """Convert a 24 bit bitmap into the modified internal format used by Windows.
    This is described in BITMAPCOREHEADER and BITMAPCOREINFO structures in the
    MSDN library.

    """
    # Open file and binmode the data in case the platform needs it.
    fh = file(bitmap, "rb")
    try:
        # Slurp the file into a string.
        data = fh.read()
    finally:
        fh.close()
    # Check that the file is big enough to be a bitmap.
    if len(data) <= 0x36:
        raise Exception("bitmap doesn't contain enough data.")
    # The first 2 bytes are used to identify the bitmap.
    if (data[:2] != "BM"):
        raise Exception("bitmap doesn't appear to to be a valid bitmap image.")
    # Remove bitmap data: ID.
    data = data[2:]
    # Read and remove the bitmap size. This is more reliable than reading
    # the data size at offset 0x22.
    #
    size = unpack("<L", data[:4])[0]
    size -=  0x36   # Subtract size of bitmap header.
    size +=  0x0C   # Add size of BIFF header.
    data = data[4:]
    # Remove bitmap data: reserved, offset, header length.
    data = data[12:]
    # Read and remove the bitmap width and height. Verify the sizes.
    width, height = unpack("<LL", data[:8])
    data = data[8:]
    if (width > 0xFFFF):
        raise Exception("bitmap: largest image width supported is 65k.")
    if (height > 0xFFFF):
        raise Exception("bitmap: largest image height supported is 65k.")
    # Read and remove the bitmap planes and bpp data. Verify them.
    planes, bitcount = unpack("<HH", data[:4])
    data = data[4:]
    if (bitcount != 24):
        raise Exception("bitmap isn't a 24bit true color bitmap.")
    if (planes != 1):
        raise Exception("bitmap: only 1 plane supported in bitmap image.")
    # Read and remove the bitmap compression. Verify compression.
    compression = unpack("<L", data[:4])[0]
    data = data[4:]
    if (compression != 0):
        raise Exception("bitmap: compression not supported in bitmap image.")
    # Remove bitmap data: data size, hres, vres, colours, imp. colours.
    data = data[20:]
    # Add the BITMAPCOREHEADER data
    header = pack("<LHHHH", 0x000c, width, height, 0x01, 0x18)
    data = header + data
    return (width, height, size, data)


class ImDataBmpRecord(BiffRecord):
    _REC_ID = 0x007F

    def __init__(self, filename):
        """Insert a 24bit bitmap image in a worksheet. The main record required is
        IMDATA but it must be proceeded by a OBJ record to define its position.

        """
        BiffRecord.__init__(self)

        self.width, self.height, self.size, data = _process_bitmap(filename)
        # Write the IMDATA record to store the bitmap data
        cf = 0x09
        env = 0x01
        lcb = self.size
        self._rec_data = pack("<HHL", cf, env, lcb) + data



########NEW FILE########
__FILENAME__ = Cell
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


__rev_id__ = """$Id: Cell.py,v 1.3 2005/08/11 08:53:48 rvk Exp $"""


import struct
import BIFFRecords


class StrCell(object):
    __slots__ = ["__init__", "get_biff_data",
                "__parent", "__idx", "__xf_idx", "__sst_idx"]

    def __init__(self, parent, idx, xf_idx, sst_idx):
        self.__parent = parent
        self.__idx = idx
        self.__xf_idx = xf_idx
        self.__sst_idx = sst_idx

    def get_biff_data(self):
        return BIFFRecords.LabelSSTRecord(self.__parent.get_index(), self.__idx, self.__xf_idx, self.__sst_idx).get()


class BlankCell(object):
    __slots__ = ["__init__", "get_biff_data",
                "__parent", "__idx", "__xf_idx"]

    def __init__(self, parent, idx, xf_idx):
        self.__parent = parent
        self.__idx = idx
        self.__xf_idx = xf_idx

    def get_biff_data(self):
        return BIFFRecords.BlankRecord(self.__parent.get_index(), self.__idx, self.__xf_idx).get()


class MulBlankCell(object):
    __slots__ = ["__init__", "get_biff_data",
                "__parent", "__col1", "__col2", "__xf_idx"]

    def __init__(self, parent, col1, col2, xf_idx):
        self.__parent = parent
        self.__col1 = col1
        self.__col2 = col2
        self.__xf_idx = xf_idx

    def get_biff_data(self):
        return BIFFRecords.MulBlankRecord(self.__parent.get_index(), self.__col1, self.__col2, self.__xf_idx).get()


class NumberCell(object):
    __slots__ = ["__init__", "get_biff_data",
                "__parent", "__idx", "__xf_idx", "__number"]


    def __init__(self, parent, idx, xf_idx, number):
        self.__parent = parent
        self.__idx = idx
        self.__xf_idx = xf_idx
        self.__number = float(number)


    def get_biff_data(self):
        rk_encoded = 0

        packed = struct.pack('<d', self.__number)

        #print self.__number
        w0, w1, w2, w3 = struct.unpack('<4H', packed)
        if w0 == 0 and w1 == 0 and w2 & 0xFFFC == w2:
            # 34 lsb are 0
            #print "float RK"
            rk_encoded = (w3 << 16) | w2
            return BIFFRecords.RKRecord(self.__parent.get_index(), self.__idx, self.__xf_idx, rk_encoded).get()

        if abs(self.__number) < 0x40000000 and int(self.__number) == self.__number:
            #print "30-bit integer RK"
            rk_encoded = 2 | (int(self.__number) << 2)
            return BIFFRecords.RKRecord(self.__parent.get_index(), self.__idx, self.__xf_idx, rk_encoded).get()

        temp = self.__number*100
        packed100 = struct.pack('<d', temp)
        w0, w1, w2, w3 = struct.unpack('<4H', packed100)
        if w0 == 0 and w1 == 0 and w2 & 0xFFFC == w2:
            # 34 lsb are 0
            #print "float RK*100"
            rk_encoded = 1 | (w3 << 16) | w2
            return BIFFRecords.RKRecord(self.__parent.get_index(), self.__idx, self.__xf_idx, rk_encoded).get()

        if abs(temp) < 0x40000000 and int(temp) == temp:
            #print "30-bit integer RK*100"
            rk_encoded = 3 | (int(temp) << 2)
            return BIFFRecords.RKRecord(self.__parent.get_index(), self.__idx, self.__xf_idx, rk_encoded).get()

        #print "Number" 
        #print
        return BIFFRecords.NumberRecord(self.__parent.get_index(), self.__idx, self.__xf_idx, self.__number).get()


class MulNumberCell(object):
    __slots__ = ["__init__", "get_biff_data"]

    def __init__(self, parent, idx, xf_idx, sst_idx):
        self.__parent = parent
        self.__idx = idx
        self.__xf_idx = xf_idx
        self.__sst_idx = sst_idx


    def get_biff_data(self):
        raise Exception


class FormulaCell(object):
    __slots__ = ["__init__", "get_biff_data",
                "__parent", "__idx", "__xf_idx", "__frmla"]

    def __init__(self, parent, idx, xf_idx, frmla):
        self.__parent = parent
        self.__idx = idx
        self.__xf_idx = xf_idx
        self.__frmla = frmla


    def get_biff_data(self):
        return BIFFRecords.FormulaRecord(self.__parent.get_index(), self.__idx, self.__xf_idx, self.__frmla.rpn()).get()




########NEW FILE########
__FILENAME__ = Column
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


__rev_id__ = """$Id: Column.py,v 1.4 2005/07/20 07:24:11 rvk Exp $"""


from BIFFRecords import ColInfoRecord
from Deco import *
from Worksheet import Worksheet


class Column(object):
    @accepts(object, int, Worksheet)
    def __init__(self, indx, parent_sheet):
        self._index = indx
        self._parent = parent_sheet
        self._parent_wb = parent_sheet.get_parent()
        self._xf_index = 0x0F
        
        self.width = 0x0B92
        self.hidden = 0
        self.level = 0
        self.collapse = 0


    def get_biff_record(self):
        options =  (self.hidden & 0x01) << 0
        options |= (self.level & 0x07) << 8
        options |= (self.collapse & 0x01) << 12
        
        return ColInfoRecord(self._index, self._index, self.width, self._xf_index, options).get()
        
        
        

########NEW FILE########
__FILENAME__ = CompoundDoc
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


import sys
import struct



__rev_id__ = """$Id: CompoundDoc.py,v 1.7 2005/10/26 07:44:24 rvk Exp $"""

        
class Reader:
    def __init__(self, filename, dump = False):
        self.dump = dump
        self.STREAMS = {}

        doc = file(filename, 'rb').read()
        self.header, self.data = doc[0:512], doc[512:]
        del doc

        self.__build_header()
        self.__build_MSAT()
        self.__build_SAT()
        self.__build_directory()
        self.__build_short_sectors_data()
        
        if len(self.short_sectors_data) > 0:
            self.__build_SSAT()
        else:
            if self.dump and (self.total_ssat_sectors != 0 or self.ssat_start_sid != -2):
                print 'NOTE: header says that must be', self.total_ssat_sectors, 'short sectors'
                print 'NOTE: starting at', self.ssat_start_sid, 'sector'
                print 'NOTE: but file does not contains data in short sectors'
            self.ssat_start_sid = -2
            self.total_ssat_sectors = 0
            self.SSAT = [-2]

        for dentry in self.dir_entry_list[1:]:
            (did, 
             sz, name, 
             t, c, 
             did_left, did_right, did_root, 
             dentry_start_sid, 
             stream_size
            ) = dentry
            stream_data = ''
            if stream_size > 0:
                if stream_size >= self.min_stream_size:
                    args = (self.data, self.SAT, dentry_start_sid, self.sect_size)
                else:
                    args = (self.short_sectors_data, self.SSAT, dentry_start_sid, self.short_sect_size)
                stream_data = self.get_stream_data(*args)

            if name != '':
                # BAD IDEA: names may be equal. NEED use full paths...
                self.STREAMS[name] = stream_data

    
    def __build_header(self):
        self.doc_magic             = self.header[0:8]

        if self.doc_magic != '\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1':
            raise Exception, 'Not an OLE file.'

        self.file_uid              = self.header[8:24]
        self.rev_num               = self.header[24:26]
        self.ver_num               = self.header[26:28]
        self.byte_order            = self.header[28:30]
        self.log2_sect_size,       = struct.unpack('<H', self.header[30:32])
        self.log2_short_sect_size, = struct.unpack('<H', self.header[32:34])
        self.total_sat_sectors,    = struct.unpack('<L', self.header[44:48])
        self.dir_start_sid,        = struct.unpack('<l', self.header[48:52])
        self.min_stream_size,      = struct.unpack('<L', self.header[56:60])
        self.ssat_start_sid,       = struct.unpack('<l', self.header[60:64])
        self.total_ssat_sectors,   = struct.unpack('<L', self.header[64:68])
        self.msat_start_sid,       = struct.unpack('<l', self.header[68:72])
        self.total_msat_sectors,   = struct.unpack('<L', self.header[72:76])
         
        self.sect_size        = 1 << self.log2_sect_size
        self.short_sect_size  = 1 << self.log2_short_sect_size

        if self.dump:
            print 'file magic: '
            print_bin_data(self.doc_magic)

            print 'file uid: '
            print_bin_data(self.file_uid)

            print 'revision number: '
            print_bin_data(self.rev_num)
         
            print 'version number: '
            print_bin_data(self.ver_num)
            
            print 'byte order: '
            print_bin_data(self.byte_order)
            
            print 'sector size                                :', hex(self.sect_size), self.sect_size
            #print 'total sectors in file                      :', hex(self.total_sectors), self.total_sectors
            print 'short sector size                          :', hex(self.short_sect_size), self.short_sect_size
            print 'Total number of sectors used for the SAT   :', hex(self.total_sat_sectors), self.total_sat_sectors
            print 'SID of first sector of the directory stream:', hex(self.dir_start_sid), self.dir_start_sid
            print 'Minimum size of a standard stream          :', hex(self.min_stream_size), self.min_stream_size
            print 'SID of first sector of the SSAT            :', hex(self.ssat_start_sid), self.ssat_start_sid
            print 'Total number of sectors used for the SSAT  :', hex(self.total_ssat_sectors), self.total_ssat_sectors
            print 'SID of first additional sector of the MSAT :', hex(self.msat_start_sid), self.msat_start_sid
            print 'Total number of sectors used for the MSAT  :', hex(self.total_msat_sectors), self.total_msat_sectors


    def __build_MSAT(self):
        self.MSAT = list(struct.unpack('<109l', self.header[76:]))
        
        next = self.msat_start_sid
        while next > 0:
           msat_sector = struct.unpack('<128l', self.data[next*self.sect_size:(next+1)*self.sect_size])
           self.MSAT.extend(msat_sector[:127])
           next = msat_sector[-1]

        if self.dump:
            print 'MSAT (header part): \n', self.MSAT[:109]
            print 'additional MSAT sectors: \n', self.MSAT[109:]


    def __build_SAT(self):
        sat_stream = ''.join([self.data[i*self.sect_size:(i+1)*self.sect_size] for i in self.MSAT if i >= 0])

        sat_sids_count = len(sat_stream) >> 2
        self.SAT = struct.unpack('<%dl' % sat_sids_count, sat_stream) # SIDs tuple

        if self.dump:
            print 'SAT sid count:\n', sat_sids_count
            print 'SAT content:\n', self.SAT


    def __build_SSAT(self):
        ssat_stream = self.get_stream_data(self.data, self.SAT, self.ssat_start_sid, self.sect_size)

        ssids_count = len(ssat_stream) >> 2
        self.SSAT = struct.unpack('<%dl' % ssids_count, ssat_stream)

        if self.dump:
            print 'SSID count:', ssids_count
            print 'SSAT content:\n', self.SSAT


    def __build_directory(self):
        dir_stream = self.get_stream_data(self.data, self.SAT, self.dir_start_sid, self.sect_size)

        self.dir_entry_list = []

        i = 0
        while i < len(dir_stream):
            dentry = dir_stream[i:i+128] # 128 -- dir entry size
            i += 128
            
            did = len(self.dir_entry_list)
            sz, = struct.unpack('<H', dentry[64:66])
            if sz > 0 :
                name = dentry[0:sz-2].decode('utf_16_le', 'replace')
            else:
                name = u''
            t,  = struct.unpack('B', dentry[66])
            c,  = struct.unpack('B', dentry[67])
            did_left ,  = struct.unpack('<l', dentry[68:72])
            did_right ,  = struct.unpack('<l', dentry[72:76])
            did_root ,  = struct.unpack('<l', dentry[76:80])
            dentry_start_sid ,  = struct.unpack('<l', dentry[116:120])
            stream_size ,  = struct.unpack('<L', dentry[120:124])

            self.dir_entry_list.extend([(did, sz, name, t, c, 
                                            did_left, did_right, did_root, 
                                            dentry_start_sid, stream_size)]) 

        if self.dump:
            dentry_types = {
                0x00: 'Empty',
                0x01: 'User storage',
                0x02: 'User stream',
                0x03: 'LockBytes',
                0x04: 'Property',
                0x05: 'Root storage'
            }
            node_colours = {
                0x00: 'Red',
                0x01: 'Black'
            }
            print 'total directory entries:', len(self.dir_entry_list)

            for dentry in self.dir_entry_list:
                (did, sz, name, t, c, 
                 did_left, did_right, did_root, 
                 dentry_start_sid, stream_size) = dentry
                print 'DID', did
                print 'Size of the used area of the character buffer of the name:', sz
                print 'dir entry name:', repr(name)
                print 'type of entry:', t, dentry_types[t]
                print 'entry colour:', c, node_colours[c]
                print 'left child DID :', did_left
                print 'right child DID:', did_right
                print 'root DID       :', did_root
                print 'start SID       :', dentry_start_sid
                print 'stream size     :', stream_size
                if stream_size == 0:
                    print 'stream is empty'
                elif stream_size >= self.min_stream_size:
                    print 'stream stored as normal stream'
                else:
                    print 'stream stored as short-stream'

    
    def __build_short_sectors_data(self):
        (did, sz, name, t, c, 
         did_left, did_right, did_root, 
         dentry_start_sid, stream_size) = self.dir_entry_list[0]
        assert t == 0x05 # Short-Stream Container Stream (SSCS) resides in Root Storage
        if stream_size == 0:
            self.short_sectors_data = ''
        else:
            self.short_sectors_data = self.get_stream_data(self.data, self.SAT, dentry_start_sid, self.sect_size)


    def get_stream_data(self, data, SAT, start_sid, sect_size):
        sid = start_sid
        chunks = [(sid, sid)]
        stream_data = ''

        while SAT[sid] >= 0:
            next_in_chain = SAT[sid]
            last_chunk_start, last_chunk_finish = chunks[-1]
            if next_in_chain - last_chunk_finish <= 1:
                chunks[-1] = last_chunk_start, next_in_chain
            else:
                chunks.extend([(next_in_chain, next_in_chain)]) 
            sid = next_in_chain
        for s, f in chunks:
            stream_data += data[s*sect_size:(f+1)*sect_size]
        #print chunks
        return stream_data

        
def print_bin_data(data):
    i = 0
    while i < len(data):
        j = 0
        while (i < len(data)) and (j < 16):
            c = '0x%02X' % ord(data[i])
            sys.stdout.write(c)
            sys.stdout.write(' ')
            i += 1
            j += 1
        print
    if i == 0:
        print '<NO DATA>'



# This implementation writes only 'Root Entry', 'Workbook' streams
# and 2 empty streams for aligning directory stream on sector boundary
# 
# LAYOUT:
# 0         header
# 76                MSAT (1st part: 109 SID)
# 512       workbook stream
# ...       additional MSAT sectors if streams' size > about 7 Mb == (109*512 * 128)
# ...       SAT
# ...       directory stream
#
# NOTE: this layout is "ad hoc". It can be more general. RTFM

class XlsDoc:
    SECTOR_SIZE = 0x0200
    MIN_LIMIT   = 0x1000

    SID_FREE_SECTOR  = -1
    SID_END_OF_CHAIN = -2
    SID_USED_BY_SAT  = -3
    SID_USED_BY_MSAT = -4

    def __init__(self):
        #self.book_stream = ''                # padded
        self.book_stream_sect = []

        self.dir_stream = ''
        self.dir_stream_sect = []

        self.packed_SAT = ''
        self.SAT_sect = []

        self.packed_MSAT_1st = ''
        self.packed_MSAT_2nd = ''
        self.MSAT_sect_2nd = []

        self.header = ''

    def __build_directory(self): # align on sector boundary
        self.dir_stream = ''

        dentry_name      = '\x00'.join('Root Entry\x00') + '\x00'
        dentry_name_sz   = len(dentry_name)
        dentry_name_pad  = '\x00'*(64 - dentry_name_sz)
        dentry_type      = 0x05 # root storage
        dentry_colour    = 0x01 # black
        dentry_did_left  = -1
        dentry_did_right = -1
        dentry_did_root  = 1
        dentry_start_sid = -2
        dentry_stream_sz = 0

        self.dir_stream += struct.pack('<64s H 2B 3l 9L l L L',
           dentry_name + dentry_name_pad,
           dentry_name_sz,
           dentry_type,
           dentry_colour,
           dentry_did_left, 
           dentry_did_right,
           dentry_did_root,
           0, 0, 0, 0, 0, 0, 0, 0, 0,
           dentry_start_sid,
           dentry_stream_sz,
           0
        )

        dentry_name      = '\x00'.join('Workbook\x00') + '\x00'
        dentry_name_sz   = len(dentry_name)
        dentry_name_pad  = '\x00'*(64 - dentry_name_sz)
        dentry_type      = 0x02 # user stream
        dentry_colour    = 0x01 # black
        dentry_did_left  = -1
        dentry_did_right = -1
        dentry_did_root  = -1
        dentry_start_sid = 0     
        dentry_stream_sz = self.book_stream_len

        self.dir_stream += struct.pack('<64s H 2B 3l 9L l L L',
           dentry_name + dentry_name_pad,
           dentry_name_sz,
           dentry_type,
           dentry_colour,
           dentry_did_left, 
           dentry_did_right,
           dentry_did_root,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 
           dentry_start_sid,
           dentry_stream_sz,
           0
        )
        
        # padding
        dentry_name      = ''
        dentry_name_sz   = len(dentry_name)
        dentry_name_pad  = '\x00'*(64 - dentry_name_sz)
        dentry_type      = 0x00 # empty
        dentry_colour    = 0x01 # black
        dentry_did_left  = -1
        dentry_did_right = -1
        dentry_did_root  = -1
        dentry_start_sid = -2
        dentry_stream_sz = 0

        self.dir_stream += struct.pack('<64s H 2B 3l 9L l L L',
           dentry_name + dentry_name_pad,
           dentry_name_sz,
           dentry_type,
           dentry_colour,
           dentry_did_left, 
           dentry_did_right,
           dentry_did_root,
           0, 0, 0, 0, 0, 0, 0, 0, 0,
           dentry_start_sid,
           dentry_stream_sz,
           0
        ) * 2
    
    def __build_sat(self):
        # Build SAT
        book_sect_count = self.book_stream_len >> 9
        dir_sect_count  = len(self.dir_stream) >> 9
        
        total_sect_count     = book_sect_count + dir_sect_count
        SAT_sect_count       = 0
        MSAT_sect_count      = 0
        SAT_sect_count_limit = 109
        while total_sect_count > 128*SAT_sect_count or SAT_sect_count > SAT_sect_count_limit:
            SAT_sect_count   += 1
            total_sect_count += 1
            if SAT_sect_count > SAT_sect_count_limit:
                MSAT_sect_count      += 1
                total_sect_count     += 1
                SAT_sect_count_limit += 127


        SAT = [self.SID_FREE_SECTOR]*128*SAT_sect_count

        sect = 0
        while sect < book_sect_count - 1:
            self.book_stream_sect.append(sect)
            SAT[sect] = sect + 1
            sect += 1
        self.book_stream_sect.append(sect)
        SAT[sect] = self.SID_END_OF_CHAIN
        sect += 1

        while sect < book_sect_count + MSAT_sect_count:
            self.MSAT_sect_2nd.append(sect)
            SAT[sect] = self.SID_USED_BY_MSAT
            sect += 1

        while sect < book_sect_count + MSAT_sect_count + SAT_sect_count:
            self.SAT_sect.append(sect)            
            SAT[sect] = self.SID_USED_BY_SAT
            sect += 1

        while sect < book_sect_count + MSAT_sect_count + SAT_sect_count + dir_sect_count - 1:
            self.dir_stream_sect.append(sect)
            SAT[sect] = sect + 1
            sect += 1
        self.dir_stream_sect.append(sect)
        SAT[sect] = self.SID_END_OF_CHAIN
        sect += 1

        self.packed_SAT = struct.pack('<%dl' % (SAT_sect_count*128), *SAT)

        MSAT_1st = [self.SID_FREE_SECTOR]*109
        for i, SAT_sect_num in zip(range(0, 109), self.SAT_sect):
            MSAT_1st[i] = SAT_sect_num
        self.packed_MSAT_1st = struct.pack('<109l', *MSAT_1st)

        MSAT_2nd = [self.SID_FREE_SECTOR]*128*MSAT_sect_count
        if MSAT_sect_count > 0:
            MSAT_2nd[- 1] = self.SID_END_OF_CHAIN

        i = 109
        msat_sect = 0
        sid_num = 0
        while i < SAT_sect_count:
            if (sid_num + 1) % 128 == 0:
                #print 'link: ',
                msat_sect += 1
                if msat_sect < len(self.MSAT_sect_2nd):
                    MSAT_2nd[sid_num] = self.MSAT_sect_2nd[msat_sect]
            else:
                #print 'sid: ',
                MSAT_2nd[sid_num] = self.SAT_sect[i]
                i += 1
            #print sid_num, MSAT_2nd[sid_num]
            sid_num += 1

        self.packed_MSAT_2nd = struct.pack('<%dl' % (MSAT_sect_count*128), *MSAT_2nd)

        #print vars()
        #print zip(range(0, sect), SAT)
        #print self.book_stream_sect
        #print self.MSAT_sect_2nd
        #print MSAT_2nd
        #print self.SAT_sect
        #print self.dir_stream_sect


    def __build_header(self):
        doc_magic             = '\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1'
        file_uid              = '\x00'*16
        rev_num               = '\x3E\x00'
        ver_num               = '\x03\x00'
        byte_order            = '\xFE\xFF'
        log_sect_size         = struct.pack('<H', 9)
        log_short_sect_size   = struct.pack('<H', 6)
        not_used0             = '\x00'*10
        total_sat_sectors     = struct.pack('<L', len(self.SAT_sect))
        dir_start_sid         = struct.pack('<l', self.dir_stream_sect[0])
        not_used1             = '\x00'*4        
        min_stream_size       = struct.pack('<L', 0x1000)
        ssat_start_sid        = struct.pack('<l', -2)
        total_ssat_sectors    = struct.pack('<L', 0)

        if len(self.MSAT_sect_2nd) == 0:
            msat_start_sid        = struct.pack('<l', -2)
        else:
            msat_start_sid        = struct.pack('<l', self.MSAT_sect_2nd[0])

        total_msat_sectors    = struct.pack('<L', len(self.MSAT_sect_2nd))

        self.header =       ''.join([  doc_magic,
                                        file_uid,
                                        rev_num,
                                        ver_num,
                                        byte_order,
                                        log_sect_size,
                                        log_short_sect_size,
                                        not_used0,
                                        total_sat_sectors,
                                        dir_start_sid,
                                        not_used1,
                                        min_stream_size,
                                        ssat_start_sid,
                                        total_ssat_sectors,
                                        msat_start_sid,
                                        total_msat_sectors
                                    ])
                                        

    def save(self, filename, stream):
        # 1. Align stream on 0x1000 boundary (and therefore on sector boundary)
        padding = '\x00' * (0x1000 - (len(stream) % 0x1000))
        self.book_stream_len = len(stream) + len(padding)

        self.__build_directory()
        self.__build_sat()
        self.__build_header()
        
        f = file(filename, 'wb')
        f.write(self.header)
        f.write(self.packed_MSAT_1st)
        f.write(stream)
        f.write(padding)
        f.write(self.packed_MSAT_2nd)
        f.write(self.packed_SAT)
        f.write(self.dir_stream)
        f.close()


if __name__ == '__main__':
    d = XlsDoc()
    d.save('a.aaa', 'b'*17000)






########NEW FILE########
__FILENAME__ = Deco
# This code from 

# PEP: 318 
# Title: Decorators for Functions and Methods 
# Version: 1.35 
# Last-Modified: 2004/09/14 07:34:23 
# Author: Kevin D. Smith, Jim Jewett, Skip Montanaro, Anthony Baxter 

# This code fixes error in example 4: 
# authors' code contains
#       @accepts(int, (int,float))
#       @returns((int,float))
#       def func(arg1, arg2):
#           return arg1 * arg2
# 
# It should be:
#       @returns((int,float))
#       @accepts(int, (int,float))
#       def func(arg1, arg2):
#           return arg1 * arg2
# because of decorators are applied from bottom to up.


__rev_id__ = """$Id: Deco.py,v 1.4 2005/07/20 07:24:11 rvk Exp $"""


def accepts(*types):
    #print types
    def check_accepts(f):
        assert len(types) == f.func_code.co_argcount
        def new_f(*args, **kwds):
            for (a, t) in zip(args, types):
                assert isinstance(a, t), \
                       "arg %r does not match %s" % (a,t)
            return f(*args, **kwds)
        new_f.func_name = f.func_name
        return new_f
    return check_accepts

def returns(rtype):
    def check_returns(f):
        def new_f(*args, **kwds):
            result = f(*args, **kwds)
            assert isinstance(result, rtype), \
                   "return value %r does not match %s" % (result,rtype)
            return result
        new_f.func_name = f.func_name
        return new_f
    return check_returns


if __name__ == '__main__':
    import types 

    @returns(types.NoneType)
    @accepts(int, (int,float))
    def func(arg1, arg2):
        #return str(arg1 * arg2)
        pass

    func(1, 2)      

########NEW FILE########
__FILENAME__ = ExcelFormula
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


__rev_id__ = """$Id: ExcelFormula.py,v 1.3 2005/08/11 08:53:48 rvk Exp $"""


import ExcelFormulaParser, ExcelFormulaLexer
import struct
from antlr import ANTLRException


class Formula(object):
    __slots__ = ["__init__", "text", "rpn", "__s", "__parser"]


    def __init__(self, s):
        try:
            self.__s = s
            lexer = ExcelFormulaLexer.Lexer(s)
            self.__parser = ExcelFormulaParser.Parser(lexer)
            self.__parser.formula()
        except ANTLRException:
            raise Exception, "can't parse formula " + s

    def text(self):
        return self.__s

    def rpn(self):
        '''
        Offset    Size    Contents
        0         2       Size of the following formula data (sz)
        2         sz      Formula data (RPN token array)
        [2+sz]    var.    (optional) Additional data for specific tokens

        '''
        return struct.pack("<H", len(self.__parser.rpn)) + self.__parser.rpn


########NEW FILE########
__FILENAME__ = ExcelFormulaLexer
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
#
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
#
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
#
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
#
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
#
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


__rev_id__ = """$Id: ExcelFormulaLexer.py,v 1.4 2005/08/14 06:40:23 rvk Exp $"""


import sys
from antlr import EOF, CommonToken as Tok, TokenStream, TokenStreamException
import struct
import ExcelFormulaParser
from re import compile as recompile, match, LOCALE, UNICODE, IGNORECASE


int_const_pattern = recompile(r"\d+")
flt_const_pattern = recompile(r"\d*\.\d+(?:[Ee][+-]?\d+)?")
str_const_pattern = recompile(r'["][^"]*["]')
#range2d_pattern   = recompile(r"\$?[A-I]?[A-Z]\$?\d+:\$?[A-I]?[A-Z]\$?\d+")
ref2d_pattern     = recompile(r"\$?[A-I]?[A-Z]\$?\d+")
true_pattern      = recompile(r"TRUE", IGNORECASE)
false_pattern     = recompile(r"FALSE", IGNORECASE)
name_pattern      = recompile(r"[\.\w]+", LOCALE)

pattern_type_tuples = (
    (flt_const_pattern, ExcelFormulaParser.NUM_CONST),
    (int_const_pattern, ExcelFormulaParser.INT_CONST),
    (str_const_pattern, ExcelFormulaParser.STR_CONST),
#    (range2d_pattern  , ExcelFormulaParser.RANGE2D),
    (ref2d_pattern    , ExcelFormulaParser.REF2D),
    (true_pattern     , ExcelFormulaParser.TRUE_CONST),
    (false_pattern    , ExcelFormulaParser.FALSE_CONST),
    (name_pattern     , ExcelFormulaParser.NAME)
)


type_text_tuples = (
    (ExcelFormulaParser.NE, '<>'),
    (ExcelFormulaParser.LE, '<='),
    (ExcelFormulaParser.GE, '>='),
    (ExcelFormulaParser.EQ, '='),
    (ExcelFormulaParser.LT, '<'),
    (ExcelFormulaParser.GT, '>'),
    (ExcelFormulaParser.ADD, '+'),
    (ExcelFormulaParser.SUB, '-'),
    (ExcelFormulaParser.MUL, '*'),
    (ExcelFormulaParser.DIV, '/'),
    (ExcelFormulaParser.COLON, ':'),
    (ExcelFormulaParser.SEMICOLON, ';'),
    (ExcelFormulaParser.COMMA, ','),
    (ExcelFormulaParser.LP, '('),
    (ExcelFormulaParser.RP, ')'),
    (ExcelFormulaParser.CONCAT, '&'),
    (ExcelFormulaParser.PERCENT, '%'),
    (ExcelFormulaParser.POWER, '^')
)


class Lexer(TokenStream):
    def __init__(self, text):
        self._text = text[:]
        self._pos = 0
        self._line = 0


    def isEOF(self):
        return len(self._text) <= self._pos


    def curr_ch(self):
        return self._text[self._pos]


    def next_ch(self, n = 1):
        self._pos += n


    def is_whitespace(self):
        return self.curr_ch() in " \t\n\r\f\v"


    def match_pattern(self, pattern, toktype):
        m = pattern.match(self._text[self._pos:])
        if m:
            start_pos = self._pos + m.start(0)
            end_pos = self._pos + m.end(0)
            tt = self._text[start_pos:end_pos]
            self._pos = end_pos
            return Tok(type = toktype, text = tt, col = start_pos + 1)
        else:
            return None


    def nextToken(self):
        # skip whitespace
        while not self.isEOF() and self.is_whitespace():
            self.next_ch()
        if self.isEOF():
            return Tok(type = EOF)
        # first, try to match token with more chars
        for ptt in pattern_type_tuples:
            t = self.match_pattern(*ptt);
            if t:
                return t
        # second, we want find short tokens
        for ty, te in type_text_tuples:
            if self.curr_ch() == te:
                self.next_ch()
                return Tok(type = ty, text = te, col = self._pos)
        # at this point, smth strange is happened
        raise TokenStreamException("Unknown char %s at %u col." % (self.curr_ch(), self._pos))


if __name__ == '__main__' :
    import locale
    locale.setlocale(locale.LC_ALL, 'russian')
    try:
        for t in Lexer('1+2+3+67.8678 + " @##$$$ klhkh kljhklhkl " + .58e-678*A1:B4 - 1lkjljlkjl3535'):
            print t
    except TokenStreamException, e:
        print "error:", e

########NEW FILE########
__FILENAME__ = ExcelFormulaParser
### $ANTLR 2.7.5 (20050128): "excel-formula.g" -> "ExcelFormulaParser.py"$
### import antlr and other modules ..
import sys
import antlr

version = sys.version.split()[0]
if version < '2.2.1':
    False = 0
if version < '2.3':
    True = not False
### header action >>> 
__rev_id__ = """$Id: ExcelFormulaParser.py,v 1.4 2005/08/14 06:40:23 rvk Exp $"""

import struct
import Utils
from UnicodeUtils import upack1
from ExcelMagic import *
### header action <<< 
### preamble action>>>

### preamble action <<<

### import antlr.Token 
from antlr import Token
### >>>The Known Token Types <<<
SKIP                = antlr.SKIP
INVALID_TYPE        = antlr.INVALID_TYPE
EOF_TYPE            = antlr.EOF_TYPE
EOF                 = antlr.EOF
NULL_TREE_LOOKAHEAD = antlr.NULL_TREE_LOOKAHEAD
MIN_USER_TYPE       = antlr.MIN_USER_TYPE
TRUE_CONST = 4
FALSE_CONST = 5
STR_CONST = 6
NUM_CONST = 7
INT_CONST = 8
NAME = 9
EQ = 10
NE = 11
GT = 12
LT = 13
GE = 14
LE = 15
ADD = 16
SUB = 17
MUL = 18
DIV = 19
POWER = 20
PERCENT = 21
LP = 22
RP = 23
LB = 24
RB = 25
COLON = 26
COMMA = 27
SEMICOLON = 28
CONCAT = 29
REF2D = 30

class Parser(antlr.LLkParser):
    ### user action >>>
    ### user action <<<
    
    def __init__(self, *args, **kwargs):
        antlr.LLkParser.__init__(self, *args, **kwargs)
        self.tokenNames = _tokenNames
        ### __init__ header action >>> 
        self.rpn = ""
        ### __init__ header action <<< 
        
    def formula(self):    
        
        pass
        self.expr("V")
    
    def expr(self,
        arg_type
    ):    
        
        pass
        self.prec0_expr(arg_type)
        while True:
            if ((self.LA(1) >= EQ and self.LA(1) <= LE)):
                pass
                la1 = self.LA(1)
                if False:
                    pass
                elif la1 and la1 in [EQ]:
                    pass
                    self.match(EQ)
                    op = struct.pack('B', ptgEQ)
                elif la1 and la1 in [NE]:
                    pass
                    self.match(NE)
                    op = struct.pack('B', ptgNE)
                elif la1 and la1 in [GT]:
                    pass
                    self.match(GT)
                    op = struct.pack('B', ptgGE)
                elif la1 and la1 in [LT]:
                    pass
                    self.match(LT)
                    op = struct.pack('B', ptgLT)
                elif la1 and la1 in [GE]:
                    pass
                    self.match(GE)
                    op = struct.pack('B', ptgGE)
                elif la1 and la1 in [LE]:
                    pass
                    self.match(LE)
                    op = struct.pack('B', ptgLE)
                else:
                        raise antlr.NoViableAltException(self.LT(1), self.getFilename())
                    
                self.prec0_expr(arg_type)
                self.rpn += op
            else:
                break
            
    
    def prec0_expr(self,
        arg_type
    ):    
        
        pass
        self.prec1_expr(arg_type)
        while True:
            if (self.LA(1)==CONCAT):
                pass
                pass
                self.match(CONCAT)
                op = struct.pack('B', ptgConcat)
                self.prec1_expr(arg_type)
                self.rpn += op
            else:
                break
            
    
    def prec1_expr(self,
        arg_type
    ):    
        
        pass
        self.prec2_expr(arg_type)
        while True:
            if (self.LA(1)==ADD or self.LA(1)==SUB):
                pass
                la1 = self.LA(1)
                if False:
                    pass
                elif la1 and la1 in [ADD]:
                    pass
                    self.match(ADD)
                    op = struct.pack('B', ptgAdd)
                elif la1 and la1 in [SUB]:
                    pass
                    self.match(SUB)
                    op = struct.pack('B', ptgSub)
                else:
                        raise antlr.NoViableAltException(self.LT(1), self.getFilename())
                    
                self.prec2_expr(arg_type)
                self.rpn += op
            else:
                break
            
    
    def prec2_expr(self,
        arg_type
    ):    
        
        pass
        self.prec3_expr(arg_type)
        while True:
            if (self.LA(1)==MUL or self.LA(1)==DIV):
                pass
                la1 = self.LA(1)
                if False:
                    pass
                elif la1 and la1 in [MUL]:
                    pass
                    self.match(MUL)
                    op = struct.pack('B', ptgMul)
                elif la1 and la1 in [DIV]:
                    pass
                    self.match(DIV)
                    op = struct.pack('B', ptgDiv)
                else:
                        raise antlr.NoViableAltException(self.LT(1), self.getFilename())
                    
                self.prec3_expr(arg_type)
                self.rpn += op
            else:
                break
            
    
    def prec3_expr(self,
        arg_type
    ):    
        
        pass
        self.prec4_expr(arg_type)
        while True:
            if (self.LA(1)==POWER):
                pass
                pass
                self.match(POWER)
                op = struct.pack('B', ptgPower)
                self.prec4_expr(arg_type)
                self.rpn += op
            else:
                break
            
    
    def prec4_expr(self,
        arg_type
    ):    
        
        pass
        self.prec5_expr(arg_type)
        la1 = self.LA(1)
        if False:
            pass
        elif la1 and la1 in [PERCENT]:
            pass
            self.match(PERCENT)
            self.rpn += struct.pack('B', ptgPercent)
        elif la1 and la1 in [EOF,EQ,NE,GT,LT,GE,LE,ADD,SUB,MUL,DIV,POWER,RP,SEMICOLON,CONCAT]:
            pass
        else:
                raise antlr.NoViableAltException(self.LT(1), self.getFilename())
            
    
    def prec5_expr(self,
        arg_type
    ):    
        
        la1 = self.LA(1)
        if False:
            pass
        elif la1 and la1 in [TRUE_CONST,FALSE_CONST,STR_CONST,NUM_CONST,INT_CONST,NAME,LP,REF2D]:
            pass
            self.primary(arg_type)
        elif la1 and la1 in [SUB]:
            pass
            self.match(SUB)
            self.primary(arg_type)
            self.rpn += struct.pack('B', ptgUminus)
        else:
                raise antlr.NoViableAltException(self.LT(1), self.getFilename())
            
    
    def primary(self,
        arg_type
    ):    
        
        str_tok = None
        int_tok = None
        num_tok = None
        ref2d_tok = None
        ref2d1_tok = None
        ref2d2_tok = None
        name_tok = None
        func_tok = None
        la1 = self.LA(1)
        if False:
            pass
        elif la1 and la1 in [TRUE_CONST]:
            pass
            self.match(TRUE_CONST)
            self.rpn += struct.pack("2B", ptgBool, 1)
        elif la1 and la1 in [FALSE_CONST]:
            pass
            self.match(FALSE_CONST)
            self.rpn += struct.pack("2B", ptgBool, 0)
        elif la1 and la1 in [STR_CONST]:
            pass
            str_tok = self.LT(1)
            self.match(STR_CONST)
            self.rpn += struct.pack("B", ptgStr) + upack1(str_tok.text[1:-1])
        elif la1 and la1 in [INT_CONST]:
            pass
            int_tok = self.LT(1)
            self.match(INT_CONST)
            self.rpn += struct.pack("<BH", ptgInt, int(int_tok.text))
        elif la1 and la1 in [NUM_CONST]:
            pass
            num_tok = self.LT(1)
            self.match(NUM_CONST)
            self.rpn += struct.pack("<Bd", ptgNum, float(num_tok.text))
        elif la1 and la1 in [LP]:
            pass
            self.match(LP)
            self.expr(arg_type)
            self.match(RP)
            self.rpn += struct.pack("B", ptgParen)
        else:
            if (self.LA(1)==REF2D) and (_tokenSet_0.member(self.LA(2))):
                pass
                ref2d_tok = self.LT(1)
                self.match(REF2D)
                r, c = Utils.cell_to_packed_rowcol(ref2d_tok.text)
                if arg_type == "R":
                   self.rpn += struct.pack("<B2H", ptgRefR, r, c)
                else:
                   self.rpn += struct.pack("<B2H", ptgRefV, r, c)
            elif (self.LA(1)==REF2D) and (self.LA(2)==COLON):
                pass
                ref2d1_tok = self.LT(1)
                self.match(REF2D)
                self.match(COLON)
                ref2d2_tok = self.LT(1)
                self.match(REF2D)
                r1, c1 = Utils.cell_to_packed_rowcol(ref2d1_tok.text)
                r2, c2 = Utils.cell_to_packed_rowcol(ref2d2_tok.text)
                if arg_type == "R":
                   self.rpn += struct.pack("<B4H", ptgAreaR, r1, r2, c1, c2)
                else:
                   self.rpn += struct.pack("<B4H", ptgAreaV, r1, r2, c1, c2)
            elif (self.LA(1)==NAME) and (_tokenSet_0.member(self.LA(2))):
                pass
                name_tok = self.LT(1)
                self.match(NAME)
                self.rpn += ""
            elif (self.LA(1)==NAME) and (self.LA(2)==LP):
                pass
                func_tok = self.LT(1)
                self.match(NAME)
                if func_tok.text.upper() in std_func_by_name:
                   (opcode,
                   min_argc,
                   max_argc,
                   func_type,
                   arg_type_list,
                   volatile_func) = std_func_by_name[func_tok.text.upper()]
                else:
                   raise Exception, "unknown function: %s" % func_tok.text
                self.match(LP)
                arg_count=self.expr_list(arg_type_list, min_argc, max_argc)
                self.match(RP)
                if arg_count > max_argc or arg_count < min_argc:
                   raise Exception, "%d parameters for function: %s" % (arg_count, func_tok.text)
                if min_argc == max_argc:
                   if func_type == "V":
                       func_ptg = ptgFuncV
                   elif func_type == "R":
                       func_ptg = ptgFuncR
                   elif func_type == "A":
                       func_ptg = ptgFuncA
                   else:
                       raise Exception, "wrong function type"
                   self.rpn += struct.pack("<BH", func_ptg, opcode)
                else:
                   if func_type == "V":
                       func_ptg = ptgFuncVarV
                   elif func_type == "R":
                       func_ptg = ptgFuncVarR
                   elif func_type == "A":
                       func_ptg = ptgFuncVarA
                   else:
                       raise Exception, "wrong function type"
                   self.rpn += struct.pack("<2BH", func_ptg, arg_count, opcode)
            else:
                raise antlr.NoViableAltException(self.LT(1), self.getFilename())
            
    
    def expr_list(self,
        arg_type_list, min_argc, max_argc
    ):    
        arg_cnt = None
        
        arg_cnt = 0
        arg_type_list = arg_type_list.split()
        arg_type = arg_type_list[arg_cnt]
        la1 = self.LA(1)
        if False:
            pass
        elif la1 and la1 in [TRUE_CONST,FALSE_CONST,STR_CONST,NUM_CONST,INT_CONST,NAME,SUB,LP,REF2D]:
            pass
            self.expr(arg_type)
            arg_cnt += 1
            while True:
                if (self.LA(1)==SEMICOLON):
                    pass
                    if arg_cnt < len(arg_type_list):
                       arg_type = arg_type_list[arg_cnt]
                    else:
                       arg_type = arg_type_list[-1]
                    if arg_type == "...":
                       arg_type = arg_type_list[-2]
                    self.match(SEMICOLON)
                    la1 = self.LA(1)
                    if False:
                        pass
                    elif la1 and la1 in [TRUE_CONST,FALSE_CONST,STR_CONST,NUM_CONST,INT_CONST,NAME,SUB,LP,REF2D]:
                        pass
                        self.expr(arg_type)
                    elif la1 and la1 in [RP,SEMICOLON]:
                        pass
                        self.rpn += struct.pack("B", ptgMissArg)
                    else:
                            raise antlr.NoViableAltException(self.LT(1), self.getFilename())
                        
                    arg_cnt += 1
                else:
                    break
                
        elif la1 and la1 in [RP]:
            pass
        else:
                raise antlr.NoViableAltException(self.LT(1), self.getFilename())
            
        return arg_cnt
    

_tokenNames = [
    "<0>", 
    "EOF", 
    "<2>", 
    "NULL_TREE_LOOKAHEAD", 
    "TRUE_CONST", 
    "FALSE_CONST", 
    "STR_CONST", 
    "NUM_CONST", 
    "INT_CONST", 
    "NAME", 
    "EQ", 
    "NE", 
    "GT", 
    "LT", 
    "GE", 
    "LE", 
    "ADD", 
    "SUB", 
    "MUL", 
    "DIV", 
    "POWER", 
    "PERCENT", 
    "LP", 
    "RP", 
    "LB", 
    "RB", 
    "COLON", 
    "COMMA", 
    "SEMICOLON", 
    "CONCAT", 
    "REF2D"
]
    

### generate bit set
def mk_tokenSet_0(): 
    ### var1
    data = [ 817888258L, 0L]
    return data
_tokenSet_0 = antlr.BitSet(mk_tokenSet_0())
    

########NEW FILE########
__FILENAME__ = ExcelMagic
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
#
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
#
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
#
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
#
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
#
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


__rev_id__ = """$Id: ExcelMagic.py,v 1.2 2005/10/26 07:44:24 rvk Exp $"""


"""
lots of Excel Magic Numbers
"""

# Boundaries BIFF8+

MAX_ROW = 65536
MAX_COL = 256


biff_records = {
    0x0000: "DIMENSIONS",
    0x0001: "BLANK",
    0x0002: "INTEGER",
    0x0003: "NUMBER",
    0x0004: "LABEL",
    0x0005: "BOOLERR",
    0x0006: "FORMULA",
    0x0007: "STRING",
    0x0008: "ROW",
    0x0009: "BOF",
    0x000A: "EOF",
    0x000B: "INDEX",
    0x000C: "CALCCOUNT",
    0x000D: "CALCMODE",
    0x000E: "PRECISION",
    0x000F: "REFMODE",
    0x0010: "DELTA",
    0x0011: "ITERATION",
    0x0012: "PROTECT",
    0x0013: "PASSWORD",
    0x0014: "HEADER",
    0x0015: "FOOTER",
    0x0016: "EXTERNCOUNT",
    0x0017: "EXTERNSHEET",
    0x0018: "NAME",
    0x0019: "WINDOWPROTECT",
    0x001A: "VERTICALPAGEBREAKS",
    0x001B: "HORIZONTALPAGEBREAKS",
    0x001C: "NOTE",
    0x001D: "SELECTION",
    0x001E: "FORMAT",
    0x001F: "FORMATCOUNT",
    0x0020: "COLUMNDEFAULT",
    0x0021: "ARRAY",
    0x0022: "1904",
    0x0023: "EXTERNNAME",
    0x0024: "COLWIDTH",
    0x0025: "DEFAULTROWHEIGHT",
    0x0026: "LEFTMARGIN",
    0x0027: "RIGHTMARGIN",
    0x0028: "TOPMARGIN",
    0x0029: "BOTTOMMARGIN",
    0x002A: "PRINTHEADERS",
    0x002B: "PRINTGRIDLINES",
    0x002F: "FILEPASS",
    0x0031: "FONT",
    0x0036: "TABLE",
    0x003C: "CONTINUE",
    0x003D: "WINDOW1",
    0x003E: "WINDOW2",
    0x0040: "BACKUP",
    0x0041: "PANE",
    0x0042: "CODEPAGE",
    0x0043: "XF",
    0x0044: "IXFE",
    0x0045: "EFONT",
    0x004D: "PLS",
    0x0050: "DCON",
    0x0051: "DCONREF",
    0x0053: "DCONNAME",
    0x0055: "DEFCOLWIDTH",
    0x0056: "BUILTINFMTCNT",
    0x0059: "XCT",
    0x005A: "CRN",
    0x005B: "FILESHARING",
    0x005C: "WRITEACCESS",
    0x005D: "OBJ",
    0x005E: "UNCALCED",
    0x005F: "SAFERECALC",
    0x0060: "TEMPLATE",
    0x0063: "OBJPROTECT",
    0x007D: "COLINFO",
    0x007E: "RK",
    0x007F: "IMDATA",
    0x0080: "GUTS",
    0x0081: "WSBOOL",
    0x0082: "GRIDSET",
    0x0083: "HCENTER",
    0x0084: "VCENTER",
    0x0085: "BOUNDSHEET",
    0x0086: "WRITEPROT",
    0x0087: "ADDIN",
    0x0088: "EDG",
    0x0089: "PUB",
    0x008C: "COUNTRY",
    0x008D: "HIDEOBJ",
    0x008E: "BUNDLESOFFSET",
    0x008F: "BUNDLEHEADER",
    0x0090: "SORT",
    0x0091: "SUB",
    0x0092: "PALETTE",
    0x0093: "STYLE",
    0x0094: "LHRECORD",
    0x0095: "LHNGRAPH",
    0x0096: "SOUND",
    0x0098: "LPR",
    0x0099: "STANDARDWIDTH",
    0x009A: "FNGROUPNAME",
    0x009B: "FILTERMODE",
    0x009C: "FNGROUPCOUNT",
    0x009D: "AUTOFILTERINFO",
    0x009E: "AUTOFILTER",
    0x00A0: "SCL",
    0x00A1: "SETUP",
    0x00A9: "COORDLIST",
    0x00AB: "GCW",
    0x00AE: "SCENMAN",
    0x00AF: "SCENARIO",
    0x00B0: "SXVIEW",
    0x00B1: "SXVD",
    0x00B2: "SXVI",
    0x00B4: "SXIVD",
    0x00B5: "SXLI",
    0x00B6: "SXPI",
    0x00B8: "DOCROUTE",
    0x00B9: "RECIPNAME",
    0x00BC: "SHRFMLA",
    0x00BD: "MULRK",
    0x00BE: "MULBLANK",
    0x00C1: "MMS",
    0x00C2: "ADDMENU",
    0x00C3: "DELMENU",
    0x00C5: "SXDI",
    0x00C6: "SXDB",
    0x00C7: "SXFIELD",
    0x00C8: "SXINDEXLIST",
    0x00C9: "SXDOUBLE",
    0x00CD: "SXSTRING",
    0x00CE: "SXDATETIME",
    0x00D0: "SXTBL",
    0x00D1: "SXTBRGITEM",
    0x00D2: "SXTBPG",
    0x00D3: "OBPROJ",
    0x00D5: "SXIDSTM",
    0x00D6: "RSTRING",
    0x00D7: "DBCELL",
    0x00DA: "BOOKBOOL",
    0x00DC: "SXEXT|PARAMQRY",
    0x00DD: "SCENPROTECT",
    0x00DE: "OLESIZE",
    0x00DF: "UDDESC",
    0x00E0: "XF",
    0x00E1: "INTERFACEHDR",
    0x00E2: "INTERFACEEND",
    0x00E3: "SXVS",
    0x00E5: "MERGEDCELLS",
    0x00E9: "BITMAP",
    0x00EB: "MSODRAWINGGROUP",
    0x00EC: "MSODRAWING",
    0x00ED: "MSODRAWINGSELECTION",
    0x00F0: "SXRULE",
    0x00F1: "SXEX",
    0x00F2: "SXFILT",
    0x00F6: "SXNAME",
    0x00F7: "SXSELECT",
    0x00F8: "SXPAIR",
    0x00F9: "SXFMLA",
    0x00FB: "SXFORMAT",
    0x00FC: "SST",
    0x00FD: "LABELSST",
    0x00FF: "EXTSST",
    0x0100: "SXVDEX",
    0x0103: "SXFORMULA",
    0x0122: "SXDBEX",
    0x0137: "CHTRINSERT",
    0x0138: "CHTRINFO",
    0x013B: "CHTRCELLCONTENT",
    0x013D: "TABID",
    0x0140: "CHTRMOVERANGE",
    0x014D: "CHTRINSERTTAB",
    0x015F: "LABELRANGES",
    0x0160: "USESELFS",
    0x0161: "DSF",
    0x0162: "XL5MODIFY",
    0x0196: "CHTRHEADER",
    0x01A9: "USERBVIEW",
    0x01AA: "USERSVIEWBEGIN",
    0x01AB: "USERSVIEWEND",
    0x01AD: "QSI",
    0x01AE: "SUPBOOK",
    0x01AF: "PROT4REV",
    0x01B0: "CONDFMT",
    0x01B1: "CF",
    0x01B2: "DVAL",
    0x01B5: "DCONBIN",
    0x01B6: "TXO",
    0x01B7: "REFRESHALL",
    0x01B8: "HLINK",
    0x01BA: "CODENAME",
    0x01BB: "SXFDBTYPE",
    0x01BC: "PROT4REVPASS",
    0x01BE: "DV",
    0x01C0: "XL9FILE",
    0x01C1: "RECALCID",
    0x0200: "DIMENSIONS",
    0x0201: "BLANK",
    0x0203: "NUMBER",
    0x0204: "LABEL",
    0x0205: "BOOLERR",
    0x0206: "FORMULA",
    0x0207: "STRING",
    0x0208: "ROW",
    0x0209: "BOF",
    0x020B: "INDEX",
    0x0218: "NAME",
    0x0221: "ARRAY",
    0x0223: "EXTERNNAME",
    0x0225: "DEFAULTROWHEIGHT",
    0x0231: "FONT",
    0x0236: "TABLE",
    0x023E: "WINDOW2",
    0x0243: "XF",
    0x027E: "RK",
    0x0293: "STYLE",
    0x0406: "FORMULA",
    0x0409: "BOF",
    0x041E: "FORMAT",
    0x0443: "XF",
    0x04BC: "SHRFMLA",
    0x0800: "SCREENTIP",
    0x0803: "WEBQRYSETTINGS",
    0x0804: "WEBQRYTABLES",
    0x0809: "BOF",
    0x0862: "SHEETLAYOUT",
    0x0867: "SHEETPROTECTION",
    0x1001: "UNITS",
    0x1002: "ChartChart",
    0x1003: "ChartSeries",
    0x1006: "ChartDataformat",
    0x1007: "ChartLineformat",
    0x1009: "ChartMarkerformat",
    0x100A: "ChartAreaformat",
    0x100B: "ChartPieformat",
    0x100C: "ChartAttachedlabel",
    0x100D: "ChartSeriestext",
    0x1014: "ChartChartformat",
    0x1015: "ChartLegend",
    0x1016: "ChartSerieslist",
    0x1017: "ChartBar",
    0x1018: "ChartLine",
    0x1019: "ChartPie",
    0x101A: "ChartArea",
    0x101B: "ChartScatter",
    0x101C: "ChartChartline",
    0x101D: "ChartAxis",
    0x101E: "ChartTick",
    0x101F: "ChartValuerange",
    0x1020: "ChartCatserrange",
    0x1021: "ChartAxislineformat",
    0x1022: "ChartFormatlink",
    0x1024: "ChartDefaulttext",
    0x1025: "ChartText",
    0x1026: "ChartFontx",
    0x1027: "ChartObjectLink",
    0x1032: "ChartFrame",
    0x1033: "BEGIN",
    0x1034: "END",
    0x1035: "ChartPlotarea",
    0x103A: "Chart3D",
    0x103C: "ChartPicf",
    0x103D: "ChartDropbar",
    0x103E: "ChartRadar",
    0x103F: "ChartSurface",
    0x1040: "ChartRadararea",
    0x1041: "ChartAxisparent",
    0x1043: "ChartLegendxn",
    0x1044: "ChartShtprops",
    0x1045: "ChartSertocrt",
    0x1046: "ChartAxesused",
    0x1048: "ChartSbaseref",
    0x104A: "ChartSerparent",
    0x104B: "ChartSerauxtrend",
    0x104E: "ChartIfmt",
    0x104F: "ChartPos",
    0x1050: "ChartAlruns",
    0x1051: "ChartAI",
    0x105B: "ChartSerauxerrbar",
    0x105D: "ChartSerfmt",
    0x105F: "Chart3DDataFormat",
    0x1060: "ChartFbi",
    0x1061: "ChartBoppop",
    0x1062: "ChartAxcext",
    0x1063: "ChartDat",
    0x1064: "ChartPlotgrowth",
    0x1065: "ChartSiindex",
    0x1066: "ChartGelframe",
    0x1067: "ChartBoppcustom",
    0xFFFF: ""
}


std_func_by_name = {
             "ABS": (0x018,  1,  1,   "V",             "V", False), # 1
            "ACOS": (0x063,  1,  1,   "V",             "V", False), # 2
           "ACOSH": (0x0e9,  1,  1,   "V",             "V", False), # 3
         "ADDRESS": (0x0db,  2,  5,   "V",     "V V V V V", False), # 4
             "AND": (0x024,  1, 30,   "V",         "R ...", False), # 5
          "ARCTAN": (0x012,  1,  1,   "V",             "V", False), # 6
           "AREAS": (0x04b,  1,  1,   "V",             "R", False), # 7
             "ASC": (0x0d6,  1,  1,   "V",             "V", False), # 8
            "ASIN": (0x062,  1,  1,   "V",             "V", False), # 9
           "ASINH": (0x0e8,  1,  1,   "V",             "V", False), # 10
           "ATAN2": (0x061,  2,  2,   "V",           "V V", False), # 11
           "ATANH": (0x0ea,  1,  1,   "V",             "V", False), # 12
          "AVEDEV": (0x10d,  1, 30,   "V",         "R ...", False), # 13
         "AVERAGE": (0x005,  1, 30,   "V",         "R ...", False), # 14
        "AVERAGEA": (0x169,  1, 30,   "V",         "R ...", False), # 15
        "BETADIST": (0x10e,  3,  5,   "V",     "V V V V V", False), # 16
         "BETAINV": (0x110,  3,  5,   "V",     "V V V V V", False), # 17
       "BINOMDIST": (0x111,  4,  4,   "V",       "V V V V", False), # 18
         "CEILING": (0x120,  2,  2,   "V",           "V V", False), # 19
            "CELL": (0x07d,  1,  2,   "V",           "V R",  True), # 20
            "CHAR": (0x06f,  1,  1,   "V",             "V", False), # 21
         "CHIDIST": (0x112,  2,  2,   "V",           "V V", False), # 22
          "CHIINV": (0x113,  2,  2,   "V",           "V V", False), # 23
         "CHITEST": (0x132,  2,  2,   "V",           "A A", False), # 24
          "CHOOSE": (0x064,  2, 30,   "R",       "V R ...", False), # 25
           "CLEAN": (0x0a2,  1,  1,   "V",             "V", False), # 26
            "CODE": (0x079,  1,  1,   "V",             "V", False), # 27
          "COLUMN": (0x009,  0,  1,   "V",             "R", False), # 28
         "COLUMNS": (0x04d,  1,  1,   "V",             "R", False), # 29
          "COMBIN": (0x114,  2,  2,   "V",           "V V", False), # 30
     "CONCATENATE": (0x150,  0, 30,   "V",         "V ...", False), # 31
      "CONFIDENCE": (0x115,  3,  3,   "V",         "V V V", False), # 32
          "CORREL": (0x133,  2,  2,   "V",           "A A", False), # 33
             "COS": (0x010,  1,  1,   "V",             "V", False), # 34
            "COSH": (0x0e6,  1,  1,   "V",             "V", False), # 35
           "COUNT": (0x000,  0, 30,   "V",         "R ...", False), # 36
          "COUNTA": (0x0a9,  0, 30,   "V",         "R ...", False), # 37
      "COUNTBLANK": (0x15b,  1,  1,   "V",             "R", False), # 38
         "COUNTIF": (0x15a,  2,  2,   "V",           "R V", False), # 39
           "COVAR": (0x134,  2,  2,   "V",           "A A", False), # 40
       "CRITBINOM": (0x116,  3,  3,   "V",         "V V V", False), # 41
            "DATE": (0x041,  3,  3,   "V",         "V V V", False), # 42
         "DATEDIF": (0x15f,  3,  3,   "V",         "V V V", False), # 43
      "DATESTRING": (0x160,  1,  1,   "V",             "V", False), # 44
       "DATEVALUE": (0x08c,  1,  1,   "V",             "V", False), # 45
        "DAVERAGE": (0x02a,  3,  3,   "V",         "R R R", False), # 46
             "DAY": (0x043,  1,  1,   "V",             "V", False), # 47
         "DAYS360": (0x0dc,  2,  3,   "V",         "V V V", False), # 48
              "DB": (0x0f7,  4,  5,   "V",     "V V V V V", False), # 49
            "DBSC": (0x0d7,  1,  1,   "V",             "V", False), # 50
          "DCOUNT": (0x028,  3,  3,   "V",         "R R R", False), # 51
         "DCOUNTA": (0x0c7,  3,  3,   "V",         "R R R", False), # 52
             "DDB": (0x090,  4,  5,   "V",     "V V V V V", False), # 53
         "DEGREES": (0x157,  1,  1,   "V",             "V", False), # 54
           "DEVSQ": (0x13e,  1, 30,   "V",         "R ...", False), # 55
            "DGET": (0x0eb,  3,  3,   "V",         "R R R", False), # 56
            "DMAX": (0x02c,  3,  3,   "V",         "R R R", False), # 57
            "DMIN": (0x02b,  3,  3,   "V",         "R R R", False), # 58
          "DOLLAR": (0x00d,  1,  2,   "V",           "V V", False), # 59
        "DPRODUCT": (0x0bf,  3,  3,   "V",         "R R R", False), # 60
          "DSTDEV": (0x02d,  3,  3,   "V",         "R R R", False), # 61
         "DSTDEVP": (0x0c3,  3,  3,   "V",         "R R R", False), # 62
            "DSUM": (0x029,  3,  3,   "V",         "R R R", False), # 63
            "DVAR": (0x02f,  3,  3,   "V",         "R R R", False), # 64
           "DVARP": (0x0c4,  3,  3,   "V",         "R R R", False), # 65
      "ERROR.TYPE": (0x105,  1,  1,   "V",             "V", False), # 66
            "EVEN": (0x117,  1,  1,   "V",             "V", False), # 67
           "EXACT": (0x075,  2,  2,   "V",           "V V", False), # 68
             "EXP": (0x015,  1,  1,   "V",             "V", False), # 69
       "EXPONDIST": (0x118,  3,  3,   "V",         "V V V", False), # 70
            "FACT": (0x0b8,  1,  1,   "V",             "V", False), # 71
           "FALSE": (0x023,  0,  0,   "V",             "-", False), # 72
           "FDIST": (0x119,  3,  3,   "V",         "V V V", False), # 73
            "FIND": (0x07c,  2,  3,   "V",         "V V V", False), # 74
           "FINDB": (0x0cd,  2,  3,   "V",         "V V V", False), # 75
            "FINV": (0x11a,  3,  3,   "V",         "V V V", False), # 76
          "FISHER": (0x11b,  1,  1,   "V",             "V", False), # 77
       "FISHERINV": (0x11c,  1,  1,   "V",             "V", False), # 78
           "FIXED": (0x00e,  2,  3,   "V",         "V V V", False), # 79
           "FLOOR": (0x11d,  2,  2,   "V",           "V V", False), # 80
        "FORECAST": (0x135,  3,  3,   "V",         "V A A", False), # 81
       "FREQUENCY": (0x0fc,  2,  2,   "A",           "R R", False), # 82
           "FTEST": (0x136,  2,  2,   "V",           "A A", False), # 83
              "FV": (0x039,  3,  5,   "V",     "V V V V V", False), # 84
       "GAMMADIST": (0x11e,  4,  4,   "V",       "V V V V", False), # 85
        "GAMMAINV": (0x11f,  3,  3,   "V",         "V V V", False), # 86
         "GAMMALN": (0x10f,  1,  1,   "V",             "V", False), # 87
         "GEOMEAN": (0x13f,  1, 30,   "V",         "R ...", False), # 88
    "GETPIVOTDATA": (0x166,  2, 30,   "A",             "-", False), # 89
          "GROWTH": (0x034,  1,  4,   "A",       "R R R V", False), # 90
         "HARMEAN": (0x140,  1, 30,   "V",         "R ...", False), # 91
         "HLOOKUP": (0x065,  3,  4,   "V",       "V R R V", False), # 92
            "HOUR": (0x047,  1,  1,   "V",             "V", False), # 93
       "HYPERLINK": (0x167,  1,  2,   "V",           "V V", False), # 94
     "HYPGEOMVERT": (0x121,  4,  4,   "V",       "V V V V", False), # 95
              "IF": (0x001,  2,  3,   "R",         "V R R", False), # 96
           "INDEX": (0x01d,  2,  4,   "R",       "R V V V", False), # 97
        "INDIRECT": (0x094,  1,  2,   "R",           "V V",  True), # 98
            "INFO": (0x0f4,  1,  1,   "V",             "V", False), # 99
             "INT": (0x019,  1,  1,   "V",             "V", False), # 100
       "INTERCEPT": (0x137,  2,  2,   "V",           "A A", False), # 101
            "IPMT": (0x0a7,  4,  6,   "V",   "V V V V V V", False), # 102
             "IRR": (0x03e,  1,  2,   "V",           "R V", False), # 103
         "ISBLANK": (0x081,  1,  1,   "V",             "V", False), # 104
           "ISERR": (0x07e,  1,  1,   "V",             "V", False), # 105
         "ISERROR": (0x003,  1,  1,   "V",             "V", False), # 106
       "ISLOGICAL": (0x0c6,  1,  1,   "V",             "V", False), # 107
            "ISNA": (0x002,  1,  1,   "V",             "V", False), # 108
       "ISNONTEXT": (0x0c0,  1,  1,   "V",             "V", False), # 109
        "ISNUMBER": (0x080,  1,  1,   "V",             "V", False), # 110
           "ISPMT": (0x15e,  4,  4,   "V",       "V V V V", False), # 111
           "ISREF": (0x069,  1,  1,   "V",             "R", False), # 112
          "ISTEXT": (0x07f,  1,  1,   "V",             "V", False), # 113
            "KURT": (0x142,  1, 30,   "V",         "R ...", False), # 114
           "LARGE": (0x145,  2,  2,   "V",           "R V", False), # 115
            "LEFT": (0x073,  1,  2,   "V",           "V V", False), # 116
           "LEFTB": (0x0d0,  1,  2,   "V",           "V V", False), # 117
             "LEN": (0x020,  1,  1,   "V",             "V", False), # 118
            "LENB": (0x0d3,  1,  1,   "V",             "V", False), # 119
          "LINEST": (0x031,  1,  4,   "A",       "R R V V", False), # 120
              "LN": (0x016,  1,  1,   "V",             "V", False), # 121
             "LOG": (0x06d,  1,  2,   "V",           "V V", False), # 122
           "LOG10": (0x017,  1,  1,   "V",             "V", False), # 123
          "LOGEST": (0x033,  1,  4,   "A",       "R R V V", False), # 124
          "LOGINV": (0x123,  3,  3,   "V",         "V V V", False), # 125
     "LOGNORMDIST": (0x122,  3,  3,   "V",         "V V V", False), # 126
          "LOOKUP": (0x01c,  2,  3,   "V",         "V R R", False), # 127
           "LOWER": (0x070,  1,  1,   "V",             "V", False), # 128
           "MATCH": (0x040,  2,  3,   "V",         "V R R", False), # 129
             "MAX": (0x007,  1, 30,   "V",         "R ...", False), # 130
            "MAXA": (0x16a,  1, 30,   "V",         "R ...", False), # 131
         "MDETERM": (0x0a3,  1,  1,   "V",             "A", False), # 132
          "MEDIAN": (0x0e3,  1, 30,   "V",         "R ...", False), # 133
             "MID": (0x01f,  3,  3,   "V",         "V V V", False), # 134
            "MIDB": (0x0d2,  3,  3,   "V",         "V V V", False), # 135
             "MIN": (0x006,  1, 30,   "V",         "R ...", False), # 136
            "MINA": (0x16b,  1, 30,   "V",         "R ...", False), # 137
          "MINUTE": (0x048,  1,  1,   "V",             "V", False), # 138
        "MINVERSE": (0x0a4,  1,  1,   "A",             "A", False), # 139
            "MIRR": (0x03d,  3,  3,   "V",         "R V V", False), # 140
           "MMULT": (0x0a5,  2,  2,   "A",           "A A", False), # 141
       "MNORMSINV": (0x128,  1,  1,   "V",             "V", False), # 142
             "MOD": (0x027,  2,  2,   "V",           "V V", False), # 143
            "MODE": (0x14a,  1, 30,   "V",         "A ...", False), # 144
           "MONTH": (0x044,  1,  1,   "V",             "V", False), # 145
               "N": (0x083,  1,  1,   "V",             "R", False), # 146
              "NA": (0x00a,  0,  0,   "V",             "-", False), # 147
    "NEGBINOMDIST": (0x124,  3,  3,   "V",         "V V V", False), # 148
        "NORMDIST": (0x125,  4,  4,   "V",       "V V V V", False), # 149
         "NORMINV": (0x127,  3,  3,   "V",         "V V V", False), # 150
       "NORMSDIST": (0x126,  1,  1,   "V",             "V", False), # 151
             "NOT": (0x026,  1,  1,   "V",             "V", False), # 152
             "NOW": (0x04a,  0,  0,   "V",             "-",  True), # 153
            "NPER": (0x03a,  3,  5,   "V",     "V V V V V", False), # 154
             "NPV": (0x00b,  2, 30,   "V",       "V R ...", False), # 155
    "NUMBERSTRING": (0x161,  2,  2,   "V",           "V V", False), # 156
             "ODD": (0x12a,  1,  1,   "V",             "V", False), # 157
          "OFFSET": (0x04e,  3,  5,   "R",     "R V V V V",  True), # 158
              "OR": (0x025,  1, 30,   "V",         "R ...", False), # 159
         "PEARSON": (0x138,  2,  2,   "V",           "A A", False), # 160
      "PERCENTILE": (0x148,  2,  2,   "V",           "R V", False), # 161
     "PERCENTRANK": (0x149,  2,  3,   "V",         "R V V", False), # 162
          "PERMUT": (0x12b,  2,  2,   "V",           "V V", False), # 163
        "PHONETIC": (0x168,  1,  1,   "V",             "R", False), # 164
              "PI": (0x013,  0,  0,   "V",             "-", False), # 165
             "PMT": (0x03b,  3,  5,   "V",     "V V V V V", False), # 166
         "POISSON": (0x12c,  3,  3,   "V",         "V V V", False), # 167
           "POWER": (0x151,  2,  2,   "V",           "V V", False), # 168
            "PPMT": (0x0a8,  4,  6,   "V",   "V V V V V V", False), # 169
            "PROB": (0x13d,  3,  4,   "V",       "A A V V", False), # 170
         "PRODUCT": (0x0b7,  0, 30,   "V",         "R ...", False), # 171
          "PROPER": (0x072,  1,  1,   "V",             "V", False), # 172
              "PV": (0x038,  3,  5,   "V",     "V V V V V", False), # 173
        "QUARTILE": (0x147,  2,  2,   "V",           "R V", False), # 174
         "RADIANS": (0x156,  1,  1,   "V",             "V", False), # 175
            "RAND": (0x03f,  0,  0,   "V",             "-",  True), # 176
            "RANK": (0x0d8,  2,  3,   "V",         "V R V", False), # 177
            "RATE": (0x03c,  3,  6,   "V",   "V V V V V V", False), # 178
         "REPLACE": (0x077,  4,  4,   "V",       "V V V V", False), # 179
        "REPLACEB": (0x0cf,  4,  4,   "V",       "V V V V", False), # 180
            "REPT": (0x01e,  2,  2,   "V",           "V V", False), # 181
           "RIGHT": (0x074,  1,  2,   "V",           "V V", False), # 182
          "RIGHTB": (0x0d1,  1,  2,   "V",           "V V", False), # 183
           "ROMAN": (0x162,  1,  2,   "V",           "V V", False), # 184
           "ROUND": (0x01b,  2,  2,   "V",           "V V", False), # 185
       "ROUNDDOWN": (0x0d5,  2,  2,   "V",           "V V", False), # 186
         "ROUNDUP": (0x0d4,  2,  2,   "V",           "V V", False), # 187
             "ROW": (0x008,  0,  1,   "V",             "R", False), # 188
            "ROWS": (0x04c,  1,  1,   "V",             "R", False), # 189
             "RSQ": (0x139,  2,  2,   "V",           "A A", False), # 190
          "SEARCH": (0x052,  2,  3,   "V",         "V V V", False), # 191
         "SEARCHB": (0x0ce,  2,  3,   "V",         "V V V", False), # 192
          "SECOND": (0x049,  1,  1,   "V",             "V", False), # 193
            "SIGN": (0x01a,  1,  1,   "V",             "V", False), # 194
             "SIN": (0x00f,  1,  1,   "V",             "V", False), # 195
            "SINH": (0x0e5,  1,  1,   "V",             "V", False), # 196
            "SKEW": (0x143,  1, 30,   "V",         "R ...", False), # 197
             "SLN": (0x08e,  3,  3,   "V",         "V V V", False), # 198
           "SLOPE": (0x13b,  2,  2,   "V",           "A A", False), # 199
           "SMALL": (0x146,  2,  2,   "V",           "R V", False), # 200
            "SQRT": (0x014,  1,  1,   "V",             "V", False), # 201
     "STANDARDIZE": (0x129,  3,  3,   "V",         "V V V", False), # 202
           "STDEV": (0x00c,  1, 30,   "V",         "R ...", False), # 203
          "STDEVA": (0x16e,  1, 30,   "V",         "R ...", False), # 204
          "STDEVP": (0x0c1,  1, 30,   "V",         "R ...", False), # 205
         "STDEVPA": (0x16c,  1, 30,   "V",         "R ...", False), # 206
           "STEYX": (0x13a,  2,  2,   "V",           "A A", False), # 207
      "SUBSTITUTE": (0x078,  3,  4,   "V",       "V V V V", False), # 208
        "SUBTOTAL": (0x158,  2, 30,   "V",       "V R ...", False), # 209
             "SUM": (0x004,  0, 30,   "V",         "R ...", False), # 210
           "SUMIF": (0x159,  2,  3,   "V",         "R V R", False), # 211
      "SUMPRODUCT": (0x0e4,  1, 30,   "V",         "A ...", False), # 212
           "SUMSQ": (0x141,  0, 30,   "V",         "R ...", False), # 213
        "SUMX2MY2": (0x130,  2,  2,   "V",           "A A", False), # 214
        "SUMX2PY2": (0x131,  2,  2,   "V",           "A A", False), # 215
         "SUMXMY2": (0x12f,  2,  2,   "V",           "A A", False), # 216
             "SYD": (0x08f,  4,  4,   "V",       "V V V V", False), # 217
               "T": (0x082,  1,  1,   "V",             "R", False), # 218
             "TAN": (0x011,  1,  1,   "V",             "V", False), # 219
            "TANH": (0x0e7,  1,  1,   "V",             "V", False), # 220
           "TDIST": (0x12d,  3,  3,   "V",         "V V V", False), # 221
            "TEXT": (0x030,  2,  2,   "V",           "V V", False), # 222
            "TIME": (0x042,  3,  3,   "V",         "V V V", False), # 223
       "TIMEVALUE": (0x08d,  1,  1,   "V",             "V", False), # 224
            "TINV": (0x14c,  2,  2,   "V",           "V V", False), # 225
           "TODAY": (0x0dd,  0,  0,   "V",             "-",  True), # 226
       "TRANSPOSE": (0x053,  1,  1,   "A",             "A", False), # 227
           "TREND": (0x032,  1,  4,   "A",       "R R R V", False), # 228
            "TRIM": (0x076,  1,  1,   "V",             "V", False), # 229
        "TRIMMEAN": (0x14b,  2,  2,   "V",           "R V", False), # 230
            "TRUE": (0x022,  0,  0,   "V",             "-", False), # 231
           "TRUNC": (0x0c5,  1,  2,   "V",           "V V", False), # 232
           "TTEST": (0x13c,  4,  4,   "V",       "A A V V", False), # 233
            "TYPE": (0x056,  1,  1,   "V",             "V", False), # 234
           "UPPER": (0x071,  1,  1,   "V",             "V", False), # 235
        "USDOLLAR": (0x0cc,  1,  2,   "V",           "V V", False), # 236
           "VALUE": (0x021,  1,  1,   "V",             "V", False), # 237
             "VAR": (0x02e,  1, 30,   "V",         "R ...", False), # 238
            "VARA": (0x16f,  1, 30,   "V",         "R ...", False), # 239
            "VARP": (0x0c2,  1, 30,   "V",         "R ...", False), # 240
           "VARPA": (0x16d,  1, 30,   "V",         "R ...", False), # 241
             "VDB": (0x0de,  5,  7,   "V", "V V V V V V V", False), # 242
         "VLOOKUP": (0x066,  3,  4,   "V",       "V R R V", False), # 243
         "WEEKDAY": (0x046,  1,  2,   "V",           "V V", False), # 244
         "WEIBULL": (0x12e,  4,  4,   "V",       "V V V V", False), # 245
            "YEAR": (0x045,  1,  1,   "V",             "V", False), # 246
           "ZTEST": (0x144,  2,  3,   "V",         "R V V", False)  # 247
}


std_func_by_num = {
    0x000: (       "COUNT",  0, 30,   "V",         "R ...", False), # 1
    0x001: (          "IF",  2,  3,   "R",         "V R R", False), # 2
    0x002: (        "ISNA",  1,  1,   "V",             "V", False), # 3
    0x003: (     "ISERROR",  1,  1,   "V",             "V", False), # 4
    0x004: (         "SUM",  0, 30,   "V",         "R ...", False), # 5
    0x005: (     "AVERAGE",  1, 30,   "V",         "R ...", False), # 6
    0x006: (         "MIN",  1, 30,   "V",         "R ...", False), # 7
    0x007: (         "MAX",  1, 30,   "V",         "R ...", False), # 8
    0x008: (         "ROW",  0,  1,   "V",             "R", False), # 9
    0x009: (      "COLUMN",  0,  1,   "V",             "R", False), # 10
    0x00a: (          "NA",  0,  0,   "V",             "-", False), # 11
    0x00b: (         "NPV",  2, 30,   "V",       "V R ...", False), # 12
    0x00c: (       "STDEV",  1, 30,   "V",         "R ...", False), # 13
    0x00d: (      "DOLLAR",  1,  2,   "V",           "V V", False), # 14
    0x00e: (       "FIXED",  2,  3,   "V",         "V V V", False), # 15
    0x00f: (         "SIN",  1,  1,   "V",             "V", False), # 16
    0x010: (         "COS",  1,  1,   "V",             "V", False), # 17
    0x011: (         "TAN",  1,  1,   "V",             "V", False), # 18
    0x012: (      "ARCTAN",  1,  1,   "V",             "V", False), # 19
    0x013: (          "PI",  0,  0,   "V",             "-", False), # 20
    0x014: (        "SQRT",  1,  1,   "V",             "V", False), # 21
    0x015: (         "EXP",  1,  1,   "V",             "V", False), # 22
    0x016: (          "LN",  1,  1,   "V",             "V", False), # 23
    0x017: (       "LOG10",  1,  1,   "V",             "V", False), # 24
    0x018: (         "ABS",  1,  1,   "V",             "V", False), # 25
    0x019: (         "INT",  1,  1,   "V",             "V", False), # 26
    0x01a: (        "SIGN",  1,  1,   "V",             "V", False), # 27
    0x01b: (       "ROUND",  2,  2,   "V",           "V V", False), # 28
    0x01c: (      "LOOKUP",  2,  3,   "V",         "V R R", False), # 29
    0x01d: (       "INDEX",  2,  4,   "R",       "R V V V", False), # 30
    0x01e: (        "REPT",  2,  2,   "V",           "V V", False), # 31
    0x01f: (         "MID",  3,  3,   "V",         "V V V", False), # 32
    0x020: (         "LEN",  1,  1,   "V",             "V", False), # 33
    0x021: (       "VALUE",  1,  1,   "V",             "V", False), # 34
    0x022: (        "TRUE",  0,  0,   "V",             "-", False), # 35
    0x023: (       "FALSE",  0,  0,   "V",             "-", False), # 36
    0x024: (         "AND",  1, 30,   "V",         "R ...", False), # 37
    0x025: (          "OR",  1, 30,   "V",         "R ...", False), # 38
    0x026: (         "NOT",  1,  1,   "V",             "V", False), # 39
    0x027: (         "MOD",  2,  2,   "V",           "V V", False), # 40
    0x028: (      "DCOUNT",  3,  3,   "V",         "R R R", False), # 41
    0x029: (        "DSUM",  3,  3,   "V",         "R R R", False), # 42
    0x02a: (    "DAVERAGE",  3,  3,   "V",         "R R R", False), # 43
    0x02b: (        "DMIN",  3,  3,   "V",         "R R R", False), # 44
    0x02c: (        "DMAX",  3,  3,   "V",         "R R R", False), # 45
    0x02d: (      "DSTDEV",  3,  3,   "V",         "R R R", False), # 46
    0x02e: (         "VAR",  1, 30,   "V",         "R ...", False), # 47
    0x02f: (        "DVAR",  3,  3,   "V",         "R R R", False), # 48
    0x030: (        "TEXT",  2,  2,   "V",           "V V", False), # 49
    0x031: (      "LINEST",  1,  4,   "A",       "R R V V", False), # 50
    0x032: (       "TREND",  1,  4,   "A",       "R R R V", False), # 51
    0x033: (      "LOGEST",  1,  4,   "A",       "R R V V", False), # 52
    0x034: (      "GROWTH",  1,  4,   "A",       "R R R V", False), # 53
    0x038: (          "PV",  3,  5,   "V",     "V V V V V", False), # 54
    0x039: (          "FV",  3,  5,   "V",     "V V V V V", False), # 55
    0x03a: (        "NPER",  3,  5,   "V",     "V V V V V", False), # 56
    0x03b: (         "PMT",  3,  5,   "V",     "V V V V V", False), # 57
    0x03c: (        "RATE",  3,  6,   "V",   "V V V V V V", False), # 58
    0x03d: (        "MIRR",  3,  3,   "V",         "R V V", False), # 59
    0x03e: (         "IRR",  1,  2,   "V",           "R V", False), # 60
    0x03f: (        "RAND",  0,  0,   "V",             "-",  True), # 61
    0x040: (       "MATCH",  2,  3,   "V",         "V R R", False), # 62
    0x041: (        "DATE",  3,  3,   "V",         "V V V", False), # 63
    0x042: (        "TIME",  3,  3,   "V",         "V V V", False), # 64
    0x043: (         "DAY",  1,  1,   "V",             "V", False), # 65
    0x044: (       "MONTH",  1,  1,   "V",             "V", False), # 66
    0x045: (        "YEAR",  1,  1,   "V",             "V", False), # 67
    0x046: (     "WEEKDAY",  1,  2,   "V",           "V V", False), # 68
    0x047: (        "HOUR",  1,  1,   "V",             "V", False), # 69
    0x048: (      "MINUTE",  1,  1,   "V",             "V", False), # 70
    0x049: (      "SECOND",  1,  1,   "V",             "V", False), # 71
    0x04a: (         "NOW",  0,  0,   "V",             "-",  True), # 72
    0x04b: (       "AREAS",  1,  1,   "V",             "R", False), # 73
    0x04c: (        "ROWS",  1,  1,   "V",             "R", False), # 74
    0x04d: (     "COLUMNS",  1,  1,   "V",             "R", False), # 75
    0x04e: (      "OFFSET",  3,  5,   "R",     "R V V V V",  True), # 76
    0x052: (      "SEARCH",  2,  3,   "V",         "V V V", False), # 77
    0x053: (   "TRANSPOSE",  1,  1,   "A",             "A", False), # 78
    0x056: (        "TYPE",  1,  1,   "V",             "V", False), # 79
    0x061: (       "ATAN2",  2,  2,   "V",           "V V", False), # 80
    0x062: (        "ASIN",  1,  1,   "V",             "V", False), # 81
    0x063: (        "ACOS",  1,  1,   "V",             "V", False), # 82
    0x064: (      "CHOOSE",  2, 30,   "R",       "V R ...", False), # 83
    0x065: (     "HLOOKUP",  3,  4,   "V",       "V R R V", False), # 84
    0x066: (     "VLOOKUP",  3,  4,   "V",       "V R R V", False), # 85
    0x069: (       "ISREF",  1,  1,   "V",             "R", False), # 86
    0x06d: (         "LOG",  1,  2,   "V",           "V V", False), # 87
    0x06f: (        "CHAR",  1,  1,   "V",             "V", False), # 88
    0x070: (       "LOWER",  1,  1,   "V",             "V", False), # 89
    0x071: (       "UPPER",  1,  1,   "V",             "V", False), # 90
    0x072: (      "PROPER",  1,  1,   "V",             "V", False), # 91
    0x073: (        "LEFT",  1,  2,   "V",           "V V", False), # 92
    0x074: (       "RIGHT",  1,  2,   "V",           "V V", False), # 93
    0x075: (       "EXACT",  2,  2,   "V",           "V V", False), # 94
    0x076: (        "TRIM",  1,  1,   "V",             "V", False), # 95
    0x077: (     "REPLACE",  4,  4,   "V",       "V V V V", False), # 96
    0x078: (  "SUBSTITUTE",  3,  4,   "V",       "V V V V", False), # 97
    0x079: (        "CODE",  1,  1,   "V",             "V", False), # 98
    0x07c: (        "FIND",  2,  3,   "V",         "V V V", False), # 99
    0x07d: (        "CELL",  1,  2,   "V",           "V R",  True), # 100
    0x07e: (       "ISERR",  1,  1,   "V",             "V", False), # 101
    0x07f: (      "ISTEXT",  1,  1,   "V",             "V", False), # 102
    0x080: (    "ISNUMBER",  1,  1,   "V",             "V", False), # 103
    0x081: (     "ISBLANK",  1,  1,   "V",             "V", False), # 104
    0x082: (           "T",  1,  1,   "V",             "R", False), # 105
    0x083: (           "N",  1,  1,   "V",             "R", False), # 106
    0x08c: (   "DATEVALUE",  1,  1,   "V",             "V", False), # 107
    0x08d: (   "TIMEVALUE",  1,  1,   "V",             "V", False), # 108
    0x08e: (         "SLN",  3,  3,   "V",         "V V V", False), # 109
    0x08f: (         "SYD",  4,  4,   "V",       "V V V V", False), # 110
    0x090: (         "DDB",  4,  5,   "V",     "V V V V V", False), # 111
    0x094: (    "INDIRECT",  1,  2,   "R",           "V V",  True), # 112
    0x0a2: (       "CLEAN",  1,  1,   "V",             "V", False), # 113
    0x0a3: (     "MDETERM",  1,  1,   "V",             "A", False), # 114
    0x0a4: (    "MINVERSE",  1,  1,   "A",             "A", False), # 115
    0x0a5: (       "MMULT",  2,  2,   "A",           "A A", False), # 116
    0x0a7: (        "IPMT",  4,  6,   "V",   "V V V V V V", False), # 117
    0x0a8: (        "PPMT",  4,  6,   "V",   "V V V V V V", False), # 118
    0x0a9: (      "COUNTA",  0, 30,   "V",         "R ...", False), # 119
    0x0b7: (     "PRODUCT",  0, 30,   "V",         "R ...", False), # 120
    0x0b8: (        "FACT",  1,  1,   "V",             "V", False), # 121
    0x0bf: (    "DPRODUCT",  3,  3,   "V",         "R R R", False), # 122
    0x0c0: (   "ISNONTEXT",  1,  1,   "V",             "V", False), # 123
    0x0c1: (      "STDEVP",  1, 30,   "V",         "R ...", False), # 124
    0x0c2: (        "VARP",  1, 30,   "V",         "R ...", False), # 125
    0x0c3: (     "DSTDEVP",  3,  3,   "V",         "R R R", False), # 126
    0x0c4: (       "DVARP",  3,  3,   "V",         "R R R", False), # 127
    0x0c5: (       "TRUNC",  1,  2,   "V",           "V V", False), # 128
    0x0c6: (   "ISLOGICAL",  1,  1,   "V",             "V", False), # 129
    0x0c7: (     "DCOUNTA",  3,  3,   "V",         "R R R", False), # 130
    0x0cc: (    "USDOLLAR",  1,  2,   "V",           "V V", False), # 131
    0x0cd: (       "FINDB",  2,  3,   "V",         "V V V", False), # 132
    0x0ce: (     "SEARCHB",  2,  3,   "V",         "V V V", False), # 133
    0x0cf: (    "REPLACEB",  4,  4,   "V",       "V V V V", False), # 134
    0x0d0: (       "LEFTB",  1,  2,   "V",           "V V", False), # 135
    0x0d1: (      "RIGHTB",  1,  2,   "V",           "V V", False), # 136
    0x0d2: (        "MIDB",  3,  3,   "V",         "V V V", False), # 137
    0x0d3: (        "LENB",  1,  1,   "V",             "V", False), # 138
    0x0d4: (     "ROUNDUP",  2,  2,   "V",           "V V", False), # 139
    0x0d5: (   "ROUNDDOWN",  2,  2,   "V",           "V V", False), # 140
    0x0d6: (         "ASC",  1,  1,   "V",             "V", False), # 141
    0x0d7: (        "DBSC",  1,  1,   "V",             "V", False), # 142
    0x0d8: (        "RANK",  2,  3,   "V",         "V R V", False), # 143
    0x0db: (     "ADDRESS",  2,  5,   "V",     "V V V V V", False), # 144
    0x0dc: (     "DAYS360",  2,  3,   "V",         "V V V", False), # 145
    0x0dd: (       "TODAY",  0,  0,   "V",             "-",  True), # 146
    0x0de: (         "VDB",  5,  7,   "V", "V V V V V V V", False), # 147
    0x0e3: (      "MEDIAN",  1, 30,   "V",         "R ...", False), # 148
    0x0e4: (  "SUMPRODUCT",  1, 30,   "V",         "A ...", False), # 149
    0x0e5: (        "SINH",  1,  1,   "V",             "V", False), # 150
    0x0e6: (        "COSH",  1,  1,   "V",             "V", False), # 151
    0x0e7: (        "TANH",  1,  1,   "V",             "V", False), # 152
    0x0e8: (       "ASINH",  1,  1,   "V",             "V", False), # 153
    0x0e9: (       "ACOSH",  1,  1,   "V",             "V", False), # 154
    0x0ea: (       "ATANH",  1,  1,   "V",             "V", False), # 155
    0x0eb: (        "DGET",  3,  3,   "V",         "R R R", False), # 156
    0x0f4: (        "INFO",  1,  1,   "V",             "V", False), # 157
    0x0f7: (          "DB",  4,  5,   "V",     "V V V V V", False), # 158
    0x0fc: (   "FREQUENCY",  2,  2,   "A",           "R R", False), # 159
    0x105: (  "ERROR.TYPE",  1,  1,   "V",             "V", False), # 160
    0x10d: (      "AVEDEV",  1, 30,   "V",         "R ...", False), # 161
    0x10e: (    "BETADIST",  3,  5,   "V",     "V V V V V", False), # 162
    0x10f: (     "GAMMALN",  1,  1,   "V",             "V", False), # 163
    0x110: (     "BETAINV",  3,  5,   "V",     "V V V V V", False), # 164
    0x111: (   "BINOMDIST",  4,  4,   "V",       "V V V V", False), # 165
    0x112: (     "CHIDIST",  2,  2,   "V",           "V V", False), # 166
    0x113: (      "CHIINV",  2,  2,   "V",           "V V", False), # 167
    0x114: (      "COMBIN",  2,  2,   "V",           "V V", False), # 168
    0x115: (  "CONFIDENCE",  3,  3,   "V",         "V V V", False), # 169
    0x116: (   "CRITBINOM",  3,  3,   "V",         "V V V", False), # 170
    0x117: (        "EVEN",  1,  1,   "V",             "V", False), # 171
    0x118: (   "EXPONDIST",  3,  3,   "V",         "V V V", False), # 172
    0x119: (       "FDIST",  3,  3,   "V",         "V V V", False), # 173
    0x11a: (        "FINV",  3,  3,   "V",         "V V V", False), # 174
    0x11b: (      "FISHER",  1,  1,   "V",             "V", False), # 175
    0x11c: (   "FISHERINV",  1,  1,   "V",             "V", False), # 176
    0x11d: (       "FLOOR",  2,  2,   "V",           "V V", False), # 177
    0x11e: (   "GAMMADIST",  4,  4,   "V",       "V V V V", False), # 178
    0x11f: (    "GAMMAINV",  3,  3,   "V",         "V V V", False), # 179
    0x120: (     "CEILING",  2,  2,   "V",           "V V", False), # 180
    0x121: ( "HYPGEOMVERT",  4,  4,   "V",       "V V V V", False), # 181
    0x122: ( "LOGNORMDIST",  3,  3,   "V",         "V V V", False), # 182
    0x123: (      "LOGINV",  3,  3,   "V",         "V V V", False), # 183
    0x124: ("NEGBINOMDIST",  3,  3,   "V",         "V V V", False), # 184
    0x125: (    "NORMDIST",  4,  4,   "V",       "V V V V", False), # 185
    0x126: (   "NORMSDIST",  1,  1,   "V",             "V", False), # 186
    0x127: (     "NORMINV",  3,  3,   "V",         "V V V", False), # 187
    0x128: (   "MNORMSINV",  1,  1,   "V",             "V", False), # 188
    0x129: ( "STANDARDIZE",  3,  3,   "V",         "V V V", False), # 189
    0x12a: (         "ODD",  1,  1,   "V",             "V", False), # 190
    0x12b: (      "PERMUT",  2,  2,   "V",           "V V", False), # 191
    0x12c: (     "POISSON",  3,  3,   "V",         "V V V", False), # 192
    0x12d: (       "TDIST",  3,  3,   "V",         "V V V", False), # 193
    0x12e: (     "WEIBULL",  4,  4,   "V",       "V V V V", False), # 194
    0x12f: (     "SUMXMY2",  2,  2,   "V",           "A A", False), # 195
    0x130: (    "SUMX2MY2",  2,  2,   "V",           "A A", False), # 196
    0x131: (    "SUMX2PY2",  2,  2,   "V",           "A A", False), # 197
    0x132: (     "CHITEST",  2,  2,   "V",           "A A", False), # 198
    0x133: (      "CORREL",  2,  2,   "V",           "A A", False), # 199
    0x134: (       "COVAR",  2,  2,   "V",           "A A", False), # 200
    0x135: (    "FORECAST",  3,  3,   "V",         "V A A", False), # 201
    0x136: (       "FTEST",  2,  2,   "V",           "A A", False), # 202
    0x137: (   "INTERCEPT",  2,  2,   "V",           "A A", False), # 203
    0x138: (     "PEARSON",  2,  2,   "V",           "A A", False), # 204
    0x139: (         "RSQ",  2,  2,   "V",           "A A", False), # 205
    0x13a: (       "STEYX",  2,  2,   "V",           "A A", False), # 206
    0x13b: (       "SLOPE",  2,  2,   "V",           "A A", False), # 207
    0x13c: (       "TTEST",  4,  4,   "V",       "A A V V", False), # 208
    0x13d: (        "PROB",  3,  4,   "V",       "A A V V", False), # 209
    0x13e: (       "DEVSQ",  1, 30,   "V",         "R ...", False), # 210
    0x13f: (     "GEOMEAN",  1, 30,   "V",         "R ...", False), # 211
    0x140: (     "HARMEAN",  1, 30,   "V",         "R ...", False), # 212
    0x141: (       "SUMSQ",  0, 30,   "V",         "R ...", False), # 213
    0x142: (        "KURT",  1, 30,   "V",         "R ...", False), # 214
    0x143: (        "SKEW",  1, 30,   "V",         "R ...", False), # 215
    0x144: (       "ZTEST",  2,  3,   "V",         "R V V", False), # 216
    0x145: (       "LARGE",  2,  2,   "V",           "R V", False), # 217
    0x146: (       "SMALL",  2,  2,   "V",           "R V", False), # 218
    0x147: (    "QUARTILE",  2,  2,   "V",           "R V", False), # 219
    0x148: (  "PERCENTILE",  2,  2,   "V",           "R V", False), # 220
    0x149: ( "PERCENTRANK",  2,  3,   "V",         "R V V", False), # 221
    0x14a: (        "MODE",  1, 30,   "V",         "A ...", False), # 222
    0x14b: (    "TRIMMEAN",  2,  2,   "V",           "R V", False), # 223
    0x14c: (        "TINV",  2,  2,   "V",           "V V", False), # 224
    0x150: ( "CONCATENATE",  0, 30,   "V",         "V ...", False), # 225
    0x151: (       "POWER",  2,  2,   "V",           "V V", False), # 226
    0x156: (     "RADIANS",  1,  1,   "V",             "V", False), # 227
    0x157: (     "DEGREES",  1,  1,   "V",             "V", False), # 228
    0x158: (    "SUBTOTAL",  2, 30,   "V",       "V R ...", False), # 229
    0x159: (       "SUMIF",  2,  3,   "V",         "R V R", False), # 230
    0x15a: (     "COUNTIF",  2,  2,   "V",           "R V", False), # 231
    0x15b: (  "COUNTBLANK",  1,  1,   "V",             "R", False), # 232
    0x15e: (       "ISPMT",  4,  4,   "V",       "V V V V", False), # 233
    0x15f: (     "DATEDIF",  3,  3,   "V",         "V V V", False), # 234
    0x160: (  "DATESTRING",  1,  1,   "V",             "V", False), # 235
    0x161: ("NUMBERSTRING",  2,  2,   "V",           "V V", False), # 236
    0x162: (       "ROMAN",  1,  2,   "V",           "V V", False), # 237
    0x166: ("GETPIVOTDATA",  2, 30,   "A",             "-", False), # 238
    0x167: (   "HYPERLINK",  1,  2,   "V",           "V V", False), # 239
    0x168: (    "PHONETIC",  1,  1,   "V",             "R", False), # 240
    0x169: (    "AVERAGEA",  1, 30,   "V",         "R ...", False), # 241
    0x16a: (        "MAXA",  1, 30,   "V",         "R ...", False), # 242
    0x16b: (        "MINA",  1, 30,   "V",         "R ...", False), # 243
    0x16c: (     "STDEVPA",  1, 30,   "V",         "R ...", False), # 244
    0x16d: (       "VARPA",  1, 30,   "V",         "R ...", False), # 245
    0x16e: (      "STDEVA",  1, 30,   "V",         "R ...", False), # 246
    0x16f: (        "VARA",  1, 30,   "V",         "R ...", False)  # 247
}


# Formulas Parse things

ptgExp          = 0x01
ptgTbl          = 0x02
ptgAdd          = 0x03
ptgSub          = 0x04
ptgMul          = 0x05
ptgDiv          = 0x06
ptgPower        = 0x07
ptgConcat       = 0x08
ptgLT           = 0x09
ptgLE           = 0x0a
ptgEQ           = 0x0b
ptgGE           = 0x0c
ptgGT           = 0x0d
ptgNE           = 0x0e
ptgIsect        = 0x0f
ptgUnion        = 0x10
ptgRange        = 0x11
ptgUplus        = 0x12
ptgUminus       = 0x13
ptgPercent      = 0x14
ptgParen        = 0x15
ptgMissArg      = 0x16
ptgStr          = 0x17
ptgExtend       = 0x18
ptgAttr         = 0x19
ptgSheet        = 0x1a
ptgEndSheet     = 0x1b
ptgErr          = 0x1c
ptgBool         = 0x1d
ptgInt          = 0x1e
ptgNum          = 0x1f

ptgArrayR       = 0x20
ptgFuncR        = 0x21
ptgFuncVarR     = 0x22
ptgNameR        = 0x23
ptgRefR         = 0x24
ptgAreaR        = 0x25
ptgMemAreaR     = 0x26
ptgMemErrR      = 0x27
ptgMemNoMemR    = 0x28
ptgMemFuncR     = 0x29
ptgRefErrR      = 0x2a
ptgAreaErrR     = 0x2b
ptgRefNR        = 0x2c
ptgAreaNR       = 0x2d
ptgMemAreaNR    = 0x2e
ptgMemNoMemNR   = 0x2f
ptgNameXR       = 0x39
ptgRef3dR       = 0x3a
ptgArea3dR      = 0x3b
ptgRefErr3dR    = 0x3c
ptgAreaErr3dR   = 0x3d

ptgArrayV       = 0x40
ptgFuncV        = 0x41
ptgFuncVarV     = 0x42
ptgNameV        = 0x43
ptgRefV         = 0x44
ptgAreaV        = 0x45
ptgMemAreaV     = 0x46
ptgMemErrV      = 0x47
ptgMemNoMemV    = 0x48
ptgMemFuncV     = 0x49
ptgRefErrV      = 0x4a
ptgAreaErrV     = 0x4b
ptgRefNV        = 0x4c
ptgAreaNV       = 0x4d
ptgMemAreaNV    = 0x4e
ptgMemNoMemNV   = 0x4f
ptgFuncCEV      = 0x58
ptgNameXV       = 0x59
ptgRef3dV       = 0x5a
ptgArea3dV      = 0x5b
ptgRefErr3dV    = 0x5c
ptgAreaErr3dV   = 0x5d

ptgArrayA       = 0x60
ptgFuncA        = 0x61
ptgFuncVarA     = 0x62
ptgNameA        = 0x63
ptgRefA         = 0x64
ptgAreaA        = 0x65
ptgMemAreaA     = 0x66
ptgMemErrA      = 0x67
ptgMemNoMemA    = 0x68
ptgMemFuncA     = 0x69
ptgRefErrA      = 0x6a
ptgAreaErrA     = 0x6b
ptgRefNA        = 0x6c
ptgAreaNA       = 0x6d
ptgMemAreaNA    = 0x6e
ptgMemNoMemNA   = 0x6f
ptgFuncCEA      = 0x78
ptgNameXA       = 0x79
ptgRef3dA       = 0x7a
ptgArea3dA      = 0x7b
ptgRefErr3dA    = 0x7c
ptgAreaErr3dA   = 0x7d


PtgNames = {
    ptgExp         : "ptgExp",
    ptgTbl         : "ptgTbl",
    ptgAdd         : "ptgAdd",
    ptgSub         : "ptgSub",
    ptgMul         : "ptgMul",
    ptgDiv         : "ptgDiv",
    ptgPower       : "ptgPower",
    ptgConcat      : "ptgConcat",
    ptgLT          : "ptgLT",
    ptgLE          : "ptgLE",
    ptgEQ          : "ptgEQ",
    ptgGE          : "ptgGE",
    ptgGT          : "ptgGT",
    ptgNE          : "ptgNE",
    ptgIsect       : "ptgIsect",
    ptgUnion       : "ptgUnion",
    ptgRange       : "ptgRange",
    ptgUplus       : "ptgUplus",
    ptgUminus      : "ptgUminus",
    ptgPercent     : "ptgPercent",
    ptgParen       : "ptgParen",
    ptgMissArg     : "ptgMissArg",
    ptgStr         : "ptgStr",
    ptgExtend      : "ptgExtend",
    ptgAttr        : "ptgAttr",
    ptgSheet       : "ptgSheet",
    ptgEndSheet    : "ptgEndSheet",
    ptgErr         : "ptgErr",
    ptgBool        : "ptgBool",
    ptgInt         : "ptgInt",
    ptgNum         : "ptgNum",
    ptgArrayR      : "ptgArrayR",
    ptgFuncR       : "ptgFuncR",
    ptgFuncVarR    : "ptgFuncVarR",
    ptgNameR       : "ptgNameR",
    ptgRefR        : "ptgRefR",
    ptgAreaR       : "ptgAreaR",
    ptgMemAreaR    : "ptgMemAreaR",
    ptgMemErrR     : "ptgMemErrR",
    ptgMemNoMemR   : "ptgMemNoMemR",
    ptgMemFuncR    : "ptgMemFuncR",
    ptgRefErrR     : "ptgRefErrR",
    ptgAreaErrR    : "ptgAreaErrR",
    ptgRefNR       : "ptgRefNR",
    ptgAreaNR      : "ptgAreaNR",
    ptgMemAreaNR   : "ptgMemAreaNR",
    ptgMemNoMemNR  : "ptgMemNoMemNR",
    ptgNameXR      : "ptgNameXR",
    ptgRef3dR      : "ptgRef3dR",
    ptgArea3dR     : "ptgArea3dR",
    ptgRefErr3dR   : "ptgRefErr3dR",
    ptgAreaErr3dR  : "ptgAreaErr3dR",
    ptgArrayV      : "ptgArrayV",
    ptgFuncV       : "ptgFuncV",
    ptgFuncVarV    : "ptgFuncVarV",
    ptgNameV       : "ptgNameV",
    ptgRefV        : "ptgRefV",
    ptgAreaV       : "ptgAreaV",
    ptgMemAreaV    : "ptgMemAreaV",
    ptgMemErrV     : "ptgMemErrV",
    ptgMemNoMemV   : "ptgMemNoMemV",
    ptgMemFuncV    : "ptgMemFuncV",
    ptgRefErrV     : "ptgRefErrV",
    ptgAreaErrV    : "ptgAreaErrV",
    ptgRefNV       : "ptgRefNV",
    ptgAreaNV      : "ptgAreaNV",
    ptgMemAreaNV   : "ptgMemAreaNV",
    ptgMemNoMemNV  : "ptgMemNoMemNV",
    ptgFuncCEV     : "ptgFuncCEV",
    ptgNameXV      : "ptgNameXV",
    ptgRef3dV      : "ptgRef3dV",
    ptgArea3dV     : "ptgArea3dV",
    ptgRefErr3dV   : "ptgRefErr3dV",
    ptgAreaErr3dV  : "ptgAreaErr3dV",
    ptgArrayA      : "ptgArrayA",
    ptgFuncA       : "ptgFuncA",
    ptgFuncVarA    : "ptgFuncVarA",
    ptgNameA       : "ptgNameA",
    ptgRefA        : "ptgRefA",
    ptgAreaA       : "ptgAreaA",
    ptgMemAreaA    : "ptgMemAreaA",
    ptgMemErrA     : "ptgMemErrA",
    ptgMemNoMemA   : "ptgMemNoMemA",
    ptgMemFuncA    : "ptgMemFuncA",
    ptgRefErrA     : "ptgRefErrA",
    ptgAreaErrA    : "ptgAreaErrA",
    ptgRefNA       : "ptgRefNA",
    ptgAreaNA      : "ptgAreaNA",
    ptgMemAreaNA   : "ptgMemAreaNA",
    ptgMemNoMemNA  : "ptgMemNoMemNA",
    ptgFuncCEA     : "ptgFuncCEA",
    ptgNameXA      : "ptgNameXA",
    ptgRef3dA      : "ptgRef3dA",
    ptgArea3dA     : "ptgArea3dA",
    ptgRefErr3dA   : "ptgRefErr3dA",
    ptgAreaErr3dA  : "ptgAreaErr3dA"
}


error_msg_by_code = {
    0x00: u"#NULL!",  # intersection of two cell ranges is empty
    0x07: u"#DIV/0!", # division by zero
    0x0F: u"#VALUE!", # wrong type of operand
    0x17: u"#REF!",   # illegal or deleted cell reference
    0x1D: u"#NAME?",  # wrong function or range name
    0x24: u"#NUM!",   # value range overflow
    0x2A: u"#N/A!"    # argument or function not available
}

########NEW FILE########
__FILENAME__ = Formatting
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.



'''
The  XF  record is able to store explicit cell formatting attributes or the
attributes  of  a cell style. Explicit formatting includes the reference to
a  cell  style  XF  record. This allows to extend a defined cell style with
some  explicit  attributes.  The  formatting  attributes  are  divided into
6 groups:

Group           Attributes
-------------------------------------
Number format   Number format index (index to FORMAT record)
Font            Font index (index to FONT record)
Alignment       Horizontal and vertical alignment, text wrap, indentation, 
                orientation/rotation, text direction
Border          Border line styles and colours
Background      Background area style and colours
Protection      Cell locked, formula hidden

For  each  group  a flag in the cell XF record specifies whether to use the
attributes  contained  in  that  XF  record  or  in  the  referenced  style
XF  record. In style XF records, these flags specify whether the attributes
will  overwrite  explicit  cell  formatting  when  the  style is applied to
a  cell. Changing a cell style (without applying this style to a cell) will
change  all  cells which already use that style and do not contain explicit
cell  attributes for the changed style attributes. If a cell XF record does
not  contain  explicit  attributes  in a group (if the attribute group flag
is not set), it repeats the attributes of its style XF record.

'''

__rev_id__ = """$Id: Formatting.py,v 1.4 2005/07/20 07:24:11 rvk Exp $"""


import BIFFRecords

class Font(object):
    ESCAPEMENT_NONE         = 0x00
    ESCAPEMENT_SUPERSCRIPT  = 0x01
    ESCAPEMENT_SUBSCRIPT    = 0x02
    
    UNDERLINE_NONE          = 0x00
    UNDERLINE_SINGLE        = 0x01
    UNDERLINE_SINGLE_ACC    = 0x21
    UNDERLINE_DOUBLE        = 0x02
    UNDERLINE_DOUBLE_ACC    = 0x22
    
    FAMILY_NONE         = 0x00
    FAMILY_ROMAN        = 0x01
    FAMILY_SWISS        = 0x02
    FAMILY_MODERN       = 0x03
    FAMILY_SCRIPT       = 0x04
    FAMILY_DECORARTIVE  = 0x05
    
    CHARSET_ANSI_LATIN          = 0x00
    CHARSET_SYS_DEFAULT         = 0x01
    CHARSET_SYMBOL              = 0x02
    CHARSET_APPLE_ROMAN         = 0x4D
    CHARSET_ANSI_JAP_SHIFT_JIS  = 0x80
    CHARSET_ANSI_KOR_HANGUL     = 0x81
    CHARSET_ANSI_KOR_JOHAB      = 0x82
    CHARSET_ANSI_CHINESE_GBK    = 0x86
    CHARSET_ANSI_CHINESE_BIG5   = 0x88
    CHARSET_ANSI_GREEK          = 0xA1
    CHARSET_ANSI_TURKISH        = 0xA2
    CHARSET_ANSI_VIETNAMESE     = 0xA3
    CHARSET_ANSI_HEBREW         = 0xB1
    CHARSET_ANSI_ARABIC         = 0xB2
    CHARSET_ANSI_BALTIC         = 0xBA
    CHARSET_ANSI_CYRILLIC       = 0xCC
    CHARSET_ANSI_THAI           = 0xDE
    CHARSET_ANSI_LATIN_II       = 0xEE
    CHARSET_OEM_LATIN_I         = 0xFF   
    
    def __init__(self):
        # twip = 1/20 of a point = 1/1440 of a inch
        # usually resolution == 96 pixels per 1 inch 
        # (rarely 120 pixels per 1 inch or another one)
        
        self.height = 0x00C8 # 200: this is font with height 10 points
        self.italic = False
        self.struck_out = False
        self.outline = False
        self.shadow = False
        self.colour_index = 0x7FFF
        self.bold = False
        self._weight = 0x0190 # 0x02BC gives bold font
        self.escapement = self.ESCAPEMENT_NONE
        self.underline = self.UNDERLINE_NONE
        self.family = self.FAMILY_NONE
        self.charset = self.CHARSET_ANSI_CYRILLIC       
        self.name = 'Arial'
                
    def get_biff_record(self):
        height = self.height
        
        options = 0x00
        if self.bold:
            options |= 0x01
            self._weight = 0x02BC
        if self.italic:
            options |= 0x02
        if self.underline != self.UNDERLINE_NONE:
            options |= 0x04
        if self.struck_out:
            options |= 0x08
        if self.outline:
            options |= 0x010
        if self.shadow:
            options |= 0x020
            
        colour_index = self.colour_index 
        weight = self._weight
        escapement = self.escapement
        underline = self.underline 
        family = self.family 
        charset = self.charset
        name = self.name
        
        return BIFFRecords.FontRecord(height, options, colour_index, weight, escapement, 
                    underline, family, charset, 
                    name)

class Alignment(object):
    HORZ_GENERAL                = 0x00
    HORZ_LEFT                   = 0x01
    HORZ_CENTER                 = 0x02
    HORZ_RIGHT                  = 0x03
    HORZ_FILLED                 = 0x04
    HORZ_JUSTIFIED              = 0x05 # BIFF4-BIFF8X
    HORZ_CENTER_ACROSS_SEL      = 0x06 # Centred across selection (BIFF4-BIFF8X)
    HORZ_DISTRIBUTED            = 0x07 # Distributed (BIFF8X)
    
    VERT_TOP                    = 0x00 
    VERT_CENTER                 = 0x01
    VERT_BOTTOM                 = 0x02
    VERT_JUSTIFIED              = 0x03 # Justified (BIFF5-BIFF8X)
    VERT_DISIRIBUTED            = 0x04 # Distributed (BIFF8X)

    DIRECTION_GENERAL           = 0x00 # BIFF8X
    DIRECTION_LR                = 0x01
    DIRECTION_RL                = 0x02

    ORIENTATION_NOT_ROTATED     = 0x00
    ORIENTATION_STACKED         = 0x01
    ORIENTATION_90_CC           = 0x02
    ORIENTATION_90_CW           = 0x03

    ROTATION_0_ANGLE            = 0x00
    ROTATION_STACKED            = 0xFF
    
    WRAP_AT_RIGHT               = 0x01
    NOT_WRAP_AT_RIGHT           = 0x00
    
    SHRINK_TO_FIT               = 0x01
    NOT_SHRINK_TO_FIT           = 0x00       

    def __init__(self):
        self.horz = self.HORZ_GENERAL
        self.vert = self.VERT_BOTTOM
        self.dire = self.DIRECTION_GENERAL
        self.orie = self.ORIENTATION_NOT_ROTATED
        self.rota = self.ROTATION_0_ANGLE
        self.wrap = self.NOT_WRAP_AT_RIGHT
        self.shri = self.NOT_SHRINK_TO_FIT
        self.inde = 0
        self.merg = 0

class Borders(object):
    NO_LINE = 0x00
    THIN    = 0x01
    MEDIUM  = 0x02
    DASHED  = 0x03
    DOTTED  = 0x04
    THICK   = 0x05
    DOUBLE  = 0x06
    HAIR    = 0x07
    #The following for BIFF8
    MEDIUM_DASHED               = 0x08
    THIN_DASH_DOTTED            = 0x09
    MEDIUM_DASH_DOTTED          = 0x0A
    THIN_DASH_DOT_DOTTED        = 0x0B
    MEDIUM_DASH_DOT_DOTTED      = 0x0C
    SLANTED_MEDIUM_DASH_DOTTED  = 0x0D
    
    NEED_DIAG1      = 0x01
    NEED_DIAG2      = 0x01
    NO_NEED_DIAG1   = 0x00
    NO_NEED_DIAG2   = 0x00

    def __init__(self):
        self.left   = self.NO_LINE
        self.right  = self.NO_LINE
        self.top    = self.NO_LINE
        self.bottom = self.NO_LINE
        self.diag   = self.NO_LINE

        self.left_colour   = 0x40
        self.right_colour  = 0x40
        self.top_colour    = 0x40
        self.bottom_colour = 0x40
        self.diag_colour   = 0x40
        
        self.need_diag1 = self.NO_NEED_DIAG1
        self.need_diag2 = self.NO_NEED_DIAG2

class Pattern(object):
    # patterns 0x00 - 0x12
    NO_PATTERN      = 0x00 
    SOLID_PATTERN   = 0x01 
    
    def __init__(self):
        self.pattern = self.NO_PATTERN
        self.pattern_fore_colour = 0x40
        self.pattern_back_colour = 0x41
        
class Protection(object):
    def __init__(self):
        self.cell_locked = 1
        self.formula_hidden = 0
        
        
if __name__ == '__main__':
    font0 = Font()
    font0.name = 'Arial'
    font1 = Font()
    font1.name = 'Arial Cyr'
    font2 = Font()
    font2.name = 'Times New Roman'
    font3 = Font()
    font3.name = 'Courier New Cyr'
    
    for font, filename in [(font0, 'font0.bin'), (font1, 'font1.bin'), (font2, 'font2.bin'), (font3, 'font3.bin')]:
        f = file(filename, 'wb')
        f.write(font.get_biff_record().get_data())
        f.close
        
########NEW FILE########
__FILENAME__ = ImportXLS
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


__rev_id__ = """$Id: ImportXLS.py,v 1.6 2005/10/26 07:44:24 rvk Exp $"""


import UnicodeUtils
import CompoundDoc
import ExcelMagic
from struct import pack, unpack


def parse_xls(filename, encoding = None):
    
    ##########################################################################

    def process_BOUNDSHEET(biff8, rec_data):
        sheet_stream_pos, visibility, sheet_type = unpack('<I2B', rec_data[:6])
        sheet_name = rec_data[6:]

        if biff8:
            chars_num, options = unpack('2B', sheet_name[:2])
            
            chars_start = 2
            runs_num = 0
            asian_phonetic_size = 0

            result = ''

            compressed = (options & 0x01) == 0
            has_asian_phonetic = (options & 0x04) != 0
            has_format_runs = (options & 0x08) != 0

            if has_format_runs:
                runs_num , = unpack('<H', sheet_name[chars_start:chars_start+2])
                chars_start += 2
            if has_asian_phonetic:
                asian_phonetic_size , = unpack('<I', sheet_name[chars_start:chars_start+4])
                chars_start += 4

            if compressed:
                chars_end = chars_start + chars_num
                result = sheet_name[chars_start:chars_end].decode('latin_1', 'replace')
            else:
                chars_end = chars_start + 2*chars_num
                result = sheet_name[chars_start:chars_end].decode('utf_16_le', 'replace')
            
            tail_size = 4*runs_num + asian_phonetic_size
        else:
            result = sheet_name[1:].decode(encoding, 'replace')
        
        return result


    def unpack2str(biff8, label_name): # 2 bytes length str
        if biff8:
            chars_num, options = unpack('<HB', label_name[:3])
            
            chars_start = 3
            runs_num = 0
            asian_phonetic_size = 0

            result = ''

            compressed = (options & 0x01) == 0
            has_asian_phonetic = (options & 0x04) != 0
            has_format_runs = (options & 0x08) != 0

            if has_format_runs:
                runs_num , = unpack('<H', label_name[chars_start:chars_start+2])
                chars_start += 2
            if has_asian_phonetic:
                asian_phonetic_size , = unpack('<I', label_name[chars_start:chars_start+4])
                chars_start += 4

            if compressed:
                chars_end = chars_start + chars_num
                result = label_name[chars_start:chars_end].decode('latin_1', 'replace')
            else:
                chars_end = chars_start + 2*chars_num
                result = label_name[chars_start:chars_end].decode('utf_16_le', 'replace')
            
            tail_size = 4*runs_num + asian_phonetic_size
        else:
            result = label_name[2:].decode(encoding, 'replace')

        return result


    def process_LABEL(biff8, rec_data):
        row_idx, col_idx, xf_idx = unpack('<3H', rec_data[:6])
        label_name = rec_data[6:]
        result = unpack2str(biff8, label_name)
        return (row_idx, col_idx, result)


    def process_LABELSST(rec_data):
        row_idx, col_idx, xf_idx, sst_idx = unpack('<3HI', rec_data)
        return (row_idx, col_idx, sst_idx)


    def process_RSTRING(biff8, rec_data):
        if biff8:
            return process_LABEL(biff8, rec_data)
        else:
            row_idx, col_idx, xf_idx, length = unpack('<4H', rec_data[:8])
            result = rec_data[8:8+length].decode(encoding, 'replace')

        return (row_idx, col_idx, result)
        

    def decode_rk(encoded):
        b0, b1, b2, b3 = unpack('4B', encoded)
        is_multed_100 = (b0 & 0x01) != 0
        is_integer = (b0 & 0x02) != 0

        if is_integer:
            result , = unpack('<i', encoded)
            result >>= 2
        else:
            ieee754 = struct.pack('8B', 0, 0, 0, 0, b0 & 0xFC, b1, b2, b3)
            result , = unpack('<d', ieee754)
        if is_multed_100:
            result /= 100.0
        
        return result


    def process_RK(rec_data):
        row_idx, col_idx, xf_idx, encoded = unpack('<3H4s', rec_data)
        result = decode_rk(encoded)
        return (row_idx, col_idx, result)


    def process_MULRK(rec_data):
        row_idx, first_col_idx = unpack('<2H', rec_data[:4])
        last_col_idx , = unpack('<H', rec_data[-2:])
        xf_rk_num = last_col_idx - first_col_idx + 1

        results = []
        for i in range(xf_rk_num):
            xf_idx, encoded = unpack('<H4s', rec_data[4+6*i : 4+6*(i+1)])
            results.append(decode_rk(encoded))

        return zip([row_idx]*xf_rk_num, range(first_col_idx, last_col_idx+1), results)


    def process_NUMBER(rec_data):
        row_idx, col_idx, xf_idx, result = unpack('<3Hd', rec_data)
        return (row_idx, col_idx, result)

    
    def process_SST(rec_data, sst_continues):
        # 0x00FC
        total_refs, total_str = unpack('<2I', rec_data[:8])
        #print total_refs, str_num

        pos = 8
        curr_block = rec_data
        curr_block_num = -1
        curr_str_num = 0
        SST = {}

        while curr_str_num < total_str:
            if pos >= len(curr_block):
                curr_block_num += 1
                curr_block = sst_continues[curr_block_num]
                pos = 0

            chars_num, options = unpack('<HB', curr_block[pos:pos+3])
            #print chars_num, options
            pos += 3

            asian_phonetic_size = 0
            runs_num = 0
            has_asian_phonetic = (options & 0x04) != 0
            has_format_runs = (options & 0x08) != 0
            if has_format_runs:
                runs_num , = unpack('<H', curr_block[pos:pos+2])
                pos += 2
            if has_asian_phonetic:
                asian_phonetic_size , = unpack('<I', curr_block[pos:pos+4])
                pos += 4

            curr_char = 0
            result = ''
            while curr_char < chars_num:
                if pos >= len(curr_block):
                    curr_block_num += 1
                    curr_block = sst_continues[curr_block_num]
                    options = ord(curr_block[0])
                    pos = 1
                #print curr_block_num

                compressed = (options & 0x01) == 0
                if compressed:
                    chars_end = pos + chars_num - curr_char
                else:
                    chars_end = pos + 2*(chars_num - curr_char)
                #print compressed, has_asian_phonetic, has_format_runs

                splitted = chars_end > len(curr_block)
                if splitted:
                    chars_end = len(curr_block)
                #print splitted, curr_char, pos, chars_end, repr(curr_block[pos:chars_end])

                if compressed:
                    result += curr_block[pos:chars_end].decode('latin_1', 'replace')
                else:
                    result += curr_block[pos:chars_end].decode('utf_16_le', 'replace')

                pos = chars_end
                curr_char = len(result)
            # end while

            # TODO: handle spanning format runs over CONTINUE blocks ???
            tail_size = 4*runs_num + asian_phonetic_size
            if len(curr_block) < pos + tail_size:
                pos = pos + tail_size - len(curr_block)
                curr_block_num += 1
                curr_block = sst_continues[curr_block_num]
            else:
                pos += tail_size

            #print result.encode('cp866')

            SST[curr_str_num] = result
            curr_str_num += 1

        return SST


    #####################################################################################
    
    import struct

    encodings = {
        0x016F: 'ascii',     #ASCII
        0x01B5: 'cp437',     #IBM PC CP-437 (US)
        0x02D0: 'cp720',     #IBM PC CP-720 (OEM Arabic)
        0x02E1: 'cp737',     #IBM PC CP-737 (Greek)
        0x0307: 'cp775',     #IBM PC CP-775 (Baltic)
        0x0352: 'cp850',     #IBM PC CP-850 (Latin I)
        0x0354: 'cp852',     #IBM PC CP-852 (Latin II (Central European))
        0x0357: 'cp855',     #IBM PC CP-855 (Cyrillic)
        0x0359: 'cp857',     #IBM PC CP-857 (Turkish)
        0x035A: 'cp858',     #IBM PC CP-858 (Multilingual Latin I with Euro)
        0x035C: 'cp860',     #IBM PC CP-860 (Portuguese)
        0x035D: 'cp861',     #IBM PC CP-861 (Icelandic)
        0x035E: 'cp862',     #IBM PC CP-862 (Hebrew)
        0x035F: 'cp863',     #IBM PC CP-863 (Canadian (French))
        0x0360: 'cp864',     #IBM PC CP-864 (Arabic)
        0x0361: 'cp865',     #IBM PC CP-865 (Nordic)
        0x0362: 'cp866',     #IBM PC CP-866 (Cyrillic (Russian))
        0x0365: 'cp869',     #IBM PC CP-869 (Greek (Modern))
        0x036A: 'cp874',     #Windows CP-874 (Thai)
        0x03A4: 'cp932',     #Windows CP-932 (Japanese Shift-JIS)
        0x03A8: 'cp936',     #Windows CP-936 (Chinese Simplified GBK)
        0x03B5: 'cp949',     #Windows CP-949 (Korean (Wansung))
        0x03B6: 'cp950',     #Windows CP-950 (Chinese Traditional BIG5)
        0x04B0: 'utf_16_le', #UTF-16 (BIFF8)
        0x04E2: 'cp1250',    #Windows CP-1250 (Latin II) (Central European)
        0x04E3: 'cp1251',    #Windows CP-1251 (Cyrillic)
        0x04E4: 'cp1252',    #Windows CP-1252 (Latin I) (BIFF4-BIFF7)
        0x04E5: 'cp1253',    #Windows CP-1253 (Greek)
        0x04E6: 'cp1254',    #Windows CP-1254 (Turkish)
        0x04E7: 'cp1255',    #Windows CP-1255 (Hebrew)
        0x04E8: 'cp1256',    #Windows CP-1256 (Arabic)
        0x04E9: 'cp1257',    #Windows CP-1257 (Baltic)
        0x04EA: 'cp1258',    #Windows CP-1258 (Vietnamese)
        0x0551: 'cp1361',    #Windows CP-1361 (Korean (Johab))
        0x2710: 'mac_roman', #Apple Roman
        0x8000: 'mac_roman', #Apple Roman
        0x8001: 'cp1252'     #Windows CP-1252 (Latin I) (BIFF2-BIFF3)
    }

    biff8 = True
    SST = {}
    sheets = []
    sheet_names = []
    values = {}
    ws_num = 0
    BOFs = 0
    EOFs = 0

    # Inside MS Office document looks like filesystem
    # We need extract stream named 'Workbook' or 'Book'
    ole_streams = CompoundDoc.Reader(filename).STREAMS

    if 'Workbook' in ole_streams:
        workbook_stream = ole_streams['Workbook']
    elif 'Book' in ole_streams:
        workbook_stream = ole_streams['Book']
    else:
        raise Exception, 'No workbook stream in file.'

    workbook_stream_len = len(workbook_stream)
    stream_pos = 0
    
    # Excel's method of data storing is based on 
    # ancient technology "TLV" (Type, Length, Value).
    # In addition, if record size grows to some limit
    # Excel writes CONTINUE records
    while stream_pos < workbook_stream_len and EOFs <= ws_num:
        rec_id, data_size = unpack('<2H', workbook_stream[stream_pos:stream_pos+4])
        stream_pos += 4
        
        rec_data = workbook_stream[stream_pos:stream_pos+data_size]
        stream_pos += data_size

        if rec_id == 0x0809: # BOF
            #print 'BOF', 
            BOFs += 1
            ver, substream_type = unpack('<2H', rec_data[:4])
            if substream_type == 0x0005:
                # workbook global substream
                biff8 = ver >= 0x0600
            elif substream_type == 0x0010:
                # worksheet substream
                pass
            else: # skip chart stream or unknown stream
            # stream offsets may be used from BOUNDSHEET record
                rec_id, data_size = unpack('<2H', workbook_stream[stream_pos:stream_pos+4])
                while rec_id != 0x000A: # EOF
                    #print 'SST CONTINUE'
                    stream_pos += 4
                    stream_pos += data_size
                    rec_id, data_size = unpack('<2H', workbook_stream[stream_pos:stream_pos+4])
            #print 'BIFF8 == ', biff8
        elif rec_id == 0x000A: # EOF
            #print 'EOF'
            if BOFs > 1:
                sheets.extend([values])
                values = {}
            EOFs += 1
        elif rec_id == 0x0042: # CODEPAGE
            cp ,  = unpack('<H', rec_data)
            #print 'CODEPAGE', hex(cp)
            if not encoding:
                encoding = encodings[cp]
            #print encoding
        elif rec_id == 0x0085: # BOUNDSHEET
            #print 'BOUNDSHEET',
            ws_num += 1
            b = process_BOUNDSHEET(biff8, rec_data)
            sheet_names.extend([b])
            #print b.encode('cp866')
        elif rec_id == 0x00FC: # SST
            #print 'SST'
            sst_data = rec_data
            sst_continues = []
            rec_id, data_size = unpack('<2H', workbook_stream[stream_pos:stream_pos+4])
            while rec_id == 0x003C: # CONTINUE
                #print 'SST CONTINUE'
                stream_pos += 4
                rec_data = workbook_stream[stream_pos:stream_pos+data_size]
                sst_continues.extend([rec_data])
                stream_pos += data_size
                rec_id, data_size = unpack('<2H', workbook_stream[stream_pos:stream_pos+4])
            SST = process_SST(sst_data, sst_continues)
        elif rec_id == 0x00FD: # LABELSST
            #print 'LABELSST',
            r, c, i = process_LABELSST(rec_data)
            values[(r, c)] = SST[i]
            #print r, c, SST[i].encode('cp866')
        elif rec_id == 0x0204: # LABEL
            #print 'LABEL',
            r, c, b = process_LABEL(biff8, rec_data)
            values[(r, c)] = b
            #print r, c, b.encode('cp866')
        elif rec_id == 0x00D6: # RSTRING
            #print 'RSTRING',
            r, c, b = process_RSTRING(biff8, rec_data)
            values[(r, c)] = b
            #print r, c, b.encode('cp866')
        elif rec_id == 0x027E: # RK
            #print 'RK',
            r, c, b = process_RK(rec_data)
            values[(r, c)] = b
            #print r, c, b
        elif rec_id == 0x00BD: # MULRK
            #print 'MULRK',
            for r, c, b in process_MULRK(rec_data):
                values[(r, c)] = b
            #print r, c, b
        elif rec_id == 0x0203: # NUMBER
            #print 'NUMBER',
            r, c, b = process_NUMBER(rec_data)
            values[(r, c)] = b
            #print r, c, b
        elif rec_id == 0x0006: # FORMULA
            #print 'FORMULA',
            r, c, x = unpack('<3H', rec_data[0:6])
            if rec_data[12] == '\xFF' and rec_data[13] == '\xFF':
                if rec_data[6] == '\x00':
                    got_str = False
                    if ord(rec_data[14]) & 8:
                        # part of shared formula
                        rec_id, data_size = unpack('<2H', workbook_stream[stream_pos:stream_pos+4])
                        stream_pos += 4                      
                        rec_data = workbook_stream[stream_pos:stream_pos+data_size]
                        stream_pos += data_size
                        if rec_id == 0x0207: # STRING
                            got_str = True
                        elif rec_id not in (0x0221, 0x04BC, 0x0236, 0x0037, 0x0036):
                            raise Exception("Expected ARRAY, SHRFMLA, TABLEOP* or STRING record")
                    if not got_str:
                        rec_id, data_size = unpack('<2H', workbook_stream[stream_pos:stream_pos+4])
                        stream_pos += 4                      
                        rec_data = workbook_stream[stream_pos:stream_pos+data_size]
                        stream_pos += data_size
                        if rec_id != 0x0207: # STRING
                            raise Exception("Expected STRING record")
                    values[(r, c)] = unpack2str(biff8, rec_data)
                elif rec_data[6] == '\x01':
                    # boolean 
                    v = ord(rec_data[8])
                    values[(r, c)] = bool(v)
                elif rec_data[6] == '\x02':
                    # error
                    v = ord(rec_data[8])
                    if v in ExcelMagic.error_msg_by_code:
                        values[(r, c)] = ExcelMagic.error_msg_by_code[v]
                    else:
                        values[(r, c)] = u'#UNKNOWN ERROR!'
                elif rec_data[6] == '\x03':
                    # empty
                    values[(r, c)] = u''
                else:
                    raise Exception("Unknown value for formula result")
            else:
                # 64-bit float
                d, = unpack("<d", rec_data[6:14])
                values[(r, c)] = d

    encoding = None
    return zip(sheet_names, sheets)

########NEW FILE########
__FILENAME__ = Row
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


__rev_id__ = """$Id: Row.py,v 1.6 2005/08/11 08:53:48 rvk Exp $"""


import BIFFRecords
from Deco import *
from Worksheet import Worksheet
import Style
import Cell
import ExcelFormula
import datetime as dt


class Row(object):
    __slots__ = ["__init__", 
                 "__adjust_height",
                 "__adjust_bound_col_idx",
                 "__excel_date_dt",
                 "get_height_in_pixels",
                 "set_style",
                 "get_xf_index",
                 "get_cells_count",
                 "get_min_col",
                 "get_max_col",
                 "get_str_count",
                 "get_row_biff_data",
                 "get_cells_biff_data",
                 "get_index",
                 "write",
                 "write_blanks",
                 # private variables
                 "__idx",
                 "__parent",
                 "__parent_wb",
                 "__cells",
                 "__min_col_idx",
                 "__max_col_idx",
                 "__total_str",
                 "__xf_index",
                 "__has_default_format",
                 "__height_in_pixels",
                 # public variables
                 "height",
                 "has_default_height",
                 "level",
                 "collapse",
                 "hidden",
                 "space_above",
                 "space_below"]

    #################################################################
    ## Constructor
    #################################################################
    def __init__(self, index, parent_sheet):
        self.__idx = index
        self.__parent = parent_sheet
        self.__parent_wb = parent_sheet.get_parent()
        self.__cells = []
        self.__min_col_idx = 0
        self.__max_col_idx = 0
        self.__total_str = 0
        self.__xf_index = 0x0F
        self.__has_default_format = 0
        self.__height_in_pixels = 0x11
        
        self.height = 0x00FF
        self.has_default_height = 0x00
        self.level = 0
        self.collapse = 0
        self.hidden = 0
        self.space_above = 0
        self.space_below = 0


    def __adjust_height(self, style):
        twips = style.font.height
        points = float(twips)/20.0
        # Cell height in pixels can be calcuted by following approx. formula:
        # cell height in pixels = font height in points * 83/50 + 2/5
        # It works when screen resolution is 96 dpi 
        pix = int(round(points*83.0/50.0 + 2.0/5.0))
        if pix > self.__height_in_pixels:
            self.__height_in_pixels = pix


    def __adjust_bound_col_idx(self, *args):
        for arg in args:
            if arg < self.__min_col_idx:
                self.__min_col_idx = arg
            elif arg > self.__max_col_idx:
                self.__max_col_idx = arg

    def __excel_date_dt(self, date):
        if isinstance(date, dt.date) and (not isinstance(date, dt.datetime)):
            epoch = dt.date(1899, 12, 31)
        elif isinstance(date, dt.time):
            date = dt.datetime.combine(dt.datetime(1900, 1, 1), date)
            epoch = dt.datetime(1900, 1, 1, 0, 0, 0)
        else:
            epoch = dt.datetime(1899, 12, 31, 0, 0, 0)
        delta = date - epoch
        xldate = delta.days + float(delta.seconds) / (24*60*60)
        # Add a day for Excel's missing leap day in 1900
        if xldate > 59:
            xldate += 1
        return xldate

    def get_height_in_pixels(self):
        return self.__height_in_pixels


    @accepts(object, Style.XFStyle)
    def set_style(self, style):
        self.__adjust_height(style)
        self.__xf_index = self.__parent_wb.add_style(style)

            
    def get_xf_index(self):
        return self.__xf_index

    
    def get_cells_count(self):
        return len(self.__cells)

    
    def get_min_col(self):
        return self.__min_col_idx

        
    def get_max_col(self):
        return self.__min_col_idx

        
    def get_str_count(self):
        return self.__total_str


    def get_row_biff_data(self):
        height_options = (self.height & 0x07FFF) 
        height_options |= (self.has_default_height & 0x01) << 15

        options =  (self.level & 0x07) << 0
        options |= (self.collapse & 0x01) << 4
        options |= (self.hidden & 0x01) << 5
        options |= (0x00 & 0x01) << 6
        options |= (0x01 & 0x01) << 8
        if self.__xf_index != 0x0F:
            options |= (0x01 & 0x01) << 7
        else:
            options |= (0x00 & 0x01) << 7
        options |= (self.__xf_index & 0x0FFF) << 16 
        options |= (0x00 & self.space_above) << 28
        options |= (0x00 & self.space_below) << 29
        
        return BIFFRecords.RowRecord(self.__idx, self.__min_col_idx, self.__max_col_idx, height_options, options).get()                                              
                        

    def get_cells_biff_data(self):
        return ''.join([ cell.get_biff_data() for cell in self.__cells ])


    def get_index(self):
        return self.__idx


    @accepts(object, int, (str, unicode, int, float, dt.datetime, dt.time, dt.date, ExcelFormula.Formula), Style.XFStyle)
    def write(self, col, label, style):
        self.__adjust_height(style)
        self.__adjust_bound_col_idx(col)
        if isinstance(label, (str, unicode)):
            if len(label) > 0:
                self.__cells.extend([ Cell.StrCell(self, col, self.__parent_wb.add_style(style), self.__parent_wb.add_str(label)) ])
                self.__total_str += 1
            else:
                self.__cells.extend([ Cell.BlankCell(self, col, self.__parent_wb.add_style(style)) ])
        elif isinstance(label, (int, float)):
            self.__cells.extend([ Cell.NumberCell(self, col, self.__parent_wb.add_style(style), label) ])            
        elif isinstance(label, (dt.datetime, dt.time)):
            self.__cells.extend([ Cell.NumberCell(self, col, self.__parent_wb.add_style(style), self.__excel_date_dt(label)) ])
        else:
            self.__cells.extend([ Cell.FormulaCell(self, col, self.__parent_wb.add_style(style), label) ])

    @accepts(object, int, int, Style.XFStyle)                        
    def write_blanks(self, c1, c2, style):
        self.__adjust_height(style)
        self.__adjust_bound_col_idx(c1, c2)
        self.__cells.extend([ Cell.MulBlankCell(self, c1, c2, self.__parent_wb.add_style(style)) ])

        
        
########NEW FILE########
__FILENAME__ = Style
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


__rev_id__ = """$Id: Style.py,v 1.4 2005/07/20 07:24:11 rvk Exp $"""


import Formatting
from BIFFRecords import *


_default_num_format = 'general'
_default_font = Formatting.Font()
_default_alignment = Formatting.Alignment()
_default_borders = Formatting.Borders()
_default_pattern = Formatting.Pattern()
_default_protection = Formatting.Protection()

class XFStyle(object):
    
    def __init__(self):
        self.num_format_str  = _default_num_format
        self.font            = _default_font 
        self.alignment       = _default_alignment
        self.borders         = _default_borders
        self.pattern         = _default_pattern 
        self.protection      = _default_protection

class StyleCollection(object):
    _std_num_fmt_list = [
            'general',
            '0',
            '0.00',
            '#,##0',
            '#,##0.00',
            '"$"#,##0_);("$"#,##',
            '"$"#,##0_);[Red]("$"#,##',
            '"$"#,##0.00_);("$"#,##',
            '"$"#,##0.00_);[Red]("$"#,##',
            '0%',
            '0.00%',
            '0.00E+00',
            '# ?/?',
            '# ??/??',
            'M/D/YY',
            'D-MMM-YY',
            'D-MMM',
            'MMM-YY',
            'h:mm AM/PM',
            'h:mm:ss AM/PM',
            'h:mm',
            'h:mm:ss',
            'M/D/YY h:mm',
            '_(#,##0_);(#,##0)',
            '_(#,##0_);[Red](#,##0)',
            '_(#,##0.00_);(#,##0.00)',
            '_(#,##0.00_);[Red](#,##0.00)',
            '_("$"* #,##0_);_("$"* (#,##0);_("$"* "-"_);_(@_)',
            '_(* #,##0_);_(* (#,##0);_(* "-"_);_(@_)',
            '_("$"* #,##0.00_);_("$"* (#,##0.00);_("$"* "-"??_);_(@_)',
            '_(* #,##0.00_);_(* (#,##0.00);_(* "-"??_);_(@_)',
            'mm:ss',
            '[h]:mm:ss',
            'mm:ss.0',
            '##0.0E+0',
            '@'   
    ]

    def __init__(self):
        self._fonts = {}
        self._fonts[Formatting.Font()] = 0
        self._fonts[Formatting.Font()] = 1
        self._fonts[Formatting.Font()] = 2
        self._fonts[Formatting.Font()] = 3
        # The font with index 4 is omitted in all BIFF versions
        self._fonts[Formatting.Font()] = 5
        
        self._num_formats = {}
        for fmtidx, fmtstr in zip(range(0, 23), StyleCollection._std_num_fmt_list[0:23]):
            self._num_formats[fmtstr] = fmtidx 
        for fmtidx, fmtstr in zip(range(37, 50), StyleCollection._std_num_fmt_list[23:]):
            self._num_formats[fmtstr] = fmtidx 

        self._xf = {}
        self.default_style = XFStyle()
        self._default_xf = self._add_style(self.default_style)[0]
        
    def add(self, style): 
        if style == None:
            return 0x10
        return self._add_style(style)[1]
    
    def _add_style(self, style):
        num_format_str = style.num_format_str
        if num_format_str in self._num_formats:
            num_format_idx = self._num_formats[num_format_str]
        else:
            num_format_idx = 163 + len(self._num_formats) - len(StyleCollection._std_num_fmt_list)
            self._num_formats[num_format_str] = num_format_idx
            
        font = style.font
        if font in self._fonts:
            font_idx = self._fonts[font]
        else:
            font_idx = len(self._fonts) + 1
            self._fonts[font] = font_idx
            
        xf = (font_idx, num_format_idx, style.alignment, style.borders, style.pattern, style.protection)
        
        if xf in self._xf:
            xf_index = self._xf[xf]
        else:
            xf_index = 0x10 + len(self._xf)
            self._xf[xf] = xf_index
            
        return xf, xf_index
        
    def get_biff_data(self):
        result = ''
        result += self._all_fonts()
        result += self._all_num_formats()
        result += self._all_cell_styles()
        result += self._all_styles()
        return result 
            
    def _all_fonts(self):
        result = ''
        i = sorted([(v, k) for k, v in self._fonts.items()])
        for font_idx, font in i:
            result += font.get_biff_record().get()
        return result
    
    def _all_num_formats(self):
        result = ''
        i = sorted([(v, k) for k, v in self._num_formats.items() if v>=163])
        for fmtidx, fmtstr in i:
            result += NumberFormatRecord(fmtidx, fmtstr).get()
        return result
    
    def _all_cell_styles(self):        
        result = ''
        for i in range(0, 16):
            result += XFRecord(self._default_xf, 'style').get()
            
        i = sorted([(v, k) for k, v in self._xf.items()])                            
        for xf_idx, xf in i:
            result += XFRecord(xf).get()
        return result
        
    def _all_styles(self):
        return StyleRecord().get()
        
if __name__ == '__main__':
    sc = StyleCollection()
    f = file('styles.bin', 'wb')
    f.write(sc.get_biff_data())
    f.close()

            
########NEW FILE########
__FILENAME__ = UnicodeUtils
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


'''
From BIFF8 on, strings are always stored using UTF-16LE  text encoding. The
character  array  is  a  sequence  of  16-bit  values4.  Additionally it is
possible  to  use  a  compressed  format, which omits the high bytes of all
characters, if they are all zero.

The following tables describe the standard format of the entire string, but
in many records the strings differ from this format. This will be mentioned
separately. It is possible (but not required) to store Rich-Text formatting
information  and  Asian  phonetic information inside a Unicode string. This
results  in  four  different  ways  to  store a string. The character array
is not zero-terminated.

The  string  consists  of  the  character count (as usual an 8-bit value or
a  16-bit value), option flags, the character array and optional formatting
information.  If the string is empty, sometimes the option flags field will
not occur. This is mentioned at the respective place.

Offset  Size    Contents
0       1 or 2  Length of the string (character count, ln)
1 or 2  1       Option flags:
                  Bit   Mask Contents
                  0     01H  Character compression (ccompr):
                               0 = Compressed (8-bit characters)
                               1 = Uncompressed (16-bit characters)
                  2     04H  Asian phonetic settings (phonetic):
                               0 = Does not contain Asian phonetic settings
                               1 = Contains Asian phonetic settings
                  3     08H  Rich-Text settings (richtext):
                               0 = Does not contain Rich-Text settings
                               1 = Contains Rich-Text settings
[2 or 3] 2      (optional, only if richtext=1) Number of Rich-Text formatting runs (rt)
[var.]   4      (optional, only if phonetic=1) Size of Asian phonetic settings block (in bytes, sz)
var.     ln or 
         2ln   Character array (8-bit characters or 16-bit characters, dependent on ccompr)
[var.]   4rt   (optional, only if richtext=1) List of rt formatting runs 
[var.]   sz     (optional, only if phonetic=1) Asian Phonetic Settings Block 
'''


__rev_id__ = """$Id: UnicodeUtils.py,v 1.4 2005/07/20 07:24:11 rvk Exp $"""


import struct


DEFAULT_ENCODING = 'cp1251'

def u2ints(ustr):
    ints = [ord(uchr) for uchr in ustr]
    return ints

def u2bytes(ustr):
    ints = u2ints(ustr)
    return struct.pack('<' + 'H'*len(ints), *ints)

def upack2(_str):
    try:
        ustr = u2bytes(unicode(_str, 'ascii'))
        return struct.pack('<HB', len(_str), 0) + _str    
    except:
        if isinstance(_str, unicode):
            ustr = u2bytes(_str)
        else:
            ustr = u2bytes(unicode(_str, DEFAULT_ENCODING))
        return struct.pack('<HB', len(_str), 1) + ustr

def upack1(_str):
    try:
        ustr = u2bytes(unicode(_str, 'ascii'))
        return struct.pack('BB', len(_str), 0) + _str    
    except:
        if isinstance(_str, unicode):
            ustr = u2bytes(_str)
        else:
            ustr = u2bytes(unicode(_str, DEFAULT_ENCODING))
        return struct.pack('BB', len(_str), 1) + ustr

if __name__ == '__main__':   
    f = file('out0.bin', 'wb')
    f.write(u2bytes(': unicode'))
    f.close()

    f = file('out1.bin', 'wb')
    f.write(upack1(': unicode'))
    f.close()

    f = file('out2.bin', 'wb')
    f.write(upack2(': unicode'))
    f.close()




########NEW FILE########
__FILENAME__ = Utils
# pyXLWriter: A library for generating Excel Spreadsheets
# Copyright (c) 2004 Evgeny Filatov <fufff@users.sourceforge.net>
# Copyright (c) 2002-2004 John McNamara (Perl Spreadsheet::WriteExcel)
#
# This library is free software; you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published by
# the Free Software Foundation; either version 2.1 of the License, or
# (at your option) any later version.
#
# This library is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser
# General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with this library; if not, write to the Free Software Foundation,
# Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
#----------------------------------------------------------------------------
# This module was written/ported from PERL Spreadsheet::WriteExcel module
# The author of the PERL Spreadsheet::WriteExcel module is John McNamara
# <jmcnamara@cpan.org>
#----------------------------------------------------------------------------
# See the README.txt distributed with pyXLWriter for more details.

# Portions are (C) Roman V. Kiseliov, 2005

"""pyXLWriter.utilites

Utilities for work with reference to cells

"""
__rev_id__ = """$Id: Utils.py,v 1.1 2005/08/11 08:53:48 rvk Exp $"""

import re
from struct import pack
from ExcelMagic import MAX_ROW, MAX_COL


_re_cell_ex = re.compile(r"(\$?)([A-I]?[A-Z])(\$?)(\d+)")
_re_row_range = re.compile(r"\$?(\d+):\$?(\d+)")
_re_col_range = re.compile(r"\$?([A-I]?[A-Z]):\$?([A-I]?[A-Z])")
_re_cell_range = re.compile(r"\$?([A-I]?[A-Z]\$?\d+):\$?([A-I]?[A-Z]\$?\d+)")
_re_cell_ref = re.compile(r"\$?([A-I]?[A-Z]\$?\d+)")


def col_by_name(colname):
    """
    """
    col = 0
    pow = 1
    for i in xrange(len(colname)-1, -1, -1):
        ch = colname[i]
        col += (ord(ch) - ord('A') + 1) * pow
        pow *= 26
    return col - 1


def cell_to_rowcol(cell):
    """Convert an Excel cell reference string in A1 notation
    to numeric row/col notation.
  
    Returns: row, col, row_abs, col_abs
 
    """
    m = _re_cell_ex.match(cell)
    if not m:
        raise Exception("Error in cell format")
    col_abs, col, row_abs, row = m.groups()
    row_abs = bool(row_abs)
    col_abs = bool(col_abs)
    row = int(row) - 1
    col = col_by_name(col)
    return row, col, row_abs, col_abs


def cell_to_rowcol2(cell):
    """Convert an Excel cell reference string in A1 notation
    to numeric row/col notation.
  
    Returns: row, col
    
    """
    m = _re_cell_ex.match(cell)
    if not m:
        raise Exception("Error in cell format")
    col_abs, col, row_abs, row = m.groups()
    # Convert base26 column string to number
    # All your Base are belong to us.
    row = int(row) - 1
    col = col_by_name(col)
    return row, col
    
    
def rowcol_to_cell(row, col, row_abs=False, col_abs=False):
    """Convert numeric row/col notation to an Excel cell reference string in
    A1 notation.
 
    """
    d = col // 26
    m = col % 26
    chr1 = ""    # Most significant character in AA1
    if row_abs:
        row_abs = '$'
    else:
        row_abs = ''
    if col_abs:
        col_abs = '$'
    else:
        col_abs = ''
    if d > 0:
        chr1 = chr(ord('A') + d  - 1)
    chr2 = chr(ord('A') + m)
    # Zero index to 1-index
    return col_abs + chr1 + chr2 + row_abs + str(row + 1)


def cellrange_to_rowcol_pair(cellrange):
    """Convert cell range string in A1 notation to numeric row/col 
    pair.

    Returns: row1, col1, row2, col2
    
    """
    cellrange = cellrange.upper()
    # Convert a row range: '1:3'
    res = _re_row_range.match(cellrange)
    if res:
        row1 = int(res.group(1)) - 1
        col1 = 0
        row2 = int(res.group(2)) - 1
        col2 = -1
        return row1, col1, row2, col2
    # Convert a column range: 'A:A' or 'B:G'.
    # A range such as A:A is equivalent to A1:A16384, so add rows as required
    res = _re_col_range.match(cellrange)
    if res:
        col1 = col_by_name(res.group(1))
        row1 = 0
        col2 = col_by_name(res.group(2))
        row2 = -1
        return row1, col1, row2, col2
    # Convert a cell range: 'A1:B7'
    res = _re_cell_range.match(cellrange)
    if res:
        row1, col1 = cell_to_rowcol2(res.group(1))
        row2, col2 = cell_to_rowcol2(res.group(2))
        return row1, col1, row2, col2
    # Convert a cell reference: 'A1' or 'AD2000'
    res = _re_cell_ref.match(cellrange)
    if res:
        row1, col1 = cell_to_rowcol2(res.group(1))
        return row1, col1, row1, col1
    raise Exception("Unknown cell reference %s" % (cell))


def cell_to_packed_rowcol(cell):
    """ pack row and column into the required 4 byte format """
    row, col, row_abs, col_abs = cell_to_rowcol(cell)
    if col >= MAX_COL:
        raise Exception("Column %s greater than IV in formula" % cell)
    if row >= MAX_ROW: # this for BIFF8. for BIFF7 available 2^14
        raise Exception("Row %s greater than %d in formula" % (cell, MAX_ROW))
    col |= int(not row_abs) << 15
    col |= int(not col_abs) << 14
    return row, col

########NEW FILE########
__FILENAME__ = Workbook
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


'''
Record Order in BIFF8
  Workbook Globals Substream
      BOF Type = workbook globals
      Interface Header
      MMS
      Interface End
      WRITEACCESS
      CODEPAGE
      DSF
      TABID
      FNGROUPCOUNT
      Workbook Protection Block
            WINDOWPROTECT
            PROTECT
            PASSWORD
            PROT4REV
            PROT4REVPASS
      BACKUP
      HIDEOBJ 
      WINDOW1 
      DATEMODE 
      PRECISION
      REFRESHALL
      BOOKBOOL 
      FONT +
      FORMAT *
      XF +
      STYLE +
    ? PALETTE
      USESELFS
    
      BOUNDSHEET +
    
      COUNTRY 
    ? Link Table 
      SST 
      ExtSST
      EOF
'''


__rev_id__ = """$Id: Workbook.py,v 1.5 2005/10/26 07:44:24 rvk Exp $"""


import BIFFRecords
import Style
from Deco import accepts, returns


class Workbook(object):

    #################################################################
    ## Constructor
    #################################################################
    @accepts(object)
    def __init__(self):
        self.__owner = 'None'       
        self.__country_code = 0x07 
        self.__wnd_protect = 0
        self.__obj_protect = 0
        self.__protect = 0        
        self.__backup_on_save = 0
        # for WINDOW1 record
        self.__hpos_twips = 0x01E0
        self.__vpos_twips = 0x005A
        self.__width_twips = 0x3FCF
        self.__height_twips = 0x2A4E
        
        self.__active_sheet = 0
        self.__first_tab_index = 0
        self.__selected_tabs = 0x01
        self.__tab_width_twips = 0x0258
        
        self.__wnd_hidden = 0
        self.__wnd_mini = 0
        self.__hscroll_visible = 1
        self.__vscroll_visible = 1
        self.__tabs_visible = 1

        self.__styles = Style.StyleCollection()
         
        self.__dates_1904 = 0
        self.__use_cell_values = 1
        
        self.__sst = BIFFRecords.SharedStringTable()
        
        self.__worksheets = []

    #################################################################
    ## Properties, "getters", "setters"
    #################################################################

    @accepts(object, str)
    def set_owner(self, value):
        self.__owner = value

    def get_owner(self):
        return self.__owner

    owner = property(get_owner, set_owner)

    #################################################################

    @accepts(object, int)
    def set_country_code(self, value):
        self.__country_code = value

    def get_country_code(self):
        return self.__country_code

    country_code = property(get_country_code, set_country_code)

    #################################################################

    @accepts(object, bool)
    def set_wnd_protect(self, value):
        self.__wnd_protect = int(value)

    def get_wnd_protect(self):
        return bool(self.__wnd_protect)

    wnd_protect = property(get_wnd_protect, set_wnd_protect)

    #################################################################

    @accepts(object, bool)
    def set_obj_protect(self, value):
        self.__obj_protect = int(value)

    def get_obj_protect(self):
        return bool(self.__obj_protect)

    obj_protect = property(get_obj_protect, set_obj_protect)

    #################################################################

    @accepts(object, bool)
    def set_protect(self, value):
        self.__protect = int(value)

    def get_protect(self):
        return bool(self.__protect)

    protect = property(get_protect, set_protect)
    
    #################################################################

    @accepts(object, bool)
    def set_backup_on_save(self, value):
        self.__backup_on_save = int(value)

    def get_backup_on_save(self):
        return bool(self.__backup_on_save)

    backup_on_save = property(get_backup_on_save, set_backup_on_save)

    #################################################################

    @accepts(object, int)
    def set_hpos(self, value):
        self.__hpos_twips = value & 0xFFFF

    def get_hpos(self):
        return self.__hpos_twips

    hpos = property(get_hpos, set_hpos)

    #################################################################

    @accepts(object, int)
    def set_vpos(self, value):
        self.__vpos_twips = value & 0xFFFF

    def get_vpos(self):
        return self.__vpos_twips

    vpos = property(get_vpos, set_vpos)

    #################################################################

    @accepts(object, int)
    def set_width(self, value):
        self.__width_twips = value & 0xFFFF

    def get_width(self):
        return self.__width_twips

    width = property(get_width, set_width)

    #################################################################

    @accepts(object, int)
    def set_height(self, value):
        self.__height_twips = value & 0xFFFF

    def get_height(self):
        return self.__height_twips

    height = property(get_height, set_height)

    #################################################################

    @accepts(object, int)
    def set_active_sheet(self, value):
        self.__active_sheet = value & 0xFFFF
        self.__first_tab_index = self.__active_sheet

    def get_active_sheet(self):
        return self.__active_sheet

    active_sheet = property(get_active_sheet, set_active_sheet)

    #################################################################

    @accepts(object, int)
    def set_tab_width(self, value):
        self.__tab_width_twips = value & 0xFFFF

    def get_tab_width(self):
        return self.__tab_width_twips

    tab_width = property(get_tab_width, set_tab_width)

    #################################################################

    @accepts(object, bool)
    def set_wnd_visible(self, value):
        self.__wnd_hidden = int(not value)

    def get_wnd_visible(self):
        return not bool(self.__wnd_hidden)

    wnd_visible = property(get_wnd_visible, set_wnd_visible)

    #################################################################

    @accepts(object, bool)
    def set_wnd_mini(self, value):
        self.__wnd_mini = int(value)

    def get_wnd_mini(self):
        return bool(self.__wnd_mini)

    wnd_mini = property(get_wnd_mini, set_wnd_mini)

    #################################################################

    @accepts(object, bool)
    def set_hscroll_visible(self, value):
        self.__hscroll_visible = int(value)

    def get_hscroll_visible(self):
        return bool(self.__hscroll_visible)

    hscroll_visible = property(get_hscroll_visible, set_hscroll_visible)

    #################################################################

    @accepts(object, bool)
    def set_vscroll_visible(self, value):
        self.__vscroll_visible = int(value)

    def get_vscroll_visible(self):
        return bool(self.__vscroll_visible)

    vscroll_visible = property(get_vscroll_visible, set_vscroll_visible)

    #################################################################

    @accepts(object, bool)
    def set_tabs_visible(self, value):
        self.__tabs_visible = int(value)

    def get_tabs_visible(self):
        return bool(self.__tabs_visible)

    tabs_visible = property(get_tabs_visible, set_tabs_visible)

    #################################################################

    @accepts(object, bool)
    def set_dates_1904(self, value):
        self.__dates_1904 = int(value)

    def get_dates_1904(self):
        return bool(self.__dates_1904)

    dates_1904 = property(get_dates_1904, set_dates_1904)

    #################################################################

    @accepts(object, bool)
    def set_use_cell_values(self, value):
        self.__use_cell_values = int(value)

    def get_use_cell_values(self):
        return bool(self.__use_cell_values)

    use_cell_values = property(get_use_cell_values, set_use_cell_values)

    #################################################################

    def get_default_style(self):
        return self.__styles.default_style

    default_style = property(get_default_style)

    ##################################################################
    ## Methods
    ##################################################################

    @accepts(object, Style.XFStyle)
    def add_style(self, style):
        return self.__styles.add(style)

    @accepts(object, (str, unicode))    
    def add_str(self, s):
        return self.__sst.add_str(s)
        
    @accepts(object, str)    
    def str_index(self, s):
        return self.__sst.str_index(s)
        
    @accepts(object, (str, unicode))    
    def add_sheet(self, sheetname):
        import Worksheet
        self.__worksheets.append(Worksheet.Worksheet(sheetname, self))
        return self.__worksheets[-1]

    @accepts(object, int)    
    def get_sheet(self, sheetnum):
        return self.__worksheets[sheetnum]
        
    ##################################################################
    ## BIFF records generation
    ##################################################################

    def __bof_rec(self):
        return BIFFRecords.Biff8BOFRecord(BIFFRecords.Biff8BOFRecord.BOOK_GLOBAL).get()

    def __eof_rec(self):
        return BIFFRecords.EOFRecord().get()
        
    def __intf_hdr_rec(self):
        return BIFFRecords.InteraceHdrRecord().get()

    def __intf_end_rec(self):
        return BIFFRecords.InteraceEndRecord().get()

    def __intf_mms_rec(self):
        return BIFFRecords.MMSRecord().get()

    def __write_access_rec(self):
        return BIFFRecords.WriteAccessRecord(self.__owner).get()

    def __wnd_protect_rec(self):
        return BIFFRecords.WindowProtectRecord(self.__wnd_protect).get()

    def __obj_protect_rec(self):
        return BIFFRecords.ObjectProtectRecord(self.__obj_protect).get()

    def __protect_rec(self):
        return BIFFRecords.ProtectRecord(self.__protect).get()

    def __password_rec(self):
        return BIFFRecords.PasswordRecord().get()

    def __prot4rev_rec(self):
        return BIFFRecords.Prot4RevRecord().get()

    def __prot4rev_pass_rec(self):
        return BIFFRecords.Prot4RevPassRecord().get()

    def __backup_rec(self):
        return BIFFRecords.BackupRecord(self.__backup_on_save).get()
        
    def __hide_obj_rec(self):
        return BIFFRecords.HideObjRecord().get()
        
    def __window1_rec(self):
        flags = 0
        flags |= (self.__wnd_hidden) << 0
        flags |= (self.__wnd_mini) << 1
        flags |= (self.__hscroll_visible) << 3
        flags |= (self.__vscroll_visible) << 4
        flags |= (self.__tabs_visible) << 5
        
        return BIFFRecords.Window1Record(self.__hpos_twips, self.__vpos_twips, 
                                self.__width_twips, self.__height_twips, 
                                flags,
                                self.__active_sheet, self.__first_tab_index, 
                                self.__selected_tabs, self.__tab_width_twips).get()
        
    def __codepage_rec(self):
        return BIFFRecords.CodepageBiff8Record().get()
        
    def __country_rec(self):
        return BIFFRecords.CountryRecord(self.__country_code, self.__country_code).get()
        
    def __dsf_rec(self):
        return BIFFRecords.DSFRecord().get()
        
    def __tabid_rec(self):
        return BIFFRecords.TabIDRecord(len(self.__worksheets)).get()
        
    def __fngroupcount_rec(self):
        return BIFFRecords.FnGroupCountRecord().get()
        
    def __datemode_rec(self):
        return BIFFRecords.DateModeRecord(self.__dates_1904).get()        

    def __precision_rec(self):
        return BIFFRecords.PrecisionRecord(self.__use_cell_values).get()         

    def __refresh_all_rec(self):
        return BIFFRecords.RefreshAllRecord().get()        

    def __bookbool_rec(self):
        return BIFFRecords.BookBoolRecord().get()         

    def __all_fonts_num_formats_xf_styles_rec(self):
        return self.__styles.get_biff_data()

    def __palette_rec(self):
        result = ''
        return result
        
    def __useselfs_rec(self):
        return BIFFRecords.UseSelfsRecord().get()
        
    def __boundsheets_rec(self, data_len_before, data_len_after, sheet_biff_lens):
        #  .................................  
        # BOUNDSEHEET0
        # BOUNDSEHEET1
        # BOUNDSEHEET2
        # ..................................
        # WORKSHEET0
        # WORKSHEET1
        # WORKSHEET2
        boundsheets_len = 0
        for sheet in self.__worksheets:
            boundsheets_len += len(BIFFRecords.BoundSheetRecord(0x00L, sheet.hidden, sheet.name).get())
        
        start = data_len_before + boundsheets_len + data_len_after
        
        result = ''
        for sheet_biff_len,  sheet in zip(sheet_biff_lens, self.__worksheets):
            result += BIFFRecords.BoundSheetRecord(start, sheet.hidden, sheet.name).get()
            start += sheet_biff_len            
        return result

    def __all_links_rec(self):
        result = ''
        return result
        
    def __sst_rec(self):
        return self.__sst.get_biff_record()
        
    def __ext_sst_rec(self, abs_stream_pos):
        return ''
        #return BIFFRecords.ExtSSTRecord(abs_stream_pos, self.sst_record.str_placement,
        #self.sst_record.portions_len).get()

    def get_biff_data(self):
        before = ''
        before += self.__bof_rec()
        before += self.__intf_hdr_rec()
        before += self.__intf_mms_rec()
        before += self.__intf_end_rec()
        before += self.__write_access_rec()
        before += self.__codepage_rec()
        before += self.__dsf_rec() 
        before += self.__tabid_rec() 
        before += self.__fngroupcount_rec()
        before += self.__wnd_protect_rec()
        before += self.__protect_rec()
        before += self.__obj_protect_rec()
        before += self.__password_rec()
        before += self.__prot4rev_rec()
        before += self.__prot4rev_pass_rec()
        before += self.__backup_rec()        
        before += self.__hide_obj_rec()        
        before += self.__window1_rec()
        before += self.__datemode_rec()
        before += self.__precision_rec()
        before += self.__refresh_all_rec()
        before += self.__bookbool_rec()
        before += self.__all_fonts_num_formats_xf_styles_rec()
        before += self.__palette_rec()
        before += self.__useselfs_rec()
        
        country            = self.__country_rec()
        all_links          = self.__all_links_rec()
        
        shared_str_table   = self.__sst_rec()        
        after = country + all_links + shared_str_table
        
        ext_sst = self.__ext_sst_rec(0) # need fake cause we need calc stream pos
        eof = self.__eof_rec()

        self.__worksheets[self.__active_sheet].selected = True
        sheets = ''
        sheet_biff_lens = []
        for sheet in self.__worksheets:
            data = sheet.get_biff_data()
            sheets += data
            sheet_biff_lens.append(len(data))
            
        bundlesheets = self.__boundsheets_rec(len(before), len(after)+len(ext_sst)+len(eof), sheet_biff_lens)       
       
        sst_stream_pos = len(before) + len(bundlesheets) + len(country)  + len(all_links)
        ext_sst = self.__ext_sst_rec(sst_stream_pos)           
        
        return before + bundlesheets + after + ext_sst + eof + sheets

    def save(self, filename):
        import CompoundDoc

        doc = CompoundDoc.XlsDoc()
        doc.save(filename, self.get_biff_data())

if __name__ == '__main__':
    wb = Workbook()
    f = file('workbook.bin', 'wb')
    f.write(wb.get_biff_data())
    f.close()
    
########NEW FILE########
__FILENAME__ = Worksheet
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
#
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
#
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
#
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
#
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
#
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.


'''
            BOF
            UNCALCED
            INDEX
            Calculation Settings Block
            PRINTHEADERS
            PRINTGRIDLINES
            GRIDSET
            GUTS
            DEFAULTROWHEIGHT
            WSBOOL
            Page Settings Block
            Worksheet Protection Block
            DEFCOLWIDTH
            COLINFO
            SORT
            DIMENSIONS
            Row Blocks
            WINDOW2
            SCL
            PANE
            SELECTION
            STANDARDWIDTH
            MERGEDCELLS
            LABELRANGES
            PHONETIC
            Conditional Formatting Table
            Hyperlink Table
            Data Validity Table
            SHEETLAYOUT (BIFF8X only)
            SHEETPROTECTION (BIFF8X only)
            RANGEPROTECTION (BIFF8X only)
            EOF
'''

__rev_id__ = """$Id: Worksheet.py,v 1.8 2005/10/26 07:44:24 rvk Exp $"""


import BIFFRecords
import Bitmap
import Formatting
import Style
from Deco import *


class Worksheet(object):
    from Workbook import Workbook

    #################################################################
    ## Constructor
    #################################################################
    @accepts(object, (str, unicode), Workbook)
    def __init__(self, sheetname, parent_book):
        import Row
        self.Row = Row.Row

        import Column
        self.Column = Column.Column

        self.__name = sheetname
        self.__parent = parent_book

        self.__rows = {}
        self.__cols = {}
        self.__merged_ranges = []
        self.__bmp_rec = ''

        self.__show_formulas = 0
        self.__show_grid = 1
        self.__show_headers = 1
        self.__panes_frozen = 0
        self.__show_empty_as_zero = 1
        self.__auto_colour_grid = 1
        self.__cols_right_to_left = 0
        self.__show_outline = 1
        self.__remove_splits = 0
        self.__selected = 0
        self.__hidden = 0
        self.__page_preview = 0

        self.__first_visible_row = 0
        self.__first_visible_col = 0
        self.__grid_colour = 0x40
        self.__preview_magn = 0
        self.__normal_magn = 0

        self.__vert_split_pos = None
        self.__horz_split_pos = None
        self.__vert_split_first_visible = None
        self.__horz_split_first_visible = None
        self.__split_active_pane = None

        self.__row_gut_width = 0
        self.__col_gut_height = 0

        self.__show_auto_page_breaks = 1
        self.__dialogue_sheet = 0
        self.__auto_style_outline = 0
        self.__outline_below = 0
        self.__outline_right = 0
        self.__fit_num_pages = 0
        self.__show_row_outline = 1
        self.__show_col_outline = 1
        self.__alt_expr_eval = 0
        self.__alt_formula_entries = 0

        self.__row_default_height = 0x00FF
        self.__col_default_width = 0x0008

        self.__calc_mode = 1
        self.__calc_count = 0x0064
        self.__RC_ref_mode = 1
        self.__iterations_on = 0
        self.__delta = 0.001
        self.__save_recalc = 0

        self.__print_headers = 0
        self.__print_grid = 0
        self.__grid_set = 1
        self.__vert_page_breaks = []
        self.__horz_page_breaks = []
        self.__header_str = '&P'
        self.__footer_str = '&F'
        self.__print_centered_vert = 0
        self.__print_centered_horz = 1
        self.__left_margin = 0.3 #0.5
        self.__right_margin = 0.3 #0.5
        self.__top_margin = 0.61 #1.0
        self.__bottom_margin = 0.37 #1.0
        self.__paper_size_code = 9 # A4
        self.__print_scaling = 100
        self.__start_page_number = 1
        self.__fit_width_to_pages = 1
        self.__fit_height_to_pages = 1
        self.__print_in_rows = 1
        self.__portrait = 1
        self.__print_not_colour = 0
        self.__print_draft = 0
        self.__print_notes = 0
        self.__print_notes_at_end = 0
        self.__print_omit_errors = 0
        self.__print_hres = 0x012C # 300 dpi
        self.__print_vres = 0x012C # 300 dpi
        self.__header_margin = 0.1
        self.__footer_margin = 0.1
        self.__copies_num = 1

        self.__wnd_protect = 0
        self.__obj_protect = 0
        self.__protect = 0
        self.__scen_protect = 0
        self.__password = ''

    #################################################################
    ## Properties, "getters", "setters"
    #################################################################

    @accepts(object, (str, unicode))
    def set_name(self, value):
        self.__name = value

    def get_name(self):
        return self.__name

    name = property(get_name, set_name)

    #################################################################

    def get_parent(self):
        return self.__parent

    parent = property(get_parent)

    #################################################################

    def get_rows(self):
        return self.__rows

    rows = property(get_rows)

    #################################################################

    def get_cols(self):
        return self.__cols

    cols = property(get_cols)

    #################################################################

    def get_merged_ranges(self):
        return self.__merged_ranges

    merged_ranges = property(get_merged_ranges)

    #################################################################

    def get_bmp_rec(self):
        return self.__bmp_rec

    bmp_rec = property(get_bmp_rec)

    #################################################################

    @accepts(object, bool)
    def set_show_formulas(self, value):
        self.__show_formulas = int(value)

    def get_show_formulas(self):
        return bool(self.__show_formulas)

    show_formulas = property(get_show_formulas, set_show_formulas)

    #################################################################

    @accepts(object, bool)
    def set_show_grid(self, value):
        self.__show_grid = int(value)

    def get_show_grid(self):
        return bool(self.__show_grid)

    show_grid = property(get_show_grid, set_show_grid)

    #################################################################

    @accepts(object, bool)
    def set_show_headers(self, value):
        self.__show_headers = int(value)

    def get_show_headers(self):
        return bool(self.__show_headers)

    show_headers = property(get_show_headers, set_show_headers)

    #################################################################

    @accepts(object, bool)
    def set_panes_frozen(self, value):
        self.__panes_frozen = int(value)

    def get_panes_frozen(self):
        return bool(self.__panes_frozen)

    panes_frozen = property(get_panes_frozen, set_panes_frozen)

    #################################################################

    @accepts(object, bool)
    def set_show_empty_as_zero(self, value):
        self.__show_empty_as_zero = int(value)

    def get_show_empty_as_zero(self):
        return bool(self.__show_empty_as_zero)

    show_empty_as_zero = property(get_show_empty_as_zero, set_show_empty_as_zero)

    #################################################################

    @accepts(object, bool)
    def set_auto_colour_grid(self, value):
        self.__auto_colour_grid = int(value)

    def get_auto_colour_grid(self):
        return bool(self.__auto_colour_grid)

    auto_colour_grid = property(get_auto_colour_grid, set_auto_colour_grid)

    #################################################################

    @accepts(object, bool)
    def set_cols_right_to_left(self, value):
        self.__cols_right_to_left = int(value)

    def get_cols_right_to_left(self):
        return bool(self.__cols_right_to_left)

    cols_right_to_left = property(get_cols_right_to_left, set_cols_right_to_left)

    #################################################################

    @accepts(object, bool)
    def set_show_outline(self, value):
        self.__show_outline = int(value)

    def get_show_outline(self):
        return bool(self.__show_outline)

    show_outline = property(get_show_outline, set_show_outline)

    #################################################################

    @accepts(object, bool)
    def set_remove_splits(self, value):
        self.__remove_splits = int(value)

    def get_remove_splits(self):
        return bool(self.__remove_splits)

    remove_splits = property(get_remove_splits, set_remove_splits)

    #################################################################

    @accepts(object, bool)
    def set_selected(self, value):
        self.__selected = int(value)

    def get_selected(self):
        return bool(self.__selected)

    selected = property(get_selected, set_selected)

    #################################################################

    @accepts(object, bool)
    def set_hidden(self, value):
        self.__hidden = int(value)

    def get_hidden(self):
        return bool(self.__hidden)

    hidden = property(get_hidden, set_hidden)

    #################################################################

    @accepts(object, bool)
    def set_page_preview(self, value):
        self.__page_preview = int(value)

    def get_page_preview(self):
        return bool(self.__page_preview)

    page_preview = property(get_page_preview, set_page_preview)

    #################################################################

    @accepts(object, int)
    def set_first_visible_row(self, value):
        self.__first_visible_row = value

    def get_first_visible_row(self):
        return self.__first_visible_row

    first_visible_row = property(get_first_visible_row, set_first_visible_row)

    #################################################################

    @accepts(object, int)
    def set_first_visible_col(self, value):
        self.__first_visible_col = value

    def get_first_visible_col(self):
        return self.__first_visible_col

    first_visible_col = property(get_first_visible_col, set_first_visible_col)

    #################################################################

    @accepts(object, int)
    def set_grid_colour(self, value):
        self.__grid_colour = value

    def get_grid_colour(self):
        return self.__grid_colour

    grid_colour = property(get_grid_colour, set_grid_colour)

    #################################################################

    @accepts(object, int)
    def set_preview_magn(self, value):
        self.__preview_magn = value

    def get_preview_magn(self):
        return self.__preview_magn

    preview_magn = property(get_preview_magn, set_preview_magn)

    #################################################################

    @accepts(object, int)
    def set_normal_magn(self, value):
        self.__normal_magn = value

    def get_normal_magn(self):
        return self.__normal_magn

    normal_magn = property(get_normal_magn, set_normal_magn)

    #################################################################

    @accepts(object, int)
    def set_vert_split_pos(self, value):
        self.__vert_split_pos = abs(value)

    def get_vert_split_pos(self):
        return self.__vert_split_pos

    vert_split_pos = property(get_vert_split_pos, set_vert_split_pos)

    #################################################################

    @accepts(object, int)
    def set_horz_split_pos(self, value):
        self.__horz_split_pos = abs(value)

    def get_horz_split_pos(self):
        return self.__horz_split_pos

    horz_split_pos = property(get_horz_split_pos, set_horz_split_pos)

    #################################################################

    @accepts(object, int)
    def set_vert_split_first_visible(self, value):
        self.__vert_split_first_visible = abs(value)

    def get_vert_split_first_visible(self):
        return self.__vert_split_first_visible

    vert_split_first_visible = property(get_vert_split_first_visible, set_vert_split_first_visible)

    #################################################################

    @accepts(object, int)
    def set_horz_split_first_visible(self, value):
        self.__horz_split_first_visible = abs(value)

    def get_horz_split_first_visible(self):
        return self.__horz_split_first_visible

    horz_split_first_visible = property(get_horz_split_first_visible, set_horz_split_first_visible)

    #################################################################

    #@accepts(object, int)
    #def set_split_active_pane(self, value):
    #    self.__split_active_pane = abs(value) & 0x03
    #
    #def get_split_active_pane(self):
    #    return self.__split_active_pane
    #
    #split_active_pane = property(get_split_active_pane, set_split_active_pane)

    #################################################################

    #@accepts(object, int)
    #def set_row_gut_width(self, value):
    #    self.__row_gut_width = value
    #
    #def get_row_gut_width(self):
    #    return self.__row_gut_width
    #
    #row_gut_width = property(get_row_gut_width, set_row_gut_width)
    #
    #################################################################
    #
    #@accepts(object, int)
    #def set_col_gut_height(self, value):
    #    self.__col_gut_height = value
    #
    #def get_col_gut_height(self):
    #    return self.__col_gut_height
    #
    #col_gut_height = property(get_col_gut_height, set_col_gut_height)
    #
    #################################################################

    @accepts(object, bool)
    def set_show_auto_page_breaks(self, value):
        self.__show_auto_page_breaks = int(value)

    def get_show_auto_page_breaks(self):
        return bool(self.__show_auto_page_breaks)

    show_auto_page_breaks = property(get_show_auto_page_breaks, set_show_auto_page_breaks)

    #################################################################

    @accepts(object, bool)
    def set_dialogue_sheet(self, value):
        self.__dialogue_sheet = int(value)

    def get_dialogue_sheet(self):
        return bool(self.__dialogue_sheet)

    dialogue_sheet = property(get_dialogue_sheet, set_dialogue_sheet)

    #################################################################

    @accepts(object, bool)
    def set_auto_style_outline(self, value):
        self.__auto_style_outline = int(value)

    def get_auto_style_outline(self):
        return bool(self.__auto_style_outline)

    auto_style_outline = property(get_auto_style_outline, set_auto_style_outline)

    #################################################################

    @accepts(object, bool)
    def set_outline_below(self, value):
        self.__outline_below = int(value)

    def get_outline_below(self):
        return bool(self.__outline_below)

    outline_below = property(get_outline_below, set_outline_below)

    #################################################################

    @accepts(object, bool)
    def set_outline_right(self, value):
        self.__outline_right = int(value)

    def get_outline_right(self):
        return bool(self.__outline_right)

    outline_right = property(get_outline_right, set_outline_right)

    #################################################################

    @accepts(object, int)
    def set_fit_num_pages(self, value):
        self.__fit_num_pages = value

    def get_fit_num_pages(self):
        return self.__fit_num_pages

    fit_num_pages = property(get_fit_num_pages, set_fit_num_pages)

    #################################################################

    @accepts(object, bool)
    def set_show_row_outline(self, value):
        self.__show_row_outline = int(value)

    def get_show_row_outline(self):
        return bool(self.__show_row_outline)

    show_row_outline = property(get_show_row_outline, set_show_row_outline)

    #################################################################

    @accepts(object, bool)
    def set_show_col_outline(self, value):
        self.__show_col_outline = int(value)

    def get_show_col_outline(self):
        return bool(self.__show_col_outline)

    show_col_outline = property(get_show_col_outline, set_show_col_outline)

    #################################################################

    @accepts(object, bool)
    def set_alt_expr_eval(self, value):
        self.__alt_expr_eval = int(value)

    def get_alt_expr_eval(self):
        return bool(self.__alt_expr_eval)

    alt_expr_eval = property(get_alt_expr_eval, set_alt_expr_eval)

    #################################################################

    @accepts(object, bool)
    def set_alt_formula_entries(self, value):
        self.__alt_formula_entries = int(value)

    def get_alt_formula_entries(self):
        return bool(self.__alt_formula_entries)

    alt_formula_entries = property(get_alt_formula_entries, set_alt_formula_entries)

    #################################################################

    @accepts(object, int)
    def set_row_default_height(self, value):
        self.__row_default_height = value

    def get_row_default_height(self):
        return self.__row_default_height

    row_default_height = property(get_row_default_height, set_row_default_height)

    #################################################################

    @accepts(object, int)
    def set_col_default_width(self, value):
        self.__col_default_width = value

    def get_col_default_width(self):
        return self.__col_default_width

    col_default_width = property(get_col_default_width, set_col_default_width)

    #################################################################

    @accepts(object, int)
    def set_calc_mode(self, value):
        self.__calc_mode = value & 0x03

    def get_calc_mode(self):
        return self.__calc_mode

    calc_mode = property(get_calc_mode, set_calc_mode)

    #################################################################

    @accepts(object, int)
    def set_calc_count(self, value):
        self.__calc_count = value

    def get_calc_count(self):
        return self.__calc_count

    calc_count = property(get_calc_count, set_calc_count)

    #################################################################

    @accepts(object, bool)
    def set_RC_ref_mode(self, value):
        self.__RC_ref_mode = int(value)

    def get_RC_ref_mode(self):
        return bool(self.__RC_ref_mode)

    RC_ref_mode = property(get_RC_ref_mode, set_RC_ref_mode)

    #################################################################

    @accepts(object, bool)
    def set_iterations_on(self, value):
        self.__iterations_on = int(value)

    def get_iterations_on(self):
        return bool(self.__iterations_on)

    iterations_on = property(get_iterations_on, set_iterations_on)

    #################################################################

    @accepts(object, float)
    def set_delta(self, value):
        self.__delta = value

    def get_delta(self):
        return self.__delta

    delta = property(get_delta, set_delta)

    #################################################################

    @accepts(object, bool)
    def set_save_recalc(self, value):
        self.__save_recalc = int(value)

    def get_save_recalc(self):
        return bool(self.__save_recalc)

    save_recalc = property(get_save_recalc, set_save_recalc)

    #################################################################

    @accepts(object, bool)
    def set_print_headers(self, value):
        self.__print_headers = int(value)

    def get_print_headers(self):
        return bool(self.__print_headers)

    print_headers = property(get_print_headers, set_print_headers)

    #################################################################

    @accepts(object, bool)
    def set_print_grid(self, value):
        self.__print_grid = int(value)

    def get_print_grid(self):
        return bool(self.__print_grid)

    print_grid = property(get_print_grid, set_print_grid)

    #################################################################
    #
    #@accepts(object, bool)
    #def set_grid_set(self, value):
    #    self.__grid_set = int(value)
    #
    #def get_grid_set(self):
    #    return bool(self.__grid_set)
    #
    #grid_set = property(get_grid_set, set_grid_set)
    #
    #################################################################

    @accepts(object, list)
    def set_vert_page_breaks(self, value):
        self.__vert_page_breaks = value

    def get_vert_page_breaks(self):
        return self.__vert_page_breaks

    vert_page_breaks = property(get_vert_page_breaks, set_vert_page_breaks)

    #################################################################

    @accepts(object, list)
    def set_horz_page_breaks(self, value):
        self.__horz_page_breaks = value

    def get_horz_page_breaks(self):
        return self.__horz_page_breaks

    horz_page_breaks = property(get_horz_page_breaks, set_horz_page_breaks)

    #################################################################

    @accepts(object, (str, unicode))
    def set_header_str(self, value):
        self.__header_str = value

    def get_header_str(self):
        return self.__header_str

    header_str = property(get_header_str, set_header_str)

    #################################################################

    @accepts(object, (str, unicode))
    def set_footer_str(self, value):
        self.__footer_str = value

    def get_footer_str(self):
        return self.__footer_str

    footer_str = property(get_footer_str, set_footer_str)

    #################################################################

    @accepts(object, bool)
    def set_print_centered_vert(self, value):
        self.__print_centered_vert = int(value)

    def get_print_centered_vert(self):
        return bool(self.__print_centered_vert)

    print_centered_vert = property(get_print_centered_vert, set_print_centered_vert)

    #################################################################

    @accepts(object, bool)
    def set_print_centered_horz(self, value):
        self.__print_centered_horz = int(value)

    def get_print_centered_horz(self):
        return bool(self.__print_centered_horz)

    print_centered_horz = property(get_print_centered_horz, set_print_centered_horz)

    #################################################################

    @accepts(object, float)
    def set_left_margin(self, value):
        self.__left_margin = value

    def get_left_margin(self):
        return self.__left_margin

    left_margin = property(get_left_margin, set_left_margin)

    #################################################################

    @accepts(object, float)
    def set_right_margin(self, value):
        self.__right_margin = value

    def get_right_margin(self):
        return self.__right_margin

    right_margin = property(get_right_margin, set_right_margin)

    #################################################################

    @accepts(object, float)
    def set_top_margin(self, value):
        self.__top_margin = value

    def get_top_margin(self):
        return self.__top_margin

    top_margin = property(get_top_margin, set_top_margin)

    #################################################################

    @accepts(object, float)
    def set_bottom_margin(self, value):
        self.__bottom_margin = value

    def get_bottom_margin(self):
        return self.__bottom_margin

    bottom_margin = property(get_bottom_margin, set_bottom_margin)

    #################################################################

    @accepts(object, int)
    def set_paper_size_code(self, value):
        self.__paper_size_code = value

    def get_paper_size_code(self):
        return self.__paper_size_code

    paper_size_code = property(get_paper_size_code, set_paper_size_code)

    #################################################################

    @accepts(object, int)
    def set_print_scaling(self, value):
        self.__print_scaling = value

    def get_print_scaling(self):
        return self.__print_scaling

    print_scaling = property(get_print_scaling, set_print_scaling)

    #################################################################

    @accepts(object, int)
    def set_start_page_number(self, value):
        self.__start_page_number = value

    def get_start_page_number(self):
        return self.__start_page_number

    start_page_number = property(get_start_page_number, set_start_page_number)

    #################################################################

    @accepts(object, int)
    def set_fit_width_to_pages(self, value):
        self.__fit_width_to_pages = value

    def get_fit_width_to_pages(self):
        return self.__fit_width_to_pages

    fit_width_to_pages = property(get_fit_width_to_pages, set_fit_width_to_pages)

    #################################################################

    @accepts(object, int)
    def set_fit_height_to_pages(self, value):
        self.__fit_height_to_pages = value

    def get_fit_height_to_pages(self):
        return self.__fit_height_to_pages

    fit_height_to_pages = property(get_fit_height_to_pages, set_fit_height_to_pages)

    #################################################################

    @accepts(object, bool)
    def set_print_in_rows(self, value):
        self.__print_in_rows = int(value)

    def get_print_in_rows(self):
        return bool(self.__print_in_rows)

    print_in_rows = property(get_print_in_rows, set_print_in_rows)

    #################################################################

    @accepts(object, bool)
    def set_portrait(self, value):
        self.__portrait = int(value)

    def get_portrait(self):
        return bool(self.__portrait)

    portrait = property(get_portrait, set_portrait)

    #################################################################

    @accepts(object, bool)
    def set_print_colour(self, value):
        self.__print_not_colour = int(not value)

    def get_print_colour(self):
        return not bool(self.__print_not_colour)

    print_colour = property(get_print_colour, set_print_colour)

    #################################################################

    @accepts(object, bool)
    def set_print_draft(self, value):
        self.__print_draft = int(value)

    def get_print_draft(self):
        return bool(self.__print_draft)

    print_draft = property(get_print_draft, set_print_draft)

    #################################################################

    @accepts(object, bool)
    def set_print_notes(self, value):
        self.__print_notes = int(value)

    def get_print_notes(self):
        return bool(self.__print_notes)

    print_notes = property(get_print_notes, set_print_notes)

    #################################################################

    @accepts(object, bool)
    def set_print_notes_at_end(self, value):
        self.__print_notes_at_end = int(value)

    def get_print_notes_at_end(self):
        return bool(self.__print_notes_at_end)

    print_notes_at_end = property(get_print_notes_at_end, set_print_notes_at_end)

    #################################################################

    @accepts(object, bool)
    def set_print_omit_errors(self, value):
        self.__print_omit_errors = int(value)

    def get_print_omit_errors(self):
        return bool(self.__print_omit_errors)

    print_omit_errors = property(get_print_omit_errors, set_print_omit_errors)

    #################################################################

    @accepts(object, int)
    def set_print_hres(self, value):
        self.__print_hres = value

    def get_print_hres(self):
        return self.__print_hres

    print_hres = property(get_print_hres, set_print_hres)

    #################################################################

    @accepts(object, int)
    def set_print_vres(self, value):
        self.__print_vres = value

    def get_print_vres(self):
        return self.__print_vres

    print_vres = property(get_print_vres, set_print_vres)

    #################################################################

    @accepts(object, float)
    def set_header_margin(self, value):
        self.__header_margin = value

    def get_header_margin(self):
        return self.__header_margin

    header_margin = property(get_header_margin, set_header_margin)

    #################################################################

    @accepts(object, float)
    def set_footer_margin(self, value):
        self.__footer_margin = value

    def get_footer_margin(self):
        return self.__footer_margin

    footer_margin = property(get_footer_margin, set_footer_margin)

    #################################################################

    @accepts(object, int)
    def set_copies_num(self, value):
        self.__copies_num = value

    def get_copies_num(self):
        return self.__copies_num

    copies_num = property(get_copies_num, set_copies_num)

    ##################################################################

    @accepts(object, bool)
    def set_wnd_protect(self, value):
        self.__wnd_protect = int(value)

    def get_wnd_protect(self):
        return bool(self.__wnd_protect)

    wnd_protect = property(get_wnd_protect, set_wnd_protect)

    #################################################################

    @accepts(object, bool)
    def set_obj_protect(self, value):
        self.__obj_protect = int(value)

    def get_obj_protect(self):
        return bool(self.__obj_protect)

    obj_protect = property(get_obj_protect, set_obj_protect)

    #################################################################

    @accepts(object, bool)
    def set_protect(self, value):
        self.__protect = int(value)

    def get_protect(self):
        return bool(self.__protect)

    protect = property(get_protect, set_protect)

    #################################################################

    @accepts(object, bool)
    def set_scen_protect(self, value):
        self.__scen_protect = int(value)

    def get_scen_protect(self):
        return bool(self.__scen_protect)

    scen_protect = property(get_scen_protect, set_scen_protect)

    #################################################################

    @accepts(object, str)
    def set_password(self, value):
        self.__password = value

    def get_password(self):
        return self.__password

    password = property(get_password, set_password)

    ##################################################################
    ## Methods
    ##################################################################

    def get_parent(self):
        return self.__parent

    def write(self, r, c, label="", style=Style.XFStyle()):
        self.row(r).write(c, label, style)

    def merge(self, r1, r2, c1, c2, style=Style.XFStyle()):
        self.row(r1).write_blanks(c1, c2,  style)
        for r in range(r1+1, r2+1):
            self.row(r).write_blanks(c1, c2,  style)
        self.__merged_ranges.append((r1, r2, c1, c2))

    def write_merge(self, r1, r2, c1, c2, label="", style=Style.XFStyle()):
        self.merge(r1, r2, c1, c2, style)
        self.write(r1, c1,  label, style)

    def insert_bitmap(self, filename, row, col, x = 0, y = 0, scale_x = 1, scale_y = 1):
        bmp = Bitmap.ImDataBmpRecord(filename)
        obj = Bitmap.ObjBmpRecord(row, col, self, bmp, x, y, scale_x, scale_y)

        self.__bmp_rec += obj.get() + bmp.get()

    def col(self, indx):
        if indx not in self.__cols:
            self.__cols[indx] = self.Column(indx, self)
        return self.__cols[indx]

    def row(self, indx):
        if indx not in self.__rows:
            self.__rows[indx] = self.Row(indx, self)
        return self.__rows[indx]

    def row_height(self, row): # in pixels
        if row in self.__rows:
            return self.__rows[row].get_height_in_pixels()
        else:
            return 17

    def col_width(self, col): # in pixels
        #if col in self.__cols:
        #    return self.__cols[col].width_in_pixels()
        #else:
            return 64

    def get_labels_count(self):
        result = 0
        for r in self.__rows:
            result += self.__rows[r].get_str_count()
        return result

    ##################################################################
    ## BIFF records generation
    ##################################################################

    def __bof_rec(self):
        return BIFFRecords.Biff8BOFRecord(BIFFRecords.Biff8BOFRecord.WORKSHEET).get()

    def __guts_rec(self):
        row_visible_levels = 0
        if len(self.__rows) != 0:
            row_visible_levels = max([self.__rows[r].level for r in self.__rows]) + 1

        col_visible_levels = 0
        if len(self.__cols) != 0:
            col_visible_levels = max([self.__cols[c].level for c in self.__cols]) + 1

        return BIFFRecords.GutsRecord(self.__row_gut_width, self.__col_gut_height, row_visible_levels, col_visible_levels).get()

    def __wsbool_rec(self):
        options = 0x00
        options |= (self.__show_auto_page_breaks & 0x01) << 0
        options |= (self.__dialogue_sheet & 0x01) << 4
        options |= (self.__auto_style_outline & 0x01) << 5
        options |= (self.__outline_below & 0x01) << 6
        options |= (self.__outline_right & 0x01) << 7
        options |= (self.__fit_num_pages & 0x01) << 8
        options |= (self.__show_row_outline & 0x01) << 10
        options |= (self.__show_col_outline & 0x01) << 11
        options |= (self.__alt_expr_eval & 0x01) << 14
        options |= (self.__alt_formula_entries & 0x01) << 15

        return BIFFRecords.WSBoolRecord(options).get()

    def __eof_rec(self):
        return BIFFRecords.EOFRecord().get()

    def __colinfo_rec(self):
        result = ''
        for col in self.__cols:
            result += self.__cols[col].get_biff_record()
        return result

    def __dimensions_rec(self):
        first_used_row = 0
        last_used_row = 0
        first_used_col = 0
        last_used_col = 0
        if len(self.__rows) > 0:
            first_used_row = min(self.__rows)
            last_used_row = max(self.__rows)
            first_used_col = 0xFFFFFFFF
            last_used_col = 0
            for r in self.__rows:
                _min = self.__rows[r].get_min_col()
                _max = self.__rows[r].get_max_col()
                if _min < first_used_col:
                    first_used_col = _min
                if _max > last_used_col:
                    last_used_col = _max

        return BIFFRecords.DimensionsRecord(first_used_row, last_used_row, first_used_col, last_used_col).get()

    def __window2_rec(self):
        options = 0
        options |= (self.__show_formulas        & 0x01) << 0
        options |= (self.__show_grid            & 0x01) << 1
        options |= (self.__show_headers         & 0x01) << 2
        options |= (self.__panes_frozen         & 0x01) << 3
        options |= (self.__show_empty_as_zero   & 0x01) << 4
        options |= (self.__auto_colour_grid     & 0x01) << 5
        options |= (self.__cols_right_to_left   & 0x01) << 6
        options |= (self.__show_outline         & 0x01) << 7
        options |= (self.__remove_splits        & 0x01) << 8
        options |= (self.__selected             & 0x01) << 9
        options |= (self.__hidden               & 0x01) << 10
        options |= (self.__page_preview         & 0x01) << 11

        return BIFFRecords.Window2Record(options, self.__first_visible_row, self.__first_visible_col,
                                        self.__grid_colour,
                                        self.__preview_magn, self.__normal_magn).get()

    def __panes_rec(self):
        if self.__vert_split_pos is None and self.__horz_split_pos is None:
            return ""

        if self.__vert_split_pos is None:
            self.__vert_split_pos = 0
        if self.__horz_split_pos is None:
            self.__horz_split_pos = 0

        if self.__panes_frozen:
            if self.__vert_split_first_visible is None:
                self.__vert_split_first_visible = self.__vert_split_pos
            if self.__horz_split_first_visible is None:
                self.__horz_split_first_visible = self.__horz_split_pos
        else:
            if self.__vert_split_first_visible is None:
                self.__vert_split_first_visible = 0
            if self.__horz_split_first_visible is None:
                self.__horz_split_first_visible = 0
            # inspired by pyXLWriter
            self.__horz_split_pos = 20*self.__horz_split_pos + 255
            self.__vert_split_pos = 113.879*self.__vert_split_pos + 390

        if self.__vert_split_pos > 0 and self.__horz_split_pos > 0:
            self.__split_active_pane = 0
        elif self.__vert_split_pos > 0 and self.__horz_split_pos == 0:
            self.__split_active_pane = 1
        elif self.__vert_split_pos == 0 and self.__horz_split_pos > 0:
            self.__split_active_pane = 2
        else:
            self.__split_active_pane = 3

        result = BIFFRecords.PanesRecord(self.__vert_split_pos,
                                         self.__horz_split_pos,
                                         self.__horz_split_first_visible,
                                         self.__vert_split_first_visible,
                                         self.__split_active_pane).get()
        return result

    def __row_blocks_rec(self):
        # this function takes almost 99% of overall execution time 
        # when file is saved
        # return '' 
        result = []
        i = 0
        used_rows = self.__rows.keys()
        while i < len(used_rows):
            j = 0
            while i < len(used_rows) and (j < 32):
                result.append(self.__rows[used_rows[i]].get_row_biff_data())
                result.append(self.__rows[used_rows[i]].get_cells_biff_data())
                j += 1
                i += 1

        return ''.join(result)

    def __merged_rec(self):
        return BIFFRecords.MergedCellsRecord(self.__merged_ranges).get()

    def __bitmaps_rec(self):
        return self.__bmp_rec

    def __calc_settings_rec(self):
        result = ''
        result += BIFFRecords.CalcModeRecord(self.__calc_mode & 0x01).get()
        result += BIFFRecords.CalcCountRecord(self.__calc_count & 0xFFFF).get()
        result += BIFFRecords.RefModeRecord(self.__RC_ref_mode & 0x01).get()
        result += BIFFRecords.IterationRecord(self.__iterations_on & 0x01).get()
        result += BIFFRecords.DeltaRecord(self.__delta).get()
        result += BIFFRecords.SaveRecalcRecord(self.__save_recalc & 0x01).get()
        return result

    def __print_settings_rec(self):
        result = ''
        result += BIFFRecords.PrintHeadersRecord(self.__print_headers).get()
        result += BIFFRecords.PrintGridLinesRecord(self.__print_grid).get()
        result += BIFFRecords.GridSetRecord(self.__grid_set).get()
        result += BIFFRecords.HorizontalPageBreaksRecord(self.__horz_page_breaks).get()
        result += BIFFRecords.VerticalPageBreaksRecord(self.__vert_page_breaks).get()
        result += BIFFRecords.HeaderRecord(self.__header_str).get()
        result += BIFFRecords.FooterRecord(self.__footer_str).get()
        result += BIFFRecords.HCenterRecord(self.__print_centered_horz).get()
        result += BIFFRecords.VCenterRecord(self.__print_centered_vert).get()
        result += BIFFRecords.LeftMarginRecord(self.__left_margin).get()
        result += BIFFRecords.RightMarginRecord(self.__right_margin).get()
        result += BIFFRecords.TopMarginRecord(self.__top_margin).get()
        result += BIFFRecords.BottomMarginRecord(self.__bottom_margin).get()

        setup_page_options =  (self.__print_in_rows & 0x01) << 0
        setup_page_options |=  (self.__portrait & 0x01) << 1
        setup_page_options |=  (0x00 & 0x01) << 2
        setup_page_options |=  (self.__print_not_colour & 0x01) << 3
        setup_page_options |=  (self.__print_draft & 0x01) << 4
        setup_page_options |=  (self.__print_notes & 0x01) << 5
        setup_page_options |=  (0x00 & 0x01) << 6
        setup_page_options |=  (0x01 & 0x01) << 7
        setup_page_options |=  (self.__print_notes_at_end & 0x01) << 9
        setup_page_options |=  (self.__print_omit_errors & 0x03) << 10

        result += BIFFRecords.SetupPageRecord(self.__paper_size_code,
                                self.__print_scaling,
                                self.__start_page_number,
                                self.__fit_width_to_pages,
                                self.__fit_height_to_pages,
                                setup_page_options,
                                self.__print_hres,
                                self.__print_vres,
                                self.__header_margin,
                                self.__footer_margin,
                                self.__copies_num).get()
        return result

    def __protection_rec(self):
        result = ''
        result += BIFFRecords.ProtectRecord(self.__protect).get()
        result += BIFFRecords.ScenProtectRecord(self.__scen_protect).get()
        result += BIFFRecords.WindowProtectRecord(self.__wnd_protect).get()
        result += BIFFRecords.ObjectProtectRecord(self.__obj_protect).get()
        result += BIFFRecords.PasswordRecord(self.__password).get()
        return result

    def get_biff_data(self):
        result = ''
        result += self.__bof_rec()
        result += self.__calc_settings_rec()
        result += self.__guts_rec()
        result += self.__wsbool_rec()
        result += self.__colinfo_rec()
        result += self.__dimensions_rec()
        result += self.__print_settings_rec()
        result += self.__protection_rec()
        result += self.__row_blocks_rec()
        result += self.__merged_rec()
        result += self.__bitmaps_rec()
        result += self.__window2_rec()
        result += self.__panes_rec()
        result += self.__eof_rec()

        return result




########NEW FILE########
__FILENAME__ = Analyzer
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.

__rev_id__ = """$Id: Analyzer.py,v 1.4 2005/05/12 06:51:17 rvk Exp $"""


# total 258 records in database
import struct

def analyze_1904_record(data):
    # 0x0022
    pass


def analyze_ADDIN_record(data):
    # 0x0087
    pass


def analyze_ADDMENU_record(data):
    # 0x00C2
    pass


def analyze_ARRAY_record(data):
    # 0x0021, 0x0221
    pass


def analyze_AUTOFILTER_record(data):
    # 0x009E
    pass


def analyze_AUTOFILTERINFO_record(data):
    # 0x009D
    pass


def analyze_BACKUP_record(data):
    # 0x0040
    pass


def analyze_BEGIN_record(data):
    # 0x1033
    pass


def analyze_BITMAP_record(data):
    # 0x00E9
    pass


def analyze_BLANK_record(data):
    # 0x0001, 0x0201
    pass


def analyze_BOF_record(data):
    # 0x0009, 0x0209, 0x0409, 0x0809
    pass


def analyze_BOOKBOOL_record(data):
    # 0x00DA
    pass


def analyze_BOOLERR_record(data):
    # 0x0005, 0x0205
    pass


def analyze_BOTTOMMARGIN_record(data):
    # 0x0029
    print struct.unpack('<d', data)[0]
    pass


def analyze_BOUNDSHEET_record(data):
    # 0x0085
    pass


def analyze_BUILTINFMTCNT_record(data):
    # 0x0056
    pass


def analyze_BUNDLEHEADER_record(data):
    # 0x008F
    pass


def analyze_BUNDLESOFFSET_record(data):
    # 0x008E
    pass


def analyze_CALCCOUNT_record(data):
    # 0x000C
    pass


def analyze_CALCMODE_record(data):
    # 0x000D
    pass


def analyze_MERGEDCELLS_record(data):
    # 0x00E5
    pass


def analyze_CF_record(data):
    # 0x01B1
    pass


def analyze_CHTRCELLCONTENT_record(data):
    # 0x013B
    pass


def analyze_CHTRHEADER_record(data):
    # 0x0196
    pass


def analyze_CHTRINFO_record(data):
    # 0x0138
    pass


def analyze_CHTRINSERT_record(data):
    # 0x0137
    pass


def analyze_CHTRINSERTTAB_record(data):
    # 0x014D
    pass


def analyze_CHTRMOVERANGE_record(data):
    # 0x0140
    pass


def analyze_CODENAME_record(data):
    # 0x01BA
    pass


def analyze_CODEPAGE_record(data):
    # 0x0042
    pass


def analyze_COLINFO_record(data):
    # 0x007D
    pass


def analyze_COLUMNDEFAULT_record(data):
    # 0x0020
    pass


def analyze_COLWIDTH_record(data):
    # 0x0024
    pass


def analyze_CONDFMT_record(data):
    # 0x01B0
    pass


def analyze_CONTINUE_record(data):
    # 0x003C
    pass


def analyze_COORDLIST_record(data):
    # 0x00A9
    pass


def analyze_COUNTRY_record(data):
    # 0x008C
    pass


def analyze_CRN_record(data):
    # 0x005A
    pass


def analyze_Chart3D_record(data):
    # 0x103A
    pass


def analyze_Chart3DDataFormat_record(data):
    # 0x105F
    pass


def analyze_ChartAI_record(data):
    # 0x1051
    pass


def analyze_ChartAlruns_record(data):
    # 0x1050
    pass


def analyze_ChartArea_record(data):
    # 0x101A
    pass


def analyze_ChartAreaformat_record(data):
    # 0x100A
    pass


def analyze_ChartAttachedlabel_record(data):
    # 0x100C
    pass


def analyze_ChartAxcext_record(data):
    # 0x1062
    pass


def analyze_ChartAxesused_record(data):
    # 0x1046
    pass


def analyze_ChartAxis_record(data):
    # 0x101D
    pass


def analyze_ChartAxislineformat_record(data):
    # 0x1021
    pass


def analyze_ChartAxisparent_record(data):
    # 0x1041
    pass


def analyze_ChartBar_record(data):
    # 0x1017
    pass


def analyze_ChartBoppcustom_record(data):
    # 0x1067
    pass


def analyze_ChartBoppop_record(data):
    # 0x1061
    pass


def analyze_ChartCatserrange_record(data):
    # 0x1020
    pass


def analyze_ChartChart_record(data):
    # 0x1002
    pass


def analyze_ChartChartformat_record(data):
    # 0x1014
    pass


def analyze_ChartChartline_record(data):
    # 0x101C
    pass


def analyze_ChartDat_record(data):
    # 0x1063
    pass


def analyze_ChartDataformat_record(data):
    # 0x1006
    pass


def analyze_ChartDefaulttext_record(data):
    # 0x1024
    pass


def analyze_ChartDropbar_record(data):
    # 0x103D
    pass


def analyze_ChartFbi_record(data):
    # 0x1060
    pass


def analyze_ChartFontx_record(data):
    # 0x1026
    pass


def analyze_ChartFormatlink_record(data):
    # 0x1022
    pass


def analyze_ChartFrame_record(data):
    # 0x1032
    pass


def analyze_ChartGelframe_record(data):
    # 0x1066
    pass


def analyze_ChartIfmt_record(data):
    # 0x104E
    pass


def analyze_ChartLegend_record(data):
    # 0x1015
    pass


def analyze_ChartLegendxn_record(data):
    # 0x1043
    pass


def analyze_ChartLine_record(data):
    # 0x1018
    pass


def analyze_ChartLineformat_record(data):
    # 0x1007
    pass


def analyze_ChartMarkerformat_record(data):
    # 0x1009
    pass


def analyze_ChartObjectLink_record(data):
    # 0x1027
    pass


def analyze_ChartPicf_record(data):
    # 0x103C
    pass


def analyze_ChartPie_record(data):
    # 0x1019
    pass


def analyze_ChartPieformat_record(data):
    # 0x100B
    pass


def analyze_ChartPlotarea_record(data):
    # 0x1035
    pass


def analyze_ChartPlotgrowth_record(data):
    # 0x1064
    pass


def analyze_ChartPos_record(data):
    # 0x104F
    pass


def analyze_ChartRadar_record(data):
    # 0x103E
    pass


def analyze_ChartRadararea_record(data):
    # 0x1040
    pass


def analyze_ChartSbaseref_record(data):
    # 0x1048
    pass


def analyze_ChartScatter_record(data):
    # 0x101B
    pass


def analyze_ChartSerauxerrbar_record(data):
    # 0x105B
    pass


def analyze_ChartSerauxtrend_record(data):
    # 0x104B
    pass


def analyze_ChartSerfmt_record(data):
    # 0x105D
    pass


def analyze_ChartSeries_record(data):
    # 0x1003
    pass


def analyze_ChartSerieslist_record(data):
    # 0x1016
    pass


def analyze_ChartSeriestext_record(data):
    # 0x100D
    pass


def analyze_ChartSerparent_record(data):
    # 0x104A
    pass


def analyze_ChartSertocrt_record(data):
    # 0x1045
    pass


def analyze_ChartShtprops_record(data):
    # 0x1044
    pass


def analyze_ChartSiindex_record(data):
    # 0x1065
    pass


def analyze_ChartSurface_record(data):
    # 0x103F
    pass


def analyze_ChartText_record(data):
    # 0x1025
    pass


def analyze_ChartTick_record(data):
    # 0x101E
    pass


def analyze_ChartValuerange_record(data):
    # 0x101F
    pass


def analyze_DBCELL_record(data):
    # 0x00D7
    pass


def analyze_DCON_record(data):
    # 0x0050
    pass


def analyze_DCONBIN_record(data):
    # 0x01B5
    pass


def analyze_DCONNAME_record(data):
    # 0x0053
    pass


def analyze_DCONREF_record(data):
    # 0x0051
    pass


def analyze_DEFAULTROWHEIGHT_record(data):
    # 0x0025, 0x0225
    pass


def analyze_DEFCOLWIDTH_record(data):
    # 0x0055
    pass


def analyze_DELMENU_record(data):
    # 0x00C3
    pass


def analyze_DELTA_record(data):
    # 0x0010
    pass


def analyze_DIMENSIONS_record(data):
    # 0x0000, 0x0200
    pass


def analyze_DOCROUTE_record(data):
    # 0x00B8
    pass


def analyze_DSF_record(data):
    # 0x0161
    pass


def analyze_DV_record(data):
    # 0x01BE
    pass


def analyze_DVAL_record(data):
    # 0x01B2
    pass


def analyze_EDG_record(data):
    # 0x0088
    pass


def analyze_EFONT_record(data):
    # 0x0045
    pass


def analyze_END_record(data):
    # 0x1034
    pass


def analyze_EOF_record(data):
    # 0x000A
    pass


def analyze_EXTERNCOUNT_record(data):
    # 0x0016
    pass


def analyze_EXTERNNAME_record(data):
    # 0x0023, 0x0223
    pass


def analyze_EXTERNSHEET_record(data):
    # 0x0017
    pass


def analyze_EXTSST_record(data):
    # 0x00FF
    pass


def analyze_FILEPASS_record(data):
    # 0x002F
    pass


def analyze_FILESHARING_record(data):
    # 0x005B
    pass


def analyze_FILTERMODE_record(data):
    # 0x009B
    pass


def analyze_FNGROUPCOUNT_record(data):
    # 0x009C
    pass


def analyze_FNGROUPNAME_record(data):
    # 0x009A
    pass


def analyze_FONT_record(data):
    # 0x0031, 0x0231
    pass


def analyze_FOOTER_record(data):
    # 0x0015
    pass


def analyze_FORMAT_record(data):
    # 0x001E, 0x041E
    fmt_idx = struct.unpack('<H', data[0:2])
    name_len = struct.unpack('<H', data[2:4])
    compressed = data[4] == '\x00'
    print 'format index      : 0x%04X' % fmt_idx
    print 'format name len   : 0x%04X' % name_len
    print 'compressed UNICODE:', compressed

    name = data[5:]
    if not compressed:
        name = [c for c in name if c != '\x00']    
        name = ''.join(name)
    print 'format str        :', name            
    pass


def analyze_FORMATCOUNT_record(data):
    # 0x001F
    pass


def analyze_FORMULA_record(data):
    # 0x0006, 0x0206, 0x0406
    pass


def analyze_GCW_record(data):
    # 0x00AB
    pass


def analyze_GRIDSET_record(data):
    # 0x0082
    pass


def analyze_GUTS_record(data):
    # 0x0080
    pass


def analyze_HCENTER_record(data):
    # 0x0083
    pass


def analyze_HEADER_record(data):
    # 0x0014
    pass


def analyze_HIDEOBJ_record(data):
    # 0x008D
    pass


def analyze_HLINK_record(data):
    # 0x01B8
    pass


def analyze_HORIZONTALPAGEBREAKS_record(data):
    # 0x001B
    pass


def analyze_IMDATA_record(data):
    # 0x007F
    pass


def analyze_INDEX_record(data):
    # 0x000B, 0x020B
    pass


def analyze_INTEGER_record(data):
    # 0x0002
    pass


def analyze_INTERFACEEND_record(data):
    # 0x00E2
    pass


def analyze_INTERFACEHDR_record(data):
    # 0x00E1
    pass


def analyze_ITERATION_record(data):
    # 0x0011
    pass


def analyze_IXFE_record(data):
    # 0x0044
    pass


def analyze_LABEL_record(data):
    # 0x0004, 0x0204
    pass


def analyze_LABELRANGES_record(data):
    # 0x015F
    pass


def analyze_LABELSST_record(data):
    # 0x00FD
    row, col, xf_idx, sst_idx = struct.unpack('<3HL', data)
    print 'row     : 0x%000X'  % row
    print 'col     : 0x%000X'  % col
    print 'xf  idx : 0x%000X'  % xf_idx
    print 'sst idx : 0x%000X'  % sst_idx

    pass


def analyze_LEFTMARGIN_record(data):
    # 0x0026
    print struct.unpack('<d', data)[0]
    pass


def analyze_LHNGRAPH_record(data):
    # 0x0095
    pass


def analyze_LHRECORD_record(data):
    # 0x0094
    pass


def analyze_LPR_record(data):
    # 0x0098
    pass


def analyze_MMS_record(data):
    # 0x00C1
    pass


def analyze_MSODRAWING_record(data):
    # 0x00EC
    pass


def analyze_MSODRAWINGGROUP_record(data):
    # 0x00EB
    pass


def analyze_MSODRAWINGSELECTION_record(data):
    # 0x00ED
    pass


def analyze_MULBLANK_record(data):
    # 0x00BE
    pass


def analyze_MULRK_record(data):
    # 0x00BD
    pass


def analyze_NAME_record(data):
    # 0x0018, 0x0218
    pass


def analyze_NOTE_record(data):
    # 0x001C
    pass


def analyze_NUMBER_record(data):
    # 0x0003, 0x0203
    pass


def analyze_OBJ_record(data):
    # 0x005D
    pass


def analyze_OBJPROTECT_record(data):
    # 0x0063
    pass


def analyze_OBPROJ_record(data):
    # 0x00D3
    pass


def analyze_OLESIZE_record(data):
    # 0x00DE
    pass


def analyze_PALETTE_record(data):
    # 0x0092
    pass


def analyze_PANE_record(data):
    # 0x0041
    pass


def analyze_PASSWORD_record(data):
    # 0x0013
    pass


def analyze_PLS_record(data):
    # 0x004D
    pass


def analyze_PRECISION_record(data):
    # 0x000E
    pass


def analyze_PRINTGRIDLINES_record(data):
    # 0x002B
    pass


def analyze_PRINTHEADERS_record(data):
    # 0x002A
    pass


def analyze_PROT4REV_record(data):
    # 0x01AF
    pass


def analyze_PROT4REVPASS_record(data):
    # 0x01BC
    pass


def analyze_PROTECT_record(data):
    # 0x0012
    pass


def analyze_PUB_record(data):
    # 0x0089
    pass


def analyze_QSI_record(data):
    # 0x01AD
    pass

def analyze_RECALCID_record(data):
    # 0x01C1: 
    pass


def analyze_RECIPNAME_record(data):
    # 0x00B9
    pass


def analyze_REFMODE_record(data):
    # 0x000F
    pass


def analyze_REFRESHALL_record(data):
    # 0x01B7
    pass


def analyze_RIGHTMARGIN_record(data):
    # 0x0027
    print struct.unpack('<d', data)[0]
    pass


def analyze_RK_record(data):
    # 0x007E, 0x027E
    pass


def analyze_ROW_record(data):
    # 0x0008, 0x0208
    idx, col1, col2, height_options, not_used, not_used, options = \
        struct.unpack('<6HL', data)
    print 'row index        : 0x%000X'  % idx
    print 'start col        : 0x%000X'  % col1
    print 'last col + 1     : 0x%000X'  % col2
    print 'height           : 0x%000X'  % (height_options & 0x7FFF)
    print 'height is custom : 0x%000X'  % ((height_options & 0x8000) >> 15)
    print 'outline level    : 0x%000X'  % ((options & 0x00000007L) >> 0)
    print 'level collapsed  : 0x%000X'  % ((options & 0x00000010L) >> 4)
    print 'row is hidden    : 0x%000X'  % ((options & 0x00000020L) >> 5)
    print 'font height match: 0x%000X'  % ((options & 0x00000040L) >> 6)
    print 'default format   : 0x%000X'  % ((options & 0x00000080L) >> 7)
    print 'default xf index : 0x%000X'  % ((options & 0x0FFF0000L) >> 16)
    print 'add space above  : 0x%000X'  % ((options & 0x10000000L) >> 28)
    print 'add space below  : 0x%000X'  % ((options & 0x20000000L) >> 29)


def analyze_RSTRING_record(data):
    # 0x00D6
    pass


def analyze_SAFERECALC_record(data):
    # 0x005F
    pass


def analyze_SCENARIO_record(data):
    # 0x00AF
    pass


def analyze_SCENMAN_record(data):
    # 0x00AE
    pass


def analyze_SCENPROTECT_record(data):
    # 0x00DD
    pass


def analyze_SCL_record(data):
    # 0x00A0
    pass


def analyze_SCREENTIP_record(data):
    # 0x0800
    pass


def analyze_SELECTION_record(data):
    # 0x001D
    pass


def analyze_SETUP_record(data):
    # 0x00A1
    pass


def analyze_SHEETLAYOUT_record(data):
    # 0x0862
    pass


def analyze_SHEETPROTECTION_record(data):
    # 0x0867
    pass


def analyze_SHRFMLA_record(data):
    # 0x00BC, 0x04BC
    pass


def analyze_SORT_record(data):
    # 0x0090
    pass


def analyze_SOUND_record(data):
    # 0x0096
    pass


def analyze_SST_record(data):
    # 0x00FC
    pass


def analyze_STANDARDWIDTH_record(data):
    # 0x0099
    pass


def analyze_STRING_record(data):
    # 0x0007, 0x0207
    pass


def analyze_STYLE_record(data):
    # 0x0093, 0x0293
    pass


def analyze_SUB_record(data):
    # 0x0091
    pass


def analyze_SUPBOOK_record(data):
    # 0x01AE
    pass


def analyze_SXDATETIME_record(data):
    # 0x00CE
    pass


def analyze_SXDB_record(data):
    # 0x00C6
    pass


def analyze_SXDBEX_record(data):
    # 0x0122
    pass


def analyze_SXDI_record(data):
    # 0x00C5
    pass


def analyze_SXDOUBLE_record(data):
    # 0x00C9
    pass


def analyze_SXEX_record(data):
    # 0x00F1
    pass


def analyze_SXEXT_PARAMQRY_record(data):
    # 0x00DC
    pass


def analyze_SXFDBTYPE_record(data):
    # 0x01BB
    pass


def analyze_SXFIELD_record(data):
    # 0x00C7
    pass


def analyze_SXFILT_record(data):
    # 0x00F2
    pass


def analyze_SXFMLA_record(data):
    # 0x00F9
    pass


def analyze_SXFORMAT_record(data):
    # 0x00FB
    pass


def analyze_SXFORMULA_record(data):
    # 0x0103
    pass


def analyze_SXIDSTM_record(data):
    # 0x00D5
    pass


def analyze_SXINDEXLIST_record(data):
    # 0x00C8
    pass


def analyze_SXIVD_record(data):
    # 0x00B4
    pass


def analyze_SXLI_record(data):
    # 0x00B5
    pass


def analyze_SXNAME_record(data):
    # 0x00F6
    pass


def analyze_SXPAIR_record(data):
    # 0x00F8
    pass


def analyze_SXPI_record(data):
    # 0x00B6
    pass


def analyze_SXRULE_record(data):
    # 0x00F0
    pass


def analyze_SXSELECT_record(data):
    # 0x00F7
    pass


def analyze_SXSTRING_record(data):
    # 0x00CD
    pass


def analyze_SXTBL_record(data):
    # 0x00D0
    pass


def analyze_SXTBPG_record(data):
    # 0x00D2
    pass


def analyze_SXTBRGITEM_record(data):
    # 0x00D1
    pass


def analyze_SXVD_record(data):
    # 0x00B1
    pass


def analyze_SXVDEX_record(data):
    # 0x0100
    pass


def analyze_SXVI_record(data):
    # 0x00B2
    pass


def analyze_SXVIEW_record(data):
    # 0x00B0
    pass


def analyze_SXVS_record(data):
    # 0x00E3
    pass


def analyze_TABID_record(data):
    # 0x013D
    pass


def analyze_TABLE_record(data):
    # 0x0036, 0x0236
    pass


def analyze_TEMPLATE_record(data):
    # 0x0060
    pass


def analyze_TOPMARGIN_record(data):
    # 0x0028
    print struct.unpack('<d', data)[0]
    pass


def analyze_TXO_record(data):
    # 0x01B6
    pass


def analyze_UDDESC_record(data):
    # 0x00DF
    pass


def analyze_UNCALCED_record(data):
    # 0x005E
    pass


def analyze_UNITS_record(data):
    # 0x1001
    pass


def analyze_USERBVIEW_record(data):
    # 0x01A9
    pass


def analyze_USERSVIEWBEGIN_record(data):
    # 0x01AA
    pass


def analyze_USERSVIEWEND_record(data):
    # 0x01AB
    pass


def analyze_USESELFS_record(data):
    # 0x0160
    pass


def analyze_VCENTER_record(data):
    # 0x0084
    pass


def analyze_VERTICALPAGEBREAKS_record(data):
    # 0x001A
    pass


def analyze_WEBQRYSETTINGS_record(data):
    # 0x0803
    pass


def analyze_WEBQRYTABLES_record(data):
    # 0x0804
    pass


def analyze_WINDOW1_record(data):
    # 0x003D
    pass


def analyze_WINDOW2_record(data):
    # 0x003E, 0x023E
    pass


def analyze_WINDOWPROTECT_record(data):
    # 0x0019
    pass


def analyze_WRITEACCESS_record(data):
    # 0x005C
    pass


def analyze_WRITEPROT_record(data):
    # 0x0086
    pass


def analyze_WSBOOL_record(data):
    # 0x0081
    pass


def analyze_XCT_record(data):
    # 0x0059
    pass


def analyze_XF_record(data):
    # 0x0043, 0x00E0, 0x0243, 0x0443
    #fnt_idx, num_ftm_idx, \
    #    cell_prot,         \
    #    align, rotation, txt_format, \
    #    used_attrib, \
    #    border0, border1, border2 = struct.unpack('<3H4B2LH', data)
    #print 'fnt idx                 : 0x%000X'  % fnt_idx
    #print 'num fmt idx             : 0x%000X'  % num_ftm_idx
    #print 'cell is locked          : 0x%0X  '  % ((cell_prot) & 0x01)
    #print 'format is hidden        : 0x%0X  '  % ((cell_prot & 0x02) >> 1)
    #print 'style XF(1), cell XF(0) : 0x%0X  '  % ((cell_prot & 0x03) >> 3)
    #print 'Index to parent style XF: 0x%0000X' % ((cell_prot & 0xFFF0) >> 4)

    pass


def analyze_XL5MODIFY_record(data):
    # 0x0162
    pass

def analyze_XL9FILE_record(data):
    # 0x01C0: 
    pass

all_records = {
    0x0000: ('DIMENSIONS', analyze_DIMENSIONS_record),
    0x0001: ('BLANK', analyze_BLANK_record),
    0x0002: ('INTEGER', analyze_INTEGER_record),
    0x0003: ('NUMBER', analyze_NUMBER_record),
    0x0004: ('LABEL', analyze_LABEL_record),
    0x0005: ('BOOLERR', analyze_BOOLERR_record),
    0x0006: ('FORMULA', analyze_FORMULA_record),
    0x0007: ('STRING', analyze_STRING_record),
    0x0008: ('ROW', analyze_ROW_record),
    0x0009: ('BOF', analyze_BOF_record),
    0x000A: ('EOF', analyze_EOF_record),
    0x000B: ('INDEX', analyze_INDEX_record),
    0x000C: ('CALCCOUNT', analyze_CALCCOUNT_record),
    0x000D: ('CALCMODE', analyze_CALCMODE_record),
    0x000E: ('PRECISION', analyze_PRECISION_record),
    0x000F: ('REFMODE', analyze_REFMODE_record),
    0x0010: ('DELTA', analyze_DELTA_record),
    0x0011: ('ITERATION', analyze_ITERATION_record),
    0x0012: ('PROTECT', analyze_PROTECT_record),
    0x0013: ('PASSWORD', analyze_PASSWORD_record),
    0x0014: ('HEADER', analyze_HEADER_record),
    0x0015: ('FOOTER', analyze_FOOTER_record),
    0x0016: ('EXTERNCOUNT', analyze_EXTERNCOUNT_record),
    0x0017: ('EXTERNSHEET', analyze_EXTERNSHEET_record),
    0x0018: ('NAME', analyze_NAME_record),
    0x0019: ('WINDOWPROTECT', analyze_WINDOWPROTECT_record),
    0x001A: ('VERTICALPAGEBREAKS', analyze_VERTICALPAGEBREAKS_record),
    0x001B: ('HORIZONTALPAGEBREAKS', analyze_HORIZONTALPAGEBREAKS_record),
    0x001C: ('NOTE', analyze_NOTE_record),
    0x001D: ('SELECTION', analyze_SELECTION_record),
    0x001E: ('FORMAT', analyze_FORMAT_record),
    0x001F: ('FORMATCOUNT', analyze_FORMATCOUNT_record),
    0x0020: ('COLUMNDEFAULT', analyze_COLUMNDEFAULT_record),
    0x0021: ('ARRAY', analyze_ARRAY_record),
    0x0022: ('1904', analyze_1904_record),
    0x0023: ('EXTERNNAME', analyze_EXTERNNAME_record),
    0x0024: ('COLWIDTH', analyze_COLWIDTH_record),
    0x0025: ('DEFAULTROWHEIGHT', analyze_DEFAULTROWHEIGHT_record),
    0x0026: ('LEFTMARGIN', analyze_LEFTMARGIN_record),
    0x0027: ('RIGHTMARGIN', analyze_RIGHTMARGIN_record),
    0x0028: ('TOPMARGIN', analyze_TOPMARGIN_record),
    0x0029: ('BOTTOMMARGIN', analyze_BOTTOMMARGIN_record),
    0x002A: ('PRINTHEADERS', analyze_PRINTHEADERS_record),
    0x002B: ('PRINTGRIDLINES', analyze_PRINTGRIDLINES_record),
    0x002F: ('FILEPASS', analyze_FILEPASS_record),
    0x0031: ('FONT', analyze_FONT_record),
    0x0036: ('TABLE', analyze_TABLE_record),
    0x003C: ('CONTINUE', analyze_CONTINUE_record),
    0x003D: ('WINDOW1', analyze_WINDOW1_record),
    0x003E: ('WINDOW2', analyze_WINDOW2_record),
    0x0040: ('BACKUP', analyze_BACKUP_record),
    0x0041: ('PANE', analyze_PANE_record),
    0x0042: ('CODEPAGE', analyze_CODEPAGE_record),
    0x0043: ('XF', analyze_XF_record),
    0x0044: ('IXFE', analyze_IXFE_record),
    0x0045: ('EFONT', analyze_EFONT_record),
    0x004D: ('PLS', analyze_PLS_record),
    0x0050: ('DCON', analyze_DCON_record),
    0x0051: ('DCONREF', analyze_DCONREF_record),
    0x0053: ('DCONNAME', analyze_DCONNAME_record),
    0x0055: ('DEFCOLWIDTH', analyze_DEFCOLWIDTH_record),
    0x0056: ('BUILTINFMTCNT', analyze_BUILTINFMTCNT_record),
    0x0059: ('XCT', analyze_XCT_record),
    0x005A: ('CRN', analyze_CRN_record),
    0x005B: ('FILESHARING', analyze_FILESHARING_record),
    0x005C: ('WRITEACCESS', analyze_WRITEACCESS_record),
    0x005D: ('OBJ', analyze_OBJ_record),
    0x005E: ('UNCALCED', analyze_UNCALCED_record),
    0x005F: ('SAFERECALC', analyze_SAFERECALC_record),
    0x0060: ('TEMPLATE', analyze_TEMPLATE_record),
    0x0063: ('OBJPROTECT', analyze_OBJPROTECT_record),
    0x007D: ('COLINFO', analyze_COLINFO_record),
    0x007E: ('RK', analyze_RK_record),
    0x007F: ('IMDATA', analyze_IMDATA_record),
    0x0080: ('GUTS', analyze_GUTS_record),
    0x0081: ('WSBOOL', analyze_WSBOOL_record),
    0x0082: ('GRIDSET', analyze_GRIDSET_record),
    0x0083: ('HCENTER', analyze_HCENTER_record),
    0x0084: ('VCENTER', analyze_VCENTER_record),
    0x0085: ('BOUNDSHEET', analyze_BOUNDSHEET_record),
    0x0086: ('WRITEPROT', analyze_WRITEPROT_record),
    0x0087: ('ADDIN', analyze_ADDIN_record),
    0x0088: ('EDG', analyze_EDG_record),
    0x0089: ('PUB', analyze_PUB_record),
    0x008C: ('COUNTRY', analyze_COUNTRY_record),
    0x008D: ('HIDEOBJ', analyze_HIDEOBJ_record),
    0x008E: ('BUNDLESOFFSET', analyze_BUNDLESOFFSET_record),
    0x008F: ('BUNDLEHEADER', analyze_BUNDLEHEADER_record),
    0x0090: ('SORT', analyze_SORT_record),
    0x0091: ('SUB', analyze_SUB_record),
    0x0092: ('PALETTE', analyze_PALETTE_record),
    0x0093: ('STYLE', analyze_STYLE_record),
    0x0094: ('LHRECORD', analyze_LHRECORD_record),
    0x0095: ('LHNGRAPH', analyze_LHNGRAPH_record),
    0x0096: ('SOUND', analyze_SOUND_record),
    0x0098: ('LPR', analyze_LPR_record),
    0x0099: ('STANDARDWIDTH', analyze_STANDARDWIDTH_record),
    0x009A: ('FNGROUPNAME', analyze_FNGROUPNAME_record),
    0x009B: ('FILTERMODE', analyze_FILTERMODE_record),
    0x009C: ('FNGROUPCOUNT', analyze_FNGROUPCOUNT_record),
    0x009D: ('AUTOFILTERINFO', analyze_AUTOFILTERINFO_record),
    0x009E: ('AUTOFILTER', analyze_AUTOFILTER_record),
    0x00A0: ('SCL', analyze_SCL_record),
    0x00A1: ('SETUP', analyze_SETUP_record),
    0x00A9: ('COORDLIST', analyze_COORDLIST_record),
    0x00AB: ('GCW', analyze_GCW_record),
    0x00AE: ('SCENMAN', analyze_SCENMAN_record),
    0x00AF: ('SCENARIO', analyze_SCENARIO_record),
    0x00B0: ('SXVIEW', analyze_SXVIEW_record),
    0x00B1: ('SXVD', analyze_SXVD_record),
    0x00B2: ('SXVI', analyze_SXVI_record),
    0x00B4: ('SXIVD', analyze_SXIVD_record),
    0x00B5: ('SXLI', analyze_SXLI_record),
    0x00B6: ('SXPI', analyze_SXPI_record),
    0x00B8: ('DOCROUTE', analyze_DOCROUTE_record),
    0x00B9: ('RECIPNAME', analyze_RECIPNAME_record),
    0x00BC: ('SHRFMLA', analyze_SHRFMLA_record),
    0x00BD: ('MULRK', analyze_MULRK_record),
    0x00BE: ('MULBLANK', analyze_MULBLANK_record),
    0x00C1: ('MMS', analyze_MMS_record),
    0x00C2: ('ADDMENU', analyze_ADDMENU_record),
    0x00C3: ('DELMENU', analyze_DELMENU_record),
    0x00C5: ('SXDI', analyze_SXDI_record),
    0x00C6: ('SXDB', analyze_SXDB_record),
    0x00C7: ('SXFIELD', analyze_SXFIELD_record),
    0x00C8: ('SXINDEXLIST', analyze_SXINDEXLIST_record),
    0x00C9: ('SXDOUBLE', analyze_SXDOUBLE_record),
    0x00CD: ('SXSTRING', analyze_SXSTRING_record),
    0x00CE: ('SXDATETIME', analyze_SXDATETIME_record),
    0x00D0: ('SXTBL', analyze_SXTBL_record),
    0x00D1: ('SXTBRGITEM', analyze_SXTBRGITEM_record),
    0x00D2: ('SXTBPG', analyze_SXTBPG_record),
    0x00D3: ('OBPROJ', analyze_OBPROJ_record),
    0x00D5: ('SXIDSTM', analyze_SXIDSTM_record),
    0x00D6: ('RSTRING', analyze_RSTRING_record),
    0x00D7: ('DBCELL', analyze_DBCELL_record),
    0x00DA: ('BOOKBOOL', analyze_BOOKBOOL_record),
    0x00DC: ('SXEXT|PARAMQRY', analyze_SXEXT_PARAMQRY_record),
    0x00DD: ('SCENPROTECT', analyze_SCENPROTECT_record),
    0x00DE: ('OLESIZE', analyze_OLESIZE_record),
    0x00DF: ('UDDESC', analyze_UDDESC_record),
    0x00E0: ('XF', analyze_XF_record),
    0x00E1: ('INTERFACEHDR', analyze_INTERFACEHDR_record),
    0x00E2: ('INTERFACEEND', analyze_INTERFACEEND_record),
    0x00E3: ('SXVS', analyze_SXVS_record),
    0x00E5: ('MERGEDCELLS', analyze_MERGEDCELLS_record),
    0x00E9: ('BITMAP', analyze_BITMAP_record),
    0x00EB: ('MSODRAWINGGROUP', analyze_MSODRAWINGGROUP_record),
    0x00EC: ('MSODRAWING', analyze_MSODRAWING_record),
    0x00ED: ('MSODRAWINGSELECTION', analyze_MSODRAWINGSELECTION_record),
    0x00F0: ('SXRULE', analyze_SXRULE_record),
    0x00F1: ('SXEX', analyze_SXEX_record),
    0x00F2: ('SXFILT', analyze_SXFILT_record),
    0x00F6: ('SXNAME', analyze_SXNAME_record),
    0x00F7: ('SXSELECT', analyze_SXSELECT_record),
    0x00F8: ('SXPAIR', analyze_SXPAIR_record),
    0x00F9: ('SXFMLA', analyze_SXFMLA_record),
    0x00FB: ('SXFORMAT', analyze_SXFORMAT_record),
    0x00FC: ('SST', analyze_SST_record),
    0x00FD: ('LABELSST', analyze_LABELSST_record),
    0x00FF: ('EXTSST', analyze_EXTSST_record),
    0x0100: ('SXVDEX', analyze_SXVDEX_record),
    0x0103: ('SXFORMULA', analyze_SXFORMULA_record),
    0x0122: ('SXDBEX', analyze_SXDBEX_record),
    0x0137: ('CHTRINSERT', analyze_CHTRINSERT_record),
    0x0138: ('CHTRINFO', analyze_CHTRINFO_record),
    0x013B: ('CHTRCELLCONTENT', analyze_CHTRCELLCONTENT_record),
    0x013D: ('TABID', analyze_TABID_record),
    0x0140: ('CHTRMOVERANGE', analyze_CHTRMOVERANGE_record),
    0x014D: ('CHTRINSERTTAB', analyze_CHTRINSERTTAB_record),
    0x015F: ('LABELRANGES', analyze_LABELRANGES_record),
    0x0160: ('USESELFS', analyze_USESELFS_record),
    0x0161: ('DSF', analyze_DSF_record),
    0x0162: ('XL5MODIFY', analyze_XL5MODIFY_record),
    0x0196: ('CHTRHEADER', analyze_CHTRHEADER_record),
    0x01A9: ('USERBVIEW', analyze_USERBVIEW_record),
    0x01AA: ('USERSVIEWBEGIN', analyze_USERSVIEWBEGIN_record),
    0x01AB: ('USERSVIEWEND', analyze_USERSVIEWEND_record),
    0x01AD: ('QSI', analyze_QSI_record),
    0x01AE: ('SUPBOOK', analyze_SUPBOOK_record),
    0x01AF: ('PROT4REV', analyze_PROT4REV_record),
    0x01B0: ('CONDFMT', analyze_CONDFMT_record),
    0x01B1: ('CF', analyze_CF_record),
    0x01B2: ('DVAL', analyze_DVAL_record),
    0x01B5: ('DCONBIN', analyze_DCONBIN_record),
    0x01B6: ('TXO', analyze_TXO_record),
    0x01B7: ('REFRESHALL', analyze_REFRESHALL_record),
    0x01B8: ('HLINK', analyze_HLINK_record),
    0x01BA: ('CODENAME', analyze_CODENAME_record),
    0x01BB: ('SXFDBTYPE', analyze_SXFDBTYPE_record),
    0x01BC: ('PROT4REVPASS', analyze_PROT4REVPASS_record),
    0x01BE: ('DV', analyze_DV_record),
    0x01C0: ('XL9FILE', analyze_XL9FILE_record),
    0x01C1: ('RECALCID',analyze_RECALCID_record),
    0x0200: ('DIMENSIONS', analyze_DIMENSIONS_record),
    0x0201: ('BLANK', analyze_BLANK_record),
    0x0203: ('NUMBER', analyze_NUMBER_record),
    0x0204: ('LABEL', analyze_LABEL_record),
    0x0205: ('BOOLERR', analyze_BOOLERR_record),
    0x0206: ('FORMULA', analyze_FORMULA_record),
    0x0207: ('STRING', analyze_STRING_record),
    0x0208: ('ROW', analyze_ROW_record),
    0x0209: ('BOF', analyze_BOF_record),
    0x020B: ('INDEX', analyze_INDEX_record),
    0x0218: ('NAME', analyze_NAME_record),
    0x0221: ('ARRAY', analyze_ARRAY_record),
    0x0223: ('EXTERNNAME', analyze_EXTERNNAME_record),
    0x0225: ('DEFAULTROWHEIGHT', analyze_DEFAULTROWHEIGHT_record),
    0x0231: ('FONT', analyze_FONT_record),
    0x0236: ('TABLE', analyze_TABLE_record),
    0x023E: ('WINDOW2', analyze_WINDOW2_record),
    0x0243: ('XF', analyze_XF_record),
    0x027E: ('RK', analyze_RK_record),
    0x0293: ('STYLE', analyze_STYLE_record),
    0x0406: ('FORMULA', analyze_FORMULA_record),
    0x0409: ('BOF', analyze_BOF_record),
    0x041E: ('FORMAT', analyze_FORMAT_record),
    0x0443: ('XF', analyze_XF_record),
    0x04BC: ('SHRFMLA', analyze_SHRFMLA_record),
    0x0800: ('SCREENTIP', analyze_SCREENTIP_record),
    0x0803: ('WEBQRYSETTINGS', analyze_WEBQRYSETTINGS_record),
    0x0804: ('WEBQRYTABLES', analyze_WEBQRYTABLES_record),
    0x0809: ('BOF', analyze_BOF_record),
    0x0862: ('SHEETLAYOUT', analyze_SHEETLAYOUT_record),
    0x0867: ('SHEETPROTECTION', analyze_SHEETPROTECTION_record),
    0x1001: ('UNITS', analyze_UNITS_record),
    0x1002: ('ChartChart', analyze_ChartChart_record),
    0x1003: ('ChartSeries', analyze_ChartSeries_record),
    0x1006: ('ChartDataformat', analyze_ChartDataformat_record),
    0x1007: ('ChartLineformat', analyze_ChartLineformat_record),
    0x1009: ('ChartMarkerformat', analyze_ChartMarkerformat_record),
    0x100A: ('ChartAreaformat', analyze_ChartAreaformat_record),
    0x100B: ('ChartPieformat', analyze_ChartPieformat_record),
    0x100C: ('ChartAttachedlabel', analyze_ChartAttachedlabel_record),
    0x100D: ('ChartSeriestext', analyze_ChartSeriestext_record),
    0x1014: ('ChartChartformat', analyze_ChartChartformat_record),
    0x1015: ('ChartLegend', analyze_ChartLegend_record),
    0x1016: ('ChartSerieslist', analyze_ChartSerieslist_record),
    0x1017: ('ChartBar', analyze_ChartBar_record),
    0x1018: ('ChartLine', analyze_ChartLine_record),
    0x1019: ('ChartPie', analyze_ChartPie_record),
    0x101A: ('ChartArea', analyze_ChartArea_record),
    0x101B: ('ChartScatter', analyze_ChartScatter_record),
    0x101C: ('ChartChartline', analyze_ChartChartline_record),
    0x101D: ('ChartAxis', analyze_ChartAxis_record),
    0x101E: ('ChartTick', analyze_ChartTick_record),
    0x101F: ('ChartValuerange', analyze_ChartValuerange_record),
    0x1020: ('ChartCatserrange', analyze_ChartCatserrange_record),
    0x1021: ('ChartAxislineformat', analyze_ChartAxislineformat_record),
    0x1022: ('ChartFormatlink', analyze_ChartFormatlink_record),
    0x1024: ('ChartDefaulttext', analyze_ChartDefaulttext_record),
    0x1025: ('ChartText', analyze_ChartText_record),
    0x1026: ('ChartFontx', analyze_ChartFontx_record),
    0x1027: ('ChartObjectLink', analyze_ChartObjectLink_record),
    0x1032: ('ChartFrame', analyze_ChartFrame_record),
    0x1033: ('BEGIN', analyze_BEGIN_record),
    0x1034: ('END', analyze_END_record),
    0x1035: ('ChartPlotarea', analyze_ChartPlotarea_record),
    0x103A: ('Chart3D', analyze_Chart3D_record),
    0x103C: ('ChartPicf', analyze_ChartPicf_record),
    0x103D: ('ChartDropbar', analyze_ChartDropbar_record),
    0x103E: ('ChartRadar', analyze_ChartRadar_record),
    0x103F: ('ChartSurface', analyze_ChartSurface_record),
    0x1040: ('ChartRadararea', analyze_ChartRadararea_record),
    0x1041: ('ChartAxisparent', analyze_ChartAxisparent_record),
    0x1043: ('ChartLegendxn', analyze_ChartLegendxn_record),
    0x1044: ('ChartShtprops', analyze_ChartShtprops_record),
    0x1045: ('ChartSertocrt', analyze_ChartSertocrt_record),
    0x1046: ('ChartAxesused', analyze_ChartAxesused_record),
    0x1048: ('ChartSbaseref', analyze_ChartSbaseref_record),
    0x104A: ('ChartSerparent', analyze_ChartSerparent_record),
    0x104B: ('ChartSerauxtrend', analyze_ChartSerauxtrend_record),
    0x104E: ('ChartIfmt', analyze_ChartIfmt_record),
    0x104F: ('ChartPos', analyze_ChartPos_record),
    0x1050: ('ChartAlruns', analyze_ChartAlruns_record),
    0x1051: ('ChartAI', analyze_ChartAI_record),
    0x105B: ('ChartSerauxerrbar', analyze_ChartSerauxerrbar_record),
    0x105D: ('ChartSerfmt', analyze_ChartSerfmt_record),
    0x105F: ('Chart3DDataFormat', analyze_Chart3DDataFormat_record),
    0x1060: ('ChartFbi', analyze_ChartFbi_record),
    0x1061: ('ChartBoppop', analyze_ChartBoppop_record),
    0x1062: ('ChartAxcext', analyze_ChartAxcext_record),
    0x1063: ('ChartDat', analyze_ChartDat_record),
    0x1064: ('ChartPlotgrowth', analyze_ChartPlotgrowth_record),
    0x1065: ('ChartSiindex', analyze_ChartSiindex_record),
    0x1066: ('ChartGelframe', analyze_ChartGelframe_record),
    0x1067: ('ChartBoppcustom', analyze_ChartBoppcustom_record),
    0xFFFF: ('', None)
}
if __name__ == '__main__':
    for r in all_records:
        if all_records[r][1]:
            all_records[r][1]('analyze it!')
            
########NEW FILE########
__FILENAME__ = biff-dumper
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.

__rev_id__ = """$Id: biff-dumper.py,v 1.5 2005/10/26 07:44:24 rvk Exp $"""


import Analyzer
from pyExcelerator import *
import sys
from struct import unpack


def print_bin_data(data):
    print
    i = 0
    while i < len(data):
        j = 0
        while (i < len(data)) and (j < 16):
            c = '0x%02X' % ord(data[i])
            sys.stdout.write(c)
            sys.stdout.write(' ')
            i += 1
            j += 1
        print
    if i == 0:
        print '<NO DATA>'


def print_ASCII_data(data):
    print
    i = 0
    while i < len(data):
        j = 0
        while (i < len(data)) and (j < 16):
            if data[i] < ' ':
                c = '.'
            else:
                c = data[i]
            sys.stdout.write(c)
            i += 1
            j += 1
        print
    if i == 0:
        print '<NO DATA>'


def main():
    if len(sys.argv) < 2:
        print 'no input files.'
        sys.exit(1)

    # Inside MS Office document looks like filesystem
    # We need extract stream named 'Workbook' or 'Book'
    ole_streams = CompoundDoc.Reader(sys.argv[1], True).STREAMS

    if 'Workbook' in ole_streams:
        workbook_stream = ole_streams['Workbook']
    elif 'Book' in ole_streams:
        workbook_stream = ole_streams['Book']
    else:
        raise Exception, 'No workbook stream in file.'

    wb_bin_data_len = len(workbook_stream)
    stream_pos = 0
    
    print 'workbook stream size 0x%X bytes '% len(workbook_stream)

    # Excel's method of data storing is based on 
    # ancient technology "TLV" (Type, Length, Value).
    # In addition, if record size grows to some limit
    # Excel writes CONTINUE records
    ws_num = 0
    EOFs = 0
    while stream_pos < len(workbook_stream) and EOFs <= ws_num:
        # header size == 4
        print 'stream position:', '0x%08X' % stream_pos
        header = workbook_stream[stream_pos:stream_pos+4]
        rec_id, data_size = unpack('<2H', header)
        print 'rec id:', '0x%04X' % rec_id
        print 'rec data size:', '0x%04X' % data_size
        stream_pos += 4
        rec_data = workbook_stream[stream_pos:stream_pos+data_size]
        stream_pos += data_size

        if rec_id == 0x000A:   # EOF 
            EOFs += 1
        elif rec_id == 0x0085: # BOUNDSHEET
            ws_num += 1
        
        if rec_id in Analyzer.all_records:
            rec_name, analyzer_func = Analyzer.all_records[rec_id]

            print 'rec name:', rec_name
            print 'rec data:',
            print_bin_data(rec_data)
            print 'ASCII data:',
            print_ASCII_data(rec_data)
            print 'analyzing...'
            analyzer_func(rec_data)
        else:
            print '<UNKNOWN RECORD>: rec_id == 0x%04X, size 0x%04X bytes' % (rec_id, data_size)
        print '---------------'


main()

########NEW FILE########
__FILENAME__ = compdoc-dumper
#!/usr/bin/env python
# -*- coding: windows-1251 -*-

#  Copyright (C) 2005 Roman V. Kiseliov
#  All rights reserved.
# 
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
# 
#  1. Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
# 
#  2. Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
# 
#  3. All advertising materials mentioning features or use of this
#     software must display the following acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  4. Redistributions of any form whatsoever must retain the following
#     acknowledgment:
#     "This product includes software developed by
#      Roman V. Kiseliov <roman@kiseliov.ru>."
# 
#  THIS SOFTWARE IS PROVIDED BY Roman V. Kiseliov ``AS IS'' AND ANY
#  EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
#  PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL Roman V. Kiseliov OR
#  ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
#  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
#  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
#  STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
#  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
#  OF THE POSSIBILITY OF SUCH DAMAGE.

__rev_id__ = """$Id: compdoc-dumper.py,v 1.10 2005/10/26 07:44:24 rvk Exp $"""


import sys, struct
from pyExcelerator import *

def main():
    if len(sys.argv) < 2:
        print 'no input files.'
        print sys.exit(1)

    CompoundDoc.Reader(sys.argv[1], True)        

main()

########NEW FILE########
__FILENAME__ = xls2csv-gerry
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman

__rev_id__ = """$Id: xls2csv-gerry.py,v 1.1 2005/10/26 07:44:24 rvk Exp $"""

###--xls2csv.py
###--modified by gerry 10/5/2005
###--with args, processes just those files
###--with no args, processes all csvs in the current directory
###--args may include or exclude the .xls suffix
###--no files are written for empty sheets

from    pyExcelerator import *
import  re
import  os
import  sys

def process(fname):
    
    if fname[-4:] != ".xls": 
        fname = fname + ".xls"    
   
    print "processing", fname
    
    for sheet_name, values in parse_xls(fname, 'cp1251'): # parse_xls(arg) -- default encoding
        print "     starting", sheet_name,
        
        keys = values.keys()
        
        rows    = []
        cols    = []
        for key in keys:
            row, col = key
            if not col in cols: cols.append(col)
            if not row in rows: rows.append(row)
        
        try:    n_rows = max(rows)
        except:
            print "which is null." 
            continue
        n_cols = max(cols)
        print "which has", n_rows+1, "rows, and", n_cols+1, "columns."
        
        ofile = open(fname + "." + sheet_name + ".csv",     "w")
        
        for row in range(n_rows+1):
            line = ""
            for col in range(n_cols+1):
                try:    cell = str(values[(row, col)])
                except: cell = ""
                if commas.search(cell) != None: cell = '"' + cell + '"'
                line = line + cell + ","
            print >> ofile, line[:-1]
            
        ofile.close()
        
    print '----------------'

commas = re.compile(",")

try:    args = sys.argv[1:]
except: args = []

if len(args) < 1:
    fnames = os.listdir(".")
    fnames.sort()
    for fname in fnames:
        parts = fname.split(".")
        if parts[-1] != "xls": continue
        process(fname) 
    sys.exit()
else:
    for arg in args:
        process(arg)

    
    
    
########NEW FILE########
__FILENAME__ = xls2csv
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman

__rev_id__ = """$Id: xls2csv.py,v 1.1 2005/10/26 07:44:24 rvk Exp $"""


from pyExcelerator import *
import sys

me, args = sys.argv[0], sys.argv[1:]


if args:
    for arg in args:
        print >>sys.stderr, 'extracting data from', arg
        for sheet_name, values in parse_xls(arg, 'cp1251'): # parse_xls(arg) -- default encoding
            matrix = [[]]
            print 'Sheet = "%s"' % sheet_name.encode('cp866', 'backslashreplace')
            print '----------------'
            for row_idx, col_idx in sorted(values.keys()):
                v = values[(row_idx, col_idx)]
                if isinstance(v, unicode):
                    v = v.encode('cp866', 'backslashreplace')
                else:
                    v = `v`
                v = '"%s"' % v.strip()
                last_row, last_col = len(matrix), len(matrix[-1])
                while last_row <= row_idx:
                    matrix.extend([[]])
                    last_row = len(matrix)

                while last_col < col_idx:
                    matrix[-1].extend([''])
                    last_col = len(matrix[-1])

                matrix[-1].extend([v])
                    
            for row in matrix:
                csv_row = ', '.join(row)
                print csv_row

else:
    print 'usage: %s (inputfile)+' % me


########NEW FILE########
__FILENAME__ = xls2html
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman

__rev_id__ = """$Id: xls2html.py,v 1.1 2005/10/26 07:44:24 rvk Exp $"""


from pyExcelerator import *
import sys

me, args = sys.argv[0], sys.argv[1:]


if args:
    for arg in args:
        print >>sys.stderr, 'extracting data from', arg
        print '<html>'
        for sheet_name, values in parse_xls(arg, 'cp1251'): # parse_xls(arg) -- default encoding
            matrix = [[]]
            print 'Sheet = "%s"' % sheet_name.encode('cp1251', 'backslashreplace')
            print '<table border="2" cellpadding="1" cellspacing="1" >'

            for row_idx, col_idx in sorted(values.keys()):
                v = values[(row_idx, col_idx)]
                if isinstance(v, unicode):
                    v = v.encode('cp1251', 'backslashreplace')
                else:
                    v = `v`
                v = '"%s"' % v.strip()
                last_row, last_col = len(matrix), len(matrix[-1])
                while last_row <= row_idx:
                    matrix.extend([[]])
                    last_row = len(matrix)

                while last_col < col_idx:
                    matrix[-1].extend([''])
                    last_col = len(matrix[-1])

                matrix[-1].extend([v])
                    
            for row in matrix:
                print '<tr>'
                for col in row:
                    print '<td> %s </td>' % col
                print '</tr>'

            print '</table>'
        print '</html>'

else:
    print 'usage: %s (inputfile)+' % me


########NEW FILE########
__FILENAME__ = xls2txt
#!/usr/bin/env python
# -*- coding: windows-1251 -*-
# Copyright (C) 2005 Kiseliov Roman

__rev_id__ = """$Id: xls2txt.py,v 1.1 2005/10/26 07:44:24 rvk Exp $"""


from pyExcelerator import *
import sys

me, args = sys.argv[0], sys.argv[1:]

if args:
    for arg in args:
        print >>sys.stderr, 'extracting data from', arg
        for sheet_name, values in parse_xls(arg, 'cp1251'): # parse_xls(arg) -- default encoding
            print 'Sheet = "%s"' % sheet_name.encode('cp866', 'backslashreplace')
            print '----------------'
            for row_idx, col_idx in sorted(values.keys()):
                v = values[(row_idx, col_idx)]
                if isinstance(v, unicode):
                    v = v.encode('cp866', 'backslashreplace')
                else:
                    v = `v`
                print '(%d, %d) =' % (row_idx, col_idx), v
            print '----------------'
else:
    print 'usage: %s (inputfile)+' % me


########NEW FILE########
__FILENAME__ = capitolwords
""" Python library for interacting with Capitol Words API.

    The Capitol Words API (http://www.capitolwords.org/api/) provides access to
    the most commonly used words in Congressional Record each day.
"""

__author__ = "James Turk (jturk@sunlightfoundation.com)"
__version__ = "0.3.0"
__copyright__ = "Copyright (c) 2008 Sunlight Labs"
__license__ = "BSD"

import urllib2
try:
    import json
except ImportError:
    import simplejson as json

class CwodApiError(Exception):
    """ Exception for Capitol Words API errors """

class WordResult(object):
    def __init__(self, d):
        self.__dict__ = d
        if not hasattr(self, 'word_date'):
            self.word_date = None

    def __str__(self):
        if self.word_date:
            return '%s said %s times on %s' % (self.word, self.word_count, self.word_date)
        else:
            return '%s said %s times' % (self.word, self.word_count)

def _get_json(url):
    try:
        response = urllib2.urlopen(url).read()
        return json.loads(response)
    except urllib2.HTTPError, e:
        raise CwodApiError('Invalid Request')
    except ValueError, e:
        raise CwodApiError('Invalid Response')

def _params_to_paramstr(year, month, day, endyear, endmonth, endday):
   # can't specify only part of the range
    if ((endyear or endmonth or endday)
        and not (endyear and endmonth and endday)):
        raise CwodApiError('Invalid number of parameters')

    # join all supplied params together with /s
    params =  (year, month, day, endyear, endmonth, endday)
    paramstr = '/'.join([str(p) for p in params if p])

    if not paramstr:
        paramstr = 'latest'

    return paramstr


def dailysum(word, year, month=None, day=None,
             endyear=None, endmonth=None, endday=None):

    paramstr = _params_to_paramstr(year, month, day, endyear, endmonth, endday)

    # get json
    url = 'http://capitolwords.org/api/word/%s/%s/feed.json' % (word, paramstr)
    result = _get_json(url)
    return [WordResult(r) for r in result]

def wordofday(year=None, month=None, day=None,
              endyear=None, endmonth=None, endday=None, maxrows=1):

    paramstr = _params_to_paramstr(year, month, day, endyear, endmonth, endday)

    url = 'http://capitolwords.org/api/wod/%s/top%s.json' % (paramstr,
                                                             maxrows)
    result = _get_json(url)
    return [WordResult(r) for r in result]


def lawmaker(lawmaker_id, year=None, month=None, day=None,
             endyear=None, endmonth=None, endday=None, maxrows=1):

    paramstr = _params_to_paramstr(year, month, day, endyear, endmonth, endday)

    url = 'http://capitolwords.org/api/lawmaker/%s/%s/top%s.json' % (lawmaker_id,
                                                                     paramstr,
                                                                     maxrows)
    result = _get_json(url)
    return [WordResult(r) for r in result]

########NEW FILE########
__FILENAME__ = test_capitolwords
import doctest
doctest.testfile('README.rst')

########NEW FILE########
__FILENAME__ = ClientForm-0.2.2
"""HTML form handling for web clients.

ClientForm is a Python module for handling HTML forms on the client
side, useful for parsing HTML forms, filling them in and returning the
completed forms to the server.  It has developed from a port of Gisle
Aas' Perl module HTML::Form, from the libwww-perl library, but the
interface is not the same.

The most useful docstring is the one for HTMLForm.

RFC 1866: HTML 2.0
RFC 1867: Form-based File Upload in HTML
RFC 2388: Returning Values from Forms: multipart/form-data
HTML 3.2 Specification, W3C Recommendation 14 January 1997 (for ISINDEX)
HTML 4.01 Specification, W3C Recommendation 24 December 1999


Copyright 2002-2006 John J. Lee <jjl@pobox.com>
Copyright 2005 Gary Poster
Copyright 2005 Zope Corporation
Copyright 1998-2000 Gisle Aas.

This code is free software; you can redistribute it and/or modify it
under the terms of the BSD License (see the file COPYING included with
the distribution).

"""

# XXX
# Remove unescape_attr method
# Remove parser testing hack
# safeUrl()-ize action
# Really should to merge CC, CF, pp and mechanize as soon as mechanize
#  goes to beta...
# Add url attribute to ParseError
# Switch to unicode throughout (would be 0.3.x)
#  See Wichert Akkerman's 2004-01-22 message to c.l.py.
# Add charset parameter to Content-type headers?  How to find value??
# Add some more functional tests
#  Especially single and multiple file upload on the internet.
#  Does file upload work when name is missing?  Sourceforge tracker form
#   doesn't like it.  Check standards, and test with Apache.  Test
#   binary upload with Apache.
# Controls can have name=None (e.g. forms constructed partly with
#  JavaScript), but find_control can't be told to find a control
#  with that name, because None there means 'unspecified'.  Can still
#  get at by nr, but would be nice to be able to specify something
#  equivalent to name=None, too.
# mailto submission & enctype text/plain
# I'm not going to fix this unless somebody tells me what real servers
#  that want this encoding actually expect: If enctype is
#  application/x-www-form-urlencoded and there's a FILE control present.
#  Strictly, it should be 'name=data' (see HTML 4.01 spec., section
#  17.13.2), but I send "name=" ATM.  What about multiple file upload??

# Would be nice, but I'm not going to do it myself:
# -------------------------------------------------
# Maybe a 0.4.x?
#   Replace by_label etc. with moniker / selector concept. Allows, eg.,
#    a choice between selection by value / id / label / element
#    contents.  Or choice between matching labels exactly or by
#    substring.  Etc.
#   Remove deprecated methods.
#   ...what else?
# Work on DOMForm.
# XForms?  Don't know if there's a need here.


try: True
except NameError:
    True = 1
    False = 0

try: bool
except NameError:
    def bool(expr):
        if expr: return True
        else: return False

try:
    import logging
except ImportError:
    def debug(msg, *args, **kwds):
        pass
else:
    _logger = logging.getLogger("ClientForm")
    OPTIMIZATION_HACK = True

    def debug(msg, *args, **kwds):
        if OPTIMIZATION_HACK:
            return

        try:
            raise Exception()
        except:
            caller_name = (
                sys.exc_info()[2].tb_frame.f_back.f_back.f_code.co_name)
        extended_msg = '%%s %s' % msg
        extended_args = (caller_name,)+args
        debug = _logger.debug(extended_msg, *extended_args, **kwds)

    def _show_debug_messages():
        global OPTIMIZATION_HACK
        OPTIMIZATION_HACK = False
        _logger.setLevel(logging.DEBUG)
        handler = logging.StreamHandler(sys.stdout)
        handler.setLevel(logging.DEBUG)
        _logger.addHandler(handler)

import sys, urllib, urllib2, types, mimetools, copy, urlparse, \
       htmlentitydefs, re, random
from urlparse import urljoin
from cStringIO import StringIO

try:
    import warnings
except ImportError:
    def deprecation(message):
        pass
else:
    def deprecation(message):
        warnings.warn(message, DeprecationWarning, stacklevel=2)

VERSION = "0.2.2"

CHUNK = 1024  # size of chunks fed to parser, in bytes

DEFAULT_ENCODING = "latin-1"

_compress_re = re.compile(r"\s+")
def compress_text(text): return _compress_re.sub(" ", text.strip())

# This version of urlencode is from my Python 1.5.2 back-port of the
# Python 2.1 CVS maintenance branch of urllib.  It will accept a sequence
# of pairs instead of a mapping -- the 2.0 version only accepts a mapping.
def urlencode(query,doseq=False,):
    """Encode a sequence of two-element tuples or dictionary into a URL query \
string.

    If any values in the query arg are sequences and doseq is true, each
    sequence element is converted to a separate parameter.

    If the query arg is a sequence of two-element tuples, the order of the
    parameters in the output will match the order of parameters in the
    input.
    """

    if hasattr(query,"items"):
        # mapping objects
        query = query.items()
    else:
        # it's a bother at times that strings and string-like objects are
        # sequences...
        try:
            # non-sequence items should not work with len()
            x = len(query)
            # non-empty strings will fail this
            if len(query) and type(query[0]) != types.TupleType:
                raise TypeError()
            # zero-length sequences of all types will get here and succeed,
            # but that's a minor nit - since the original implementation
            # allowed empty dicts that type of behavior probably should be
            # preserved for consistency
        except TypeError:
            ty,va,tb = sys.exc_info()
            raise TypeError("not a valid non-string sequence or mapping "
                            "object", tb)

    l = []
    if not doseq:
        # preserve old behavior
        for k, v in query:
            k = urllib.quote_plus(str(k))
            v = urllib.quote_plus(str(v))
            l.append(k + '=' + v)
    else:
        for k, v in query:
            k = urllib.quote_plus(str(k))
            if type(v) == types.StringType:
                v = urllib.quote_plus(v)
                l.append(k + '=' + v)
            elif type(v) == types.UnicodeType:
                # is there a reasonable way to convert to ASCII?
                # encode generates a string, but "replace" or "ignore"
                # lose information and "strict" can raise UnicodeError
                v = urllib.quote_plus(v.encode("ASCII","replace"))
                l.append(k + '=' + v)
            else:
                try:
                    # is this a sufficient test for sequence-ness?
                    x = len(v)
                except TypeError:
                    # not a sequence
                    v = urllib.quote_plus(str(v))
                    l.append(k + '=' + v)
                else:
                    # loop over the sequence
                    for elt in v:
                        l.append(k + '=' + urllib.quote_plus(str(elt)))
    return '&'.join(l)

def unescape(data, entities, encoding=DEFAULT_ENCODING):
    if data is None or "&" not in data:
        return data

    def replace_entities(match, entities=entities, encoding=encoding):
        ent = match.group()
        if ent[1] == "#":
            return unescape_charref(ent[2:-1], encoding)

        repl = entities.get(ent)
        if repl is not None:
            if type(repl) != type(""):
                try:
                    repl = repl.encode(encoding)
                except UnicodeError:
                    repl = ent
        else:
            repl = ent

        return repl

    return re.sub(r"&#?[A-Za-z0-9]+?;", replace_entities, data)

def unescape_charref(data, encoding):
    name, base = data, 10
    if name.startswith("x"):
        name, base= name[1:], 16
    uc = unichr(int(name, base))
    if encoding is None:
        return uc
    else:
        try:
            repl = uc.encode(encoding)
        except UnicodeError:
            repl = "&#%s;" % data
        return repl

def get_entitydefs():
    import htmlentitydefs
    from codecs import latin_1_decode
    entitydefs = {}
    try:
        htmlentitydefs.name2codepoint
    except AttributeError:
        entitydefs = {}
        for name, char in htmlentitydefs.entitydefs.items():
            uc = latin_1_decode(char)[0]
            if uc.startswith("&#") and uc.endswith(";"):
                uc = unescape_charref(uc[2:-1], None)
            entitydefs["&%s;" % name] = uc
    else:
        for name, codepoint in htmlentitydefs.name2codepoint.items():
            entitydefs["&%s;" % name] = unichr(codepoint)
    return entitydefs


def issequence(x):
    try:
        x[0]
    except (TypeError, KeyError):
        return False
    except IndexError:
        pass
    return True

def isstringlike(x):
    try: x+""
    except: return False
    else: return True


def choose_boundary():
    """Return a string usable as a multipart boundary."""
    # follow IE and firefox
    nonce = "".join([str(random.randint(0, sys.maxint-1)) for i in 0,1,2])
    return "-"*27 + nonce

# This cut-n-pasted MimeWriter from standard library is here so can add
# to HTTP headers rather than message body when appropriate.  It also uses
# \r\n in place of \n.  This is a bit nasty.
class MimeWriter:

    """Generic MIME writer.

    Methods:

    __init__()
    addheader()
    flushheaders()
    startbody()
    startmultipartbody()
    nextpart()
    lastpart()

    A MIME writer is much more primitive than a MIME parser.  It
    doesn't seek around on the output file, and it doesn't use large
    amounts of buffer space, so you have to write the parts in the
    order they should occur on the output file.  It does buffer the
    headers you add, allowing you to rearrange their order.

    General usage is:

    f = <open the output file>
    w = MimeWriter(f)
    ...call w.addheader(key, value) 0 or more times...

    followed by either:

    f = w.startbody(content_type)
    ...call f.write(data) for body data...

    or:

    w.startmultipartbody(subtype)
    for each part:
        subwriter = w.nextpart()
        ...use the subwriter's methods to create the subpart...
    w.lastpart()

    The subwriter is another MimeWriter instance, and should be
    treated in the same way as the toplevel MimeWriter.  This way,
    writing recursive body parts is easy.

    Warning: don't forget to call lastpart()!

    XXX There should be more state so calls made in the wrong order
    are detected.

    Some special cases:

    - startbody() just returns the file passed to the constructor;
      but don't use this knowledge, as it may be changed.

    - startmultipartbody() actually returns a file as well;
      this can be used to write the initial 'if you can read this your
      mailer is not MIME-aware' message.

    - If you call flushheaders(), the headers accumulated so far are
      written out (and forgotten); this is useful if you don't need a
      body part at all, e.g. for a subpart of type message/rfc822
      that's (mis)used to store some header-like information.

    - Passing a keyword argument 'prefix=<flag>' to addheader(),
      start*body() affects where the header is inserted; 0 means
      append at the end, 1 means insert at the start; default is
      append for addheader(), but insert for start*body(), which use
      it to determine where the Content-type header goes.

    """

    def __init__(self, fp, http_hdrs=None):
        self._http_hdrs = http_hdrs
        self._fp = fp
        self._headers = []
        self._boundary = []
        self._first_part = True

    def addheader(self, key, value, prefix=0,
                  add_to_http_hdrs=0):
        """
        prefix is ignored if add_to_http_hdrs is true.
        """
        lines = value.split("\r\n")
        while lines and not lines[-1]: del lines[-1]
        while lines and not lines[0]: del lines[0]
        if add_to_http_hdrs:
            value = "".join(lines)
            self._http_hdrs.append((key, value))
        else:
            for i in range(1, len(lines)):
                lines[i] = "    " + lines[i].strip()
            value = "\r\n".join(lines) + "\r\n"
            line = key + ": " + value
            if prefix:
                self._headers.insert(0, line)
            else:
                self._headers.append(line)

    def flushheaders(self):
        self._fp.writelines(self._headers)
        self._headers = []

    def startbody(self, ctype=None, plist=[], prefix=1,
                  add_to_http_hdrs=0, content_type=1):
        """
        prefix is ignored if add_to_http_hdrs is true.
        """
        if content_type and ctype:
            for name, value in plist:
                ctype = ctype + ';\r\n %s=%s' % (name, value)
            self.addheader("Content-type", ctype, prefix=prefix,
                           add_to_http_hdrs=add_to_http_hdrs)
        self.flushheaders()
        if not add_to_http_hdrs: self._fp.write("\r\n")
        self._first_part = True
        return self._fp

    def startmultipartbody(self, subtype, boundary=None, plist=[], prefix=1,
                           add_to_http_hdrs=0, content_type=1):
        boundary = boundary or choose_boundary()
        self._boundary.append(boundary)
        return self.startbody("multipart/" + subtype,
                              [("boundary", boundary)] + plist,
                              prefix=prefix,
                              add_to_http_hdrs=add_to_http_hdrs,
                              content_type=content_type)

    def nextpart(self):
        boundary = self._boundary[-1]
        if self._first_part:
            self._first_part = False
        else:
            self._fp.write("\r\n")
        self._fp.write("--" + boundary + "\r\n")
        return self.__class__(self._fp)

    def lastpart(self):
        if self._first_part:
            self.nextpart()
        boundary = self._boundary.pop()
        self._fp.write("\r\n--" + boundary + "--\r\n")


class LocateError(ValueError): pass
class AmbiguityError(LocateError): pass
class ControlNotFoundError(LocateError): pass
class ItemNotFoundError(LocateError): pass

class ItemCountError(ValueError): pass


class ParseError(Exception): pass


class _AbstractFormParser:
    """forms attribute contains HTMLForm instances on completion."""
    # thanks to Moshe Zadka for an example of sgmllib/htmllib usage
    def __init__(self, entitydefs=None, encoding=DEFAULT_ENCODING):
        if entitydefs is None:
            entitydefs = get_entitydefs()
        self._entitydefs = entitydefs
        self._encoding = encoding

        self.base = None
        self.forms = []
        self.labels = []
        self._current_label = None
        self._current_form = None
        self._select = None
        self._optgroup = None
        self._option = None
        self._textarea = None

    def do_base(self, attrs):
        debug("%s", attrs)
        for key, value in attrs:
            if key == "href":
                self.base = value

    def end_body(self):
        debug("")
        if self._current_label is not None:
            self.end_label()
        if self._current_form is not None:
            self.end_form()

    def start_form(self, attrs):
        debug("%s", attrs)
        if self._current_form is not None:
            raise ParseError("nested FORMs")
        name = None
        action = None
        enctype = "application/x-www-form-urlencoded"
        method = "GET"
        d = {}
        for key, value in attrs:
            if key == "name":
                name = value
            elif key == "action":
                action = value
            elif key == "method":
                method = value.upper()
            elif key == "enctype":
                enctype = value.lower()
            d[key] = value
        controls = []
        self._current_form = (name, action, method, enctype), d, controls

    def end_form(self):
        debug("")
        if self._current_label is not None:
            self.end_label()
        if self._current_form is None:
            raise ParseError("end of FORM before start")
        self.forms.append(self._current_form)
        self._current_form = None

    def start_select(self, attrs):
        debug("%s", attrs)
        if self._current_form is None:
            raise ParseError("start of SELECT before start of FORM")
        if self._select is not None:
            raise ParseError("nested SELECTs")
        if self._textarea is not None:
            raise ParseError("SELECT inside TEXTAREA")
        d = {}
        for key, val in attrs:
            d[key] = val

        self._select = d
        self._add_label(d)

        self._append_select_control({"__select": d})

    def end_select(self):
        debug("")
        if self._current_form is None:
            raise ParseError("end of SELECT before start of FORM")
        if self._select is None:
            raise ParseError("end of SELECT before start")

        if self._option is not None:
            self._end_option()

        self._select = None

    def start_optgroup(self, attrs):
        debug("%s", attrs)
        if self._select is None:
            raise ParseError("OPTGROUP outside of SELECT")
        d = {}
        for key, val in attrs:
            d[key] = val

        self._optgroup = d

    def end_optgroup(self):
        debug("")
        if self._optgroup is None:
            raise ParseError("end of OPTGROUP before start")
        self._optgroup = None

    def _start_option(self, attrs):
        debug("%s", attrs)
        if self._select is None:
            raise ParseError("OPTION outside of SELECT")
        if self._option is not None:
            self._end_option()

        d = {}
        for key, val in attrs:
            d[key] = val

        self._option = {}
        self._option.update(d)
        if (self._optgroup and self._optgroup.has_key("disabled") and
            not self._option.has_key("disabled")):
            self._option["disabled"] = None

    def _end_option(self):
        debug("")
        if self._option is None:
            raise ParseError("end of OPTION before start")

        contents = self._option.get("contents", "").strip()
        self._option["contents"] = contents
        if not self._option.has_key("value"):
            self._option["value"] = contents
        if not self._option.has_key("label"):
            self._option["label"] = contents
        # stuff dict of SELECT HTML attrs into a special private key
        #  (gets deleted again later)
        self._option["__select"] = self._select
        self._append_select_control(self._option)
        self._option = None

    def _append_select_control(self, attrs):
        debug("%s", attrs)
        controls = self._current_form[2]
        name = self._select.get("name")
        controls.append(("select", name, attrs))

    def start_textarea(self, attrs):
        debug("%s", attrs)
        if self._current_form is None:
            raise ParseError("start of TEXTAREA before start of FORM")
        if self._textarea is not None:
            raise ParseError("nested TEXTAREAs")
        if self._select is not None:
            raise ParseError("TEXTAREA inside SELECT")
        d = {}
        for key, val in attrs:
            d[key] = val
        self._add_label(d)

        self._textarea = d

    def end_textarea(self):
        debug("")
        if self._current_form is None:
            raise ParseError("end of TEXTAREA before start of FORM")
        if self._textarea is None:
            raise ParseError("end of TEXTAREA before start")
        controls = self._current_form[2]
        name = self._textarea.get("name")
        controls.append(("textarea", name, self._textarea))
        self._textarea = None

    def start_label(self, attrs):
        debug("%s", attrs)
        if self._current_label:
            self.end_label()
        d = {}
        for key, val in attrs:
            d[key] = val
        taken = bool(d.get("for"))  # empty id is invalid
        d["__text"] = ""
        d["__taken"] = taken
        if taken:
            self.labels.append(d)
        self._current_label = d

    def end_label(self):
        debug("")
        label = self._current_label
        if label is None:
            # something is ugly in the HTML, but we're ignoring it
            return
        self._current_label = None
        label["__text"] = label["__text"]
        # if it is staying around, it is True in all cases
        del label["__taken"]

    def _add_label(self, d):
        #debug("%s", d)
        if self._current_label is not None:
            if self._current_label["__taken"]:
                self.end_label()  # be fuzzy
            else:
                self._current_label["__taken"] = True
                d["__label"] = self._current_label

    def handle_data(self, data):
        # according to http://www.w3.org/TR/html4/appendix/notes.html#h-B.3.1
        # line break immediately after start tags or immediately before end
        # tags must be ignored, but real browsers only ignore a line break
        # after a start tag, so we'll do that.
        if data[0:1] == '\n':
            data = data[1:]

        debug("%s", data)
        if self._option is not None:
            # self._option is a dictionary of the OPTION element's HTML
            # attributes, but it has two special keys, one of which is the
            # special "contents" key contains text between OPTION tags (the
            # other is the "__select" key: see the end_option method)
            map = self._option
            key = "contents"
        elif self._textarea is not None:
            map = self._textarea
            key = "value"
        # not if within option or textarea
        elif self._current_label is not None:
            map = self._current_label
            key = "__text"
        else:
            return

        if not map.has_key(key):
            map[key] = data
        else:
            map[key] = map[key] + data

    def do_button(self, attrs):
        debug("%s", attrs)
        if self._current_form is None:
            raise ParseError("start of BUTTON before start of FORM")
        d = {}
        d["type"] = "submit"  # default
        for key, val in attrs:
            d[key] = val
        controls = self._current_form[2]

        type = d["type"]
        name = d.get("name")
        # we don't want to lose information, so use a type string that
        # doesn't clash with INPUT TYPE={SUBMIT,RESET,BUTTON}
        # e.g. type for BUTTON/RESET is "resetbutton"
        #     (type for INPUT/RESET is "reset")
        type = type+"button"
        self._add_label(d)
        controls.append((type, name, d))

    def do_input(self, attrs):
        debug("%s", attrs)
        if self._current_form is None:
            raise ParseError("start of INPUT before start of FORM")
        d = {}
        d["type"] = "text"  # default
        for key, val in attrs:
            d[key] = val
        controls = self._current_form[2]

        type = d["type"]
        name = d.get("name")
        self._add_label(d)
        controls.append((type, name, d))

    def do_isindex(self, attrs):
        debug("%s", attrs)
        if self._current_form is None:
            raise ParseError("start of ISINDEX before start of FORM")
        d = {}
        for key, val in attrs:
            d[key] = val
        controls = self._current_form[2]

        self._add_label(d)
        # isindex doesn't have type or name HTML attributes
        controls.append(("isindex", None, d))

    def handle_entityref(self, name):
        #debug("%s", name)
        self.handle_data(unescape(
            '&%s;' % name, self._entitydefs, self._encoding))

    def handle_charref(self, name):
        #debug("%s", name)
        self.handle_data(unescape_charref(name, self._encoding))

    def unescape_attr(self, name):
        #debug("%s", name)
        return unescape(name, self._entitydefs, self._encoding)

    def unescape_attrs(self, attrs):
        #debug("%s", attrs)
        escaped_attrs = {}
        for key, val in attrs.items():
            try:
                val.items
            except AttributeError:
                escaped_attrs[key] = self.unescape_attr(val)
            else:
                # e.g. "__select" -- yuck!
                escaped_attrs[key] = self.unescape_attrs(val)
        return escaped_attrs

    def unknown_entityref(self, ref): self.handle_data("&%s;" % ref)
    def unknown_charref(self, ref): self.handle_data("&#%s;" % ref)


# HTMLParser.HTMLParser is recent, so live without it if it's not available
# (also, htmllib.HTMLParser is much more tolerant of bad HTML)
try:
    import HTMLParser
except ImportError:
    class XHTMLCompatibleFormParser:
        def __init__(self, entitydefs=None, encoding=DEFAULT_ENCODING):
            raise ValueError("HTMLParser could not be imported")
else:
    class XHTMLCompatibleFormParser(_AbstractFormParser, HTMLParser.HTMLParser):
        """Good for XHTML, bad for tolerance of incorrect HTML."""
        # thanks to Michael Howitz for this!
        def __init__(self, entitydefs=None, encoding=DEFAULT_ENCODING):
            HTMLParser.HTMLParser.__init__(self)
            _AbstractFormParser.__init__(self, entitydefs, encoding)

        def start_option(self, attrs):
            _AbstractFormParser._start_option(self, attrs)

        def end_option(self):
            _AbstractFormParser._end_option(self)

        def handle_starttag(self, tag, attrs):
            try:
                method = getattr(self, "start_" + tag)
            except AttributeError:
                try:
                    method = getattr(self, "do_" + tag)
                except AttributeError:
                    pass  # unknown tag
                else:
                    method(attrs)
            else:
                method(attrs)

        def handle_endtag(self, tag):
            try:
                method = getattr(self, "end_" + tag)
            except AttributeError:
                pass  # unknown tag
            else:
                method()

        def unescape(self, name):
            # Use the entitydefs passed into constructor, not
            # HTMLParser.HTMLParser's entitydefs.
            return self.unescape_attr(name)

        def unescape_attr_if_required(self, name):
            return name  # HTMLParser.HTMLParser already did it
        def unescape_attrs_if_required(self, attrs):
            return attrs  # ditto

import sgmllib
# monkeypatch to fix http://www.python.org/sf/803422 :-(
sgmllib.charref = re.compile("&#(x?[0-9a-fA-F]+)[^0-9a-fA-F]")
class _AbstractSgmllibParser(_AbstractFormParser):
    def do_option(self, attrs):
        _AbstractFormParser._start_option(self, attrs)

    def unescape_attr_if_required(self, name):
        return self.unescape_attr(name)
    def unescape_attrs_if_required(self, attrs):
        return self.unescape_attrs(attrs)

class FormParser(_AbstractSgmllibParser, sgmllib.SGMLParser):
    """Good for tolerance of incorrect HTML, bad for XHTML."""
    def __init__(self, entitydefs=None, encoding=DEFAULT_ENCODING):
        sgmllib.SGMLParser.__init__(self)
        _AbstractFormParser.__init__(self, entitydefs, encoding)

try:
    if sys.version_info[:2] < (2, 2):
        raise ImportError  # BeautifulSoup uses generators
    import BeautifulSoup
except ImportError:
    pass
else:
    class _AbstractBSFormParser(_AbstractSgmllibParser):
        bs_base_class = None
        def __init__(self, entitydefs=None, encoding=DEFAULT_ENCODING):
            _AbstractFormParser.__init__(self, entitydefs, encoding)
            self.bs_base_class.__init__(self)
        def handle_data(self, data):
            _AbstractFormParser.handle_data(self, data)
            self.bs_base_class.handle_data(self, data)

    class RobustFormParser(_AbstractBSFormParser, BeautifulSoup.BeautifulSoup):
        """Tries to be highly tolerant of incorrect HTML."""
        bs_base_class = BeautifulSoup.BeautifulSoup
    class NestingRobustFormParser(_AbstractBSFormParser,
                                  BeautifulSoup.ICantBelieveItsBeautifulSoup):
        """Tries to be highly tolerant of incorrect HTML.

        Different from RobustFormParser in that it more often guesses nesting
        above missing end tags (see BeautifulSoup docs).

        """
        bs_base_class = BeautifulSoup.ICantBelieveItsBeautifulSoup

#FormParser = XHTMLCompatibleFormParser  # testing hack
#FormParser = RobustFormParser  # testing hack

def ParseResponse(response, select_default=False,
                  ignore_errors=False,  # ignored!
                  form_parser_class=FormParser,
                  request_class=urllib2.Request,
                  entitydefs=None,
                  backwards_compat=True,
                  encoding=DEFAULT_ENCODING,
                  ):
    """Parse HTTP response and return a list of HTMLForm instances.

    The return value of urllib2.urlopen can be conveniently passed to this
    function as the response parameter.

    ClientForm.ParseError is raised on parse errors.

    response: file-like object (supporting read() method) with a method
     geturl(), returning the URI of the HTTP response
    select_default: for multiple-selection SELECT controls and RADIO controls,
     pick the first item as the default if none are selected in the HTML
    form_parser_class: class to instantiate and use to pass
    request_class: class to return from .click() method (default is
     urllib2.Request)
    entitydefs: mapping like {"&amp;": "&", ...} containing HTML entity
     definitions (a sensible default is used)
    encoding: character encoding used for encoding numeric character references
     when matching link text.  ClientForm does not attempt to find the encoding
     in a META HTTP-EQUIV attribute in the document itself (mechanize, for
     example, does do that and will pass the correct value to ClientForm using
     this parameter).

    backwards_compat: boolean that determines whether the returned HTMLForm
     objects are backwards-compatible with old code.  If backwards_compat is
     true:

     - ClientForm 0.1 code will continue to work as before.

     - Label searches that do not specify a nr (number or count) will always
       get the first match, even if other controls match.  If
       backwards_compat is False, label searches that have ambiguous results
       will raise an AmbiguityError.

     - Item label matching is done by strict string comparison rather than
       substring matching.

     - De-selecting individual list items is allowed even if the Item is
       disabled.

    The backwards_compat argument will be deprecated in a future release.

    Pass a true value for select_default if you want the behaviour specified by
    RFC 1866 (the HTML 2.0 standard), which is to select the first item in a
    RADIO or multiple-selection SELECT control if none were selected in the
    HTML.  Most browsers (including Microsoft Internet Explorer (IE) and
    Netscape Navigator) instead leave all items unselected in these cases.  The
    W3C HTML 4.0 standard leaves this behaviour undefined in the case of
    multiple-selection SELECT controls, but insists that at least one RADIO
    button should be checked at all times, in contradiction to browser
    behaviour.

    There is a choice of parsers.  ClientForm.XHTMLCompatibleFormParser (uses
    HTMLParser.HTMLParser) works best for XHTML, ClientForm.FormParser (uses
    sgmllib.SGMLParser) (the default) works better for ordinary grubby HTML.
    Note that HTMLParser is only available in Python 2.2 and later.  You can
    pass your own class in here as a hack to work around bad HTML, but at your
    own risk: there is no well-defined interface.

    """
    return ParseFile(response, response.geturl(), select_default,
                     False,
                     form_parser_class,
                     request_class,
                     entitydefs,
                     backwards_compat,
                     encoding,
                     )

def ParseFile(file, base_uri, select_default=False,
              ignore_errors=False,  # ignored!
              form_parser_class=FormParser,
              request_class=urllib2.Request,
              entitydefs=None,
              backwards_compat=True,
              encoding=DEFAULT_ENCODING,
              ):
    """Parse HTML and return a list of HTMLForm instances.

    ClientForm.ParseError is raised on parse errors.

    file: file-like object (supporting read() method) containing HTML with zero
     or more forms to be parsed
    base_uri: the URI of the document (note that the base URI used to submit
     the form will be that given in the BASE element if present, not that of
     the document)

    For the other arguments and further details, see ParseResponse.__doc__.

    """
    if backwards_compat:
        deprecation("operating in backwards-compatibility mode")
    fp = form_parser_class(entitydefs, encoding)
    while 1:
        data = file.read(CHUNK)
        try:
            fp.feed(data)
        except ParseError, e:
            e.base_uri = base_uri
            raise
        if len(data) != CHUNK: break
    if fp.base is not None:
        # HTML BASE element takes precedence over document URI
        base_uri = fp.base
    labels = []  # Label(label) for label in fp.labels]
    id_to_labels = {}
    for l in fp.labels:
        label = Label(l)
        labels.append(label)
        for_id = l["for"]
        coll = id_to_labels.get(for_id)
        if coll is None:
            id_to_labels[for_id] = [label]
        else:
            coll.append(label)
    forms = []
    for (name, action, method, enctype), attrs, controls in fp.forms:
        if action is None:
            action = base_uri
        else:
            action = urljoin(base_uri, action)
        action = fp.unescape_attr_if_required(action)
        name = fp.unescape_attr_if_required(name)
        attrs = fp.unescape_attrs_if_required(attrs)
        # would be nice to make HTMLForm class (form builder) pluggable
        form = HTMLForm(
            action, method, enctype, name, attrs, request_class,
            forms, labels, id_to_labels, backwards_compat)
        for ii in range(len(controls)):
            type, name, attrs = controls[ii]
            attrs = fp.unescape_attrs_if_required(attrs)
            name = fp.unescape_attr_if_required(name)
            # index=ii*10 allows ImageControl to return multiple ordered pairs
            form.new_control(type, name, attrs, select_default=select_default,
                             index=ii*10)
        forms.append(form)
    for form in forms:
        form.fixup()
    return forms


class Label:
    def __init__(self, attrs):
        self.id = attrs.get("for")
        self._text = attrs.get("__text").strip()
        self._ctext = compress_text(self._text)
        self.attrs = attrs
        self._backwards_compat = False  # maintained by HTMLForm

    def __getattr__(self, name):
        if name == "text":
            if self._backwards_compat:
                return self._text
            else:
                return self._ctext
        return getattr(Label, name)

    def __setattr__(self, name, value):
        if name == "text":
            # don't see any need for this, so make it read-only
            raise AttributeError("text attribute is read-only")
        self.__dict__[name] = value

    def __str__(self):
        return "<Label(id=%r, text=%r)>" % (self.id, self.text)


def _get_label(attrs):
    text = attrs.get("__label")
    if text is not None:
        return Label(text)
    else:
        return None

class Control:
    """An HTML form control.

    An HTMLForm contains a sequence of Controls.  The Controls in an HTMLForm
    are accessed using the HTMLForm.find_control method or the
    HTMLForm.controls attribute.

    Control instances are usually constructed using the ParseFile /
    ParseResponse functions.  If you use those functions, you can ignore the
    rest of this paragraph.  A Control is only properly initialised after the
    fixup method has been called.  In fact, this is only strictly necessary for
    ListControl instances.  This is necessary because ListControls are built up
    from ListControls each containing only a single item, and their initial
    value(s) can only be known after the sequence is complete.

    The types and values that are acceptable for assignment to the value
    attribute are defined by subclasses.

    If the disabled attribute is true, this represents the state typically
    represented by browsers by 'greying out' a control.  If the disabled
    attribute is true, the Control will raise AttributeError if an attempt is
    made to change its value.  In addition, the control will not be considered
    'successful' as defined by the W3C HTML 4 standard -- ie. it will
    contribute no data to the return value of the HTMLForm.click* methods.  To
    enable a control, set the disabled attribute to a false value.

    If the readonly attribute is true, the Control will raise AttributeError if
    an attempt is made to change its value.  To make a control writable, set
    the readonly attribute to a false value.

    All controls have the disabled and readonly attributes, not only those that
    may have the HTML attributes of the same names.

    On assignment to the value attribute, the following exceptions are raised:
    TypeError, AttributeError (if the value attribute should not be assigned
    to, because the control is disabled, for example) and ValueError.

    If the name or value attributes are None, or the value is an empty list, or
    if the control is disabled, the control is not successful.

    Public attributes:

    type: string describing type of control (see the keys of the
     HTMLForm.type2class dictionary for the allowable values) (readonly)
    name: name of control (readonly)
    value: current value of control (subclasses may allow a single value, a
     sequence of values, or either)
    disabled: disabled state
    readonly: readonly state
    id: value of id HTML attribute

    """
    def __init__(self, type, name, attrs, index=None):
        """
        type: string describing type of control (see the keys of the
         HTMLForm.type2class dictionary for the allowable values)
        name: control name
        attrs: HTML attributes of control's HTML element

        """
        raise NotImplementedError()

    def add_to_form(self, form):
        self._form = form
        form.controls.append(self)

    def fixup(self):
        pass

    def is_of_kind(self, kind):
        raise NotImplementedError()

    def clear(self):
        raise NotImplementedError()

    def __getattr__(self, name): raise NotImplementedError()
    def __setattr__(self, name, value): raise NotImplementedError()

    def pairs(self):
        """Return list of (key, value) pairs suitable for passing to urlencode.
        """
        return [(k, v) for (i, k, v) in self._totally_ordered_pairs()]

    def _totally_ordered_pairs(self):
        """Return list of (key, value, index) tuples.

        Like pairs, but allows preserving correct ordering even where several
        controls are involved.

        """
        raise NotImplementedError()

    def _write_mime_data(self, mw, name, value):
        """Write data for a subitem of this control to a MimeWriter."""
        # called by HTMLForm
        mw2 = mw.nextpart()
        mw2.addheader("Content-disposition",
                      'form-data; name="%s"' % name, 1)
        f = mw2.startbody(prefix=0)
        f.write(value)

    def __str__(self):
        raise NotImplementedError()

    def get_labels(self):
        """Return all labels (Label instances) for this control.
        
        If the control was surrounded by a <label> tag, that will be the first
        label; all other labels, connected by 'for' and 'id', are in the order
        that appear in the HTML.

        """
        res = []
        if self._label:
            res.append(self._label)
        if self.id:
            res.extend(self._form._id_to_labels.get(self.id, ()))
        return res


#---------------------------------------------------
class ScalarControl(Control):
    """Control whose value is not restricted to one of a prescribed set.

    Some ScalarControls don't accept any value attribute.  Otherwise, takes a
    single value, which must be string-like.

    Additional read-only public attribute:

    attrs: dictionary mapping the names of original HTML attributes of the
     control to their values

    """
    def __init__(self, type, name, attrs, index=None):
        self._index = index
        self._label = _get_label(attrs)
        self.__dict__["type"] = type.lower()
        self.__dict__["name"] = name
        self._value = attrs.get("value")
        self.disabled = attrs.has_key("disabled")
        self.readonly = attrs.has_key("readonly")
        self.id = attrs.get("id")

        self.attrs = attrs.copy()

        self._clicked = False

    def __getattr__(self, name):
        if name == "value":
            return self.__dict__["_value"]
        else:
            raise AttributeError("%s instance has no attribute '%s'" %
                                 (self.__class__.__name__, name))

    def __setattr__(self, name, value):
        if name == "value":
            if not isstringlike(value):
                raise TypeError("must assign a string")
            elif self.readonly:
                raise AttributeError("control '%s' is readonly" % self.name)
            elif self.disabled:
                raise AttributeError("control '%s' is disabled" % self.name)
            self.__dict__["_value"] = value
        elif name in ("name", "type"):
            raise AttributeError("%s attribute is readonly" % name)
        else:
            self.__dict__[name] = value

    def _totally_ordered_pairs(self):
        name = self.name
        value = self.value
        if name is None or value is None or self.disabled:
            return []
        return [(self._index, name, value)]

    def clear(self):
        if self.readonly:
            raise AttributeError("control '%s' is readonly" % self.name)
        self.__dict__["_value"] = None

    def __str__(self):
        name = self.name
        value = self.value
        if name is None: name = "<None>"
        if value is None: value = "<None>"

        infos = []
        if self.disabled: infos.append("disabled")
        if self.readonly: infos.append("readonly")
        info = ", ".join(infos)
        if info: info = " (%s)" % info

        return "<%s(%s=%s)%s>" % (self.__class__.__name__, name, value, info)


#---------------------------------------------------
class TextControl(ScalarControl):
    """Textual input control.

    Covers:

    INPUT/TEXT
    INPUT/PASSWORD
    INPUT/HIDDEN
    TEXTAREA

    """
    def __init__(self, type, name, attrs, index=None):
        ScalarControl.__init__(self, type, name, attrs, index)
        if self.type == "hidden": self.readonly = True
        if self._value is None:
            self._value = ""

    def is_of_kind(self, kind): return kind == "text"

#---------------------------------------------------
class FileControl(ScalarControl):
    """File upload with INPUT TYPE=FILE.

    The value attribute of a FileControl is always None.  Use add_file instead.

    Additional public method: add_file

    """

    def __init__(self, type, name, attrs, index=None):
        ScalarControl.__init__(self, type, name, attrs, index)
        self._value = None
        self._upload_data = []

    def is_of_kind(self, kind): return kind == "file"

    def clear(self):
        if self.readonly:
            raise AttributeError("control '%s' is readonly" % self.name)
        self._upload_data = []

    def __setattr__(self, name, value):
        if name in ("value", "name", "type"):
            raise AttributeError("%s attribute is readonly" % name)
        else:
            self.__dict__[name] = value

    def add_file(self, file_object, content_type=None, filename=None):
        if not hasattr(file_object, "read"):
            raise TypeError("file-like object must have read method")
        if content_type is not None and not isstringlike(content_type):
            raise TypeError("content type must be None or string-like")
        if filename is not None and not isstringlike(filename):
            raise TypeError("filename must be None or string-like")
        if content_type is None:
            content_type = "application/octet-stream"
        self._upload_data.append((file_object, content_type, filename))

    def _totally_ordered_pairs(self):
        # XXX should it be successful even if unnamed?
        if self.name is None or self.disabled:
            return []
        return [(self._index, self.name, "")]

    def _write_mime_data(self, mw, _name, _value):
        # called by HTMLForm
        # assert _name == self.name and _value == ''
        if len(self._upload_data) == 1:
            # single file
            file_object, content_type, filename = self._upload_data[0]
            mw2 = mw.nextpart()
            fn_part = filename and ('; filename="%s"' % filename) or ""
            disp = 'form-data; name="%s"%s' % (self.name, fn_part)
            mw2.addheader("Content-disposition", disp, prefix=1)
            fh = mw2.startbody(content_type, prefix=0)
            fh.write(file_object.read())
        elif len(self._upload_data) != 0:
            # multiple files
            mw2 = mw.nextpart()
            disp = 'form-data; name="%s"' % self.name
            mw2.addheader("Content-disposition", disp, prefix=1)
            fh = mw2.startmultipartbody("mixed", prefix=0)
            for file_object, content_type, filename in self._upload_data:
                mw3 = mw2.nextpart()
                fn_part = filename and ('; filename="%s"' % filename) or ""
                disp = "file%s" % fn_part
                mw3.addheader("Content-disposition", disp, prefix=1)
                fh2 = mw3.startbody(content_type, prefix=0)
                fh2.write(file_object.read())
            mw2.lastpart()

    def __str__(self):
        name = self.name
        if name is None: name = "<None>"

        if not self._upload_data:
            value = "<No files added>"
        else:
            value = []
            for file, ctype, filename in self._upload_data:
                if filename is None:
                    value.append("<Unnamed file>")
                else:
                    value.append(filename)
            value = ", ".join(value)

        info = []
        if self.disabled: info.append("disabled")
        if self.readonly: info.append("readonly")
        info = ", ".join(info)
        if info: info = " (%s)" % info

        return "<%s(%s=%s)%s>" % (self.__class__.__name__, name, value, info)


#---------------------------------------------------
class IsindexControl(ScalarControl):
    """ISINDEX control.

    ISINDEX is the odd-one-out of HTML form controls.  In fact, it isn't really
    part of regular HTML forms at all, and predates it.  You're only allowed
    one ISINDEX per HTML document.  ISINDEX and regular form submission are
    mutually exclusive -- either submit a form, or the ISINDEX.

    Having said this, since ISINDEX controls may appear in forms (which is
    probably bad HTML), ParseFile / ParseResponse will include them in the
    HTMLForm instances it returns.  You can set the ISINDEX's value, as with
    any other control (but note that ISINDEX controls have no name, so you'll
    need to use the type argument of set_value!).  When you submit the form,
    the ISINDEX will not be successful (ie., no data will get returned to the
    server as a result of its presence), unless you click on the ISINDEX
    control, in which case the ISINDEX gets submitted instead of the form:

    form.set_value("my isindex value", type="isindex")
    urllib2.urlopen(form.click(type="isindex"))

    ISINDEX elements outside of FORMs are ignored.  If you want to submit one
    by hand, do it like so:

    url = urlparse.urljoin(page_uri, "?"+urllib.quote_plus("my isindex value"))
    result = urllib2.urlopen(url)

    """
    def __init__(self, type, name, attrs, index=None):
        ScalarControl.__init__(self, type, name, attrs, index)
        if self._value is None:
            self._value = ""

    def is_of_kind(self, kind): return kind in ["text", "clickable"]

    def _totally_ordered_pairs(self):
        return []

    def _click(self, form, coord, return_type, request_class=urllib2.Request):
        # Relative URL for ISINDEX submission: instead of "foo=bar+baz",
        # want "bar+baz".
        # This doesn't seem to be specified in HTML 4.01 spec. (ISINDEX is
        # deprecated in 4.01, but it should still say how to submit it).
        # Submission of ISINDEX is explained in the HTML 3.2 spec, though.
        parts = urlparse.urlparse(form.action)
        rest, (query, frag) = parts[:-2], parts[-2:]
        parts = rest + (urllib.quote_plus(self.value), "")
        url = urlparse.urlunparse(parts)
        req_data = url, None, []

        if return_type == "pairs":
            return []
        elif return_type == "request_data":
            return req_data
        else:
            return request_class(url)

    def __str__(self):
        value = self.value
        if value is None: value = "<None>"

        infos = []
        if self.disabled: infos.append("disabled")
        if self.readonly: infos.append("readonly")
        info = ", ".join(infos)
        if info: info = " (%s)" % info

        return "<%s(%s)%s>" % (self.__class__.__name__, value, info)


#---------------------------------------------------
class IgnoreControl(ScalarControl):
    """Control that we're not interested in.

    Covers:

    INPUT/RESET
    BUTTON/RESET
    INPUT/BUTTON
    BUTTON/BUTTON

    These controls are always unsuccessful, in the terminology of HTML 4 (ie.
    they never require any information to be returned to the server).

    BUTTON/BUTTON is used to generate events for script embedded in HTML.

    The value attribute of IgnoreControl is always None.

    """
    def __init__(self, type, name, attrs, index=None):
        ScalarControl.__init__(self, type, name, attrs, index)
        self._value = None

    def is_of_kind(self, kind): return False

    def __setattr__(self, name, value):
        if name == "value":
            raise AttributeError(
                "control '%s' is ignored, hence read-only" % self.name)
        elif name in ("name", "type"):
            raise AttributeError("%s attribute is readonly" % name)
        else:
            self.__dict__[name] = value


#---------------------------------------------------
# ListControls

# helpers and subsidiary classes

class Item:
    def __init__(self, control, attrs, index=None):
        label = _get_label(attrs)
        self.__dict__.update({
            "name": attrs["value"],
            "_labels": label and [label] or [],
            "attrs": attrs,
            "_control": control,
            "disabled": attrs.has_key("disabled"),
            "_selected": False,
            "id": attrs.get("id"),
            "_index": index,
            })
        control.items.append(self)

    def get_labels(self):
        """Return all labels (Label instances) for this item.
        
        For items that represent radio buttons or checkboxes, if the item was
        surrounded by a <label> tag, that will be the first label; all other
        labels, connected by 'for' and 'id', are in the order that appear in
        the HTML.
        
        For items that represent select options, if the option had a label
        attribute, that will be the first label.  If the option has contents
        (text within the option tags) and it is not the same as the label
        attribute (if any), that will be a label.  There is nothing in the
        spec to my knowledge that makes an option with an id unable to be the
        target of a label's for attribute, so those are included, if any, for
        the sake of consistency and completeness.

        """
        res = []
        res.extend(self._labels)
        if self.id:
            res.extend(self._control._form._id_to_labels.get(self.id, ()))
        return res

    def __getattr__(self, name):
        if name=="selected":
            return self._selected
        raise AttributeError(name)

    def __setattr__(self, name, value):
        if name == "selected":
            self._control._set_selected_state(self, value)
        elif name == "disabled":
            self.__dict__["disabled"] = bool(value)
        else:
            raise AttributeError(name)

    def __str__(self):
        res = self.name
        if self.selected:
            res = "*" + res
        if self.disabled:
            res = "(%s)" % res
        return res

    def __repr__(self):
        attrs = [("name", self.name), ("id", self.id)]+self.attrs.items()
        return "<%s %s>" % (
            self.__class__.__name__,
            " ".join(["%s=%r" % (k, v) for k, v in attrs])
            )

def disambiguate(items, nr, **kwds):
    msgs = []
    for key, value in kwds.items():
        msgs.append("%s=%r" % (key, value))
    msg = " ".join(msgs)
    if not items:
        raise ItemNotFoundError(msg)
    if nr is None:
        if len(items) > 1:
            raise AmbiguityError(msg)
        nr = 0
    if len(items) <= nr:
        raise ItemNotFoundError(msg)
    return items[nr]

class ListControl(Control):
    """Control representing a sequence of items.

    The value attribute of a ListControl represents the successful list items
    in the control.  The successful list items are those that are selected and
    not disabled.

    ListControl implements both list controls that take a length-1 value
    (single-selection) and those that take length >1 values
    (multiple-selection).

    ListControls accept sequence values only.  Some controls only accept
    sequences of length 0 or 1 (RADIO, and single-selection SELECT).
    In those cases, ItemCountError is raised if len(sequence) > 1.  CHECKBOXes
    and multiple-selection SELECTs (those having the "multiple" HTML attribute)
    accept sequences of any length.

    Note the following mistake:

    control.value = some_value
    assert control.value == some_value    # not necessarily true

    The reason for this is that the value attribute always gives the list items
    in the order they were listed in the HTML.

    ListControl items can also be referred to by their labels instead of names.
    Use the label argument to .get(), and the .set_value_by_label(),
    .get_value_by_label() methods.

    Note that, rather confusingly, though SELECT controls are represented in
    HTML by SELECT elements (which contain OPTION elements, representing
    individual list items), CHECKBOXes and RADIOs are not represented by *any*
    element.  Instead, those controls are represented by a collection of INPUT
    elements.  For example, this is a SELECT control, named "control1":

    <select name="control1">
     <option>foo</option>
     <option value="1">bar</option>
    </select>

    and this is a CHECKBOX control, named "control2":

    <input type="checkbox" name="control2" value="foo" id="cbe1">
    <input type="checkbox" name="control2" value="bar" id="cbe2">

    The id attribute of a CHECKBOX or RADIO ListControl is always that of its
    first element (for example, "cbe1" above).


    Additional read-only public attribute: multiple.

    """

    # ListControls are built up by the parser from their component items by
    # creating one ListControl per item, consolidating them into a single
    # master ListControl held by the HTMLForm:

    # -User calls form.new_control(...)
    # -Form creates Control, and calls control.add_to_form(self).
    # -Control looks for a Control with the same name and type in the form,
    #  and if it finds one, merges itself with that control by calling
    #  control.merge_control(self).  The first Control added to the form, of
    #  a particular name and type, is the only one that survives in the
    #  form.
    # -Form calls control.fixup for all its controls.  ListControls in the
    #  form know they can now safely pick their default values.

    # To create a ListControl without an HTMLForm, use:

    # control.merge_control(new_control)

    # (actually, it's much easier just to use ParseFile)

    _label = None

    def __init__(self, type, name, attrs={}, select_default=False,
                 called_as_base_class=False, index=None):
        """
        select_default: for RADIO and multiple-selection SELECT controls, pick
         the first item as the default if no 'selected' HTML attribute is
         present

        """
        if not called_as_base_class:
            raise NotImplementedError()

        self.__dict__["type"] = type.lower()
        self.__dict__["name"] = name
        self._value = attrs.get("value")
        self.disabled = False
        self.readonly = False
        self.id = attrs.get("id")

        # As Controls are merged in with .merge_control(), self.attrs will
        # refer to each Control in turn -- always the most recently merged
        # control.  Each merged-in Control instance corresponds to a single
        # list item: see ListControl.__doc__.
        self.items = []
        self._form = None

        self._select_default = select_default
        self._clicked = False

    def clear(self):
        self.value = []

    def is_of_kind(self, kind):
        if kind  == "list":
            return True
        elif kind == "multilist":
            return bool(self.multiple)
        elif kind == "singlelist":
            return not self.multiple
        else:
            return False

    def get_items(self, name=None, label=None, id=None,
                  exclude_disabled=False):
        """Return matching items by name or label.

        For argument docs, see the docstring for .get()

        """
        if name is not None and not isstringlike(name):
            raise TypeError("item name must be string-like")
        if label is not None and not isstringlike(label):
            raise TypeError("item label must be string-like")
        if id is not None and not isstringlike(id):
            raise TypeError("item id must be string-like")
        items = []  # order is important
        compat = self._form.backwards_compat
        for o in self.items:
            if exclude_disabled and o.disabled:
                continue
            if name is not None and o.name != name:
                continue
            if label is not None:
                for l in o.get_labels():
                    if ((compat and l.text == label) or
                        (not compat and l.text.find(label) > -1)):
                        break
                else:
                    continue
            if id is not None and o.id != id:
                continue
            items.append(o)
        return items

    def get(self, name=None, label=None, id=None, nr=None,
            exclude_disabled=False):
        """Return item by name or label, disambiguating if necessary with nr.

        All arguments must be passed by name, with the exception of 'name',
        which may be used as a positional argument.

        If name is specified, then the item must have the indicated name.

        If label is specified, then the item must have a label whose
        whitespace-compressed, stripped, text substring-matches the indicated
        label string (eg. label="please choose" will match
        "  Do  please  choose an item ").

        If id is specified, then the item must have the indicated id.

        nr is an optional 0-based index of the items matching the query.

        If nr is the default None value and more than item is found, raises
        AmbiguityError (unless the HTMLForm instance's backwards_compat
        attribute is true).

        If no item is found, or if items are found but nr is specified and not
        found, raises ItemNotFoundError.

        Optionally excludes disabled items.

        """
        if nr is None and self._form.backwards_compat:
            nr = 0  # :-/
        items = self.get_items(name, label, id, exclude_disabled)
        return disambiguate(items, nr, name=name, label=label, id=id)

    def _get(self, name, by_label=False, nr=None, exclude_disabled=False):
        # strictly for use by deprecated methods
        if by_label:
            name, label = None, name
        else:
            name, label = name, None
        return self.get(name, label, nr, exclude_disabled)

    def toggle(self, name, by_label=False, nr=None):
        """Deprecated: given a name or label and optional disambiguating index
        nr, toggle the matching item's selection.

        Selecting items follows the behavior described in the docstring of the
        'get' method.

        if the item is disabled, or this control is disabled or readonly,
        raise AttributeError.

        """
        deprecation(
            "item = control.get(...); item.selected = not item.selected")
        o = self._get(name, by_label, nr)
        self._set_selected_state(o, not o.selected)

    def set(self, selected, name, by_label=False, nr=None):
        """Deprecated: given a name or label and optional disambiguating index
        nr, set the matching item's selection to the bool value of selected.

        Selecting items follows the behavior described in the docstring of the
        'get' method.

        if the item is disabled, or this control is disabled or readonly,
        raise AttributeError.

        """
        deprecation(
            "control.get(...).selected = <boolean>")
        self._set_selected_state(self._get(name, by_label, nr), selected)

    def _set_selected_state(self, item, action):
        # action:
        # bool False: off
        # bool True: on
        if self.disabled:
            raise AttributeError("control '%s' is disabled" % self.name)
        if self.readonly:
            raise AttributeError("control '%s' is readonly" % self.name)
        action == bool(action)
        compat = self._form.backwards_compat
        if not compat and item.disabled:
            raise AttributeError("item is disabled")
        else:
            if compat and item.disabled and action:
                raise AttributeError("item is disabled")
            if self.multiple:
                item.__dict__["_selected"] = action
            else:
                if not action:
                    item.__dict__["_selected"] = False
                else:
                    for o in self.items:
                        o.__dict__["_selected"] = False
                    item.__dict__["_selected"] = True

    def toggle_single(self, by_label=None):
        """Deprecated: toggle the selection of the single item in this control.
        
        Raises ItemCountError if the control does not contain only one item.
        
        by_label argument is ignored, and included only for backwards
        compatibility.

        """
        deprecation(
            "control.items[0].selected = not control.items[0].selected")
        if len(self.items) != 1:
            raise ItemCountError(
                "'%s' is not a single-item control" % self.name)
        item = self.items[0]
        self._set_selected_state(item, not item.selected)

    def set_single(self, selected, by_label=None):
        """Deprecated: set the selection of the single item in this control.
        
        Raises ItemCountError if the control does not contain only one item.
        
        by_label argument is ignored, and included only for backwards
        compatibility.

        """
        deprecation(
            "control.items[0].selected = <boolean>")
        if len(self.items) != 1:
            raise ItemCountError(
                "'%s' is not a single-item control" % self.name)
        self._set_selected_state(self.items[0], selected)

    def get_item_disabled(self, name, by_label=False, nr=None):
        """Get disabled state of named list item in a ListControl."""
        deprecation(
            "control.get(...).disabled")
        return self._get(name, by_label, nr).disabled

    def set_item_disabled(self, disabled, name, by_label=False, nr=None):
        """Set disabled state of named list item in a ListControl.

        disabled: boolean disabled state

        """
        deprecation(
            "control.get(...).disabled = <boolean>")
        self._get(name, by_label, nr).disabled = disabled

    def set_all_items_disabled(self, disabled):
        """Set disabled state of all list items in a ListControl.

        disabled: boolean disabled state

        """
        for o in self.items:
            o.disabled = disabled

    def get_item_attrs(self, name, by_label=False, nr=None):
        """Return dictionary of HTML attributes for a single ListControl item.

        The HTML element types that describe list items are: OPTION for SELECT
        controls, INPUT for the rest.  These elements have HTML attributes that
        you may occasionally want to know about -- for example, the "alt" HTML
        attribute gives a text string describing the item (graphical browsers
        usually display this as a tooltip).

        The returned dictionary maps HTML attribute names to values.  The names
        and values are taken from the original HTML.

        """
        deprecation(
            "control.get(...).attrs")
        return self._get(name, by_label, nr).attrs

    def add_to_form(self, form):
        assert self._form is None or form == self._form, (
            "can't add control to more than one form")
        self._form = form
        try:
            control = form.find_control(self.name, self.type)
        except ControlNotFoundError:
            Control.add_to_form(self, form)
        else:
            control.merge_control(self)

    def merge_control(self, control):
        assert bool(control.multiple) == bool(self.multiple)
        # usually, isinstance(control, self.__class__)
        self.items.extend(control.items)

    def fixup(self):
        """
        ListControls are built up from component list items (which are also
        ListControls) during parsing.  This method should be called after all
        items have been added.  See ListControl.__doc__ for the reason this is
        required.

        """
        # Need to set default selection where no item was indicated as being
        # selected by the HTML:

        # CHECKBOX:
        #  Nothing should be selected.
        # SELECT/single, SELECT/multiple and RADIO:
        #  RFC 1866 (HTML 2.0): says first item should be selected.
        #  W3C HTML 4.01 Specification: says that client behaviour is
        #   undefined in this case.  For RADIO, exactly one must be selected,
        #   though which one is undefined.
        #  Both Netscape and Microsoft Internet Explorer (IE) choose first
        #   item for SELECT/single.  However, both IE5 and Mozilla (both 1.0
        #   and Firebird 0.6) leave all items unselected for RADIO and
        #   SELECT/multiple.

        # Since both Netscape and IE all choose the first item for
        # SELECT/single, we do the same.  OTOH, both Netscape and IE
        # leave SELECT/multiple with nothing selected, in violation of RFC 1866
        # (but not in violation of the W3C HTML 4 standard); the same is true
        # of RADIO (which *is* in violation of the HTML 4 standard).  We follow
        # RFC 1866 if the _select_default attribute is set, and Netscape and IE
        # otherwise.  RFC 1866 and HTML 4 are always violated insofar as you
        # can deselect all items in a RadioControl.
        
        for o in self.items: 
            # set items' controls to self, now that we've merged
            o.__dict__["_control"] = self

    def __getattr__(self, name):
        if name == "value":
            compat = self._form.backwards_compat
            return [o.name for o in self.items if o.selected and
                    (not o.disabled or compat)]
        else:
            raise AttributeError("%s instance has no attribute '%s'" %
                                 (self.__class__.__name__, name))

    def __setattr__(self, name, value):
        if name == "value":
            if self.disabled:
                raise AttributeError("control '%s' is disabled" % self.name)
            if self.readonly:
                raise AttributeError("control '%s' is readonly" % self.name)
            self._set_value(value)
        elif name in ("name", "type", "multiple"):
            raise AttributeError("%s attribute is readonly" % name)
        else:
            self.__dict__[name] = value

    def _set_value(self, value):
        if value is None or isstringlike(value):
            raise TypeError("ListControl, must set a sequence")
        if not value:
            compat = self._form.backwards_compat
            for o in self.items:
                if not o.disabled or compat:
                    o.selected = False
        elif self.multiple:
            self._multiple_set_value(value)
        elif len(value) > 1:
            raise ItemCountError(
                "single selection list, must set sequence of "
                "length 0 or 1")
        else:
            self._single_set_value(value)

    def _get_items(self, name, target=1):
        all_items = self.get_items(name)
        items = [o for o in all_items if not o.disabled]
        if len(items) < target:
            if len(all_items) < target:
                raise ItemNotFoundError(
                    "insufficient items with name %r" % name)
            else:
                raise AttributeError(
                    "insufficient non-disabled items with name %s" % name)
        on = []
        off = []
        for o in items:
            if o.selected:
                on.append(o)
            else:
                off.append(o)
        return on, off

    def _single_set_value(self, value):
        assert len(value) == 1
        on, off = self._get_items(value[0])
        assert len(on) <= 1
        if not on:
            off[0].selected = True

    def _multiple_set_value(self, value):
        compat = self._form.backwards_compat
        turn_on = []  # transactional-ish
        turn_off = [item for item in self.items if
                    item.selected and (not item.disabled or compat)]
        names = {}
        for nn in value:
            if nn in names.keys():
                names[nn] += 1
            else:
                names[nn] = 1
        for name, count in names.items():
            on, off = self._get_items(name, count)
            for i in range(count):
                if on:
                    item = on[0]
                    del on[0]
                    del turn_off[turn_off.index(item)]
                else:
                    item = off[0]
                    del off[0]
                    turn_on.append(item)
        for item in turn_off:
            item.selected = False
        for item in turn_on:
            item.selected = True

    def set_value_by_label(self, value):
        """Set the value of control by item labels.

        value is expected to be an iterable of strings that are substrings of
        the item labels that should be selected.  Before substring matching is
        performed, the original label text is whitespace-compressed
        (consecutive whitespace characters are converted to a single space
        character) and leading and trailing whitespace is stripped.  Ambiguous
        labels are accepted without complaint if the form's backwards_compat is
        True; otherwise, it will not complain as long as all ambiguous labels
        share the same item name (e.g. OPTION value).

        """
        if isstringlike(value):
            raise TypeError(value)
        if not self.multiple and len(value) > 1:
            raise ItemCountError(
                "single selection list, must set sequence of "
                "length 0 or 1")
        items = []
        for nn in value:
            found = self.get_items(label=nn)
            if len(found) > 1:
                if not self._form.backwards_compat:
                    # ambiguous labels are fine as long as item names (e.g.
                    # OPTION values) are same
                    opt_name = found[0].name
                    if [o for o in found[1:] if o.name != opt_name]:
                        raise AmbiguityError(nn)
                else:
                    # OK, we'll guess :-(  Assume first available item.
                    found = found[:1]
            for o in found:
                # For the multiple-item case, we could try to be smarter,
                # saving them up and trying to resolve, but that's too much.
                if self._form.backwards_compat or o not in items:
                    items.append(o)
                    break
            else:  # all of them are used
                raise ItemNotFoundError(nn)
        # now we have all the items that should be on
        # let's just turn everything off and then back on.
        self.value = []
        for o in items:
            o.selected = True

    def get_value_by_label(self):
        """Return the value of the control as given by normalized labels."""
        res = []
        compat = self._form.backwards_compat
        for o in self.items:
            if (not o.disabled or compat) and o.selected:
                for l in o.get_labels():
                    if l.text:
                        res.append(l.text)
                        break
                else:
                    res.append(None)
        return res

    def possible_items(self, by_label=False):
        """Deprecated: return the names or labels of all possible items.

        Includes disabled items, which may be misleading for some use cases.

        """
        deprecation(
            "[item.name for item in self.items]")
        if by_label:
            res = []
            for o in self.items:
                for l in o.get_labels():
                    if l.text:
                        res.append(l.text)
                        break
                else:
                    res.append(None)
            return res
        return [o.name for o in self.items]

    def _totally_ordered_pairs(self):
        if self.disabled:
            return []
        else:
            return [(o._index, self.name, o.name) for o in self.items
                    if o.selected and not o.disabled]

    def __str__(self):
        name = self.name
        if name is None: name = "<None>"

        display = [str(o) for o in self.items]

        infos = []
        if self.disabled: infos.append("disabled")
        if self.readonly: infos.append("readonly")
        info = ", ".join(infos)
        if info: info = " (%s)" % info

        return "<%s(%s=[%s])%s>" % (self.__class__.__name__,
                                    name, ", ".join(display), info)


class RadioControl(ListControl):
    """
    Covers:

    INPUT/RADIO

    """
    def __init__(self, type, name, attrs, select_default=False, index=None):
        attrs.setdefault("value", "on")
        ListControl.__init__(self, type, name, attrs, select_default,
                             called_as_base_class=True, index=index)
        self.__dict__["multiple"] = False
        o = Item(self, attrs, index)
        o.__dict__["_selected"] = attrs.has_key("checked")

    def fixup(self):
        ListControl.fixup(self)
        found = [o for o in self.items if o.selected and not o.disabled]
        if not found:
            if self._select_default:
                for o in self.items:
                    if not o.disabled:
                        o.selected = True
                        break
        else:
            # Ensure only one item selected.  Choose the last one,
            # following IE and Firefox.
            for o in found[:-1]:
                o.selected = False

    def get_labels(self):
        return []

class CheckboxControl(ListControl):
    """
    Covers:

    INPUT/CHECKBOX

    """
    def __init__(self, type, name, attrs, select_default=False, index=None):
        attrs.setdefault("value", "on")
        ListControl.__init__(self, type, name, attrs, select_default,
                             called_as_base_class=True, index=index)
        self.__dict__["multiple"] = True
        o = Item(self, attrs, index)
        o.__dict__["_selected"] = attrs.has_key("checked")

    def get_labels(self):
        return []


class SelectControl(ListControl):
    """
    Covers:

    SELECT (and OPTION)


    OPTION 'values', in HTML parlance, are Item 'names' in ClientForm parlance.

    SELECT control values and labels are subject to some messy defaulting
    rules.  For example, if the HTML representation of the control is:

    <SELECT name=year>
      <OPTION value=0 label="2002">current year</OPTION>
      <OPTION value=1>2001</OPTION>
      <OPTION>2000</OPTION>
    </SELECT>

    The items, in order, have labels "2002", "2001" and "2000", whereas their
    names (the OPTION values) are "0", "1" and "2000" respectively.  Note that
    the value of the last OPTION in this example defaults to its contents, as
    specified by RFC 1866, as do the labels of the second and third OPTIONs.

    The OPTION labels are sometimes more meaningful than the OPTION values,
    which can make for more maintainable code.

    Additional read-only public attribute: attrs

    The attrs attribute is a dictionary of the original HTML attributes of the
    SELECT element.  Other ListControls do not have this attribute, because in
    other cases the control as a whole does not correspond to any single HTML
    element.  control.get(...).attrs may be used as usual to get at the HTML
    attributes of the HTML elements corresponding to individual list items (for
    SELECT controls, these are OPTION elements).

    Another special case is that the Item.attrs dictionaries have a special key
    "contents" which does not correspond to any real HTML attribute, but rather
    contains the contents of the OPTION element:

    <OPTION>this bit</OPTION>

    """
    # HTML attributes here are treated slightly differently from other list
    # controls:
    # -The SELECT HTML attributes dictionary is stuffed into the OPTION
    #  HTML attributes dictionary under the "__select" key.
    # -The content of each OPTION element is stored under the special
    #  "contents" key of the dictionary.
    # After all this, the dictionary is passed to the SelectControl constructor
    # as the attrs argument, as usual.  However:
    # -The first SelectControl constructed when building up a SELECT control
    #  has a constructor attrs argument containing only the __select key -- so
    #  this SelectControl represents an empty SELECT control.
    # -Subsequent SelectControls have both OPTION HTML-attribute in attrs and
    #  the __select dictionary containing the SELECT HTML-attributes.

    def __init__(self, type, name, attrs, select_default=False, index=None):
        # fish out the SELECT HTML attributes from the OPTION HTML attributes
        # dictionary
        self.attrs = attrs["__select"].copy()
        self.__dict__["_label"] = _get_label(self.attrs)
        self.__dict__["id"] = self.attrs.get("id")
        self.__dict__["multiple"] = self.attrs.has_key("multiple")
        # the majority of the contents, label, and value dance already happened
        contents = attrs.get("contents")
        attrs = attrs.copy()
        del attrs["__select"]

        ListControl.__init__(self, type, name, self.attrs, select_default,
                             called_as_base_class=True, index=index)
        self.disabled = self.attrs.has_key("disabled")
        self.readonly = self.attrs.has_key("readonly")
        if attrs.has_key("value"):
            # otherwise it is a marker 'select started' token
            o = Item(self, attrs, index)
            o.__dict__["_selected"] = attrs.has_key("selected")
            # add 'label' label and contents label, if different.  If both are
            # provided, the 'label' label is used for display in HTML 
            # 4.0-compliant browsers (and any lower spec? not sure) while the
            # contents are used for display in older or less-compliant
            # browsers.  We make label objects for both, if the values are
            # different.
            label = attrs.get("label")
            if label:
                o._labels.append(Label({"__text": label}))
                if contents and contents != label:
                    o._labels.append(Label({"__text": contents}))
            elif contents:
                o._labels.append(Label({"__text": contents}))

    def fixup(self):
        ListControl.fixup(self)
        # Firefox doesn't exclude disabled items from those considered here
        # (i.e. from 'found', for both branches of the if below).  Note that
        # IE6 doesn't support the disabled attribute on OPTIONs at all.
        found = [o for o in self.items if o.selected]
        if not found:
            if not self.multiple or self._select_default:
                for o in self.items:
                    if not o.disabled:
                        was_disabled = self.disabled
                        self.disabled = False
                        try:
                            o.selected = True
                        finally:
                            o.disabled = was_disabled
                        break
        elif not self.multiple:
            # Ensure only one item selected.  Choose the last one,
            # following IE and Firefox.
            for o in found[:-1]:
                o.selected = False


#---------------------------------------------------
class SubmitControl(ScalarControl):
    """
    Covers:

    INPUT/SUBMIT
    BUTTON/SUBMIT

    """
    def __init__(self, type, name, attrs, index=None):
        ScalarControl.__init__(self, type, name, attrs, index)
        # IE5 defaults SUBMIT value to "Submit Query"; Firebird 0.6 leaves it
        # blank, Konqueror 3.1 defaults to "Submit".  HTML spec. doesn't seem
        # to define this.
        if self.value is None: self.value = ""
        self.readonly = True

    def get_labels(self):
        res = []
        if self.value:
            res.append(Label({"__text": self.value}))
        res.extend(ScalarControl.get_labels(self))
        return res

    def is_of_kind(self, kind): return kind == "clickable"

    def _click(self, form, coord, return_type, request_class=urllib2.Request):
        self._clicked = coord
        r = form._switch_click(return_type, request_class)
        self._clicked = False
        return r

    def _totally_ordered_pairs(self):
        if not self._clicked:
            return []
        return ScalarControl._totally_ordered_pairs(self)


#---------------------------------------------------
class ImageControl(SubmitControl):
    """
    Covers:

    INPUT/IMAGE

    Coordinates are specified using one of the HTMLForm.click* methods.

    """
    def __init__(self, type, name, attrs, index=None):
        SubmitControl.__init__(self, type, name, attrs, index)
        self.readonly = False

    def _totally_ordered_pairs(self):
        clicked = self._clicked
        if self.disabled or not clicked:
            return []
        name = self.name
        if name is None: return []
        pairs = [
            (self._index, "%s.x" % name, str(clicked[0])),
            (self._index+1, "%s.y" % name, str(clicked[1])),
            ]
        value = self._value
        if value:
            pairs.append((self._index+2, name, value))
        return pairs

    get_labels = ScalarControl.get_labels

# aliases, just to make str(control) and str(form) clearer
class PasswordControl(TextControl): pass
class HiddenControl(TextControl): pass
class TextareaControl(TextControl): pass
class SubmitButtonControl(SubmitControl): pass


def is_listcontrol(control): return control.is_of_kind("list")


class HTMLForm:
    """Represents a single HTML <form> ... </form> element.

    A form consists of a sequence of controls that usually have names, and
    which can take on various values.  The values of the various types of
    controls represent variously: text, zero-or-one-of-many or many-of-many
    choices, and files to be uploaded.  Some controls can be clicked on to
    submit the form, and clickable controls' values sometimes include the
    coordinates of the click.

    Forms can be filled in with data to be returned to the server, and then
    submitted, using the click method to generate a request object suitable for
    passing to urllib2.urlopen (or the click_request_data or click_pairs
    methods if you're not using urllib2).

    import ClientForm
    forms = ClientForm.ParseFile(html, base_uri)
    form = forms[0]

    form["query"] = "Python"
    form.find_control("nr_results").get("lots").selected = True

    response = urllib2.urlopen(form.click())

    Usually, HTMLForm instances are not created directly.  Instead, the
    ParseFile or ParseResponse factory functions are used.  If you do construct
    HTMLForm objects yourself, however, note that an HTMLForm instance is only
    properly initialised after the fixup method has been called (ParseFile and
    ParseResponse do this for you).  See ListControl.__doc__ for the reason
    this is required.

    Indexing a form (form["control_name"]) returns the named Control's value
    attribute.  Assignment to a form index (form["control_name"] = something)
    is equivalent to assignment to the named Control's value attribute.  If you
    need to be more specific than just supplying the control's name, use the
    set_value and get_value methods.

    ListControl values are lists of item names (specifically, the names of the
    items that are selected and not disabled, and hence are "successful" -- ie.
    cause data to be returned to the server).  The list item's name is the
    value of the corresponding HTML element's"value" attribute.

    Example:

      <INPUT type="CHECKBOX" name="cheeses" value="leicester"></INPUT>
      <INPUT type="CHECKBOX" name="cheeses" value="cheddar"></INPUT>

    defines a CHECKBOX control with name "cheeses" which has two items, named
    "leicester" and "cheddar".

    Another example:

      <SELECT name="more_cheeses">
        <OPTION>1</OPTION>
        <OPTION value="2" label="CHEDDAR">cheddar</OPTION>
      </SELECT>

    defines a SELECT control with name "more_cheeses" which has two items,
    named "1" and "2" (because the OPTION element's value HTML attribute
    defaults to the element contents -- see SelectControl.__doc__ for more on
    these defaulting rules).

    To select, deselect or otherwise manipulate individual list items, use the
    HTMLForm.find_control() and ListControl.get() methods.  To set the whole
    value, do as for any other control: use indexing or the set_/get_value
    methods.

    Example:

    # select *only* the item named "cheddar"
    form["cheeses"] = ["cheddar"]
    # select "cheddar", leave other items unaffected
    form.find_control("cheeses").get("cheddar").selected = True

    Some controls (RADIO and SELECT without the multiple attribute) can only
    have zero or one items selected at a time.  Some controls (CHECKBOX and
    SELECT with the multiple attribute) can have multiple items selected at a
    time.  To set the whole value of a ListControl, assign a sequence to a form
    index:

    form["cheeses"] = ["cheddar", "leicester"]

    If the ListControl is not multiple-selection, the assigned list must be of
    length one.

    To check if a control has an item, if an item is selected, or if an item is
    successful (selected and not disabled), respectively:

    "cheddar" in [item.name for item in form.find_control("cheeses").items]
    "cheddar" in [item.name for item in form.find_control("cheeses").items and
                  item.selected]
    "cheddar" in form["cheeses"]  # (or "cheddar" in form.get_value("cheeses"))

    Note that some list items may be disabled (see below).

    Note the following mistake:

    form[control_name] = control_value
    assert form[control_name] == control_value  # not necessarily true

    The reason for this is that form[control_name] always gives the list items
    in the order they were listed in the HTML.

    List items (hence list values, too) can be referred to in terms of list
    item labels rather than list item names using the appropriate label
    arguments.  Note that each item may have several labels.

    The question of default values of OPTION contents, labels and values is
    somewhat complicated: see SelectControl.__doc__ and
    ListControl.get_item_attrs.__doc__ if you think you need to know.

    Controls can be disabled or readonly.  In either case, the control's value
    cannot be changed until you clear those flags (see example below).
    Disabled is the state typically represented by browsers by 'greying out' a
    control.  Disabled controls are not 'successful' -- they don't cause data
    to get returned to the server.  Readonly controls usually appear in
    browsers as read-only text boxes.  Readonly controls are successful.  List
    items can also be disabled.  Attempts to select or deselect disabled items
    fail with AttributeError.

    If a lot of controls are readonly, it can be useful to do this:

    form.set_all_readonly(False)

    To clear a control's value attribute, so that it is not successful (until a
    value is subsequently set):

    form.clear("cheeses")

    More examples:

    control = form.find_control("cheeses")
    control.disabled = False
    control.readonly = False
    control.get("gruyere").disabled = True
    control.items[0].selected = True

    See the various Control classes for further documentation.  Many methods
    take name, type, kind, id, label and nr arguments to specify the control to
    be operated on: see HTMLForm.find_control.__doc__.

    ControlNotFoundError (subclass of ValueError) is raised if the specified
    control can't be found.  This includes occasions where a non-ListControl
    is found, but the method (set, for example) requires a ListControl.
    ItemNotFoundError (subclass of ValueError) is raised if a list item can't
    be found.  ItemCountError (subclass of ValueError) is raised if an attempt
    is made to select more than one item and the control doesn't allow that, or
    set/get_single are called and the control contains more than one item.
    AttributeError is raised if a control or item is readonly or disabled and
    an attempt is made to alter its value.

    Security note: Remember that any passwords you store in HTMLForm instances
    will be saved to disk in the clear if you pickle them (directly or
    indirectly).  The simplest solution to this is to avoid pickling HTMLForm
    objects.  You could also pickle before filling in any password, or just set
    the password to "" before pickling.


    Public attributes:

    action: full (absolute URI) form action
    method: "GET" or "POST"
    enctype: form transfer encoding MIME type
    name: name of form (None if no name was specified)
    attrs: dictionary mapping original HTML form attributes to their values

    controls: list of Control instances; do not alter this list
     (instead, call form.new_control to make a Control and add it to the
     form, or control.add_to_form if you already have a Control instance)



    Methods for form filling:
    -------------------------

    Most of the these methods have very similar arguments.  See
    HTMLForm.find_control.__doc__ for details of the name, type, kind, label
    and nr arguments.

    def find_control(self,
                     name=None, type=None, kind=None, id=None, predicate=None,
                     nr=None, label=None)

    get_value(name=None, type=None, kind=None, id=None, nr=None,
              by_label=False,  # by_label is deprecated
              label=None)
    set_value(value,
              name=None, type=None, kind=None, id=None, nr=None,
              by_label=False,  # by_label is deprecated
              label=None)

    clear_all()
    clear(name=None, type=None, kind=None, id=None, nr=None, label=None)

    set_all_readonly(readonly)


    Method applying only to FileControls:

    add_file(file_object,
             content_type="application/octet-stream", filename=None,
             name=None, id=None, nr=None, label=None)


    Methods applying only to clickable controls:

    click(name=None, type=None, id=None, nr=0, coord=(1,1), label=None)
    click_request_data(name=None, type=None, id=None, nr=0, coord=(1,1),
                       label=None)
    click_pairs(name=None, type=None, id=None, nr=0, coord=(1,1), label=None)

    """

    type2class = {
        "text": TextControl,
        "password": PasswordControl,
        "hidden": HiddenControl,
        "textarea": TextareaControl,

        "isindex": IsindexControl,

        "file": FileControl,

        "button": IgnoreControl,
        "buttonbutton": IgnoreControl,
        "reset": IgnoreControl,
        "resetbutton": IgnoreControl,

        "submit": SubmitControl,
        "submitbutton": SubmitButtonControl,
        "image": ImageControl,

        "radio": RadioControl,
        "checkbox": CheckboxControl,
        "select": SelectControl,
        }

#---------------------------------------------------
# Initialisation.  Use ParseResponse / ParseFile instead.

    def __init__(self, action, method="GET",
                 enctype="application/x-www-form-urlencoded",
                 name=None, attrs=None,
                 request_class=urllib2.Request,
                 forms=None, labels=None, id_to_labels=None,
                 backwards_compat=True):
        """
        In the usual case, use ParseResponse (or ParseFile) to create new
        HTMLForm objects.

        action: full (absolute URI) form action
        method: "GET" or "POST"
        enctype: form transfer encoding MIME type
        name: name of form
        attrs: dictionary mapping original HTML form attributes to their values

        """
        self.action = action
        self.method = method
        self.enctype = enctype
        self.name = name
        if attrs is not None:
            self.attrs = attrs.copy()
        else:
            self.attrs = {}
        self.controls = []
        self._request_class = request_class

        # these attributes are used by zope.testbrowser
        self._forms = forms  # this is a semi-public API!
        self._labels = labels  # this is a semi-public API!
        self._id_to_labels = id_to_labels  # this is a semi-public API!

        self.backwards_compat = backwards_compat  # note __setattr__

    def __getattr__(self, name):
        if name == "backwards_compat":
            return self._backwards_compat
        return getattr(HTMLForm, name)

    def __setattr__(self, name, value):
        # yuck
        if name == "backwards_compat":
            name = "_backwards_compat"
            value = bool(value)
            for cc in self.controls:
                try:
                    items = cc.items 
                except AttributeError:
                    continue
                else:
                    for ii in items:
                        for ll in ii.get_labels():
                            ll._backwards_compat = value
        self.__dict__[name] = value

    def new_control(self, type, name, attrs,
                    ignore_unknown=False, select_default=False, index=None):
        """Adds a new control to the form.

        This is usually called by ParseFile and ParseResponse.  Don't call it
        youself unless you're building your own Control instances.

        Note that controls representing lists of items are built up from
        controls holding only a single list item.  See ListControl.__doc__ for
        further information.

        type: type of control (see Control.__doc__ for a list)
        attrs: HTML attributes of control
        ignore_unknown: if true, use a dummy Control instance for controls of
         unknown type; otherwise, use a TextControl
        select_default: for RADIO and multiple-selection SELECT controls, pick
         the first item as the default if no 'selected' HTML attribute is
         present (this defaulting happens when the HTMLForm.fixup method is
         called)
        index: index of corresponding element in HTML (see
         MoreFormTests.test_interspersed_controls for motivation)

        """
        type = type.lower()
        klass = self.type2class.get(type)
        if klass is None:
            if ignore_unknown:
                klass = IgnoreControl
            else:
                klass = TextControl

        a = attrs.copy()
        if issubclass(klass, ListControl):
            control = klass(type, name, a, select_default, index)
        else:
            control = klass(type, name, a, index)
        control.add_to_form(self)

    def fixup(self):
        """Normalise form after all controls have been added.

        This is usually called by ParseFile and ParseResponse.  Don't call it
        youself unless you're building your own Control instances.

        This method should only be called once, after all controls have been
        added to the form.

        """
        for control in self.controls:
            control.fixup()
        self.backwards_compat = self._backwards_compat

#---------------------------------------------------
    def __str__(self):
        header = "%s%s %s %s" % (
            (self.name and self.name+" " or ""),
            self.method, self.action, self.enctype)
        rep = [header]
        for control in self.controls:
            rep.append("  %s" % str(control))
        return "<%s>" % "\n".join(rep)

#---------------------------------------------------
# Form-filling methods.

    def __getitem__(self, name):
        return self.find_control(name).value
    def __contains__(self, name):
        return bool(self.find_control(name))
    def __setitem__(self, name, value):
        control = self.find_control(name)
        try:
            control.value = value
        except AttributeError, e:
            raise ValueError(str(e))

    def get_value(self,
                  name=None, type=None, kind=None, id=None, nr=None,
                  by_label=False,  # by_label is deprecated
                  label=None):
        """Return value of control.

        If only name and value arguments are supplied, equivalent to

        form[name]

        """
        if by_label:
            deprecation("form.get_value_by_label(...)")
        c = self.find_control(name, type, kind, id, label=label, nr=nr)
        if by_label:
            try:
                meth = c.get_value_by_label
            except AttributeError:
                raise NotImplementedError(
                    "control '%s' does not yet support by_label" % c.name)
            else:
                return meth()
        else:
            return c.value
    def set_value(self, value,
                  name=None, type=None, kind=None, id=None, nr=None,
                  by_label=False,  # by_label is deprecated
                  label=None):
        """Set value of control.

        If only name and value arguments are supplied, equivalent to

        form[name] = value

        """
        if by_label:
            deprecation("form.get_value_by_label(...)")
        c = self.find_control(name, type, kind, id, label=label, nr=nr)
        if by_label:
            try:
                meth = c.set_value_by_label
            except AttributeError:
                raise NotImplementedError(
                    "control '%s' does not yet support by_label" % c.name)
            else:
                meth(value)
        else:
            c.value = value
    def get_value_by_label(
        self, name=None, type=None, kind=None, id=None, label=None, nr=None):
        """

        All arguments should be passed by name.

        """
        c = self.find_control(name, type, kind, id, label=label, nr=nr)
        return c.get_value_by_label()

    def set_value_by_label(
        self, value,
        name=None, type=None, kind=None, id=None, label=None, nr=None):
        """

        All arguments should be passed by name.

        """
        c = self.find_control(name, type, kind, id, label=label, nr=nr)
        c.set_value_by_label(value)

    def set_all_readonly(self, readonly):
        for control in self.controls:
            control.readonly = bool(readonly)

    def clear_all(self):
        """Clear the value attributes of all controls in the form.

        See HTMLForm.clear.__doc__.

        """
        for control in self.controls:
            control.clear()

    def clear(self,
              name=None, type=None, kind=None, id=None, nr=None, label=None):
        """Clear the value attribute of a control.

        As a result, the affected control will not be successful until a value
        is subsequently set.  AttributeError is raised on readonly controls.

        """
        c = self.find_control(name, type, kind, id, label=label, nr=nr)
        c.clear()


#---------------------------------------------------
# Form-filling methods applying only to ListControls.

    def possible_items(self,  # deprecated
                       name=None, type=None, kind=None, id=None,
                       nr=None, by_label=False, label=None):
        """Return a list of all values that the specified control can take."""
        c = self._find_list_control(name, type, kind, id, label, nr)
        return c.possible_items(by_label)

    def set(self, selected, item_name,  # deprecated
            name=None, type=None, kind=None, id=None, nr=None,
            by_label=False, label=None):
        """Select / deselect named list item.

        selected: boolean selected state

        """
        self._find_list_control(name, type, kind, id, label, nr).set(
            selected, item_name, by_label)
    def toggle(self, item_name,  # deprecated
               name=None, type=None, kind=None, id=None, nr=None,
               by_label=False, label=None):
        """Toggle selected state of named list item."""
        self._find_list_control(name, type, kind, id, label, nr).toggle(
            item_name, by_label)

    def set_single(self, selected,  # deprecated
                   name=None, type=None, kind=None, id=None,
                   nr=None, by_label=None, label=None):
        """Select / deselect list item in a control having only one item.

        If the control has multiple list items, ItemCountError is raised.

        This is just a convenience method, so you don't need to know the item's
        name -- the item name in these single-item controls is usually
        something meaningless like "1" or "on".

        For example, if a checkbox has a single item named "on", the following
        two calls are equivalent:

        control.toggle("on")
        control.toggle_single()

        """  # by_label ignored and deprecated
        self._find_list_control(
            name, type, kind, id, label, nr).set_single(selected)
    def toggle_single(self, name=None, type=None, kind=None, id=None,
                      nr=None, by_label=None, label=None):  # deprecated
        """Toggle selected state of list item in control having only one item.

        The rest is as for HTMLForm.set_single.__doc__.

        """  # by_label ignored and deprecated
        self._find_list_control(name, type, kind, id, label, nr).toggle_single()

#---------------------------------------------------
# Form-filling method applying only to FileControls.

    def add_file(self, file_object, content_type=None, filename=None,
                 name=None, id=None, nr=None, label=None):
        """Add a file to be uploaded.

        file_object: file-like object (with read method) from which to read
         data to upload
        content_type: MIME content type of data to upload
        filename: filename to pass to server

        If filename is None, no filename is sent to the server.

        If content_type is None, the content type is guessed based on the
        filename and the data from read from the file object.

        XXX
        At the moment, guessed content type is always application/octet-stream.
        Use sndhdr, imghdr modules.  Should also try to guess HTML, XML, and
        plain text.

        Note the following useful HTML attributes of file upload controls (see
        HTML 4.01 spec, section 17):

        accept: comma-separated list of content types that the server will
         handle correctly; you can use this to filter out non-conforming files
        size: XXX IIRC, this is indicative of whether form wants multiple or
         single files
        maxlength: XXX hint of max content length in bytes?

        """
        self.find_control(name, "file", id=id, label=label, nr=nr).add_file(
            file_object, content_type, filename)

#---------------------------------------------------
# Form submission methods, applying only to clickable controls.

    def click(self, name=None, type=None, id=None, nr=0, coord=(1,1),
              request_class=urllib2.Request,
              label=None):
        """Return request that would result from clicking on a control.

        The request object is a urllib2.Request instance, which you can pass to
        urllib2.urlopen (or ClientCookie.urlopen).

        Only some control types (INPUT/SUBMIT & BUTTON/SUBMIT buttons and
        IMAGEs) can be clicked.

        Will click on the first clickable control, subject to the name, type
        and nr arguments (as for find_control).  If no name, type, id or number
        is specified and there are no clickable controls, a request will be
        returned for the form in its current, un-clicked, state.

        IndexError is raised if any of name, type, id or nr is specified but no
        matching control is found.  ValueError is raised if the HTMLForm has an
        enctype attribute that is not recognised.

        You can optionally specify a coordinate to click at, which only makes a
        difference if you clicked on an image.

        """
        return self._click(name, type, id, label, nr, coord, "request",
                           self._request_class)

    def click_request_data(self,
                           name=None, type=None, id=None,
                           nr=0, coord=(1,1),
                           request_class=urllib2.Request,
                           label=None):
        """As for click method, but return a tuple (url, data, headers).

        You can use this data to send a request to the server.  This is useful
        if you're using httplib or urllib rather than urllib2.  Otherwise, use
        the click method.

        # Untested.  Have to subclass to add headers, I think -- so use urllib2
        # instead!
        import urllib
        url, data, hdrs = form.click_request_data()
        r = urllib.urlopen(url, data)

        # Untested.  I don't know of any reason to use httplib -- you can get
        # just as much control with urllib2.
        import httplib, urlparse
        url, data, hdrs = form.click_request_data()
        tup = urlparse(url)
        host, path = tup[1], urlparse.urlunparse((None, None)+tup[2:])
        conn = httplib.HTTPConnection(host)
        if data:
            httplib.request("POST", path, data, hdrs)
        else:
            httplib.request("GET", path, headers=hdrs)
        r = conn.getresponse()

        """
        return self._click(name, type, id, label, nr, coord, "request_data",
                           self._request_class)

    def click_pairs(self, name=None, type=None, id=None,
                    nr=0, coord=(1,1),
                    label=None):
        """As for click_request_data, but returns a list of (key, value) pairs.

        You can use this list as an argument to ClientForm.urlencode.  This is
        usually only useful if you're using httplib or urllib rather than
        urllib2 or ClientCookie.  It may also be useful if you want to manually
        tweak the keys and/or values, but this should not be necessary.
        Otherwise, use the click method.

        Note that this method is only useful for forms of MIME type
        x-www-form-urlencoded.  In particular, it does not return the
        information required for file upload.  If you need file upload and are
        not using urllib2, use click_request_data.

        Also note that Python 2.0's urllib.urlencode is slightly broken: it
        only accepts a mapping, not a sequence of pairs, as an argument.  This
        messes up any ordering in the argument.  Use ClientForm.urlencode
        instead.

        """
        return self._click(name, type, id, label, nr, coord, "pairs",
                           self._request_class)

#---------------------------------------------------

    def find_control(self,
                     name=None, type=None, kind=None, id=None,
                     predicate=None, nr=None,
                     label=None):
        """Locate and return some specific control within the form.

        At least one of the name, type, kind, predicate and nr arguments must
        be supplied.  If no matching control is found, ControlNotFoundError is
        raised.

        If name is specified, then the control must have the indicated name.

        If type is specified then the control must have the specified type (in
        addition to the types possible for <input> HTML tags: "text",
        "password", "hidden", "submit", "image", "button", "radio", "checkbox",
        "file" we also have "reset", "buttonbutton", "submitbutton",
        "resetbutton", "textarea", "select" and "isindex").

        If kind is specified, then the control must fall into the specified
        group, each of which satisfies a particular interface.  The types are
        "text", "list", "multilist", "singlelist", "clickable" and "file".

        If id is specified, then the control must have the indicated id.

        If predicate is specified, then the control must match that function.
        The predicate function is passed the control as its single argument,
        and should return a boolean value indicating whether the control
        matched.

        nr, if supplied, is the sequence number of the control (where 0 is the
        first).  Note that control 0 is the first control matching all the
        other arguments (if supplied); it is not necessarily the first control
        in the form.  If no nr is supplied, AmbiguityError is raised if
        multiple controls match the other arguments (unless the
        .backwards-compat attribute is true).

        If label is specified, then the control must have this label.  Note
        that radio controls and checkboxes never have labels: their items do.

        """
        if ((name is None) and (type is None) and (kind is None) and
            (id is None) and (label is None) and (predicate is None) and
            (nr is None)):
            raise ValueError(
                "at least one argument must be supplied to specify control")
        return self._find_control(name, type, kind, id, label, predicate, nr)

#---------------------------------------------------
# Private methods.

    def _find_list_control(self,
                           name=None, type=None, kind=None, id=None, 
                           label=None, nr=None):
        if ((name is None) and (type is None) and (kind is None) and
            (id is None) and (label is None) and (nr is None)):
            raise ValueError(
                "at least one argument must be supplied to specify control")

        return self._find_control(name, type, kind, id, label, 
                                  is_listcontrol, nr)

    def _find_control(self, name, type, kind, id, label, predicate, nr):
        if (name is not None) and not isstringlike(name):
            raise TypeError("control name must be string-like")
        if (type is not None) and not isstringlike(type):
            raise TypeError("control type must be string-like")
        if (kind is not None) and not isstringlike(kind):
            raise TypeError("control kind must be string-like")
        if (id is not None) and not isstringlike(id):
            raise TypeError("control id must be string-like")
        if (label is not None) and not isstringlike(label):
            raise TypeError("control label must be string-like")
        if (predicate is not None) and not callable(predicate):
            raise TypeError("control predicate must be callable")
        if (nr is not None) and nr < 0:
            raise ValueError("control number must be a positive integer")

        orig_nr = nr
        found = None
        ambiguous = False
        if nr is None and self.backwards_compat:
            nr = 0

        for control in self.controls:
            if name is not None and name != control.name:
                continue
            if type is not None and type != control.type:
                continue
            if kind is not None and not control.is_of_kind(kind):
                continue
            if id is not None and id != control.id:
                continue
            if predicate and not predicate(control):
                continue
            if label:
                for l in control.get_labels():
                    if l.text.find(label) > -1:
                        break
                else:
                    continue
            if nr is not None:
                if nr == 0:
                    return control  # early exit: unambiguous due to nr
                nr -= 1
                continue
            if found:
                ambiguous = True
                break
            found = control

        if found and not ambiguous:
            return found

        description = []
        if name is not None: description.append("name '%s'" % name)
        if type is not None: description.append("type '%s'" % type)
        if kind is not None: description.append("kind '%s'" % kind)
        if id is not None: description.append("id '%s'" % id)
        if label is not None: description.append("label '%s'" % label)
        if predicate is not None:
            description.append("predicate %s" % predicate)
        if orig_nr: description.append("nr %d" % orig_nr)
        description = ", ".join(description)

        if ambiguous:
            raise AmbiguityError("more than one control matching "+description)
        elif not found:
            raise ControlNotFoundError("no control matching "+description)
        assert False

    def _click(self, name, type, id, label, nr, coord, return_type,
               request_class=urllib2.Request):
        try:
            control = self._find_control(
                name, type, "clickable", id, label, None, nr)
        except ControlNotFoundError:
            if ((name is not None) or (type is not None) or (id is not None) or
                (nr != 0)):
                raise
            # no clickable controls, but no control was explicitly requested,
            # so return state without clicking any control
            return self._switch_click(return_type, request_class)
        else:
            return control._click(self, coord, return_type, request_class)

    def _pairs(self):
        """Return sequence of (key, value) pairs suitable for urlencoding."""
        return [(k, v) for (i, k, v, c_i) in self._pairs_and_controls()]


    def _pairs_and_controls(self):
        """Return sequence of (index, key, value, control_index)
        of totally ordered pairs suitable for urlencoding.

        control_index is the index of the control in self.controls
        """
        pairs = []
        for control_index in range(len(self.controls)):
            control = self.controls[control_index]
            for ii, key, val in control._totally_ordered_pairs():
                pairs.append((ii, key, val, control_index))

        # stable sort by ONLY first item in tuple
        pairs.sort()

        return pairs

    def _request_data(self):
        """Return a tuple (url, data, headers)."""
        method = self.method.upper()
        #scheme, netloc, path, parameters, query, frag = urlparse.urlparse(self.action)
        parts = urlparse.urlparse(self.action)
        rest, (query, frag) = parts[:-2], parts[-2:]

        if method == "GET":
            if self.enctype != "application/x-www-form-urlencoded":
                raise ValueError(
                    "unknown GET form encoding type '%s'" % self.enctype)
            parts = rest + (urlencode(self._pairs()), "")
            uri = urlparse.urlunparse(parts)
            return uri, None, []
        elif method == "POST":
            parts = rest + (query, "")
            uri = urlparse.urlunparse(parts)
            if self.enctype == "application/x-www-form-urlencoded":
                return (uri, urlencode(self._pairs()),
                        [("Content-type", self.enctype)])
            elif self.enctype == "multipart/form-data":
                data = StringIO()
                http_hdrs = []
                mw = MimeWriter(data, http_hdrs)
                f = mw.startmultipartbody("form-data", add_to_http_hdrs=True,
                                          prefix=0)
                for ii, k, v, control_index in self._pairs_and_controls():
                    self.controls[control_index]._write_mime_data(mw, k, v)
                mw.lastpart()
                return uri, data.getvalue(), http_hdrs
            else:
                raise ValueError(
                    "unknown POST form encoding type '%s'" % self.enctype)
        else:
            raise ValueError("Unknown method '%s'" % method)

    def _switch_click(self, return_type, request_class=urllib2.Request):
        # This is called by HTMLForm and clickable Controls to hide switching
        # on return_type.
        if return_type == "pairs":
            return self._pairs()
        elif return_type == "request_data":
            return self._request_data()
        else:
            req_data = self._request_data()
            req = request_class(req_data[0], req_data[1])
            for key, val in req_data[2]:
                add_hdr = req.add_header
                if key.lower() == "content-type":
                    try:
                        add_hdr = req.add_unredirected_header
                    except AttributeError:
                        # pre-2.4 and not using ClientCookie
                        pass
                add_hdr(key, val)
            return req

########NEW FILE########
__FILENAME__ = ClientForm
"""HTML form handling for web clients.

ClientForm is a Python module for handling HTML forms on the client
side, useful for parsing HTML forms, filling them in and returning the
completed forms to the server.  It has developed from a port of Gisle
Aas' Perl module HTML::Form, from the libwww-perl library, but the
interface is not the same.

The most useful docstring is the one for HTMLForm.

RFC 1866: HTML 2.0
RFC 1867: Form-based File Upload in HTML
RFC 2388: Returning Values from Forms: multipart/form-data
HTML 3.2 Specification, W3C Recommendation 14 January 1997 (for ISINDEX)
HTML 4.01 Specification, W3C Recommendation 24 December 1999


Copyright 2002-2007 John J. Lee <jjl@pobox.com>
Copyright 2005 Gary Poster
Copyright 2005 Zope Corporation
Copyright 1998-2000 Gisle Aas.

This code is free software; you can redistribute it and/or modify it
under the terms of the BSD or ZPL 2.1 licenses (see the file
COPYING.txt included with the distribution).

"""

# XXX
# Remove parser testing hack
# safeUrl()-ize action
# Switch to unicode throughout (would be 0.3.x)
#  See Wichert Akkerman's 2004-01-22 message to c.l.py.
# Add charset parameter to Content-type headers?  How to find value??
# Add some more functional tests
#  Especially single and multiple file upload on the internet.
#  Does file upload work when name is missing?  Sourceforge tracker form
#   doesn't like it.  Check standards, and test with Apache.  Test
#   binary upload with Apache.
# mailto submission & enctype text/plain
# I'm not going to fix this unless somebody tells me what real servers
#  that want this encoding actually expect: If enctype is
#  application/x-www-form-urlencoded and there's a FILE control present.
#  Strictly, it should be 'name=data' (see HTML 4.01 spec., section
#  17.13.2), but I send "name=" ATM.  What about multiple file upload??

# Would be nice, but I'm not going to do it myself:
# -------------------------------------------------
# Maybe a 0.4.x?
#   Replace by_label etc. with moniker / selector concept. Allows, eg.,
#    a choice between selection by value / id / label / element
#    contents.  Or choice between matching labels exactly or by
#    substring.  Etc.
#   Remove deprecated methods.
#   ...what else?
# Work on DOMForm.
# XForms?  Don't know if there's a need here.

__all__ = ['AmbiguityError', 'CheckboxControl', 'Control',
           'ControlNotFoundError', 'FileControl', 'FormParser', 'HTMLForm',
           'HiddenControl', 'IgnoreControl', 'ImageControl', 'IsindexControl',
           'Item', 'ItemCountError', 'ItemNotFoundError', 'Label',
           'ListControl', 'LocateError', 'Missing', 'ParseError', 'ParseFile',
           'ParseFileEx', 'ParseResponse', 'ParseResponseEx','PasswordControl',
           'RadioControl', 'ScalarControl', 'SelectControl',
           'SubmitButtonControl', 'SubmitControl', 'TextControl',
           'TextareaControl', 'XHTMLCompatibleFormParser']

try: True
except NameError:
    True = 1
    False = 0

try: bool
except NameError:
    def bool(expr):
        if expr: return True
        else: return False

try:
    import logging
    import inspect
except ImportError:
    def debug(msg, *args, **kwds):
        pass
else:
    _logger = logging.getLogger("ClientForm")
    OPTIMIZATION_HACK = True

    def debug(msg, *args, **kwds):
        if OPTIMIZATION_HACK:
            return

        caller_name = inspect.stack()[1][3]
        extended_msg = '%%s %s' % msg
        extended_args = (caller_name,)+args
        debug = _logger.debug(extended_msg, *extended_args, **kwds)

    def _show_debug_messages():
        global OPTIMIZATION_HACK
        OPTIMIZATION_HACK = False
        _logger.setLevel(logging.DEBUG)
        handler = logging.StreamHandler(sys.stdout)
        handler.setLevel(logging.DEBUG)
        _logger.addHandler(handler)

import sys, urllib, urllib2, types, mimetools, copy, urlparse, \
       htmlentitydefs, re, random
from cStringIO import StringIO

import sgmllib
# monkeypatch to fix http://www.python.org/sf/803422 :-(
sgmllib.charref = re.compile("&#(x?[0-9a-fA-F]+)[^0-9a-fA-F]")

# HTMLParser.HTMLParser is recent, so live without it if it's not available
# (also, sgmllib.SGMLParser is much more tolerant of bad HTML)
try:
    import HTMLParser
except ImportError:
    HAVE_MODULE_HTMLPARSER = False
else:
    HAVE_MODULE_HTMLPARSER = True

try:
    import warnings
except ImportError:
    def deprecation(message, stack_offset=0):
        pass
else:
    def deprecation(message, stack_offset=0):
        warnings.warn(message, DeprecationWarning, stacklevel=3+stack_offset)

VERSION = "0.2.9"

CHUNK = 1024  # size of chunks fed to parser, in bytes

DEFAULT_ENCODING = "latin-1"

class Missing: pass

_compress_re = re.compile(r"\s+")
def compress_text(text): return _compress_re.sub(" ", text.strip())

def normalize_line_endings(text):
    return re.sub(r"(?:(?<!\r)\n)|(?:\r(?!\n))", "\r\n", text)


# This version of urlencode is from my Python 1.5.2 back-port of the
# Python 2.1 CVS maintenance branch of urllib.  It will accept a sequence
# of pairs instead of a mapping -- the 2.0 version only accepts a mapping.
def urlencode(query,doseq=False,):
    """Encode a sequence of two-element tuples or dictionary into a URL query \
string.

    If any values in the query arg are sequences and doseq is true, each
    sequence element is converted to a separate parameter.

    If the query arg is a sequence of two-element tuples, the order of the
    parameters in the output will match the order of parameters in the
    input.
    """

    if hasattr(query,"items"):
        # mapping objects
        query = query.items()
    else:
        # it's a bother at times that strings and string-like objects are
        # sequences...
        try:
            # non-sequence items should not work with len()
            x = len(query)
            # non-empty strings will fail this
            if len(query) and type(query[0]) != types.TupleType:
                raise TypeError()
            # zero-length sequences of all types will get here and succeed,
            # but that's a minor nit - since the original implementation
            # allowed empty dicts that type of behavior probably should be
            # preserved for consistency
        except TypeError:
            ty,va,tb = sys.exc_info()
            raise TypeError("not a valid non-string sequence or mapping "
                            "object", tb)

    l = []
    if not doseq:
        # preserve old behavior
        for k, v in query:
            k = urllib.quote_plus(str(k))
            v = urllib.quote_plus(str(v))
            l.append(k + '=' + v)
    else:
        for k, v in query:
            k = urllib.quote_plus(str(k))
            if type(v) == types.StringType:
                v = urllib.quote_plus(v)
                l.append(k + '=' + v)
            elif type(v) == types.UnicodeType:
                # is there a reasonable way to convert to ASCII?
                # encode generates a string, but "replace" or "ignore"
                # lose information and "strict" can raise UnicodeError
                v = urllib.quote_plus(v.encode("ASCII","replace"))
                l.append(k + '=' + v)
            else:
                try:
                    # is this a sufficient test for sequence-ness?
                    x = len(v)
                except TypeError:
                    # not a sequence
                    v = urllib.quote_plus(str(v))
                    l.append(k + '=' + v)
                else:
                    # loop over the sequence
                    for elt in v:
                        l.append(k + '=' + urllib.quote_plus(str(elt)))
    return '&'.join(l)

def unescape(data, entities, encoding=DEFAULT_ENCODING):
    if data is None or "&" not in data:
        return data

    def replace_entities(match, entities=entities, encoding=encoding):
        ent = match.group()
        if ent[1] == "#":
            return unescape_charref(ent[2:-1], encoding)

        repl = entities.get(ent)
        if repl is not None:
            if type(repl) != type(""):
                try:
                    repl = repl.encode(encoding)
                except UnicodeError:
                    repl = ent
        else:
            repl = ent

        return repl

    return re.sub(r"&#?[A-Za-z0-9]+?;", replace_entities, data)

def unescape_charref(data, encoding):
    name, base = data, 10
    if name.startswith("x"):
        name, base= name[1:], 16
    uc = unichr(int(name, base))
    if encoding is None:
        return uc
    else:
        try:
            repl = uc.encode(encoding)
        except UnicodeError:
            repl = "&#%s;" % data
        return repl

def get_entitydefs():
    import htmlentitydefs
    from codecs import latin_1_decode
    entitydefs = {}
    try:
        htmlentitydefs.name2codepoint
    except AttributeError:
        entitydefs = {}
        for name, char in htmlentitydefs.entitydefs.items():
            uc = latin_1_decode(char)[0]
            if uc.startswith("&#") and uc.endswith(";"):
                uc = unescape_charref(uc[2:-1], None)
            entitydefs["&%s;" % name] = uc
    else:
        for name, codepoint in htmlentitydefs.name2codepoint.items():
            entitydefs["&%s;" % name] = unichr(codepoint)
    return entitydefs


def issequence(x):
    try:
        x[0]
    except (TypeError, KeyError):
        return False
    except IndexError:
        pass
    return True

def isstringlike(x):
    try: x+""
    except: return False
    else: return True


def choose_boundary():
    """Return a string usable as a multipart boundary."""
    # follow IE and firefox
    nonce = "".join([str(random.randint(0, sys.maxint-1)) for i in 0,1,2])
    return "-"*27 + nonce

# This cut-n-pasted MimeWriter from standard library is here so can add
# to HTTP headers rather than message body when appropriate.  It also uses
# \r\n in place of \n.  This is a bit nasty.
class MimeWriter:

    """Generic MIME writer.

    Methods:

    __init__()
    addheader()
    flushheaders()
    startbody()
    startmultipartbody()
    nextpart()
    lastpart()

    A MIME writer is much more primitive than a MIME parser.  It
    doesn't seek around on the output file, and it doesn't use large
    amounts of buffer space, so you have to write the parts in the
    order they should occur on the output file.  It does buffer the
    headers you add, allowing you to rearrange their order.

    General usage is:

    f = <open the output file>
    w = MimeWriter(f)
    ...call w.addheader(key, value) 0 or more times...

    followed by either:

    f = w.startbody(content_type)
    ...call f.write(data) for body data...

    or:

    w.startmultipartbody(subtype)
    for each part:
        subwriter = w.nextpart()
        ...use the subwriter's methods to create the subpart...
    w.lastpart()

    The subwriter is another MimeWriter instance, and should be
    treated in the same way as the toplevel MimeWriter.  This way,
    writing recursive body parts is easy.

    Warning: don't forget to call lastpart()!

    XXX There should be more state so calls made in the wrong order
    are detected.

    Some special cases:

    - startbody() just returns the file passed to the constructor;
      but don't use this knowledge, as it may be changed.

    - startmultipartbody() actually returns a file as well;
      this can be used to write the initial 'if you can read this your
      mailer is not MIME-aware' message.

    - If you call flushheaders(), the headers accumulated so far are
      written out (and forgotten); this is useful if you don't need a
      body part at all, e.g. for a subpart of type message/rfc822
      that's (mis)used to store some header-like information.

    - Passing a keyword argument 'prefix=<flag>' to addheader(),
      start*body() affects where the header is inserted; 0 means
      append at the end, 1 means insert at the start; default is
      append for addheader(), but insert for start*body(), which use
      it to determine where the Content-type header goes.

    """

    def __init__(self, fp, http_hdrs=None):
        self._http_hdrs = http_hdrs
        self._fp = fp
        self._headers = []
        self._boundary = []
        self._first_part = True

    def addheader(self, key, value, prefix=0,
                  add_to_http_hdrs=0):
        """
        prefix is ignored if add_to_http_hdrs is true.
        """
        lines = value.split("\r\n")
        while lines and not lines[-1]: del lines[-1]
        while lines and not lines[0]: del lines[0]
        if add_to_http_hdrs:
            value = "".join(lines)
            self._http_hdrs.append((key, value))
        else:
            for i in range(1, len(lines)):
                lines[i] = "    " + lines[i].strip()
            value = "\r\n".join(lines) + "\r\n"
            line = key + ": " + value
            if prefix:
                self._headers.insert(0, line)
            else:
                self._headers.append(line)

    def flushheaders(self):
        self._fp.writelines(self._headers)
        self._headers = []

    def startbody(self, ctype=None, plist=[], prefix=1,
                  add_to_http_hdrs=0, content_type=1):
        """
        prefix is ignored if add_to_http_hdrs is true.
        """
        if content_type and ctype:
            for name, value in plist:
                ctype = ctype + ';\r\n %s=%s' % (name, value)
            self.addheader("Content-type", ctype, prefix=prefix,
                           add_to_http_hdrs=add_to_http_hdrs)
        self.flushheaders()
        if not add_to_http_hdrs: self._fp.write("\r\n")
        self._first_part = True
        return self._fp

    def startmultipartbody(self, subtype, boundary=None, plist=[], prefix=1,
                           add_to_http_hdrs=0, content_type=1):
        boundary = boundary or choose_boundary()
        self._boundary.append(boundary)
        return self.startbody("multipart/" + subtype,
                              [("boundary", boundary)] + plist,
                              prefix=prefix,
                              add_to_http_hdrs=add_to_http_hdrs,
                              content_type=content_type)

    def nextpart(self):
        boundary = self._boundary[-1]
        if self._first_part:
            self._first_part = False
        else:
            self._fp.write("\r\n")
        self._fp.write("--" + boundary + "\r\n")
        return self.__class__(self._fp)

    def lastpart(self):
        if self._first_part:
            self.nextpart()
        boundary = self._boundary.pop()
        self._fp.write("\r\n--" + boundary + "--\r\n")


class LocateError(ValueError): pass
class AmbiguityError(LocateError): pass
class ControlNotFoundError(LocateError): pass
class ItemNotFoundError(LocateError): pass

class ItemCountError(ValueError): pass

# for backwards compatibility, ParseError derives from exceptions that were
# raised by versions of ClientForm <= 0.2.5
if HAVE_MODULE_HTMLPARSER:
    SGMLLIB_PARSEERROR = sgmllib.SGMLParseError
    class ParseError(sgmllib.SGMLParseError,
                     HTMLParser.HTMLParseError,
                     ):
        pass
else:
    if hasattr(sgmllib, "SGMLParseError"):
        SGMLLIB_PARSEERROR = sgmllib.SGMLParseError
        class ParseError(sgmllib.SGMLParseError):
            pass
    else:
        SGMLLIB_PARSEERROR = RuntimeError
        class ParseError(RuntimeError):
            pass


class _AbstractFormParser:
    """forms attribute contains HTMLForm instances on completion."""
    # thanks to Moshe Zadka for an example of sgmllib/htmllib usage
    def __init__(self, entitydefs=None, encoding=DEFAULT_ENCODING):
        if entitydefs is None:
            entitydefs = get_entitydefs()
        self._entitydefs = entitydefs
        self._encoding = encoding

        self.base = None
        self.forms = []
        self.labels = []
        self._current_label = None
        self._current_form = None
        self._select = None
        self._optgroup = None
        self._option = None
        self._textarea = None

        # forms[0] will contain all controls that are outside of any form
        # self._global_form is an alias for self.forms[0]
        self._global_form = None
        self.start_form([])
        self.end_form()
        self._current_form = self._global_form = self.forms[0]

    def do_base(self, attrs):
        debug("%s", attrs)
        for key, value in attrs:
            if key == "href":
                self.base = self.unescape_attr_if_required(value)

    def end_body(self):
        debug("")
        if self._current_label is not None:
            self.end_label()
        if self._current_form is not self._global_form:
            self.end_form()

    def start_form(self, attrs):
        debug("%s", attrs)
        if self._current_form is not self._global_form:
            raise ParseError("nested FORMs")
        name = None
        action = None
        enctype = "application/x-www-form-urlencoded"
        method = "GET"
        d = {}
        for key, value in attrs:
            if key == "name":
                name = self.unescape_attr_if_required(value)
            elif key == "action":
                action = self.unescape_attr_if_required(value)
            elif key == "method":
                method = self.unescape_attr_if_required(value.upper())
            elif key == "enctype":
                enctype = self.unescape_attr_if_required(value.lower())
            d[key] = self.unescape_attr_if_required(value)
        controls = []
        self._current_form = (name, action, method, enctype), d, controls

    def end_form(self):
        debug("")
        if self._current_label is not None:
            self.end_label()
        if self._current_form is self._global_form:
            raise ParseError("end of FORM before start")
        self.forms.append(self._current_form)
        self._current_form = self._global_form

    def start_select(self, attrs):
        debug("%s", attrs)
        if self._select is not None:
            raise ParseError("nested SELECTs")
        if self._textarea is not None:
            raise ParseError("SELECT inside TEXTAREA")
        d = {}
        for key, val in attrs:
            d[key] = self.unescape_attr_if_required(val)

        self._select = d
        self._add_label(d)

        self._append_select_control({"__select": d})

    def end_select(self):
        debug("")
        if self._select is None:
            raise ParseError("end of SELECT before start")

        if self._option is not None:
            self._end_option()

        self._select = None

    def start_optgroup(self, attrs):
        debug("%s", attrs)
        if self._select is None:
            raise ParseError("OPTGROUP outside of SELECT")
        d = {}
        for key, val in attrs:
            d[key] = self.unescape_attr_if_required(val)

        self._optgroup = d

    def end_optgroup(self):
        debug("")
        if self._optgroup is None:
            raise ParseError("end of OPTGROUP before start")
        self._optgroup = None

    def _start_option(self, attrs):
        debug("%s", attrs)
        if self._select is None:
            raise ParseError("OPTION outside of SELECT")
        if self._option is not None:
            self._end_option()

        d = {}
        for key, val in attrs:
            d[key] = self.unescape_attr_if_required(val)

        self._option = {}
        self._option.update(d)
        if (self._optgroup and self._optgroup.has_key("disabled") and
            not self._option.has_key("disabled")):
            self._option["disabled"] = None

    def _end_option(self):
        debug("")
        if self._option is None:
            raise ParseError("end of OPTION before start")

        contents = self._option.get("contents", "").strip()
        self._option["contents"] = contents
        if not self._option.has_key("value"):
            self._option["value"] = contents
        if not self._option.has_key("label"):
            self._option["label"] = contents
        # stuff dict of SELECT HTML attrs into a special private key
        #  (gets deleted again later)
        self._option["__select"] = self._select
        self._append_select_control(self._option)
        self._option = None

    def _append_select_control(self, attrs):
        debug("%s", attrs)
        controls = self._current_form[2]
        name = self._select.get("name")
        controls.append(("select", name, attrs))

    def start_textarea(self, attrs):
        debug("%s", attrs)
        if self._textarea is not None:
            raise ParseError("nested TEXTAREAs")
        if self._select is not None:
            raise ParseError("TEXTAREA inside SELECT")
        d = {}
        for key, val in attrs:
            d[key] = self.unescape_attr_if_required(val)
        self._add_label(d)

        self._textarea = d

    def end_textarea(self):
        debug("")
        if self._textarea is None:
            raise ParseError("end of TEXTAREA before start")
        controls = self._current_form[2]
        name = self._textarea.get("name")
        controls.append(("textarea", name, self._textarea))
        self._textarea = None

    def start_label(self, attrs):
        debug("%s", attrs)
        if self._current_label:
            self.end_label()
        d = {}
        for key, val in attrs:
            d[key] = self.unescape_attr_if_required(val)
        taken = bool(d.get("for"))  # empty id is invalid
        d["__text"] = ""
        d["__taken"] = taken
        if taken:
            self.labels.append(d)
        self._current_label = d

    def end_label(self):
        debug("")
        label = self._current_label
        if label is None:
            # something is ugly in the HTML, but we're ignoring it
            return
        self._current_label = None
        # if it is staying around, it is True in all cases
        del label["__taken"]

    def _add_label(self, d):
        #debug("%s", d)
        if self._current_label is not None:
            if not self._current_label["__taken"]:
                self._current_label["__taken"] = True
                d["__label"] = self._current_label

    def handle_data(self, data):
        debug("%s", data)

        # according to http://www.w3.org/TR/html4/appendix/notes.html#h-B.3.1
        # line break immediately after start tags or immediately before end
        # tags must be ignored, but real browsers only ignore a line break
        # after a start tag, so we'll do that.
        if data[0:2] == "\r\n":
            data = data[2:]
        if data[0:1] in ["\n", "\r"]:
            data = data[1:]

        if self._option is not None:
            # self._option is a dictionary of the OPTION element's HTML
            # attributes, but it has two special keys, one of which is the
            # special "contents" key contains text between OPTION tags (the
            # other is the "__select" key: see the end_option method)
            map = self._option
            key = "contents"
        elif self._textarea is not None:
            map = self._textarea
            key = "value"
            data = normalize_line_endings(data)
        # not if within option or textarea
        elif self._current_label is not None:
            map = self._current_label
            key = "__text"
        else:
            return

        if not map.has_key(key):
            map[key] = data
        else:
            map[key] = map[key] + data

    def do_button(self, attrs):
        debug("%s", attrs)
        d = {}
        d["type"] = "submit"  # default
        for key, val in attrs:
            d[key] = self.unescape_attr_if_required(val)
        controls = self._current_form[2]

        type = d["type"]
        name = d.get("name")
        # we don't want to lose information, so use a type string that
        # doesn't clash with INPUT TYPE={SUBMIT,RESET,BUTTON}
        # e.g. type for BUTTON/RESET is "resetbutton"
        #     (type for INPUT/RESET is "reset")
        type = type+"button"
        self._add_label(d)
        controls.append((type, name, d))

    def do_input(self, attrs):
        debug("%s", attrs)
        d = {}
        d["type"] = "text"  # default
        for key, val in attrs:
            d[key] = self.unescape_attr_if_required(val)
        controls = self._current_form[2]

        type = d["type"]
        name = d.get("name")
        self._add_label(d)
        controls.append((type, name, d))

    def do_isindex(self, attrs):
        debug("%s", attrs)
        d = {}
        for key, val in attrs:
            d[key] = self.unescape_attr_if_required(val)
        controls = self._current_form[2]

        self._add_label(d)
        # isindex doesn't have type or name HTML attributes
        controls.append(("isindex", None, d))

    def handle_entityref(self, name):
        #debug("%s", name)
        self.handle_data(unescape(
            '&%s;' % name, self._entitydefs, self._encoding))

    def handle_charref(self, name):
        #debug("%s", name)
        self.handle_data(unescape_charref(name, self._encoding))

    def unescape_attr(self, name):
        #debug("%s", name)
        return unescape(name, self._entitydefs, self._encoding)

    def unescape_attrs(self, attrs):
        #debug("%s", attrs)
        escaped_attrs = {}
        for key, val in attrs.items():
            try:
                val.items
            except AttributeError:
                escaped_attrs[key] = self.unescape_attr(val)
            else:
                # e.g. "__select" -- yuck!
                escaped_attrs[key] = self.unescape_attrs(val)
        return escaped_attrs

    def unknown_entityref(self, ref): self.handle_data("&%s;" % ref)
    def unknown_charref(self, ref): self.handle_data("&#%s;" % ref)


if not HAVE_MODULE_HTMLPARSER:
    class XHTMLCompatibleFormParser:
        def __init__(self, entitydefs=None, encoding=DEFAULT_ENCODING):
            raise ValueError("HTMLParser could not be imported")
else:
    class XHTMLCompatibleFormParser(_AbstractFormParser, HTMLParser.HTMLParser):
        """Good for XHTML, bad for tolerance of incorrect HTML."""
        # thanks to Michael Howitz for this!
        def __init__(self, entitydefs=None, encoding=DEFAULT_ENCODING):
            HTMLParser.HTMLParser.__init__(self)
            _AbstractFormParser.__init__(self, entitydefs, encoding)

        def feed(self, data):
            try:
                HTMLParser.HTMLParser.feed(self, data)
            except HTMLParser.HTMLParseError, exc:
                raise ParseError(exc)

        def start_option(self, attrs):
            _AbstractFormParser._start_option(self, attrs)

        def end_option(self):
            _AbstractFormParser._end_option(self)

        def handle_starttag(self, tag, attrs):
            try:
                method = getattr(self, "start_" + tag)
            except AttributeError:
                try:
                    method = getattr(self, "do_" + tag)
                except AttributeError:
                    pass  # unknown tag
                else:
                    method(attrs)
            else:
                method(attrs)

        def handle_endtag(self, tag):
            try:
                method = getattr(self, "end_" + tag)
            except AttributeError:
                pass  # unknown tag
            else:
                method()

        def unescape(self, name):
            # Use the entitydefs passed into constructor, not
            # HTMLParser.HTMLParser's entitydefs.
            return self.unescape_attr(name)

        def unescape_attr_if_required(self, name):
            return name  # HTMLParser.HTMLParser already did it
        def unescape_attrs_if_required(self, attrs):
            return attrs  # ditto


class _AbstractSgmllibParser(_AbstractFormParser):

    def do_option(self, attrs):
        _AbstractFormParser._start_option(self, attrs)

    if sys.version_info[:2] >= (2,5):
        # we override this attr to decode hex charrefs
        entity_or_charref = re.compile(
            '&(?:([a-zA-Z][-.a-zA-Z0-9]*)|#(x?[0-9a-fA-F]+))(;?)')
        def convert_entityref(self, name):
            return unescape("&%s;" % name, self._entitydefs, self._encoding)
        def convert_charref(self, name):
            return unescape_charref("%s" % name, self._encoding)
        def unescape_attr_if_required(self, name):
            return name  # sgmllib already did it
        def unescape_attrs_if_required(self, attrs):
            return attrs  # ditto
    else:
        def unescape_attr_if_required(self, name):
            return self.unescape_attr(name)
        def unescape_attrs_if_required(self, attrs):
            return self.unescape_attrs(attrs)


class FormParser(_AbstractSgmllibParser, sgmllib.SGMLParser):
    """Good for tolerance of incorrect HTML, bad for XHTML."""
    def __init__(self, entitydefs=None, encoding=DEFAULT_ENCODING):
        sgmllib.SGMLParser.__init__(self)
        _AbstractFormParser.__init__(self, entitydefs, encoding)

    def feed(self, data):
        try:
            sgmllib.SGMLParser.feed(self, data)
        except SGMLLIB_PARSEERROR, exc:
            raise ParseError(exc)



# sigh, must support mechanize by allowing dynamic creation of classes based on
# its bundled copy of BeautifulSoup (which was necessary because of dependency
# problems)

def _create_bs_classes(bs,
                       icbinbs,
                       ):
    class _AbstractBSFormParser(_AbstractSgmllibParser):
        bs_base_class = None
        def __init__(self, entitydefs=None, encoding=DEFAULT_ENCODING):
            _AbstractFormParser.__init__(self, entitydefs, encoding)
            self.bs_base_class.__init__(self)
        def handle_data(self, data):
            _AbstractFormParser.handle_data(self, data)
            self.bs_base_class.handle_data(self, data)
        def feed(self, data):
            try:
                self.bs_base_class.feed(self, data)
            except SGMLLIB_PARSEERROR, exc:
                raise ParseError(exc)


    class RobustFormParser(_AbstractBSFormParser, bs):
        """Tries to be highly tolerant of incorrect HTML."""
        pass
    RobustFormParser.bs_base_class = bs
    class NestingRobustFormParser(_AbstractBSFormParser, icbinbs):
        """Tries to be highly tolerant of incorrect HTML.

        Different from RobustFormParser in that it more often guesses nesting
        above missing end tags (see BeautifulSoup docs).

        """
        pass
    NestingRobustFormParser.bs_base_class = icbinbs

    return RobustFormParser, NestingRobustFormParser

try:
    if sys.version_info[:2] < (2, 2):
        raise ImportError  # BeautifulSoup uses generators
    import BeautifulSoup
except ImportError:
    pass
else:
    RobustFormParser, NestingRobustFormParser = _create_bs_classes(
        BeautifulSoup.BeautifulSoup, BeautifulSoup.ICantBelieveItsBeautifulSoup
        )
    __all__ += ['RobustFormParser', 'NestingRobustFormParser']


#FormParser = XHTMLCompatibleFormParser  # testing hack
#FormParser = RobustFormParser  # testing hack


def ParseResponseEx(response,
                    select_default=False,
                    form_parser_class=FormParser,
                    request_class=urllib2.Request,
                    entitydefs=None,
                    encoding=DEFAULT_ENCODING,

                    # private
                    _urljoin=urlparse.urljoin,
                    _urlparse=urlparse.urlparse,
                    _urlunparse=urlparse.urlunparse,
                    ):
    """Identical to ParseResponse, except that:

    1. The returned list contains an extra item.  The first form in the list
    contains all controls not contained in any FORM element.

    2. The arguments ignore_errors and backwards_compat have been removed.

    3. Backwards-compatibility mode (backwards_compat=True) is not available.
    """
    return _ParseFileEx(response, response.geturl(),
                        select_default,
                        False,
                        form_parser_class,
                        request_class,
                        entitydefs,
                        False,
                        encoding,
                        _urljoin=_urljoin,
                        _urlparse=_urlparse,
                        _urlunparse=_urlunparse,
                        )

def ParseFileEx(file, base_uri,
                select_default=False,
                form_parser_class=FormParser,
                request_class=urllib2.Request,
                entitydefs=None,
                encoding=DEFAULT_ENCODING,

                # private
                _urljoin=urlparse.urljoin,
                _urlparse=urlparse.urlparse,
                _urlunparse=urlparse.urlunparse,
                ):
    """Identical to ParseFile, except that:

    1. The returned list contains an extra item.  The first form in the list
    contains all controls not contained in any FORM element.

    2. The arguments ignore_errors and backwards_compat have been removed.

    3. Backwards-compatibility mode (backwards_compat=True) is not available.
    """
    return _ParseFileEx(file, base_uri,
                        select_default,
                        False,
                        form_parser_class,
                        request_class,
                        entitydefs,
                        False,
                        encoding,
                        _urljoin=_urljoin,
                        _urlparse=_urlparse,
                        _urlunparse=_urlunparse,
                        )

def ParseResponse(response, *args, **kwds):
    """Parse HTTP response and return a list of HTMLForm instances.

    The return value of urllib2.urlopen can be conveniently passed to this
    function as the response parameter.

    ClientForm.ParseError is raised on parse errors.

    response: file-like object (supporting read() method) with a method
     geturl(), returning the URI of the HTTP response
    select_default: for multiple-selection SELECT controls and RADIO controls,
     pick the first item as the default if none are selected in the HTML
    form_parser_class: class to instantiate and use to pass
    request_class: class to return from .click() method (default is
     urllib2.Request)
    entitydefs: mapping like {"&amp;": "&", ...} containing HTML entity
     definitions (a sensible default is used)
    encoding: character encoding used for encoding numeric character references
     when matching link text.  ClientForm does not attempt to find the encoding
     in a META HTTP-EQUIV attribute in the document itself (mechanize, for
     example, does do that and will pass the correct value to ClientForm using
     this parameter).

    backwards_compat: boolean that determines whether the returned HTMLForm
     objects are backwards-compatible with old code.  If backwards_compat is
     true:

     - ClientForm 0.1 code will continue to work as before.

     - Label searches that do not specify a nr (number or count) will always
       get the first match, even if other controls match.  If
       backwards_compat is False, label searches that have ambiguous results
       will raise an AmbiguityError.

     - Item label matching is done by strict string comparison rather than
       substring matching.

     - De-selecting individual list items is allowed even if the Item is
       disabled.

    The backwards_compat argument will be deprecated in a future release.

    Pass a true value for select_default if you want the behaviour specified by
    RFC 1866 (the HTML 2.0 standard), which is to select the first item in a
    RADIO or multiple-selection SELECT control if none were selected in the
    HTML.  Most browsers (including Microsoft Internet Explorer (IE) and
    Netscape Navigator) instead leave all items unselected in these cases.  The
    W3C HTML 4.0 standard leaves this behaviour undefined in the case of
    multiple-selection SELECT controls, but insists that at least one RADIO
    button should be checked at all times, in contradiction to browser
    behaviour.

    There is a choice of parsers.  ClientForm.XHTMLCompatibleFormParser (uses
    HTMLParser.HTMLParser) works best for XHTML, ClientForm.FormParser (uses
    sgmllib.SGMLParser) (the default) works better for ordinary grubby HTML.
    Note that HTMLParser is only available in Python 2.2 and later.  You can
    pass your own class in here as a hack to work around bad HTML, but at your
    own risk: there is no well-defined interface.

    """
    return _ParseFileEx(response, response.geturl(), *args, **kwds)[1:]

def ParseFile(file, base_uri, *args, **kwds):
    """Parse HTML and return a list of HTMLForm instances.

    ClientForm.ParseError is raised on parse errors.

    file: file-like object (supporting read() method) containing HTML with zero
     or more forms to be parsed
    base_uri: the URI of the document (note that the base URI used to submit
     the form will be that given in the BASE element if present, not that of
     the document)

    For the other arguments and further details, see ParseResponse.__doc__.

    """
    return _ParseFileEx(file, base_uri, *args, **kwds)[1:]

def _ParseFileEx(file, base_uri,
                 select_default=False,
                 ignore_errors=False,
                 form_parser_class=FormParser,
                 request_class=urllib2.Request,
                 entitydefs=None,
                 backwards_compat=True,
                 encoding=DEFAULT_ENCODING,
                 _urljoin=urlparse.urljoin,
                 _urlparse=urlparse.urlparse,
                 _urlunparse=urlparse.urlunparse,
                 ):
    if backwards_compat:
        deprecation("operating in backwards-compatibility mode", 1)
    fp = form_parser_class(entitydefs, encoding)
    while 1:
        data = file.read(CHUNK)
        try:
            fp.feed(data)
        except ParseError, e:
            e.base_uri = base_uri
            raise
        if len(data) != CHUNK: break
    if fp.base is not None:
        # HTML BASE element takes precedence over document URI
        base_uri = fp.base
    labels = []  # Label(label) for label in fp.labels]
    id_to_labels = {}
    for l in fp.labels:
        label = Label(l)
        labels.append(label)
        for_id = l["for"]
        coll = id_to_labels.get(for_id)
        if coll is None:
            id_to_labels[for_id] = [label]
        else:
            coll.append(label)
    forms = []
    for (name, action, method, enctype), attrs, controls in fp.forms:
        if action is None:
            action = base_uri
        else:
            action = _urljoin(base_uri, action)
        # would be nice to make HTMLForm class (form builder) pluggable
        form = HTMLForm(
            action, method, enctype, name, attrs, request_class,
            forms, labels, id_to_labels, backwards_compat)
        form._urlparse = _urlparse
        form._urlunparse = _urlunparse
        for ii in range(len(controls)):
            type, name, attrs = controls[ii]
            # index=ii*10 allows ImageControl to return multiple ordered pairs
            form.new_control(
                type, name, attrs, select_default=select_default, index=ii*10)
        forms.append(form)
    for form in forms:
        form.fixup()
    return forms


class Label:
    def __init__(self, attrs):
        self.id = attrs.get("for")
        self._text = attrs.get("__text").strip()
        self._ctext = compress_text(self._text)
        self.attrs = attrs
        self._backwards_compat = False  # maintained by HTMLForm

    def __getattr__(self, name):
        if name == "text":
            if self._backwards_compat:
                return self._text
            else:
                return self._ctext
        return getattr(Label, name)

    def __setattr__(self, name, value):
        if name == "text":
            # don't see any need for this, so make it read-only
            raise AttributeError("text attribute is read-only")
        self.__dict__[name] = value

    def __str__(self):
        return "<Label(id=%r, text=%r)>" % (self.id, self.text)


def _get_label(attrs):
    text = attrs.get("__label")
    if text is not None:
        return Label(text)
    else:
        return None

class Control:
    """An HTML form control.

    An HTMLForm contains a sequence of Controls.  The Controls in an HTMLForm
    are accessed using the HTMLForm.find_control method or the
    HTMLForm.controls attribute.

    Control instances are usually constructed using the ParseFile /
    ParseResponse functions.  If you use those functions, you can ignore the
    rest of this paragraph.  A Control is only properly initialised after the
    fixup method has been called.  In fact, this is only strictly necessary for
    ListControl instances.  This is necessary because ListControls are built up
    from ListControls each containing only a single item, and their initial
    value(s) can only be known after the sequence is complete.

    The types and values that are acceptable for assignment to the value
    attribute are defined by subclasses.

    If the disabled attribute is true, this represents the state typically
    represented by browsers by 'greying out' a control.  If the disabled
    attribute is true, the Control will raise AttributeError if an attempt is
    made to change its value.  In addition, the control will not be considered
    'successful' as defined by the W3C HTML 4 standard -- ie. it will
    contribute no data to the return value of the HTMLForm.click* methods.  To
    enable a control, set the disabled attribute to a false value.

    If the readonly attribute is true, the Control will raise AttributeError if
    an attempt is made to change its value.  To make a control writable, set
    the readonly attribute to a false value.

    All controls have the disabled and readonly attributes, not only those that
    may have the HTML attributes of the same names.

    On assignment to the value attribute, the following exceptions are raised:
    TypeError, AttributeError (if the value attribute should not be assigned
    to, because the control is disabled, for example) and ValueError.

    If the name or value attributes are None, or the value is an empty list, or
    if the control is disabled, the control is not successful.

    Public attributes:

    type: string describing type of control (see the keys of the
     HTMLForm.type2class dictionary for the allowable values) (readonly)
    name: name of control (readonly)
    value: current value of control (subclasses may allow a single value, a
     sequence of values, or either)
    disabled: disabled state
    readonly: readonly state
    id: value of id HTML attribute

    """
    def __init__(self, type, name, attrs, index=None):
        """
        type: string describing type of control (see the keys of the
         HTMLForm.type2class dictionary for the allowable values)
        name: control name
        attrs: HTML attributes of control's HTML element

        """
        raise NotImplementedError()

    def add_to_form(self, form):
        self._form = form
        form.controls.append(self)

    def fixup(self):
        pass

    def is_of_kind(self, kind):
        raise NotImplementedError()

    def clear(self):
        raise NotImplementedError()

    def __getattr__(self, name): raise NotImplementedError()
    def __setattr__(self, name, value): raise NotImplementedError()

    def pairs(self):
        """Return list of (key, value) pairs suitable for passing to urlencode.
        """
        return [(k, v) for (i, k, v) in self._totally_ordered_pairs()]

    def _totally_ordered_pairs(self):
        """Return list of (key, value, index) tuples.

        Like pairs, but allows preserving correct ordering even where several
        controls are involved.

        """
        raise NotImplementedError()

    def _write_mime_data(self, mw, name, value):
        """Write data for a subitem of this control to a MimeWriter."""
        # called by HTMLForm
        mw2 = mw.nextpart()
        mw2.addheader("Content-disposition",
                      'form-data; name="%s"' % name, 1)
        f = mw2.startbody(prefix=0)
        f.write(value)

    def __str__(self):
        raise NotImplementedError()

    def get_labels(self):
        """Return all labels (Label instances) for this control.
        
        If the control was surrounded by a <label> tag, that will be the first
        label; all other labels, connected by 'for' and 'id', are in the order
        that appear in the HTML.

        """
        res = []
        if self._label:
            res.append(self._label)
        if self.id:
            res.extend(self._form._id_to_labels.get(self.id, ()))
        return res


#---------------------------------------------------
class ScalarControl(Control):
    """Control whose value is not restricted to one of a prescribed set.

    Some ScalarControls don't accept any value attribute.  Otherwise, takes a
    single value, which must be string-like.

    Additional read-only public attribute:

    attrs: dictionary mapping the names of original HTML attributes of the
     control to their values

    """
    def __init__(self, type, name, attrs, index=None):
        self._index = index
        self._label = _get_label(attrs)
        self.__dict__["type"] = type.lower()
        self.__dict__["name"] = name
        self._value = attrs.get("value")
        self.disabled = attrs.has_key("disabled")
        self.readonly = attrs.has_key("readonly")
        self.id = attrs.get("id")

        self.attrs = attrs.copy()

        self._clicked = False

        self._urlparse = urlparse.urlparse
        self._urlunparse = urlparse.urlunparse

    def __getattr__(self, name):
        if name == "value":
            return self.__dict__["_value"]
        else:
            raise AttributeError("%s instance has no attribute '%s'" %
                                 (self.__class__.__name__, name))

    def __setattr__(self, name, value):
        if name == "value":
            if not isstringlike(value):
                raise TypeError("must assign a string")
            elif self.readonly:
                raise AttributeError("control '%s' is readonly" % self.name)
            elif self.disabled:
                raise AttributeError("control '%s' is disabled" % self.name)
            self.__dict__["_value"] = value
        elif name in ("name", "type"):
            raise AttributeError("%s attribute is readonly" % name)
        else:
            self.__dict__[name] = value

    def _totally_ordered_pairs(self):
        name = self.name
        value = self.value
        if name is None or value is None or self.disabled:
            return []
        return [(self._index, name, value)]

    def clear(self):
        if self.readonly:
            raise AttributeError("control '%s' is readonly" % self.name)
        self.__dict__["_value"] = None

    def __str__(self):
        name = self.name
        value = self.value
        if name is None: name = "<None>"
        if value is None: value = "<None>"

        infos = []
        if self.disabled: infos.append("disabled")
        if self.readonly: infos.append("readonly")
        info = ", ".join(infos)
        if info: info = " (%s)" % info

        return "<%s(%s=%s)%s>" % (self.__class__.__name__, name, value, info)


#---------------------------------------------------
class TextControl(ScalarControl):
    """Textual input control.

    Covers:

    INPUT/TEXT
    INPUT/PASSWORD
    INPUT/HIDDEN
    TEXTAREA

    """
    def __init__(self, type, name, attrs, index=None):
        ScalarControl.__init__(self, type, name, attrs, index)
        if self.type == "hidden": self.readonly = True
        if self._value is None:
            self._value = ""

    def is_of_kind(self, kind): return kind == "text"

#---------------------------------------------------
class FileControl(ScalarControl):
    """File upload with INPUT TYPE=FILE.

    The value attribute of a FileControl is always None.  Use add_file instead.

    Additional public method: add_file

    """

    def __init__(self, type, name, attrs, index=None):
        ScalarControl.__init__(self, type, name, attrs, index)
        self._value = None
        self._upload_data = []

    def is_of_kind(self, kind): return kind == "file"

    def clear(self):
        if self.readonly:
            raise AttributeError("control '%s' is readonly" % self.name)
        self._upload_data = []

    def __setattr__(self, name, value):
        if name in ("value", "name", "type"):
            raise AttributeError("%s attribute is readonly" % name)
        else:
            self.__dict__[name] = value

    def add_file(self, file_object, content_type=None, filename=None):
        if not hasattr(file_object, "read"):
            raise TypeError("file-like object must have read method")
        if content_type is not None and not isstringlike(content_type):
            raise TypeError("content type must be None or string-like")
        if filename is not None and not isstringlike(filename):
            raise TypeError("filename must be None or string-like")
        if content_type is None:
            content_type = "application/octet-stream"
        self._upload_data.append((file_object, content_type, filename))

    def _totally_ordered_pairs(self):
        # XXX should it be successful even if unnamed?
        if self.name is None or self.disabled:
            return []
        return [(self._index, self.name, "")]

    def _write_mime_data(self, mw, _name, _value):
        # called by HTMLForm
        # assert _name == self.name and _value == ''
        if len(self._upload_data) == 1:
            # single file
            file_object, content_type, filename = self._upload_data[0]
            mw2 = mw.nextpart()
            fn_part = filename and ('; filename="%s"' % filename) or ""
            disp = 'form-data; name="%s"%s' % (self.name, fn_part)
            mw2.addheader("Content-disposition", disp, prefix=1)
            fh = mw2.startbody(content_type, prefix=0)
            fh.write(file_object.read())
        elif len(self._upload_data) != 0:
            # multiple files
            mw2 = mw.nextpart()
            disp = 'form-data; name="%s"' % self.name
            mw2.addheader("Content-disposition", disp, prefix=1)
            fh = mw2.startmultipartbody("mixed", prefix=0)
            for file_object, content_type, filename in self._upload_data:
                mw3 = mw2.nextpart()
                fn_part = filename and ('; filename="%s"' % filename) or ""
                disp = "file%s" % fn_part
                mw3.addheader("Content-disposition", disp, prefix=1)
                fh2 = mw3.startbody(content_type, prefix=0)
                fh2.write(file_object.read())
            mw2.lastpart()

    def __str__(self):
        name = self.name
        if name is None: name = "<None>"

        if not self._upload_data:
            value = "<No files added>"
        else:
            value = []
            for file, ctype, filename in self._upload_data:
                if filename is None:
                    value.append("<Unnamed file>")
                else:
                    value.append(filename)
            value = ", ".join(value)

        info = []
        if self.disabled: info.append("disabled")
        if self.readonly: info.append("readonly")
        info = ", ".join(info)
        if info: info = " (%s)" % info

        return "<%s(%s=%s)%s>" % (self.__class__.__name__, name, value, info)


#---------------------------------------------------
class IsindexControl(ScalarControl):
    """ISINDEX control.

    ISINDEX is the odd-one-out of HTML form controls.  In fact, it isn't really
    part of regular HTML forms at all, and predates it.  You're only allowed
    one ISINDEX per HTML document.  ISINDEX and regular form submission are
    mutually exclusive -- either submit a form, or the ISINDEX.

    Having said this, since ISINDEX controls may appear in forms (which is
    probably bad HTML), ParseFile / ParseResponse will include them in the
    HTMLForm instances it returns.  You can set the ISINDEX's value, as with
    any other control (but note that ISINDEX controls have no name, so you'll
    need to use the type argument of set_value!).  When you submit the form,
    the ISINDEX will not be successful (ie., no data will get returned to the
    server as a result of its presence), unless you click on the ISINDEX
    control, in which case the ISINDEX gets submitted instead of the form:

    form.set_value("my isindex value", type="isindex")
    urllib2.urlopen(form.click(type="isindex"))

    ISINDEX elements outside of FORMs are ignored.  If you want to submit one
    by hand, do it like so:

    url = urlparse.urljoin(page_uri, "?"+urllib.quote_plus("my isindex value"))
    result = urllib2.urlopen(url)

    """
    def __init__(self, type, name, attrs, index=None):
        ScalarControl.__init__(self, type, name, attrs, index)
        if self._value is None:
            self._value = ""

    def is_of_kind(self, kind): return kind in ["text", "clickable"]

    def _totally_ordered_pairs(self):
        return []

    def _click(self, form, coord, return_type, request_class=urllib2.Request):
        # Relative URL for ISINDEX submission: instead of "foo=bar+baz",
        # want "bar+baz".
        # This doesn't seem to be specified in HTML 4.01 spec. (ISINDEX is
        # deprecated in 4.01, but it should still say how to submit it).
        # Submission of ISINDEX is explained in the HTML 3.2 spec, though.
        parts = self._urlparse(form.action)
        rest, (query, frag) = parts[:-2], parts[-2:]
        parts = rest + (urllib.quote_plus(self.value), None)
        url = self._urlunparse(parts)
        req_data = url, None, []

        if return_type == "pairs":
            return []
        elif return_type == "request_data":
            return req_data
        else:
            return request_class(url)

    def __str__(self):
        value = self.value
        if value is None: value = "<None>"

        infos = []
        if self.disabled: infos.append("disabled")
        if self.readonly: infos.append("readonly")
        info = ", ".join(infos)
        if info: info = " (%s)" % info

        return "<%s(%s)%s>" % (self.__class__.__name__, value, info)


#---------------------------------------------------
class IgnoreControl(ScalarControl):
    """Control that we're not interested in.

    Covers:

    INPUT/RESET
    BUTTON/RESET
    INPUT/BUTTON
    BUTTON/BUTTON

    These controls are always unsuccessful, in the terminology of HTML 4 (ie.
    they never require any information to be returned to the server).

    BUTTON/BUTTON is used to generate events for script embedded in HTML.

    The value attribute of IgnoreControl is always None.

    """
    def __init__(self, type, name, attrs, index=None):
        ScalarControl.__init__(self, type, name, attrs, index)
        self._value = None

    def is_of_kind(self, kind): return False

    def __setattr__(self, name, value):
        if name == "value":
            raise AttributeError(
                "control '%s' is ignored, hence read-only" % self.name)
        elif name in ("name", "type"):
            raise AttributeError("%s attribute is readonly" % name)
        else:
            self.__dict__[name] = value


#---------------------------------------------------
# ListControls

# helpers and subsidiary classes

class Item:
    def __init__(self, control, attrs, index=None):
        label = _get_label(attrs)
        self.__dict__.update({
            "name": attrs["value"],
            "_labels": label and [label] or [],
            "attrs": attrs,
            "_control": control,
            "disabled": attrs.has_key("disabled"),
            "_selected": False,
            "id": attrs.get("id"),
            "_index": index,
            })
        control.items.append(self)

    def get_labels(self):
        """Return all labels (Label instances) for this item.
        
        For items that represent radio buttons or checkboxes, if the item was
        surrounded by a <label> tag, that will be the first label; all other
        labels, connected by 'for' and 'id', are in the order that appear in
        the HTML.
        
        For items that represent select options, if the option had a label
        attribute, that will be the first label.  If the option has contents
        (text within the option tags) and it is not the same as the label
        attribute (if any), that will be a label.  There is nothing in the
        spec to my knowledge that makes an option with an id unable to be the
        target of a label's for attribute, so those are included, if any, for
        the sake of consistency and completeness.

        """
        res = []
        res.extend(self._labels)
        if self.id:
            res.extend(self._control._form._id_to_labels.get(self.id, ()))
        return res

    def __getattr__(self, name):
        if name=="selected":
            return self._selected
        raise AttributeError(name)

    def __setattr__(self, name, value):
        if name == "selected":
            self._control._set_selected_state(self, value)
        elif name == "disabled":
            self.__dict__["disabled"] = bool(value)
        else:
            raise AttributeError(name)

    def __str__(self):
        res = self.name
        if self.selected:
            res = "*" + res
        if self.disabled:
            res = "(%s)" % res
        return res

    def __repr__(self):
        # XXX appending the attrs without distinguishing them from name and id
        # is silly
        attrs = [("name", self.name), ("id", self.id)]+self.attrs.items()
        return "<%s %s>" % (
            self.__class__.__name__,
            " ".join(["%s=%r" % (k, v) for k, v in attrs])
            )

def disambiguate(items, nr, **kwds):
    msgs = []
    for key, value in kwds.items():
        msgs.append("%s=%r" % (key, value))
    msg = " ".join(msgs)
    if not items:
        raise ItemNotFoundError(msg)
    if nr is None:
        if len(items) > 1:
            raise AmbiguityError(msg)
        nr = 0
    if len(items) <= nr:
        raise ItemNotFoundError(msg)
    return items[nr]

class ListControl(Control):
    """Control representing a sequence of items.

    The value attribute of a ListControl represents the successful list items
    in the control.  The successful list items are those that are selected and
    not disabled.

    ListControl implements both list controls that take a length-1 value
    (single-selection) and those that take length >1 values
    (multiple-selection).

    ListControls accept sequence values only.  Some controls only accept
    sequences of length 0 or 1 (RADIO, and single-selection SELECT).
    In those cases, ItemCountError is raised if len(sequence) > 1.  CHECKBOXes
    and multiple-selection SELECTs (those having the "multiple" HTML attribute)
    accept sequences of any length.

    Note the following mistake:

    control.value = some_value
    assert control.value == some_value    # not necessarily true

    The reason for this is that the value attribute always gives the list items
    in the order they were listed in the HTML.

    ListControl items can also be referred to by their labels instead of names.
    Use the label argument to .get(), and the .set_value_by_label(),
    .get_value_by_label() methods.

    Note that, rather confusingly, though SELECT controls are represented in
    HTML by SELECT elements (which contain OPTION elements, representing
    individual list items), CHECKBOXes and RADIOs are not represented by *any*
    element.  Instead, those controls are represented by a collection of INPUT
    elements.  For example, this is a SELECT control, named "control1":

    <select name="control1">
     <option>foo</option>
     <option value="1">bar</option>
    </select>

    and this is a CHECKBOX control, named "control2":

    <input type="checkbox" name="control2" value="foo" id="cbe1">
    <input type="checkbox" name="control2" value="bar" id="cbe2">

    The id attribute of a CHECKBOX or RADIO ListControl is always that of its
    first element (for example, "cbe1" above).


    Additional read-only public attribute: multiple.

    """

    # ListControls are built up by the parser from their component items by
    # creating one ListControl per item, consolidating them into a single
    # master ListControl held by the HTMLForm:

    # -User calls form.new_control(...)
    # -Form creates Control, and calls control.add_to_form(self).
    # -Control looks for a Control with the same name and type in the form,
    #  and if it finds one, merges itself with that control by calling
    #  control.merge_control(self).  The first Control added to the form, of
    #  a particular name and type, is the only one that survives in the
    #  form.
    # -Form calls control.fixup for all its controls.  ListControls in the
    #  form know they can now safely pick their default values.

    # To create a ListControl without an HTMLForm, use:

    # control.merge_control(new_control)

    # (actually, it's much easier just to use ParseFile)

    _label = None

    def __init__(self, type, name, attrs={}, select_default=False,
                 called_as_base_class=False, index=None):
        """
        select_default: for RADIO and multiple-selection SELECT controls, pick
         the first item as the default if no 'selected' HTML attribute is
         present

        """
        if not called_as_base_class:
            raise NotImplementedError()

        self.__dict__["type"] = type.lower()
        self.__dict__["name"] = name
        self._value = attrs.get("value")
        self.disabled = False
        self.readonly = False
        self.id = attrs.get("id")
        self._closed = False

        # As Controls are merged in with .merge_control(), self.attrs will
        # refer to each Control in turn -- always the most recently merged
        # control.  Each merged-in Control instance corresponds to a single
        # list item: see ListControl.__doc__.
        self.items = []
        self._form = None

        self._select_default = select_default
        self._clicked = False

    def clear(self):
        self.value = []

    def is_of_kind(self, kind):
        if kind  == "list":
            return True
        elif kind == "multilist":
            return bool(self.multiple)
        elif kind == "singlelist":
            return not self.multiple
        else:
            return False

    def get_items(self, name=None, label=None, id=None,
                  exclude_disabled=False):
        """Return matching items by name or label.

        For argument docs, see the docstring for .get()

        """
        if name is not None and not isstringlike(name):
            raise TypeError("item name must be string-like")
        if label is not None and not isstringlike(label):
            raise TypeError("item label must be string-like")
        if id is not None and not isstringlike(id):
            raise TypeError("item id must be string-like")
        items = []  # order is important
        compat = self._form.backwards_compat
        for o in self.items:
            if exclude_disabled and o.disabled:
                continue
            if name is not None and o.name != name:
                continue
            if label is not None:
                for l in o.get_labels():
                    if ((compat and l.text == label) or
                        (not compat and l.text.find(label) > -1)):
                        break
                else:
                    continue
            if id is not None and o.id != id:
                continue
            items.append(o)
        return items

    def get(self, name=None, label=None, id=None, nr=None,
            exclude_disabled=False):
        """Return item by name or label, disambiguating if necessary with nr.

        All arguments must be passed by name, with the exception of 'name',
        which may be used as a positional argument.

        If name is specified, then the item must have the indicated name.

        If label is specified, then the item must have a label whose
        whitespace-compressed, stripped, text substring-matches the indicated
        label string (eg. label="please choose" will match
        "  Do  please  choose an item ").

        If id is specified, then the item must have the indicated id.

        nr is an optional 0-based index of the items matching the query.

        If nr is the default None value and more than item is found, raises
        AmbiguityError (unless the HTMLForm instance's backwards_compat
        attribute is true).

        If no item is found, or if items are found but nr is specified and not
        found, raises ItemNotFoundError.

        Optionally excludes disabled items.

        """
        if nr is None and self._form.backwards_compat:
            nr = 0  # :-/
        items = self.get_items(name, label, id, exclude_disabled)
        return disambiguate(items, nr, name=name, label=label, id=id)

    def _get(self, name, by_label=False, nr=None, exclude_disabled=False):
        # strictly for use by deprecated methods
        if by_label:
            name, label = None, name
        else:
            name, label = name, None
        return self.get(name, label, nr, exclude_disabled)

    def toggle(self, name, by_label=False, nr=None):
        """Deprecated: given a name or label and optional disambiguating index
        nr, toggle the matching item's selection.

        Selecting items follows the behavior described in the docstring of the
        'get' method.

        if the item is disabled, or this control is disabled or readonly,
        raise AttributeError.

        """
        deprecation(
            "item = control.get(...); item.selected = not item.selected")
        o = self._get(name, by_label, nr)
        self._set_selected_state(o, not o.selected)

    def set(self, selected, name, by_label=False, nr=None):
        """Deprecated: given a name or label and optional disambiguating index
        nr, set the matching item's selection to the bool value of selected.

        Selecting items follows the behavior described in the docstring of the
        'get' method.

        if the item is disabled, or this control is disabled or readonly,
        raise AttributeError.

        """
        deprecation(
            "control.get(...).selected = <boolean>")
        self._set_selected_state(self._get(name, by_label, nr), selected)

    def _set_selected_state(self, item, action):
        # action:
        # bool False: off
        # bool True: on
        if self.disabled:
            raise AttributeError("control '%s' is disabled" % self.name)
        if self.readonly:
            raise AttributeError("control '%s' is readonly" % self.name)
        action == bool(action)
        compat = self._form.backwards_compat
        if not compat and item.disabled:
            raise AttributeError("item is disabled")
        else:
            if compat and item.disabled and action:
                raise AttributeError("item is disabled")
            if self.multiple:
                item.__dict__["_selected"] = action
            else:
                if not action:
                    item.__dict__["_selected"] = False
                else:
                    for o in self.items:
                        o.__dict__["_selected"] = False
                    item.__dict__["_selected"] = True

    def toggle_single(self, by_label=None):
        """Deprecated: toggle the selection of the single item in this control.
        
        Raises ItemCountError if the control does not contain only one item.
        
        by_label argument is ignored, and included only for backwards
        compatibility.

        """
        deprecation(
            "control.items[0].selected = not control.items[0].selected")
        if len(self.items) != 1:
            raise ItemCountError(
                "'%s' is not a single-item control" % self.name)
        item = self.items[0]
        self._set_selected_state(item, not item.selected)

    def set_single(self, selected, by_label=None):
        """Deprecated: set the selection of the single item in this control.
        
        Raises ItemCountError if the control does not contain only one item.
        
        by_label argument is ignored, and included only for backwards
        compatibility.

        """
        deprecation(
            "control.items[0].selected = <boolean>")
        if len(self.items) != 1:
            raise ItemCountError(
                "'%s' is not a single-item control" % self.name)
        self._set_selected_state(self.items[0], selected)

    def get_item_disabled(self, name, by_label=False, nr=None):
        """Get disabled state of named list item in a ListControl."""
        deprecation(
            "control.get(...).disabled")
        return self._get(name, by_label, nr).disabled

    def set_item_disabled(self, disabled, name, by_label=False, nr=None):
        """Set disabled state of named list item in a ListControl.

        disabled: boolean disabled state

        """
        deprecation(
            "control.get(...).disabled = <boolean>")
        self._get(name, by_label, nr).disabled = disabled

    def set_all_items_disabled(self, disabled):
        """Set disabled state of all list items in a ListControl.

        disabled: boolean disabled state

        """
        for o in self.items:
            o.disabled = disabled

    def get_item_attrs(self, name, by_label=False, nr=None):
        """Return dictionary of HTML attributes for a single ListControl item.

        The HTML element types that describe list items are: OPTION for SELECT
        controls, INPUT for the rest.  These elements have HTML attributes that
        you may occasionally want to know about -- for example, the "alt" HTML
        attribute gives a text string describing the item (graphical browsers
        usually display this as a tooltip).

        The returned dictionary maps HTML attribute names to values.  The names
        and values are taken from the original HTML.

        """
        deprecation(
            "control.get(...).attrs")
        return self._get(name, by_label, nr).attrs

    def close_control(self):
        self._closed = True

    def add_to_form(self, form):
        assert self._form is None or form == self._form, (
            "can't add control to more than one form")
        self._form = form
        if self.name is None:
            # always count nameless elements as separate controls
            Control.add_to_form(self, form)
        else:
            for ii in range(len(form.controls)-1, -1, -1):
                control = form.controls[ii]
                if control.name == self.name and control.type == self.type:
                    if control._closed:
                        Control.add_to_form(self, form)
                    else:
                        control.merge_control(self)
                    break
            else:
                Control.add_to_form(self, form)

    def merge_control(self, control):
        assert bool(control.multiple) == bool(self.multiple)
        # usually, isinstance(control, self.__class__)
        self.items.extend(control.items)

    def fixup(self):
        """
        ListControls are built up from component list items (which are also
        ListControls) during parsing.  This method should be called after all
        items have been added.  See ListControl.__doc__ for the reason this is
        required.

        """
        # Need to set default selection where no item was indicated as being
        # selected by the HTML:

        # CHECKBOX:
        #  Nothing should be selected.
        # SELECT/single, SELECT/multiple and RADIO:
        #  RFC 1866 (HTML 2.0): says first item should be selected.
        #  W3C HTML 4.01 Specification: says that client behaviour is
        #   undefined in this case.  For RADIO, exactly one must be selected,
        #   though which one is undefined.
        #  Both Netscape and Microsoft Internet Explorer (IE) choose first
        #   item for SELECT/single.  However, both IE5 and Mozilla (both 1.0
        #   and Firebird 0.6) leave all items unselected for RADIO and
        #   SELECT/multiple.

        # Since both Netscape and IE all choose the first item for
        # SELECT/single, we do the same.  OTOH, both Netscape and IE
        # leave SELECT/multiple with nothing selected, in violation of RFC 1866
        # (but not in violation of the W3C HTML 4 standard); the same is true
        # of RADIO (which *is* in violation of the HTML 4 standard).  We follow
        # RFC 1866 if the _select_default attribute is set, and Netscape and IE
        # otherwise.  RFC 1866 and HTML 4 are always violated insofar as you
        # can deselect all items in a RadioControl.
        
        for o in self.items: 
            # set items' controls to self, now that we've merged
            o.__dict__["_control"] = self

    def __getattr__(self, name):
        if name == "value":
            compat = self._form.backwards_compat
            if self.name is None:
                return []
            return [o.name for o in self.items if o.selected and
                    (not o.disabled or compat)]
        else:
            raise AttributeError("%s instance has no attribute '%s'" %
                                 (self.__class__.__name__, name))

    def __setattr__(self, name, value):
        if name == "value":
            if self.disabled:
                raise AttributeError("control '%s' is disabled" % self.name)
            if self.readonly:
                raise AttributeError("control '%s' is readonly" % self.name)
            self._set_value(value)
        elif name in ("name", "type", "multiple"):
            raise AttributeError("%s attribute is readonly" % name)
        else:
            self.__dict__[name] = value

    def _set_value(self, value):
        if value is None or isstringlike(value):
            raise TypeError("ListControl, must set a sequence")
        if not value:
            compat = self._form.backwards_compat
            for o in self.items:
                if not o.disabled or compat:
                    o.selected = False
        elif self.multiple:
            self._multiple_set_value(value)
        elif len(value) > 1:
            raise ItemCountError(
                "single selection list, must set sequence of "
                "length 0 or 1")
        else:
            self._single_set_value(value)

    def _get_items(self, name, target=1):
        all_items = self.get_items(name)
        items = [o for o in all_items if not o.disabled]
        if len(items) < target:
            if len(all_items) < target:
                raise ItemNotFoundError(
                    "insufficient items with name %r" % name)
            else:
                raise AttributeError(
                    "insufficient non-disabled items with name %s" % name)
        on = []
        off = []
        for o in items:
            if o.selected:
                on.append(o)
            else:
                off.append(o)
        return on, off

    def _single_set_value(self, value):
        assert len(value) == 1
        on, off = self._get_items(value[0])
        assert len(on) <= 1
        if not on:
            off[0].selected = True

    def _multiple_set_value(self, value):
        compat = self._form.backwards_compat
        turn_on = []  # transactional-ish
        turn_off = [item for item in self.items if
                    item.selected and (not item.disabled or compat)]
        names = {}
        for nn in value:
            if nn in names.keys():
                names[nn] += 1
            else:
                names[nn] = 1
        for name, count in names.items():
            on, off = self._get_items(name, count)
            for i in range(count):
                if on:
                    item = on[0]
                    del on[0]
                    del turn_off[turn_off.index(item)]
                else:
                    item = off[0]
                    del off[0]
                    turn_on.append(item)
        for item in turn_off:
            item.selected = False
        for item in turn_on:
            item.selected = True

    def set_value_by_label(self, value):
        """Set the value of control by item labels.

        value is expected to be an iterable of strings that are substrings of
        the item labels that should be selected.  Before substring matching is
        performed, the original label text is whitespace-compressed
        (consecutive whitespace characters are converted to a single space
        character) and leading and trailing whitespace is stripped.  Ambiguous
        labels are accepted without complaint if the form's backwards_compat is
        True; otherwise, it will not complain as long as all ambiguous labels
        share the same item name (e.g. OPTION value).

        """
        if isstringlike(value):
            raise TypeError(value)
        if not self.multiple and len(value) > 1:
            raise ItemCountError(
                "single selection list, must set sequence of "
                "length 0 or 1")
        items = []
        for nn in value:
            found = self.get_items(label=nn)
            if len(found) > 1:
                if not self._form.backwards_compat:
                    # ambiguous labels are fine as long as item names (e.g.
                    # OPTION values) are same
                    opt_name = found[0].name
                    if [o for o in found[1:] if o.name != opt_name]:
                        raise AmbiguityError(nn)
                else:
                    # OK, we'll guess :-(  Assume first available item.
                    found = found[:1]
            for o in found:
                # For the multiple-item case, we could try to be smarter,
                # saving them up and trying to resolve, but that's too much.
                if self._form.backwards_compat or o not in items:
                    items.append(o)
                    break
            else:  # all of them are used
                raise ItemNotFoundError(nn)
        # now we have all the items that should be on
        # let's just turn everything off and then back on.
        self.value = []
        for o in items:
            o.selected = True

    def get_value_by_label(self):
        """Return the value of the control as given by normalized labels."""
        res = []
        compat = self._form.backwards_compat
        for o in self.items:
            if (not o.disabled or compat) and o.selected:
                for l in o.get_labels():
                    if l.text:
                        res.append(l.text)
                        break
                else:
                    res.append(None)
        return res

    def possible_items(self, by_label=False):
        """Deprecated: return the names or labels of all possible items.

        Includes disabled items, which may be misleading for some use cases.

        """
        deprecation(
            "[item.name for item in self.items]")
        if by_label:
            res = []
            for o in self.items:
                for l in o.get_labels():
                    if l.text:
                        res.append(l.text)
                        break
                else:
                    res.append(None)
            return res
        return [o.name for o in self.items]

    def _totally_ordered_pairs(self):
        if self.disabled or self.name is None:
            return []
        else:
            return [(o._index, self.name, o.name) for o in self.items
                    if o.selected and not o.disabled]

    def __str__(self):
        name = self.name
        if name is None: name = "<None>"

        display = [str(o) for o in self.items]

        infos = []
        if self.disabled: infos.append("disabled")
        if self.readonly: infos.append("readonly")
        info = ", ".join(infos)
        if info: info = " (%s)" % info

        return "<%s(%s=[%s])%s>" % (self.__class__.__name__,
                                    name, ", ".join(display), info)


class RadioControl(ListControl):
    """
    Covers:

    INPUT/RADIO

    """
    def __init__(self, type, name, attrs, select_default=False, index=None):
        attrs.setdefault("value", "on")
        ListControl.__init__(self, type, name, attrs, select_default,
                             called_as_base_class=True, index=index)
        self.__dict__["multiple"] = False
        o = Item(self, attrs, index)
        o.__dict__["_selected"] = attrs.has_key("checked")

    def fixup(self):
        ListControl.fixup(self)
        found = [o for o in self.items if o.selected and not o.disabled]
        if not found:
            if self._select_default:
                for o in self.items:
                    if not o.disabled:
                        o.selected = True
                        break
        else:
            # Ensure only one item selected.  Choose the last one,
            # following IE and Firefox.
            for o in found[:-1]:
                o.selected = False

    def get_labels(self):
        return []

class CheckboxControl(ListControl):
    """
    Covers:

    INPUT/CHECKBOX

    """
    def __init__(self, type, name, attrs, select_default=False, index=None):
        attrs.setdefault("value", "on")
        ListControl.__init__(self, type, name, attrs, select_default,
                             called_as_base_class=True, index=index)
        self.__dict__["multiple"] = True
        o = Item(self, attrs, index)
        o.__dict__["_selected"] = attrs.has_key("checked")

    def get_labels(self):
        return []


class SelectControl(ListControl):
    """
    Covers:

    SELECT (and OPTION)


    OPTION 'values', in HTML parlance, are Item 'names' in ClientForm parlance.

    SELECT control values and labels are subject to some messy defaulting
    rules.  For example, if the HTML representation of the control is:

    <SELECT name=year>
      <OPTION value=0 label="2002">current year</OPTION>
      <OPTION value=1>2001</OPTION>
      <OPTION>2000</OPTION>
    </SELECT>

    The items, in order, have labels "2002", "2001" and "2000", whereas their
    names (the OPTION values) are "0", "1" and "2000" respectively.  Note that
    the value of the last OPTION in this example defaults to its contents, as
    specified by RFC 1866, as do the labels of the second and third OPTIONs.

    The OPTION labels are sometimes more meaningful than the OPTION values,
    which can make for more maintainable code.

    Additional read-only public attribute: attrs

    The attrs attribute is a dictionary of the original HTML attributes of the
    SELECT element.  Other ListControls do not have this attribute, because in
    other cases the control as a whole does not correspond to any single HTML
    element.  control.get(...).attrs may be used as usual to get at the HTML
    attributes of the HTML elements corresponding to individual list items (for
    SELECT controls, these are OPTION elements).

    Another special case is that the Item.attrs dictionaries have a special key
    "contents" which does not correspond to any real HTML attribute, but rather
    contains the contents of the OPTION element:

    <OPTION>this bit</OPTION>

    """
    # HTML attributes here are treated slightly differently from other list
    # controls:
    # -The SELECT HTML attributes dictionary is stuffed into the OPTION
    #  HTML attributes dictionary under the "__select" key.
    # -The content of each OPTION element is stored under the special
    #  "contents" key of the dictionary.
    # After all this, the dictionary is passed to the SelectControl constructor
    # as the attrs argument, as usual.  However:
    # -The first SelectControl constructed when building up a SELECT control
    #  has a constructor attrs argument containing only the __select key -- so
    #  this SelectControl represents an empty SELECT control.
    # -Subsequent SelectControls have both OPTION HTML-attribute in attrs and
    #  the __select dictionary containing the SELECT HTML-attributes.

    def __init__(self, type, name, attrs, select_default=False, index=None):
        # fish out the SELECT HTML attributes from the OPTION HTML attributes
        # dictionary
        self.attrs = attrs["__select"].copy()
        self.__dict__["_label"] = _get_label(self.attrs)
        self.__dict__["id"] = self.attrs.get("id")
        self.__dict__["multiple"] = self.attrs.has_key("multiple")
        # the majority of the contents, label, and value dance already happened
        contents = attrs.get("contents")
        attrs = attrs.copy()
        del attrs["__select"]

        ListControl.__init__(self, type, name, self.attrs, select_default,
                             called_as_base_class=True, index=index)
        self.disabled = self.attrs.has_key("disabled")
        self.readonly = self.attrs.has_key("readonly")
        if attrs.has_key("value"):
            # otherwise it is a marker 'select started' token
            o = Item(self, attrs, index)
            o.__dict__["_selected"] = attrs.has_key("selected")
            # add 'label' label and contents label, if different.  If both are
            # provided, the 'label' label is used for display in HTML 
            # 4.0-compliant browsers (and any lower spec? not sure) while the
            # contents are used for display in older or less-compliant
            # browsers.  We make label objects for both, if the values are
            # different.
            label = attrs.get("label")
            if label:
                o._labels.append(Label({"__text": label}))
                if contents and contents != label:
                    o._labels.append(Label({"__text": contents}))
            elif contents:
                o._labels.append(Label({"__text": contents}))

    def fixup(self):
        ListControl.fixup(self)
        # Firefox doesn't exclude disabled items from those considered here
        # (i.e. from 'found', for both branches of the if below).  Note that
        # IE6 doesn't support the disabled attribute on OPTIONs at all.
        found = [o for o in self.items if o.selected]
        if not found:
            if not self.multiple or self._select_default:
                for o in self.items:
                    if not o.disabled:
                        was_disabled = self.disabled
                        self.disabled = False
                        try:
                            o.selected = True
                        finally:
                            o.disabled = was_disabled
                        break
        elif not self.multiple:
            # Ensure only one item selected.  Choose the last one,
            # following IE and Firefox.
            for o in found[:-1]:
                o.selected = False


#---------------------------------------------------
class SubmitControl(ScalarControl):
    """
    Covers:

    INPUT/SUBMIT
    BUTTON/SUBMIT

    """
    def __init__(self, type, name, attrs, index=None):
        ScalarControl.__init__(self, type, name, attrs, index)
        # IE5 defaults SUBMIT value to "Submit Query"; Firebird 0.6 leaves it
        # blank, Konqueror 3.1 defaults to "Submit".  HTML spec. doesn't seem
        # to define this.
        if self.value is None: self.value = ""
        self.readonly = True

    def get_labels(self):
        res = []
        if self.value:
            res.append(Label({"__text": self.value}))
        res.extend(ScalarControl.get_labels(self))
        return res

    def is_of_kind(self, kind): return kind == "clickable"

    def _click(self, form, coord, return_type, request_class=urllib2.Request):
        self._clicked = coord
        r = form._switch_click(return_type, request_class)
        self._clicked = False
        return r

    def _totally_ordered_pairs(self):
        if not self._clicked:
            return []
        return ScalarControl._totally_ordered_pairs(self)


#---------------------------------------------------
class ImageControl(SubmitControl):
    """
    Covers:

    INPUT/IMAGE

    Coordinates are specified using one of the HTMLForm.click* methods.

    """
    def __init__(self, type, name, attrs, index=None):
        SubmitControl.__init__(self, type, name, attrs, index)
        self.readonly = False

    def _totally_ordered_pairs(self):
        clicked = self._clicked
        if self.disabled or not clicked:
            return []
        name = self.name
        if name is None: return []
        pairs = [
            (self._index, "%s.x" % name, str(clicked[0])),
            (self._index+1, "%s.y" % name, str(clicked[1])),
            ]
        value = self._value
        if value:
            pairs.append((self._index+2, name, value))
        return pairs

    get_labels = ScalarControl.get_labels

# aliases, just to make str(control) and str(form) clearer
class PasswordControl(TextControl): pass
class HiddenControl(TextControl): pass
class TextareaControl(TextControl): pass
class SubmitButtonControl(SubmitControl): pass


def is_listcontrol(control): return control.is_of_kind("list")


class HTMLForm:
    """Represents a single HTML <form> ... </form> element.

    A form consists of a sequence of controls that usually have names, and
    which can take on various values.  The values of the various types of
    controls represent variously: text, zero-or-one-of-many or many-of-many
    choices, and files to be uploaded.  Some controls can be clicked on to
    submit the form, and clickable controls' values sometimes include the
    coordinates of the click.

    Forms can be filled in with data to be returned to the server, and then
    submitted, using the click method to generate a request object suitable for
    passing to urllib2.urlopen (or the click_request_data or click_pairs
    methods if you're not using urllib2).

    import ClientForm
    forms = ClientForm.ParseFile(html, base_uri)
    form = forms[0]

    form["query"] = "Python"
    form.find_control("nr_results").get("lots").selected = True

    response = urllib2.urlopen(form.click())

    Usually, HTMLForm instances are not created directly.  Instead, the
    ParseFile or ParseResponse factory functions are used.  If you do construct
    HTMLForm objects yourself, however, note that an HTMLForm instance is only
    properly initialised after the fixup method has been called (ParseFile and
    ParseResponse do this for you).  See ListControl.__doc__ for the reason
    this is required.

    Indexing a form (form["control_name"]) returns the named Control's value
    attribute.  Assignment to a form index (form["control_name"] = something)
    is equivalent to assignment to the named Control's value attribute.  If you
    need to be more specific than just supplying the control's name, use the
    set_value and get_value methods.

    ListControl values are lists of item names (specifically, the names of the
    items that are selected and not disabled, and hence are "successful" -- ie.
    cause data to be returned to the server).  The list item's name is the
    value of the corresponding HTML element's"value" attribute.

    Example:

      <INPUT type="CHECKBOX" name="cheeses" value="leicester"></INPUT>
      <INPUT type="CHECKBOX" name="cheeses" value="cheddar"></INPUT>

    defines a CHECKBOX control with name "cheeses" which has two items, named
    "leicester" and "cheddar".

    Another example:

      <SELECT name="more_cheeses">
        <OPTION>1</OPTION>
        <OPTION value="2" label="CHEDDAR">cheddar</OPTION>
      </SELECT>

    defines a SELECT control with name "more_cheeses" which has two items,
    named "1" and "2" (because the OPTION element's value HTML attribute
    defaults to the element contents -- see SelectControl.__doc__ for more on
    these defaulting rules).

    To select, deselect or otherwise manipulate individual list items, use the
    HTMLForm.find_control() and ListControl.get() methods.  To set the whole
    value, do as for any other control: use indexing or the set_/get_value
    methods.

    Example:

    # select *only* the item named "cheddar"
    form["cheeses"] = ["cheddar"]
    # select "cheddar", leave other items unaffected
    form.find_control("cheeses").get("cheddar").selected = True

    Some controls (RADIO and SELECT without the multiple attribute) can only
    have zero or one items selected at a time.  Some controls (CHECKBOX and
    SELECT with the multiple attribute) can have multiple items selected at a
    time.  To set the whole value of a ListControl, assign a sequence to a form
    index:

    form["cheeses"] = ["cheddar", "leicester"]

    If the ListControl is not multiple-selection, the assigned list must be of
    length one.

    To check if a control has an item, if an item is selected, or if an item is
    successful (selected and not disabled), respectively:

    "cheddar" in [item.name for item in form.find_control("cheeses").items]
    "cheddar" in [item.name for item in form.find_control("cheeses").items and
                  item.selected]
    "cheddar" in form["cheeses"]  # (or "cheddar" in form.get_value("cheeses"))

    Note that some list items may be disabled (see below).

    Note the following mistake:

    form[control_name] = control_value
    assert form[control_name] == control_value  # not necessarily true

    The reason for this is that form[control_name] always gives the list items
    in the order they were listed in the HTML.

    List items (hence list values, too) can be referred to in terms of list
    item labels rather than list item names using the appropriate label
    arguments.  Note that each item may have several labels.

    The question of default values of OPTION contents, labels and values is
    somewhat complicated: see SelectControl.__doc__ and
    ListControl.get_item_attrs.__doc__ if you think you need to know.

    Controls can be disabled or readonly.  In either case, the control's value
    cannot be changed until you clear those flags (see example below).
    Disabled is the state typically represented by browsers by 'greying out' a
    control.  Disabled controls are not 'successful' -- they don't cause data
    to get returned to the server.  Readonly controls usually appear in
    browsers as read-only text boxes.  Readonly controls are successful.  List
    items can also be disabled.  Attempts to select or deselect disabled items
    fail with AttributeError.

    If a lot of controls are readonly, it can be useful to do this:

    form.set_all_readonly(False)

    To clear a control's value attribute, so that it is not successful (until a
    value is subsequently set):

    form.clear("cheeses")

    More examples:

    control = form.find_control("cheeses")
    control.disabled = False
    control.readonly = False
    control.get("gruyere").disabled = True
    control.items[0].selected = True

    See the various Control classes for further documentation.  Many methods
    take name, type, kind, id, label and nr arguments to specify the control to
    be operated on: see HTMLForm.find_control.__doc__.

    ControlNotFoundError (subclass of ValueError) is raised if the specified
    control can't be found.  This includes occasions where a non-ListControl
    is found, but the method (set, for example) requires a ListControl.
    ItemNotFoundError (subclass of ValueError) is raised if a list item can't
    be found.  ItemCountError (subclass of ValueError) is raised if an attempt
    is made to select more than one item and the control doesn't allow that, or
    set/get_single are called and the control contains more than one item.
    AttributeError is raised if a control or item is readonly or disabled and
    an attempt is made to alter its value.

    Security note: Remember that any passwords you store in HTMLForm instances
    will be saved to disk in the clear if you pickle them (directly or
    indirectly).  The simplest solution to this is to avoid pickling HTMLForm
    objects.  You could also pickle before filling in any password, or just set
    the password to "" before pickling.


    Public attributes:

    action: full (absolute URI) form action
    method: "GET" or "POST"
    enctype: form transfer encoding MIME type
    name: name of form (None if no name was specified)
    attrs: dictionary mapping original HTML form attributes to their values

    controls: list of Control instances; do not alter this list
     (instead, call form.new_control to make a Control and add it to the
     form, or control.add_to_form if you already have a Control instance)



    Methods for form filling:
    -------------------------

    Most of the these methods have very similar arguments.  See
    HTMLForm.find_control.__doc__ for details of the name, type, kind, label
    and nr arguments.

    def find_control(self,
                     name=None, type=None, kind=None, id=None, predicate=None,
                     nr=None, label=None)

    get_value(name=None, type=None, kind=None, id=None, nr=None,
              by_label=False,  # by_label is deprecated
              label=None)
    set_value(value,
              name=None, type=None, kind=None, id=None, nr=None,
              by_label=False,  # by_label is deprecated
              label=None)

    clear_all()
    clear(name=None, type=None, kind=None, id=None, nr=None, label=None)

    set_all_readonly(readonly)


    Method applying only to FileControls:

    add_file(file_object,
             content_type="application/octet-stream", filename=None,
             name=None, id=None, nr=None, label=None)


    Methods applying only to clickable controls:

    click(name=None, type=None, id=None, nr=0, coord=(1,1), label=None)
    click_request_data(name=None, type=None, id=None, nr=0, coord=(1,1),
                       label=None)
    click_pairs(name=None, type=None, id=None, nr=0, coord=(1,1), label=None)

    """

    type2class = {
        "text": TextControl,
        "password": PasswordControl,
        "hidden": HiddenControl,
        "textarea": TextareaControl,

        "isindex": IsindexControl,

        "file": FileControl,

        "button": IgnoreControl,
        "buttonbutton": IgnoreControl,
        "reset": IgnoreControl,
        "resetbutton": IgnoreControl,

        "submit": SubmitControl,
        "submitbutton": SubmitButtonControl,
        "image": ImageControl,

        "radio": RadioControl,
        "checkbox": CheckboxControl,
        "select": SelectControl,
        }

#---------------------------------------------------
# Initialisation.  Use ParseResponse / ParseFile instead.

    def __init__(self, action, method="GET",
                 enctype="application/x-www-form-urlencoded",
                 name=None, attrs=None,
                 request_class=urllib2.Request,
                 forms=None, labels=None, id_to_labels=None,
                 backwards_compat=True):
        """
        In the usual case, use ParseResponse (or ParseFile) to create new
        HTMLForm objects.

        action: full (absolute URI) form action
        method: "GET" or "POST"
        enctype: form transfer encoding MIME type
        name: name of form
        attrs: dictionary mapping original HTML form attributes to their values

        """
        self.action = action
        self.method = method
        self.enctype = enctype
        self.name = name
        if attrs is not None:
            self.attrs = attrs.copy()
        else:
            self.attrs = {}
        self.controls = []
        self._request_class = request_class

        # these attributes are used by zope.testbrowser
        self._forms = forms  # this is a semi-public API!
        self._labels = labels  # this is a semi-public API!
        self._id_to_labels = id_to_labels  # this is a semi-public API!

        self.backwards_compat = backwards_compat  # note __setattr__

        self._urlunparse = urlparse.urlunparse
        self._urlparse = urlparse.urlparse

    def __getattr__(self, name):
        if name == "backwards_compat":
            return self._backwards_compat
        return getattr(HTMLForm, name)

    def __setattr__(self, name, value):
        # yuck
        if name == "backwards_compat":
            name = "_backwards_compat"
            value = bool(value)
            for cc in self.controls:
                try:
                    items = cc.items 
                except AttributeError:
                    continue
                else:
                    for ii in items:
                        for ll in ii.get_labels():
                            ll._backwards_compat = value
        self.__dict__[name] = value

    def new_control(self, type, name, attrs,
                    ignore_unknown=False, select_default=False, index=None):
        """Adds a new control to the form.

        This is usually called by ParseFile and ParseResponse.  Don't call it
        youself unless you're building your own Control instances.

        Note that controls representing lists of items are built up from
        controls holding only a single list item.  See ListControl.__doc__ for
        further information.

        type: type of control (see Control.__doc__ for a list)
        attrs: HTML attributes of control
        ignore_unknown: if true, use a dummy Control instance for controls of
         unknown type; otherwise, use a TextControl
        select_default: for RADIO and multiple-selection SELECT controls, pick
         the first item as the default if no 'selected' HTML attribute is
         present (this defaulting happens when the HTMLForm.fixup method is
         called)
        index: index of corresponding element in HTML (see
         MoreFormTests.test_interspersed_controls for motivation)

        """
        type = type.lower()
        klass = self.type2class.get(type)
        if klass is None:
            if ignore_unknown:
                klass = IgnoreControl
            else:
                klass = TextControl

        a = attrs.copy()
        if issubclass(klass, ListControl):
            control = klass(type, name, a, select_default, index)
        else:
            control = klass(type, name, a, index)

        if type == "select" and len(attrs) == 1:
            for ii in range(len(self.controls)-1, -1, -1):
                ctl = self.controls[ii]
                if ctl.type == "select":
                    ctl.close_control()
                    break

        control.add_to_form(self)
        control._urlparse = self._urlparse
        control._urlunparse = self._urlunparse

    def fixup(self):
        """Normalise form after all controls have been added.

        This is usually called by ParseFile and ParseResponse.  Don't call it
        youself unless you're building your own Control instances.

        This method should only be called once, after all controls have been
        added to the form.

        """
        for control in self.controls:
            control.fixup()
        self.backwards_compat = self._backwards_compat

#---------------------------------------------------
    def __str__(self):
        header = "%s%s %s %s" % (
            (self.name and self.name+" " or ""),
            self.method, self.action, self.enctype)
        rep = [header]
        for control in self.controls:
            rep.append("  %s" % str(control))
        return "<%s>" % "\n".join(rep)

#---------------------------------------------------
# Form-filling methods.

    def __getitem__(self, name):
        return self.find_control(name).value
    def __contains__(self, name):
        return bool(self.find_control(name))
    def __setitem__(self, name, value):
        control = self.find_control(name)
        try:
            control.value = value
        except AttributeError, e:
            raise ValueError(str(e))

    def get_value(self,
                  name=None, type=None, kind=None, id=None, nr=None,
                  by_label=False,  # by_label is deprecated
                  label=None):
        """Return value of control.

        If only name and value arguments are supplied, equivalent to

        form[name]

        """
        if by_label:
            deprecation("form.get_value_by_label(...)")
        c = self.find_control(name, type, kind, id, label=label, nr=nr)
        if by_label:
            try:
                meth = c.get_value_by_label
            except AttributeError:
                raise NotImplementedError(
                    "control '%s' does not yet support by_label" % c.name)
            else:
                return meth()
        else:
            return c.value
    def set_value(self, value,
                  name=None, type=None, kind=None, id=None, nr=None,
                  by_label=False,  # by_label is deprecated
                  label=None):
        """Set value of control.

        If only name and value arguments are supplied, equivalent to

        form[name] = value

        """
        if by_label:
            deprecation("form.get_value_by_label(...)")
        c = self.find_control(name, type, kind, id, label=label, nr=nr)
        if by_label:
            try:
                meth = c.set_value_by_label
            except AttributeError:
                raise NotImplementedError(
                    "control '%s' does not yet support by_label" % c.name)
            else:
                meth(value)
        else:
            c.value = value
    def get_value_by_label(
        self, name=None, type=None, kind=None, id=None, label=None, nr=None):
        """

        All arguments should be passed by name.

        """
        c = self.find_control(name, type, kind, id, label=label, nr=nr)
        return c.get_value_by_label()

    def set_value_by_label(
        self, value,
        name=None, type=None, kind=None, id=None, label=None, nr=None):
        """

        All arguments should be passed by name.

        """
        c = self.find_control(name, type, kind, id, label=label, nr=nr)
        c.set_value_by_label(value)

    def set_all_readonly(self, readonly):
        for control in self.controls:
            control.readonly = bool(readonly)

    def clear_all(self):
        """Clear the value attributes of all controls in the form.

        See HTMLForm.clear.__doc__.

        """
        for control in self.controls:
            control.clear()

    def clear(self,
              name=None, type=None, kind=None, id=None, nr=None, label=None):
        """Clear the value attribute of a control.

        As a result, the affected control will not be successful until a value
        is subsequently set.  AttributeError is raised on readonly controls.

        """
        c = self.find_control(name, type, kind, id, label=label, nr=nr)
        c.clear()


#---------------------------------------------------
# Form-filling methods applying only to ListControls.

    def possible_items(self,  # deprecated
                       name=None, type=None, kind=None, id=None,
                       nr=None, by_label=False, label=None):
        """Return a list of all values that the specified control can take."""
        c = self._find_list_control(name, type, kind, id, label, nr)
        return c.possible_items(by_label)

    def set(self, selected, item_name,  # deprecated
            name=None, type=None, kind=None, id=None, nr=None,
            by_label=False, label=None):
        """Select / deselect named list item.

        selected: boolean selected state

        """
        self._find_list_control(name, type, kind, id, label, nr).set(
            selected, item_name, by_label)
    def toggle(self, item_name,  # deprecated
               name=None, type=None, kind=None, id=None, nr=None,
               by_label=False, label=None):
        """Toggle selected state of named list item."""
        self._find_list_control(name, type, kind, id, label, nr).toggle(
            item_name, by_label)

    def set_single(self, selected,  # deprecated
                   name=None, type=None, kind=None, id=None,
                   nr=None, by_label=None, label=None):
        """Select / deselect list item in a control having only one item.

        If the control has multiple list items, ItemCountError is raised.

        This is just a convenience method, so you don't need to know the item's
        name -- the item name in these single-item controls is usually
        something meaningless like "1" or "on".

        For example, if a checkbox has a single item named "on", the following
        two calls are equivalent:

        control.toggle("on")
        control.toggle_single()

        """  # by_label ignored and deprecated
        self._find_list_control(
            name, type, kind, id, label, nr).set_single(selected)
    def toggle_single(self, name=None, type=None, kind=None, id=None,
                      nr=None, by_label=None, label=None):  # deprecated
        """Toggle selected state of list item in control having only one item.

        The rest is as for HTMLForm.set_single.__doc__.

        """  # by_label ignored and deprecated
        self._find_list_control(name, type, kind, id, label, nr).toggle_single()

#---------------------------------------------------
# Form-filling method applying only to FileControls.

    def add_file(self, file_object, content_type=None, filename=None,
                 name=None, id=None, nr=None, label=None):
        """Add a file to be uploaded.

        file_object: file-like object (with read method) from which to read
         data to upload
        content_type: MIME content type of data to upload
        filename: filename to pass to server

        If filename is None, no filename is sent to the server.

        If content_type is None, the content type is guessed based on the
        filename and the data from read from the file object.

        XXX
        At the moment, guessed content type is always application/octet-stream.
        Use sndhdr, imghdr modules.  Should also try to guess HTML, XML, and
        plain text.

        Note the following useful HTML attributes of file upload controls (see
        HTML 4.01 spec, section 17):

        accept: comma-separated list of content types that the server will
         handle correctly; you can use this to filter out non-conforming files
        size: XXX IIRC, this is indicative of whether form wants multiple or
         single files
        maxlength: XXX hint of max content length in bytes?

        """
        self.find_control(name, "file", id=id, label=label, nr=nr).add_file(
            file_object, content_type, filename)

#---------------------------------------------------
# Form submission methods, applying only to clickable controls.

    def click(self, name=None, type=None, id=None, nr=0, coord=(1,1),
              request_class=urllib2.Request,
              label=None):
        """Return request that would result from clicking on a control.

        The request object is a urllib2.Request instance, which you can pass to
        urllib2.urlopen (or ClientCookie.urlopen).

        Only some control types (INPUT/SUBMIT & BUTTON/SUBMIT buttons and
        IMAGEs) can be clicked.

        Will click on the first clickable control, subject to the name, type
        and nr arguments (as for find_control).  If no name, type, id or number
        is specified and there are no clickable controls, a request will be
        returned for the form in its current, un-clicked, state.

        IndexError is raised if any of name, type, id or nr is specified but no
        matching control is found.  ValueError is raised if the HTMLForm has an
        enctype attribute that is not recognised.

        You can optionally specify a coordinate to click at, which only makes a
        difference if you clicked on an image.

        """
        return self._click(name, type, id, label, nr, coord, "request",
                           self._request_class)

    def click_request_data(self,
                           name=None, type=None, id=None,
                           nr=0, coord=(1,1),
                           request_class=urllib2.Request,
                           label=None):
        """As for click method, but return a tuple (url, data, headers).

        You can use this data to send a request to the server.  This is useful
        if you're using httplib or urllib rather than urllib2.  Otherwise, use
        the click method.

        # Untested.  Have to subclass to add headers, I think -- so use urllib2
        # instead!
        import urllib
        url, data, hdrs = form.click_request_data()
        r = urllib.urlopen(url, data)

        # Untested.  I don't know of any reason to use httplib -- you can get
        # just as much control with urllib2.
        import httplib, urlparse
        url, data, hdrs = form.click_request_data()
        tup = urlparse(url)
        host, path = tup[1], urlparse.urlunparse((None, None)+tup[2:])
        conn = httplib.HTTPConnection(host)
        if data:
            httplib.request("POST", path, data, hdrs)
        else:
            httplib.request("GET", path, headers=hdrs)
        r = conn.getresponse()

        """
        return self._click(name, type, id, label, nr, coord, "request_data",
                           self._request_class)

    def click_pairs(self, name=None, type=None, id=None,
                    nr=0, coord=(1,1),
                    label=None):
        """As for click_request_data, but returns a list of (key, value) pairs.

        You can use this list as an argument to ClientForm.urlencode.  This is
        usually only useful if you're using httplib or urllib rather than
        urllib2 or ClientCookie.  It may also be useful if you want to manually
        tweak the keys and/or values, but this should not be necessary.
        Otherwise, use the click method.

        Note that this method is only useful for forms of MIME type
        x-www-form-urlencoded.  In particular, it does not return the
        information required for file upload.  If you need file upload and are
        not using urllib2, use click_request_data.

        Also note that Python 2.0's urllib.urlencode is slightly broken: it
        only accepts a mapping, not a sequence of pairs, as an argument.  This
        messes up any ordering in the argument.  Use ClientForm.urlencode
        instead.

        """
        return self._click(name, type, id, label, nr, coord, "pairs",
                           self._request_class)

#---------------------------------------------------

    def find_control(self,
                     name=None, type=None, kind=None, id=None,
                     predicate=None, nr=None,
                     label=None):
        """Locate and return some specific control within the form.

        At least one of the name, type, kind, predicate and nr arguments must
        be supplied.  If no matching control is found, ControlNotFoundError is
        raised.

        If name is specified, then the control must have the indicated name.

        If type is specified then the control must have the specified type (in
        addition to the types possible for <input> HTML tags: "text",
        "password", "hidden", "submit", "image", "button", "radio", "checkbox",
        "file" we also have "reset", "buttonbutton", "submitbutton",
        "resetbutton", "textarea", "select" and "isindex").

        If kind is specified, then the control must fall into the specified
        group, each of which satisfies a particular interface.  The types are
        "text", "list", "multilist", "singlelist", "clickable" and "file".

        If id is specified, then the control must have the indicated id.

        If predicate is specified, then the control must match that function.
        The predicate function is passed the control as its single argument,
        and should return a boolean value indicating whether the control
        matched.

        nr, if supplied, is the sequence number of the control (where 0 is the
        first).  Note that control 0 is the first control matching all the
        other arguments (if supplied); it is not necessarily the first control
        in the form.  If no nr is supplied, AmbiguityError is raised if
        multiple controls match the other arguments (unless the
        .backwards-compat attribute is true).

        If label is specified, then the control must have this label.  Note
        that radio controls and checkboxes never have labels: their items do.

        """
        if ((name is None) and (type is None) and (kind is None) and
            (id is None) and (label is None) and (predicate is None) and
            (nr is None)):
            raise ValueError(
                "at least one argument must be supplied to specify control")
        return self._find_control(name, type, kind, id, label, predicate, nr)

#---------------------------------------------------
# Private methods.

    def _find_list_control(self,
                           name=None, type=None, kind=None, id=None, 
                           label=None, nr=None):
        if ((name is None) and (type is None) and (kind is None) and
            (id is None) and (label is None) and (nr is None)):
            raise ValueError(
                "at least one argument must be supplied to specify control")

        return self._find_control(name, type, kind, id, label, 
                                  is_listcontrol, nr)

    def _find_control(self, name, type, kind, id, label, predicate, nr):
        if ((name is not None) and (name is not Missing) and
            not isstringlike(name)):
            raise TypeError("control name must be string-like")
        if (type is not None) and not isstringlike(type):
            raise TypeError("control type must be string-like")
        if (kind is not None) and not isstringlike(kind):
            raise TypeError("control kind must be string-like")
        if (id is not None) and not isstringlike(id):
            raise TypeError("control id must be string-like")
        if (label is not None) and not isstringlike(label):
            raise TypeError("control label must be string-like")
        if (predicate is not None) and not callable(predicate):
            raise TypeError("control predicate must be callable")
        if (nr is not None) and nr < 0:
            raise ValueError("control number must be a positive integer")

        orig_nr = nr
        found = None
        ambiguous = False
        if nr is None and self.backwards_compat:
            nr = 0

        for control in self.controls:
            if ((name is not None and name != control.name) and
                (name is not Missing or control.name is not None)):
                continue
            if type is not None and type != control.type:
                continue
            if kind is not None and not control.is_of_kind(kind):
                continue
            if id is not None and id != control.id:
                continue
            if predicate and not predicate(control):
                continue
            if label:
                for l in control.get_labels():
                    if l.text.find(label) > -1:
                        break
                else:
                    continue
            if nr is not None:
                if nr == 0:
                    return control  # early exit: unambiguous due to nr
                nr -= 1
                continue
            if found:
                ambiguous = True
                break
            found = control

        if found and not ambiguous:
            return found

        description = []
        if name is not None: description.append("name %s" % repr(name))
        if type is not None: description.append("type '%s'" % type)
        if kind is not None: description.append("kind '%s'" % kind)
        if id is not None: description.append("id '%s'" % id)
        if label is not None: description.append("label '%s'" % label)
        if predicate is not None:
            description.append("predicate %s" % predicate)
        if orig_nr: description.append("nr %d" % orig_nr)
        description = ", ".join(description)

        if ambiguous:
            raise AmbiguityError("more than one control matching "+description)
        elif not found:
            raise ControlNotFoundError("no control matching "+description)
        assert False

    def _click(self, name, type, id, label, nr, coord, return_type,
               request_class=urllib2.Request):
        try:
            control = self._find_control(
                name, type, "clickable", id, label, None, nr)
        except ControlNotFoundError:
            if ((name is not None) or (type is not None) or (id is not None) or
                (nr != 0)):
                raise
            # no clickable controls, but no control was explicitly requested,
            # so return state without clicking any control
            return self._switch_click(return_type, request_class)
        else:
            return control._click(self, coord, return_type, request_class)

    def _pairs(self):
        """Return sequence of (key, value) pairs suitable for urlencoding."""
        return [(k, v) for (i, k, v, c_i) in self._pairs_and_controls()]


    def _pairs_and_controls(self):
        """Return sequence of (index, key, value, control_index)
        of totally ordered pairs suitable for urlencoding.

        control_index is the index of the control in self.controls
        """
        pairs = []
        for control_index in range(len(self.controls)):
            control = self.controls[control_index]
            for ii, key, val in control._totally_ordered_pairs():
                pairs.append((ii, key, val, control_index))

        # stable sort by ONLY first item in tuple
        pairs.sort()

        return pairs

    def _request_data(self):
        """Return a tuple (url, data, headers)."""
        method = self.method.upper()
        #scheme, netloc, path, parameters, query, frag = urlparse.urlparse(self.action)
        parts = self._urlparse(self.action)
        rest, (query, frag) = parts[:-2], parts[-2:]

        if method == "GET":
            if self.enctype != "application/x-www-form-urlencoded":
                raise ValueError(
                    "unknown GET form encoding type '%s'" % self.enctype)
            parts = rest + (urlencode(self._pairs()), None)
            uri = self._urlunparse(parts)
            return uri, None, []
        elif method == "POST":
            parts = rest + (query, None)
            uri = self._urlunparse(parts)
            if self.enctype == "application/x-www-form-urlencoded":
                return (uri, urlencode(self._pairs()),
                        [("Content-type", self.enctype)])
            elif self.enctype == "multipart/form-data":
                data = StringIO()
                http_hdrs = []
                mw = MimeWriter(data, http_hdrs)
                f = mw.startmultipartbody("form-data", add_to_http_hdrs=True,
                                          prefix=0)
                for ii, k, v, control_index in self._pairs_and_controls():
                    self.controls[control_index]._write_mime_data(mw, k, v)
                mw.lastpart()
                return uri, data.getvalue(), http_hdrs
            else:
                raise ValueError(
                    "unknown POST form encoding type '%s'" % self.enctype)
        else:
            raise ValueError("Unknown method '%s'" % method)

    def _switch_click(self, return_type, request_class=urllib2.Request):
        # This is called by HTMLForm and clickable Controls to hide switching
        # on return_type.
        if return_type == "pairs":
            return self._pairs()
        elif return_type == "request_data":
            return self._request_data()
        else:
            req_data = self._request_data()
            req = request_class(req_data[0], req_data[1])
            for key, val in req_data[2]:
                add_hdr = req.add_header
                if key.lower() == "content-type":
                    try:
                        add_hdr = req.add_unredirected_header
                    except AttributeError:
                        # pre-2.4 and not using ClientCookie
                        pass
                add_hdr(key, val)
            return req

########NEW FILE########
__FILENAME__ = example
import logging

# Configure how we want rdflib logger to log messages
_logger = logging.getLogger("rdflib")
_logger.setLevel(logging.DEBUG)
_hdlr = logging.StreamHandler()
_hdlr.setFormatter(logging.Formatter('%(name)s %(levelname)s: %(message)s'))
_logger.addHandler(_hdlr)

from rdflib.Graph import Graph
from rdflib import URIRef, Literal, BNode, Namespace
from rdflib import RDF

store = Graph()

# Bind a few prefix, namespace pairs.
store.bind("dc", "http://http://purl.org/dc/elements/1.1/")
store.bind("foaf", "http://xmlns.com/foaf/0.1/")

# Create a namespace object for the Friend of a friend namespace.
FOAF = Namespace("http://xmlns.com/foaf/0.1/")

# Create an identifier to use as the subject for Donna.
donna = BNode()

# Add triples using store's add method.
store.add((donna, RDF.type, FOAF["Person"]))
store.add((donna, FOAF["nick"], Literal("donna", lang="foo")))
store.add((donna, FOAF["name"], Literal("Donna Fales")))

# Iterate over triples in store and print them out.
print "--- printing raw triples ---"
for s, p, o in store:
    print s, p, o

# For each foaf:Person in the store print out its mbox property.
print "--- printing mboxes ---"
for person in store.subjects(RDF.type, FOAF["Person"]):
    for mbox in store.objects(person, FOAF["mbox"]):
        print mbox

# Serialize the store as RDF/XML to the file foaf.rdf.
store.serialize("foaf.rdf", format="pretty-xml", max_depth=3)

# Let's show off the serializers

print "RDF Serializations:"

# Serialize as XML
print "--- start: rdf-xml ---"
print store.serialize(format="pretty-xml")
print "--- end: rdf-xml ---\n"

# Serialize as NTriples
print "--- start: ntriples ---"
print store.serialize(format="nt")
print "--- end: ntriples ---\n"


########NEW FILE########
__FILENAME__ = swap_primer
# http://www.w3.org/2000/10/swap/Primer

# This is a simple primer using some of the 
# example stuff in the above Primer on N3
# get RDFLib at http://rdflib.net/ 


# Load up RDFLib

from rdflib import *

# Firstly, it doesn't have to be so complex.
# Here we create a "Graph" of our work.
# Think of it as a blank piece of graph paper!

primer = ConjunctiveGraph()
myNS = Namespace('#')

primer.add((myNS.pat, myNS.knows, myNS.jo))
# or:
primer.add((myNS['pat'], myNS['age'], long(24)))


# Now, with just that, lets see how the system
# recorded *way* too many details about what
# you just asserted as fact.
#

from pprint import pprint
pprint(list(primer))


# just think .whatever((s, p, o))
# here we report on what we know

pprint(list(primer.subjects()))
pprint(list(primer.predicates()))
pprint(list(primer.objects()))

# and other things that make sense

# what do we know about pat?
pprint(list(primer.predicate_objects(myNS.pat)))

# who is what age?
pprint(list(primer.subject_objects(myNS.age)))



# Okay, so lets now work with a bigger
# dataset from the example, and start
# with a fresh new graph.


primer = ConjunctiveGraph()


# Lets start with a verbatim string straight from the primer text:

mySource = """


@prefix : <http://www.w3.org/2000/10/swap/Primer#>.
@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix owl:  <http://www.w3.org/2002/07/owl#> .
@prefix dc:  <http://purl.org/dc/elements/1.1/> .
@prefix foo: <http://www.w3.org/2000/10/swap/Primer#>.
@prefix swap: <http://www.w3.org/2000/10/swap/>.

<> dc:title
  "Primer - Getting into the Semantic Web and RDF using N3".

<#pat> <#knows> <#jo> .
<#pat> <#age> 24 .
<#al> is <#child> of <#pat> .

<#pat> <#child>  <#al>, <#chaz>, <#mo> ;
       <#age>    24 ;
       <#eyecolor> "blue" .


:Person a rdfs:Class.

:Pat a :Person.

:Woman a rdfs:Class; rdfs:subClassOf :Person .

:sister a rdf:Property.

:sister rdfs:domain :Person; 
        rdfs:range :Woman.

:Woman = foo:FemaleAdult .
:Title a rdf:Property; = dc:title .



""" # --- End of primer code

# To make this go easier to spit back out... 
# technically, we already created a namespace
# with the object init (and it added some namespaces as well)
# By default, your main namespace is the URI of your 
# current working directory, so lets make that simpler:

myNS = Namespace(URIRef('http://www.w3.org/2000/10/swap/Primer#'))
primer.bind('', myNS)
primer.bind('owl', 'http://www.w3.org/2002/07/owl#')
primer.bind('dc', 'http://purl.org/dc/elements/1.1/')
primer.bind('swap', 'http://www.w3.org/2000/10/swap/')
sourceCode = StringInputSource(mySource, myNS)

# Lets load it up!

primer.parse(sourceCode, format='n3')


# Now you can query, either directly straight into a list:

[(x, y, z) for x, y, z in primer]

# or spit it back out (mostly) the way we created it:

print primer.serialize(format='n3')

# for more insight into things already done, lets see the namespaces

list(primer.namespaces())

# lets ask something about the data

list(primer.objects(myNS.pat, myNS.child))



########NEW FILE########
__FILENAME__ = BNode
# TODO: where can we move _unique_id and _serial_number_generator?
from string import ascii_letters
from random import choice

try:
    from hashlib import md5
except ImportError:
    from md5 import md5    

def _unique_id():
    """Create a (hopefully) unique prefix"""
    id = ""
    for i in xrange(0,8):
        id += choice(ascii_letters)
    return id

def _serial_number_generator():
    i = 0
    while 1:
        yield i
        i = i + 1

from rdflib.Identifier import Identifier
from rdflib.syntax.xml_names import is_ncname
import threading

bNodeLock = threading.RLock()

class BNode(Identifier):
    """
    Blank Node: http://www.w3.org/TR/rdf-concepts/#section-blank-nodes

    "In non-persistent O-O software construction, support for object
    identity is almost accidental: in the simplest implementation,
    each object resides at a certain address, and a reference to the
    object uses that address, which serves as immutable object
    identity.

    ...

    Maintaining object identity in shared databases raises problems:
    every client that needs to create objects must obtain a unique
    identity for them; " -- Bertand Meyer
    """
    __slots__ = ()

    def __new__(cls, value=None, # only store implementations should pass in a value
                _sn_gen=_serial_number_generator(), _prefix=_unique_id()):
        if value==None:
            # so that BNode values do not
            # collide with ones created with a different instance of this module
            # at some other time.
            bNodeLock.acquire()
            node_id = _sn_gen.next()
            bNodeLock.release()
            value = "%s%s" % (_prefix, node_id)
        else:
            # TODO: check that value falls within acceptable bnode value range
            # for RDF/XML needs to be something that can be serialzed as a nodeID
            # for N3 ??
            # Unless we require these constraints be enforced elsewhere?
            pass #assert is_ncname(unicode(value)), "BNode identifiers must be valid NCNames"

        return Identifier.__new__(cls, value)

    def n3(self):
        return "_:%s" % self

    def __getnewargs__(self):
        return (unicode(self), )

    def __reduce__(self):
        return (BNode, (unicode(self),))


    def __str__(self):
        return self.encode("unicode-escape")

    def __repr__(self):
        return """rdflib.BNode('%s')""" % str(self)

    def md5_term_hash(self):
        d = md5(str(self))
        d.update("B")
        return d.hexdigest()


########NEW FILE########
__FILENAME__ = Collection
from rdflib import RDF, BNode, Literal
from rdflib.Graph import Graph

class Collection(object):
    """
    See 3.3.5 Emulating container types: http://docs.python.org/ref/sequence-types.html#l2h-232
    
    >>> listName = BNode()
    >>> g = Graph('IOMemory')
    >>> listItem1 = BNode()
    >>> listItem2 = BNode()
    >>> g.add((listName,RDF.first,Literal(1)))
    >>> g.add((listName,RDF.rest,listItem1))
    >>> g.add((listItem1,RDF.first,Literal(2)))
    >>> g.add((listItem1,RDF.rest,listItem2))
    >>> g.add((listItem2,RDF.rest,RDF.nil))
    >>> g.add((listItem2,RDF.first,Literal(3)))
    >>> c=Collection(g,listName)
    >>> print list(c)
    [rdflib.Literal('1', language=None, datatype=rdflib.URIRef('http://www.w3.org/2001/XMLSchema#int')), rdflib.Literal('2', language=None, datatype=rdflib.URIRef('http://www.w3.org/2001/XMLSchema#int')), rdflib.Literal('3', language=None, datatype=rdflib.URIRef('http://www.w3.org/2001/XMLSchema#int'))]
    >>> 1 in c
    True
    >>> len(c)
    3
    >>> c._get_container(1) == listItem1
    True
    >>> c.index(Literal(2)) == 1
    True
    """
    def __init__(self, graph, uri, seq=[]):
        self.graph = graph
        self.uri = uri or BNode()
        for item in seq:
            self.append(item)

    def _get_container(self, index):
        """Gets the first, rest holding node at index."""
        assert isinstance(index, int)
        graph = self.graph
        container = self.uri
        i = 0
        while i<index:
            i += 1
            container = graph.value(container, RDF.rest)
            if container is None:
                break
        return container

    def __len__(self):
        """length of items in collection."""
        count = 0
        for item in self.graph.items(self.uri):
            count += 1
        return count

    def index(self, item):
        """
        Returns the 0-based numerical index of the item in the list          
        """
        listName = self.uri
        index = 0
        while True:
            if (listName,RDF.first,item) in self.graph:
                return index
            else:
                newLink = list(self.graph.objects(listName,RDF.rest))
                index += 1
                if newLink == [RDF.nil]:
                    raise ValueError("%s is not in %s"%(item,self.uri))
                elif not newLink:
                    raise Exception("Malformed RDF Collection: %s"%self.uri)
                else:
                    assert len(newLink)==1, "Malformed RDF Collection: %s"%self.uri
                    listName = newLink[0]

    def __getitem__(self, key):
        """TODO"""
        c = self._get_container(key)
        if c:
            v = self.graph.value(c, RDF.first)
            if v:
                return v
            else:
                raise KeyError, key
        else:
            raise IndexError, key

    def __setitem__(self, key, value):
        """TODO"""
        c = self._get_container(key)
        if c:
            self.graph.add((c, RDF.first, value))
        else:
            raise IndexError, key


    def __delitem__(self, key):
        """..."""
        self[key] # to raise any potential key exceptions
        graph = self.graph
        current = self._get_container(key)
        assert current
        if key==len(self)-1:
            graph.remove((current, RDF.first, None))
            graph.remove((current, RDF.rest, None))
        else:
            next = self._get_container(key+1)
            assert next
            first = graph.value(next, RDF.first)
            rest = graph.value(next, RDF.rest)

            graph.set((current, RDF.first, first))
            graph.set((current, RDF.rest, rest))

    def __iter__(self):
        """Iterator over items in Collections"""
        return self.graph.items(self.uri)

    def append(self, item):
        container = self.uri
        graph = self.graph
        while True:
            first = graph.value(container, RDF.first)
            if first is None:
                graph.add((container, RDF.first, item))
                return
            else:
                rest = graph.value(container, RDF.rest)
                if rest:
                    container = rest
                else:
                    node = BNode()
                    graph.add((container, RDF.rest, node))
                    container = node

    def clear(self):
        container = self.uri
        graph = self.graph
        while container:
            rest = graph.value(container, RDF.rest)
            graph.remove((container, RDF.first, None))
            graph.remove((container, RDF.rest, None))
            container = rest
def test():
    import doctest
    doctest.testmod()

if __name__=="__main__":
    test()

    g = Graph()

    c = Collection(g, BNode())

    assert len(c)==0

    c = Collection(g, BNode(), [Literal("1"), Literal("2"), Literal("3"), Literal("4")])

    assert len(c)==4

    assert c[1]==Literal("2"), c[1]

    del c[1]

    assert list(c)==[Literal("1"), Literal("3"), Literal("4")], list(c)

    try:
        del c[500]
    except IndexError, i:
        pass

    c.append(Literal("5"))

    print list(c)

    for i in c:
        print i

    del c[3]

    c.clear()

    assert len(c)==0


########NEW FILE########
__FILENAME__ = compat
import sys

if sys.version_info < (2, 4, 1, 'alpha', 1):
    def rsplit(value, char=None, count=-1):
        # rsplit is not available in Python < 2.4a1
        if char is None:
            char = ' '
        parts = value.split(char)
        return [char.join(parts[:-count])] + parts[-count:]
else:
    from string import rsplit


########NEW FILE########
__FILENAME__ = constants
"""
Deprecated. Use rdflib.RDF and rdflib.RDFS instead.
"""

import warnings 

warnings.warn("Use rdflib.RDF and rdflib.RDFS instead.", DeprecationWarning, stacklevel=2)


from rdflib import RDF as _RDF
from rdflib import RDFS as _RDFS

RDFNS = _RDF.RDFNS

# Syntax names
RDF = _RDF.RDF
DESCRIPTION = _RDF.Description
ID = _RDF.ID
ABOUT = _RDF.about
PARSE_TYPE = _RDF.parseType
RESOURCE = _RDF.resource
LI = _RDF.li
NODE_ID = _RDF.nodeID
DATATYPE = _RDF.datatype

# RDF Classes
SEQ = _RDF.Seq
BAG = _RDF.Bag
ALT = _RDF.Alt
STATEMENT = _RDF.Statement
PROPERTY = _RDF.Property
XMLLiteral = _RDF.XMLLiteral
LIST = _RDF.List

# RDF Properties
SUBJECT = _RDF.subject
PREDICATE = _RDF.predicate
OBJECT = _RDF.object
TYPE = _RDF.type
VALUE = _RDF.value
FIRST = _RDF.first
REST = _RDF.rest
# and _n where n is a non-negative integer

# RDF Resources
NIL = _RDF.nil


# SCHEMA
RDFSNS = _RDFS.RDFSNS

RDFS_CLASS = _RDFS.Class
RDFS_RESOURCE = _RDFS.Resource
RDFS_SUBCLASSOF = _RDFS.subClassOf
RDFS_SUBPROPERTYOF = _RDFS.subPropertyOf
RDFS_ISDEFINEDBY = _RDFS.isDefinedBy
RDFS_LABEL = _RDFS.label
RDFS_COMMENT = _RDFS.comment
RDFS_RANGE = _RDFS.range
RDFS_DOMAIN = _RDFS.domain
RDFS_LITERAL = _RDFS.Literal
RDFS_CONTAINER = _RDFS.Container
RDFS_SEEALSO = _RDFS.seeAlso

########NEW FILE########
__FILENAME__ = events

__doc__ = """
Dirt Simple Events

A Dispatcher (or a subclass of Dispatcher) stores event handlers that
are 'fired' simple event objects when interesting things happen.

Create a dispatcher:

  >>> d = Dispatcher()

Now create a handler for the event and subscribe it to the dispatcher
to handle Event events.  A handler is a simple function or method that
accepts the event as an argument:

  >>> def handler1(event): print `event`
  >>> d.subscribe(Event, handler1)

Now dispatch a new event into the dispatcher, and see handler1 get
fired:

  >>> d.dispatch(Event(foo='bar', data='yours', used_by='the event handlers'))
  <rdflib.events.Event ['data', 'foo', 'used_by']>
"""

class Event(object):
    """
    An event is a container for attributes.  The source of an event
    creates this object, or a subclass, gives it any kind of data that
    the events handlers need to handle the event, and then calls
    notify(event).

    The target of an event registers a function to handle the event it
    is interested with subscribe().  When a sources calls
    notify(event), each subscriber to that even will be called i no
    particular order.
    """

    def __init__(self, **kw):
        self.__dict__.update(kw)

    def __repr__(self):
        attrs = self.__dict__.keys()
        attrs.sort()
        return '<rdflib.events.Event %s>' % ([a for a in attrs],)


class Dispatcher(object):
    """
    An object that can dispatch events to a privately managed group of
    subscribers.
    """

    _dispatch_map = None

    def set_map(self, amap):
        self._dispatch_map = amap

    def get_map(self):
        return self._dispatch_map

    def subscribe(self, event_type, handler):
        """ Subscribe the given handler to an event_type.  Handlers
        are called in the order they are subscribed.
        """
        if self._dispatch_map is None:
            self.set_map({})
        lst = self._dispatch_map.get(event_type, None)
        if lst is None:
            lst = [handler]
        else:
            lst.append(handler)
        self._dispatch_map[event_type] = lst

    def dispatch(self, event):
        """ Dispatch the given event to the subscribed handlers for
        the event's type"""
        if self._dispatch_map is not None:
            lst = self._dispatch_map.get(type(event), None)
            if lst is None:
                raise ValueError("unknown event type: %s" % type(event))
            for l in lst:
                l(event)

def test():
    import doctest
    doctest.testmod()

if __name__ == '__main__':
    test()

########NEW FILE########
__FILENAME__ = exceptions

class Error(Exception):
    """Base class for rdflib exceptions."""
    def __init__(self, msg=None):
        Exception.__init__(self, msg)
        self.msg = msg


class TypeCheckError(Error):
    """Parts of assertions are subject to type checks."""

    def __init__(self, node):
        Error.__init__(self, node)
        self.type = type(node)
        self.node = node


class SubjectTypeError(TypeCheckError):
    """Subject of an assertion must be an instance of URIRef."""
    def __init__(self, node):
        TypeCheckError.__init__(self, node)
        self.msg = "Subject must be instance of URIRef or BNode: %s(%s)" \
                       % (self.node, self.type)


class PredicateTypeError(TypeCheckError):
    """Predicate of an assertion must be an instance of URIRef."""
    def __init__(self, node):
        TypeCheckError.__init__(self, node)
        self.msg = "Predicate must be a URIRef instance: %s(%s)" \
                       % (self.node, self.type)


class ObjectTypeError(TypeCheckError):
    """Object of an assertion must be an instance of URIRef, Literal,
    or BNode."""
    def __init__(self, node):
        TypeCheckError.__init__(self, node)
        self.msg = "Object must be instance of URIRef, Literal, or BNode: %s(%s)" % \
                       (self.node, self.type)

class ContextTypeError(TypeCheckError):
    """Context of an assertion must be an instance of URIRef."""
    def __init__(self, node):
        TypeCheckError.__init__(self, node)
        self.msg = "Context must be instance of URIRef or BNode: %s(%s)" \
                       % (self.node, self.type)

class ParserError(Error):
    """RDF Parser error."""
    def __init__(self, msg):
        self.msg = msg

    def __str__(self):
        return self.msg


class UniquenessError(Error) :
    """A uniqueness assumption was made in the context, and that is not true"""
    def __init__(self, values):
        Error.__init__(self, "Uniqueness assumption is not fulfilled. Multiple values are: %s" % values)



########NEW FILE########
__FILENAME__ = FileInputSource
from xml.sax.xmlreader import InputSource

class FileInputSource(InputSource, object):
    def __init__(self, file):
        super(FileInputSource, self).__init__(`file`)
        self.file = file
        self.setByteStream(file)
        # TODO: self.setEncoding(encoding)

    def __repr__(self):
        return `self.file`

########NEW FILE########
__FILENAME__ = Graph
from __future__ import generators

__doc__="""
Instanciating Graphs with default store (IOMemory) and default identifier (a BNode):

    >>> g=Graph()
    >>> g.store.__class__
    <class 'rdflib.store.IOMemory.IOMemory'>
    >>> g.identifier.__class__
    <class 'rdflib.BNode.BNode'>

Instanciating Graphs with a specific kind of store (IOMemory) and a default identifier (a BNode):

Other store kinds: Sleepycat, MySQL, ZODB, SQLite

    >>> store = plugin.get('IOMemory',Store)()
    >>> store.__class__.__name__
    'IOMemory'
    >>> graph = Graph(store)
    >>> graph.store.__class__
    <class 'rdflib.store.IOMemory.IOMemory'>

Instanciating Graphs with Sleepycat store and an identifier - <http://rdflib.net>:

    >>> g=Graph('Sleepycat',URIRef("http://rdflib.net"))
    >>> g.identifier
    rdflib.URIRef('http://rdflib.net')
    >>> str(g)
    "<http://rdflib.net> a rdfg:Graph;rdflib:storage [a rdflib:Store;rdfs:label 'Sleepycat']."

Creating a ConjunctiveGraph - The top level container for all named Graphs in a 'database':

    >>> g=ConjunctiveGraph()
    >>> str(g.default_context)
    "[a rdfg:Graph;rdflib:storage [a rdflib:Store;rdfs:label 'IOMemory']]."

Adding / removing reified triples to Graph and iterating over it directly or via triple pattern:
    
    >>> g=Graph('IOMemory')
    >>> statementId = BNode()
    >>> print len(g)
    0
    >>> g.add((statementId,RDF.type,RDF.Statement))
    >>> g.add((statementId,RDF.subject,URIRef('http://rdflib.net/store/ConjunctiveGraph')))
    >>> g.add((statementId,RDF.predicate,RDFS.label))
    >>> g.add((statementId,RDF.object,Literal("Conjunctive Graph")))
    >>> print len(g)
    4
    >>> for s,p,o in g:  print type(s)
    ...
    <class 'rdflib.BNode.BNode'>
    <class 'rdflib.BNode.BNode'>
    <class 'rdflib.BNode.BNode'>
    <class 'rdflib.BNode.BNode'>
    
    >>> for s,p,o in g.triples((None,RDF.object,None)):  print o
    ...
    Conjunctive Graph
    >>> g.remove((statementId,RDF.type,RDF.Statement))
    >>> print len(g)
    3

None terms in calls to triple can be thought of as 'open variables'  

Graph Aggregation - ConjunctiveGraphs and ReadOnlyGraphAggregate within the same store:
    
    >>> store = plugin.get('IOMemory',Store)()
    >>> g1 = Graph(store)
    >>> g2 = Graph(store)
    >>> g3 = Graph(store)
    >>> stmt1 = BNode()
    >>> stmt2 = BNode()
    >>> stmt3 = BNode()
    >>> g1.add((stmt1,RDF.type,RDF.Statement))
    >>> g1.add((stmt1,RDF.subject,URIRef('http://rdflib.net/store/ConjunctiveGraph')))
    >>> g1.add((stmt1,RDF.predicate,RDFS.label))
    >>> g1.add((stmt1,RDF.object,Literal("Conjunctive Graph")))
    >>> g2.add((stmt2,RDF.type,RDF.Statement))
    >>> g2.add((stmt2,RDF.subject,URIRef('http://rdflib.net/store/ConjunctiveGraph')))
    >>> g2.add((stmt2,RDF.predicate,RDF.type))
    >>> g2.add((stmt2,RDF.object,RDFS.Class))
    >>> g3.add((stmt3,RDF.type,RDF.Statement))
    >>> g3.add((stmt3,RDF.subject,URIRef('http://rdflib.net/store/ConjunctiveGraph')))
    >>> g3.add((stmt3,RDF.predicate,RDFS.comment))
    >>> g3.add((stmt3,RDF.object,Literal("The top-level aggregate graph - The sum of all named graphs within a Store")))
    >>> len(list(ConjunctiveGraph(store).subjects(RDF.type,RDF.Statement)))
    3
    >>> len(list(ReadOnlyGraphAggregate([g1,g2]).subjects(RDF.type,RDF.Statement)))
    2

ConjunctiveGraphs have a 'quads' method which returns quads instead of triples, where the fourth item
is the Graph (or subclass thereof) instance in which the triple was asserted:
    
    >>> from sets import Set    
    >>> uniqueGraphNames = Set([graph.identifier for s,p,o,graph in ConjunctiveGraph(store).quads((None,RDF.predicate,None))])
    >>> len(uniqueGraphNames)
    3
    >>> unionGraph = ReadOnlyGraphAggregate([g1,g2])
    >>> uniqueGraphNames = Set([graph.identifier for s,p,o,graph in unionGraph.quads((None,RDF.predicate,None))])
    >>> len(uniqueGraphNames)
    2
     
Parsing N3 from StringIO

    >>> g2=Graph()
    >>> src = \"\"\"
    ... @prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
    ... @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
    ... [ a rdf:Statement ;
    ...   rdf:subject <http://rdflib.net/store#ConjunctiveGraph>;
    ...   rdf:predicate rdfs:label;
    ...   rdf:object "Conjunctive Graph" ] \"\"\"
    >>> g2=g2.parse(StringIO(src),format='n3')
    >>> print len(g2)
    4

Using Namespace class:

    >>> RDFLib = Namespace('http://rdflib.net')
    >>> RDFLib.ConjunctiveGraph
    rdflib.URIRef('http://rdflib.netConjunctiveGraph')
    >>> RDFLib['Graph']
    rdflib.URIRef('http://rdflib.netGraph')

SPARQL Queries

    >>> print len(g)
    3
    >>> q = \'\'\'
    ... PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> SELECT ?pred WHERE { ?stmt rdf:predicate ?pred. }
    ... \'\'\'   
    >>> for pred in g.query(q):  print pred
    (rdflib.URIRef('http://www.w3.org/2000/01/rdf-schema#label'),)

SPARQL Queries with namespace bindings as argument

    >>> nsMap = {u"rdf":RDF.RDFNS}
    >>> for pred in g.query("SELECT ?pred WHERE { ?stmt rdf:predicate ?pred. }", initNs=nsMap): print pred
    (rdflib.URIRef('http://www.w3.org/2000/01/rdf-schema#label'),)

Parameterized SPARQL Queries

    >>> top = { Variable("?term") : RDF.predicate }
    >>> for pred in g.query("SELECT ?pred WHERE { ?stmt ?term ?pred. }", initBindings=top): print pred
    (rdflib.URIRef('http://www.w3.org/2000/01/rdf-schema#label'),)

"""


from cStringIO import StringIO
from rdflib import URIRef, BNode, Namespace, Literal, Variable
from rdflib import RDF, RDFS

from rdflib.Node import Node

from rdflib import plugin, exceptions

from rdflib.store import Store

from rdflib.syntax.serializer import Serializer
from rdflib.syntax.parsers import Parser
from rdflib.syntax.NamespaceManager import NamespaceManager
from rdflib import sparql
from rdflib.QueryResult import QueryResult
from rdflib.URLInputSource import URLInputSource

from xml.sax.xmlreader import InputSource
from xml.sax.saxutils import prepare_input_source

import logging
_logger = logging.getLogger("rdflib.Graph")

#import md5
import random
import warnings

try:
    from hashlib import md5
except ImportError:
    from md5 import md5    

class Graph(Node):
    """An RDF Graph

    The constructor accepts one argument, the 'store'
    that will be used to store the graph data (see the 'store'
    package for stores currently shipped with rdflib).

    Stores can be context-aware or unaware.  Unaware stores take up
    (some) less space but cannot support features that require
    context, such as true merging/demerging of sub-graphs and
    provenance.

    The Graph constructor can take an identifier which identifies the Graph
    by name.  If none is given, the graph is assigned a BNode for it's identifier.
    For more on named graphs, see: http://www.w3.org/2004/03/trix/

    Ontology for __str__ provenance terms:

    @prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
    @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
    @prefix : <http://rdflib.net/store#> .
    @prefix rdfg: <http://www.w3.org/2004/03/trix/rdfg-1/>.
    @prefix owl: <http://www.w3.org/2002/07/owl#>.
    @prefix log: <http://www.w3.org/2000/10/swap/log#>.
    @prefix xsd: <http://www.w3.org/2001/XMLSchema#>.

    :Store a owl:Class;
        rdfs:subClassOf <http://xmlns.com/wordnet/1.6/Electronic_database>;
        rdfs:subClassOf
            [a owl:Restriction;
             owl:onProperty rdfs:label;
             owl:allValuesFrom [a owl:DataRange;
                                owl:oneOf ("IOMemory"
                                           "Sleepcat"
                                           "MySQL"
                                           "Redland"
                                           "REGEXMatching"
                                           "ZODB"
                                           "AuditableStorage"
                                           "Memory")]
            ].

    :ConjunctiveGraph a owl:Class;
        rdfs:subClassOf rdfg:Graph;
        rdfs:label "The top-level graph within the store - the union of all the Graphs within."
        rdfs:seeAlso <http://rdflib.net/rdf_store/#ConjunctiveGraph>.

    :DefaultGraph a owl:Class;
        rdfs:subClassOf rdfg:Graph;
        rdfs:label "The 'default' subgraph of a conjunctive graph".


    :identifier a owl:Datatypeproperty;
        rdfs:label "The store-associated identifier of the formula. ".
        rdfs:domain log:Formula
        rdfs:range xsd:anyURI;

    :storage a owl:ObjectProperty;
        rdfs:domain [
            a owl:Class;
            owl:unionOf (log:Formula rdfg:Graph :ConjunctiveGraph)
        ];
        rdfs:range :Store.

    :default_context a owl:FunctionalProperty;
        rdfs:label "The default context for a conjunctive graph";
        rdfs:domain :ConjunctiveGraph;
        rdfs:range :DefaultGraph.


    {?cg a :ConjunctiveGraph;:storage ?store}
      => {?cg owl:sameAs ?store}.

    {?subGraph rdfg:subGraphOf ?cg;a :DefaultGraph}
      => {?cg a :ConjunctiveGraph;:default_context ?subGraphOf} .
    """

    def __init__(self, store='default', identifier=None,
                 namespace_manager=None):
        super(Graph, self).__init__()
        self.__identifier = identifier or BNode()
        if not isinstance(store, Store):
            # TODO: error handling
            self.__store = store = plugin.get(store, Store)()
        else:
            self.__store = store
        self.__namespace_manager = namespace_manager
        self.context_aware = False
        self.formula_aware = False

    def __get_store(self):
        return self.__store
    store = property(__get_store)

    def __get_identifier(self):
        return self.__identifier
    identifier = property(__get_identifier)

    def _get_namespace_manager(self):
        if self.__namespace_manager is None:
            self.__namespace_manager = NamespaceManager(self)
        return self.__namespace_manager

    def _set_namespace_manager(self, nm):
        self.__namespace_manager = nm
    namespace_manager = property(_get_namespace_manager, _set_namespace_manager)

    def __repr__(self):
        return "<Graph identifier=%s (%s)>" % (self.identifier, type(self))

    def __str__(self):
        if isinstance(self.identifier,URIRef):
            return "%s a rdfg:Graph;rdflib:storage [a rdflib:Store;rdfs:label '%s']."%(self.identifier.n3(),self.store.__class__.__name__)
        else:
            return "[a rdfg:Graph;rdflib:storage [a rdflib:Store;rdfs:label '%s']]."%(self.store.__class__.__name__)

    def destroy(self, configuration):
        """Destroy the store identified by `configuration` if supported"""
        self.__store.destroy(configuration)

    #Transactional interfaces (optional)
    def commit(self):
        """Commits active transactions"""
        self.__store.commit()

    def rollback(self):
        """Rollback active transactions"""
        self.__store.rollback()

    def open(self, configuration, create=False):
        """Open the graph store

        Might be necessary for stores that require opening a connection to a
        database or acquiring some resource.
        """
        return self.__store.open(configuration, create)

    def close(self, commit_pending_transaction=False):
        """Close the graph store

        Might be necessary for stores that require closing a connection to a
        database or releasing some resource.
        """
        self.__store.close(commit_pending_transaction=commit_pending_transaction)

    def add(self, (s, p, o)):
        """Add a triple with self as context"""
        self.__store.add((s, p, o), self, quoted=False)

    def addN(self, quads):
        """Add a sequence of triple with context"""
        self.__store.addN([(s, p, o, c) for s, p, o, c in quads
                                        if isinstance(c, Graph)
                                        and c.identifier is self.identifier])

    def remove(self, (s, p, o)):
        """Remove a triple from the graph

        If the triple does not provide a context attribute, removes the triple
        from all contexts.
        """
        self.__store.remove((s, p, o), context=self)

    def triples(self, (s, p, o)):
        """Generator over the triple store

        Returns triples that match the given triple pattern. If triple pattern
        does not provide a context, all contexts will be searched.
        """
        for (s, p, o), cg in self.__store.triples((s, p, o), context=self):
            yield (s, p, o)

    def __len__(self):
        """Returns the number of triples in the graph

        If context is specified then the number of triples in the context is
        returned instead.
        """
        return self.__store.__len__(context=self)

    def __iter__(self):
        """Iterates over all triples in the store"""
        return self.triples((None, None, None))

    def __contains__(self, triple):
        """Support for 'triple in graph' syntax"""
        for triple in self.triples(triple):
            return 1
        return 0

    def __hash__(self):
        return hash(self.identifier)

    def md5_term_hash(self):
        d = md5(str(self.identifier))
        d.update("G")
        return d.hexdigest()

    def __cmp__(self, other):
        if other is None:
            return -1
        elif isinstance(other, Graph):
            return cmp(self.identifier, other.identifier)
        else:
            #Note if None is considered equivalent to owl:Nothing
            #Then perhaps a graph with length 0 should be considered
            #equivalent to None (if compared to it)?
            return 1

    def __iadd__(self, other):
        """Add all triples in Graph other to Graph"""
        for triple in other:
            self.add(triple)
        return self

    def __isub__(self, other):
        """Subtract all triples in Graph other from Graph"""
        for triple in other:
            self.remove(triple)
        return self

    def __add__(self,other) :
        """Set theoretical union"""
        retval = Graph()
        for x in self.graph:
            retval.add(x)
        for y in other.graph:
            retval.add(y)
        return retval

    def __mul__(self,other) :
        """Set theoretical intersection"""
        retval = Graph()
        for x in other.graph:
            if x in self.graph: 
                retval.add(x)
        return retval

    def __sub__(self,other) :
        """Set theoretical difference"""
        retval = Graph()
        for x in self.graph:
            if not x in other.graph : 
                retval.add(x)
        return retval

    # Conv. methods

    def set(self, (subject, predicate, object)):
        """Convenience method to update the value of object

        Remove any existing triples for subject and predicate before adding
        (subject, predicate, object).
        """
        self.remove((subject, predicate, None))
        self.add((subject, predicate, object))

    def subjects(self, predicate=None, object=None):
        """A generator of subjects with the given predicate and object"""
        for s, p, o in self.triples((None, predicate, object)):
            yield s

    def predicates(self, subject=None, object=None):
        """A generator of predicates with the given subject and object"""
        for s, p, o in self.triples((subject, None, object)):
            yield p

    def objects(self, subject=None, predicate=None):
        """A generator of objects with the given subject and predicate"""
        for s, p, o in self.triples((subject, predicate, None)):
            yield o

    def subject_predicates(self, object=None):
        """A generator of (subject, predicate) tuples for the given object"""
        for s, p, o in self.triples((None, None, object)):
            yield s, p

    def subject_objects(self, predicate=None):
        """A generator of (subject, object) tuples for the given predicate"""
        for s, p, o in self.triples((None, predicate, None)):
            yield s, o

    def predicate_objects(self, subject=None):
        """A generator of (predicate, object) tuples for the given subject"""
        for s, p, o in self.triples((subject, None, None)):
            yield p, o

    def triples_choices(self, (subject, predicate, object_),context=None):
        for (s, p, o), cg in self.store.triples_choices(
            (subject, predicate, object_), context=self):
            yield (s, p, o)

    def value(self, subject=None, predicate=RDF.value, object=None,
              default=None, any=True):
        """Get a value for a pair of two criteria

        Exactly one of subject, predicate, object must be None. Useful if one
        knows that there may only be one value.

        It is one of those situations that occur a lot, hence this
        'macro' like utility

        Parameters:
        -----------
        subject, predicate, object  -- exactly one must be None
        default -- value to be returned if no values found
        any -- if True:
                 return any value in the case there is more than one
               else:
                 raise UniquenessError
        """
        retval = default

        if (subject is None and predicate is None) or \
                (subject is None and object is None) or \
                (predicate is None and object is None):
            return None
        
        if object is None:
            values = self.objects(subject, predicate)
        if subject is None:
            values = self.subjects(predicate, object)
        if predicate is None:
            values = self.predicates(subject, object)

        try:
            retval = values.next()
        except StopIteration, e:
            retval = default
        else:
            if any is False:
                try:
                    next = values.next()
                    msg = ("While trying to find a value for (%s, %s, %s) the "
                           "following multiple values where found:\n" %
                           (subject, predicate, object))
                    triples = self.store.triples((subject, predicate, object), None)
                    for (s, p, o), contexts in triples:
                        msg += "(%s, %s, %s)\n (contexts: %s)\n" % (
                            s, p, o, list(contexts))
                    raise exceptions.UniquenessError(msg)
                except StopIteration, e:
                    pass
        return retval

    def label(self, subject, default=''):
        """Query for the RDFS.label of the subject

        Return default if no label exists
        """
        if subject is None:
            return default
        return self.value(subject, RDFS.label, default=default, any=True)

    def comment(self, subject, default=''):
        """Query for the RDFS.comment of the subject

        Return default if no comment exists
        """
        if subject is None:
            return default
        return self.value(subject, RDFS.comment, default=default, any=True)

    def items(self, list):
        """Generator over all items in the resource specified by list

        list is an RDF collection.
        """
        while list:
            item = self.value(list, RDF.first)
            if item:
                yield item
            list = self.value(list, RDF.rest)

    def transitive_objects(self, subject, property, remember=None):
        """Transitively generate objects for the `property` relationship

        Generated objects belong to the depth first transitive closure of the
        `property` relationship starting at `subject`.
        """
        if remember is None:
            remember = {}
        if subject in remember:
            return
        remember[subject] = 1
        yield subject
        for object in self.objects(subject, property):
            for o in self.transitive_objects(object, property, remember):
                yield o

    def transitive_subjects(self, predicate, object, remember=None):
        """Transitively generate objects for the `property` relationship

        Generated objects belong to the depth first transitive closure of the
        `property` relationship starting at `subject`.
        """
        if remember is None:
            remember = {}
        if object in remember:
            return
        remember[object] = 1
        yield object
        for subject in self.subjects(predicate, object):
            for s in self.transitive_subjects(predicate, subject, remember):
                yield s

    def seq(self, subject):
        """Check if subject is an rdf:Seq

        If yes, it returns a Seq class instance, None otherwise.
        """
        if (subject, RDF.type, RDF.Seq) in self:
            return Seq(self, subject)
        else:
            return None

    def qname(self, uri):
        return self.namespace_manager.qname(uri)

    def compute_qname(self, uri):
        return self.namespace_manager.compute_qname(uri)

    def bind(self, prefix, namespace, override=True):
        """Bind prefix to namespace

        If override is True will bind namespace to given prefix if namespace
        was already bound to a different prefix.
        """
        return self.namespace_manager.bind(prefix, namespace, override=override)

    def namespaces(self):
        """Generator over all the prefix, namespace tuples"""
        for prefix, namespace in self.namespace_manager.namespaces():
            yield prefix, namespace

    def absolutize(self, uri, defrag=1):
        """Turn uri into an absolute URI if it's not one already"""
        return self.namespace_manager.absolutize(uri, defrag)

    def serialize(self, destination=None, format="xml", base=None, encoding=None, **args):
        """Serialize the Graph to destination

        If destination is None serialize method returns the serialization as a
        string. Format defaults to xml (AKA rdf/xml).
        """
        serializer = plugin.get(format, Serializer)(self)
        return serializer.serialize(destination, base=base, encoding=encoding, **args)

    def prepare_input_source(self, source, publicID=None):
        if isinstance(source, InputSource):
            input_source = source
        else:
            if hasattr(source, "read") and not isinstance(source, Namespace):
                # we need to make sure it's not an instance of Namespace since
                # Namespace instances have a read attr
                input_source = prepare_input_source(source)
            else:
                location = self.absolutize(source)
                input_source = URLInputSource(location)
                publicID = publicID or location
        if publicID:
            input_source.setPublicId(publicID)
        id = input_source.getPublicId()
        if id is None:
            #_logger.warning("no publicID set for source. Using '' for publicID.")
            input_source.setPublicId("")
        return input_source

    def parse(self, source, publicID=None, format="xml", **args):
        """ Parse source into Graph

        If Graph is context-aware it'll get loaded into it's own context
        (sub graph). Format defaults to xml (AKA rdf/xml). The publicID
        argument is for specifying the logical URI for the case that it's
        different from the physical source URI. Returns the context into which
        the source was parsed.
        """
        source = self.prepare_input_source(source, publicID)
        parser = plugin.get(format, Parser)()
        parser.parse(source, self, **args)
        return self

    def load(self, source, publicID=None, format="xml"):
        self.parse(source, publicID, format)

    def query(self, strOrQuery, initBindings={}, initNs={}, DEBUG=False,
              processor="sparql"):
        """
        Executes a SPARQL query (eventually will support Versa queries with same method) against this Graph
        strOrQuery - Is either a string consisting of the SPARQL query or an instance of rdflib.sparql.bison.Query.Query
        initBindings - A mapping from a Variable to an RDFLib term (used as initial bindings for SPARQL query)
        initNS - A mapping from a namespace prefix to an instance of rdflib.Namespace (used for SPARQL query)
        DEBUG - A boolean flag passed on to the SPARQL parser and evaluation engine
        processor - The kind of RDF query (must be 'sparql' until Versa is ported)
        """
        assert processor == 'sparql',"SPARQL is currently the only supported RDF query language"
        p = plugin.get(processor, sparql.Processor)(self)
        return plugin.get('SPARQLQueryResult',QueryResult)(p.query(strOrQuery, initBindings, initNs, DEBUG))

        processor_plugin = plugin.get(processor, sparql.Processor)(self.store)
        qresult_plugin = plugin.get('SPARQLQueryResult', QueryResult)

        res = processor_plugin.query(strOrQuery, initBindings, initNs, DEBUG)
        return qresult_plugin(res)

    def n3(self):
        """return an n3 identifier for the Graph"""
        return "[%s]" % self.identifier.n3()

    def __reduce__(self):
        return (Graph, (self.store, self.identifier,))

    def isomorphic(self, other):
        # TODO: this is only an approximation.
        if len(self) != len(other):
            return False
        for s, p, o in self:
            if not isinstance(s, BNode) and not isinstance(o, BNode):
                if not (s, p, o) in other:
                    return False
        for s, p, o in other:
            if not isinstance(s, BNode) and not isinstance(o, BNode):
                if not (s, p, o) in self:
                    return False
        # TODO: very well could be a false positive at this point yet.
        return True

    def connected(self):
        """Check if the Graph is connected

        The Graph is considered undirectional.

        Performs a search on the Graph, starting from a random node. Then
        iteratively goes depth-first through the triplets where the node is
        subject and object. Return True if all nodes have been visited and
        False if it cannot continue and there are still unvisited nodes left.
        """
        all_nodes = list(self.all_nodes())
        discovered = []

        # take a random one, could also always take the first one, doesn't
        # really matter.
        visiting = [all_nodes[random.randrange(len(all_nodes))]]
        while visiting:
            x = visiting.pop()
            if x not in discovered:
                discovered.append(x)
            for new_x in self.objects(subject=x):
                if new_x not in discovered and new_x not in visiting:
                    visiting.append(new_x)
            for new_x in self.subjects(object=x):
                if new_x not in discovered and new_x not in visiting:
                    visiting.append(new_x)

        # optimisation by only considering length, since no new objects can
        # be introduced anywhere.
        if len(all_nodes) == len(discovered):
            return True
        else:
            return False

    def all_nodes(self):
        obj = set(self.objects())
        allNodes = obj.union(set(self.subjects()))
        return allNodes


class ConjunctiveGraph(Graph):

    def __init__(self, store='default', identifier=None):
        super(ConjunctiveGraph, self).__init__(store)
        assert self.store.context_aware, ("ConjunctiveGraph must be backed by"
                                          " a context aware store.")
        self.context_aware = True
        self.default_context = Graph(store=self.store,
                                     identifier=identifier or BNode())

    def __str__(self):
        pattern = ("[a rdflib:ConjunctiveGraph;rdflib:storage "
                   "[a rdflib:Store;rdfs:label '%s']]")
        return pattern % self.store.__class__.__name__

    def add(self, (s, p, o)):
        """Add the triple to the default context"""
        self.store.add((s, p, o), context=self.default_context, quoted=False)

    def addN(self, quads):
        """Add a sequence of triple with context"""
        self.store.addN(quads)

    def remove(self, (s, p, o)):
        """Removes from all its contexts"""
        self.store.remove((s, p, o), context=None)

    def triples(self, (s, p, o)):
        """Iterate over all the triples in the entire conjunctive graph"""
        for (s, p, o), cg in self.store.triples((s, p, o), context=None):
            yield s, p, o

    def quads(self,(s,p,o)):
        """Iterate over all the quads in the entire conjunctive graph"""
        for (s, p, o), cg in self.store.triples((s, p, o), context=None):
            for ctx in cg:
                yield s, p, o, ctx
            
    def triples_choices(self, (s, p, o)):
        """Iterate over all the triples in the entire conjunctive graph"""
        for (s1, p1, o1), cg in self.store.triples_choices((s, p, o),
                                                           context=None):
            yield (s1, p1, o1)

    def __len__(self):
        """Number of triples in the entire conjunctive graph"""
        return self.store.__len__()

    def contexts(self, triple=None):
        """Iterate over all contexts in the graph

        If triple is specified, iterate over all contexts the triple is in.
        """
        for context in self.store.contexts(triple):
            yield context

    def remove_context(self, context):
        """Removes the given context from the graph"""
        self.store.remove((None, None, None), context)

    def context_id(self, uri, context_id=None):
        """URI#context"""
        uri = uri.split("#", 1)[0]
        if context_id is None:
            context_id = "#context"
        return URIRef(context_id, base=uri)

    def parse(self, source, publicID=None, format="xml", **args):
        """Parse source into Graph into it's own context (sub graph)

        Format defaults to xml (AKA rdf/xml). The publicID argument is for
        specifying the logical URI for the case that it's different from the
        physical source URI. Returns the context into which the source was
        parsed. In the case of n3 it returns the root context.
        """
        source = self.prepare_input_source(source, publicID)
        id = self.context_id(self.absolutize(source.getPublicId()))
        context = Graph(store=self.store, identifier=id)
        context.remove((None, None, None))
        context.parse(source, publicID=publicID, format=format, **args)
        return context

    def __reduce__(self):
        return (ConjunctiveGraph, (self.store, self.identifier))


class QuotedGraph(Graph):

    def __init__(self, store, identifier):
        super(QuotedGraph, self).__init__(store, identifier)

    def add(self, triple):
        """Add a triple with self as context"""
        self.store.add(triple, self, quoted=True)

    def addN(self,quads):
        """Add a sequence of triple with context"""
        self.store.addN([(s,p,o,c) for s,p,o,c in quads
                                   if isinstance(c, QuotedGraph)
                                   and c.identifier is self.identifier])

    def n3(self):
        """Return an n3 identifier for the Graph"""
        return "{%s}" % self.identifier.n3()

    def __str__(self):
        identifier = self.identifier.n3()
        label = self.store.__class__.__name__
        pattern = ("{this rdflib.identifier %s;rdflib:storage "
                   "[a rdflib:Store;rdfs:label '%s']}")
        return pattern % (identifier, label)

    def __reduce__(self):
        return (QuotedGraph, (self.store, self.identifier))


class GraphValue(QuotedGraph):
    def __init__(self, store, identifier=None, graph=None):
        if graph is not None:
            assert identifier is None
            np = store.node_pickler
            identifier = md5()
            s = list(graph.triples((None, None, None)))
            s.sort()
            for t in s:
                identifier.update("^".join((np.dumps(i) for i in t)))
            identifier = URIRef("data:%s" % identifier.hexdigest())
            super(GraphValue, self).__init__(store, identifier)
            for t in graph:
                store.add(t, context=self)
        else:
            super(GraphValue, self).__init__(store, identifier)


    def add(self, triple):
        raise Exception("not mutable")

    def remove(self, triple):
        raise Exception("not mutable")

    def __reduce__(self):
        return (GraphValue, (self.store, self.identifier,))


class Seq(object):
    """Wrapper around an RDF Seq resource

    It implements a container type in Python with the order of the items
    returned corresponding to the Seq content. It is based on the natural
    ordering of the predicate names _1, _2, _3, etc, which is the
    'implementation' of a sequence in RDF terms.
    """

    def __init__(self, graph, subject):
        """Parameters:

        - graph:
            the graph containing the Seq

        - subject:
            the subject of a Seq. Note that the init does not
            check whether this is a Seq, this is done in whoever
            creates this instance!
        """

        _list = self._list = list()
        LI_INDEX = RDF.RDFNS["_"]
        for (p, o) in graph.predicate_objects(subject):
            if p.startswith(LI_INDEX): #!= RDF.Seq: #
                i = int(p.replace(LI_INDEX, ''))
                _list.append((i, o))

        # here is the trick: the predicates are _1, _2, _3, etc. Ie,
        # by sorting the keys (by integer) we have what we want!
        _list.sort()

    def __iter__(self):
        """Generator over the items in the Seq"""
        for _, item in self._list:
            yield item

    def __len__(self):
        """Length of the Seq"""
        return len(self._list)

    def __getitem__(self, index):
        """Item given by index from the Seq"""
        index, item = self._list.__getitem__(index)
        return item


class BackwardCompatGraph(ConjunctiveGraph):

    def __init__(self, backend='default'):
        warnings.warn("Use ConjunctiveGraph instead. "
                      "( from rdflib.Graph import ConjunctiveGraph )",
                      DeprecationWarning, stacklevel=2)
        super(BackwardCompatGraph, self).__init__(store=backend)

    def __get_backend(self):
        return self.store
    backend = property(__get_backend)

    def open(self, configuration, create=True):
        return ConjunctiveGraph.open(self, configuration, create)

    def add(self, (s, p, o), context=None):
        """Add to to the given context or to the default context"""
        if context is not None:
            c = self.get_context(context)
            assert c.identifier == context, "%s != %s" % (c.identifier, context)
        else:
            c = self.default_context
        self.store.add((s, p, o), context=c, quoted=False)

    def remove(self, (s, p, o), context=None):
        """Remove from the given context or from the default context"""
        if context is not None:
            context = self.get_context(context)
        self.store.remove((s, p, o), context)

    def triples(self, (s, p, o), context=None):
        """Iterate over all the triples in the entire graph"""
        if context is not None:
            c = self.get_context(context)
            assert c.identifier == context
        else:
            c = None
        for (s, p, o), cg in self.store.triples((s, p, o), c):
            yield (s, p, o)

    def __len__(self, context=None):
        """Number of triples in the entire graph"""
        if context is not None:
            context = self.get_context(context)
        return self.store.__len__(context)

    def get_context(self, identifier, quoted=False):
        """Return a context graph for the given identifier

        identifier must be a URIRef or BNode.
        """
        assert isinstance(identifier, URIRef) or \
               isinstance(identifier, BNode), type(identifier)
        if quoted:
            assert False
            return QuotedGraph(self.store, identifier)
            #return QuotedGraph(self.store, Graph(store=self.store,
            #                                     identifier=identifier))
        else:
            return Graph(store=self.store, identifier=identifier,
                         namespace_manager=self)
            #return Graph(self.store, Graph(store=self.store,
            #                               identifier=identifier))

    def remove_context(self, context):
        """Remove the given context from the graph"""
        self.store.remove((None, None, None), self.get_context(context))

    def contexts(self, triple=None):
        """Iterate over all contexts in the graph

        If triple is specified, iterate over all contexts the triple is in.
        """
        for context in self.store.contexts(triple):
            yield context.identifier

    def subjects(self, predicate=None, object=None, context=None):
        """Generate subjects with the given predicate and object"""
        for s, p, o in self.triples((None, predicate, object), context):
            yield s

    def predicates(self, subject=None, object=None, context=None):
        """Generate predicates with the given subject and object"""
        for s, p, o in self.triples((subject, None, object), context):
            yield p

    def objects(self, subject=None, predicate=None, context=None):
        """Generate objects with the given subject and predicate"""
        for s, p, o in self.triples((subject, predicate, None), context):
            yield o

    def subject_predicates(self, object=None, context=None):
        """Generate (subject, predicate) tuples for the given object"""
        for s, p, o in self.triples((None, None, object), context):
            yield s, p

    def subject_objects(self, predicate=None, context=None):
        """Generate (subject, object) tuples for the given predicate"""
        for s, p, o in self.triples((None, predicate, None), context):
            yield s, o

    def predicate_objects(self, subject=None, context=None):
        """Generate (predicate, object) tuples for the given subject"""
        for s, p, o in self.triples((subject, None, None), context):
            yield p, o

    def __reduce__(self):
        return (BackwardCompatGraph, (self.store, self.identifier))

    def save(self, destination, format="xml", base=None, encoding=None):
        warnings.warn("Use serialize method instead. ",
                      DeprecationWarning, stacklevel=2)
        self.serialize(destination=destination, format=format, base=base,
                       encoding=encoding)

class ModificationException(Exception):

    def __init__(self):
        pass

    def __str__(self):
        return ("Modifications and transactional operations not allowed on "
                "ReadOnlyGraphAggregate instances")

class UnSupportedAggregateOperation(Exception):

    def __init__(self):
        pass

    def __str__(self):
        return ("This operation is not supported by ReadOnlyGraphAggregate "
                "instances")

class ReadOnlyGraphAggregate(ConjunctiveGraph):
    """Utility class for treating a set of graphs as a single graph

    Only read operations are supported (hence the name). Essentially a
    ConjunctiveGraph over an explicit subset of the entire store.
    """

    def __init__(self, graphs,store='default'):
        if store is not None:
            super(ReadOnlyGraphAggregate, self).__init__(store)
        assert isinstance(graphs, list) and graphs\
               and [g for g in graphs if isinstance(g, Graph)],\
               "graphs argument must be a list of Graphs!!"
        self.graphs = graphs

    def __repr__(self):
        return "<ReadOnlyGraphAggregate: %s graphs>" % len(self.graphs)

    def destroy(self, configuration):
        raise ModificationException()

    #Transactional interfaces (optional)
    def commit(self):
        raise ModificationException()

    def rollback(self):
        raise ModificationException()

    def open(self, configuration, create=False):
        # TODO: is there a use case for this method?
        for graph in self.graphs:
            graph.open(self, configuration, create)

    def close(self):
        for graph in self.graphs:
            graph.close()

    def add(self, (s, p, o)):
        raise ModificationException()

    def addN(self, quads):
        raise ModificationException()

    def remove(self, (s, p, o)):
        raise ModificationException()

    def triples(self, (s, p, o)):
        for graph in self.graphs:
            for s1, p1, o1 in graph.triples((s, p, o)):
                yield (s1, p1, o1)

    def quads(self,(s,p,o)):
        """Iterate over all the quads in the entire aggregate graph"""
        for graph in self.graphs:
            for s1, p1, o1 in graph.triples((s, p, o)):
                yield (s1, p1, o1, graph)

    def __len__(self):
        return reduce(lambda x, y: x + y, [len(g) for g in self.graphs])

    def __hash__(self):
        raise UnSupportedAggregateOperation()

    def __cmp__(self, other):
        if other is None:
            return -1
        elif isinstance(other, Graph):
            return -1
        elif isinstance(other, ReadOnlyGraphAggregate):
            return cmp(self.graphs, other.graphs)
        else:
            return -1

    def __iadd__(self, other):
        raise ModificationException()

    def __isub__(self, other):
        raise ModificationException()

    # Conv. methods

    def triples_choices(self, (subject, predicate, object_), context=None):
        for graph in self.graphs:
            choices = graph.triples_choices((subject, predicate, object_))
            for (s, p, o) in choices:
                yield (s, p, o)

    def qname(self, uri):
        raise UnSupportedAggregateOperation()

    def compute_qname(self, uri):
        raise UnSupportedAggregateOperation()

    def bind(self, prefix, namespace, override=True):
        raise UnSupportedAggregateOperation()

    def namespaces(self):
        if hasattr(self,'namespace_manager'):
            for prefix, namespace in self.namespace_manager.namespaces():
                yield prefix, namespace
        else:
            for graph in self.graphs:
                for prefix, namespace in graph.namespaces():
                    yield prefix, namespace

    def absolutize(self, uri, defrag=1):
        raise UnSupportedAggregateOperation()

    def parse(self, source, publicID=None, format="xml", **args):
        raise ModificationException()

    def n3(self):
        raise UnSupportedAggregateOperation()

    def __reduce__(self):
        raise UnSupportedAggregateOperation()


def test():
    import doctest
    doctest.testmod()

if __name__ == '__main__':
    test()

########NEW FILE########
__FILENAME__ = Identifier
from rdflib.Node import Node

class Identifier(Node,unicode): # we allow Identifiers to be Nodes in our Graph
    """
    See http://www.w3.org/2002/07/rdf-identifer-terminology/
    regarding choice of terminology.
    """
    __slots__ = ()
    def __new__(cls, value):
        return unicode.__new__(cls,value)

########NEW FILE########
__FILENAME__ = interfaces

try:
    from zope.interface import Interface, classImplements, implements
except ImportError:
    class Interface(object): pass
    def classImplements(c, i): pass
    def implements(*args): pass

from rdflib import RDF

class IGraph(Interface):
    """\
    An rdflib.Graph indexes data expressed in the Resource Description
    Framework (RDF).  Any kind of content, whether inside Zope or from
    some outside source, can be cataloged if it can describe itself
    using the RDF standard.  Any kind of RDF vocabulary like RSS, OWL,
    DAML+OIL, Dublin Core, or any kind of XML schema or data can be
    expressed into the graph.

    Once data is graphed it can be queried using either the Python
    query interface, a TALES-based RDF query expression language, or
    the sparql rdf query language.  Results of a query can be either a
    generator of result records or RDF in xml or NT format.

    In Semantic Web terms, a graph is a persistent triple store.  RDF
    is broken down into subject, predicate, and object relations
    (called triples) and each relation is indexed.  The triple store
    can then be queried for triples that match patterns.
    """

    def parse(rdf, format="xml"):
        """ Parse RDF-XML into the catalog. """

    def add((subject, predicate, object)):
        """ Add one triple to the catalog.  """

    def remove((subject, predicate, object)):
        """ Remove one triple from the catalog. """

    def triples((subject, predicate, object), *args):
        """ Query the triple store. """

    def contexts(triple=None):
        """ Generator over all contexts in the graph. If triple is
        specified, a generator over all contexts the triple is in."""

    def value(subject, predicate=RDF.value, object=None, default=None, any=False):
        """ Get a value for a subject/predicate, predicate/object, or
        subject/object pair -- exactly one of subject, predicate,
        object must be None. Useful if one knows that there may only
        be one value.

        It is one of those situations that occur a lot, hence this
        'macro' like utility

        Parameters:
        -----------
        subject, predicate, object  -- exactly one must be None
        default -- value to be returned if no values found
        any -- if True:
                 return any value in the case there is more than one
               else:
                 raise UniquenessError
        """

    def label(subject, default=''):
        """ Queries for the RDFS.label of the subject, returns default
        if no label exists."""

    def comment(subject, default=''):
        """ Queries for the RDFS.comment of the subject, returns
        default if no comment exists."""

    def items(list):
        """Generator over all items in the resource specified by list
        (an RDF collection)"""

    def __iter__():
        """ Iterates over all triples in the store."""

    def __contains__(triple):
        """ Support for 'triple in graph' syntax."""

    def __len__(context=None):
        """ Returns the number of triples in the graph. If context is
        specified then the number of triples in the context is
        returned instead."""

    def __eq__(other):
        """ Test if Graph is exactly equal to Graph other."""

    def __iadd__(other):
        """ Add all triples in Graph other to Graph."""

    def __isub__(other):
        """ Subtract all triples in Graph other from Graph."""

    def subjects(predicate=None, object=None):
        """ A generator of subjects with the given predicate and
        object."""

    def predicates(subject=None, object=None):
        """ A generator of predicates with the given subject and
        object."""

    def objects(subject=None, predicate=None):
        """ A generator of objects with the given subject and
        predicate."""

    def subject_predicates(object=None):
        """ A generator of (subject, predicate) tuples for the given
        object"""

    def subject_objects(predicate=None):
        """ A generator of (subject, object) tuples for the given
        predicate"""

    def predicate_objects(subject=None):
        """ A generator of (predicate, object) tuples for the given
        subject"""

    def get_context(identifier):
        """ Returns a Context graph for the given identifier, which
        must be a URIRef or BNode."""

    def remove_context(identifier):
        """ Removes the given context from the graph. """

    def transitive_objects(subject, property, remember=None):
        """ """

    def transitive_subjects(predicate, object, remember=None):
        """ """

    def load(location, publicID=None, format="xml"):
        """ for b/w compat. See parse."""

    def save(location, format="xml", base=None, encoding=None):
        """ for b/x compat. See serialize."""

    def context_id(uri):
        pass

    def parse(source, publicID=None, format="xml"):
        """ Parse source into Graph. If Graph is context-aware it'll
        get loaded into it's own context (sub graph). Format defaults
        to xml (AKA rdf/xml). The publicID argument is for specifying
        the logical URI for the case that it's different from the
        physical source URI. Returns the context into which the source
        was parsed."""

    def serialize(destination=None, format="xml", base=None, encoding=None):
        """ Serialize the Graph to destination. If destination is None
        serialize method returns the serialization as a string. Format
        defaults to xml (AKA rdf/xml)."""

    def seq(subject):
        """
        Check if subject is an rdf:Seq. If yes, it returns a Seq
        class instance, None otherwise.
        """

    def absolutize(uri, defrag=1):
        """ Will turn uri into an absolute URI if it's not one already. """

    def bind(prefix, namespace, override=True):
        """Bind prefix to namespace. If override is True will bind
        namespace to given prefix if namespace was already bound to a
        different prefix."""

    def namespaces():
        """Generator over all the prefix, namespace tuples.
        """

class IIdentifier(Interface):

    def n3():
        """ Return N3 representation of identifier. """

    def startswith(string):
        """ dummy. """

    def __cmp__(other):
        """ dummy. """

########NEW FILE########
__FILENAME__ = Journal
import logging

_logger = logging.getLogger(__name__)

from rdflib.Graph import QuotedGraph
from rdflib.events import Event, Dispatcher
from rdflib.store import TripleAddedEvent, TripleRemovedEvent, StoreCreatedEvent


class JournalWriter(object):
    """
    Writes a journal of the store events.
    """

    def __init__(self, store, stream=None, filename=None):
        if stream is None:
            assert filename, "Must specify either stream or filename"
            stream = file(filename, "ab")
        dispatcher = store.dispatcher
        dispatcher.subscribe(TripleAddedEvent, self.journal_event)
        dispatcher.subscribe(TripleRemovedEvent, self.journal_event)
        dispatcher.subscribe(StoreCreatedEvent, self.journal_event)
        self._dumps = store.node_pickler.dumps
        self._write = stream.write

    def journal_event(self, event):
        self._write(self._dumps(event))
        self._write("\n\n")


class JournalReader(object):
    """
    Reads a journal of store events into a store.
    """

    def __init__(self, store, filename):
        self.stream = file(filename, "rb")
        self.store = store
        dispatcher = Dispatcher()
        dispatcher.subscribe(TripleAddedEvent, self.add)
        dispatcher.subscribe(TripleRemovedEvent, self.remove)
        dispatcher.subscribe(StoreCreatedEvent, self.store_created)
        loads = store.node_pickler.loads
        dispatch = dispatcher.dispatch
        lines = []
        for line in self.stream:
            if line=="\n":
                try:
                    event = loads("".join(lines))
                    dispatch(event)
                    lines = []
                except Exception, e:
                    _logger.exception(e)
                    _logger.debug("lines: '%s'" % lines)
                    lines = []
            else:
                lines.append(line)

    def add(self, event):
        context = event.context
        quoted = isinstance(context, QuotedGraph)
        self.store.add(event.triple, context, quoted)

    def remove(self, event):
        self.store.remove(event.triple, event.context)
        
    def store_created(self, event):
        n = len(self.store)
        if n>0:
            _logger.warning("Store not empty for 'store created'. Contains '%s' assertions" % n)
        # TODO: clear store

########NEW FILE########
__FILENAME__ = Literal
from rdflib.Identifier import Identifier
from rdflib.URIRef import URIRef
from rdflib.Namespace import Namespace
from rdflib.exceptions import Error
from datetime import date,time,datetime
from time import strptime
import base64

try:
    from hashlib import md5
except ImportError:
    from md5 import md5    

import logging

_logger = logging.getLogger(__name__)

class Literal(Identifier):
    """
    RDF Literal: http://www.w3.org/TR/rdf-concepts/#section-Graph-Literal

    >>> Literal(1).toPython()
    1L
    >>> cmp(Literal("adsf"), 1)
    1
    >>> lit2006 = Literal('2006-01-01',datatype=_XSD_NS.date)
    >>> lit2006.toPython()
    datetime.date(2006, 1, 1)
    >>> lit2006 < Literal('2007-01-01',datatype=_XSD_NS.date)
    True
    >>> oneInt     = Literal(1)
    >>> twoInt     = Literal(2)
    >>> twoInt < oneInt
    False
    >>> Literal('1') < Literal(1)
    False
    >>> Literal('1') < Literal('1')
    False
    >>> Literal(1) < Literal('1')
    True
    >>> Literal(1) < Literal(2.0)
    True
    >>> Literal(1) < URIRef('foo')
    True
    >>> Literal(1) < 2.0
    True
    >>> Literal(1) < object  
    True
    >>> lit2006 < "2007"
    True
    >>> "2005" < lit2006
    True
    """

    __slots__ = ("language", "datatype", "_cmp_value")

    def __new__(cls, value, lang=None, datatype=None):
        if datatype:
            lang = None
        else:
            value,datatype = _castPythonToLiteral(value)
            if datatype:
                lang = None
        if datatype:
            datatype = URIRef(datatype)
        try:
            inst = unicode.__new__(cls,value)
        except UnicodeDecodeError:
            inst = unicode.__new__(cls,value,'utf-8')
        inst.language = lang
        inst.datatype = datatype
        inst._cmp_value = inst._toCompareValue()
        return inst

    def __reduce__(self):
        return (Literal, (unicode(self), self.language, self.datatype),)

    def __getstate__(self):
        return (None, dict(language=self.language, datatype=self.datatype))

    def __setstate__(self, arg):
        _, d = arg
        self.language = d["language"]
        self.datatype = d["datatype"]

    def __add__(self, val):
        """
        >>> Literal(1) + 1
        2L
        >>> Literal("1") + "1"
        rdflib.Literal('11', language=None, datatype=None)
        """

        py = self.toPython()
        if isinstance(py, Literal):
            s = super(Literal, self).__add__(val)            
            return Literal(s, self.language, self.datatype)
        else:
            return py + val 


    
    def __lt__(self, other):
        if other is None:
            return False # Nothing is less than None
        try:
            return self._cmp_value < other
        except TypeError, te:
            return unicode(self._cmp_value) < other

    def __le__(self, other):
        if other is None:
            return False
        if self==other:
            return True
        else:
            return self < other

    def __gt__(self, other):
        if other is None:
            return True # Everything is greater than None
        try:
            return self._cmp_value > other
        except TypeError, te:
            return unicode(self._cmp_value) > other

    def __ge__(self, other):
        if other is None:
            return False
        if self==other:
            return True
        else:
            return self > other

    def __ne__(self, other):
        """
        Overriden to ensure property result for comparisons with None via !=.
        Routes all other such != and <> comparisons to __eq__
        
        >>> Literal('') != None
        True
        >>> Literal('2') <> Literal('2')
        False
         
        """
        if other is None:
            return True
        else:
            return not self.__eq__(other)

    def __eq__(self, other):
        """        
        >>> f = URIRef("foo")
        >>> f is None or f == ''
        False
        >>> Literal("1", datatype=URIRef("foo")) == Literal("1", datatype=URIRef("foo"))
        True
        >>> Literal("1", datatype=URIRef("foo")) == Literal("2", datatype=URIRef("foo"))
        False
        >>> Literal("1", datatype=URIRef("foo")) == "asdf"
        False
        >>> oneInt     = Literal(1)
        >>> oneNoDtype = Literal('1')
        >>> oneInt == oneNoDtype
        False
        >>> Literal("1",_XSD_NS[u'string']) == Literal("1",_XSD_NS[u'string']) 
        True
        >>> Literal("one",lang="en") == Literal("one",lang="en")
        True
        >>> Literal("hast",lang='en') == Literal("hast",lang='de')
        False
        >>> oneInt == Literal(1)
        True
        >>> oneFloat   = Literal(1.0)
        >>> oneInt == oneFloat
        True
        >>> oneInt == 1
        True
        """
        if other is None:
            return False
        else:
            return self._cmp_value==other

    def n3(self):
        language = self.language
        datatype = self.datatype
        # unfortunately this doesn't work: a newline gets encoded as \\n, which is ok in sourcecode, but we want \n
        #encoded = self.encode('unicode-escape').replace('\\', '\\\\').replace('"','\\"')
        #encoded = self.replace.replace('\\', '\\\\').replace('"','\\"')

        # TODO: We could also chose quotes based on the quotes appearing in the string, i.e. '"' and "'" ...

        # which is nicer?
        #if self.find("\"")!=-1 or self.find("'")!=-1 or self.find("\n")!=-1:
        if self.find("\n")!=-1:
            # Triple quote this string.
            encoded=self.replace('\\', '\\\\')
            if self.find('"""')!=-1: 
                # is this ok?
                encoded=encoded.replace('"""','\\"""')
            if encoded.endswith('"'): encoded=encoded[:-1]+"\\\""
            encoded='"""%s"""'%encoded
        else: 
            encoded='"%s"'%self.replace('\n','\\n').replace('\\', '\\\\').replace('"','\\"')
        if language:
            if datatype:    
                return '%s@%s^^<%s>' % (encoded, language, datatype)
            else:
                return '%s@%s' % (encoded, language)
        else:
            if datatype:
                return '%s^^<%s>' % (encoded, datatype)
            else:
                return '%s' % encoded

    def __str__(self):
        return self.encode("unicode-escape")

    def __repr__(self):
        return """rdflib.Literal('%s', language=%s, datatype=%s)""" % (str(self), repr(self.language), repr(self.datatype))

    def toPython(self):
        """
        Returns an appropriate python datatype derived from this RDF Literal
        """
        convFunc = _toPythonMapping.get(self.datatype, None)
        
        if convFunc:
            rt = convFunc(self)
        else:
            rt = self
        return rt

    def _toCompareValue(self):
        try:
            rt = self.toPython()
        except Exception, e:
            _logger.warning("could not convert %s to a Python datatype" % repr(self))
            rt = self
                
        if rt is self:
            if self.language is None and self.datatype is None:
                return unicode(rt)
            else:
                return (unicode(rt), rt.datatype, rt.language)
        return rt

    def md5_term_hash(self):
        d = md5(str(self))
        d.update("L")
        return d.hexdigest()


_XSD_NS = Namespace(u'http://www.w3.org/2001/XMLSchema#')

#Casts a python datatype to a tuple of the lexical value and a datatype URI (or None)
def _castPythonToLiteral(obj):
    for pType,(castFunc,dType) in _PythonToXSD.items():
        if isinstance(obj,pType):
            if castFunc:
                return castFunc(obj),dType
            elif dType:
                return obj,dType
            else:
                return obj,None
    return obj, None # TODO: is this right for the fall through case?

#Mappings from Python types to XSD datatypes and back (burrowed from sparta)
_PythonToXSD = {
    basestring : (None,None),
    float      : (None,_XSD_NS[u'float']),
    int        : (None,_XSD_NS[u'int']),
    long       : (None,_XSD_NS[u'long']),
    bool       : (None,_XSD_NS[u'boolean']),
    date       : (lambda i:i.isoformat(),_XSD_NS[u'date']),
    time       : (lambda i:i.isoformat(),_XSD_NS[u'time']),
    datetime   : (lambda i:i.isoformat(),_XSD_NS[u'dateTime']),
}

def _strToTime(v) :
    return strptime(v,"%H:%M:%S")

def _strToDate(v) :
    tstr = strptime(v,"%Y-%m-%d")
    return date(tstr.tm_year,tstr.tm_mon,tstr.tm_mday)

def _strToDateTime(v) :
    """
    Attempt to cast to datetime, or just return the string (otherwise)
    """
    try:
        tstr = strptime(v,"%Y-%m-%dT%H:%M:%S")
    except:
        try:
            tstr = strptime(v,"%Y-%m-%dT%H:%M:%SZ")
        except:
            try:
                tstr = strptime(v,"%Y-%m-%dT%H:%M:%S%Z")
            except:
                return v

    return datetime(tstr.tm_year,tstr.tm_mon,tstr.tm_mday,tstr.tm_hour,tstr.tm_min,tstr.tm_sec)

XSDToPython = {
    _XSD_NS[u'time']               : _strToTime,
    _XSD_NS[u'date']               : _strToDate,
    _XSD_NS[u'dateTime']           : _strToDateTime,
    _XSD_NS[u'string']             : None,
    _XSD_NS[u'normalizedString']   : None,
    _XSD_NS[u'token']              : None,
    _XSD_NS[u'language']           : None,
    _XSD_NS[u'boolean']            : lambda i:i.lower() in ['1','true'],
    _XSD_NS[u'decimal']            : float,
    _XSD_NS[u'integer']            : long,
    _XSD_NS[u'nonPositiveInteger'] : int,
    _XSD_NS[u'long']               : long,
    _XSD_NS[u'nonNegativeInteger'] : int,
    _XSD_NS[u'negativeInteger']    : int,
    _XSD_NS[u'int']                : long,
    _XSD_NS[u'unsignedLong']       : long,
    _XSD_NS[u'positiveInteger']    : int,
    _XSD_NS[u'short']              : int,
    _XSD_NS[u'unsignedInt']        : long,
    _XSD_NS[u'byte']               : int,
    _XSD_NS[u'unsignedShort']      : int,
    _XSD_NS[u'unsignedByte']       : int,
    _XSD_NS[u'float']              : float,
    _XSD_NS[u'double']             : float,
    _XSD_NS[u'base64Binary']       : base64.decodestring,
    _XSD_NS[u'anyURI']             : None,
}

_toPythonMapping = {}
_toPythonMapping.update(XSDToPython)

def bind(datatype, conversion_function):
    """bind a datatype to a function for converting it into a Python instance."""
    if datatype in _toPythonMapping:
        _logger.warning("datatype '%s' was already bound. Rebinding." % datatype)
    _toPythonMapping[datatype] = conversion_function



def test():
    import doctest
    doctest.testmod()

if __name__ == '__main__':
    test()

########NEW FILE########
__FILENAME__ = Namespace
from rdflib.URIRef import URIRef

import logging

_logger = logging.getLogger(__name__)


class Namespace(URIRef):

    def term(self, name):
        return URIRef(self + name)

    def __getitem__(self, key, default=None):
        return self.term(key)

    def __getattr__(self, name):
        if name.startswith("__"): # ignore any special Python names!
            raise AttributeError
        else:
            return self.term(name)


class NamespaceDict(dict):

    def __new__(cls, uri=None, context=None):
        inst = dict.__new__(cls)
        inst.uri = uri # TODO: do we need to set these both here and in __init__ ??
        inst.__context = context
        return inst

    def __init__(self, uri, context=None):
        self.uri = uri
        self.__context = context

    def term(self, name):
        uri = self.get(name)
        if uri is None:
            uri = URIRef(self.uri + name)
            if self.__context and (uri, None, None) not in self.__context:
                _logger.warning("%s not defined" % uri)
            self[name] = uri
        return uri 

    def __getattr__(self, name):
        return self.term(name)

    def __getitem__(self, key, default=None):
        return self.term(key) or default

    def __str__(self):
        return self.uri

    def __repr__(self):
        return """rdflib.NamespaceDict('%s')""" % str(self.uri)


########NEW FILE########
__FILENAME__ = Node
class Node(object):
    """
    A Node in the Graph.
    """
    __slots__ = ()

########NEW FILE########
__FILENAME__ = plugin
from rdflib.store import Store
from rdflib.syntax import serializer, serializers
from rdflib.syntax import parsers
from rdflib import sparql
from rdflib.QueryResult import QueryResult

_kinds = {}
_adaptors = {}

def register(name, kind, module_path, class_name):
    _module_info = _kinds.get(kind, None)
    if _module_info is None:
        _module_info = _kinds[kind] = {}
    _module_info[name] = (module_path, class_name)

def get(name, kind):
    _module_info = _kinds.get(kind)
    if _module_info and name in _module_info:
        module_path, class_name = _module_info[name]
        module = __import__(module_path, globals(), locals(), True)
        return getattr(module, class_name)
    else:
        Adaptor = kind # TODO: look up of adaptor, for now just use kind
        try:
            Adaptee = get(name, _adaptors[kind])
        except Exception, e:
            raise Exception("could not get plugin for %s, %s: %s" % (name, kind, e))
        def const(*args, **keywords):
            return Adaptor(Adaptee(*args, **keywords))
        return const

def register_adaptor(adaptor, adaptee):
    _adaptors[adaptor] = adaptee


register_adaptor(serializer.Serializer, serializers.Serializer)
#register_adaptor(parser.Parser, parsers.Parser)


register('rdf', serializers.Serializer,
         'rdflib.syntax.serializers.XMLSerializer', 'XMLSerializer')

register('xml', serializers.Serializer,
         'rdflib.syntax.serializers.XMLSerializer', 'XMLSerializer')

register('rdf/xml', serializers.Serializer,
         'rdflib.syntax.serializers.XMLSerializer', 'XMLSerializer')

register('pretty-xml', serializers.Serializer,
         'rdflib.syntax.serializers.PrettyXMLSerializer', 'PrettyXMLSerializer')

register('nt', serializers.Serializer,
         'rdflib.syntax.serializers.NTSerializer', 'NTSerializer')

register('turtle', serializers.Serializer,
         'rdflib.syntax.serializers.TurtleSerializer', 'TurtleSerializer')

register('n3', serializers.Serializer,
         'rdflib.syntax.serializers.N3Serializer', 'N3Serializer')

register('xml', parsers.Parser,
         'rdflib.syntax.parsers.RDFXMLParser', 'RDFXMLParser')

register('trix', parsers.Parser,
         'rdflib.syntax.parsers.TriXParser', 'TriXParser')

register('n3', parsers.Parser,
         'rdflib.syntax.parsers.N3Parser', 'N3Parser')

register('notation3', parsers.Parser,
         'rdflib.syntax.parsers.N3Parser', 'N3Parser')

register('nt', parsers.Parser,
         'rdflib.syntax.parsers.NTParser', 'NTParser')

register('n3', parsers.Parser,
         'rdflib.syntax.parsers.N3Parser', 'N3Parser')

register('rdfa', parsers.Parser,
         'rdflib.syntax.parsers.RDFaParser', 'RDFaParser')

register('default', Store,
         'rdflib.store.IOMemory', 'IOMemory')

register('IOMemory', Store,
         'rdflib.store.IOMemory', 'IOMemory')

register('Memory', Store,
         'rdflib.store.Memory', 'Memory')

register('Sleepycat', Store,
         'rdflib.store.Sleepycat', 'Sleepycat')

register('BerkeleyDB', Store,
         'rdflib.store.BerkeleyDB', 'BerkeleyDB')

register('MySQL', Store,
         'rdflib.store.MySQL', 'MySQL')

register('SQLite', Store,
         'rdflib.store.SQLite', 'SQLite')

register('ZODB', Store,
         'rdflib.store.ZODB', 'ZODB')

register('sqlobject', Store,
         'rdflib.store._sqlobject', 'SQLObject')

register('Redland', Store,
         'rdflib.store.Redland', 'Redland')

register('MySQL', Store,
         'rdflib.store.MySQL', 'MySQL')

register("sparql", sparql.Processor,
         'rdflib.sparql.bison.Processor', 'Processor')

register("SPARQLQueryResult", QueryResult,
         'rdflib.sparql.QueryResult', 'SPARQLQueryResult')
         

########NEW FILE########
__FILENAME__ = QueryResult
class QueryResult(object):
    """
    A common class for representing query result in a variety of formats, namely:

    xml   : as an XML string using the XML result format of the query language
    python: as Python objects
    json  : as JSON
    """
    def __init__(self,pythonResult):
        self.rt = pythonResult

    def serialize(self,format='xml'):
        pass
########NEW FILE########
__FILENAME__ = RDF
from rdflib.Namespace import Namespace

RDFNS = Namespace("http://www.w3.org/1999/02/22-rdf-syntax-ns#")

# Syntax names
RDF = RDFNS["RDF"]
Description = RDFNS["Description"]
ID = RDFNS["ID"]
about = RDFNS["about"]
parseType = RDFNS["parseType"]
resource = RDFNS["resource"]
li = RDFNS["li"]
nodeID = RDFNS["nodeID"]
datatype = RDFNS["datatype"]

# RDF Classes
Seq = RDFNS["Seq"]
Bag = RDFNS["Bag"]
Alt = RDFNS["Alt"]
Statement = RDFNS["Statement"]
Property = RDFNS["Property"]
XMLLiteral = RDFNS["XMLLiteral"]
List = RDFNS["List"]

# RDF Properties
subject = RDFNS["subject"]
predicate = RDFNS["predicate"]
object = RDFNS["object"]
type = RDFNS["type"]
value = RDFNS["value"]
first = RDFNS["first"]
rest = RDFNS["rest"]
# and _n where n is a non-negative integer

# RDF Resources
nil = RDFNS["nil"]

########NEW FILE########
__FILENAME__ = RDFS
from rdflib.Namespace import Namespace

RDFSNS = Namespace("http://www.w3.org/2000/01/rdf-schema#")

Resource = RDFSNS["Resource"]
Class = RDFSNS["Class"]
subClassOf = RDFSNS["subClassOf"]
subPropertyOf = RDFSNS["subPropertyOf"]
comment = RDFSNS["comment"]
label = RDFSNS["label"]
domain = RDFSNS["domain"]
range = RDFSNS["range"]
seeAlso = RDFSNS["seeAlso"]
isDefinedBy = RDFSNS["isDefinedBy"]
Literal = RDFSNS["Literal"]
Container = RDFSNS["Container"]
ContainerMembershipProperty = RDFSNS["ContainerMembershipProperty"]
member = RDFSNS["member"]
Datatype = RDFSNS["Datatype"]


########NEW FILE########
__FILENAME__ = Bindings
from rdflib import URIRef, Namespace

class PrefixDeclaration(object):
    """
    PrefixDecl ::= 'PREFIX' QNAME_NS Q_IRI_REF
    See: http://www.w3.org/TR/rdf-sparql-query/#rPrefixDecl
    """
    def __init__(self,qName,iriRef):
        self.namespaceMapping = Namespace(iriRef)
        self.qName = qName[:-1]
        self.base = iriRef
        #print self.base,self.qName,self.namespaceMapping.knows

    def __repr__(self):
        return "%s -> %s"%(self.base,self.qName[:-1])

class BaseDeclaration(URIRef):
    """
    BaseDecl ::= 'BASE' Q_IRI_REF
    See: http://www.w3.org/TR/rdf-sparql-query/#rBaseDecl
    """
    pass
    
########NEW FILE########
__FILENAME__ = CompositionalEvaluation
"""
This module implements (with sparql-p) compositional forms and semantics as outlined in
the pair of Jorge Perez et. al papers:

- Semantics of SPARQL                (http://ing.utalca.cl/~jperez/papers/sparql_semantics.pdf)
- Semantics and Complexity of SPARQL (http://arxiv.org/abs/cs.DB/0605124)

It also implements rewrite rules expressed in the SPARQL Algebra:

- http://www.w3.org/TR/rdf-sparql-query/#sparqlAlgebra

Compositional Semantics (Jorge P. et. al syntax)

== Definition 3.7 (Set of Mappings and Operations) ==

Omega1 and Omega2 are sets of mappings

I.   Omega1  Omega2          = {1  2 | 1  Omega1, 2  Omega2 are compatible mappings } 
II.  Omega1  Omega2           = { | 1  Omega1 or 2  Omega2 }
III. Omega1 \ Omega2           = {1  Omega1 | for all   Omega2,  and  are not compatible }
IV.  LeftJoin1(Omega1, Omega2) = ( Omega1  Omega2 )  ( Omega1 \ Omega2 ) 

NOTE: sparql-p implements the notion of compatible mappings with the 'clash' attribute
defined on instances of _SPARQLNode (in the evaluation expansion tree)   

An RDF dataset is a set D = {G0, <u1,G1>,... <un,Gn>}

where G0, . . . ,Gn are RDF graphs, u1, . . . , un are IRIs, and n  0.

NOTE: A SPARQL RDF dataset is equivalent to an RDFLib ConjunctiveGraph so we introduce
a function rdflibDS(D) which returns the ConjunctiveGraph instance associated
with the dataset D

Every dataset D is equipped with a function dD such that
dD(u) = G if u,Gi  D and dD(u) =  otherwise

Let D be an RDF dataset and G an RDF graph in D

== Definition 3.9 (Graph Pattern Evaluation): ==

[[.]](D,G) Is the notation used to indicate the evaluation
of a graph pattern.  

I.  [[(P1 AND P2)]](D,G)   = [[P1]](D,G)  [[P2]](D,G)
II. [[(P1 UNION P2)]](D,G) = [[P1]](D,G)  [[P2]](D,G)
III.[[(P1 OPT P2)]](D,G)   = LeftJoin1([[P1]](D,G),[[P2]](D,G))  
IV. If u  I, then 
      [[(u GRAPH P)]](D,G)  = [[P]](D,dD(u))
    if ?X  V , then
      [[(?X GRAPH P)]](D,G) =
        [[P]](D,G)  { ?X -> rdflibDS(D).contexts(P) }
V. [[(P FILTER R)]](D,G) = {  [[P]](D,G) |  |= R}.

NOTE: RDFLib's ConjunctiveGraph.contexts method is used to append bindings
for GRAPH variables.  The FILTER semantics are implemented 'natively'
in sparql-p by python functions 

(http://dev.w3.org/cvsweb/2004/PythonLib-IH/Doc/sparqlDesc.html?rev=1.11#Constraini)

== Equivalence with SPARQL  Algebra (*from* DAWG SPARQL Algebra *to* Jorge.P et. al forms) ==

merge(1,2)             = 1  2
Join(Omega1,Omega2)      = Filter(R,Omega1  Omega2)
Filter(R,Omega)          = [[(P FILTER R)]](D,G)
Diff(Omega1,Omega2,R)    = (Omega1 \ Omega2)  { |  in Omega1  Omega2 and *not*  |= R} 
Union(Omega1,Omega2)     = Omega1  Omega2 

#LeftJoin(Omega1,Omega2,R)= Filter(R,Join(Omega1,Omega2))  Diff(Omega1,Omega2,R)

== Graph Pattern rewrites and reductions ==

[[{t1, t2, . . . , tn}]]D = [[({t1} AND {t2} AND    AND {tn})]]D

Proposition 3.13

The above proposition implies that it is equivalent to consider basic
graph patterns or triple patterns as the base case when defining SPARQL general graph patterns.    

== BGP reduction and Disjunctive Normal Forms or Union-Free BGP ==

Step 5 of http://www.w3.org/TR/rdf-sparql-query/#convertGraphPattern
    
Replace Join({}, A) by A
Replace Join(A, {}) by A

=== Disjunctive Normal Form of SPARQL Patterns ==

See: http://en.wikipedia.org/wiki/Disjunctive_normal_form

From Proposition 1 of 'Semantics and Complexity of SPARQL'

I.  (P1 AND (P2 UNION P3))  ((P1 AND P2) UNION (P1 AND P3))
II. (P1 OPT (P2 UNION P3))  ((P1 OPT P2) UNION (P1 OPT P3))
III.((P1 UNION P2) OPT P3)  ((P1 OPT P3) UNION (P2 OPT P3)) 
IV. ((P1 UNION P2) FILTER R)  ((P1 FILTER R) UNION (P2 FILTER R)) 

The application of the above equivalences permits to translate any graph pattern
into an equivalent one of the form:

P1 UNION P2 UNION P3 UNION ... UNION P

NOTE: sprarql-p SPARQL.query API is geared for evaluation of SPARQL patterns already in DNF:
 - http://dev.w3.org/cvsweb/~checkout~/2004/PythonLib-IH/Doc/Attic/pythondoc-sparql.html?rev=1.5&content-type=text/html;%20charset=iso-8859-1#sparql.SPARQL.query-method

"""

def ReduceToDNF(ggp,prolog):
    """
    From: Semantics of SPARQL
    
    [[{t1, t2, . . . , tn}]]D = [[({t1} AND {t2} AND    AND {tn})]]D
    
    Proposition 3.13
    
    The above proposition implies that it is equivalent to consider basic
    graph patterns or triple patterns as the base case when defining SPARQL general graph patterns.    
    """
    pass

def CompositionalEvaluate(tripleStore,ggp,prolog,passedBindings):
    pass
########NEW FILE########
__FILENAME__ = Expression
from Util import ListRedirect

class ParsedConditionalAndExpressionList(ListRedirect):
    """
    A list of ConditionalAndExpressions, joined by '||'
    """
    pyBooleanOperator = ' or '
    def __init__(self,conditionalAndExprList):
        if isinstance(conditionalAndExprList,list):
            self._list = conditionalAndExprList
        else:
            self._list = [conditionalAndExprList]

    def __repr__(self):
        return "<ConditionalExpressionList: %s>"%self._list

class ParsedRelationalExpressionList(ListRedirect):
    """
    A list of RelationalExpressions, joined by '&&'s
    """
    pyBooleanOperator = ' and '
    def __init__(self,relationalExprList):        
        if isinstance(relationalExprList,list):
            self._list = relationalExprList
        else:
            self._list = [relationalExprList]
    def __repr__(self):
        return "<RelationalExpressionList: %s>"%self._list

class ParsedPrefixedMultiplicativeExpressionList(ListRedirect):
    """
    A ParsedMultiplicativeExpressionList lead by a '+' or '-'
    """
    def __init__(self,prefix,mulExprList):
        self.prefix = prefix
        assert prefix != '-',"arithmetic '-' operator not supported"
        if isinstance(mulExprList,list):
            self._list = mulExprList
        else:
            self._list = [mulExprList]
    def __repr__(self):
        return "%s %s"%(self.prefix,self.reduce())

class ParsedMultiplicativeExpressionList(ListRedirect):
    """
    A list of UnaryExpressions, joined by '/' or '*' s
    """
    def __init__(self,unaryExprList):
        if isinstance(unaryExprList,list):
            self._list = unaryExprList
        else:
            self._list = [unaryExprList]
    def __repr__(self):
        return "<MultiplicativeExpressionList: %s>"%self.reduce()

class ParsedAdditiveExpressionList(ListRedirect):
    """
    A list of MultiplicativeExpressions, joined by '+' or '-' s
    """
    def __init__(self,multiplicativeExprList):
        if isinstance(multiplicativeExprList,list):
            self._list = multiplicativeExprList
        else:
            self._list = [multiplicativeExprList]
    def __repr__(self):
        return "<AdditiveExpressionList: %s>"%self._list

class ParsedString(unicode):
    def __init__(self,value=None):
        val = value is None and u"" or value
        super(ParsedString,self).__init__(val)

class ParsedDatatypedLiteral(object):
    """
    Placeholder for Datatyped literals
    This is neccessary (instead of instanciating Literals directly)
    when datatypes IRIRefs are QNames (in which case the prefix needs to be resolved at some point)
    """
    def __init__(self,value,dType):
        self.value = value
        self.dataType = dType

    def __repr__(self):
        return "'%s'^^%s"%(self.value,self.dataType)
########NEW FILE########
__FILENAME__ = Filter
from Util import ListRedirect

class ParsedFilter(object):
    def __init__(self,filter):
        self.filter = filter

    def __repr__(self):
        return "FILTER %s"%self.filter

class ParsedExpressionFilter(ParsedFilter):
    def __repr__(self):
        return "FILTER %s"%(isinstance(self.filter,ListRedirect) and self.filter.reduce() or self.filter)

class ParsedFunctionFilter(ParsedFilter):
    pass
########NEW FILE########
__FILENAME__ = FunctionLibrary
"""
[28] FunctionCall ::= IRIref ArgList
http://www.w3.org/TR/rdf-sparql-query/#evaluation
"""
from Util import ListRedirect

STR         = 0
LANG        = 1
LANGMATCHES = 2
DATATYPE    = 3
BOUND       = 4
isIRI       = 5
isURI       = 6
isBLANK     = 7
isLITERAL   = 8

FUNCTION_NAMES = {
    STR : 'STR',
    LANG : 'LANG',
    LANGMATCHES : 'LANGMATCHES',
    DATATYPE : 'DATATYPE',
    BOUND : 'BOUND',
    isIRI : 'isIRI',
    isURI : 'isURI',
    isBLANK : 'isBLANK',
    isLITERAL : 'isLITERAL',
}

class FunctionCall(object):
    def __init__(self,name,arguments=None):
        self.name = name
        self.arguments = arguments is None and [] or arguments

    def __repr__(self):
        return "%s(%s)"%(self.name,','.join([isinstance(i,ListRedirect) and i.reduce() or i for i in self.arguments]))

class ParsedArgumentList(ListRedirect):
    def __init__(self,arguments):
        self._list = arguments

class ParsedREGEXInvocation(object):
    def __init__(self,arg1,arg2,arg3=None):
        self.arg1 = arg1
        self.arg2 = arg2
        self.arg3 = arg3

    def __repr__(self):
        return "REGEX(%s,%s%s)"%(
                                 isinstance(self.arg1,ListRedirect) and self.arg1.reduce() or self.arg1,
                                 isinstance(self.arg2,ListRedirect) and self.arg2.reduce() or self.arg2,
                                 isinstance(self.arg3,ListRedirect) and self.arg3.reduce() or self.arg3,)

class BuiltinFunctionCall(FunctionCall):
    def __init__(self,name,arg1,arg2=None):
        if arg2:
            arguments = [arg1,arg2]
        else:
            arguments = [arg1]
        super(BuiltinFunctionCall,self).__init__(name,arguments)

    def __repr__(self):
        #print self.name
        #print [type(i) for i in self.arguments]
        return "%s(%s)"%(FUNCTION_NAMES[self.name],','.join([isinstance(i,ListRedirect) and str(i.reduce()) or i for i in self.arguments]))
########NEW FILE########
__FILENAME__ = GraphPattern
"""
See: http://www.w3.org/TR/rdf-sparql-query/#GraphPattern
[20] GraphPattern ::=  FilteredBasicGraphPattern ( GraphPatternNotTriples '.'? GraphPattern )?
[21] FilteredBasicGraphPattern ::= BlockOfTriples? ( Constraint '.'? FilteredBasicGraphPattern )?
[23] GraphPatternNotTriples  ::=  OptionalGraphPattern | GroupOrUnionGraphPattern | GraphGraphPattern
[24] OptionalGraphPattern  ::=  'OPTIONAL' GroupGraphPattern
[25] GraphGraphPattern  ::=  'GRAPH' VarOrBlankNodeOrIRIref GroupGraphPattern
[26] GroupOrUnionGraphPattern ::=  GroupGraphPattern ( 'UNION' GroupGraphPattern )*
[27] Constraint ::= 'FILTER' ( BrackettedExpression | BuiltInCall | FunctionCall )
"""

class ParsedGroupGraphPattern(object):
    """
    See: http://www.w3.org/TR/rdf-sparql-query/#GroupPatterns
    A group graph pattern GP is a set of graph patterns, GPi.
    This class is defined to behave (literally) like a set of GraphPattern instances.
    """
    def __init__(self,graphPatterns):
        self.graphPatterns = graphPatterns
    def __iter__(self):
        for g in self.graphPatterns:
            if not g.triples and g.nonTripleGraphPattern is None:
                continue
            else:
                yield g
    def __len__(self):
        return len([g for g in self.graphPatterns if g.triples or g.nonTripleGraphPattern is not None])
    def __getitem__(self, k):
        return list(self.graphPatterns)[k]
    def __repr__(self):
        return "{ %s }"%repr(list(self))

class BlockOfTriples(object):
    """
    A Basic Graph Pattern is a set of Triple Patterns.
    """
    def __init__(self,statementList):
        self.statementList = statementList
    def __getattr__(self, attr):
        if hasattr(self.statementList, attr):
            return getattr(self.statementList, attr)
        raise AttributeError, '%s has no such attribute %s' % (repr(self), attr)
    def __repr__(self):
        return "<SPARQLParser.BasicGraphPattern: %s>"%repr(self.statementList)

class GraphPattern(object):
    """
    Complex graph patterns can be made by combining simpler graph patterns. The ways of creating graph patterns are:
    * Basic Graph Patterns, where a set of triple patterns must match
    * Group Graph Pattern, where a set of graph patterns must all match using the same variable substitution
    * Value constraints, which restrict RDF terms in a solution
    * Optional Graph patterns, where additional patterns may extend the solution
    * Alternative Graph Pattern, where two or more possible patterns are tried
    * Patterns on Named Graphs, where patterns are matched against named graphs

    This class is defined as a direct analogy of Grammar rule [20]:
s    """
    def __init__(self,triples,nonTripleGraphPattern=None):
        triples = triples and triples or []
        self.triples = triples
        self.nonTripleGraphPattern = nonTripleGraphPattern

    def __repr__(self):
        if not self.triples and self.nonTripleGraphPattern is None:
            return "<SPARQLParser.EmptyGraphPattern>"
        return "<SPARQLParser.GraphPattern: %s%s>"%(
                    self.triples is not None and self.triples or '',
                    self.nonTripleGraphPattern is not None and ' %s'%self.nonTripleGraphPattern or '')

class ParsedOptionalGraphPattern(ParsedGroupGraphPattern):
    """
    An optional graph pattern is a combination of a pair of graph patterns.
    The second pattern modifies pattern solutions of the first pattern but
    does not fail matching of the overall optional graph pattern.
    """
    def __init__(self,groupGraphPattern):
        super(ParsedOptionalGraphPattern,self).__init__(groupGraphPattern.graphPatterns)

    def __repr__(self):
        return "OPTIONAL {%s}"%self.graphPatterns

class ParsedAlternativeGraphPattern(object):
    """
    A union graph pattern is a set of group graph patterns GPi.
    A union graph pattern matches a graph G with solution S
    if there is some GPi such that GPi matches G with solution S.
    """
    def __init__(self,alternativePatterns):
        self.alternativePatterns = alternativePatterns
    def __repr__(self):
        return " UNION ".join(["{%s}"%g for g in self.alternativePatterns])
    def __iter__(self):
        for g in self.alternativePatterns:
            yield g
    def __len__(self):
        return len(self.alternativePatterns)

class ParsedGraphGraphPattern(ParsedGroupGraphPattern):
    """
    Patterns on Named Graphs, where patterns are matched against named graphs
    """
    def __init__(self,graphName,groupGraphPattern):
        self.name = graphName
        super(ParsedGraphGraphPattern,self).__init__(groupGraphPattern.graphPatterns)

    def __repr__(self):
        return "GRAPH %s { %s }"%(self.name,self.graphPatterns)

    
########NEW FILE########
__FILENAME__ = IRIRef
"""
DatasetClause ::= 'FROM' ( IRIref | 'NAMED' IRIref )
See: http://www.w3.org/TR/rdf-sparql-query/#specifyingDataset

'A SPARQL query may specify the dataset to be used for matching.  The FROM clauses
give IRIs that the query processor can use to create the default graph and the
FROM NAMED clause can be used to specify named graphs. '
"""

from rdflib import URIRef

class IRIRef(URIRef):
    pass

class RemoteGraph(URIRef):
    pass

class NamedGraph(IRIRef):
    pass

########NEW FILE########
__FILENAME__ = Operators
from Util import ListRedirect

class BinaryOperator(object):
    NAME = ''
    def __init__(self,left,right):
        self.left = left
        self.right = right

    def __repr__(self):
        return "(%s %s %s)"%(
            isinstance(self.left,ListRedirect) and self.left.reduce() or self.left,
            self.NAME,
            isinstance(self.right,ListRedirect) and self.right.reduce() or self.right)

class EqualityOperator(BinaryOperator):
    NAME = '='

class NotEqualOperator(BinaryOperator):
    NAME = '!='

class LessThanOperator(BinaryOperator):
    NAME = '<'

class LessThanOrEqualOperator(BinaryOperator):
    NAME = '>='

class GreaterThanOperator(BinaryOperator):
    NAME = '>'

class GreaterThanOrEqualOperator(BinaryOperator):
    NAME = '>='

class UnaryOperator(object):
    NAME = ''
    def __init__(self,argument):
        self.argument = argument
    def __repr__(self):
        return "(%s %s)"%(
            self.NAME,
            isinstance(self.argument,ListRedirect) and self.argument.reduce() or self.argument)

class LogicalNegation(UnaryOperator):
    NAME = '!'

class NumericPositive(UnaryOperator):
    NAME = '+'

class NumericNegative(UnaryOperator):
    NAME = '-'        
########NEW FILE########
__FILENAME__ = PreProcessor
### Utilities for performing preprocessing (flattening and reordering) of Group Graph Patterns
from sets import Set
from GraphPattern import ParsedAlternativeGraphPattern,ParsedOptionalGraphPattern,ParsedGraphGraphPattern,ParsedGroupGraphPattern
from Triples import ParsedConstrainedTriples

def flattenGroupGraphPattern(groupGraphPattern):
    """
    Recursively 'Flattens' nested Group Graph Patterns using the reduction below
    { patternA { patternB } } => { patternA. patternB }
    """
    for g in groupGraphPattern:
        if g.nonTripleGraphPattern:
            if isinstance(g.nonTripleGraphPattern,ParsedAlternativeGraphPattern):
                #It's a union graph pattern, flatten each group graph pattern
                g.nonTripleGraphPattern.alternativePatterns \
                    = Set([ParsedGroupGraphPattern(list(flattenGroupGraphPattern(gGP))) for gGP in g.nonTripleGraphPattern])
                yield g
            elif isinstance(g.nonTripleGraphPattern,ParsedOptionalGraphPattern):
                #A parsed optional group graph pattern, flatten it (in place)
                g.nonTripleGraphPattern.graphPatterns = \
                    Set(flattenGroupGraphPattern(g.nonTripleGraphPattern))
                yield g
            elif isinstance(g.nonTripleGraphPattern,ParsedGraphGraphPattern):
                #A graph graph patten, flatten it (in place)
                g.nonTripleGraphPattern.graphPatterns = \
                    Set(flattenGroupGraphPattern(g.nonTripleGraphPattern))
                yield g
            else:
                #It's a nested Group Graph Patter, flatten it into
                for g in flattenGroupGraphPattern(g.nonTripleGraphPattern):
                    yield g
        else:
            #It's a Basic Graph Patternr
            yield g

def reorderBasicGraphPattern(filteredBasicGraphPattern):
    """
    Takes a list of Triples (nested lists or ParsedConstrainedTriples),
    collects the constraints, and returns the TriplePatterns and a list of global constraints
    """
    triplePatterns = []
    constraints = []
    #print "Reordering Basic Graph Pattern: ", filteredBasicGraphPattern
    for tripleList in filteredBasicGraphPattern.triples:
        #print type(tripleList)
        if isinstance(tripleList,ParsedConstrainedTriples):
            if tripleList.triples:
                triplePatterns.extend(tripleList.triples)
            constraints.append(tripleList.constraint)
        else:
            for item in tripleList:
                if isinstance(item,ParsedConstrainedTriples):
                    if item.triples:
                        triplePatterns.extend(item.triples)
                    constraints.append(item.constraint)
                else:
                    triplePatterns.append(item)
    #print "Results: ",triplePatterns,constraints
    return triplePatterns,constraints

def reorderGroupGraphPattern(groupGraphPattern):
    """
    Recursively reorders Group Graph Patterns by shifting BasicGraphPatterns to the front

    { basicGraphPatternA OPTIONAL { .. } basicGraphPatternB }
      =>
    { basicGraphPatternA+B OPTIONAL { .. }}
    """
    #print "Reordering: ", groupGraphPattern
    firstGraphPattern = groupGraphPattern[0]
    otherGraphPatterns = len(groupGraphPattern) > 1 and groupGraphPattern[1:] or []
    reorderCandidates = []
    prunedGraphPatterns = []
    #Iterate through the GP2,GP3,... removing their triples to merge into GP1
    for g in otherGraphPatterns:
        if g.nonTripleGraphPattern:
            if isinstance(g.nonTripleGraphPattern,(ParsedGroupGraphPattern)):
                #Group Graph Patterns should be reordered 'in=place'
                g.nonTripleGraphPattern = reorderGroupGraphPattern(g.nonTripleGraphPattern)
            elif isinstance(g.nonTripleGraphPattern,ParsedAlternativeGraphPattern):
                #Each Group Graph Pattern in a Alternative graph pattern should be reordered in-place
                g.nonTripleGraphPattern.alternativePatterns \
                    = Set([ParsedGroupGraphPattern(list(flattenGroupGraphPattern(gGP))) for gGP in g.nonTripleGraphPattern])
        #Tally up the Basic Graph Patterns
        reorderCandidates.append(g.triples)
        #Remove them from their source
        g.triples = []
        #Tally up the remaining non-triple graph patterns
        prunedGraphPatterns.append(g)

    firstGraphPattern.triples.extend(reorderCandidates)
    if isinstance(groupGraphPattern,ParsedGraphGraphPattern):
        rt=type(groupGraphPattern)(groupGraphPattern.name,ParsedGroupGraphPattern([firstGraphPattern]+prunedGraphPatterns))
    else:
        rt=type(groupGraphPattern)(ParsedGroupGraphPattern([firstGraphPattern]+prunedGraphPatterns))
    #print "Reordered to ", rt
    return rt


########NEW FILE########
__FILENAME__ = Processor
from rdflib import sparql
from rdflib.sparql.bison.Query import Query, Prolog
from rdflib.sparql.bison.SPARQLEvaluate import Evaluate        
from rdflib.sparql.bison import SPARQLParserc as SPARQLParser

def CreateSPARQLParser():
    return SPARQLParser.new()    

def Parse(query,debug = False):    
    p = CreateSPARQLParser()
    if debug:
        try:
           p.debug_mode(1)
        except:
            p.debug = 1    
    if not isinstance(query, unicode):
        query = unicode(query,'utf-8')
    return p.parse(query)

class Processor(sparql.Processor):

    def __init__(self, graph):
        self.graph = graph

    def query(self, strOrQuery, initBindings={}, initNs={}, DEBUG=False):
        assert isinstance(strOrQuery, (basestring, Query)), "%s must be a string or an rdflib.sparql.bison.Query.Query instance"%strOrQuery
        if isinstance(strOrQuery, basestring):
            strOrQuery = Parse(strOrQuery, DEBUG)
        if not strOrQuery.prolog:
                strOrQuery.prolog = Prolog(None, [])
                strOrQuery.prolog.prefixBindings.update(initNs)
        else:
            for prefix, nsInst in initNs.items():
                if prefix not in strOrQuery.prolog.prefixBindings:
                    strOrQuery.prolog.prefixBindings[prefix] = nsInst
        return  Evaluate(self.graph, strOrQuery, initBindings, DEBUG=DEBUG)

########NEW FILE########
__FILENAME__ = QName
from rdflib import URIRef
from rdflib.Identifier import Identifier

class QName(Identifier):
    __slots__ = ("localname", "prefix")
    def __new__(cls,value):
        try:
            inst = unicode.__new__(cls,value)
        except UnicodeDecodeError:
            inst = unicode.__new__(cls,value,'utf-8')

        inst.prefix,inst.localname = value.split(':')
        return inst

class QNamePrefix(Identifier):
    def __init__(self,prefix):
        super(QNamePrefix,self).__init__(prefix)
        
########NEW FILE########
__FILENAME__ = Query
class Query(object):
    """
    Query ::= Prolog ( SelectQuery | ConstructQuery | DescribeQuery | AskQuery )
    See: http://www.w3.org/TR/rdf-sparql-query/#rQuery
    """
    def __init__(self,prolog,query):
        self.prolog = prolog
        self.query = query

    def __repr__(self):
        return repr(self.query)

class WhereClause(object):
    """
    The where clause is essentially a wrapper for an instance of a ParsedGraphPattern
    """
    def __init__(self,parsedGraphPattern):
        self.parsedGraphPattern = parsedGraphPattern

class SelectQuery(object):
    """
    SelectQuery ::= 'SELECT' 'DISTINCT'? ( Var+ | '*' ) DatasetClause* WhereClause SolutionModifier
    See: http://www.w3.org/TR/rdf-sparql-query/#rSelectQuery
    """
    def __init__(self,variables,dataSetList,whereClause,solutionModifier,distinct=None):
        self.variables = variables is not None and variables or []
        self.dataSets = dataSetList and dataSetList or []
        self.whereClause = whereClause
        self.solutionModifier = solutionModifier
        self.distinct = distinct is not None

    def __repr__(self):
        return "SELECT %s %s %s %s %s"%(self.distinct and 'DISTINCT' or '',self.variables and self.variables or '*',self.dataSets,self.whereClause.parsedGraphPattern,self.solutionModifier and self.solutionModifier or '')

class AskQuery(object):
    """
    AskQuery ::= 'ASK' DatasetClause* WhereClause
    See: http://www.w3.org/TR/rdf-sparql-query/#rAskQuery
    """
    def __init__(self,dataSetList,whereClause):
        self.dataSets = dataSetList and dataSetList or []
        self.whereClause = whereClause

    def __repr__(self):
        return "ASK %s %s"%(self.dataSets,self.whereClause.parsedGraphPattern)

class ConstructQuery(object):
    """
    ConstructQuery ::= 'CONSTRUCT' ConstructTemplate DatasetClause* WhereClause SolutionModifier
    See: http://www.w3.org/TR/rdf-sparql-query/#rConstructQuery
    """
    pass

class DescribeQuery(object):
    """
    DescribeQuery ::= 'DESCRIBE' ( VarOrIRIref+ | '*' ) DatasetClause* WhereClause? SolutionModifier
    http://www.w3.org/TR/rdf-sparql-query/#rConstructQuery
    """
    pass
#    def __init__(self,dataSetList,whereClause):
#        self.dataSets = dataSetList and dataSetList or []
#        self.whereClause = whereClause
#
#    def __repr__(self):
#        return "ASK %s %s"%(self.dataSets,self.whereClause.parsedGraphPattern)


class Prolog(object):
    """
    Prolog ::= BaseDecl? PrefixDecl*
    See: http://www.w3.org/TR/rdf-sparql-query/#rProlog
    """
    def __init__(self,baseDeclaration,prefixDeclarations):
        self.baseDeclaration = baseDeclaration
        self.prefixBindings = {}
        if prefixDeclarations:
            for prefixBind in prefixDeclarations:
                self.prefixBindings[prefixBind.qName] = prefixBind.base

    def __repr__(self):
        return repr(self.prefixBindings)
    
########NEW FILE########
__FILENAME__ = Resource
from rdflib import URIRef, BNode
from Util import ListRedirect
from sets import Set

class RDFTerm(object):
    """
    Common class for RDF terms
    """

class Resource(RDFTerm):
    """
    Represents a sigle resource in a triple pattern.  It consists of an identifier
    (URIReff or BNode) and a list of rdflib.sparql.bison.Triples.PropertyValue instances
    """
    def __init__(self,identifier=None,propertyValueList=None):
        self.identifier = identifier is not None and identifier or BNode()
        self.propVals = propertyValueList is not None and propertyValueList or []

    def __repr__(self):
        resId = isinstance(self.identifier,BNode) and '_:'+self.identifier or self.identifier
        #print type(self.identifier)
        return "%s%s"%(resId,self.propVals and ' %s'%self.propVals or '')

    def extractPatterns(self) :
        for prop,objs in self.propVals:
            for obj in objs:
                yield (self.identifier,prop,obj)

class TwiceReferencedBlankNode(RDFTerm):
    """
    Represents BNode in triple patterns in this form:
    [ :prop1 :val1 ] :prop2 :val2
    """
    def __init__(self,props1,props2):
        self.identifier = BNode()
        self.propVals = list(Set(props1+props2))

class ParsedCollection(ListRedirect,RDFTerm):
    """
    An RDF Collection
    """
    reducable = False
    def __init__(self,graphNodeList):
        self.identifier = BNode()
        self.propVals = []
        self._list = graphNodeList
        
    def setPropertyValueList(self,propertyValueList):
        self.propVals = propertyValueList
        
    def __repr__(self):
        return "<RDF Collection: %s>"%self._list
        
########NEW FILE########
__FILENAME__ = SolutionModifier
ASCENDING_ORDER   = 1
DESCENDING_ORDER  = 2
UNSPECIFIED_ORDER = 3

ORDER_VALUE_MAPPING = {
    ASCENDING_ORDER   : 'Ascending',
    DESCENDING_ORDER  : 'Descending',
    UNSPECIFIED_ORDER : 'Default',
}

class SolutionModifier(object):
    def __init__(self,orderClause=None,limitClause=None,offsetClause=None):
        self.orderClause = orderClause
        self.limitClause = limitClause
        self.offsetClause = offsetClause

    def __repr__(self):
        if not(self.orderClause or self.limitClause or self.offsetClause):
            return ""
        return "<SoutionModifier:%s%s%s>"%(
            self.orderClause and  ' ORDER BY %s'%self.orderClause or '',
            self.limitClause and  ' LIMIT %s'%self.limitClause or '',
            self.offsetClause and ' OFFSET %s'%self.offsetClause or '')

class ParsedOrderConditionExpression(object):
    """
    A list of OrderConditions
    OrderCondition ::= (('ASC'|'DESC')BrackettedExpression )|(FunctionCall|Var|BrackettedExpression)
    """
    def __init__(self,expression,order):
        self.expression = expression
        self.order = order

    def __repr__(self):
        return "%s(%s)"%(ORDER_VALUE_MAPPING[self.order],self.expression.reduce())
    
########NEW FILE########
__FILENAME__ = SPARQLEvaluate
### Utilities for evaluating a parsed SPARQL expression using sparql-p
import rdflib
from rdflib.sparql import sparqlGraph, sparqlOperators, SPARQLError
from rdflib.sparql.sparqlOperators import getValue
from rdflib.sparql.graphPattern import BasicGraphPattern
from rdflib.sparql.Unbound import Unbound
from rdflib.sparql.Query import _variablesToArray, queryObject, SessionBNode
from rdflib.Graph import ConjunctiveGraph, Graph, BackwardCompatGraph,ReadOnlyGraphAggregate
from rdflib import URIRef,Variable,BNode, Literal, plugin, RDF
from rdflib.store import Store
from rdflib.Literal import XSDToPython
from IRIRef import NamedGraph,RemoteGraph
from GraphPattern import ParsedAlternativeGraphPattern,ParsedOptionalGraphPattern
from Resource import *
from Triples import ParsedConstrainedTriples
from QName import *
from PreProcessor import *
from Expression import *
from Util import ListRedirect
from Operators import *
from FunctionLibrary import *
from SolutionModifier import ASCENDING_ORDER
from Query import AskQuery, SelectQuery

DEBUG = False

BinaryOperatorMapping = {
    LessThanOperator           : 'sparqlOperators.lt(%s,%s)%s',
    EqualityOperator           : 'sparqlOperators.eq(%s,%s)%s',
    NotEqualOperator           : 'sparqlOperators.neq(%s,%s)%s',
    LessThanOrEqualOperator    : 'sparqlOperators.le(%s,%s)%s',
    GreaterThanOperator        : 'sparqlOperators.gt(%s,%s)%s',
    GreaterThanOrEqualOperator : 'sparqlOperators.ge(%s,%s)%s',
}

UnaryOperatorMapping = {
    LogicalNegation : 'not(%s)',
    NumericNegative : '-(%s)',
}

CAMEL_CASE_BUILTINS = {
    'isuri':'sparqlOperators.isURI',
    'isiri':'sparqlOperators.isIRI',
    'isblank':'sparqlOperators.isBlank',
    'isliteral':'sparqlOperators.isLiteral',
}

def convertTerm(term,queryProlog):
    """
    Utility function  for converting parsed Triple components into Unbound 
    """
    if isinstance(term,Variable):
        return Unbound(term[1:])
    elif isinstance(term,BNode):
        return term
    elif isinstance(term,QName):
        #QNames and QName prefixes are the same in the grammar
        if not term.prefix:
            return URIRef(queryProlog.baseDeclaration + term.localname)
        elif term.prefix == '_':
            #Told BNode See: http://www.w3.org/2001/sw/DataAccess/issues#bnodeRef
            import warnings
            warnings.warn("The verbatim interpretation of explicit bnode identifiers is contrary to (current) DAWG stance",SyntaxWarning)
            return SessionBNode(term.localname)        
        else:
            return URIRef(queryProlog.prefixBindings[term.prefix] + term.localname)
    elif isinstance(term,QNamePrefix):
        return URIRef(queryProlog.baseDeclaration + term)
    elif isinstance(term,ParsedString):
        return Literal(term)
    else:
        return term

def unRollCollection(collection,queryProlog):
    nestedComplexTerms = []
    listStart = convertTerm(collection.identifier,queryProlog)
    if not collection._list:
        yield (listStart,RDF.rest,RDF.nil)
    elif len(collection._list) == 1:
        singleItem = collection._list[0]
        if isinstance(singleItem,RDFTerm):
            nestedComplexTerms.append(singleItem)
            yield (listStart,RDF.first,convertTerm(singleItem.identifier,queryProlog))
        else:
            yield (listStart,RDF.first,convertTerm(singleItem,queryProlog))
        yield (listStart,RDF.rest,RDF.nil)
    else:
        yield (listStart,RDF.first,collection._list[0].identifier)
        prevLink = listStart
        for colObj in collection._list[1:]:
            linkNode = convertTerm(BNode(),queryProlog)
            if isinstance(colObj,RDFTerm):
                nestedComplexTerms.append(colObj)
                yield (linkNode,RDF.first,convertTerm(colObj.identifier,queryProlog))
            else:
                yield (linkNode,RDF.first,convertTerm(colObj,queryProlog))            
            yield (prevLink,RDF.rest,linkNode)            
            prevLink = linkNode                        
        yield (prevLink,RDF.rest,RDF.nil)
    
    for additionalItem in nestedComplexTerms:
        for item in unRollRDFTerm(additionalItem,queryProlog):
            yield item    

def unRollRDFTerm(item,queryProlog):
    nestedComplexTerms = []
    for propVal in item.propVals:
        for propObj in propVal.objects:
            if isinstance(propObj,RDFTerm):
                nestedComplexTerms.append(propObj)
                yield (convertTerm(item.identifier,queryProlog),
                       convertTerm(propVal.property,queryProlog),
                       convertTerm(propObj.identifier,queryProlog))
            else:
               yield (convertTerm(item.identifier,queryProlog),
                      convertTerm(propVal.property,queryProlog),
                      convertTerm(propObj,queryProlog))
    if isinstance(item,ParsedCollection):
        for rt in unRollCollection(item,queryProlog):
            yield rt  
    for additionalItem in nestedComplexTerms:
        for item in unRollRDFTerm(additionalItem,queryProlog):
            yield item

def unRollTripleItems(items,queryProlog):
    """
    Takes a list of Triples (nested lists or ParsedConstrainedTriples)
    and (recursively) returns a generator over all the contained triple patterns
    """ 
    if isinstance(items,RDFTerm):
        for item in unRollRDFTerm(items,queryProlog):
            yield item
    else:
        for item in items:
            if isinstance(item,RDFTerm):
                for i in unRollRDFTerm(item,queryProlog):
                    yield i
            else:
                for i in unRollTripleItems(item,queryProlog):
                    yield item

def mapToOperator(expr,prolog,combinationArg=None):
    """
    Reduces certain expressions (operator expressions, function calls, terms, and combinator expressions)
    into strings of their Python equivalent
    """
    combinationInvokation = combinationArg and '(%s)'%combinationArg or ""
    if isinstance(expr,ListRedirect):
        expr = expr.reduce()
    if isinstance(expr,UnaryOperator):
        return UnaryOperatorMapping[type(expr)]%(mapToOperator(expr.argument,prolog,combinationArg))
    elif isinstance(expr,BinaryOperator):
        return BinaryOperatorMapping[type(expr)]%(mapToOperator(expr.left,prolog,combinationArg),mapToOperator(expr.right,prolog,combinationArg),combinationInvokation)
    elif isinstance(expr,(Variable,Unbound)):
        return '"%s"'%expr
    elif isinstance(expr,ParsedREGEXInvocation):
        return 'sparqlOperators.regex(%s,%s%s)%s'%(mapToOperator(expr.arg1,prolog,combinationArg),
                                                 mapToOperator(expr.arg2,prolog,combinationArg),
                                                 expr.arg3 and ',"'+expr.arg3 + '"' or '',
                                                 combinationInvokation)
    elif isinstance(expr,BuiltinFunctionCall):
        normBuiltInName = FUNCTION_NAMES[expr.name].lower()
        normBuiltInName = CAMEL_CASE_BUILTINS.get(normBuiltInName,'sparqlOperators.'+normBuiltInName)
        return "%s(%s)%s"%(normBuiltInName,",".join([mapToOperator(i,prolog,combinationArg) for i in expr.arguments]),combinationInvokation)
    elif isinstance(expr,Literal):
        return str(expr)
    elif isinstance(expr,URIRef):
        import warnings
        warnings.warn("There is the possibility of __repr__ being deprecated in python3K",DeprecationWarning,stacklevel=3)        
        return repr(expr)    
    elif isinstance(expr,(QName,basestring)):
        return "'%s'"%convertTerm(expr,prolog)
    elif isinstance(expr,ParsedAdditiveExpressionList):
        return 'Literal(%s)'%(sparqlOperators.addOperator([mapToOperator(item,prolog,combinationArg='i') for item in expr],combinationArg))
    elif isinstance(expr,FunctionCall):
        if isinstance(expr.name,QName):
            fUri = convertTerm(expr.name,prolog)
        if fUri in XSDToPython:
            return "sparqlOperators.XSDCast(%s,'%s')%s"%(mapToOperator(expr.arguments[0],prolog,combinationArg='i'),fUri,combinationInvokation)
        raise Exception("Whats do i do with %s (a %s)?"%(expr,type(expr).__name__))
    else:
        if isinstance(expr,ListRedirect):
            expr = expr.reduce()
            if expr.pyBooleanOperator:
                return expr.pyBooleanOperator.join([mapToOperator(i,prolog) for i in expr]) 
        raise Exception("What do i do with %s (a %s)?"%(expr,type(expr).__name__))

def createSPARQLPConstraint(filter,prolog):
    """
    Takes an instance of either ParsedExpressionFilter or ParsedFunctionFilter
    and converts it to a sparql-p operator by composing a python string of lambda functions and SPARQL operators
    This string is then evaluated to return the actual function for sparql-p
    """
    reducedFilter = isinstance(filter.filter,ListRedirect) and filter.filter.reduce() or filter.filter
    if isinstance(reducedFilter,ParsedConditionalAndExpressionList):
        combinationLambda = 'lambda(i): %s'%(' or '.join(['%s'%mapToOperator(expr,prolog,combinationArg='i') for expr in reducedFilter]))
        if prolog.DEBUG:
            print "sparql-p operator(s): %s"%combinationLambda
        return eval(combinationLambda)
    elif isinstance(reducedFilter,ParsedRelationalExpressionList):
        combinationLambda = 'lambda(i): %s'%(' and '.join(['%s'%mapToOperator(expr,prolog,combinationArg='i') for expr in reducedFilter]))
        if prolog.DEBUG:
            print "sparql-p operator(s): %s"%combinationLambda
        return eval(combinationLambda)
    elif isinstance(reducedFilter,BuiltinFunctionCall):
        rt=mapToOperator(reducedFilter,prolog)
        if prolog.DEBUG:
            print "sparql-p operator(s): %s"%rt
        return eval(rt)
    elif isinstance(reducedFilter,(ParsedAdditiveExpressionList,UnaryOperator,FunctionCall)):
        rt='lambda(i): %s'%(mapToOperator(reducedFilter,prolog,combinationArg='i'))
        if prolog.DEBUG:
            print "sparql-p operator(s): %s"%rt
        return eval(rt)
    else:
        rt=mapToOperator(reducedFilter,prolog)
        if prolog.DEBUG:
            print "sparql-p operator(s): %s"%rt
        return eval(rt)

def sparqlPSetup(groupGraphPattern,prolog):
    """
    This core function takes Where Clause and two lists of rdflib.sparql.graphPattern.BasicGraphPatterns
    (the main patterns - connected by UNION - and an optional patterns)
    This is the core SELECT API of sparql-p
    """
    basicGraphPatterns = []
    patternList = []
    graphGraphPatterns,optionalGraphPatterns,alternativeGraphPatterns = categorizeGroupGraphPattern(groupGraphPattern)
    globalTPs,globalConstraints = reorderBasicGraphPattern(groupGraphPattern[0])
    #UNION alternative graph patterns
    if alternativeGraphPatterns:
        #Global constraints / optionals must be distributed within each alternative GP via:
        #((P1 UNION P2) FILTER R) AND ((P1 FILTER R) UNION (P2 FILTER R)).
        for alternativeGPBlock in alternativeGraphPatterns:
            for alternativeGPs in alternativeGPBlock.nonTripleGraphPattern:
                triples,constraints = reorderBasicGraphPattern(alternativeGPs[0])
                constraints.extend(globalConstraints)
                alternativeGPInst = BasicGraphPattern([t for t in unRollTripleItems(triples,prolog)])
                alternativeGPInst.addConstraints([createSPARQLPConstraint(constr,prolog) for constr in constraints])
                basicGraphPatterns.append(alternativeGPInst)
    elif graphGraphPatterns:
        triples,constraints = reorderBasicGraphPattern(graphGraphPatterns[0].nonTripleGraphPattern[0])
        for t in unRollTripleItems(triples,prolog):
            patternList.append(t)
        basicGraphPattern = BasicGraphPattern(patternList)
        for constr in constraints:
            basicGraphPattern.addConstraint(createSPARQLPConstraint(constr,prolog))
        basicGraphPatterns.append(basicGraphPattern)
    else:
        triples,constraints = reorderBasicGraphPattern(groupGraphPattern[0])
        for t in unRollTripleItems(triples,prolog):
            patternList.append(t)
        basicGraphPattern = BasicGraphPattern(patternList)
        for constr in constraints:
            basicGraphPattern.addConstraint(createSPARQLPConstraint(constr,prolog))
        basicGraphPatterns.append(basicGraphPattern)

    #Global optional patterns
    rtOptionalGraphPatterns = []
    for opGGP in [g.nonTripleGraphPattern for g in optionalGraphPatterns]:
        opTriples,opConstraints = reorderBasicGraphPattern(opGGP[0])
        #FIXME how do deal with data/local-constr/expr-2.rq?
        #opConstraints.extend(globalConstraints)
        opPatternList = []
        for t in unRollTripleItems(opTriples,prolog):
            opPatternList.append(t)
        opBasicGraphPattern = BasicGraphPattern(opPatternList)
        for constr in opConstraints:# + constraints:
            opBasicGraphPattern.addConstraint(createSPARQLPConstraint(constr,prolog))
            
        rtOptionalGraphPatterns.append(opBasicGraphPattern)
    return basicGraphPatterns,rtOptionalGraphPatterns

def isTriplePattern(nestedTriples):
    """
    Determines (recursively) if the BasicGraphPattern contains any Triple Patterns
    returning a boolean flag indicating if it does or not
    """
    if isinstance(nestedTriples,list):
        for i in nestedTriples:
            if isTriplePattern(i):
                return True
            return False
    elif isinstance(nestedTriples,ParsedConstrainedTriples):
        if nestedTriples.triples:
            return isTriplePattern(nestedTriples.triples)
        else:
            return False
    elif isinstance(nestedTriples,ParsedConstrainedTriples) and not nestedTriples.triples:
        return isTriplePattern(nestedTriples.triples)
    else:
        return True

def categorizeGroupGraphPattern(gGP):
    """
    Breaks down a ParsedGroupGraphPattern into mutually exclusive sets of
    ParsedGraphGraphPattern, ParsedOptionalGraphPattern, and ParsedAlternativeGraphPattern units
    """
    assert isinstance(gGP,ParsedGroupGraphPattern), "%s is not a ParsedGroupGraphPattern"%gGP
    graphGraphPatterns       = [gP for gP in gGP if gP.nonTripleGraphPattern and isinstance(gP.nonTripleGraphPattern,ParsedGraphGraphPattern)]
    optionalGraphPatterns    = [gP for gP in gGP if gP.nonTripleGraphPattern and isinstance(gP.nonTripleGraphPattern,ParsedOptionalGraphPattern)]
    alternativeGraphPatterns = [gP for gP in gGP if gP.nonTripleGraphPattern and isinstance(gP.nonTripleGraphPattern,ParsedAlternativeGraphPattern)]
    return graphGraphPatterns,optionalGraphPatterns,alternativeGraphPatterns

def validateGroupGraphPattern(gGP,noNesting = False):
    """
    Verifies (recursively) that the Group Graph Pattern is supported
    """
    firstGP = gGP[0]
    graphGraphPatternNo,optionalGraphPatternNo,alternativeGraphPatternNo = [len(gGPKlass) for gGPKlass in categorizeGroupGraphPattern(gGP)]
    if firstGP.triples and isTriplePattern(firstGP.triples) and  isinstance(firstGP.nonTripleGraphPattern,ParsedAlternativeGraphPattern):
        raise NotImplemented(UNION_GRAPH_PATTERN_NOT_SUPPORTED,"%s"%firstGP)
    elif graphGraphPatternNo > 1 or graphGraphPatternNo and alternativeGraphPatternNo:
        raise NotImplemented(GRAPH_GRAPH_PATTERN_NOT_SUPPORTED,"%s"%gGP)
    for gP in gGP:
        if noNesting and isinstance(gP.nonTripleGraphPattern,(ParsedOptionalGraphPattern,ParsedGraphGraphPattern,ParsedAlternativeGraphPattern)):
            raise NotImplemented(GROUP_GRAPH_PATTERN_NESTING_NOT_SUPPORTED,"%s"%gGP)
        if isinstance(gP.nonTripleGraphPattern,ParsedAlternativeGraphPattern):
            for _gGP in gP.nonTripleGraphPattern:
                validateGroupGraphPattern(_gGP,noNesting = True)
        elif gP.nonTripleGraphPattern:
            validateGroupGraphPattern(gP.nonTripleGraphPattern,noNesting = True)

def Evaluate(graph,query,passedBindings = {},DEBUG = False):
    """
    Takes:
        1. a rdflib.Graph.Graph instance 
        2. a SPARQL query instance (parsed using the BisonGen parser)
        3. A dictionary of initial variable bindings (varName -> .. rdflib Term .. )
        4. DEBUG Flag

    Returns a list of tuples - each a binding of the selected variables in query order
    """
    if query.prolog:
        query.prolog.DEBUG = DEBUG    
    if query.query.dataSets:
        graphs = []
        for dtSet in query.query.dataSets:
            if isinstance(dtSet,NamedGraph):
                graphs.append(Graph(graph.store,dtSet))
            else:
                memStore = plugin.get('IOMemory',Store)()
                memGraph = Graph(memStore)
                try:
                    memGraph.parse(dtSet,format='n3')
                except:
                    #Parse as RDF/XML instead
                    memGraph.parse(dtSet)
                graphs.append(memGraph)
        tripleStore = sparqlGraph.SPARQLGraph(ReadOnlyGraphAggregate(graphs))
    else:        
        tripleStore = sparqlGraph.SPARQLGraph(graph)    

    if isinstance(query.query,SelectQuery) and query.query.variables:
        query.query.variables = [convertTerm(item,query.prolog) for item in query.query.variables]
    else:
        query.query.variables = []

    #Interpret Graph Graph Patterns as Named Graphs
    graphGraphPatterns = categorizeGroupGraphPattern(query.query.whereClause.parsedGraphPattern)[0]
#    rt = categorizeGroupGraphPattern(query.query.whereClause.parsedGraphPattern)[0]
#    print rt[0], rt[1]
    if graphGraphPatterns:
        graphGraphP = graphGraphPatterns[0].nonTripleGraphPattern
        if isinstance(graphGraphP.name,Variable):
            if graphGraphP.name in passedBindings:
                tripleStore = sparqlGraph.SPARQLGraph(Graph(graph.store,passedBindings[graphGraphP.name]))
            else: 
                #print graphGraphP 
                #raise Exception("Graph Graph Patterns can only be used with variables bound at the top level or a URIRef or BNode term")
                tripleStore = sparqlGraph.SPARQLGraph(graph,graphVariable = graphGraphP.name)
        else:
            graphName =  isinstance(graphGraphP.name,Variable) and passedBindings[graphGraphP.name] or graphGraphP.name
            graphName  = convertTerm(graphName,query.prolog)
            if isinstance(graph,ReadOnlyGraphAggregate) and not graph.store:
                targetGraph = [g for g in graph.graphs if g.identifier == graphName]
                assert len(targetGraph) == 1
                targetGraph = targetGraph[0]
            else:
                targetGraph = Graph(graph.store,graphName)
            tripleStore = sparqlGraph.SPARQLGraph(targetGraph)

    gp = reorderGroupGraphPattern(query.query.whereClause.parsedGraphPattern)
    validateGroupGraphPattern(gp)
    basicPatterns,optionalPatterns = sparqlPSetup(gp,query.prolog)

    if DEBUG:
        print "## Select Variables ##\n",query.query.variables
        print "## Patterns ##\n",basicPatterns
        print "## OptionalPatterns ##\n",optionalPatterns

    result = queryObject(tripleStore, basicPatterns,optionalPatterns,passedBindings)
    if result == None :
        # generate some proper output for the exception :-)
        msg = "Errors in the patterns, no valid query object generated; "
        msg += ("pattern:\n%s\netc..." % basicPatterns[0])
        raise SPARQLError(msg)

    if isinstance(query.query,AskQuery):
        return result.ask()

    elif isinstance(query.query,SelectQuery):
        orderBy = None
        orderAsc = None
        if query.query.solutionModifier.orderClause:
            orderBy     = []
            orderAsc    = []
            for orderCond in query.query.solutionModifier.orderClause:
                # is it a variable?
                if isinstance(orderCond,Variable):
                    orderBy.append(orderCond)
                    orderAsc.append(ASCENDING_ORDER)
                # is it another expression, only variables are supported
                else:
                    expr = orderCond.expression
                    assert isinstance(expr,Variable),"Support for ORDER BY with anything other than a variable is not supported: %s"%expr
                    orderBy.append(expr)                    
                    orderAsc.append(orderCond.order == ASCENDING_ORDER)

        limit = query.query.solutionModifier.limitClause and int(query.query.solutionModifier.limitClause) or None

        offset = query.query.solutionModifier.offsetClause and int(query.query.solutionModifier.offsetClause) or 0
        return result.select(query.query.variables,
                             query.query.distinct,
                             limit,
                             orderBy,
                             orderAsc,
                             offset
                             ),_variablesToArray(query.query.variables,"selection"),result._getAllVariables(),orderBy,query.query.distinct
    else:
        raise NotImplemented(CONSTRUCT_NOT_SUPPORTED,repr(query))

OPTIONALS_NOT_SUPPORTED                   = 1
#GRAPH_PATTERN_NOT_SUPPORTED               = 2
UNION_GRAPH_PATTERN_NOT_SUPPORTED         = 3
GRAPH_GRAPH_PATTERN_NOT_SUPPORTED         = 4
GROUP_GRAPH_PATTERN_NESTING_NOT_SUPPORTED = 5
CONSTRUCT_NOT_SUPPORTED                   = 6

ExceptionMessages = {
    OPTIONALS_NOT_SUPPORTED                   : 'Nested OPTIONAL not currently supported',
    #GRAPH_PATTERN_NOT_SUPPORTED               : 'Graph Pattern not currently supported',
    UNION_GRAPH_PATTERN_NOT_SUPPORTED         : 'UNION Graph Pattern (currently) can only be combined with OPTIONAL Graph Patterns',
    GRAPH_GRAPH_PATTERN_NOT_SUPPORTED         : 'Graph Graph Pattern (currently) cannot only be used once by themselves or with OPTIONAL Graph Patterns',
    GROUP_GRAPH_PATTERN_NESTING_NOT_SUPPORTED : 'Nesting of Group Graph Pattern (currently) not supported',
    CONSTRUCT_NOT_SUPPORTED                   : '"Construct" is not (currently) supported',
}


class NotImplemented(Exception):
    def __init__(self,code,msg):
        self.code = code
        self.msg = msg
    def __str__(self):
        return ExceptionMessages[self.code] + ' :' + self.msg

########NEW FILE########
__FILENAME__ = Triples
class PropertyValue(object):
    def __init__(self,property,objects):
        self.property = property
        self.objects = objects
        #print
    def __repr__(self):
        return "%s(%s)"%(self.property,self.objects)

class ParsedConstrainedTriples(object):
    """
    A list of Resources associated with a constraint
    """
    def __init__(self,triples,constraint):
        self.triples = triples
        self.constraint = constraint

    def __repr__(self):
        return "%s%s"%(self.triples,self.constraint and ' %s'%self.constraint or '')
########NEW FILE########
__FILENAME__ = Util
class ListRedirect(object):
    """
    A utility class for lists of items joined by an operator.  ListRedirects with length 1
    are a special case and are considered equivalent to the item instead of a list containing it.
    The reduce function is used for normalizing ListRedirect to the single item (and calling reduce on it recursively)
    """
    reducable = True
    def __getattr__(self, attr):
        if hasattr(self._list, attr):
            return getattr(self._list, attr)
        raise AttributeError, '%s has no such attribute %s' % (repr(self), attr)

    def __iter__(self):
        for i in self._list:
            yield i

    def reduce(self):
        if self.reducable and len(self._list) == 1:
            singleItem = self._list[0]
            if isinstance(singleItem,ListRedirect):
                return singleItem.reduce()
            else:
                return singleItem
        else:
            return type(self)([isinstance(item,ListRedirect) and item.reduce() or item for item in self._list])

#Utility function for adding items to the front of a list
def ListPrepend(item,list):
    #print "adding %s to front of %s"%(item,list)
    return [item] + list
########NEW FILE########
__FILENAME__ = graphPattern
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/11/04 14:06:36 $, by $Author: ivan $, $Revision: 1.1 $
#
"""
Graph pattern class used by the SPARQL implementation
"""
import sys, os, time, datetime
from rdflib.Literal     import Literal
from rdflib.BNode       import BNode
from rdflib.URIRef      import URIRef
from types import *

from rdflib.sparql import _questChar, Debug, SPARQLError
from rdflib.sparql.Unbound import Unbound

def _createResource(v) :
    """Create an RDFLib Literal instance with the corresponding XML
    Schema datatype set. If the variable is already an RDFLib
    resource, it simply returns the resource; otherwise the
    corresponding Literal.  A SPARQLError Exception is raised if the
    type is not implemented.

    The Literal contains the string representation of the variable (as
    Python does it by default) with the corresponding XML Schema URI
    set.

    @param v: Python variable
    @return: either an RDFLib Literal (if 'v' is not an RDFLib Resource), or the same variable if it is already
    an RDFLib resource (ie, Literal, BNode, or URIRef)
    @raise SPARQLError: if the type of 'v' is not implemented
    """
    if isinstance(v,Literal) or isinstance(v,BNode) or isinstance(v,URIRef) :
        # just do nothing
        return v
    else :
        return Literal(v) # Literal now does the datatype bits


def _isResQuest(r) :
    """
    Is 'r' a request string (ie, of the form "?XXX")?

    @rtype: Boolean
    """
    if r and isinstance(r,basestring) and r[0] == _questChar :
        return True
    return False


class GraphPattern :
    """
    Storage of one Graph Pattern, ie, the pattern tuples and the
    possible (functional) constraints (filters)
    """
    def __init__(self,patterns=[]) :
        """
        @param patterns: an initial list of graph pattern tuples
        """
        self.patterns    = []
        self.constraints = []
        self.unbounds    = []
        self.bnodes      = {}
        if type(patterns) == list :
            self.addPatterns(patterns)
        elif type(patterns) == tuple :
            self.addPattern(patterns)
        else :
            raise SPARQLError("illegal argument, pattern must be a tuple or a list of tuples")

    def _generatePattern(self,tupl) :
        """
        Append a tuple to the local patterns. Possible type literals
        are converted to real literals on the fly.  Each tuple should
        be contain either 3 elements (for an RDF Triplet pattern) or
        four, where the fourth element is a per-pattern constraint
        (filter). (The general constraint of SPARQL can be optimized
        by assigning a constraint to a specific pattern; because it
        stops the graph expansion, its usage might be much more
        optimal than the the 'global' constraint).

        @param tupl: either a three or four element tuple
        """
        if type(tupl) != tuple :
            raise SPARQLError("illegal argument, pattern must be a tuple, got %s" % type(tupl))
        if len(tupl) != 3 and len(tupl) != 4 :
            raise SPARQLError("illegal argument, pattern must be a tuple of 3 or 4 element, got %s" % len(tupl))
        if len(tupl) == 3 :
            (s,p,o)   = tupl
            f         = None
        else :
            (s,p,o,f) = tupl
        final=[]
        for c in (s,p,o) :
            if _isResQuest(c) :
                if not c in self.unbounds :
                    self.unbounds.append(c)
                final.append(c)
            elif isinstance(c, BNode):
                #Do nothing - BNode name management is handled by SPARQL parser
#                if not c in self.bnodes :
#                    self.bnodes[c] = BNode()
                final.append(c)
            else :
                final.append(_createResource(c))
        final.append(f)
        return tuple(final)

    def addPattern(self,tupl) :
        """
        Append a tuple to the local patterns. Possible type literals
        are converted to real literals on the fly.  Each tuple should
        be contain either 3 elements (for an RDF Triplet pattern) or
        four, where the fourth element is a per-pattern constraint
        (filter). (The general constraint of SPARQL can be optimized
        by assigning a constraint to a specific pattern; because it
        stops the graph expansion, its usage might be much more
        optimal than the the 'global' constraint).

        @param tupl: either a three or four element tuple
        """
        self.patterns.append(self._generatePattern(tupl))

    def insertPattern(self,tupl) :
        """
        Insert a tuple to to the start of local patterns. Possible
        type literals are converted to real literals on the fly.  Each
        tuple should be contain either 3 elements (for an RDF Triplet
        pattern) or four, where the fourth element is a per-pattern
        constraint (filter). (The general constraint of SPARQL can be
        optimized by assigning a constraint to a specific pattern;
        because it stops the graph expansion, its usage might be much
        more optimal than the the 'global' constraint).

        Semantically, the behaviour induced by a graphPattern does not
        depend on the order of the patterns. However, due to the
        behaviour of the expansion algorithm, users may control the
        speed somewhat by adding patterns that would 'cut' the
        expansion tree soon (ie, patterns that reduce the available
        triplets significantly). API users may be able to do that,
        hence this additional method.

        @param tupl: either a three or four element tuple
        """
        self.patterns.insert(0,self._generatePattern(tupl))


    def addPatterns(self,lst) :
        """
        Append a list of tuples to the local patterns. Possible type
        literals are converted to real literals on the fly.  Each
        tuple should be contain either three elements (for an RDF
        Triplet pattern) or four, where the fourth element is a
        per-pattern constraint. (The general constraint of SPARQL can
        be optimized by assigning a constraint to a specific pattern;
        because it stops the graph expansion, its usage might be much
        more optimal than the the 'global' constraint).

        @param lst: list consisting of either a three or four element tuples
        """
        for l in lst:
            self.addPattern(l)

    def insertPatterns(self,lst) :
        """
        Insert a list of tuples to the start of the local
        patterns. Possible type literals are converted to real
        literals on the fly.  Each tuple should be contain either
        three elements (for an RDF Triplet pattern) or four, where the
        fourth element is a per-pattern constraint. (The general
        constraint of SPARQL can be optimized by assigning a
        constraint to a specific pattern; because it stops the graph
        expansion, its usage might be much more optimal than the the
        'global' constraint).

        Semantically, the behaviour induced by a graphPattern does not
        depend on the order of the patterns. However, due to the
        behaviour of the expansion algorithm, users may control the
        speed somewhat by adding patterns that would 'cut' the
        expansion tree soon (ie, patterns that reduce the available
        triplets significantly). API users may be able to do that,
        hence this additional method.

        @param lst: list consisting of either a three or four element tuples
        """
        for i in xrange(len(lst)-1,-1,-1) :
            self.insertPattern(lst[i])

    def addConstraint(self,func) :
        """
        Add a global filter constraint to the graph pattern. 'func'
        must be a method with a single input parameter (a dictionary)
        returning a boolean. This method is I{added} to previously
        added methods, ie, I{all} methods must return True to accept a
        binding.

        @param func: filter function
        """
        if type(func) == FunctionType :
            self.constraints.append(func)
        else :
            raise SPARQLError("illegal argument, constraint must be a function type, got %s" % type(func))

    def addConstraints(self,lst) :
        """
        Add a list of global filter constraints to the graph
        pattern. Each function in the list must be a method with a
        single input parameter (a dictionary) returning a
        boolean. These methods are I{added} to previously added
        methods, ie, I{all} methods must return True to accept a
        binding.

        @param lst: list of functions
        """
        for l in lst:
            self.addConstraint(l)

    def construct(self,tripleStore,bindings) :
        """
        Add triples to a tripleStore based on a variable bindings of
        the patterns stored locally.  The triples are patterned by the
        current Graph Pattern. The method is used to construct a graph
        after a successful querying.

        @param tripleStore: an (rdflib) Triple Store
        @param bindings: dictionary
        """
        localBnodes = {}
        for c in self.bnodes :
            localBnodes[c] = BNode()
        def bind(st) :
            if _isResQuest(st) :
                if st in bindings :
                    return bindings[st]
                else :
					if isinstance(self,GraphPattern2) :
						return st
					else :
						return None
            elif isinstance(st,BNode) :
                for c in self.bnodes :
                    if self.bnodes[c] == st :
                        # this is a BNode that was created as part of building up the pattern
                        return localBnodes[c]
                # if we got here, the BNode comes from somewhere else...
                return st
            else :
                return st

        for pattern in self.patterns :
            (s,p,o,f) = pattern
            triplet = []
            valid = True
            for res in (s,p,o) :
                val = bind(res)
                if val != None :
                    triplet.append(val)
                else :
                    valid = False
                    break
            if valid :
                tripleStore.add(tuple(triplet))

    def __add__(self,other) :
        """Adding means concatenating all the patterns and filters arrays"""
        retval = GraphPattern()
        retval += self
        retval += other
        return retval

    def __iadd__(self,other) :
        """Adding means concatenating all the patterns and filters arrays"""
        self.patterns    += other.patterns
        self.constraints += other.constraints
        for c in other.unbounds :
            if not c in self.unbounds :
                self.unbounds.append(c)
        for c in other.bnodes :
            if not c in self.bnodes :
                self.bnodes[c] = other.bnodes[c]
        return self

    def __repr__(self) :
        retval  = "   Patterns:    %s\n" % self.patterns
        retval += "   Constraints: %s\n" % self.constraints
        retval += "   Unbounds:    %s\n" % self.unbounds
        return retval

    def __str__(self) :
        return self.__repr__()

    def isEmpty(self) :
        """Is the pattern empty?
        @rtype: Boolean
        """
        return len(self.patterns) == 0

		
class BasicGraphPattern(GraphPattern) :
    """One, justified, problem with the current definition of L{GraphPattern<GraphPattern>} is that it
    makes it difficult for users to use a literal of the type "?XXX", because any string beginning
    with "?" will be considered to be an unbound variable. The only way of doing this is that the user
    explicitly creates a Literal object and uses that as part of the pattern.

    This class is a superclass of L{GraphPattern<GraphPattern>} which does I{not} do this, but requires the
    usage of a separate variable class instance"""

    def __init__(self,patterns=[]) :
        """
        @param patterns: an initial list of graph pattern tuples
        """
        GraphPattern.__init__(self,patterns)	
	
    def _generatePattern(self,tupl) :
        """
        Append a tuple to the local patterns. Possible type literals
        are converted to real literals on the fly.  Each tuple should
        be contain either 3 elements (for an RDF Triplet pattern) or
        four, where the fourth element is a per-pattern constraint
        (filter). (The general constraint of SPARQL can be optimized
        by assigning a constraint to a specific pattern; because it
        stops the graph expansion, its usage might be much more
        optimal than the the 'global' constraint).

        @param tupl: either a three or four element tuple
        """
        if type(tupl) != tuple :
            raise SPARQLError("illegal argument, pattern must be a tuple, got %s" % type(tupl))
        if len(tupl) != 3 and len(tupl) != 4 :
            raise SPARQLError("illegal argument, pattern must be a tuple of 3 or 4 element, got %s" % len(tupl))
        if len(tupl) == 3 :
            (s,p,o)   = tupl
            f         = None
        else :
            (s,p,o,f) = tupl
        final=[]
        for c in (s,p,o) :
            if isinstance(c,Unbound) :
                if not c.name in self.unbounds :
                    self.unbounds.append(c.name)
                final.append(c.name)
            elif isinstance(c, BNode):
                #Do nothing - BNode name management is handled by SPARQL parser
                final.append(c)
            else :
                final.append(_createResource(c))
        final.append(f)
        return tuple(final)
		
if __name__ == '__main__' :
    v1 = Unbound("a")
    g = BasicGraphPattern([("a","?b",24),("?r","?c",12345),(v1,"?c",3333)])
    print g








########NEW FILE########
__FILENAME__ = parser
#!/usr/bin/python
""" SPARQL Lexer, Parser and Function-Mapper
By Shawn Brown <http://shawnbrown.com/contact>

TO DO:
  swap current parser functions for Michelp's pyparsing setup
  add mapping for FILTER/constraints
  typed literals
  integer, double or boolean abbreviations
  language tags (e.g., @fr)
  nested OPTIONALs ???
  blank node and RDF collection syntax ???
  GRAPH statements ???

CURRENTLY SUPPORTED:
  Simple SELECT queries
  Predicate-object and object list shorthand
    (e.g., ?x  foaf:name  ?name ; foaf:mbox  ?mbox ; vcard:TITLE  ?title)
  Multi-line/triple-quoted literals
  BASE, PREFIX, SELECT, WHERE, UNION, OPTIONAL, multiple UNIONs and multiple
    OPTIONALs (but not nested OPTIONALs)

USAGE:
    #from sparql_lpm import doSPARQL
    from rdflib.sparql.parser import doSPARQL
    ...load graph...
    ...define SPARQL query as string...
    result = doSPARQL(queryStr, sparqlGr)

"""

import base64
import re
from rdflib.URIRef import URIRef
from rdflib.sparql.graphPattern import GraphPattern

def _escape(text): return base64.encodestring(text).replace("\n", "")
def _unescape(text): return base64.decodestring(text)

def _escapeLiterals(query):
    """ escape all literals with escape() """
    fn = lambda m: "'" + _escape(m.group(2)) + "'" + m.group(3)
    pat = r"(\"\"\"|'''|[\"'])([^\1]*?[^\\]?)\1" # literal
    return re.sub(pat+"(\s*[.,;\}])", fn, query)

def _resolveShorthand(query):
    """ resolve some of the syntactic shorthand (2.8 Other Syntactic Forms) """
    def doList(pat, text):
        pat = re.compile(pat)
        while pat.search(text): text = re.sub(pat, r"\1\2\3 . \2\4", text)
        return text
    # 2.8.1 Predicate-Object Lists
    pat = r"(\{.*?)([^ ]+ )([^ ]+ [^ ]+)\s?; ([^ ]+ [^ ]+\s?[,;\.\}])"
    query = doList(pat, query)
    # 2.8.2 Object Lists
    pat = r"(\{.*?)([^ ]+ [^ ]+ )([^ ]+\s?), ([^ ]+\s?[,\.\}])"
    query = doList(pat, query)
    # TO DO: look at adding all that other crazy stuff!!!
    return query

def _resolvePrefixes(query):
    """ resolve prefixed IRIs, remove PREFIX statements """
    # parse PREFIX statements
    prefixes = re.findall("PREFIX ([\w\d]+:) <([^<>]+)>", query) # get list of prefix tuples
    prefixes.extend([
        ("rdf:", "http://www.w3.org/1999/02/22-rdf-syntax-ns#"),
        ("rdfs:", "http://www.w3.org/2000/01/rdf-schema#"),
        ("xsd:", "http://www.w3.org/2001/XMLSchema#"),
        ("fn:", "http://www.w3.org/2004/07/xpath-functions")])
    matches = re.search("PREFIX : <([^<>]+)>", query) # parse colon-only PREFIX
    if matches != None: prefixes.append((":", matches.group(1)))
    query = re.sub("PREFIX [\w\d]*:[ ]?<[^<>]+>[ ]?", "", query) # remove PREFIX statements
    # escape IRIs (unescaped in ??)
    fn = lambda m: "<" + _escape(m.group(1)) + ">"
    query = re.sub("<([^<>]+)>", fn, query)
    # resolve prefixed IRIs
    for pair in prefixes:
        fn = lambda m: "<" + _escape(pair[1]+m.group(1)) + ">" # escaped too
        query = re.sub(pair[0]+"([^ .\}]+)", fn, query)
    return query

def _resolveBase(query):
    """ resolve relative IRIs using BASE IRI, remove BASE statement """
    pat = re.compile("BASE <([^<>]+)>\s?")
    base = pat.search(query)
    if base != None:
        fn = lambda m: "<" + base.group(1) + m.group(1) + ">"
        query = re.sub("<([^<>: ]+)>", fn, query) # resolve relative IRIs
        query = re.sub(pat, "", query) # remove BASE statement
    return query

def _parseSelect(query):
    """ returns tuple of SELECTed variables or None """
    var = "[?$][\\w\\d]+" # SELECT variable pattern
    select = re.search("SELECT(?: " + var + ")+", query)
    if select != None:
        select = re.findall(var, select.group(0))
        select = tuple(select)
    return select

class _StackManager:
    """ manages token stack for _parser() """
    def __tokenGen(self, tokens):
        for token in tokens:
            yield token
    def __init__(self, tokenList):
        self.stack = self.__tokenGen(tokenList)
        self.current = self.stack.next()
    def next(self):
        try:
            self.current = self.stack.next()
            if self.current == "":
                self.next() # if blank, move to next
        except StopIteration:
            self.current = None
    def token(self):
        return self.current

#
# The following classes, _listTypes dictionary and _makeList() function are
# used to test for recognized keywords and to create "typed" lists for nested
# statements when parsing the SPARQL query's WHERE statement
#
class Where(list): pass
class Union(list): pass
class Optional(list): pass
_listTypes = {
    "OPTIONAL": lambda : Optional([]),
    "UNION": lambda : Union([]),
    "WHERE": lambda : Where([])
}
def _makeList(keyword):
    """ return list of given type or None """
    global _listTypes
    if keyword in _listTypes:
        return _listTypes[keyword]()
    return None

def _parser(stack, listType="WHERE"):
    """ simple recursive descent SPARQL parser """
    typedList = _makeList(listType)
    nestedType = listType
    while stack.token() != None:
        token = stack.token()
        if _makeList(token) != None:
            nestedType = token
        elif token == "{":
            stack.next() # iterate to next token
            typedList.append(_parser(stack, nestedType))
            nestedType = listType # reset nestedType
        elif token == "}":
            return typedList
        elif token != ".":
            statement = ""
            while token != None and token != "." and token != "{" and token != "}":
                statement += " " + token
                stack.next()
                token = stack.token()
            statement = statement.strip()
            typedList.append(statement)
            continue
        stack.next()
    return typedList

def _parseWhere(query):
    """ split query into tokens, return parsed object """
    stackObj = _StackManager(query)
    return _parser(stackObj)

def _findStatements(stmntType, stmntList):
    """ recurse over nested list, compile & return flat list of matching
        statement strings used by _getStatements() """
    statements = []
    typedList = _makeList(stmntType)
    for stmnt in stmntList:
        if type(stmnt) is str:
            statements.append(stmnt)
        if type(stmnt) == type(typedList):
            statements.extend(_findStatements(stmntType, stmnt))
    return statements

def _getStatements(stmntType, stmntList):
    """ gets statements of given type from given list """
    statements = []
    typedList = _makeList(stmntType)
    for item in stmntList:
        if type(item) == type(typedList):
            statements.append(_findStatements(stmntType, item))
    return statements

def _buildGraphPattern(triples):
    # split strings into tuples of strings
    triples = map((lambda x: tuple(re.split(" ", x))), triples)
    # convert tuples of strings into tuples of RDFLib objects
    isIRI = lambda x: x[0]=="<" and x[-1]==">"
    isLit = lambda x: x[0]=="'" and x[-1]=="'" or x[0]=='"' and x[-1]=='"'
    for i in range(len(triples)):
        sub = triples[i][0]
        pred = triples[i][1]
        obj = triples[i][2]
        # unescape and define objects for IRIs and literals
        if isIRI(sub): sub = URIRef(_unescape(sub[1:-1]))
        if isIRI(pred): pred = URIRef(_unescape(pred[1:-1]))
        if isIRI(obj): obj = URIRef(_unescape(obj[1:-1]))
        elif isLit(obj): obj = _unescape(obj[1:-1])
        # build final triple
        triples[i] = (sub, pred, obj)
    return GraphPattern(triples)

def _buildQueryArgs(query):
    """ """
    # query lexer
    query = _escapeLiterals(query) # are unescaped in _buildGraphPattern()
    query = re.sub("\s+", " ", query).strip() # normalize whitespace
    query = _resolveShorthand(query) # resolve pred-obj and obj lists
    query = _resolveBase(query) # resolve relative IRIs
    query = _resolvePrefixes(query) # resolve prefixes
    query = re.sub(r"\s*([.;,\{\}])\s*", r" \1 ", query) # normalize punctuation
    whereObj = query[query.find("{")+1:query.rfind("}")].strip() # strip non-WHERE bits
    whereObj = whereObj.split(" ") # split into token stack
    # query parser
    select = _parseSelect(query) # select is tuple of select variables
    whereObj = _parseWhere(whereObj) # stack parsed into nested list of typed lists
    # map parsed object to arrays of RDFLib graphPattern objects
    where = _getStatements("WHERE", [whereObj]) # pass whereObj as nested list
    where.extend(_getStatements("UNION", whereObj))
    where = map(_buildGraphPattern, where)
    optional = _getStatements("OPTIONAL", whereObj)
    optional = map(_buildGraphPattern, optional)
    # run query
    #return sparqlGr.query(select, where, optional)
    return { "select":select, "where":where, "optional":optional }

def doSPARQL(query, sparqlGr):
    """ Takes SPARQL query & SPARQL graph, returns SPARQL query result object. """
    x = _buildQueryArgs(query)
    return sparqlGr.query(x["select"], x["where"], x["optional"])


if __name__ == "__main__":
    testCases = [
# basic
"""
SELECT ?name
WHERE { ?a <http://xmlns.com/foaf/0.1/name> ?name }
""",
# simple prefix
"""
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?a foaf:name ?name }
""",
# base statement
"""
BASE <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { ?a <name> ?name }
""",
# prefix and colon-only prefix
"""
PREFIX : <http://xmlns.com/foaf/0.1/>
PREFIX vcard: <http://www.w3.org/2001/vcard-rdf/3.0#>
SELECT ?name ?title
WHERE {
    ?a :name ?name .
    ?a vcard:TITLE ?title
}
""",
# predicate-object list notation
"""
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name ?mbox
WHERE {
    ?x  foaf:name  ?name ;
        foaf:mbox  ?mbox .
}
""",
# object list notation
"""
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?x
WHERE {
    ?x foaf:nick  "Alice" ,
                  "Alice_" .
}
""",
# escaped literals
"""
PREFIX tag: <http://xmlns.com/foaf/0.1/>
PREFIX vcard: <http://www.w3.org/2001/vcard-rdf/3.0#>
SELECT ?name
WHERE {
    ?a tag:name ?name ;
       vcard:TITLE "escape test vcard:TITLE " ;
       <tag://test/escaping> "This is a ''' Test \"\"\"" ;
       <tag://test/escaping> ?d
}
""",
# key word as variable
"""
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?PREFIX ?WHERE
WHERE {
    ?x  foaf:name  ?PREFIX ;
        foaf:mbox  ?WHERE .
}
""",
# key word as prefix
"""
PREFIX WHERE: <http://xmlns.com/foaf/0.1/>
SELECT ?name ?mbox
WHERE {
    ?x  WHERE:name  ?name ;
        WHERE:mbox  ?mbox .
}
""",
# some test cases from grammar.py
"SELECT ?title WHERE { <http://example.org/book/book1> <http://purl.org/dc/elements/1.1/title> ?title . }",

"""PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name ?mbox
WHERE { ?person foaf:name ?name .
OPTIONAL { ?person foaf:mbox ?mbox}
}""",

"""PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name ?name2
WHERE { ?person foaf:name ?name .
OPTIONAL { ?person foaf:knows ?p2 . ?p2 foaf:name   ?name2 . }
}""",

"""PREFIX foaf: <http://xmlns.com/foaf/0.1/>
#PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
SELECT ?name ?mbox
WHERE
{
{ ?person rdf:type foaf:Person } .
OPTIONAL { ?person foaf:name  ?name } .
OPTIONAL {?person foaf:mbox  ?mbox} .
}"""
    ]

    print "Content-type: text/plain\n\n"
    for query in testCases:
        print "\n-----\n"
        print '>>> query = """' + query.replace("\n", "\n... ") + '"""'
        print ">>> result = doSPARQL(query, sparqlGr)\n"
        result = _buildQueryArgs(query);
        print "select = ", result["select"], "\n"
        print "where = ", result["where"], "\n"
        print "optional = ", result["optional"], "\n"
        print "result = sparqlGr.query(select, where, optional)"

########NEW FILE########
__FILENAME__ = Query
import types, sets

from rdflib import URIRef, BNode, Literal
from rdflib.Identifier import Identifier

from rdflib.util import check_subject, list2set

from rdflib.sparql import SPARQLError
from rdflib.sparql.Unbound import Unbound
from rdflib.sparql.sparqlGraph import SPARQLGraph
from rdflib.sparql.graphPattern import GraphPattern

class SessionBNode(BNode):
    """
    Special 'session' BNodes.  I.e., BNodes at the query side which refer to 
    BNodes in persistence
    """
    pass        

def _checkOptionals(pattern,optionals) :
    """
    The following remark in the SPARQL document is important:

    'If a new variable is mentioned in an optional block (as mbox and
    hpage are mentioned in the previous example), that variable can be
    mentioned in that block and can not be mentioned in a subsequent
    block.'

    What this means is that the various optional blocks do not
    interefere at this level and there is no need for a check whether
    a binding in a subsequent block clashes with an earlier optional
    block.

    This method checks whether this requirement is fulfilled. Raises a
    SPARQLError exception if it is not (the rest of the algorithm
    relies on this, so checking it is a good idea...)

    @param pattern: graph pattern
    @type pattern: L{GraphPattern<rdflib.sparql.GraphPattern>}
    @param optionals: graph pattern
    @type optionals: L{GraphPattern<rdflib.sparql.GraphPattern>}
    @raise SPARQLError: if the requirement is not fulfilled
    """
    for i in xrange(0,len(optionals)) :
        for c in optionals[i].unbounds :
            if c in pattern.unbounds :
                # this is fine, an optional query variable can appear in the main pattern, too
                continue
            if i > 0 :
                for j in xrange(0,i) :
                    if c in optionals[j].unbounds :
                        # This means that:
                        #   - the variable is not in the main pattern (because the previous if would have taken care of it)
                        #   - the variable is in the previous optional: ie, Error!
                        raise SPARQLError("%s is an illegal query string, it appear in a previous OPTIONAL clause" % c)


def _variablesToArray(variables,name='') :
    """Turn an array of Variables or query strings into an array of query strings. If the 'variables'
    is in fact a single string or Variable, then it is also put into an array.

    @param variables: a string, a unicode, or a Variable, or an array of those (can be mixed, actually). As a special case,
    if the value is "*", it returns None (this corresponds to the wildcard in SPARQL)
    @param name: the string to be used in the error message
    """
    if isinstance(variables,basestring) :
        if variables == "*" :
            return None
        else :
            return [variables]
    elif isinstance(variables,Unbound) :
        return [variables.name]
    elif type(variables) == list or type(variables) == tuple :
        retval = []
        for s in variables :
            if isinstance(s,basestring) :
                retval.append(s)
            elif isinstance(s,Unbound) :
                retval.append(s.name)
            else :
                raise SPARQLError("illegal type in '%s'; must be a string, unicode, or a Variable" % name)
    else :
        raise SPARQLError("'%s' argument must be a string, a Variable, or a list of those" % name)
    return retval

def _createInitialBindings(pattern) :
    """Creates an initial binding directory for the Graph Pattern by putting a None as a value for each
    query variable.

    @param pattern: graph pattern
    @type pattern: L{GraphPattern<rdflib.sparql.GraphPattern>}
    """
    bindings = {}
    for c in pattern.unbounds :
        bindings[c] = None
    return bindings


class _SPARQLNode:
    """
    The SPARQL implementation is based on the creation of a tree, each
    level for each statement in the 'where' clause of SPARQL.

    Each node maintains a 'binding' dictionary, with the variable
    names and either a None if not yet bound, or the binding
    itself. The method 'expand' tries to make one more step of binding
    by looking at the next statement: it takes the statement of the
    current node, binds the variables if there is already a binding,
    and looks at the triple store for the possibilities. If it finds
    valid new triplets, that will bind some more variables, and
    children will be created with the next statement in the 'where'
    array with a new level of bindings. This is done for each triplet
    found in the store, thereby branching off the tree. If all
    variables are already bound but the statement, with the bound
    variables, is not 'true' (ie, there is no such triple in the
    store), the node is marked as 'clash' and no more expansion is
    made; this node will then be thrown away by the parent. If I{all}
    children of a node is a clash, then it is marked as a clash
    itself.

    At the end of the process, the leaves of the tree are searched; if
    a leaf is such that:

      - all variables are bound
      - there is no clash

    then the bindings are returned as possible answers to the query.

    The optional clauses are treated separately: each 'valid' leaf is
    assigned an array of expansion trees that contain the optional
    clauses (that may have some unbound variables bound at the leaf,
    though).

    @ivar parent: parent in the tree
    @type parent: _SPARQLNode
    @ivar children: the children (in an array)
    @type children: array of _SPARQLNode
    @ivar bindings:  copy of the bindings locally
    @type bindings: dictionary
    @ivar statement:  the current statement
    @type statement: a (s,p,o,f) tuple ('f' is the local filter or None)
    @ivar rest:  the rest of the statements (an array)
    @ivar clash: intialized to False
    @type clash: Boolean
    @ivar bound:  True or False depending on whether all variables are bound in self.binding
    @type bound: Boolean
    @ivar optionalTrees: expansion trees for optional statements
    @type optionalTrees: array of _SPARQLNode instances
    """
    def __init__(self,parent,bindings,statements,tripleStore) :
        """
        @param parent:     parent node
        @param bindings:   a dictionary with the bindings that are already done or with None value if no binding yet
        @param statements: array of statements from the 'where' clause. The first element is
        for the current node, the rest for the children. If empty, then no
        expansion occurs (ie, the node is a leaf)
        @param tripleStore: the 'owner' triple store
        @type tripleStore: L{sparqlGraph<rdflib.sparql.sparqlGraph.sparqlGraph>}
        """
        self.tripleStore         = tripleStore
        self.bindings            = bindings
        self.optionalTrees       = []
        if None in bindings.values() :
            self.bound = False
        else :
            self.bound = True
        self.clash     = False

        self.parent    = parent
        self.children  = []

        if len(statements) > 0 :
            self.statement = statements[0]
            self.rest      = statements[1:]
        else :
            self.statement = None
            self.rest      = None

    def returnResult(self,select) :
        """
        Collect the result by search the leaves of the the tree. The
        variables in the select are exchanged against their bound
        equivalent (if applicable). This action is done on the valid
        leaf nodes only, the intermediate nodes only gather the
        children's results and combine it in one array.

        @param select: the array of unbound variables in the original
        select that do not appear in any of the optionals. If None,
        the full binding should be considered (this is the case for
        the SELECT * feature of SPARQL)
        @returns: an array of dictionaries with non-None bindings.
        """
        if len(self.children) > 0 :
            # combine all the results of all the kids into one array
            retval = []
            for c in self.children :
                res = c.returnResult(select)
                # res is a list of dictionaries, so each tuple should be taken out and added to the result
                for t in res :
                    retval.append(t)
            return retval
        else :
            retval = []
            if self.bound == True and self.clash == False :
                # This node should be able to contribute to the final results:
                result = {}
                # This where the essential happens: the binding values are used to construct the selection result
                if select :
                    for a in select :
                        if a in self.bindings :
                            result[a] = self.bindings[a]
                else :
                    result = self.bindings.copy()
                # Initial return block. If there is no optional processing, that is the result, in fact,
                # because the for cycle below will not happen
                retval = [result]
                # The following remark in the SPARQL document is important at this point:
                # "If a new variable is mentioned in an optional block (as mbox and hpage are mentioned
                #  in the previous example), that variable can be mentioned in that block and can not be
                #  mentioned in a subsequent block."
                # What this means is that the various optional blocks do not interefere at this point
                # and there is no need for a check whether a binding in a subsequent block
                # clashes with an earlier optional block.
                # The API checks this at the start.
                # What happens here is that the result of the optional expantion is added to what is already
                # there. Note that this may lead to a duplication of the result so far, if there are several
                # alternatives returned by the optionals!
                for optTree in self.optionalTrees :
                    # get the results from the optional Tree...
                    optionals = optTree.returnResult(select)
                    # ... and extend the results accumulated so far with the new bindings
                    # It is worth separating the case when there is only one optional block; it avoids
                    # unnecessary copying
                    if len(optionals) == 0 :
                        # no contribution at all :-(
                        continue
                    elif len(optionals) == 1 :
                        optResult = optionals[0]
                        for res in retval :
                            for k in optResult :
                                if optResult[k] != None :
                                    res[k] = optResult[k]
                    else :
                        newRetval = []
                        for optResult in optionals :
                            # Each binding dictionary we have so far should be copied with the new values
                            for res in retval :
                                dct = {}
                                # copy the content of the exisiting bindings ...
                                dct = res.copy()
                                # ... and extend it with the optional results
                                for k in optResult :
                                    if optResult[k] != None :
                                        dct[k] = optResult[k]
                                newRetval.append(dct)
                        retval = newRetval
            return retval


    def expandSubgraph(self,subTriples,pattern) :
        """
        Method used to collect the results. There are two ways to
        invoke the method:

          - if the pattern argument is not None, then this means the
          construction of a separate triple store with the
          results. This means taking the bindings in the node, and
          constuct the graph via the
          L{construct<rdflib.sparql.graphPattern.GraphPattern.construct>}
          method. This happens on the valid leafs; intermediate nodes
          call the same method recursively - otherwise, a leaf returns
          an array of the bindings, and intermediate methods aggregate
          those.

        In both cases, leaf nodes may successifely expand the optional
        trees that they may have.

        @param subTriples: the triples so far
        @type subTriples: L{sparqlGraph<rdflib.sparql.sparqlGraph.sparqlGraph>}
        @param pattern: a graph pattern used to construct a graph
        @type pattern: L{GraphPattern<rdflib.sparql.graphPattern.GraphPattern>}
        @return: if pattern is not None, an array of binding dictionaries
        """
        def b(r,bind) :
            if type(r) == str :
                val = bind[r]
                if val == None :
                    raise RuntimeError()
                return bind[r]
            else :
                return r
        if len(self.children) > 0 :
            # all children return an array of bindings (each element being a dictionary)
            if pattern == None :
                retval = reduce(lambda x,y: x+y, [x.expandSubgraph(subTriples,None) for x in self.children],[])
                (s,p,o,func) = self.statement
                for bind in retval :
                    try :
                        st = (b(s,bind),b(p,bind),b(o,bind))
                        subTriples.add(st)
                    except :
                        # any exception means a None value creeping in, or something similar..
                        pass
                return retval
            else :
                for x in self.children :
                    x.expandSubgraph(subTriples,pattern)
        else :
            # return the local bindings if any. Not the optional trees should be added, too!
            if self.bound == True and self.clash == False :
                # Get the possible optional branches:
                for t in self.optionalTrees :
                    t.expandSubgraph(subTriples,pattern)
                if pattern == None :
                    return [self.bindings]
                else :
                    pattern.construct(subTriples,self.bindings)
            else :
                return []


    def _bind(self,r) :
        """
        @param r: string
        @return: returns None if no bindings occured yet, the binding otherwise
        """
        if isinstance(r,basestring) and not isinstance(r,Identifier)  :
            if self.bindings[r] == None :
                return None
            else :
                return self.bindings[r]
        elif isinstance(r,(SessionBNode)):
            return r            
        elif isinstance(r,(BNode)):
            return self.bindings.get(r)            
        else :
            return r

    def expand(self,constraints) :
        """
        The expansion itself. See class comments for details.

        @param constraints: array of global constraining (filter) methods
        """
        # if there are no more statements, that means that the constraints have been fully expanded
        if self.statement :
            # decompose the statement into subject, predicate and object
            # default setting for the search statement
            # see if subject (resp. predicate and object) is already bound. This
            # is done by taking over the content of self.dict if not None and replacing
            # the subject with that binding
            # the (search_subject,search_predicate,search_object) is then created
            (s,p,o,func) = self.statement
            # put the bindings we have so far into the statement; this may add None values,
            # but that is exactly what RDFLib uses in its own search methods!
            (search_s,search_p,search_o) = (self._bind(s),self._bind(p),self._bind(o))
            if self.tripleStore.graphVariable:
                assert hasattr(self.tripleStore.graph,'quads'),\
                  "Graph graph patterns can only be used with Graph instances with a quad method"
                
                searchRT = self.tripleStore.graph.quads((search_s,search_p,search_o))
            else:
                searchRT = self.tripleStore.graph.triples((search_s,search_p,search_o))
            for tripleOrQuad in searchRT:
            #for (result_s,result_p,result_o) in self.tripleStore.graph.triples((search_s,search_p,search_o)) :
                if self.tripleStore.graphVariable:
                    (result_s,result_p,result_o,parentGraph) = tripleOrQuad
                else:
                    (result_s,result_p,result_o) = tripleOrQuad
                # if a user defined constraint has been added, it should be checked now
                if func != None and func(result_s,result_p,result_o) == False :
                    # Oops, this result is not acceptable, jump over it!
                    continue
                # create a copy of the current bindings, by also adding the new ones from result of the search
                new_bindings = self.bindings.copy()
                if search_s == None : new_bindings[s] = result_s
                if search_p == None : new_bindings[p] = result_p
                if search_o == None : new_bindings[o] = result_o
                if self.tripleStore.graphVariable:
                    new_bindings[self.tripleStore.graphVariable] = parentGraph.identifier

                # Recursion starts here: create and expand a new child
                child = _SPARQLNode(self,new_bindings,self.rest,self.tripleStore)
                child.expand(constraints)                
                # if the child is a clash then no use adding it to the tree, it can be forgotten
                if self.clash == False :
                    self.children.append(child)

            if len(self.children) == 0 :
                # this means that the constraints could not be met at all with this binding!!!!
                self.clash = True
        else :
            # this is if all bindings are done; the conditions (ie, global constraints) are still to be checked
            if self.bound == True and self.clash == False :
                for func in constraints :
                    if func(self.bindings) == False :
                        self.clash = True
                        break

    def expandOptions(self,bindings,statements,constraints) :
        """
        Managing optional statements. These affect leaf nodes only, if
        they contain 'real' results. A separate Expansion tree is
        appended to such a node, one for each optional call.

        @param bindings: current bindings dictionary

        @param statements: array of statements from the 'where'
        clause. The first element is for the current node, the rest
        for the children. If empty, then no expansion occurs (ie, the
        node is a leaf). The bindings at this node are taken into
        account (replacing the unbound variables with the real
        resources) before expansion

        @param constraints: array of constraint (filter) methods
        """
        def replace(key,resource,tupl) :
            s,p,o,func = tupl
            if key == s : s = resource
            if key == p : p = resource
            if key == o : o = resource
            return (s,p,o,func)

        if len(self.children) == 0  :
            # this is a leaf in the original expansion
            if self.bound == True and self.clash == False :
                # see if the optional bindings can be reduced because they are already
                # bound by this node
                toldBNodeLookup = {}
                for key in self.bindings :
                    normalizedStatements = []
                    for t in statements:
                        val = self.bindings[key]
                        if isinstance(val,BNode) and val not in toldBNodeLookup:
                            toldBNodeLookup[val] = val
                        normalizedStatements.append(replace(key,self.bindings[key],t))
                    statements = normalizedStatements
                    if key in bindings :
                        del bindings[key]
                bindings.update(toldBNodeLookup)
                optTree = _SPARQLNode(None,bindings,statements,self.tripleStore)
                self.optionalTrees.append(optTree)
                optTree.expand(constraints)
        else :
            for c in self.children :
                c.expandOptions(bindings,statements,constraints)


def _processResults(select,arr) :
    '''
    The result in an expansion node is in the form of an array of
    binding dictionaries.  The caller should receive an array of
    tuples, each tuple representing the final binding (or None) I{in
    the order of the original select}. This method is the last step of
    processing by processing these values to produce the right result.

    @param select: the original selection list. If None, then the
    binding should be taken as a whole (this corresponds to the SELECT * feature of SPARQL)
    @param arr: the array of bindings
    @type arr:
    an array of dictionaries
    @return: a list of tuples with the selection results
    '''
    retval = []
    if select :
        for bind in arr :
            # each result binding must be taken separately
            qresult = []
            for s in select :
                if s in bind :
                    qresult.append(bind[s])
                else :
                    qresult.append(None)
            # as a courtesy to the user, if the selection has one single element only, than we do no
            # put in a tuple, just add it that way:
            if len(select) == 1 :
                retval.append(qresult[0])
            else :
                retval.append(tuple(qresult))
    else :
        # this is the case corresponding to a SELECT * query call
        for bind in arr:
            qresult = [val for key,val in bind.items()]
            if len(qresult) == 1 :
                retval.append(qresult[0])
            else :
                retval.append(tuple(qresult))
    return retval


def query(graph, selection, patterns, optionalPatterns=[], initialBindings = {}) :
    """
    A shorthand for the creation of a L{Query} instance, returning
    the result of a L{Query.select} right away. Good for most of
    the usage, when no more action (clustering, etc) is required.

    @param selection: a list or tuple with the selection criteria,
    or a single string. Each entry is a string that begins with a"?".

    @param patterns: either a
    L{GraphPattern<rdflib.sparql.graphPattern.GraphPattern>}
    instance or a list of instances thereof. Each pattern in the
    list represent an 'OR' (or 'UNION') branch in SPARQL.

    @param optionalPatterns: either a
    L{GraphPattern<rdflib.sparql.graphPattern.GraphPattern>}
    instance or a list of instances thereof. For each elements in
    the 'patterns' parameter is combined with each of the optional
    patterns and the results are concatenated. The list may be
    empty.

    @return: list of query results
    @rtype: list of tuples
    """
    result = queryObject(graph, patterns,optionalPatterns,initialBindings)
    if result == None :
        # generate some proper output for the exception :-)
        msg = "Errors in the patterns, no valid query object generated; "
        if isinstance(patterns,GraphPattern) :
            msg += ("pattern:\n%s" % patterns)
        else :
            msg += ("pattern:\n%s\netc..." % patterns[0])
        raise SPARQLError(msg)
    return result.select(selection)

def queryObject(graph, patterns, optionalPatterns=[], initialBindings = None) :
    """
    Creation of a L{Query} instance.

    @param patterns: either a
    L{GraphPattern<rdflib.sparql.graphPattern.GraphPattern>}
    instance or a list of instances thereof. Each pattern in the
    list represent an 'OR' (or 'UNION') branch in SPARQL.

    @param optionalPatterns: either a
    L{GraphPattern<rdflib.sparql.graphPattern.GraphPattern>}
    instance or a list of instances thereof. For each elements in
    the 'patterns' parameter is combined with each of the optional
    patterns and the results are concatenated. The list may be
    empty.

    @return: Query object
    @rtype: L{Query}
    """
    def checkArg(arg,error) :
        if arg == None :
            return []
        elif isinstance(arg,GraphPattern) :
            return [arg]
        elif type(arg) == list or type(arg) == tuple :
            for p in arg :
                if not isinstance(p,GraphPattern) :
                    raise SPARQLError("'%s' argument must be a GraphPattern or a list of those" % error)
            return arg
        else :
            raise SPARQLError("'%s' argument must be a GraphPattern or a list of those" % error)

    finalPatterns         = checkArg(patterns,"patterns")
    finalOptionalPatterns = checkArg(optionalPatterns,"optionalPatterns")

    retval = None
    if not initialBindings:
        initialBinding = {}
    for pattern in finalPatterns :
        # Check whether the query strings in the optional clauses are fine. If a problem occurs,
        # an exception is raised by the function
        _checkOptionals(pattern,finalOptionalPatterns)
        bindings = _createInitialBindings(pattern)
        if initialBindings:
            bindings.update(initialBindings)
        # This is the crucial point: the creation of the expansion tree and the expansion. That
        # is where the real meal is, we had only an apetizer until now :-)
        top = _SPARQLNode(None,bindings,pattern.patterns, graph)
        top.expand(pattern.constraints)
        for opt in finalOptionalPatterns :
            bindings = _createInitialBindings(opt)
            if initialBindings:
                bindings.update(initialBindings)
            top.expandOptions(bindings,opt.patterns,opt.constraints)
        r = Query(top, graph)
        if retval == None :
            retval = r
        else :
            # This branch is, effectively, the UNION clause of the draft
            retval = retval + r
    return retval


class Query :
    """
    Result of a SPARQL query. It stores to the top of the query tree, and allows some subsequent
    inquiries on the expanded tree. B{This class should not be
    instantiated by the user,} it is done by the L{queryObject<SPARQL.queryObject>} method.

    """
    def __init__(self,sparqlnode,triples,parent1=None,parent2=None) :
        """
        @param sparqlnode: top of the expansion tree
        @type sparqlnode: _SPARQLNode
        @param triples: triple store
        @type triples: L{sparqlGraph<rdflib.sparql.sparqlGraph>}
        @param parent1: possible parent Query when queries are combined by summing them up
        @type parent1: L{Query}
        @param parent2: possible parent Query when queries are combined by summing them up
        @type parent2: L{Query}
        """
        self.top             = sparqlnode
        self.triples         = triples
        # if this node is the result of a sum...
        self.parent1         = parent1
        self.parent2         = parent2

    def __add__(self,other) :
        """This may be useful when several queries are performed and
        one wants the 'union' of those.  Caveat: the triple store must
        be the same for each argument. This method is used internally
        only anyway...  Efficiency trick (I hope it works): the
        various additions on subgraphs are not done here; the results
        are calculated only if really necessary, ie, in a lazy
        evaluation manner.  This is achieved by storing self and the
        'other' in the new object
        """
        return Query(None,self.triples,self,other)

    def _getFullBinding(self) :
        """Retrieve the full binding, ie, an array of binding dictionaries
        """
        if self.parent1 != None and self.parent2 != None :
            return self.parent1._getFullBinding() + self.parent2._getFullBinding()
        else :
            # remember: returnResult returns an array of dictionaries
            return self.top.returnResult(None)

    def _getAllVariables(self):
       """Retrieve the list of all variables, to be returned"""
       if self.parent1 and self.parent2:
           return list2set(self.parent1._getAllVariables() + self.parent2._getAllVariables())
       else:
           return list2set(self.top.bindings.keys())

    def _orderedSelect(self,selection,orderedBy,orderDirection) :
        """
        The variant of the selection (as below) that also includes the sorting. Because that is much less efficient, this is
        separated into a distinct method that is called only if necessary. It is called from the L{select<select>} method.
		
        Because order can be made on variables that are not part of the final selection, this method retrieves a I{full}
        binding from the result to be able to order it (whereas the core L{select<select>} method retrieves from the result
        the selected bindings only). The full binding is an array of (binding) dictionaries; the sorting sorts this array
        by comparing the bound variables in the respective dictionaries. Once this is done, the final selection is done.

        @param selection: Either a single query string, or an array or tuple thereof.
        @param orderBy: either a function or a list of strings (corresponding to variables in the query). If None, no sorting occurs
        on the results. If the parameter is a function, it must take two dictionary arguments (the binding dictionaries), return
        -1, 0, and 1, corresponding to smaller, equal, and greater, respectively.
        @param orderDirection: if not None, then an array of integers of the same length as orderBy, with values the constants
        ASC or DESC (defined in the module). If None, an ascending order is used.
        @return: selection results
        @rtype: list of tuples
        @raise SPARQLError: invalid sorting arguments
        """
        fullBinding = self._getFullBinding()
        if type(orderedBy) is types.FunctionType :
            _sortBinding = orderedBy
        else :
            orderKeys = _variablesToArray(orderedBy,"orderBy")
            # see the direction
            oDir = None # this is just to fool the interpreter's error message
            if orderDirection is None :
                oDir = [ True for i in xrange(0,len(orderKeys)) ]
            elif type(orderDirection) is types.BooleanType :
                oDir = [ orderDirection ]
            elif type(orderDirection) is not types.ListType and type(orderDirection) is not types.TupleType :
                raise SPARQLError("'orderDirection' argument must be a list")
            elif len(orderDirection) != len(orderKeys) :
                raise SPARQLError("'orderDirection' must be of an equal length to 'orderBy'")
            else :
                oDir = orderDirection
            def _sortBinding(b1,b2) :
                """The sorting method used by the array sort, with return values as required by the python run-time
                The to-be-compared data are dictionaries of bindings
                """
                for i in xrange(0,len(orderKeys)) :
					# each key has to be compared separately. If there is a clear comparison result on that key
					# then we are done, but when that is not the case, the next in line should be used
                    key       = orderKeys[i]
                    direction = oDir[i]
                    if key in b1 and key in b2 :
                        val1 = b1[key]
                        val2 = b2[key]
                        if val1 != None and val2 != None :
                            if direction :
                                if   val1 < val2 : return -1
                                elif val1 > val2 : return 1
                            else :
                                if   val1 > val2 : return -1
                                elif val1 < val2 : return 1
                return 0
        # get the full Binding sorted
        fullBinding.sort(_sortBinding)
        # remember: _processResult turns the expansion results (an array of dictionaries)
        # into an array of tuples in the right, original order
        retval = _processResults(selection,fullBinding)
        return retval

    def select(self,selection,distinct=True,limit=None,orderBy=None,orderAscend=None,offset=0) :
        """
        Run a selection on the query.

        @param selection: Either a single query string, or an array or tuple thereof.
        @param distinct: if True, identical results are filtered out
        @type distinct: Boolean
        @param limit: if set to an integer value, the first 'limit' number of results are returned; all of them otherwise
        @type limit: non negative integer
        @param orderBy: either a function or a list of strings (corresponding to variables in the query). If None, no sorting occurs
        on the results. If the parameter is a function, it must take two dictionary arguments (the binding dictionaries), return
        -1, 0, and 1, corresponding to smaller, equal, and greater, respectively.
        @param orderAscend: if not None, then an array of booelans of the same length as orderBy, True for ascending and False
		for descending. If None, an ascending order is used.
        @offset the starting point of return values in the array of results. Obviously, this parameter makes real sense if
        some sort of order is defined.
        @return: selection results
        @rtype: list of tuples
        @raise SPARQLError: invalid selection argument
        """
        def _uniquefyList(lst) :
            """Return a copy of the list but possible duplicate elements are taken out. Used to
            post-process the outcome of the query
            @param lst: input list
            @return: result list
            """
            if len(lst) <= 1 :
                return lst
            else :
                # must be careful! Using the quick method of Sets destroy the order. Ie, if this was ordered, then
                # a slower but more secure method should be used
                if orderBy != None :
                    retval = []
                    for i in xrange(0,len(lst)) :
                        v = lst[i]
                        skip = False
                        for w in retval :
                            if w == v :
                                skip = True
                                break
                        if not skip :
                            retval.append(v)
                    return retval
                else :
                    return list(sets.Set(lst))
        # Select may be a single query string, or an array/tuple thereof
        selectionF = _variablesToArray(selection,"selection")

        if type(offset) is not types.IntType or offset < 0 :
            raise SPARQLError("'offset' argument is invalid")

        if limit != None :
            if type(limit) is not types.IntType or limit < 0 :
                raise SPARQLError("'offset' argument is invalid")

        if orderBy != None :
            results = self._orderedSelect(selectionF,orderBy,orderAscend)
        else :
            if self.parent1 != None and self.parent2 != None :
                results = self.parent1.select(selectionF) + self.parent2.select(selectionF)
            else :
                # remember: _processResult turns the expansion results (an array of dictionaries)
                # into an array of tuples in the right, original order
                results = _processResults(selectionF,self.top.returnResult(selectionF))
        if distinct :
            retval = _uniquefyList(results)
        else :
            retval = results

        if limit != None :
            return retval[offset:limit+offset]
        elif offset > 0 :
            return retval[offset:]
        else :
            return retval

    def construct(self,pattern=None) :
        """
        Expand the subgraph based on the pattern or, if None, the
        internal bindings.

        In the former case the binding is used to instantiate the
        triplets in the patterns; in the latter, the original
        statements are used as patterns.

        The result is a separate triple store containing the subgraph.

        @param pattern: a L{GraphPattern<rdflib.sparql.graphPattern.GraphPattern>} instance or None
        @return: a new triple store
        @rtype: L{sparqlGraph<rdflib.sparql.sparqlGraph>}
        """
        if self.parent1 != None and self.parent2 != None :
            return self.parent1.construct(pattern) + self.parent2.construct(pattern)
        else :
            subgraph = SPARQLGraph()
            self.top.expandSubgraph(subgraph,pattern)
            return subgraph

    def ask(self) :
        """
        Whether a specific pattern has a solution or not.
        @rtype: Boolean
        """
        return len(self.select('*')) != 0

    #########################################################################################################
    # The methods below are not really part of SPARQL, or may be used to a form of DESCRIBE. However, that latter
    # is still in a flux in the draft, so we leave it here, pending

    def clusterForward(self,selection) :
        """
        Forward clustering, using all the results of the query as
        seeds (when appropriate). It is based on the usage of the
        L{cluster forward<rdflib.sparql.sparqlGraph.clusterForward>}
        method for triple store.

        @param selection: a selection to define the seeds for
        clustering via the selection; the result of select used for
        the clustering seed

        @return: a new triple store
        @rtype: L{sparqlGraph<rdflib.sparql.sparqlGraph>}
        """
        if self.parent1 != None and self.parent2 != None :
            return self.parent1.clusterForward(selection) + self.parent2.clusterForward(selection)
        else :
            clusterF = SPARQLGraph()
            for r in reduce(lambda x,y: list(x) + list(y),self.select(selection),()) :
                try :
                    check_subject(r)
                    self.triples.clusterForward(r,clusterF)
                except :
                    # no real problem, this is a literal, just forget about it
                    continue
            return clusterF

    def clusterBackward(self,selection) :
        """
        Backward clustering, using all the results of the query as
        seeds (when appropriate). It is based on the usage of the
        L{cluster backward<rdflib.sparql.sparqlGraph.clusterBackward>}
        method for triple store.

        @param selection: a selection to define the seeds for
        clustering via the selection; the result of select used for
        the clustering seed

        @return: a new triple store
        @rtype: L{sparqlGraph<rdflib.sparql.sparqlGraph>}
        """
        if self.parent1 != None and self.parent2 != None :
            return self.parent1.clusterBackward(selection) + self.parent2.clusterBackward(selection)
        else :
            clusterB = SPARQLGraph()
            # to be on the safe side, see if the query has been properly finished
            for r in reduce(lambda x,y: list(x) + list(y),self.select(selection),()) :
                self.triples.clusterBackward(r,clusterB)
            return clusterB

    def cluster(self,selection) :
        """
        Cluster: a combination of L{Query.clusterBackward} and
        L{Query.clusterForward}.  @param selection: a selection to
        define the seeds for clustering via the selection; the result
        of select used for the clustering seed
        """
        return self.clusterBackward(selection) + self.clusterForward(selection)

    def describe(self,selection,forward=True,backward=True) :
        """
        The DESCRIBE Form in the SPARQL draft is still in state of
        flux, so this is just a temporary method, in fact.  It may not
        correspond to what the final version of describe will be (if
        it stays in the draft at all, that is).  At present, it is
        simply a wrapper around L{cluster}.

        @param selection: a selection to define the seeds for
        clustering via the selection; the result of select used for
        the clustering seed

        @param forward: cluster forward yes or no
        @type forward: Boolean
        @param backward: cluster backward yes or no
        @type backward: Boolean
        """
        if forward and backward :
            return self.cluster(selection)
        elif forward :
            return self.clusterForward(selection)
        elif backward :
            return self.clusterBackward(selection)
        else :
            return SPARQLGraph()


########NEW FILE########
__FILENAME__ = QueryResult
from rdflib import QueryResult,URIRef,BNode,Literal, Namespace
from xml.dom import XML_NAMESPACE
from xml.sax.saxutils import XMLGenerator
from xml.sax.xmlreader import AttributesNSImpl
from cStringIO import StringIO

SPARQL_XML_NAMESPACE = u'http://www.w3.org/2005/sparql-results#'

try:
    from Ft.Xml import MarkupWriter
    class SPARQLXMLWriter:
        """
        4Suite-based SPARQL XML Writer
        """
        def __init__(self,output):
            self.writer = MarkupWriter(output, indent=u"yes")
            self.writer.startDocument()
            self.writer.startElement(u'sparql',namespace=SPARQL_XML_NAMESPACE)

        def write_header(self,allvarsL):
            self.writer.startElement(u'head', namespace=SPARQL_XML_NAMESPACE)
            for i in xrange(0,len(allvarsL)) :
                self.writer.startElement(u'variable',namespace=SPARQL_XML_NAMESPACE,attributes={u'name':unicode(allvarsL[i][1:])})
                self.writer.endElement(u'variable')
            self.writer.endElement( u'head')

        def write_results_header(self,orderBy,distinct):
            self.writer.startElement(u'results',namespace=SPARQL_XML_NAMESPACE,attributes={u'ordered' : unicode(orderBy and 'true' or 'false'),
                                                                                           u'distinct': unicode(distinct and 'true' or 'false')})

        def write_start_result(self):
            self.writer.startElement(u'result',namespace=SPARQL_XML_NAMESPACE)
            self._resultStarted = True

        def write_end_result(self):
            assert self._resultStarted
            self.writer.endElement(u'result',namespace=SPARQL_XML_NAMESPACE)
            self._resultStarted = False

        def write_binding(self,name,val):
            assert self._resultStarted
            if val:
                attrs = {u'name':unicode(name)}
                self.writer.startElement(u'binding', namespace=SPARQL_XML_NAMESPACE, attributes=attrs)
                if isinstance(val,URIRef) :
                    self.writer.startElement(u'uri', namespace=SPARQL_XML_NAMESPACE)
                    self.writer.text(val)
                    self.writer.endElement(u'uri')
                elif isinstance(val,BNode) :
                    self.writer.startElement(u'bnode', namespace=SPARQL_XML_NAMESPACE)
                    self.writer.text(val)
                    self.writer.endElement(u'bnode')
                elif isinstance(val,Literal) :
                    attrs = {}
                    if val.language :
                        attrs[(u'lang', XML_NAMESPACE)] = unicode(val.language)
                    elif val.datatype:
                        attrs[u'datatype'] = unicode(val.datatype)
                    self.writer.startElement(u'literal', namespace=SPARQL_XML_NAMESPACE, attributes=attrs)
                    self.writer.text(val)
                    self.writer.endElement(u'literal')

                else:
                    raise Exception("Unsupported RDF term: %s"%val)

                self.writer.endElement(u'binding')

        def close(self):
            self.writer.endElement(u'results')
            self.writer.endElement(u'sparql')
except:
    class SPARQLXMLWriter:
        """
        Python saxutils-based SPARQL XML Writer
        """
        def __init__(self, output, encoding='utf-8'):
            writer = XMLGenerator(output, encoding)
            writer.startDocument()
            writer.startPrefixMapping(u'sparql',SPARQL_XML_NAMESPACE)
            writer.startPrefixMapping(u'xml', XML_NAMESPACE)
            writer.startElementNS((SPARQL_XML_NAMESPACE, u'sparql'), u'sparql', AttributesNSImpl({}, {}))
            self.writer = writer
            self._output = output
            self._encoding = encoding

        def write_header(self,allvarsL):
            self.writer.startElementNS((SPARQL_XML_NAMESPACE, u'head'), u'head', AttributesNSImpl({}, {}))
            for i in xrange(0,len(allvarsL)) :
                attr_vals = {
                    (None, u'name'): unicode(allvarsL[i][1:]),
                    }
                attr_qnames = {
                    (None, u'name'): u'name',
                    }
                self.writer.startElementNS((SPARQL_XML_NAMESPACE, u'variable'),
                                             u'variable',
                                             AttributesNSImpl(attr_vals, attr_qnames))
                self.writer.endElementNS((SPARQL_XML_NAMESPACE, u'variable'), u'variable')
            self.writer.endElementNS((SPARQL_XML_NAMESPACE, u'head'), u'head')

        def write_results_header(self,orderBy,distinct):
            attr_vals = {
                (None, u'ordered')  : unicode(orderBy and 'true' or 'false'),
                (None, u'distinct') : unicode(distinct and 'true' or 'false'),
                }
            attr_qnames = {
                (None, u'ordered')  : u'ordered',
                (None, u'distinct') : u'distinct'
                }
            self.writer.startElementNS((SPARQL_XML_NAMESPACE, u'results'),
                                         u'results',
                                         AttributesNSImpl(attr_vals, attr_qnames))

        def write_start_result(self):
            self.writer.startElementNS(
                    (SPARQL_XML_NAMESPACE, u'result'), u'result', AttributesNSImpl({}, {}))
            self._resultStarted = True

        def write_end_result(self):
            assert self._resultStarted
            self.writer.endElementNS((SPARQL_XML_NAMESPACE, u'result'), u'result')
            self._resultStarted = False

        def write_binding(self,name,val):
            assert self._resultStarted
            if val:
                attr_vals = {
                    (None, u'name')  : unicode(name),
                    }
                attr_qnames = {
                    (None, u'name')  : u'name',
                    }
                self.writer.startElementNS((SPARQL_XML_NAMESPACE, u'binding'),
                                       u'binding',
                                       AttributesNSImpl(attr_vals, attr_qnames))

                if isinstance(val,URIRef) :
                    self.writer.startElementNS((SPARQL_XML_NAMESPACE, u'uri'),
                                           u'uri',
                                           AttributesNSImpl({}, {}))
                    self.writer.characters(val)
                    self.writer.endElementNS((SPARQL_XML_NAMESPACE, u'uri'),u'uri')
                elif isinstance(val,BNode) :
                    self.writer.startElementNS((SPARQL_XML_NAMESPACE, u'bnode'),
                                           u'bnode',
                                           AttributesNSImpl({}, {}))
                    self.writer.characters(val)
                    self.writer.endElementNS((SPARQL_XML_NAMESPACE, u'bnode'),u'bnode')
                elif isinstance(val,Literal) :
                    attr_vals = {}
                    attr_qnames = {}
                    if val.language :
                        attr_vals[(XML_NAMESPACE, u'lang')] = val.language
                        attr_qnames[(XML_NAMESPACE, u'lang')] = u"xml:lang"
                    elif val.datatype:
                        attr_vals[(None,u'datatype')] = val.datatype
                        attr_qnames[(None,u'datatype')] = u'datatype'

                    self.writer.startElementNS((SPARQL_XML_NAMESPACE, u'literal'),
                                           u'literal',
                                           AttributesNSImpl(attr_vals, attr_qnames))
                    self.writer.characters(val)
                    self.writer.endElementNS((SPARQL_XML_NAMESPACE, u'literal'),u'literal')

                else:
                    raise Exception("Unsupported RDF term: %s"%val)

                self.writer.endElementNS((SPARQL_XML_NAMESPACE, u'binding'),u'binding')

        def close(self):
            self.writer.endElementNS((SPARQL_XML_NAMESPACE, u'results'), u'results')
            self.writer.endElementNS((SPARQL_XML_NAMESPACE, u'sparql'), u'sparql')
            self.writer.endDocument()

def retToJSON(val):
    if isinstance(val,URIRef):
        return '"type": "uri", "value" : "%s"' % val
    elif isinstance(val,BNode) :
        return '"type": "bnode", "value" : "%s"' % val
    elif isinstance(val,Literal):
        if val.language != "":
            return '"type": "literal", "xml:lang" : "%s", "value" : "%s"' % (val.language, val)
            attr += ' xml:lang="%s" ' % val.language
        elif val.datatype != "" and val.datatype != None:
            return '"type": "typed=literal", "datatype" : "%s", "value" : "%s"' % (val.datatype, val)
        else:
            return '"type": "literal", "value" : "%s"' % val
    else:
        return '"type": "literal", "value" : "%s"' % val

def bindingJSON(name, val):
    if val == None:
        return ""
    retval = ''
    retval += '                   "%s" : {' % name
    retval += retToJSON(val)
#    retval += '}\n'
    return retval

class SPARQLQueryResult(QueryResult.QueryResult):
    """
    Query result class for SPARQL

    xml   : as an XML string conforming to the SPARQL XML result format: http://www.w3.org/TR/rdf-sparql-XMLres/
    python: as Python objects
    json  : as JSON
    graph : as an RDFLib Graph - for CONSTRUCT and DESCRIBE queries
    """
    def __init__(self,qResult):
        """
        The constructor is the result straight from sparql-p, which is uple of 1) a list of tuples
        (in select order, each item is the valid binding for the corresponding variable or 'None') for SELECTs
        , a SPARQLGraph for DESCRIBE/CONSTRUCT, and boolean for ASK  2) the variables selected 3) *all*
        the variables in the Graph Patterns 4) the order clause 5) the DISTINCT clause
        """
        result,selectionF,allVars,orderBy,distinct = qResult
        self.selected = result
        self.selectionF = selectionF
        self.allVariables = allVars
        self.orderBy = orderBy
        self.distinct = distinct

    def __len__(self):
        if isinstance(self.selected,list):
            return len(self.selected)
        else:
            return 1

    def __iter__(self):
        """Iterates over the result entries"""
        if isinstance(self.selected,list):
            for item in self.selected:
                if isinstance(item,basestring):
                    yield (item,)
                else:
                    yield item
        else:
            yield self.selected

    def serialize(self,format='xml'):
        if format == 'python':
            return self.selected
        elif format in ['json','xml']:
           retval = ""
           allvarsL = self.allVariables
           if format == "json" :
               retval += '    "results" : {\n'
               retval += '          "ordered" : %s,\n' % (self.orderBy and 'true' or 'false')
               retval += '          "distinct" : %s,\n' % (self.distinct and 'true' or 'false')
               retval += '          "bindings" : [\n'
               for i in xrange(0,len(self.selected)):
                   hit = self.selected[i]
                   retval += '               {\n'
                   bindings = []
                   if len(self.selectionF) == 0:
                        for j in xrange(0, len(allvarsL)):
                            b = bindingJSON(allvarsL[j][1:],hit[j])
                            if b != "":
                                bindings.append(b)
                   elif len(self.selectionF) == 1:
                       bindings.append(bindingJSON(self.selectionF[0][1:],hit))
                   else:
                        for j in xrange(0, len(self.selectionF)):
                            b = bindingJSON(self.selectionF[j][1:],hit[j])
                            if b != "":
                                bindings.append(b)
                           
                   retval += "},\n".join(bindings)
                   retval += "}\n"
                   retval += '                }'
                   if i != len(self.selected) -1:
                       retval += ',\n'
                   else:
                       retval += '\n'
               retval += '           ]\n'
               retval += '    }\n'
               retval += '}\n'
               
               selected_vars = self.selectionF
               
               if len(selected_vars) == 0:
                   selected_vars = allvarsL
                   
               header = ""
               header += '{\n'
               header += '   "head" : {\n        "vars" : [\n'
               for i in xrange(0,len(selected_vars)) :
                   header += '             "%s"' % selected_vars[i][1:]
                   if i == len(selected_vars) - 1 :
                       header += '\n'
                   else :
                       header += ',\n'
               header += '         ]\n'
               header += '    },\n'
               
               retval = header + retval
               
           elif format == "xml" :
               # xml output
               out = StringIO()
               writer = SPARQLXMLWriter(out)
               writer.write_header(allvarsL)
               writer.write_results_header(self.orderBy,self.distinct)
               for i in xrange(0,len(self.selected)) :
                   hit = self.selected[i]
                   if len(self.selectionF) == 0 :
                       writer.write_start_result()
                       if len(allvarsL) == 1:
                           hit = (hit,) # Not an iterable - a parser bug?
                       for j in xrange(0,len(allvarsL)) :
                           writer.write_binding(allvarsL[j][1:],hit[j])
                       writer.write_end_result()
                   elif len(self.selectionF) == 1 :
                       writer.write_start_result()
                       writer.write_binding(self.selectionF[0][1:],hit)
                       writer.write_end_result()
                   else:
                       writer.write_start_result()
                       for j in xrange(0,len(self.selectionF)) :
                           writer.write_binding(self.selectionF[j][1:],hit[j])
                       writer.write_end_result()
               writer.close()
               return out.getvalue()

           return retval
        else:
           raise Exception("Result format not implemented: %s"%format)


########NEW FILE########
__FILENAME__ = sparqlGraph
from rdflib.Graph import Graph


class SPARQLGraph(Graph):
    """
    A subclass of Graph with a few extra SPARQL bits.
    """
    def __init__(self, graph, graphVariable = None):
        self.graphVariable = graphVariable
        self.graph = graph # TODO
        store = graph.store
        identifier = graph.identifier
        super(SPARQLGraph, self).__init__(store, identifier)

    ##############################################################################################################
    # Clustering methods
    def _clusterForward(self,seed,Cluster) :
        """Cluster the triple store: from a seed, transitively get all
        properties and objects in direction of the arcs.

        @param seed: RDFLib Resource

        @param Cluster: a L{sparqlGraph} instance, that has to be
        expanded with the new arcs
        """
        try :
            # get all predicate and object pairs for the seed.
            # *If not yet in the new cluster, then go with a recursive round with those*
            for (p,o) in self.graph.predicate_objects(seed) :
                if not (seed,p,o) in Cluster.graph :
                    Cluster.add((seed,p,o))
                    self._clusterForward(p,Cluster)
                    self._clusterForward(o,Cluster)
        except :
            pass


    def clusterForward(self,seed,Cluster=None) :
        """
        Cluster the triple store: from a seed, transitively get all
        properties and objects in direction of the arcs.

        @param seed: RDFLib Resource

        @param Cluster: another sparqlGraph instance; if None, a new
        one will be created. The subgraph will be added to this graph.

        @returns: The triple store containing the cluster

        @rtype: L{sparqlGraph}
        """
        if Cluster == None :
            Cluster = SPARQLGraph()

        # This will raise an exception if not kosher...
        check_subject(seed) #print "Wrong type for clustering (probably a literal): %s" % seed
        self._clusterForward(seed,Cluster)
        return Cluster


    def _clusterBackward(self,seed,Cluster) :
        """Cluster the triple store: from a seed, transitively get all
        properties and objects in backward direction of the arcs.

        @param seed: RDFLib Resource

        @param Cluster: a L{sparqlGraph} instance, that has to be
        expanded with the new arcs
        """
        try :
            for (s,p) in self.graph.subject_predicates(seed) :
                if not (s,p,seed) in Cluster.graph :
                    Cluster.add((s,p,seed))
                    self._clusterBackward(s,Cluster)
                    self._clusterBackward(p,Cluster)
        except :
            pass

    def clusterBackward(self,seed,Cluster=None) :
        """
        Cluster the triple store: from a seed, transitively get all
        properties and objects 'backward', ie, following the link back
        in the graph.

        @param seed: RDFLib Resource

        @param Cluster: another sparqlGraph instance; if None, a new
        one will be created. The subgraph will be added to this graph.

        @returns: The triple store containing the cluster

        @rtype: L{sparqlGraph}
        """
        if Cluster == None :
            Cluster = SPARQLGraph()

        # This will raise an exception if not kosher...
        check_object(seed) # print "Wrong type for clustering: %s" % seed
        self._clusterBackward(seed,Cluster)
        return Cluster

    def cluster(self,seed) :
        """
        Cluster up and down, by summing up the forward and backward
        clustering

        @param seed: RDFLib Resource

        @returns: The triple store containing the cluster

        @rtype: L{sparqlGraph}
        """
        raise "Am I getting here?"
        return self.clusterBackward(seed) + self.clusterForward(seed)

########NEW FILE########
__FILENAME__ = sparqlOperators
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/11/04 14:06:36 $, by $Author: ivan $, $Revision: 1.1 $
#
##
# API for the SPARQL operators. The operators (eg, 'lt')
# return a <em>function</em> that can be added to the AND clause of a query. The parameters are either regular values
# or query strings. The resulting function has one parameter (the binding directory), it can be combined with others or
# be plugged to into an array of constraints. For example:
# <pre>
#   constraints = [lt("?m",42)]
#</pre>
# <p>for checking whether "?m" is smaller than the (integer) value 42. It can be combined using the lambda function, for
# example:</p>
# <pre>
#    constraints = [lambda(b) : lt("?m",42")(b) or lt("?n",134)(b)]
# </pre>
# <p>is the expression for:</p>
# <pre>
#    AND ?m < 42 || ?n < 134
# </pre>
# <p>(Clearly, the relative complexity is only on the API level; a SPARQL language parser that starts with a SPARQL
# expression can map on this API).</p>
#
##

import sys, os, re

from rdflib.Literal     import Literal
from rdflib.BNode       import BNode
from rdflib.URIRef      import URIRef

from rdflib.sparql.graphPattern import _createResource
from rdflib.sparql import _questChar, Debug
from rdflib.sparql.Unbound import Unbound

##
# Boolean test whether this is a a query string or not
# @param v the value to be checked
# @return True if it is a query string
def queryString(v) :
    return isinstance(v,basestring) and len(v) != 0 and v[0] == _questChar

##
# Return the value in a literal, making on the fly conversion on datatype (using the datatypes that are implemented)
# @param v the Literal to be converted
# @return the result of the conversion.
def getLiteralValue(v) :
    return v

##
# Returns a <em>value retrieval function</em>. The return value can be plugged in a query; it would return
# the value of param directly if param is a real value, and the run-time value if param is a query string of the type
# "?xxx". If no binding is defined at the time of call, the return value is None
# @param param query string, Unbound instance, or real value
# @return a function taking one parameter (the binding directory)
def getValue(param) :
    if isinstance(param,Unbound) :
        param = param.name
        unBound = True
    else :
        unBound = queryString(param)
        if not unBound :
            if isinstance(param,Literal) :
                value = getLiteralValue(param)
            elif callable(param):
                return param
            else :
                value = param
            return lambda(bindings): value
    def f(bindings) :
        if unBound :
            val = bindings[param]
            if isinstance(val,Literal) :
                return getLiteralValue(val)
            else :
                return val
        else :
            return value
    return f

##
# Operator for '&lt;'
# @param a value or query string
# @param b value or query string
# @return comparison method
def lt(a,b) :
    fa = getValue(a)
    fb = getValue(b)
    def f(bindings) :
        try :
            return fa(bindings) < fb(bindings)
        except:
            # this is the case when the operators are incompatible
            if Debug :
                (typ,val,traceback) = sys.exc_info()
                sys.excepthook(typ,val,traceback)
            return False
    return f

##
# Operator for '&lt;='
# @param a value or query string
# @param b value or query string
# @return comparison method
def le(a,b) :
    fa = getValue(a)
    fb = getValue(b)
    def f(bindings) :
        try :
            return fa(bindings) <= fb(bindings)
        except :
            # this is the case when the operators are incompatible
            if Debug :
                (typ,val,traceback) = sys.exc_info()
                sys.excepthook(typ,val,traceback)
            return False
    return f

##
# Operator for '&gt;'
# @param a value or query string
# @param b value or query string
# @return comparison method
def gt(a,b) :
    fa = getValue(a)
    fb = getValue(b)
    def f(bindings) :
        try :
            return fa(bindings) > fb(bindings)
        except :
            # this is the case when the operators are incompatible
            if Debug :
                (typ,val,traceback) = sys.exc_info()
                sys.excepthook(typ,val,traceback)
            return False
    return f

##
# Operator for '&gt;='
# @param a value or query string
# @param b value or query string
# @return comparison method
def ge(a,b) :
    fa = getValue(a)
    fb = getValue(b)
    def f(bindings) :
        try :
            return fa(bindings) >= fb(bindings)
        except :
            # this is the case when the operators are incompatible
            if Debug :
                (typ,val,traceback) = sys.exc_info()
                sys.excepthook(typ,val,traceback)
            return False
    return f

##
# Operator for '='
# @param a value or query string
# @param b value or query string
# @return comparison method
def eq(a,b) :
    fa = getValue(a)
    fb = getValue(b)
    def f(bindings) :
        try :
            return fa(bindings) == fb(bindings)
        except :
            # this is the case when the operators are incompatible
            if Debug :
                (typ,val,traceback) = sys.exc_info()
                sys.excepthook(typ,val,traceback)
            return False
    return f
##
# Operator for '!='
# @param a value or query string
# @param b value or query string
# @return comparison method
def neq(a,b) :
    fa = getValue(a)
    fb = getValue(b)
    def f(bindings) :
        try :
            return fa(bindings) != fb(bindings)
        except :
            # this is the case when the operators are incompatible
            if Debug :
                (typ,val,traceback) = sys.exc_info()
                sys.excepthook(typ,val,traceback)
            return False
    return f

def __getQueryString(v) :
    if isinstance(v,Unbound) :
        return v.name
    elif queryString(v) :
        return v
    else :
        return None


##
# Is the variable bound
# @param a value or query string
# @return check method
def bound(a) :
    v = __getQueryString(a)
    def f(bindings) :
        if v == None :
            return False
        if v in bindings :
            val = bindings[v]
            return not (val == None)
        else :
            return False
    return f

##
# Is the variable bound to a URIRef
# @param a value or query string
# @return check method
def isURI(a) :
    v = __getQueryString(a)
    def f(bindings) :
        if v == None :
            return False
        try :
            val = bindings[v]
            if val == None:
                return False
            else :
                return isinstance(val,URIRef)
        except :
            return False
    return f

##
# Is the variable bound to a IRIRef (this is just an alias for URIRef)
# @param a value or query string
# @return check method
def isIRI(a) :
    return isURI(a)

##
# Is the variable bound to a Blank Node
# @param a value or query string
# @return check method
def isBlank(a) :
    v = __getQueryString(a)
    def f(bindings) :
        if v == None :
            return False
        try :
            val = bindings[v]
            if val == None:
                return False
            else :
                return isinstance(val,BNode)
        except :
            return False
    return f

##
# Is the variable bound to a Literal
# @param a value or query string
# @return check method
def isLiteral(a) :
    v = __getQueryString(a)
    def f(bindings) :
        if v == None :
            return False
        try :
            val = bindings[v]
            if val == None:
                return False
            else :
                return isinstance(val,Literal)
        except :
            return False
    return f

##
# Return the string version of a resource
# @param a value or query string
# @return check method
def str(a) :
    v = __getQueryString(a)
    def f(bindings) :
        if v == None :
            return ""
        try :
            val = bindings[v]
            if val == None:
                return ""
            else :
                return `val`
        except :
            return ""
    return f

##
# Return the lang value of a literal
# @param a value or query string
# @return check method
def lang(a) :
    v = __getQueryString(a)
    def f(bindings) :
        if v == None: return ""
        try :
            val = bindings[v]
            if val == None:
                return ""
            else :
                return val.lang
        except :
            return ""
    return f

##
# Return the datatype URI of a literal
# @param a value or query string
# @return check method
def datatype(a) :
    v = __getQueryString(a)
    def f(bindings) :
        if v == None:
            if isinstance(a,Literal):
                return a.datatype
            else:
                return ""

        try :
            val = bindings[v]
            if val == None:
                return ""
            else :
                return val.datatype
        except :
            return ""
    return f


##
# Is a resource on a collection. The operator can be used to check whether
#    the 'item' is an element of the 'collection' (a.k.a. list). Both collection and item can
#    be a real resource or a query string.
# @param collection is either a query string (that has to be bound by the query) or an RDFLib Resource
# representing the collection
# @param item is either a query string (that has to be bound by the query), an RDFLib Resource, or
# a data type value that is turned into a corresponding Literal (with possible datatype)
# that must be tested to be part of the collection
# @defreturn a function
def isOnCollection(collection,item, triplets) :
    """Generate a method that can be used as a global constaint in sparql to check whether
    the 'item' is an element of the 'collection' (a.k.a. list). Both collection and item can
    be a real resource or a query string. Furthermore, item might be a plain string, that is
    then turned into a literal run-time.
    The method returns an adapted method.
    """
    #check_subject(collection)
    collUnbound = False
    if isinstance(collection,Unbound) :
        collUnbound = True
        collection  = collection.name
    elif queryString(collection) :
        # just keep 'collection', no reason to reassign
        collUnbound = True
    else:
        collUnbound = False
        # if we got here, this is a valid collection resource
    if isinstance(item,Unbound) :
        queryItem = item.name
        itUnbund  = True
    elif queryString(item) :
        queryItem = item
        itUnbound = True
    else :
        # Note that an exception is raised if the 'item' is invalid
        queryItem = _createResource(item)
        itUnbound = False
    def checkCollection(bindings) :
        try :
            if collUnbound == True :
                # the binding should come from the binding
                coll = bindings[collection]
            else :
                coll = collection
            if itUnbound == True :
                it = bindings[queryItem]
            else :
                it = queryItem
            return it in triplets.items(coll)
        except :
            # this means that the binding is not available. But that also means that
            # the global constraint was used, for example, with the optional triplets;
            # not available binding means that the method is irrelevant for those
            # ie, it should not become a show-stopper, hence it returns True
            return True
    return checkCollection


def addOperator(args,combinationArg):
    """
    SPARQL numeric + operator implemented via Python
    """
    return ' + '.join(["sparqlOperators.getValue(%s)%s"%(i,combinationArg and "(%s)"%combinationArg or '') for i in args])

def XSDCast(source,target=None):
    """
    XSD Casting/Construction Support
    For now (this may be an issue since Literal doesn't override comparisons) it simply creates
    a Literal with the target datatype using the 'lexical' value of the source
    """
    sFunc = getValue(source)
    def f(bindings):
        rt = sFunc(bindings)
        if isinstance(rt,Literal) and rt.datatype == target:
            #Literal already has target datatype
            return rt
        else:
            return Literal(rt,datatype=target)
    return f

def regex(item,pattern,flag=None):
    """
    Invokes the XPath fn:matches function to match text against a regular expression pattern.
    The regular expression language is defined in XQuery 1.0 and XPath 2.0 Functions and Operators section 7.6.1 Regular Expression Syntax
    """
    a = getValue(item)
    b = getValue(pattern)
    if flag:
        cFlag = 0
        usedFlags = []
        #Maps XPath REGEX flags (http://www.w3.org/TR/xpath-functions/#flags) to Python's re flags
        for fChar,_flag in [('i',re.IGNORECASE),('s',re.DOTALL),('m',re.MULTILINE)]:
            if fChar in flag and fChar not in usedFlags:
                cFlag |= _flag
                usedFlags.append(fChar)
        def f1(bindings):
            try:
                return bool(re.compile(b(bindings),cFlag).search(a(bindings)))
            except:
                return False
        return f1
    else:
        def f2(bindings):
            try:
                return bool(re.compile(b(bindings)).search(a(bindings)))
            except:
                return False
        return f2

    def f(bindings):
        try:
            print "%s %s"%(a(bindings),b(bindings))
            return bool(re.compile(a(bindings)).search(b(bindings)))
        except Exception,e:
            print e
            return False
    return f

########NEW FILE########
__FILENAME__ = Unbound
from rdflib.sparql import _questChar


class Unbound :
    """A class to encapsulate a query variable. This class should be used in conjunction with L{BasicGraphPattern<graphPattern.BasicGraphPattern>}."""
    def __init__(self,name) :
        """
        @param name: the name of the variable (without the '?' character)
        @type name: unicode or string
        """
        if isinstance(name,basestring) :
            self.name     = _questChar + name
            self.origName = name
        else :
            raise SPARQLError("illegal argument, variable name must be a string or unicode")

    def __repr__(self) :
        retval  = "?%s" % self.origName
        return retval

    def __str__(self) :
        return self.__repr__()



########NEW FILE########
__FILENAME__ = Statement
from rdflib.Node import Node


class Statement(Node, tuple):

    def __new__(cls, (subject, predicate, object), context):
        return tuple.__new__(cls, ((subject, predicate, object), context))

    def __reduce__(self):
        return (Statement, (self[0], self[1]))


########NEW FILE########
__FILENAME__ = AbstractSQLStore
from __future__ import generators
from rdflib import BNode
from rdflib import RDF
from rdflib.Literal import Literal
from rdflib.URIRef import URIRef
from rdflib.BNode import BNode
from pprint import pprint
import sha,sys, weakref
from rdflib.term_utils import *
from rdflib.Graph import QuotedGraph
from rdflib.store.REGEXMatching import REGEXTerm, PYTHON_REGEX
from rdflib.store import Store
Any = None

COUNT_SELECT   = 0
CONTEXT_SELECT = 1
TRIPLE_SELECT  = 2
TRIPLE_SELECT_NO_ORDER = 3

ASSERTED_NON_TYPE_PARTITION = 3
ASSERTED_TYPE_PARTITION     = 4
QUOTED_PARTITION            = 5
ASSERTED_LITERAL_PARTITION  = 6

FULL_TRIPLE_PARTITIONS = [QUOTED_PARTITION,ASSERTED_LITERAL_PARTITION]

INTERNED_PREFIX = 'kb_'

#Helper function for executing EXPLAIN on all dispatched SQL statements - for the pupose of analyzing
#index usage
def queryAnalysis(query,store,cursor):
    cursor.execute(store._normalizeSQLCmd('explain '+query))
    rt=cursor.fetchall()[0]
    table,joinType,posKeys,_key,key_len,comparedCol,rowsExamined,extra = rt
    if not _key:
        assert joinType == 'ALL'
        if not hasattr(store,'queryOptMarks'):
            store.queryOptMarks = {}
        hits = store.queryOptMarks.get(('FULL SCAN',table),0)
        store.queryOptMarks[('FULL SCAN',table)] = hits + 1

    if not hasattr(store,'queryOptMarks'):
        store.queryOptMarks = {}
    hits = store.queryOptMarks.get((_key,table),0)
    store.queryOptMarks[(_key,table)] = hits + 1


#Terms: u - uri refs  v - variables  b - bnodes l - literal f - formula

#Helper function for building union all select statement
#Takes a list of:
# - table name
# - table alias
# - table type (literal, type, asserted, quoted)
# - where clause string
def unionSELECT(selectComponents,distinct=False,selectType=TRIPLE_SELECT):
    selects = []
    for tableName,tableAlias,whereClause,tableType in selectComponents:

        if selectType == COUNT_SELECT:
            selectString = "select count(*)"
            tableSource = " from %s "%tableName
        elif selectType == CONTEXT_SELECT:
            selectString = "select %s.context"%tableAlias
            tableSource = " from %s as %s "%(tableName,tableAlias)
        elif tableType in FULL_TRIPLE_PARTITIONS:
            selectString = "select *"#%(tableAlias)
            tableSource = " from %s as %s "%(tableName,tableAlias)
        elif tableType == ASSERTED_TYPE_PARTITION:
            selectString =\
            """select %s.member as subject, "%s" as predicate, %s.klass as object, %s.context as context, %s.termComb as termComb, NULL as objLanguage, NULL as objDatatype"""%(tableAlias,RDF.type,tableAlias,tableAlias,tableAlias)
            tableSource = " from %s as %s "%(tableName,tableAlias)
        elif tableType == ASSERTED_NON_TYPE_PARTITION:
            selectString =\
            """select *,NULL as objLanguage, NULL as objDatatype"""
            tableSource = " from %s as %s "%(tableName,tableAlias)

        #selects.append('('+selectString + tableSource + whereClause+')')
        selects.append(selectString + tableSource + whereClause)

    orderStmt = ''
    if selectType == TRIPLE_SELECT:
        orderStmt = ' order by subject,predicate,object'
    if distinct:
        return ' union '.join(selects) + orderStmt
    else:
        return ' union all '.join(selects) + orderStmt

#Takes a tuple which represents an entry in a result set and
#converts it to a tuple of terms using the termComb integer
#to interpret how to instanciate each term
def extractTriple(tupleRt,store,hardCodedContext=None):
    subject,predicate,obj,rtContext,termComb,objLanguage,objDatatype = tupleRt
    context = rtContext is not None and rtContext or hardCodedContext.identifier
    termCombString=REVERSE_TERM_COMBINATIONS[termComb]
    subjTerm,predTerm,objTerm,ctxTerm = termCombString

    s=createTerm(subject,subjTerm,store)
    p=createTerm(predicate,predTerm,store)
    o=createTerm(obj,objTerm,store,objLanguage,objDatatype)

    graphKlass, idKlass = constructGraph(ctxTerm)
    return s,p,o,(graphKlass,idKlass,context)
#TODO: Stuff
#Takes a term value, term type, and store intance
#and Creates a term object.  QuotedGraphs are instanciated differently
def createTerm(termString,termType,store,objLanguage=None,objDatatype=None):
    if termType == 'L':
        cache = store.literalCache.get((termString,objLanguage,objDatatype))
        if cache is not None:
            #store.cacheHits += 1
            return cache
        else:
            #store.cacheMisses += 1
            rt = Literal(termString,objLanguage,objDatatype)
            store.literalCache[((termString,objLanguage,objDatatype))] = rt
            return rt
    elif termType=='F':
        cache = store.otherCache.get((termType,termString))
        if cache is not None:
            #store.cacheHits += 1
            return cache
        else:
            #store.cacheMisses += 1
            rt = QuotedGraph(store,URIRef(termString))
            store.otherCache[(termType,termString)] = rt
            return rt
    elif termType == 'B':
        cache = store.bnodeCache.get((termString))
        if cache is not None:
            #store.cacheHits += 1
            return cache
        else:
            #store.cacheMisses += 1
            rt = TERM_INSTANCIATION_DICT[termType](termString)
            store.bnodeCache[(termString)] = rt
            return rt
    elif termType =='U':
        cache = store.uriCache.get((termString))
        if cache is not None:
            #store.cacheHits += 1
            return cache
        else:
            #store.cacheMisses += 1
            rt = URIRef(termString)
            store.uriCache[(termString)] = rt
            return rt
    else:
        cache = store.otherCache.get((termType,termString))
        if cache is not None:
            #store.cacheHits += 1
            return cache
        else:
            #store.cacheMisses += 1
            rt = TERM_INSTANCIATION_DICT[termType](termString)
            store.otherCache[(termType,termString)] = rt
            return rt

class SQLGenerator:
    def executeSQL(self,cursor,qStr,params=None,paramList=False):
        """
        This takes the query string and parameters and (depending on the SQL implementation) either fill in
        the parameter in-place or pass it on to the Python DB impl (if it supports this).
        The default (here) is to fill the parameters in-place surrounding each param with quote characters
        """
        #print qStr,params
        if not params:
            cursor.execute(unicode(qStr))
        elif paramList:
            raise Exception("Not supported!")
        else:
            params = tuple([not isinstance(item,int) and u'"%s"'%item or item for item in params])
            cursor.execute(qStr%params)

    #FIXME:  This *may* prove to be a performance bottleneck and should perhaps be implemented in C (as it was in 4Suite RDF)
    def EscapeQuotes(self,qstr):
        """
        Ported from Ft.Lib.DbUtil
        """
        if qstr is None:
            return ''
        tmp = qstr.replace("\\","\\\\")
        tmp = tmp.replace("'", "\\'")
        return tmp

    #Normalize a SQL command before executing it.  Commence unicode black magic
    def _normalizeSQLCmd(self,cmd):
        import types
        if not isinstance(cmd, types.UnicodeType):
            cmd = unicode(cmd, 'ascii')

        return cmd.encode('utf-8')

    #Takes a term and 'normalizes' it.
    #Literals are escaped, Graphs are replaced with just their identifiers
    def normalizeTerm(self,term):
        if isinstance(term,(QuotedGraph,Graph)):
            return term.identifier.encode('utf-8')
        elif isinstance(term,Literal):
            return self.EscapeQuotes(term).encode('utf-8')
        elif term is None or isinstance(term,(list,REGEXTerm)):
            return term
        else:
            return term.encode('utf-8')

    #Builds an insert command for a type table
    def buildTypeSQLCommand(self,member,klass,context,storeId):
        #columns: member,klass,context
        rt= "INSERT INTO %s_type_statements"%storeId + " VALUES (%s, %s, %s,%s)"
        return rt,[
            self.normalizeTerm(member),
            self.normalizeTerm(klass),
            self.normalizeTerm(context.identifier),
            int(type2TermCombination(member,klass,context))]

    #Builds an insert command for literal triples (statements where the object is a Literal)
    def buildLiteralTripleSQLCommand(self,subject,predicate,obj,context,storeId):
        triplePattern = int(statement2TermCombination(subject,predicate,obj,context))
        literal_table = "%s_literal_statements"%storeId
        command="INSERT INTO %s "%literal_table +"VALUES (%s, %s, %s, %s, %s,%s,%s)"
        return command,[
            self.normalizeTerm(subject),
            self.normalizeTerm(predicate),
            self.normalizeTerm(obj),
            self.normalizeTerm(context.identifier),
            triplePattern,
            isinstance(obj,Literal) and obj.language or 'NULL',
            isinstance(obj,Literal) and obj.datatype or 'NULL']

    #Builds an insert command for regular triple table
    def buildTripleSQLCommand(self,subject,predicate,obj,context,storeId,quoted):
        stmt_table = quoted and "%s_quoted_statements"%storeId or "%s_asserted_statements"%storeId
        triplePattern = statement2TermCombination(subject,predicate,obj,context)
        if quoted:
            command="INSERT INTO %s"%stmt_table +" VALUES (%s, %s, %s, %s, %s,%s,%s)"
            params = [
                self.normalizeTerm(subject),
                self.normalizeTerm(predicate),
                self.normalizeTerm(obj),
                self.normalizeTerm(context.identifier),
                triplePattern,
                isinstance(obj,Literal) and  obj.language or 'NULL',
                isinstance(obj,Literal) and obj.datatype or 'NULL']
        else:
            command="INSERT INTO %s"%stmt_table + " VALUES (%s, %s, %s, %s, %s)"
            params = [
                self.normalizeTerm(subject),
                self.normalizeTerm(predicate),
                self.normalizeTerm(obj),
                self.normalizeTerm(context.identifier),
                triplePattern]
        return command,params

    #Builds WHERE clauses for the supplied terms and, context
    def buildClause(self,tableName,subject,predicate, obj,context=None,typeTable=False):
        parameters=[]
        if typeTable:
            rdf_type_memberClause = rdf_type_contextClause = rdf_type_contextClause = None

            clauseParts = self.buildTypeMemberClause(self.normalizeTerm(subject),tableName)
            if clauseParts is not None:
                rdf_type_memberClause = clauseParts[0]
                parameters.extend([param for param in clauseParts[-1] if param])

            clauseParts = self.buildTypeClassClause(self.normalizeTerm(obj),tableName)
            if clauseParts is not None:
                rdf_type_klassClause = clauseParts[0]
                parameters.extend(clauseParts[-1])

            clauseParts = self.buildContextClause(context,tableName)
            if clauseParts is not None:
                rdf_type_contextClause = clauseParts[0]
                parameters.extend([param for param in clauseParts[-1] if param])

            typeClauses = [rdf_type_memberClause,rdf_type_klassClause,rdf_type_contextClause]
            clauseString = ' and '.join([clause for clause in typeClauses if clause])
            clauseString = clauseString and 'where '+clauseString or ''
        else:
            subjClause = predClause = objClause = contextClause = litDTypeClause = litLanguageClause = None

            clauseParts = self.buildSubjClause(self.normalizeTerm(subject),tableName)
            if clauseParts is not None:
                subjClause = clauseParts[0]
                parameters.extend([param for param in clauseParts[-1] if param])

            clauseParts = self.buildPredClause(self.normalizeTerm(predicate),tableName)
            if clauseParts is not None:
                predClause = clauseParts[0]
                parameters.extend([param for param in clauseParts[-1] if param])

            clauseParts = self.buildObjClause(self.normalizeTerm(obj),tableName)
            if clauseParts is not None:
                objClause = clauseParts[0]
                parameters.extend([param for param in clauseParts[-1] if param])

            clauseParts = self.buildContextClause(context,tableName)
            if clauseParts is not None:
                contextClause = clauseParts[0]
                parameters.extend([param for param in clauseParts[-1] if param])

            clauseParts = self.buildLitDTypeClause(obj,tableName)
            if clauseParts is not None:
                litDTypeClause = clauseParts[0]
                parameters.extend([param for param in clauseParts[-1] if param])

            clauseParts = self.buildLitLanguageClause(obj,tableName)
            if clauseParts is not None:
                litLanguageClause = clauseParts[0]
                parameters.extend([param for param in clauseParts[-1] if param])

            clauses=[subjClause,predClause,objClause,contextClause,litDTypeClause,litLanguageClause]
            clauseString = ' and '.join([clause for clause in clauses if clause])
            clauseString = clauseString and 'where '+clauseString or ''

        return clauseString, [p for p in parameters if p]

    def buildLitDTypeClause(self,obj,tableName):
        if isinstance(obj,Literal):
            return obj.datatype is not None and ("%s.objDatatype="%(tableName)+"%s",[obj.datatype.encode('utf-8')]) or None
        else:
            return None

    def buildLitLanguageClause(self,obj,tableName):
        if isinstance(obj,Literal):
            return obj.language is not None and ("%s.objLanguage="%(tableName)+"%s",[obj.language.encode('utf-8')]) or None
        else:
            return None

    #Stubs for Clause Functions that are overridden by specific implementations (MySQL vs SQLite for instance)
    def buildSubjClause(self,subject,tableName):
        pass
    def buildPredClause(self,predicate,tableName):
        pass
    def buildObjClause(self,obj,tableName):
        pass
    def buildContextClause(self,context,tableName):
        pass
    def buildTypeMemberClause(self,subject,tableName):
        pass
    def buildTypeClassClause(self,obj,tableName):
        pass

class AbstractSQLStore(SQLGenerator,Store):
    """
    SQL-92 formula-aware implementation of an rdflib Store.
    It stores it's triples in the following partitions:

    - Asserted non rdf:type statements
    - Asserted literal statements
    - Asserted rdf:type statements (in a table which models Class membership)
    The motivation for this partition is primarily query speed and scalability as most graphs will always have more rdf:type statements than others
    - All Quoted statements

    In addition it persists namespace mappings in a seperate table
    """
    context_aware = True
    formula_aware = True
    transaction_aware = True
    regex_matching = PYTHON_REGEX
    autocommit_default = True

    #Stubs for overidden

    def __init__(self, identifier=None, configuration=None):
        """
        identifier: URIRef of the Store. Defaults to CWD
        configuration: string containing infomation open can use to
        connect to datastore.
        """
        self.identifier = identifier and identifier or 'hardcoded'
        #Use only the first 10 bytes of the digest
        self._internedId = INTERNED_PREFIX + sha.new(self.identifier).hexdigest()[:10]

        #This parameter controls how exlusively the literal table is searched
        #If true, the Literal partition is searched *exclusively* if the object term
        #in a triple pattern is a Literal or a REGEXTerm.  Note, the latter case
        #prevents the matching of URIRef nodes as the objects of a triple in the store.
        #If the object term is a wildcard (None)
        #Then the Literal paritition is searched in addition to the others
        #If this parameter is false, the literal partition is searched regardless of what the object
        #of the triple pattern is
        self.STRONGLY_TYPED_TERMS = False

        if configuration is not None:
            self.open(configuration)

        self.cacheHits = 0
        self.cacheMisses = 0

        self.literalCache = {}
        self.uriCache = {}
        self.bnodeCache = {}
        self.otherCache = {}
        self._db = None

    def close(self, commit_pending_transaction=False):
        """
        FIXME:  Add documentation!!
        """
        if commit_pending_transaction:
            self._db.commit()
        self._db.close()

    #Triple Methods
    def add(self, (subject, predicate, obj), context=None, quoted=False):
        """ Add a triple to the store of triples. """
        c=self._db.cursor()
        if self.autocommit_default:
            c.execute("""SET AUTOCOMMIT=0""")
        if quoted or predicate != RDF.type:
            #quoted statement or non rdf:type predicate
            #check if object is a literal
            if isinstance(obj,Literal):
                addCmd,params=self.buildLiteralTripleSQLCommand(subject,predicate,obj,context,self._internedId)
            else:
                addCmd,params=self.buildTripleSQLCommand(subject,predicate,obj,context,self._internedId,quoted)
        elif predicate == RDF.type:
            #asserted rdf:type statement
            addCmd,params=self.buildTypeSQLCommand(subject,obj,context,self._internedId)
        self.executeSQL(c,addCmd,params)
        c.close()

    def addN(self,quads):
        c=self._db.cursor()
        if self.autocommit_default:
            c.execute("""SET AUTOCOMMIT=0""")
        literalTriples = []
        typeTriples = []
        otherTriples = []
        literalTripleInsertCmd = None
        typeTripleInsertCmd = None
        otherTripleInsertCmd = None
        for subject,predicate,obj,context in quads:
            if isinstance(context,QuotedGraph) or predicate != RDF.type:
                #quoted statement or non rdf:type predicate
                #check if object is a literal
                if isinstance(obj,Literal):
                    cmd,params=self.buildLiteralTripleSQLCommand(subject,predicate,obj,context,self._internedId)
                    literalTripleInsertCmd = literalTripleInsertCmd is not None and literalTripleInsertCmd or cmd
                    literalTriples.append(params)
                else:
                    cmd,params=self.buildTripleSQLCommand(subject,predicate,obj,context,self._internedId,isinstance(context,QuotedGraph))
                    otherTripleInsertCmd = otherTripleInsertCmd is not None and otherTripleInsertCmd or cmd
                    otherTriples.append(params)
            elif predicate == RDF.type:
                #asserted rdf:type statement
                cmd,params=self.buildTypeSQLCommand(subject,obj,context,self._internedId)
                typeTripleInsertCmd = typeTripleInsertCmd is not None and typeTripleInsertCmd or cmd
                typeTriples.append(params)

        if literalTriples:
            self.executeSQL(c,literalTripleInsertCmd,literalTriples,paramList=True)
        if typeTriples:
            self.executeSQL(c,typeTripleInsertCmd,typeTriples,paramList=True)
        if otherTriples:
            self.executeSQL(c,otherTripleInsertCmd,otherTriples,paramList=True)

        c.close()

    def remove(self, (subject, predicate, obj), context):
        """ Remove a triple from the store """
        if context is not None:
            if subject is None and predicate is None and object is None:
                self._remove_context(context)
                return
        c=self._db.cursor()
        if self.autocommit_default:
            c.execute("""SET AUTOCOMMIT=0""")
        quoted_table="%s_quoted_statements"%self._internedId
        asserted_table="%s_asserted_statements"%self._internedId
        asserted_type_table="%s_type_statements"%self._internedId
        literal_table = "%s_literal_statements"%self._internedId
        if not predicate or predicate != RDF.type:
            #Need to remove predicates other than rdf:type

            if not self.STRONGLY_TYPED_TERMS or isinstance(obj,Literal):
                #remove literal triple
                clauseString,params = self.buildClause(literal_table,subject,predicate, obj,context)
                if clauseString:
                    cmd ="DELETE FROM " + " ".join([literal_table,clauseString])
                else:
                    cmd ="DELETE FROM " + literal_table
                self.executeSQL(c,self._normalizeSQLCmd(cmd),params)

            for table in [quoted_table,asserted_table]:
                #If asserted non rdf:type table and obj is Literal, don't do anything (already taken care of)
                if table == asserted_table and isinstance(obj,Literal):
                    continue
                else:
                    clauseString,params = self.buildClause(table,subject,predicate,obj,context)
                    if clauseString:
                        cmd="DELETE FROM " + " ".join([table,clauseString])
                    else:
                        cmd = "DELETE FROM " + table

                    self.executeSQL(c,self._normalizeSQLCmd(cmd),params)

        if predicate == RDF.type or not predicate:
            #Need to check rdf:type and quoted partitions (in addition perhaps)
            clauseString,params = self.buildClause(asserted_type_table,subject,RDF.type,obj,context,True)
            if clauseString:
                cmd="DELETE FROM " + " ".join([asserted_type_table,clauseString])
            else:
                cmd='DELETE FROM '+asserted_type_table

            self.executeSQL(c,self._normalizeSQLCmd(cmd),params)

            clauseString,params = self.buildClause(quoted_table,subject,predicate, obj,context)
            if clauseString:
                cmd=clauseString and "DELETE FROM " + " ".join([quoted_table,clauseString])
            else:
                cmd = "DELETE FROM " + quoted_table

            self.executeSQL(c,self._normalizeSQLCmd(cmd),params)
        c.close()

    def triples(self, (subject, predicate, obj), context=None):
        """
        A generator over all the triples matching pattern. Pattern can
        be any objects for comparing against nodes in the store, for
        example, RegExLiteral, Date? DateRange?

        quoted table:                <id>_quoted_statements
        asserted rdf:type table:     <id>_type_statements
        asserted non rdf:type table: <id>_asserted_statements

        triple columns: subject,predicate,object,context,termComb,objLanguage,objDatatype
        class membership columns: member,klass,context termComb

        FIXME:  These union all selects *may* be further optimized by joins

        """
        quoted_table="%s_quoted_statements"%self._internedId
        asserted_table="%s_asserted_statements"%self._internedId
        asserted_type_table="%s_type_statements"%self._internedId
        literal_table = "%s_literal_statements"%self._internedId
        c=self._db.cursor()

        parameters = []

        if predicate == RDF.type:
            #select from asserted rdf:type partition and quoted table (if a context is specified)
            clauseString,params = self.buildClause('typeTable',subject,RDF.type, obj,context,True)
            parameters.extend(params)
            selects = [
                (
                  asserted_type_table,
                  'typeTable',
                  clauseString,
                  ASSERTED_TYPE_PARTITION
                ),
            ]

        elif isinstance(predicate,REGEXTerm) and predicate.compiledExpr.match(RDF.type) or not predicate:
            #Select from quoted partition (if context is specified), literal partition if (obj is Literal or None) and asserted non rdf:type partition (if obj is URIRef or None)
            selects = []
            if not self.STRONGLY_TYPED_TERMS or isinstance(obj,Literal) or not obj or (self.STRONGLY_TYPED_TERMS and isinstance(obj,REGEXTerm)):
                clauseString,params = self.buildClause('literal',subject,predicate,obj,context)
                parameters.extend(params)
                selects.append((
                  literal_table,
                  'literal',
                  clauseString,
                  ASSERTED_LITERAL_PARTITION
                ))
            if not isinstance(obj,Literal) and not (isinstance(obj,REGEXTerm) and self.STRONGLY_TYPED_TERMS) or not obj:
                clauseString,params = self.buildClause('asserted',subject,predicate,obj,context)
                parameters.extend(params)
                selects.append((
                  asserted_table,
                  'asserted',
                  clauseString,
                  ASSERTED_NON_TYPE_PARTITION
                ))

            clauseString,params = self.buildClause('typeTable',subject,RDF.type,obj,context,True)
            parameters.extend(params)
            selects.append(
                (
                  asserted_type_table,
                  'typeTable',
                  clauseString,
                  ASSERTED_TYPE_PARTITION
                )
            )


        elif predicate:
            #select from asserted non rdf:type partition (optionally), quoted partition (if context is speciied), and literal partition (optionally)
            selects = []
            if not self.STRONGLY_TYPED_TERMS or isinstance(obj,Literal) or not obj or (self.STRONGLY_TYPED_TERMS and isinstance(obj,REGEXTerm)):
                clauseString,params = self.buildClause('literal',subject,predicate,obj,context)
                parameters.extend(params)
                selects.append((
                  literal_table,
                  'literal',
                  clauseString,
                  ASSERTED_LITERAL_PARTITION
                ))
            if not isinstance(obj,Literal) and not (isinstance(obj,REGEXTerm) and self.STRONGLY_TYPED_TERMS) or not obj:
                clauseString,params = self.buildClause('asserted',subject,predicate,obj,context)
                parameters.extend(params)
                selects.append((
                  asserted_table,
                  'asserted',
                  clauseString,
                  ASSERTED_NON_TYPE_PARTITION
                ))

        if context is not None:
            clauseString,params = self.buildClause('quoted',subject,predicate, obj,context)
            parameters.extend(params)
            selects.append(
                (
                  quoted_table,
                  'quoted',
                  clauseString,
                  QUOTED_PARTITION
                )
            )


        q=self._normalizeSQLCmd(unionSELECT(selects))
        self.executeSQL(c,q,parameters)
        rt = c.fetchone()
        while rt:
            s,p,o,(graphKlass,idKlass,graphId) = extractTriple(rt,self,context)
            currentContext=graphKlass(self,idKlass(graphId))
            contexts = [currentContext]
            rt = next = c.fetchone()
            sameTriple = next and extractTriple(next,self,context)[:3] == (s,p,o)
            while sameTriple:
                s2,p2,o2,(graphKlass,idKlass,graphId) = extractTriple(next,self,context)
                c2 = graphKlass(self,idKlass(graphId))
                contexts.append(c2)
                rt = next = c.fetchone()
                sameTriple = next and extractTriple(next,self,context)[:3] == (s,p,o)

            yield (s,p,o),(c for c in contexts)

    def triples_choices(self, (subject, predicate, object_),context=None):
        """
        A variant of triples that can take a list of terms instead of a single
        term in any slot.  Stores can implement this to optimize the response time
        from the import default 'fallback' implementation, which will iterate
        over each term in the list and dispatch to tripless
        """
        if isinstance(object_,list):
            assert not isinstance(subject,list), "object_ / subject are both lists"
            assert not isinstance(predicate,list), "object_ / predicate are both lists"
            if not object_:
                object_ = None
            for (s1, p1, o1), cg in self.triples((subject,predicate,object_),context):
                yield (s1, p1, o1), cg

        elif isinstance(subject,list):
            assert not isinstance(predicate,list), "subject / predicate are both lists"
            if not subject:
                subject = None
            for (s1, p1, o1), cg in self.triples((subject,predicate,object_),context):
                yield (s1, p1, o1), cg

        elif isinstance(predicate,list):
            assert not isinstance(subject,list), "predicate / subject are both lists"
            if not predicate:
                predicate = None
            for (s1, p1, o1), cg in self.triples((subject,predicate,object_),context):
                yield (s1, p1, o1), cg


    def __repr__(self):
        c=self._db.cursor()
        quoted_table="%s_quoted_statements"%self._internedId
        asserted_table="%s_asserted_statements"%self._internedId
        asserted_type_table="%s_type_statements"%self._internedId
        literal_table = "%s_literal_statements"%self._internedId

        selects = [
            (
              asserted_type_table,
              'typeTable',
              '',
              ASSERTED_TYPE_PARTITION
            ),
            (
              quoted_table,
              'quoted',
              '',
              QUOTED_PARTITION
            ),
            (
              asserted_table,
              'asserted',
              '',
              ASSERTED_NON_TYPE_PARTITION
            ),
            (
              literal_table,
              'literal',
              '',
              ASSERTED_LITERAL_PARTITION
            ),
        ]
        q=unionSELECT(selects,distinct=False,selectType=COUNT_SELECT)
        self.executeSQL(c,self._normalizeSQLCmd(q))
        rt=c.fetchall()
        typeLen,quotedLen,assertedLen,literalLen = [rtTuple[0] for rtTuple in rt]
        return "<Parititioned MySQL N3 Store: %s contexts, %s classification assertions, %s quoted statements, %s property/value assertions, and %s other assertions>"%(len([c for c in self.contexts()]),typeLen,quotedLen,literalLen,assertedLen)

    def __len__(self, context=None):
        """ Number of statements in the store. """
        c=self._db.cursor()
        quoted_table="%s_quoted_statements"%self._internedId
        asserted_table="%s_asserted_statements"%self._internedId
        asserted_type_table="%s_type_statements"%self._internedId
        literal_table = "%s_literal_statements"%self._internedId

        parameters = []
        quotedContext = assertedContext = typeContext = literalContext = None

        clauseParts = self.buildContextClause(context,quoted_table)
        if clauseParts:
            quotedContext,params = clauseParts
            parameters.extend([p for p in params if p])

        clauseParts = self.buildContextClause(context,asserted_table)
        if clauseParts:
            assertedContext,params = clauseParts
            parameters.extend([p for p in params if p])

        clauseParts = self.buildContextClause(context,asserted_type_table)
        if clauseParts:
            typeContext ,params = clauseParts
            parameters.extend([p for p in params if p])

        clauseParts = self.buildContextClause(context,literal_table)
        if clauseParts:
            literalContext,params = clauseParts
            parameters.extend([p for p in params if p])

        if context is not None:
            selects = [
                (
                  asserted_type_table,
                  'typeTable',
                  typeContext and 'where ' + typeContext or '',
                  ASSERTED_TYPE_PARTITION
                ),
                (
                  quoted_table,
                  'quoted',
                  quotedContext and 'where ' + quotedContext or '',
                  QUOTED_PARTITION
                ),
                (
                  asserted_table,
                  'asserted',
                  assertedContext and 'where ' + assertedContext or '',
                  ASSERTED_NON_TYPE_PARTITION
                ),
                (
                  literal_table,
                  'literal',
                  literalContext and 'where ' + literalContext or '',
                  ASSERTED_LITERAL_PARTITION
                ),
            ]
            q=unionSELECT(selects,distinct=True,selectType=COUNT_SELECT)
        else:
            selects = [
                (
                  asserted_type_table,
                  'typeTable',
                  typeContext and 'where ' + typeContext or '',
                  ASSERTED_TYPE_PARTITION
                ),
                (
                  asserted_table,
                  'asserted',
                  assertedContext and 'where ' + assertedContext or '',
                  ASSERTED_NON_TYPE_PARTITION
                ),
                (
                  literal_table,
                  'literal',
                  literalContext and 'where ' + literalContext or '',
                  ASSERTED_LITERAL_PARTITION
                ),
            ]
            q=unionSELECT(selects,distinct=False,selectType=COUNT_SELECT)

        self.executeSQL(c,self._normalizeSQLCmd(q),parameters)
        rt=c.fetchall()
        c.close()
        return reduce(lambda x,y: x+y,  [rtTuple[0] for rtTuple in rt])

    def contexts(self, triple=None):
        c=self._db.cursor()
        quoted_table="%s_quoted_statements"%self._internedId
        asserted_table="%s_asserted_statements"%self._internedId
        asserted_type_table="%s_type_statements"%self._internedId
        literal_table = "%s_literal_statements"%self._internedId

        parameters = []

        if triple is not None:
            subject,predicate,obj=triple
            if predicate == RDF.type:
                #select from asserted rdf:type partition and quoted table (if a context is specified)
                clauseString,params = self.buildClause('typeTable',subject,RDF.type, obj,Any,True)
                parameters.extend(params)
                selects = [
                    (
                      asserted_type_table,
                      'typeTable',
                      clauseString,
                      ASSERTED_TYPE_PARTITION
                    ),
                ]

            elif isinstance(predicate,REGEXTerm) and predicate.compiledExpr.match(RDF.type) or not predicate:
                #Select from quoted partition (if context is specified), literal partition if (obj is Literal or None) and asserted non rdf:type partition (if obj is URIRef or None)
                clauseString,params = self.buildClause('typeTable',subject,RDF.type,obj,Any,True)
                parameters.extend(params)
                selects = [
                    (
                      asserted_type_table,
                      'typeTable',
                      clauseString,
                      ASSERTED_TYPE_PARTITION
                    ),
                ]

                if not self.STRONGLY_TYPED_TERMS or isinstance(obj,Literal) or not obj or (self.STRONGLY_TYPED_TERMS and isinstance(obj,REGEXTerm)):
                    clauseString,params = self.buildClause('literal',subject,predicate,obj)
                    parameters.extend(params)
                    selects.append((
                      literal_table,
                      'literal',
                      clauseString,
                      ASSERTED_LITERAL_PARTITION
                    ))
                if not isinstance(obj,Literal) and not (isinstance(obj,REGEXTerm) and self.STRONGLY_TYPED_TERMS) or not obj:
                    clauseString,params = self.buildClause('asserted',subject,predicate,obj)
                    parameters.extend(params)
                    selects.append((
                      asserted_table,
                      'asserted',
                      clauseString,
                      ASSERTED_NON_TYPE_PARTITION
                    ))

            elif predicate:
                #select from asserted non rdf:type partition (optionally), quoted partition (if context is speciied), and literal partition (optionally)
                selects = []
                if not self.STRONGLY_TYPED_TERMS or isinstance(obj,Literal) or not obj or (self.STRONGLY_TYPED_TERMS and isinstance(obj,REGEXTerm)):
                    clauseString,params = self.buildClause('literal',subject,predicate,obj)
                    parameters.extend(params)
                    selects.append((
                      literal_table,
                      'literal',
                      clauseString,
                      ASSERTED_LITERAL_PARTITION
                    ))
                if not isinstance(obj,Literal) and not (isinstance(obj,REGEXTerm) and self.STRONGLY_TYPED_TERMS) or not obj:
                    clauseString,params = self.buildClause('asserted',subject,predicate,obj)
                    parameters.extend(params)
                    selects.append((
                      asserted_table,
                      'asserted',
                      clauseString,
                      ASSERTED_NON_TYPE_PARTITION
                ))

            clauseString,params = self.buildClause('quoted',subject,predicate, obj)
            parameters.extend(params)
            selects.append(
                (
                  quoted_table,
                  'quoted',
                  clauseString,
                  QUOTED_PARTITION
                )
            )
            q=unionSELECT(selects,distinct=True,selectType=CONTEXT_SELECT)
        else:
            selects = [
                (
                  asserted_type_table,
                  'typeTable',
                  '',
                  ASSERTED_TYPE_PARTITION
                ),
                (
                  quoted_table,
                  'quoted',
                  '',
                  QUOTED_PARTITION
                ),
                (
                  asserted_table,
                  'asserted',
                  '',
                  ASSERTED_NON_TYPE_PARTITION
                ),
                (
                  literal_table,
                  'literal',
                  '',
                  ASSERTED_LITERAL_PARTITION
                ),
            ]
            q=unionSELECT(selects,distinct=True,selectType=CONTEXT_SELECT)

        self.executeSQL(c,self._normalizeSQLCmd(q),parameters)
        rt=c.fetchall()
        for context in [rtTuple[0] for rtTuple in rt]:
            yield context
        c.close()

    def _remove_context(self, identifier):
        """ """
        assert identifier
        c=self._db.cursor()
        if self.autocommit_default:
            c.execute("""SET AUTOCOMMIT=0""")
        quoted_table="%s_quoted_statements"%self._internedId
        asserted_table="%s_asserted_statements"%self._internedId
        asserted_type_table="%s_type_statements"%self._internedId
        literal_table = "%s_literal_statements"%self._internedId
        for table in [quoted_table,asserted_table,asserted_type_table,literal_table]:
            clauseString,params = self.buildContextClause(identifier,table)
            self.executeSQL(
                c,
                self._normalizeSQLCmd("DELETE from %s "%table + "where %s"%clauseString),
                [p for p in params if p]
            )
        c.close()

    # Optional Namespace methods
    #optimized interfaces (those needed in order to port Versa)
    def subjects(self, predicate=None, obj=None):
        """
        A generator of subjects with the given predicate and object.
        """
        raise Exception("Not implemented")

    #capable of taking a list of predicate terms instead of a single term
    def objects(self, subject=None, predicate=None):
        """
        A generator of objects with the given subject and predicate.
        """
        raise Exception("Not implemented")

    #optimized interfaces (others)
    def predicate_objects(self, subject=None):
        """
        A generator of (predicate, object) tuples for the given subject
        """
        raise Exception("Not implemented")

    def subject_objects(self, predicate=None):
        """
        A generator of (subject, object) tuples for the given predicate
        """
        raise Exception("Not implemented")

    def subject_predicates(self, object=None):
        """
        A generator of (subject, predicate) tuples for the given object
        """
        raise Exception("Not implemented")

    def value(self, subject, predicate=u'http://www.w3.org/1999/02/22-rdf-syntax-ns#value', object=None, default=None, any=False):
        """
        Get a value for a subject/predicate, predicate/object, or
        subject/object pair -- exactly one of subject, predicate,
        object must be None. Useful if one knows that there may only
        be one value.

        It is one of those situations that occur a lot, hence this
        'macro' like utility

        Parameters:
        -----------
        subject, predicate, object  -- exactly one must be None
        default -- value to be returned if no values found
        any -- if True:
                 return any value in the case there is more than one
               else:
                 raise UniquenessError"""
        raise Exception("Not implemented")



    #Namespace persistence interface implementation
    def bind(self, prefix, namespace):
        """ """
        c=self._db.cursor()
        try:
            c.execute("INSERT INTO %s_namespace_binds VALUES ('%s', '%s')"%(
                self._internedId,
                prefix,
                namespace)
            )
        except:
            pass
        c.close()

    def prefix(self, namespace):
        """ """
        c=self._db.cursor()
        c.execute("select prefix from %s_namespace_binds where uri = '%s'"%(
            self._internedId,
            namespace)
        )
        rt = [rtTuple[0] for rtTuple in c.fetchall()]
        c.close()
        return rt and rt[0] or None

    def namespace(self, prefix):
        """ """
        c=self._db.cursor()
        try:
            c.execute("select uri from %s_namespace_binds where prefix = '%s'"%(
                self._internedId,
                prefix)
                      )
        except:
            return None
        rt = [rtTuple[0] for rtTuple in c.fetchall()]
        c.close()
        return rt and rt[0] or None

    def namespaces(self):
        """ """
        c=self._db.cursor()
        c.execute("select prefix, uri from %s_namespace_binds where 1;"%(
            self._internedId
            )
        )
        rt=c.fetchall()
        c.close()
        for prefix,uri in rt:
            yield prefix,uri


    #Transactional interfaces
    def commit(self):
        """ """
        self._db.commit()

    def rollback(self):
        """ """
        self._db.rollback()

table_name_prefixes = [
    '%s_asserted_statements',
    '%s_type_statements',
    '%s_quoted_statements',
    '%s_namespace_binds',
    '%s_literal_statements'
]

########NEW FILE########
__FILENAME__ = AuditableStorage
"""
This wrapper intercepts calls through the store interface
And implements thread-safe logging of destructive operations (adds / removes) in reverse.
This is persisted on the store instance and the reverse operations are executed
In order to return the store to the state it was when the transaction began
Since the reverse operations are persisted on the store, the store itself acts
as a transaction.  Calls to commit or rollback, flush the list of reverse operations
This provides thread-safe atomicity and isolation (assuming concurrent operations occur with different
store instances), but no durability (transactions are persisted in memory and wont
 be available to reverse operations after the systeme fails): A and I out of ACID.
"""

from rdflib.store import Store
from rdflib.Graph import Graph, ConjunctiveGraph
from pprint import pprint
import threading

destructiveOpLocks = {
    'add':None,
    'remove':None,
}

class AuditableStorage(Store):
    def __init__(self, storage):
        self.storage = storage
        self.context_aware = storage.context_aware
        #NOTE: this store can't be formula_aware as it doesn't have enough info to reverse
        #The removal of a quoted statement
        self.formula_aware = False#storage.formula_aware
        self.transaction_aware = True #This is only half true
        self.reverseOps = []
        self.rollbackLock = threading.RLock()

    def open(self, configuration, create=True):
        return self.storage.open(configuration,create)

    def close(self, commit_pending_transaction=False):
        self.storage.close()

    def destroy(self, configuration):
        self.storage.destroy(configuration)

    def add(self, (subject, predicate, object_), context, quoted=False):
        lock = destructiveOpLocks['add']
        lock = lock and lock or threading.RLock()
        lock.acquire()
        context = context is not None and context.__class__(self.storage,context.identifier) or None
        ctxId = context is not None and context.identifier or None
        self.reverseOps.append((subject,predicate,object_,ctxId,'remove'))
        if (subject,predicate,object_,ctxId,'add') in self.reverseOps:
            self.reverseOps.remove((subject,predicate,object_,context,'add'))
        self.storage.add((subject, predicate, object_), context, quoted)
        lock.release()

    def remove(self, (subject, predicate, object_), context=None):
        lock = destructiveOpLocks['remove']
        lock = lock and lock or threading.RLock()
        lock.acquire()
        #Need to determine which quads will be removed if any term is a wildcard
        context = context is not None and context.__class__(self.storage,context.identifier) or None
        ctxId = context is not None and context.identifier or None
        if None in [subject,predicate,object_,context]:
            if ctxId:                
                for s,p,o in context.triples((subject,predicate,object_)):
                    if (s,p,o,ctxId,'remove') in self.reverseOps:
                        self.reverseOps.remove((s,p,o,ctxId,'remove'))
                    else:
                        self.reverseOps.append((s,p,o,ctxId,'add'))
            else:
                for s,p,o,ctx in ConjunctiveGraph(self.storage).quads((subject,predicate,object_)):
                    if (s,p,o,ctx.identifier,'remove') in self.reverseOps:
                        self.reverseOps.remove((s,p,o,ctx.identifier,'remove'))
                    else:
                        self.reverseOps.append((s,p,o,ctx.identifier,'add'))
                
        elif (subject,predicate,object_,ctxId,'add') in self.reverseOps:
            self.reverseOps.remove((subject,predicate,object_,ctxId,'add'))
        else:
            self.reverseOps.append((subject,predicate,object_,ctxId,'add'))
        self.storage.remove((subject,predicate,object_),context)
        lock.release()

    def triples(self, (subject, predicate, object_), context=None):
        context = context is not None and context.__class__(self.storage,context.identifier) or None
        for (s,p,o),cg in self.storage.triples((subject, predicate, object_), context):
            yield (s,p,o),cg

    def __len__(self, context=None):
        context = context is not None and context.__class__(self.storage,context.identifier) or None
        return self.storage.__len__(context)

    def contexts(self, triple=None):
        for ctx in self.storage.contexts(triple):
            yield ctx

    def bind(self, prefix, namespace):
        self.storage.bind(prefix, namespace)

    def prefix(self, namespace):
        return self.storage.prefix(namespace)

    def namespace(self, prefix):
        return self.storage.namespace(prefix)

    def namespaces(self):
        return self.storage.namespaces()

    def commit(self):
        self.storage.commit()
        self.reverseOps = []

    def rollback(self):
        #Aquire Rollback lock and apply reverse operations in the forward order
        self.rollbackLock.acquire()
        for subject,predicate,obj,context,op in self.reverseOps:
            if op == 'add':
                self.storage.add((subject,predicate,obj),Graph(self.storage,context))
            else:
                self.storage.remove((subject,predicate,obj),Graph(self.storage,context))

        self.reverseOps = []
        self.rollbackLock.release()

########NEW FILE########
__FILENAME__ = BerkeleyDB
import warnings
warnings.warn("This Store implementation is still being debugged. It is currently running out of db lockers after adding around 2k triples.")

from rdflib.store import Store, VALID_STORE, CORRUPTED_STORE, NO_STORE, UNKNOWN
from rdflib.URIRef import URIRef
from bsddb import db
from os import mkdir, rmdir, makedirs
from os.path import exists, abspath, join
from urllib import pathname2url
from threading import Thread
from time import sleep, time
import logging

SUPPORT_MULTIPLE_STORE_ENVIRON = False

_logger = logging.getLogger(__name__)


class BerkeleyDB(Store):
    """
    A transaction-capable BerkeleyDB implementation
    The major difference are:
      - a dbTxn attribute which is the transaction object used for all bsddb databases
      - All operations (put,delete,get) take the dbTxn instance
      - The actual directory used for the bsddb persistence is the name of the identifier as a subdirectory of the 'path'
      
    """
    context_aware = True
    formula_aware = True
    transaction_aware = True
    def __init__(self, configuration=None, identifier=None):
        self.__open = False
        self.__identifier = identifier and identifier or 'home'
        super(BerkeleyDB, self).__init__(configuration)
        self.configuration = configuration
        self._loads = self.node_pickler.loads
        self._dumps = self.node_pickler.dumps
        #This state is needed to handle all possible combinations of calls to tx methods (close/rollback/commit)
        self.__dbTxn = None

    def __get_identifier(self):
        return self.__identifier
    identifier = property(__get_identifier)

    def destroy(self, configuration):
        """
        Destroy the underlying bsddb persistence for this store
        """
        if SUPPORT_MULTIPLE_STORE_ENVIRON:
            fullDir = join(configuration,self.identifier)
        else:
            fullDir = configuration
        if exists(configuration):
            #From bsddb docs:
            #A DB_ENV handle that has already been used to open an environment 
            #should not be used to call the DB_ENV->remove function; a new DB_ENV handle should be created for that purpose.
            self.close()
            db.DBEnv().remove(fullDir,db.DB_FORCE)

    def open(self, path, create=True):
        if self.__open:
            return
        homeDir = path
        #NOTE: The identifeir is appended to the path as the location for the db
        #This provides proper isolation for stores which have the same path but different identifiers
        if SUPPORT_MULTIPLE_STORE_ENVIRON:
            fullDir = join(homeDir,self.identifier)
        else:
            fullDir = homeDir
        envsetflags  = db.DB_CDB_ALLDB
        envflags = db.DB_INIT_MPOOL | db.DB_INIT_LOCK | db.DB_THREAD | db.DB_INIT_TXN | db.DB_RECOVER
        if not exists(fullDir):
            if create==True:
                makedirs(fullDir)
                self.create(path)
            else:                
                return NO_STORE
        if self.__identifier is None:
            self.__identifier = URIRef(pathname2url(abspath(fullDir)))
        self.db_env = db_env = db.DBEnv()
        db_env.set_cachesize(0, 1024*1024*50) # TODO
        #db_env.set_lg_max(1024*1024)
        #db_env.set_flags(envsetflags, 1)
        db_env.open(fullDir, envflags | db.DB_CREATE,0)

        #Transaction object
        self.dbTxn = db_env.txn_begin()

        self.__open = True

        dbname = None
        dbtype = db.DB_BTREE
        dbopenflags = db.DB_THREAD

        dbmode = 0660
        dbsetflags   = 0

        # create and open the DBs
        self.__indicies = [None,] * 3
        self.__indicies_info = [None,] * 3
        for i in xrange(0, 3):
            index_name = to_key_func(i)(("s", "p", "o"), "c")
            index = db.DB(db_env)
            index.set_flags(dbsetflags)
            index.open(index_name, dbname, dbtype, dbopenflags|db.DB_CREATE, dbmode,txn=self.dbTxn)
            self.__indicies[i] = index
            self.__indicies_info[i] = (index, to_key_func(i), from_key_func(i))

        lookup = {}
        for i in xrange(0, 8):
            results = []
            for start in xrange(0, 3):
                score = 1
                len = 0
                for j in xrange(start, start+3):
                    if i & (1<<(j%3)):
                        score = score << 1
                        len += 1
                    else:
                        break
                tie_break = 2-start
                results.append(((score, tie_break), start, len))

            results.sort()
            score, start, len = results[-1]

            def get_prefix_func(start, end):
                def get_prefix(triple, context):
                    if context is None:
                        yield ""
                    else:
                        yield context
                    i = start
                    while i<end:
                        yield triple[i%3]
                        i += 1
                    yield ""
                return get_prefix

            lookup[i] = (self.__indicies[start], get_prefix_func(start, start + len), from_key_func(start), results_from_key_func(start, self._from_string))


        self.__lookup_dict = lookup

        self.__contexts = db.DB(db_env)
        self.__contexts.set_flags(dbsetflags)
        self.__contexts.open("contexts", dbname, dbtype, dbopenflags|db.DB_CREATE, dbmode,txn=self.dbTxn)

        self.__namespace = db.DB(db_env)
        self.__namespace.set_flags(dbsetflags)
        self.__namespace.open("namespace", dbname, dbtype, dbopenflags|db.DB_CREATE, dbmode,txn=self.dbTxn)

        self.__prefix = db.DB(db_env)
        self.__prefix.set_flags(dbsetflags)
        self.__prefix.open("prefix", dbname, dbtype, dbopenflags|db.DB_CREATE, dbmode,txn=self.dbTxn)

        self.__i2k = db.DB(db_env)
        self.__i2k.set_flags(dbsetflags)
        self.__i2k.open("i2k", dbname, db.DB_HASH, dbopenflags|db.DB_CREATE, dbmode,txn=self.dbTxn)

        self.__needs_sync = False
        t = Thread(target=self.__sync_run)
        t.setDaemon(True)
        t.start()
        self.__sync_thread = t
        return VALID_STORE

    def __sync_run(self):
        min_seconds, max_seconds = 10, 300
        while self.__open:
            if self.__needs_sync:
                t0 = t1 = time()
                self.__needs_sync = False
                while self.__open:
                    sleep(.1)
                    if self.__needs_sync:
                        t1 = time()
                        self.__needs_sync = False
                    if time()-t1 > min_seconds or time()-t0 > max_seconds:
                        self.__needs_sync = False
                        _logger.debug("sync")
                        self.sync()
                        break
            else:
                sleep(1)

    def sync(self):
        if self.__open:
            for i in self.__indicies:
                i.sync()
            self.__contexts.sync()
            self.__namespace.sync()
            self.__prefix.sync()
            self.__i2k.sync()
            #self.__k2i.sync()

    #Transactional interfaces
    def commit(self):
        """
        Bsddb tx objects cannot be reused after commit 
        """         
        if self.dbTxn:
            _logger.debug("commiting")
            self.dbTxn.commit(0)
            #Note a new transaction handle is created to support
            #subsequent commit calls (bsddb doesn't support multiple commits by the same
            #tx handle)
            self.dbTxn = self.db_env.txn_begin()
        else:
            _logger.warning("No transaction to commit")

    def rollback(self):
        """
        Bsddb tx objects cannot be reused after commit
        """           
        if self.dbTxn is not None:
            _logger.debug("rollingback")
            self.dbTxn.abort()
            #The dbTxn is set to None to indicate to a susequent close
            #call that a rollback is not needed
            self.dbTxn = None
        else:
            _logger.warning("No transaction to rollback")
        
    def __del__(self):
        """
        Redirects python's native garbage collection into Store.close 
        """
        self.close()    

    def close(self, commit_pending_transaction=False):
        """
        Properly handles transactions explicitely (with parameter) or by default
        """
        if not self.__open:
            return
        if self.dbTxn:
            if not commit_pending_transaction:
                self.rollback()
            else:
                self.commit()            
                self.dbTxn.abort() # abort the new transaction commit just started since we're closing.
        self.__open = False
        self.__sync_thread.join()
        for i in self.__indicies:
            i.close()
        self.__contexts.close()
        self.__namespace.close()
        self.__prefix.close()
        self.__i2k.close()
        #self.__k2i.close()      
        self.db_env.close()

    def add(self, (subject, predicate, object_), context, quoted=False):
        """\
        Add a triple to the store of triples.
        """
        assert self.__open, "The Store must be open."
        assert context!=self, "Can not add triple directly to store"
        Store.add(self, (subject, predicate, object_), context, quoted)

        _to_string = self._to_string

        s = _to_string(subject)
        p = _to_string(predicate)
        o = _to_string(object_)
        c = _to_string(context)

        cspo, cpos, cosp = self.__indicies

        value = cspo.get("%s^%s^%s^%s^" % (c, s, p, o),txn=self.dbTxn)
        if value is None:
            self.__contexts.put(c, "",self.dbTxn)

            contexts_value = cspo.get("%s^%s^%s^%s^" % ("", s, p, o),txn=self.dbTxn) or ""
            contexts = set(contexts_value.split("^"))
            contexts.add(c)
            contexts_value = "^".join(contexts)
            assert contexts_value!=None

            cspo.put("%s^%s^%s^%s^" % (c, s, p, o), "",self.dbTxn)
            cpos.put("%s^%s^%s^%s^" % (c, p, o, s), "",self.dbTxn)
            cosp.put("%s^%s^%s^%s^" % (c, o, s, p), "",self.dbTxn)
            if not quoted:
                cspo.put("%s^%s^%s^%s^" % ("", s, p, o), contexts_value,self.dbTxn)
                cpos.put("%s^%s^%s^%s^" % ("", p, o, s), contexts_value,self.dbTxn)
                cosp.put("%s^%s^%s^%s^" % ("", o, s, p), contexts_value,self.dbTxn)

            self.__needs_sync = True

    def __remove(self, (s, p, o), c, quoted=False):
        cspo, cpos, cosp = self.__indicies
        contexts_value = cspo.get("^".join(("", s, p, o, "")),txn=self.dbTxn) or ""
        contexts = set(contexts_value.split("^"))
        contexts.discard(c)
        contexts_value = "^".join(contexts)
        for i, _to_key, _from_key in self.__indicies_info:
            i.delete(_to_key((s, p, o), c),txn=self.dbTxn)
        if not quoted:
            if contexts_value:
                for i, _to_key, _from_key in self.__indicies_info:
                    i.put(_to_key((s, p, o), ""), contexts_value,self.dbTxn)
            else:
                for i, _to_key, _from_key in self.__indicies_info:
                    try:
                        i.delete(_to_key((s, p, o), ""),txn=self.dbTxn)
                    except db.DBNotFoundError, e: 
                        pass # TODO: is it okay to ignore these?

    def remove(self, (subject, predicate, object_), context):
        assert self.__open, "The Store must be open."
        Store.remove(self, (subject, predicate, object_), context)
        _to_string = self._to_string
        if context is not None:
            if context == self:
                context = None

        if subject is not None and predicate is not None and object_ is not None and context is not None:
            s = _to_string(subject)
            p = _to_string(predicate)
            o = _to_string(object_)
            c = _to_string(context)
            value = self.__indicies[0].get("%s^%s^%s^%s^" % (c, s, p, o),txn=self.dbTxn)
            if value is not None:
                self.__remove((s, p, o), c)
                self.__needs_sync = True
        else:
            cspo, cpos, cosp = self.__indicies
            index, prefix, from_key, results_from_key = self.__lookup((subject, predicate, object_), context)

            cursor = index.cursor(txn=self.dbTxn)
            try:
                current = cursor.set_range(prefix)
                needs_sync = True
            except db.DBNotFoundError:
                current = None
                needs_sync = False
            cursor.close()
            while current:
                key, value = current
                cursor = index.cursor(txn=self.dbTxn)
                try:
                    cursor.set_range(key)
                    current = cursor.next()
                except db.DBNotFoundError:
                    current = None
                cursor.close()
                if key.startswith(prefix):
                    c, s, p, o = from_key(key)
                    if context is None:
                        contexts_value = index.get(key,txn=self.dbTxn) or ""
                        contexts = set(contexts_value.split("^")) # remove triple from all non quoted contexts
                        contexts.add("") # and from the conjunctive index
                        for c in contexts:
                            for i, _to_key, _ in self.__indicies_info:
                                i.delete(_to_key((s, p, o), c),txn=self.dbTxn)
                    else:
                        self.__remove((s, p, o), c)
                else:
                    break

            if context is not None:
                if subject is None and predicate is None and object_ is None:
                    # TODO: also if context becomes empty and not just on remove((None, None, None), c)
                    try:
                        self.__contexts.delete(_to_string(context),txn=self.dbTxn)
                    except db.DBNotFoundError, e:
                        pass

            self.__needs_sync = needs_sync

    def triples(self, (subject, predicate, object_), context=None):
        """A generator over all the triples matching """
        assert self.__open, "The Store must be open."

        if context is not None:
            if context == self:
                context = None

        _from_string = self._from_string
        index, prefix, from_key, results_from_key = self.__lookup((subject, predicate, object_), context)

        cursor = index.cursor(txn=self.dbTxn)
        try:
            current = cursor.set_range(prefix)
        except db.DBNotFoundError:
            current = None
        cursor.close()
        while current:
            key, value = current
            cursor = index.cursor(txn=self.dbTxn)
            try:
                cursor.set_range(key)
                current = cursor.next()
            except db.DBNotFoundError:
                current = None
            cursor.close()
            if key and key.startswith(prefix):
                contexts_value = index.get(key,txn=self.dbTxn)
                yield results_from_key(key, subject, predicate, object_, contexts_value)
            else:
                break

    def __len__(self, context=None):
        assert self.__open, "The Store must be open."
        if context is not None:
            if context == self:
                context = None

        if context is None:
            prefix = "^"
        else:
            prefix = "%s^" % self._to_string(context)

        index = self.__indicies[0]
        cursor = index.cursor(txn=self.dbTxn)
        current = cursor.set_range(prefix)
        count = 0
        while current:
            key, value = current
            if key.startswith(prefix):
                count +=1
                current = cursor.next()
            else:
                break
        cursor.close()
        return count

    def bind(self, prefix, namespace):
        prefix = prefix.encode("utf-8")
        namespace = namespace.encode("utf-8")
        bound_prefix = self.__prefix.get(namespace,txn=self.dbTxn)
        if bound_prefix:
            self.__namespace.delete(bound_prefix,txn=self.dbTxn)
        self.__prefix.put(namespace, prefix,self.dbTxn)
        #self.__prefix[namespace] = prefix
        self.__namespace.put(prefix, namespace,self.dbTxn)
        #self.__namespace[prefix] = namespace

    def namespace(self, prefix):
        prefix = prefix.encode("utf-8")
        return self.__namespace.get(prefix, None,txn=self.dbTxn)

    def prefix(self, namespace):
        namespace = namespace.encode("utf-8")
        return self.__prefix.get(namespace, None,txn=self.dbTxn)

    def namespaces(self):
        cursor = self.__namespace.cursor(txn=self.dbTxn)
        results = []
        current = cursor.first()
        while current:
            prefix, namespace = current
            results.append((prefix, namespace))
            current = cursor.next()
        cursor.close()
        for prefix, namespace in results:
            yield prefix, URIRef(namespace)

    def contexts(self, triple=None):
        _from_string = self._from_string
        _to_string = self._to_string

        if triple:
            s, p, o = triple
            s = _to_string(s)
            p = _to_string(p)
            o = _to_string(o)
            contexts = self.__indicies[0].get("%s^%s^%s^%s^" % ("", s, p, o),txn=self.dbTxn)
            if contexts:
                for c in contexts.split("^"):
                    if c:
                        yield _from_string(c)
        else:
            index = self.__contexts
            cursor = index.cursor(txn=self.dbTxn)
            current = cursor.first()
            cursor.close()
            while current:
                key, value = current
                context = _from_string(key)
                yield context
                cursor = index.cursor(txn=self.dbTxn)
                try:
                    cursor.set_range(key)
                    current = cursor.next()
                except db.DBNotFoundError:
                    current = None
                cursor.close()

    def _from_string(self, i):
        k = self.__i2k.get(i,txn=self.dbTxn)
        return self._loads(k)

    def _to_string(self, term):
        """
        i2k:  hashString -> pickledTerm
        
        i2k basically stores the reverse lookup of the MD5 hash of the term 
        
        """
        assert term is not None
        # depending on what the space time trade off looks like we
        # might still want to record k2i as well. Also recording would
        # protect against hash algo changing.
        i = term.md5_term_hash()
        k = self.__i2k.get(i, txn=self.dbTxn)
        if k is None:
            self.__i2k.put(i,self._dumps(term),txn=self.dbTxn)
        return i

    def __lookup(self, (subject, predicate, object_), context):
        _to_string = self._to_string
        if context is not None:
            context = _to_string(context)
        i = 0
        if subject is not None:
            i += 1
            subject = _to_string(subject)
        if predicate is not None:
            i += 2
            predicate = _to_string(predicate)
        if object_ is not None:
            i += 4
            object_ = _to_string(object_)
        index, prefix_func, from_key, results_from_key = self.__lookup_dict[i]
        prefix = "^".join(prefix_func((subject, predicate, object_), context))
        return index, prefix, from_key, results_from_key


def to_key_func(i):
    def to_key(triple, context):
        "Takes a string; returns key"
        return "^".join((context, triple[i%3], triple[(i+1)%3], triple[(i+2)%3], "")) # "" to tac on the trailing ^
    return to_key

def from_key_func(i):
    def from_key(key):
        "Takes a key; returns string"
        parts = key.split("^")
        return parts[0], parts[(3-i+0)%3+1], parts[(3-i+1)%3+1], parts[(3-i+2)%3+1]
    return from_key

def results_from_key_func(i, from_string):
    def from_key(key, subject, predicate, object_, contexts_value):
        "Takes a key and subject, predicate, object; returns tuple for yield"
        parts = key.split("^")
        if subject is None:
            # TODO: i & 1: # dis assemble and/or measure to see which is faster
            # subject is None or i & 1
            s = from_string(parts[(3-i+0)%3+1])
        else:
            s = subject
        if predicate is None:#i & 2:
            p = from_string(parts[(3-i+1)%3+1])
        else:
            p = predicate
        if object_ is None:#i & 4:
            o = from_string(parts[(3-i+2)%3+1])
        else:
            o = object_
        return (s, p, o), (from_string(c) for c in contexts_value.split("^") if c)
    return from_key

def readable_index(i):
    s, p, o = "?" * 3
    if i & 1: s = "s"
    if i & 2: p = "p"
    if i & 4: o = "o"
    return "%s,%s,%s" % (s, p, o)

########NEW FILE########
__FILENAME__ = Concurrent
from __future__ import generators

from threading import Lock

class ResponsibleGenerator(object):
    """A generator that will help clean up when it is done being used."""

    __slots__ = ['cleanup', 'gen']

    def __init__(self, gen, cleanup):
        self.cleanup = cleanup
        self.gen = gen

    def __del__(self):
        self.cleanup()

    def __iter__(self):
        return self

    def next(self):
        return self.gen.next()


class Concurrent(object):

    def __init__(self, store):
        self.store = store

        # number of calls to visit still in progress
        self.__visit_count = 0

        # lock for locking down the indices
        self.__lock = Lock()

        # lists for keeping track of added and removed triples while
        # we wait for the lock
        self.__pending_removes = []
        self.__pending_adds = []

    def add(self, (s, p, o)):
        if self.__visit_count==0:
            self.store.add((s, p, o))
        else:
            self.__pending_adds.append((s, p, o))

    def remove(self, (subject, predicate, object)):
        if self.__visit_count==0:
            self.store.remove((subject, predicate, object))
        else:
            self.__pending_removes.append((subject, predicate, object))

    def triples(self, (subject, predicate, object)):
        g = self.store.triples((subject, predicate, object))
        pending_removes = self.__pending_removes
        self.__begin_read()
        for s, p, o in ResponsibleGenerator(g, self.__end_read):
            if not (s, p, o) in pending_removes:
                yield s, p, o

        for (s, p, o) in self.__pending_adds:
            if (subject==None or subject==s) and (predicate==None or predicate==p) and (object==None or object==o):
                yield s, p, o

    def __len__(self):
        return self.store.__len__()

    def __begin_read(self):
        lock = self.__lock
        lock.acquire()
        self.__visit_count = self.__visit_count + 1
        lock.release()

    def __end_read(self):
        lock = self.__lock
        lock.acquire()
        self.__visit_count = self.__visit_count - 1
        if self.__visit_count==0:
            pending_removes = self.__pending_removes
            while pending_removes:
                (s, p, o) = pending_removes.pop()
                try:
                    self.store.remove((s, p, o))
                except:
                    # TODO: change to try finally?
                    print s, p, o, "Not in store to remove"
            pending_adds = self.__pending_adds
            while pending_adds:
                (s, p, o) = pending_adds.pop()
                self.store.add((s, p, o))
        lock.release()




########NEW FILE########
__FILENAME__ = BinaryRelationPartition
"""
The set of classes used to model the 3 'partitions' for N3 assertions.
There is a top level class which implements operations common to all partitions as
well as a class for each partition.  These classes are meant to allow the underlying
SQL schema to be completely configurable as well as to automate the generation
of SQL queries for adding,updating,removing,resolving triples from the partitions.
These classes work in tandem with the RelationHashes to automate all (or most) of
the SQL processing associated with this FOPL Relational Model

NOTE: The use of foreign keys (which - unfortunately - bumps the minimum MySQL version to 5.0) allows for
the efficient removal of all statements about a particular resource using cascade on delete (currently not used)

see: http://dev.mysql.com/doc/refman/5.0/en/ansi-diff-foreign-keys.html
"""
from rdflib.URIRef import URIRef
from rdflib import BNode
from rdflib import RDF
from rdflib.Literal import Literal
from rdflib.URIRef import URIRef
from pprint import pprint
from rdflib.term_utils import *
from rdflib.store.REGEXMatching import REGEXTerm
from QuadSlot import *
Any = None

CONTEXT_COLUMN = 'context'
ANY_TERM = ['U','B','F','V','L']
CONTEXT_TERMS   = ['U','B','F']
IDENTIFIER_TERMS   = ['U','B']
GROUND_IDENTIFIERS = ['U']
NON_LITERALS = ['U','B','F','V']
CLASS_TERMS = ['U','B','V']
PREDICATE_NAMES = ['U','V']

NAMED_BINARY_RELATION_PREDICATES = GROUND_IDENTIFIERS
NAMED_BINARY_RELATION_OBJECTS    = ['U','B','L']

NAMED_LITERAL_PREDICATES = GROUND_IDENTIFIERS
NAMED_LITERAL_OBJECTS    = ['L']

ASSOCIATIVE_BOX_CLASSES    = GROUND_IDENTIFIERS

CREATE_BRP_TABLE = """
CREATE TABLE %s (
    %s
) ENGINE=InnoDB"""

LOOKUP_INTERSECTION_SQL = "INNER JOIN %s %s ON (%s)"
LOOKUP_UNION_SQL        = "LEFT JOIN %s %s ON (%s)"

class BinaryRelationPartition(object):
    """
    The common ancestor of the three partitions for assertions.
    Implements behavior common to all 3.  Each subclass is expected to define the following:

    nameSuffix - The suffix appended to the name of the table
    termEnumerations - a 4 item list (for each quad 'slot') of lists (or None) which enumerate the allowable term types
                       for each quad slot (one of 'U' - URIs,'V' - Variable,'L' - Literals,'B' - BNodes,'F' - Formulae)
    columnNames - a list of column names for each quad slot (can be of additional length where each item is a 3-item tuple of:
                  column name, column type, index)
    columnIntersectionList - a list of 2 item tuples (the quad index and a boolean indicating whether or not the associated term is an identifier)
                             this list (the order of which is very important) is used for generating intersections between the partition and the identifier / value hash
    hardCodedResultFields - a dictionary mapping quad slot indices to their hardcoded value (for partitions - such as ABOX - which have a hardcoded value for a particular quad slot)
    hardCodedResultTermsTypes - a dictionary mapping quad slot indices to their hardcoded term type (for partitions - such as Literal properties - which have hardcoded values for a particular quad slot's term type)
    """
    assertedColumnName = 'asserted'
    indexSuffix = 'Index'
    def __init__(self,identifier,idHash,valueHash):
        self.identifier = identifier
        self.idHash    = idHash
        self.valueHash = valueHash
        self._repr = self.identifier+'_'+self.nameSuffix
        self.singularInsertionSQLCmd = self.insertRelationsSQLCMD()
        self._resetPendingInsertions()
        self._intersectionSQL = self.generateHashIntersections()
        self._selectFieldsLeading    = self._selectFields(True)  + ['NULL as '+SlotPrefixes[DATATYPE_INDEX],'NULL as '+SlotPrefixes[LANGUAGE_INDEX]]
        self._selectFieldsNonLeading = self._selectFields(False) + ['NULL','NULL']

    def __repr__(self):
        return self._repr

    def foreignKeySQL(self,slot):
        """
        Generates foreign key expression relating a particular quad term with
        the identifier hash
        """
        rt = ["\tCONSTRAINT %s_%s_lookup FOREIGN KEY (%s) REFERENCES %s (%s)"%(
                    self,
                    self.columnNames[slot],
                    self.columnNames[slot],
                    self.idHash,
                    self.idHash.columns[0][0])]
        return rt

    def IndexManagementSQL(self,create=False):
        idxSQLStmts = []
        for slot in POSITION_LIST:
            if self.columnNames[slot]:
                if create:
                    idxSQLStmts.append("create INDEX %s%s on %s (%s)"%(self.columnNames[slot],self.indexSuffix,self,self.columnNames[slot]))
                    idxSQLStmts.append("ALTER TABLE %s ADD %s"%(self,self.foreignKeySQL(slot)[0]))
                else:
                    idxSQLStmts.append("ALTER TABLE %s DROP FOREIGN KEY %s_%s_lookup"%(self,self,self.columnNames[slot]))
                    idxSQLStmts.append("ALTER TABLE %s DROP INDEX %s%s"%(self,self.columnNames[slot],self.indexSuffix))
                if self.termEnumerations[slot]:
                    if create:
                        idxSQLStmts.append("create INDEX %s_term%s on %s (%s_term)"%(self.columnNames[slot],self.indexSuffix,self,self.columnNames[slot]))
                    else:
                        idxSQLStmts.append("drop index %s_term%s on %s"%(self.columnNames[slot],self.indexSuffix,self))
        if len(self.columnNames) > 4:
            for otherSlot in range(4,len(self.columnNames)):
                colMD = self.columnNames[otherSlot]
                if isinstance(colMD,tuple):
                    colName,colType,indexStr = colMD
                    if create:
                        idxSQLStmts.append("create INDEX %s%s on %s (%s)"%(colName,self.indexSuffix,self,indexStr%colName))
                    else:
                        idxSQLStmts.append("drop index %s%s on %s"%(colName,self.indexSuffix,self))
                else:
                    if create:
                        idxSQLStmts.append("create INDEX %s%s on (%s)"%(colMD,self.indexSuffix,self,colMD))
                        idxSQLStmts.append("ALTER TABLE %s ADD %s"%(self,self.foreignKeySQL(otherSlot)[0]))
                    else:
                        idxSQLStmts.append("ALTER TABLE %s DROP FOREIGN KEY %s_%s_lookup"%(self,self,colMD))
                        idxSQLStmts.append("drop index %s%s on %s"%(colMD,self.indexSuffix,self))

        return idxSQLStmts

    def createSQL(self):
        """
        Generates a CREATE TABLE statement which creates a SQL table used for
        persisting assertions associated with this partition
        """
        columnSQLStmts = []
        for slot in POSITION_LIST:
            if self.columnNames[slot]:
                columnSQLStmts.append("\t%s\tBIGINT unsigned not NULL"%(self.columnNames[slot]))
                columnSQLStmts.append("\tINDEX %s%s (%s)"%(self.columnNames[slot],self.indexSuffix,self.columnNames[slot]))
                if self.termEnumerations[slot]:
                    columnSQLStmts.append("\t%s_term enum(%s) not NULL"%(self.columnNames[slot],','.join(["'%s'"%tType for tType in self.termEnumerations[slot]])))
                    columnSQLStmts.append("\tINDEX %s_term%s (%s_term)"%(self.columnNames[slot],self.indexSuffix,self.columnNames[slot]))
                columnSQLStmts.extend(self.foreignKeySQL(slot))

        if len(self.columnNames) > 4:
            for otherSlot in range(4,len(self.columnNames)):
                colMD = self.columnNames[otherSlot]
                if isinstance(colMD,tuple):
                    colName,colType,indexStr = colMD
                    columnSQLStmts.append("\t%s %s"%(colName,colType))
                    columnSQLStmts.append("\tINDEX %s%s (%s)"%(colName,self.indexSuffix,indexStr%colName))
                else:
                    columnSQLStmts.append("\t%s BIGINT unsigned not NULL"%colMD)
                    columnSQLStmts.append("\tINDEX %s%s (%s)"%(colMD,self.indexSuffix,colMD))
                    columnSQLStmts.extend(self.foreignKeySQL(otherSlot))

        return CREATE_BRP_TABLE%(
            self,
            ',\n'.join(columnSQLStmts)
        )

    def _resetPendingInsertions(self):
        """
        Resets the cache for pending insertions
        """
        self.pendingInsertions = []

    def insertRelationsSQLCMD(self):
        """
        Generates a SQL command with parameter references (%s) in order to facilitate
        efficient batch insertion of multiple assertions by Python DB implementations (such as MySQLdb)
        """
        vals = 0
        insertColNames = []
        for colName in self.columnNames:
            colIdx = self.columnNames.index(colName)
            if colName:
                insertColNames.append(colName)
                vals += 1
            if colIdx < len(self.termEnumerations) and self.termEnumerations[colIdx]:
                insertColNames.append(colName+'_term')
                vals += 1
        insertColsExpr = "(%s)"%(','.join([isinstance(i,tuple) and i[0] or i for i in insertColNames]))
        return "INSERT INTO %s %s VALUES "%(self,insertColsExpr)+"(%s)"%(','.join(['%s' for i in range(vals)]))

    def insertRelations(self,quadSlots):
        """
        Takes a list of QuadSlot objects and queues the new identifiers / values to insert and
        the assertions as well (so they can be added in a batch for maximum efficiency)
        """
        for quadSlot in quadSlots:
            self.extractIdentifiers(quadSlot)
            self.pendingInsertions.append(self.compileQuadToParams(quadSlot))

    def flushInsertions(self,db):
        """
        Adds the pending identifiers / values and assertions (using executemany for
        maximum efficiency), and resets the queue.
        """
        self.idHash.insertIdentifiers(db)
        self.valueHash.insertIdentifiers(db)
        cursor = db.cursor()
        cursor.executemany(self.singularInsertionSQLCmd,self.pendingInsertions)
        cursor.close()
        self._resetPendingInsertions()

    def selectContextFields(self,first):
        """
        Generates a list of column aliases for the SELECT SQL command used in order
        to fetch contexts from each partition
        """
        rt = []
        idHashLexicalCol = self.idHash.columns[-1][0]
        idHashTermTypeCol = self.idHash.columns[-2][0]
        termNameAlias = first and ' as %s'%SlotPrefixes[CONTEXT] or ''
        rt.append('rt_'+SlotPrefixes[CONTEXT]+'.'+idHashLexicalCol + termNameAlias)
        termTypeAlias = first and ' as %sTermType'%SlotPrefixes[CONTEXT] or ''
        if self.termEnumerations[CONTEXT]:
            rt.append('rt_'+SlotPrefixes[CONTEXT]+'.'+idHashTermTypeCol+termTypeAlias)
        else:
            rt.append("'%s'"%self.hardCodedResultTermsTypes[CONTEXT]+termTypeAlias)
        return rt

    def _selectFields(self,first):
        rt = []
        idHashLexicalCol = self.idHash.columns[-1][0]
        idHashTermTypeCol = self.idHash.columns[-2][0]
        for idx in range(len(POSITION_LIST)):
            termNameAlias = first and ' as %s'%SlotPrefixes[idx] or ''
            if idx < len(self.columnNames) and self.columnNames[idx]:
                rt.append('rt_'+SlotPrefixes[idx]+'.'+idHashLexicalCol + termNameAlias)
                termTypeAlias = first and ' as %sTermType'%SlotPrefixes[idx] or ''
                if self.termEnumerations[idx]:
                    rt.append('rt_'+SlotPrefixes[idx]+'.'+idHashTermTypeCol+termTypeAlias)
                else:
                    rt.append("'%s'"%self.hardCodedResultTermsTypes[idx]+termTypeAlias)
            else:
                rt.append("'%s'"%self.hardCodedResultFields[idx]+termNameAlias)
                if self.hardCodedResultTermsTypes[idx]:
                    rt.append("'%s'"%self.hardCodedResultTermsTypes[idx]+termNameAlias)
        return rt

    def selectFields(self,first=False):
        """
        Returns a list of column aliases for the SELECT SQL command used to fetch quads from
        a partition
        """
        return first and self._selectFieldsLeading or self._selectFieldsNonLeading

    def generateHashIntersections(self):
        """
        Generates the SQL JOINS (INNER and LEFT) used to intersect the identifier and value hashes
        with this partition.  This relies on each parition setting up an ordered list of
        intersections (ordered with optimization in mind).  For instance the ABOX partition
        would want to intersect on classes first (since this will have a lower cardinality than any other field)
        wherease the Literal Properties partition would want to intersect on datatypes first.
        The paritions and hashes are joined on the integer half-MD5-hash of the URI (or literal) as well
        as the 'Term Type'
        """
        intersections = []
        for idx,isId in self.columnIntersectionList:
            lookup = isId and self.idHash or self.valueHash
            lookupAlias = idx < len(POSITION_LIST) and 'rt_'+SlotPrefixes[idx] or 'rt_'+self.columnNames[idx][0]
            lookupKeyCol = lookup.columns[0][0]
            if idx < len(POSITION_LIST) or len(self.columnNames) > len(POSITION_LIST):
                colName = idx < len(POSITION_LIST) and self.columnNames[idx] or self.columnNames[idx][0]
                intersectionClauses = ["%s.%s = %s.%s"%(self,colName,lookupAlias,lookupKeyCol)]
                if idx < len(POSITION_LIST) and self.termEnumerations[idx]:
                    intersectionClauses.append("%s.%s_term = %s.%s"%(self,colName,lookupAlias,lookup.columns[1][0]))
                if isId and idx < len(POSITION_LIST) and idx in self.hardCodedResultTermsTypes:
                    intersectionClauses.append("%s.%s = '%s'"%(lookupAlias,lookup.columns[1][0],self.hardCodedResultTermsTypes[idx]))
            if idx == DATATYPE_INDEX and len(self.columnNames) > len(POSITION_LIST):
                intersections.append(LOOKUP_UNION_SQL%(lookup,lookupAlias,' AND '.join(intersectionClauses)))
            else:
                intersections.append(LOOKUP_INTERSECTION_SQL%(lookup,lookupAlias,' AND '.join(intersectionClauses)))
        return ' '.join(intersections)

    def generateWhereClause(self,queryPattern):
        """
        Takes a query pattern (a list of quad terms - subject,predicate,object,context)
        and generates a SQL WHERE clauses which works in conjunction to the intersections
        to filter the result set by partial matching (by REGEX), full matching (by integer half-hash),
        and term types.  For maximally efficient SELECT queries
        """
        whereClauses = []
        whereParameters = []
        asserted = dereferenceQuad(CONTEXT,queryPattern) is None
        for idx in SlotPrefixes.keys():
            queryTerm = dereferenceQuad(idx,queryPattern)
            lookupAlias = 'rt_'+SlotPrefixes[idx]
            if idx == CONTEXT and asserted:
                whereClauses.append("%s.%s_term != 'F'"%(self,self.columnNames[idx]))

            if idx < len(POSITION_LIST) and isinstance(queryTerm,REGEXTerm):
                whereClauses.append("%s.lexical REGEXP "%lookupAlias+"%s")
                whereParameters.append(queryTerm)
            elif idx == CONTEXT and isinstance(queryTerm,Graph) and isinstance(queryTerm.identifier,REGEXTerm):
                whereClauses.append("%s.lexical REGEXP "%lookupAlias+"%s")
                whereParameters.append(queryTerm.identifier)
            elif idx < len(POSITION_LIST) and queryTerm is not Any:
                if self.columnNames[idx]:

                    if isinstance(queryTerm,list):
                        whereClauses.append("%s.%s"%(self,self.columnNames[idx])+" in (%s)"%','.join(['%s' for item in range(len(queryTerm))]))
                        whereParameters.extend([normalizeValue(item,term2Letter(item)) for item in queryTerm])
                    else:
                        whereClauses.append("%s.%s"%(self,self.columnNames[idx])+" = %s")
                        whereParameters.append(normalizeValue(queryTerm,term2Letter(queryTerm)))

                if not idx in self.hardCodedResultTermsTypes and self.termEnumerations[idx] and not isinstance(queryTerm,list):
                    whereClauses.append("%s.%s_term"%(self,self.columnNames[idx])+" = %s")
                    whereParameters.append(term2Letter(queryTerm))
            elif idx >= len(POSITION_LIST) and len(self.columnNames) > len(POSITION_LIST) and queryTerm is not None:
                compVal = idx == DATATYPE_INDEX and normalizeValue(queryTerm,term2Letter(queryTerm)) or queryTerm
                whereClauses.append("%s.%s"%(self,self.columnNames[idx][0])+" = %s")
                whereParameters.append(compVal)

        return ' AND '.join(whereClauses),whereParameters# + "#{%s}\n"%(str(queryPattern)),whereParameters


class AssociativeBox(BinaryRelationPartition):
    """
    The partition associated with assertions of class membership (formally known - in Description Logics - as an Associative Box)
    This partition is for all assertions where the property is rdf:type
    see: http://en.wikipedia.org/wiki/Description_Logic#Modelling_in_Description_Logics
    """
    nameSuffix = 'associativeBox'
    termEnumerations=[NON_LITERALS,None,CLASS_TERMS,CONTEXT_TERMS]
    columnNames = ['member',None,'class',CONTEXT_COLUMN]
    columnIntersectionList = [
                               (OBJECT,True),
                               (CONTEXT,True),
                               (SUBJECT,True)]

    hardCodedResultFields = {
        PREDICATE      : RDF.type,
    }
    hardCodedResultTermsTypes = {
        PREDICATE : 'U',
    }

    def compileQuadToParams(self,quadSlots):
        subjSlot,predSlot,objSlot,conSlot = quadSlots
        return (subjSlot.md5Int,
                term2Letter(subjSlot.term),
                objSlot.md5Int,
                term2Letter(objSlot.term),
                conSlot.md5Int,
                term2Letter(conSlot.term))

    def extractIdentifiers(self,quadSlots):
        subjSlot,predSlot,objSlot,conSlot = quadSlots
        self.idHash.updateIdentifierQueue([
                                           (subjSlot.term,subjSlot.termType),
                                           (objSlot.term,objSlot.termType),
                                           (conSlot.term,conSlot.termType)
                                           ])

class NamedLiteralProperties(BinaryRelationPartition):
    """
    The partition associated with assertions where the object is a Literal.
    """
    nameSuffix = 'literalProperties'
    termEnumerations=[NON_LITERALS,PREDICATE_NAMES,None,CONTEXT_TERMS]
    columnNames = ['subject','predicate','object',CONTEXT_COLUMN,('data_type','BIGINT unsigned','%s'),('language','varchar(3)','%s(3)')]
    columnIntersectionList = [
                               (DATATYPE_INDEX,True),
                               (PREDICATE,True),
                               (CONTEXT,True),
                               (OBJECT,False),
                               (SUBJECT,True)]

    hardCodedResultFields = {}
    hardCodedResultTermsTypes = {
        OBJECT    : 'L'
    }

    def foreignKeySQL(self,slot):
        hash = slot == OBJECT and self.valueHash or self.idHash
        rt = ["\tCONSTRAINT %s_%s_lookup FOREIGN KEY  (%s) REFERENCES %s (%s)"%(
                    self,
                    self.columnNames[slot],
                    self.columnNames[slot],
                    hash,
                    hash.columns[0][0])]
        return rt

    def __init__(self,identifier,idHash,valueHash):
        super(NamedLiteralProperties,self).__init__(identifier,idHash,valueHash)
        self.insertSQLCmds = {
           (False,False): self.insertRelationsSQLCMD(),
           (False,True) : self.insertRelationsSQLCMD(language=True),
           (True,False) : self.insertRelationsSQLCMD(dataType=True),
           (True,True)  : self.insertRelationsSQLCMD(dataType=True,language=True)
        }
        idHashLexicalCol = self.idHash.columns[-1][0]
        self._selectFieldsLeading = self._selectFields(True) + \
          [
            'rt_%s.%s'%(self.columnNames[DATATYPE_INDEX][0],idHashLexicalCol) + ' as %s'%SlotPrefixes[DATATYPE_INDEX],
            str(self)+'.'+self.columnNames[LANGUAGE_INDEX][0]+' as %s'%SlotPrefixes[LANGUAGE_INDEX],
          ]
        self._selectFields        = self._selectFields(False) + \
          [
            'rt_%s.%s'%(self.columnNames[DATATYPE_INDEX][0],idHashLexicalCol),
            str(self)+'.'+self.columnNames[LANGUAGE_INDEX][0],
          ]

    def _resetPendingInsertions(self):
        self.pendingInsertions = {
           (False,False): [],
           (False,True) : [],
           (True,False) : [],
           (True,True)  : [],
        }

    def insertRelationsSQLCMD(self,dataType=None,language=None):
        vals = 0
        insertColNames = []
        for colName in self.columnNames:
            colIdx = self.columnNames.index(colName)
            if colName:
                if isinstance(colName,tuple):
                    colName = colName[0]
                    for argColName,arg in [(self.columnNames[DATATYPE_INDEX][0],dataType),(self.columnNames[LANGUAGE_INDEX][0],language)]:
                        if colName == argColName and arg:
                            insertColNames.append(colName)
                            vals += 1
                else:
                    insertColNames.append(colName)
                    vals += 1
            if colIdx < len(self.termEnumerations) and self.termEnumerations[colIdx]:
                insertColNames.append(colName+'_term')
                vals += 1

        insertColsExpr = "(%s)"%(','.join([i for i in insertColNames]))
        return "INSERT INTO %s %s VALUES "%(self,insertColsExpr)+"(%s)"%(','.join(['%s' for i in range(vals)]))

    def insertRelations(self,quadSlots):
        for quadSlot in quadSlots:
            self.extractIdentifiers(quadSlot)
            literal = quadSlot[OBJECT].term
            insertionCMDKey = (bool(literal.datatype),bool(literal.language))
            self.pendingInsertions[insertionCMDKey].append(self.compileQuadToParams(quadSlot))

    def flushInsertions(self,db):
        self.idHash.insertIdentifiers(db)
        self.valueHash.insertIdentifiers(db)
        cursor = db.cursor()
        for key,paramList in self.pendingInsertions.items():
            if paramList:
                cursor.executemany(self.insertSQLCmds[key],paramList)
        cursor.close()
        self._resetPendingInsertions()

    def compileQuadToParams(self,quadSlots):
        subjSlot,predSlot,objSlot,conSlot = quadSlots
        dTypeParam = objSlot.term.datatype and normalizeValue(objSlot.term.datatype,objSlot.termType) or None
        langParam  = objSlot.term.language and objSlot.term.language or None
        rtList = [
                    subjSlot.md5Int,
                    term2Letter(subjSlot.term),
                    predSlot.md5Int,
                    term2Letter(predSlot.term),
                    objSlot.md5Int,
                    conSlot.md5Int,
                    term2Letter(conSlot.term)]
        for item in [dTypeParam,langParam]:
            if item:
                rtList.append(item)
        return tuple(rtList)

    def extractIdentifiers(self,quadSlots):
        subjSlot,predSlot,objSlot,conSlot = quadSlots
        idTerms = [
                    (subjSlot.term,subjSlot.termType),
                    (predSlot.term,predSlot.termType),
                    (conSlot.term,conSlot.termType)]
        if objSlot.term.datatype:
            idTerms.append((objSlot.term.datatype,objSlot.termType))
        self.idHash.updateIdentifierQueue(idTerms)
        self.valueHash.updateIdentifierQueue([(objSlot.term,objSlot.termType)])

    def selectFields(self,first=False):
        return first and self._selectFieldsLeading or self._selectFieldsNonLeading

class NamedBinaryRelations(BinaryRelationPartition):
    """
    Partition associated with assertions where the predicate isn't rdf:type and the object isn't a literal
    """
    nameSuffix = 'relations'
    termEnumerations=[NON_LITERALS,PREDICATE_NAMES,NON_LITERALS,CONTEXT_TERMS]
    columnNames = ['subject','predicate','object',CONTEXT_COLUMN]
    columnIntersectionList = [
                               (PREDICATE,True),
                               (CONTEXT,True),
                               (OBJECT,True),
                               (SUBJECT,True)]

    hardCodedResultFields = {}
    hardCodedResultTermsTypes = {}

    def compileQuadToParams(self,quadSlots):
        subjSlot,predSlot,objSlot,conSlot = quadSlots
        return (subjSlot.md5Int,
                term2Letter(subjSlot.term),
                predSlot.md5Int,
                term2Letter(predSlot.term),
                objSlot.md5Int,
                term2Letter(objSlot.term),
                conSlot.md5Int,
                term2Letter(conSlot.term))

    def extractIdentifiers(self,quadSlots):
        subjSlot,predSlot,objSlot,conSlot = quadSlots
        self.idHash.updateIdentifierQueue([
                                           (subjSlot.term,subjSlot.termType),
                                           (predSlot.term,predSlot.termType),
                                           (objSlot.term,objSlot.termType),
                                           (conSlot.term,conSlot.termType)])

def BinaryRelationPartitionCoverage((subject,predicate,object_,context),BRPs):
    """
    This function takes a quad pattern (where any term is one of: URIRef,BNode,Literal,None,or REGEXTerm)
    ,a list of 3 live partitions and returns a list of only those partitions that need to be searched
    in order to resolve the pattern.  This function relies on the BRPQueryDecisionMap dictionary
    to determine which partitions to use.  Note that the dictionary as it is currently constituted
    requres that REGEXTerms in the object slot require that *both* the binary relation partition and
    the literal properties partitions are searched when this search could be limited to the literal
    properties only (for more efficient REGEX evaluation of literal values).  Given the nature of the
    REGEX function in SPARQL and the way Versa matches by REGEX, this seperation couldn't be done
    """
    if isinstance(predicate,list) and len(predicate) == 1:
        predicate = predicate[0]
    if isinstance(predicate,REGEXTerm):
        pId = predicate.compiledExpr.match(RDF.type) and 'RT' or 'U_RNT'
    elif isinstance(predicate,(URIRef,BNode)):
        pId = predicate == RDF.type and 'T' or 'U_RNT'
    elif predicate is None or predicate is []:
        pId = 'W'
    elif isinstance(predicate,list):
        if [p for p in predicate if p == RDF.type or isinstance(p,REGEXTerm) and p.compiledExpr.match(RDF.type)]:
            #One of the predicates is (or matches) rdf:type, so can be treated as a REGEX term that matches rdf:type
            pId = 'RT'
        else:
            #Otherwise, can be treated as a REGEXTerm that *doesn't* match rdf:type
            pId = 'U_RNT'
    elif isinstance(predicate,Variable):
        #Predicates as variables would only exist in literal property assertions and 'other' Relations partition
        #(same as URIs or REGEX Terms that don't match rdf:type)
        pId = 'U_RNT'
    else:
        raise Exception("Unable to determine a parition to cover with the given predicate %s (a %s)"%(predicate,type(predicate).__name__))

    if isinstance(object_,list) and len(object_) == 1:
        object_ = object_[0]
    if isinstance(object_,REGEXTerm):
        oId = 'R'
    elif isinstance(object_,Literal):
        oId = 'L'
    elif isinstance(object_,(URIRef,BNode,Graph)):
        oId = 'U'
    elif object_ is None:
        oId = 'W'
    elif isinstance(object_,list):
        if [o for o in object_ if isinstance(o,REGEXTerm)]:
            #If there are any REGEXTerms in the list then the list behaves as a REGEX / Wildcard
            oId = 'R'
        elif not [o for o in object_ if isinstance(o,REGEXTerm) or isinstance(o,Literal)]:
            #There are no Literals or REGEXTerms, the list behaves as a URI (i.e., it never checks literal partition)
            oId = 'U'
        elif len([o for o in object_ if isinstance(o,Literal)]) == len(object_):
            #They are all literals
            oId = 'L'
        else:
            #Treat as a wildcard
            oId = 'R'
    elif isinstance(object_,Variable):
        #Variables would only exist in the ABOX and 'other' Relations partition (same as URIs)
        oId = 'U'
    else:
        raise Exception("Unable to determine a parition to cover with the given object %s (a %s)"%(object_,type(object_).__name__))

    targetBRPs = [brp for brp in BRPs if isinstance(brp,BRPQueryDecisionMap[pId+oId])]
    return targetBRPs

def PatternResolution(quad,cursor,BRPs,orderByTriple=True,fetchall=True,fetchContexts=False):
    """
    This function implements query pattern resolution against a list of partition objects and
    3 parameters specifying whether to sort the result set (in order to group identical triples
    by the contexts in which they appear), whether to fetch the entire result set or one at a time,
    and whether to fetch the matching contexts only or the assertions.
    This function uses BinaryRelationPartitionCoverage to whittle out the partitions that don't need
    to be searched, generateHashIntersections / generateWhereClause to generate the SQL query
    and the parameter fill-ins and creates a single UNION query against the relevant partitions.

    Note the use of UNION syntax requires that the literal properties partition is first (since it
    uses the first select to determine the column types for the resulting rows from the subsequent
    SELECT queries)

    see: http://dev.mysql.com/doc/refman/5.0/en/union.html
    """
    subject,predicate,object_,context = quad
    targetBRPs = BinaryRelationPartitionCoverage((subject,predicate,object_,context),BRPs)
    unionQueries = []
    unionQueriesParams = []
    for brp in targetBRPs:
        first = targetBRPs.index(brp) == 0
        if fetchContexts:
            query = "SELECT DISTINCT %s FROM %s %s WHERE "%(
                                          ','.join(brp.selectContextFields(first)),
                                          brp,
                                          brp._intersectionSQL
                                        )
        else:
            query = CROSS_BRP_QUERY_SQL%(
                                          ','.join(brp.selectFields(first)),
                                          brp,
                                          brp._intersectionSQL
                                        )
        whereClause,whereParameters = brp.generateWhereClause((subject,predicate,object_,context))
        unionQueries.append(query+whereClause)
        unionQueriesParams.extend(whereParameters)

    if fetchContexts:
        orderBySuffix = ''
    else:
        orderBySuffix = orderByTriple and ' ORDER BY %s,%s,%s'%(SlotPrefixes[SUBJECT],SlotPrefixes[PREDICATE],SlotPrefixes[OBJECT]) or ''
    if len(unionQueries) == 1:
        query = unionQueries[0] + orderBySuffix
    else:
        query = ' union all '.join(['('+q+')' for q in unionQueries]) + orderBySuffix
    query = query + '  # %s'%str(quad)
    try:
        cursor.execute(query,tuple(unionQueriesParams))
    except ValueError,e:
        print "## Query ##\n",query
        print "## Parameters ##\n",unionQueriesParams
        raise e
    if fetchall:
        qRT = cursor.fetchall()
    else:
        qRT = cursor.fetchone()
    return qRT

CREATE_RESULT_TABLE = \
"""
CREATE TEMPORARY TABLE result (
    subject text NOT NULL,
    subjectTerm enum('F','V','U','B','L') NOT NULL,
    predicate text NOT NULL,
    predicateTerm enum('F','V','U','B','L') NOT NULL,
    object text NOT NULL,
    objectTerm enum('F','V','U','B','L') NOT NULL,
    context text not NULL,
    contextTerm enum('F','V','U','B','L') NOT NULL,
    dataType text,
    language char(3),
    INDEX USING BTREE (context(50))
)
"""
CROSS_BRP_QUERY_SQL="SELECT DISTINCT %s FROM %s %s WHERE "
CROSS_BRP_RESULT_QUERY_SQL="SELECT * FROM result ORDER BY context"
DROP_RESULT_TABLE_SQL = "DROP result"

BRPQueryDecisionMap = {
    'WL':(NamedLiteralProperties),
    'WU':(AssociativeBox,NamedBinaryRelations),
    'WW':(NamedLiteralProperties,AssociativeBox,NamedBinaryRelations),
    'WR':(NamedLiteralProperties,AssociativeBox,NamedBinaryRelations),  #Could be optimized to not include NamedBinaryRelations
    'RTL':(NamedLiteralProperties),
    'RTU':(NamedBinaryRelations,AssociativeBox),
    'RTR':(NamedLiteralProperties,AssociativeBox,NamedBinaryRelations), #Could be optimized to not include NamedBinaryRelations
    'TU':(AssociativeBox),
    'TW':(AssociativeBox),
    'TR':(AssociativeBox),
    'U_RNTL':(NamedLiteralProperties),
    'U_RNTU':(NamedBinaryRelations),
    'U_RNTW':(NamedLiteralProperties,NamedBinaryRelations),
    'U_RNTR':(NamedLiteralProperties,NamedBinaryRelations), #Could be optimized to not include NamedBinaryRelations
}


########NEW FILE########
__FILENAME__ = QuadSlot
"""
Utility functions associated with RDF terms:

- normalizing (to 64 bit integers via half-md5-hashes)
- escaping literal's for SQL persistence
"""
from rdflib import BNode
from rdflib import RDF
from rdflib.Literal import Literal
from rdflib.URIRef import URIRef
import md5
from rdflib.term_utils import *
from rdflib.Graph import QuotedGraph
from rdflib.store.REGEXMatching import REGEXTerm

Any = None

SUBJECT    = 0
PREDICATE  = 1
OBJECT     = 2
CONTEXT    = 3

DATATYPE_INDEX = CONTEXT + 1
LANGUAGE_INDEX = CONTEXT + 2

SlotPrefixes = {
     SUBJECT   : 'subject',
     PREDICATE : 'predicate',
     OBJECT    : 'object',
     CONTEXT   : 'context',
     DATATYPE_INDEX : 'dataType',
     LANGUAGE_INDEX : 'language'
}

POSITION_LIST = [SUBJECT,PREDICATE,OBJECT,CONTEXT]

def EscapeQuotes(qstr):
    """
    Ported from Ft.Lib.DbUtil
    """
    if qstr is None:
        return ''
    tmp = qstr.replace("\\","\\\\")
    tmp = tmp.replace("'", "\\'")
    return tmp

def dereferenceQuad(index,quad):
    assert index <= LANGUAGE_INDEX, "Invalid Quad Index"
    if index == DATATYPE_INDEX:
        return isinstance(quad[OBJECT],Literal) and quad[OBJECT].datatype or None
    elif index == LANGUAGE_INDEX:
        return isinstance(quad[OBJECT],Literal) and quad[OBJECT].language or None
    else:
        return quad[index]

def genQuadSlots(quads):
    return [QuadSlot(index,quads[index])for index in POSITION_LIST]

def normalizeValue(value,termType):
    if value is None:
        value = u'http://www.w3.org/2002/07/owl#NothingU'
    else:
        value = (isinstance(value,Graph) and value.identifier or str(value)) + termType
    return int(md5.new(isinstance(value,unicode) and value.encode('utf-8') or value).hexdigest()[:16],16)

class QuadSlot:
    def __repr__(self):
        #NOTE: http://docs.python.org/ref/customization.html
        return "QuadSlot(%s,%s,%s)"%(SlotPrefixes[self.position],self.term,self.md5Int)

    def __init__(self,position,term):
        assert position in POSITION_LIST, "Unknown quad position: %s"%position
        self.position = position
        self.term = term
        self.md5Int = normalizeValue(term,term2Letter(term))
        self.termType = term2Letter(term)

    def EscapeQuotes(self,qstr):
        """
        Ported from Ft.Lib.DbUtil
        """
        if qstr is None:
            return ''
        tmp = qstr.replace("\\","\\\\")
        tmp = tmp.replace("'", "\\'")
        return tmp

    def normalizeTerm(self):
        if isinstance(self.term,(QuotedGraph,Graph)):
            return self.term.identifier.encode('utf-8')
        elif isinstance(self.term,Literal):
            return self.EscapeQuotes(self.term).encode('utf-8')
        elif self.term is None or isinstance(self.term,(list,REGEXTerm)):
            return self.term
        else:
            return self.term.encode('utf-8')
########NEW FILE########
__FILENAME__ = RelationalHash
"""
This module implements two hash tables for identifiers and values that
facilitate maximal index lookups and minimal redundancy (since identifiers and values are stored once
only and referred to by integer half-md5-hashes).  The identifier hash uses
the half-md5-hash (converted by base conversion to an integer) to key on the identifier's full
lexical form (for partial matching by REGEX) and their term types.
The use of a half-hash introduces a collision risk that is currently not accounted for.  The volume at which
the risk becomes significant is calculable, though through the 'birthday paradox'.

The value hash is keyed off the half-md5-hash (as an integer also) and stores the identifier's full lexical
representation (for partial matching by REGEX)

These classes are meant to automate the creation, management, linking, insertion of these hashes (by SQL)
automatically

see: http://en.wikipedia.org/wiki/Birthday_Paradox
"""

from rdflib import BNode
from rdflib import RDF
from rdflib.Literal import Literal
from rdflib.URIRef import URIRef
from rdflib.term_utils import *
from rdflib.Graph import QuotedGraph
from rdflib.store.REGEXMatching import REGEXTerm
from QuadSlot import POSITION_LIST, normalizeValue
Any = None

COLLISION_DETECTION = False

CREATE_HASH_TABLE = """
CREATE TABLE %s (
    %s
) ENGINE=InnoDB;"""

IDENTIFIER_GARBAGE_COLLECTION_SQL="CREATE TEMPORARY TABLE danglingIds SELECT %s.%s FROM %s %s where %s and %s.%s <> %s;"
VALUE_GARBAGE_COLLECTION_SQL="CREATE TEMPORARY TABLE danglingIds SELECT %s.%s FROM %s %s where %s"
PURGE_KEY_SQL="DELETE %s FROM %s INNER JOIN danglingIds on danglingIds.%s = %s.%s;"

def GarbageCollectionQUERY(idHash,valueHash,aBoxPart,binRelPart,litPart):
    """
    Performs garbage collection on interned identifiers and their references.  Joins
    the given KB parititions against the identifiers and values and removes the 'danglers'.  This
    must be performed after every removal of an assertion and so becomes a primary bottleneck
    """
    purgeQueries = ["drop temporary table if exists danglingIds"]
    rdfTypeInt = normalizeValue(RDF.type,'U')
    idHashKeyName = idHash.columns[0][0]
    valueHashKeyName = valueHash.columns[0][0]
    idHashJoinees    = [aBoxPart,binRelPart,litPart]
    idJoinClauses = []
    idJoinColumnCandidates = []
    explicitJoins = []
    for part in idHashJoinees:
        partJoinClauses = []
        for colName in part.columnNames:
            if part.columnNames.index(colName) >= 4:
                colName,sqlType,index = colName
                if sqlType.lower()[:6]=='bigint':
                    partJoinClauses.append("%s.%s = %s.%s"%(part,colName,idHash,idHashKeyName))
                    idJoinColumnCandidates.append("%s.%s"%(part,colName))
            elif colName:
                partJoinClauses.append("%s.%s = %s.%s"%(part,colName,idHash,idHashKeyName))
                idJoinColumnCandidates.append("%s.%s"%(part,colName))
        explicitJoins.append("left join %s on (%s)"%(part,' or '.join(partJoinClauses)))
        idJoinClauses.extend(partJoinClauses)

    intersectionClause = " and ".join([col + " is NULL" for col in idJoinColumnCandidates])
    idGCQuery = IDENTIFIER_GARBAGE_COLLECTION_SQL%(
        idHash,
        idHashKeyName,
        idHash,
        ' '.join(explicitJoins),
        intersectionClause,
        idHash,
        idHashKeyName,
        rdfTypeInt
    )

    idPurgeQuery = PURGE_KEY_SQL%(idHash,idHash,idHashKeyName,idHash,idHashKeyName)
    purgeQueries.append(idGCQuery)
    purgeQueries.append(idPurgeQuery)

    partJoinClauses = []
    idJoinColumnCandidates = []
    explicitJoins = []
    partJoinClauses.append("%s.%s = %s.%s"%(litPart,litPart.columnNames[OBJECT],valueHash,valueHashKeyName))
    idJoinColumnCandidates.append("%s.%s"%(litPart,litPart.columnNames[OBJECT]))

    intersectionClause = " and ".join([col + " is NULL" for col in idJoinColumnCandidates])
    valueGCQuery = VALUE_GARBAGE_COLLECTION_SQL%(
        valueHash,
        valueHashKeyName,
        valueHash,
        "left join %s on (%s)"%(litPart,' or '.join(partJoinClauses)),
        intersectionClause
    )

    valuePurgeQuery = PURGE_KEY_SQL%(valueHash,valueHash,valueHashKeyName,valueHash,valueHashKeyName)
    purgeQueries.append("drop temporary table if exists danglingIds")
    purgeQueries.append(valueGCQuery)
    purgeQueries.append(valuePurgeQuery)
    return purgeQueries

class RelationalHash:
    def __init__(self,identifier):
        self.identifier = identifier
        self.hashUpdateQueue = {}

    def defaultSQL(self):
        return ''

    def EscapeQuotes(self,qstr):
        if qstr is None:
            return ''
        tmp = qstr.replace("\\","\\\\")
        tmp = tmp.replace("'", "\\'")
        return tmp

    def normalizeTerm(self,term):
        if isinstance(term,(QuotedGraph,Graph)):
            return term.identifier.encode('utf-8')
        elif isinstance(term,Literal):
            return self.EscapeQuotes(term).encode('utf-8')
        elif term is None or isinstance(term,(list,REGEXTerm)):
            return term
        else:
            return term.encode('utf-8')

    def __repr__(self):
        return "%s_%s"%(self.identifier,self.tableNameSuffix)

    def IndexManagementSQL(self,create=False):
        idxSQLStmts = []#'ALTER TABLE %s DROP PRIMARY KEY'%self]
        for colName,colType,indexMD in self.columns:
            assert indexMD
            indexName,indexCol = indexMD
            if indexName:
                if create:
                    idxSQLStmts.append("create INDEX %s on %s (%s)"%(indexName,self,indexCol))
                else:
                    idxSQLStmts.append("drop INDEX %s on %s"%(indexName,self))
        return idxSQLStmts

    def createSQL(self):
        columnSQLStmts = []
        for colName,colType,indexMD in self.columns:
            assert indexMD
            indexName,indexCol = indexMD
            if indexName:
                columnSQLStmts.append("\t%s\t%s not NULL"%(colName,colType))
                columnSQLStmts.append("\tINDEX %s (%s)"%(indexName,indexCol))
            else:
                columnSQLStmts.append("\t%s\t%s not NULL PRIMARY KEY"%(colName,colType))

        return CREATE_HASH_TABLE%(
            self,
            ',\n'.join(columnSQLStmts)
        )
    def dropSQL(self):
        pass

class IdentifierHash(RelationalHash):
    columns = [
                ('id','BIGINT unsigned',[None,'id']),
                ('term_type',"enum('U','B','F','V','L')",['termTypeIndex','term_type']),
                ('lexical','text',['lexical_index','lexical(100)'])
    ]

    tableNameSuffix = 'identifiers'

    def defaultSQL(self):
        """
        Since rdf:type is modeled explicitely (in the ABOX partition) it must be inserted as a 'default'
        identifier
        """
        return 'INSERT into %s values (%s,"U","%s");'%(self,normalizeValue(RDF.type,'U'),RDF.type)

    def generateDict(self,db):
        c=db.cursor()
        c.execute("select * from %s"%self)
        rtDict = {}
        for rt in c.fetchall():
            rtDict[rt[0]] = (rt[1],rt[2])
        c.close()
        return rtDict

    def updateIdentifierQueue(self,termList):
        for term,termType in termList:
            md5Int = normalizeValue(term,termType)
            self.hashUpdateQueue[md5Int]=(termType,self.normalizeTerm(term))

    def insertIdentifiers(self,db):
        c=db.cursor()
        keyCol = self.columns[0][0]
        if self.hashUpdateQueue:
            params = [(md5Int,termType,lexical) for md5Int,(termType,lexical) in self.hashUpdateQueue.items()]
            c.executemany("INSERT IGNORE INTO %s"%(self)+" VALUES (%s,%s,%s)",params)
            if COLLISION_DETECTION:
                insertedIds = self.hashUpdateQueue.keys()
                if len(insertedIds) > 1:
                    c.execute("SELECT * FROM %s"%(self)+" WHERE %s"%keyCol+" in %s",(tuple(insertedIds),))
                else:
                    c.execute("SELECT * FROM %s"%(self)+" WHERE %s"%keyCol+" = %s",tuple(insertedIds))
                for key,termType,lexical in c.fetchall():
                    if self.hashUpdateQueue[key] != (termType,lexical):
                        #Collision!!! Raise an exception (allow the app to rollback the transaction if it wants to)
                        raise Exception("Hash Collision (in %s) on %s,%s vs %s,%s!"%(self,termType,lexical,self.hashUpdateQueue[key][0],self.hashUpdateQueue[key][1]))

            self.hashUpdateQueue = {}
        c.close()

class LiteralHash(RelationalHash):
    columns = [
                ('id','BIGINT unsigned',[None,'id']),
                ('lexical','text',['lexicalIndex','lexical(100)']),
                ]
    tableNameSuffix = 'literals'

    def generateDict(self,db):
        c=db.cursor()
        c.execute("select * from %s"%self)
        rtDict = {}
        for rt in c.fetchall():
            rtDict[rt[0]] = rt[1]
        c.close()
        return rtDict

    def updateIdentifierQueue(self,termList):
        for term,termType in termList:
            md5Int = normalizeValue(term,termType)
            self.hashUpdateQueue[md5Int]=self.normalizeTerm(term)

    def insertIdentifiers(self,db):
        c=db.cursor()
        keyCol = self.columns[0][0]
        if self.hashUpdateQueue:
            params = [(md5Int,lexical) for md5Int,lexical in self.hashUpdateQueue.items()]
            c.executemany("INSERT IGNORE INTO %s"%(self)+" VALUES (%s,%s)",params)
            if COLLISION_DETECTION:
                insertedIds = self.hashUpdateQueue.keys()
                if len(insertedIds) > 1:
                    c.execute("SELECT * FROM %s"%(self)+" WHERE %s"%keyCol+" in %s",(tuple(insertedIds),))
                else:
                    c.execute("SELECT * FROM %s"%(self)+" WHERE %s"%keyCol+" = %s",tuple(insertedIds))
                for key,lexical in c.fetchall():
                    if self.hashUpdateQueue[key] != lexical:
                        #Collision!!! Raise an exception (allow the app to rollback the transaction if it wants to)
                        raise Exception("Hash Collision (in %s) on %s vs %s!"%(self,lexical,self.hashUpdateQueue[key][0]))
            self.hashUpdateQueue = {}
        c.close()
########NEW FILE########
__FILENAME__ = IOMemory
# Authors: Michel Pelletier, Daniel Krech, Stefan Niederhauser
from __future__ import generators

Any = None

from rdflib import BNode
from rdflib.store import Store

class IOMemory(Store):
    """\
    An integer-key-optimized-context-aware-in-memory store.

    Uses nested dictionaries to store triples and context. Each triple
    is stored in six such indices as follows cspo[c][s][p][o] = 1
    and cpos[c][p][o][s] = 1 and cosp[c][o][s][p] = 1 as well as
    spo[s][p][o] = [c] and pos[p][o][s] = [c] and pos[o][s][p] = [c]

    Context information is used to track the 'source' of the triple
    data for merging, unmerging, remerging purposes.  context aware
    store stores consume more memory size than non context stores.

    """

    context_aware = True
    formula_aware = True

    def __init__(self, configuration=None, identifier=None):
        super(IOMemory, self).__init__()

        # indexed by [context][subject][predicate][object] = 1
        self.cspo = self.createIndex()

        # indexed by [context][predicate][object][subject] = 1
        self.cpos = self.createIndex()

        # indexed by [context][object][subject][predicate] = 1
        self.cosp = self.createIndex()

        # indexed by [subject][predicate][object] = [context]
        self.spo = self.createIndex()

        # indexed by [predicate][object][subject] = [context]
        self.pos = self.createIndex()

        # indexed by [object][subject][predicate] = [context]
        self.osp = self.createIndex()

        # indexes integer keys to identifiers
        self.forward = self.createForward()

        # reverse index of forward
        self.reverse = self.createReverse()

        self.identifier = identifier or BNode()

        self.__namespace = self.createPrefixMap()
        self.__prefix = self.createPrefixMap()

    def bind(self, prefix, namespace):
        self.__prefix[namespace] = prefix
        self.__namespace[prefix] = namespace

    def namespace(self, prefix):
        return self.__namespace.get(prefix, None)

    def prefix(self, namespace):
        return self.__prefix.get(namespace, None)

    def namespaces(self):
        for prefix, namespace in self.__namespace.iteritems():
            yield prefix, namespace

    def defaultContext(self):
        return self.default_context

    def addContext(self, context):
        """ Add context w/o adding statement. Dan you can remove this if you want """

        if not self.reverse.has_key(context):
            ci=randid()
            while not self.forward.insert(ci, context):
                ci=randid()
            self.reverse[context] = ci

    def intToIdentifier(self, (si, pi, oi)):
        """ Resolve an integer triple into identifers. """
        return (self.forward[si], self.forward[pi], self.forward[oi])

    def identifierToInt(self, (s, p, o)):
        """ Resolve an identifier triple into integers. """
        return (self.reverse[s], self.reverse[p], self.reverse[o])

    def uniqueSubjects(self, context=None):
        if context is None:
            index = self.spo
        else:
            index = self.cspo[context]
        for si in index.keys():
            yield self.forward[si]

    def uniquePredicates(self, context=None):
        if context is None:
            index = self.pos
        else:
            index = self.cpos[context]
        for pi in index.keys():
            yield self.forward[pi]

    def uniqueObjects(self, context=None):
        if context is None:
            index = self.osp
        else:
            index = self.cosp[context]
        for oi in index.keys():
            yield self.forward[oi]

    def createForward(self):
        return {}

    def createReverse(self):
        return {}

    def createIndex(self):
        return {}

    def createPrefixMap(self):
        return {}

    def add(self, triple, context, quoted=False):
        """\
        Add a triple to the store.
        """
        Store.add(self, triple, context, quoted)
        for triple, cg in self.triples(triple, context):
            #triple is already in the store.
            return

        subject, predicate, object = triple

        f = self.forward
        r = self.reverse

        # assign keys for new identifiers

        if not r.has_key(subject):
            si=randid()
            while f.has_key(si):
                si=randid()
            f[si] = subject
            r[subject] = si
        else:
            si = r[subject]

        if not r.has_key(predicate):
            pi=randid()
            while f.has_key(pi):
                pi=randid()
            f[pi] = predicate
            r[predicate] = pi
        else:
            pi = r[predicate]

        if not r.has_key(object):
            oi=randid()
            while f.has_key(oi):
                oi=randid()
            f[oi] = object
            r[object] = oi
        else:
            oi = r[object]

        if not r.has_key(context):
            ci=randid()
            while f.has_key(ci):
                ci=randid()
            f[ci] = context
            r[context] = ci
        else:
            ci = r[context]

        # add dictionary entries for cspo[c][s][p][o] = 1,
        # cpos[c][p][o][s] = 1, and cosp[c][o][s][p] = 1, creating the
        # nested {} where they do not yet exits.
        self._setNestedIndex(self.cspo, ci, si, pi, oi)
        self._setNestedIndex(self.cpos, ci, pi, oi, si)
        self._setNestedIndex(self.cosp, ci, oi, si, pi)

        if not quoted:
            self._setNestedIndex(self.spo, si, pi, oi, ci)
            self._setNestedIndex(self.pos, pi, oi, si, ci)
            self._setNestedIndex(self.osp, oi, si, pi, ci)

    def _setNestedIndex(self, index, *keys):
        for key in keys[:-1]:
            if not index.has_key(key):
                index[key] = self.createIndex()
            index = index[key]
        index[keys[-1]] = 1


    def _removeNestedIndex(self, index, *keys):
        """ Remove context from the list of contexts in a nested index.

        Afterwards, recursively remove nested indexes when they became empty.
        """
        parents = []
        for key in keys[:-1]:
            parents.append(index)
            index = index[key]
        del index[keys[-1]]

        n = len(parents)
        for i in xrange(n):
            index = parents[n-1-i]
            key = keys[n-1-i]
            if len(index[key]) == 0:
                del index[key]

    def remove(self, triple, context=None):
        Store.remove(self, triple, context)
        if context is not None:
            if context == self:
                context = None

        f = self.forward
        r = self.reverse
        if context is None:
            for triple, cg in self.triples(triple):
                subject, predicate, object = triple
                si, pi, oi = self.identifierToInt((subject, predicate, object))
                contexts = list(self.contexts(triple))
                for context in contexts:
                    ci = r[context]
                    del self.cspo[ci][si][pi][oi]
                    del self.cpos[ci][pi][oi][si]
                    del self.cosp[ci][oi][si][pi]

                    self._removeNestedIndex(self.spo, si, pi, oi, ci)
                    self._removeNestedIndex(self.pos, pi, oi, si, ci)
                    self._removeNestedIndex(self.osp, oi, si, pi, ci)
                    # grr!! hafta ref-count these before you can collect them dumbass!
                    #del f[si], f[pi], f[oi]
                    #del r[subject], r[predicate], r[object]
        else:
            subject, predicate, object = triple
            ci = r.get(context, None)
            if ci:
                for triple, cg in self.triples(triple, context):
                    si, pi, oi = self.identifierToInt(triple)
                    del self.cspo[ci][si][pi][oi]
                    del self.cpos[ci][pi][oi][si]
                    del self.cosp[ci][oi][si][pi]

                    try:
                        self._removeNestedIndex(self.spo, si, pi, oi, ci)
                        self._removeNestedIndex(self.pos, pi, oi, si, ci)
                        self._removeNestedIndex(self.osp, oi, si, pi, ci)
                    except KeyError:
                        # the context may be a quoted one in which
                        # there will not be a triple in spo, pos or
                        # osp. So ignore any KeyErrors
                        pass
                    # TODO delete references to resources in self.forward/self.reverse
                    # that are not in use anymore...

            if subject is None and predicate is None and object is None:
                # remove context
                try:
                    ci = self.reverse[context]
                    del self.cspo[ci], self.cpos[ci], self.cosp[ci]
                except KeyError:
                    # TODO: no exception when removing non-existant context?
                    pass


    def triples(self, triple, context=None):
        """A generator over all the triples matching """

        if context is not None:
            if context == self:
                context = None

        subject, predicate, object = triple
        ci = si = pi = oi = Any

        if context is None:
            spo = self.spo
            pos = self.pos
            osp = self.osp
        else:
            try:
                ci = self.reverse[context]  # TODO: Really ignore keyerror here
                spo = self.cspo[ci]
                pos = self.cpos[ci]
                osp = self.cosp[ci]
            except KeyError:
                return
        try:
            if subject is not Any:
                si = self.reverse[subject] # throws keyerror if subject doesn't exist ;(
            if predicate is not Any:
                pi = self.reverse[predicate]
            if object is not Any:
                oi = self.reverse[object]
        except KeyError, e:
            return #raise StopIteration

        if si != Any: # subject is given
            if spo.has_key(si):
                subjectDictionary = spo[si]
                if pi != Any: # subject+predicate is given
                    if subjectDictionary.has_key(pi):
                        if oi!= Any: # subject+predicate+object is given
                            if subjectDictionary[pi].has_key(oi):
                                ss, pp, oo = self.intToIdentifier((si, pi, oi))
                                yield (ss, pp, oo), (c for c in self.contexts((ss, pp, oo)))
                            else: # given object not found
                                pass
                        else: # subject+predicate is given, object unbound
                            for o in subjectDictionary[pi].keys():
                                ss, pp, oo = self.intToIdentifier((si, pi, o))
                                yield (ss, pp, oo), (c for c in self.contexts((ss, pp, oo)))
                    else: # given predicate not found
                        pass
                else: # subject given, predicate unbound
                    for p in subjectDictionary.keys():
                        if oi != Any: # object is given
                            if subjectDictionary[p].has_key(oi):
                                ss, pp, oo = self.intToIdentifier((si, p, oi))
                                yield (ss, pp, oo), (c for c in self.contexts((ss, pp, oo)))
                            else: # given object not found
                                pass
                        else: # object unbound
                            for o in subjectDictionary[p].keys():
                                ss, pp, oo = self.intToIdentifier((si, p, o))
                                yield (ss, pp, oo), (c for c in self.contexts((ss, pp, oo)))
            else: # given subject not found
                pass
        elif pi != Any: # predicate is given, subject unbound
            if pos.has_key(pi):
                predicateDictionary = pos[pi]
                if oi != Any: # predicate+object is given, subject unbound
                    if predicateDictionary.has_key(oi):
                        for s in predicateDictionary[oi].keys():
                            ss, pp, oo = self.intToIdentifier((s, pi, oi))
                            yield (ss, pp, oo), (c for c in self.contexts((ss, pp, oo)))
                    else: # given object not found
                        pass
                else: # predicate is given, object+subject unbound
                    for o in predicateDictionary.keys():
                        for s in predicateDictionary[o].keys():
                            ss, pp, oo = self.intToIdentifier((s, pi, o))
                            yield (ss, pp, oo), (c for c in self.contexts((ss, pp, oo)))
        elif oi != Any: # object is given, subject+predicate unbound
            if osp.has_key(oi):
                objectDictionary = osp[oi]
                for s in objectDictionary.keys():
                    for p in objectDictionary[s].keys():
                        ss, pp, oo = self.intToIdentifier((s, p, oi))
                        yield (ss, pp, oo), (c for c in self.contexts((ss, pp, oo)))
        else: # subject+predicate+object unbound
            for s in spo.keys():
                subjectDictionary = spo[s]
                for p in subjectDictionary.keys():
                    for o in subjectDictionary[p].keys():
                        ss, pp, oo = self.intToIdentifier((s, p, o))
                        yield (ss, pp, oo), (c for c in self.contexts((ss, pp, oo)))

    def __len__(self, context=None):

        if context is not None:
            if context == self:
                context = None

        # TODO: for eff. implementation
        count = 0
        for triple, cg in self.triples((Any, Any, Any), context):
            count += 1
        return count

    def contexts(self, triple=None):
        if triple:
            si, pi, oi = self.identifierToInt(triple)
            for ci in self.spo[si][pi][oi]:
                yield self.forward[ci]
        else:
            for ci in self.cspo.keys():
                yield self.forward[ci]




import random

def randid(randint=random.randint, choice=random.choice, signs=(-1,1)):
    return choice(signs)*randint(1,2000000000)

del random

########NEW FILE########
__FILENAME__ = Memory
from __future__ import generators

ANY = None

from rdflib.store import Store

class Memory(Store):
    """\
An in memory implementation of a triple store.

This triple store uses nested dictionaries to store triples. Each
triple is stored in two such indices as follows spo[s][p][o] = 1 and
pos[p][o][s] = 1.
    """
    def __init__(self, configuration=None, identifier=None):
        super(Memory, self).__init__(configuration)
        self.identifier = identifier

        # indexed by [subject][predicate][object]
        self.__spo = {}

        # indexed by [predicate][object][subject]
        self.__pos = {}

        # indexed by [predicate][object][subject]
        self.__osp = {}

        self.__namespace = {}
        self.__prefix = {}

    def add(self, (subject, predicate, object), context, quoted=False):
        """\
        Add a triple to the store of triples.
        """
        # add dictionary entries for spo[s][p][p] = 1 and pos[p][o][s]
        # = 1, creating the nested dictionaries where they do not yet
        # exits.
        spo = self.__spo
        try:
            po = spo[subject]
        except:
            po = spo[subject] = {}
        try:
            o = po[predicate]
        except:
            o = po[predicate] = {}
        o[object] = 1

        pos = self.__pos
        try:
            os = pos[predicate]
        except:
            os = pos[predicate] = {}
        try:
            s = os[object]
        except:
            s = os[object] = {}
        s[subject] = 1

        osp = self.__osp
        try:
            sp = osp[object]
        except:
            sp = osp[object] = {}
        try:
            p = sp[subject]
        except:
            p = sp[subject] = {}
        p[predicate] = 1

    def remove(self, (subject, predicate, object), context=None):
        for (subject, predicate, object), c in self.triples((subject, predicate, object)):
            del self.__spo[subject][predicate][object]
            del self.__pos[predicate][object][subject]
            del self.__osp[object][subject][predicate]

    def triples(self, (subject, predicate, object), context=None):
        """A generator over all the triples matching """
        if subject!=ANY: # subject is given
            spo = self.__spo
            if subject in spo:
                subjectDictionary = spo[subject]
                if predicate!=ANY: # subject+predicate is given
                    if predicate in subjectDictionary:
                        if object!=ANY: # subject+predicate+object is given
                            if object in subjectDictionary[predicate]:
                                yield (subject, predicate, object), self.__contexts()
                            else: # given object not found
                                pass
                        else: # subject+predicate is given, object unbound
                            for o in subjectDictionary[predicate].keys():
                                yield (subject, predicate, o), self.__contexts()
                    else: # given predicate not found
                        pass
                else: # subject given, predicate unbound
                    for p in subjectDictionary.keys():
                        if object!=ANY: # object is given
                            if object in subjectDictionary[p]:
                                yield (subject, p, object), self.__contexts()
                            else: # given object not found
                                pass
                        else: # object unbound
                            for o in subjectDictionary[p].keys():
                                yield (subject, p, o), self.__contexts()
            else: # given subject not found
                pass
        elif predicate!=ANY: # predicate is given, subject unbound
            pos = self.__pos
            if predicate in pos:
                predicateDictionary = pos[predicate]
                if object!=ANY: # predicate+object is given, subject unbound
                    if object in predicateDictionary:
                        for s in predicateDictionary[object].keys():
                            yield (s, predicate, object), self.__contexts()
                    else: # given object not found
                        pass
                else: # predicate is given, object+subject unbound
                    for o in predicateDictionary.keys():
                        for s in predicateDictionary[o].keys():
                            yield (s, predicate, o), self.__contexts()
        elif object!=ANY: # object is given, subject+predicate unbound
            osp = self.__osp
            if object in osp:
                objectDictionary = osp[object]
                for s in objectDictionary.keys():
                    for p in objectDictionary[s].keys():
                        yield (s, p, object), self.__contexts()
        else: # subject+predicate+object unbound
            spo = self.__spo
            for s in spo.keys():
                subjectDictionary = spo[s]
                for p in subjectDictionary.keys():
                    for o in subjectDictionary[p].keys():
                        yield (s, p, o), self.__contexts()

    def __len__(self, context=None):
        #@@ optimize
        i = 0
        for triple in self.triples((None, None, None)):
            i += 1
        return i

    def bind(self, prefix, namespace):
        self.__prefix[namespace] = prefix
        self.__namespace[prefix] = namespace

    def namespace(self, prefix):
        return self.__namespace.get(prefix, None)

    def prefix(self, namespace):
        return self.__prefix.get(namespace, None)

    def namespaces(self):
        for prefix, namespace in self.__namespace.iteritems():
            yield prefix, namespace

    def __contexts(self):
        return (c for c in []) # TODO: best way to return empty generator


########NEW FILE########
__FILENAME__ = MySQL
from __future__ import generators
from rdflib import BNode
from rdflib.store import Store,VALID_STORE, CORRUPTED_STORE, NO_STORE, UNKNOWN
from rdflib.Literal import Literal
from pprint import pprint
import MySQLdb,sys
from rdflib.term_utils import *
from rdflib.Graph import QuotedGraph
from rdflib.store.REGEXMatching import REGEXTerm, NATIVE_REGEX, PYTHON_REGEX
from rdflib.store.AbstractSQLStore import *
from FOPLRelationalModel.RelationalHash import IdentifierHash, LiteralHash, RelationalHash, GarbageCollectionQUERY
from FOPLRelationalModel.BinaryRelationPartition import *
from FOPLRelationalModel.QuadSlot import *

Any = None

def ParseConfigurationString(config_string):
    """
    Parses a configuration string in the form:
    key1=val1,key2=val2,key3=val3,...
    The following configuration keys are expected (not all are required):
    user
    password
    db
    host
    port (optional - defaults to 3306)
    """
    kvDict = dict([(part.split('=')[0],part.split('=')[-1]) for part in config_string.split(',')])
    for requiredKey in ['user','db','host']:
        assert requiredKey in kvDict
    if 'port' not in kvDict:
        kvDict['port']=3306
    if 'password' not in kvDict:
        kvDict['password']=''
    return kvDict

def createTerm(termString,termType,store,objLanguage=None,objDatatype=None):
    if termType == 'L':
        cache = store.literalCache.get((termString,objLanguage,objDatatype))
        if cache is not None:
            #store.cacheHits += 1
            return cache
        else:
            #store.cacheMisses += 1
            rt = Literal(termString,objLanguage,objDatatype)
            store.literalCache[((termString,objLanguage,objDatatype))] = rt
            return rt
    elif termType=='F':
        cache = store.otherCache.get((termType,termString))
        if cache is not None:
            #store.cacheHits += 1
            return cache
        else:
            #store.cacheMisses += 1
            rt = QuotedGraph(store,URIRef(termString))
            store.otherCache[(termType,termString)] = rt
            return rt
    elif termType == 'B':
        cache = store.bnodeCache.get((termString))
        if cache is not None:
            #store.cacheHits += 1
            return cache
        else:
            #store.cacheMisses += 1
            rt = TERM_INSTANCIATION_DICT[termType](termString)
            store.bnodeCache[(termString)] = rt
            return rt
    elif termType =='U':
        cache = store.uriCache.get((termString))
        if cache is not None:
            #store.cacheHits += 1
            return cache
        else:
            #store.cacheMisses += 1
            rt = URIRef(termString)
            store.uriCache[(termString)] = rt
            return rt
    else:
        cache = store.otherCache.get((termType,termString))
        if cache is not None:
            #store.cacheHits += 1
            return cache
        else:
            #store.cacheMisses += 1
            rt = TERM_INSTANCIATION_DICT[termType](termString)
            store.otherCache[(termType,termString)] = rt
            return rt

def extractTriple(tupleRt,store,hardCodedContext=None):
    subject,sTerm,predicate,pTerm,obj,oTerm,rtContext,cTerm,objDatatype,objLanguage = tupleRt
    context = rtContext is not None and rtContext or hardCodedContext.identifier

    s=createTerm(subject,sTerm,store)
    p=createTerm(predicate,pTerm,store)
    o=createTerm(obj,oTerm,store,objLanguage,objDatatype)

    graphKlass, idKlass = constructGraph(cTerm)
    return s,p,o,(graphKlass,idKlass,context)


class MySQL(Store):
    """
    MySQL implementation of FOPL Relational Model as an rdflib Store
    """
    context_aware = True
    formula_aware = True
    transaction_aware = True
    regex_matching = NATIVE_REGEX

    def __init__(self, identifier=None, configuration=None):
        self.identifier = identifier and identifier or 'hardcoded'
        #Use only the first 10 bytes of the digest
        self._internedId = INTERNED_PREFIX + sha.new(self.identifier).hexdigest()[:10]

        #Setup FOPL RelationalModel objects
        self.idHash = IdentifierHash(self._internedId)
        self.valueHash = LiteralHash(self._internedId)
        self.binaryRelations = NamedBinaryRelations(self._internedId,self.idHash,self.valueHash)
        self.literalProperties = NamedLiteralProperties(self._internedId,self.idHash,self.valueHash)
        self.aboxAssertions = AssociativeBox(self._internedId,self.idHash,self.valueHash)
        self.tables = [
                       self.binaryRelations,
                       self.literalProperties,
                       self.aboxAssertions,
                       self.idHash,
                       self.valueHash
                       ]
        self.createTables = [
                       self.idHash,
                       self.valueHash,
                       self.binaryRelations,
                       self.literalProperties,
                       self.aboxAssertions
                       ]
        self.hashes = [self.idHash,self.valueHash]
        self.partitions = [self.literalProperties,self.binaryRelations,self.aboxAssertions,]

        #This parameter controls how exlusively the literal table is searched
        #If true, the Literal partition is searched *exclusively* if the object term
        #in a triple pattern is a Literal or a REGEXTerm.  Note, the latter case
        #prevents the matching of URIRef nodes as the objects of a triple in the store.
        #If the object term is a wildcard (None)
        #Then the Literal paritition is searched in addition to the others
        #If this parameter is false, the literal partition is searched regardless of what the object
        #of the triple pattern is
        self.STRONGLY_TYPED_TERMS = False
        self._db = None
        if configuration is not None:
            self.open(configuration)

        self.cacheHits = 0
        self.cacheMisses = 0

        self.literalCache = {}
        self.uriCache = {}
        self.bnodeCache = {}
        self.otherCache = {}

    def executeSQL(self,cursor,qStr,params=None,paramList=False):
        """
        Overridded in order to pass params seperate from query for MySQLdb
        to optimize
        """
        #self._db.autocommit(False)
        if params is None:
            cursor.execute(qStr)
        elif paramList:
            cursor.executemany(qStr,[tuple(item) for item in params])
        else:
            cursor.execute(qStr,tuple(params))

    #Database Management Methods
    def open(self, configuration, create=False):
        """
        Opens the store specified by the configuration string. If
        create is True a store will be created if it does not already
        exist. If create is False and a store does not already exist
        an exception is raised. An exception is also raised if a store
        exists, but there is insufficient permissions to open the
        store.
        """
        configDict = ParseConfigurationString(configuration)
        if create:
            test_db = MySQLdb.connect(user=configDict['user'],
                                      passwd=configDict['password'],
                                      db='test',
                                      port=configDict['port'],
                                      host=configDict['host'],
                                      #use_unicode=True,
                                      #read_default_file='/etc/my-client.cnf'
                                      )
            c=test_db.cursor()
            c.execute("""SET AUTOCOMMIT=0""")
            c.execute("""SHOW DATABASES""")
            if not (configDict['db'].encode('utf-8'),) in c.fetchall():
                print "creating %s (doesn't exist)"%(configDict['db'])
                c.execute("""CREATE DATABASE %s"""%(configDict['db'],))
                test_db.commit()
                c.close()
                test_db.close()

            db = MySQLdb.connect(user = configDict['user'],
                                 passwd = configDict['password'],
                                 db=configDict['db'],
                                 port=configDict['port'],
                                 host=configDict['host'],
                                 #use_unicode=True,
                                 #read_default_file='/etc/my-client.cnf'
                                 )
            c=db.cursor()
            c.execute("""SET AUTOCOMMIT=0""")
            c.execute(CREATE_NS_BINDS_TABLE%(self._internedId))
            for kb in self.createTables:
                c.execute(kb.createSQL())
                if isinstance(kb,RelationalHash) and kb.defaultSQL():
                    c.execute(kb.defaultSQL())

            db.commit()
            c.close()
            db.close()
        try:
            port = int(configDict['port'])
        except:
            raise ArithmeticError('MySQL port must be a valid integer')
        self._db = MySQLdb.connect(user = configDict['user'],
                                   passwd = configDict['password'],
                                   db=configDict['db'],
                                   port=port,
                                   host=configDict['host'],
                                   #use_unicode=True,
                                   #read_default_file='/etc/my.cnf'
                                  )
        self._db.autocommit(False)
        c=self._db.cursor()
        c.execute("""SHOW DATABASES""")
        #FIXME This is a character set hack.  See: http://sourceforge.net/forum/forum.php?thread_id=1448424&forum_id=70461
        #self._db.charset = 'utf8'
        rt = c.fetchall()
        if (configDict['db'].encode('utf-8'),) in rt:
            for tn in self.tables:
                c.execute("""show tables like '%s'"""%(tn,))
                rt=c.fetchall()
                if not rt:
                    sys.stderr.write("table %s Doesn't exist\n" % (tn));
                    #The database exists, but one of the partitions doesn't exist
                    return CORRUPTED_STORE
            #Everything is there (the database and the partitions)
            return VALID_STORE
        #The database doesn't exist - nothing is there
        return NO_STORE

    def destroy(self, configuration):
        """
        FIXME: Add documentation
        """
        configDict = ParseConfigurationString(configuration)
        msql_db = MySQLdb.connect(user=configDict['user'],
                                passwd=configDict['password'],
                                db=configDict['db'],
                                port=configDict['port'],
                                host=configDict['host']
                                )
        msql_db.autocommit(False)
        c=msql_db.cursor()
        for tbl in self.tables + ["%s_namespace_binds"%self._internedId]:
            try:
                c.execute('DROP table %s'%tbl)
                #print "dropped table: %s"%(tblsuffix%(self._internedId))
            except Exception, e:
                print "unable to drop table: %s"%(tbl)
                print e

        #Note, this only removes the associated tables for the closed world universe given by the identifier
        print "Destroyed Close World Universe %s ( in MySQL database %s)"%(self.identifier,configDict['db'])
        msql_db.commit()
        msql_db.close()

    #Transactional interfaces
    def commit(self):
        """ """
        self._db.commit()

    def rollback(self):
        """ """
        self._db.rollback()

    def gc(self):
        """
        Purges unreferenced identifiers / values - expensive
        """
        c=self._db.cursor()
        purgeQueries = GarbageCollectionQUERY(
                                               self.idHash,
                                               self.valueHash,
                                               self.binaryRelations,
                                               self.aboxAssertions,
                                               self.literalProperties)

        for q in purgeQueries:
            self.executeSQL(c,q)

    def add(self, (subject, predicate, obj), context=None, quoted=False):
        """ Add a triple to the store of triples. """
        qSlots = genQuadSlots([subject,predicate,obj,context])
        if predicate == RDF.type:
            kb = self.aboxAssertions
        elif isinstance(obj,Literal):
            kb = self.literalProperties
        else:
            kb = self.binaryRelations
        kb.insertRelations([qSlots])
        kb.flushInsertions(self._db)

    def addN(self, quads):
        """
        Adds each item in the list of statements to a specific context. The quoted argument
        is interpreted by formula-aware stores to indicate this statement is quoted/hypothetical.
        Note that the default implementation is a redirect to add
        """
        for s,p,o,c in quads:
            assert c is not None, "Context associated with %s %s %s is None!"%(s,p,o)
            qSlots = genQuadSlots([s,p,o,c])
            if p == RDF.type:
                kb = self.aboxAssertions
            elif isinstance(o,Literal):
                kb = self.literalProperties
            else:
                kb = self.binaryRelations

            kb.insertRelations([qSlots])

        for kb in self.partitions:
            if kb.pendingInsertions:
                kb.flushInsertions(self._db)

    def remove(self, (subject, predicate, obj), context):
        """ Remove a triple from the store """
        targetBRPs = BinaryRelationPartitionCoverage((subject,predicate,obj,context),self.partitions)
        c=self._db.cursor()
        for brp in targetBRPs:
            query = "DELETE %s from %s %s WHERE "%(
                                          brp,
                                          brp,
                                          brp.generateHashIntersections()
                                        )
            whereClause,whereParameters = brp.generateWhereClause((subject,predicate,obj,context))
            self.executeSQL(c,query+whereClause,params=whereParameters)

        c.close()

    def triples(self, (subject, predicate, obj), context=None):
        c=self._db.cursor()
        if context is None or isinstance(context.identifier,REGEXTerm):
            rt=PatternResolution((subject,predicate,obj,context),c,self.partitions,fetchall=False)
        else:
            #No need to order by triple (expensive), all result sets will be in the same context
            rt=PatternResolution((subject,predicate,obj,context),c,self.partitions,orderByTriple=False,fetchall=False)
        while rt:
            s,p,o,(graphKlass,idKlass,graphId) = extractTriple(rt,self,context)
            currentContext=(context is None or isinstance(context.identifier,REGEXTerm)) and graphKlass(self,idKlass(graphId)) or context
            contexts = [currentContext]
            rt = next = c.fetchone()
            if context is None or isinstance(context.identifier,REGEXTerm):
                sameTriple = next and extractTriple(next,self,context)[:3] == (s,p,o)
                while sameTriple:
                    s2,p2,o2,(graphKlass,idKlass,graphId) = extractTriple(next,self,context)
                    c2 = graphKlass(self,idKlass(graphId))
                    contexts.append(c2)
                    rt = next = c.fetchone()
                    sameTriple = next and extractTriple(next,self,context)[:3] == (s,p,o)

            yield (s,p,o),(c for c in contexts)

    def triples_choices(self, (subject, predicate, object_),context=None):
        """
        A variant of triples that can take a list of terms instead of a single
        term in any slot.  Stores can implement this to optimize the response time
        from the import default 'fallback' implementation, which will iterate
        over each term in the list and dispatch to tripless
        """
        if isinstance(object_,list):
            assert not isinstance(subject,list), "object_ / subject are both lists"
            assert not isinstance(predicate,list), "object_ / predicate are both lists"
            if not object_:
                object_ = None
            for (s1, p1, o1), cg in self.triples((subject,predicate,object_),context):
                yield (s1, p1, o1), cg

        elif isinstance(subject,list):
            assert not isinstance(predicate,list), "subject / predicate are both lists"
            if not subject:
                subject = None
            for (s1, p1, o1), cg in self.triples((subject,predicate,object_),context):
                yield (s1, p1, o1), cg

        elif isinstance(predicate,list):
            assert not isinstance(subject,list), "predicate / subject are both lists"
            if not predicate:
                predicate = None
            for (s1, p1, o1), cg in self.triples((subject,predicate,object_),context):
                yield (s1, p1, o1), cg

    def __repr__(self):
        c=self._db.cursor()

        rtDict = {}
        countRows = "select count(*) from %s"
        countContexts = "select DISTINCT %s from %s"
        unionSelect = ' union '.join([countContexts%(part.columnNames[CONTEXT],str(part)) for part in self.partitions])
        self.executeSQL(c,unionSelect)
        ctxCount = len(c.fetchall())
        for part in self.partitions:
            self.executeSQL(c,countRows%part)
            rowCount = c.fetchone()[0]
            rtDict[str(part)]=rowCount
        return "<Parititioned MySQL N3 Store: %s context(s), %s classification(s), %s property/value assertion(s), and %s other relation(s)>"%(
            ctxCount,
            rtDict[str(self.aboxAssertions)],
            rtDict[str(self.literalProperties)],
            rtDict[str(self.binaryRelations)],
        )

    def __len__(self, context=None):
        rows = []
        countRows = "select count(*) from %s"
        c=self._db.cursor()
        for part in self.partitions:
            if context is not None:
                whereClause,whereParams = part.generateWhereClause((None,None,None,context.identifier)) 
                self.executeSQL(c,countRows%part + " where " + whereClause,whereParams)
            else:
                self.executeSQL(c,countRows%part)
            rowCount = c.fetchone()[0]
            rows.append(rowCount)
        return reduce(lambda x,y: x+y,rows)

    def contexts(self, triple=None):
        c=self._db.cursor()
        if triple:
            subject,predicate,obj = triple
        else:
            subject = predicate = obj = None
        rt=PatternResolution((subject,predicate,obj,None),
                              c,
                              self.partitions,
                              fetchall=False,
                              fetchContexts=True)
        while rt:
            contextId,cTerm = rt
            graphKlass, idKlass = constructGraph(cTerm)
            yield graphKlass(self,idKlass(contextId))
            rt = c.fetchone()

    #Namespace persistence interface implementation
    def bind(self, prefix, namespace):
        """ """
        c=self._db.cursor()
        try:
            self.executeSQL(
                c,
                "INSERT INTO %s_namespace_binds VALUES ('%s', '%s')"%(
                self._internedId,
                prefix,
                namespace)
            )
        except:
            pass
        c.close()

    def prefix(self, namespace):
        """ """
        c=self._db.cursor()
        self.executeSQL(c,"select prefix from %s_namespace_binds where uri = '%s'"%(
            self._internedId,
            namespace)
        )
        rt = [rtTuple[0] for rtTuple in c.fetchall()]
        c.close()
        return rt and rt[0] or None

    def namespace(self, prefix):
        """ """
        c=self._db.cursor()
        try:
            self.executeSQL(c,"select uri from %s_namespace_binds where prefix = '%s'"%(
                self._internedId,
                prefix)
                      )
        except:
            return None
        rt = [rtTuple[0] for rtTuple in c.fetchall()]
        c.close()
        return rt and rt[0] or None

    def namespaces(self):
        """ """
        c=self._db.cursor()
        self.executeSQL(c,"select prefix, uri from %s_namespace_binds where 1;"%(
            self._internedId
            )
        )
        rt=c.fetchall()
        c.close()
        for prefix,uri in rt:
            yield prefix,uri


CREATE_NS_BINDS_TABLE = """
CREATE TABLE %s_namespace_binds (
    prefix        varchar(20) UNIQUE not NULL,
    uri           text,
    PRIMARY KEY (prefix),
    INDEX uri_index (uri(100))) ENGINE=InnoDB"""        

########NEW FILE########
__FILENAME__ = NodePickler
##############

from cPickle import Pickler, Unpickler, UnpicklingError
from cStringIO import StringIO


class NodePickler(object):
    def __init__(self):
        self._objects = {}
        self._ids = {}
        self._get_object = self._objects.__getitem__

    def _get_ids(self, key):
        try:
            return self._ids.get(key)
        except TypeError, e:
            return None

    def register(self, object, id):
        self._objects[id] = object
        self._ids[object] = id

    def loads(self, s):
        up = Unpickler(StringIO(s))
        up.persistent_load = self._get_object
        try:
            return up.load()
        except KeyError, e:
            raise UnpicklingError, "Could not find Node class for %s" % e

    def dumps(self, obj, protocol=None, bin=None):
        src = StringIO()
        p = Pickler(src)
        p.persistent_id = self._get_ids
        p.dump(obj)
        return src.getvalue()


########NEW FILE########
__FILENAME__ = Redland

import rdflib
from rdflib.Graph import Graph
from rdflib.URIRef import URIRef
from rdflib.Node import Node
from rdflib.BNode import BNode
from rdflib.Literal import Literal
import RDF

from rdflib.store import Store

def _t(i):
    if isinstance(i, rdflib.URIRef):
        return RDF.Node(RDF.Uri(unicode(i)))
    if isinstance(i, rdflib.BNode):
        return RDF.Node(blank=str(i))
    if isinstance(i, rdflib.Literal):
        return RDF.Node(literal=str(i))
    if isinstance(i, Graph):
        return _t(i.identifier)
    if i is None:
        return None
    raise TypeError, 'Cannot convert %s' % `i`

def _c(i):
    return _t(i)


def _f(i):
    if isinstance(i, RDF.Uri):
        return rdflib.URIRef(i)
    if isinstance(i, RDF.Node):
        if i.is_blank():
            return rdflib.BNode(i.blank_identifier)
        elif i.is_literal():
            return rdflib.Literal(i)
        else:
            return URIRef(i.uri)
    if i is None:
        return None
    raise TypeError, 'Cannot convert %s' % `i`


class Redland(Store):
    context_aware = True
    def __init__(self, model=None):
        super(Redland, self).__init__()
        if model is None:
            model = RDF.Model(RDF.MemoryStorage(options_string="contexts='yes'"))
        self.model = model

    def __len__(self, context=None):
        """ Return number of triples (statements in librdf). """

        count = 0
        for triple, cg in self.triples((None, None, None), context):
            count += 1
        return count

    def add(self, (subject, predicate, object), context=None, quoted=False):
        """\
        Add a triple to the store of triples.
        """
        if context is not None:
            self.model.append(RDF.Statement(_t(subject), _t(predicate), _t(object)), _c(context))
        else:
            self.model.append(RDF.Statement(_t(subject), _t(predicate), _t(object)))

    def remove(self, (subject, predicate, object), context, quoted=False):
        if context is None:
            contexts = self.contexts()
        else:
            contexts = [context]
        for context in contexts:
            if subject is None and predicate is None and object is None:
                self.model.remove_statements_with_context(_c(context))
            else:
                del self.model[RDF.Statement(_t(subject), _t(predicate), _t(object)), _c(context)]

    def triples(self, (subject, predicate, object), context=None):
        """A generator over all the triples matching """
        cgraph = RDF.Model()
        triple = RDF.Statement(_t(subject), _t(predicate), _t(object))
        for statement, c in self.model.find_statements_context(triple):
            if context is None or _f(c) == context.identifier:
                cgraph.append(statement)
        for statement in cgraph.find_statements(triple):
            ret = []
            for c in self.model.get_contexts():
                if self.model.contains_statement_context(statement, _c(context)):
                    ret.append(c)
            yield (_f(statement.subject), _f(statement.predicate), _f(statement.object)), iter(ret)

    def contexts(self, triple=None): # TODO: have Graph support triple?
        for context in self.model.get_contexts():
            yield Graph(self, _f(context))

    def bind(self, prefix, namespace):
        pass

    def namespace(self, prefix):
        pass

    def prefix(self, namespace):
        pass

    def namespaces(self):
        pass


########NEW FILE########
__FILENAME__ = REGEXMatching
"""
This wrapper intercepts calls through the store interface which  make use of
The REGEXTerm class to represent matches by REGEX instead of literal comparison
Implemented for stores that don't support this and essentially provides the support
by replacing the REGEXTerms by wildcards (None) and matching against the results
from the store it's wrapping
"""

from rdflib.store import Store
from pprint import pprint
from rdflib.Graph import Graph, QuotedGraph, ConjunctiveGraph, BackwardCompatGraph
import re

#Store is capable of doing it's own REGEX matching
NATIVE_REGEX = 0
#Store uses python's re module internally for REGEX matching (SQLite for instance)
PYTHON_REGEX = 1

#REGEXTerm can be used in any term slot and is interpreted as
#a request to perform a REGEX match (not a string comparison) using the value
#(pre-compiled) for checkin rdf:type matches
class REGEXTerm(unicode):
    def __init__(self,expr):
        self.compiledExpr = re.compile(expr)

    def __reduce__(self):
        return (REGEXTerm, (unicode(''),))

def regexCompareQuad(quad,regexQuad):
    for index in range(4):
        if isinstance(regexQuad[index],REGEXTerm) and not regexQuad[index].compiledExpr.match(quad[index]):
            return False
    return True

class REGEXMatching(Store):
    def __init__(self, storage):
        self.storage = storage
        self.context_aware = storage.context_aware
        #NOTE: this store can't be formula_aware as it doesn't have enough info to reverse
        #The removal of a quoted statement
        self.formula_aware = storage.formula_aware
        self.transaction_aware = storage.transaction_aware

    def open(self, configuration, create=True):
        return self.storage.open(configuration,create)

    def close(self, commit_pending_transaction=False):
        self.storage.close()

    def destroy(self, configuration):
        self.storage.destroy(configuration)

    def add(self, (subject, predicate, object_), context, quoted=False):
        self.storage.add((subject, predicate, object_), context, quoted)

    def remove(self, (subject, predicate, object_), context=None):
        if isinstance(subject,REGEXTerm) or \
           isinstance(predicate,REGEXTerm) or \
           isinstance(object_,REGEXTerm) or \
           (context is not None and isinstance(context.identifier,REGEXTerm)):
            #One or more of the terms is a REGEX expression, so we must replace it / them with wildcard(s)
            #and match after we query
            s = not isinstance(subject,REGEXTerm) and subject or None
            p = not isinstance(predicate,REGEXTerm) and predicate or None
            o = not isinstance(object_,REGEXTerm) and object_ or None
            c = (context is not None and not isinstance(context.identifier,REGEXTerm)) and context or None

            removeQuadList = []
            for (s1,p1,o1),cg in self.storage.triples((s,p,o),c):
                for ctx in cg:
                    ctx = ctx.identifier
                    if regexCompareQuad((s1,p1,o1,ctx),(subject,predicate,object_,context is not None and context.identifier or context)):
                        removeQuadList.append((s1,p1,o1,ctx))
            for s,p,o,c in removeQuadList:
                self.storage.remove((s,p,o),c and Graph(self,c) or c)
        else:
            self.storage.remove((subject,predicate,object_),context)

    def triples(self, (subject, predicate, object_), context=None):
        if isinstance(subject,REGEXTerm) or \
           isinstance(predicate,REGEXTerm) or \
           isinstance(object_,REGEXTerm) or \
           (context is not None and isinstance(context.identifier,REGEXTerm)):
            #One or more of the terms is a REGEX expression, so we must replace it / them with wildcard(s)
            #and match after we query
            s = not isinstance(subject,REGEXTerm) and subject or None
            p = not isinstance(predicate,REGEXTerm) and predicate or None
            o = not isinstance(object_,REGEXTerm) and object_ or None
            c = (context is not None and not isinstance(context.identifier,REGEXTerm)) and context or None
            for (s1,p1,o1),cg in self.storage.triples((s,p,o),c):
                matchingCtxs = []
                for ctx in cg:
                    if c is None:
                        if context is None or context.identifier.compiledExpr.match(ctx.identifier):
                            matchingCtxs.append(ctx)
                    else:
                        matchingCtxs.append(ctx)
                if matchingCtxs and regexCompareQuad((s1,p1,o1,None),(subject,predicate,object_,None)):
                    yield (s1,p1,o1),(c for c in matchingCtxs)
        else:
            for (s1,p1,o1),cg in self.storage.triples((subject, predicate, object_), context):
                yield (s1,p1,o1),cg

    def __len__(self, context=None):
        #NOTE: If the context is a REGEX this could be an expensive proposition
        return self.storage.__len__(context)

    def contexts(self, triple=None):
        #NOTE: There is no way to control REGEX matching for this method at this level
        #(as it only returns the contexts, not the matching triples
        for ctx in self.storage.contexts(triple):
            yield ctx

    def remove_context(self, identifier):
        self.storage.remove((None,None,None),identifier)

    def bind(self, prefix, namespace):
        self.storage.bind(prefix, namespace)

    def prefix(self, namespace):
        return self.storage.prefix(namespace)

    def namespace(self, prefix):
        return self.storage.namespace(prefix)

    def namespaces(self):
        return self.storage.namespaces()

    def commit(self):
        self.storage.commit()

    def rollback(self):
        self.storage.rollback()

########NEW FILE########
__FILENAME__ = Sleepycat
from rdflib.store import Store
from rdflib.URIRef import URIRef

from bsddb import db

from os import mkdir
from os.path import exists, abspath
from urllib import pathname2url
from threading import Thread

import logging
_logger = logging.getLogger(__name__)

class Sleepycat(Store):
    context_aware = True
    formula_aware = True

    def __init__(self, configuration=None, identifier=None):
        self.__open = False
        self.__identifier = identifier
        super(Sleepycat, self).__init__(configuration)
        self.configuration = configuration
        self._loads = self.node_pickler.loads
        self._dumps = self.node_pickler.dumps

    def __get_identifier(self):
        return self.__identifier
    identifier = property(__get_identifier)

    def open(self, path, create=True):
        homeDir = path
        envsetflags  = db.DB_CDB_ALLDB
        envflags = db.DB_INIT_MPOOL | db.DB_INIT_CDB | db.DB_THREAD
        if not exists(homeDir):
            if create==True:
                mkdir(homeDir) # TODO: implement create method and refactor this to it
                self.create(path)
            else:
                return -1
        if self.__identifier is None:
            self.__identifier = URIRef(pathname2url(abspath(homeDir)))
        self.db_env = db_env = db.DBEnv()
        db_env.set_cachesize(0, 1024*1024*50) # TODO
        #db_env.set_lg_max(1024*1024)
        db_env.set_flags(envsetflags, 1)
        db_env.open(homeDir, envflags | db.DB_CREATE)

        self.__open = True

        dbname = None
        dbtype = db.DB_BTREE
        dbopenflags = db.DB_THREAD

        dbmode = 0660
        dbsetflags   = 0

        # create and open the DBs
        self.__indicies = [None,] * 3
        self.__indicies_info = [None,] * 3
        for i in xrange(0, 3):
            index_name = to_key_func(i)(("s", "p", "o"), "c")
            index = db.DB(db_env)
            index.set_flags(dbsetflags)
            index.open(index_name, dbname, dbtype, dbopenflags|db.DB_CREATE, dbmode)
            self.__indicies[i] = index
            self.__indicies_info[i] = (index, to_key_func(i), from_key_func(i))

        lookup = {}
        for i in xrange(0, 8):
            results = []
            for start in xrange(0, 3):
                score = 1
                len = 0
                for j in xrange(start, start+3):
                    if i & (1<<(j%3)):
                        score = score << 1
                        len += 1
                    else:
                        break
                tie_break = 2-start
                results.append(((score, tie_break), start, len))

            results.sort()
            score, start, len = results[-1]

            def get_prefix_func(start, end):
                def get_prefix(triple, context):
                    if context is None:
                        yield ""
                    else:
                        yield context
                    i = start
                    while i<end:
                        yield triple[i%3]
                        i += 1
                    yield ""
                return get_prefix

            lookup[i] = (self.__indicies[start], get_prefix_func(start, start + len), from_key_func(start), results_from_key_func(start, self._from_string))


        self.__lookup_dict = lookup

        self.__contexts = db.DB(db_env)
        self.__contexts.set_flags(dbsetflags)
        self.__contexts.open("contexts", dbname, dbtype, dbopenflags|db.DB_CREATE, dbmode)

        self.__namespace = db.DB(db_env)
        self.__namespace.set_flags(dbsetflags)
        self.__namespace.open("namespace", dbname, dbtype, dbopenflags|db.DB_CREATE, dbmode)

        self.__prefix = db.DB(db_env)
        self.__prefix.set_flags(dbsetflags)
        self.__prefix.open("prefix", dbname, dbtype, dbopenflags|db.DB_CREATE, dbmode)

        self.__k2i = db.DB(db_env)
        self.__k2i.set_flags(dbsetflags)
        self.__k2i.open("k2i", dbname, db.DB_HASH, dbopenflags|db.DB_CREATE, dbmode)

        self.__i2k = db.DB(db_env)
        self.__i2k.set_flags(dbsetflags)
        self.__i2k.open("i2k", dbname, db.DB_RECNO, dbopenflags|db.DB_CREATE, dbmode)

        self.__needs_sync = False
        t = Thread(target=self.__sync_run)
        t.setDaemon(True)
        t.start()
        self.__sync_thread = t
        return 1


    def __sync_run(self):
        from time import sleep, time
        try:
            min_seconds, max_seconds = 10, 300
            while self.__open:
                if self.__needs_sync:
                    t0 = t1 = time()
                    self.__needs_sync = False
                    while self.__open:
                        sleep(.1)
                        if self.__needs_sync:
                            t1 = time()
                            self.__needs_sync = False
                        if time()-t1 > min_seconds or time()-t0 > max_seconds:
                            self.__needs_sync = False
                            _logger.debug("sync")
                            self.sync()
                            break
                else:
                    sleep(1)
        except Exception, e:
            _logger.exception(e)

    def sync(self):
        if self.__open:
            for i in self.__indicies:
                i.sync()
            self.__contexts.sync()
            self.__namespace.sync()
            self.__prefix.sync()
            self.__i2k.sync()
            self.__k2i.sync()

    def close(self, commit_pending_transaction=False):
        self.__open = False
        self.__sync_thread.join()
        for i in self.__indicies:
            i.close()
        self.__contexts.close()
        self.__namespace.close()
        self.__prefix.close()
        self.__i2k.close()
        self.__k2i.close()
        self.db_env.close()

    def add(self, (subject, predicate, object), context, quoted=False):
        """\
        Add a triple to the store of triples.
        """
        assert self.__open, "The Store must be open."
        assert context!=self, "Can not add triple directly to store"
        Store.add(self, (subject, predicate, object), context, quoted)

        _to_string = self._to_string

        s = _to_string(subject)
        p = _to_string(predicate)
        o = _to_string(object)
        c = _to_string(context)

        cspo, cpos, cosp = self.__indicies

        value = cspo.get("%s^%s^%s^%s^" % (c, s, p, o))
        if value is None:
            self.__contexts.put(c, "")

            contexts_value = cspo.get("%s^%s^%s^%s^" % ("", s, p, o)) or ""
            contexts = set(contexts_value.split("^"))
            contexts.add(c)
            contexts_value = "^".join(contexts)
            assert contexts_value!=None

            cspo.put("%s^%s^%s^%s^" % (c, s, p, o), "")
            cpos.put("%s^%s^%s^%s^" % (c, p, o, s), "")
            cosp.put("%s^%s^%s^%s^" % (c, o, s, p), "")
            if not quoted:
                cspo.put("%s^%s^%s^%s^" % ("", s, p, o), contexts_value)
                cpos.put("%s^%s^%s^%s^" % ("", p, o, s), contexts_value)
                cosp.put("%s^%s^%s^%s^" % ("", o, s, p), contexts_value)

            self.__needs_sync = True

    def __remove(self, (s, p, o), c, quoted=False):
        cspo, cpos, cosp = self.__indicies
        contexts_value = cspo.get("^".join(("", s, p, o, ""))) or ""
        contexts = set(contexts_value.split("^"))
        contexts.discard(c)
        contexts_value = "^".join(contexts)
        for i, _to_key, _from_key in self.__indicies_info:
            i.delete(_to_key((s, p, o), c))
        if not quoted:
            if contexts_value:
                for i, _to_key, _from_key in self.__indicies_info:
                    i.put(_to_key((s, p, o), ""), contexts_value)
            else:
                for i, _to_key, _from_key in self.__indicies_info:
                    try:
                        i.delete(_to_key((s, p, o), ""))
                    except db.DBNotFoundError, e: 
                        pass # TODO: is it okay to ignore these?

    def remove(self, (subject, predicate, object), context):
        assert self.__open, "The Store must be open."
        Store.remove(self, (subject, predicate, object), context)
        _to_string = self._to_string
        if context is not None:
            if context == self:
                context = None

        if subject is not None and predicate is not None and object is not None and context is not None:
            s = _to_string(subject)
            p = _to_string(predicate)
            o = _to_string(object)
            c = _to_string(context)
            value = self.__indicies[0].get("%s^%s^%s^%s^" % (c, s, p, o))
            if value is not None:
                self.__remove((s, p, o), c)
                self.__needs_sync = True
        else:
            cspo, cpos, cosp = self.__indicies
            index, prefix, from_key, results_from_key = self.__lookup((subject, predicate, object), context)

            cursor = index.cursor()
            try:
                current = cursor.set_range(prefix)
                needs_sync = True
            except db.DBNotFoundError:
                current = None
                needs_sync = False
            cursor.close()
            while current:
                key, value = current
                cursor = index.cursor()
                try:
                    cursor.set_range(key)
                    current = cursor.next()
                except db.DBNotFoundError:
                    current = None
                cursor.close()
                if key.startswith(prefix):
                    c, s, p, o = from_key(key)
                    if context is None:
                        contexts_value = index.get(key) or ""
                        contexts = set(contexts_value.split("^")) # remove triple from all non quoted contexts
                        contexts.add("") # and from the conjunctive index
                        for c in contexts:
                            for i, _to_key, _ in self.__indicies_info:
                                i.delete(_to_key((s, p, o), c))
                    else:
                        self.__remove((s, p, o), c)
                else:
                    break

            if context is not None:
                if subject is None and predicate is None and object is None:
                    # TODO: also if context becomes empty and not just on remove((None, None, None), c)
                    try:
                        self.__contexts.delete(_to_string(context))
                    except db.DBNotFoundError, e:
                        pass

            self.__needs_sync = needs_sync

    def triples(self, (subject, predicate, object), context=None):
        """A generator over all the triples matching """
        assert self.__open, "The Store must be open."

        if context is not None:
            if context == self:
                context = None

        _from_string = self._from_string
        index, prefix, from_key, results_from_key = self.__lookup((subject, predicate, object), context)

        cursor = index.cursor()
        try:
            current = cursor.set_range(prefix)
        except db.DBNotFoundError:
            current = None
        cursor.close()
        while current:
            key, value = current
            cursor = index.cursor()
            try:
                cursor.set_range(key)
                current = cursor.next()
            except db.DBNotFoundError:
                current = None
            cursor.close()
            if key and key.startswith(prefix):
                contexts_value = index.get(key)
                yield results_from_key(key, subject, predicate, object, contexts_value)
            else:
                break

    def __len__(self, context=None):
        assert self.__open, "The Store must be open."
        if context is not None:
            if context == self:
                context = None

        if context is None:
            prefix = "^"
        else:
            prefix = "%s^" % self._to_string(context)

        index = self.__indicies[0]
        cursor = index.cursor()
        current = cursor.set_range(prefix)
        count = 0
        while current:
            key, value = current
            if key.startswith(prefix):
                count +=1
                current = cursor.next()
            else:
                break
        cursor.close()
        return count

    def bind(self, prefix, namespace):
        prefix = prefix.encode("utf-8")
        namespace = namespace.encode("utf-8")
        bound_prefix = self.__prefix.get(namespace)
        if bound_prefix:
            self.__namespace.delete(bound_prefix)
        self.__prefix[namespace] = prefix
        self.__namespace[prefix] = namespace

    def namespace(self, prefix):
        prefix = prefix.encode("utf-8")
        return self.__namespace.get(prefix, None)

    def prefix(self, namespace):
        namespace = namespace.encode("utf-8")
        return self.__prefix.get(namespace, None)

    def namespaces(self):
        cursor = self.__namespace.cursor()
        results = []
        current = cursor.first()
        while current:
            prefix, namespace = current
            results.append((prefix, namespace))
            current = cursor.next()
        cursor.close()
        for prefix, namespace in results:
            yield prefix, URIRef(namespace)

    def contexts(self, triple=None):
        _from_string = self._from_string
        _to_string = self._to_string

        if triple:
            s, p, o = triple
            s = _to_string(s)
            p = _to_string(p)
            o = _to_string(o)
            contexts = self.__indicies[0].get("%s^%s^%s^%s^" % ("", s, p, o))
            if contexts:
                for c in contexts.split("^"):
                    if c:
                        yield _from_string(c)
        else:
            index = self.__contexts
            cursor = index.cursor()
            current = cursor.first()
            cursor.close()
            while current:
                key, value = current
                context = _from_string(key)
                yield context
                cursor = index.cursor()
                try:
                    cursor.set_range(key)
                    current = cursor.next()
                except db.DBNotFoundError:
                    current = None
                cursor.close()

    def _from_string(self, i):
        k = self.__i2k.get(int(i))
        return self._loads(k)

    def _to_string(self, term):
        k = self._dumps(term)
        i = self.__k2i.get(k)
        if i is None:
            i = "%s" % self.__i2k.append(k)
            self.__k2i.put(k, i)
        return i

    def __lookup(self, (subject, predicate, object), context):
        _to_string = self._to_string
        if context is not None:
            context = _to_string(context)
        i = 0
        if subject is not None:
            i += 1
            subject = _to_string(subject)
        if predicate is not None:
            i += 2
            predicate = _to_string(predicate)
        if object is not None:
            i += 4
            object = _to_string(object)
        index, prefix_func, from_key, results_from_key = self.__lookup_dict[i]
        prefix = "^".join(prefix_func((subject, predicate, object), context))
        return index, prefix, from_key, results_from_key


def to_key_func(i):
    def to_key(triple, context):
        "Takes a string; returns key"
        return "^".join((context, triple[i%3], triple[(i+1)%3], triple[(i+2)%3], "")) # "" to tac on the trailing ^
    return to_key

def from_key_func(i):
    def from_key(key):
        "Takes a key; returns string"
        parts = key.split("^")
        return parts[0], parts[(3-i+0)%3+1], parts[(3-i+1)%3+1], parts[(3-i+2)%3+1]
    return from_key

def results_from_key_func(i, from_string):
    def from_key(key, subject, predicate, object, contexts_value):
        "Takes a key and subject, predicate, object; returns tuple for yield"
        parts = key.split("^")
        if subject is None:
            # TODO: i & 1: # dis assemble and/or measure to see which is faster
            # subject is None or i & 1
            s = from_string(parts[(3-i+0)%3+1])
        else:
            s = subject
        if predicate is None:#i & 2:
            p = from_string(parts[(3-i+1)%3+1])
        else:
            p = predicate
        if object is None:#i & 4:
            o = from_string(parts[(3-i+2)%3+1])
        else:
            o = object
        return (s, p, o), (from_string(c) for c in contexts_value.split("^") if c)
    return from_key

def readable_index(i):
    s, p, o = "?" * 3
    if i & 1: s = "s"
    if i & 2: p = "p"
    if i & 4: o = "o"
    return "%s,%s,%s" % (s, p, o)

########NEW FILE########
__FILENAME__ = SQLite
from __future__ import generators
from rdflib import BNode
from rdflib.Literal import Literal
from pprint import pprint
from pysqlite2 import dbapi2
import sha,sys,re,os
from rdflib.term_utils import *
from rdflib.Graph import QuotedGraph
from rdflib.store.REGEXMatching import REGEXTerm, NATIVE_REGEX, PYTHON_REGEX
from rdflib.store.AbstractSQLStore import *
Any = None

#User-defined REGEXP operator
def regexp(expr, item):
    r = re.compile(expr)
    return r.match(item) is not None

class SQLite(AbstractSQLStore):
    """
    SQLite store formula-aware implementation.  It stores it's triples in the following partitions:

    - Asserted non rdf:type statements
    - Asserted rdf:type statements (in a table which models Class membership)
    The motivation for this partition is primarily query speed and scalability as most graphs will always have more rdf:type statements than others
    - All Quoted statements

    In addition it persists namespace mappings in a seperate table
    """
    context_aware = True
    formula_aware = True
    transaction_aware = True
    regex_matching = PYTHON_REGEX
    autocommit_default = False

    def open(self, home, create=True):
        """
        Opens the store specified by the configuration string. If
        create is True a store will be created if it does not already
        exist. If create is False and a store does not already exist
        an exception is raised. An exception is also raised if a store
        exists, but there is insufficient permissions to open the
        store."""
        if create:
            db = dbapi2.connect(os.path.join(home,self.identifier))
            c=db.cursor()
            c.execute(CREATE_ASSERTED_STATEMENTS_TABLE%(self._internedId))
            c.execute(CREATE_ASSERTED_TYPE_STATEMENTS_TABLE%(self._internedId))
            c.execute(CREATE_QUOTED_STATEMENTS_TABLE%(self._internedId))
            c.execute(CREATE_NS_BINDS_TABLE%(self._internedId))
            c.execute(CREATE_LITERAL_STATEMENTS_TABLE%(self._internedId))
            for tblName,indices in [
                (
                    "%s_asserted_statements",
                    [
                        ("%s_A_termComb_index",('termComb',)),
                        ("%s_A_s_index",('subject',)),
                        ("%s_A_p_index",('predicate',)),
                        ("%s_A_o_index",('object',)),
                        ("%s_A_c_index",('context',)),
                    ],
                ),
                (
                    "%s_type_statements",
                    [
                        ("%s_T_termComb_index",('termComb',)),
                        ("%s_member_index",('member',)),
                        ("%s_klass_index",('klass',)),
                        ("%s_c_index",('context',)),
                    ],
                ),
                (
                    "%s_literal_statements",
                    [
                        ("%s_L_termComb_index",('termComb',)),
                        ("%s_L_s_index",('subject',)),
                        ("%s_L_p_index",('predicate',)),
                        ("%s_L_c_index",('context',)),
                    ],
                ),
                (
                    "%s_quoted_statements",
                    [
                        ("%s_Q_termComb_index",('termComb',)),
                        ("%s_Q_s_index",('subject',)),
                        ("%s_Q_p_index",('predicate',)),
                        ("%s_Q_o_index",('object',)),
                        ("%s_Q_c_index",('context',)),
                    ],
                ),
                (
                    "%s_namespace_binds",
                    [
                        ("%s_uri_index",('uri',)),
                    ],
                )]:
                for indexName,columns in indices:
                    c.execute("CREATE INDEX %s on %s (%s)"%(indexName%self._internedId,tblName%(self._internedId),','.join(columns)))
            c.close()
            db.commit()
            db.close()

        self._db = dbapi2.connect(os.path.join(home,self.identifier))
        self._db.create_function("regexp", 2, regexp)

        if os.path.exists(os.path.join(home,self.identifier)):
            c = self._db.cursor()
            c.execute("SELECT * FROM sqlite_master WHERE type='table'")
            tbls = [rt[1] for rt in c.fetchall()]
            c.close()
            for tn in [tbl%(self._internedId) for tbl in table_name_prefixes]:
                if tn not in tbls:
                    sys.stderr.write("table %s Doesn't exist\n" % (tn));
                    #The database exists, but one of the partitions doesn't exist
                    return 0
            #Everything is there (the database and the partitions)
            return 1
        #The database doesn't exist - nothing is there
        #return -1

    def destroy(self, home):
        """
        FIXME: Add documentation
        """
        db = dbapi2.connect(os.path.join(home,self.identifier))
        c=db.cursor()
        for tblsuffix in table_name_prefixes:
            try:
                c.execute('DROP table %s'%tblsuffix%(self._internedId))
            except:
                print "unable to drop table: %s"%(tblsuffix%(self._internedId))

        #Note, this only removes the associated tables for the closed world universe given by the identifier
        print "Destroyed Close World Universe %s ( in SQLite database %s)"%(self.identifier,home)
        db.commit()
        c.close()
        db.close()
        os.remove(os.path.join(home,self.identifier))

    def EscapeQuotes(self,qstr):
        """
        Ported from Ft.Lib.DbUtil
        """
        if qstr is None:
            return ''
        tmp = qstr.replace("\\","\\\\")
        tmp = tmp.replace('"', '""')
        tmp = tmp.replace("'", "\\'")
        return tmp

    #This is overridden to leave unicode terms as is
    #Instead of converting them to ascii (the default behavior)
    def normalizeTerm(self,term):
        if isinstance(term,(QuotedGraph,Graph)):
            return term.identifier
        elif isinstance(term,Literal):
            return self.EscapeQuotes(term)
        elif term is None or isinstance(term,(list,REGEXTerm)):
            return term
        else:
            return term

    #Where Clause  utility Functions
    #The predicate and object clause builders are modified in order to optimize
    #subjects and objects utility functions which can take lists as their last argument (object,predicate - respectively)
    def buildSubjClause(self,subject,tableName):
        if isinstance(subject,REGEXTerm):
            return " REGEXP (%s,"+" %s)"%(tableName and '%s.subject'%tableName or 'subject'),[subject]
        elif isinstance(subject,list):
            clauseStrings=[]
            paramStrings = []
            for s in subject:
                if isinstance(s,REGEXTerm):
                    clauseStrings.append(" REGEXP (%s,"+" %s)"%(tableName and '%s.subject'%tableName or 'subject') + " %s")
                    paramStrings.append(self.normalizeTerm(s))
                elif isinstance(s,(QuotedGraph,Graph)):
                    clauseStrings.append("%s="%(tableName and '%s.subject'%tableName or 'subject')+"%s")
                    paramStrings.append(self.normalizeTerm(s.identifier))
                else:
                    clauseStrings.append("%s="%(tableName and '%s.subject'%tableName or 'subject')+"%s")
                    paramStrings.append(self.normalizeTerm(s))
            return '('+ ' or '.join(clauseStrings) + ')', paramStrings
        elif isinstance(subject,(QuotedGraph,Graph)):
            return "%s="%(tableName and '%s.subject'%tableName or 'subject')+"%s",[self.normalizeTerm(subject.identifier)]
        else:
            return subject is not None and "%s="%(tableName and '%s.subject'%tableName or 'subject')+"%s",[subject] or None

    #Capable off taking a list of predicates as well (in which case sub clauses are joined with 'OR')
    def buildPredClause(self,predicate,tableName):
        if isinstance(predicate,REGEXTerm):
            return " REGEXP (%s,"+" %s)"%(tableName and '%s.predicate'%tableName or 'predicate'),[predicate]
        elif isinstance(predicate,list):
            clauseStrings=[]
            paramStrings = []
            for p in predicate:
                if isinstance(p,REGEXTerm):
                    clauseStrings.append(" REGEXP (%s,"+" %s)"%(tableName and '%s.predicate'%tableName or 'predicate'))
                else:
                    clauseStrings.append("%s="%(tableName and '%s.predicate'%tableName or 'predicate')+"%s")
                paramStrings.append(self.normalizeTerm(p))
            return '('+ ' or '.join(clauseStrings) + ')', paramStrings
        else:
            return predicate is not None and "%s="%(tableName and '%s.predicate'%tableName or 'predicate')+"%s",[predicate] or None

    #Capable of taking a list of objects as well (in which case sub clauses are joined with 'OR')
    def buildObjClause(self,obj,tableName):
        if isinstance(obj,REGEXTerm):
            return " REGEXP (%s,"+" %s)"%(tableName and '%s.object'%tableName or 'object'),[obj]
        elif isinstance(obj,list):
            clauseStrings=[]
            paramStrings = []
            for o in obj:
                if isinstance(o,REGEXTerm):
                    clauseStrings.append(" REGEXP (%s,"+" %s)"%(tableName and '%s.object'%tableName or 'object'))
                    paramStrings.append(self.normalizeTerm(o))
                elif isinstance(o,(QuotedGraph,Graph)):
                    clauseStrings.append("%s="%(tableName and '%s.object'%tableName or 'object')+"%s")
                    paramStrings.append(self.normalizeTerm(o.identifier))
                else:
                    clauseStrings.append("%s="%(tableName and '%s.object'%tableName or 'object')+"%s")
                    paramStrings.append(self.normalizeTerm(o))
            return '('+ ' or '.join(clauseStrings) + ')', paramStrings
        elif isinstance(obj,(QuotedGraph,Graph)):
            return "%s="%(tableName and '%s.object'%tableName or 'object')+"%s",[self.normalizeTerm(obj.identifier)]
        else:
            return obj is not None and "%s="%(tableName and '%s.object'%tableName or 'object')+"%s",[obj] or None

    def buildContextClause(self,context,tableName):
        context = context is not None and self.normalizeTerm(context.identifier) or context
        if isinstance(context,REGEXTerm):
            return " REGEXP (%s,"+" %s)"%(tableName and '%s.context'%tableName or 'context'),[context]
        else:
            return context is not None and "%s="%(tableName and '%s.context'%tableName or 'context')+"%s",[context] or None

    def buildTypeMemberClause(self,subject,tableName):
        if isinstance(subject,REGEXTerm):
            return " REGEXP (%s,"+" %s)"%(tableName and '%s.member'%tableName or 'member'),[subject]
        elif isinstance(subject,list):
            clauseStrings=[]
            paramStrings = []
            for s in subject:
                clauseStrings.append("%s.member="%tableName+"%s")
                if isinstance(s,(QuotedGraph,Graph)):
                    paramStrings.append(self.normalizeTerm(s.identifier))
                else:
                    paramStrings.append(self.normalizeTerm(s))
            return '('+ ' or '.join(clauseStrings) + ')', paramStrings
        else:
            return subject and u"%s.member = "%(tableName)+"%s",[subject]

    def buildTypeClassClause(self,obj,tableName):
        if isinstance(obj,REGEXTerm):
            return " REGEXP (%s,"+" %s)"%(tableName and '%s.klass'%tableName or 'klass'),[obj]
        elif isinstance(obj,list):
            clauseStrings=[]
            paramStrings = []
            for o in obj:
                clauseStrings.append("%s.klass="%tableName+"%s")
                if isinstance(o,(QuotedGraph,Graph)):
                    paramStrings.append(self.normalizeTerm(o.identifier))
                else:
                    paramStrings.append(self.normalizeTerm(o))
            return '('+ ' or '.join(clauseStrings) + ')', paramStrings
        else:
            return obj is not None and "%s.klass = "%tableName+"%s",[obj] or None

    def triples(self, (subject, predicate, obj), context=None):
        """
        A generator over all the triples matching pattern. Pattern can
        be any objects for comparing against nodes in the store, for
        example, RegExLiteral, Date? DateRange?

        quoted table:                <id>_quoted_statements
        asserted rdf:type table:     <id>_type_statements
        asserted non rdf:type table: <id>_asserted_statements

        triple columns: subject,predicate,object,context,termComb,objLanguage,objDatatype
        class membership columns: member,klass,context termComb

        FIXME:  These union all selects *may* be further optimized by joins

        """
        quoted_table="%s_quoted_statements"%self._internedId
        asserted_table="%s_asserted_statements"%self._internedId
        asserted_type_table="%s_type_statements"%self._internedId
        literal_table = "%s_literal_statements"%self._internedId
        c=self._db.cursor()

        parameters = []

        if predicate == RDF.type:
            #select from asserted rdf:type partition and quoted table (if a context is specified)
            clauseString,params = self.buildClause('typeTable',subject,RDF.type, obj,context,True)
            parameters.extend(params)
            selects = [
                (
                  asserted_type_table,
                  'typeTable',
                  clauseString,
                  ASSERTED_TYPE_PARTITION
                ),
            ]

        elif isinstance(predicate,REGEXTerm) and predicate.compiledExpr.match(RDF.type) or not predicate:
            #Select from quoted partition (if context is specified), literal partition if (obj is Literal or None) and asserted non rdf:type partition (if obj is URIRef or None)
            selects = []
            if not self.STRONGLY_TYPED_TERMS or isinstance(obj,Literal) or not obj or (self.STRONGLY_TYPED_TERMS and isinstance(obj,REGEXTerm)):
                clauseString,params = self.buildClause('literal',subject,predicate,obj,context)
                parameters.extend(params)
                selects.append((
                  literal_table,
                  'literal',
                  clauseString,
                  ASSERTED_LITERAL_PARTITION
                ))
            if not isinstance(obj,Literal) and not (isinstance(obj,REGEXTerm) and self.STRONGLY_TYPED_TERMS) or not obj:
                clauseString,params = self.buildClause('asserted',subject,predicate,obj,context)
                parameters.extend(params)
                selects.append((
                  asserted_table,
                  'asserted',
                  clauseString,
                  ASSERTED_NON_TYPE_PARTITION
                ))

            clauseString,params = self.buildClause('typeTable',subject,RDF.type,obj,context,True)
            parameters.extend(params)
            selects.append(
                (
                  asserted_type_table,
                  'typeTable',
                  clauseString,
                  ASSERTED_TYPE_PARTITION
                )
            )


        elif predicate:
            #select from asserted non rdf:type partition (optionally), quoted partition (if context is speciied), and literal partition (optionally)
            selects = []
            if not self.STRONGLY_TYPED_TERMS or isinstance(obj,Literal) or not obj or (self.STRONGLY_TYPED_TERMS and isinstance(obj,REGEXTerm)):
                clauseString,params = self.buildClause('literal',subject,predicate,obj,context)
                parameters.extend(params)
                selects.append((
                  literal_table,
                  'literal',
                  clauseString,
                  ASSERTED_LITERAL_PARTITION
                ))
            if not isinstance(obj,Literal) and not (isinstance(obj,REGEXTerm) and self.STRONGLY_TYPED_TERMS) or not obj:
                clauseString,params = self.buildClause('asserted',subject,predicate,obj,context)
                parameters.extend(params)
                selects.append((
                  asserted_table,
                  'asserted',
                  clauseString,
                  ASSERTED_NON_TYPE_PARTITION
                ))

        if context is not None:
            clauseString,params = self.buildClause('quoted',subject,predicate, obj,context)
            parameters.extend(params)
            selects.append(
                (
                  quoted_table,
                  'quoted',
                  clauseString,
                  QUOTED_PARTITION
                )
            )

        q=self._normalizeSQLCmd(unionSELECT(selects,selectType=TRIPLE_SELECT_NO_ORDER))
        self.executeSQL(c,q,parameters)
        #NOTE: SQLite does not support ORDER BY terms that aren't integers, so the entire result set must be iterated
        #in order to be able to return a generator of contexts
        tripleCoverage = {}
        result = c.fetchall()
        c.close()
        for rt in result:
            s,p,o,(graphKlass,idKlass,graphId) = extractTriple(rt,self,context)
            contexts = tripleCoverage.get((s,p,o),[])
            contexts.append(graphKlass(self,idKlass(graphId)))
            tripleCoverage[(s,p,o)] = contexts

        for (s,p,o),contexts in tripleCoverage.items():
            yield (s,p,o),(c for c in contexts)

CREATE_ASSERTED_STATEMENTS_TABLE = """
CREATE TABLE %s_asserted_statements (
    subject       text not NULL,
    predicate     text not NULL,
    object        text not NULL,
    context       text not NULL,
    termComb      tinyint unsigned not NULL)"""

CREATE_ASSERTED_TYPE_STATEMENTS_TABLE = """
CREATE TABLE %s_type_statements (
    member        text not NULL,
    klass         text not NULL,
    context       text not NULL,
    termComb      tinyint unsigned not NULL)"""

CREATE_LITERAL_STATEMENTS_TABLE = """
CREATE TABLE %s_literal_statements (
    subject       text not NULL,
    predicate     text not NULL,
    object        text,
    context       text not NULL,
    termComb      tinyint unsigned not NULL,
    objLanguage   varchar(3),
    objDatatype   text)"""

CREATE_QUOTED_STATEMENTS_TABLE = """
CREATE TABLE %s_quoted_statements (
    subject       text not NULL,
    predicate     text not NULL,
    object        text,
    context       text not NULL,
    termComb      tinyint unsigned not NULL,
    objLanguage   varchar(3),
    objDatatype   text)"""

CREATE_NS_BINDS_TABLE = """
CREATE TABLE %s_namespace_binds (
    prefix        varchar(20) UNIQUE not NULL,
    uri           text,
    PRIMARY KEY (prefix))"""

########NEW FILE########
__FILENAME__ = ZODB
# Author: Michel Pelletier

Any = None

from rdflib.store.IOMemory import IOMemory

# you must export your PYTHONPATH to point to a Z2.8 or Z3+ installation to get this to work!, like:
#export PYTHONPATH="/home/michel/dev/Zope3Trunk/src"

try:
    # Zope 3
    from persistent import Persistent
except ImportError:
    # < Zope 2.8?
    from Persistence import Persistent

from BTrees.IOBTree import IOBTree
from BTrees.OIBTree import OIBTree
from BTrees.OOBTree import OOBTree

class ZODB(Persistent, IOMemory):

    def createForward(self):
        return IOBTree()

    def createReverse(self):
        return OIBTree()

    def createIndex(self):
        return IOBTree()

    def createPrefixMap(self):
        return OOBTree()

########NEW FILE########
__FILENAME__ = _sqlobject
from __future__ import generators

__metaclass__ = type

import logging
_logger = logging.getLogger("rdflib.store._sqlobject")

import re
_literal = re.compile(r'''"(?P<value>[^@&]*)"(?:@(?P<lang>[^&]*))?(?:&<(?P<datatype>.*)>)?''')

from urllib import quote, unquote

from rdflib.store import Store
from rdflib.Literal import Literal
from rdflib.URIRef import URIRef
from rdflib.BNode import BNode
from rdflib.exceptions import ContextTypeError

from rdflib.compat import rsplit

import sqlobject
from sqlobject import *

LITERAL = 0
URI = 1
NO_URI = 'uri://oops/'
Any = None

class BaseObject(sqlobject.SQLObject):

    _lazyUpdate = True
    _cacheValues = False

class Literals(BaseObject):

    hash = IntCol(notNull=1)
    value = StringCol(notNull=1, validator=validators.String(strip_spaces=1))
    hashIndex = DatabaseIndex('hash')

class Namespaces(BaseObject):

    hash = IntCol(notNull=1)
    value = StringCol(length=255, notNull=1,
                      validator=validators.String(strip_spaces=1))
    hashIndex = DatabaseIndex('hash')

class PrefixNamespace(BaseObject):

    prefix = StringCol(length=255, notNull=1,
                       validator=validators.String(strip_spaces=1))
    ns = StringCol(length=255, notNull=1,
                   validator=validators.String(strip_spaces=1))
    prefixIndex = DatabaseIndex('prefix')
    nsIndex = DatabaseIndex('ns')
    prefixNsIndex = DatabaseIndex('ns', 'prefix')

class Resources(BaseObject):

    hash = IntCol(notNull=1)
    ns = IntCol(notNull=1)
    name = StringCol(length=255, notNull=1,
                     validator=validators.String(strip_spaces=1))
    hashIndex = DatabaseIndex('hash')
    nsIndex = DatabaseIndex('ns')
    nameIndex = DatabaseIndex('name')
    nsNameIndex = DatabaseIndex('ns', 'name')
    hashNsNameIndex = DatabaseIndex('hash', 'ns', 'name')

class Triples(BaseObject):

    subject = IntCol(notNull=1)
    predicate = IntCol(notNull=1)
    object = IntCol(notNull=1)
    objtype = IntCol(notNull=1, default=LITERAL)

    subjectIndex = DatabaseIndex('subject')
    predicateIndex = DatabaseIndex('predicate')
    objectIndex = DatabaseIndex('object', 'objtype')
    subjectPredicateIndex = DatabaseIndex('subject', 'predicate')
    subjectObjectIndex = DatabaseIndex('subject', 'object', 'objtype')
    predicateObjectIndex = DatabaseIndex('predicate', 'object', 'objtype')

def splituri(uri):
    if uri.startswith('<') and uri.endswith('>'):
        uri = uri[1:-1]
    if uri.startswith('_'):
        uid = ''.join(uri.split('_'))
        return '_', uid
    if '#' in uri:
        ns, local = rsplit(uri, '#', 1)
        return ns + '#', local
    if '/' in uri:
        ns, local = rsplit(uri, '/', 1)
        return ns + '/', local
    return NO_URI, uri

def _fromkey(key):
    if key.startswith("<") and key.endswith(">"):
        key = key[1:-1].decode("UTF-8")
        if key.startswith("_"):
            key = ''.join(splituri(key))
            return BNode(key)
        return URIRef(key)
    elif key.startswith("_"):
        return BNode(key)
    else:
        m = _literal.match(key)
        if m:
            d = m.groupdict()
            value = d["value"]
            value = unquote(value)
            value = value.decode("UTF-8")
            lang = d["lang"] or ''
            datatype = d["datatype"]
            return Literal(value, lang, datatype)
        else:
            msg = "Unknown Key Syntax: '%s'" % key
            raise Exception(msg)

def _tokey(term):
    if isinstance(term, URIRef):
        term = term.encode("UTF-8")
        if not '#' in term and not '/' in term:
            term = '%s%s' % (NO_URI, term)
        return '<%s>' % term
    elif isinstance(term, BNode):
        return '<%s>' % ''.join(splituri(term.encode("UTF-8")))
    elif isinstance(term, Literal):
        language = term.language
        datatype = term.datatype
        value = quote(term.encode("UTF-8"))
        if language:
            language = language.encode("UTF-8")
            if datatype:
                datatype = datatype.encode("UTF-8")
                n3 = '"%s"@%s&<%s>' % (value, language, datatype)
            else:
                n3 = '"%s"@%s' % (value, language)
        else:
            if datatype:
                datatype = datatype.encode("UTF-8")
                n3 = '"%s"&<%s>' % (value, datatype)
            else:
                n3 = '"%s"' % value
        return n3
    else:
        msg = "Unknown term Type for: %s" % term
        raise Exception(msg)

class SQLObject(Store):

    context_aware = False
    __open = False
    _triples = Triples
    _literals = Literals
    _ns = Namespaces
    _prefix_ns = PrefixNamespace
    _resources = Resources
    tables = ('_triples', '_literals', '_ns',
              '_prefix_ns', '_resources')

    def __init__(self):
        pass

    def open(self, uri, create=True):
        if self.__open:
            return
        self.__open = True
        self.connection = connection = connectionForURI(uri)
        # useful for debugging
        # self.connection.debug = True
        for att in self.tables:
            table = getattr(self, att)
            table._connection = connection
            try:
                table.createTable(ifNotExists=create)
            except Exception, e: # TODO: should catch more specific exception
                _logger.warning(e)
                return 0

        self.transaction = transaction = connection.transaction()
        for att in self.tables:
            table = getattr(self, att)
            table._connection = transaction
        return 1

    def close(self):
        if not self.__open:
            raise ValueError, 'Not open'
        self.__open = False
        self.transaction.commit()

    def _makeHash(self, value):
        # XXX We will be using python's hash, but it should be a database
        # hash eventually.
        return hash(value)

    def _insertLiteral(self, value):
        v_hash = self._makeHash(value)
        lit = self._literals
        if not lit.select(lit.q.hash == v_hash).count():
            lit(hash=v_hash, value=value)
        return v_hash

    def _makeURIHash(self, value=None, namespace=None, local_name=None):
        if namespace is None and local_name is None:
            namespace, local_name = splituri(value)
        ns_hash = self._makeHash(namespace)
        rsrc_hash = self._makeHash((ns_hash, local_name))
        return ns_hash, rsrc_hash

    def _insertURI(self, value=None, namespace=None, local_name=None):
        if namespace is None and local_name is None:
            namespace, local_name = splituri(value)
        ns_hash, rsrc_hash = self._makeURIHash(value, namespace, local_name)
        ns = self._ns
        if not ns.select(ns.q.hash == ns_hash).count():
            ns(hash=ns_hash, value=namespace)
        rsrc = self._resources
        if not rsrc.select(rsrc.q.hash == rsrc_hash).count():
            rsrc(hash=rsrc_hash, ns=ns_hash, name=local_name)
        return rsrc_hash

    def _insertTriple(self, s_hash, p_hash, o_hash, objtype=URI):
        trip = self._triples
        clause = AND(trip.q.subject == s_hash,
                     trip.q.predicate == p_hash,
                     trip.q.object == o_hash)
        if not trip.select(clause).count():
            trip(subject=s_hash, predicate=p_hash,
                 object=o_hash, objtype=objtype)

    def tokey(self, obj):
        if isinstance(obj, (URIRef, BNode)):
            return URI, self._makeURIHash(_tokey(obj))[1]
        elif isinstance(obj, Literal):
            return LITERAL, self._makeHash(_tokey(obj))
        elif obj is Any:
            return None, Any
        raise ValueError, obj

    def insert(self, obj):
        if isinstance(obj, (URIRef, BNode)):
            return URI, self._insertURI(_tokey(obj))
        elif isinstance(obj, Literal):
            return LITERAL, self._insertLiteral(_tokey(obj))
        raise ValueError, obj

    def add(self, (subject, predicate, object), context=None):
        """\
        Add a triple to the store of triples.
        """
        tokey = self.insert
        ts, s = tokey(subject)
        tp, p = tokey(predicate)
        to, o = tokey(object)

        self._insertTriple(s, p, o, to)

    def remove(self, (subject, predicate, object), context=None):
        tokey = self.tokey
        where_clause = ''

        if subject is not Any:
            ts, s = tokey(subject)
            where_clause += 'subject = %s' % s
        if predicate is not Any:
            if where_clause:
                where_clause += ' AND '
            tp, p = tokey(predicate)
            where_clause += 'predicate = %s' % p
        if object is not Any:
            if where_clause:
                where_clause += ' AND '
            to, o = tokey(object)
            where_clause += 'object = %s AND objtype = %s' % (o, to)

        trip = self._triples
        conn = trip._connection
        query = 'DELETE from %s' % conn.sqlrepr(trip.q)
        if where_clause:
            query += ' WHERE %s' % where_clause
        conn.query(query)

    def triples(self, (subject, predicate, object), context=None):
        conn = self._triples._connection

        tokey = self.tokey
        where_clause = ''
        if subject is not Any:
            ts, s = tokey(subject)
            where_clause += 'r1.hash = %s' % s
        if predicate is not Any:
            if where_clause:
                where_clause += ' AND '
            tp, p = tokey(predicate)
            where_clause += 'r2.hash = %s' % p
        if object is not Any:
            if where_clause:
                where_clause += ' AND '
            to, o = tokey(object)
            if to == URI:
                where_clause += 'r3.hash = %s' % o
            else:
                where_clause += 'l.hash = %s' % o

        query = ("SELECT '<'||n1.value||r1.name||'>' AS subj, "
                 "'<'||n2.value||r2.name||'>' AS pred, "
                 "CASE WHEN t.objtype = %d "
                 "THEN '<'||n3.value||r3.name||'>' "
                 "ELSE l.value END AS obj, "
                 "l.hash, r3.hash "
                 "FROM resources r1, resources r2, "
                 "namespaces n1, namespaces n2, triples t "
                 "LEFT JOIN literals l ON t.object = l.hash "
                 "LEFT JOIN resources r3 ON t.object = r3.hash "
                 "LEFT JOIN namespaces n3 ON r3.ns = n3.hash "
                 "WHERE t.subject = r1.hash AND "
                 "r1.ns = n1.hash AND "
                 "t.predicate = r2.hash AND "
                 "r2.ns = n2.hash" % URI)
        if where_clause:
            query += ' AND %s' % where_clause
        query += ' ORDER BY subj, pred'
        for t in conn.queryAll(query):
            triple = _fromkey(t[0]), _fromkey(t[1]), _fromkey(t[2])
            yield triple

    def namespace(self, prefix):
        prefix = prefix.encode("utf-8")
        pns = self._prefix_ns
        res = pns.select(pns.q.prefix == prefix)
        if not res.count():
            return None
        return iter(res).next().ns

    def prefix(self, namespace):
        namespace = namespace.encode("utf-8")
        pns = self._prefix_ns
        res = pns.select(pns.q.ns == namespace)
        if not res.count():
            return None
        return iter(res).next().prefix

    def bind(self, prefix, namespace):
        if namespace[-1] == "-":
            raise Exception("??")
        pns = self._prefix_ns
        prefix = prefix.encode("utf-8")
        namespace = namespace.encode("utf-8")
        res = pns.select(AND(pns.q.ns == namespace,
                             pns.q.prefix == prefix))
        if not res.count():
            pns(prefix=prefix, ns=namespace)

    def namespaces(self):
        pns = self._prefix_ns
        for p in pns.select():
            yield p.prefix, URIRef(p.ns)

    def __len__(self):
        return self._triples.select().count()

########NEW FILE########
__FILENAME__ = StringInputSource
from urllib2 import urlopen, Request

from xml.sax.xmlreader import InputSource

from rdflib import __version__
from StringIO import StringIO

class StringInputSource(InputSource, object):
    def __init__(self, value, system_id=None):
        super(StringInputSource, self).__init__(system_id)
        stream = StringIO(value)
        self.setByteStream(stream)
        # TODO:
        #   encoding = value.encoding
        #   self.setEncoding(encoding)

########NEW FILE########
__FILENAME__ = NamespaceManager
from __future__ import generators

from rdflib import URIRef, Literal, RDFS, Variable
from rdflib.syntax.xml_names import split_uri

from urlparse import urljoin, urldefrag
from urllib import pathname2url, url2pathname
import os, sys, new


class NamespaceManager(object):
    def __init__(self, graph):
        self.graph = graph
        self.__cache = {}
        self.__log = None
        self.bind("xml", u"http://www.w3.org/XML/1998/namespace")
        self.bind("rdf", "http://www.w3.org/1999/02/22-rdf-syntax-ns#")
        self.bind("rdfs", "http://www.w3.org/2000/01/rdf-schema#")

    def reset(self):
        self.__cache = {}

    def __get_store(self):
        return self.graph.store
    store = property(__get_store)

    def qname(self, uri):
        prefix, namespace, name = self.compute_qname(uri)
        if prefix=="":
            return name
        else:
            return ":".join((prefix, name))

    def normalizeUri(self,rdfTerm):
        """
        Takes an RDF Term and 'normalizes' it into a QName (using the registered prefix)
        or (unlike compute_qname) the Notation 3 form for URIs: <...URI...> 
        """
        try:
            namespace, name = split_uri(rdfTerm)
            namespace = URIRef(namespace)
        except:
            if isinstance(rdfTerm,Variable):
                return "?%s"%rdfTerm
            else:
                return "<%s>"%rdfTerm
        prefix = self.store.prefix(namespace)
        if prefix is None and isinstance(rdfTerm,Variable):
            return "?%s"%rdfTerm
        elif prefix is None:
            return "<%s>"%rdfTerm
        else:
            qNameParts = self.compute_qname(rdfTerm)         
            return ':'.join([qNameParts[0],qNameParts[-1]])    

    def compute_qname(self, uri):
        if not uri in self.__cache:
            namespace, name = split_uri(uri)
            namespace = URIRef(namespace)
            prefix = self.store.prefix(namespace)
            if prefix is None:
                prefix = "_%s" % len(list(self.store.namespaces()))
                self.bind(prefix, namespace)
            self.__cache[uri] = (prefix, namespace, name)
        return self.__cache[uri]

    def bind(self, prefix, namespace, override=True):
        namespace = URIRef(namespace)
        # When documenting explain that override only applies in what cases
        if prefix is None:
            prefix = ''
        bound_namespace = self.store.namespace(prefix)
        if bound_namespace and bound_namespace!=namespace:
            # prefix already in use for different namespace
            #
            # append number to end of prefix until we find one
            # that's not in use.
            if not prefix:
                prefix = "default"
            num = 1
            while 1:
                new_prefix = "%s%s" % (prefix, num)
                if not self.store.namespace(new_prefix):
                    break
                num +=1
            self.store.bind(new_prefix, namespace)
        else:
            bound_prefix = self.store.prefix(namespace)
            if bound_prefix is None:
                self.store.bind(prefix, namespace)
            elif bound_prefix == prefix:
                pass # already bound
            else:
                if override or bound_prefix.startswith("_"): # or a generated prefix
                    self.store.bind(prefix, namespace)

    def namespaces(self):
        for prefix, namespace in self.store.namespaces():
            namespace = URIRef(namespace)
            yield prefix, namespace

    def absolutize(self, uri, defrag=1):
        base = urljoin("file:", pathname2url(os.getcwd()))
        result = urljoin("%s/" % base, uri, allow_fragments=not defrag)
        if defrag:
            result = urldefrag(result)[0]
        if not defrag:
            if uri and uri[-1]=="#" and result[-1]!="#":
                result = "%s#" % result
        return URIRef(result)

########NEW FILE########
__FILENAME__ = n3meta
#!/usr/bin/env python
"""n3meta - For use with n3p.py."""
# Automatically generated by pkltopy.py

import re

branches = {u'_:jcOJHCYs16': {u',': [u',',
                          'http://www.w3.org/2000/10/swap/grammar/n3#symbol',
                          '_:jcOJHCYs16'],
                   u'.': [],
                   u'}': []},
 u'_:jcOJHCYs20': {u',': [u',',
                          'http://www.w3.org/2000/10/swap/grammar/n3#symbol',
                          '_:jcOJHCYs20'],
                   u'.': [],
                   u'}': []},
 u'_:jcOJHCYs33': {u'.': [],
                   u':': ['http://www.w3.org/2000/10/swap/grammar/n3#symbol',
                          '_:jcOJHCYs16'],
                   u'<': ['http://www.w3.org/2000/10/swap/grammar/n3#symbol',
                          '_:jcOJHCYs16'],
                   u'_': ['http://www.w3.org/2000/10/swap/grammar/n3#symbol',
                          '_:jcOJHCYs16'],
                   u'a': ['http://www.w3.org/2000/10/swap/grammar/n3#symbol',
                          '_:jcOJHCYs16'],
                   u'}': []},
 u'_:jcOJHCYs36': {u'.': [],
                   u':': ['http://www.w3.org/2000/10/swap/grammar/n3#symbol',
                          '_:jcOJHCYs20'],
                   u'<': ['http://www.w3.org/2000/10/swap/grammar/n3#symbol',
                          '_:jcOJHCYs20'],
                   u'_': ['http://www.w3.org/2000/10/swap/grammar/n3#symbol',
                          '_:jcOJHCYs20'],
                   u'a': ['http://www.w3.org/2000/10/swap/grammar/n3#symbol',
                          '_:jcOJHCYs20'],
                   u'}': []},
 u'_:jcOJHCYs44': {u'.': [],
                   u'_': ['http://www.w3.org/2000/10/swap/grammar/n3#barename',
                          '_:jcOJHCYs9'],
                   u'a': ['http://www.w3.org/2000/10/swap/grammar/n3#barename',
                          '_:jcOJHCYs9'],
                   u'}': []},
 u'_:jcOJHCYs9': {u',': [u',',
                         'http://www.w3.org/2000/10/swap/grammar/n3#barename',
                         '_:jcOJHCYs9'],
                  u'.': [],
                  u'}': []},
 u'http://www.w3.org/2000/10/swap/grammar/n3#declaration': {u'@keywords': [u'@keywords',
                                                                           '_:jcOJHCYs44'],
                                                            u'@prefix': [u'@prefix',
                                                                         'http://www.w3.org/2000/10/swap/grammar/n3#qname',
                                                                         'http://www.w3.org/2000/10/swap/grammar/n3#explicituri']},
 u'http://www.w3.org/2000/10/swap/grammar/n3#document': {u'"': ['http://www.w3.org/2000/10/swap/grammar/n3#statements_optional',
                                                                'http://www.w3.org/2000/10/swap/grammar/bnf#eof'],
                                                         u'(': ['http://www.w3.org/2000/10/swap/grammar/n3#statements_optional',
                                                                'http://www.w3.org/2000/10/swap/grammar/bnf#eof'],
                                                         u'+': ['http://www.w3.org/2000/10/swap/grammar/n3#statements_optional',
                                                                'http://www.w3.org/2000/10/swap/grammar/bnf#eof'],
                                                         u'-': ['http://www.w3.org/2000/10/swap/grammar/n3#statements_optional',
                                                                'http://www.w3.org/2000/10/swap/grammar/bnf#eof'],
                                                         u'0': ['http://www.w3.org/2000/10/swap/grammar/n3#statements_optional',
                                                                'http://www.w3.org/2000/10/swap/grammar/bnf#eof'],
                                                         u':': ['http://www.w3.org/2000/10/swap/grammar/n3#statements_optional',
                                                                'http://www.w3.org/2000/10/swap/grammar/bnf#eof'],
                                                         u'<': ['http://www.w3.org/2000/10/swap/grammar/n3#statements_optional',
                                                                'http://www.w3.org/2000/10/swap/grammar/bnf#eof'],
                                                         u'?': ['http://www.w3.org/2000/10/swap/grammar/n3#statements_optional',
                                                                'http://www.w3.org/2000/10/swap/grammar/bnf#eof'],
                                                         u'@EOFDUMMY': ['http://www.w3.org/2000/10/swap/grammar/n3#statements_optional',
                                                                        'http://www.w3.org/2000/10/swap/grammar/bnf#eof'],
                                                         u'@forAll': ['http://www.w3.org/2000/10/swap/grammar/n3#statements_optional',
                                                                      'http://www.w3.org/2000/10/swap/grammar/bnf#eof'],
                                                         u'@forSome': ['http://www.w3.org/2000/10/swap/grammar/n3#statements_optional',
                                                                       'http://www.w3.org/2000/10/swap/grammar/bnf#eof'],
                                                         u'@keywords': ['http://www.w3.org/2000/10/swap/grammar/n3#statements_optional',
                                                                        'http://www.w3.org/2000/10/swap/grammar/bnf#eof'],
                                                         u'@prefix': ['http://www.w3.org/2000/10/swap/grammar/n3#statements_optional',
                                                                      'http://www.w3.org/2000/10/swap/grammar/bnf#eof'],
                                                         u'@this': ['http://www.w3.org/2000/10/swap/grammar/n3#statements_optional',
                                                                    'http://www.w3.org/2000/10/swap/grammar/bnf#eof'],
                                                         u'[': ['http://www.w3.org/2000/10/swap/grammar/n3#statements_optional',
                                                                'http://www.w3.org/2000/10/swap/grammar/bnf#eof'],
                                                         u'_': ['http://www.w3.org/2000/10/swap/grammar/n3#statements_optional',
                                                                'http://www.w3.org/2000/10/swap/grammar/bnf#eof'],
                                                         u'a': ['http://www.w3.org/2000/10/swap/grammar/n3#statements_optional',
                                                                'http://www.w3.org/2000/10/swap/grammar/bnf#eof'],
                                                         u'{': ['http://www.w3.org/2000/10/swap/grammar/n3#statements_optional',
                                                                'http://www.w3.org/2000/10/swap/grammar/bnf#eof']},
 u'http://www.w3.org/2000/10/swap/grammar/n3#dtlang': {u'!': [],
                                                       u'"': [],
                                                       u'(': [],
                                                       u')': [],
                                                       u'+': [],
                                                       u',': [],
                                                       u'-': [],
                                                       u'.': [],
                                                       u'0': [],
                                                       u':': [],
                                                       u';': [],
                                                       u'<': [],
                                                       u'<=': [],
                                                       u'=': [],
                                                       u'=>': [],
                                                       u'?': [],
                                                       u'@': [u'@',
                                                              'http://www.w3.org/2000/10/swap/grammar/n3#langcode'],
                                                       u'@a': [],
                                                       u'@has': [],
                                                       u'@is': [],
                                                       u'@of': [],
                                                       u'@this': [],
                                                       u'[': [],
                                                       u']': [],
                                                       u'^': [],
                                                       u'^^': [u'^^',
                                                               'http://www.w3.org/2000/10/swap/grammar/n3#symbol'],
                                                       u'_': [],
                                                       u'a': [],
                                                       u'{': [],
                                                       u'}': []},
 u'http://www.w3.org/2000/10/swap/grammar/n3#existential': {u'@forSome': [u'@forSome',
                                                                          '_:jcOJHCYs36']},
 u'http://www.w3.org/2000/10/swap/grammar/n3#formulacontent': {u'"': ['http://www.w3.org/2000/10/swap/grammar/n3#statementlist'],
                                                               u'(': ['http://www.w3.org/2000/10/swap/grammar/n3#statementlist'],
                                                               u'+': ['http://www.w3.org/2000/10/swap/grammar/n3#statementlist'],
                                                               u'-': ['http://www.w3.org/2000/10/swap/grammar/n3#statementlist'],
                                                               u'0': ['http://www.w3.org/2000/10/swap/grammar/n3#statementlist'],
                                                               u':': ['http://www.w3.org/2000/10/swap/grammar/n3#statementlist'],
                                                               u'<': ['http://www.w3.org/2000/10/swap/grammar/n3#statementlist'],
                                                               u'?': ['http://www.w3.org/2000/10/swap/grammar/n3#statementlist'],
                                                               u'@forAll': ['http://www.w3.org/2000/10/swap/grammar/n3#statementlist'],
                                                               u'@forSome': ['http://www.w3.org/2000/10/swap/grammar/n3#statementlist'],
                                                               u'@keywords': ['http://www.w3.org/2000/10/swap/grammar/n3#statementlist'],
                                                               u'@prefix': ['http://www.w3.org/2000/10/swap/grammar/n3#statementlist'],
                                                               u'@this': ['http://www.w3.org/2000/10/swap/grammar/n3#statementlist'],
                                                               u'[': ['http://www.w3.org/2000/10/swap/grammar/n3#statementlist'],
                                                               u'_': ['http://www.w3.org/2000/10/swap/grammar/n3#statementlist'],
                                                               u'a': ['http://www.w3.org/2000/10/swap/grammar/n3#statementlist'],
                                                               u'{': ['http://www.w3.org/2000/10/swap/grammar/n3#statementlist'],
                                                               u'}': []},
 u'http://www.w3.org/2000/10/swap/grammar/n3#literal': {u'"': ['http://www.w3.org/2000/10/swap/grammar/n3#string',
                                                               'http://www.w3.org/2000/10/swap/grammar/n3#dtlang']},
 u'http://www.w3.org/2000/10/swap/grammar/n3#node': {u'"': ['http://www.w3.org/2000/10/swap/grammar/n3#literal'],
                                                     u'(': [u'(',
                                                            'http://www.w3.org/2000/10/swap/grammar/n3#pathlist',
                                                            u')'],
                                                     u'+': ['http://www.w3.org/2000/10/swap/grammar/n3#numericliteral'],
                                                     u'-': ['http://www.w3.org/2000/10/swap/grammar/n3#numericliteral'],
                                                     u'0': ['http://www.w3.org/2000/10/swap/grammar/n3#numericliteral'],
                                                     u':': ['http://www.w3.org/2000/10/swap/grammar/n3#symbol'],
                                                     u'<': ['http://www.w3.org/2000/10/swap/grammar/n3#symbol'],
                                                     u'?': ['http://www.w3.org/2000/10/swap/grammar/n3#variable'],
                                                     u'@this': [u'@this'],
                                                     u'[': [u'[',
                                                            'http://www.w3.org/2000/10/swap/grammar/n3#propertylist',
                                                            u']'],
                                                     u'_': ['http://www.w3.org/2000/10/swap/grammar/n3#symbol'],
                                                     u'a': ['http://www.w3.org/2000/10/swap/grammar/n3#symbol'],
                                                     u'{': [u'{',
                                                            'http://www.w3.org/2000/10/swap/grammar/n3#formulacontent',
                                                            u'}']},
 u'http://www.w3.org/2000/10/swap/grammar/n3#object': {u'"': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                       u'(': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                       u'+': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                       u'-': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                       u'0': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                       u':': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                       u'<': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                       u'?': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                       u'@this': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                       u'[': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                       u'_': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                       u'a': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                       u'{': ['http://www.w3.org/2000/10/swap/grammar/n3#path']},
 u'http://www.w3.org/2000/10/swap/grammar/n3#objecttail': {u',': [u',',
                                                                  'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                  'http://www.w3.org/2000/10/swap/grammar/n3#objecttail'],
                                                           u'.': [],
                                                           u';': [],
                                                           u']': [],
                                                           u'}': []},
 u'http://www.w3.org/2000/10/swap/grammar/n3#path': {u'"': ['http://www.w3.org/2000/10/swap/grammar/n3#node',
                                                            'http://www.w3.org/2000/10/swap/grammar/n3#pathtail'],
                                                     u'(': ['http://www.w3.org/2000/10/swap/grammar/n3#node',
                                                            'http://www.w3.org/2000/10/swap/grammar/n3#pathtail'],
                                                     u'+': ['http://www.w3.org/2000/10/swap/grammar/n3#node',
                                                            'http://www.w3.org/2000/10/swap/grammar/n3#pathtail'],
                                                     u'-': ['http://www.w3.org/2000/10/swap/grammar/n3#node',
                                                            'http://www.w3.org/2000/10/swap/grammar/n3#pathtail'],
                                                     u'0': ['http://www.w3.org/2000/10/swap/grammar/n3#node',
                                                            'http://www.w3.org/2000/10/swap/grammar/n3#pathtail'],
                                                     u':': ['http://www.w3.org/2000/10/swap/grammar/n3#node',
                                                            'http://www.w3.org/2000/10/swap/grammar/n3#pathtail'],
                                                     u'<': ['http://www.w3.org/2000/10/swap/grammar/n3#node',
                                                            'http://www.w3.org/2000/10/swap/grammar/n3#pathtail'],
                                                     u'?': ['http://www.w3.org/2000/10/swap/grammar/n3#node',
                                                            'http://www.w3.org/2000/10/swap/grammar/n3#pathtail'],
                                                     u'@this': ['http://www.w3.org/2000/10/swap/grammar/n3#node',
                                                                'http://www.w3.org/2000/10/swap/grammar/n3#pathtail'],
                                                     u'[': ['http://www.w3.org/2000/10/swap/grammar/n3#node',
                                                            'http://www.w3.org/2000/10/swap/grammar/n3#pathtail'],
                                                     u'_': ['http://www.w3.org/2000/10/swap/grammar/n3#node',
                                                            'http://www.w3.org/2000/10/swap/grammar/n3#pathtail'],
                                                     u'a': ['http://www.w3.org/2000/10/swap/grammar/n3#node',
                                                            'http://www.w3.org/2000/10/swap/grammar/n3#pathtail'],
                                                     u'{': ['http://www.w3.org/2000/10/swap/grammar/n3#node',
                                                            'http://www.w3.org/2000/10/swap/grammar/n3#pathtail']},
 u'http://www.w3.org/2000/10/swap/grammar/n3#pathlist': {u'"': ['http://www.w3.org/2000/10/swap/grammar/n3#path',
                                                                'http://www.w3.org/2000/10/swap/grammar/n3#pathlist'],
                                                         u'(': ['http://www.w3.org/2000/10/swap/grammar/n3#path',
                                                                'http://www.w3.org/2000/10/swap/grammar/n3#pathlist'],
                                                         u')': [],
                                                         u'+': ['http://www.w3.org/2000/10/swap/grammar/n3#path',
                                                                'http://www.w3.org/2000/10/swap/grammar/n3#pathlist'],
                                                         u'-': ['http://www.w3.org/2000/10/swap/grammar/n3#path',
                                                                'http://www.w3.org/2000/10/swap/grammar/n3#pathlist'],
                                                         u'0': ['http://www.w3.org/2000/10/swap/grammar/n3#path',
                                                                'http://www.w3.org/2000/10/swap/grammar/n3#pathlist'],
                                                         u':': ['http://www.w3.org/2000/10/swap/grammar/n3#path',
                                                                'http://www.w3.org/2000/10/swap/grammar/n3#pathlist'],
                                                         u'<': ['http://www.w3.org/2000/10/swap/grammar/n3#path',
                                                                'http://www.w3.org/2000/10/swap/grammar/n3#pathlist'],
                                                         u'?': ['http://www.w3.org/2000/10/swap/grammar/n3#path',
                                                                'http://www.w3.org/2000/10/swap/grammar/n3#pathlist'],
                                                         u'@this': ['http://www.w3.org/2000/10/swap/grammar/n3#path',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#pathlist'],
                                                         u'[': ['http://www.w3.org/2000/10/swap/grammar/n3#path',
                                                                'http://www.w3.org/2000/10/swap/grammar/n3#pathlist'],
                                                         u'_': ['http://www.w3.org/2000/10/swap/grammar/n3#path',
                                                                'http://www.w3.org/2000/10/swap/grammar/n3#pathlist'],
                                                         u'a': ['http://www.w3.org/2000/10/swap/grammar/n3#path',
                                                                'http://www.w3.org/2000/10/swap/grammar/n3#pathlist'],
                                                         u'{': ['http://www.w3.org/2000/10/swap/grammar/n3#path',
                                                                'http://www.w3.org/2000/10/swap/grammar/n3#pathlist']},
 u'http://www.w3.org/2000/10/swap/grammar/n3#pathtail': {u'!': [u'!',
                                                                'http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                         u'"': [],
                                                         u'(': [],
                                                         u')': [],
                                                         u'+': [],
                                                         u',': [],
                                                         u'-': [],
                                                         u'.': [],
                                                         u'0': [],
                                                         u':': [],
                                                         u';': [],
                                                         u'<': [],
                                                         u'<=': [],
                                                         u'=': [],
                                                         u'=>': [],
                                                         u'?': [],
                                                         u'@a': [],
                                                         u'@has': [],
                                                         u'@is': [],
                                                         u'@of': [],
                                                         u'@this': [],
                                                         u'[': [],
                                                         u']': [],
                                                         u'^': [u'^',
                                                                'http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                         u'_': [],
                                                         u'a': [],
                                                         u'{': [],
                                                         u'}': []},
 u'http://www.w3.org/2000/10/swap/grammar/n3#propertylist': {u'"': ['http://www.w3.org/2000/10/swap/grammar/n3#verb',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#objecttail',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail'],
                                                             u'(': ['http://www.w3.org/2000/10/swap/grammar/n3#verb',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#objecttail',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail'],
                                                             u'+': ['http://www.w3.org/2000/10/swap/grammar/n3#verb',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#objecttail',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail'],
                                                             u'-': ['http://www.w3.org/2000/10/swap/grammar/n3#verb',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#objecttail',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail'],
                                                             u'.': [],
                                                             u'0': ['http://www.w3.org/2000/10/swap/grammar/n3#verb',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#objecttail',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail'],
                                                             u':': ['http://www.w3.org/2000/10/swap/grammar/n3#verb',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#objecttail',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail'],
                                                             u'<': ['http://www.w3.org/2000/10/swap/grammar/n3#verb',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#objecttail',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail'],
                                                             u'<=': ['http://www.w3.org/2000/10/swap/grammar/n3#verb',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#objecttail',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail'],
                                                             u'=': ['http://www.w3.org/2000/10/swap/grammar/n3#verb',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#objecttail',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail'],
                                                             u'=>': ['http://www.w3.org/2000/10/swap/grammar/n3#verb',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#objecttail',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail'],
                                                             u'?': ['http://www.w3.org/2000/10/swap/grammar/n3#verb',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#objecttail',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail'],
                                                             u'@a': ['http://www.w3.org/2000/10/swap/grammar/n3#verb',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#objecttail',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail'],
                                                             u'@has': ['http://www.w3.org/2000/10/swap/grammar/n3#verb',
                                                                       'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                       'http://www.w3.org/2000/10/swap/grammar/n3#objecttail',
                                                                       'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail'],
                                                             u'@is': ['http://www.w3.org/2000/10/swap/grammar/n3#verb',
                                                                      'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                      'http://www.w3.org/2000/10/swap/grammar/n3#objecttail',
                                                                      'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail'],
                                                             u'@this': ['http://www.w3.org/2000/10/swap/grammar/n3#verb',
                                                                        'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                        'http://www.w3.org/2000/10/swap/grammar/n3#objecttail',
                                                                        'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail'],
                                                             u'[': ['http://www.w3.org/2000/10/swap/grammar/n3#verb',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#objecttail',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail'],
                                                             u']': [],
                                                             u'_': ['http://www.w3.org/2000/10/swap/grammar/n3#verb',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#objecttail',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail'],
                                                             u'a': ['http://www.w3.org/2000/10/swap/grammar/n3#verb',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#objecttail',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail'],
                                                             u'{': ['http://www.w3.org/2000/10/swap/grammar/n3#verb',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#object',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#objecttail',
                                                                    'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail'],
                                                             u'}': []},
 u'http://www.w3.org/2000/10/swap/grammar/n3#propertylisttail': {u'.': [],
                                                                 u';': [u';',
                                                                        'http://www.w3.org/2000/10/swap/grammar/n3#propertylist'],
                                                                 u']': [],
                                                                 u'}': []},
 u'http://www.w3.org/2000/10/swap/grammar/n3#simpleStatement': {u'"': ['http://www.w3.org/2000/10/swap/grammar/n3#subject',
                                                                       'http://www.w3.org/2000/10/swap/grammar/n3#propertylist'],
                                                                u'(': ['http://www.w3.org/2000/10/swap/grammar/n3#subject',
                                                                       'http://www.w3.org/2000/10/swap/grammar/n3#propertylist'],
                                                                u'+': ['http://www.w3.org/2000/10/swap/grammar/n3#subject',
                                                                       'http://www.w3.org/2000/10/swap/grammar/n3#propertylist'],
                                                                u'-': ['http://www.w3.org/2000/10/swap/grammar/n3#subject',
                                                                       'http://www.w3.org/2000/10/swap/grammar/n3#propertylist'],
                                                                u'0': ['http://www.w3.org/2000/10/swap/grammar/n3#subject',
                                                                       'http://www.w3.org/2000/10/swap/grammar/n3#propertylist'],
                                                                u':': ['http://www.w3.org/2000/10/swap/grammar/n3#subject',
                                                                       'http://www.w3.org/2000/10/swap/grammar/n3#propertylist'],
                                                                u'<': ['http://www.w3.org/2000/10/swap/grammar/n3#subject',
                                                                       'http://www.w3.org/2000/10/swap/grammar/n3#propertylist'],
                                                                u'?': ['http://www.w3.org/2000/10/swap/grammar/n3#subject',
                                                                       'http://www.w3.org/2000/10/swap/grammar/n3#propertylist'],
                                                                u'@this': ['http://www.w3.org/2000/10/swap/grammar/n3#subject',
                                                                           'http://www.w3.org/2000/10/swap/grammar/n3#propertylist'],
                                                                u'[': ['http://www.w3.org/2000/10/swap/grammar/n3#subject',
                                                                       'http://www.w3.org/2000/10/swap/grammar/n3#propertylist'],
                                                                u'_': ['http://www.w3.org/2000/10/swap/grammar/n3#subject',
                                                                       'http://www.w3.org/2000/10/swap/grammar/n3#propertylist'],
                                                                u'a': ['http://www.w3.org/2000/10/swap/grammar/n3#subject',
                                                                       'http://www.w3.org/2000/10/swap/grammar/n3#propertylist'],
                                                                u'{': ['http://www.w3.org/2000/10/swap/grammar/n3#subject',
                                                                       'http://www.w3.org/2000/10/swap/grammar/n3#propertylist']},
 u'http://www.w3.org/2000/10/swap/grammar/n3#statement': {u'"': ['http://www.w3.org/2000/10/swap/grammar/n3#simpleStatement'],
                                                          u'(': ['http://www.w3.org/2000/10/swap/grammar/n3#simpleStatement'],
                                                          u'+': ['http://www.w3.org/2000/10/swap/grammar/n3#simpleStatement'],
                                                          u'-': ['http://www.w3.org/2000/10/swap/grammar/n3#simpleStatement'],
                                                          u'0': ['http://www.w3.org/2000/10/swap/grammar/n3#simpleStatement'],
                                                          u':': ['http://www.w3.org/2000/10/swap/grammar/n3#simpleStatement'],
                                                          u'<': ['http://www.w3.org/2000/10/swap/grammar/n3#simpleStatement'],
                                                          u'?': ['http://www.w3.org/2000/10/swap/grammar/n3#simpleStatement'],
                                                          u'@forAll': ['http://www.w3.org/2000/10/swap/grammar/n3#universal'],
                                                          u'@forSome': ['http://www.w3.org/2000/10/swap/grammar/n3#existential'],
                                                          u'@keywords': ['http://www.w3.org/2000/10/swap/grammar/n3#declaration'],
                                                          u'@prefix': ['http://www.w3.org/2000/10/swap/grammar/n3#declaration'],
                                                          u'@this': ['http://www.w3.org/2000/10/swap/grammar/n3#simpleStatement'],
                                                          u'[': ['http://www.w3.org/2000/10/swap/grammar/n3#simpleStatement'],
                                                          u'_': ['http://www.w3.org/2000/10/swap/grammar/n3#simpleStatement'],
                                                          u'a': ['http://www.w3.org/2000/10/swap/grammar/n3#simpleStatement'],
                                                          u'{': ['http://www.w3.org/2000/10/swap/grammar/n3#simpleStatement']},
 u'http://www.w3.org/2000/10/swap/grammar/n3#statementlist': {u'"': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#statementtail'],
                                                              u'(': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#statementtail'],
                                                              u'+': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#statementtail'],
                                                              u'-': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#statementtail'],
                                                              u'0': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#statementtail'],
                                                              u':': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#statementtail'],
                                                              u'<': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#statementtail'],
                                                              u'?': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#statementtail'],
                                                              u'@forAll': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                           'http://www.w3.org/2000/10/swap/grammar/n3#statementtail'],
                                                              u'@forSome': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                            'http://www.w3.org/2000/10/swap/grammar/n3#statementtail'],
                                                              u'@keywords': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                             'http://www.w3.org/2000/10/swap/grammar/n3#statementtail'],
                                                              u'@prefix': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                           'http://www.w3.org/2000/10/swap/grammar/n3#statementtail'],
                                                              u'@this': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                         'http://www.w3.org/2000/10/swap/grammar/n3#statementtail'],
                                                              u'[': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#statementtail'],
                                                              u'_': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#statementtail'],
                                                              u'a': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#statementtail'],
                                                              u'{': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#statementtail'],
                                                              u'}': []},
 u'http://www.w3.org/2000/10/swap/grammar/n3#statements_optional': {u'"': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                           u'.',
                                                                           'http://www.w3.org/2000/10/swap/grammar/n3#statements_optional'],
                                                                    u'(': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                           u'.',
                                                                           'http://www.w3.org/2000/10/swap/grammar/n3#statements_optional'],
                                                                    u'+': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                           u'.',
                                                                           'http://www.w3.org/2000/10/swap/grammar/n3#statements_optional'],
                                                                    u'-': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                           u'.',
                                                                           'http://www.w3.org/2000/10/swap/grammar/n3#statements_optional'],
                                                                    u'0': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                           u'.',
                                                                           'http://www.w3.org/2000/10/swap/grammar/n3#statements_optional'],
                                                                    u':': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                           u'.',
                                                                           'http://www.w3.org/2000/10/swap/grammar/n3#statements_optional'],
                                                                    u'<': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                           u'.',
                                                                           'http://www.w3.org/2000/10/swap/grammar/n3#statements_optional'],
                                                                    u'?': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                           u'.',
                                                                           'http://www.w3.org/2000/10/swap/grammar/n3#statements_optional'],
                                                                    u'@EOFDUMMY': [],
                                                                    u'@forAll': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                                 u'.',
                                                                                 'http://www.w3.org/2000/10/swap/grammar/n3#statements_optional'],
                                                                    u'@forSome': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                                  u'.',
                                                                                  'http://www.w3.org/2000/10/swap/grammar/n3#statements_optional'],
                                                                    u'@keywords': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                                   u'.',
                                                                                   'http://www.w3.org/2000/10/swap/grammar/n3#statements_optional'],
                                                                    u'@prefix': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                                 u'.',
                                                                                 'http://www.w3.org/2000/10/swap/grammar/n3#statements_optional'],
                                                                    u'@this': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                               u'.',
                                                                               'http://www.w3.org/2000/10/swap/grammar/n3#statements_optional'],
                                                                    u'[': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                           u'.',
                                                                           'http://www.w3.org/2000/10/swap/grammar/n3#statements_optional'],
                                                                    u'_': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                           u'.',
                                                                           'http://www.w3.org/2000/10/swap/grammar/n3#statements_optional'],
                                                                    u'a': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                           u'.',
                                                                           'http://www.w3.org/2000/10/swap/grammar/n3#statements_optional'],
                                                                    u'{': ['http://www.w3.org/2000/10/swap/grammar/n3#statement',
                                                                           u'.',
                                                                           'http://www.w3.org/2000/10/swap/grammar/n3#statements_optional']},
 u'http://www.w3.org/2000/10/swap/grammar/n3#statementtail': {u'.': [u'.',
                                                                     'http://www.w3.org/2000/10/swap/grammar/n3#statementlist'],
                                                              u'}': []},
 u'http://www.w3.org/2000/10/swap/grammar/n3#subject': {u'"': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                        u'(': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                        u'+': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                        u'-': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                        u'0': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                        u':': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                        u'<': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                        u'?': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                        u'@this': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                        u'[': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                        u'_': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                        u'a': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                        u'{': ['http://www.w3.org/2000/10/swap/grammar/n3#path']},
 u'http://www.w3.org/2000/10/swap/grammar/n3#symbol': {u':': ['http://www.w3.org/2000/10/swap/grammar/n3#qname'],
                                                       u'<': ['http://www.w3.org/2000/10/swap/grammar/n3#explicituri'],
                                                       u'_': ['http://www.w3.org/2000/10/swap/grammar/n3#qname'],
                                                       u'a': ['http://www.w3.org/2000/10/swap/grammar/n3#qname']},
 u'http://www.w3.org/2000/10/swap/grammar/n3#universal': {u'@forAll': [u'@forAll',
                                                                       '_:jcOJHCYs33']},
 u'http://www.w3.org/2000/10/swap/grammar/n3#verb': {u'"': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                     u'(': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                     u'+': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                     u'-': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                     u'0': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                     u':': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                     u'<': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                     u'<=': [u'<='],
                                                     u'=': [u'='],
                                                     u'=>': [u'=>'],
                                                     u'?': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                     u'@a': [u'@a'],
                                                     u'@has': [u'@has',
                                                               'http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                     u'@is': [u'@is',
                                                              'http://www.w3.org/2000/10/swap/grammar/n3#path',
                                                              u'@of'],
                                                     u'@this': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                     u'[': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                     u'_': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                     u'a': ['http://www.w3.org/2000/10/swap/grammar/n3#path'],
                                                     u'{': ['http://www.w3.org/2000/10/swap/grammar/n3#path']}}
regexps = {
   u'http://www.w3.org/2000/10/swap/grammar/n3#barename': re.compile(u'[a-zA-Z_][a-zA-Z0-9_]*'),
   u'http://www.w3.org/2000/10/swap/grammar/n3#variable': re.compile(u'\\?[a-zA-Z_][a-zA-Z0-9_]*'),
   u'http://www.w3.org/2000/10/swap/grammar/n3#qname': re.compile(u'(([a-zA-Z_][a-zA-Z0-9_]*)?:)?([a-zA-Z_][a-zA-Z0-9_-]*)?'),
   u'http://www.w3.org/2000/10/swap/grammar/n3#string': re.compile(u'("""[^"\\\\]*(?:(?:\\\\.|"(?!""))[^"\\\\]*)*""")|("[^"\\\\]*(?:\\\\.[^"\\\\]*)*")'),
   u'http://www.w3.org/2000/10/swap/grammar/n3#explicituri': re.compile(u'<[^>]*>'),
   u'http://www.w3.org/2000/10/swap/grammar/n3#langcode': re.compile(u'[a-z]+(-[a-z0-9]+)*'),
   u'http://www.w3.org/2000/10/swap/grammar/n3#numericliteral': re.compile(u'[-+]?[0-9]+(\\.[0-9]+)?(e[-+]?[0-9]+)?'),
}

if __name__=="__main__":
   print __doc__

########NEW FILE########
__FILENAME__ = n3p
#!/usr/bin/env python
"""
N3P - An N3 Parser using n3.n3
Author: Sean B. Palmer, inamidst.com
Licence: GPL 2; share and enjoy!
License: http://www.w3.org/Consortium/Legal/copyright-software
Documentation: http://inamidst.com/n3p/
Derived from:
   http://www.w3.org/2000/10/swap/grammar/predictiveParser.py
   - predictiveParser.py, Tim Berners-Lee, 2004
Issues:
   http://lists.w3.org/Archives/Public/public-cwm-bugs/2005Jan/0006
   http://lists.w3.org/Archives/Public/public-cwm-talk/2005JanMar/0015
"""

import sys, os, re, urllib
import cPickle as pickle

try: set()
except NameError:
   from sets import Set as set

try:
   import n3meta
   branches = n3meta.branches
   regexps = n3meta.regexps
except ImportError:
   for path in sys.path:
      fn = os.path.join(path, 'n3meta.pkl')
      if os.path.isfile(fn):
         f = open(fn, 'rb')
         n3meta = pickle.load(f)
         f.close()

         branches = n3meta['branches']
         regexps = n3meta['regexps']
         break

start = 'http://www.w3.org/2000/10/swap/grammar/n3#document'

r_whitespace = re.compile(r'[ \t\r\n]*(?:(?:#[^\n]*)?\r?(?:$|\n))?')
singleCharacterSelectors = "\t\r\n !\"#$%&'()*.,+/;<=>?[\\]^`{|}~"
r_qname = re.compile(r'([A-Za-z0-9_:]*)')
r_name = re.compile(r'([A-Za-z0-9_]*)')
notQNameChars = singleCharacterSelectors + "@"
notNameChars = notQNameChars + ":"

def abbr(prodURI):
   return prodURI.split('#').pop()

class N3Parser(object):
   def __init__(self, uri, branches, regexps):
      if uri == 'nowhere': pass
      elif (uri != 'file:///dev/stdin'):
         u = urllib.urlopen(uri)
         self.data = u.read()
         u.close()
      else: self.data = sys.stdin.read()
      self.pos = 0
      self.branches = branches
      self.regexps = regexps
      self.keywordMode = False
      self.keywords = set(("a", "is", "of", "this", "has"))
      self.productions = []
      self.memo = {}

   def parse(self, prod):
      todoStack = [[prod, None]]
      while todoStack:
         if todoStack[-1][1] is None:
            todoStack[-1][1] = []
            tok = self.token()
            # Got an opened production
            self.onStart(abbr(todoStack[-1][0]))
            if not tok: return tok # EOF

            prodBranch = self.branches[todoStack[-1][0]]
            sequence = prodBranch.get(tok, None)
            if sequence is None:
               #print >> sys.stderr, 'prodBranch', prodBranch
               msg = "Found %s when expecting a %s . todoStack=%r"
               args = (tok, todoStack[-1][0], todoStack)
               raise ValueError, (msg % args)
            for term in sequence:
               todoStack[-1][1].append(term)
         while todoStack[-1][1]:
            term = todoStack[-1][1].pop(0)
            if isinstance(term, unicode):
               j = self.pos + len(term)
               word = self.data[self.pos:j]
               if word == term:
                  self.onToken(term, word)
                  self.pos = j
               elif '@' + word[:-1] == term:
                  self.onToken(term, word[:-1])
                  self.pos = j - 1
               else:
                  msg = "Found %s; %s expected"
                  args = (self.data[self.pos:self.pos+10], term)
                  raise ValueError, (msg % args)
            elif not self.regexps.has_key(term):
               todoStack.append([term, None])
               continue
            else:
               regexp = self.regexps[term]
               m = regexp.match(self.data, self.pos)
               if not m:
                  msg = "Token: %r should match %s"
                  args = (self.data[self.pos:self.pos+10], regexp.pattern)
                  raise ValueError, (msg % args)
               end = m.end()
               self.onToken(abbr(term), self.data[self.pos:end])
               self.pos = end
            self.token()
         while todoStack[-1][1] == []:
            todoStack.pop()
            self.onFinish()

   def token(self):
      """Memoizer for getToken."""
      if self.memo.has_key(self.pos):
         return self.memo[self.pos]
      result = self.getToken()
      pos = self.pos
      self.memo[pos] = result
      return result

   def getToken(self):
      self.whitespace()
      if self.pos == len(self.data):
         return '' # EOF!

      ch2 = self.data[self.pos:self.pos+2]
      for double in ('=>', '<=', '^^'):
         if ch2 == double: return double

      ch = self.data[self.pos]
      if ch == '.' and self.keywordMode:
         self.keywordMode = False

      if ch in singleCharacterSelectors + '"':
         return ch
      elif ch in '+-0123456789':
         return '0'

      if ch == '@':
         if self.pos and (self.data[self.pos-1] == '"'):
            return '@'
         name = r_name.match(self.data, self.pos + 1).group(1)
         if name == 'keywords':
            self.keywords = set()
            self.keywordMode = True
         return '@' + name

      word = r_qname.match(self.data, self.pos).group(1)
      if self.keywordMode:
         self.keywords.add(word)
      elif word in self.keywords:
         if word == 'keywords':
            self.keywords = set()
            self.keywordMode = True
         return '@' + word # implicit keyword
      return 'a'

   def whitespace(self):
      while True:
         end = r_whitespace.match(self.data, self.pos).end()
         if end <= self.pos: break
         self.pos = end

   def onStart(self, prod):
      print (' ' * len(self.productions)) + prod
      self.productions.append(prod)

   def onFinish(self):
      prod = self.productions.pop()
      print (' ' * len(self.productions)) + '/' + prod

   def onToken(self, prod, tok):
      print (' ' * len(self.productions)) + prod, tok

def main(argv=None):
   if argv is None:
      argv = sys.argv
   if len(argv) == 2:
      p = N3Parser(argv[1], branches, regexps)
      p.parse(start)

if __name__=="__main__":
   main()

########NEW FILE########
__FILENAME__ = n3proc
#!/usr/bin/env python
"""
n3proc - An N3 Processor using n3.n3
Author: Sean B. Palmer, inamidst.com
Licence: GPL 2; share and enjoy!
License: http://www.w3.org/Consortium/Legal/copyright-software
Documentation: http://inamidst.com/n3p/

usage:
   %prog [options] <URI>
"""

from rdflib import URIRef, BNode, Literal, Variable, Namespace
from rdflib.Graph import QuotedGraph

import sys, os.path, re, time, urllib
import n3p

try: from uripath import join as urijoin
except ImportError:
   print >> sys.stderr, "uripath.py not found"
   from urlparse import urljoin as urijoin

RDF = Namespace('http://www.w3.org/1999/02/22-rdf-syntax-ns#')
OWL = Namespace('http://www.w3.org/2002/07/owl#')
LOG = Namespace('http://www.w3.org/2000/10/swap/log#')
XSD = Namespace('http://www.w3.org/2001/XMLSchema#')
N3R = Namespace('http://www.w3.org/2000/10/swap/reify#')

r_unilower = re.compile(r'(?<=\\u)([0-9a-f]{4})|(?<=\\U)([0-9a-f]{8})')
r_hibyte = re.compile(r'[\x00-\x08\x0B-\x0C\x0E-\x1F\x7F-\xFF]')

def quote(s):
   if not isinstance(s, unicode):
      s = unicode(s, 'utf-8') # @@ not required?
   if not (u'\\'.encode('unicode-escape') == '\\\\'):
      s = s.replace('\\', r'\\')
   s = s.replace('"', r'\"')
   # s = s.replace(r'\\"', r'\"')
   s = r_hibyte.sub(lambda m: '\\u00%02X' % ord(m.group(0)), s)
   s = s.encode('unicode-escape')
   s = r_unilower.sub(lambda m: (m.group(1) or m.group(2)).upper(), s)
   return str(s)


quot = {'t': '\t', 'n': '\n', 'r': '\r', '"': '"', '\\': '\\'}

r_quot = re.compile(r'\\(t|n|r|"|\\)')
r_uniquot = re.compile(r'\\u([0-9A-F]{4})|\\U([0-9A-F]{8})')

class ParseError(Exception):
   pass

def unquote(s, triplequoted=False, r_safe = re.compile(ur'([\x20\x21\x23-\x5B\x5D-\x7E\u00A0-\uFFFF]+)')):
   """Unquote an N-Triples string.
      Derived from: http://inamidst.com/proj/rdf/ntriples.py
   """
   result = []
   while s:
      m = r_safe.match(s)
      if m:
         s = s[m.end():]
         result.append(m.group(1))
         continue

      m = r_quot.match(s)
      if m:
         s = s[2:]
         result.append(quot[m.group(1)])
         continue

      m = r_uniquot.match(s)
      if m:
         s = s[m.end():]
         u, U = m.groups()
         codepoint = int(u or U, 16)
         if codepoint > 0x10FFFF:
            raise ParseError("Disallowed codepoint: %08X" % codepoint)
         result.append(unichr(codepoint))
      elif s.startswith('\\'):
         raise ParseError("Illegal escape at: %s..." % s[:10])
      elif triplequoted and (s[0] in '\n"'):
         result.append(s[0])
         s = s[1:]
      else: raise ParseError("Illegal literal character: %r" % s[0])
   return unicode(''.join(result))

branches = n3p.branches
regexps = n3p.regexps
start = n3p.start

class N3Processor(n3p.N3Parser):
   def __init__(self, uri, sink, baseURI=False):
      super(N3Processor, self).__init__(uri, branches, regexps)
      if baseURI is False:
         self.baseURI = uri
      else: self.baseURI = baseURI
      self.sink = sink
      self.bindings = {'': urijoin(self.baseURI, '#')}
      self.counter = 0
      self.prefix = False
      self.userkeys = False
      self.anonsubj = False
      self.litinfo = False
      self.forAll = False
      self.forSome = False
      self.universals = {}
      self.existentials = {}
      self.formulae = []
      self.labels = []
      self.mode = []
      self.triples = []
      self.pathmode = 'path'
      self.paths = []
      self.lists = []
      self.bnodes = {}

   def parse(self, start=start):
      super(N3Processor, self).parse(start)

   def onStart(self, prod):
      self.productions.append(prod)
      handler = prod + 'Start'
      if hasattr(self, handler):
         getattr(self, handler)(prod)

   def onFinish(self):
      prod = self.productions.pop()
      handler = prod + 'Finish'
      if hasattr(self, handler):
         getattr(self, handler)()

   def onToken(self, prod, tok):
      if self.productions:
         parentProd = self.productions[-1]
         handler = parentProd + 'Token'
         if hasattr(self, handler):
            getattr(self, handler)(prod, tok)
      else: raise Exception("Token has no parent production.")

   def documentStart(self, prod):
      formula = self.sink.graph
      self.formulae.append(formula)
      self.sink.start(formula)

   def declarationToken(self, prod, tok):
      if prod == '@prefix':
         self.prefix = []
      elif prod == '@keywords':
         self.userkeys = True # bah
      elif (self.prefix is not False) and prod == 'qname':
         self.prefix.append(tok[:-1])
      elif prod == 'explicituri':
         self.prefix.append(tok[1:-1])

   def declarationFinish(self):
      if self.prefix:
         self.bindings[self.prefix[0]] = self.prefix[1]
         self.prefix = False

   def universalStart(self, prod):
      self.forAll = []

   def universalFinish(self):
      for term in self.forAll:
         v = self.univar('var')
         self.universals[term] = (self.formulae[-1], v)
         self.sink.quantify(self.formulae[-1], v)
      self.forAll = False

   def existentialStart(self, prod):
      self.forSome = []

   def existentialFinish(self):
      for term in self.forSome:
         b = BNode()
         self.existentials[term] = (self.formulae[-1], b)
         self.sink.quantify(self.formulae[-1], b)
      self.forSome = False

   def simpleStatementStart(self, prod):
      self.triples.append([])

   def simpleStatementFinish(self):
      if self.triples:
         self.triples.pop()

   def pathStart(self, prod):
      # p = self.paths
      # if not (p and p[-1] and (p[-1][-1] in '!^')):
      if (not self.paths) or (self.pathmode == 'path'):
         self.paths.append([])
         self.pathcounter = 1
      else: self.pathcounter += 1
      self.pathmode = 'path'

   def pathtailStart(self, prod):
      self.pathcounter += 1
      self.pathmode = 'pathtail'

   def pathtailToken(self, prod, tok):
      if prod == '!':
         self.paths[-1].append('!')
      elif prod == '^':
         self.paths[-1].append('^')

   def pathtailFinish(self):
      self.pathcounter -= 1

   def pathFinish(self):
      self.pathcounter -= 1
      self.pathmode = 'path'
      if self.paths and (self.pathcounter < 1):
         path = self.paths.pop()
         if not path: pass
         elif len(path) == 1:
            term = path.pop()
            if self.mode and self.mode[-1] == 'list':
               self.lists[-1].append(term)
            else: self.triples[-1].append(term)
         else: # A path traversal
            objt, path = path[0], path[1:]
            for (i, pred) in enumerate(path):
               if (i % 2) != 0:
                  subj = objt
                  objt = BNode()
                  if path[i-1] == '!':
                     self.triple(subj, pred, objt)
                  elif path[i-1] == '^':
                     self.triple(objt, pred, subj)
            # @@ nested paths?
            if self.mode and self.mode[-1] == 'list':
               self.lists[-1].append(objt)
            else: self.triples[-1].append(objt)
      # if self.anonsubj is True:
      #    self.anonsubj = False
      # self.path = False

   def nodeToken(self, prod, tok):
      nodedict = {}

      def ointerp(prod, tok):
         b = BNode()
         # Record here if it's a subject node
         if self.anonsubj:
            self.anonsubj = False
         if ((not self.triples) or
             (False not in map(lambda s: not len(s), self.triples)) or
             (len(self.triples[-1]) == 3) or
             (len(self.triples) > 1 and
              len(self.triples[-2]) == 3 and
              not len(self.triples[-1]))):
            self.anonsubj = True
         if (self.paths and
             self.paths[-1] and
             self.paths[-1][-1] in '!^'):
            self.anonsubj = 'path'

         if self.paths:
            self.paths[-1].append(b)
            self.triples.append([b])
         elif self.mode and self.mode[-1] == 'list':
            self.lists[-1].append(b)
            self.triples.append([b])
         # else: self.triples[-1].append(b)

         elif len(self.triples[-1]) > 1:
            self.triples.append([b])
         self.mode.append('triple')
      nodedict['['] = ointerp

      def cinterp(prod, tok):
         if ((not self.anonsubj) or
             (self.paths and len(self.paths[-1]) == 1)):
            self.triples.pop()
         elif self.anonsubj == 'path':
            self.triples.pop()
            self.triples.append([])
         else: self.anonsubj = False
         self.mode.pop()
      nodedict[']'] = cinterp

      def oparen(prod, tok):
         self.lists.append([])
         self.mode.append('list')
      nodedict['('] = oparen

      def cparen(prod, tok):
         items = self.lists.pop()
         if items:
            first = head = BNode()
            for (i, item) in enumerate(items):
               if i < len(items) - 1:
                  rest = BNode()
               else: rest = RDF.nil
               self.triple(first, RDF.first, item)
               self.triple(first, RDF.rest, rest)
               first = rest
         else: head = RDF.nil
         self.mode.pop()
         if self.paths:
            self.paths[-1].append(head)
         elif self.mode and self.mode[-1] == 'list':
            self.lists[-1].append(head)
         else: self.triples[-1].append(head)
      nodedict[')'] = cparen

      def obrace(prod, tok):
         f = self.formula()
         if self.paths:
            self.paths[-1].append(f)
         elif self.mode and self.mode[-1] == 'list':
            self.lists[-1].append(f)
         else: self.triples[-1].append(f)

         self.formulae.append(f)
         self.labels.append('f' + str(self.counter))
      nodedict['{'] = obrace

      def cbrace(prod, tok):
         self.formulae.pop()
         self.labels.pop()
         if self.triples and (len(self.triples[-1]) == 3):
            self.triple(*self.triples[-1])
            self.triples[-1].pop()
      nodedict['}'] = cbrace

      def numericliteral(prod, tok):
         if '.' in tok:
            tok = str(float(tok))
            lit = Literal(tok, datatype=XSD.double)
         else:
            tok = str(int(tok))
            lit = Literal(tok, datatype=XSD.integer)
         if self.paths:
            self.paths[-1].append(lit)
         elif self.mode and self.mode[-1] == 'list':
            self.lists[-1].append(lit)
         else: self.triples[-1].append(lit)
      nodedict['numericliteral'] = numericliteral

      def variable(prod, tok):
         var = self.univar(tok[1:], sic=True)
         if self.paths:
            self.paths[-1].append(var)
         elif self.mode and self.mode[-1] == 'list':
            self.lists[-1].append(var)
         else: self.triples[-1].append(var)
      nodedict['variable'] = variable

      def this(prod, tok):
         formula = self.formulae[-1]
         if self.paths:
            self.paths[-1].append(formula)
         elif self.mode and self.mode[-1] == 'list':
            self.lists[-1].append(formula)
         else: self.triples[-1].append(formula)
      nodedict['@this'] = this

      try: nodedict[prod](prod, tok)
      except KeyError: pass

   def literalStart(self, prod):
      self.litinfo = {}

   def literalToken(self, prod, tok):
      if prod == 'string':
         self.litinfo['content'] = tok

   def dtlangToken(self, prod, tok):
      if prod == 'langcode':
         self.litinfo['language'] = tok

   def symbolToken(self, prod, tok):
      if prod == 'explicituri':
         term = self.uri(tok[1:-1])
      elif prod == 'qname':
         term = self.qname(tok)

      if self.litinfo:
         self.litinfo['datatype'] = term
      elif self.forAll is not False:
         self.forAll.append(term)
      elif self.forSome is not False:
         self.forSome.append(term)
      elif self.paths:
         self.paths[-1].append(term)
      elif self.mode and self.mode[-1] == 'list':
         self.lists[-1].append(term)
      else: self.triples[-1].append(term)

   def literalFinish(self):
      content = self.litinfo['content']
      language = self.litinfo.get('language')
      datatype = self.litinfo.get('datatype')

      lit = self.literal(content, language, datatype)
      if self.paths:
         self.paths[-1].append(lit)
      elif self.mode and self.mode[-1] == 'list':
         self.lists[-1].append(lit)
      else: self.triples[-1].append(lit)
      self.litinfo = False

   def objectFinish(self):
      if self.triples and (len(self.triples[-1]) == 3):
         self.triple(*self.triples[-1])
         self.triples[-1].pop()

   def propertylisttailToken(self, prod, tok):
      if prod == ';':
         self.triples[-1] = [self.triples[-1][0]]

   def verbToken(self, prod, tok):
      vkwords ={'@a': RDF.type, '=': OWL.sameAs,
                '=>': LOG.implies, '<=': LOG.implies}
      if vkwords.has_key(prod):
         term = vkwords[prod]
         # if self.paths:
         #    self.paths[-1].append(term)
         if self.mode and self.mode[-1] == 'list':
            self.lists[-1].append(term)
         else: self.triples[-1].append(term)

      if prod in ('@of', '<='):
         # @@ test <= in CWM
         verb = (self.triples[-1][1],)
         self.triples[-1][1] = verb

   def triple(self, subj, pred, objt):
      scp = self.formulae[-1]
      if not isinstance(pred, tuple):
         self.sink.statement(subj, pred, objt, scp)
      else: self.sink.statement(objt, pred[0], subj, scp)

   def qname(self, tok):
      if ':' in tok:
         prefix, name = tok.split(':')
      elif self.userkeys:
         prefix, name = '', tok
      else: raise ParseError("Set user @keywords to use barenames.")
      if (prefix == '_') and (not self.bindings.has_key('_')):
         if name in self.bnodes:
            bnode = self.bnodes[name]
         else:
            bnode = BNode()
            self.bnodes[name] = bnode
         return bnode

      elif not self.bindings.has_key(prefix):
         print >> sys.stderr, "Prefix not bound: %s" % prefix
      return self.uri(self.bindings[prefix] + name)

   def uri(self, tok):
      u = URIRef(urijoin(self.baseURI, tok))
      if self.universals.has_key(u):
         formula, var = self.universals[u]
         if formula in self.formulae:
            return var
      if self.existentials.has_key(u): # @@ elif?
         formula, bnode = self.existentials[u]
         if formula in self.formulae:
            return bnode
      return u

   def formula(self):
      formula_id = BNode()
      if formula_id == self.sink.graph.identifier:
         return self.sink.graph
      else:
         return QuotedGraph(store=self.sink.graph.store, identifier=formula_id)
         #return self.sink.graph.get_context(formula_id, quoted=True)

   def literal(self, content, language, datatype):
      if content.startswith('"""'):
         content = unquote(content[3:-3].decode('utf-8'), triplequoted=True)
      else: content = unquote(content[1:-1].decode('utf-8'))
      return Literal(content, language, datatype)

   def univar(self, label, sic=False):
      if not sic:
         self.counter += 1
         label += str(self.counter)
      return Variable(label)


class NTriplesSink(object):
   def __init__(self, out=None):
      self.out = out or sys.stdout
      self.counter = 0

   def start(self, root):
      self.root = root

   def statement(self, s, p, o, f):
      if f == self.root:
         self.out.write("%s %s %s .\n" % (s, p, o))
      else: self.flatten(s, p, o, f)

   def quantify(self, formula, var):
      if formula != self.root:
         if var.startswith('_'): pred = N3R.existential
         elif var.startswith('?'): pred = N3R.universal
         self.out.write("%s %s %s .\n" % (formula, pred, var))

   def makeStatementID(self):
      return BNode()

   def flatten(self, s, p, o, f):
      fs = self.makeStatementID()
      self.out.write("%s %s %s .\n" % (f, N3R.statement, fs))
      self.out.write("%s %s %s .\n" % (fs, N3R.subject, s))
      self.out.write("%s %s %s .\n" % (fs, N3R.predicate, p))
      self.out.write("%s %s %s .\n" % (fs, N3R.object, o))

def parse(uri, options):
   baseURI = options.baseURI
   sink = NTriplesSink()
   if options.root:
      sink.quantify = lambda *args: True
      sink.flatten = lambda *args: True
   if ':' not in uri:
      uri = 'file://' + os.path.join(os.getcwd(), uri)
   if baseURI and (':' not in baseURI):
      baseURI = 'file://' + os.path.join(os.getcwd(), baseURI)
   p = N3Processor(uri, sink, baseURI=baseURI)
   p.parse()

def main(argv=None):
   import optparse

   class MyHelpFormatter(optparse.HelpFormatter):
      def __init__(self):
         kargs = {'indent_increment': 2, 'short_first': 1,
                  'max_help_position': 25, 'width': None}
         optparse.HelpFormatter.__init__(self, **kargs)
      def format_usage(self, usage):
         return optparse._("%s") % usage.lstrip()
      def format_heading(self, heading):
         return "%*s%s:\n" % (self.current_indent, "", heading)
   formatter = MyHelpFormatter()

   parser = optparse.OptionParser(usage=__doc__, formatter=formatter)
   parser.add_option("-b", "--baseURI", dest="baseURI", default=False,
                     help="set the baseURI", metavar="URI")
   parser.add_option("-r", "--root", dest="root",
                     action="store_true", default=False,
                     help="print triples in the root formula only")
   options, args = parser.parse_args(argv)

   if len(args) == 1:
      parse(args[0], options)
   else: parser.print_help()

if __name__=="__main__":
   main()

########NEW FILE########
__FILENAME__ = uripath
#!/bin/env python
"""
Uniform Resource Identifier (URI) path manipulation,
above the access layer

The name of this module and the functions are somewhat
arbitrary; they hark to other parts of the python
library; e.g. uripath.join() is somewhat like os.path.join().

REFERENCES

  Uniform Resource Identifiers (URI): Generic Syntax
  http://www.ietf.org/rfc/rfc2396.txt

  The Web Model: Information hiding and URI syntax (Jan 98)
  http://www.w3.org/DesignIssues/Model.html

  URI API design [was: URI Test Suite] Dan Connolly (Sun, Aug 12 2001)
  http://lists.w3.org/Archives/Public/uri/2001Aug/0021.html

"""

__version__ = "$Id: uripath.py,v 1.16 2004/03/21 04:24:35 timbl Exp $"

from string import find, rfind, index


def splitFrag(uriref):
    """split a URI reference between the fragment and the rest.

    Punctuation is thrown away.

    e.g.

    >>> splitFrag("abc#def")
    ('abc', 'def')

    >>> splitFrag("abcdef")
    ('abcdef', None)

    """

    i = rfind(uriref, "#")
    if i>= 0: return uriref[:i], uriref[i+1:]
    else: return uriref, None

def splitFragP(uriref, punct=0):
    """split a URI reference before the fragment

    Punctuation is kept.

    e.g.

    >>> splitFragP("abc#def")
    ('abc', '#def')

    >>> splitFragP("abcdef")
    ('abcdef', '')

    """

    i = rfind(uriref, "#")
    if i>= 0: return uriref[:i], uriref[i:]
    else: return uriref, ''


def join(here, there):
    """join an absolute URI and URI reference
    (non-ascii characters are supported/doctested;
    haven't checked the details of the IRI spec though)

    here is assumed to be absolute.
    there is URI reference.

    >>> join('http://example/x/y/z', '../abc')
    'http://example/x/abc'

    Raise ValueError if there uses relative path
    syntax but here has no hierarchical path.

    >>> join('mid:foo@example', '../foo')
    Traceback (most recent call last):
        raise ValueError, here
    ValueError: Base <mid:foo@example> has no slash after colon - with relative '../foo'.


    We grok IRIs

    >>> len(u'Andr\\xe9')
    5

    >>> join('http://example.org/', u'#Andr\\xe9')
    u'http://example.org/#Andr\\xe9'
    """

    assert(find(here, "#") < 0), "Base may not contain hash: '%s'"% here # caller must splitFrag (why?)

    slashl = find(there, '/')
    colonl = find(there, ':')

    # join(base, 'foo:/') -- absolute
    if colonl >= 0 and (slashl < 0 or colonl < slashl):
        return there

    bcolonl = find(here, ':')
    assert(bcolonl >= 0), "Base uri '%s' is not absolute" % here # else it's not absolute

    # join('mid:foo@example', '../foo') bzzt
    if here[bcolonl+1:bcolonl+2] <> '/':
        raise ValueError ("Base <%s> has no slash after colon - with relative '%s'." %(here, there))

    if here[bcolonl+1:bcolonl+3] == '//':
        bpath = find(here, '/', bcolonl+3)
    else:
        bpath = bcolonl+1

    # join('http://xyz', 'foo')
    if bpath < 0:
        bpath = len(here)
        here = here + '/'

    # join('http://xyz/', '//abc') => 'http://abc'
    if there[:2] == '//':
        return here[:bcolonl+1] + there

    # join('http://xyz/', '/abc') => 'http://xyz/abc'
    if there[:1] == '/':
        return here[:bpath] + there

    slashr = rfind(here, '/')

    path, frag = splitFragP(there)
    if not path: return here + frag

    while 1:
        if path[:2] == './':
            path = path[2:]
        if path == '.':
            path = ''
        elif path[:3] == '../' or path == '..':
            path = path[3:]
            i = rfind(here, '/', bpath, slashr)
            if i >= 0:
                here = here[:i+1]
                slashr = i
        else:
            break

    return here[:slashr+1] + path + frag



import re
import string
commonHost = re.compile(r'^[-_a-zA-Z0-9.]+:(//[^/]*)?/[^/]*$')


def refTo(base, uri):
    """figure out a relative URI reference from base to uri

    >>> refTo('http://example/x/y/z', 'http://example/x/abc')
    '../abc'

    >>> refTo('file:/ex/x/y', 'file:/ex/x/q/r#s')
    'q/r#s'

    >>> refTo(None, 'http://ex/x/y')
    'http://ex/x/y'

    >>> refTo('http://ex/x/y', 'http://ex/x/y')
    ''

    Note the relationship between refTo and join:
    join(x, refTo(x, y)) == y
    which points out certain strings which cannot be URIs. e.g.
    >>> x='http://ex/x/y';y='http://ex/x/q:r';join(x, refTo(x, y)) == y
    0

    So 'http://ex/x/q:r' is not a URI. Use 'http://ex/x/q%3ar' instead:
    >>> x='http://ex/x/y';y='http://ex/x/q%3ar';join(x, refTo(x, y)) == y
    1

    This one checks that it uses a root-realtive one where that is
    all they share.  Now uses root-relative where no path is shared.
    This is a matter of taste but tends to give more resilience IMHO
    -- and shorter paths

    Note that base may be None, meaning no base.  In some situations, there
    just ain't a base. Slife. In these cases, relTo returns the absolute value.
    The axiom abs(,rel(b,x))=x still holds.
    This saves people having to set the base to "bogus:".

    >>> refTo('http://ex/x/y/z', 'http://ex/r')
    '/r'

    """

#    assert base # don't mask bugs -danc # not a bug. -tim
    if not base: return uri
    if base == uri: return ""

    # Find how many path segments in common
    i=0
    while i<len(uri) and i<len(base):
        if uri[i] == base[i]: i = i + 1
        else: break
    # print "# relative", base, uri, "   same up to ", i
    # i point to end of shortest one or first difference

    m = commonHost.match(base[:i])
    if m:
        k=uri.find("//")
        if k<0: k=-2 # no host
        l=uri.find("/", k+2)
        if uri[l+1:l+2] != "/" and base[l+1:l+2] != "/" and uri[:l]==base[:l]:
            return uri[l:]

    if uri[i:i+1] =="#" and len(base) == i: return uri[i:] # fragment of base

    while i>0 and uri[i-1] != '/' : i=i-1  # scan for slash

    if i < 3: return uri  # No way.
    if string.find(base, "//", i-2)>0 \
       or string.find(uri, "//", i-2)>0: return uri # An unshared "//"
    if string.find(base, ":", i)>0: return uri  # An unshared ":"
    n = string.count(base, "/", i)
    if n == 0 and i<len(uri) and uri[i] == '#':
        return "./" + uri[i:]
    elif n == 0 and i == len(uri):
        return "./"
    else:
        return ("../" * n) + uri[i:]

import os
def base():
        """The base URI for this process - the Web equiv of cwd

        Relative or abolute unix-standard filenames parsed relative to
        this yeild the URI of the file.
        If we had a reliable way of getting a computer name,
        we should put it in the hostname just to prevent ambiguity

        """
#	return "file://" + hostname + os.getcwd() + "/"
        return "file:" + _fixslash(os.getcwd()) + "/"


def _fixslash(str):
    """ Fix windowslike filename to unixlike - (#ifdef WINDOWS)"""
    s = str
    for i in range(len(s)):
        if s[i] == "\\": s = s[:i] + "/" + s[i+1:]
    if s[0] != "/" and s[1] == ":": s = s[2:]  # @@@ Hack when drive letter present
    return s


import unittest

class Tests(unittest.TestCase):
    def testPaths(self):
        cases = (("foo:xyz", "bar:abc", "bar:abc"),
                 ('http://example/x/y/z', 'http://example/x/abc', '../abc'),
                 ('http://example2/x/y/z', 'http://example/x/abc', 'http://example/x/abc'),
                 ('http://ex/x/y/z', 'http://ex/x/r', '../r'),
                 #             ('http://ex/x/y/z', 'http://ex/r', '../../r'),    # DanC had this.
                 ('http://ex/x/y', 'http://ex/x/q/r', 'q/r'),
                 ('http://ex/x/y', 'http://ex/x/q/r#s', 'q/r#s'),
                 ('http://ex/x/y', 'http://ex/x/q/r#s/t', 'q/r#s/t'),
                 ('http://ex/x/y', 'ftp://ex/x/q/r', 'ftp://ex/x/q/r'),
                 ('http://ex/x/y', 'http://ex/x/y', ''),
                 ('http://ex/x/y/', 'http://ex/x/y/', ''),
                 ('http://ex/x/y/pdq', 'http://ex/x/y/pdq', ''),
                 ('http://ex/x/y/', 'http://ex/x/y/z/', 'z/'),
                 ('file:/swap/test/animal.rdf', 'file:/swap/test/animal.rdf#Animal', '#Animal'),
                 ('file:/e/x/y/z', 'file:/e/x/abc', '../abc'),
                 ('file:/example2/x/y/z', 'file:/example/x/abc', '/example/x/abc'),   # TBL
                 ('file:/ex/x/y/z', 'file:/ex/x/r', '../r'),
                 ('file:/ex/x/y/z', 'file:/r', '/r'),        # I prefer this. - tbl
                 ('file:/ex/x/y', 'file:/ex/x/q/r', 'q/r'),
                 ('file:/ex/x/y', 'file:/ex/x/q/r#s', 'q/r#s'),
                 ('file:/ex/x/y', 'file:/ex/x/q/r#', 'q/r#'),
                 ('file:/ex/x/y', 'file:/ex/x/q/r#s/t', 'q/r#s/t'),
                 ('file:/ex/x/y', 'ftp://ex/x/q/r', 'ftp://ex/x/q/r'),
                 ('file:/ex/x/y', 'file:/ex/x/y', ''),
                 ('file:/ex/x/y/', 'file:/ex/x/y/', ''),
                 ('file:/ex/x/y/pdq', 'file:/ex/x/y/pdq', ''),
                 ('file:/ex/x/y/', 'file:/ex/x/y/z/', 'z/'),
                 ('file:/devel/WWW/2000/10/swap/test/reluri-1.n3',
                  'file://meetings.example.com/cal#m1', 'file://meetings.example.com/cal#m1'),
                 ('file:/home/connolly/w3ccvs/WWW/2000/10/swap/test/reluri-1.n3', 'file://meetings.example.com/cal#m1', 'file://meetings.example.com/cal#m1'),
                 ('file:/some/dir/foo', 'file:/some/dir/#blort', './#blort'),
                 ('file:/some/dir/foo', 'file:/some/dir/#', './#'),

                 # From Graham Klyne Thu, 20 Feb 2003 18:08:17 +0000
                 ("http://example/x/y%2Fz", "http://example/x/abc", "abc"),
                 ("http://example/x/y/z", "http://example/x%2Fabc", "/x%2Fabc"),
                 ("http://example/x/y%2Fz", "http://example/x%2Fabc", "/x%2Fabc"),
                 ("http://example/x%2Fy/z", "http://example/x%2Fy/abc", "abc"),
                 # Ryan Lee
                 ("http://example/x/abc.efg", "http://example/x/", "./")
                 )

        for inp1, inp2, exp in cases:
            self.assertEquals(refTo(inp1, inp2), exp)
            self.assertEquals(join(inp1, exp), inp2)


    def testSplit(self):
        cases = (
            ("abc#def", "abc", "def"),
            ("abc", "abc", None),
            ("#def", "", "def"),
            ("", "", None),
            ("abc#de:f", "abc", "de:f"),
            ("abc#de?f", "abc", "de?f"),
            ("abc#de/f", "abc", "de/f"),
            )
        for inp, exp1, exp2 in cases:
            self.assertEquals(splitFrag(inp), (exp1, exp2))

    def testRFCCases(self):

        base = 'http://a/b/c/d;p?q'

        # C.1.  Normal Examples

        normalExamples = (
            (base, 'g:h', 'g:h'),
            (base, 'g', 'http://a/b/c/g'),
            (base, './g', 'http://a/b/c/g'),
            (base, 'g/', 'http://a/b/c/g/'),
            (base, '/g', 'http://a/g'),
            (base, '//g', 'http://g'),
            (base, '?y', 'http://a/b/c/?y'), #@@wow... really?
            (base, 'g?y', 'http://a/b/c/g?y'),
            (base, '#s', 'http://a/b/c/d;p?q#s'), #@@ was: (current document)#s
            (base, 'g#s', 'http://a/b/c/g#s'),
            (base, 'g?y#s', 'http://a/b/c/g?y#s'),
            (base, ';x', 'http://a/b/c/;x'),
            (base, 'g;x', 'http://a/b/c/g;x'),
            (base, 'g;x?y#s', 'http://a/b/c/g;x?y#s'),
            (base, '.', 'http://a/b/c/'),
            (base, './', 'http://a/b/c/'),
            (base, '..', 'http://a/b/'),
            (base, '../', 'http://a/b/'),
            (base, '../g', 'http://a/b/g'),
            (base, '../..', 'http://a/'),
            (base, '../../', 'http://a/'),
            (base, '../../g', 'http://a/g')
            )

        otherExamples = (
            (base, '', base),
            (base, '../../../g', 'http://a/g'), #@@disagree with RFC2396
            (base, '../../../../g', 'http://a/g'), #@@disagree with RFC2396
            (base, '/./g', 'http://a/./g'),
            (base, '/../g', 'http://a/../g'),
            (base, 'g.', 'http://a/b/c/g.'),
            (base, '.g', 'http://a/b/c/.g'),
            (base, 'g..', 'http://a/b/c/g..'),
            (base, '..g', 'http://a/b/c/..g'),

            (base, './../g', 'http://a/b/g'),
            (base, './g/.', 'http://a/b/c/g/.'), #@@hmmm...
            (base, 'g/./h', 'http://a/b/c/g/./h'), #@@hmm...
            (base, 'g/../h', 'http://a/b/c/g/../h'),
            (base, 'g;x=1/./y', 'http://a/b/c/g;x=1/./y'), #@@hmmm...
            (base, 'g;x=1/../y', 'http://a/b/c/g;x=1/../y'),  #@@hmmm...

            (base, 'g?y/./x', 'http://a/b/c/g?y/./x'),
            (base, 'g?y/../x', 'http://a/b/c/g?y/../x'),
            (base, 'g#s/./x', 'http://a/b/c/g#s/./x'),
            (base, 'g#s/../x', 'http://a/b/c/g#s/../x')
            )

        for b, inp, exp in normalExamples + otherExamples:
            if exp is None:
                self.assertRaises(ValueError, join, b, inp)
            else:
                self.assertEquals(join(b, inp), exp)

def _test():
    import doctest, uripath
    doctest.testmod(uripath)
    unittest.main()

if __name__ == '__main__':
    _test()


# $Log: uripath.py,v $
# Revision 1.16  2004/03/21 04:24:35  timbl
# (See doc/changes.html)
# on xml output, nodeID was incorrectly spelled.
# update.py provides cwm's --patch option.
# diff.py as independent progrem generates patch files for cwm --patch
#
# Revision 1.15  2004/01/28 22:22:10  connolly
# tested that IRIs work in uripath.join()
#
# Revision 1.14  2003/10/20 17:31:55  timbl
# Added @keyword support.
# (eventually got python+expat to wrok on fink, with patch)
# Trig functions are in, thanks to Karl, with some changes, but NOT in regeression.n3
# see test/math/test-trigo.n3 for now.
#
# Revision 1.13  2003/07/03 21:04:39  timbl
# New string function to compare strings normalizing case and whitespace string:containsRoughly
#
# Revision 1.12  2003/04/03 22:35:12  ryanlee
# fixed previous fix, added test case
#
# Revision 1.11  2003/04/03 22:06:54  ryanlee
# small fix in if, line 217
#
# Revision 1.10  2003/02/24 15:06:38  connolly
# some more tests from Graham
#
# Revision 1.9  2002/12/25 20:01:32  timbl
# some --flatten tests fail. --why fails. Formulae must be closed to be referenced in a add()
#
# Revision 1.8  2002/11/24 03:12:02  timbl
# base can be None in uripath:refTo
#
# Revision 1.7  2002/09/04 05:03:07  connolly
# convertet unittests to use python doctest and unittest modules; cleaned up docstrings a bit
#
# Revision 1.6  2002/09/04 04:07:50  connolly
# fixed uripath.refTo
#
# Revision 1.5  2002/08/23 04:36:15  connolly
# fixed refTo case: file:/some/dir/foo  ->  file:/some/dir/#blort
#
# Revision 1.4  2002/08/07 14:32:21  timbl
# uripath changes. passes 51 general tests and 25 loopback tests
#
# Revision 1.3  2002/08/06 01:36:09  connolly
# cleanup: diagnostic interface, relative/absolute uri handling
#
# Revision 1.2  2002/03/15 23:53:02  connolly
# handle no-auth case
#
# Revision 1.1  2002/02/19 22:52:42  connolly
# renamed uritools.py to uripath.py
#
# Revision 1.2  2002/02/18 07:33:51  connolly
# pathTo seems to work
#

########NEW FILE########
__FILENAME__ = N3Parser
from rdflib import URIRef, BNode, Literal, RDF, Variable

from rdflib.util import from_n3

from rdflib.syntax.parsers import Parser
from rdflib.syntax.parsers.n3p.n3proc import N3Processor

from rdflib.Graph import Graph, QuotedGraph, ConjunctiveGraph


class N3Parser(Parser):

    def __init__(self):
        pass

    def parse(self, source, graph):
        # we're currently being handed a Graph, not a ConjunctiveGraph
        assert graph.store.context_aware # is this implied by formula_aware
        assert graph.store.formula_aware

        conj_graph = ConjunctiveGraph(store=graph.store)
        conj_graph.default_context = graph # TODO: CG __init__ should have a default_context arg
        # TODO: update N3Processor so that it can use conj_graph as the sink
        sink = Sink(conj_graph)
        if False:
            sink.quantify = lambda *args: True
            sink.flatten = lambda *args: True
        baseURI = graph.absolutize(source.getPublicId() or source.getSystemId() or "")
        p = N3Processor("nowhere", sink, baseURI=baseURI) # pass in "nowhere" so we can set data instead
        p.userkeys = True # bah
        p.data = source.getByteStream().read() # TODO getCharacterStream?
        p.parse()
        for prefix, namespace in p.bindings.items():
            conj_graph.bind(prefix, namespace)


class Sink(object):
    def __init__(self, graph):
        self.graph = graph

    def start(self, root):
        pass

    def statement(self, s, p, o, f):
        f.add((s, p, o))

    def quantify(self, formula, var):
        #print "quantify(%s, %s)" % (formula, var)
        pass


########NEW FILE########
__FILENAME__ = NTParser
from rdflib.syntax.parsers import Parser
from rdflib.syntax.parsers.ntriples import NTriplesParser


class NTSink(object):
    def __init__(self, graph):
        self.graph = graph

    def triple(self, s, p, o):
        self.graph.add((s, p, o))


import codecs

class NTParser(Parser):

    def __init__(self):
        super(NTParser, self).__init__()

    def parse(self, source, sink, baseURI=None):
        f = source.getByteStream() # TODO getCharacterStream?
        parser = NTriplesParser(NTSink(sink))
        parser.parse(f)
        f.close()



########NEW FILE########
__FILENAME__ = ntriples
#!/usr/bin/env python
"""
N-Triples Parser
License: GPL 2, W3C, BSD, or MIT
Author: Sean B. Palmer, inamidst.com
Documentation:
   http://inamidst.com/proj/rdf/ntriples-doc

Command line usage:
   ./ntriples.py <URI>    - parses URI as N-Triples
   ./ntriples.py --help   - prints out this help message
# @@ fully empty document?
"""

import re

uriref = r'<([^:]+:[^\s"<>]+)>'
literal = r'"([^"\\]*(?:\\.[^"\\]*)*)"'
litinfo = r'(?:@([a-z]+(?:-[a-z0-9]+)*)|\^\^' + uriref + r')?'

r_line = re.compile(r'([^\r\n]*)(?:\r\n|\r|\n)')
r_wspace = re.compile(r'[ \t]*')
r_wspaces = re.compile(r'[ \t]+')
r_tail = re.compile(r'[ \t]*\.[ \t]*')
r_uriref = re.compile(uriref)
r_nodeid = re.compile(r'_:([A-Za-z][A-Za-z0-9]*)')
r_literal = re.compile(literal + litinfo)

bufsiz = 2048
validate = False

class Node(unicode): pass

# class URI(Node): pass
# class bNode(Node): pass
# class Literal(Node):
#    def __new__(cls, lit, lang=None, dtype=None):
#       n = str(lang) + ' ' + str(dtype) + ' ' + lit
#       return unicode.__new__(cls, n)

from rdflib import URIRef as URI
from rdflib import BNode as bNode
from rdflib import Literal

class Sink(object):
   def __init__(self):
      self.length = 0

   def triple(self, s, p, o):
      self.length += 1
      print (s, p, o)

class ParseError(Exception): pass

quot = {'t': '\t', 'n': '\n', 'r': '\r', '"': '"', '\\': '\\'}
r_safe = re.compile(r'([\x20\x21\x23-\x5B\x5D-\x7E]+)')
r_quot = re.compile(r'\\(t|n|r|"|\\)')
r_uniquot = re.compile(r'\\u([0-9A-F]{4})|\\U([0-9A-F]{8})')

def unquote(s):
   """Unquote an N-Triples string."""
   result = []
   while s:
      m = r_safe.match(s)
      if m:
         s = s[m.end():]
         result.append(m.group(1))
         continue

      m = r_quot.match(s)
      if m:
         s = s[2:]
         result.append(quot[m.group(1)])
         continue

      m = r_uniquot.match(s)
      if m:
         s = s[m.end():]
         u, U = m.groups()
         codepoint = int(u or U, 16)
         if codepoint > 0x10FFFF:
            raise ParseError("Disallowed codepoint: %08X" % codepoint)
         result.append(unichr(codepoint))
      elif s.startswith('\\'):
         raise ParseError("Illegal escape at: %s..." % s[:10])
      else: raise ParseError("Illegal literal character: %r" % s[0])
   return unicode(''.join(result))

if not validate:
   def unquote(s):
      return s.decode('unicode-escape')

r_hibyte = re.compile(r'([\x80-\xFF])')

def uriquote(uri):
   return r_hibyte.sub(lambda m: '%%%02X' % ord(m.group(1)), uri)
if not validate:
   def uriquote(uri):
      return uri

class NTriplesParser(object):
   """An N-Triples Parser.
      Usage:
         p = NTriplesParser(sink=MySink())
         sink = p.parse(f) # file; use parsestring for a string
   """

   def __init__(self, sink=None):
      if sink is not None:
         self.sink = sink
      else: self.sink = Sink()

   def parse(self, f):
      """Parse f as an N-Triples file."""
      if not hasattr(f, 'read'):
         raise ParseError("Item to parse must be a file-like object.")

      self.file = f
      self.buffer = ''
      while True:
         self.line = self.readline()
         if self.line is None: break
         try: self.parseline()
         except ParseError:
            raise ParseError("Invalid line: %r" % self.line)
      return self.sink

   def parsestring(self, s):
      """Parse s as an N-Triples string."""
      if not isinstance(s, basestring):
         raise ParseError("Item to parse must be a string instance.")
      from cStringIO import StringIO
      f = StringIO()
      f.write(s)
      f.seek(0)
      self.parse(f)

   def readline(self):
      """Read an N-Triples line from buffered input."""
      # N-Triples lines end in either CRLF, CR, or LF
      # Therefore, we can't just use f.readline()
      if not self.buffer:
         buffer = self.file.read(bufsiz)
         if not buffer: return None
         self.buffer = buffer

      while True:
         m = r_line.match(self.buffer)
         if m: # the more likely prospect
            self.buffer = self.buffer[m.end():]
            return m.group(1)
         else:
            buffer = self.file.read(bufsiz)
            if not buffer:
               raise ParseError("EOF in line")
            self.buffer += buffer

   def parseline(self):
      self.eat(r_wspace)
      if (not self.line) or self.line.startswith('#'):
         return # The line is empty or a comment

      subject = self.subject()
      self.eat(r_wspaces)

      predicate = self.predicate()
      self.eat(r_wspaces)

      object = self.object()
      self.eat(r_tail)

      if self.line:
         raise ParseError("Trailing garbage")
      self.sink.triple(subject, predicate, object)

   def peek(self, token):
      return self.line.startswith(token)

   def eat(self, pattern):
      m = pattern.match(self.line)
      if not m: # @@ Why can't we get the original pattern?
         raise ParseError("Failed to eat %s" % pattern)
      self.line = self.line[m.end():]
      return m

   def subject(self):
      # @@ Consider using dictionary cases
      subj = self.uriref() or self.nodeid()
      if not subj:
        raise ParseError("Subject must be uriref or nodeID")
      return subj

   def predicate(self):
      pred = self.uriref()
      if not pred:
         raise ParseError("Predicate must be uriref")
      return pred

   def object(self):
      objt = self.uriref() or self.nodeid() or self.literal()
      if objt is False:
         raise ParseError("Unrecognised object type")
      return objt

   def uriref(self):
      if self.peek('<'):
         uri = self.eat(r_uriref).group(1)
         uri = unquote(uri)
         uri = uriquote(uri)
         return URI(uri)
      return False

   def nodeid(self):
      if self.peek('_'):
         return bNode(self.eat(r_nodeid).group(1))
      return False

   def literal(self):
      if self.peek('"'):
         lit, lang, dtype = self.eat(r_literal).groups()
         lang = lang or None
         dtype = dtype or None
         if lang and dtype:
            raise ParseError("Can't have both a language and a datatype")
         lit = unquote(lit)
         return Literal(lit, lang, dtype)
      return False

def parseURI(uri):
   import urllib
   parser = NTriplesParser()
   u = urllib.urlopen(uri)
   sink = parser.parse(u)
   u.close()
   # for triple in sink:
   #    print triple
   print 'Length of input:', sink.length

def main():
   import sys
   if len(sys.argv) == 2:
      parseURI(sys.argv[1])
   else: print __doc__

if __name__=="__main__":
   main()

########NEW FILE########
__FILENAME__ = RDFaParser
"""
RDFa parser.

RDFa is a set of attributes used to embed RDF in XHTML. An important goal of
RDFa is to achieve this RDF embedding without repeating existing XHTML content
when that content is the metadata.

REFERENCES:

	http://www.w3.org/2001/sw/BestPractices/HTML/2005-rdfa-syntax

LICENSE:

  BSD

CHANGE HISTORY:

  2006/06/03 - Initial Version
  2006/06/08 - Added support for role (as per primer not syntax spec)
               Added support for plaintext and flattening of XMLLiterals
               ... (Sections 5.1.1.2 and 5.1.2.1)
               Fixed plaintext bug where it was being resolved as CURIE
               Added support to skip reserved @rel keywords from:
                 http://www.w3.org/TR/REC-html40/types.html#h-6.12
  2006/08/12 - Changed reserved @rel resolution to include a '#'
               Fixed subject resolution for LINK/META when inside HEAD
               Fixed blank node extraction [_:address] -> [_:_:address]
               Added support for passing prefix mappings to the Graph
               via RDFaSink
               Added @id support as part of subject resolution

Copyright (c) 2006, Elias Torres <elias@torrez.us>

"""

import sys, re, urllib, urlparse, cStringIO, string
from xml.dom import pulldom
from rdflib.syntax.parsers import Parser
from rdflib.Graph import ConjunctiveGraph
from rdflib import URIRef
from rdflib import BNode
from rdflib import Literal
from rdflib import Namespace

__version__ = "$Id: RDFaParser.py 1072 2007-03-30 18:12:54Z eliast $"

rdfa_attribs = ["about","property","rel","rev","href","content","role","id"]

reserved_links = ['alternate', 'stylesheet', 'start', 'next', 'prev',
                 'contents', 'index', 'glossary', 'copyright', 'chapter',
                 'section', 'subsection', 'appendix', 'help', 'bookmark']

xhtml = Namespace("http://www.w3.org/1999/xhtml")
xml = Namespace("http://www.w3.org/XML/1998/namespace")
rdf = Namespace("http://www.w3.org/1999/02/22-rdf-syntax-ns#")

class RDFaSink(object):
  def __init__(self, graph):
    self.graph = graph
  def __str__(self):
    return self.graph.serialize(format="pretty-xml")
  def triple(self, s, p, o):
    self.graph.add((s, p, o))
  def prefix(self, prefix, ns):
    self.graph.bind(prefix, ns, override=False)

_urifixer = re.compile('^([A-Za-z][A-Za-z0-9+-.]*://)(/*)(.*?)')

def _urljoin(base, uri):
  uri = _urifixer.sub(r'\1\3', uri)
  return urlparse.urljoin(base, uri)

class RDFaParser(Parser):
  def __init__(self):
    self.lang = None
    self.abouts = []
    self.xmlbases = []
    self.langs = []
    self.elementStack = [None]
    self.bcounter = {}
    self.bnodes = {}
    self.sink = None

  def parse(self, source, sink, baseURI=None):
    self.sink = RDFaSink(sink)
    self.triple = self.sink.triple
    self.prefix = self.sink.prefix
    self.baseuri = baseURI or source.getPublicId()
    f = source.getByteStream()
    events = pulldom.parse(f)
    self.handler = events.pulldom
    for (event, node) in events:

      if event == pulldom.START_DOCUMENT:
        self.abouts += [(URIRef(""), node)]

      if event == pulldom.END_DOCUMENT:
        assert len(self.elementStack) == 0

      if event == pulldom.START_ELEMENT:

        # keep track of parent node
        self.elementStack += [node]

        #if __debug__: print [e.tagName for e in self.elementStack if e]

        found = filter(lambda x:x in node.attributes.keys(),rdfa_attribs)

        # keep track of xml:lang xml:base
        baseuri = node.getAttributeNS(xml,"base") or node.getAttribute("xml:base") or self.baseuri
        self.baseuri = _urljoin(self.baseuri, baseuri)
        self.xmlbases.append(self.baseuri)

        if node.hasAttributeNS(xml,"lang") or node.hasAttribute("xml:lang"):
          lang = node.getAttributeNS(xml, 'lang') or node.getAttribute('xml:lang')

          if lang == '':
            # xml:lang could be explicitly set to '', we need to capture that
            lang = None
        else:
          # if no xml:lang is specified, use parent lang
          lang = self.lang

        self.lang = lang
        self.langs.append(lang)

        # node is not an RDFa element.
        if len(found) == 0: continue

        parentNode = self.elementStack[-2]

        if "about" in found:
          self.abouts += [(self.extractCURIEorURI(node.getAttribute("about")),node)]
        elif "id" in found:
          self.abouts += [(self.extractCURIEorURI("#" + node.getAttribute("id")),node)]

        subject = self.abouts[-1][0]

        # meta/link subject processing
        if(node.tagName == "meta" or node.tagName == "link"):
          if not("about" in found) and parentNode:
            if parentNode and parentNode.tagName == "head":
              subject = URIRef("")
            elif(parentNode.hasAttribute("about")):
              subject = self.extractCURIEorURI(parentNode.getAttribute("about"))
            elif parentNode.hasAttributeNS(xml,"id") or parentNode.hasAttribute("id"):
              # TODO: is this the right way to process xml:id by adding a '#'
              id = parentNode.getAttributeNS(xml,"id") or parentNode.getAttribute("id")
              subject = self.extractCURIEorURI("#" + id)
            else:
              subject = self.generateBlankNode(parentNode)

        if 'property' in found:
          predicate = self.extractCURIEorURI(node.getAttribute('property'))
          literal = None
          datatype = None
          plaintext = False

          if node.hasAttribute('datatype'):
            sdt = node.getAttribute('datatype')
            if sdt <> 'plaintext':
              datatype = self.extractCURIEorURI(sdt)
            else:
              plaintext = True

          if node.hasAttribute("content"):
            literal = Literal(node.getAttribute("content"), lang=lang, datatype=datatype)
          else:
            events.expandNode(node)

            # because I expanded, I won't get an END_ELEMENT
            self._popStacks(event, node)

            content = ""
            for child in node.childNodes:
              if datatype or plaintext:
                  content += self._getNodeText(child)
              else:
                content += child.toxml()
            content = content.strip()
            literal = Literal(content,datatype=datatype or rdf.XMLLiteral)

          if literal:
            self.triple(subject, predicate, literal)

        if "rel" in found:
          rel = node.getAttribute("rel").strip()
          if string.lower(rel) in reserved_links:
            rel = xhtml["#" + string.lower(rel)]

          predicate = self.extractCURIEorURI(rel)
          if node.hasAttribute("href"):
            object = self.extractCURIEorURI(node.getAttribute("href"))
            self.triple(subject, predicate, object)

        if "rev" in found:
          predicate = self.extractCURIEorURI(node.getAttribute("rev"))
          if node.hasAttribute("href"):
            object = self.extractCURIEorURI(node.getAttribute("href"))
            self.triple(object, predicate, subject)

        # role is in the primer, but not in the syntax.
        # could be deprecated.
        # Assumptions:
        # - Subject resolution as always (including meta/link)
        # - Attribute Value is a CURIE or URI
        # - It adds another triple, besides prop, rel, rev.
        if "role" in found:
          type = self.extractCURIEorURI(node.getAttribute('role'))
          self.triple(subject, rdf.type, type)

      if event == pulldom.END_ELEMENT:
        self._popStacks(event, node)
     
    # share with sink any prefix mappings
    for nsc in self.handler._ns_contexts:
      for ns, prefix in nsc.items():
        self.prefix(prefix, ns)

    f.close()

  def _getNodeText(self, node):
    if node.nodeType in (3,4): return node.nodeValue
    text = ''
    for child in node.childNodes:
      if child.nodeType in (3,4):
        text = text + child.nodeValue
    return text

  def generateBlankNode(self, parentNode):
    name = parentNode.tagName
    if self.bnodes.has_key(parentNode):
      return self.bnodes[parentNode]

    if self.bcounter.has_key(name):
      self.bcounter[name] = self.bcounter[name] + 1
    else:
      self.bcounter[name] = 0

    self.bnodes[parentNode] = BNode("%s%d" % (name, self.bcounter[name]))

    return self.bnodes[parentNode]

  def extractCURIEorURI(self, resource):
    if(len(resource) > 0 and resource[0] == "[" and resource[-1] == "]"):
      resource = resource[1:-1]

    # resolve prefixes
    # TODO: check whether I need to reverse the ns_contexts
    if(resource.find(":") > -1):
      rpre,rsuf = resource.split(":", 1)
      for nsc in self.handler._ns_contexts:
        for ns, prefix in nsc.items():
          if prefix == rpre:
            resource = ns + rsuf

    # TODO: is this enough to check for bnodes?
    if(len(resource) > 0 and resource[0:2] == "_:"):
      return BNode(resource[2:])

    return URIRef(self.resolveURI(resource))

  def resolveURI(self, uri):
    return _urljoin(self.baseuri or '', uri)

  def _popStacks(self, event, node):
    # check abouts
    if len(self.abouts) <> 0:
      about, aboutnode = self.abouts[-1]
      if aboutnode == node:
        self.abouts.pop()

    # keep track of nodes going out of scope
    self.elementStack.pop()

    # track xml:base and xml:lang going out of scope
    if self.xmlbases:
      self.xmlbases.pop()
      if self.xmlbases and self.xmlbases[-1]:
        self.baseuri = self.xmlbases[-1]

    if self.langs:
      self.langs.pop()
      if self.langs and self.langs[-1]:
        self.lang = self.langs[-1]

if __name__ == "__main__":
    store = ConjunctiveGraph()
    store.load(sys.argv[1], format="rdfa") 
    print store.serialize(format="pretty-xml")


########NEW FILE########
__FILENAME__ = RDFXMLHandler
# Copyright (c) 2002, Daniel Krech, http://eikeon.com/
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are
# met:
#
#   * Redistributions of source code must retain the above copyright
# notice, this list of conditions and the following disclaimer.
#
#   * Redistributions in binary form must reproduce the above
# copyright notice, this list of conditions and the following
# disclaimer in the documentation and/or other materials provided
# with the distribution.
#
#   * Neither the name of Daniel Krech nor the names of its
# contributors may be used to endorse or promote products derived
# from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

"""
"""
from rdflib import RDF, RDFS
from rdflib import URIRef, BNode, Literal
from rdflib.exceptions import ParserError, Error
from rdflib.syntax.xml_names import is_ncname

from xml.sax.saxutils import handler, quoteattr, escape
from urlparse import urljoin, urldefrag

RDFNS = RDF.RDFNS

# http://www.w3.org/TR/rdf-syntax-grammar/#eventterm-attribute-URI
# A mapping from unqualified terms to there qualified version.
UNQUALIFIED = {"about" : RDF.about,
               "ID" : RDF.ID,
               "type" : RDF.type,
               "resource": RDF.resource,
               "parseType": RDF.parseType}

# http://www.w3.org/TR/rdf-syntax-grammar/#coreSyntaxTerms
CORE_SYNTAX_TERMS = [RDF.RDF, RDF.ID, RDF.about, RDF.parseType, RDF.resource, RDF.nodeID, RDF.datatype]

# http://www.w3.org/TR/rdf-syntax-grammar/#syntaxTerms
SYNTAX_TERMS = CORE_SYNTAX_TERMS + [RDF.Description, RDF.li]

# http://www.w3.org/TR/rdf-syntax-grammar/#oldTerms
OLD_TERMS = [RDFNS["aboutEach"], RDFNS["aboutEachPrefix"], RDFNS["bagID"]]

NODE_ELEMENT_EXCEPTIONS = CORE_SYNTAX_TERMS + [RDF.li,] + OLD_TERMS
NODE_ELEMENT_ATTRIBUTES = [RDF.ID, RDF.nodeID, RDF.about]

PROPERTY_ELEMENT_EXCEPTIONS = CORE_SYNTAX_TERMS + [RDF.Description,] + OLD_TERMS
PROPERTY_ATTRIBUTE_EXCEPTIONS = CORE_SYNTAX_TERMS + [RDF.Description, RDF.li] + OLD_TERMS
PROPERTY_ELEMENT_ATTRIBUTES = [RDF.ID, RDF.resource, RDF.nodeID]

XMLNS = "http://www.w3.org/XML/1998/namespace"
BASE = (XMLNS, "base")
LANG = (XMLNS, "lang")


class BagID(URIRef):
    __slots__ = ['li']
    def __init__(self, val):
        super(URIRef, self).__init__(val)
        self.li = 0

    def next_li(self):
        self.li += 1
        return URIRef(RDFNS + "_%s" % self.li)


class ElementHandler(object):
    __slots__ = ['start', 'char', 'end', 'li', 'id',
                 'base', 'subject', 'predicate', 'object',
                 'list', 'language', 'datatype', 'declared', 'data']
    def __init__(self):
        self.start = None
        self.char = None
        self.end = None
        self.li = 0
        self.id = None
        self.base = None
        self.subject = None
        self.object = None
        self.list = None
        self.language = None
        self.datatype = None
        self.declared = None
        self.data = None

    def next_li(self):
        self.li += 1
        return URIRef(RDFNS + "_%s" % self.li)


class RDFXMLHandler(handler.ContentHandler):

    def __init__(self, store):
        self.store = store
        self.preserve_bnode_ids = False
        self.reset()

    def reset(self):
        document_element = ElementHandler()
        document_element.start = self.document_element_start
        document_element.end = lambda name, qname: None
        self.stack = [None, document_element,]
        self.ids = {} # remember IDs we have already seen
        self.bnode = {}
        self._ns_contexts = [{}] # contains uri -> prefix dicts
        self._current_context = self._ns_contexts[-1]

    # ContentHandler methods

    def setDocumentLocator(self, locator):
        self.locator = locator

    def startDocument(self):
        pass

    def startPrefixMapping(self, prefix, namespace):
        self._ns_contexts.append(self._current_context.copy())
        self._current_context[namespace] = prefix
        self.store.bind(prefix, URIRef(namespace), override=False)

    def endPrefixMapping(self, prefix):
        self._current_context = self._ns_contexts[-1]
        del self._ns_contexts[-1]

    def startElementNS(self, name, qname, attrs):
        stack = self.stack
        stack.append(ElementHandler())
        current = self.current
        parent = self.parent
        base = attrs.get(BASE, None)
        if base is not None:
            base, frag = urldefrag(base)
        else:
            if parent:
                base = parent.base
            if base is None:
                systemId = self.locator.getPublicId() or self.locator.getSystemId()
                if systemId:
                    base, frag = urldefrag(systemId)
        current.base = base
        language = attrs.get(LANG, None)
        if language is None:
            if parent:
                language = parent.language
        current.language = language
        current.start(name, qname, attrs)

    def endElementNS(self, name, qname):
        self.current.end(name, qname)
        self.stack.pop()

    def characters(self, content):
        char = self.current.char
        if char:
            char(content)

    def ignorableWhitespace(self, content):
        pass

    def processingInstruction(self, target, data):
        pass

    def add_reified(self, sid, (s, p, o)):
        self.store.add((sid, RDF.type, RDF.Statement))
        self.store.add((sid, RDF.subject, s))
        self.store.add((sid, RDF.predicate, p))
        self.store.add((sid, RDF.object, o))

    def error(self, message):
        locator = self.locator
        info = "%s:%s:%s: " % (locator.getSystemId(),
                            locator.getLineNumber(), locator.getColumnNumber())
        raise ParserError(info + message)

    def get_current(self):
        return self.stack[-2]
    # Create a read only property called current so that self.current
    # give the current element handler.
    current = property(get_current)

    def get_next(self):
        return self.stack[-1]
    # Create a read only property that gives the element handler to be
    # used for the next element.
    next = property(get_next)

    def get_parent(self):
        return self.stack[-3]
    # Create a read only property that gives the current parent
    # element handler
    parent = property(get_parent)

    def absolutize(self, uri):
        result = urljoin(self.current.base, uri, allow_fragments=1)
        if uri and uri[-1]=="#" and result[-1]!="#":
            result = "%s#" % result
        return URIRef(result)

    def convert(self, name, qname, attrs):
        if name[0] is None:
            name = URIRef(name[1])
        else:
            name = URIRef("".join(name))
        atts = {}
        for (n, v) in attrs.items(): #attrs._attrs.iteritems(): #
            if n[0] is None:
                att = URIRef(n[1])
            else:
                att = URIRef("".join(n))
            if att.startswith(XMLNS) or att[0:3].lower()=="xml":
                pass
            elif att in UNQUALIFIED:
                #if not RDFNS[att] in atts:
                atts[RDFNS[att]] = v
            else:
                atts[URIRef(att)] = v
        return name, atts

    def document_element_start(self, name, qname, attrs):
        if name[0] and URIRef("".join(name)) == RDF.RDF:
            next = self.next
            next.start = self.node_element_start
            next.end = self.node_element_end
        else:
            self.node_element_start(name, qname, attrs)
            #self.current.end = self.node_element_end
            # TODO... set end to something that sets start such that
            # another element will cause error


    def node_element_start(self, name, qname, attrs):
        name, atts = self.convert(name, qname, attrs)
        current = self.current
        absolutize = self.absolutize
        next = self.next
        next.start = self.property_element_start
        next.end = self.property_element_end

        if name in NODE_ELEMENT_EXCEPTIONS:
            self.error("Invalid node element URI: %s" % name)

        if RDF.ID in atts:
            if RDF.about in atts or RDF.nodeID in atts:
                self.error("Can have at most one of rdf:ID, rdf:about, and rdf:nodeID")

            id = atts[RDF.ID]
            if not is_ncname(id):
                self.error("rdf:ID value is not a valid NCName: %s" % id)
            subject = absolutize("#%s" % id)
            if subject in self.ids:
                self.error("two elements cannot use the same ID: '%s'" % subject)
            self.ids[subject] = 1 # IDs can only appear once within a document
        elif RDF.nodeID in atts:
            if RDF.ID in atts or RDF.about in atts:
                self.error("Can have at most one of rdf:ID, rdf:about, and rdf:nodeID")
            nodeID = atts[RDF.nodeID]
            if not is_ncname(nodeID):
                self.error("rdf:nodeID value is not a valid NCName: %s" % nodeID)
            if self.preserve_bnode_ids is False:
                if nodeID in self.bnode:
                    subject = self.bnode[nodeID]
                else:
                    subject = BNode()
                    self.bnode[nodeID] = subject
            else:
                subject = BNode(nodeID)
        elif RDF.about in atts:
            if RDF.ID in atts or RDF.nodeID in atts:
                self.error("Can have at most one of rdf:ID, rdf:about, and rdf:nodeID")
            subject = absolutize(atts[RDF.about])
        else:
            subject = BNode()

        if name!=RDF.Description: # S1
            self.store.add((subject, RDF.type, absolutize(name)))

        language = current.language
        for att in atts:
            if not att.startswith(RDFNS):
                predicate = absolutize(att)
                try:
                    object = Literal(atts[att], language)
                except Error, e:
                    self.error(e.msg)
            elif att==RDF.type: #S2
                predicate = RDF.type
                object = absolutize(atts[RDF.type])
            elif att in NODE_ELEMENT_ATTRIBUTES:
                continue
            elif att in PROPERTY_ATTRIBUTE_EXCEPTIONS: #S3
                self.error("Invalid property attribute URI: %s" % att)
                continue # for when error does not throw an exception
            else:
                predicate = absolutize(att)
                try:
                    object = Literal(atts[att], language)
                except Error, e:
                    self.error(e.msg)
            self.store.add((subject, predicate, object))

        current.subject = subject


    def node_element_end(self, name, qname):
        self.parent.object = self.current.subject

    def property_element_start(self, name, qname, attrs):
        name, atts = self.convert(name, qname, attrs)
        current = self.current
        absolutize = self.absolutize
        next = self.next
        object = None
        current.data = None
        current.list = None

        if not name.startswith(RDFNS):
            current.predicate = absolutize(name)
        elif name==RDF.li:
            current.predicate = current.next_li()
        elif name in PROPERTY_ELEMENT_EXCEPTIONS:
            self.error("Invalid property element URI: %s" % name)
        else:
            current.predicate = absolutize(name)

        id = atts.get(RDF.ID, None)
        if id is not None:
            if not is_ncname(id):
                self.error("rdf:ID value is not a value NCName: %s" % id)
            current.id = absolutize("#%s" % id)
        else:
            current.id = None

        resource = atts.get(RDF.resource, None)
        nodeID = atts.get(RDF.nodeID, None)
        parse_type = atts.get(RDF.parseType, None)
        if resource is not None and nodeID is not None:
            self.error("Property element cannot have both rdf:nodeID and rdf:resource")
        if resource is not None:
            object = absolutize(resource)
            next.start = self.node_element_start
            next.end = self.node_element_end
        elif nodeID is not None:
            if not is_ncname(nodeID):
                self.error("rdf:nodeID value is not a valid NCName: %s" % nodeID)
            if self.preserve_bnode_ids is False:
                if nodeID in self.bnode:
                    object = self.bnode[nodeID]
                else:
                    subject = BNode()
                    self.bnode[nodeID] = subject
                    object = subject
            else:
                object = subject = BNode(nodeID)
            next.start = self.node_element_start
            next.end = self.node_element_end
        else:
            if parse_type is not None:
                for att in atts:
                    if att!=RDF.parseType and att!=RDF.ID:
                        self.error("Property attr '%s' now allowed here" % att)
                if parse_type=="Resource":
                    current.subject = object = BNode()
                    current.char = self.property_element_char
                    next.start = self.property_element_start
                    next.end = self.property_element_end
                elif parse_type=="Collection":
                    current.char = None
                    object = current.list = RDF.nil #BNode()#self.parent.subject
                    next.start = self.node_element_start
                    next.end = self.list_node_element_end
                else: #if parse_type=="Literal":
                     # All other values are treated as Literal
                     # See: http://www.w3.org/TR/rdf-syntax-grammar/#parseTypeOtherPropertyElt
                    object = Literal("", None, RDF.XMLLiteral)
                    current.char = self.literal_element_char
                    current.declared = {}
                    next.start = self.literal_element_start
                    next.char = self.literal_element_char
                    next.end = self.literal_element_end
                current.object = object
                return
            else:
                object = None
                current.char = self.property_element_char
                next.start = self.node_element_start
                next.end = self.node_element_end

        datatype = current.datatype = atts.get(RDF.datatype, None)
        language = current.language
        if datatype is not None:
            # TODO: check that there are no atts other than datatype and id
            datatype = absolutize(datatype)
        else:
            for att in atts:
                if not att.startswith(RDFNS):
                    predicate = absolutize(att)
                elif att in PROPERTY_ELEMENT_ATTRIBUTES:
                    continue
                elif att in PROPERTY_ATTRIBUTE_EXCEPTIONS:
                    self.error("""Invalid property attribute URI: %s""" % att)
                else:
                    predicate = absolutize(att)

                if att==RDF.type:
                    o = URIRef(atts[att])
                else:
                    o = Literal(atts[att], language, datatype)

                if object is None:
                    object = BNode()
                self.store.add((object, predicate, o))
        if object is None:
            current.data = ""
            current.object = None
        else:
            current.data = None
            current.object = object

    def property_element_char(self, data):
        current = self.current
        if current.data is not None:
            current.data += data

    def property_element_end(self, name, qname):
        current = self.current
        if current.data is not None and current.object is None:
            current.object = Literal(current.data, current.language, current.datatype)
            current.data = None
        if self.next.end==self.list_node_element_end:
            if current.object!=RDF.nil:
                self.store.add((current.list, RDF.rest, RDF.nil))
        if current.object is not None:
            self.store.add((self.parent.subject, current.predicate, current.object))
            if current.id is not None:
                self.add_reified(current.id, (self.parent.subject,
                                 current.predicate, current.object))
        current.subject = None

    def list_node_element_end(self, name, qname):
        current = self.current
        if self.parent.list==RDF.nil:
            list = BNode()
            # Removed between 20030123 and 20030905
            #self.store.add((list, RDF.type, LIST))
            self.parent.list = list
            self.store.add((self.parent.list, RDF.first, current.subject))
            self.parent.object = list
            self.parent.char = None
        else:
            list = BNode()
            # Removed between 20030123 and 20030905
            #self.store.add((list, RDF.type, LIST))
            self.store.add((self.parent.list, RDF.rest, list))
            self.store.add((list, RDF.first, current.subject))
            self.parent.list = list

    def literal_element_start(self, name, qname, attrs):
        current = self.current
        self.next.start = self.literal_element_start
        self.next.char = self.literal_element_char
        self.next.end = self.literal_element_end
        current.declared = self.parent.declared.copy()
        if name[0]:
            prefix = self._current_context[name[0]]
            if prefix:
                current.object = "<%s:%s" % (prefix, name[1])
            else:
                current.object = "<%s" % name[1]
            if not name[0] in current.declared:
                current.declared[name[0]] = prefix
                if prefix:
                    current.object += (' xmlns:%s="%s"' % (prefix, name[0]))
                else:
                    current.object += (' xmlns="%s"' % name[0])
        else:
            current.object = "<%s" % name[1]

        for (name, value) in attrs.items():
            if name[0]:
                if not name[0] in current.declared:
                    current.declared[name[0]] = self._current_context[name[0]]
                name = current.declared[name[0]] + ":" + name[1]
            else:
                name = name[1]
            current.object += (' %s=%s' % (name, quoteattr(value)))
        current.object += ">"

    def literal_element_char(self, data):
        self.current.object += escape(data)

    def literal_element_end(self, name, qname):
        if name[0]:
            prefix = self._current_context[name[0]]
            if prefix:
                end = u"</%s:%s>" % (prefix, name[1])
            else:
                end = u"</%s>" % name[1]
        else:
            end = u"</%s>" % name[1]
        self.parent.object += self.current.object + end

########NEW FILE########
__FILENAME__ = RDFXMLParser
from rdflib.syntax.parsers import Parser

from xml.sax import make_parser
from xml.sax.saxutils import handler
from xml.sax.handler import ErrorHandler

from rdflib.syntax.parsers.RDFXMLHandler import RDFXMLHandler


def create_parser(store):
    parser = make_parser()
    # Workaround for bug in expatreader.py. Needed when
    # expatreader is trying to guess a prefix.
    parser.start_namespace_decl("xml", "http://www.w3.org/XML/1998/namespace")
    parser.setFeature(handler.feature_namespaces, 1)
    rdfxml = RDFXMLHandler(store)
    #rdfxml.setDocumentLocator(_Locator(self.url, self.parser))
    parser.setContentHandler(rdfxml)
    parser.setErrorHandler(ErrorHandler())
    return parser


class RDFXMLParser(Parser):

    def __init__(self):
        pass

    def parse(self, source, sink, **args):
        self._parser = create_parser(sink)
        content_handler = self._parser.getContentHandler()
        preserve_bnode_ids = args.get("preserve_bnode_ids", None)
        if preserve_bnode_ids is not None:
            content_handler.preserve_bnode_ids = preserve_bnode_ids
        # We're only using it once now
        #content_handler.reset()
        #self._parser.reset()
        self._parser.parse(source)




########NEW FILE########
__FILENAME__ = TriXHandler
# Copyright (c) 2002, Daniel Krech, http://eikeon.com/
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are
# met:
#
#   * Redistributions of source code must retain the above copyright
# notice, this list of conditions and the following disclaimer.
#
#   * Redistributions in binary form must reproduce the above
# copyright notice, this list of conditions and the following
# disclaimer in the documentation and/or other materials provided
# with the distribution.
#
#   * Neither the name of Daniel Krech nor the names of its
# contributors may be used to endorse or promote products derived
# from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

"""
"""
from rdflib import RDF, RDFS, Namespace
from rdflib import URIRef, BNode, Literal
from rdflib.Graph import Graph
from rdflib.exceptions import ParserError, Error
from rdflib.syntax.xml_names import is_ncname

from xml.sax.saxutils import handler, quoteattr, escape
from urlparse import urljoin, urldefrag

RDFNS = RDF.RDFNS

TRIXNS=Namespace("http://www.w3.org/2004/03/trix/trix-1/")


class TriXHandler(handler.ContentHandler):
    """An Sax Handler for TriX. See http://swdev.nokia.com/trix/TriX.html"""

    def __init__(self, store):
        self.store = store
        self.preserve_bnode_ids = False
        self.reset()

    def reset(self):
        self.bnode = {}
        self.graph=self.store
        self.triple=None
        self.state=0
        self.lang=None
        self.datatype=None

    # ContentHandler methods

    def setDocumentLocator(self, locator):
        self.locator = locator

    def startDocument(self):
        pass

    def startPrefixMapping(self, prefix, namespace):
        pass

    def endPrefixMapping(self, prefix):
        pass

    def startElementNS(self, name, qname, attrs):
    
        if name[0]!=TRIXNS:
            self.error("Only elements in the TriX namespace are allowed.")

        if name[1]=="TriX":
            if self.state==0:
                self.state=1
            else:
                self.error("Unexpected TriX element")

        elif name[1]=="graph":
            if self.state==1:
                self.state=2
            else:
                self.error("Unexpected graph element")

        elif name[1]=="uri":
            if self.state==2:
                # the context uri
                self.state=3
            elif self.state==4:
                # part of a triple
                pass
            else:
                self.error("Unexpected uri element")

        elif name[1]=="triple":
            if self.state==2:
                # start of a triple
                self.triple=[]
                self.state=4
            else:
                self.error("Unexpected triple element")

        elif name[1]=="typedLiteral":
            if self.state==4:
                # part of triple
                self.lang=None
                self.datatype=None

                try:
                    self.lang=attrs.getValueByQName("lang")
                except:
                    # language not required - ignore
                    pass
                try: 
                    self.datatype=attrs.getValueByQName("datatype")
                except KeyError:
                    self.error("No required attribute 'datatype'")
            else:
                self.error("Unexpected typedLiteral element")
                
        elif name[1]=="plainLiteral":
            if self.state==4:
                # part of triple
                self.lang=None
                self.datatype=None
                try:
                    self.lang=attrs.getValueByQName("lang")
                except:
                    # language not required - ignore
                    pass

            else:
                self.error("Unexpected plainLiteral element")

        elif name[1]=="id":
            if self.state==2:
                # the context uri
                self.state=3

            elif self.state==4:
                # part of triple
                pass
            else:
                self.error("Unexpected id element")
        
        else:
            self.error("Unknown element %s in TriX namespace"%name[1])

        self.chars=""

    
    def endElementNS(self, name, qname):
        if name[0]!=TRIXNS:
            self.error("Only elements in the TriX namespace are allowed.")

        if name[1]=="uri":
            if self.state==3:
                self.graph=Graph(store=self.store.store, identifier=URIRef(self.chars.strip()))
                self.state=2
            elif self.state==4:
                self.triple+=[URIRef(self.chars.strip())]
            else:
                self.error("Illegal internal self.state - This should never happen if the SAX parser ensures XML syntax correctness")

        if name[1]=="id":
            if self.state==3:
                self.graph=Graph(self.store.store,identifier=self.get_bnode(self.chars.strip()))
                self.state=2
            elif self.state==4:
                self.triple+=[self.get_bnode(self.chars.strip())]
            else:
                self.error("Illegal internal self.state - This should never happen if the SAX parser ensures XML syntax correctness")

        if name[1]=="plainLiteral" or name[1]=="typedLiteral":
            if self.state==4:
                self.triple+=[Literal(self.chars, lang=self.lang, datatype=self.datatype)]
            else:
                self.error("This should never happen if the SAX parser ensures XML syntax correctness")

        if name[1]=="triple":
            if self.state==4:
                if len(self.triple)!=3:
                    self.error("Triple has wrong length, got %d elements: %s"%(len(self.triple),self.triple))

                self.graph.add(self.triple)
                #self.store.store.add(self.triple,context=self.graph)
                #self.store.addN([self.triple+[self.graph]])
                self.state=2
            else:
                self.error("This should never happen if the SAX parser ensures XML syntax correctness")

        if name[1]=="graph":
            self.state=1

        if name[1]=="TriX":
            self.state=0


    def get_bnode(self,label):
        if self.preserve_bnode_ids:
            bn=BNode(label)
        else:
            if label in self.bnode:
                bn=self.bnode[label]
            else: 
                bn=BNode(label)
                self.bnode[label]=bn
        return bn
                

    def characters(self, content):
        self.chars+=content

    
    def ignorableWhitespace(self, content):
        pass

    def processingInstruction(self, target, data):
        pass

    
    def error(self, message):
        locator = self.locator
        info = "%s:%s:%s: " % (locator.getSystemId(),
                            locator.getLineNumber(), locator.getColumnNumber())
        raise ParserError(info + message)

########NEW FILE########
__FILENAME__ = TriXParser
from rdflib.syntax.parsers import Parser
from rdflib.Graph import ConjunctiveGraph

from xml.sax import make_parser
from xml.sax.saxutils import handler
from xml.sax.handler import ErrorHandler

from rdflib.syntax.parsers.TriXHandler import TriXHandler


def create_parser(store):
    parser = make_parser()
    # Workaround for bug in expatreader.py. Needed when
    # expatreader is trying to guess a prefix.
    parser.start_namespace_decl("xml", "http://www.w3.org/XML/1998/namespace")
    parser.setFeature(handler.feature_namespaces, 1)
    trix = TriXHandler(store)
    parser.setContentHandler(trix)
    parser.setErrorHandler(ErrorHandler())
    return parser


class TriXParser(Parser):
    """A parser for TriX. See http://swdev.nokia.com/trix/TriX.html"""

    def __init__(self):
        pass

    def parse(self, source, sink, **args):
        assert sink.store.context_aware
        g=ConjunctiveGraph(store=sink.store)
        
        self._parser = create_parser(g)
        content_handler = self._parser.getContentHandler()
        preserve_bnode_ids = args.get("preserve_bnode_ids", None)
        if preserve_bnode_ids is not None:
            content_handler.preserve_bnode_ids = preserve_bnode_ids
        # We're only using it once now
        #content_handler.reset()
        #self._parser.reset()
        self._parser.parse(source)




########NEW FILE########
__FILENAME__ = serializer
import tempfile, shutil, os
from threading import Lock
from urlparse import urlparse

try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO


class Serializer(object):

    def __init__(self, serializer):
        self.serializer = serializer
        self.__save_lock = Lock()

    def _get_store(self):
        return self.serializer.store

    def _set_store(self, store):
        self.serializer.store = store

    store = property(_get_store, _set_store)

    def serialize(self, destination=None, format="xml", base=None, encoding=None, **args):
        if destination is None:
            stream = StringIO()
            self.serializer.serialize(stream, base=base, encoding=encoding)
            return stream.getvalue()
        if hasattr(destination, "write"):
            stream = destination
            self.serializer.serialize(stream, base=base, encoding=encoding)
        else:
            location = destination
            try:
                self.__save_lock.acquire()
                scheme, netloc, path, params, query, fragment = urlparse(location)
                if netloc!="":
                    print "WARNING: not saving as location is not a local file reference"
                    return
                name = tempfile.mktemp()
                stream = open(name, 'wb')
                self.serializer.serialize(stream, base=base, encoding=encoding, **args)
                stream.close()
                if hasattr(shutil,"move"):
                    shutil.move(name, path)
                else:
                    shutil.copy(name, path)
                    os.remove(name)
            finally:
                self.__save_lock.release()

########NEW FILE########
__FILENAME__ = AbstractSerializer
from rdflib import URIRef

class AbstractSerializer(object):

    def __init__(self, store):
        self.store = store
        self.encoding = "UTF-8"
        self.base = None

    def serialize(self, stream, base=None, encoding=None, **args):
        """Abstract method"""

    def relativize(self, uri):
        base = self.base
        if base is not None and uri.startswith(base):
            uri = URIRef(uri.replace(base, "", 1))
        return uri


########NEW FILE########
__FILENAME__ = N3Serializer
# rdflib/syntax/serializers/N3Serializer.py

from rdflib.syntax.serializers.TurtleSerializer import TurtleSerializer, SUBJECT, VERB, OBJECT
from rdflib.Graph import Graph

class N3Serializer(TurtleSerializer):
    short_name = "n3"

    def __init__(self, store, parent=None):
        super(N3Serializer, self).__init__(store)
        self.parent = parent


    def reset(self):
        super(N3Serializer, self).reset()
        self._stores = {}
        
    def getQName(self, uri):
        qname = None
        if self.parent is not None:
            qname = self.parent.getQName(uri)
        if qname is None:
            qname = super(N3Serializer, self).getQName(uri)
        return qname
        
    def indent(self, modifier=0):
        indent = super(N3Serializer, self).indent(modifier)
        if self.parent is not None:
            indent += self.parent.indent(modifier)
        return indent
    

    def p_clause(self, node, ignore=SUBJECT):
        if isinstance(node, Graph):
            self.subjectDone(node)
            self.write(' {')
            self.depth += 1
            serializer = N3Serializer(node, parent=self)
            serializer.serialize(self.stream)
            self.depth -= 1
            self.write('\n'+self.indent()+' }')
            return True
        else:
            return False

    def s_clause(self, subject):
        if isinstance(subject, Graph):
            self.write('\n'+self.indent())
            self.p_clause(subject, SUBJECT)
            self.predicateList(subject)
            self.write('. ')
            return True
        else:
            return False
    
    def statement(self, subject):
        self.subjectDone(subject)
        properties = self.buildPredicateHash(subject)
        if len(properties) == 0:
            return
        
        if not self.s_clause(subject):
            super(N3Serializer, self).statement(subject)
            
    def path(self, node, position):
        if not self.p_clause(node, position):
            super(N3Serializer, self).path(node, position)
            
    def startDocument(self):
        ns_list= list(self.namespaces.items())
        ns_list.sort()
                
        for prefix, uri in ns_list:
            self.write('\n'+self.indent()+'@prefix %s: <%s>.'%(prefix, uri))

        if len(ns_list) > 0:
            self.write('\n')
        #if not isinstance(self.store, N3Store):
        #    return
        
        #all_list = [self.label(var) for var in self.store.get_universals(recurse=False)]
        #all_list.sort()
        #some_list = [self.label(var) for var in self.store.get_existentials(recurse=False)]
        #some_list.sort()
        
        
        #for var in all_list:
        #    self.write('\n'+self.indent()+'@forAll %s. '%var)
        #for var in some_list:
        #    self.write('\n'+self.indent()+'@forSome %s. '%var)
        
            
        #if (len(all_list) + len(some_list)) > 0:
        #    self.write('\n')

########NEW FILE########
__FILENAME__ = NTSerializer
#$Id: NTSerializer.py,v 1.6 2003/10/29 15:25:24 kendall Exp $

from rdflib.syntax.serializers import Serializer

class NTSerializer(Serializer):

    def __init__(self, store):
        """
        I serialize RDF graphs in NTriples format.
        """
        super(NTSerializer, self).__init__(store)

    def serialize(self, stream, base=None, encoding=None, **args):
        if base is not None:
            print "TODO: NTSerializer does not support base"
        encoding = self.encoding
        write = lambda triple: stream.write((triple[0].n3() + u" " + \
                                             triple[1].n3() + u" " + triple[2].n3() + u".\n").encode(encoding, "replace"))
        map(write, self.store)

########NEW FILE########
__FILENAME__ = PrettyXMLSerializer
from rdflib import RDF

from rdflib import URIRef, Literal, BNode
from rdflib.util import first, uniq, more_than

from rdflib.syntax.serializers import Serializer
from rdflib.syntax.serializers.XMLWriter import XMLWriter

XMLLANG = "http://www.w3.org/XML/1998/namespacelang"


# TODO:
def fix(val):
    "strip off _: from nodeIDs... as they are not valid NCNames"
    if val.startswith("_:"):
        return val[2:]
    else:
        return val


class PrettyXMLSerializer(Serializer):

    def __init__(self, store, max_depth=3):
        super(PrettyXMLSerializer, self).__init__(store)

    def serialize(self, stream, base=None, encoding=None, **args):
        self.__serialized = {}
        store = self.store
        self.base = base
        self.max_depth = args.get("max_depth", 3)
        assert self.max_depth>0, "max_depth must be greater than 0"

        self.nm = nm = store.namespace_manager
        self.writer = writer = XMLWriter(stream, nm, encoding)

        namespaces = {}
        possible = uniq(store.predicates()) + uniq(store.objects(None, RDF.type))
        for predicate in possible:
            prefix, namespace, local = nm.compute_qname(predicate)
            namespaces[prefix] = namespace
        namespaces["rdf"] = "http://www.w3.org/1999/02/22-rdf-syntax-ns#"
        writer.push(RDF.RDF)
        writer.namespaces(namespaces.iteritems())

        # Write out subjects that can not be inline
        for subject in store.subjects():
            if (None, None, subject) in store:
                if (subject, None, subject) in store:
                    self.subject(subject, 1)
            else:
                self.subject(subject, 1)

        # write out anything that has not yet been reached
        for subject in store.subjects():
            self.subject(subject, 1)

        writer.pop(RDF.RDF)

        # Set to None so that the memory can get garbage collected.
        self.__serialized = None


    def subject(self, subject, depth=1):
        store = self.store
        writer = self.writer
        if not subject in self.__serialized:
            self.__serialized[subject] = 1
            type = first(store.objects(subject, RDF.type))
            try:
                self.nm.qname(type)
            except:
                type = None
            element = type or RDF.Description
            writer.push(element)
            if isinstance(subject, BNode):
                def subj_as_obj_more_than(ceil):
                    return more_than(store.triples((None, None, subject)), ceil)
                if (depth == 1 and subj_as_obj_more_than(0)
                        ) or subj_as_obj_more_than(1):
                    writer.attribute(RDF.nodeID, fix(subject))
            else:
                writer.attribute(RDF.about, self.relativize(subject))
            if (subject, None, None) in store:
                for predicate, object in store.predicate_objects(subject):
                    if not (predicate==RDF.type and object==type):
                        self.predicate(predicate, object, depth+1)
            writer.pop(element)

    def predicate(self, predicate, object, depth=1):
        writer = self.writer
        store = self.store
        writer.push(predicate)
        if isinstance(object, Literal):
            attributes = ""
            if object.language:
                writer.attribute(XMLLANG, object.language)
            if object.datatype:
                writer.attribute(RDF.datatype, object.datatype)
            writer.text(object)
        elif object in self.__serialized or not (object, None, None) in store:
            if isinstance(object, BNode):
                if more_than(store.triples((None, None, object)), 0):
                    writer.attribute(RDF.nodeID, fix(object))
            else:
                writer.attribute(RDF.resource, self.relativize(object))
        else:
            items = []
            for item in store.items(object): # add a strict option to items?
                if isinstance(item, Literal):
                    items = None # can not serialize list with literal values in them with rdf/xml
                else:
                    items.append(item)

            if first(store.objects(object, RDF.first)): # may not have type RDF.List
                collection = object
                self.__serialized[object] = 1
                # TODO: warn that any assertions on object other than
                # RDF.first and RDF.rest are ignored... including RDF.List
                writer.attribute(RDF.parseType, "Collection")
                while collection:
                    item = first(store.objects(collection, RDF.first))
                    if item:
                        self.subject(item)
                    collection = first(store.objects(collection, RDF.rest))
                    self.__serialized[collection] = 1
            else:
                if depth<=self.max_depth:
                    self.subject(object, depth+1)
                elif isinstance(object, BNode):
                    writer.attribute(RDF.nodeID, fix(object))
                else:
                    writer.attribute(RDF.resource, self.relativize(object))
        writer.pop(predicate)


########NEW FILE########
__FILENAME__ = QNameProvider

from rdflib.syntax.xml_names import split_uri

XMLLANG = u"http://www.w3.org/XML/1998/namespace#lang"


class QNameProvider(object):
    def __init__(self):
        self.__cache = {}
        self.__namespace = {} # mapping for prefix to namespace
        self.__prefix = {}
        self.set_prefix("xml", u"http://www.w3.org/XML/1998/namespace")
        # TODO: explain -- the following is needed for XMLLANG as defined above to work
        self.__prefix[u"http://www.w3.org/XML/1998/namespace#"] = "xml"

    def get(self, uri):
        qname = self.__cache.get(uri, None)
        if qname is None:
            self.compute(uri)
            return self.get(uri)
        else:
            return qname

    def compute(self, uri):
        if not uri in self.__cache:
            namespace, name = split_uri(uri)
            prefix = self.__prefix.get(namespace, None)
            if prefix is None:
                prefix = "_%s" % len(self.__namespace)
                self.set_prefix(prefix, namespace)
            if prefix=="":
                self.__cache[uri] = name
            else:
                self.__cache[uri] = ":".join((prefix, name))

    def set_prefix(self, prefix, namespace):
        if prefix in self.__namespace:
            raise "NYI: prefix already set"
        self.__namespace[prefix] = namespace
        self.__prefix[namespace] = prefix

    def namespaces(self):
        for prefix, namespace in self.__namespace.iteritems():
            yield prefix, namespace



########NEW FILE########
__FILENAME__ = RecursiveSerializer
from rdflib.BNode import BNode
from rdflib.Literal import Literal
from rdflib.URIRef import URIRef

from rdflib.syntax.serializers.AbstractSerializer import AbstractSerializer

from rdflib import RDF, RDFS


class RecursiveSerializer(AbstractSerializer):

    topClasses = [RDFS.Class]
    predicateOrder = [RDF.type, RDFS.label]
    maxDepth = 10
    indentString = u"  "
    
    def __init__(self, store):

        super(RecursiveSerializer, self).__init__(store)
        self.stream = None
        self.reset()

    def addNamespace(self, prefix, uri):
        self.namespaces[prefix] = uri
        
    def checkSubject(self, subject):
        """Check to see if the subject should be serialized yet"""
        if ((self.isDone(subject))
            or (subject not in self._subjects)
            or ((subject in self._topLevels) and (self.depth > 1))
            or (isinstance(subject, URIRef) and (self.depth >= self.maxDepth))
            ):
            return False
        return True

    def isDone(self, subject):
        """Return true if subject is serialized"""
        return subject in self._serialized
    
    def orderSubjects(self):
        seen = {}
        subjects = []

        for classURI in self.topClasses:
            members = list(self.store.subjects(RDF.type, classURI))
            members.sort()
            
            for member in members:
                subjects.append(member)
                self._topLevels[member] = True
                seen[member] = True

        recursable = [(isinstance(subject,BNode), self.refCount(subject), subject) for subject in self._subjects
                      if subject not in seen]

        recursable.sort()
        subjects.extend([subject for (isbnode, refs, subject) in recursable])
                
        return subjects
    
    def preprocess(self):
        for triple in self.store.triples((None,None,None)):
            self.preprocessTriple(triple)

    def preprocessTriple(self, (s,p,o)):
        references = self.refCount(o) + 1
        self._references[o] = references
        self._subjects[s] = True

    def refCount(self, node):
        """Return the number of times this node has been referenced in the object position"""
        return self._references.get(node, 0)
    
    def reset(self):
        self.depth = 0
        self.lists = {}
        self.namespaces = {}
        self._references = {}
        self._serialized = {}
        self._subjects = {}
        self._topLevels = {}

    def buildPredicateHash(self, subject):
        """Build a hash key by predicate to a list of objects for the given subject"""
        properties = {}
        for s,p,o in self.store.triples((subject, None, None)):
            oList = properties.get(p, [])
            oList.append(o)
            properties[p] = oList
        return properties
            
    def sortProperties(self, properties):
        """Take a hash from predicate uris to lists of values.
           Sort the lists of values.  Return a sorted list of properties."""
        # Sort object lists
        for prop, objects in properties.items():
            objects.sort()

        # Make sorted list of properties
        propList = []
        seen = {}
        for prop in self.predicateOrder:
            if (prop in properties) and (prop not in seen):
                propList.append(prop)
                seen[prop] = True
        props = properties.keys()
        props.sort()
        for prop in props:
            if prop not in seen:
                propList.append(prop)
                seen[prop] = True
        return propList

    def subjectDone(self, subject):
        """Mark a subject as done."""
        self._serialized[subject] = True

    def indent(self, modifier=0):
        """Returns indent string multiplied by the depth"""
        return (self.depth+modifier)*self.indentString
        
    def write(self, text):
        """Write text in given encoding."""
        self.stream.write(text.encode(self.encoding, 'replace'))

    

########NEW FILE########
__FILENAME__ = TurtleSerializer
import urlparse
from xml.sax.saxutils import escape, quoteattr

from rdflib.BNode import BNode
from rdflib.Literal import Literal
from rdflib.URIRef import URIRef
from rdflib.syntax.xml_names import split_uri 

from rdflib.syntax.serializers.RecursiveSerializer import RecursiveSerializer
from rdflib.exceptions import Error

from rdflib import RDF, RDFS

SUBJECT = 0
VERB = 1
OBJECT = 2



class TurtleSerializer(RecursiveSerializer):

    short_name="turtle"
    indentString = "    "
    def __init__(self, store):
        super(TurtleSerializer, self).__init__(store)
        self.reset()
        self.stream = None

    def reset(self):
        super(TurtleSerializer, self).reset()
        self._shortNames = {}
        self._started = False
    
    def getQName(self, uri):
        if isinstance(uri, URIRef):
            try:
                parts = self.store.compute_qname(uri)
            except Exception, e:
                parts = None
            if parts:
                
                prefix, namespace, local = parts
                if local.find(".")!=-1:
                    # Local parts with . will mess up serialization
                    return None
                
                self.addNamespace(prefix, namespace)
                return u"%s:%s" % (prefix, local)
        return None

    def preprocessTriple(self, triple):
        super(TurtleSerializer, self).preprocessTriple(triple)
        for node in triple:
            self.getQName(node)
        p = triple[1]
        if isinstance(p, BNode):
            self._references[p] = self.refCount(p) +1
            
    def label(self, node):
        qname = self.getQName(node)
        if qname is None:
            return node.n3()
        return qname

    def startDocument(self):
        self._started = True
        ns_list= list(self.store.namespaces())
        ns_list.sort()
        if len(ns_list) == 0:
            return
        
        for prefix, uri in ns_list:
            self.write('\n'+self.indent()+'@prefix %s: %s.'%(prefix, uri))
        self.write('\n')

    def endDocument(self):
        pass

    def isValidList(self,l): 
        """Checks if l is a valid RDF list, i.e. no nodes have other properties."""
        try:
            if not self.store.value(l, RDF.first):
                return False
        except: 
            return False
        while l:
            if l!=RDF.nil and len(list(self.store.predicate_objects(l)))!=2: return False
            l = self.store.value(l, RDF.rest)
        return True
        
    def doList(self,l):
        while l:
            item = self.store.value(l, RDF.first)
            if item:
                self.path(item, SUBJECT)
                self.subjectDone(l)
            l = self.store.value(l, RDF.rest)
            
    def p_squared(self, node, position):
        if (not isinstance(node, BNode)
            or node in self._serialized
            or self.refCount(node) > 1
            or position == SUBJECT):
            return False
       
        if self.isValidList(node): 
            # this is a list
            self.write(' (')
            self.depth+=2
            self.doList(node)
            self.depth-=2            
            self.write(' )')
            return True
        
        self.subjectDone(node)
        self.write(' [')
        self.depth += 2
        self.predicateList(node)
        self.depth -= 2
        self.write(']')
        return True

    def p_default(self, node, ignore):
        self.write(" "+self.label(node))
        return True
    
    def path(self, node, position):
        if not (self.p_squared(node, position)
                or self.p_default(node, position)):
            raise Error("Cannot serialize node '%s'"%(node, ))

    def verb(self, node):
        if node == RDF.type:
            self.write(' a')
        else:
            self.path(node, VERB)
    
    def objectList(self, objects):
        if len(objects) == 0:
            return

        self.path(objects[0], OBJECT)
        for obj in objects[1:]:
            self.write(',\n'+self.indent(2))
            self.path(obj, OBJECT)

    def predicateList(self, subject):
        properties = self.buildPredicateHash(subject)
        propList = self.sortProperties(properties)
        if len(propList) == 0:
            return

        self.verb(propList[0])
        self.objectList(properties[propList[0]])
        for predicate in propList[1:]:
            self.write(';\n'+self.indent(1))
            self.verb(predicate)
            self.objectList(properties[predicate])

    def s_squared(self, subject):
        if (self.refCount(subject) > 0) or not isinstance(subject, BNode):
            return False
        self.write('\n'+self.indent()+" [")
        self.depth+=1
        self.predicateList(subject)
        self.depth-=1
        self.write('].')
        return True

    def s_default(self, subject):
        self.write('\n'+self.indent())
        self.path(subject, SUBJECT)
        self.predicateList(subject)
        self.write('. ')
        return True
    
    def statement(self, subject):
        self.subjectDone(subject)
        if not self.s_squared(subject):
            self.s_default(subject)
            

    def serialize(self, stream, base=None, encoding=None, **args):
        self.reset()
        self.stream = stream
        self.base=base
        
        # In newer rdflibs these are always in the namespace manager
        #self.store.prefix_mapping('rdf', RDFNS)
        #self.store.prefix_mapping('rdfs', RDFSNS)
        
        self.preprocess()
        subjects_list = self.orderSubjects()

        self.startDocument()

        firstTime = True
        for subject in subjects_list:
            if not self.isDone(subject):
                if firstTime:
                    firstTime = False
                else:
                    self.write('\n')
                self.statement(subject)
        
        self.endDocument()

########NEW FILE########
__FILENAME__ = XMLSerializer
from __future__ import generators

from rdflib.syntax.serializers import Serializer

from rdflib.URIRef import URIRef
from rdflib.Literal import Literal
from rdflib.BNode import BNode

from rdflib.util import uniq
from rdflib.exceptions import Error
from rdflib.syntax.xml_names import split_uri

from xml.sax.saxutils import quoteattr, escape


class XMLSerializer(Serializer):

    def __init__(self, store):
        super(XMLSerializer, self).__init__(store)

    def __bindings(self):
        store = self.store
        nm = store.namespace_manager
        bindings = {}
        for predicate in uniq(store.predicates()):
            prefix, namespace, name = nm.compute_qname(predicate)
            bindings[prefix] = URIRef(namespace)
        RDFNS = URIRef("http://www.w3.org/1999/02/22-rdf-syntax-ns#")
        if "rdf" in bindings:
            assert bindings["rdf"]==RDFNS
        else:
            bindings["rdf"] = RDFNS
        for prefix, namespace in bindings.iteritems():
            yield prefix, namespace


    def serialize(self, stream, base=None, encoding=None, **args):
        self.base = base
        self.__stream = stream
        self.__serialized = {}
        encoding = self.encoding
        self.write = write = lambda uni: stream.write(uni.encode(encoding, 'replace'))

        # startDocument
        write('<?xml version="1.0" encoding="%s"?>\n' % self.encoding)

        # startRDF
        write('<rdf:RDF\n')
        # TODO: assert(namespaces["http://www.w3.org/1999/02/22-rdf-syntax-ns#"]=='rdf')
        bindings = list(self.__bindings())
        bindings.sort()
        for prefix, namespace in bindings:
            if prefix:
                write('   xmlns:%s="%s"\n' % (prefix, namespace))
            else:
                write('   xmlns="%s"\n' % namespace)
        write('>\n')

        # write out triples by subject
        for subject in self.store.subjects():
            self.subject(subject, 1)

        # endRDF
        write( "</rdf:RDF>\n" )

        # Set to None so that the memory can get garbage collected.
        #self.__serialized = None
        del self.__serialized


    def subject(self, subject, depth=1):
        if not subject in self.__serialized:
            self.__serialized[subject] = 1
            if isinstance(subject, (BNode,URIRef)):
                write = self.write
                indent = "  " * depth
                element_name = "rdf:Description"
                if isinstance(subject, BNode):
                    write( '%s<%s rdf:nodeID="%s"' %
                       (indent, element_name, subject))
                else:
                    uri = quoteattr(self.relativize(subject))
                    write( "%s<%s rdf:about=%s" % (indent, element_name, uri))
                if (subject, None, None) in self.store:
                    write( ">\n" )
                    for predicate, object in self.store.predicate_objects(subject):
                        self.predicate(predicate, object, depth+1)
                    write( "%s</%s>\n" % (indent, element_name))
                else:
                    write( "/>\n" )

    def predicate(self, predicate, object, depth=1):
        write = self.write
        indent = "  " * depth
        qname = self.store.namespace_manager.qname(predicate)
        if isinstance(object, Literal):
            attributes = ""
            if object.language:
                attributes += ' xml:lang="%s"'%object.language

            if object.datatype:
                attributes += ' rdf:datatype="%s"'%object.datatype

            write("%s<%s%s>%s</%s>\n" %
                  (indent, qname, attributes,
                   escape(object), qname) )
        else:
            if isinstance(object, BNode):
                write('%s<%s rdf:nodeID="%s"/>\n' %
                      (indent, qname, object))
            else:
                write("%s<%s rdf:resource=%s/>\n" %
                      (indent, qname, quoteattr(self.relativize(object))))


########NEW FILE########
__FILENAME__ = XMLWriter
import codecs
from xml.sax.saxutils import quoteattr, escape


class XMLWriter(object):
    def __init__(self, stream, namespace_manager, encoding=None, decl=1):
        encoding = encoding or 'utf-8'
        encoder, decoder, stream_reader, stream_writer = codecs.lookup(encoding)
        self.stream = stream = stream_writer(stream)
        if decl:
            stream.write('<?xml version="1.0" encoding="%s"?>' % encoding)
        self.element_stack = []
        self.nm = namespace_manager
        self.closed = True

    def __get_indent(self):
        return "  " * len(self.element_stack)
    indent = property(__get_indent)

    def __close_start_tag(self):
        if not self.closed: # TODO:
            self.closed = True
            self.stream.write(">")

    def push(self, uri):
        nm = self.nm
        self.__close_start_tag()
        write = self.stream.write
        write("\n")
        write(self.indent)
        write("<%s" % nm.qname(uri))
        self.element_stack.append(uri)
        self.closed = False
        self.parent = False

    def pop(self, uri=None):
        top = self.element_stack.pop()
        if uri:
            assert uri==top
        write = self.stream.write
        if not self.closed:
            self.closed = True
            write("/>")
        else:
            if self.parent:
                write("\n")
                write(self.indent)
            write("</%s>" % self.nm.qname(uri))
        self.parent = True

    def namespaces(self, namespaces):
        write = self.stream.write
        write("\n")
        for prefix, namespace in namespaces:
            if prefix:
                write("  xmlns:%s='%s'\n" % (prefix, namespace))
            else:
                write("  xmlns='%s'\n" % namespace)

    def attribute(self, uri, value):
        write = self.stream.write
        write(" %s=%s" % (self.nm.qname(uri), quoteattr(value)))


    def text(self, text):
        self.__close_start_tag()
        if "<" in text and ">" in text and not "]]>" in text:
            self.stream.write("<![CDATA[")
            self.stream.write(text)
            self.stream.write("]]>")
        else:
            self.stream.write(escape(text))

########NEW FILE########
__FILENAME__ = xml_names
# From: http://www.w3.org/TR/REC-xml#NT-CombiningChar
#
# * Name start characters must have one of the categories Ll, Lu, Lo,
#   Lt, Nl.
#
# * Name characters other than Name-start characters must have one of
#   the categories Mc, Me, Mn, Lm, or Nd.
#
# * Characters in the compatibility area (i.e. with character code
#   greater than #xF900 and less than #xFFFE) are not allowed in XML
#   names.
#
# * Characters which have a font or compatibility decomposition
#   (i.e. those with a "compatibility formatting tag" in field 5 of the
#   database -- marked by field 5 beginning with a "<") are not allowed.
#
# * The following characters are treated as name-start characters rather
#   than name characters, because the property file classifies them as
#   Alphabetic: [#x02BB-#x02C1], #x0559, #x06E5, #x06E6.
#
# * Characters #x20DD-#x20E0 are excluded (in accordance with Unicode
#   2.0, section 5.14).
#
# * Character #x00B7 is classified as an extender, because the property
#   list so identifies it.
#
# * Character #x0387 is added as a name character, because #x00B7 is its
#   canonical equivalent.
#
# * Characters ':' and '_' are allowed as name-start characters.
#
# * Characters '-' and '.' are allowed as name characters.

from unicodedata import category, decomposition

NAME_START_CATEGORIES = ["Ll", "Lu", "Lo", "Lt", "Nl"]
NAME_CATEGORIES = NAME_START_CATEGORIES + ["Mc", "Me", "Mn", "Lm", "Nd"]
ALLOWED_NAME_CHARS = [u"\u00B7", u"\u0387", u"-", u".", u"_"]

# http://www.w3.org/TR/REC-xml-names/#NT-NCName
#  [4] NCName ::= (Letter | '_') (NCNameChar)* /* An XML Name, minus
#      the ":" */
#  [5] NCNameChar ::= Letter | Digit | '.' | '-' | '_' | CombiningChar
#      | Extender

def is_ncname(name):
    first = name[0]
    if first=="_" or category(first) in NAME_START_CATEGORIES:
        for i in xrange(1, len(name)):
            c = name[i]
            if not category(c) in NAME_CATEGORIES:
                if c in ALLOWED_NAME_CHARS:
                    continue
                return 0
            #if in compatibility area
            #if decomposition(c)!='':
            #    return 0

        return 1
    else:
        return 0

XMLNS = "http://www.w3.org/XML/1998/namespace"

def split_uri(uri):
    if uri.startswith(XMLNS):
        return (XMLNS, uri.split(XMLNS)[1])
    length = len(uri)
    for i in xrange(0, length):
        c = uri[-i-1]
        if not category(c) in NAME_CATEGORIES:
            if c in ALLOWED_NAME_CHARS:
                continue
            for j in xrange(-1-i, length):
                if category(uri[j]) in NAME_START_CATEGORIES or uri[j]=="_":
                    ns = uri[:j]
                    if not ns:
                        break
                    ln = uri[j:]
                    return (ns, ln)
            break
    raise Exception("Can't split '%s'" % uri)




########NEW FILE########
__FILENAME__ = term_utils
from rdflib import *
from rdflib.Graph import QuotedGraph, Graph, ConjunctiveGraph, BackwardCompatGraph

#Takes an instance of a Graph (Graph, QuotedGraph, ConjunctiveGraph, or BackwardCompatGraph)
#and returns the Graphs identifier and 'type' ('U' for Graphs, 'F' for QuotedGraphs ).
def normalizeGraph(graph):
    if isinstance(graph,QuotedGraph):
        return graph.identifier, 'F'
    else:
        return graph.identifier , term2Letter(graph.identifier)

TERM_INSTANCIATION_DICT ={
    'U':URIRef,
    'B':BNode,
    'V':Variable,
    'L':Literal
}

GRAPH_TERM_DICT = {
    'F': (QuotedGraph, URIRef),
    'U': (Graph, URIRef),
    'B': (Graph, BNode)
}

def term2Letter(term):
    if isinstance(term,URIRef):
        return 'U'
    elif isinstance(term,BNode):
        return 'B'
    elif isinstance(term,Literal):
        return 'L'
    elif isinstance(term,QuotedGraph):
        return 'F'
    elif isinstance(term,Variable):
        return 'V'
    elif isinstance(term,Graph):
        return term2Letter(term.identifier)
    elif term is None:
        return 'L'
    else:
        raise Exception("The given term (%s) is not an instance of any of the known types (URIRef,BNode,Literal,QuotedGraph, or Variable).  It is a %s"%(term,type(term)))

def constructGraph(term):
    return GRAPH_TERM_DICT[term]

def triplePattern2termCombinations((s,p,o)):
    combinations=[]
    #combinations.update(TERM_COMBINATIONS)
    if isinstance(o,Literal):
        for key,val in TERM_COMBINATIONS.items():
            if key[OBJECT] == 'O':
                combinations.append(val)
    return combinations

def type2TermCombination(member,klass,context):
    try:
        rt = TERM_COMBINATIONS['%sU%s%s'%(term2Letter(member),term2Letter(klass),normalizeGraph(context)[-1])]
        return rt
    except:
        raise Exception("Unable to persist classification triple: %s %s %s"%(member,'rdf:type',klass,context))

def statement2TermCombination(subject,predicate,obj,context):
    return TERM_COMBINATIONS['%s%s%s%s'%(term2Letter(subject),term2Letter(predicate),term2Letter(obj),normalizeGraph(context)[-1])]

SUBJECT = 0
PREDICATE = 1
OBJECT = 2
CONTEXT = 3

TERM_COMBINATIONS = {
    'UUUU' : 0,
    'UUUB' : 1,
    'UUUF' : 2,
    'UUVU' : 3,
    'UUVB' : 4,
    'UUVF' : 5,
    'UUBU' : 6,
    'UUBB' : 7,
    'UUBF' : 8,
    'UULU' : 9,
    'UULB' : 10,
    'UULF' : 11,
    'UUFU' : 12,
    'UUFB' : 13,
    'UUFF' : 14,

    'UVUU' : 15,
    'UVUB' : 16,
    'UVUF' : 17,
    'UVVU' : 18,
    'UVVB' : 19,
    'UVVF' : 20,
    'UVBU' : 21,
    'UVBB' : 22,
    'UVBF' : 23,
    'UVLU' : 24,
    'UVLB' : 25,
    'UVLF' : 26,
    'UVFU' : 27,
    'UVFB' : 28,
    'UVFF' : 29,

    'VUUU' : 30,
    'VUUB' : 31,
    'VUUF' : 33,
    'VUVU' : 34,
    'VUVB' : 35,
    'VUVF' : 36,
    'VUBU' : 37,
    'VUBB' : 38,
    'VUBF' : 39,
    'VULU' : 40,
    'VULB' : 41,
    'VULF' : 42,
    'VUFU' : 43,
    'VUFB' : 44,
    'VUFF' : 45,

    'VVUU' : 46,
    'VVUB' : 47,
    'VVUF' : 48,
    'VVVU' : 49,
    'VVVB' : 50,
    'VVVF' : 51,
    'VVBU' : 52,
    'VVBB' : 53,
    'VVBF' : 54,
    'VVLU' : 55,
    'VVLB' : 56,
    'VVLF' : 57,
    'VVFU' : 58,
    'VVFB' : 59,
    'VVFF' : 60,

    'BUUU' : 61,
    'BUUB' : 62,
    'BUUF' : 63,
    'BUVU' : 64,
    'BUVB' : 65,
    'BUVF' : 66,
    'BUBU' : 67,
    'BUBB' : 68,
    'BUBF' : 69,
    'BULU' : 70,
    'BULB' : 71,
    'BULF' : 72,
    'BUFU' : 73,
    'BUFB' : 74,
    'BUFF' : 75,

    'BVUU' : 76,
    'BVUB' : 77,
    'BVUF' : 78,
    'BVVU' : 79,
    'BVVB' : 80,
    'BVVF' : 81,
    'BVBU' : 82,
    'BVBB' : 83,
    'BVBF' : 84,
    'BVLU' : 85,
    'BVLB' : 86,
    'BVLF' : 87,
    'BVFU' : 88,
    'BVFB' : 89,
    'BVFF' : 90,

    'FUUU' : 91,
    'FUUB' : 92,
    'FUUF' : 93,
    'FUVU' : 94,
    'FUVB' : 95,
    'FUVF' : 96,
    'FUBU' : 97,
    'FUBB' : 98,
    'FUBF' : 99,
    'FULU' : 100,
    'FULB' : 101,
    'FULF' : 102,
    'FUFU' : 103,
    'FUFB' : 104,
    'FUFF' : 105,

    'FVUU' : 106,
    'FVUB' : 107,
    'FVUF' : 108,
    'FVVU' : 109,
    'FVVB' : 110,
    'FVVF' : 111,
    'FVBU' : 112,
    'FVBB' : 113,
    'FVBF' : 114,
    'FVLU' : 115,
    'FVLB' : 116,
    'FVLF' : 117,
    'FVFU' : 118,
    'FVFB' : 119,
    'FVFF' : 120,
}

REVERSE_TERM_COMBINATIONS = dict([(value,key) for key,value in TERM_COMBINATIONS.items()])

########NEW FILE########
__FILENAME__ = TextIndex
import logging
_logger = logging.getLogger(__name__)

import re #, stopdict

def get_stopdict():
    """Return a dictionary of stopwords."""
    return _dict

_words = [
    "a", "and", "are", "as", "at", "be", "but", "by",
    "for", "if", "in", "into", "is", "it",
    "no", "not", "of", "on", "or", "such",
    "that", "the", "their", "then", "there", "these",
    "they", "this", "to", "was", "will", "with"
]

_dict = {}
for w in _words:
    _dict[w] = None

word_pattern = re.compile(r"(?u)\w+")
has_stop = get_stopdict().has_key

def splitter(s):
    return word_pattern.findall(s)

def stopper(s):
    return [w.lower() for w in s if not has_stop(w)]


try:
    from hashlib import md5
except ImportError:
    from md5 import md5    

from rdflib.store.IOMemory import IOMemory
from rdflib import URIRef, Literal, RDF, BNode
from rdflib.Namespace import NamespaceDict as Namespace
from rdflib.Graph import ConjunctiveGraph
from rdflib.store import TripleAddedEvent, TripleRemovedEvent


class TextIndex(ConjunctiveGraph):
    """
    An rdflib graph event handler than indexes text literals that are
    added to a another graph.

    This class lets you 'search' the text literals in an RDF graph.
    Typically in RDF to search for a substring in an RDF graph you
    would have to 'brute force' search every literal string looking
    for your substring.

    Instead, this index stores the words in literals into another
    graph whose structure makes searching for terms much less
    expensive.  It does this by chopping up the literals into words,
    removing very common words (currently only in English) and then
    adding each of those words into an RDF graph that describes the
    statements in the original graph that the word came from.

    First, let's create a graph that will transmit events and a text
    index that will receive those events, and then subscribe the text
    index to the event graph:

      >>> e = ConjunctiveGraph()
      >>> t = TextIndex()
      >>> t.subscribe_to(e)

    When triples are added to the event graph (e) events will be fired
    that trigger event handlers in subscribers.  In this case our only
    subscriber is a text index and its action is to index triples that
    contain literal RDF objects.  Here are 3 such triples:

      >>> e.add((URIRef('a'), URIRef('title'), Literal('one two three')))
      >>> e.add((URIRef('b'), URIRef('title'), Literal('two three four')))
      >>> e.add((URIRef('c'), URIRef('title'), Literal('three four five')))

    Of the three literal objects that were added, they all contain
    five unique terms.  These terms can be queried directly from the
    text index:
    
      >>> t.term_strings() ==  set(['four', 'five', 'three', 'two', 'one'])
      True

    Now we can search for statement that contain certain terms.  Let's
    search for 'one' which occurs in only one of the literals
    provided, 'a'.  This can be queried for:

      >>> t.search('one')
      set([(rdflib.URIRef('a'), rdflib.URIRef('title'), None)])

    'one' and 'five' only occur in one statement each, 'two' and
    'four' occur in two, and 'three' occurs in three statements:

      >>> len(list(t.search('one')))
      1
      >>> len(list(t.search('two')))
      2
      >>> len(list(t.search('three')))
      3
      >>> len(list(t.search('four')))
      2
      >>> len(list(t.search('five')))
      1

    Lets add some more statements with different predicates.

      >>> e.add((URIRef('a'), URIRef('creator'), Literal('michel')))
      >>> e.add((URIRef('b'), URIRef('creator'), Literal('Atilla the one Hun')))
      >>> e.add((URIRef('c'), URIRef('creator'), Literal('michel')))
      >>> e.add((URIRef('d'), URIRef('creator'), Literal('Hun Mung two')))

    Now 'one' occurs in two statements:

      >>> assert len(list(t.search('one'))) == 2

    And 'two' occurs in three statements, here they are:

      >>> t.search('two')
      set([(rdflib.URIRef('d'), rdflib.URIRef('creator'), None), (rdflib.URIRef('a'), rdflib.URIRef('title'), None), (rdflib.URIRef('b'), rdflib.URIRef('title'), None)])

    The predicates that are searched can be restricted by provding an
    argument to 'search()':

      >>> t.search('two', URIRef('creator'))
      set([(rdflib.URIRef('d'), rdflib.URIRef('creator'), None)])

      >>> t.search('two', URIRef(u'title'))
      set([(rdflib.URIRef('a'), rdflib.URIRef('title'), None), (rdflib.URIRef('b'), rdflib.URIRef('title'), None)])

    You can search for more than one term by simply including it in
    the query:
    
      >>> t.search('two three', URIRef(u'title'))
      set([(rdflib.URIRef('c'), rdflib.URIRef('title'), None), (rdflib.URIRef('a'), rdflib.URIRef('title'), None), (rdflib.URIRef('b'), rdflib.URIRef('title'), None)])

    The above query returns all the statements that contain 'two' OR
    'three'.  For the documents that contain 'two' AND 'three', do an
    intersection of two queries:

      >>> t.search('two', URIRef(u'title')).intersection(t.search(u'three', URIRef(u'title')))
      set([(rdflib.URIRef('a'), rdflib.URIRef('title'), None), (rdflib.URIRef('b'), rdflib.URIRef('title'), None)])

    Intersection two queries like this is probably not the most
    efficient way to do it, but for reasonable data sets this isn't a
    problem.  Larger data sets will want to query the graph with
    sparql or something else more efficient.

    In all the above queries, the object of each statement was always
    'None'.  This is because the index graph does not store the object
    data, that would make it very large, and besides the data is
    available in the original data graph.  For convenience, a method
    is provides to 'link' an index graph to a data graph.  This allows
    the index to also provide object data in query results.

      >>> t.link_to(e)
      >>> set([str(i[2]) for i in t.search('two', URIRef(u'title')).intersection(t.search(u'three', URIRef(u'title')))]) ==  set(['two three four', 'one two three'])
      True

    You can remove the link by assigning None:

      >>> t.link_to(None)

    Unindexing means to remove statments from the index graph that
    corespond to a statement in the data graph.  Note that while it is
    possible to remove the index information of the occurances of
    terms in statements, it is not possible to remove the terms
    themselves, terms are 'absolute' and are never removed from the
    index graph.  This is not a problem since languages have finite
    terms:

      >>> e.remove((URIRef('a'), URIRef('creator'), Literal('michel')))
      >>> e.remove((URIRef('b'), URIRef('creator'), Literal('Atilla the one Hun')))
      >>> e.remove((URIRef('c'), URIRef('creator'), Literal('michel')))
      >>> e.remove((URIRef('d'), URIRef('creator'), Literal('Hun Mung two')))

    Now 'one' only occurs in one statement:

      >>> assert len(list(t.search('one'))) == 1

    And 'two' only occurs in two statements, here they are:

      >>> t.search('two')
      set([(rdflib.URIRef('a'), rdflib.URIRef('title'), None), (rdflib.URIRef('b'), rdflib.URIRef('title'), None)])

    The predicates that are searched can be restricted by provding an
    argument to 'search()':

      >>> t.search('two', URIRef(u'creator'))
      set([])

      >>> t.search('two', URIRef(u'title'))
      set([(rdflib.URIRef('a'), rdflib.URIRef('title'), None), (rdflib.URIRef('b'), rdflib.URIRef('title'), None)])

    """

    linked_data = None

    text_index = Namespace('http://rdflib.net/text_index#')
    term = Namespace('http://rdflib.net/text_index#')["term"]
    termin = Namespace('http://rdflib.net/text_index#')["termin"]

    def __init__(self, store='default'):
        super(TextIndex, self).__init__(store)

    def add_handler(self, event):
        if type(event.triple[2]) is Literal:
            self.index(event.triple)
        
    def remove_handler(self, event):
        if type(event.triple[2]) is Literal:
            self.unindex(event.triple)

    def index(self, (s, p, o)):
        # this code is tricky so it's annotated.  unindex is the reverse of this method.
                
        if type(o) is Literal:                            # first, only index statements that have a literal object
            for word in stopper(splitter(o)):             # split the literal and remove any stopwords
                word = Literal(word)                      # create a new literal for each word in the object
                
                # if that word already exists in the statement
                # loop over each context the term occurs in
                if self.value(predicate=self.term, object=word, any=True): 
                    for t in set(self.triples((None, self.term, word))):
                        t = t[0]
                        # if the graph does not contain an occurance of the term in the statement's subject
                        # then add it
                        if not (t, self.termin, s) in self:
                            self.add((t, self.termin, s))

                        # ditto for the predicate
                        if not (p, t, s) in self:
                            self.add((p, t, s))

                else: # if the term does not exist in the graph, add it, and the references to the statement.
                    # t gets used as a predicate, create identifier accordingly (AKA can't be a BNode)
                    h = md5(word); h.update(s); h.update(p)
                    t = self.text_index["term_%s" % h.hexdigest()]
                    self.add((t, self.term, word))
                    self.add((t, self.termin, s))
                    self.add((p, t, s))
        
    def unindex(self, (s, p, o)):
        if type(o) is Literal:
            for word in stopper(splitter(o)):
                word = Literal(word)
                if self.value(predicate=self.term, object=word, any=True):
                    for t in self.triples((None, self.term, word)):
                        t = t[0]
                        if (t, self.termin, s) in self:
                            self.remove((t, self.termin, s))
                        if (p, t, s) in self:
                            self.remove((p, t, s))

    def terms(self):
        """ Returns a generator that yields all of the term literals in the graph. """
        return set(self.objects(None, self.term))

    def term_strings(self):
        """ Return a list of term strings. """
        return set([str(i) for i in self.terms()])

    def search(self, terms, predicate=None):
        """ Returns a set of all the statements the term occurs in. """
        if predicate and not isinstance(predicate, URIRef):
            _logger.warning("predicate is not a URIRef")
            predicate = URIRef(predicate)
        results = set()
        terms = [Literal(term) for term in stopper(splitter(terms))]    

        for term in terms:
            for t in self.triples((None, self.term, term)):
                for o in self.objects(t[0], self.termin):
                    for p in self.triples((predicate, t[0], o)):
                        if self.linked_data is None:
                            results.add((o, p[0], None))
                        else:
                            results.add((o, p[0], self.linked_data.value(o, p[0])))
        return results

    def index_graph(self, graph):
        """
        Index a whole graph.  Must be a conjunctive graph.
        """
        for t in graph.triples((None,None,None)):
            self.index(t)

    def link_to(self, graph):
        """
        Link to a graph
        """
        self.linked_data = graph

    def subscribe_to(self, graph):
        """
        Subscribe this index to a graph.
        """
        graph.store.dispatcher.subscribe(TripleAddedEvent, self.add_handler)
        graph.store.dispatcher.subscribe(TripleRemovedEvent, self.remove_handler)


def test():
    import doctest
    doctest.testmod()

if __name__ == '__main__':
    test()

########NEW FILE########
__FILENAME__ = TripleStore
"""Deprecated; use Graph."""

from rdflib import Graph

from rdflib.store.Memory import Memory

class TripleStore(Graph):
    """
    Depcrecated. Use Graph instead.
    """

    def __init__(self, location=None, backend=None):
        if backend==None:
            backend = Memory()
        super(TripleStore, self).__init__(backend=backend)
        if location:
            self.load(location)

    def prefix_mapping(self, prefix, namespace):
        self.bind(prefix, namespace)


########NEW FILE########
__FILENAME__ = URIRef
from sys import version_info

try:
    from hashlib import md5
except ImportError:
    from md5 import md5    

if version_info[0:2] > (2, 2):
    from unicodedata import normalize
else:
    normalize = None

from urlparse import urlparse, urljoin, urldefrag

from rdflib.Identifier import Identifier
from rdflib.compat import rsplit


class URIRef(Identifier):
    """
    RDF URI Reference: http://www.w3.org/TR/rdf-concepts/#section-Graph-URIref
    """

    __slots__ = ()

    def __new__(cls, value, base=None):
        if base is not None:
            ends_in_hash = value.endswith("#")
            value = urljoin(base, value, allow_fragments=1)
            if ends_in_hash:
                if not value.endswith("#"):
                    value += "#"
        #if normalize and value and value != normalize("NFC", value):
        #    raise Error("value must be in NFC normalized form.")
        try:
            rt = unicode.__new__(cls,value)
        except UnicodeDecodeError:
            rt = unicode.__new__(cls,value,'utf-8')
        return rt

    def n3(self):
        return "<%s>" % self

    def concrete(self):
        if "#" in self:
            return URIRef("/".join(rsplit(self, "#", 1)))
        else:
            return self

    def abstract(self):
        if "#" not in self:
            scheme, netloc, path, params, query, fragment = urlparse(self)
            if path:
                return URIRef("#".join(rsplit(self, "/", 1)))
            else:
                if not self.endswith("#"):
                    return URIRef("%s#" % self)
                else:
                    return self
        else:
            return self


    def defrag(self):
        if "#" in self:
            url, frag = urldefrag(self)
            return URIRef(url)
        else:
            return self

    def __reduce__(self):
        return (URIRef, (unicode(self),))

    def __getnewargs__(self):
        return (unicode(self), )


    def __eq__(self, other):
        if isinstance(other, URIRef):
            return unicode(self)==unicode(other)
        else:
            return False

    def __str__(self):
        return self.encode("unicode-escape")

    def __repr__(self):
        return """rdflib.URIRef('%s')""" % str(self)

    def md5_term_hash(self):
        d = md5(str(self))
        d.update("U")
        return d.hexdigest()

########NEW FILE########
__FILENAME__ = URLInputSource
from urllib2 import urlopen, Request

from xml.sax.xmlreader import InputSource

from rdflib import __version__

# TODO: add types for n3. text/rdf+n3 ?
headers = {
    'Accept': 'application/rdf+xml,application/xhtml+xml;q=0.5',
    'User-agent':
    'rdflib-%s (http://rdflib.net/; eikeon@eikeon.com)' % __version__
    }


class URLInputSource(InputSource, object):
    def __init__(self, system_id=None):
        super(URLInputSource, self).__init__(system_id)
        self.url = system_id
        # So that we send the headers we want to...
        req = Request(system_id, None, headers)
        file = urlopen(req)
        self.setByteStream(file)
        # TODO: self.setEncoding(encoding)

    def __repr__(self):
        return self.url

########NEW FILE########
__FILENAME__ = util
from rdflib.URIRef import URIRef
from rdflib.BNode import BNode
from rdflib.Literal import Literal
from rdflib.Variable import Variable
from rdflib.Graph import Graph, QuotedGraph
from rdflib.Statement import Statement

from rdflib.exceptions import SubjectTypeError, PredicateTypeError, ObjectTypeError, ContextTypeError
from rdflib.compat import rsplit
from cPickle import loads

def list2set(seq):
    seen = set()
    return [ x for x in seq if x not in seen and not seen.add(x)]

def first(seq):
    for result in seq:
        return result
    return None

def uniq(sequence, strip=0):
    """removes duplicate strings from the sequence."""
    set = {}
    if strip:
        map(lambda val, default: set.__setitem__(val.strip(), default),
            sequence, [])
    else:
        map(set.__setitem__, sequence, [])
    return set.keys()

def more_than(sequence, number):
    "Returns 1 if sequence has more items than number and 0 if not."
    i = 0
    for item in sequence:
        i += 1
        if i > number:
            return 1
    return 0

def term(str, default=None):
    """See also from_n3"""
    if not str:
        return default
    elif str.startswith("<") and str.endswith(">"):
        return URIRef(str[1:-1])
    elif str.startswith('"') and str.endswith('"'):
        return Literal(str[1:-1])
    elif str.startswith("_"):
        return BNode(str)
    else:
        msg = "Unknown Term Syntax: '%s'" % str
        raise Exception(msg)



from time import mktime, time, gmtime, localtime, timezone, altzone, daylight

def date_time(t=None, local_time_zone=False):
    """http://www.w3.org/TR/NOTE-datetime ex: 1997-07-16T19:20:30Z

    >>> date_time(1126482850)
    '2005-09-11T23:54:10Z'

    >>> date_time(1126482850, local_time_zone=True)
    '2005-09-11T19:54:10-04:00'

    >>> date_time(1)
    '1970-01-01T00:00:01Z'

    >>> date_time(0)
    '1970-01-01T00:00:00Z'
    """
    if t is None:
        t = time()

    if local_time_zone:
        time_tuple = localtime(t)
        if time_tuple[8]:
            tz_mins = altzone // 60
        else:
            tz_mins = timezone // 60
        tzd = "-%02d:%02d" % (tz_mins // 60, tz_mins % 60)
    else:
        time_tuple = gmtime(t)
        tzd = "Z"

    year, month, day, hh, mm, ss, wd, y, z = time_tuple
    s = "%0004d-%02d-%02dT%02d:%02d:%02d%s" % ( year, month, day, hh, mm, ss, tzd)
    return s

def parse_date_time(val):
    """always returns seconds in UTC

    # tests are written like this to make any errors easier to understand
    >>> parse_date_time('2005-09-11T23:54:10Z') - 1126482850.0
    0.0

    >>> parse_date_time('2005-09-11T16:54:10-07:00') - 1126482850.0
    0.0

    >>> parse_date_time('1970-01-01T00:00:01Z') - 1.0
    0.0

    >>> parse_date_time('1970-01-01T00:00:00Z') - 0.0
    0.0
    >>> parse_date_time("2005-09-05T10:42:00") - 1125916920.0
    0.0
    """

    if "T" not in val:
        val += "T00:00:00Z"

    ymd, time = val.split("T")
    hms, tz_str = time[0:8], time[8:]

    if not tz_str or tz_str=="Z":
        time = time[:-1]
        tz_offset = 0
    else:
        signed_hrs = int(tz_str[:3])
        mins = int(tz_str[4:6])
        secs = (cmp(signed_hrs, 0) * mins + signed_hrs * 60) * 60
        tz_offset = -secs

    year, month, day = ymd.split("-")
    hour, minute, second = hms.split(":")

    t = mktime((int(year), int(month), int(day), int(hour),
                int(minute), int(second), 0, 0, 0))
    t = t - timezone + tz_offset
    return t

def from_n3(s, default=None, backend=None):
    """ Creates the Identifier corresponding to the given n3 string. WARNING: untested, may contain bugs. TODO: add test cases."""
    if not s:
        return default
    if s.startswith('<'):
        return URIRef(s[1:-1])
    elif s.startswith('"'):
        # TODO: would a regex be faster?
        value, rest = rsplit(s, '"', 1)
        value = value[1:] # strip leading quote
        if rest.startswith("@"):
            if "^^" in rest:
                language, rest = rsplit(rest, '^^', 1)
                language = language[1:] # strip leading at sign
            else:
                language = rest[1:] # strip leading at sign
                rest = ''
        else:
            language = None
        if rest.startswith("^^"):
            datatype = rest[3:-1]
        else:
            datatype = None
        value = value.replace('\\"', '"').replace('\\\\', '\\').decode("unicode-escape")
        return Literal(value, language, datatype)
    elif s.startswith('{'):
        identifier = from_n3(s[1:-1])
        return QuotedGraph(backend, identifier)
    elif s.startswith('['):
        identifier = from_n3(s[1:-1])
        return Graph(backend, identifier)
    else:
        if s.startswith("_:"):
            return BNode(s[2:])
        else:
            return BNode(s)

def check_context(c):
    if not (isinstance(c, URIRef) or \
            isinstance(c, BNode)):
        raise ContextTypeError("%s:%s" % (c, type(c)))

def check_subject(s):
    """ Test that s is a valid subject identifier."""
    if not (isinstance(s, URIRef) or isinstance(s, BNode)):
        raise SubjectTypeError(s)

def check_predicate(p):
    """ Test that p is a valid predicate identifier."""
    if not isinstance(p, URIRef):
        raise PredicateTypeError(p)

def check_object(o):
    """ Test that o is a valid object identifier."""
    if not (isinstance(o, URIRef) or \
            isinstance(o, Literal) or \
            isinstance(o, BNode)):
        raise ObjectTypeError(o)

def check_statement((s, p, o)):
    if not (isinstance(s, URIRef) or isinstance(s, BNode)):
        raise SubjectTypeError(s)

    if not isinstance(p, URIRef):
        raise PredicateTypeError(p)

    if not (isinstance(o, URIRef) or \
            isinstance(o, Literal) or \
            isinstance(o, BNode)):
        raise ObjectTypeError(o)

def check_pattern((s, p, o)):
    if s and not (isinstance(s, URIRef) or isinstance(s, BNode)):
        raise SubjectTypeError(s)

    if p and not isinstance(p, URIRef):
        raise PredicateTypeError(p)

    if o and not (isinstance(o, URIRef) or \
                  isinstance(o, Literal) or \
                  isinstance(o, BNode)):
        raise ObjectTypeError(o)

def graph_to_dot(graph, dot):
    """ Turns graph into dot (graphviz graph drawing format) using pydot. """
    import pydot
    nodes = {}
    for s, o in graph.subject_objects():
        for i in s,o:
            if i not in nodes.keys():
                nodes[i] = i
    for s, p, o in graph.triples((None,None,None)):
        dot.add_edge(pydot.Edge(nodes[s], nodes[o], label=p))


if __name__ == "__main__":
    # try to make the tests work outside of the time zone they were written in
    #import os, time
    #os.environ['TZ'] = 'US/Pacific'
    #try:
    #    time.tzset()
    #except AttributeError, e:
    #    print e
        #pass
        # tzset missing! see
        # http://mail.python.org/pipermail/python-dev/2003-April/034480.html
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = Variable
from rdflib.Identifier import Identifier
try:
    from hashlib import md5
except ImportError:
    from md5 import md5    

class Variable(Identifier):
    """
    """
    __slots__ = ()
    def __new__(cls, value):
        return Identifier.__new__(cls, value)

    def n3(self):
        return "?%s" % self

    def __reduce__(self):
        return (Variable, (unicode(self),))

    def md5_term_hash(self):
        d = md5(str(self))
        d.update("V")
        return d.hexdigest()
########NEW FILE########
__FILENAME__ = EARLPlugin
""" A Nose Plugin for EARL.

See Also: 
  http://nose.python-hosting.com/
  http://www.w3.org/TR/EARL10-Schema/ 

"""

import logging
import sys

from nose.plugins import Plugin
from nose.suite import TestModule

from rdflib import URIRef, BNode, Literal
from rdflib import RDF, RDFS
from rdflib.Graph import Graph
from rdflib.Namespace import NamespaceDict as Namespace
from rdflib.util import date_time

log = logging.getLogger(__name__)

EARL = Namespace("http://www.w3.org/ns/earl#")


class EARLPlugin(Plugin):
    """
    Activate the EARL plugin to generate a report of the test results
    using EARL.
    """
    name = 'EARL'
    
    def begin(self):
        self.graph = Graph()
        self.graph.bind("earl", EARL.uri)

    def finalize(self, result):
        # TODO: add plugin options for specifying where to send
        # output.
        self.graph.serialize("file:results-%s.rdf" % date_time(), format="pretty-xml")

    def addDeprecated(self, test):
        print "Deprecated: %s" % test

    def addError(self, test, err, capt):
        print "Error: %s" % test

    def addFailure(self, test, err, capt, tb_info):
        print "Failure: %s" % test

    def addSkip(self, test):
        print "Skip: %s" % test

    def addSuccess(self, test, capt):
        result = BNode() # TODO: coin URIRef
        self.graph.add((result, RDFS.label, Literal(test)))
        self.graph.add((result, RDFS.comment, Literal(type(test))))
        self.graph.add((result, RDF.type, EARL.TestResult))
        self.graph.add((result, EARL.outcome, EARL["pass"]))
        # etc

"""
<earl:TestResult rdf:about="#result">
  <earl:outcome rdf:resource="http://www.w3.org/ns/earl#fail"/>
  <dc:title xml:lang="en">Invalid Markup (code #353)</dc:title>
  <dc:description rdf:parseType="Literal" xml:lang="en">
    <div xmlns="http://www.w3.org/1999/xhtml">
      <p>The <code>table</code> element is not allowed to appear
        inside a <code>p</code> element</p>
    </div>
  </dc:description>
  <dc:date rdf:datatype="http://www.w3.org/2001/XMLSchema#date">2006-08-13</dc:date>
  <earl:pointer rdf:resource="#xpointer"/>
  <earl:info rdf:parseType="Literal" xml:lang="en">
    <div xmlns="http://www.w3.org/1999/xhtml">
      <p>It seems the <code>p</code> element has not been closed</p>
    </div>
  </earl:info>
</earl:TestResult>
"""



########NEW FILE########
__FILENAME__ = RDFPipe
#!/usr/bin/env python
from pprint import pprint
from rdflib.Namespace import Namespace
from rdflib import plugin,RDF,RDFS,URIRef
from rdflib.store import Store
from rdflib.Graph import Graph
from rdflib.syntax.NamespaceManager import NamespaceManager

RDFLIB_CONNECTION=''
RDFLIB_STORE='IOMemory'

import getopt, sys

def usage():
    print """USAGE: RDFPipe.py [options]
    
    Options:
    
      --stdin                     Parse RDF from STDIN (useful for piping)
      --help                      
      --input-format              Format of the input document(s).  One of:
                                  'xml','trix','n3','nt','rdfa'
      --output                    Format of the final serialized RDF graph.  One of:
                                  'n3','xml','pretty-xml','turtle',or 'nt'
      --ns=prefix=namespaceUri    Register a namespace binding (QName prefix to a 
                                  base URI).  This can be used more than once"""

def main():
    try:
        opts, args = getopt.getopt(sys.argv[1:], "", ["output=","ns=","input=","stdin","help","input-format="])
    except getopt.GetoptError, e:
        # print help information and exit:
        print e
        usage()
        sys.exit(2)

    factGraphs = []
    factFormat = 'xml'
    useRuleFacts = False
    nsBinds = {
        'rdf' : RDF.RDFNS,
        'rdfs': RDFS.RDFSNS,
        'owl' : "http://www.w3.org/2002/07/owl#",       
        'dc'  : "http://purl.org/dc/elements/1.1/",
        'foaf': "http://xmlns.com/foaf/0.1/",
        'wot' : "http://xmlns.com/wot/0.1/"        
    }
    outMode = 'n3'
    stdIn = False
    if not opts:
        usage()
        sys.exit()        
    for o, a in opts:
        if o == '--input-format':
            factFormat = a
        elif o == '--stdin':
            stdIn = True
        elif o == '--output':
            outMode = a
        elif o == '--ns':            
            pref,nsUri = a.split('=')
            nsBinds[pref]=nsUri
        elif o == "--input":
            factGraphs = a.split(',')
        elif o == "--help":
            usage()
            sys.exit()
        
    store = plugin.get(RDFLIB_STORE,Store)()        
    store.open(RDFLIB_CONNECTION)
    namespace_manager = NamespaceManager(Graph())
    for prefix,uri in nsBinds.items():
        namespace_manager.bind(prefix, uri, override=False)    
    factGraph = Graph(store) 
    factGraph.namespace_manager = namespace_manager
    if factGraphs:
        for fileN in factGraphs:
            factGraph.parse(fileN,format=factFormat)
    if stdIn:
        factGraph.parse(sys.stdin,format=factFormat)
    print factGraph.serialize(destination=None, format=outMode, base=None)
    store.rollback()

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = run_tests
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Testing with Nose
=================

This test runner uses Nose for test discovery and running. It uses the argument
spec of Nose, but with some options pre-set. To begin with, make sure you have
Nose installed, e.g.:

    $ sudo easy_install nose

For daily test runs, use:

    $ ./run_tests.py

If you supply attributes, the default ones defined in ``DEFAULT_ATTRS`` will be
ignored. So to run e.g. all tests marked ``slowtest`` or ``non_standard_dep``,
do:

    $ ./run_tests.py -a slowtest,non_standard_dep

See <http://code.google.com/p/python-nose/> for furher details. An excellent
article is also available at <http://ivory.idyll.org/articles/nose-intro.html>.

Note that this is just a convenience script. You can use ``nosetests`` directly
if it's on $PATH, with the difference that you have to supply the options
pre-set here manually.

Coverage
========

If ``coverage.py`` is placed in $PYTHONPATH, it can be used to create coverage
information (using the built-in coverage plugin of Nose) if the default
option "--with-coverage" is supplied (which also enables some additional
coverage options).

See <http://nedbatchelder.com/code/modules/coverage.html> for details.

"""


NOSE_ARGS = [
        '--where=./',
        '--with-doctest',
        '--doctest-extension=.doctest',
        '--doctest-tests',
#        '--with-EARL',
    ]

COVERAGE_EXTRA_ARGS = [
        '--cover-package=rdflib',
        '--cover-inclusive',
    ]

DEFAULT_ATTRS = ['!slowtest', '!unstable', '!non_standard_dep']

DEFAULT_DIRS = ['test', 'rdflib']


if __name__ == '__main__':

    from sys import argv, exit, stderr
    try: import nose
    except ImportError:
        print >>stderr, """\
    Requires Nose. Try:

        $ sudo easy_install nose

    Exiting. """; exit(1)


    if '--with-coverage' in argv:
        try: import coverage
        except ImportError:
            print >>stderr, "No coverage module found, skipping code coverage."
            argv.remove('--with-coverage')
        else:
            NOSE_ARGS += COVERAGE_EXTRA_ARGS


    if True not in [a.startswith('-a') or a.startswith('--attr=') for a in argv]:
        argv.append('--attr=' + ','.join(DEFAULT_ATTRS))

    if not [a for a in argv[1:] if not a.startswith('-')]:
        argv += DEFAULT_DIRS # since nose doesn't look here by default..


    finalArgs = argv + NOSE_ARGS
    print "Running nose with:", " ".join(finalArgs[1:])
    nose.run(argv=finalArgs)


# TODO: anything from the following we've left behind?
old_run_tests = """
import logging

_logger = logging.getLogger()
_logger.setLevel(logging.ERROR)
_formatter = logging.Formatter('%(name)s %(levelname)s %(message)s')
_handler = logging.StreamHandler()
_handler.setFormatter(_formatter)
_logger.addHandler(_handler)

import unittest, inspect
import rdflib

quick = True
verbose = True

from test.IdentifierEquality import IdentifierEquality
from test.sparql.QueryTestCase import QueryTestCase

from test.graph import *

from test.triple_store import *
from test.context import *

# # Graph no longer has the type checking at the moment. Do we want to
# # put it back? Should we?
# #
# # from test.type_check import *

from test.parser import *

if not quick:
    from test import parser_rdfcore
    if verbose:
        parser_rdfcore.verbose = 1
    from test.parser_rdfcore import *

    from test.Sleepycat import *

from test.rdf import * # how does this manage to be 9 tests?

from test.n3 import *
from test.n3_quoting import *
from test.nt import *

from test.util import *
from test.seq import SeqTestCase

#from test.store_performace import *

from test.rules import *

from test.n3Test import *

from test.JSON import JSON

import test.rdfa

from test.events import *

def run():
    # TODO: Fix failed test and comment back in.
    # test.rdfa.main()

    if verbose:
        ts = unittest.makeSuite
        tests = [
            c for c in vars().values()
            if inspect.isclass(c)
                and not isinstance(c, rdflib.Namespace)
                and issubclass(c, unittest.TestCase)
        ]
        suite = unittest.TestSuite(map(ts, tests))
        unittest.TextTestRunner(verbosity=2).run(suite)
    else:
        unittest.main()

"""

########NEW FILE########
__FILENAME__ = advanced_sparql_constructs
import unittest
from rdflib.Namespace import Namespace
from rdflib import plugin,RDF,RDFS,URIRef
from rdflib.store import Store
from cStringIO import StringIO
from rdflib.Graph import Graph,ReadOnlyGraphAggregate,ConjunctiveGraph
import sys
from pprint import pprint

testGraph1N3="""
@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix : <http://test/> .
:foo :relatedTo [ a rdfs:Class ];
     :parentOf ( [ a rdfs:Class ] ).
:bar :relatedTo [ a rdfs:Resource ];
     :parentOf ( [ a rdfs:Resource ] ).
     
( [ a rdfs:Resource ] ) :childOf :bar.     
( [ a rdfs:Class ] )    :childOf :foo.
"""

sparqlQ1 = \
"""
BASE <http://test/>
PREFIX rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> 
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?node WHERE { ?node :relatedTo [ a rdfs:Class ] }"""


sparqlQ2 = \
"""
BASE <http://test/>
PREFIX rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> 
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?node WHERE { ?node :parentOf ( [ a rdfs:Class ] ) }"""


sparqlQ3 = \
"""
BASE <http://test/>
PREFIX rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> 
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?node WHERE { ( [ a rdfs:Resource ] ) :childOf ?node }"""

sparqlQ4 = \
"""
PREFIX owl:  <http://www.w3.org/2002/07/owl#> 

SELECT DISTINCT ?class 
FROM <http://www.w3.org/2002/07/owl#>
WHERE { ?thing a ?class }"""

class AdvancedTests(unittest.TestCase):
    def setUp(self):
        memStore = plugin.get('IOMemory',Store)()
        self.testGraph = Graph(memStore)
        self.testGraph.parse(StringIO(testGraph1N3),format='n3')
        
    def testNamedGraph(self):
        from sets import Set
        OWL_NS = Namespace("http://www.w3.org/2002/07/owl#")
        rt =  self.testGraph.query(sparqlQ4)
        self.assertEquals(Set(rt.serialize('python')),Set([OWL_NS.OntologyProperty,OWL_NS.Class,OWL_NS.Ontology,OWL_NS.AnnotationProperty,RDF.Property,RDFS.Class]))

    def testScopedBNodes(self):
        rt =  self.testGraph.query(sparqlQ1)
        self.assertEquals(rt.serialize('python')[0],URIRef("http://test/foo"))

    def testCollectionContentWithinAndWithout(self):
        rt =  self.testGraph.query(sparqlQ3)
        self.assertEquals(rt.serialize('python')[0],URIRef("http://test/bar"))

    def testCollectionAsObject(self):
        rt =  self.testGraph.query(sparqlQ2)
        self.assertEquals(rt.serialize('python')[0],URIRef("http://test/foo"))
        self.assertEquals(1,len(rt))

if __name__ == '__main__':
    suite = unittest.makeSuite(AdvancedTests)
    unittest.TextTestRunner(verbosity=3).run(suite)
########NEW FILE########
__FILENAME__ = aggregate_graphs
from rdflib.Namespace import Namespace
from rdflib import plugin,RDF,RDFS,URIRef
from rdflib.store import Store
from cStringIO import StringIO
from rdflib.Graph import Graph,ReadOnlyGraphAggregate,ConjunctiveGraph
import sys
from pprint import pprint

testGraph1N3="""
@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix : <http://test/> .
:foo a rdfs:Class.
:bar :d :c.
:a :d :c.
"""


testGraph2N3="""
@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix : <http://test/> .
@prefix log: <http://www.w3.org/2000/10/swap/log#>.
:foo a rdfs:Resource.
:bar rdfs:isDefinedBy [ a log:Formula ].
:a :d :e.
"""

testGraph3N3="""
@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix log: <http://www.w3.org/2000/10/swap/log#>.
@prefix : <http://test/> .
<> a log:N3Document.
"""

sparqlQ = \
"""
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
SELECT *
FROM NAMED <graph1>
FROM NAMED <graph2>
FROM NAMED <graph3>
FROM <http://www.w3.org/2000/01/rdf-schema#>

WHERE {?sub ?pred rdfs:Class }"""

sparqlQ2 =\
"""
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
SELECT ?class
WHERE { GRAPH ?graph { ?member a ?class } }"""

sparqlQ3 =\
"""
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> 
PREFIX log: <http://www.w3.org/2000/10/swap/log#>
SELECT ?n3Doc
WHERE {?n3Doc a log:N3Document }"""


def testAggregateRaw():
    memStore = plugin.get('IOMemory',Store)()
    graph1 = Graph(memStore)
    graph2 = Graph(memStore)
    graph3 = Graph(memStore)

    for n3Str,graph in [(testGraph1N3,graph1),
                        (testGraph2N3,graph2),
                        (testGraph3N3,graph3)]:
        graph.parse(StringIO(n3Str),format='n3')

    G = ReadOnlyGraphAggregate([graph1,graph2,graph3])

    #Test triples
    assert len(list(G.triples((None,RDF.type,None))))                  == 4
    assert len(list(G.triples((URIRef("http://test/bar"),None,None)))) == 2
    assert len(list(G.triples((None,URIRef("http://test/d"),None))))   == 3

    #Test __len__
    assert len(G) == 8

    #Test __contains__
    assert (URIRef("http://test/foo"),RDF.type,RDFS.Resource) in G

    barPredicates = [URIRef("http://test/d"),RDFS.isDefinedBy]
    assert len(list(G.triples_choices((URIRef("http://test/bar"),barPredicates,None)))) == 2

def testAggregateSPARQL():
    memStore = plugin.get('IOMemory',Store)()
    graph1 = Graph(memStore,URIRef("graph1"))
    graph2 = Graph(memStore,URIRef("graph2"))
    graph3 = Graph(memStore,URIRef("graph3"))

    for n3Str,graph in [(testGraph1N3,graph1),
                        (testGraph2N3,graph2),
                        (testGraph3N3,graph3)]:
        graph.parse(StringIO(n3Str),format='n3')

    graph4 = Graph(memStore,RDFS.RDFSNS)
    graph4.parse(RDFS.RDFSNS)
    G = ConjunctiveGraph(memStore)
    rt =  G.query(sparqlQ)
    assert len(rt) > 1
    #print rt.serialize(format='xml')
    LOG_NS = Namespace(u'http://www.w3.org/2000/10/swap/log#')
    rt=G.query(sparqlQ2,initBindings={u'?graph' : URIRef("graph3")})
    #print rt.serialize(format='json')
    assert rt.serialize('python')[0] == LOG_NS.N3Document,str(rt)

def testDefaultGraph():
    memStore = plugin.get('IOMemory',Store)()
    graph1 = Graph(memStore,URIRef("graph1"))
    graph2 = Graph(memStore,URIRef("graph2"))
    graph3 = Graph(memStore,URIRef("graph3"))
    
    for n3Str,graph in [(testGraph1N3,graph1),
                        (testGraph2N3,graph2),
                        (testGraph3N3,graph3)]:
        graph.parse(StringIO(n3Str),format='n3')
    G = ConjunctiveGraph(memStore)
    #test that CG includes triples from all 3
    assert G.query(sparqlQ3),"CG as default graph should *all* triples"
    assert not graph2.query(sparqlQ3),"Graph as default graph should *not* include triples from other graphs"

if __name__ == '__main__':
    #testAggregateRaw()
    #testAggregateSPARQL()
    testDefaultGraph()
########NEW FILE########
__FILENAME__ = test
from rdflib.sparql.bison import Parse
from rdflib.sparql.bison.SPARQLEvaluate import Evaluate
from rdflib import plugin, Namespace,URIRef, RDF
from rdflib.store import Store, VALID_STORE, CORRUPTED_STORE, NO_STORE, UNKNOWN
from rdflib.Graph import Graph, ConjunctiveGraph
from sets import Set
import os
from cStringIO import StringIO
from pprint import pprint

EVALUATE = True
DEBUG_PARSE = False
STORE='IOMemory'
configString = ''

#class TestClassAndType(unittest.TestCase):
#
#    def setUp(self):
#
#    def tearDown(self):
#
#    def testType(self):
#
#    def testClass1(self):

test = [
    'data/local-constr/expr-2.rq',
    #'data/examples/ex11.2.3.2_1.rq',
    #'data/TypePromotion/tP-unsignedByte-short.rq'
    #'data/examples/ex11.2.3.1_0.rq',
    #'data/ValueTesting/typePromotion-decimal-decimal-pass.rq',
#    'data/examples/ex11.2.3.2_0.rq',
#    'data/SyntaxFull/syntax-union-02.rq',
    #'data/part1/dawg-query-004.rq',

]

tests2Skip = [
    'data/examples/ex11.2.3.1_1.rq',#Compares dateTime with same time, different time-zones
    'data/examples/ex11_1.rq', #Compares with literal BNode labels!
    'data/SyntaxFull/syntax-bnodes-03.rq', #BNode as a predicate (not allowed by grammar)
    'data/SyntaxFull/syntax-qname-04.rq', #Grammar Ambiguity with ':' matching as QNAME & QNAME_NS
    'data/SyntaxFull/syntax-qname-05.rq', #Same as above
    'data/SyntaxFull/syntax-qname-11.rq', #Same as above
    'data/SyntaxFull/syntax-lit-10.rq'  , #BisonGen's Lexer is chopping up STRING_LITERAL_LONG1 tokens
    'data/SyntaxFull/syntax-lit-12.rq'  , #same as above
    'data/SyntaxFull/syntax-lit-14.rq'  , #same as above
    'data/SyntaxFull/syntax-lit-15.rq'  , #same as above
    'data/SyntaxFull/syntax-lit-16.rq'  , #same as above
    'data/SyntaxFull/syntax-lit-17.rq'  , #same as above
    'data/SyntaxFull/syntax-lit-20.rq'  , #same as above
    'data/unsaid-inference/query-01.rq' , #WHERE without '{ }'
    'data/unsaid-inference/query-02.rq' , #same as above
    'data/unsaid-inference/query-03.rq' , #same as above
    'data/part1/dawg-query-001.rq'      , #no space between variable name and }: .. OPTIONAL { ?person foaf:mbox ?mbox}
    'data/part1/dawg-query-003.rq'      , #Same as above
    'data/regex/regex-query-003.rq'     , #BisonGen's Lexer is chopping up STRING_LITERAL_LONG1 tokens
    'data/regex/regex-query-004.rq'     , #Same as above
    'data/simple2/dawg-tp-01.rq'        , #WHERE without '{ }'
    'data/simple2/dawg-tp-02.rq'        , #same as above
    'data/simple2/dawg-tp-03.rq'        , #same as above
    'data/simple2/dawg-tp-04.rq'        , #same as above
    'data/SourceSimple/source-simple-01.rq', #WHERE without '{ }'
    'data/SourceSimple/source-simple-02.rq', #Illegal syntax
    'data/SourceSimple/source-simple-03.rq', #Illegal syntax
    'data/SourceSimple/source-simple-04.rq', #Illegal syntax
    'data/SourceSimple/source-simple-05.rq', #Illegal syntax
    'data/source-named/query-8.1.rq', #WHERE without '{ }'
    'data/source-named/query-8.2.rq', #same as above
    'data/source-named/query-8.3.rq', #same as above
    'data/source-named/query-8.4.rq', #same as above
    'data/source-named/query-8.5.rq', #same as above
    'data/source-named/query-9.1.rq', #same as above
    'data/source-named/query-9.2.rq', #same as above
    'data/survey/query-survey-1.rq', #not sure if the VARNAME token includes ']'.  If it does then the test is invalid
    'data/survey/query-survey-9.rq', #same as above
    'data/Sorting/one-of-one-column.rq' #same as above
    'data/ValueTesting/dateTime-tz0.rq', #bad syntax
    'data/Sorting/one-of-one-column.rq',#not sure if the VARNAME token includes ']'.  If it does then the test is invalid
    'data/ValueTesting/dateTime-tz0.rq',#bad syntax
    'data/ValueTesting/dateTime-tz1.rq',#same as above
    'data/ValueTesting/boolean-logical-OR.rq',#boolean literal is lowercase not uppercase
    'data/ValueTesting/boolean-true-canonical.rq',#same as above
    'data/ValueTesting/boolean-EBV-canonical.rq',#samve as above
    'data/ValueTesting/boolean-equiv-TRUE.rq',#same as above
    'data/ValueTesting/boolean-false-canonical.r',#same as above
    'data/ValueTesting/boolean-false-canonical.rq',#
    'data/ValueTesting/boolean-equiv-FALSE.rq',#
    'data/ValueTesting/extendedType-ne-pass.rq',#[27] Constraint ::= 'FILTER' BrackettedExpression <--
    'data/examples/ex11_0.rq', #TimeZone info on xsd:dateTime
    'data/local-constr/expr-2.rq', #Unable to deal with external filter against variable visible only to OPTIONAL
]


MANIFEST_NS = Namespace('http://www.w3.org/2001/sw/DataAccess/tests/test-manifest#')
MANIFEST_QUERY_NS = Namespace('http://www.w3.org/2001/sw/DataAccess/tests/test-query#')
TEST_BASE = Namespace('http://www.w3.org/2001/sw/DataAccess/tests/')
RESULT_NS = Namespace('http://www.w3.org/2001/sw/DataAccess/tests/result-set#')

manifestNS = {
    u"rdfs": Namespace("http://www.w3.org/2000/01/rdf-schema#"),
    u"mf"  : Namespace("http://www.w3.org/2001/sw/DataAccess/tests/test-manifest#"),
    u"qt"  : Namespace("http://www.w3.org/2001/sw/DataAccess/tests/test-query#"),
}

MANIFEST_QUERY = \
"""
SELECT ?source ?testName ?testComment ?result
WHERE {
  ?testCase mf:action    ?testAction;
            mf:name      ?testName;
            mf:result    ?result.
  ?testAction qt:query ?query;
              qt:data  ?source.

  OPTIONAL { ?testCase rdfs:comment ?testComment }

}"""

PARSED_MANIFEST_QUERY = Parse(MANIFEST_QUERY)

def bootStrapStore(store):
    rt = store.open(configString,create=False)
    if rt == NO_STORE:
        store.open(configString,create=True)
    else:
        store.destroy(configString)
        store.open(configString,create=True)

def trialAndErrorRTParse(graph,queryLoc,DEBUG):
    qstr = StringIO(open(queryLoc).read())
    try:
        graph.parse(qstr,format='n3')
        return True
    except Exception, e:
        if DEBUG:
            print e
            print "#### Parse Failure (N3) ###"
            print qstr.getvalue()
            print "#####"*5
        try:
            graph.parse(qstr)
            assert list(graph.objects(None,RESULT_NS.resultVariable))
            return True
        except Exception, e:
            if DEBUG:
                print e
                print "#### Parse Failure (RDF/XML) ###"
                print qstr.getvalue()
                print "#### ######### ###"
            return False

def testBasic(DEBUG = False):
    from glob import glob
    from sre import sub
    for testFile in glob('data/examples/*.rq'):#glob('data/*/*.rq'):
        store = plugin.get(STORE,Store)()
        bootStrapStore(store)
        store.commit()

        prefix = testFile.split('.rq')[-1]
        manifestPath = '/'.join(testFile.split('/')[:-1]+['manifest.n3'])
        manifestPath2 = '/'.join(testFile.split('/')[:-1]+['manifest.ttl'])
        queryFileName = testFile.split('/')[-1]
        store = plugin.get(STORE,Store)()
        store.open(configString,create=False)
        assert len(store) == 0
        manifestG=ConjunctiveGraph(store)
        if not os.path.exists(manifestPath):
            assert os.path.exists(manifestPath2)
            manifestPath = manifestPath2
        manifestG.default_context.parse(open(manifestPath),publicID=TEST_BASE,format='n3')
        manifestData = \
           manifestG.query(
                                  PARSED_MANIFEST_QUERY,
                                  initBindings={'?query' : TEST_BASE[queryFileName]},
                                  initNs=manifestNS,
                                  DEBUG = False)
        store.rollback()
        store.close()
        for source,testCaseName,testCaseComment,expectedRT in manifestData:

            if expectedRT:
                expectedRT = '/'.join(testFile.split('/')[:-1]+[expectedRT.replace(TEST_BASE,'')])
            if source:
                source = '/'.join(testFile.split('/')[:-1]+[source.replace(TEST_BASE,'')])

            testCaseName = testCaseComment and testCaseComment or testCaseName
            print "## Source: %s ##"%source
            print "## Test: %s ##"%testCaseName
            print "## Result: %s ##"%expectedRT

            #Expected results
            if expectedRT:
                store = plugin.get(STORE,Store)()
                store.open(configString,create=False)
                resultG=ConjunctiveGraph(store).default_context
#                if DEBUG:
#                    print "###"*10
#                    print "parsing: ", open(expectedRT).read()
#                    print "###"*10
                assert len(store) == 0
                print "## Parsing (%s) ##"%(expectedRT)
                if not trialAndErrorRTParse(resultG,expectedRT,DEBUG):
                    if DEBUG:
                        print "Unexpected result format (for %s), skipping"%(expectedRT)
                    store.rollback()
                    store.close()
                    continue
                if DEBUG:
                    print "## Done .. ##"

                rtVars = [rtVar for rtVar in resultG.objects(None,RESULT_NS.resultVariable)]
                bindings = []
                resultSetNode = resultG.value(predicate=RESULT_NS.value,object=RESULT_NS.ResultSet)
                for solutionNode in resultG.objects(resultSetNode,RESULT_NS.solution):
                    bindingDict = dict([(key,None) for key in rtVars])
                    for bindingNode in resultG.objects(solutionNode,RESULT_NS.binding):
                        value = resultG.value(subject=bindingNode,predicate=RESULT_NS.value)
                        name  = resultG.value(subject=bindingNode,predicate=RESULT_NS.variable)
                        bindingDict[name] = value
                    bindings.append(tuple([bindingDict[vName] for vName in rtVars]))
                if DEBUG:
                    print "Expected bindings: ", bindings
                    print open(expectedRT).read()
                store.rollback()
                store.close()

            if testFile.startswith('data/NegativeSyntax'):
                try:
                    query = open(testFile).read()
                    p = Parse(query,DEBUG)
                except:
                    continue
                else:
                    raise Exception("Test %s should have failed!"%testFile)
            if testFile in tests2Skip:
                print "Skipping test (%s)"%testCaseName
                continue
            query = open(testFile).read()
            print "### %s (%s) ###"%(testCaseName,testFile)
            print query
            p = Parse(query,DEBUG_PARSE)
            if DEBUG:
                print p
            if EVALUATE and source:
                if DEBUG:
                    print "### Source Graph: ###"
                    print open(source).read()
                store = plugin.get(STORE,Store)()
                store.open(configString,create=False)
                g=ConjunctiveGraph(store)
                try:
                    g.parse(open(source),format='n3')
                except:
                    print "Unexpected data format (for %s), skipping"%(source)
                    store.rollback()
                    store.close()
                    continue
                #print store
                rt = g.query(p,DEBUG = DEBUG)
                if expectedRT:
                    if rt != bindings and Set([Set(i) for i in rt]) != Set([Set(i) for i in bindings]):#unorderedComparison(rt,bindings):
                        print "### Expected Result (%s) ###"%expectedRT
                        pprint(bindings)
                        print "### Actual Results ###"
                        pprint(rt)
                        raise Exception("### TEST FAILED!: %s ###"%testCaseName)
                    else:
                        print "### TEST PASSED!: %s ###"%testCaseName
                store.rollback()
if __name__ == '__main__':
    import sys
    if len(sys.argv) > 1:
        testBasic(bool(int(sys.argv[1])))
    else:
        testBasic()
#    suite1 = unittest.makeSuite(TestClassAndType)
#    suite2 = unittest.makeSuite(TestReason)
#    unittest.TextTestRunner(verbosity=3).run(suite1)
#    unittest.TextTestRunner(verbosity=3).run(suite2)

########NEW FILE########
__FILENAME__ = context
import unittest

from tempfile import mkdtemp
from rdflib import *
from rdflib.Graph import Graph

class ContextTestCase(unittest.TestCase):
    #store = 'Memory'
    store = 'default'
    slowtest = True

    def setUp(self):
        self.graph = ConjunctiveGraph(store=self.store)
        if self.store == "MySQL":
            from mysql import configString
            from rdflib.store.MySQL import MySQL
            path=configString
            MySQL().destroy(path)
        else:
            path = a_tmp_dir = mkdtemp()
        self.graph.open(path, create=True)
        self.michel = URIRef(u'michel')
        self.tarek = URIRef(u'tarek')
        self.bob = URIRef(u'bob')
        self.likes = URIRef(u'likes')
        self.hates = URIRef(u'hates')
        self.pizza = URIRef(u'pizza')
        self.cheese = URIRef(u'cheese')

        self.c1 = URIRef(u'context-1')
        self.c2 = URIRef(u'context-2')

        # delete the graph for each test!
        self.graph.remove((None, None, None))

    def tearDown(self):
        self.graph.close()

    def get_context(self, identifier):
        assert isinstance(identifier, URIRef) or \
               isinstance(identifier, BNode), type(identifier)
        return Graph(store=self.graph.store, identifier=identifier,
                         namespace_manager=self)
    def addStuff(self):
        tarek = self.tarek
        michel = self.michel
        bob = self.bob
        likes = self.likes
        hates = self.hates
        pizza = self.pizza
        cheese = self.cheese
        c1 = self.c1
        graph = Graph(self.graph.store, c1)

        graph.add((tarek, likes, pizza))
        graph.add((tarek, likes, cheese))
        graph.add((michel, likes, pizza))
        graph.add((michel, likes, cheese))
        graph.add((bob, likes, cheese))
        graph.add((bob, hates, pizza))
        graph.add((bob, hates, michel)) # gasp!

    def removeStuff(self):
        tarek = self.tarek
        michel = self.michel
        bob = self.bob
        likes = self.likes
        hates = self.hates
        pizza = self.pizza
        cheese = self.cheese
        c1 = self.c1
        graph = Graph(self.graph.store, c1)

        graph.remove((tarek, likes, pizza))
        graph.remove((tarek, likes, cheese))
        graph.remove((michel, likes, pizza))
        graph.remove((michel, likes, cheese))
        graph.remove((bob, likes, cheese))
        graph.remove((bob, hates, pizza))
        graph.remove((bob, hates, michel)) # gasp!

    def addStuffInMultipleContexts(self):
        c1 = self.c1
        c2 = self.c2
        triple = (self.pizza, self.hates, self.tarek) # revenge!

        # add to default context
        self.graph.add(triple)
        # add to context 1
        graph = Graph(self.graph.store, c1)
        graph.add(triple)
        # add to context 2
        graph = Graph(self.graph.store, c2)
        graph.add(triple)

    def testConjunction(self):
        self.addStuffInMultipleContexts()
        triple = (self.pizza, self.likes, self.pizza)
        # add to context 1
        graph = Graph(self.graph.store, self.c1)
        graph.add(triple)
        self.assertEquals(len(self.graph), len(graph))

    def testAdd(self):
        self.addStuff()

    def testRemove(self):
        self.addStuff()
        self.removeStuff()

    def testLenInOneContext(self):
        c1 = self.c1
        # make sure context is empty

        self.graph.remove_context(self.get_context(c1))
        graph = Graph(self.graph.store, c1)
        oldLen = len(self.graph)

        for i in range(0, 10):
            graph.add((BNode(), self.hates, self.hates))
        self.assertEquals(len(graph), oldLen + 10)
        self.assertEquals(len(self.get_context(c1)), oldLen + 10)
        self.graph.remove_context(self.get_context(c1))
        self.assertEquals(len(self.graph), oldLen)
        self.assertEquals(len(graph), 0)

    def testLenInMultipleContexts(self):
        oldLen = len(self.graph)
        self.addStuffInMultipleContexts()

        # addStuffInMultipleContexts is adding the same triple to
        # three different contexts. So it's only + 1
        self.assertEquals(len(self.graph), oldLen + 1) 

        graph = Graph(self.graph.store, self.c1)
        self.assertEquals(len(graph), oldLen + 1)

    def testRemoveInMultipleContexts(self):
        c1 = self.c1
        c2 = self.c2
        triple = (self.pizza, self.hates, self.tarek) # revenge!

        self.addStuffInMultipleContexts()

        # triple should be still in store after removing it from c1 + c2
        self.assert_(triple in self.graph)
        graph = Graph(self.graph.store, c1)
        graph.remove(triple)
        self.assert_(triple in self.graph)
        graph = Graph(self.graph.store, c2)
        graph.remove(triple)
        self.assert_(triple in self.graph)
        self.graph.remove(triple)
        # now gone!
        self.assert_(triple not in self.graph)

        # add again and see if remove without context removes all triples!
        self.addStuffInMultipleContexts()
        self.graph.remove(triple)
        self.assert_(triple not in self.graph)

    def testContexts(self):
        triple = (self.pizza, self.hates, self.tarek) # revenge!

        self.addStuffInMultipleContexts()
        def cid(c):
            return c.identifier
        self.assert_(self.c1 in map(cid, self.graph.contexts()))
        self.assert_(self.c2 in map(cid, self.graph.contexts()))

        contextList = map(cid, list(self.graph.contexts(triple)))
        self.assert_(self.c1 in contextList)
        self.assert_(self.c2 in contextList)

    def testRemoveContext(self):
        c1 = self.c1

        self.addStuffInMultipleContexts()
        self.assertEquals(len(Graph(self.graph.store, c1)), 1)
        self.assertEquals(len(self.get_context(c1)), 1)

        self.graph.remove_context(self.get_context(c1))
        self.assert_(self.c1 not in self.graph.contexts())

    def testRemoveAny(self):
        Any = None
        self.addStuffInMultipleContexts()
        self.graph.remove((Any, Any, Any))
        self.assertEquals(len(self.graph), 0)

    def testTriples(self):
        tarek = self.tarek
        michel = self.michel
        bob = self.bob
        likes = self.likes
        hates = self.hates
        pizza = self.pizza
        cheese = self.cheese
        c1 = self.c1
        asserte = self.assertEquals
        triples = self.graph.triples
        graph = self.graph
        c1graph = Graph(self.graph.store, c1)
        c1triples = c1graph.triples
        Any = None

        self.addStuff()

        # unbound subjects with context
        asserte(len(list(c1triples((Any, likes, pizza)))), 2)
        asserte(len(list(c1triples((Any, hates, pizza)))), 1)
        asserte(len(list(c1triples((Any, likes, cheese)))), 3)
        asserte(len(list(c1triples((Any, hates, cheese)))), 0)

        # unbound subjects without context, same results!
        asserte(len(list(triples((Any, likes, pizza)))), 2)
        asserte(len(list(triples((Any, hates, pizza)))), 1)
        asserte(len(list(triples((Any, likes, cheese)))), 3)
        asserte(len(list(triples((Any, hates, cheese)))), 0)

        # unbound objects with context
        asserte(len(list(c1triples((michel, likes, Any)))), 2)
        asserte(len(list(c1triples((tarek, likes, Any)))), 2)
        asserte(len(list(c1triples((bob, hates, Any)))), 2)
        asserte(len(list(c1triples((bob, likes, Any)))), 1)

        # unbound objects without context, same results!
        asserte(len(list(triples((michel, likes, Any)))), 2)
        asserte(len(list(triples((tarek, likes, Any)))), 2)
        asserte(len(list(triples((bob, hates, Any)))), 2)
        asserte(len(list(triples((bob, likes, Any)))), 1)

        # unbound predicates with context
        asserte(len(list(c1triples((michel, Any, cheese)))), 1)
        asserte(len(list(c1triples((tarek, Any, cheese)))), 1)
        asserte(len(list(c1triples((bob, Any, pizza)))), 1)
        asserte(len(list(c1triples((bob, Any, michel)))), 1)

        # unbound predicates without context, same results!
        asserte(len(list(triples((michel, Any, cheese)))), 1)
        asserte(len(list(triples((tarek, Any, cheese)))), 1)
        asserte(len(list(triples((bob, Any, pizza)))), 1)
        asserte(len(list(triples((bob, Any, michel)))), 1)

        # unbound subject, objects with context
        asserte(len(list(c1triples((Any, hates, Any)))), 2)
        asserte(len(list(c1triples((Any, likes, Any)))), 5)

        # unbound subject, objects without context, same results!
        asserte(len(list(triples((Any, hates, Any)))), 2)
        asserte(len(list(triples((Any, likes, Any)))), 5)

        # unbound predicates, objects with context
        asserte(len(list(c1triples((michel, Any, Any)))), 2)
        asserte(len(list(c1triples((bob, Any, Any)))), 3)
        asserte(len(list(c1triples((tarek, Any, Any)))), 2)

        # unbound predicates, objects without context, same results!
        asserte(len(list(triples((michel, Any, Any)))), 2)
        asserte(len(list(triples((bob, Any, Any)))), 3)
        asserte(len(list(triples((tarek, Any, Any)))), 2)

        # unbound subjects, predicates with context
        asserte(len(list(c1triples((Any, Any, pizza)))), 3)
        asserte(len(list(c1triples((Any, Any, cheese)))), 3)
        asserte(len(list(c1triples((Any, Any, michel)))), 1)

        # unbound subjects, predicates without context, same results!
        asserte(len(list(triples((Any, Any, pizza)))), 3)
        asserte(len(list(triples((Any, Any, cheese)))), 3)
        asserte(len(list(triples((Any, Any, michel)))), 1)

        # all unbound with context
        asserte(len(list(c1triples((Any, Any, Any)))), 7)
        # all unbound without context, same result!
        asserte(len(list(triples((Any, Any, Any)))), 7)

        for c in [graph, self.get_context(c1)]:
            # unbound subjects
            asserte(set(c.subjects(likes, pizza)), set((michel, tarek)))
            asserte(set(c.subjects(hates, pizza)), set((bob,)))
            asserte(set(c.subjects(likes, cheese)), set([tarek, bob, michel]))
            asserte(set(c.subjects(hates, cheese)), set())

            # unbound objects
            asserte(set(c.objects(michel, likes)), set([cheese, pizza]))
            asserte(set(c.objects(tarek, likes)), set([cheese, pizza]))
            asserte(set(c.objects(bob, hates)), set([michel, pizza]))
            asserte(set(c.objects(bob, likes)), set([cheese]))

            # unbound predicates
            asserte(set(c.predicates(michel, cheese)), set([likes]))
            asserte(set(c.predicates(tarek, cheese)), set([likes]))
            asserte(set(c.predicates(bob, pizza)), set([hates]))
            asserte(set(c.predicates(bob, michel)), set([hates]))

            asserte(set(c.subject_objects(hates)), set([(bob, pizza), (bob, michel)]))
            asserte(set(c.subject_objects(likes)), set([(tarek, cheese), (michel, cheese), (michel, pizza), (bob, cheese), (tarek, pizza)]))

            asserte(set(c.predicate_objects(michel)), set([(likes, cheese), (likes, pizza)]))
            asserte(set(c.predicate_objects(bob)), set([(likes, cheese), (hates, pizza), (hates, michel)]))
            asserte(set(c.predicate_objects(tarek)), set([(likes, cheese), (likes, pizza)]))

            asserte(set(c.subject_predicates(pizza)), set([(bob, hates), (tarek, likes), (michel, likes)]))
            asserte(set(c.subject_predicates(cheese)), set([(bob, likes), (tarek, likes), (michel, likes)]))
            asserte(set(c.subject_predicates(michel)), set([(bob, hates)]))

            asserte(set(c), set([(bob, hates, michel), (bob, likes, cheese), (tarek, likes, pizza), (michel, likes, pizza), (michel, likes, cheese), (bob, hates, pizza), (tarek, likes, cheese)]))

        # remove stuff and make sure the graph is empty again
        self.removeStuff()
        asserte(len(list(c1triples((Any, Any, Any)))), 0)
        asserte(len(list(triples((Any, Any, Any)))), 0)

# tested via ContextTestCase
#class IOMemoryContextTestCase(ContextTestCase):
#    store = "IOMemory"
#    slowtest = False

try:
    import persistent
    # If we can import persistent then test ZODB store
    class ZODBContextTestCase(ContextTestCase):
        store = "ZODB"
        slowtest = False
except ImportError, e:
    print "Can not test ZODB store: %s" % e

try:
    import MySQLdb
    # If we can import RDF then test Redland store
    class MySQLContextTestCase(ContextTestCase):
        store = "MySQL"
        slowtest = False
except ImportError, e:
    "Can not test MySQL store: %s" % e

try:
    import RDF
    # If we can import RDF then test Redland store
    class RedlandContextTestCase(ContextTestCase):
        store = "Redland"
        slowtest = False
except ImportError, e:
    print "Can not test Redland store: %s" % e


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = events

import unittest
from rdflib import events

class AddedEvent(events.Event): pass

class RemovedEvent(events.Event): pass

def subscribe_to(source, target):
    target.subscribe(AddedEvent, source._add_handler)
    target.subscribe(RemovedEvent, source._remove_handler)

def subscribe_all(caches):
    for cache in caches:
        for other in caches:
            if other != cache:
                subscribe_to(cache, other)

class Cache(events.Dispatcher):

    def __init__(self, data=None):
        if data is None: data = {}
        self._data = data
        self.subscribe(AddedEvent, self._add_handler)
        self.subscribe(RemovedEvent, self._remove_handler)        

    def _add_handler(self, event):
        self._data[event.key] = event.value

    def _remove_handler(self, event):
        del self._data[event.key]

    def __getitem__(self, key):
        return self._data[key]

    def __setitem__(self, key, value):
        self.dispatch(AddedEvent(key=key, value=value))
        
    def __delitem__(self, key):
        self.dispatch(RemovedEvent(key=key))

    def has_key(self, key):
        return self._data.has_key(key)


class EventTestCase(unittest.TestCase):

    def testEvents(self):
        c1 = Cache()
        c2 = Cache()
        c3 = Cache()
        subscribe_all([c1,c2,c3])
        c1['bob'] = 'uncle'
        assert c2['bob'] == 'uncle'
        assert c3['bob'] == 'uncle'
        del c3['bob']
        assert c1.has_key('bob') == False
        assert c2.has_key('bob') == False

if __name__ == "__main__":
    unitest.main()

########NEW FILE########
__FILENAME__ = graph
import unittest

from tempfile import mkdtemp

from rdflib import URIRef, BNode, Literal, RDF
from rdflib.Graph import Graph

class GraphTestCase(unittest.TestCase):
    store_name = 'default'
    path = None
    slowtest = True

    def setUp(self):
        self.graph = Graph(store=self.store_name)
        a_tmp_dir = mkdtemp()
        self.path = self.path or a_tmp_dir
        self.graph.open(self.path)

        self.michel = URIRef(u'michel')
        self.tarek = URIRef(u'tarek')
        self.bob = URIRef(u'bob')
        self.likes = URIRef(u'likes')
        self.hates = URIRef(u'hates')
        self.pizza = URIRef(u'pizza')
        self.cheese = URIRef(u'cheese')

    def tearDown(self):
        self.graph.close()

    def addStuff(self):
        tarek = self.tarek
        michel = self.michel
        bob = self.bob
        likes = self.likes
        hates = self.hates
        pizza = self.pizza
        cheese = self.cheese

        self.graph.add((tarek, likes, pizza))
        self.graph.add((tarek, likes, cheese))
        self.graph.add((michel, likes, pizza))
        self.graph.add((michel, likes, cheese))
        self.graph.add((bob, likes, cheese))
        self.graph.add((bob, hates, pizza))
        self.graph.add((bob, hates, michel)) # gasp!

    def removeStuff(self):
        tarek = self.tarek
        michel = self.michel
        bob = self.bob
        likes = self.likes
        hates = self.hates
        pizza = self.pizza
        cheese = self.cheese

        self.graph.remove((tarek, likes, pizza))
        self.graph.remove((tarek, likes, cheese))
        self.graph.remove((michel, likes, pizza))
        self.graph.remove((michel, likes, cheese))
        self.graph.remove((bob, likes, cheese))
        self.graph.remove((bob, hates, pizza))
        self.graph.remove((bob, hates, michel)) # gasp!

    def testAdd(self):
        self.addStuff()

    def testRemove(self):
        self.addStuff()
        self.removeStuff()

    def testTriples(self):
        tarek = self.tarek
        michel = self.michel
        bob = self.bob
        likes = self.likes
        hates = self.hates
        pizza = self.pizza
        cheese = self.cheese
        asserte = self.assertEquals
        triples = self.graph.triples
        Any = None

        self.addStuff()

        # unbound subjects
        asserte(len(list(triples((Any, likes, pizza)))), 2)
        asserte(len(list(triples((Any, hates, pizza)))), 1)
        asserte(len(list(triples((Any, likes, cheese)))), 3)
        asserte(len(list(triples((Any, hates, cheese)))), 0)

        # unbound objects
        asserte(len(list(triples((michel, likes, Any)))), 2)
        asserte(len(list(triples((tarek, likes, Any)))), 2)
        asserte(len(list(triples((bob, hates, Any)))), 2)
        asserte(len(list(triples((bob, likes, Any)))), 1)

        # unbound predicates
        asserte(len(list(triples((michel, Any, cheese)))), 1)
        asserte(len(list(triples((tarek, Any, cheese)))), 1)
        asserte(len(list(triples((bob, Any, pizza)))), 1)
        asserte(len(list(triples((bob, Any, michel)))), 1)

        # unbound subject, objects
        asserte(len(list(triples((Any, hates, Any)))), 2)
        asserte(len(list(triples((Any, likes, Any)))), 5)

        # unbound predicates, objects
        asserte(len(list(triples((michel, Any, Any)))), 2)
        asserte(len(list(triples((bob, Any, Any)))), 3)
        asserte(len(list(triples((tarek, Any, Any)))), 2)

        # unbound subjects, predicates
        asserte(len(list(triples((Any, Any, pizza)))), 3)
        asserte(len(list(triples((Any, Any, cheese)))), 3)
        asserte(len(list(triples((Any, Any, michel)))), 1)

        # all unbound
        asserte(len(list(triples((Any, Any, Any)))), 7)
        self.removeStuff()
        asserte(len(list(triples((Any, Any, Any)))), 0)


    def testStatementNode(self):
        graph = self.graph

        from rdflib.Statement import Statement
        c = URIRef("http://example.org/foo#c")
        r = URIRef("http://example.org/foo#r")
        s = Statement((self.michel, self.likes, self.pizza), c)
        graph.add((s, RDF.value, r))
        self.assertEquals(r, graph.value(s, RDF.value))
        self.assertEquals(s, graph.value(predicate=RDF.value, object=r))

    def testGraphValue(self):
        from rdflib.Graph import GraphValue

        graph = self.graph

        alice = URIRef("alice")
        bob = URIRef("bob")
        pizza = URIRef("pizza")
        cheese = URIRef("cheese")

        g1 = Graph()
        g1.add((alice, RDF.value, pizza))
        g1.add((bob, RDF.value, cheese))
        g1.add((bob, RDF.value, pizza))

        g2 = Graph()
        g2.add((bob, RDF.value, pizza))
        g2.add((bob, RDF.value, cheese))
        g2.add((alice, RDF.value, pizza))

        gv1 = GraphValue(store=graph.store, graph=g1)
        gv2 = GraphValue(store=graph.store, graph=g2)
        graph.add((gv1, RDF.value, gv2))
        v = graph.value(gv1)
        #print type(v)
        self.assertEquals(gv2, v)
        #print list(gv2)
        #print gv2.identifier
        graph.remove((gv1, RDF.value, gv2))

    def testConnected(self):
        graph = self.graph
        self.addStuff()
        self.assertEquals(True, graph.connected())

        jeroen = URIRef("jeroen")
        unconnected = URIRef("unconnected")

        graph.add((jeroen,self.likes,unconnected))

        self.assertEquals(False, graph.connected())

#class MemoryGraphTestCase(GraphTestCase):
#    store_name = "Memory"
#    slowtest = False


try:
    import persistent
    # If we can import persistent then test ZODB store
    class ZODBGraphTestCase(GraphTestCase):
        store_name = "ZODB"
        slowtest = False
except ImportError, e:
    print "Can not test ZODB store: %s" % e


try:
    import RDF as Redland # don't shadow RDF ns imported above
    # If we can import RDF then test Redland store
    class RedLandTestCase(GraphTestCase):
        store_name = "Redland"
        slowtest = False
except ImportError, e:
    print "Can not test Redland store: %s" % e

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = IdentifierEquality
import unittest

from rdflib import URIRef, BNode, Literal

from rdflib.syntax.parsers.RDFXMLHandler import CORE_SYNTAX_TERMS

from rdflib.Graph import Graph
from rdflib import RDF

"""
Ah... it's coming back to me...
[6:32p] eikeon: think it's so transitivity holds...
[6:32p] eikeon: if a==b and b==c then a should == c
[6:32p] eikeon: "foo"==Literal("foo")
[6:33p] eikeon: We don't want URIRef("foo")==Literal("foo")
[6:33p] eikeon: But if we have URIRef("foo")=="foo" then it implies it.
[6:33p] chimezie: yes, definately not the other RDFLib 'typed' RDF (and N3) terms
[6:34p] eikeon: Why do you need URIRef("foo")=="foo" ?
[6:34p] chimezie: i'm just wondering if a URI and a string with the same lexical value, are by definition 'different'
[6:35p] eikeon: Think so, actually. Think of trying to serialize some triples.
[6:36p] eikeon: If they are the same you'd serialize them the same, no?
[6:36p] chimezie: I guess I was thinking of a 'string' in a native datatype sense, not in the RDF sense (where they would be distinctly different)
[6:37p] eikeon: We should try and brain dump some of this...
[6:37p] eikeon: it look a fairly long time to work out.
[6:37p] eikeon: But think we finally landed in the right spot.
[6:38p] eikeon: I know many of the backends break if URIRef("foo")==Literal("foo")
[6:39p] eikeon: And if we want "foo"==Literal("foo") --- then we really can't have URIRef("foo") also == "foo"
"""
class IdentifierEquality(unittest.TestCase):

    def setUp(self):
        self.uriref = URIRef("http://example.org/")
        self.bnode = BNode()
        self.literal = Literal("http://example.org/")
        self.python_literal = u"http://example.org/"
        self.python_literal_2 = u"foo"

    def testA(self):
        self.assertEquals(self.uriref==self.literal, False)

    def testB(self):
        self.assertEquals(self.literal==self.uriref, False)

    def testC(self):
        self.assertEquals(self.uriref==self.python_literal, False)

    def testD(self):
        self.assertEquals(self.python_literal==self.uriref, False)

    def testE(self):
        self.assertEquals(self.literal==self.python_literal, True)

    def testF(self):
        self.assertEquals(self.python_literal==self.literal, True)

    def testG(self):
        self.assertEquals("foo" in CORE_SYNTAX_TERMS, False)

    def testH(self):
        self.assertEquals(URIRef("http://www.w3.org/1999/02/22-rdf-syntax-ns#RDF") in CORE_SYNTAX_TERMS, True)

    def testI(self):
        g = Graph()
        g.add((self.uriref, RDF.value, self.literal))
        g.add((self.uriref, RDF.value, self.uriref))
        self.assertEqual(len(g), 2)


if __name__ == "__main__":
    unittest.main()



########NEW FILE########
__FILENAME__ = JSON
from rdflib import ConjunctiveGraph, plugin
from rdflib.store import Store
from StringIO import StringIO
import unittest

test_data = """ 
@prefix foaf:       <http://xmlns.com/foaf/0.1/> .
@prefix rdf:        <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .

<http://example.org/alice>  foaf:name       "Alice" .
<http://example.org/alice>  foaf:knows      <http://example.org/bob> .
<http://example.org/bob>  foaf:name       "Bob" .
"""

test_query = """
PREFIX foaf: <http://xmlns.com/foaf/0.1/>

SELECT ?name ?x ?friend
WHERE { ?x foaf:name ?name .
        OPTIONAL { ?x foaf:knows ?friend . }
}
"""

correct = """"name" : {"type": "literal", "xml:lang" : "None", "value" : "Bob"}
                   ,
                   "x" : {"type": "uri", "value" : "http://example.org/bob"}
                }"""

test_header_query = """
PREFIX foaf: <http://xmlns.com/foaf/0.1/>

SELECT ?name ?friend
WHERE { ?x foaf:name ?name .
        OPTIONAL { ?x foaf:knows ?friend . }
}
"""

# See Also: http://rdflib.net/pipermail/dev/2006-November/000112.html


class JSON(unittest.TestCase):

    def testComma(self):
        graph = ConjunctiveGraph(plugin.get('IOMemory',Store)())
        graph.parse(StringIO(test_data), format="n3")
        results = graph.query(test_query)
        result_json = results.serialize(format='json')
        self.failUnless(result_json.find(correct) > 0)

    def testHeader(self):
        graph = ConjunctiveGraph(plugin.get('IOMemory',Store)())
        graph.parse(StringIO(test_data), format="n3")
        results = graph.query(test_header_query)
        result_json = results.serialize(format='json')
        self.failUnless(result_json.find('"x",') == -1)                
        
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = mysql
from n3_2 import testN3Store,testN3,implies
from rdflib.Graph import QuotedGraph
try:
    from rdflib.store.MySQL import REGEXTerm
except ImportError, e:
    print "Can not test REGEX bits:", e
from rdflib import *
configString="user=,password=,host=localhost,db=test"

def testRegex():
    g = Graph(backend='MySQL')
    g.open(configString)
    g.parse(StringInputSource(testN3), format="n3")
    try:
        for s,p,o in g.triples((None,implies,None)):
            formulaA = s
            formulaB = o

        assert type(formulaA)==QuotedGraph and type(formulaB)==QuotedGraph
        a = URIRef('http://test/a')
        b = URIRef('http://test/b')
        c = URIRef('http://test/c')
        d = URIRef('http://test/d')

        universe = ConjunctiveGraph(g.backend)

        #REGEX triple matching
        assert len(list(universe.triples((None,REGEXTerm('.*22-rdf-syntax-ns.*'),None))))==1
        assert len(list(universe.triples((None,REGEXTerm('.*'),None))))==3
        assert len(list(universe.triples((REGEXTerm('.*formula.*$'),None,None))))==1
        assert len(list(universe.triples((None,None,REGEXTerm('.*formula.*$')))))==1
        assert len(list(universe.triples((None,REGEXTerm('.*implies$'),None))))==1
        for s,p,o in universe.triples((None,REGEXTerm('.*test.*'),None)):
            assert s==a
            assert o==c

        for s,p,o in formulaA.triples((None,REGEXTerm('.*type.*'),None)):
            assert o!=c or isinstance(o,BNode)

        #REGEX context matching
        assert len(list(universe.contexts((None,None,REGEXTerm('.*schema.*')))))==1
        assert len(list(universe.contexts((None,REGEXTerm('.*'),None))))==3

        #test optimized interfaces
        assert len(list(g.backend.subjects(RDF.type,[RDFS.Class,c])))==1
        for subj in g.backend.subjects(RDF.type,[RDFS.Class,c]):
            assert isinstance(subj,BNode)

        assert len(list(g.backend.subjects(implies,[REGEXTerm('.*')])))==1

        for subj in g.backend.subjects(implies,[formulaB,RDFS.Class]):
            assert subj.identifier == formulaA.identifier

        assert len(list(g.backend.subjects(REGEXTerm('.*'),[formulaB,c])))==2
        assert len(list(g.backend.subjects(None,[formulaB,c])))==2
        assert len(list(g.backend.subjects(None,[formulaB,c])))==2
        assert len(list(g.backend.subjects([REGEXTerm('.*rdf-syntax.*'),d],None)))==2

        assert len(list(g.backend.objects(None,RDF.type)))==1
        assert len(list(g.backend.objects(a,[d,RDF.type])))==1
        assert len(list(g.backend.objects(a,[d])))==1
        assert len(list(g.backend.objects(a,None)))==1
        assert len(list(g.backend.objects(a,[REGEXTerm('.*')])))==1
        assert len(list(g.backend.objects([a,c],None)))==1

    except:
        g.backend.destroy(configString)
        raise

testRegex.non_standard_dep = True

def testRun():
    testN3Store('MySQL',configString)
    testRegex()

testRun.non_standard_dep = True

def profileTests():
    from hotshot import Profile, stats
    p = Profile('rdflib-mysql.profile')
    p.runcall(testRun)
    p.close()

    s = stats.load('rdflib-mysql.profile')
    s.strip_dirs()
    s.sort_stats('time','cumulative','pcalls')
    #s.sort_stats('time','pcalls')
    s.print_stats(.1)
    s.print_callers(.1)
    s.print_callees(.1)

profileTests.non_standard_dep = True

if __name__=='__main__':
    testN3Store('MySQL',configString)
    testRegex()
    #profileTests()


########NEW FILE########
__FILENAME__ = n3
from rdflib import *

input = """
#  Definitions of terms describing the n3 model
#

@keywords a.

@prefix n3: <#>.
@prefix log: <log.n3#> .
@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix : <#> .

@forAll :s, :p, :x, :y, :z.

n3:Statement    a rdf:Class .
n3:StatementSet a rdf:Class .

n3:includes     a rdfs:Property .   # Cf rdf:li

n3:predicate    a rdf:Property; rdfs:domain n3:statement .
n3:subject      a rdf:Property; rdfs:domain n3:statement .
n3:object       a rdf:Property; rdfs:domain n3:statement .

n3:context      a rdf:Property; rdfs:domain n3:statement;
                rdfs:range n3:StatementSet .



########### Rules

{ :x :p :y . } log:means { [
                n3:subject :x;
                n3:predicate :p;
                n3:object :y ] a log:Truth}.

# Needs more thought ... ideally, we have the implcit AND rules of
# juxtaposition (introduction and elimination)

{
    {
        {  :x n3:includes :s. } log:implies { :y n3:includes :s. } .
    } forall :s1 .
} log:implies { :x log:implies :y } .

{
    {
        {  :x n3:includes :s. } log:implies { :y n3:includes :s. } .
    } forall :s1
} log:implies { :x log:implies :y } .

# I think n3:includes has to be axiomatic builtin. - unless you go to syntax description.
# syntax.n3?
"""



import unittest

from rdflib.Graph import Graph, ConjunctiveGraph


class N3TestCase(unittest.TestCase):

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def testFileName(self):
        input = """
@prefix : <http://www.example.com/> .

:foo.txt :p :q .
"""
        g = Graph()
        try:
            g.parse(StringInputSource(input), format="n3")
        except:
            pass
            #foo.txt is not a valid qname in n3/turtle
        else:
            self.assertEquals(True, False) # Didn't get expected result of a parse exception
        # This isn't the expected result based on my reading of n3 bits
        #s = g.value(predicate=URIRef("http://www.example.com/p"), object=URIRef("http://www.example.com/q"))
        #self.assertEquals(s, URIRef("http://www.example.org/foo.txt"))


    def testModel(self):
        g = ConjunctiveGraph()
        g.parse(StringInputSource(input), format="n3")
        i = 0
        for s, p, o in g:
            if isinstance(s, Graph):
                i += 1
        self.assertEquals(i, 3)
        self.assertEquals(len(list(g.contexts())), 13)

        g.close()


    def testParse(self):
        g = ConjunctiveGraph()
        g.parse("http://groups.csail.mit.edu/dig/2005/09/rein/examples/troop42-policy.n3", format="n3")

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = n3Test
#!/usr/bin/env python2.4 

import os, traceback, sys, unittest

#sys.path[:0]=[".."]

import rdflib

def crapCompare(g1,g2):
    "A really crappy way to 'check' if two graphs are equal. It ignores blank nodes completely"
    if len(g1)!=len(g2):
        raise Exception("Graphs dont have same length")
    for t in g1: 
        if not isinstance(t[0],rdflib.BNode):
            s=t[0]
        else:
            s=None
        if not isinstance(t[2],rdflib.BNode):
            o=t[2]
        else:
            o=None
        if not (s,t[1],o) in g2: 
            e="(%s,%s,%s) is not in both graphs!"%(s,t[1],o)
            raise Exception, e
        

def test(f, prt=False):
    g=rdflib.ConjunctiveGraph()
    if f.endswith('rdf'):
        g.parse(f)
    else: 
        g.parse(f, format='n3')
    if prt:
        for t in g:
            print t
        print "========================================\nParsed OK!"
    s=g.serialize(format='n3')
    if prt: 
        print s
    g2=rdflib.ConjunctiveGraph()
    g2.parse(rdflib.StringInputSource(s),format='n3')
    if prt: 
        print g2.serialize()

    crapCompare(g,g2)
        

if len(sys.argv)>1:
    test(sys.argv[1], True)
    sys.exit()

class TestN3Writing(unittest.TestCase):
    def testWriting(self): 
        for f in os.listdir('test/n3'):
            if f!='.svn':
                test("test/n3/"+f)
        
if __name__ == "__main__":
    unittest.main()
########NEW FILE########
__FILENAME__ = n3_2
from rdflib import URIRef, BNode, Literal, Variable
from rdflib import RDF, RDFS
from rdflib import StringInputSource
from rdflib.Graph import QuotedGraph,ConjunctiveGraph
import sys
from pprint import pprint

implies = URIRef("http://www.w3.org/2000/10/swap/log#implies")
testN3="""
@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix : <http://test/> .
{:a :b :c;a :foo} => {:a :d :c,?y}.
_:foo a rdfs:Class.
:a :d :c."""


#Thorough test suite for formula-aware store
def testN3Store(store="default", configString=None):
    g = ConjunctiveGraph(store=store)
    if configString:
        g.destroy(configString)
        g.open(configString)
    g.parse(StringInputSource(testN3), format="n3")
    print g.store
    try:
        for s,p,o in g.triples((None,implies,None)):
            formulaA = s
            formulaB = o

        assert type(formulaA)==QuotedGraph and type(formulaB)==QuotedGraph
        a = URIRef('http://test/a')
        b = URIRef('http://test/b')
        c = URIRef('http://test/c')
        d = URIRef('http://test/d')
        v = Variable('y')

        universe = ConjunctiveGraph(g.store)

        #test formula as terms
        assert len(list(universe.triples((formulaA,implies,formulaB))))==1

        #test variable as term and variable roundtrip
        assert len(list(formulaB.triples((None,None,v))))==1
        for s,p,o in formulaB.triples((None,d,None)):
            if o != c:
                assert isinstance(o,Variable)
                assert o == v
        s = list(universe.subjects(RDF.type, RDFS.Class))[0]
        assert isinstance(s,BNode)
        assert len(list(universe.triples((None,implies,None)))) == 1
        assert len(list(universe.triples((None,RDF.type,None)))) ==1
        assert len(list(formulaA.triples((None,RDF.type,None))))==1
        assert len(list(formulaA.triples((None,None,None))))==2
        assert len(list(formulaB.triples((None,None,None))))==2
        assert len(list(universe.triples((None,None,None))))==3
        assert len(list(formulaB.triples((None,URIRef('http://test/d'),None))))==2
        assert len(list(universe.triples((None,URIRef('http://test/d'),None))))==1

        #context tests
        #test contexts with triple argument
        assert len(list(universe.contexts((a,d,c))))==1

        #Remove test cases
        universe.remove((None,implies,None))
        assert len(list(universe.triples((None,implies,None))))==0
        assert len(list(formulaA.triples((None,None,None))))==2
        assert len(list(formulaB.triples((None,None,None))))==2

        formulaA.remove((None,b,None))
        assert len(list(formulaA.triples((None,None,None))))==1
        formulaA.remove((None,RDF.type,None))
        assert len(list(formulaA.triples((None,None,None))))==0

        universe.remove((None,RDF.type,RDFS.Class))


        #remove_context tests
        universe.remove_context(formulaB)
        assert len(list(universe.triples((None,RDF.type,None))))==0
        assert len(universe)==1
        assert len(formulaB)==0

        universe.remove((None,None,None))
        assert len(universe)==0

        g.store.destroy(configString)
    except:
        g.store.destroy(configString)
        raise


########NEW FILE########
__FILENAME__ = n3_quoting
import unittest

from rdflib import Literal, Namespace, StringInputSource
from rdflib.Graph import Graph

cases = ['no quotes',
         "single ' quote",
         'double " quote',
         '"',
         "'",
         '"\'"',
         '\\', # len 1
         '\\"', # len 2
         '\\\\"', # len 3
         '\\"\\', # len 3
         '<a some="typical" html="content">here</a>',
         ]

class N3Quoting(unittest.TestCase):
    def test(self):
        g = Graph()
        NS = Namespace("http://quoting.test/")
        for i, case in enumerate(cases):
            g.add((NS['subj'], NS['case%s' % i], Literal(case)))
        n3txt = g.serialize(format="n3")
        #print n3txt

        g2 = Graph()
        g2.parse(StringInputSource(n3txt), format="n3")
        for i, case in enumerate(cases):
            l = g2.value(NS['subj'], NS['case%s' % i])
            #print repr(l), repr(case)
            self.assertEqual(l, Literal(case))


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = nt
import unittest

from rdflib import *
from rdflib.Graph import Graph

class NTTestCase(unittest.TestCase):

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def testModel(self):
        g = Graph()
        g.load("http://www.w3.org/2000/10/rdf-tests/rdfcore/rdfms-empty-property-elements/test002.nt", format="nt")


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = ntriples
#!/usr/bin/env python
"""
N-Triples Parser
License: GPL 2; share and enjoy!
Author: Sean B. Palmer, inamidst.com
Documentation:
   http://inamidst.com/proj/rdf/ntriples-doc

Command line usage:
   ./ntriples.py <URI>    - parses URI as N-Triples
   ./ntriples.py --help   - prints out this help message
# @@ fully empty document?
"""

import re

uriref = r'<([^:]+:[^\s"<>]+)>'
literal = r'"([^"\\]*(?:\\.[^"\\]*)*)"'
litinfo = r'(?:@([a-z]+(?:-[a-z0-9]+)*)|\^\^' + uriref + r')?'

r_line = re.compile(r'([^\r\n]*)(?:\r\n|\r|\n)')
r_wspace = re.compile(r'[ \t]*')
r_wspaces = re.compile(r'[ \t]+')
r_tail = re.compile(r'[ \t]*\.[ \t]*')
r_uriref = re.compile(uriref)
r_nodeid = re.compile(r'_:([A-Za-z][A-Za-z0-9]*)')
r_literal = re.compile(literal + litinfo)

bufsiz = 2048
validate = False

class Node(unicode): pass

class URI(Node): pass
class bNode(Node): pass
class Literal(Node):
   def __new__(cls, lit, lang=None, dtype=None):
      n = str(lang) + ' ' + str(dtype) + ' ' + lit
      return unicode.__new__(cls, n)

class Sink(object):
   def __init__(self):
      self.length = 0

   def triple(self, s, p, o):
      self.length += 1

class ParseError(Exception): pass

quot = {'t': '\t', 'n': '\n', 'r': '\r', '"': '"', '\\': '\\'}
r_safe = re.compile(r'([\x20\x21\x23-\x5B\x5D-\x7E]+)')
r_quot = re.compile(r'\\(t|n|r|"|\\)')
r_uniquot = re.compile(r'\\u([0-9A-F]{4})|\\U([0-9A-F]{8})')

def unquote(s):
   """Unquote an N-Triples string."""
   result = []
   while s:
      m = r_safe.match(s)
      if m:
         s = s[m.end():]
         result.append(m.group(1))
         continue

      m = r_quot.match(s)
      if m:
         s = s[2:]
         result.append(quot[m.group(1)])
         continue

      m = r_uniquot.match(s)
      if m:
         s = s[m.end():]
         u, U = m.groups()
         codepoint = int(u or U, 16)
         if codepoint > 0x10FFFF:
            raise ParseError("Disallowed codepoint: %08X" % codepoint)
         result.append(unichr(codepoint))
      elif s.startswith('\\'):
         raise ParseError("Illegal escape at: %s..." % s[:10])
      else: raise ParseError("Illegal literal character: %r" % s[0])
   return unicode(''.join(result))

if not validate:
   def unquote(s):
      return s.decode('unicode-escape')

r_hibyte = re.compile(r'([\x80-\xFF])')

def uriquote(uri):
   return r_hibyte.sub(lambda m: '%%%02X' % ord(m.group(1)), uri)
if not validate:
   def uriquote(uri):
      return uri

class NTriplesParser(object):
   """An N-Triples Parser.
      Usage:
         p = NTriplesParser(sink=MySink())
         sink = p.parse(f) # file; use parsestring for a string
   """

   def __init__(self, sink=None):
      if sink is not None:
         self.sink = sink
      else: self.sink = Sink()

   def parse(self, f):
      """Parse f as an N-Triples file."""
      if not hasattr(f, 'read'):
         raise ParseError("Item to parse must be a file-like object.")

      self.file = f
      self.buffer = ''
      while True:
         self.line = self.readline()
         if self.line is None: break
         try: self.parseline()
         except ParseError:
            raise ParseError("Invalid line: %r" % self.line)
      return self.sink

   def parsestring(self, s):
      """Parse s as an N-Triples string."""
      if not isinstance(s, basestring):
         raise ParseError("Item to parse must be a string instance.")
      from cStringIO import StringIO
      f = StringIO()
      f.write(s)
      f.seek(0)
      self.parse(f)

   def readline(self):
      """Read an N-Triples line from buffered input."""
      # N-Triples lines end in either CRLF, CR, or LF
      # Therefore, we can't just use f.readline()
      if not self.buffer:
         buffer = self.file.read(bufsiz)
         if not buffer: return None
         self.buffer = buffer

      while True:
         m = r_line.match(self.buffer)
         if m: # the more likely prospect
            self.buffer = self.buffer[m.end():]
            return m.group(1)
         else:
            buffer = self.file.read(bufsiz)
            if not buffer:
               raise ParseError("EOF in line")
            self.buffer += buffer

   def parseline(self):
      self.eat(r_wspace)
      if (not self.line) or self.line.startswith('#'):
         return # The line is empty or a comment

      subject = self.subject()
      self.eat(r_wspaces)

      predicate = self.predicate()
      self.eat(r_wspaces)

      object = self.object()
      self.eat(r_tail)

      if self.line:
         raise ParseError("Trailing garbage")
      self.sink.triple(subject, predicate, object)

   def peek(self, token):
      return self.line.startswith(token)

   def eat(self, pattern):
      m = pattern.match(self.line)
      if not m: # @@ Why can't we get the original pattern?
         raise ParseError("Failed to eat %s" % pattern)
      self.line = self.line[m.end():]
      return m

   def subject(self):
      # @@ Consider using dictionary cases
      subj = self.uriref() or self.nodeid()
      if not subj:
        raise ParseError("Subject must be uriref or nodeID")
      return subj

   def predicate(self):
      pred = self.uriref()
      if not pred:
         raise ParseError("Predicate must be uriref")
      return pred

   def object(self):
      objt = self.uriref() or self.nodeid() or self.literal()
      if not objt:
         raise ParseError("Unrecognised object type")
      return objt

   def uriref(self):
      if self.peek('<'):
         uri = self.eat(r_uriref).group(1)
         uri = unquote(uri)
         uri = uriquote(uri)
         return URI(uri)
      return False

   def nodeid(self):
      if self.peek('_'):
         return bNode(self.eat(r_nodeid).group(1))
      return False

   def literal(self):
      if self.peek('"'):
         lit, lang, dtype = self.eat(r_literal).groups()
         if lang and dtype:
            raise ParseError("Can't have both a language and a datatype")
         lit = unquote(lit)
         return Literal(lit, lang, dtype)
      return False

def parseURI(uri):
   import urllib
   parser = NTriplesParser()
   u = urllib.urlopen(uri)
   sink = parser.parse(u)
   u.close()
   # for triple in sink:
   #    print triple
   print 'Length of input:', sink.length

def main():
   import sys
   if len(sys.argv) == 2:
      parseURI(sys.argv[1])
   else: print __doc__

if __name__=="__main__":
   main()

########NEW FILE########
__FILENAME__ = parser
import unittest

from rdflib.Graph import Graph
from rdflib import URIRef, BNode, Literal, RDF, RDFS
from rdflib.StringInputSource import StringInputSource


class ParserTestCase(unittest.TestCase):
    backend = 'default'
    path = 'store'

    def setUp(self):
        self.graph = Graph(store=self.backend)
        self.graph.open(self.path)

    def tearDown(self):
        self.graph.close()

    def testNoPathWithHash(self):
        g = self.graph
        g.parse(StringInputSource("""\
<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<rdf:RDF
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
  xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
>

<rdfs:Class rdf:about="http://example.org#">
  <rdfs:label>testing</rdfs:label>
</rdfs:Class>

</rdf:RDF>
"""), publicID="http://example.org")

        subject = URIRef("http://example.org#")
        label = g.value(subject, RDFS.label)
        self.assertEquals(label, Literal("testing"))
        type = g.value(subject, RDF.type)
        self.assertEquals(type, RDFS.Class)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = parser_rdfcore
import unittest

from rdflib import URIRef, BNode, Literal, RDF, RDFS
from rdflib.Namespace import Namespace
from rdflib.exceptions import ParserError

from rdflib.Graph import Graph
from rdflib.util import first


import logging

_logger = logging.getLogger("parser_rdfcore")

verbose = 0

from encodings.utf_8 import StreamWriter

import sys
sw = StreamWriter(sys.stdout)
def write(msg):
    _logger.info(msg+"\n")
    #sw.write(msg+"\n")

class TestStore(Graph):
    def __init__(self, expected):
        super(TestStore, self).__init__()
        self.expected = expected

    def add(self, (s, p, o)):
        if not isinstance(s, BNode) and not isinstance(o, BNode):
            if not (s, p, o) in self.expected:
                m = u"Triple not in expected result: %s, %s, %s" % (s.n3(), p.n3(), o.n3())
                if verbose: write(m)
                #raise Exception(m)
        super(TestStore, self).add((s, p, o))


TEST = Namespace("http://www.w3.org/2000/10/rdf-tests/rdfcore/testSchema#")

import os
def resolve(rel):
    return "http://www.w3.org/2000/10/rdf-tests/rdfcore/" + rel

def _testPositive(uri, manifest):
    if verbose: write(u"TESTING: %s" % uri)
    result = 0 # 1=failed, 0=passed
    inDoc = first(manifest.objects(uri, TEST["inputDocument"]))
    outDoc = first(manifest.objects(uri, TEST["outputDocument"]))
    expected = Graph()
    if outDoc[-3:]==".nt":
        format = "nt"
    else:
        format = "xml"
    expected.load(outDoc, format=format)
    store = TestStore(expected)
    if inDoc[-3:]==".nt":
        format = "nt"
    else:
        format = "xml"

    try:
        store.load(inDoc, format=format)
    except ParserError, pe:
        write("Failed '")
        write(inDoc)
        write("' failed with")
        raise pe
        try:
            write(type(pe))
        except:
            write("sorry could not dump out error.")
        result = 1
    else:
        if not store.isomorphic(expected):
            write(u"""Failed: '%s'""" % uri)
            if verbose:
                write("""  In:\n""")
                for s, p, o in store:
                    write("%s %s %s." % (repr(s), repr(p), repr(o)))
                write("""  Out:\n""")
                for s, p, o in expected:
                    write("%s %s %s." % (repr(s), repr(p), repr(o)))
            result += 1
    return result

def _testNegative(uri, manifest):
    if verbose: write(u"TESTING: %s" % uri)
    result = 0 # 1=failed, 0=passed
    inDoc = first(manifest.objects(uri, TEST["inputDocument"]))
    store = Graph()

    test = BNode()
    results.add((test, RESULT["test"], uri))
    results.add((test, RESULT["system"], system))

    try:
        if inDoc[-3:]==".nt":
            format = "nt"
        else:
            format = "xml"
        store.load(inDoc, format=format)
    except ParserError, pe:
        results.add((test, RDF.type, RESULT["PassingRun"]))
        #pass
    else:
        write(u"""Failed: '%s'""" % uri)
        results.add((test, RDF.type, RESULT["FailingRun"]))
        result = 1
    return result

class ParserTestCase(unittest.TestCase):
    store = 'default'
    path = 'store'
    slowtest = True

    def setUp(self):
        self.manifest = manifest = Graph(store=self.store)
        manifest.open(self.path)
        manifest.load("http://www.w3.org/2000/10/rdf-tests/rdfcore/Manifest.rdf")

    def tearDown(self):
        self.manifest.close()

    def testNegative(self):
        manifest = self.manifest
        num_failed = total = 0
        negs = list(manifest.subjects(RDF.type, TEST["NegativeParserTest"]))
        negs.sort()
        for neg in negs:
            status = first(manifest.objects(neg, TEST["status"]))
            if status==Literal("APPROVED"):
                result = _testNegative(neg, manifest)
                total += 1
                num_failed += result
        self.assertEquals(num_failed, 0, "Failed: %s of %s." % (num_failed, total))

    def testPositive(self):
        manifest = self.manifest
        uris = list(manifest.subjects(RDF.type, TEST["PositiveParserTest"]))
        uris.sort()
        num_failed = total = 0
        for uri in uris:
            status = first(manifest.objects(uri, TEST["status"]))
            if status==Literal("APPROVED"):
                result = _testPositive(uri, manifest)
                test = BNode()
                results.add((test, RESULT["test"], uri))
                results.add((test, RESULT["system"], system))
                if not result:
                    results.add((test, RDF.type, RESULT["PassingRun"]))
                else:
                   results.add((test, RDF.type, RESULT["FailingRun"]))
                total += 1
                num_failed += result
        self.assertEquals(num_failed, 0, "Failed: %s of %s." % (num_failed, total))

RESULT = Namespace("http://www.w3.org/2002/03owlt/resultsOntology#")
FOAF = Namespace("http://xmlns.com/foaf/0.1/")


results = Graph()

system = BNode("system")
results.add((system, FOAF["homepage"], URIRef("http://rdflib.net/")))
results.add((system, RDFS.label, Literal("RDFLib")))
results.add((system, RDFS.comment, Literal("")))


if __name__ == "__main__":
    manifest = Graph()
    manifest.load("http://www.w3.org/2000/10/rdf-tests/rdfcore/Manifest.rdf")
    import sys, getopt
    try:
        optlist, args = getopt.getopt(sys.argv[1:], 'h:', ["help"])
    except getopt.GetoptError, msg:
        write(msg)
        usage()

    try:
        argv = sys.argv
        for arg in sys.argv[1:]:
            verbose = 1
            case = URIRef(arg)
            write(u"Testing: %s" % case)
            if (case, RDF.type, TEST["PositiveParserTest"]) in manifest:
                result = _testPositive(case, manifest)
                write(u"Positive test %s" % ["PASSED", "FAILED"][result])
            elif (case, RDF.type, TEST["NegativeParserTest"]) in manifest:
                result = _testNegative(case, manifest)
                write(u"Negative test %s" % ["PASSED", "FAILED"][result])
            else:
                write(u"%s not ??" % case)

        if len(argv)<=1:
            unittest.main()
    finally:
        results.serialize("results.rdf")

########NEW FILE########
__FILENAME__ = rdf
import unittest

from rdflib import *
from rdflib.Graph import Graph
from rdflib import RDF
from rdflib.StringInputSource import StringInputSource

FOAF = Namespace("http://xmlns.com/foaf/0.1/")


rdfxml = """\
<?xml version="1.0" encoding="utf-8"?>
<rdf:RDF
  xmlns:xml='http://www.w3.org/XML/1998/namespace'
  xmlns:foaf='http://xmlns.com/foaf/0.1/'
  xmlns:rdf='http://www.w3.org/1999/02/22-rdf-syntax-ns#'
  xmlns:dc='http://http://purl.org/dc/elements/1.1/'
  xmlns:rdfs='http://www.w3.org/2000/01/rdf-schema#'
>
  <foaf:Person>
    <foaf:name>Donna Fales</foaf:name>
    <foaf:nick>donna</foaf:nick>
  </foaf:Person>
</rdf:RDF>"""

class RDFTestCase(unittest.TestCase):
    backend = 'default'
    path = 'store'

    def setUp(self):
        self.store = Graph(store=self.backend)
        self.store.open(self.path)
        self.store.bind("dc", "http://http://purl.org/dc/elements/1.1/")
        self.store.bind("foaf", "http://xmlns.com/foaf/0.1/")

    def tearDown(self):
        self.store.close()

    def addDonna(self):
        self.donna = donna = BNode()
        self.store.add((donna, RDF.type, FOAF["Person"]))
        self.store.add((donna, FOAF["nick"], Literal("donna")))
        self.store.add((donna, FOAF["name"], Literal("Donna Fales")))

    def testRDFXML(self):
        self.addDonna()
        g = Graph()
        g.parse(StringInputSource(self.store.serialize(format="pretty-xml")))
        self.assertEquals(self.store.isomorphic(g), True)

def test_suite():
    return unittest.makeSuite(RDFTestCase)

if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')

########NEW FILE########
__FILENAME__ = rdfa
#!/usr/bin/python
#
# test.py - RDFa Test Suite
#

import os, sys, string
import rdfdiff
import unittest
import ntriples

from rdfdiff import Graph
from rdflib import ConjunctiveGraph as RGraph
from rdflib import StringInputSource
from rdflib import URIRef
from rdflib import BNode
from rdflib import Literal


def main():
    suite = unittest.TestSuite()
    for test in make_cases():
        suite.addTest(test)
    print "\n------\nRDFa Parser Tests\n-----\n"
    unittest.TextTestRunner(verbosity=2,descriptions=1).run(suite)


def make_cases():
    testdir = "test/rdfa"
    verbose = False
    tests = [os.path.splitext(f)[0]
      for f in os.listdir(testdir)
      if os.path.splitext(f)[1] == ".htm" ]
    tests.sort()
    for testname in tests:
        yield RDFaTestStub(os.path.abspath(os.path.join(testdir,testname)))


# expose each test for e.g. Nose to run
def all_tests():
    for test in make_cases():
        yield test.runTest,
all_tests.unstable = False

class RDFaTestStub(unittest.TestCase):

    def __init__(self, testbase):
        unittest.TestCase.__init__(self)
        self.testbase = testbase
        self.pubId = 'http://example.com/'

    def shortDescription(self):
        return str(os.path.basename(self.testbase))

    def nodeToString(self, node):
        if isinstance(node, BNode):
            bid = node.n3()
            if(bid[0:4] == '_:_:'):
                    bid = bid[2:]
            return ntriples.bNode(str(bid))
        elif isinstance(node, URIRef):
            if len(str(node)) == 0:
                    return ntriples.URI(self.pubId)
            return ntriples.URI(str(node))
        elif isinstance(node, Literal):
            return ntriples.Literal(str(node), lang= node.language or None,
                    dtype= node.datatype  or None)
        else:
            raise Exception("unexpected node value")

    def runTest(self):
        testfile = self.testbase + ".htm"
        resultsf = self.testbase + ".ttl"
        self.failIf(not os.path.isfile(resultsf), "missing expected results file.")

        store1 = RGraph()
        store1.load(resultsf, publicID=self.pubId, format="n3")
        pcontents = store1.serialize(format='nt')
        pg = Graph()
        for a, b, c in store1:
            pg.triples.add(tuple(map(self.nodeToString, (a,b,c))))
            #print tuple(map(self.nodeToString, (a,b,c)))

        store2 = RGraph()
        store2.load(testfile, publicID=self.pubId, format="rdfa")
        qcontents = store2.serialize(format='nt')
        qg = Graph()
        for a, b, c in store2:
            qg.triples.add(tuple(map(self.nodeToString, (a,b,c))))

        self.failIf(not hash(pg) == hash(qg),
                "In %s: results do not match.\n%s\n\n%s" % (self.shortDescription(), pcontents, qcontents))


if __name__ == '__main__':
    main()



########NEW FILE########
__FILENAME__ = rdfdiff
#!/usr/bin/env python
"""
RDF Graph Isomorphism Tester
Author: Sean B. Palmer, inamidst.com
Uses the pyrple algorithm
Requirements:
   Python2.4+
   http://inamidst.com/proj/rdf/ntriples.py
Usage: ./rdfdiff.py <ntriplesP> <ntriplesQ>
"""

import sys, re, urllib
import ntriples
from ntriples import bNode

ntriples.r_uriref = re.compile(r'<([^\s"<>]+)>')

class Graph(object):
   def __init__(self, uri=None, content=None):
      self.triples = set()
      if uri:
          self.parse(uri)
      elif content:
          self.parse_string(content)

   def parse(self, uri):
      class Sink(object):
         def triple(sink, s, p, o):
            self.triples.add((s, p, o))

      p = ntriples.NTriplesParser(sink=Sink())
      u = urllib.urlopen(uri)
      p.parse(u)
      u.close()

   def parse_string(self, content):
      class Sink(object):
         def triple(sink, s, p, o):
            self.triples.add((s, p, o))

      p = ntriples.NTriplesParser(sink=Sink())
      p.parsestring(content)

   def __hash__(self):
      return hash(tuple(sorted(self.hashtriples())))

   def hashtriples(self):
      for triple in self.triples:
         g = ((isinstance(t, bNode) and self.vhash(t)) or t for t in triple)
         yield hash(tuple(g))

   def vhash(self, term, done=False):
      return tuple(sorted(self.vhashtriples(term, done)))

   def vhashtriples(self, term, done):
      for t in self.triples:
         if term in t: yield tuple(self.vhashtriple(t, term, done))

   def vhashtriple(self, triple, term, done):
      for p in xrange(3):
         if not isinstance(triple[p], bNode): yield triple[p]
         elif done or (triple[p] == term): yield p
         else: yield self.vhash(triple[p], done=True)

def compare(p, q):
   return hash(Graph(p)) == hash(Graph(q))

def compare_from_string(p, q):
   return hash(Graph(content=p)) == hash(Graph(content=q))

def main():
   result = compare(sys.argv[1], sys.argv[2])
   print ('no', 'yes')[result]

if __name__=="__main__":
   main()

########NEW FILE########
__FILENAME__ = rules
import unittest
from tempfile import mkdtemp

from rdflib import *
from rdflib.Graph import Graph

LOG = Namespace("http://www.w3.org/2000/10/swap/log#")


try:
    from pychinko import terms
    from pychinko.interpreter import Interpreter

    def _convert(node):
        if isinstance(node, Variable):
            return terms.Variable(node)
            #return node
        elif isinstance(node, BNode):
            return terms.Exivar(node)
        elif isinstance(node, URIRef):
            #return terms.URI(node)
            return node
        elif isinstance(node, Literal):
            return node
        else:
            raise Exception("Unexpected Type: %s" % type(node))

    def patterns(g):
        for s, p, o in g:
            yield terms.Pattern(_convert(s), _convert(p), _convert(o))

    def facts(g):
        for s, p, o in g:
            if p!=LOG.implies and not isinstance(s, BNode) and not isinstance(o, BNode):
                yield terms.Fact(_convert(s), _convert(p), _convert(o))

    class PychinkoTestCase(unittest.TestCase):
        backend = 'default'
        def setUp(self):
            self.g = Graph(store=self.backend)
            self.g.open(configuration=mkdtemp())
            self.g.parse("test/a.n3", format="n3")

        def tearDown(self):
            self.g.close()

        def testPychinko(self):
            rules = []
            for s, p, o in self.g.triples((None, LOG.implies, None)):
                lhs = list(patterns(s))
                rhs = list(patterns(o))
                rules.append(terms.Rule(lhs, rhs, (s, p, o)))
            interp = Interpreter(rules)
            f = Graph()
            f.parse("http://eikeon.com/")
            source = f
            source = self.g
            interp.addFacts(set(facts(source)), initialSet=True)
            interp.run()
            #_logger.debug("inferred facts: %s" % interp.inferredFacts)

except ImportError, e:
    print "Could not test Pychinko: %s" % e



########NEW FILE########
__FILENAME__ = seq
import unittest

from rdflib import *
from rdflib.Graph import Graph
from rdflib.StringInputSource import StringInputSource

class SeqTestCase(unittest.TestCase):
    backend = 'default'
    path = 'store'

    def setUp(self):
        store = self.store = Graph(store=self.backend)
        store.open(self.path)
        store.parse(StringInputSource(s))

    def tearDown(self):
        self.store.close()

    def testSeq(self):
        items = self.store.seq(URIRef("http://example.org/Seq"))
        self.assertEquals(len(items), 6)
        self.assertEquals(items[-1].concrete(), URIRef("http://example.org/six"))
        self.assertEquals(items[2].concrete(), URIRef("http://example.org/three"))
        # just make sure we can serialize
        self.store.serialize()

def test_suite():
    return unittest.makeSuite(SeqTestCase)

if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')

s = """\
<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:nzgls="http://www.nzgls.govt.nz/standard/"
>
 <rdf:Seq rdf:about="http://example.org/Seq">
   <rdf:li rdf:resource="http://example.org/one" />
   <rdf:li rdf:resource="http://example.org/two" />
   <rdf:li rdf:resource="http://example.org/three" />
   <rdf:li rdf:resource="http://example.org/four" />
   <rdf:li rdf:resource="http://example.org/five_five" />
   <rdf:li rdf:resource="http://example.org/six" />
 </rdf:Seq>
</rdf:RDF>
"""


########NEW FILE########
__FILENAME__ = test_prettyxml
# -*- coding: UTF-8 -*-
#=======================================================================
from rdflib import ConjunctiveGraph, URIRef, Literal, BNode, RDFS
from rdflib.syntax.serializers.PrettyXMLSerializer import PrettyXMLSerializer
from test.serializers import SerializerTestBase, serialize, serialize_and_load
#=======================================================================


class TestPrettyXmlSerializer(SerializerTestBase):

    serializer = PrettyXMLSerializer

    testContent = """
        @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
        @prefix owl:  <http://www.w3.org/2002/07/owl#> .
        @prefix : <http://example.org/model/test#> .

        :value rdfs:domain :Test .

        :Test rdfs:subClassOf
            [ a owl:Restriction;
                owl:onProperty :value ],
            [ a owl:Restriction;
                owl:onProperty :name ] .

        <http://example.org/data/a> a :Test;
            rdfs:seeAlso <http://example.org/data/b>;
            :value "A" .

        <http://example.org/data/b>
            :name "Bee"@en, "Be"@sv;
            :value "B" .

        <http://example.org/data/c> a rdfs:Resource;
            rdfs:seeAlso <http://example.org/data/c>;
            :value 3 .

        <http://example.org/data/d> a rdfs:Resource;
            rdfs:seeAlso <http://example.org/data/c> ;
            rdfs:seeAlso <http://example.org/data/b> ;
            rdfs:seeAlso <http://example.org/data/a> .

        _:bnode1 a :BNode;
            rdfs:seeAlso _:bnode2 .

        _:bnode2 a :BNode ;
            rdfs:seeAlso _:bnode3 .

        _:bnode3 a :BNode ;
            rdfs:seeAlso _:bnode2 .

        """
    testContentFormat = 'n3'

    def test_result_fragments(self):
        rdfXml = serialize(self.sourceGraph, self.serializer)
        assert '<Test rdf:about="http://example.org/data/a">' in rdfXml
        assert '<rdf:Description rdf:about="http://example.org/data/b">' in rdfXml
        assert '<name xml:lang="en">Bee</name>' in rdfXml
        assert '<value rdf:datatype="http://www.w3.org/2001/XMLSchema#integer">3</value>' in rdfXml
        assert '<BNode rdf:nodeID="' in rdfXml, "expected one identified bnode in serialized graph"
        #onlyBNodesMsg = "expected only inlined subClassOf-bnodes in serialized graph"
        #assert '<rdfs:subClassOf>' in rdfXml, onlyBNodesMsg
        #assert not '<rdfs:subClassOf ' in rdfXml, onlyBNodesMsg

    def test_subClassOf_objects(self):
        reparsedGraph = serialize_and_load(self.sourceGraph, self.serializer)
        _assert_expected_object_types_for_predicates(reparsedGraph,
                [RDFS.seeAlso, RDFS.subClassOf],
                [URIRef, BNode])


def _assert_expected_object_types_for_predicates(graph, predicates, types):
    for s, p, o in graph:
        if p in predicates:
            someTrue = [isinstance(o, t) for t in types]
            assert True in someTrue, \
                    "Bad type %s for object when predicate is <%s>." % (type(o), p)



########NEW FILE########
__FILENAME__ = Sleepycat
from test.graph import GraphTestCase

try:
    from rdflib.store.Sleepycat import Sleepycat
    class SleepycatGraphTestCase(GraphTestCase):
        store_name = "Sleepycat"
except ImportError, e:
    _logger.warning("Can not test Sleepycat store: %s" % e)


from test.context import ContextTestCase

try:
    from rdflib.store.Sleepycat import Sleepycat
    class SleepycatStoreTestCase(ContextTestCase):
        store = "Sleepycat"
except ImportError, e:
    _logger.warning("Can not test Sleepycat store: %s" % e)


#class Sleepycat(PychinkoTestCase):
#    backend = 'Sleepycat'


########NEW FILE########
__FILENAME__ = constuctTest
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:30:02 $, by $Author: ivan $, $Revision: 1.1 $
#
"""

"""
import sys, os, time, datetime, imp, sys, StringIO
sys.path.insert(0,"../")

from testSPARQL import ns_rdf
from testSPARQL import ns_rdfs
from testSPARQL import ns_foaf
from testSPARQL import ns_vcard
from testSPARQL import ns_person

from rdflib.sparql import sparqlGraph
from rdflib.FileInputSource import FileInputSource

tests = {
        1021: "Test10_21",
        1022: "Test10_22",
        1023: "Test10_23",
}

Debug = False



def run(modName) :
        # Import the python module
        defs = None
        (fl,realpath,descr) = imp.find_module(modName,["."])
        mod = imp.load_module(modName,fl,realpath,descr)
        defs = mod.__dict__

        ##################################################
        # Two ways of identifying the RDF data:
        # 1. A Triple Store generated in the module
        graph = None
        try :
                graph = defs["graph"]
        except :
                pass
        # 2. Directly in the test module as a string
        rdfData = None
        try :
                rdfData     = defs["rdfData"]
        except :
                pass

        # Get the final of the triple store...
        if graph == None :
                stream = FileInputSource(StringIO.StringIO(rdfData))
                graph = sparqlGraph.SPARQLGraph()
                graph.parse(stream,format="xml")

        ###############################################
        # Retrive the query data
        pattern     = defs["pattern"]
        optPattern  = defs["optional"]	
        construct   = defs["construct"]	


        ###############################################		
        print "\n============= Test Module: %s =============" % modName			

        results     = graph.queryObject(pattern,optPattern)
        graph = results.construct(construct)
        graph.serialize("output.rdf")

        print "=== generated RDF file (output.rdf):\n"
        for l in file("output.rdf") :
                sys.stdout.write(l)

if __name__ == '__main__' :
        if len(sys.argv) == 1 :
                #print "Usage: %s modname1 modname2 ..." % sys.argv[0]
                for mod in tests.values():
                        run(mod)
        else :
                for mod in sys.argv[1:] :
                        if mod.endswith(".py") :
                                run(mod[0:-3])
                        else :
                                run(mod)



########NEW FILE########
__FILENAME__ = Test10_21
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:30:02 $, by $Author: ivan $, $Revision: 1.1 $
#

from testSPARQL import ns_rdf
from testSPARQL import ns_rdfs
from testSPARQL import ns_dc0
from testSPARQL import ns_foaf
from testSPARQL import ns_ns
from testSPARQL import ns_book
from testSPARQL import ns_vcard
from testSPARQL import ns_person

from rdflib.Literal     import Literal
from rdflib.sparql.sparqlOperators import lt, ge
import datetime
from rdflib.sparql.graphPattern import GraphPattern

thresholdDate = datetime.date(2005,01,01)

rdfData = """<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:foaf="http://xmlns.com/foaf/0.1/"
   xmlns:ns = "http://example.org/ns#"
>
        <rdf:Description>
                <foaf:name>Alice</foaf:name>
                <foaf:mbox rdf:resource="mailto:alice@example.com"/>
        </rdf:Description>
</rdf:RDF>
"""

select      = []
pattern     = GraphPattern([("?x",ns_foaf["name"],"?name")])
optional    = []
construct   = GraphPattern([(ns_person["Alice"],ns_vcard["FN"],"?name")])
tripleStore = None




########NEW FILE########
__FILENAME__ = Test10_22
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:30:02 $, by $Author: ivan $, $Revision: 1.1 $
#

from testSPARQL import ns_rdf
from testSPARQL import ns_rdfs
from testSPARQL import ns_dc0
from testSPARQL import ns_foaf
from testSPARQL import ns_ns
from testSPARQL import ns_book
from testSPARQL import ns_vcard
from testSPARQL import ns_person

from rdflib.Literal     import Literal
from rdflib import BNode
from rdflib.sparql.sparql import PatternBNode
from rdflib.sparql.sparqlOperators import lt, ge
import datetime
from rdflib.sparql.graphPattern import GraphPattern

thresholdDate = datetime.date(2005,01,01)
rdfData = """<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:foaf="http://xmlns.com/foaf/0.1/"
   xmlns:ns = "http://example.org/ns#"
>
        <rdf:Description>
                <foaf:givenname>Alice</foaf:givenname>
                <foaf:family_name>Hacker</foaf:family_name>
        </rdf:Description>
        <rdf:Description>
                <foaf:givenname>Bob</foaf:givenname>
                <foaf:family_name>Hacker</foaf:family_name>
        </rdf:Description>
</rdf:RDF>
"""
select      = []
pattern     = GraphPattern([("?x",ns_foaf["givenname"],"?name"),("?x",ns_foaf["family_name"],"?fname")])
optional    = []
bnode = BNode("v") #PatternBNode("")
construct   = GraphPattern([("?x", ns_vcard["N"],bnode),(bnode,ns_vcard["givenName"],"?name"),(bnode,ns_vcard["familyName"],"?fname")])
tripleStore = None




########NEW FILE########
__FILENAME__ = Test10_23
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:30:02 $, by $Author: ivan $, $Revision: 1.1 $
#

from testSPARQL import ns_rdf
from testSPARQL import ns_rdfs
from testSPARQL import ns_dc0
from testSPARQL import ns_foaf
from testSPARQL import ns_ns
from testSPARQL import ns_book
from testSPARQL import ns_vcard
from testSPARQL import ns_person

from rdflib.Literal     import Literal
from rdflib.sparql.sparqlOperators import lt, ge
import datetime
from rdflib.sparql.graphPattern import GraphPattern

thresholdDate = datetime.date(2005,01,01)
rdfData = """<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:foaf="http://xmlns.com/foaf/0.1/"
   xmlns:ns = "http://example.org/ns#"
>
        <rdf:Description>
                <foaf:name>Alice</foaf:name>
                <foaf:mbox rdf:resource="mailto:alice@example.com"/>
        </rdf:Description>
        <rdf:Description>
                <foaf:name>Bob</foaf:name>
                <foaf:mbox rdf:resource="mailto:bob@example.org"/>
        </rdf:Description>
</rdf:RDF>
"""

select      = []
pattern     = GraphPattern([("?x",ns_foaf["name"],"?name")])
optional    = []
construct   = None
tripleStore = None




########NEW FILE########
__FILENAME__ = QueryTestCase
import unittest

from rdflib.Graph import Graph

class QueryTestCase(unittest.TestCase):

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def testUnicodeString(self):
        from rdflib.sparql.bison import Parse
        from cStringIO import StringIO

        q = \
          u"""
          PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
          SELECT ?pred
          WHERE { rdf:foobar rdf:predicate ?pred. }
          """ 

        p = Parse(q)

if __name__ == '__main__':
    unittest.main()


########NEW FILE########
__FILENAME__ = queryTest
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:29:46 $, by $Author: ivan $, $Revision: 1.1 $
#
"""

"""
import sys, os, time, datetime, imp, sys, StringIO

sys.path.insert(0,"../")

from rdflib import sparql
from rdflib.sparql import sparqlGraph
from testSPARQL import ns_rdf
from testSPARQL import ns_rdfs
from testSPARQL import ns_dc
from testSPARQL import ns_dc0
from testSPARQL import ns_foaf
from rdflib.FileInputSource import FileInputSource

def run(modName) :
        # Import the python module
        defs = None
        (fl,realpath,descr) = imp.find_module(modName,["."])
        mod = imp.load_module(modName,fl,realpath,descr)
        defs = mod.__dict__

        ##################################################
        # Three ways of identifying the RDF data:
        # 1. A Triple Store generated in the module
        tripleStore = None
        try :
                tripleStore = defs["tripleStore"]
        except :
                pass
        # 2. A reference to a set of RDF Files
        fils = None
        try :
                fils        = defs["datafiles"]
        except :
                pass
        # 3. Directly in the test module as a string
        rdfData = None
        try :
                rdfData     = defs["rdfData"]
        except :
                pass

        # Get the final of the triple store...
        if tripleStore == None :
                if rdfData == None :
                        tripleStore = retrieveRDFFiles(fils)
                else :
                        stream = StringIO.StringIO(rdfData)
                        tripleStore = sparqlGraph.SPARQLGraph()
                        tripleStore.parse(FileInputSource(stream),format="xml")

        ###############################################
        # Retrive the query data
        pattern     = defs["pattern"]
        optPattern  = defs["optional"]	
        select      = defs["select"]


        ###############################################		
        print "\n============= Test Module: %s =============" % modName			
        # better test modules describe their expected results...
        try :
                expected = defs["expected"]
                print "expected: %s" % expected
                print "=======\n"
        except :
                pass

        # Run the query and print the results						
        results = tripleStore.query(select,pattern,optPattern)
        num = len(results)
        print "Number of hits: %d" % num
        print
        for i in range(0,num) :
                hit = results[i]
                if len(select) == 1 :
                        print "%s: %s" % (select[0],hit)
                else :
                        for j in range(0,len(select)) :
                                var = select[j]
                                val = hit[j]
                                print "%s: %s" % (var,val)
                        print

if __name__ == '__main__' :
        if len(sys.argv) == 1 :
                print "Usage: %s modname1 modname2 ..." % sys.argv[0]
        else :
                for mod in sys.argv[1:] :
                        if mod.endswith(".py") :
                                run(mod[0:-3])
                        else :
                                run(mod)



########NEW FILE########
__FILENAME__ = Test1
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:29:46 $, by $Author: ivan $, $Revision: 1.1 $
#

from testSPARQL import ns_rdf
from testSPARQL import ns_rdfs
from testSPARQL import ns_dc
from testSPARQL import ns_dc0
from testSPARQL import ns_foaf

from rdflib.sparql.graphPattern import GraphPattern


# Careful to keep the <?xml declaration at the very beginning, otherwise the parser will fail...
rdfData ="""<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:foaf="http://xmlns.com/foaf/0.1/"
>
        <rdf:Description>
                <foaf:name>Johny Lee Outlaw</foaf:name>	
                <foaf:mbox rdf:resource="mailto:jlow@example.com"/>
        </rdf:Description>
</rdf:RDF>
"""

select      = ["?mbox","?junk"]
pattern     = GraphPattern([("?x",ns_foaf["name"],"Johny Lee Outlaw"),("?x",ns_foaf["mbox"],"?mbox")])
optional    = None
tripleStore = None
expected = '''
?mbox: mailto:jlow@example.com
?junk: None
'''




########NEW FILE########
__FILENAME__ = Test11_3
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:29:46 $, by $Author: ivan $, $Revision: 1.1 $
#
"""
   Datatype test. Note that this is not 100% kosher. The problem is that the Literal of rdflib does not check the
   datatypes. In theory, if the data contains:

           x ns:p 42.

   instead of:

       x ns:p 42^^http://www.w3.org/2001/XMLSchema#integer

    the query should return no results, because the first object is of datatype string. However, Literal does not
        implement this...

"""

rdfData ="""<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:foaf="http://xmlns.com/foaf/0.1/"
   xmlns:ns = "http://example.org/ns#"
>
        <rdf:Description>
                <foaf:name>Alice</foaf:name>
                <foaf:mbox rdf:resource="mailto:alice@work.example"/>
        </rdf:Description>
        <rdf:Description>
                <foaf:name>Bob</foaf:name>
                <foaf:mbox>bob@work.example</foaf:mbox>
        </rdf:Description>
</rdf:RDF>
"""



from testSPARQL import ns_rdf
from testSPARQL import ns_rdfs
from testSPARQL import ns_dc
from testSPARQL import ns_foaf
from testSPARQL import ns_ns
from testSPARQL import ns_book

from rdflib.Literal     import Literal
from rdflib.sparql.graphPattern import GraphPattern

from rdflib.sparql.sparqlOperators import isURI


select      = ["?name", "?mbox"]
pattern     = GraphPattern([("?x", ns_foaf["name"],"?name"),("?x",ns_foaf["mbox"],"?mbox")])
pattern.addConstraint(isURI("?mbox"))
optional    = []
tripleStore = None
expected = '''
  ?name: Alice
  ?mbox: mailto:alice@work.example
'''




########NEW FILE########
__FILENAME__ = Test2_5
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:29:46 $, by $Author: ivan $, $Revision: 1.1 $
#
"""
   Datatype test. Note that this is not 100% kosher. The problem is that the Literal of rdflib does not check the
   datatypes. In theory, if the data contains:

           x ns:p 42.

   instead of:

       x ns:p 42^^http://www.w3.org/2001/XMLSchema#integer

    the query should return no results, because the first object is of datatype string. However, Literal does not
        implement this...

"""

from testSPARQL import ns_rdf
from testSPARQL import ns_rdfs
from testSPARQL import ns_dc
from testSPARQL import ns_foaf
from testSPARQL import ns_ns
from testSPARQL import ns_book

from rdflib.Literal     import Literal
from rdflib.sparql.graphPattern import GraphPattern
rdfData ="""<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:foaf="http://xmlns.com/foaf/0.1/"
   xmlns:ns = "http://example.org/ns#"
>
        <rdf:Description>
                <foaf:name>Johny Lee Outlaw</foaf:name>
                <foaf:mbox rdf:resource="mailto:jlow@example.com"/>
        </rdf:Description>
        <rdf:Description>
                <foaf:name>Peter Goodguy</foaf:name>
                <foaf:mbox rdf:resource="mailto:peter@example.org"/>
        </rdf:Description>
</rdf:RDF>
"""

select      = ["?name", "?mbox"]
pattern     = GraphPattern([("?x", ns_foaf["name"],"?name"),("?x",ns_foaf["mbox"],"?mbox")])
optional    = []
tripleStore = None
expected = '''
  ?name: Johny Lee Outlaw
  ?mbox: mailto:jlow@example.com

  ?name: Peter Goodguy
  ?mbox: mailto:peter@example.org
'''



########NEW FILE########
__FILENAME__ = Test2_6
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:29:46 $, by $Author: ivan $, $Revision: 1.1 $
#
"""
   Datatype test. Note that this is not 100% kosher. The problem is that the Literal of rdflib does not check the
   datatypes. In theory, if the data contains:

           x ns:p 42.

   instead of:

       x ns:p 42^^http://www.w3.org/2001/XMLSchema#integer

    the query should return no results, because the first object is of datatype string. However, Literal does not
        implement this...

"""
rdfData ="""<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:foaf="http://xmlns.com/foaf/0.1/"
   xmlns:ns = "http://example.org/ns#"
>
        <rdf:Description>
                <foaf:name>Alice</foaf:name>
                <foaf:mbox rdf:resource="mailto:jlow@example.com"/>
        </rdf:Description>
        <rdf:Description>
                <foaf:name>Bob</foaf:name>
                <foaf:mbox rdf:resource="mailto:peter@example.org"/>
        </rdf:Description>
</rdf:RDF>
"""

from testSPARQL import ns_rdf
from testSPARQL import ns_rdfs
from testSPARQL import ns_dc
from testSPARQL import ns_foaf
from testSPARQL import ns_ns
from testSPARQL import ns_book

from rdflib.Literal     import Literal

from rdflib.sparql.graphPattern import GraphPattern

select      = ["?x", "?name"]
pattern     = GraphPattern([("?x", ns_foaf["name"],"?name")])
optional    = []
tripleStore = None
expected = '''
  ?x:   (some bnode)
  ?name: Alice

  ?x:   (some bnode)
  ?name: Bob
'''



########NEW FILE########
__FILENAME__ = Test3_1_1
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:29:46 $, by $Author: ivan $, $Revision: 1.1 $
#
"""
   Datatype test. Note that this is not 100% kosher. The problem is that the Literal of rdflib does not check the
   datatypes. In theory, if the data contains:

           x ns:p 42.

   instead of:

       x ns:p 42^^http://www.w3.org/2001/XMLSchema#integer

    the query should return no results, because the first object is of datatype string. However, Literal does not
        implement this...

"""
rdfData ="""<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:rdfs ="http://www.w3.org/2000/01/rdf-schema#"
   xmlns:rdf  ="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dc   ="http://purl.org/dc/elements/1.1/"
   xmlns:foaf ="http://xmlns.com/foaf/0.1/"
   xmlns:ns   ="http://example.org/ns#"
   xmlns:dt   ="http://example.org/datatype#"
>
        <rdf:Description>
                <ns:p rdf:datatype="http://www.w3.org/2001/XMLSchema#integer">42</ns:p>
                <ns:p rdf:datatype="http://example.org/datatype#specialDatatype">abc</ns:p>
                <ns:p>2005-02-27</ns:p>
                <ns:p xml:lang="en">cat</ns:p>
        </rdf:Description>
</rdf:RDF>
"""

from testSPARQL import ns_rdf
from testSPARQL import ns_rdfs
from testSPARQL import ns_dc
from testSPARQL import ns_foaf
from testSPARQL import ns_ns

from rdflib.Literal import Literal
import datetime
from rdflib.sparql.graphPattern import GraphPattern

select      = ["?v"]
pattern     = GraphPattern([("?v","?p",42)])
optional    = []
tripleStore = None
expected = '''
?v : (some bnode id)
'''



########NEW FILE########
__FILENAME__ = Test3_1_2
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:29:46 $, by $Author: ivan $, $Revision: 1.1 $
#
"""
   Datatype test. Note that this is not 100% kosher. The problem is that the Literal of rdflib does not check the
   datatypes. In theory, if the data contains:

           x ns:p 42.

   instead of:

       x ns:p 42^^http://www.w3.org/2001/XMLSchema#integer

    the query should return no results, because the first object is of datatype string. However, Literal does not
        implement this...

"""
rdfData ="""<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:rdfs ="http://www.w3.org/2000/01/rdf-schema#"
   xmlns:rdf  ="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dc   ="http://purl.org/dc/elements/1.1/"
   xmlns:foaf ="http://xmlns.com/foaf/0.1/"
   xmlns:ns   ="http://example.org/ns#"
   xmlns:dt   ="http://example.org/datatype#"
>
        <rdf:Description>
                <ns:p rdf:datatype="http://www.w3.org/2001/XMLSchema#integer">42</ns:p>
                <ns:p rdf:datatype="http://example.org/datatype#specialDatatype">abc</ns:p>
                <ns:p>2005-02-27</ns:p>
                <ns:p xml:lang="en">cat</ns:p>
        </rdf:Description>
</rdf:RDF>


"""

from testSPARQL import ns_rdf
from testSPARQL import ns_rdfs
from testSPARQL import ns_dc
from testSPARQL import ns_foaf
from testSPARQL import ns_ns

from rdflib.Literal import Literal
import datetime
from rdflib.sparql.graphPattern import GraphPattern

select      = ["?v"]
#pattern     = GraphPattern([("?v","?p",Literal("abc",datatype="http://example.org/datatype#specialDatatype"))])
pattern     = GraphPattern([("?v","?p","abc")])
optional    = []
tripleStore = None
expected = '''
EMPTY
'''




########NEW FILE########
__FILENAME__ = Test3_1_3
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:29:46 $, by $Author: ivan $, $Revision: 1.1 $
#
"""
   Datatype test. Note that this is not 100% kosher. The problem is that the Literal of rdflib does not check the
   datatypes. In theory, if the data contains:

           x ns:p 42.

   instead of:

       x ns:p 42^^http://www.w3.org/2001/XMLSchema#integer

    the query should return no results, because the first object is of datatype string. However, Literal does not
        implement this...

"""
from testSPARQL import ns_rdf
from testSPARQL import ns_rdfs
from testSPARQL import ns_dc
from testSPARQL import ns_foaf
from testSPARQL import ns_ns

rdfData ="""<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:rdfs ="http://www.w3.org/2000/01/rdf-schema#"
   xmlns:rdf  ="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dc   ="http://purl.org/dc/elements/1.1/"
   xmlns:foaf ="http://xmlns.com/foaf/0.1/"
   xmlns:ns   ="http://example.org/ns#"
   xmlns:dt   ="http://example.org/datatype#"
>
        <rdf:Description>
                <ns:p rdf:datatype="http://www.w3.org/2001/XMLSchema#integer">42</ns:p>
                <ns:p rdf:datatype="http://example.org/datatype#specialDatatype">abc</ns:p>
                <ns:p>2005-02-27</ns:p>
                <ns:p xml:lang="en">cat</ns:p>
        </rdf:Description>
</rdf:RDF>
"""

from rdflib.Literal import Literal
import datetime
from rdflib.sparql.graphPattern import GraphPattern

select      = ["?v"]
pattern     = GraphPattern([("?v","?p","cat")])
optional    = []
tripleStore = None
expected = '''
EMPTY
'''




########NEW FILE########
__FILENAME__ = Test3_1_4
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:29:46 $, by $Author: ivan $, $Revision: 1.1 $
#
"""
   Datatype test. Note that this is not 100% kosher. The problem is that the Literal of rdflib does not check the
   datatypes. In theory, if the data contains:

           x ns:p 42.

   instead of:

       x ns:p 42^^http://www.w3.org/2001/XMLSchema#integer

    the query should return no results, because the first object is of datatype string. However, Literal does not
        implement this...

"""
rdfData ="""<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:rdfs ="http://www.w3.org/2000/01/rdf-schema#"
   xmlns:rdf  ="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dc   ="http://purl.org/dc/elements/1.1/"
   xmlns:foaf ="http://xmlns.com/foaf/0.1/"
   xmlns:ns   ="http://example.org/ns#"
   xmlns:dt   ="http://example.org/datatype#"
>
        <rdf:Description>
                <ns:p rdf:datatype="http://www.w3.org/2001/XMLSchema#integer">42</ns:p>
                <ns:p rdf:datatype="http://example.org/datatype#specialDatatype">abc</ns:p>
                <ns:p>2005-02-27</ns:p>
                <ns:p xml:lang="en">cat</ns:p>
        </rdf:Description>
</rdf:RDF>
"""
from testSPARQL import ns_rdf
from testSPARQL import ns_rdfs
from testSPARQL import ns_dc
from testSPARQL import ns_foaf
from testSPARQL import ns_ns

from rdflib.Literal import Literal
import datetime
from rdflib.sparql.graphPattern import GraphPattern

select      = ["?v"]
pattern     = GraphPattern([("?v","?p",Literal("cat",lang="en"))])
optional    = []
tripleStore = None
expected = '''
?v : (some Bnode id)
'''




########NEW FILE########
__FILENAME__ = Test3_2
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:29:46 $, by $Author: ivan $, $Revision: 1.1 $
#

from testSPARQL import ns_rdf
from testSPARQL import ns_rdfs
from testSPARQL import ns_dc
from testSPARQL import ns_foaf
from testSPARQL import ns_ns
from testSPARQL import ns_book

from rdflib.Literal     import Literal
from rdflib.sparql.sparqlOperators import lt, ge
import datetime
from rdflib.sparql.graphPattern import GraphPattern

thresholdDate = datetime.date(2005,01,01)
rdfData ="""<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:foaf="http://xmlns.com/foaf/0.1/"
   xmlns:ns = "http://example.org/ns#"
   xmlns:book = "http://example.org/book"
>
        <rdf:Description rdf:ID="book1">
                <dc:title>SPARQL Tutorial</dc:title>
                <ns:price rdf:datatype="http://www.w3.org/2001/XMLSchema#integer">42</ns:price>
        </rdf:Description>
        <rdf:Description rdf:ID="book2">
                <dc:title>The Semantic Web</dc:title>
                <ns:price rdf:datatype="http://www.w3.org/2001/XMLSchema#integer">23</ns:price>
        </rdf:Description>
        <rdf:Description rdf:ID="book3">
                <dc:title>The Semantic Web Old</dc:title>
                <dc:date rdf:datatype="http://www.w3.org/2001/XMLSchema#date">2000-03-12</dc:date>
        </rdf:Description>
        <rdf:Description rdf:ID="book4">
                <dc:title>The Semantic Web New</dc:title>
                <dc:date rdf:datatype="http://www.w3.org/2001/XMLSchema#date">2005-03-02</dc:date>
        </rdf:Description>
</rdf:RDF>
"""

select      = ["?title", "?price"]
pattern     = GraphPattern([("?x", ns_dc["title"],"?title"),("?x",ns_ns["price"],"?price")])
pattern.addConstraint(lt("?price",30))
optional    = []
tripleStore = None
expected = '''
  ?title: The Semantic Web
  ?price: 23
'''




########NEW FILE########
__FILENAME__ = Test5_1
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:29:46 $, by $Author: ivan $, $Revision: 1.1 $
#
"""
   Datatype test. Note that this is not 100% kosher. The problem is that the Literal of rdflib does not check the
   datatypes. In theory, if the data contains:

           x ns:p 42.

   instead of:

       x ns:p 42^^http://www.w3.org/2001/XMLSchema#integer

    the query should return no results, because the first object is of datatype string. However, Literal does not
        implement this...

"""

from testSPARQL import ns_rdf
from testSPARQL import ns_rdfs
from testSPARQL import ns_dc
from testSPARQL import ns_foaf
from testSPARQL import ns_ns
from testSPARQL import ns_book

from rdflib.Literal     import Literal
from rdflib.sparql.graphPattern import GraphPattern

rdfData ="""<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:foaf="http://xmlns.com/foaf/0.1/"
   xmlns:ns = "http://example.org/ns#"
>
        <foaf:Person>
                <foaf:name>Alice</foaf:name>
                <foaf:mbox rdf:resource="mailto:alice@work.example"/>
        </foaf:Person>
        <foaf:Person>
                <foaf:name>Bob</foaf:name>
        </foaf:Person>
</rdf:RDF>
"""

select      = ["?name", "?mbox"]
pattern     = GraphPattern([("?x", ns_foaf["name"],"?name")])
#optional    = None
optional    = GraphPattern([("?x",ns_foaf["mbox"],"?mbox")])
tripleStore = None
expected = '''
  ?name: Alice
  ?mbox: mailto:alice@work.example

  ?name: Bob
  ?mbox: None
'''




########NEW FILE########
__FILENAME__ = Test5_2
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:29:46 $, by $Author: ivan $, $Revision: 1.1 $
#

from testSPARQL import ns_rdf
from testSPARQL import ns_rdfs
from testSPARQL import ns_dc
from testSPARQL import ns_foaf
from testSPARQL import ns_ns
from testSPARQL import ns_book

from rdflib.Literal     import Literal
from rdflib.sparql.sparqlOperators import lt, ge
import datetime
from rdflib.sparql.graphPattern import GraphPattern

thresholdDate = datetime.date(2005,01,01)

rdfData ="""<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:foaf="http://xmlns.com/foaf/0.1/"
   xmlns:ns = "http://example.org/ns#"
   xmlns:book = "http://example.org/book"
>
        <rdf:Description rdf:ID="book1">
                <dc:title>SPARQL Tutorial</dc:title>
                <ns:price rdf:datatype="http://www.w3.org/2001/XMLSchema#integer">42</ns:price>
        </rdf:Description>
        <rdf:Description rdf:ID="book2">
                <dc:title>The Semantic Web</dc:title>
                <ns:price rdf:datatype="http://www.w3.org/2001/XMLSchema#integer">23</ns:price>
        </rdf:Description>
</rdf:RDF>
"""

select      = ["?title", "?price"]
pattern     = GraphPattern([("?x", ns_dc["title"],"?title")])
optional    = GraphPattern([("?x",ns_ns["price"],"?price")])
optional.addConstraint(lt("?price",30))
tripleStore = None
expected = '''
  ?title: SPARQL Tutorial
  ?price: None

  ?title: The Semantic Web
  ?price: 23
'''




########NEW FILE########
__FILENAME__ = Test5_3
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:29:46 $, by $Author: ivan $, $Revision: 1.1 $
#
"""
   Datatype test. Note that this is not 100% kosher. The problem is that the Literal of rdflib does not check the
   datatypes. In theory, if the data contains:

           x ns:p 42.

   instead of:

       x ns:p 42^^http://www.w3.org/2001/XMLSchema#integer

    the query should return no results, because the first object is of datatype string. However, Literal does not
        implement this...

"""

from testSPARQL import ns_rdf
from testSPARQL import ns_rdfs
from testSPARQL import ns_dc
from testSPARQL import ns_foaf
from testSPARQL import ns_ns
from testSPARQL import ns_book

from rdflib.Literal     import Literal
from rdflib.sparql.graphPattern import GraphPattern


rdfData ="""<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:foaf="http://xmlns.com/foaf/0.1/"
   xmlns:ns = "http://example.org/ns#"
>
        <foaf:Person>
                <foaf:name>Alice</foaf:name>
                <foaf:homepage rdf:resource="http://work.example.org"/>	
        </foaf:Person>
        <foaf:Person>
                <foaf:name>Bob</foaf:name>
                <foaf:mbox rdf:resource="mailto:bob@work.example"/>
        </foaf:Person>
</rdf:RDF>
"""

select      = ["?name", "?mbox", "?hpage"]
pattern     = GraphPattern([("?x", ns_foaf["name"],"?name")])
#optional    = None
optional    = [
        GraphPattern([("?x",ns_foaf["mbox"],"?mbox")]),
        GraphPattern([("?x",ns_foaf["homepage"],"?hpage")])
]
tripleStore = None
expected = '''
  ?name:  Alice
  ?mbox:  None
  ?hpage: http://work.example.org

  ?name:  Bob
  ?mbox:  mailto:bob@work.example
  ?hpage: None
'''




########NEW FILE########
__FILENAME__ = Test6_11
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:29:46 $, by $Author: ivan $, $Revision: 1.1 $
#

from testSPARQL import ns_rdf
from testSPARQL import ns_rdfs
from testSPARQL import ns_dc
from testSPARQL import ns_dc0
from testSPARQL import ns_foaf
from testSPARQL import ns_ns
from testSPARQL import ns_book

from rdflib.Literal     import Literal
from rdflib.sparql.sparqlOperators import lt, ge
import datetime
from rdflib.sparql.graphPattern import GraphPattern

thresholdDate = datetime.date(2005,01,01)

rdfData ="""<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dc0="http://purl.org/dc/elements/1.0/"
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:foaf="http://xmlns.com/foaf/0.1/"
   xmlns:ns = "http://example.org/ns#"
   xmlns:book = "http://example.org/book"
>
        <rdf:Description rdf:ID="book2">
                <dc0:title>SPARQL Query Language Tutorial</dc0:title>
                <dc0:creator>Alice</dc0:creator>
        </rdf:Description>
        <rdf:Description rdf:ID="book1">
                <dc:title>SPARQL Protocol Tutorial</dc:title>
                <dc:creator>Bob</dc:creator>
        </rdf:Description>
</rdf:RDF>
"""

select      = ["?title"]
patt1       = GraphPattern([("?book",ns_dc0["title"],"?title")])
patt2       = GraphPattern([("?book",ns_dc["title"],"?title")])
pattern     = [patt1,patt2]
optional    = []
tripleStore = None
expected = '''
  ?title: SPARQL Query Language Tutorial

  ?title: SPARQL Protocol Tutorial
'''




########NEW FILE########
__FILENAME__ = Test6_12
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:29:46 $, by $Author: ivan $, $Revision: 1.1 $
#

from testSPARQL import ns_rdf
from testSPARQL import ns_rdfs
from testSPARQL import ns_dc
from testSPARQL import ns_dc0
from testSPARQL import ns_foaf
from testSPARQL import ns_ns
from testSPARQL import ns_book

from rdflib.Literal     import Literal
from rdflib.sparql.sparqlOperators import lt, ge
import datetime
from rdflib.sparql.graphPattern import GraphPattern

thresholdDate = datetime.date(2005,01,01)


rdfData ="""<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF
   xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:dc0="http://purl.org/dc/elements/1.0/"
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:foaf="http://xmlns.com/foaf/0.1/"
   xmlns:ns = "http://example.org/ns#"
   xmlns:book = "http://example.org/book"
>
        <rdf:Description rdf:ID="book2">
                <dc0:title>SPARQL Query Language Tutorial</dc0:title>
                <dc0:creator>Alice</dc0:creator>
        </rdf:Description>
        <rdf:Description rdf:ID="book1">
                <dc:title>SPARQL Protocol Tutorial</dc:title>
                <dc:creator>Bob</dc:creator>
        </rdf:Description>
</rdf:RDF>
"""

select      = ["?x","?y"]
patt1       = GraphPattern([("?book",ns_dc0["title"],"?x")])
patt2       = GraphPattern([("?book",ns_dc["title"],"?y")])
pattern     = [patt1,patt2]
optional    = []
tripleStore = None
expected = '''
  ?x: SPARQL Query Language Tutorial
  ?y: None

  ?x: None
  ?y: SPARQL Protocol Tutorial
'''




########NEW FILE########
__FILENAME__ = testSPARQL
#!/d/Bin/Python/python.exe
# -*- coding: utf-8 -*-
#
#
# $Date: 2005/04/02 07:29:30 $, by $Author: ivan $, $Revision: 1.1 $
#
"""

"""
import sys, os, time, datetime

from rdflib.constants   import RDFNS  as ns_rdf
from rdflib.constants   import RDFSNS as ns_rdfs
#from rdflib.sparql import ns_dc   as ns_dc
#from rdflib.sparql import ns_owl  as ns_owl

from rdflib.sparql.sparql import type_integer
from rdflib.sparql.sparql import type_double
from rdflib.sparql.sparql import type_float
from rdflib.sparql.sparql import type_decimal
from rdflib.sparql.sparql import type_dateTime

from rdflib.Namespace import Namespace

ns_foaf   = Namespace("http://xmlns.com/foaf/0.1/")
ns_ns     = Namespace("http://example.org/ns#")
ns_book   = Namespace("http://example.org/book")
ns_person = Namespace("http://example.org/person#")
ns_dt     = Namespace("http://example.org/datatype#")
ns_dc0    = Namespace("http://purl.org/dc/elements/1.0/")
ns_dc     = Namespace("http://purl.org/dc/elements/1.1/")
ns_vcard  = Namespace("http://www.w3.org/2001/vcard-rdf/3.0#")


########NEW FILE########
__FILENAME__ = sparql_empty_prefix
from rdflib import ConjunctiveGraph
from StringIO import StringIO
import unittest

test_data = """
@prefix foaf:       <http://xmlns.com/foaf/0.1/> .
@prefix rdf:        <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .

_:a  foaf:name       "Alice" .
"""

test_query = """PREFIX :<http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE {
    ?x :name ?name .
}"""

correct = '"name" : {"type": "literal", "xml:lang" : "None", "value" : "Alice"}'
                
class Query(unittest.TestCase):

    def testQueryPlus(self):
        graph = ConjunctiveGraph()
        graph.parse(StringIO(test_data), format="n3")
        result_json = graph.query(test_query).serialize(format='json')
        self.failUnless(result_json.find(correct) > 0)

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = sparql_limit
from rdflib import ConjunctiveGraph, plugin
from rdflib.store import Store
from StringIO import StringIO
import unittest

test_data = """ 
@prefix foaf:       <http://xmlns.com/foaf/0.1/> .
@prefix rdf:        <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .

<http://example.org/bob>  foaf:name       "Bob" .
<http://example.org/dave>  foaf:name       "Dave" .
<http://example.org/alice>  foaf:name       "Alice" .
<http://example.org/charlie>  foaf:name       "Charlie" .
"""

test_query = """
PREFIX foaf: <http://xmlns.com/foaf/0.1/>

SELECT ?name
WHERE { ?x foaf:name ?name . }
LIMIT 2
"""

class TestLimit(unittest.TestCase):

    def testLimit(self):
        graph = ConjunctiveGraph(plugin.get('IOMemory',Store)())
        graph.parse(StringIO(test_data), format="n3")
        results = graph.query(test_query)
        self.failUnless(len(results) == 2)

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = sparql_order_by
from rdflib import ConjunctiveGraph, plugin, Literal
from rdflib.store import Store
from StringIO import StringIO
import unittest

test_data = """ 
@prefix foaf:       <http://xmlns.com/foaf/0.1/> .
@prefix rdf:        <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .

<http://example.org/bob>  foaf:name       "Bob" .
<http://example.org/dave>  foaf:name       "Dave" .
<http://example.org/alice>  foaf:name       "Alice" .
<http://example.org/charlie>  foaf:name       "Charlie" .
"""

test_query = """
PREFIX foaf: <http://xmlns.com/foaf/0.1/>

SELECT ?name
WHERE { ?x foaf:name ?name . }
ORDER BY ?name
"""

class TestOrderBy(unittest.TestCase):

    def testOrderBy(self):
        graph = ConjunctiveGraph(plugin.get('IOMemory',Store)())
        graph.parse(StringIO(test_data), format="n3")
        results = graph.query(test_query)

        self.failUnless(False not in [r[0] == a for r, a in zip(results, ['Alice', 'Bob', 'Charlie', 'Dave'])])

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = sparql_parser_instability
BAD_SPARQL=\
"""
BASE <tag:chimezie@ogbuji.net,2007:exampleNS>.
SELECT ?s
WHERE { ?s ?p ?o }"""

def test_bad_sparql():
    from rdflib.Graph import Graph
    Graph().query(BAD_SPARQL)
test_bad_sparql.unstable = True

if __name__ == '__main__':
    test_bad_sparql()

########NEW FILE########
__FILENAME__ = sparql_regex
from rdflib import ConjunctiveGraph, plugin
from rdflib.store import Store
from StringIO import StringIO
import unittest

test_data = """ 
@prefix foaf:       <http://xmlns.com/foaf/0.1/> .
@prefix rdf:        <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .

<http://example.org/bob>  foaf:name       "Bob" .
<http://example.org/dave>  foaf:name       "Dave" .
<http://example.org/alice>  foaf:name       "Alice" .
<http://example.org/charlie>  foaf:name       "Charlie" .
"""

test_query = """
PREFIX foaf: <http://xmlns.com/foaf/0.1/>

SELECT ?name
WHERE { ?x foaf:name ?name .
        FILTER regex(?name, "a", "i") 
        }
"""

class TestRegex(unittest.TestCase):

    def testRegex(self):
        graph = ConjunctiveGraph(plugin.get('IOMemory',Store)())
        graph.parse(StringIO(test_data), format="n3")
        results = graph.query(test_query)
        self.failUnless(len([a for a in results if 'a' in a[0] or 'A' in a[0]]) == 3)

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = store_performace
import unittest

from rdflib.Graph import Graph
from rdflib import URIRef

import gc
import itertools
from time import time
from random import random

from tempfile import mkdtemp

def random_uri():
    return URIRef("%s" % random())

class StoreTestCase(unittest.TestCase):
    """
    Test case for testing store performance... probably should be
    something other than a unit test... but for now we'll add it as a
    unit test.
    """
    store = 'default'

    def setUp(self):
        self.gcold = gc.isenabled()
        gc.collect()
        gc.disable()
        self.graph = Graph(store=self.store)
        if self.store == "MySQL":
            from test.mysql import configString
            from rdflib.store.MySQL import MySQL
            path=configString
            MySQL().destroy(path)
        else:
            path = a_tmp_dir = mkdtemp()
        self.graph.open(path, create=True)
        self.input = input = Graph()
        input.parse("http://eikeon.com")

    def tearDown(self):
        self.graph.close()
        if self.gcold:
            gc.enable()
        # TODO: delete a_tmp_dir
        del self.graph

    def testTime(self):
        number = 1
        print self.store
        print "input:",
        for i in itertools.repeat(None, number):
            self._testInput()
        print "random:",
        for i in itertools.repeat(None, number):
            self._testRandom()
        print "."

    def _testRandom(self):
        number = len(self.input)
        store = self.graph

        def add_random():
            s = random_uri()
            p = random_uri()
            o = random_uri()
            store.add((s, p, o))

        it = itertools.repeat(None, number)
        t0 = time()
        for _i in it:
            add_random()
        t1 = time()
        print "%.3g" % (t1 - t0),

    def _testInput(self):
        number = 1
        store = self.graph

        def add_from_input():
            for t in self.input:
                store.add(t)

        it = itertools.repeat(None, number)
        t0 = time()
        for _i in it:
            add_from_input()
        t1 = time()
        print "%.3g" % (t1 - t0),


class MemoryStoreTestCase(StoreTestCase):
    store = "Memory"

try:
    from rdflib.store.Sleepycat import Sleepycat
    class SleepycatStoreTestCase(StoreTestCase):
        store = "Sleepycat"
except ImportError, e:
    print "Can not test Sleepycat store:", e

try:
    import persistent
    # If we can import persistent then test ZODB store
    class ZODBStoreTestCase(StoreTestCase):
        non_standard_dep = True
        store = "ZODB"
except ImportError, e:
    print "Can not test ZODB store:", e


try:
    import RDF
    # If we can import RDF then test Redland store
    class RedLandTestCase(StoreTestCase):
        non_standard_dep = True
        store = "Redland"
except ImportError, e:
    print "Can not test Redland store:", e

# TODO: add test case for 4Suite backends?  from Ft import Rdf

try:
#     import todo # what kind of configuration string does open need?

    import MySQLdb,sha,sys
    # If we can import RDF then test Redland store
    class MySQLTestCase(StoreTestCase):
        non_standard_dep = True
        store = "MySQL"
except ImportError, e:
    print "Can not test MySQL store:", e

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_not_equals
from rdflib.Namespace import Namespace
from rdflib import plugin,RDF,RDFS,URIRef, StringInputSource, Literal
from rdflib.Graph import Graph,ReadOnlyGraphAggregate,ConjunctiveGraph

import sys
from pprint import pprint

def testSPARQLNotEquals():
    NS = u"http://example.org/"
    graph = ConjunctiveGraph()
    graph.parse(StringInputSource("""
       @prefix    : <http://example.org/> .
       @prefix rdf: <%s> .
       :foo rdf:value 1.
       :bar rdf:value 2."""%RDF.RDFNS), format="n3")
    rt = graph.query("""SELECT ?node 
                        WHERE {
                                ?node rdf:value ?val.
                                FILTER (?val != 1)
                               }""",
                           initNs={'rdf':RDF.RDFNS},                           
                           DEBUG=False)
    for row in rt:        
        item = row[0]
        assert item == URIRef("http://example.org/bar")

if __name__ == '__main__':
    testSPARQLNotEquals()

########NEW FILE########
__FILENAME__ = test_sparql_base_ref
from rdflib import ConjunctiveGraph, Literal
from StringIO import StringIO
import unittest


test_data = """
@prefix foaf:       <http://xmlns.com/foaf/0.1/> .
@prefix rdf:        <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .

<http://example.org/alice> a foaf:Person;
    foaf:name "Alice";
    foaf:knows <http://example.org/bob> ."""

test_query = """
BASE <http://xmlns.com/foaf/0.1/>
SELECT ?name
WHERE { [ a :Person ; :name ?name ] }"""

class TestSparqlJsonResults(unittest.TestCase):

    def setUp(self):
        self.graph = ConjunctiveGraph()
        self.graph.parse(StringIO(test_data), format="n3")

    def test_base_ref(self):
        rt=self.graph.query(test_query).serialize("python")
        self.failUnless(rt[0] == Literal("Alice"),"Expected:\n 'Alice' \nGot:\n %s" % rt)

if __name__ == "__main__":
    unittest.main()


########NEW FILE########
__FILENAME__ = test_sparql_equals
# -*- coding: UTF-8 -*-
from rdflib import ConjunctiveGraph, URIRef
from StringIO import StringIO
import unittest

class TestSparqlEquals(unittest.TestCase):

    PREFIXES = {
        'rdfs': "http://www.w3.org/2000/01/rdf-schema#"
    }

    def setUp(self):
        testContent = """
            @prefix rdfs: <%(rdfs)s> .
            <http://example.org/doc/1> rdfs:label "Document 1"@en .
            <http://example.org/doc/2> rdfs:label "Document 2"@en .
            <http://example.org/doc/3> rdfs:label "Document 3"@en .
        """ % self.PREFIXES
        self.graph = graph = ConjunctiveGraph()
        self.graph.load(StringIO(testContent), format='n3')

    def test_uri_equals(self):
        uri = URIRef("http://example.org/doc/1")
        query = ("""
            PREFIX rdfs: <%(rdfs)s>

            SELECT ?uri WHERE {
                ?uri rdfs:label ?label .
                FILTER( ?uri = <"""+uri+"""> )
            }
        """) % self.PREFIXES
        res = self.graph.query(query)
        expected = [uri]
        self.assertEqual(res.selected,expected)

if __name__ == "__main__":
    unittest.main()
########NEW FILE########
__FILENAME__ = test_sparql_graph_graph_pattern
from rdflib import Literal, URIRef, Namespace
from rdflib.Graph import Graph, ReadOnlyGraphAggregate
from StringIO import StringIO
import unittest

FOAF = Namespace("http://xmlns.com/foaf/0.1/")

#See: http://www.w3.org/TR/rdf-sparql-query/#queryDataset

test_graph_a = """
@prefix  foaf:     <http://xmlns.com/foaf/0.1/> .
@prefix  rdf:      <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix  rdfs:     <http://www.w3.org/2000/01/rdf-schema#> .

_:a  foaf:name     "Alice" .
_:a  foaf:mbox     <mailto:alice@work.example> .
_:a  foaf:knows    _:b .

_:b  foaf:name     "Bob" .
_:b  foaf:mbox     <mailto:bob@work.example> .
_:b  foaf:nick     "Bobby" .
_:b  rdfs:seeAlso  <http://example.org/foaf/bobFoaf> .

<http://example.org/foaf/bobFoaf>
     rdf:type      foaf:PersonalProfileDocument ."""
     
test_graph_b = """
@prefix  foaf:     <http://xmlns.com/foaf/0.1/> .
@prefix  rdf:      <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix  rdfs:     <http://www.w3.org/2000/01/rdf-schema#> .

_:z  foaf:mbox     <mailto:bob@work.example> .
_:z  rdfs:seeAlso  <http://example.org/foaf/bobFoaf> .
_:z  foaf:nick     "Robert" .

<http://example.org/foaf/bobFoaf>
     rdf:type      foaf:PersonalProfileDocument ."""     

test_query1 = """
PREFIX foaf: <http://xmlns.com/foaf/0.1/>

SELECT ?src ?bobNick
FROM NAMED <http://example.org/foaf/aliceFoaf>
FROM NAMED <http://example.org/foaf/bobFoaf>
WHERE
  {
    GRAPH ?src
    { ?x foaf:mbox <mailto:bob@work.example> .
      ?x foaf:nick ?bobNick
    }
  }"""

test_query2= """
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
PREFIX data: <http://example.org/foaf/>

SELECT ?nick
FROM NAMED <http://example.org/foaf/aliceFoaf>
FROM NAMED <http://example.org/foaf/bobFoaf>
WHERE
  {
     GRAPH data:bobFoaf {
         ?x foaf:mbox <mailto:bob@work.example> .
         ?x foaf:nick ?nick }
  }"""

test_query3= """
PREFIX  data:  <http://example.org/foaf/>
PREFIX  foaf:  <http://xmlns.com/foaf/0.1/>
PREFIX  rdfs:  <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?mbox ?nick ?ppd
FROM NAMED <http://example.org/foaf/aliceFoaf>
FROM NAMED <http://example.org/foaf/bobFoaf>
WHERE
{
  GRAPH data:aliceFoaf
  {
    ?alice foaf:mbox <mailto:alice@work.example> ;
           foaf:knows ?whom .
    ?whom  foaf:mbox ?mbox ;
           rdfs:seeAlso ?ppd .
    ?ppd  a foaf:PersonalProfileDocument .
  } .
  GRAPH ?ppd
  {
      ?w foaf:mbox ?mbox ;
         foaf:nick ?nick
  }
}"""

class TestGraphGraphPattern(unittest.TestCase):

    def setUp(self):
        self.graph1 = Graph(identifier=URIRef('http://example.org/foaf/aliceFoaf'))
        self.graph1.parse(StringIO(test_graph_a), format="n3")
        self.graph2 = Graph(identifier=URIRef('http://example.org/foaf/bobFoaf'))
        self.graph2.parse(StringIO(test_graph_b), format="n3")
        self.unionGraph = ReadOnlyGraphAggregate(graphs=[self.graph1,self.graph2])

    def test_8_3_1(self):
        rt=self.unionGraph.query(test_query1,DEBUG=False).serialize("python")
        self.failUnless(len(rt) == 2,"Expected 2 item solution set")
        for src,bobNick in rt:
            self.failUnless(src in [URIRef('http://example.org/foaf/aliceFoaf'),URIRef('http://example.org/foaf/bobFoaf')],
                            "Unexpected ?src binding :\n %s" % src)
            self.failUnless(bobNick in [Literal("Bobby"),Literal("Robert")],
                            "Unexpected ?bobNick binding :\n %s" % bobNick)
            
    def test_8_3_2(self):
        rt=self.unionGraph.query(test_query2,DEBUG=False).serialize("python")
        self.failUnless(len(rt) == 1,"Expected 1 item solution set")
        self.failUnless(rt[0]  == Literal("Robert"),"Unexpected ?nick binding :\n %s" % rt[0])            
        
#    def test_8_3_3(self):
#        rt=self.unionGraph.query(test_query3,DEBUG=False).serialize("python")
#        self.failUnless(len(rt) == 1,"Expected 1 item solution set")
#        for mbox,nick,ppd in rt:
#            self.failUnless(mbox == URIRef('mailto:bob@work.example'),
#                            "Unexpected ?mbox binding :\n %s" % mbox)
#            self.failUnless(nick  == Literal("Robert"),
#                            "Unexpected ?nick binding :\n %s" % nick)
#            self.failUnless(ppd == URIRef('http://example.org/foaf/bobFoaf'),
#                            "Unexpected ?ppd binding :\n %s" % ppd)

if __name__ == "__main__":
    unittest.main()


########NEW FILE########
__FILENAME__ = test_sparql_json_results
from rdflib import ConjunctiveGraph
from StringIO import StringIO
import unittest


test_data = """
@prefix foaf:       <http://xmlns.com/foaf/0.1/> .
@prefix rdf:        <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .

<http://example.org/alice> a foaf:Person;
    foaf:name "Alice";
    foaf:knows <http://example.org/bob> .

<http://example.org/bob> a foaf:Person;
    foaf:name "Bob" .
"""


PROLOGUE = """
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
"""


test_material = {}

test_material['optional'] = (PROLOGUE+"""
    SELECT ?name ?x ?friend
    WHERE { ?x foaf:name ?name .
            OPTIONAL { ?x foaf:knows ?friend . }
    }
    """,
    """"name" : {"type": "literal", "xml:lang" : "None", "value" : "Bob"},
                   "x" : {"type": "uri", "value" : "http://example.org/bob"}
                }"""
    )

test_material['select_vars'] = (PROLOGUE+"""
    SELECT ?name ?friend
    WHERE { ?x foaf:name ?name .
            OPTIONAL { ?x foaf:knows ?friend . }
    }""",
    """"vars" : [
             "name",
             "friend"
         ]"""
    )

test_material['wildcard'] = (PROLOGUE+"""
    SELECT * WHERE { ?x foaf:name ?name . }
    """,
    """"name" : {"type": "literal", "xml:lang" : "None", "value" : "Bob"},
                   "x" : {"type": "uri", "value" : "http://example.org/bob"}
                }"""
    )

test_material['wildcard_vars'] = (PROLOGUE+"""
    SELECT * WHERE { ?x foaf:name ?name . }
    """,
    """"vars" : [
             "name",
             "x"
         ]"""
    )

test_material['union'] = (PROLOGUE+"""
    SELECT DISTINCT ?name WHERE {
                { <http://example.org/alice> foaf:name ?name . } UNION { <http://example.org/bob> foaf:name ?name . }
    }
    """,
    """{
                   "name" : {"type": "literal", "xml:lang" : "None", "value" : "Bob"}
                },
               {
                   "name" : {"type": "literal", "xml:lang" : "None", "value" : "Alice"}
                }"""
    )

test_material['union3'] = (PROLOGUE+"""
    SELECT DISTINCT ?name WHERE {
                { <http://example.org/alice> foaf:name ?name . }
                UNION { <http://example.org/bob> foaf:name ?name . }
                UNION { <http://example.org/nobody> foaf:name ?name . }
    }
            """, '"Alice"'
    )


def make_method(testname):
    def test(self):
        query, correct = test_material[testname]
        self._query_result_contains(query, correct)
    test.__name__ = 'test%s' % testname.title()
    return test


class TestSparqlJsonResults(unittest.TestCase):

    def setUp(self):
        self.graph = ConjunctiveGraph()
        self.graph.parse(StringIO(test_data), format="n3")

    def _query_result_contains(self, query, correct):
        results = self.graph.query(query)
        result_json = results.serialize(format='json')
        self.failUnless(result_json.find(correct) >= 0,
                "Expected:\n %s \n- to contain:\n%s" % (result_json, correct))

    testOptional = make_method('optional')

    testWildcard = make_method('wildcard')

    testUnion = make_method('union')

    testUnion3 = make_method('union3')

    testSelectVars = make_method('select_vars')
    
    testWildcardVars = make_method('wildcard_vars')
    
if __name__ == "__main__":
    unittest.main()


########NEW FILE########
__FILENAME__ = test_sparql_told_bnodes
from rdflib.Namespace import Namespace
from rdflib import plugin,RDF,RDFS,URIRef, StringInputSource, Literal, BNode
from rdflib.Graph import Graph,ReadOnlyGraphAggregate,ConjunctiveGraph
import unittest,sys
from pprint import pprint

class TestSPARQLToldBNodes(unittest.TestCase):
    def setUp(self):
        NS = u"http://example.org/"
        self.graph = ConjunctiveGraph()
        self.graph.parse(StringInputSource("""
           @prefix    : <http://example.org/> .
           @prefix rdf: <%s> .
           @prefix rdfs: <%s> .
           [ :prop :val ].
           [ a rdfs:Class ]."""%(RDF.RDFNS,RDFS.RDFSNS)), format="n3")
    def testToldBNode(self):
        for s,p,o in self.graph.triples((None,RDF.type,None)):
            pass
        query = """SELECT ?obj WHERE { %s ?prop ?obj }"""%s.n3()
        print query
        rt = self.graph.query(query)
        self.failUnless(len(rt) == 1,"BGP should only match the 'told' BNode by name (result set size: %s)"%len(rt))

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_sparql_xml_results
from rdflib import ConjunctiveGraph
from StringIO import StringIO
import re
import unittest

test_data = """
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

<http://example.org/word>
    rdfs:label "Word"@en;
    rdf:value 1;
    rdfs:seeAlso [] .

"""

PROLOGUE = """
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX owl:  <http://www.w3.org/2002/07/owl#>
"""

query = PROLOGUE+"""
SELECT ?s ?o WHERE { ?s ?p ?o . }
"""

try:
    from Ft.Xml import MarkupWriter

    expected_fragments = [
        u"""<sparql xmlns="http://www.w3.org/2005/sparql-results#"> <head>""",

        u"""</head> <results distinct="false" ordered="false">""",

        u"""<binding name="s"> <uri>http://example.org/word</uri> </binding>""",

        u"""<binding name="o"> <bnode>""",

        u"""<binding name="o"> <literal datatype="http://www.w3.org/2001/XMLSchema#integer">1</literal> </binding>""",

        (u"""<result> <binding name="s"> <uri>http://example.org/word</uri> </binding>"""
        """ <binding name="o"> <literal xml:lang="en">Word</literal> </binding> </result>""")
    ]

except ImportError:
    expected_fragments = [
        #u"""<sparql:sparql xmlns="http://www.w3.org/2005/sparql-results#"><sparql:head>""",

        u"""</sparql:head><sparql:results distinct="false" ordered="false">""",

        u"""<sparql:binding name="s"><sparql:uri>http://example.org/word</sparql:uri></sparql:binding>""",

        u"""<sparql:binding name="o"><sparql:bnode>""",

        u"""<sparql:binding name="o"><sparql:literal datatype="http://www.w3.org/2001/XMLSchema#integer">1</sparql:literal></sparql:binding>""",

        u"""<sparql:result><sparql:binding name="s"><sparql:uri>http://example.org/word</sparql:uri></sparql:binding><sparql:binding name="o"><sparql:literal xml:lang="en">Word</sparql:literal></sparql:binding></sparql:result>"""
    ]


# TODO:
#   - better canonicalization of results to compare with (4Suite-XML has support for this)
#   - test expected 'variable'-elems in head


class TestSparqlXmlResults(unittest.TestCase):

    def setUp(self):
        self.graph = ConjunctiveGraph()
        self.graph.parse(StringIO(test_data), format="n3")

    def testSimple(self):
        self._query_result_contains(query, expected_fragments)

    def _query_result_contains(self, query, fragments):
        results = self.graph.query(query)
        result_xml = results.serialize(format='xml')
        result_xml = normalize(result_xml) # TODO: poor mans c14n..
        print result_xml
        for frag in fragments:
            print frag
            self.failUnless(frag in result_xml)


def normalize(s, exp=re.compile(r'\s+', re.MULTILINE)):
    return exp.sub(' ', s)


if __name__ == "__main__":
    unittest.main()



########NEW FILE########
__FILENAME__ = triple_store
import unittest

from rdflib import URIRef, BNode, Literal, RDFS
from rdflib.Graph import Graph


class GraphTest(unittest.TestCase):
    backend = 'default'
    path = 'store'

    def setUp(self):
        self.store = Graph(store=self.backend)
        self.store.open(self.path)
        self.remove_me = (BNode(), RDFS.label, Literal("remove_me"))
        self.store.add(self.remove_me)

    def tearDown(self):
        self.store.close()

    def testAdd(self):
        subject = BNode()
        self.store.add((subject, RDFS.label, Literal("foo")))

    def testRemove(self):
        self.store.remove(self.remove_me)
        self.store.remove((None, None, None))

    def testTriples(self):
        for s, p, o in self.store:
            pass

if __name__ == "__main__":
    unittest.main()



########NEW FILE########
__FILENAME__ = trix
#!/usr/bin/env python

import sys
sys.path[0:0]+=[".."]


import rdflib
import unittest


class TriXTestCase(unittest.TestCase):

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def testAperture(self): 

        g=rdflib.Graph()

        g.parse("trix/aperture.trix",format="trix")

        c=list(g.contexts())

        #print list(g.contexts())
        t=sum(map(lambda x: len(g.get_context(x)),g.contexts()))

        self.assertEquals(t,24)
        self.assertEquals(len(c),4)
        
        #print "Parsed %d triples"%t

    def testSpec(self): 

        g=rdflib.Graph()
        
        g.parse("trix/nokia_example.trix",format="trix")
        
        #print "Parsed %d triples"%len(g)
        



if __name__=='__main__':
    unittest.main()
                          

########NEW FILE########
__FILENAME__ = type_check
import unittest

from rdflib.Graph import Graph
from rdflib.exceptions import SubjectTypeError
from rdflib.exceptions import PredicateTypeError
from rdflib.exceptions import ObjectTypeError
from rdflib.URIRef import URIRef

foo = URIRef("foo")


class TypeCheckCase(unittest.TestCase):
    unstable = True # TODO: until we decide if we want to add type checking back to rdflib
    backend = 'default'
    path = 'store'

    def setUp(self):
        self.store = Graph(backend=self.backend)
        self.store.open(self.path)

    def tearDown(self):
        self.store.close()

    def testSubjectTypeCheck(self):
        self.assertRaises(SubjectTypeError,
                          self.store.add, (None, foo, foo))

    def testPredicateTypeCheck(self):
        self.assertRaises(PredicateTypeError,
                          self.store.add, (foo, None, foo))

    def testObjectTypeCheck(self):
        self.assertRaises(ObjectTypeError,
                          self.store.add, (foo, foo, None))

########NEW FILE########
__FILENAME__ = util
import unittest

from rdflib import Literal

from rdflib.store.NodePickler import NodePickler


class UtilTestCase(unittest.TestCase):

    def test_to_bits_from_bits_round_trip(self):
        np = NodePickler()

        a = Literal(u'''A test with a \\n (backslash n), "\u00a9" , and newline \n and a second line.
''')
        b = np.loads(np.dumps(a))
        self.assertEquals(a, b)


if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')

########NEW FILE########
__FILENAME__ = decoder
"""
Implementation of JSONDecoder
"""
import re
import sys
import struct

from simplejson.scanner import make_scanner
try:
    from simplejson._speedups import scanstring as c_scanstring
except ImportError:
    c_scanstring = None

FLAGS = re.VERBOSE | re.MULTILINE | re.DOTALL

def _floatconstants():
    _BYTES = '7FF80000000000007FF0000000000000'.decode('hex')
    if sys.byteorder != 'big':
        _BYTES = _BYTES[:8][::-1] + _BYTES[8:][::-1]
    nan, inf = struct.unpack('dd', _BYTES)
    return nan, inf, -inf

NaN, PosInf, NegInf = _floatconstants()


def linecol(doc, pos):
    lineno = doc.count('\n', 0, pos) + 1
    if lineno == 1:
        colno = pos
    else:
        colno = pos - doc.rindex('\n', 0, pos)
    return lineno, colno


def errmsg(msg, doc, pos, end=None):
    # Note that this function is called from _speedups
    lineno, colno = linecol(doc, pos)
    if end is None:
        return '%s: line %d column %d (char %d)' % (msg, lineno, colno, pos)
    endlineno, endcolno = linecol(doc, end)
    return '%s: line %d column %d - line %d column %d (char %d - %d)' % (
        msg, lineno, colno, endlineno, endcolno, pos, end)


_CONSTANTS = {
    '-Infinity': NegInf,
    'Infinity': PosInf,
    'NaN': NaN,
}

STRINGCHUNK = re.compile(r'(.*?)(["\\\x00-\x1f])', FLAGS)
BACKSLASH = {
    '"': u'"', '\\': u'\\', '/': u'/',
    'b': u'\b', 'f': u'\f', 'n': u'\n', 'r': u'\r', 't': u'\t',
}

DEFAULT_ENCODING = "utf-8"

def py_scanstring(s, end, encoding=None, strict=True, _b=BACKSLASH, _m=STRINGCHUNK.match):
    if encoding is None:
        encoding = DEFAULT_ENCODING
    chunks = []
    _append = chunks.append
    begin = end - 1
    while 1:
        chunk = _m(s, end)
        if chunk is None:
            raise ValueError(
                errmsg("Unterminated string starting at", s, begin))
        end = chunk.end()
        content, terminator = chunk.groups()
        if content:
            if not isinstance(content, unicode):
                content = unicode(content, encoding)
            _append(content)
        if terminator == '"':
            break
        elif terminator != '\\':
            if strict:
                raise ValueError(errmsg("Invalid control character %r at", s, end))
            else:
                _append(terminator)
                continue
        try:
            esc = s[end]
        except IndexError:
            raise ValueError(
                errmsg("Unterminated string starting at", s, begin))
        if esc != 'u':
            try:
                m = _b[esc]
            except KeyError:
                raise ValueError(
                    errmsg("Invalid \\escape: %r" % (esc,), s, end))
            end += 1
        else:
            esc = s[end + 1:end + 5]
            next_end = end + 5
            msg = "Invalid \\uXXXX escape"
            try:
                if len(esc) != 4:
                    raise ValueError
                uni = int(esc, 16)
                if 0xd800 <= uni <= 0xdbff and sys.maxunicode > 65535:
                    msg = "Invalid \\uXXXX\\uXXXX surrogate pair"
                    if not s[end + 5:end + 7] == '\\u':
                        raise ValueError
                    esc2 = s[end + 7:end + 11]
                    if len(esc2) != 4:
                        raise ValueError
                    uni2 = int(esc2, 16)
                    uni = 0x10000 + (((uni - 0xd800) << 10) | (uni2 - 0xdc00))
                    next_end += 6
                m = unichr(uni)
            except ValueError:
                raise ValueError(errmsg(msg, s, end))
            end = next_end
        _append(m)
    return u''.join(chunks), end


# Use speedup if available
scanstring = c_scanstring or py_scanstring

WHITESPACE = re.compile(r'[ \t\n\r]*', FLAGS)
WHITESPACE_STR = ' \t\n\r'

def JSONObject((s, end), encoding, strict, scan_once, object_hook, _w=WHITESPACE.match, _ws=WHITESPACE_STR):
    pairs = {}
    nextchar = s[end:end + 1]
    # Normally we expect nextchar == '"'
    if nextchar != '"':
        if nextchar in _ws:
            end = _w(s, end).end()
            nextchar = s[end:end + 1]
        # Trivial empty object
        if nextchar == '}':
            return pairs, end + 1
        elif nextchar != '"':
            raise ValueError(errmsg("Expecting property name", s, end))
    end += 1
    while True:
        key, end = scanstring(s, end, encoding, strict)

        # To skip some function call overhead we optimize the fast paths where
        # the JSON key separator is ": " or just ":".
        if s[end:end + 1] != ':':
            end = _w(s, end).end()
            if s[end:end + 1] != ':':
                raise ValueError(errmsg("Expecting : delimiter", s, end))

        end += 1

        try:
            if s[end] in _ws:
                end += 1
                if s[end] in _ws:
                    end = _w(s, end + 1).end()
        except IndexError:
            pass

        try:
            value, end = scan_once(s, end)
        except StopIteration:
            raise ValueError(errmsg("Expecting object", s, end))
        pairs[key] = value
        
        try:
            nextchar = s[end]
            if nextchar in _ws:
                end = _w(s, end + 1).end()
                nextchar = s[end]
        except IndexError:
            nextchar = ''
        end += 1

        if nextchar == '}':
            break
        elif nextchar != ',':
            raise ValueError(errmsg("Expecting , delimiter", s, end - 1))

        try:
            nextchar = s[end]
            if nextchar in _ws:
                end += 1
                nextchar = s[end]
                if nextchar in _ws:
                    end = _w(s, end + 1).end()
                    nextchar = s[end]
        except IndexError:
            nextchar = ''

        end += 1
        if nextchar != '"':
            raise ValueError(errmsg("Expecting property name", s, end - 1))

    if object_hook is not None:
        pairs = object_hook(pairs)
    return pairs, end

def JSONArray((s, end), scan_once, _w=WHITESPACE.match, _ws=WHITESPACE_STR):
    values = []
    nextchar = s[end:end + 1]
    if nextchar in _ws:
        end = _w(s, end + 1).end()
        nextchar = s[end:end + 1]
    # Look-ahead for trivial empty array
    if nextchar == ']':
        return values, end + 1
    _append = values.append
    while True:
        try:
            value, end = scan_once(s, end)
        except StopIteration:
            raise ValueError(errmsg("Expecting object", s, end))
        _append(value)
        nextchar = s[end:end + 1]
        if nextchar in _ws:
            end = _w(s, end + 1).end()
            nextchar = s[end:end + 1]
        end += 1
        if nextchar == ']':
            break
        elif nextchar != ',':
            raise ValueError(errmsg("Expecting , delimiter", s, end))
        
        try:
            if s[end] in _ws:
                end += 1
                if s[end] in _ws:
                    end = _w(s, end + 1).end()
        except IndexError:
            pass

    return values, end

class JSONDecoder(object):
    """
    Simple JSON <http://json.org> decoder

    Performs the following translations in decoding by default:
    
    +---------------+-------------------+
    | JSON          | Python            |
    +===============+===================+
    | object        | dict              |
    +---------------+-------------------+
    | array         | list              |
    +---------------+-------------------+
    | string        | unicode           |
    +---------------+-------------------+
    | number (int)  | int, long         |
    +---------------+-------------------+
    | number (real) | float             |
    +---------------+-------------------+
    | true          | True              |
    +---------------+-------------------+
    | false         | False             |
    +---------------+-------------------+
    | null          | None              |
    +---------------+-------------------+

    It also understands ``NaN``, ``Infinity``, and ``-Infinity`` as
    their corresponding ``float`` values, which is outside the JSON spec.
    """

    __all__ = ['__init__', 'decode', 'raw_decode']

    def __init__(self, encoding=None, object_hook=None, parse_float=None,
            parse_int=None, parse_constant=None, strict=True):
        """
        ``encoding`` determines the encoding used to interpret any ``str``
        objects decoded by this instance (utf-8 by default).  It has no
        effect when decoding ``unicode`` objects.
        
        Note that currently only encodings that are a superset of ASCII work,
        strings of other encodings should be passed in as ``unicode``.

        ``object_hook``, if specified, will be called with the result
        of every JSON object decoded and its return value will be used in
        place of the given ``dict``.  This can be used to provide custom
        deserializations (e.g. to support JSON-RPC class hinting).

        ``parse_float``, if specified, will be called with the string
        of every JSON float to be decoded. By default this is equivalent to
        float(num_str). This can be used to use another datatype or parser
        for JSON floats (e.g. decimal.Decimal).

        ``parse_int``, if specified, will be called with the string
        of every JSON int to be decoded. By default this is equivalent to
        int(num_str). This can be used to use another datatype or parser
        for JSON integers (e.g. float).

        ``parse_constant``, if specified, will be called with one of the
        following strings: -Infinity, Infinity, NaN.
        This can be used to raise an exception if invalid JSON numbers
        are encountered.
        """
        self.encoding = encoding
        self.object_hook = object_hook
        self.parse_float = parse_float or float
        self.parse_int = parse_int or int
        self.parse_constant = parse_constant or _CONSTANTS.__getitem__
        self.strict = strict
        self.parse_object = JSONObject
        self.parse_array = JSONArray
        self.parse_string = scanstring
        self.scan_once = make_scanner(self)

    def decode(self, s, _w=WHITESPACE.match):
        """
        Return the Python representation of ``s`` (a ``str`` or ``unicode``
        instance containing a JSON document)
        """
        obj, end = self.raw_decode(s, idx=_w(s, 0).end())
        end = _w(s, end).end()
        if end != len(s):
            raise ValueError(errmsg("Extra data", s, end, len(s)))
        return obj

    def raw_decode(self, s, idx=0):
        """
        Decode a JSON document from ``s`` (a ``str`` or ``unicode`` beginning
        with a JSON document) and return a 2-tuple of the Python
        representation and the index in ``s`` where the document ended.

        This can be used to decode a JSON document from a string that may
        have extraneous data at the end.
        """
        try:
            obj, end = self.scan_once(s, idx)
        except StopIteration:
            raise ValueError("No JSON object could be decoded")
        return obj, end

__all__ = ['JSONDecoder']

########NEW FILE########
__FILENAME__ = encoder
"""
Implementation of JSONEncoder
"""
import re

try:
    from simplejson._speedups import encode_basestring_ascii as c_encode_basestring_ascii
except ImportError:
    c_encode_basestring_ascii = None
try:
    from simplejson._speedups import make_encoder as c_make_encoder
except ImportError:
    c_make_encoder = None

ESCAPE = re.compile(r'[\x00-\x1f\\"\b\f\n\r\t]')
ESCAPE_ASCII = re.compile(r'([\\"]|[^\ -~])')
HAS_UTF8 = re.compile(r'[\x80-\xff]')
ESCAPE_DCT = {
    '\\': '\\\\',
    '"': '\\"',
    '\b': '\\b',
    '\f': '\\f',
    '\n': '\\n',
    '\r': '\\r',
    '\t': '\\t',
}
for i in range(0x20):
    ESCAPE_DCT.setdefault(chr(i), '\\u%04x' % (i,))

# Assume this produces an infinity on all machines (probably not guaranteed)
INFINITY = float('1e66666')
FLOAT_REPR = repr

def encode_basestring(s):
    """
    Return a JSON representation of a Python string
    """
    def replace(match):
        return ESCAPE_DCT[match.group(0)]
    return '"' + ESCAPE.sub(replace, s) + '"'


def py_encode_basestring_ascii(s):
    if isinstance(s, str) and HAS_UTF8.search(s) is not None:
        s = s.decode('utf-8')
    def replace(match):
        s = match.group(0)
        try:
            return ESCAPE_DCT[s]
        except KeyError:
            n = ord(s)
            if n < 0x10000:
                return '\\u%04x' % (n,)
            else:
                # surrogate pair
                n -= 0x10000
                s1 = 0xd800 | ((n >> 10) & 0x3ff)
                s2 = 0xdc00 | (n & 0x3ff)
                return '\\u%04x\\u%04x' % (s1, s2)
    return '"' + str(ESCAPE_ASCII.sub(replace, s)) + '"'


encode_basestring_ascii = c_encode_basestring_ascii or py_encode_basestring_ascii

class JSONEncoder(object):
    """
    Extensible JSON <http://json.org> encoder for Python data structures.

    Supports the following objects and types by default:
    
    +-------------------+---------------+
    | Python            | JSON          |
    +===================+===============+
    | dict              | object        |
    +-------------------+---------------+
    | list, tuple       | array         |
    +-------------------+---------------+
    | str, unicode      | string        |
    +-------------------+---------------+
    | int, long, float  | number        |
    +-------------------+---------------+
    | True              | true          |
    +-------------------+---------------+
    | False             | false         |
    +-------------------+---------------+
    | None              | null          |
    +-------------------+---------------+

    To extend this to recognize other objects, subclass and implement a
    ``.default()`` method with another method that returns a serializable
    object for ``o`` if possible, otherwise it should call the superclass
    implementation (to raise ``TypeError``).
    """
    __all__ = ['__init__', 'default', 'encode', 'iterencode']
    item_separator = ', '
    key_separator = ': '
    def __init__(self, skipkeys=False, ensure_ascii=True,
            check_circular=True, allow_nan=True, sort_keys=False,
            indent=None, separators=None, encoding='utf-8', default=None):
        """
        Constructor for JSONEncoder, with sensible defaults.

        If skipkeys is False, then it is a TypeError to attempt
        encoding of keys that are not str, int, long, float or None.  If
        skipkeys is True, such items are simply skipped.

        If ensure_ascii is True, the output is guaranteed to be str
        objects with all incoming unicode characters escaped.  If
        ensure_ascii is false, the output will be unicode object.

        If check_circular is True, then lists, dicts, and custom encoded
        objects will be checked for circular references during encoding to
        prevent an infinite recursion (which would cause an OverflowError).
        Otherwise, no such check takes place.

        If allow_nan is True, then NaN, Infinity, and -Infinity will be
        encoded as such.  This behavior is not JSON specification compliant,
        but is consistent with most JavaScript based encoders and decoders.
        Otherwise, it will be a ValueError to encode such floats.

        If sort_keys is True, then the output of dictionaries will be
        sorted by key; this is useful for regression tests to ensure
        that JSON serializations can be compared on a day-to-day basis.

        If indent is a non-negative integer, then JSON array
        elements and object members will be pretty-printed with that
        indent level.  An indent level of 0 will only insert newlines.
        None is the most compact representation.

        If specified, separators should be a (item_separator, key_separator)
        tuple.  The default is (', ', ': ').  To get the most compact JSON
        representation you should specify (',', ':') to eliminate whitespace.

        If specified, default is a function that gets called for objects
        that can't otherwise be serialized.  It should return a JSON encodable
        version of the object or raise a ``TypeError``.

        If encoding is not None, then all input strings will be
        transformed into unicode using that encoding prior to JSON-encoding.
        The default is UTF-8.
        """

        self.skipkeys = skipkeys
        self.ensure_ascii = ensure_ascii
        self.check_circular = check_circular
        self.allow_nan = allow_nan
        self.sort_keys = sort_keys
        self.indent = indent
        if separators is not None:
            self.item_separator, self.key_separator = separators
        if default is not None:
            self.default = default
        self.encoding = encoding

    def default(self, o):
        """
        Implement this method in a subclass such that it returns
        a serializable object for ``o``, or calls the base implementation
        (to raise a ``TypeError``).

        For example, to support arbitrary iterators, you could
        implement default like this::
            
            def default(self, o):
                try:
                    iterable = iter(o)
                except TypeError:
                    pass
                else:
                    return list(iterable)
                return JSONEncoder.default(self, o)
        """
        raise TypeError("%r is not JSON serializable" % (o,))

    def encode(self, o):
        """
        Return a JSON string representation of a Python data structure.

        >>> JSONEncoder().encode({"foo": ["bar", "baz"]})
        '{"foo": ["bar", "baz"]}'
        """
        # This is for extremely simple cases and benchmarks.
        if isinstance(o, basestring):
            if isinstance(o, str):
                _encoding = self.encoding
                if (_encoding is not None 
                        and not (_encoding == 'utf-8')):
                    o = o.decode(_encoding)
            if self.ensure_ascii:
                return encode_basestring_ascii(o)
            else:
                return encode_basestring(o)
        # This doesn't pass the iterator directly to ''.join() because the
        # exceptions aren't as detailed.  The list call should be roughly
        # equivalent to the PySequence_Fast that ''.join() would do.
        chunks = self.iterencode(o, _one_shot=True)
        if not isinstance(chunks, (list, tuple)):
            chunks = list(chunks)
        return ''.join(chunks)

    def iterencode(self, o, _one_shot=False):
        """
        Encode the given object and yield each string
        representation as available.
        
        For example::
            
            for chunk in JSONEncoder().iterencode(bigobject):
                mysocket.write(chunk)
        """
        if self.check_circular:
            markers = {}
        else:
            markers = None
        if self.ensure_ascii:
            _encoder = encode_basestring_ascii
        else:
            _encoder = encode_basestring
        if self.encoding != 'utf-8':
            def _encoder(o, _orig_encoder=_encoder, _encoding=self.encoding):
                if isinstance(o, str):
                    o = o.decode(_encoding)
                return _orig_encoder(o)

        def floatstr(o, allow_nan=self.allow_nan, _repr=FLOAT_REPR, _inf=INFINITY, _neginf=-INFINITY):
            # Check for specials.  Note that this type of test is processor- and/or
            # platform-specific, so do tests which don't depend on the internals.

            if o != o:
                text = 'NaN'
            elif o == _inf:
                text = 'Infinity'
            elif o == _neginf:
                text = '-Infinity'
            else:
                return _repr(o)

            if not allow_nan:
                raise ValueError("Out of range float values are not JSON compliant: %r"
                    % (o,))

            return text
        
        
        if _one_shot and c_make_encoder is not None and not self.indent and not self.sort_keys:
            _iterencode = c_make_encoder(
                markers, self.default, _encoder, self.indent,
                self.key_separator, self.item_separator, self.sort_keys,
                self.skipkeys, self.allow_nan)
        else:
            _iterencode = _make_iterencode(
                markers, self.default, _encoder, self.indent, floatstr,
                self.key_separator, self.item_separator, self.sort_keys,
                self.skipkeys, _one_shot)
        return _iterencode(o, 0)

def _make_iterencode(markers, _default, _encoder, _indent, _floatstr, _key_separator, _item_separator, _sort_keys, _skipkeys, _one_shot,
        ## HACK: hand-optimized bytecode; turn globals into locals
        False=False,
        True=True,
        ValueError=ValueError,
        basestring=basestring,
        dict=dict,
        float=float,
        id=id,
        int=int,
        isinstance=isinstance,
        list=list,
        long=long,
        str=str,
        tuple=tuple,
    ):

    def _iterencode_list(lst, _current_indent_level):
        if not lst:
            yield '[]'
            return
        if markers is not None:
            markerid = id(lst)
            if markerid in markers:
                raise ValueError("Circular reference detected")
            markers[markerid] = lst
        buf = '['
        if _indent is not None:
            _current_indent_level += 1
            newline_indent = '\n' + (' ' * (_indent * _current_indent_level))
            separator = _item_separator + newline_indent
            buf += newline_indent
        else:
            newline_indent = None
            separator = _item_separator
        first = True
        for value in lst:
            if first:
                first = False
            else:
                buf = separator
            if isinstance(value, basestring):
                yield buf + _encoder(value)
            elif value is None:
                yield buf + 'null'
            elif value is True:
                yield buf + 'true'
            elif value is False:
                yield buf + 'false'
            elif isinstance(value, (int, long)):
                yield buf + str(value)
            elif isinstance(value, float):
                yield buf + _floatstr(value)
            else:
                yield buf
                if isinstance(value, (list, tuple)):
                    chunks = _iterencode_list(value, _current_indent_level)
                elif isinstance(value, dict):
                    chunks = _iterencode_dict(value, _current_indent_level)
                else:
                    chunks = _iterencode(value, _current_indent_level)
                for chunk in chunks:
                    yield chunk
        if newline_indent is not None:
            _current_indent_level -= 1
            yield '\n' + (' ' * (_indent * _current_indent_level))
        yield ']'
        if markers is not None:
            del markers[markerid]

    def _iterencode_dict(dct, _current_indent_level):
        if not dct:
            yield '{}'
            return
        if markers is not None:
            markerid = id(dct)
            if markerid in markers:
                raise ValueError("Circular reference detected")
            markers[markerid] = dct
        yield '{'
        if _indent is not None:
            _current_indent_level += 1
            newline_indent = '\n' + (' ' * (_indent * _current_indent_level))
            item_separator = _item_separator + newline_indent
            yield newline_indent
        else:
            newline_indent = None
            item_separator = _item_separator
        first = True
        if _sort_keys:
            items = dct.items()
            items.sort(key=lambda kv: kv[0])
        else:
            items = dct.iteritems()
        for key, value in items:
            if isinstance(key, basestring):
                pass
            # JavaScript is weakly typed for these, so it makes sense to
            # also allow them.  Many encoders seem to do something like this.
            elif isinstance(key, float):
                key = _floatstr(key)
            elif isinstance(key, (int, long)):
                key = str(key)
            elif key is True:
                key = 'true'
            elif key is False:
                key = 'false'
            elif key is None:
                key = 'null'
            elif _skipkeys:
                continue
            else:
                raise TypeError("key %r is not a string" % (key,))
            if first:
                first = False
            else:
                yield item_separator
            yield _encoder(key)
            yield _key_separator
            if isinstance(value, basestring):
                yield _encoder(value)
            elif value is None:
                yield 'null'
            elif value is True:
                yield 'true'
            elif value is False:
                yield 'false'
            elif isinstance(value, (int, long)):
                yield str(value)
            elif isinstance(value, float):
                yield _floatstr(value)
            else:
                if isinstance(value, (list, tuple)):
                    chunks = _iterencode_list(value, _current_indent_level)
                elif isinstance(value, dict):
                    chunks = _iterencode_dict(value, _current_indent_level)
                else:
                    chunks = _iterencode(value, _current_indent_level)
                for chunk in chunks:
                    yield chunk
        if newline_indent is not None:
            _current_indent_level -= 1
            yield '\n' + (' ' * (_indent * _current_indent_level))
        yield '}'
        if markers is not None:
            del markers[markerid]

    def _iterencode(o, _current_indent_level):
        if isinstance(o, basestring):
            yield _encoder(o)
        elif o is None:
            yield 'null'
        elif o is True:
            yield 'true'
        elif o is False:
            yield 'false'
        elif isinstance(o, (int, long)):
            yield str(o)
        elif isinstance(o, float):
            yield _floatstr(o)
        elif isinstance(o, (list, tuple)):
            for chunk in _iterencode_list(o, _current_indent_level):
                yield chunk
        elif isinstance(o, dict):
            for chunk in _iterencode_dict(o, _current_indent_level):
                yield chunk
        else:
            if markers is not None:
                markerid = id(o)
                if markerid in markers:
                    raise ValueError("Circular reference detected")
                markers[markerid] = o
            o = _default(o)
            for chunk in _iterencode(o, _current_indent_level):
                yield chunk
            if markers is not None:
                del markers[markerid]

    return _iterencode



__all__ = ['JSONEncoder']

########NEW FILE########
__FILENAME__ = scanner
"""
JSON token scanner
"""
import re
try:
    from simplejson._speedups import make_scanner as c_make_scanner
except ImportError:
    c_make_scanner = None

__all__ = ['make_scanner']

NUMBER_RE = re.compile(
    r'(-?(?:0|[1-9]\d*))(\.\d+)?([eE][-+]?\d+)?',
    (re.VERBOSE | re.MULTILINE | re.DOTALL))

def py_make_scanner(context):
    parse_object = context.parse_object
    parse_array = context.parse_array
    parse_string = context.parse_string
    match_number = NUMBER_RE.match
    encoding = context.encoding
    strict = context.strict
    parse_float = context.parse_float
    parse_int = context.parse_int
    parse_constant = context.parse_constant
    object_hook = context.object_hook

    def _scan_once(string, idx):
        try:
            nextchar = string[idx]
        except IndexError:
            raise StopIteration
        
        if nextchar == '"':
            return parse_string(string, idx + 1, encoding, strict)
        elif nextchar == '{':
            return parse_object((string, idx + 1), encoding, strict, _scan_once, object_hook)
        elif nextchar == '[':
            return parse_array((string, idx + 1), _scan_once)
        elif nextchar == 'n' and string[idx:idx + 4] == 'null':
            return None, idx + 4
        elif nextchar == 't' and string[idx:idx + 4] == 'true':
            return True, idx + 4
        elif nextchar == 'f' and string[idx:idx + 5] == 'false':
            return False, idx + 5
        
        m = match_number(string, idx)
        if m is not None:
            integer, frac, exp = m.groups()
            if frac or exp:
                res = parse_float(integer + (frac or '') + (exp or ''))
            else:
                res = parse_int(integer)
            return res, m.end()
        elif nextchar == 'N' and string[idx:idx + 3] == 'NaN':
            return parse_constant('NaN'), idx + 3
        elif nextchar == 'I' and string[idx:idx + 8] == 'Infinity':
            return parse_constant('Infinity'), idx + 8
        elif nextchar == '-' and string[idx:idx + 9] == '-Infinity':
            return parse_constant('-Infinity'), idx + 9
        else:
            raise StopIteration
    
    return _scan_once

make_scanner = c_make_scanner or py_make_scanner
########NEW FILE########
__FILENAME__ = test_decode
import decimal
from unittest import TestCase

import simplejson as S

class TestDecode(TestCase):
    def test_decimal(self):
        rval = S.loads('1.1', parse_float=decimal.Decimal)
        self.assert_(isinstance(rval, decimal.Decimal))
        self.assertEquals(rval, decimal.Decimal('1.1'))

    def test_float(self):
        rval = S.loads('1', parse_int=float)
        self.assert_(isinstance(rval, float))
        self.assertEquals(rval, 1.0)

    def test_decoder_optimizations(self):
        # Several optimizations were made that skip over calls to 
        # the whitespace regex, so this test is designed to try and
        # exercise the uncommon cases. The array cases are already covered.
        rval = S.loads('{   "key"    :    "value"    ,  "k":"v"    }')
        self.assertEquals(rval, {"key":"value", "k":"v"})

########NEW FILE########
__FILENAME__ = test_default
from unittest import TestCase

import simplejson as S

class TestDefault(TestCase):
    def test_default(self):
        self.assertEquals(
            S.dumps(type, default=repr),
            S.dumps(repr(type)))

########NEW FILE########
__FILENAME__ = test_dump
from unittest import TestCase
from cStringIO import StringIO

import simplejson as S

class TestDump(TestCase):
    def test_dump(self):
        sio = StringIO()
        S.dump({}, sio)
        self.assertEquals(sio.getvalue(), '{}')
    
    def test_dumps(self):
        self.assertEquals(S.dumps({}), '{}')

########NEW FILE########
__FILENAME__ = test_encode_basestring_ascii
from unittest import TestCase

import simplejson.encoder

CASES = [
    (u'/\\"\ucafe\ubabe\uab98\ufcde\ubcda\uef4a\x08\x0c\n\r\t`1~!@#$%^&*()_+-=[]{}|;:\',./<>?', '"/\\\\\\"\\ucafe\\ubabe\\uab98\\ufcde\\ubcda\\uef4a\\b\\f\\n\\r\\t`1~!@#$%^&*()_+-=[]{}|;:\',./<>?"'),
    (u'\u0123\u4567\u89ab\ucdef\uabcd\uef4a', '"\\u0123\\u4567\\u89ab\\ucdef\\uabcd\\uef4a"'),
    (u'controls', '"controls"'),
    (u'\x08\x0c\n\r\t', '"\\b\\f\\n\\r\\t"'),
    (u'{"object with 1 member":["array with 1 element"]}', '"{\\"object with 1 member\\":[\\"array with 1 element\\"]}"'),
    (u' s p a c e d ', '" s p a c e d "'),
    (u'\U0001d120', '"\\ud834\\udd20"'),
    (u'\u03b1\u03a9', '"\\u03b1\\u03a9"'),
    ('\xce\xb1\xce\xa9', '"\\u03b1\\u03a9"'),
    (u'\u03b1\u03a9', '"\\u03b1\\u03a9"'),
    ('\xce\xb1\xce\xa9', '"\\u03b1\\u03a9"'),
    (u'\u03b1\u03a9', '"\\u03b1\\u03a9"'),
    (u'\u03b1\u03a9', '"\\u03b1\\u03a9"'),
    (u"`1~!@#$%^&*()_+-={':[,]}|;.</>?", '"`1~!@#$%^&*()_+-={\':[,]}|;.</>?"'),
    (u'\x08\x0c\n\r\t', '"\\b\\f\\n\\r\\t"'),
    (u'\u0123\u4567\u89ab\ucdef\uabcd\uef4a', '"\\u0123\\u4567\\u89ab\\ucdef\\uabcd\\uef4a"'),
]

class TestEncodeBaseStringAscii(TestCase):
    def test_py_encode_basestring_ascii(self):
        self._test_encode_basestring_ascii(simplejson.encoder.py_encode_basestring_ascii)

    def test_c_encode_basestring_ascii(self):
        if not simplejson.encoder.c_encode_basestring_ascii:
            return
        self._test_encode_basestring_ascii(simplejson.encoder.c_encode_basestring_ascii)

    def _test_encode_basestring_ascii(self, encode_basestring_ascii):
        fname = encode_basestring_ascii.__name__
        for input_string, expect in CASES:
            result = encode_basestring_ascii(input_string)
            self.assertEquals(result, expect,
                '%r != %r for %s(%r)' % (result, expect, fname, input_string))

########NEW FILE########
__FILENAME__ = test_fail
from unittest import TestCase

import simplejson as S

# Fri Dec 30 18:57:26 2005
JSONDOCS = [
    # http://json.org/JSON_checker/test/fail1.json
    '"A JSON payload should be an object or array, not a string."',
    # http://json.org/JSON_checker/test/fail2.json
    '["Unclosed array"',
    # http://json.org/JSON_checker/test/fail3.json
    '{unquoted_key: "keys must be quoted}',
    # http://json.org/JSON_checker/test/fail4.json
    '["extra comma",]',
    # http://json.org/JSON_checker/test/fail5.json
    '["double extra comma",,]',
    # http://json.org/JSON_checker/test/fail6.json
    '[   , "<-- missing value"]',
    # http://json.org/JSON_checker/test/fail7.json
    '["Comma after the close"],',
    # http://json.org/JSON_checker/test/fail8.json
    '["Extra close"]]',
    # http://json.org/JSON_checker/test/fail9.json
    '{"Extra comma": true,}',
    # http://json.org/JSON_checker/test/fail10.json
    '{"Extra value after close": true} "misplaced quoted value"',
    # http://json.org/JSON_checker/test/fail11.json
    '{"Illegal expression": 1 + 2}',
    # http://json.org/JSON_checker/test/fail12.json
    '{"Illegal invocation": alert()}',
    # http://json.org/JSON_checker/test/fail13.json
    '{"Numbers cannot have leading zeroes": 013}',
    # http://json.org/JSON_checker/test/fail14.json
    '{"Numbers cannot be hex": 0x14}',
    # http://json.org/JSON_checker/test/fail15.json
    '["Illegal backslash escape: \\x15"]',
    # http://json.org/JSON_checker/test/fail16.json
    '["Illegal backslash escape: \\\'"]',
    # http://json.org/JSON_checker/test/fail17.json
    '["Illegal backslash escape: \\017"]',
    # http://json.org/JSON_checker/test/fail18.json
    '[[[[[[[[[[[[[[[[[[[["Too deep"]]]]]]]]]]]]]]]]]]]]',
    # http://json.org/JSON_checker/test/fail19.json
    '{"Missing colon" null}',
    # http://json.org/JSON_checker/test/fail20.json
    '{"Double colon":: null}',
    # http://json.org/JSON_checker/test/fail21.json
    '{"Comma instead of colon", null}',
    # http://json.org/JSON_checker/test/fail22.json
    '["Colon instead of comma": false]',
    # http://json.org/JSON_checker/test/fail23.json
    '["Bad value", truth]',
    # http://json.org/JSON_checker/test/fail24.json
    "['single quote']",
    # http://code.google.com/p/simplejson/issues/detail?id=3
    u'["A\u001FZ control characters in string"]',
]

SKIPS = {
    1: "why not have a string payload?",
    18: "spec doesn't specify any nesting limitations",
}

class TestFail(TestCase):
    def test_failures(self):
        for idx, doc in enumerate(JSONDOCS):
            idx = idx + 1
            if idx in SKIPS:
                S.loads(doc)
                continue
            try:
                S.loads(doc)
            except ValueError:
                pass
            else:
                self.fail("Expected failure for fail%d.json: %r" % (idx, doc))

########NEW FILE########
__FILENAME__ = test_float
import math
from unittest import TestCase

import simplejson as S

class TestFloat(TestCase):
    def test_floats(self):
        for num in [1617161771.7650001, math.pi, math.pi**100, math.pi**-100]:
            self.assertEquals(float(S.dumps(num)), num)

    def test_ints(self):
        for num in [1, 1L, 1<<32, 1<<64]:
            self.assertEquals(S.dumps(num), str(num))
            self.assertEquals(int(S.dumps(num)), num)

########NEW FILE########
__FILENAME__ = test_indent
from unittest import TestCase

import simplejson as S
import textwrap

class TestIndent(TestCase):
    def test_indent(self):
        h = [['blorpie'], ['whoops'], [], 'd-shtaeou', 'd-nthiouh', 'i-vhbjkhnth',
             {'nifty': 87}, {'field': 'yes', 'morefield': False} ]

        expect = textwrap.dedent("""\
        [
          [
            "blorpie"
          ],
          [
            "whoops"
          ],
          [],
          "d-shtaeou",
          "d-nthiouh",
          "i-vhbjkhnth",
          {
            "nifty": 87
          },
          {
            "field": "yes",
            "morefield": false
          }
        ]""")


        d1 = S.dumps(h)
        d2 = S.dumps(h, indent=2, sort_keys=True, separators=(',', ': '))

        h1 = S.loads(d1)
        h2 = S.loads(d2)

        self.assertEquals(h1, h)
        self.assertEquals(h2, h)
        self.assertEquals(d2, expect)

########NEW FILE########
__FILENAME__ = test_pass1
from unittest import TestCase

import simplejson as S

# from http://json.org/JSON_checker/test/pass1.json
JSON = r'''
[
    "JSON Test Pattern pass1",
    {"object with 1 member":["array with 1 element"]},
    {},
    [],
    -42,
    true,
    false,
    null,
    {
        "integer": 1234567890,
        "real": -9876.543210,
        "e": 0.123456789e-12,
        "E": 1.234567890E+34,
        "":  23456789012E666,
        "zero": 0,
        "one": 1,
        "space": " ",
        "quote": "\"",
        "backslash": "\\",
        "controls": "\b\f\n\r\t",
        "slash": "/ & \/",
        "alpha": "abcdefghijklmnopqrstuvwyz",
        "ALPHA": "ABCDEFGHIJKLMNOPQRSTUVWYZ",
        "digit": "0123456789",
        "special": "`1~!@#$%^&*()_+-={':[,]}|;.</>?",
        "hex": "\u0123\u4567\u89AB\uCDEF\uabcd\uef4A",
        "true": true,
        "false": false,
        "null": null,
        "array":[  ],
        "object":{  },
        "address": "50 St. James Street",
        "url": "http://www.JSON.org/",
        "comment": "// /* <!-- --",
        "# -- --> */": " ",
        " s p a c e d " :[1,2 , 3

,

4 , 5        ,          6           ,7        ],
        "compact": [1,2,3,4,5,6,7],
        "jsontext": "{\"object with 1 member\":[\"array with 1 element\"]}",
        "quotes": "&#34; \u0022 %22 0x22 034 &#x22;",
        "\/\\\"\uCAFE\uBABE\uAB98\uFCDE\ubcda\uef4A\b\f\n\r\t`1~!@#$%^&*()_+-=[]{}|;:',./<>?"
: "A key can be any string"
    },
    0.5 ,98.6
,
99.44
,

1066


,"rosebud"]
'''

class TestPass1(TestCase):
    def test_parse(self):
        # test in/out equivalence and parsing
        res = S.loads(JSON)
        out = S.dumps(res)
        self.assertEquals(res, S.loads(out))
        try:
            S.dumps(res, allow_nan=False)
        except ValueError:
            pass
        else:
            self.fail("23456789012E666 should be out of range")

########NEW FILE########
__FILENAME__ = test_pass2
from unittest import TestCase
import simplejson as S

# from http://json.org/JSON_checker/test/pass2.json
JSON = r'''
[[[[[[[[[[[[[[[[[[["Not too deep"]]]]]]]]]]]]]]]]]]]
'''

class TestPass2(TestCase):
    def test_parse(self):
        # test in/out equivalence and parsing
        res = S.loads(JSON)
        out = S.dumps(res)
        self.assertEquals(res, S.loads(out))

########NEW FILE########
__FILENAME__ = test_pass3
from unittest import TestCase

import simplejson as S

# from http://json.org/JSON_checker/test/pass3.json
JSON = r'''
{
    "JSON Test Pattern pass3": {
        "The outermost value": "must be an object or array.",
        "In this test": "It is an object."
    }
}
'''

class TestPass3(TestCase):
    def test_parse(self):
        # test in/out equivalence and parsing
        res = S.loads(JSON)
        out = S.dumps(res)
        self.assertEquals(res, S.loads(out))

########NEW FILE########
__FILENAME__ = test_recursion
from unittest import TestCase

import simplejson as S

class JSONTestObject:
    pass

class RecursiveJSONEncoder(S.JSONEncoder):
    recurse = False
    def default(self, o):
        if o is JSONTestObject:
            if self.recurse:
                return [JSONTestObject]
            else:
                return 'JSONTestObject'
        return S.JSONEncoder.default(o)

class TestRecursion(TestCase):
    def test_listrecursion(self):
        x = []
        x.append(x)
        try:
            S.dumps(x)
        except ValueError:
            pass
        else:
            self.fail("didn't raise ValueError on list recursion")
        x = []
        y = [x]
        x.append(y)
        try:
            S.dumps(x)
        except ValueError:
            pass
        else:
            self.fail("didn't raise ValueError on alternating list recursion")
        y = []
        x = [y, y]
        # ensure that the marker is cleared
        S.dumps(x)

    def test_dictrecursion(self):
        x = {}
        x["test"] = x
        try:
            S.dumps(x)
        except ValueError:
            pass
        else:
            self.fail("didn't raise ValueError on dict recursion")
        x = {}
        y = {"a": x, "b": x}
        # ensure that the marker is cleared
        S.dumps(x)

    def test_defaultrecursion(self):
        enc = RecursiveJSONEncoder()
        self.assertEquals(enc.encode(JSONTestObject), '"JSONTestObject"')
        enc.recurse = True
        try:
            enc.encode(JSONTestObject)
        except ValueError:
            pass
        else:
            self.fail("didn't raise ValueError on default recursion")

########NEW FILE########
__FILENAME__ = test_scanstring
import sys
import decimal
from unittest import TestCase

import simplejson.decoder

class TestScanString(TestCase):
    def test_py_scanstring(self):
        self._test_scanstring(simplejson.decoder.py_scanstring)

    def test_c_scanstring(self):
        if not simplejson.decoder.c_scanstring:
            return
        self._test_scanstring(simplejson.decoder.c_scanstring)

    def _test_scanstring(self, scanstring):
        self.assertEquals(
            scanstring('"z\\ud834\\udd20x"', 1, None, True),
            (u'z\U0001d120x', 16))

        if sys.maxunicode == 65535:
            self.assertEquals(
                scanstring(u'"z\U0001d120x"', 1, None, True),
                (u'z\U0001d120x', 6))
        else:
            self.assertEquals(
                scanstring(u'"z\U0001d120x"', 1, None, True),
                (u'z\U0001d120x', 5))

        self.assertEquals(
            scanstring('"\\u007b"', 1, None, True),
            (u'{', 8))

        self.assertEquals(
            scanstring('"A JSON payload should be an object or array, not a string."', 1, None, True),
            (u'A JSON payload should be an object or array, not a string.', 60))
        
        self.assertEquals(
            scanstring('["Unclosed array"', 2, None, True),
            (u'Unclosed array', 17))
        
        self.assertEquals(
            scanstring('["extra comma",]', 2, None, True),
            (u'extra comma', 14))
        
        self.assertEquals(
            scanstring('["double extra comma",,]', 2, None, True),
            (u'double extra comma', 21))
        
        self.assertEquals(
            scanstring('["Comma after the close"],', 2, None, True),
            (u'Comma after the close', 24))
        
        self.assertEquals(
            scanstring('["Extra close"]]', 2, None, True),
            (u'Extra close', 14))
        
        self.assertEquals(
            scanstring('{"Extra comma": true,}', 2, None, True),
            (u'Extra comma', 14))
        
        self.assertEquals(
            scanstring('{"Extra value after close": true} "misplaced quoted value"', 2, None, True),
            (u'Extra value after close', 26))
        
        self.assertEquals(
            scanstring('{"Illegal expression": 1 + 2}', 2, None, True),
            (u'Illegal expression', 21))
        
        self.assertEquals(
            scanstring('{"Illegal invocation": alert()}', 2, None, True),
            (u'Illegal invocation', 21))
        
        self.assertEquals(
            scanstring('{"Numbers cannot have leading zeroes": 013}', 2, None, True),
            (u'Numbers cannot have leading zeroes', 37))
        
        self.assertEquals(
            scanstring('{"Numbers cannot be hex": 0x14}', 2, None, True),
            (u'Numbers cannot be hex', 24))
        
        self.assertEquals(
            scanstring('[[[[[[[[[[[[[[[[[[[["Too deep"]]]]]]]]]]]]]]]]]]]]', 21, None, True),
            (u'Too deep', 30))
        
        self.assertEquals(
            scanstring('{"Missing colon" null}', 2, None, True),
            (u'Missing colon', 16))
        
        self.assertEquals(
            scanstring('{"Double colon":: null}', 2, None, True),
            (u'Double colon', 15))
        
        self.assertEquals(
            scanstring('{"Comma instead of colon", null}', 2, None, True),
            (u'Comma instead of colon', 25))
        
        self.assertEquals(
            scanstring('["Colon instead of comma": false]', 2, None, True),
            (u'Colon instead of comma', 25))
        
        self.assertEquals(
            scanstring('["Bad value", truth]', 2, None, True),
            (u'Bad value', 12))

########NEW FILE########
__FILENAME__ = test_separators
import textwrap
from unittest import TestCase

import simplejson as S


class TestSeparators(TestCase):
    def test_separators(self):
        h = [['blorpie'], ['whoops'], [], 'd-shtaeou', 'd-nthiouh', 'i-vhbjkhnth',
             {'nifty': 87}, {'field': 'yes', 'morefield': False} ]

        expect = textwrap.dedent("""\
        [
          [
            "blorpie"
          ] ,
          [
            "whoops"
          ] ,
          [] ,
          "d-shtaeou" ,
          "d-nthiouh" ,
          "i-vhbjkhnth" ,
          {
            "nifty" : 87
          } ,
          {
            "field" : "yes" ,
            "morefield" : false
          }
        ]""")


        d1 = S.dumps(h)
        d2 = S.dumps(h, indent=2, sort_keys=True, separators=(' ,', ' : '))

        h1 = S.loads(d1)
        h2 = S.loads(d2)

        self.assertEquals(h1, h)
        self.assertEquals(h2, h)
        self.assertEquals(d2, expect)

########NEW FILE########
__FILENAME__ = test_unicode
from unittest import TestCase

import simplejson as S

class TestUnicode(TestCase):
    def test_encoding1(self):
        encoder = S.JSONEncoder(encoding='utf-8')
        u = u'\N{GREEK SMALL LETTER ALPHA}\N{GREEK CAPITAL LETTER OMEGA}'
        s = u.encode('utf-8')
        ju = encoder.encode(u)
        js = encoder.encode(s)
        self.assertEquals(ju, js)
    
    def test_encoding2(self):
        u = u'\N{GREEK SMALL LETTER ALPHA}\N{GREEK CAPITAL LETTER OMEGA}'
        s = u.encode('utf-8')
        ju = S.dumps(u, encoding='utf-8')
        js = S.dumps(s, encoding='utf-8')
        self.assertEquals(ju, js)

    def test_encoding3(self):
        u = u'\N{GREEK SMALL LETTER ALPHA}\N{GREEK CAPITAL LETTER OMEGA}'
        j = S.dumps(u)
        self.assertEquals(j, '"\\u03b1\\u03a9"')

    def test_encoding4(self):
        u = u'\N{GREEK SMALL LETTER ALPHA}\N{GREEK CAPITAL LETTER OMEGA}'
        j = S.dumps([u])
        self.assertEquals(j, '["\\u03b1\\u03a9"]')

    def test_encoding5(self):
        u = u'\N{GREEK SMALL LETTER ALPHA}\N{GREEK CAPITAL LETTER OMEGA}'
        j = S.dumps(u, ensure_ascii=False)
        self.assertEquals(j, u'"%s"' % (u,))

    def test_encoding6(self):
        u = u'\N{GREEK SMALL LETTER ALPHA}\N{GREEK CAPITAL LETTER OMEGA}'
        j = S.dumps([u], ensure_ascii=False)
        self.assertEquals(j, u'["%s"]' % (u,))

    def test_big_unicode_encode(self):
        u = u'\U0001d120'
        self.assertEquals(S.dumps(u), '"\\ud834\\udd20"')
        self.assertEquals(S.dumps(u, ensure_ascii=False), u'"\U0001d120"')

    def test_big_unicode_decode(self):
        u = u'z\U0001d120x'
        self.assertEquals(S.loads('"' + u + '"'), u)
        self.assertEquals(S.loads('"z\\ud834\\udd20x"'), u)

    def test_unicode_decode(self):
        for i in range(0, 0xd7ff):
            u = unichr(i)
            json = '"\\u%04x"' % (i,)
            self.assertEquals(S.loads(json), u)
    
    def test_default_encoding(self):
        self.assertEquals(S.loads(u'{"a": "\xe9"}'.encode('utf-8')),
            {'a': u'\xe9'})

########NEW FILE########
__FILENAME__ = tool
r"""
Using simplejson from the shell to validate and
pretty-print::
    
    $ echo '{"json":"obj"}' | python -msimplejson
    {
        "json": "obj"
    }
    $ echo '{ 1.2:3.4}' | python -msimplejson
    Expecting property name: line 1 column 2 (char 2)

Note that the JSON produced by this module's default settings
is a subset of YAML, so it may be used as a serializer for that as well.
"""
import simplejson

#
# Pretty printer:
#     curl http://mochikit.com/examples/ajax_tables/domains.json | python -msimplejson.tool
#

def main():
    import sys
    if len(sys.argv) == 1:
        infile = sys.stdin
        outfile = sys.stdout
    elif len(sys.argv) == 2:
        infile = open(sys.argv[1], 'rb')
        outfile = sys.stdout
    elif len(sys.argv) == 3:
        infile = open(sys.argv[1], 'rb')
        outfile = open(sys.argv[2], 'wb')
    else:
        raise SystemExit("%s [infile [outfile]]" % (sys.argv[0],))
    try:
        obj = simplejson.load(infile)
    except ValueError, e:
        raise SystemExit(e)
    simplejson.dump(obj, outfile, sort_keys=True, indent=4)
    outfile.write('\n')


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = smartersql
"""
smartersql: A smarter interface to SQL.
"""
import web

## lazy loading

def lazylookup(obj, column_name):
    def inner(obj2):
        # obj is obj2
        
        # If we're being called, that means someone's trying to access
        # a lazy reference on obj.

        column = obj.columns[column_name]
        objs = obj._objs

        # First, we need to get the answer for all of us:
        if isinstance(column, Backreference):
            column.target = column._target()
            local_column = column.local_column.sql_name
            target_column = column.target_column
            order = column.order
            plural = column.plural

        else:
            local_column = column.sql_name
            target_column = column.target_column.sql_name
            order = None
            plural = False
            
        newobjs = {}
        for k in column.target.select(
          order=order,
          where=web.sqlors(target_column + ' = ', 
          [getattr(x, local_column) for x in objs])):
            val = getattr(k, target_column)
            if plural:
                newobjs.setdefault(val, []).append(k)
            else:
                newobjs[val] = k
    
        # Then we need to add it to all of us:
        for xobj in objs:
            k = getattr(xobj, local_column)
            if k in newobjs:
                setattr(xobj.__class__, column_name, newobjs[k])
    
        # Finally, we need to return it:
        return newobjs.get(getattr(obj, local_column), [])
    return inner
    
## table generation

_all_tables = []

class metatracker(type):
    def __init__(self, name, bases, *a, **kw):
        type.__init__(self, name, bases, *a, **kw)
        if bases[0] != object and not hasattr(self, 'columns'):
            _all_tables.append(self)
            self.columns = self._analyze(init=True)
            self.primary = self._primary(self.columns)
            self.sql_name = self._sql_name_()
            

class Table(object):
    __metaclass__ = metatracker

    @classmethod
    def _sql_name_(cls):
        return cls.__name__.lower()
    
    @classmethod
    def _analyze(cls, init=False):
        columns = web.Storage()
        
        for k in dir(cls):
            if isinstance(getattr(cls, k), Column):
                v = getattr(cls, k)
                v.sql_name = v._sql_name_(k)
                if not hasattr(v, 'label'):
                    v.label = k.replace('_', ' ')
                if init and hasattr(v, '_delayed_init'):
                    v._delayed_init(cls)
                columns[k] = v
        return columns
    
    @staticmethod
    def _primary(columns):
        primary = web.Storage()
        for k, v in columns.iteritems():
            if v.primary:
                primary[k] = v
        return primary
    
    @classmethod
    def _createSQL(cls):        
        x = 'CREATE TABLE %s (\n' % cls.sql_name
        for k, v in cls.columns.iteritems():
            if not v.sql_type: continue # not for sql
            
            x += '  %s %s' % (v.sql_name, v.sql_type)
            if v.unique: x += ' UNIQUE'
            if v.notnull: x += ' NOT NULL'
            if v.default: x += ' DEFAULT %s' % x.default
            x += ',\n'
        if cls.primary:
            x += '  PRIMARY KEY (%s)\n' % ', '.join(v.sql_name for v in cls.primary.itervalues())
        else:
            x = x[:-2] + '\n' # remove last comma
            
        x += ')'
        return x
    
    @classmethod
    def _dropSQL(cls, cascade=False):
        cascade = " CASCADE" if cascade else ""
        return 'DROP TABLE IF EXISTS %s%s' % (cls.sql_name, cascade)
    
    @classmethod
    def create(cls): cls.db.query(cls._createSQL())
    @classmethod
    def drop(cls, cascade=False): cls.db.query(cls._dropSQL(cascade))
    @classmethod
    def insert(cls, *a, **kw):
        #@@ deal with seqname
        #@@ convert objects to the proper identifiers
        cls.db.insert(cls.sql_name, *a, **kw)
    @classmethod
    def select(cls, what='*', **kw):
        rows = cls.db.select(cls.sql_name, what=what, **kw)
        objs = [cls(x) for x in rows]
        for o in objs:
            #@@ make weakref?
            o._objs = objs # so lazy references can fill in their neighbors
        return objs
        
    @classmethod
    def where(cls, **clauses):
        out = ""
        for k, v in clauses.items():
            out += k + '=' + web.sqlquote(v)
        
        return cls.select(where=out)
    
    def __init__(self, row, ids=None):
        if ids: self._ids = ids
        
        # divorce ourself from the parent class, so we can edit it
        c = self.__class__
        self.__class__ = type(c.__name__, c.__bases__, dict(c.__dict__))
        
        for k, v in self.columns.iteritems():
            if isinstance(v, Reference):
                setattr(self.__class__, k, property(lazylookup(self, k)))
            
            if v.sql_type:
                setattr(self, v.sql_name, row[v.sql_name])


## columns

class Column(object):
    def __init__(self, **kw):
        for k in kw:
            setattr(self, k, kw[k])
    
    _sql_name_ = lambda self, k: k

    default = None
    primary = False
    unique = False
    notnull = False

    display = lambda self, x: unicode(x)
    toxml = lambda self, obj: '>' + web.htmlquote(unicode(obj))

class Reference (Column):
    def __init__(self, target, **kw):
        super(Reference, self).__init__(**kw)
        assert len(target.primary) == 1, \
          "Referenced column must have exactly 1 primary key."

        self.target = target
        self.target_column = target.primary.values()[0]
        
        self.sql_type = self.target_column.sql_type + ' REFERENCES ' + target.sql_name
        self._sql_name_ = lambda k: k + '_id'
    
    def toxml(self, obj):
        if hasattr(obj, '_uri_'):
            return ' rdf:resource="%s">' % web.websafe(obj._uri_)
        else:
            return None
    def ton3(self, obj, indent):
        if hasattr(obj, '_uri_'):
            return '<%s>' % obj._uri_
        else:
            return None

class Backreference (Reference):
    def __init__(self, target, target_column, plural=True, order=None):
        self._targetk = target
        self.target_column = target_column + '_id'
        self.plural = plural
        self.order = order
        self.sql_type = None
    
    def _delayed_init(self, cls):
        primary = cls._primary(cls._analyze())
        assert len(primary) == 1, \
          "Backreferences with composite primary keys isn't supported."
        self.local_column = primary.values()[0]
    
    def _target(self):
        return [x for x in _all_tables if x.__name__ == self._targetk][0]
    
        
class String (Column):
    sql_type = 'text'
    def __init__(self, length=None, **kw):
        super(String, self).__init__(**kw)
        if length:
            self.sql_type = 'varchar(%s)' % length

class Boolean(Column):
    sql_type = 'bool'
    display = lambda self, x: {True: 'Yes', False: 'No', None: 'Unknown'}
    toxml = lambda self, obj: ' rdf:datatype="http://www.w3.org/2001/XMLSchema#boolean">%s' % web.websafe(obj).lower()

class Integer(Column):
    sql_type = 'int'
    toxml = lambda self, obj: ' rdf:datatype="http://www.w3.org/2001/XMLSchema#integer">%s' % web.websafe(obj)
    
class Float(Column):
    sql_type = 'real'
    toxml = lambda self, obj: ' rdf:datatype="http://www.w3.org/2001/XMLSchema#double">%s' % web.websafe(obj)
    

class Serial(Integer): sql_type = 'serial'
class Int2(Integer): sql_type = 'int2'
class BigInteger(Integer): sql_type = 'bigint'
class Date(Column): sql_type = 'date'

class Year(Integer): pass

class Number(Integer):
    display = lambda self, x: web.commify(x)

class Dollars(Integer):
    display = lambda self, x: '$' + web.commify(x)

class Percentage(Float):
    precision = 3
    display = lambda self, x: str(x*100)[:self.precision + 1] + '%' 
    # add one for the decimal point

class URL(String):
    toxml = lambda self, obj: ' rdf:resource="%s">' % web.websafe(obj)
    ton3 = lambda self, obj, indent: '<%s>' % obj

## module functions

def create():
    for table in _all_tables:
        table.create()

def drop():
    x = list(_all_tables)
    x.reverse()
    for table in x:
        table.drop(cascade=True)

def recreate():
    drop()
    create()

def grantall(username):
    for table in _all_tables:
        try:
            table.db.query('GRANT ALL ON %s TO %s' % (table.sql_name, username))
        except:
            pass

########NEW FILE########
__FILENAME__ = pwt
import web
import simplejson, sudo
urls = (
    '/sudo', 'sudoku',
    '/length', 'length',
)


class pwt(object):
    _inFunc = False
    updated = {}
    page = """
<script src="/static/prototype.js"></script>
<script src="/static/behaviour.js"></script>
<script>
Behaviour.register({'input': function (e) { 
    e.onmouseup = e.onkeyup = e.onchange = function () { send(e) }
}})
</script>

<form name="main" onsubmit="return false;">%s</form>

<script>
function send(e) {
    ajax =  new Ajax.Request(document.location, {method:'post', parameters: 
      Form.serialize(document.forms.main)
    });
}

function receive(d) {
    $H(d).keys().each(function (key) {
        v = d[key];
        k = document.forms.main[key];

        if (k) k.value = v;
        else $(key).innerHTML = v;
    })
}
</script>
"""

    def GET(self):
        web.header('Content-Type', 'text/html')
        print self.page % self.form()
    
    def POST(self):
        i = web.input()
        if '_' in i: del i['_']
        #for k, v in i.iteritems(): setattr(self, k, v)
        
        self._inFunc = True
        self.work(**i)
        self._inFunc = False
        
        web.header('Content-Type', 'text/javascript')
        print 'receive('+simplejson.dumps(self.updated)+');'
    
    def __setattr__(self, k, v):
        if self._inFunc and k != '_inFunc':
            self.updated[k] = v
        object.__setattr__(self, k, v)

class sudoku(pwt):
    def form(self):
        import sudo
        out = ''
        n = 0
        for i in range(9):
            for j in range(9):
                out += '<input type="text" size="1" name="%s" />' % (sudo.squares[n])
                n += 1
            out += '<br />'

        return out
    
    def work(self, **kw):
        values = dict((s, sudo.digits) for s in sudo.squares)
        for k, v in kw.iteritems():
            if v:
                sudo.assign(values, k, v)

        for k, v in values.iteritems():
            if len(v) == 1:
                setattr(self, k, v)

        return values

class length(pwt):
    def form(self):
        return '<p id="output">&nbsp;</p><input type="range" name="n" value="0" />'
    
    def work(self):
        self.output = ('a' * web.intget(self.n, 0) or '&nbsp;')

if __name__ == "__main__":
    web.run(urls, globals(), web.reloader)
########NEW FILE########
__FILENAME__ = untwisted
import random

from twisted.internet import reactor, defer
from twisted.web import http

import simplejson

import web

class Request(http.Request):
    def process(self):
        self.content.seek(0, 0)
        env = {
          'REMOTE_ADDR': self.client.host,
          'REQUEST_METHOD': self.method,
          'PATH_INFO': self.path,
          'CONTENT_LENGTH': web.intget(self.getHeader('content-length'), 0),
          'wsgi.input': self.content
        }
        if '?' in self.uri:
            env['QUERY_STRING'] = self.uri.split('?', 1)[1]

        for k, v in self.received_headers.iteritems():
            env['HTTP_' + k.upper()] = v
        
        if self.path.startswith('/static/'):
            f = web.lstrips(self.path, '/static/')
            assert '/' not in f
            #@@@ big security hole
            self.write(file('static/' + f).read())
            return self.finish()

        web.webapi._load(env)
        web.ctx.trequest = self
        result = self.actualfunc()
        self.setResponseCode(int(web.ctx.status.split()[0]))
        for (h, v) in web.ctx.headers:
            self.setHeader(h, v)
        self.write(web.ctx.output)
        if not web.ctx.get('persist'):
            self.finish()

class Server(http.HTTPFactory):
    def __init__(self, func):
        self.func = func

    def buildProtocol(self, addr):
        """Generate a channel attached to this site.
        """
        channel = http.HTTPFactory.buildProtocol(self, addr)
        class MyRequest(Request):
            actualfunc = staticmethod(self.func)
        channel.requestFactory = MyRequest
        channel.site = self
        return channel

def runtwisted(func):
    reactor.listenTCP(8086, Server(func))
    reactor.run()

def newrun(inp, fvars):
    print "Running on http://0.0.0.0:8086/"
    runtwisted(web.webpyfunc(inp, fvars, False))

def iframe(url):
    return """
    <iframe height="0" width="0" style="display: none" src="%s"/></iframe>
    """ % url #("http://%s.ajaxpush.lh.theinfo.org:8086%s" % (random.random(), url))

class Feed:
    def __init__(self):
        self.sessions = []
    
    def subscribe(self):
        request = web.ctx.trequest
        self.sessions.append(request)
        request.connectionLost = lambda reason: self.sessions.remove(request)
        web.ctx.persist = True
    
    def publish(self, text):
        for x in self.sessions:
            x.write(text)

class JSFeed(Feed):
    def __init__(self, callback="callback"):
        Feed.__init__(self)
        self.callback = callback
        
    def publish(self, obj):
        web.debug("publishing")
        Feed.publish(self, 
          '<script type="text/javascript">window.parent.%s(%s)</script>' % (self.callback, simplejson.dumps(obj) + 
          " " * 2048))

if __name__ == "__main__":
    mfeed = JSFeed()

    urls = (
      '/', 'view',
      '/js', 'js',
      '/send', 'send'
    )

    class view:
        def GET(self):
            print """
<script type="text/javascript">
function callback(item) {
  document.getElementById('content').innerHTML += "<p>" + item + "</p>";
}
</script>

<h2>Today's News</h2>

<div id="content"></div>

<h2>Contribute</h2>
<form method="post" action="/send">
  <textarea name="text"></textarea>
  <input type="submit" value="send" />
</form>
<iframe id="foo" height="0" width="0" style="display: none" src="/js"/></iframe>
            """
        
    class js:
        def GET(self):
            mfeed.subscribe()
    
    class send:
        def POST(self):
            mfeed.publish('<p>%s</p>' % web.input().text + (" " * 2048))
            web.seeother('/')
    
    newrun(urls, globals())
########NEW FILE########
__FILENAME__ = alltests
import webtest

def suite():
    modules = ["doctests", "db", "application", "session"]
    return webtest.suite(modules)
    
if __name__ == "__main__":
    webtest.main()

########NEW FILE########
__FILENAME__ = application
import webtest
import time

import web
import urllib

data = """
import web

urls = ("/", "%(classname)s")
app = web.application(urls, globals(), autoreload=True)

class %(classname)s:
    def GET(self):
        return "%(output)s"

"""

urls = (
    "/iter", "do_iter",
)
app = web.application(urls, globals())

class do_iter:
    def GET(self):
        yield 'hello, '
        yield web.input(name='world').name

    POST = GET

def write(filename, data):
    f = open(filename, 'w')
    f.write(data)
    f.close()

class ApplicationTest(webtest.TestCase):
    def test_reloader(self):
        write('foo.py', data % dict(classname='a', output='a'))
        import foo
        app = foo.app
        
        self.assertEquals(app.request('/').data, 'a')
        
        # test class change
        time.sleep(1)
        write('foo.py', data % dict(classname='a', output='b'))
        self.assertEquals(app.request('/').data, 'b')

        # test urls change
        time.sleep(1)
        write('foo.py', data % dict(classname='c', output='c'))
        self.assertEquals(app.request('/').data, 'c')
        
    def testUppercaseMethods(self):
        urls = ("/", "hello")
        app = web.application(urls, locals())
        class hello:
            def GET(self): return "hello"
            def internal(self): return "secret"
            
        response = app.request('/', method='internal')
        self.assertEquals(response.status, '405 Method Not Allowed')
        
    def testRedirect(self):
        urls = (
            "/a", "redirect /hello/",
            "/b/(.*)", r"redirect /hello/\1",
            "/hello/(.*)", "hello"
        )
        app = web.application(urls, locals())
        class hello:
            def GET(self, name): 
                name = name or 'world'
                return "hello " + name
            
        response = app.request('/a')
        self.assertEquals(response.status, '301 Moved Permanently')
        self.assertEquals(response.headers['Location'], 'http://0.0.0.0:8080/hello/')

        response = app.request('/a?x=2')
        self.assertEquals(response.status, '301 Moved Permanently')
        self.assertEquals(response.headers['Location'], 'http://0.0.0.0:8080/hello/?x=2')

        response = app.request('/b/foo?x=2')
        self.assertEquals(response.status, '301 Moved Permanently')
        self.assertEquals(response.headers['Location'], 'http://0.0.0.0:8080/hello/foo?x=2')
        
    def test_subdirs(self):
        urls = (
            "/(.*)", "blog"
        )
        class blog:
            def GET(self, path):
                return "blog " + path
        app_blog = web.application(urls, locals())
        
        urls = (
            "/blog", app_blog,
            "/(.*)", "index"
        )
        class index:
            def GET(self, path):
                return "hello " + path
        app = web.application(urls, locals())
        
        self.assertEquals(app.request('/blog/foo').data, 'blog foo')
        self.assertEquals(app.request('/foo').data, 'hello foo')
        
        def processor(handler):
            return web.ctx.path + ":" + handler()
        app.add_processor(processor)
        self.assertEquals(app.request('/blog/foo').data, '/blog/foo:blog foo')
    
    def test_subdomains(self):
        def create_app(name):
            urls = ("/", "index")
            class index:
                def GET(self):
                    return name
            return web.application(urls, locals())
        
        urls = (
            "a.example.com", create_app('a'),
            "b.example.com", create_app('b'),
            ".*.example.com", create_app('*')
        )
        app = web.subdomain_application(urls, locals())
        
        def test(host, expected_result):
            result = app.request('/', host=host)
            self.assertEquals(result.data, expected_result)
            
        test('a.example.com', 'a')
        test('b.example.com', 'b')
        test('c.example.com', '*')
        test('d.example.com', '*')
        
    def test_redirect(self):
        urls = (
            "/(.*)", "blog"
        )
        class blog:
            def GET(self, path):
                if path == 'foo':
                    raise web.seeother('/login', absolute=True)
                else:
                    raise web.seeother('/bar')
        app_blog = web.application(urls, locals())
        
        urls = (
            "/blog", app_blog,
            "/(.*)", "index"
        )
        class index:
            def GET(self, path):
                return "hello " + path
        app = web.application(urls, locals())
        
        response = app.request('/blog/foo')
        self.assertEquals(response.headers['Location'], 'http://0.0.0.0:8080/login')
        
        response = app.request('/blog/foo', env={'SCRIPT_NAME': '/x'})
        self.assertEquals(response.headers['Location'], 'http://0.0.0.0:8080/x/login')

        response = app.request('/blog/foo2')
        self.assertEquals(response.headers['Location'], 'http://0.0.0.0:8080/blog/bar')
        
        response = app.request('/blog/foo2', env={'SCRIPT_NAME': '/x'})
        self.assertEquals(response.headers['Location'], 'http://0.0.0.0:8080/x/blog/bar')

    def test_processors(self):
        urls = (
            "/(.*)", "blog"
        )
        class blog:
            def GET(self, path):
                return 'blog ' + path

        state = web.storage(x=0, y=0)
        def f():
            state.x += 1

        app_blog = web.application(urls, locals())
        app_blog.add_processor(web.loadhook(f))
        
        urls = (
            "/blog", app_blog,
            "/(.*)", "index"
        )
        class index:
            def GET(self, path):
                return "hello " + path
        app = web.application(urls, locals())
        def g():
            state.y += 1
        app.add_processor(web.loadhook(g))

        app.request('/blog/foo')
        assert state.x == 1 and state.y == 1, repr(state)
        app.request('/foo')
        assert state.x == 1 and state.y == 2, repr(state)
        
    def testUnicodeInput(self):
        urls = (
            "(/.*)", "foo"
        )
        class foo:
            def GET(self, path):
                i = web.input(name='')
                return repr(i.name)
                
            def POST(self, path):
                if path == '/multipart':
                    i = web.input(file={})
                    return i.file.value
                else:
                    i = web.input()
                    return repr(dict(i))
                
        app = web.application(urls, locals())
        
        def f(name):
            path = '/?' + urllib.urlencode({"name": name.encode('utf-8')})
            self.assertEquals(app.request(path).data, repr(name))
            
        f(u'\u1234')
        f(u'foo')

        response = app.request('/', method='POST', data=dict(name='foo'))
        self.assertEquals(response.data, "{'name': u'foo'}")
        
        data = '--boundary\r\nContent-Disposition: form-data; name="x"\r\nfoo\r\n--boundary\r\nContent-Disposition: form-data; name="file"; filename="a.txt"\r\nContent-Type: text/plain\r\n\r\na\r\n--boundary--\r\n'
        headers = {'Content-Type': 'multipart/form-data; boundary=boundary'}
        response = app.request('/multipart', method="POST", data=data, headers=headers)
        self.assertEquals(response.data, 'a')
        
    def testCustomNotFound(self):
        urls_a = ("/", "a")
        urls_b = ("/", "b")
        
        app_a = web.application(urls_a, locals())
        app_b = web.application(urls_b, locals())
        
        app_a.notfound = lambda: web.HTTPError("404 Not Found", {}, "not found 1")
        
        urls = (
            "/a", app_a,
            "/b", app_b
        )
        app = web.application(urls, locals())
        
        def assert_notfound(path, message):
            response = app.request(path)
            self.assertEquals(response.status.split()[0], "404")
            self.assertEquals(response.data, message)
            
        assert_notfound("/a/foo", "not found 1")
        assert_notfound("/b/foo", "not found")
        
        app.notfound = lambda: web.HTTPError("404 Not Found", {}, "not found 2")
        assert_notfound("/a/foo", "not found 1")
        assert_notfound("/b/foo", "not found 2")

    def testIter(self):
        self.assertEquals(app.request('/iter').data, 'hello, world')
        self.assertEquals(app.request('/iter?name=web').data, 'hello, web')

        self.assertEquals(app.request('/iter', method='POST').data, 'hello, world')
        self.assertEquals(app.request('/iter', method='POST', data='name=web').data, 'hello, web')

    def testUnload(self):
        x = web.storage(a=0)

        urls = (
            "/foo", "foo",
            "/bar", "bar"
        )
        class foo:
            def GET(self):
                return "foo"
        class bar:
            def GET(self):
                raise web.notfound()

        app = web.application(urls, locals())
        def unload():
            x.a += 1
        app.add_processor(web.unloadhook(unload))

        app.request('/foo')
        self.assertEquals(x.a, 1)

        app.request('/bar')
        self.assertEquals(x.a, 2)

if __name__ == '__main__':
    webtest.main()

########NEW FILE########
__FILENAME__ = db
"""DB test"""
import webtest
import web

class DBTest(webtest.TestCase):
    dbname = 'postgres'
    driver = None
    
    def setUp(self):
        self.db = webtest.setup_database(self.dbname, driver=self.driver)
        self.db.query("CREATE TABLE person (name text, email text, active boolean)")

    def tearDown(self):
        # there might be some error with the current connection, delete from a new connection
        self.db = webtest.setup_database(self.dbname, driver=self.driver)
        self.db.query('DROP TABLE person')
        
    def _testable(self):
        try:
            webtest.setup_database(self.dbname, driver=self.driver)
            return True
        except ImportError, e:
            print >> web.debug, str(e), "(ignoring %s)" % self.__class__.__name__
            return False
    
    def testUnicode(self):
        # Bug#177265: unicode queries throw errors
        self.db.select('person', where='name=$name', vars={'name': u'\xf4'})
    
    def assertRows(self, n):
        result = self.db.select('person')
        self.assertEquals(len(list(result)), n)
        
    def testCommit(self):
        t = self.db.transaction()
        self.db.insert('person', False, name='user1')
        t.commit()

        t = self.db.transaction()
        self.db.insert('person', False, name='user2')
        self.db.insert('person', False, name='user3')
        t.commit()
    
        self.assertRows(3)
        
    def testRollback(self):
        t = self.db.transaction()
        self.db.insert('person', False, name='user1')
        self.db.insert('person', False, name='user2')
        self.db.insert('person', False, name='user3')
        t.rollback()        
        self.assertRows(0)
        
    def testWrongQuery(self):
        # It should be possible to run a correct query after getting an error from a wrong query.
        try:
            self.db.select('notthere')
        except:
            pass
        self.db.select('person')
        
    def testNestedTransactions(self):
        t1 = self.db.transaction()
        self.db.insert('person', False, name='user1')
        self.assertRows(1)        
        
        t2 = self.db.transaction()
        self.db.insert('person', False, name='user2')
        self.assertRows(2)  
        t2.rollback()
        self.assertRows(1)  
        t3 = self.db.transaction()
        self.db.insert('person', False, name='user3')
        self.assertRows(2)  
        t3.commit()
        t1.commit()
        self.assertRows(2)
        
    def testPooling(self):
        # can't test pooling if DBUtils is not installed
        try:
            import DBUtils
        except ImportError:
            return
        db = webtest.setup_database(self.dbname, pooling=True)
        self.assertEquals(db.ctx.db.__class__.__module__, 'DBUtils.PooledDB')
        db.select('person', limit=1)

    def test_multiple_insert(self):
        db = webtest.setup_database(self.dbname)
        db.multiple_insert('person', [dict(name='a'), dict(name='b')], seqname=False)

        assert db.select("person", where="name='a'")
        assert db.select("person", where="name='b'")

    def test_result_is_unicode(self):
        db = webtest.setup_database(self.dbname)
        self.db.insert('person', False, name='user')
        name = db.select('person')[0].name
        self.assertEquals(type(name), unicode)

    def testBoolean(self):
        def t(active):
            name ='name-%s' % active
            self.db.insert('person', False, name=name, active=active)
            a = self.db.select('person', where='name=$name', vars=locals())[0].active
            self.assertEquals(a, active)
        t(False)
        t(True)

class PostgresTest(DBTest):
    dbname = "postgres"
    driver = "psycopg2"

class PostgresTest_psycopg(PostgresTest):
    driver = "psycopg"

class PostgresTest_pgdb(PostgresTest):
    driver = "pgdb"

class SqliteTest(DBTest):
    dbname = "sqlite"
    driver = "sqlite3"
    
    def testNestedTransactions(self):
        #nested transactions does not work with sqlite
        pass

class SqliteTest_pysqlite2(SqliteTest):
    driver = "pysqlite2.dbapi2"

class MySQLTest(DBTest):
    dbname = "mysql"
    
    def setUp(self):
        self.db = webtest.setup_database(self.dbname)
        # In mysql, transactions are supported only with INNODB engine.
        self.db.query("CREATE TABLE person (name text, email text) ENGINE=INNODB")

    def testBoolean(self):
        # boolean datatype is not suppoted in MySQL (at least until v5.0)
        pass

del DBTest

def is_test(cls):
    import inspect
    return inspect.isclass(cls) and webtest.TestCase in inspect.getmro(cls)

# ignore db tests when the required db adapter is not found.
for t in globals().values():
    if is_test(t) and not t('_testable')._testable():
        del globals()[t.__name__]
del t

try:
    import DBUtils
except ImportError, e:
    print >> web.debug, str(e) + "(ignoring testPooling)"

if __name__ == '__main__':
    webtest.main()

########NEW FILE########
__FILENAME__ = doctests
"""Run all doctests in web.py.
"""
import webtest

def suite():
    modules = [
        "web.application",
        "web.db", 
        "web.http", 
        "web.net", 
        "web.session",
        "web.template",
        "web.utils", 
#        "web.webapi", 
#        "web.wsgi", 
    ]
    return webtest.doctest_suite(modules)
    
if __name__ == "__main__":
    webtest.main()

########NEW FILE########
__FILENAME__ = session
import webtest
import web
import tempfile

class SessionTest(webtest.TestCase):
    def setUp(self):
        app = web.auto_application()
        session = self.make_session(app)
        class count(app.page):
            def GET(self):
                session.count += 1
                return str(session.count)
        
        class reset(app.page):
            def GET(self):
                session.kill()
                return ""
                
        self.app = app
        self.session = session
        
    def make_session(self, app):
        dir = tempfile.mkdtemp()
        store = web.session.DiskStore(tempfile.mkdtemp())
        return web.session.Session(app, store, {'count': 0})
        
    def testSession(self):
        b = self.app.browser() 
        self.assertEquals(b.open('/count').read(), '1')
        self.assertEquals(b.open('/count').read(), '2')
        self.assertEquals(b.open('/count').read(), '3')
        b.open('/reset')
        self.assertEquals(b.open('/count').read(), '1')

    def testParallelSessions(self):
        b1 = self.app.browser()
        b2 = self.app.browser()
        
        b1.open('/count')
        
        for i in range(1, 10):
            self.assertEquals(b1.open('/count').read(), str(i+1))
            self.assertEquals(b2.open('/count').read(), str(i))

    def testBadSessionId(self):
        b = self.app.browser()
        self.assertEquals(b.open('/count').read(), '1')
        self.assertEquals(b.open('/count').read(), '2')
        
        cookie = b.cookiejar._cookies['0.0.0.0']['/']['webpy_session_id']
        cookie.value = '/etc/password'
        self.assertEquals(b.open('/count').read(), '1')

class DBSessionTest(SessionTest):
    """Session test with db store."""
    def make_session(self, app):
        db = webtest.setup_database("postgres")
        #db.printing = True
        db.query("" 
            + "CREATE TABLE session ("
            + "    session_id char(128) unique not null,"
            + "    atime timestamp default (current_timestamp at time zone 'utc'),"
            + "    data text)"
        )
        store = web.session.DBStore(db, 'session')
        return web.session.Session(app, store, {'count': 0})
         
    def tearDown(self):
        # there might be some error with the current connection, delete from a new connection
        self.db = webtest.setup_database("postgres")
        self.db.query('DROP TABLE session')

if __name__ == "__main__":
    webtest.main()

########NEW FILE########
__FILENAME__ = webtest
"""webtest: test utilities.
"""
import sys, os

# adding current directory to path to make sure local modules can be imported
sys.path.insert(0, '.')

from web.test import *
    
def setup_database(dbname, driver=None, pooling=False):
    if dbname == 'sqlite':
        db = web.database(dbn=dbname, db='webpy.db', pooling=pooling, driver=driver)
    elif dbname == 'postgres':
        user = os.getenv('USER')
        db = web.database(dbn=dbname, db='webpy', user=user, pw='', pooling=pooling, driver=driver)
    else:
        db = web.database(dbn=dbname, db='webpy', user='scott', pw='tiger', pooling=pooling, driver=driver)

    db.printing = '-v' in sys.argv
    return db

########NEW FILE########
__FILENAME__ = markdown
#!/usr/bin/python
import re, md5, sys, string

"""markdown.py: A Markdown-styled-text to HTML converter in Python.

Usage:
  ./markdown.py textfile.markdown
 
Calling:
  import markdown
  somehtml = markdown.markdown(sometext)

For other versions of markdown, see: 
  http://www.freewisdom.org/projects/python-markdown/
  http://en.wikipedia.org/wiki/Markdown
"""

__version__ = '1.0.1-2' # port of 1.0.1
__license__ = "GNU GPL 2"
__author__ = [
  'John Gruber <http://daringfireball.net/>',
  'Tollef Fog Heen <tfheen@err.no>', 
  'Aaron Swartz <me@aaronsw.com>'
]

def htmlquote(text):
    """Encodes `text` for raw use in HTML."""
    text = text.replace("&", "&amp;") # Must be done first!
    text = text.replace("<", "&lt;")
    text = text.replace(">", "&gt;")
    text = text.replace("'", "&#39;")
    text = text.replace('"', "&quot;")
    return text

def semirandom(seed):
    x = 0
    for c in md5.new(seed).digest(): x += ord(c)
    return x / (255*16.)

class _Markdown:
    emptyelt = " />"
    tabwidth = 4

    escapechars = '\\`*_{}[]()>#+-.!'
    escapetable = {}
    for char in escapechars:
        escapetable[char] = md5.new(char).hexdigest()
    
    r_multiline = re.compile("\n{2,}")
    r_stripspace = re.compile(r"^[ \t]+$", re.MULTILINE)
    def parse(self, text):
        self.urls = {}
        self.titles = {}
        self.html_blocks = {}
        self.list_level = 0
        
        text = text.replace("\r\n", "\n")
        text = text.replace("\r", "\n")
        text += "\n\n"
        text = self._Detab(text)
        text = self.r_stripspace.sub("", text)
        text = self._HashHTMLBlocks(text)
        text = self._StripLinkDefinitions(text)
        text = self._RunBlockGamut(text)
        text = self._UnescapeSpecialChars(text)
        return text
    
    r_StripLinkDefinitions = re.compile(r"""
    ^[ ]{0,%d}\[(.+)\]:  # id = $1
      [ \t]*\n?[ \t]*
    <?(\S+?)>?           # url = $2
      [ \t]*\n?[ \t]*
    (?:
      (?<=\s)            # lookbehind for whitespace
      [\"\(]             # " is backlashed so it colorizes our code right
      (.+?)              # title = $3
      [\"\)]
      [ \t]*
    )?                   # title is optional
    (?:\n+|\Z)
    """ % (tabwidth-1), re.MULTILINE|re.VERBOSE)
    def _StripLinkDefinitions(self, text):
        def replacefunc(matchobj):
            (t1, t2, t3) = matchobj.groups()
            #@@ case sensitivity?
            self.urls[t1.lower()] = self._EncodeAmpsAndAngles(t2)
            if t3 is not None:
                self.titles[t1.lower()] = t3.replace('"', '&quot;')
            return ""

        text = self.r_StripLinkDefinitions.sub(replacefunc, text)
        return text

    blocktagsb = r"p|div|h[1-6]|blockquote|pre|table|dl|ol|ul|script|math"
    blocktagsa = blocktagsb + "|ins|del"
    
    r_HashHTMLBlocks1 = re.compile(r"""
    (            # save in $1
    ^            # start of line  (with /m)
    <(%s)        # start tag = $2
    \b           # word break
    (.*\n)*?     # any number of lines, minimally matching
    </\2>        # the matching end tag
    [ \t]*       # trailing spaces/tabs
    (?=\n+|$)    # followed by a newline or end of document
    )
    """ % blocktagsa, re.MULTILINE | re.VERBOSE)

    r_HashHTMLBlocks2 = re.compile(r"""
    (            # save in $1
    ^            # start of line  (with /m)
    <(%s)        # start tag = $2
    \b           # word break
    (.*\n)*?     # any number of lines, minimally matching
    .*</\2>      # the matching end tag
    [ \t]*       # trailing spaces/tabs
    (?=\n+|\Z)   # followed by a newline or end of document
    )
    """ % blocktagsb, re.MULTILINE | re.VERBOSE)

    r_HashHR = re.compile(r"""
    (?:
    (?<=\n\n)    # Starting after a blank line
    |            # or
    \A\n?        # the beginning of the doc
    )
    (            # save in $1
    [ ]{0,%d}
    <(hr)        # start tag = $2
    \b           # word break
    ([^<>])*?    # 
    /?>          # the matching end tag
    [ \t]*
    (?=\n{2,}|\Z)# followed by a blank line or end of document
    )
    """ % (tabwidth-1), re.VERBOSE)
    r_HashComment = re.compile(r"""
    (?:
    (?<=\n\n)    # Starting after a blank line
    |            # or
    \A\n?        # the beginning of the doc
    )
    (            # save in $1
    [ ]{0,%d}
    (?: 
      <!
      (--.*?--\s*)+
      >
    )
    [ \t]*
    (?=\n{2,}|\Z)# followed by a blank line or end of document
    )
    """ % (tabwidth-1), re.VERBOSE)

    def _HashHTMLBlocks(self, text):
        def handler(m):
            key = md5.new(m.group(1)).hexdigest()
            self.html_blocks[key] = m.group(1)
            return "\n\n%s\n\n" % key

        text = self.r_HashHTMLBlocks1.sub(handler, text)
        text = self.r_HashHTMLBlocks2.sub(handler, text)
        oldtext = text
        text = self.r_HashHR.sub(handler, text)
        text = self.r_HashComment.sub(handler, text)
        return text

    #@@@ wrong!
    r_hr1 = re.compile(r'^[ ]{0,2}([ ]?\*[ ]?){3,}[ \t]*$', re.M)
    r_hr2 = re.compile(r'^[ ]{0,2}([ ]?-[ ]?){3,}[ \t]*$', re.M)
    r_hr3 = re.compile(r'^[ ]{0,2}([ ]?_[ ]?){3,}[ \t]*$', re.M)
	
    def _RunBlockGamut(self, text):
        text = self._DoHeaders(text)
        for x in [self.r_hr1, self.r_hr2, self.r_hr3]:
            text = x.sub("\n<hr%s\n" % self.emptyelt, text);
        text = self._DoLists(text)
        text = self._DoCodeBlocks(text)
        text = self._DoBlockQuotes(text)

    	# We did this in parse()
    	# to escape the source
    	# now it's stuff _we_ made
    	# so we don't wrap it in <p>s.
        text = self._HashHTMLBlocks(text)
        text = self._FormParagraphs(text)
        return text

    r_NewLine = re.compile(" {2,}\n")
    def _RunSpanGamut(self, text):
        text = self._DoCodeSpans(text)
        text = self._EscapeSpecialChars(text)
        text = self._DoImages(text)
        text = self._DoAnchors(text)
        text = self._DoAutoLinks(text)
        text = self._EncodeAmpsAndAngles(text)
        text = self._DoItalicsAndBold(text)
        text = self.r_NewLine.sub(" <br%s\n" % self.emptyelt, text)
        return text

    def _EscapeSpecialChars(self, text):
        tokens = self._TokenizeHTML(text)
        text = ""
        for cur_token in tokens:
            if cur_token[0] == "tag":
                cur_token[1] = cur_token[1].replace('*', self.escapetable["*"])
                cur_token[1] = cur_token[1].replace('_', self.escapetable["_"])
                text += cur_token[1]
            else:
                text += self._EncodeBackslashEscapes(cur_token[1])
        return text

    r_DoAnchors1 = re.compile(
          r""" (                 # wrap whole match in $1
                  \[
                    (.*?)        # link text = $2 
                    # [for bracket nesting, see below]
                  \]

                  [ ]?           # one optional space
                  (?:\n[ ]*)?    # one optional newline followed by spaces

                  \[
                    (.*?)        # id = $3
                  \]
                )
    """, re.S|re.VERBOSE)
    r_DoAnchors2 = re.compile(
          r""" (                   # wrap whole match in $1
                  \[
                    (.*?)          # link text = $2
                  \]
                  \(               # literal paren
                        [ \t]*
                        <?(.+?)>?  # href = $3
                        [ \t]*
                        (          # $4
                          ([\'\"]) # quote char = $5
                          (.*?)    # Title = $6
                          \5       # matching quote
                        )?         # title is optional
                  \)
                )
    """, re.S|re.VERBOSE)
    def _DoAnchors(self, text): 
        # We here don't do the same as the perl version, as python's regex
        # engine gives us no way to match brackets.

        def handler1(m):
            whole_match = m.group(1)
            link_text = m.group(2)
            link_id = m.group(3).lower()
            if not link_id: link_id = link_text.lower()
            title = self.titles.get(link_id, None)
                

            if self.urls.has_key(link_id):
                url = self.urls[link_id]
                url = url.replace("*", self.escapetable["*"])
                url = url.replace("_", self.escapetable["_"])
                res = '<a href="%s"' % htmlquote(url)

                if title:
                    title = title.replace("*", self.escapetable["*"])
                    title = title.replace("_", self.escapetable["_"])
                    res += ' title="%s"' % htmlquote(title)
                res += ">%s</a>" % htmlquote(link_text)
            else:
                res = whole_match
            return res

        def handler2(m):
            whole_match = m.group(1)
            link_text = m.group(2)
            url = m.group(3)
            title = m.group(6)

            url = url.replace("*", self.escapetable["*"])
            url = url.replace("_", self.escapetable["_"])
            res = '''<a href="%s"''' % htmlquote(url)
            
            if title:
                title = title.replace('"', '&quot;')
                title = title.replace("*", self.escapetable["*"])
                title = title.replace("_", self.escapetable["_"])
                res += ' title="%s"' % htmlquote(title)
            res += ">%s</a>" % htmlquote(link_text)
            return res

        text = self.r_DoAnchors1.sub(handler1, text)
        text = self.r_DoAnchors2.sub(handler2, text)
        return text

    r_DoImages1 = re.compile(
           r""" (                       # wrap whole match in $1
                  !\[
                    (.*?)               # alt text = $2
                  \]

                  [ ]?                  # one optional space
                  (?:\n[ ]*)?           # one optional newline followed by spaces

                  \[
                    (.*?)               # id = $3
                  \]

                )
    """, re.VERBOSE|re.S)

    r_DoImages2 = re.compile(
          r""" (                        # wrap whole match in $1
                  !\[
                    (.*?)               # alt text = $2
                  \]
                  \(                    # literal paren
                        [ \t]*
                        <?(\S+?)>?      # src url = $3
                        [ \t]*
                        (               # $4
                        ([\'\"])        # quote char = $5
                          (.*?)         # title = $6
                          \5            # matching quote
                          [ \t]*
                        )?              # title is optional
                  \)
                )
    """, re.VERBOSE|re.S)

    def _DoImages(self, text):
        def handler1(m):
            whole_match = m.group(1)
            alt_text = m.group(2)
            link_id = m.group(3).lower()

            if not link_id:
                link_id = alt_text.lower()

            alt_text = alt_text.replace('"', "&quot;")
            if self.urls.has_key(link_id):
                url = self.urls[link_id]
                url = url.replace("*", self.escapetable["*"])
                url = url.replace("_", self.escapetable["_"])
                res = '''<img src="%s" alt="%s"''' % (htmlquote(url), htmlquote(alt_text))
                if self.titles.has_key(link_id):
                    title = self.titles[link_id]
                    title = title.replace("*", self.escapetable["*"])
                    title = title.replace("_", self.escapetable["_"])
                    res += ' title="%s"' % htmlquote(title)
                res += self.emptyelt
            else:
                res = whole_match
            return res

        def handler2(m):
            whole_match = m.group(1)
            alt_text = m.group(2)
            url = m.group(3)
            title = m.group(6) or ''
            
            alt_text = alt_text.replace('"', "&quot;")
            title = title.replace('"', "&quot;")
            url = url.replace("*", self.escapetable["*"])
            url = url.replace("_", self.escapetable["_"])
            res = '<img src="%s" alt="%s"' % (htmlquote(url), htmlquote(alt_text))
            if title is not None:
                title = title.replace("*", self.escapetable["*"])
                title = title.replace("_", self.escapetable["_"])
                res += ' title="%s"' % htmlquote(title)
            res += self.emptyelt
            return res

        text = self.r_DoImages1.sub(handler1, text)
        text = self.r_DoImages2.sub(handler2, text)
        return text
    
    r_DoHeaders = re.compile(r"^(\#{1,6})[ \t]*(.+?)[ \t]*\#*\n+", re.VERBOSE|re.M)
    def _DoHeaders(self, text):
        def findheader(text, c, n):
            textl = text.split('\n')
            for i in xrange(len(textl)):
                if i >= len(textl): continue
                count = textl[i].strip().count(c)
                if count > 0 and count == len(textl[i].strip()) and textl[i+1].strip() == '' and textl[i-1].strip() != '':
                    textl = textl[:i] + textl[i+1:]
                    textl[i-1] = '<h'+n+'>'+self._RunSpanGamut(textl[i-1])+'</h'+n+'>'
                    textl = textl[:i] + textl[i+1:]
            text = '\n'.join(textl)
            return text
        
        def handler(m):
            level = len(m.group(1))
            header = self._RunSpanGamut(m.group(2))
            return "<h%s>%s</h%s>\n\n" % (level, header, level)

        text = findheader(text, '=', '1')
        text = findheader(text, '-', '2')
        text = self.r_DoHeaders.sub(handler, text)
        return text
    
    rt_l = r"""
    (
      (
        [ ]{0,%d}
        ([*+-]|\d+[.])
        [ \t]+
      )
      (?:.+?)
      (
        \Z
      |
        \n{2,}
        (?=\S)
        (?![ \t]* ([*+-]|\d+[.])[ \t]+)
      )
    )
    """ % (tabwidth - 1)
    r_DoLists = re.compile('^'+rt_l, re.M | re.VERBOSE | re.S)
    r_DoListsTop = re.compile(
      r'(?:\A\n?|(?<=\n\n))'+rt_l, re.M | re.VERBOSE | re.S)
    
    def _DoLists(self, text):
        def handler(m):
            list_type = "ol"
            if m.group(3) in [ "*", "-", "+" ]:
                list_type = "ul"
            listn = m.group(1)
            listn = self.r_multiline.sub("\n\n\n", listn)
            res = self._ProcessListItems(listn)
            res = "<%s>\n%s</%s>\n" % (list_type, res, list_type)
            return res
            
        if self.list_level:
            text = self.r_DoLists.sub(handler, text)
        else:
            text = self.r_DoListsTop.sub(handler, text)
        return text

    r_multiend = re.compile(r"\n{2,}\Z")
    r_ProcessListItems = re.compile(r"""
    (\n)?                            # leading line = $1
    (^[ \t]*)                        # leading whitespace = $2
    ([*+-]|\d+[.]) [ \t]+            # list marker = $3
    ((?:.+?)                         # list item text = $4
    (\n{1,2}))
    (?= \n* (\Z | \2 ([*+-]|\d+[.]) [ \t]+))
    """, re.VERBOSE | re.M | re.S)

    def _ProcessListItems(self, text):
        self.list_level += 1
        text = self.r_multiend.sub("\n", text)
        
        def handler(m):
            item = m.group(4)
            leading_line = m.group(1)
            leading_space = m.group(2)

            if leading_line or self.r_multiline.search(item):
                item = self._RunBlockGamut(self._Outdent(item))
            else:
                item = self._DoLists(self._Outdent(item))
                if item[-1] == "\n": item = item[:-1] # chomp
                item = self._RunSpanGamut(item)
            return "<li>%s</li>\n" % item

        text = self.r_ProcessListItems.sub(handler, text)
        self.list_level -= 1
        return text
    
    r_DoCodeBlocks = re.compile(r"""
    (?:\n\n|\A)
    (                 # $1 = the code block
    (?:
    (?:[ ]{%d} | \t)  # Lines must start with a tab or equiv
    .*\n+
    )+
    )
    ((?=^[ ]{0,%d}\S)|\Z) # Lookahead for non-space/end of doc
    """ % (tabwidth, tabwidth), re.M | re.VERBOSE)
    def _DoCodeBlocks(self, text):
        def handler(m):
            codeblock = m.group(1)
            codeblock = self._EncodeCode(self._Outdent(codeblock))
            codeblock = self._Detab(codeblock)
            codeblock = codeblock.lstrip("\n")
            codeblock = codeblock.rstrip()
            res = "\n\n<pre><code>%s\n</code></pre>\n\n" % codeblock
            return res

        text = self.r_DoCodeBlocks.sub(handler, text)
        return text
    r_DoCodeSpans = re.compile(r"""
    (`+)            # $1 = Opening run of `
    (.+?)           # $2 = The code block
    (?<!`)
    \1              # Matching closer
    (?!`)
    """, re.I|re.VERBOSE)
    def _DoCodeSpans(self, text):
        def handler(m):
            c = m.group(2)
            c = c.strip()
            c = self._EncodeCode(c)
            return "<code>%s</code>" % c

        text = self.r_DoCodeSpans.sub(handler, text)
        return text
    
    def _EncodeCode(self, text):
        text = text.replace("&","&amp;")
        text = text.replace("<","&lt;")
        text = text.replace(">","&gt;")
        for c in "*_{}[]\\":
            text = text.replace(c, self.escapetable[c])
        return text

    
    r_DoBold = re.compile(r"(\*\*|__) (?=\S) (.+?[*_]*) (?<=\S) \1", re.VERBOSE | re.S)
    r_DoItalics = re.compile(r"(\*|_) (?=\S) (.+?) (?<=\S) \1", re.VERBOSE | re.S)
    def _DoItalicsAndBold(self, text):
        text = self.r_DoBold.sub(r"<strong>\2</strong>", text)
        text = self.r_DoItalics.sub(r"<em>\2</em>", text)
        return text
    
    r_start = re.compile(r"^", re.M)
    r_DoBlockQuotes1 = re.compile(r"^[ \t]*>[ \t]?", re.M)
    r_DoBlockQuotes2 = re.compile(r"^[ \t]+$", re.M)
    r_DoBlockQuotes3 = re.compile(r"""
    (                       # Wrap whole match in $1
     (
       ^[ \t]*>[ \t]?       # '>' at the start of a line
       .+\n                 # rest of the first line
       (.+\n)*              # subsequent consecutive lines
       \n*                  # blanks
      )+
    )""", re.M | re.VERBOSE)
    r_protectpre = re.compile(r'(\s*<pre>.+?</pre>)', re.S)
    r_propre = re.compile(r'^  ', re.M)

    def _DoBlockQuotes(self, text):
        def prehandler(m):
            return self.r_propre.sub('', m.group(1))
                
        def handler(m):
            bq = m.group(1)
            bq = self.r_DoBlockQuotes1.sub("", bq)
            bq = self.r_DoBlockQuotes2.sub("", bq)
            bq = self._RunBlockGamut(bq)
            bq = self.r_start.sub("  ", bq)
            bq = self.r_protectpre.sub(prehandler, bq)
            return "<blockquote>\n%s\n</blockquote>\n\n" % bq
            
        text = self.r_DoBlockQuotes3.sub(handler, text)
        return text

    r_tabbed = re.compile(r"^([ \t]*)")
    def _FormParagraphs(self, text):
        text = text.strip("\n")
        grafs = self.r_multiline.split(text)

        for g in xrange(len(grafs)):
            t = grafs[g].strip() #@@?
            if not self.html_blocks.has_key(t):
                t = self._RunSpanGamut(t)
                t = self.r_tabbed.sub(r"<p>", t)
                t += "</p>"
                grafs[g] = t

        for g in xrange(len(grafs)):
            t = grafs[g].strip()
            if self.html_blocks.has_key(t):
                grafs[g] = self.html_blocks[t]
        
        return "\n\n".join(grafs)

    r_EncodeAmps = re.compile(r"&(?!#?[xX]?(?:[0-9a-fA-F]+|\w+);)")
    r_EncodeAngles = re.compile(r"<(?![a-z/?\$!])")
    def _EncodeAmpsAndAngles(self, text):
        text = self.r_EncodeAmps.sub("&amp;", text)
        text = self.r_EncodeAngles.sub("&lt;", text)
        return text

    def _EncodeBackslashEscapes(self, text):
        for char in self.escapechars:
            text = text.replace("\\" + char, self.escapetable[char])
        return text
    
    r_link = re.compile(r"<((https?|ftp):[^\'\">\s]+)>", re.I)
    r_email = re.compile(r"""
      <
      (?:mailto:)?
      (
         [-.\w]+
         \@
         [-a-z0-9]+(\.[-a-z0-9]+)*\.[a-z]+
      )
      >""", re.VERBOSE|re.I)
    def _DoAutoLinks(self, text):
        text = self.r_link.sub(r'<a href="\1">\1</a>', text)

        def handler(m):
            l = m.group(1)
            return self._EncodeEmailAddress(self._UnescapeSpecialChars(l))
    
        text = self.r_email.sub(handler, text)
        return text
    
    r_EncodeEmailAddress = re.compile(r">.+?:")
    def _EncodeEmailAddress(self, text):
        encode = [
            lambda x: "&#%s;" % ord(x),
            lambda x: "&#x%X;" % ord(x),
            lambda x: x
        ]

        text = "mailto:" + text
        addr = ""
        for c in text:
            if c == ':': addr += c; continue
            
            r = semirandom(addr)
            if r < 0.45:
                addr += encode[1](c)
            elif r > 0.9 and c != '@':
                addr += encode[2](c)
            else:
                addr += encode[0](c)

        text = '<a href="%s">%s</a>' % (addr, addr)
        text = self.r_EncodeEmailAddress.sub('>', text)
        return text

    def _UnescapeSpecialChars(self, text):
        for key in self.escapetable.keys():
            text = text.replace(self.escapetable[key], key)
        return text
    
    tokenize_depth = 6
    tokenize_nested_tags = '|'.join([r'(?:<[a-z/!$](?:[^<>]'] * tokenize_depth) + (')*>)' * tokenize_depth)
    r_TokenizeHTML = re.compile(
      r"""(?: <! ( -- .*? -- \s* )+ > ) |  # comment
          (?: <\? .*? \?> ) |              # processing instruction
          %s                               # nested tags
    """ % tokenize_nested_tags, re.I|re.VERBOSE)
    def _TokenizeHTML(self, text):
        pos = 0
        tokens = []
        matchobj = self.r_TokenizeHTML.search(text, pos)
        while matchobj:
            whole_tag = matchobj.string[matchobj.start():matchobj.end()]
            sec_start = matchobj.end()
            tag_start = sec_start - len(whole_tag)
            if pos < tag_start:
                tokens.append(["text", matchobj.string[pos:tag_start]])

            tokens.append(["tag", whole_tag])
            pos = sec_start
            matchobj = self.r_TokenizeHTML.search(text, pos)

        if pos < len(text):
            tokens.append(["text", text[pos:]])
        return tokens

    r_Outdent = re.compile(r"""^(\t|[ ]{1,%d})""" % tabwidth, re.M)
    def _Outdent(self, text):
        text = self.r_Outdent.sub("", text)
        return text    

    def _Detab(self, text): return text.expandtabs(self.tabwidth)

def Markdown(*args, **kw): return _Markdown().parse(*args, **kw)
markdown = Markdown

if __name__ == '__main__':
    if len(sys.argv) > 1:
        print Markdown(open(sys.argv[1]).read())
    else:
        print Markdown(sys.stdin.read())

########NEW FILE########
__FILENAME__ = _makedoc
import os
import web

class Parser:
    def __init__(self):
        self.mode = 'normal'
        self.text = ''
        
    def go(self, pyfile):
        for line in file(pyfile):
            if self.mode == 'in def':
                self.text += ' ' + line.strip()
                if line.strip().endswith(':'):
                    if self.definition(self.text):
                        self.text = ''
                        self.mode = 'in func'
                    else:
                        self.text = ''
                        self.mode = 'normal'

            elif self.mode == 'in func':
                if '"""' in line:
                    self.text += line.strip().strip('"')
                    self.mode = 'in doc'
                    if line.count('"""') == 2:
                        self.mode = 'normal'
                        self.docstring(self.text)
                        self.text = ''
                else:
                    self.mode = 'normal'

            elif self.mode == 'in doc':
                self.text += ' ' + line
                if '"""' in line:
                    self.mode = 'normal'
                    self.docstring(self.text.strip().strip('"'))
                    self.text = ''
            
            elif line.startswith('## '):
                self.header(line.strip().strip('#'))
            
            elif line.startswith('def ') or line.startswith('class '):
                self.text += line.strip().strip(':')
                if line.strip().endswith(':'):
                    if self.definition(self.text):
                        self.text = ''
                        self.mode = 'in func'
                    else:
                        self.text = ''
                        self.mode = 'normal'
                else:
                    self.mode = 'in def'
    
    def clean(self, text):
        text = text.strip()
        text = text.replace('*', r'\*')
        return text
    
    def definition(self, text):
        text = web.lstrips(text, 'def ')
        if text.startswith('_') or text.startswith('class _'):
            return False
        print '`'+text.strip()+'`'
        return True
    
    def docstring(self, text):
        print '   :', text.strip()
        print
    
    def header(self, text):
        print '##', text.strip()
        print
        
for pyfile in os.listdir('trunk/web'):
    if pyfile[-2:] == 'py':
        print
        print '## ' + pyfile
        print
        Parser().go('trunk/web/' + pyfile)
print '`ctx`\n   :',
print '\n'.join('    '+x for x in web.ctx.__doc__.strip().split('\n'))

########NEW FILE########
__FILENAME__ = application
#!/usr/bin/python
"""
Web application
(from web.py)
"""
import webapi as web
import webapi, wsgi, utils
import debugerror
from utils import lstrips, safeunicode
import sys

import urllib
import traceback
import itertools
import os
import re
import types
from exceptions import SystemExit

try:
    import wsgiref.handlers
except ImportError:
    pass # don't break people with old Pythons

__all__ = [
    "application", "auto_application",
    "subdir_application", "subdomain_application", 
    "loadhook", "unloadhook",
    "autodelegate"
]

class application:
    """
    Application to delegate requests based on path.
    
        >>> urls = ("/hello", "hello")
        >>> app = application(urls, globals())
        >>> class hello:
        ...     def GET(self): return "hello"
        >>>
        >>> app.request("/hello").data
        'hello'
    """
    def __init__(self, mapping=(), fvars={}, autoreload=None):
        if autoreload is None:
            autoreload = web.config.get('debug', False)
        self.mapping = mapping
        self.fvars = fvars
        self.processors = []
        
        self.add_processor(loadhook(self._load))
        self.add_processor(unloadhook(self._unload))
        
        if autoreload:
            def main_module_name():
                mod = sys.modules['__main__']
                file = getattr(mod, '__file__', None) # make sure this works even from python interpreter
                return file and os.path.splitext(os.path.basename(file))[0]

            def modname(fvars):
                """find name of the module name from fvars."""
                file, name = fvars.get('__file__'), fvars.get('__name__')
                if file is None or name is None:
                    return None

                if name == '__main__':
                    # Since the __main__ module can't be reloaded, the module has 
                    # to be imported using its file name.                    
                    name = main_module_name()
                return name
                
            mapping_name = utils.dictfind(fvars, mapping)
            module_name = modname(fvars)
            
            def reload_mapping():
                """loadhook to reload mapping and fvars."""
                mod = __import__(module_name)
                mapping = getattr(mod, mapping_name, None)
                if mapping:
                    self.fvars = mod.__dict__
                    self.mapping = mapping

            self.add_processor(loadhook(Reloader()))
            if mapping_name and module_name:
                self.add_processor(loadhook(reload_mapping))

            # load __main__ module usings its filename, so that it can be reloaded.
            if main_module_name() and '__main__' in sys.argv:
                try:
                    __import__(main_module_name())
                except ImportError:
                    pass
                    
    def _load(self):
        web.ctx.app_stack.append(self)
        
    def _unload(self):
        web.ctx.app_stack = web.ctx.app_stack[:-1]
        
        if web.ctx.app_stack:
            # this is a sub-application, revert ctx to earlier state.
            oldctx = web.ctx.get('_oldctx')
            if oldctx:
                web.ctx.home = oldctx.home
                web.ctx.homepath = oldctx.homepath
                web.ctx.path = oldctx.path
                web.ctx.fullpath = oldctx.fullpath
                
    def _cleanup(self):
        #@@@
        # Since the CherryPy Webserver uses thread pool, the thread-local state is never cleared.
        # This interferes with the other requests. 
        # clearing the thread-local storage to avoid that.
        # see utils.ThreadedDict for details
        import threading
        t = threading.currentThread()
        if hasattr(t, '_d'):
            del t._d
    
    def add_mapping(self, pattern, classname):
        self.mapping += (pattern, classname)
        
    def add_processor(self, processor):
        """
        Adds a processor to the application. 
        
            >>> urls = ("/(.*)", "echo")
            >>> app = application(urls, globals())
            >>> class echo:
            ...     def GET(self, name): return name
            ...
            >>>
            >>> def hello(handler): return "hello, " +  handler()
            ...
            >>> app.add_processor(hello)
            >>> app.request("/web.py").data
            'hello, web.py'
        """
        self.processors.append(processor)

    def request(self, localpart='/', method='GET', data=None,
                host="0.0.0.0:8080", headers=None, https=False, **kw):
        """Makes request to this application for the specified path and method.
        Response will be a storage object with data, status and headers.

            >>> urls = ("/hello", "hello")
            >>> app = application(urls, globals())
            >>> class hello:
            ...     def GET(self): 
            ...         web.header('Content-Type', 'text/plain')
            ...         return "hello"
            ...
            >>> response = app.request("/hello")
            >>> response.data
            'hello'
            >>> response.status
            '200 OK'
            >>> response.headers['Content-Type']
            'text/plain'

        To use https, use https=True.

            >>> urls = ("/redirect", "redirect")
            >>> app = application(urls, globals())
            >>> class redirect:
            ...     def GET(self): raise web.seeother("/foo")
            ...
            >>> response = app.request("/redirect")
            >>> response.headers['Location']
            'http://0.0.0.0:8080/foo'
            >>> response = app.request("/redirect", https=True)
            >>> response.headers['Location']
            'https://0.0.0.0:8080/foo'

        The headers argument specifies HTTP headers as a mapping object
        such as a dict.

            >>> urls = ('/ua', 'uaprinter')
            >>> class uaprinter:
            ...     def GET(self):
            ...         return 'your user-agent is ' + web.ctx.env['HTTP_USER_AGENT']
            ... 
            >>> app = application(urls, globals())
            >>> app.request('/ua', headers = {
            ...      'User-Agent': 'a small jumping bean/1.0 (compatible)'
            ... }).data
            'your user-agent is a small jumping bean/1.0 (compatible)'

        """
        path, maybe_query = urllib.splitquery(localpart)
        query = maybe_query or ""
        
        if 'env' in kw:
            env = kw['env']
        else:
            env = {}
        env = dict(env, HTTP_HOST=host, REQUEST_METHOD=method, PATH_INFO=path, QUERY_STRING=query, HTTPS=str(https))
        headers = headers or {}

        for k, v in headers.items():
            env['HTTP_' + k.upper().replace('-', '_')] = v

        if 'HTTP_CONTENT_LENGTH' in env:
            env['CONTENT_LENGTH'] = env.pop('HTTP_CONTENT_LENGTH')

        if 'HTTP_CONTENT_TYPE' in env:
            env['CONTENT_TYPE'] = env.pop('HTTP_CONTENT_TYPE')

        if method in ["POST", "PUT"]:
            data = data or ''
            import StringIO
            if isinstance(data, dict):
                q = urllib.urlencode(data)
            else:
                q = data
            env['wsgi.input'] = StringIO.StringIO(q)
            if not env.get('CONTENT_TYPE', '').lower().startswith('multipart/') and 'CONTENT_LENGTH' not in env:
                env['CONTENT_LENGTH'] = len(q)
        response = web.storage()
        def start_response(status, headers):
            response.status = status
            response.headers = dict(headers)
            response.header_items = headers
        response.data = "".join(self.wsgifunc()(env, start_response))
        return response

    def browser(self):
        import browser
        return browser.AppBrowser(self)

    def handle(self):
        fn, args = self._match(self.mapping, web.ctx.path)
        return self._delegate(fn, self.fvars, args)
        
    def handle_with_processors(self):
        def process(processors):
            try:
                if processors:
                    p, processors = processors[0], processors[1:]
                    return p(lambda: process(processors))
                else:
                    return self.handle()
            except web.HTTPError:
                raise
            except (KeyboardInterrupt, SystemExit):
                raise
            except:
                print >> web.debug, traceback.format_exc()
                raise self.internalerror()
        
        # processors must be applied in the resvere order. (??)
        return process(self.processors)
                        
    def wsgifunc(self, *middleware):
        """Returns a WSGI-compatible function for this application."""
        def peep(iterator):
            """Peeps into an iterator by doing an iteration
            and returns an equivalent iterator.
            """
            # wsgi requires the headers first
            # so we need to do an iteration
            # and save the result for later
            try:
                firstchunk = iterator.next()
            except StopIteration:
                firstchunk = ''

            return itertools.chain([firstchunk], iterator)    
                                
        def is_generator(x): return x and hasattr(x, 'next')
        
        def wsgi(env, start_resp):
            self.load(env)
            try:
                # allow uppercase methods only
                if web.ctx.method.upper() != web.ctx.method:
                    raise web.nomethod()

                result = self.handle_with_processors()
                if is_generator(result):
                    result = peep(result)
                else:
                    result = [result]
            except web.HTTPError, e:
                result = [e.data]

            result = web.utf8(iter(result))

            status, headers = web.ctx.status, web.ctx.headers
            start_resp(status, headers)
            
            def cleanup():
                self._cleanup()
                yield '' # force this function to be a generator
                            
            return itertools.chain(result, cleanup())

        for m in middleware: 
            wsgi = m(wsgi)

        return wsgi

    def run(self, *middleware):
        """
        Starts handling requests. If called in a CGI or FastCGI context, it will follow
        that protocol. If called from the command line, it will start an HTTP
        server on the port named in the first command line argument, or, if there
        is no argument, on port 8080.
        
        `middleware` is a list of WSGI middleware which is applied to the resulting WSGI
        function.
        """
        return wsgi.runwsgi(self.wsgifunc(*middleware))
    
    def cgirun(self, *middleware):
        """
        Return a CGI handler. This is mostly useful with Google App Engine.
        There you can just do:
        
            main = app.cgirun()
        """
        wsgiapp = self.wsgifunc(*middleware)

        try:
            from google.appengine.ext.webapp.util import run_wsgi_app
            return run_wsgi_app(wsgiapp)
        except ImportError:
            # we're not running from within Google App Engine
            return wsgiref.handlers.CGIHandler().run(wsgiapp)
    
    def load(self, env):
        """Initializes ctx using env."""
        ctx = web.ctx
        ctx.clear()
        ctx.status = '200 OK'
        ctx.headers = []
        ctx.output = ''
        ctx.environ = ctx.env = env
        ctx.host = env.get('HTTP_HOST')

        if env.get('wsgi.url_scheme') in ['http', 'https']:
            ctx.protocol = env['wsgi.url_scheme']
        elif env.get('HTTPS', '').lower() in ['on', 'true', '1']:
            ctx.protocol = 'https'
        else:
            ctx.protocol = 'http'
        ctx.homedomain = ctx.protocol + '://' + env.get('HTTP_HOST', '[unknown]')
        ctx.homepath = os.environ.get('REAL_SCRIPT_NAME', env.get('SCRIPT_NAME', ''))
        ctx.home = ctx.homedomain + ctx.homepath
        #@@ home is changed when the request is handled to a sub-application.
        #@@ but the real home is required for doing absolute redirects.
        ctx.realhome = ctx.home
        ctx.ip = env.get('REMOTE_ADDR')
        ctx.method = env.get('REQUEST_METHOD')
        ctx.path = env.get('PATH_INFO')
        # http://trac.lighttpd.net/trac/ticket/406 requires:
        if env.get('SERVER_SOFTWARE', '').startswith('lighttpd/'):
            ctx.path = lstrips(env.get('REQUEST_URI').split('?')[0], ctx.homepath)
            # Apache and CherryPy webservers unquote the url but lighttpd doesn't. 
            # unquote explicitly for lighttpd to make ctx.path uniform across all servers.
            ctx.path = urllib.unquote(ctx.path)

        if env.get('QUERY_STRING'):
            ctx.query = '?' + env.get('QUERY_STRING', '')
        else:
            ctx.query = ''

        ctx.fullpath = ctx.path + ctx.query
        
        for k, v in ctx.iteritems():
            if isinstance(v, str):
                ctx[k] = safeunicode(v)

        # status must always be str
        ctx.status = '200 OK'
        
        ctx.app_stack = []

    def _delegate(self, f, fvars, args=[]):
        def handle_class(cls):
            meth = web.ctx.method
            if meth == 'HEAD' and not hasattr(cls, meth):
                meth = 'GET'
            if not hasattr(cls, meth):
                raise web.nomethod(cls)
            tocall = getattr(cls(), meth)
            return tocall(*args)
            
        def is_class(o): return isinstance(o, (types.ClassType, type))
            
        if f is None:
            raise web.notfound()
        elif isinstance(f, application):
            return f.handle_with_processors()
        elif is_class(f):
            return handle_class(f)
        elif isinstance(f, basestring):
            if f.startswith('redirect '):
                url = f.split(' ', 1)[1]
                if web.ctx.method == "GET":
                    x = web.ctx.env.get('QUERY_STRING', '')
                    if x:
                        url += '?' + x
                raise web.redirect(url)
            elif '.' in f:
                x = f.split('.')
                mod, cls = '.'.join(x[:-1]), x[-1]
                mod = __import__(mod, globals(), locals(), [""])
                cls = getattr(mod, cls)
            else:
                cls = fvars[f]
            return handle_class(cls)
        elif hasattr(f, '__call__'):
            return f()
        else:
            return web.notfound()

    def _match(self, mapping, value):
        for pat, what in utils.group(mapping, 2):
            if isinstance(what, application):
                if value.startswith(pat):
                    f = lambda: self._delegate_sub_application(pat, what)
                    return f, None
                else:
                    continue
            elif isinstance(what, basestring):
                what, result = utils.re_subm('^' + pat + '$', what, value)
            else:
                result = utils.re_compile('^' + pat + '$').match(value)
                
            if result: # it's a match
                return what, [x for x in result.groups()]
        return None, None
        
    def _delegate_sub_application(self, dir, app):
        """Deletes request to sub application `app` rooted at the directory `dir`.
        The home, homepath, path and fullpath values in web.ctx are updated to mimic request
        to the subapp and are restored after it is handled. 
        
        @@Any issues with when used with yield?
        """
        web.ctx._oldctx = web.storage(web.ctx)
        web.ctx.home += dir
        web.ctx.homepath += dir
        web.ctx.path = web.ctx.path[len(dir):]
        web.ctx.fullpath = web.ctx.fullpath[len(dir):]
        return app.handle_with_processors()
            
    def get_parent_app(self):
        if self in web.ctx.app_stack:
            index = web.ctx.app_stack.index(self)
            if index > 0:
                return web.ctx.app_stack[index-1]
        
    def notfound(self):
        """Returns HTTPError with '404 not found' message"""
        parent = self.get_parent_app()
        if parent:
            return parent.notfound()
        else:
            return web._NotFound()
            
    def internalerror(self):
        """Returns HTTPError with '500 internal error' message"""
        parent = self.get_parent_app()
        if parent:
            return parent.internalerror()
        elif web.config.get('debug'):
            import debugerror
            return debugerror.debugerror()
        else:
            return web._InternalError()

class auto_application(application):
    """Application similar to `application` but urls are constructed 
    automatiacally using metaclass.

        >>> app = auto_application()
        >>> class hello(app.page):
        ...     def GET(self): return "hello, world"
        ...
        >>> class foo(app.page):
        ...     path = '/foo/.*'
        ...     def GET(self): return "foo"
        >>> app.request("/hello").data
        'hello, world'
        >>> app.request('/foo/bar').data
        'foo'
    """
    def __init__(self):
        application.__init__(self)

        class metapage(type):
            def __init__(klass, name, bases, attrs):
                type.__init__(klass, name, bases, attrs)
                path = attrs.get('path', '/' + name)

                # path can be specified as None to ignore that class
                # typically required to create a abstract base class.
                if path is not None:
                    self.add_mapping(path, klass)

        class page:
            path = None
            __metaclass__ = metapage

        self.page = page

# The application class already has the required functionality of subdir_application
subdir_application = application
                
class subdomain_application(application):
    """
    Application to delegate requests based on the host.

        >>> urls = ("/hello", "hello")
        >>> app = application(urls, globals())
        >>> class hello:
        ...     def GET(self): return "hello"
        >>>
        >>> mapping = (r"hello\.example\.com", app)
        >>> app2 = subdomain_application(mapping)
        >>> app2.request("/hello", host="hello.example.com").data
        'hello'
        >>> response = app2.request("/hello", host="something.example.com")
        >>> response.status
        '404 Not Found'
        >>> response.data
        'not found'
    """
    def handle(self):
        host = web.ctx.host.split(':')[0] #strip port
        fn, args = self._match(self.mapping, host)
        return self._delegate(fn, self.fvars, args)
        
    def _match(self, mapping, value):
        for pat, what in utils.group(mapping, 2):
            if isinstance(what, basestring):
                what, result = utils.re_subm('^' + pat + '$', what, value)
            else:
                result = utils.re_compile('^' + pat + '$').match(value)

            if result: # it's a match
                return what, [x for x in result.groups()]
        return None, None
        
def loadhook(h):
    """
    Converts a load hook into an application processor.
    
        >>> app = auto_application()
        >>> def f(): "something done before handling request"
        ...
        >>> app.add_processor(loadhook(f))
    """
    def processor(handler):
        h()
        return handler()
        
    return processor
    
def unloadhook(h):
    """
    Converts an unload hook into an application processor.
    
        >>> app = auto_application()
        >>> def f(): "something done after handling request"
        ...
        >>> app.add_processor(unloadhook(f))    
    """
    def processor(handler):
        try:
            result = handler()
            is_generator = result and hasattr(result, 'next')
        except:
            # run the hook even when handler raises some exception
            h()
            raise

        if is_generator:
            return wrap(result)
        else:
            h()
            return result
            
    def wrap(result):
        def next():
            try:
                return result.next()
            except:
                # call the hook at the and of iterator
                h()
                raise

        result = iter(result)
        while True:
            yield next()
            
    return processor

def autodelegate(prefix=''):
    """
    Returns a method that takes one argument and calls the method named prefix+arg,
    calling `notfound()` if there isn't one. Example:

        urls = ('/prefs/(.*)', 'prefs')

        class prefs:
            GET = autodelegate('GET_')
            def GET_password(self): pass
            def GET_privacy(self): pass

    `GET_password` would get called for `/prefs/password` while `GET_privacy` for 
    `GET_privacy` gets called for `/prefs/privacy`.
    
    If a user visits `/prefs/password/change` then `GET_password(self, '/change')`
    is called.
    """
    def internal(self, arg):
        if '/' in arg:
            first, rest = arg.split('/', 1)
            func = prefix + first
            args = ['/' + rest]
        else:
            func = prefix + arg
            args = []
        
        if hasattr(self, func):
            try:
                return getattr(self, func)(*args)
            except TypeError:
                return web.notfound()
        else:
            return web.notfound()
    return internal

class Reloader:
    """Checks to see if any loaded modules have changed on disk and, 
    if so, reloads them.
    """
    def __init__(self):
        self.mtimes = {}

    def __call__(self):
        for mod in sys.modules.values():
            self.check(mod)
            
    def check(self, mod):
        try: 
            mtime = os.stat(mod.__file__).st_mtime
        except (AttributeError, OSError, IOError):
            return
        if mod.__file__.endswith('.pyc') and os.path.exists(mod.__file__[:-1]):
            mtime = max(os.stat(mod.__file__[:-1]).st_mtime, mtime)
            
        if mod not in self.mtimes:
            self.mtimes[mod] = mtime
        elif self.mtimes[mod] < mtime:
            try: 
                reload(mod)
                self.mtimes[mod] = mtime
            except ImportError: 
                pass
                
if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = browser
"""Browser to test web applications.
(from web.py)
"""
from utils import re_compile
from net import htmlunquote

import httplib, urllib, urllib2
import copy
from StringIO import StringIO

DEBUG = False

__all__ = [
    "BrowserError",
    "Browser", "AppBrowser",
    "AppHandler"
]

class BrowserError(Exception):
    pass

class Browser:
    def __init__(self):
        import cookielib
        self.cookiejar = cookielib.CookieJar()
        self._cookie_processor = urllib2.HTTPCookieProcessor(self.cookiejar)
        self.form = None

        self.url = "http://0.0.0.0:8080/"
        self.path = "/"
        
        self.status = None
        self.data = None
        self._response = None
        self._forms = None

    def reset(self):
        """Clears all cookies and history."""
        self.cookiejar.clear()

    def build_opener(self):
        """Builds the opener using urllib2.build_opener. 
        Subclasses can override this function to prodive custom openers.
        """
        return urllib2.build_opener()

    def do_request(self, req):
        if DEBUG:
            print 'requesting', req.get_method(), req.get_full_url()
        opener = self.build_opener()
        opener.add_handler(self._cookie_processor)
        try:
            self._response = opener.open(req)
        except urllib2.HTTPError, e:
            self._response = e

        self.url = self._response.geturl()
        self.path = urllib2.Request(self.url).get_selector()
        self.data = self._response.read()
        self.status = self._response.code
        self._forms = None
        self.form = None
        return self.get_response()

    def open(self, url, data=None, headers={}):
        """Opens the specified url."""
        url = urllib.basejoin(self.url, url)
        req = urllib2.Request(url, data, headers)
        return self.do_request(req)

    def show(self):
        """Opens the current page in real web browser."""
        f = open('page.html', 'w')
        f.write(self.data)
        f.close()

        import webbrowser, os
        url = 'file://' + os.path.abspath('page.html')
        webbrowser.open(url)

    def get_response(self):
        """Returns a copy of the current response."""
        return urllib.addinfourl(StringIO(self.data), self._response.info(), self._response.geturl())

    def get_soup(self):
        """Returns beautiful soup of the current document."""
        import BeautifulSoup
        return BeautifulSoup.BeautifulSoup(self.data)

    def get_text(self, e=None):
        """Returns content of e or the current document as plain text."""
        e = e or self.get_soup()
        return ''.join([htmlunquote(c) for c in e.recursiveChildGenerator() if isinstance(c, unicode)])

    def _get_links(self):
        soup = self.get_soup()
        return [a for a in soup.findAll(name='a')]
        
    def get_links(self, text=None, text_regex=None, url=None, url_regex=None, predicate=None):
        """Returns all links in the document."""
        return self._filter_links(self._get_links(),
            text=text, text_regex=text_regex, url=url, url_regex=url_regex, predicate=predicate)

    def follow_link(self, link=None, text=None, text_regex=None, url=None, url_regex=None, predicate=None):
        if link is None:
            links = self._filter_links(self.get_links(),
                text=text, text_regex=text_regex, url=url, url_regex=url_regex, predicate=predicate)
            link = links and links[0]
            
        if link:
            return self.open(link['href'])
        else:
            raise BrowserError("No link found")
            
    def find_link(self, text=None, text_regex=None, url=None, url_regex=None, predicate=None):
        links = self._filter_links(self.get_links(), 
            text=text, text_regex=text_regex, url=url, url_regex=url_regex, predicate=predicate)
        return links and links[0] or None
            
    def _filter_links(self, links, 
            text=None, text_regex=None,
            url=None, url_regex=None,
            predicate=None):
        predicates = []
        if text is not None:
            predicates.append(lambda link: link.string == text)
        if text_regex is not None:
            predicates.append(lambda link: re_compile(text_regex).search(link.string or ''))
        if url is not None:
            predicates.append(lambda link: link.get('href') == url)
        if url_regex is not None:
            predicates.append(lambda link: re_compile(url_regex).search(link.get('href', '')))
        if predicate:
            predicate.append(predicate)

        def f(link):
            for p in predicates:
                if not p(link):
                    return False
            return True

        return [link for link in links if f(link)]

    def get_forms(self):
        """Returns all forms in the current document.
        The returned form objects implement the ClientForm.HTMLForm interface.
        """
        if self._forms is None:
            import ClientForm
            self._forms = ClientForm.ParseResponse(self.get_response(), backwards_compat=False)
        return self._forms

    def select_form(self, name=None, predicate=None, index=0):
        """Selects the specified form."""
        forms = self.get_forms()

        if name is not None:
            forms = [f for f in forms if f.name == name]
        if predicate:
            forms = [f for f in forms if predicate(f)]
            
        if forms:
            self.form = forms[index]
            return self.form
        else:
            raise BrowserError("No form selected.")
        
    def submit(self, **kw):
        """submits the currently selected form."""
        if self.form is None:
            raise BrowserError("No form selected.")
        req = self.form.click(**kw)
        return self.do_request(req)

    def __getitem__(self, key):
        return self.form[key]

    def __setitem__(self, key, value):
        self.form[key] = value

class AppBrowser(Browser):
    """Browser interface to test web.py apps.
    
        b = AppBrowser(app)
        b.open('/')
        b.follow_link(text='Login')
        
        b.select_form(name='login')
        b['username'] = 'joe'
        b['password'] = 'secret'
        b.submit()

        assert b.path == '/'
        assert 'Welcome joe' in b.get_text()
    """
    def __init__(self, app):
        Browser.__init__(self)
        self.app = app

    def build_opener(self):
        return urllib2.build_opener(AppHandler(self.app))

class AppHandler(urllib2.HTTPHandler):
    """urllib2 handler to handle requests using web.py application."""
    handler_order = 100

    def __init__(self, app):
        self.app = app

    def http_open(self, req):
        result = self.app.request(
            localpart=req.get_selector(),
            method=req.get_method(),
            host=req.get_host(),
            data=req.get_data(),
            headers=dict(req.header_items()),
            https=req.get_type() == "https"
        )
        return self._make_response(result, req.get_full_url())

    def https_open(self, req):
        return self.http_open(req)
    
    try:
        https_request = urllib2.HTTPHandler.do_request_
    except AttributeError:
        # for python 2.3
        pass

    def _make_response(self, result, url):
        data = "\r\n".join(["%s: %s" % (k, v) for k, v in result.header_items])
        headers = httplib.HTTPMessage(StringIO(data))
        response = urllib.addinfourl(StringIO(result.data), headers, url)
        code, msg = result.status.split(None, 1)
        response.code, response.msg = int(code), msg
        return response

########NEW FILE########
__FILENAME__ = template
"""
Interface to various templating engines.
"""
import os.path

__all__ = [
    "render_cheetah", "render_genshi", "render_mako",
    "cache", 
]

class render_cheetah:
    """Rendering interface to Cheetah Templates.

    Example:

        render = render_cheetah('templates')
        render.hello(name="cheetah")
    """
    def __init__(self, path):
        # give error if Chetah is not installed
        from Cheetah.Template import Template
        self.path = path

    def __getattr__(self, name):
        from Cheetah.Template import Template
        path = os.path.join(self.path, name + ".html")
        
        def template(**kw):
            t = Template(file=path, searchList=[kw])
            return t.respond()

        return template
    
class render_genshi:
    """Rendering interface genshi templates.
    Example:

    for xml/html templates.

        render = render_genshi(['templates/'])
        render.hello(name='genshi')

    For text templates:

        render = render_genshi(['templates/'], type='text')
        render.hello(name='genshi')
    """

    def __init__(self, *a, **kwargs):
        from genshi.template import TemplateLoader

        self._type = kwargs.pop('type', None)
        self._loader = TemplateLoader(*a, **kwargs)

    def __getattr__(self, name):
        # Assuming all templates are html
        path = name + ".html"

        if self._type == "text":
            from genshi.template import TextTemplate
            cls = TextTemplate
            type = "text"
        else:
            cls = None
            type = None

        t = self._loader.load(path, cls=cls)
        def template(**kw):
            stream = t.generate(**kw)
            if type:
                return stream.render(type)
            else:
                return stream.render()
        return template

class render_jinja:
    """Rendering interface to Jinja2 Templates
    
    Example:

        render= render_jinja('templates')
        render.hello(name='jinja2')
    """
    def __init__(self, *a, **kwargs):
        extensions = kwargs.pop('extensions', [])
        globals = kwargs.pop('globals', {})

        from jinja2 import Environment,FileSystemLoader
        self._lookup = Environment(loader=FileSystemLoader(*a, **kwargs), extensions=extensions)
        self._lookup.globals.update(globals)
        
    def __getattr__(self, name):
        # Assuming all templates end with .html
        path = name + '.html'
        t = self._lookup.get_template(path)
        return t.render
        
class render_mako:
    """Rendering interface to Mako Templates.

    Example:

        render = render_mako(directories=['templates'])
        render.hello(name="mako")
    """
    def __init__(self, *a, **kwargs):
        from mako.lookup import TemplateLookup
        self._lookup = TemplateLookup(*a, **kwargs)

    def __getattr__(self, name):
        # Assuming all templates are html
        path = name + ".html"
        t = self._lookup.get_template(path)
        return t.render

class cache:
    """Cache for any rendering interface.
    
    Example:

        render = cache(render_cheetah("templates/"))
        render.hello(name='cache')
    """
    def __init__(self, render):
        self._render = render
        self._cache = {}

    def __getattr__(self, name):
        if name not in self._cache:
            self._cache[name] = getattr(self._render, name)
        return self._cache[name]

########NEW FILE########
__FILENAME__ = db
"""
Database API
(part of web.py)
"""

__all__ = [
  "UnknownParamstyle", "UnknownDB", "TransactionError", 
  "sqllist", "sqlors", "reparam", "sqlquote",
  "SQLQuery", "SQLParam", "sqlparam",
  "SQLLiteral", "sqlliteral",
  "database", 'DB',
]

import time
try:
    import datetime
except ImportError:
    datetime = None

from utils import threadeddict, storage, iters, iterbetter

try:
    # db module can work independent of web.py
    from webapi import debug, config
except:
    import sys
    debug = sys.stderr
    config = storage()

class UnknownDB(Exception):
    """raised for unsupported dbms"""
    pass

class _ItplError(ValueError): 
    def __init__(self, text, pos):
        ValueError.__init__(self)
        self.text = text
        self.pos = pos
    def __str__(self):
        return "unfinished expression in %s at char %d" % (
            repr(self.text), self.pos)

class TransactionError(Exception): pass

class UnknownParamstyle(Exception): 
    """
    raised for unsupported db paramstyles

    (currently supported: qmark, numeric, format, pyformat)
    """
    pass
    
class SQLParam:
    """
    Parameter in SQLQuery.
    
        >>> q = SQLQuery(["SELECT * FROM test WHERE name=", SQLParam("joe")])
        >>> q
        <sql: "SELECT * FROM test WHERE name='joe'">
        >>> q.query()
        'SELECT * FROM test WHERE name=%s'
        >>> q.values()
        ['joe']
    """
    def __init__(self, value):
        self.value = value
        
    def get_marker(self, paramstyle='pyformat'):
        if paramstyle == 'qmark':
            return '?'
        elif paramstyle == 'numeric':
            return ':1'
        elif paramstyle is None or paramstyle in ['format', 'pyformat']:
            return '%s'
        raise UnknownParamstyle, paramstyle
        
    def sqlquery(self): 
        return SQLQuery([self])
        
    def __add__(self, other):
        return self.sqlquery() + other
        
    def __radd__(self, other):
        return other + self.sqlquery() 
            
    def __str__(self): 
        return str(self.value)
    
    def __repr__(self):
        return '<param: %s>' % repr(self.value)

sqlparam =  SQLParam

class SQLQuery:
    """
    You can pass this sort of thing as a clause in any db function.
    Otherwise, you can pass a dictionary to the keyword argument `vars`
    and the function will call reparam for you.

    Internally, consists of `items`, which is a list of strings and
    SQLParams, which get concatenated to produce the actual query.
    """
    # tested in sqlquote's docstring
    def __init__(self, items=[]):
        """Creates a new SQLQuery.
        
            >>> SQLQuery("x")
            <sql: 'x'>
            >>> q = SQLQuery(['SELECT * FROM ', 'test', ' WHERE x=', SQLParam(1)])
            >>> q
            <sql: 'SELECT * FROM test WHERE x=1'>
            >>> q.query(), q.values()
            ('SELECT * FROM test WHERE x=%s', [1])
            >>> SQLQuery(SQLParam(1))
            <sql: '1'>
        """
        if isinstance(items, list):
            self.items = items
        elif isinstance(items, SQLParam):
            self.items = [items]
        elif isinstance(items, SQLQuery):
            self.items = list(items.items)
        else:
            self.items = [str(items)]
            
        # Take care of SQLLiterals
        for i, item in enumerate(self.items):
            if isinstance(item, SQLParam) and isinstance(item.value, SQLLiteral):
                self.items[i] = item.value.v

    def __add__(self, other):
        if isinstance(other, basestring):
            items = [other]
        elif isinstance(other, SQLQuery):
            items = other.items
        else:
            return NotImplemented
        return SQLQuery(self.items + items)

    def __radd__(self, other):
        if isinstance(other, basestring):
            items = [other]
        else:
            return NotImplemented
            
        return SQLQuery(items + self.items)

    def __iadd__(self, other):
        if isinstance(other, basestring):
            items = [other]
        elif isinstance(other, SQLQuery):
            items = other.items
        else:
            return NotImplemented
        self.items.extend(items)
        return self

    def __len__(self):
        return len(self.query())
        
    def query(self, paramstyle=None):
        """
        Returns the query part of the sql query.
            >>> q = SQLQuery(["SELECT * FROM test WHERE name=", SQLParam('joe')])
            >>> q.query()
            'SELECT * FROM test WHERE name=%s'
            >>> q.query(paramstyle='qmark')
            'SELECT * FROM test WHERE name=?'
        """
        s = ''
        for x in self.items:
            if isinstance(x, SQLParam):
                x = x.get_marker(paramstyle)
            s += x
        return s
    
    def values(self):
        """
        Returns the values of the parameters used in the sql query.
            >>> q = SQLQuery(["SELECT * FROM test WHERE name=", SQLParam('joe')])
            >>> q.values()
            ['joe']
        """
        return [i.value for i in self.items if isinstance(i, SQLParam)]
        
    def join(items, sep=' '):
        """
        Joins multiple queries.
        
        >>> SQLQuery.join(['a', 'b'], ', ')
        <sql: 'a, b'>
        """
        if len(items) == 0:
            return SQLQuery("")

        q = SQLQuery(items[0])
        for item in items[1:]:
            q += sep
            q += item
        return q
    
    join = staticmethod(join)

    def __str__(self):
        try:
            return self.query() % tuple([sqlify(x) for x in self.values()])
        except (ValueError, TypeError):
            return self.query()

    def __repr__(self):
        return '<sql: %s>' % repr(str(self))

class SQLLiteral: 
    """
    Protects a string from `sqlquote`.

        >>> sqlquote('NOW()')
        <sql: "'NOW()'">
        >>> sqlquote(SQLLiteral('NOW()'))
        <sql: 'NOW()'>
    """
    def __init__(self, v): 
        self.v = v

    def __repr__(self): 
        return self.v

sqlliteral = SQLLiteral

def _sqllist(values):
    """
        >>> _sqllist([1, 2, 3])
        <sql: '(1, 2, 3)'>
    """
    items = []
    items.append('(')
    for i, v in enumerate(values):
        if i != 0:
            items.append(', ')
        items.append(sqlparam(v))
    items.append(')')
    return SQLQuery(items)

def reparam(string_, dictionary): 
    """
    Takes a string and a dictionary and interpolates the string
    using values from the dictionary. Returns an `SQLQuery` for the result.

        >>> reparam("s = $s", dict(s=True))
        <sql: "s = 't'">
        >>> reparam("s IN $s", dict(s=[1, 2]))
        <sql: 's IN (1, 2)'>
    """
    dictionary = dictionary.copy() # eval mucks with it
    vals = []
    result = []
    for live, chunk in _interpolate(string_):
        if live:
            v = eval(chunk, dictionary)
            result.append(sqlquote(v))
        else: 
            result.append(chunk)
    return SQLQuery.join(result, '')

def sqlify(obj): 
    """
    converts `obj` to its proper SQL version

        >>> sqlify(None)
        'NULL'
        >>> sqlify(True)
        "'t'"
        >>> sqlify(3)
        '3'
    """
    # because `1 == True and hash(1) == hash(True)`
    # we have to do this the hard way...

    if obj is None:
        return 'NULL'
    elif obj is True:
        return "'t'"
    elif obj is False:
        return "'f'"
    elif datetime and isinstance(obj, datetime.datetime):
        return repr(obj.isoformat())
    else:
        return repr(obj)

def sqllist(lst): 
    """
    Converts the arguments for use in something like a WHERE clause.
    
        >>> sqllist(['a', 'b'])
        'a, b'
        >>> sqllist('a')
        'a'
        >>> sqllist(u'abc')
        u'abc'
    """
    if isinstance(lst, basestring): 
        return lst
    else:
        return ', '.join(lst)

def sqlors(left, lst):
    """
    `left is a SQL clause like `tablename.arg = ` 
    and `lst` is a list of values. Returns a reparam-style
    pair featuring the SQL that ORs together the clause
    for each item in the lst.

        >>> sqlors('foo = ', [])
        <sql: '1=2'>
        >>> sqlors('foo = ', [1])
        <sql: 'foo = 1'>
        >>> sqlors('foo = ', 1)
        <sql: 'foo = 1'>
        >>> sqlors('foo = ', [1,2,3])
        <sql: '(foo = 1 OR foo = 2 OR foo = 3 OR 1=2)'>
    """
    if isinstance(lst, iters):
        lst = list(lst)
        ln = len(lst)
        if ln == 0:
            return SQLQuery("1=2")
        if ln == 1:
            lst = lst[0]

    if isinstance(lst, iters):
        return SQLQuery(['('] + 
          sum([[left, sqlparam(x), ' OR '] for x in lst], []) +
          ['1=2)']
        )
    else:
        return left + sqlparam(lst)
        
def sqlwhere(dictionary, grouping=' AND '): 
    """
    Converts a `dictionary` to an SQL WHERE clause `SQLQuery`.
    
        >>> sqlwhere({'cust_id': 2, 'order_id':3})
        <sql: 'order_id = 3 AND cust_id = 2'>
        >>> sqlwhere({'cust_id': 2, 'order_id':3}, grouping=', ')
        <sql: 'order_id = 3, cust_id = 2'>
        >>> sqlwhere({'a': 'a', 'b': 'b'}).query()
        'a = %s AND b = %s'
    """
    return SQLQuery.join([k + ' = ' + sqlparam(v) for k, v in dictionary.items()], grouping)

def sqlquote(a): 
    """
    Ensures `a` is quoted properly for use in a SQL query.

        >>> 'WHERE x = ' + sqlquote(True) + ' AND y = ' + sqlquote(3)
        <sql: "WHERE x = 't' AND y = 3">
        >>> 'WHERE x = ' + sqlquote(True) + ' AND y IN ' + sqlquote([2, 3])
        <sql: "WHERE x = 't' AND y IN (2, 3)">
    """
    if isinstance(a, list):
        return _sqllist(a)
    else:
        return sqlparam(a).sqlquery()

class Transaction:
    """Database transaction."""
    def __init__(self, ctx):
        self.ctx = ctx
        self.transaction_count = transaction_count = len(ctx.transactions)

        class transaction_engine:
            """Transaction Engine used in top level transactions."""
            def do_transact(self):
                ctx.commit(unload=False)

            def do_commit(self):
                ctx.commit()

            def do_rollback(self):
                ctx.rollback()

        class subtransaction_engine:
            """Transaction Engine used in sub transactions."""
            def query(self, q):
                db_cursor = ctx.db.cursor()
                ctx.db_execute(db_cursor, SQLQuery(q % transaction_count))

            def do_transact(self):
                self.query('SAVEPOINT webpy_sp_%s')

            def do_commit(self):
                self.query('RELEASE SAVEPOINT webpy_sp_%s')

            def do_rollback(self):
                self.query('ROLLBACK TO SAVEPOINT webpy_sp_%s')

        class dummy_engine:
            """Transaction Engine used instead of subtransaction_engine 
            when sub transactions are not supported."""
            do_transact = do_commit = do_rollback = lambda self: None

        if self.transaction_count:
            # nested transactions are not supported in some databases
            if self.ctx.get('ignore_nested_transactions'):
                self.engine = dummy_engine()
            else:
                self.engine = subtransaction_engine()
        else:
            self.engine = transaction_engine()

        self.engine.do_transact()
        self.ctx.transactions.append(self)

    def __enter__(self):
        return self

    def __exit__(self, exctype, excvalue, traceback):
        if exctype is not None:
            self.rollback()
        else:
            self.commit()

    def commit(self):
        if len(self.ctx.transactions) > self.transaction_count:
            self.engine.do_commit()
            self.ctx.transactions = self.ctx.transactions[:self.transaction_count]

    def rollback(self):
        if len(self.ctx.transactions) > self.transaction_count:
            self.engine.do_rollback()
            self.ctx.transactions = self.ctx.transactions[:self.transaction_count]

class DB: 
    """Database"""
    def __init__(self, db_module, keywords):
        """Creates a database.
        """
        # some DB implementaions take optional paramater `driver` to use a specific driver modue
        # but it should not be passed to connect
        keywords.pop('driver', None)

        self.db_module = db_module
        self.keywords = keywords

        
        self._ctx = threadeddict()
        # flag to enable/disable printing queries
        self.printing = config.get('debug', False)
        self.supports_multiple_insert = False
        
        try:
            import DBUtils
            # enable pooling if DBUtils module is available.
            self.has_pooling = True
        except ImportError:
            self.has_pooling = False
            
        # Pooling can be disabled by passing pooling=False in the keywords.
        self.has_pooling = self.keywords.pop('pooling', True) and self.has_pooling
            
    def _getctx(self): 
        if not self._ctx.get('db'):
            self._load_context(self._ctx)
        return self._ctx
    ctx = property(_getctx)
    
    def _load_context(self, ctx):
        ctx.dbq_count = 0
        ctx.transactions = [] # stack of transactions
        
        if self.has_pooling:
            ctx.db = self._connect_with_pooling(self.keywords)
        else:
            ctx.db = self._connect(self.keywords)
        ctx.db_execute = self._db_execute
        
        if not hasattr(ctx.db, 'commit'):
            ctx.db.commit = lambda: None

        if not hasattr(ctx.db, 'rollback'):
            ctx.db.rollback = lambda: None
            
        def commit(unload=True):
            # do db commit and release the connection if pooling is enabled.            
            ctx.db.commit()
            if unload and self.has_pooling:
                self._unload_context(self._ctx)
                
        def rollback():
            # do db rollback and release the connection if pooling is enabled.
            ctx.db.rollback()
            if self.has_pooling:
                self._unload_context(self._ctx)
                
        ctx.commit = commit
        ctx.rollback = rollback
            
    def _unload_context(self, ctx):
        del ctx.db
            
    def _connect(self, keywords):
        return self.db_module.connect(**keywords)
        
    def _connect_with_pooling(self, keywords):
        def get_pooled_db():
            from DBUtils import PooledDB

            # In DBUtils 0.9.3, `dbapi` argument is renamed as `creator`
            # see Bug#122112
            
            if PooledDB.__version__.split('.') < '0.9.3'.split('.'):
                return PooledDB.PooledDB(dbapi=self.db_module, **keywords)
            else:
                return PooledDB.PooledDB(creator=self.db_module, **keywords)
        
        if getattr(self, '_pooleddb', None) is None:
            self._pooleddb = get_pooled_db()
        
        return self._pooleddb.connection()
        
    def _db_cursor(self):
        return self.ctx.db.cursor()

    def _param_marker(self):
        """Returns parameter marker based on paramstyle attribute if this database."""
        style = getattr(self, 'paramstyle', 'pyformat')

        if style == 'qmark':
            return '?'
        elif style == 'numeric':
            return ':1'
        elif style in ['format', 'pyformat']:
            return '%s'
        raise UnknownParamstyle, style

    def _db_execute(self, cur, sql_query): 
        """executes an sql query"""
        self.ctx.dbq_count += 1
        
        try:
            a = time.time()
            paramstyle = getattr(self, 'paramstyle', 'pyformat')
            out = cur.execute(sql_query.query(paramstyle), sql_query.values())
            b = time.time()
        except:
            if self.printing:
                print >> debug, 'ERR:', str(sql_query)
            if self.ctx.transactions:
                self.ctx.transactions[-1].rollback()
            else:
                self.ctx.rollback()
            raise

        if self.printing:
            print >> debug, '%s (%s): %s' % (round(b-a, 2), self.ctx.dbq_count, str(sql_query))
        return out
    
    def _where(self, where, vars): 
        if isinstance(where, (int, long)):
            where = "id = " + sqlparam(where)
        #@@@ for backward-compatibility
        elif isinstance(where, (list, tuple)) and len(where) == 2:
            where = SQLQuery(where[0], where[1])
        elif isinstance(where, SQLQuery):
            pass
        else:
            where = reparam(where, vars)        
        return where
    
    def query(self, sql_query, vars=None, processed=False, _test=False): 
        """
        Execute SQL query `sql_query` using dictionary `vars` to interpolate it.
        If `processed=True`, `vars` is a `reparam`-style list to use 
        instead of interpolating.
        
            >>> db = DB(None, {})
            >>> db.query("SELECT * FROM foo", _test=True)
            <sql: 'SELECT * FROM foo'>
            >>> db.query("SELECT * FROM foo WHERE x = $x", vars=dict(x='f'), _test=True)
            <sql: "SELECT * FROM foo WHERE x = 'f'">
            >>> db.query("SELECT * FROM foo WHERE x = " + sqlquote('f'), _test=True)
            <sql: "SELECT * FROM foo WHERE x = 'f'">
        """
        if vars is None: vars = {}
        
        if not processed and not isinstance(sql_query, SQLQuery):
            sql_query = reparam(sql_query, vars)
        
        if _test: return sql_query
        
        db_cursor = self._db_cursor()
        self._db_execute(db_cursor, sql_query)
        
        if db_cursor.description:
            names = [x[0] for x in db_cursor.description]
            def iterwrapper():
                row = db_cursor.fetchone()
                while row:
                    yield storage(dict(zip(names, row)))
                    row = db_cursor.fetchone()
            out = iterbetter(iterwrapper())
            out.__len__ = lambda: int(db_cursor.rowcount)
            out.list = lambda: [storage(dict(zip(names, x))) \
                               for x in db_cursor.fetchall()]
        else:
            out = db_cursor.rowcount
        
        if not self.ctx.transactions: 
            self.ctx.commit()
        return out
    
    def select(self, tables, vars=None, what='*', where=None, order=None, group=None, 
               limit=None, offset=None, _test=False): 
        """
        Selects `what` from `tables` with clauses `where`, `order`, 
        `group`, `limit`, and `offset`. Uses vars to interpolate. 
        Otherwise, each clause can be a SQLQuery.
        
            >>> db = DB(None, {})
            >>> db.select('foo', _test=True)
            <sql: 'SELECT * FROM foo'>
            >>> db.select(['foo', 'bar'], where="foo.bar_id = bar.id", limit=5, _test=True)
            <sql: 'SELECT * FROM foo, bar WHERE foo.bar_id = bar.id LIMIT 5'>
        """
        if vars is None: vars = {}
        sql_clauses = self.sql_clauses(what, tables, where, group, order, limit, offset)
        clauses = [self.gen_clause(sql, val, vars) for sql, val in sql_clauses if val is not None]
        qout = SQLQuery.join(clauses)
        if _test: return qout
        return self.query(qout, processed=True)
    
    def where(self, table, what='*', order=None, group=None, limit=None, 
              offset=None, _test=False, **kwargs):
        """
        Selects from `table` where keys are equal to values in `kwargs`.
        
            >>> db = DB(None, {})
            >>> db.where('foo', bar_id=3, _test=True)
            <sql: 'SELECT * FROM foo WHERE bar_id = 3'>
            >>> db.where('foo', source=2, crust='dewey', _test=True)
            <sql: "SELECT * FROM foo WHERE source = 2 AND crust = 'dewey'">
        """
        where = []
        for k, v in kwargs.iteritems():
            where.append(k + ' = ' + sqlquote(v))
        return self.select(table, what=what, order=order, 
               group=group, limit=limit, offset=offset, _test=_test, 
               where=SQLQuery.join(where, ' AND '))
    
    def sql_clauses(self, what, tables, where, group, order, limit, offset): 
        return (
            ('SELECT', what),
            ('FROM', sqllist(tables)),
            ('WHERE', where),
            ('GROUP BY', group),
            ('ORDER BY', order),
            ('LIMIT', limit),
            ('OFFSET', offset))
    
    def gen_clause(self, sql, val, vars): 
        if isinstance(val, (int, long)):
            if sql == 'WHERE':
                nout = 'id = ' + sqlquote(val)
            else:
                nout = SQLQuery(val)
        #@@@
        elif isinstance(val, (list, tuple)) and len(val) == 2:
            nout = SQLQuery(val[0], val[1]) # backwards-compatibility
        elif isinstance(val, SQLQuery):
            nout = val
        else:
            nout = reparam(val, vars)

        def xjoin(a, b):
            if a and b: return a + ' ' + b
            else: return a or b

        return xjoin(sql, nout)

    def insert(self, tablename, seqname=None, _test=False, **values): 
        """
        Inserts `values` into `tablename`. Returns current sequence ID.
        Set `seqname` to the ID if it's not the default, or to `False`
        if there isn't one.
        
            >>> db = DB(None, {})
            >>> q = db.insert('foo', name='bob', age=2, created=SQLLiteral('NOW()'), _test=True)
            >>> q
            <sql: "INSERT INTO foo (age, name, created) VALUES (2, 'bob', NOW())">
            >>> q.query()
            'INSERT INTO foo (age, name, created) VALUES (%s, %s, NOW())'
            >>> q.values()
            [2, 'bob']
        """
        def q(x): return "(" + x + ")"
        
        if values:
            _keys = SQLQuery.join(values.keys(), ', ')
            _values = SQLQuery.join([sqlparam(v) for v in values.values()], ', ')
            sql_query = "INSERT INTO %s " % tablename + q(_keys) + ' VALUES ' + q(_values)
        else:
            sql_query = SQLQuery("INSERT INTO %s DEFAULT VALUES" % tablename)

        if _test: return sql_query
        
        db_cursor = self._db_cursor()
        if seqname is not False:
            sql_query = self._process_insert_query(sql_query, tablename, seqname)

        if isinstance(sql_query, tuple):
            # for some databases, a separate query has to be made to find 
            # the id of the inserted row.
            q1, q2 = sql_query
            self._db_execute(db_cursor, q1)
            self._db_execute(db_cursor, q2)
        else:
            self._db_execute(db_cursor, sql_query)

        try: 
            out = db_cursor.fetchone()[0]
        except Exception: 
            out = None
        
        if not self.ctx.transactions: 
            self.ctx.commit()
        return out
        
    def multiple_insert(self, tablename, values, seqname=None, percall=5000, _test=False):
        """
        Inserts multiple rows into `tablename`. The `values` must be a list of dictioanries, 
        one for each row to be inserted, each with the same set of keys.
        Set `seqname` to the ID if it's not the default, or to `False`
        if there isn't one.
        
            >>> db = DB(None, {})
            >>> db.supports_multiple_insert = True
            >>> values = [{"name": "foo", "email": "foo@example.com"}, {"name": "bar", "email": "bar@example.com"}]
            >>> db.multiple_insert('person', values=values, _test=True)
            <sql: "INSERT INTO person (name, email) VALUES ('foo', 'foo@example.com'), ('bar', 'bar@example.com')">
        """
        doit = lambda x: self._multiple_insert(tablename, x, seqname, _test)
        rows = []
        for row in values:
            rows.append(row)
            if len(rows) >= percall:
                doit(rows)
                rows = []
        doit(rows)
    
    def _multiple_insert(self, tablename, values, seqname=None, _test=False):
        if not values:
            return []
            
        if not self.supports_multiple_insert:
            out = [self.insert(tablename, seqname=seqname, _test=_test, **v) for v in values]
            if seqname is False:
                return None
            else:
                return out
        
        data = []
        keys = None
        count = 0
        for row in values:
            if keys is None:
                keys = row.keys()
                sql_query = SQLQuery('INSERT INTO %s (%s) VALUES ' % (tablename, ', '.join(keys))) 
            elif row.keys() != keys:
                raise ValueError, 'Inconsistent keys'
            
            d = SQLQuery.join([SQLParam(row[k]) for k in keys], ', ')
            data.append('(' + d + ')')
            count += 1
        sql_query += SQLQuery.join(data, ', ')

        if _test: return sql_query

        db_cursor = self._db_cursor()
        if seqname is not False: 
            sql_query = self._process_insert_query(sql_query, tablename, seqname)

        if isinstance(sql_query, tuple):
            # for some databases, a separate query has to be made to find 
            # the id of the inserted row.
            q1, q2 = sql_query
            self._db_execute(db_cursor, q1)
            self._db_execute(db_cursor, q2)
        else:
            self._db_execute(db_cursor, sql_query)

        if not self.ctx.transactions: 
            self.ctx.commit()
    
    def update(self, tables, where, vars=None, _test=False, **values): 
        """
        Update `tables` with clause `where` (interpolated using `vars`)
        and setting `values`.

            >>> db = DB(None, {})
            >>> name = 'Joseph'
            >>> q = db.update('foo', where='name = $name', name='bob', age=2,
            ...     created=SQLLiteral('NOW()'), vars=locals(), _test=True)
            >>> q
            <sql: "UPDATE foo SET age = 2, name = 'bob', created = NOW() WHERE name = 'Joseph'">
            >>> q.query()
            'UPDATE foo SET age = %s, name = %s, created = NOW() WHERE name = %s'
            >>> q.values()
            [2, 'bob', 'Joseph']
        """
        if vars is None: vars = {}
        where = self._where(where, vars)

        query = (
          "UPDATE " + sqllist(tables) + 
          " SET " + sqlwhere(values, ', ') + 
          " WHERE " + where)

        if _test: return query
        
        db_cursor = self._db_cursor()
        self._db_execute(db_cursor, query)
        if not self.ctx.transactions: 
            self.ctx.commit()
        return db_cursor.rowcount
    
    def delete(self, table, where, using=None, vars=None, _test=False): 
        """
        Deletes from `table` with clauses `where` and `using`.

            >>> db = DB(None, {})
            >>> name = 'Joe'
            >>> db.delete('foo', where='name = $name', vars=locals(), _test=True)
            <sql: "DELETE FROM foo WHERE name = 'Joe'">
        """
        if vars is None: vars = {}
        where = self._where(where, vars)

        q = 'DELETE FROM ' + table
        if where: q += ' WHERE ' + where
        if using: q += ' USING ' + sqllist(using)

        if _test: return q

        db_cursor = self._db_cursor()
        self._db_execute(db_cursor, q)
        if not self.ctx.transactions: 
            self.ctx.commit()
        return db_cursor.rowcount

    def _process_insert_query(self, query, tablename, seqname):
        return query

    def transaction(self): 
        """Start a transaction."""
        return Transaction(self.ctx)
    
class PostgresDB(DB): 
    """Postgres driver."""
    def __init__(self, **keywords):
        if 'pw' in keywords:
            keywords['password'] = keywords['pw']
            del keywords['pw']
            
        db_module = import_driver(["psycopg2", "psycopg", "pgdb"], preferred=keywords.pop('driver', None))
        if db_module.__name__ == "psycopg2":
            import psycopg2.extensions
            psycopg2.extensions.register_type(psycopg2.extensions.UNICODE)

        keywords['database'] = keywords.pop('db')
        self.dbname = "postgres"
        self.paramstyle = db_module.paramstyle
        DB.__init__(self, db_module, keywords)
        self.supports_multiple_insert = True
        
    def _process_insert_query(self, query, tablename, seqname):
        if seqname is None: 
            seqname = tablename + "_id_seq"
        return query + "; SELECT currval('%s')" % seqname

    def _connect(self, keywords):
        conn = DB._connect(self, keywords)
        conn.set_client_encoding('UTF8')
        return conn
        
    def _connect_with_pooling(self, keywords):
        conn = DB._connect_with_pooling(self, keywords)
        conn._con._con.set_client_encoding('UTF8')
        return conn

class MySQLDB(DB): 
    def __init__(self, **keywords):
        import MySQLdb as db
        if 'pw' in keywords:
            keywords['passwd'] = keywords['pw']
            del keywords['pw']

        if 'charset' not in keywords:
            keywords['charset'] = 'utf8'
        elif keywords['charset'] is None:
            del keywords['charset']

        self.paramstyle = db.paramstyle = 'pyformat' # it's both, like psycopg
        self.dbname = "mysql"
        DB.__init__(self, db, keywords)
        self.supports_multiple_insert = True
        
    def _process_insert_query(self, query, tablename, seqname):
        return query, SQLQuery('SELECT last_insert_id();')

def import_driver(drivers, preferred=None):
    """Import the first available driver or preferred driver.
    """
    if preferred:
        drivers = [preferred]

    for d in drivers:
        try:
            return __import__(d, None, None, ['x'])
        except ImportError:
            pass
    raise ImportError("Unable to import " + " or ".join(drivers))

class SqliteDB(DB): 
    def __init__(self, **keywords):
        db = import_driver(["sqlite3", "pysqlite2.dbapi2", "sqlite"], preferred=keywords.pop('driver', None))

        if db.__name__ in ["sqlite3", "pysqlite2.dbapi2"]:
            db.paramstyle = 'qmark'

        self.paramstyle = db.paramstyle
        keywords['database'] = keywords.pop('db')
        self.dbname = "sqlite"        
        DB.__init__(self, db, keywords)

    def _process_insert_query(self, query, tablename, seqname):
        return query, SQLQuery('SELECT last_insert_rowid();')
    
    def query(self, *a, **kw):
        out = DB.query(self, *a, **kw)
        if isinstance(out, iterbetter):
            # rowcount is not provided by sqlite
            del out.__len__
        return out

class FirebirdDB(DB):
    """Firebird Database.
    """
    def __init__(self, **keywords):
        try:
            import kinterbasdb as db
        except Exception:
            db = None
            pass
        if 'pw' in keywords:
            keywords['passwd'] = keywords['pw']
            del keywords['pw']
        keywords['database'] = keywords['db']
        del keywords['db']
        DB.__init__(self, db, keywords)
        
    def delete(self, table, where=None, using=None, vars=None, _test=False):
        # firebird doesn't support using clause
        using=None
        return DB.delete(self, table, where, using, vars, _test)

    def sql_clauses(self, what, tables, where, group, order, limit, offset):
        return (
            ('SELECT', ''),
            ('FIRST', limit),
            ('SKIP', offset),
            ('', what),
            ('FROM', sqllist(tables)),
            ('WHERE', where),
            ('GROUP BY', group),
            ('ORDER BY', order)
        )

class MSSQLDB(DB):
    def __init__(self, **keywords):
        import pymssql as db    
        if 'pw' in keywords:
            keywords['password'] = keywords.pop('pw')
        keywords['database'] = keywords.pop('db')
        self.dbname = "mssql"
        DB.__init__(self, db, keywords)

    def sql_clauses(self, what, tables, where, group, order, limit, offset): 
        return (
            ('SELECT', what),
            ('TOP', limit),
            ('FROM', sqllist(tables)),
            ('WHERE', where),
            ('GROUP BY', group),
            ('ORDER BY', order),
            ('OFFSET', offset))
            
    def _test(self):
        """Test LIMIT.

            Fake presence of pymssql module for running tests.
            >>> import sys
            >>> sys.modules['pymssql'] = sys.modules['sys']
            
            MSSQL has TOP clause instead of LIMIT clause.
            >>> db = MSSQLDB(db='test', user='joe', pw='secret')
            >>> db.select('foo', limit=4, _test=True)
            <sql: 'SELECT * TOP 4 FROM foo'>
        """
        pass

class OracleDB(DB): 
    def __init__(self, **keywords): 
        import cx_Oracle as db 
        if 'pw' in keywords: 
            keywords['password'] = keywords.pop('pw') 

        #@@ TODO: use db.makedsn if host, port is specified 
        keywords['dsn'] = keywords.pop('db') 
        self.dbname = 'oracle' 
        db.paramstyle = 'numeric' 
        self.paramstyle = db.paramstyle

        # oracle doesn't support pooling 
        keywords.pop('pooling', None) 
        DB.__init__(self, db, keywords) 

    def _process_insert_query(self, query, tablename, seqname): 
        if seqname is None: 
            # It is not possible to get seq name from table name in Oracle
            return query
        else:
            return query + "; SELECT %s.currval FROM dual" % seqname 

_databases = {}
def database(dburl=None, **params):
    """Creates appropriate database using params.
    
    Pooling will be enabled if DBUtils module is available. 
    Pooling can be disabled by passing pooling=False in params.
    """
    dbn = params.pop('dbn')
    if dbn in _databases:
        return _databases[dbn](**params)
    else:
        raise UnknownDB, dbn

def register_database(name, clazz):
    """
    Register a database.

        >>> class LegacyDB(DB): 
        ...     def __init__(self, **params): 
        ...        pass 
        ...
        >>> register_database('legacy', LegacyDB)
        >>> db = database(dbn='legacy', db='test', user='joe', passwd='secret') 
    """
    _databases[name] = clazz

register_database('mysql', MySQLDB)
register_database('postgres', PostgresDB)
register_database('sqlite', SqliteDB)
register_database('firebird', FirebirdDB)
register_database('mssql', MSSQLDB)
register_database('oracle', OracleDB)

def _interpolate(format): 
    """
    Takes a format string and returns a list of 2-tuples of the form
    (boolean, string) where boolean says whether string should be evaled
    or not.

    from <http://lfw.org/python/Itpl.py> (public domain, Ka-Ping Yee)
    """
    from tokenize import tokenprog

    def matchorfail(text, pos):
        match = tokenprog.match(text, pos)
        if match is None:
            raise _ItplError(text, pos)
        return match, match.end()

    namechars = "abcdefghijklmnopqrstuvwxyz" \
        "ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_";
    chunks = []
    pos = 0

    while 1:
        dollar = format.find("$", pos)
        if dollar < 0: 
            break
        nextchar = format[dollar + 1]

        if nextchar == "{":
            chunks.append((0, format[pos:dollar]))
            pos, level = dollar + 2, 1
            while level:
                match, pos = matchorfail(format, pos)
                tstart, tend = match.regs[3]
                token = format[tstart:tend]
                if token == "{": 
                    level = level + 1
                elif token == "}":  
                    level = level - 1
            chunks.append((1, format[dollar + 2:pos - 1]))

        elif nextchar in namechars:
            chunks.append((0, format[pos:dollar]))
            match, pos = matchorfail(format, dollar + 1)
            while pos < len(format):
                if format[pos] == "." and \
                    pos + 1 < len(format) and format[pos + 1] in namechars:
                    match, pos = matchorfail(format, pos + 1)
                elif format[pos] in "([":
                    pos, level = pos + 1, 1
                    while level:
                        match, pos = matchorfail(format, pos)
                        tstart, tend = match.regs[3]
                        token = format[tstart:tend]
                        if token[0] in "([": 
                            level = level + 1
                        elif token[0] in ")]":  
                            level = level - 1
                else: 
                    break
            chunks.append((1, format[dollar + 1:pos]))
        else:
            chunks.append((0, format[pos:dollar + 1]))
            pos = dollar + 1 + (nextchar == "$")

    if pos < len(format): 
        chunks.append((0, format[pos:]))
    return chunks

if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = debugerror
"""
pretty debug errors
(part of web.py)

portions adapted from Django <djangoproject.com> 
Copyright (c) 2005, the Lawrence Journal-World
Used under the modified BSD license:
http://www.xfree86.org/3.3.6/COPYRIGHT2.html#5
"""

__all__ = ["debugerror", "djangoerror", "emailerrors"]

import sys, urlparse, pprint, traceback
from net import websafe
from template import Template
from utils import sendmail
import webapi as web

import os, os.path
whereami = os.path.join(os.getcwd(), __file__)
whereami = os.path.sep.join(whereami.split(os.path.sep)[:-1])
djangoerror_t = """\
$def with (exception_type, exception_value, frames)
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html lang="en">
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <meta name="robots" content="NONE,NOARCHIVE" />
  <title>$exception_type at $ctx.path</title>
  <style type="text/css">
    html * { padding:0; margin:0; }
    body * { padding:10px 20px; }
    body * * { padding:0; }
    body { font:small sans-serif; }
    body>div { border-bottom:1px solid #ddd; }
    h1 { font-weight:normal; }
    h2 { margin-bottom:.8em; }
    h2 span { font-size:80%; color:#666; font-weight:normal; }
    h3 { margin:1em 0 .5em 0; }
    h4 { margin:0 0 .5em 0; font-weight: normal; }
    table { 
        border:1px solid #ccc; border-collapse: collapse; background:white; }
    tbody td, tbody th { vertical-align:top; padding:2px 3px; }
    thead th { 
        padding:1px 6px 1px 3px; background:#fefefe; text-align:left; 
        font-weight:normal; font-size:11px; border:1px solid #ddd; }
    tbody th { text-align:right; color:#666; padding-right:.5em; }
    table.vars { margin:5px 0 2px 40px; }
    table.vars td, table.req td { font-family:monospace; }
    table td.code { width:100%;}
    table td.code div { overflow:hidden; }
    table.source th { color:#666; }
    table.source td { 
        font-family:monospace; white-space:pre; border-bottom:1px solid #eee; }
    ul.traceback { list-style-type:none; }
    ul.traceback li.frame { margin-bottom:1em; }
    div.context { margin: 10px 0; }
    div.context ol { 
        padding-left:30px; margin:0 10px; list-style-position: inside; }
    div.context ol li { 
        font-family:monospace; white-space:pre; color:#666; cursor:pointer; }
    div.context ol.context-line li { color:black; background-color:#ccc; }
    div.context ol.context-line li span { float: right; }
    div.commands { margin-left: 40px; }
    div.commands a { color:black; text-decoration:none; }
    #summary { background: #ffc; }
    #summary h2 { font-weight: normal; color: #666; }
    #explanation { background:#eee; }
    #template, #template-not-exist { background:#f6f6f6; }
    #template-not-exist ul { margin: 0 0 0 20px; }
    #traceback { background:#eee; }
    #requestinfo { background:#f6f6f6; padding-left:120px; }
    #summary table { border:none; background:transparent; }
    #requestinfo h2, #requestinfo h3 { position:relative; margin-left:-100px; }
    #requestinfo h3 { margin-bottom:-1em; }
    .error { background: #ffc; }
    .specific { color:#cc3300; font-weight:bold; }
  </style>
  <script type="text/javascript">
  //<!--
    function getElementsByClassName(oElm, strTagName, strClassName){
        // Written by Jonathan Snook, http://www.snook.ca/jon; 
        // Add-ons by Robert Nyman, http://www.robertnyman.com
        var arrElements = (strTagName == "*" && document.all)? document.all :
        oElm.getElementsByTagName(strTagName);
        var arrReturnElements = new Array();
        strClassName = strClassName.replace(/\-/g, "\\-");
        var oRegExp = new RegExp("(^|\\s)" + strClassName + "(\\s|$$)");
        var oElement;
        for(var i=0; i<arrElements.length; i++){
            oElement = arrElements[i];
            if(oRegExp.test(oElement.className)){
                arrReturnElements.push(oElement);
            }
        }
        return (arrReturnElements)
    }
    function hideAll(elems) {
      for (var e = 0; e < elems.length; e++) {
        elems[e].style.display = 'none';
      }
    }
    window.onload = function() {
      hideAll(getElementsByClassName(document, 'table', 'vars'));
      hideAll(getElementsByClassName(document, 'ol', 'pre-context'));
      hideAll(getElementsByClassName(document, 'ol', 'post-context'));
    }
    function toggle() {
      for (var i = 0; i < arguments.length; i++) {
        var e = document.getElementById(arguments[i]);
        if (e) {
          e.style.display = e.style.display == 'none' ? 'block' : 'none';
        }
      }
      return false;
    }
    function varToggle(link, id) {
      toggle('v' + id);
      var s = link.getElementsByTagName('span')[0];
      var uarr = String.fromCharCode(0x25b6);
      var darr = String.fromCharCode(0x25bc);
      s.innerHTML = s.innerHTML == uarr ? darr : uarr;
      return false;
    }
    //-->
  </script>
</head>
<body>

$def dicttable (d, kls='req', id=None):
    $ items = d and d.items() or []
    $items.sort()
    $:dicttable_items(items, kls, id)
        
$def dicttable_items(items, kls='req', id=None):
    $if items:
        <table class="$kls"
        $if id: id="$id"
        ><thead><tr><th>Variable</th><th>Value</th></tr></thead>
        <tbody>
        $for k, v in items:
            <tr><td>$k</td><td class="code"><div>$prettify(v)</div></td></tr>
        </tbody>
        </table>
    $else:
        <p>No data.</p>

<div id="summary">
  <h1>$exception_type at $ctx.path</h1>
  <h2>$exception_value</h2>
  <table><tr>
    <th>Python</th>
    <td>$frames[0].filename in $frames[0].function, line $frames[0].lineno</td>
  </tr><tr>
    <th>Web</th>
    <td>$ctx.method $ctx.home$ctx.path</td>
  </tr></table>
</div>
<div id="traceback">
<h2>Traceback <span>(innermost first)</span></h2>
<ul class="traceback">
$for frame in frames:
    <li class="frame">
    <code>$frame.filename</code> in <code>$frame.function</code>
    $if frame.context_line:
        <div class="context" id="c$frame.id">
        $if frame.pre_context:
            <ol start="$frame.pre_context_lineno" class="pre-context" id="pre$frame.id">
            $for line in frame.pre_context:
                <li onclick="toggle('pre$frame.id', 'post$frame.id')">$line</li>
            </ol>
            <ol start="$frame.lineno" class="context-line"><li onclick="toggle('pre$frame.id', 'post$frame.id')">$frame.context_line <span>...</span></li></ol>
        $if frame.post_context:
            <ol start='${frame.lineno + 1}' class="post-context" id="post$frame.id">
            $for line in frame.post_context:
                <li onclick="toggle('pre$frame.id', 'post$frame.id')">$line</li>
            </ol>
      </div>
    
    $if frame.vars:
        <div class="commands">
        <a href='#' onclick="return varToggle(this, '$frame.id')"><span>&#x25b6;</span> Local vars</a>
        $# $inspect.formatargvalues(*inspect.getargvalues(frame['tb'].tb_frame))
        </div>
        $:dicttable(frame.vars, kls='vars', id=('v' + str(frame.id)))
      </li>
  </ul>
</div>

<div id="requestinfo">
$if ctx.output or ctx.headers:
    <h2>Response so far</h2>
    <h3>HEADERS</h3>
    $:dicttable_items(ctx.headers)

    <h3>BODY</h3>
    <p class="req" style="padding-bottom: 2em"><code>
    $ctx.output
    </code></p>
  
<h2>Request information</h2>

<h3>INPUT</h3>
$:dicttable(web.input())

<h3 id="cookie-info">COOKIES</h3>
$:dicttable(web.cookies())

<h3 id="meta-info">META</h3>
$ newctx = [(k, v) for (k, v) in ctx.iteritems() if not k.startswith('_') and not isinstance(v, dict)]
$:dicttable(dict(newctx))

<h3 id="meta-info">ENVIRONMENT</h3>
$:dicttable(ctx.env)
</div>

<div id="explanation">
  <p>
    You're seeing this error because you have <code>web.config.debug</code>
    set to <code>True</code>. Set that to <code>False</code> if you don't to see this.
  </p>
</div>

</body>
</html>
"""

djangoerror_r = None

def djangoerror():
    def _get_lines_from_file(filename, lineno, context_lines):
        """
        Returns context_lines before and after lineno from file.
        Returns (pre_context_lineno, pre_context, context_line, post_context).
        """
        try:
            source = open(filename).readlines()
            lower_bound = max(0, lineno - context_lines)
            upper_bound = lineno + context_lines

            pre_context = \
                [line.strip('\n') for line in source[lower_bound:lineno]]
            context_line = source[lineno].strip('\n')
            post_context = \
                [line.strip('\n') for line in source[lineno + 1:upper_bound]]

            return lower_bound, pre_context, context_line, post_context
        except (OSError, IOError):
            return None, [], None, []    
    
    exception_type, exception_value, tback = sys.exc_info()
    frames = []
    while tback is not None:
        filename = tback.tb_frame.f_code.co_filename
        function = tback.tb_frame.f_code.co_name
        lineno = tback.tb_lineno - 1
        pre_context_lineno, pre_context, context_line, post_context = \
            _get_lines_from_file(filename, lineno, 7)
        frames.append(web.storage({
            'tback': tback,
            'filename': filename,
            'function': function,
            'lineno': lineno,
            'vars': tback.tb_frame.f_locals,
            'id': id(tback),
            'pre_context': pre_context,
            'context_line': context_line,
            'post_context': post_context,
            'pre_context_lineno': pre_context_lineno,
        }))
        tback = tback.tb_next
    frames.reverse()
    urljoin = urlparse.urljoin
    def prettify(x):
        try: 
            out = pprint.pformat(x)
        except Exception, e: 
            out = '[could not display: <' + e.__class__.__name__ + \
                  ': '+str(e)+'>]'
        return out
        
    global djangoerror_r
    if djangoerror_r is None:
        djangoerror_r = Template(djangoerror_t, filename=__file__, filter=websafe)
        
    t = djangoerror_r
    globals = {'ctx': web.ctx, 'web':web, 'dict':dict, 'str':str, 'prettify': prettify}
    t.t.func_globals.update(globals)
    return t(exception_type, exception_value, frames)

def debugerror():
    """
    A replacement for `internalerror` that presents a nice page with lots
    of debug information for the programmer.

    (Based on the beautiful 500 page from [Django](http://djangoproject.com/), 
    designed by [Wilson Miner](http://wilsonminer.com/).)
    """
    return web._InternalError(djangoerror())

def emailerrors(to_address, olderror, from_address=None):
    """
    Wraps the old `internalerror` handler (pass as `olderror`) to 
    additionally email all errors to `to_address`, to aid in
    debugging production websites.
    
    Emails contain a normal text traceback as well as an
    attachment containing the nice `debugerror` page.
    """
    from_address = from_address or to_address

    def emailerrors_internal():
        error = olderror()
        tb = sys.exc_info()
        error_name = tb[0]
        error_value = tb[1]
        tb_txt = ''.join(traceback.format_exception(*tb))
        path = web.ctx.path
        request = web.ctx.method + ' ' + web.ctx.home + web.ctx.fullpath
        text = ("""\
------here----
Content-Type: text/plain
Content-Disposition: inline

%(request)s

%(tb_txt)s

------here----
Content-Type: text/html; name="bug.html"
Content-Disposition: attachment; filename="bug.html"

""" % locals()) + str(djangoerror())
        sendmail(
          "your buggy site <%s>" % from_address,
          "the bugfixer <%s>" % to_address,
          "bug: %(error_name)s: %(error_value)s (%(path)s)" % locals(),
          text, 
          headers={'Content-Type': 'multipart/mixed; boundary="----here----"'})
        return error
    
    return emailerrors_internal

if __name__ == "__main__":
    urls = (
        '/', 'index'
    )
    from application import application
    app = application(urls, globals())
    app.internalerror = debugerror
    
    class index:
        def GET(self):
            thisdoesnotexist

    app.run()

########NEW FILE########
__FILENAME__ = form
"""
HTML forms
(part of web.py)
"""

import copy, re
import webapi as web
import utils, net

def attrget(obj, attr, value=None):
    if hasattr(obj, 'has_key') and obj.has_key(attr): return obj[attr]
    if hasattr(obj, attr): return getattr(obj, attr)
    return value

class Form:
    r"""
    HTML form.
    
        >>> f = Form(Textbox("x"))
        >>> f.render()
        '<table>\n    <tr><th><label for="x">x</label></th><td><input type="text" name="x" id="x" /></td></tr>\n</table>'
    """
    def __init__(self, *inputs, **kw):
        self.inputs = inputs
        self.valid = True
        self.note = None
        self.validators = kw.pop('validators', [])

    def __call__(self, x=None):
        o = copy.deepcopy(self)
        if x: o.validates(x)
        return o
    
    def render(self):
        out = ''
        out += self.rendernote(self.note)
        out += '<table>\n'
        for i in self.inputs:
            out += '    <tr><th><label for="%s">%s</label></th>' % (i.id, net.websafe(i.description))
            out += "<td>"+i.pre+i.render()+i.post+"</td></tr>\n"
        out += "</table>"
        return out
        
    def render_css(self): 
        out = [] 
        out.append(self.rendernote(self.note)) 
        for i in self.inputs: 
            out.append('<label for="%s">%s</label>' % (i.id, net.websafe(i.description))) 
            out.append(i.pre) 
            out.append(i.render()) 
            out.append(i.post) 
            out.append('\n') 
        return ''.join(out) 
        
    def rendernote(self, note):
        if note: return '<strong class="wrong">%s</strong>' % net.websafe(note)
        else: return ""
    
    def validates(self, source=None, _validate=True, **kw):
        source = source or kw or web.input()
        out = True
        for i in self.inputs:
            v = attrget(source, i.name)
            if _validate:
                out = i.validate(v) and out
            else:
                i.value = v
        if _validate:
            out = out and self._validate(source)
            self.valid = out
        return out

    def _validate(self, value):
        self.value = value
        for v in self.validators:
            if not v.valid(value):
                self.note = v.msg
                return False
        return True

    def fill(self, source=None, **kw):
        return self.validates(source, _validate=False, **kw)
    
    def __getitem__(self, i):
        for x in self.inputs:
            if x.name == i: return x
        raise KeyError, i

    def __getattr__(self, name):
        # don't interfere with deepcopy
        inputs = self.__dict__.get('inputs') or []
        for x in inputs:
            if x.name == name: return x
        raise AttributeError, name
    
    def get(self, i, default=None):
        try:
            return self[i]
        except KeyError:
            return default
            
    def _get_d(self): #@@ should really be form.attr, no?
        return utils.storage([(i.name, i.value) for i in self.inputs])
    d = property(_get_d)

class Input(object):
    def __init__(self, name, *validators, **attrs):
        self.description = attrs.pop('description', name)
        self.value = attrs.pop('value', None)
        self.pre = attrs.pop('pre', "")
        self.post = attrs.pop('post', "")
        self.id = attrs.setdefault('id', name)
        if 'class_' in attrs:
            attrs['class'] = attrs['class_']
            del attrs['class_']
        self.name, self.validators, self.attrs, self.note = name, validators, attrs, None

    def validate(self, value):
        self.value = value
        for v in self.validators:
            if not v.valid(value):
                self.note = v.msg
                return False
        return True

    def render(self): raise NotImplementedError

    def rendernote(self, note):
        if note: return '<strong class="wrong">%s</strong>' % net.websafe(note)
        else: return ""
        
    def addatts(self):
        str = ""
        for (n, v) in self.attrs.items():
            str += ' %s="%s"' % (n, net.websafe(v))
        return str
    
#@@ quoting

class Textbox(Input):
    def render(self, shownote=True):
        x = '<input type="text" name="%s"' % net.websafe(self.name)
        if self.value: x += ' value="%s"' % net.websafe(self.value)
        x += self.addatts()
        x += ' />'
        if shownote:
            x += self.rendernote(self.note)
        return x

class Password(Input):
    def render(self):
        x = '<input type="password" name="%s"' % net.websafe(self.name)
        if self.value: x += ' value="%s"' % net.websafe(self.value)
        x += self.addatts()
        x += ' />'
        x += self.rendernote(self.note)
        return x

class Textarea(Input):
    def render(self):
        x = '<textarea name="%s"' % net.websafe(self.name)
        x += self.addatts()
        x += '>'
        if self.value is not None: x += net.websafe(self.value)
        x += '</textarea>'
        x += self.rendernote(self.note)
        return x

class Dropdown(Input):
    def __init__(self, name, args, *validators, **attrs):
        self.args = args
        super(Dropdown, self).__init__(name, *validators, **attrs)

    def render(self):
        x = '<select name="%s"%s>\n' % (net.websafe(self.name), self.addatts())
        for arg in self.args:
            if isinstance(arg, (tuple, list)):
                value, desc= arg
            else:
                value, desc = arg, arg 

            if self.value == value: select_p = ' selected="selected"'
            else: select_p = ''
            x += '  <option %s value="%s">%s</option>\n' % (select_p, net.websafe(value), net.websafe(desc))
        x += '</select>\n'
        x += self.rendernote(self.note)
        return x

class Radio(Input):
    def __init__(self, name, args, *validators, **attrs):
        self.args = args
        super(Radio, self).__init__(name, *validators, **attrs)

    def render(self):
        x = '<span>'
        for arg in self.args:
            if self.value == arg: select_p = ' checked="checked"'
            else: select_p = ''
            x += '<input type="radio" name="%s" value="%s"%s%s /> %s ' % (net.websafe(self.name), net.websafe(arg), select_p, self.addatts(), net.websafe(arg))
            x += '</span>'
            x += self.rendernote(self.note)    
        return x

class Checkbox(Input):
    def render(self):
        x = '<input name="%s" type="checkbox"' % net.websafe(self.name)
        if self.value: x += ' checked="checked"'
        x += self.addatts()
        x += ' />'
        x += self.rendernote(self.note)
        return x

class Button(Input):
    def __init__(self, name, *validators, **attrs):
        super(Button, self).__init__(name, *validators, **attrs)
        self.description = ""

    def render(self):
        safename = net.websafe(self.name)
        x = '<button name="%s"%s>%s</button>' % (safename, self.addatts(), safename)
        x += self.rendernote(self.note)
        return x

class Hidden(Input):
    def __init__(self, name, *validators, **attrs):
        super(Hidden, self).__init__(name, *validators, **attrs)
        # it doesnt make sence for a hidden field to have description
        self.description = ""

    def render(self):
        x = '<input type="hidden" name="%s"' % net.websafe(self.name)
        if self.value: x += ' value="%s"' % net.websafe(self.value)
        x += self.addatts()
        x += ' />'
        return x

class File(Input):
    def render(self):
        x = '<input type="file" name="%s"' % net.websafe(self.name)
        x += self.addatts()
        x += ' />'
        x += self.rendernote(self.note)
        return x
    
class Validator:
    def __deepcopy__(self, memo): return copy.copy(self)
    def __init__(self, msg, test, jstest=None): utils.autoassign(self, locals())
    def valid(self, value): 
        try: return self.test(value)
        except: return False

notnull = Validator("Required", bool)

class regexp(Validator):
    def __init__(self, rexp, msg):
        self.rexp = re.compile(rexp)
        self.msg = msg
    
    def valid(self, value):
        return bool(self.rexp.match(value))

if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = http
"""
HTTP Utilities
(from web.py)
"""

__all__ = [
  "expires", "lastmodified", 
  "prefixurl", "modified", 
  "write",
  "changequery", "url",
  "profiler",
]

import sys, os, threading, urllib, urlparse
try: import datetime
except ImportError: pass
import net, utils, webapi as web

def prefixurl(base=''):
    """
    Sorry, this function is really difficult to explain.
    Maybe some other time.
    """
    url = web.ctx.path.lstrip('/')
    for i in xrange(url.count('/')): 
        base += '../'
    if not base: 
        base = './'
    return base

def expires(delta):
    """
    Outputs an `Expires` header for `delta` from now. 
    `delta` is a `timedelta` object or a number of seconds.
    """
    if isinstance(delta, (int, long)):
        delta = datetime.timedelta(seconds=delta)
    date_obj = datetime.datetime.utcnow() + delta
    web.header('Expires', net.httpdate(date_obj))

def lastmodified(date_obj):
    """Outputs a `Last-Modified` header for `datetime`."""
    web.header('Last-Modified', net.httpdate(date_obj))

def modified(date=None, etag=None):
    """
    Checks to see if the page has been modified since the version in the
    requester's cache.
    
    When you publish pages, you can include `Last-Modified` and `ETag`
    with the date the page was last modified and an opaque token for
    the particular version, respectively. When readers reload the page, 
    the browser sends along the modification date and etag value for
    the version it has in its cache. If the page hasn't changed, 
    the server can just return `304 Not Modified` and not have to 
    send the whole page again.
    
    This function takes the last-modified date `date` and the ETag `etag`
    and checks the headers to see if they match. If they do, it returns 
    `True` and sets the response status to `304 Not Modified`. It also
    sets `Last-Modified and `ETag` output headers.
    """
    try:
        from __builtin__ import set
    except ImportError:
        # for python 2.3
        from sets import Set as set

    n = set([x.strip('" ') for x in web.ctx.env.get('HTTP_IF_NONE_MATCH', '').split(',')])
    m = net.parsehttpdate(web.ctx.env.get('HTTP_IF_MODIFIED_SINCE', '').split(';')[0])
    validate = False
    if etag:
        if '*' in n or etag in n:
            validate = True
    if date and m:
        # we subtract a second because 
        # HTTP dates don't have sub-second precision
        if date-datetime.timedelta(seconds=1) <= m:
            validate = True
    
    if validate: web.ctx.status = '304 Not Modified'
    if date: lastmodified(date)
    if etag: web.header('ETag', '"' + etag + '"')
    return not validate

def write(cgi_response):
    """
    Converts a standard CGI-style string response into `header` and 
    `output` calls.
    """
    cgi_response = str(cgi_response)
    cgi_response.replace('\r\n', '\n')
    head, body = cgi_response.split('\n\n', 1)
    lines = head.split('\n')

    for line in lines:
        if line.isspace(): 
            continue
        hdr, value = line.split(":", 1)
        value = value.strip()
        if hdr.lower() == "status": 
            web.ctx.status = value
        else: 
            web.header(hdr, value)

    web.output(body)

def urlencode(query):
    """
    Same as urllib.urlencode, but supports unicode strings.
    
        >>> urlencode({'text':'foo bar'})
        'text=foo+bar'
    """
    query = dict([(k, utils.utf8(v)) for k, v in query.items()])
    return urllib.urlencode(query)

def changequery(query=None, **kw):
    """
    Imagine you're at `/foo?a=1&b=2`. Then `changequery(a=3)` will return
    `/foo?a=3&b=2` -- the same URL but with the arguments you requested
    changed.
    """
    if query is None:
        query = web.input(_method='get')
    for k, v in kw.iteritems():
        if v is None:
            query.pop(k, None)
        else:
            query[k] = v
    out = web.ctx.path
    if query:
        out += '?' + urlencode(query)
    return out

def url(path=None, **kw):
    """
    Makes url by concatinating web.ctx.homepath and path and the 
    query string created using the arguments.
    """
    if path is None:
        path = web.ctx.path
    if path.startswith("/"):
        out = web.ctx.homepath + path
    else:
        out = path

    if kw:
        out += '?' + urlencode(kw)
    
    return out

def profiler(app):
    """Outputs basic profiling information at the bottom of each response."""
    from utils import profile
    def profile_internal(e, o):
        out, result = profile(app)(e, o)
        return list(out) + ['<pre>' + net.websafe(result) + '</pre>']
    return profile_internal

if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = httpserver
__all__ = ["runsimple"]

import sys, os
import webapi as web
import net
import utils

def runbasic(func, server_address=("0.0.0.0", 8080)):
    """
    Runs a simple HTTP server hosting WSGI app `func`. The directory `static/` 
    is hosted statically.

    Based on [WsgiServer][ws] from [Colin Stewart][cs].
    
  [ws]: http://www.owlfish.com/software/wsgiutils/documentation/wsgi-server-api.html
  [cs]: http://www.owlfish.com/
    """
    # Copyright (c) 2004 Colin Stewart (http://www.owlfish.com/)
    # Modified somewhat for simplicity
    # Used under the modified BSD license:
    # http://www.xfree86.org/3.3.6/COPYRIGHT2.html#5

    import SimpleHTTPServer, SocketServer, BaseHTTPServer, urlparse
    import socket, errno
    import traceback

    class WSGIHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):
        def run_wsgi_app(self):
            protocol, host, path, parameters, query, fragment = \
                urlparse.urlparse('http://dummyhost%s' % self.path)

            # we only use path, query
            env = {'wsgi.version': (1, 0)
                   ,'wsgi.url_scheme': 'http'
                   ,'wsgi.input': self.rfile
                   ,'wsgi.errors': sys.stderr
                   ,'wsgi.multithread': 1
                   ,'wsgi.multiprocess': 0
                   ,'wsgi.run_once': 0
                   ,'REQUEST_METHOD': self.command
                   ,'REQUEST_URI': self.path
                   ,'PATH_INFO': path
                   ,'QUERY_STRING': query
                   ,'CONTENT_TYPE': self.headers.get('Content-Type', '')
                   ,'CONTENT_LENGTH': self.headers.get('Content-Length', '')
                   ,'REMOTE_ADDR': self.client_address[0]
                   ,'SERVER_NAME': self.server.server_address[0]
                   ,'SERVER_PORT': str(self.server.server_address[1])
                   ,'SERVER_PROTOCOL': self.request_version
                   }

            for http_header, http_value in self.headers.items():
                env ['HTTP_%s' % http_header.replace('-', '_').upper()] = \
                    http_value

            # Setup the state
            self.wsgi_sent_headers = 0
            self.wsgi_headers = []

            try:
                # We have there environment, now invoke the application
                result = self.server.app(env, self.wsgi_start_response)
                try:
                    try:
                        for data in result:
                            if data: 
                                self.wsgi_write_data(data)
                    finally:
                        if hasattr(result, 'close'): 
                            result.close()
                except socket.error, socket_err:
                    # Catch common network errors and suppress them
                    if (socket_err.args[0] in \
                       (errno.ECONNABORTED, errno.EPIPE)): 
                        return
                except socket.timeout, socket_timeout: 
                    return
            except:
                print >> web.debug, traceback.format_exc(),

            if (not self.wsgi_sent_headers):
                # We must write out something!
                self.wsgi_write_data(" ")
            return

        do_POST = run_wsgi_app
        do_PUT = run_wsgi_app
        do_DELETE = run_wsgi_app

        def do_GET(self):
            if self.path.startswith('/static/'):
                SimpleHTTPServer.SimpleHTTPRequestHandler.do_GET(self)
            else:
                self.run_wsgi_app()

        def wsgi_start_response(self, response_status, response_headers, 
                              exc_info=None):
            if (self.wsgi_sent_headers):
                raise Exception \
                      ("Headers already sent and start_response called again!")
            # Should really take a copy to avoid changes in the application....
            self.wsgi_headers = (response_status, response_headers)
            return self.wsgi_write_data

        def wsgi_write_data(self, data):
            if (not self.wsgi_sent_headers):
                status, headers = self.wsgi_headers
                # Need to send header prior to data
                status_code = status[:status.find(' ')]
                status_msg = status[status.find(' ') + 1:]
                self.send_response(int(status_code), status_msg)
                for header, value in headers:
                    self.send_header(header, value)
                self.end_headers()
                self.wsgi_sent_headers = 1
            # Send the data
            self.wfile.write(data)

    class WSGIServer(SocketServer.ThreadingMixIn, BaseHTTPServer.HTTPServer):
        def __init__(self, func, server_address):
            BaseHTTPServer.HTTPServer.__init__(self, 
                                               server_address, 
                                               WSGIHandler)
            self.app = func
            self.serverShuttingDown = 0

    print "http://%s:%d/" % server_address
    WSGIServer(func, server_address).serve_forever()

def runsimple(func, server_address=("0.0.0.0", 8080)):
    """
    Runs [CherryPy][cp] WSGI server hosting WSGI app `func`. 
    The directory `static/` is hosted statically.

    [cp]: http://www.cherrypy.org
    """
    from wsgiserver import CherryPyWSGIServer
    from SimpleHTTPServer import SimpleHTTPRequestHandler
    from BaseHTTPServer import BaseHTTPRequestHandler

    class StaticApp(SimpleHTTPRequestHandler):
        """WSGI application for serving static files."""
        def __init__(self, environ, start_response):
            self.headers = []
            self.environ = environ
            self.start_response = start_response

        def send_response(self, status, msg=""):
            self.status = str(status) + " " + msg

        def send_header(self, name, value):
            self.headers.append((name, value))

        def end_headers(self):
            pass

        def log_message(*a): pass

        def __iter__(self):
            environ = self.environ

            self.path = environ.get('PATH_INFO', '')
            self.client_address = environ.get('REMOTE_ADDR','-'), \
                                  environ.get('REMOTE_PORT','-')
            self.command = environ.get('REQUEST_METHOD', '-')

            from cStringIO import StringIO
            self.wfile = StringIO() # for capturing error

            f = self.send_head()
            self.start_response(self.status, self.headers)

            if f:
                block_size = 16 * 1024
                while True:
                    buf = f.read(block_size)
                    if not buf:
                        break
                    yield buf
                f.close()
            else:
                value = self.wfile.getvalue()
                yield value
                    
    class WSGIWrapper(BaseHTTPRequestHandler):
        """WSGI wrapper for logging the status and serving static files."""
        def __init__(self, app):
            self.app = app
            self.format = '%s - - [%s] "%s %s %s" - %s'

        def __call__(self, environ, start_response):
            def xstart_response(status, response_headers, *args):
                write = start_response(status, response_headers, *args)
                self.log(status, environ)
                return write

            path = environ.get('PATH_INFO', '')
            if path.startswith('/static/'):
                return StaticApp(environ, xstart_response)
            else:
                return self.app(environ, xstart_response)

        def log(self, status, environ):
            outfile = environ.get('wsgi.errors', web.debug)
            req = environ.get('PATH_INFO', '_')
            protocol = environ.get('ACTUAL_SERVER_PROTOCOL', '-')
            method = environ.get('REQUEST_METHOD', '-')
            host = "%s:%s" % (environ.get('REMOTE_ADDR','-'), 
                              environ.get('REMOTE_PORT','-'))

            #@@ It is really bad to extend from 
            #@@ BaseHTTPRequestHandler just for this method
            time = self.log_date_time_string()

            msg = self.format % (host, time, protocol, method, req, status)
            print >> outfile, utils.safestr(msg)
            
    func = WSGIWrapper(func)
    server = CherryPyWSGIServer(server_address, func, server_name="localhost")

    print "http://%s:%d/" % server_address
    try:
        server.start()
    except KeyboardInterrupt:
        server.stop()

########NEW FILE########
__FILENAME__ = net
"""
Network Utilities
(from web.py)
"""

__all__ = [
  "validipaddr", "validipport", "validip", "validaddr", 
  "urlquote",
  "httpdate", "parsehttpdate", 
  "htmlquote", "htmlunquote", "websafe",
]

import urllib, time
try: import datetime
except ImportError: pass

def validipaddr(address):
    """
    Returns True if `address` is a valid IPv4 address.
    
        >>> validipaddr('192.168.1.1')
        True
        >>> validipaddr('192.168.1.800')
        False
        >>> validipaddr('192.168.1')
        False
    """
    try:
        octets = address.split('.')
        if len(octets) != 4:
            return False
        for x in octets:
            if not (0 <= int(x) <= 255):
                return False
    except ValueError:
        return False
    return True

def validipport(port):
    """
    Returns True if `port` is a valid IPv4 port.
    
        >>> validipport('9000')
        True
        >>> validipport('foo')
        False
        >>> validipport('1000000')
        False
    """
    try:
        if not (0 <= int(port) <= 65535):
            return False
    except ValueError:
        return False
    return True

def validip(ip, defaultaddr="0.0.0.0", defaultport=8080):
    """Returns `(ip_address, port)` from string `ip_addr_port`"""
    addr = defaultaddr
    port = defaultport
    
    ip = ip.split(":", 1)
    if len(ip) == 1:
        if not ip[0]:
            pass
        elif validipaddr(ip[0]):
            addr = ip[0]
        elif validipport(ip[0]):
            port = int(ip[0])
        else:
            raise ValueError, ':'.join(ip) + ' is not a valid IP address/port'
    elif len(ip) == 2:
        addr, port = ip
        if not validipaddr(addr) and validipport(port):
            raise ValueError, ':'.join(ip) + ' is not a valid IP address/port'
        port = int(port)
    else:
        raise ValueError, ':'.join(ip) + ' is not a valid IP address/port'
    return (addr, port)

def validaddr(string_):
    """
    Returns either (ip_address, port) or "/path/to/socket" from string_
    
        >>> validaddr('/path/to/socket')
        '/path/to/socket'
        >>> validaddr('8000')
        ('0.0.0.0', 8000)
        >>> validaddr('127.0.0.1')
        ('127.0.0.1', 8080)
        >>> validaddr('127.0.0.1:8000')
        ('127.0.0.1', 8000)
        >>> validaddr('fff')
        Traceback (most recent call last):
            ...
        ValueError: fff is not a valid IP address/port
    """
    if '/' in string_:
        return string_
    else:
        return validip(string_)

def urlquote(val):
    """
    Quotes a string for use in a URL.
    
        >>> urlquote('://?f=1&j=1')
        '%3A//%3Ff%3D1%26j%3D1'
        >>> urlquote(None)
        ''
        >>> urlquote(u'\u203d')
        '%E2%80%BD'
    """
    if val is None: return ''
    if not isinstance(val, unicode): val = str(val)
    else: val = val.encode('utf-8')
    return urllib.quote(val)

def httpdate(date_obj):
    """
    Formats a datetime object for use in HTTP headers.
    
        >>> import datetime
        >>> httpdate(datetime.datetime(1970, 1, 1, 1, 1, 1))
        'Thu, 01 Jan 1970 01:01:01 GMT'
    """
    return date_obj.strftime("%a, %d %b %Y %H:%M:%S GMT")

def parsehttpdate(string_):
    """
    Parses an HTTP date into a datetime object.

        >>> parsehttpdate('Thu, 01 Jan 1970 01:01:01 GMT')
        datetime.datetime(1970, 1, 1, 1, 1, 1)
    """
    try:
        t = time.strptime(string_, "%a, %d %b %Y %H:%M:%S %Z")
    except ValueError:
        return None
    return datetime.datetime(*t[:6])

def htmlquote(text):
    """
    Encodes `text` for raw use in HTML.
    
        >>> htmlquote("<'&\\">")
        '&lt;&#39;&amp;&quot;&gt;'
    """
    text = text.replace("&", "&amp;") # Must be done first!
    text = text.replace("<", "&lt;")
    text = text.replace(">", "&gt;")
    text = text.replace("'", "&#39;")
    text = text.replace('"', "&quot;")
    return text

def htmlunquote(text):
    """
    Decodes `text` that's HTML quoted.

        >>> htmlunquote('&lt;&#39;&amp;&quot;&gt;')
        '<\\'&">'
    """
    text = text.replace("&quot;", '"')
    text = text.replace("&#39;", "'")
    text = text.replace("&gt;", ">")
    text = text.replace("&lt;", "<")
    text = text.replace("&amp;", "&") # Must be done last!
    return text

def websafe(val):
    """
    Converts `val` so that it's safe for use in UTF-8 HTML.
    
        >>> websafe("<'&\\">")
        '&lt;&#39;&amp;&quot;&gt;'
        >>> websafe(None)
        ''
        >>> websafe(u'\u203d')
        '\\xe2\\x80\\xbd'
    """
    if val is None:
        return ''
    if isinstance(val, unicode):
        val = val.encode('utf-8')
    val = str(val)
    return htmlquote(val)

if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = session
"""
Session Management
(from web.py)
"""

import os, time, datetime, random, base64
try:
    import cPickle as pickle
except ImportError:
    import pickle
try:
    import hashlib
    sha1 = hashlib.sha1
except ImportError:
    import sha
    sha1 = sha.new

import utils
import webapi as web

__all__ = [
    'Session', 'SessionExpired',
    'Store', 'DiskStore', 'DBStore',
]

web.config.session_parameters = utils.storage({
    'cookie_name': 'webpy_session_id',
    'cookie_domain': None,
    'timeout': 86400, #24 * 60 * 60, # 24 hours in seconds
    'ignore_expiry': True,
    'ignore_change_ip': True,
    'secret_key': 'fLjUfxqXtfNoIldA0A0J',
    'expired_message': 'Session expired',
})

class SessionExpired(web.HTTPError): 
    def __init__(self, message):
        web.HTTPError.__init__(self, '200 OK', {}, data=message)

class Session(utils.ThreadedDict):
    """Session management for web.py
    """

    def __init__(self, app, store, initializer=None):
        self.__dict__['store'] = store
        self.__dict__['_initializer'] = initializer
        self.__dict__['_last_cleanup_time'] = 0
        self.__dict__['_config'] = utils.storage(web.config.session_parameters)

        if app:
            app.add_processor(self._processor)

    def _processor(self, handler):
        """Application processor to setup session for every request"""
        self._cleanup()
        self._load()

        try:
            return handler()
        finally:
            self._save()

    def _load(self):
        """Load the session from the store, by the id from cookie"""
        cookie_name = self._config.cookie_name
        cookie_domain = self._config.cookie_domain
        self.session_id = web.cookies().get(cookie_name)

        # protection against session_id tampering
        if self.session_id and not self._valid_session_id(self.session_id):
            self.session_id = None

        self._check_expiry()
        if self.session_id:
            d = self.store[self.session_id]
            self.update(d)
            self._validate_ip()
        
        if not self.session_id:
            self.session_id = self._generate_session_id()

            if self._initializer:
                if isinstance(self._initializer, dict):
                    self.update(self._initializer)
                elif hasattr(self._initializer, '__call__'):
                    self._initializer()
 
        self.ip = web.ctx.ip

    def _check_expiry(self):
        # check for expiry
        if self.session_id and self.session_id not in self.store:
            if self._config.ignore_expiry:
                self.session_id = None
            else:
                return self.expired()

    def _validate_ip(self):
        # check for change of IP
        if self.session_id and self.get('ip', None) != web.ctx.ip:
            if not self._config.ignore_change_ip:
               return self.expired() 
    
    def _save(self):
        cookie_name = self._config.cookie_name
        cookie_domain = self._config.cookie_domain
        if not self.get('_killed'):
            web.setcookie(cookie_name, self.session_id, domain=cookie_domain)
            self.store[self.session_id] = dict(self)
        else:
            web.setcookie(cookie_name, self.session_id, expires=-1, domain=cookie_domain)
    
    def _generate_session_id(self):
        """Generate a random id for session"""

        while True:
            rand = os.urandom(16)
            now = time.time()
            secret_key = self._config.secret_key
            session_id = sha1("%s%s%s%s" %(rand, now, utils.safestr(web.ctx.ip), secret_key))
            session_id = session_id.hexdigest()
            if session_id not in self.store:
                break
        return session_id

    def _valid_session_id(self, session_id):
        rx = utils.re_compile('^[0-9a-fA-F]+$')
        return rx.match(session_id)
        
    def _cleanup(self):
        """Cleanup the stored sessions"""
        current_time = time.time()
        timeout = self._config.timeout
        if current_time - self._last_cleanup_time > timeout:
            self.store.cleanup(timeout)
            self.__dict__['_last_cleanup_time'] = current_time

    def expired(self):
        """Called when an expired session is atime"""
        self._killed = True
        self._save()
        raise SessionExpired(self._config.expired_message)
 
    def kill(self):
        """Kill the session, make it no longer available"""
        del self.store[self.session_id]
        self._killed = True

class Store:
    """Base class for session stores"""

    def __contains__(self, key):
        raise NotImplementedError

    def __getitem__(self, key):
        raise NotImplementedError

    def __setitem__(self, key, value):
        raise NotImplementedError

    def cleanup(self, timeout):
        """removes all the expired sessions"""
        raise NotImplementedError

    def encode(self, session_dict):
        """encodes session dict as a string"""
        pickled = pickle.dumps(session_dict)
        return base64.encodestring(pickled)

    def decode(self, session_data):
        """decodes the data to get back the session dict """
        pickled = base64.decodestring(session_data)
        return pickle.loads(pickled)

class DiskStore(Store):
    """
    Store for saving a session on disk.

        >>> import tempfile
        >>> root = tempfile.mkdtemp()
        >>> s = DiskStore(root)
        >>> s['a'] = 'foo'
        >>> s['a']
        'foo'
        >>> time.sleep(0.01)
        >>> s.cleanup(0.01)
        >>> s['a']
        Traceback (most recent call last):
            ...
        KeyError: 'a'
    """
    def __init__(self, root):
        # if the storage root doesn't exists, create it.
        if not os.path.exists(root):
            os.mkdir(root)
        self.root = root

    def _get_path(self, key):
        if os.path.sep in key: 
            raise ValueError, "Bad key: %s" % repr(key)
        return os.path.join(self.root, key)
    
    def __contains__(self, key):
        path = self._get_path(key)
        return os.path.exists(path)

    def __getitem__(self, key):
        path = self._get_path(key)
        if os.path.exists(path): 
            pickled = open(path).read()
            return self.decode(pickled)
        else:
            raise KeyError, key

    def __setitem__(self, key, value):
        path = self._get_path(key)
        pickled = self.encode(value)    
        try:
            f = open(path, 'w')
            try:
                f.write(pickled)
            finally: 
                f.close()
        except IOError:
            pass

    def __delitem__(self, key):
        path = self._get_path(key)
        if os.path.exists(path):
            os.remove(path)
    
    def cleanup(self, timeout):
        now = time.time()
        for f in os.listdir(self.root):
            path = self._get_path(f)
            atime = os.stat(path).st_atime
            if now - atime > timeout :
                os.remove(path)

class DBStore(Store):
    """Store for saving a session in database
    Needs a table with the following columns:

        session_id CHAR(128) UNIQUE NOT NULL,
        atime DATETIME NOT NULL default current_timestamp,
        data TEXT
    """
    def __init__(self, db, table_name):
        self.db = db
        self.table = table_name
    
    def __contains__(self, key):
        data = self.db.select(self.table, where="session_id=$key", vars=locals())
        return bool(list(data)) 

    def __getitem__(self, key):
        now = datetime.datetime.now()
        try:
            s = self.db.select(self.table, where="session_id=$key", vars=locals())[0]
            self.db.update(self.table, where="session_id=$key", atime=now, vars=locals())
        except IndexError:
            raise KeyError
        else:
            return self.decode(s.data)

    def __setitem__(self, key, value):
        pickled = self.encode(value)
        now = datetime.datetime.now()
        if key in self:
            self.db.update(self.table, where="session_id=$key", data=pickled, vars=locals())
        else:
            self.db.insert(self.table, False, session_id=key, data=pickled )
                
    def __delitem__(self, key):
        self.db.delete(self.table, where="session_id=$key", vars=locals())

    def cleanup(self, timeout):
        timeout = datetime.timedelta(timeout/(24.0*60*60)) #timedelta takes numdays as arg
        last_allowed_time = datetime.datetime.now() - timeout
        self.db.delete(self.table, where="$last_allowed_time > atime", vars=locals())

class ShelfStore:
    """Store for saving session using `shelve` module.

        import shelve
        store = ShelfStore(shelve.open('session.shelf'))

    XXX: is shelve thread-safe?
    """
    def __init__(self, shelf):
        self.shelf = shelf

    def __contains__(self, key):
        return key in self.shelf

    def __getitem__(self, key):
        atime, v = self.shelf[key]
        self[key] = v # update atime
        return v

    def __setitem__(self, key, value):
        self.shelf[key] = time.time(), value
        
    def __delitem__(self, key):
        try:
            del self.shelf[key]
        except KeyError:
            pass

    def cleanup(self, timeout):
        now = time.time()
        for k in self.shelf.keys():
            atime, v = self.shelf[k]
            if now - atime > timeout :
                del self[k]

if __name__ == '__main__' :
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = template
"""
simple, elegant templating
(part of web.py)

Template design:

Template string is split into tokens and the tokens are combined into nodes. 
Parse tree is a nodelist. TextNode and ExpressionNode are simple nodes and 
for-loop, if-loop etc are block nodes, which contain multiple child nodes. 

Each node can emit some python string. python string emitted by the 
root node is validated for safeeval and executed using python in the given environment.

Enough care is taken to make sure the generated code and the template has line to line match, 
so that the error messages can point to exact line number in template. (It doesn't work in some cases still.)

Grammar:

    template -> defwith sections 
    defwith -> '$def with (' arguments ')' | ''
    sections -> section*
    section -> block | assignment | line

    assignment -> '$ ' <assignment expression>
    line -> (text|expr)*
    text -> <any characters other than $>
    expr -> '$' pyexpr | '$(' pyexpr ')' | '${' pyexpr '}'
    pyexpr -> <python expression>

"""

__all__ = [
    "Template",
    "Render", "render", "frender",
    "ParseError", "SecurityError",
    "test"
]

import tokenize
import os
import glob
import re

from utils import storage, safeunicode, safestr, re_compile
from webapi import config
from net import websafe

def splitline(text):
    r"""
    Splits the given text at newline.
    
        >>> splitline('foo\nbar')
        ('foo\n', 'bar')
        >>> splitline('foo')
        ('foo', '')
        >>> splitline('')
        ('', '')
    """
    index = text.find('\n') + 1
    if index:
        return text[:index], text[index:]
    else:
        return text, ''

class Parser:
    """Parser Base.
    """
    def __init__(self, text, name="<template>"):
        self.text = text
        self.name = name

    def parse(self):
        text = self.text
        defwith, text = self.read_defwith(text)
        suite = self.read_suite(text)
        return DefwithNode(defwith, suite)

    def read_defwith(self, text):
        if text.startswith('$def with'):
            defwith, text = splitline(text)
            defwith = defwith[1:].strip() # strip $ and spaces
            return defwith, text
        else:
            return '', text
    
    def read_section(self, text):
        r"""Reads one section from the given text.
        
        section -> block | assignment | line
        
            >>> read_section = Parser('').read_section
            >>> read_section('foo\nbar\n')
            (<line: [t'foo\n']>, 'bar\n')
            >>> read_section('$ a = b + 1\nfoo\n')
            (<assignment: 'a = b + 1'>, 'foo\n')
            
        read_section('$for in range(10):\n    hello $i\nfoo)
        """
        if text.lstrip(' ').startswith('$'):
            index = text.index('$')
            begin_indent, text2 = text[:index], text[index+1:]
            ahead = self.python_lookahead(text2)
            
            if ahead == 'var':
                return self.read_var(text2)
            elif ahead in STATEMENT_NODES:
                return self.read_block_section(text2, begin_indent)
            elif ahead in KEYWORDS:
                return self.read_keyword(text2)
            elif ahead.strip() == '':
                # assignments starts with a space after $
                # ex: $ a = b + 2
                return self.read_assignment(text2)
        return self.readline(text)
        
    def read_var(self, text):
        r"""Reads a var statement.
        
            >>> read_var = Parser('').read_var
            >>> read_var('var x=10\nfoo')
            (<var: x = 10>, 'foo')
            >>> read_var('var x: hello $name\nfoo')
            (<var: x = join_('hello ', escape_(name, True))>, 'foo')
        """
        line, text = splitline(text)
        tokens = self.python_tokens(line)
        if len(tokens) < 4:
            raise SyntaxError('Invalid var statement')
            
        name = tokens[1]
        sep = tokens[2]
        value = line.split(sep, 1)[1].strip()
        
        if sep == '=':
            pass # no need to process value
        elif sep == ':': 
            #@@ Hack for backward-compatability
            if tokens[3] == '\n': # multi-line var statement
                block, text = self.read_indented_block(text, '    ')
                lines = [self.readline(x)[0] for x in block.splitlines()]
                nodes = []
                for x in lines:
                    nodes.extend(x.nodes)
                    nodes.append(TextNode('\n'))         
            else: # single-line var statement
                linenode, _ = self.readline(value)
                nodes = linenode.nodes                
            parts = [node.emit('') for node in nodes]
            value = "join_(%s)" % ", ".join(parts)
        else:
            raise SyntaxError('Invalid var statement')
        return VarNode(name, value), text
                    
    def read_suite(self, text):
        r"""Reads section by section till end of text.
        
            >>> read_suite = Parser('').read_suite
            >>> read_suite('hello $name\nfoo\n')
            [<line: [t'hello ', $name, t'\n']>, <line: [t'foo\n']>]
        """
        sections = []
        while text:
            section, text = self.read_section(text)
            sections.append(section)
        return SuiteNode(sections)
    
    def readline(self, text):
        r"""Reads one line from the text. Newline is supressed if the line ends with \.
        
            >>> readline = Parser('').readline
            >>> readline('hello $name!\nbye!')
            (<line: [t'hello ', $name, t'!\n']>, 'bye!')
            >>> readline('hello $name!\\\nbye!')
            (<line: [t'hello ', $name, t'!']>, 'bye!')
            >>> readline('$f()\n\n')
            (<line: [$f(), t'\n']>, '\n')
        """
        line, text = splitline(text)

        # supress new line if line ends with \
        if line.endswith('\\\n'):
            line = line[:-2]
                
        nodes = []
        while line:
            node, line = self.read_node(line)
            nodes.append(node)
            
        return LineNode(nodes), text

    def read_node(self, text):
        r"""Reads a node from the given text and returns the node and remaining text.

            >>> read_node = Parser('').read_node
            >>> read_node('hello $name')
            (t'hello ', '$name')
            >>> read_node('$name')
            ($name, '')
        """
        if text.startswith('$$'):
            return TextNode('$'), text[2:]
        elif text.startswith('$#'): # comment
            line, text = splitline(text)
            return TextNode('\n'), text
        elif text.startswith('$'):
            text = text[1:] # strip $
            if text.startswith(':'):
                escape = False
                text = text[1:] # strip :
            else:
                escape = True
            return self.read_expr(text, escape=escape)
        else:
            return self.read_text(text)
    
    def read_text(self, text):
        r"""Reads a text node from the given text.
        
            >>> read_text = Parser('').read_text
            >>> read_text('hello $name')
            (t'hello ', '$name')
        """
        index = text.find('$')
        if index < 0:
            return TextNode(text), ''
        else:
            return TextNode(text[:index]), text[index:]
            
    def read_keyword(self, text):
        line, text = splitline(text)
        return CodeNode(None, line.strip() + "\n"), text

    def read_expr(self, text, escape=True):
        """Reads a python expression from the text and returns the expression and remaining text.

        expr -> simple_expr | paren_expr
        simple_expr -> id extended_expr
        extended_expr -> attr_access | paren_expr extended_expr | ''
        attr_access -> dot id extended_expr
        paren_expr -> [ tokens ] | ( tokens ) | { tokens }
     
            >>> read_expr = Parser('').read_expr
            >>> read_expr("name")
            ($name, '')
            >>> read_expr("a.b and c")
            ($a.b, ' and c')
            >>> read_expr("a. b")
            ($a, '. b')
            >>> read_expr("name</h1>")
            ($name, '</h1>')
            >>> read_expr("(limit)ing")
            ($(limit), 'ing')
            >>> read_expr('a[1, 2][:3].f(1+2, "weird string[).", 3 + 4) done.')
            ($a[1, 2][:3].f(1+2, "weird string[).", 3 + 4), ' done.')
        """
        def simple_expr():
            identifier()
            extended_expr()
        
        def identifier():
            tokens.next()
        
        def extended_expr():
            lookahead = tokens.lookahead()
            if lookahead is None:
                return
            elif lookahead.value == '.':
                attr_access()
            elif lookahead.value in parens:
                paren_expr()
                extended_expr()
            else:
                return
        
        def attr_access():
            from token import NAME # python token constants
            dot = tokens.lookahead()
            if tokens.lookahead2().type == NAME:
                tokens.next() # consume dot
                identifier()
                extended_expr()
        
        def paren_expr():
            begin = tokens.next().value
            end = parens[begin]
            while True:
                if tokens.lookahead().value in parens:
                    paren_expr()
                else:
                    t = tokens.next()
                    if t.value == end:
                        break
            return

        parens = {
            "(": ")",
            "[": "]",
            "{": "}"
        }
        
        def get_tokens(text):
            """tokenize text using python tokenizer.
            Python tokenizer ignores spaces, but they might be important in some cases. 
            This function introduces dummy space tokens when it identifies any ignored space.
            Each token is a storage object containing type, value, begin and end.
            """
            readline = iter([text]).next
            end = None
            for t in tokenize.generate_tokens(readline):
                t = storage(type=t[0], value=t[1], begin=t[2], end=t[3])
                if end is not None and end != t.begin:
                    _, x1 = end
                    _, x2 = t.begin
                    yield storage(type=-1, value=text[x1:x2], begin=end, end=t.begin)
                end = t.end
                yield t
                
        class BetterIter:
            """Iterator like object with 2 support for 2 look aheads."""
            def __init__(self, items):
                self.iteritems = iter(items)
                self.items = []
                self.position = 0
                self.current_item = None
            
            def lookahead(self):
                if len(self.items) <= self.position:
                    self.items.append(self._next())
                return self.items[self.position]

            def _next(self):
                try:
                    return self.iteritems.next()
                except StopIteration:
                    return None
                
            def lookahead2(self):
                if len(self.items) <= self.position+1:
                    self.items.append(self._next())
                return self.items[self.position+1]
                    
            def next(self):
                self.current_item = self.lookahead()
                self.position += 1
                return self.current_item

        tokens = BetterIter(get_tokens(text))
                
        if tokens.lookahead().value in parens:
            paren_expr()
        else:
            simple_expr()
        row, col = tokens.current_item.end
        return ExpressionNode(text[:col], escape=escape), text[col:]    

    def read_assignment(self, text):
        r"""Reads assignment statement from text.
    
            >>> read_assignment = Parser('').read_assignment
            >>> read_assignment('a = b + 1\nfoo')
            (<assignment: 'a = b + 1'>, 'foo')
        """
        line, text = splitline(text)
        return AssignmentNode(line.strip()), text
    
    def python_lookahead(self, text):
        """Returns the first python token from the given text.
        
            >>> python_lookahead = Parser('').python_lookahead
            >>> python_lookahead('for i in range(10):')
            'for'
            >>> python_lookahead('else:')
            'else'
            >>> python_lookahead(' x = 1')
            ' '
        """
        readline = iter([text]).next
        tokens = tokenize.generate_tokens(readline)
        return tokens.next()[1]
        
    def python_tokens(self, text):
        readline = iter([text]).next
        tokens = tokenize.generate_tokens(readline)
        return [t[1] for t in tokens]
        
    def read_indented_block(self, text, indent):
        r"""Read a block of text. A block is what typically follows a for or it statement.
        It can be in the same line as that of the statement or an indented block.

            >>> read_indented_block = Parser('').read_indented_block
            >>> read_indented_block('  a\n  b\nc', '  ')
            ('a\nb\n', 'c')
            >>> read_indented_block('  a\n    b\n  c\nd', '  ')
            ('a\n  b\nc\n', 'd')
        """
        if indent == '':
            return '', text
            
        block = ""
        while True:
            if text.startswith(indent):
                line, text = splitline(text)
                block += line[len(indent):]
            else:
                break
        return block, text

    def read_statement(self, text):
        r"""Reads a python statement.
        
            >>> read_statement = Parser('').read_statement
            >>> read_statement('for i in range(10): hello $name')
            ('for i in range(10):', ' hello $name')
        """
        tok = PythonTokenizer(text)
        tok.consume_till(':')
        return text[:tok.index], text[tok.index:]
        
    def read_block_section(self, text, begin_indent=''):
        r"""
            >>> read_block_section = Parser('').read_block_section
            >>> read_block_section('for i in range(10): hello $i\nfoo')
            (<block: 'for i in range(10):', [<line: [t'hello ', $i, t'\n']>]>, 'foo')
            >>> read_block_section('for i in range(10):\n        hello $i\n    foo', begin_indent='    ')
            (<block: 'for i in range(10):', [<line: [t'hello ', $i, t'\n']>]>, '    foo')
            >>> read_block_section('for i in range(10):\n  hello $i\nfoo')
            (<block: 'for i in range(10):', [<line: [t'hello ', $i, t'\n']>]>, 'foo')
        """
        line, text = splitline(text)
        stmt, line = self.read_statement(line)
        keyword = self.python_lookahead(stmt)
        
        # if there is some thing left in the line
        if line.strip():
            block = line.lstrip()
        else:
            def find_indent(text):
                rx = re_compile('  +')
                match = rx.match(text)    
                first_indent = match and match.group(0)
                return first_indent or ""

            # find the indentation of the block by looking at the first line
            first_indent = find_indent(text)[len(begin_indent):]
            indent = begin_indent + min(first_indent, INDENT)
            
            block, text = self.read_indented_block(text, indent)
            
        return self.create_block_node(keyword, stmt, block, begin_indent), text
        
    def create_block_node(self, keyword, stmt, block, begin_indent):
        if keyword in STATEMENT_NODES:
            return STATEMENT_NODES[keyword](stmt, block, begin_indent)
        else:
            raise ParseError, 'Unknown statement: %s' % repr(keyword)
        
class PythonTokenizer:
    """Utility wrapper over python tokenizer."""
    def __init__(self, text):
        self.text = text
        readline = iter([text]).next
        self.tokens = tokenize.generate_tokens(readline)
        self.index = 0
        
    def consume_till(self, delim):        
        """Consumes tokens till colon.
        
            >>> tok = PythonTokenizer('for i in range(10): hello $i')
            >>> tok.consume_till(':')
            >>> tok.text[:tok.index]
            'for i in range(10):'
            >>> tok.text[tok.index:]
            ' hello $i'
        """
        try:
            while True:
                t = self.next()
                if t.value == delim:
                    break
                elif t.value == '(':
                    self.consume_till(')')
                elif t.value == '[':
                    self.consume_till(']')
                elif t.value == '{':
                    self.consume_till('}')

                # if end of line is found, it is an exception.
                # Since there is no easy way to report the line number,
                # leave the error reporting to the python parser later  
                #@@ This should be fixed.
                if t.value == '\n':
                    break
        except:
            #raise ParseError, "Expected %s, found end of line." % repr(delim)

            # raising ParseError doesn't show the line number. 
            # if this error is ignored, then it will be caught when compiling the python code.
            return
    
    def next(self):
        type, t, begin, end, line = self.tokens.next()
        row, col = end
        self.index = col
        return storage(type=type, value=t, begin=begin, end=end)
        
class DefwithNode:
    def __init__(self, defwith, suite):
        if defwith:
            self.defwith = defwith.replace('with', '__template__') + ':'
        else:
            self.defwith = 'def __template__():'
        self.suite = suite

    def emit(self, indent):
        return self.defwith + self.suite.emit(indent + INDENT)

    def __repr__(self):
        return "<defwith: %s, %s>" % (self.defwith, self.nodes)

class TextNode:
    def __init__(self, value):
        self.value = value

    def emit(self, indent):
        return repr(self.value)
        
    def __repr__(self):
        return 't' + repr(self.value)
        
class ExpressionNode:
    def __init__(self, value, escape=True):
        self.value = value.strip()
        
        # convert ${...} to $(...)
        if value.startswith('{') and value.endswith('}'):
            self.value = '(' + self.value[1:-1] + ')'
            
        self.escape = escape

    def emit(self, indent):
        return 'escape_(%s, %s)' % (self.value, bool(self.escape))
        
    def __repr__(self):
        if self.escape:
            escape = ''
        else:
            escape = ':'
        return "$%s%s" % (escape, self.value)
        
class AssignmentNode:
    def __init__(self, code):
        self.code = code
        
    def emit(self, indent, begin_indent=''):
        return indent + self.code + "\n"
        
    def __repr__(self):
        return "<assignment: %s>" % repr(self.code)
        
class LineNode:
    def __init__(self, nodes):
        self.nodes = nodes
        
    def emit(self, indent, text_indent='', name=''):
        text = [node.emit('') for node in self.nodes]
        if text_indent:
            text = [repr(text_indent)] + text
        return indent + 'yield %s, join_(%s)\n' % (repr(name), ', '.join(text))
    
    def __repr__(self):
        return "<line: %s>" % repr(self.nodes)

INDENT = '    ' # 4 spaces
        
class BlockNode:
    def __init__(self, stmt, block, begin_indent=''):
        self.stmt = stmt
        self.suite = Parser('').read_suite(block)
        self.begin_indent = begin_indent

    def emit(self, indent, text_indent=''):
        text_indent = self.begin_indent + text_indent
        out = indent + self.stmt + self.suite.emit(indent + INDENT, text_indent)
        return out
        
    def text(self):
        return '${' + self.stmt + '}' + "".join([node.text(indent) for node in self.nodes])
        
    def __repr__(self):
        return "<block: %s, %s>" % (repr(self.stmt), repr(self.nodelist))

class ForNode(BlockNode):
    def __init__(self, stmt, block, begin_indent=''):
        self.original_stmt = stmt
        tok = PythonTokenizer(stmt)
        tok.consume_till('in')
        a = stmt[:tok.index] # for i in
        b = stmt[tok.index:-1] # rest of for stmt excluding :
        stmt = a + ' loop.setup(' + b.strip() + '):'
        BlockNode.__init__(self, stmt, block, begin_indent)
        
    def __repr__(self):
        return "<block: %s, %s>" % (repr(self.original_stmt), repr(self.suite))

class CodeNode:
    def __init__(self, stmt, block, begin_indent=''):
        self.code = block
        
    def emit(self, indent, text_indent=''):
        import re
        rx = re.compile('^', re.M)
        return rx.sub(indent, self.code).rstrip(' ')
        
    def __repr__(self):
        return "<code: %s>" % repr(self.code)
        
class IfNode(BlockNode):
    pass

class ElseNode(BlockNode):
    pass

class ElifNode(BlockNode):
    pass

class DefNode(BlockNode):
    pass

class VarNode:
    def __init__(self, name, value):
        self.name = name
        self.value = value
        
    def emit(self, indent, text_indent):
        return indent + 'yield %s, %s\n' % (repr(self.name), self.value)
        
    def __repr__(self):
        return "<var: %s = %s>" % (self.name, self.value)

class SuiteNode:
    """Suite is a list of sections."""
    def __init__(self, sections):
        self.sections = sections
        
    def emit(self, indent, text_indent=''):
        return "\n" + "".join([s.emit(indent, text_indent) for s in self.sections])
        
    def __repr__(self):
        return repr(self.sections)

STATEMENT_NODES = {
    'for': ForNode,
    'while': BlockNode,
    'if': IfNode,
    'elif': ElifNode,
    'else': ElseNode,
    'def': DefNode,
    'code': CodeNode
}

KEYWORDS = [
    "pass",
    "break",
    "continue",
    "return"
]

TEMPLATE_BUILTIN_NAMES = [
    "dict", "enumerate", "float", "int", "bool", "list", "long", "reversed", 
    "set", "slice", "tuple", "xrange",
    "abs", "all", "any", "callable", "chr", "cmp", "divmod", "filter", "hex", 
    "id", "isinstance", "iter", "len", "max", "min", "oct", "ord", "pow", "range",
    "True", "False",
    "None",
    "__import__", # some c-libraries like datetime requires __import__ to present in the namespace
]

import __builtin__
TEMPLATE_BUILTINS = dict([(name, getattr(__builtin__, name)) for name in TEMPLATE_BUILTIN_NAMES if name in __builtin__.__dict__])

class ForLoop:
    """
    Wrapper for expression in for stament to support loop.xxx helpers.
    
        >>> loop = ForLoop()
        >>> for x in loop.setup(['a', 'b', 'c']):
        ...     print loop.index, loop.revindex, loop.parity, x
        ...
        1 3 odd a
        2 2 even b
        3 1 odd c
        >>> loop.index
        Traceback (most recent call last):
            ...
        AttributeError: index
    """
    def __init__(self):
        self._ctx = None
        
    def __getattr__(self, name):
        if self._ctx is None:
            raise AttributeError, name
        else:
            return getattr(self._ctx, name)
        
    def setup(self, seq):        
        self._push()
        return self._ctx.setup(seq)
        
    def _push(self):
        self._ctx = ForLoopContext(self, self._ctx)
        
    def _pop(self):
        self._ctx = self._ctx.parent
                
class ForLoopContext:
    """Stackable context for ForLoop to support nested for loops.
    """
    def __init__(self, forloop, parent):
        self._forloop = forloop
        self.parent = parent
        
    def setup(self, seq):
        if hasattr(seq, '__len__'):
            n = len(seq)
        else:
            n = 0
            
        self.index = 0
        seq = iter(seq)
        
        # Pre python-2.5 does not support yield in try-except.
        # This is a work-around to overcome that limitation.
        def next(seq):
            try:
                return seq.next()
            except:
                self._forloop._pop()
                raise
        
        while True:
            self._next(self.index + 1, n)
            yield next(seq)
            
    def _next(self, i, n):
        self.index = i
        self.index0 = i - 1
        self.first = (i == 1)
        self.last = (i == n)
        self.odd = (i % 2 == 1)
        self.even = (i % 2 == 0)
        self.parity = ['odd', 'even'][self.even]
        if n:
            self.length = n
            self.revindex0 = n - i
            self.revindex = self.revindex0 + 1
        
class BaseTemplate:
    def __init__(self, code, filename, filter, globals, builtins):
        self.filename = filename
        self.filter = filter
        self._globals = globals
        self._builtins = builtins
        if code:
            self.t = self._compile(code)
        else:
            self.t = lambda: ''
        
    def _compile(self, code):
        env = self.make_env(self._globals or {}, self._builtins)
        exec(code, env)
        return env['__template__']

    def __call__(self, *a, **kw):
        out = self.t(*a, **kw)
        return self._join_output(out)
        
    def _join_output(self, out):
        d = TemplateResult()
        data = []
        
        for name, value in out:
            if name:
                d[name] = value
            else:
                data.append(value)
                            
        d.__body__ = u"".join(data)
        return d       

    def make_env(self, globals, builtins):
        return dict(globals,
            __builtins__=builtins, 
            loop=ForLoop(),
            escape_=self._escape,
            join_=self._join
        )
    
    def _join(self, *items):
        return u"".join([safeunicode(item) for item in items])
        
    def _escape(self, value, escape=False):
        import types
        if value is None: 
            value = ''
        elif isinstance(value, types.GeneratorType):
            value = self._join_output(value)
            
        value = safeunicode(value)
        if escape and self.filter:
            value = self.filter(value)
        return value

class Template(BaseTemplate):
    CONTENT_TYPES = {
        '.html' : 'text/html; charset=utf-8',
        '.xhtml' : 'application/xhtml+xml; charset=utf-8',
        '.txt' : 'text/plain',
    }
    FILTERS = {
        '.html': websafe,
        '.xhtml': websafe,
        '.xml': websafe
    }
    globals = {}
    
    def __init__(self, text, filename='<template>', filter=None, globals=None, builtins=None):
        text = Template.normalize_text(text)
        code = self.compile_template(text, filename)
                
        _, ext = os.path.splitext(filename)
        filter = filter or self.FILTERS.get(ext, None)
        self.content_type = self.CONTENT_TYPES.get(ext, None)

        if globals is None:
            globals = self.globals
        if builtins is None:
            builtins = TEMPLATE_BUILTINS
                
        BaseTemplate.__init__(self, code=code, filename=filename, filter=filter, globals=globals, builtins=builtins)
        
    def normalize_text(text):
        """Normalizes template text by correcting \r\n, tabs and BOM chars."""
        text = text.replace('\r\n', '\n').replace('\r', '\n').expandtabs()
        if not text.endswith('\n'):
            text += '\n'

        # ignore BOM chars at the begining of template
        BOM = '\xef\xbb\xbf'
        if isinstance(text, str) and text.startswith(BOM):
            text = text[len(BOM):]
        
        # support fort \$ for backward-compatibility 
        text = text.replace(r'\$', '$$')
        return text
    normalize_text = staticmethod(normalize_text)
                
    def __call__(self, *a, **kw):
        import webapi as web
        if 'headers' in web.ctx and self.content_type:
            web.header('Content-Type', self.content_type, unique=True)
            
        return BaseTemplate.__call__(self, *a, **kw)
        
    def generate_code(text, filename):
        # parse the text
        rootnode = Parser(text, filename).parse()
                
        # generate python code from the parse tree
        code = rootnode.emit(indent="").strip()
        return safestr(code)
        
    generate_code = staticmethod(generate_code)
        
    def compile_template(self, template_string, filename):
        code = Template.generate_code(template_string, filename)
    
        def get_source_line(filename, lineno):
            try:
                lines = open(filename).read().splitlines()
                return lines[lineno]
            except:
                return None
        
        try:
            # compile the code first to report the errors, if any, with the filename
            compiled_code = compile(code, filename, 'exec')
        except SyntaxError, e:
            # display template line that caused the error along with the traceback.
            try:
                e.msg += '\n\nTemplate traceback:\n    File %s, line %s\n        %s' % \
                    (repr(e.filename), e.lineno, get_source_line(e.filename, e.lineno-1))
            except: 
                pass
            raise
        
        # make sure code is safe
        import compiler
        ast = compiler.parse(code)
        SafeVisitor().walk(ast, filename)

        return compiled_code
        
class CompiledTemplate(Template):
    def __init__(self, f, filename):
        Template.__init__(self, '', filename)
        self.t = f
        
    def compile_template(self, *a):
        return None
    
    def _compile(self, *a):
        return None
                
class Render:
    """The most preferred way of using templates.
    
        render = web.template.render('templates')
        print render.foo()
        
    Optional parameter can be `base` can be used to pass output of 
    every template through the base template.
    
        render = web.template.render('templates', base='layout')
    """
    def __init__(self, loc='templates', cache=None, base=None, **keywords):
        self._loc = loc
        self._keywords = keywords

        if cache is None:
            cache = not config.get('debug', False)
        
        if cache:
            self._cache = {}
        else:
            self._cache = None
        
        if base and not hasattr(base, '__call__'):
            # make base a function, so that it can be passed to sub-renders
            self._base = lambda page: self._template(base)(page)
        else:
            self._base = base
            
    def _lookup(self, name):
        path = os.path.join(self._loc, name)
        if os.path.isdir(path):
            return 'dir', path
        else:
            path = self._findfile(path)
            if path:
                return 'file', path
            else:
                return 'none', None
        
    def _load_template(self, name):
        kind, path = self._lookup(name)
        
        if kind == 'dir':
            return Render(path, cache=self._cache is not None, base=self._base, **self._keywords)
        elif kind == 'file':
            return Template(open(path).read(), filename=path, **self._keywords)
        else:
            raise AttributeError, "No template named " + name            

    def _findfile(self, path_prefix): 
        p = [f for f in glob.glob(path_prefix + '.*') if not f.endswith('~')] # skip backup files
        return p and p[0]
            
    def _template(self, name):
        if self._cache is not None:
            if name not in self._cache:
                self._cache[name] = self._load_template(name)
            return self._cache[name]
        else:
            return self._load_template(name)
        
    def __getattr__(self, name):
        t = self._template(name)
        if self._base and isinstance(t, Template):
            def template(*a, **kw):
                return self._base(t(*a, **kw))
            return template
        else:
            return self._template(name)

class GAE_Render(Render):
    # Render gets over-written. make a copy here.
    super = Render
    def __init__(self, loc, *a, **kw):
        GAE_Render.super.__init__(self, loc, *a, **kw)
        
        import types
        if isinstance(loc, types.ModuleType):
            self.mod = loc
        else:
            name = loc.rstrip('/').replace('/', '.')
            self.mod = __import__(name, None, None, ['x'])

        self.mod.__dict__.update(kw.get('builtins', TEMPLATE_BUILTINS))
        self.mod.__dict__.update(Template.globals)
        self.mod.__dict__.update(kw.get('globals', {}))

    def _load_template(self, name):
        t = getattr(self.mod, name)
        import types
        if isinstance(t, types.ModuleType):
            return GAE_Render(t, cache=self._cache is not None, base=self._base, **self._keywords)
        else:
            return t

render = Render
# setup render for Google App Engine.
try:
    from google import appengine
    render = Render = GAE_Render
except ImportError:
    pass
        
def frender(path, **keywords):
    """Creates a template from the given file path.
    """
    return Template(open(path).read(), filename=path, **keywords)
    
def compile_templates(root):
    """Compiles templates to python code."""
    re_start = re_compile('^', re.M)
    
    for dirpath, dirnames, filenames in os.walk(root):
        filenames = [f for f in filenames if not f.startswith('.') and not f.endswith('~') and not f.startswith('__init__.py')]

        for d in dirnames[:]:
            if d.startswith('.'):
                dirnames.remove(d) # don't visit this dir

        out = open(os.path.join(dirpath, '__init__.py'), 'w')
        out.write('from web.template import CompiledTemplate, ForLoop\n\n')
        if dirnames:
            out.write("import " + ", ".join(dirnames))

        for f in filenames:
            path = os.path.join(dirpath, f)

            if '.' in f:
                name, _ = f.split('.', 1)
            else:
                name = f
                
            text = open(path).read()
            text = Template.normalize_text(text)
            code = Template.generate_code(text, path)
            code = re_start.sub('    ', code)
                        
            _gen = '' + \
            '\ndef %s():' + \
            '\n    loop = ForLoop()' + \
            '\n    _dummy  = CompiledTemplate(lambda: None, "dummy")' + \
            '\n    join_ = _dummy._join' + \
            '\n    escape_ = _dummy._escape' + \
            '\n' + \
            '\n%s' + \
            '\n    return __template__'
            
            gen_code = _gen % (name, code)
            out.write(gen_code)
            out.write('\n\n')
            out.write('%s = CompiledTemplate(%s(), %s)\n\n' % (name, name, repr(path)))

            # create template to make sure it compiles
            t = Template(open(path).read(), path)
        out.close()
                
class ParseError(Exception):
    pass
    
class SecurityError(Exception):
    """The template seems to be trying to do something naughty."""
    pass

# Enumerate all the allowed AST nodes
ALLOWED_AST_NODES = [
    "Add", "And",
#   "AssAttr",
    "AssList", "AssName", "AssTuple",
#   "Assert",
    "Assign", "AugAssign",
#   "Backquote",
    "Bitand", "Bitor", "Bitxor", "Break",
    "CallFunc","Class", "Compare", "Const", "Continue",
    "Decorators", "Dict", "Discard", "Div",
    "Ellipsis", "EmptyNode",
#   "Exec",
    "Expression", "FloorDiv", "For",
#   "From",
    "Function", 
    "GenExpr", "GenExprFor", "GenExprIf", "GenExprInner",
    "Getattr", 
#   "Global", 
    "If", "IfExp",
#   "Import",
    "Invert", "Keyword", "Lambda", "LeftShift",
    "List", "ListComp", "ListCompFor", "ListCompIf", "Mod",
    "Module",
    "Mul", "Name", "Not", "Or", "Pass", "Power",
#   "Print", "Printnl", "Raise",
    "Return", "RightShift", "Slice", "Sliceobj",
    "Stmt", "Sub", "Subscript",
#   "TryExcept", "TryFinally",
    "Tuple", "UnaryAdd", "UnarySub",
    "While", "With", "Yield",
]

class SafeVisitor(object):
    """
    Make sure code is safe by walking through the AST.
    
    Code considered unsafe if:
        * it has restricted AST nodes
        * it is trying to access resricted attributes   
        
    Adopted from http://www.zafar.se/bkz/uploads/safe.txt (public domain, Babar K. Zafar)
    """
    def __init__(self):
        "Initialize visitor by generating callbacks for all AST node types."
        self.errors = []

    def walk(self, ast, filename):
        "Validate each node in AST and raise SecurityError if the code is not safe."
        self.filename = filename
        self.visit(ast)
        
        if self.errors:        
            raise SecurityError, '\n'.join([str(err) for err in self.errors])
        
    def visit(self, node, *args):
        "Recursively validate node and all of its children."
        def classname(obj):
            return obj.__class__.__name__
        nodename = classname(node)
        fn = getattr(self, 'visit' + nodename, None)
        
        if fn:
            fn(node, *args)
        else:
            if nodename not in ALLOWED_AST_NODES:
                self.fail(node, *args)
            
        for child in node.getChildNodes():
            self.visit(child, *args)

    def visitName(self, node, *args):
        "Disallow any attempts to access a restricted attr."
        #self.assert_attr(node.getChildren()[0], node)
        pass
        
    def visitGetattr(self, node, *args):
        "Disallow any attempts to access a restricted attribute."
        self.assert_attr(node.attrname, node)
            
    def assert_attr(self, attrname, node):
        if self.is_unallowed_attr(attrname):
            lineno = self.get_node_lineno(node)
            e = SecurityError("%s:%d - access to attribute '%s' is denied" % (self.filename, lineno, attrname))
            self.errors.append(e)

    def is_unallowed_attr(self, name):
        return name.startswith('_') \
            or name.startswith('func_') \
            or name.startswith('im_')
            
    def get_node_lineno(self, node):
        return (node.lineno) and node.lineno or 0
        
    def fail(self, node, *args):
        "Default callback for unallowed AST nodes."
        lineno = self.get_node_lineno(node)
        nodename = node.__class__.__name__
        e = SecurityError("%s:%d - execution of '%s' statements is denied" % (self.filename, lineno, nodename))
        self.errors.append(e)

class TemplateResult(storage):
    """Dictionary like object for storing template output.
    
    A template can specify key-value pairs in the output using 
    `var` statements. Each `var` statement adds a new key to the 
    template output and the main output is stored with key 
    __body__.
    
        >>> d = TemplateResult(__body__='hello, world', x='foo')
        >>> d
        <TemplateResult: {'__body__': 'hello, world', 'x': 'foo'}>
        >>> print d
        hello, world
    """
    def __unicode__(self): 
        return safeunicode(self.get('__body__', ''))
    
    def __str__(self):
        return safestr(self.get('__body__', ''))
        
    def __repr__(self):
        return "<TemplateResult: %s>" % dict.__repr__(self)
    
def test():
    r"""Doctest for testing template module.

    Define a utility function to run template test.
    
        >>> class TestResult(TemplateResult):
        ...     def __repr__(self): return repr(unicode(self))
        ...
        >>> def t(code, **keywords):
        ...     tmpl = Template(code, **keywords)
        ...     return lambda *a, **kw: TestResult(tmpl(*a, **kw))
        ...
    
    Simple tests.
    
        >>> t('1')()
        u'1\n'
        >>> t('$def with ()\n1')()
        u'1\n'
        >>> t('$def with (a)\n$a')(1)
        u'1\n'
        >>> t('$def with (a=0)\n$a')(1)
        u'1\n'
        >>> t('$def with (a=0)\n$a')(a=1)
        u'1\n'
    
    Test complicated expressions.
        
        >>> t('$def with (x)\n$x.upper()')('hello')
        u'HELLO\n'
        >>> t('$(2 * 3 + 4 * 5)')()
        u'26\n'
        >>> t('${2 * 3 + 4 * 5}')()
        u'26\n'
        >>> t('$def with (limit)\nkeep $(limit)ing.')('go')
        u'keep going.\n'
        >>> t('$def with (a)\n$a.b[0]')(storage(b=[1]))
        u'1\n'
        
    Test html escaping.
    
        >>> t('$def with (x)\n$x', filename='a.html')('<html>')
        u'&lt;html&gt;\n'
        >>> t('$def with (x)\n$x', filename='a.txt')('<html>')
        u'<html>\n'
                
    Test if, for and while.
    
        >>> t('$if 1: 1')()
        u'1\n'
        >>> t('$if 1:\n    1')()
        u'1\n'
        >>> t('$if 1:\n    1\\')()
        u'1'
        >>> t('$if 0: 0\n$elif 1: 1')()
        u'1\n'
        >>> t('$if 0: 0\n$elif None: 0\n$else: 1')()
        u'1\n'
        >>> t('$if 0 < 1 and 1 < 2: 1')()
        u'1\n'
        >>> t('$for x in [1, 2, 3]: $x')()
        u'1\n2\n3\n'
        >>> t('$def with (d)\n$for k, v in d.iteritems(): $k')({1: 1})
        u'1\n'
        >>> t('$for x in [1, 2, 3]:\n\t$x')()
        u'    1\n    2\n    3\n'
        >>> t('$def with (a)\n$while a and a.pop():1')([1, 2, 3])
        u'1\n1\n1\n'

    The space after : must be ignored.
    
        >>> t('$if True: foo')()
        u'foo\n'
    
    Test loop.xxx.

        >>> t("$for i in range(5):$loop.index, $loop.parity")()
        u'1, odd\n2, even\n3, odd\n4, even\n5, odd\n'
        >>> t("$for i in range(2):\n    $for j in range(2):$loop.parent.parity $loop.parity")()
        u'odd odd\nodd even\neven odd\neven even\n'
        
    Test assignment.
    
        >>> t('$ a = 1\n$a')()
        u'1\n'
        >>> t('$ a = [1]\n$a[0]')()
        u'1\n'
        >>> t('$ a = {1: 1}\n$a.keys()[0]')()
        u'1\n'
        >>> t('$ a = []\n$if not a: 1')()
        u'1\n'
        >>> t('$ a = {}\n$if not a: 1')()
        u'1\n'
        >>> t('$ a = -1\n$a')()
        u'-1\n'
        >>> t('$ a = "1"\n$a')()
        u'1\n'

    Test comments.
    
        >>> t('$# 0')()
        u'\n'
        >>> t('hello$#comment1\nhello$#comment2')()
        u'hello\nhello\n'
        >>> t('$#comment0\nhello$#comment1\nhello$#comment2')()
        u'\nhello\nhello\n'
        
    Test unicode.
    
        >>> t('$def with (a)\n$a')(u'\u203d')
        u'\u203d\n'
        >>> t('$def with (a)\n$a')(u'\u203d'.encode('utf-8'))
        u'\u203d\n'
        >>> t(u'$def with (a)\n$a $:a')(u'\u203d')
        u'\u203d \u203d\n'
        >>> t(u'$def with ()\nfoo')()
        u'foo\n'
        >>> def f(x): return x
        ...
        >>> t(u'$def with (f)\n$:f("x")')(f)
        u'x\n'
        >>> t('$def with (f)\n$:f("x")')(f)
        u'x\n'
    
    Test dollar escaping.
    
        >>> t("Stop, $$money isn't evaluated.")()
        u"Stop, $money isn't evaluated.\n"
        >>> t("Stop, \$money isn't evaluated.")()
        u"Stop, $money isn't evaluated.\n"
        
    Test space sensitivity.
    
        >>> t('$def with (x)\n$x')(1)
        u'1\n'
        >>> t('$def with(x ,y)\n$x')(1, 1)
        u'1\n'
        >>> t('$(1 + 2*3 + 4)')()
        u'11\n'
        
    Make sure globals are working.
            
        >>> t('$x')()
        Traceback (most recent call last):
            ...
        NameError: global name 'x' is not defined
        >>> t('$x', globals={'x': 1})()
        u'1\n'
        
    Can't change globals.
    
        >>> t('$ x = 2\n$x', globals={'x': 1})()
        u'2\n'
        >>> t('$ x = x + 1\n$x', globals={'x': 1})()
        Traceback (most recent call last):
            ...
        UnboundLocalError: local variable 'x' referenced before assignment
    
    Make sure builtins are customizable.
    
        >>> t('$min(1, 2)')()
        u'1\n'
        >>> t('$min(1, 2)', builtins={})()
        Traceback (most recent call last):
            ...
        NameError: global name 'min' is not defined
        
    Test vars.
    
        >>> x = t('$var x: 1')()
        >>> x.x
        u'1'
        >>> x = t('$var x = 1')()
        >>> x.x
        1
        >>> x = t('$var x:  \n    foo\n    bar')()
        >>> x.x
        u'foo\nbar\n'

    Test BOM chars.

        >>> t('\xef\xbb\xbf$def with(x)\n$x')('foo')
        u'foo\n'

    Test for with weird cases.

        >>> t('$for i in range(10)[1:5]:\n    $i')()
        u'1\n2\n3\n4\n'
        >>> t("$for k, v in {'a': 1, 'b': 2}.items():\n    $k $v")()
        u'a 1\nb 2\n'
        >>> t("$for k, v in ({'a': 1, 'b': 2}.items():\n    $k $v")()
        Traceback (most recent call last):
            ...
        SyntaxError: invalid syntax

    Test datetime.

        >>> import datetime
        >>> t("$def with (date)\n$date.strftime('%m %Y')")(datetime.datetime(2009, 1, 1))
        u'01 2009\n'
    """
    pass
            
if __name__ == "__main__":
    import sys
    if '--compile' in sys.argv:
        compile_templates(sys.argv[2])
    else:
        import doctest
        doctest.testmod()

########NEW FILE########
__FILENAME__ = test
"""test utilities
(part of web.py)
"""
import unittest
import sys, os
import web

TestCase = unittest.TestCase
TestSuite = unittest.TestSuite

def load_modules(names):
    return [__import__(name, None, None, "x") for name in names]

def module_suite(module, classnames=None):
    """Makes a suite from a module."""
    if classnames:
        return unittest.TestLoader().loadTestsFromNames(classnames, module)
    elif hasattr(module, 'suite'):
        return module.suite()
    else:
        return unittest.TestLoader().loadTestsFromModule(module)

def doctest_suite(module_names):
    """Makes a test suite from doctests."""
    import doctest
    suite = TestSuite()
    for mod in load_modules(module_names):
        suite.addTest(doctest.DocTestSuite(mod))
    return suite
    
def suite(module_names):
    """Creates a suite from multiple modules."""
    suite = TestSuite()
    for mod in load_modules(module_names):
        suite.addTest(module_suite(mod))
    return suite

def runTests(suite):
    runner = unittest.TextTestRunner()
    return runner.run(suite)

def main(suite=None):
    if not suite:
        main_module = __import__('__main__')
        # allow command line switches
        args = [a for a in sys.argv[1:] if not a.startswith('-')]
        suite = module_suite(main_module, args or None)

    result = runTests(suite)
    sys.exit(not result.wasSuccessful())


########NEW FILE########
__FILENAME__ = utils
#!/usr/bin/env python
"""
General Utilities
(part of web.py)
"""

__all__ = [
  "Storage", "storage", "storify", 
  "iters", 
  "rstrips", "lstrips", "strips", 
  "safeunicode", "safestr", "utf8",
  "TimeoutError", "timelimit",
  "Memoize", "memoize",
  "re_compile", "re_subm",
  "group", "uniq", "iterview",
  "IterBetter", "iterbetter",
  "dictreverse", "dictfind", "dictfindall", "dictincr", "dictadd",
  "listget", "intget", "datestr",
  "numify", "denumify", "commify", "dateify",
  "nthstr",
  "CaptureStdout", "capturestdout", "Profile", "profile",
  "tryall",
  "ThreadedDict", "threadeddict",
  "autoassign",
  "to36",
  "safemarkdown",
  "sendmail"
]

import re, sys, time, threading, itertools

try:
    import subprocess
except ImportError: 
    subprocess = None

try: import datetime
except ImportError: pass

try: set
except NameError:
    from sets import Set as set

class Storage(dict):
    """
    A Storage object is like a dictionary except `obj.foo` can be used
    in addition to `obj['foo']`.
    
        >>> o = storage(a=1)
        >>> o.a
        1
        >>> o['a']
        1
        >>> o.a = 2
        >>> o['a']
        2
        >>> del o.a
        >>> o.a
        Traceback (most recent call last):
            ...
        AttributeError: 'a'
    
    """
    def __getattr__(self, key): 
        try:
            return self[key]
        except KeyError, k:
            raise AttributeError, k
    
    def __setattr__(self, key, value): 
        self[key] = value
    
    def __delattr__(self, key):
        try:
            del self[key]
        except KeyError, k:
            raise AttributeError, k
    
    def __repr__(self):     
        return '<Storage ' + dict.__repr__(self) + '>'

storage = Storage

def storify(mapping, *requireds, **defaults):
    """
    Creates a `storage` object from dictionary `mapping`, raising `KeyError` if
    d doesn't have all of the keys in `requireds` and using the default 
    values for keys found in `defaults`.

    For example, `storify({'a':1, 'c':3}, b=2, c=0)` will return the equivalent of
    `storage({'a':1, 'b':2, 'c':3})`.
    
    If a `storify` value is a list (e.g. multiple values in a form submission), 
    `storify` returns the last element of the list, unless the key appears in 
    `defaults` as a list. Thus:
    
        >>> storify({'a':[1, 2]}).a
        2
        >>> storify({'a':[1, 2]}, a=[]).a
        [1, 2]
        >>> storify({'a':1}, a=[]).a
        [1]
        >>> storify({}, a=[]).a
        []
    
    Similarly, if the value has a `value` attribute, `storify will return _its_
    value, unless the key appears in `defaults` as a dictionary.
    
        >>> storify({'a':storage(value=1)}).a
        1
        >>> storify({'a':storage(value=1)}, a={}).a
        <Storage {'value': 1}>
        >>> storify({}, a={}).a
        {}
        
    Optionally, keyword parameter `_unicode` can be passed to convert all values to unicode.
    
        >>> storify({'x': 'a'}, _unicode=True)
        <Storage {'x': u'a'}>
        >>> storify({'x': storage(value='a')}, x={}, _unicode=True)
        <Storage {'x': <Storage {'value': 'a'}>}>
        >>> storify({'x': storage(value='a')}, _unicode=True)
        <Storage {'x': u'a'}>
    """
    _unicode = defaults.pop('_unicode', False)
    def unicodify(s):
        if _unicode and isinstance(s, str): return safeunicode(s)
        else: return s
        
    def getvalue(x):
        if hasattr(x, 'value'):
            return unicodify(x.value)
        else:
            return unicodify(x)
    
    stor = Storage()
    for key in requireds + tuple(mapping.keys()):
        value = mapping[key]
        if isinstance(value, list):
            if isinstance(defaults.get(key), list):
                value = [getvalue(x) for x in value]
            else:
                value = value[-1]
        if not isinstance(defaults.get(key), dict):
            value = getvalue(value)
        if isinstance(defaults.get(key), list) and not isinstance(value, list):
            value = [value]
        setattr(stor, key, value)

    for (key, value) in defaults.iteritems():
        result = value
        if hasattr(stor, key): 
            result = stor[key]
        if value == () and not isinstance(result, tuple): 
            result = (result,)
        setattr(stor, key, result)
    
    return stor

iters = [list, tuple]
import __builtin__
if hasattr(__builtin__, 'set'):
    iters.append(set)
if hasattr(__builtin__, 'frozenset'):
    iters.append(set)
if sys.version_info < (2,6): # sets module deprecated in 2.6
    try:
        from sets import Set
        iters.append(Set)
    except ImportError: 
        pass
    
class _hack(tuple): pass
iters = _hack(iters)
iters.__doc__ = """
A list of iterable items (like lists, but not strings). Includes whichever
of lists, tuples, sets, and Sets are available in this version of Python.
"""

def _strips(direction, text, remove):
    if direction == 'l': 
        if text.startswith(remove): 
            return text[len(remove):]
    elif direction == 'r':
        if text.endswith(remove):   
            return text[:-len(remove)]
    else: 
        raise ValueError, "Direction needs to be r or l."
    return text

def rstrips(text, remove):
    """
    removes the string `remove` from the right of `text`

        >>> rstrips("foobar", "bar")
        'foo'
    
    """
    return _strips('r', text, remove)

def lstrips(text, remove):
    """
    removes the string `remove` from the left of `text`
    
        >>> lstrips("foobar", "foo")
        'bar'
    
    """
    return _strips('l', text, remove)

def strips(text, remove):
    """
    removes the string `remove` from the both sides of `text`

        >>> strips("foobarfoo", "foo")
        'bar'
    
    """
    return rstrips(lstrips(text, remove), remove)

def safeunicode(obj, encoding='utf-8'):
    r"""
    Converts any given object to unicode string.
    
        >>> safeunicode('hello')
        u'hello'
        >>> safeunicode(2)
        u'2'
        >>> safeunicode('\xe1\x88\xb4')
        u'\u1234'
    """
    if isinstance(obj, unicode):
        return obj
    elif isinstance(obj, str):
        return obj.decode(encoding)
    else:
        if hasattr(obj, '__unicode__'):
            return unicode(obj)
        else:
            return str(obj).decode(encoding)
    
def safestr(obj, encoding='utf-8'):
    r"""
    Converts any given object to utf-8 encoded string. 
    
        >>> safestr('hello')
        'hello'
        >>> safestr(u'\u1234')
        '\xe1\x88\xb4'
        >>> safestr(2)
        '2'
    """
    if isinstance(obj, unicode):
        return obj.encode('utf-8')
    elif isinstance(obj, str):
        return obj
    elif hasattr(obj, 'next') and hasattr(obj, '__iter__'): # iterator
        return itertools.imap(safestr, obj)
    else:
        return str(obj)

# for backward-compatibility
utf8 = safestr
    
class TimeoutError(Exception): pass
def timelimit(timeout):
    """
    A decorator to limit a function to `timeout` seconds, raising `TimeoutError`
    if it takes longer.
    
        >>> import time
        >>> def meaningoflife():
        ...     time.sleep(.2)
        ...     return 42
        >>> 
        >>> timelimit(.1)(meaningoflife)()
        Traceback (most recent call last):
            ...
        TimeoutError: took too long
        >>> timelimit(1)(meaningoflife)()
        42

    _Caveat:_ The function isn't stopped after `timeout` seconds but continues 
    executing in a separate thread. (There seems to be no way to kill a thread.)

    inspired by <http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/473878>
    """
    def _1(function):
        def _2(*args, **kw):
            class Dispatch(threading.Thread):
                def __init__(self):
                    threading.Thread.__init__(self)
                    self.result = None
                    self.error = None

                    self.setDaemon(True)
                    self.start()

                def run(self):
                    try:
                        self.result = function(*args, **kw)
                    except:
                        self.error = sys.exc_info()

            c = Dispatch()
            c.join(timeout)
            if c.isAlive():
                raise TimeoutError, 'took too long'
            if c.error:
                raise c.error[0], c.error[1]
            return c.result
        return _2
    return _1

class Memoize:
    """
    'Memoizes' a function, caching its return values for each input.
    If `expires` is specified, values are recalculated after `expires` seconds.
    If `background` is specified, values are recalculated in a separate thread.
    
        >>> calls = 0
        >>> def howmanytimeshaveibeencalled():
        ...     global calls
        ...     calls += 1
        ...     return calls
        >>> fastcalls = memoize(howmanytimeshaveibeencalled)
        >>> howmanytimeshaveibeencalled()
        1
        >>> howmanytimeshaveibeencalled()
        2
        >>> fastcalls()
        3
        >>> fastcalls()
        3
        >>> import time
        >>> fastcalls = memoize(howmanytimeshaveibeencalled, .1, background=False)
        >>> fastcalls()
        4
        >>> fastcalls()
        4
        >>> time.sleep(.2)
        >>> fastcalls()
        5
        >>> def slowfunc():
        ...     time.sleep(.1)
        ...     return howmanytimeshaveibeencalled()
        >>> fastcalls = memoize(slowfunc, .2, background=True)
        >>> fastcalls()
        6
        >>> timelimit(.05)(fastcalls)()
        6
        >>> time.sleep(.2)
        >>> timelimit(.05)(fastcalls)()
        6
        >>> timelimit(.05)(fastcalls)()
        6
        >>> time.sleep(.2)
        >>> timelimit(.05)(fastcalls)()
        7
        >>> fastcalls = memoize(slowfunc, None, background=True)
        >>> threading.Thread(target=fastcalls).start()
        >>> time.sleep(.01)
        >>> fastcalls()
        9
    """
    def __init__(self, func, expires=None, background=True): 
        self.func = func
        self.cache = {}
        self.expires = expires
        self.background = background
        self.running = {}
    
    def __call__(self, *args, **keywords):
        key = (args, tuple(keywords.items()))
        if not self.running.get(key):
            self.running[key] = threading.Lock()
        def update(block=False):
            if self.running[key].acquire(block):
                try:
                    self.cache[key] = (self.func(*args, **keywords), time.time())
                finally:
                    self.running[key].release()
        
        if key not in self.cache: 
            update(block=True)
        elif self.expires and (time.time() - self.cache[key][1]) > self.expires:
            if self.background:
                threading.Thread(target=update).start()
            else:
                update()
        return self.cache[key][0]

memoize = Memoize

re_compile = memoize(re.compile) #@@ threadsafe?
re_compile.__doc__ = """
A memoized version of re.compile.
"""

class _re_subm_proxy:
    def __init__(self): 
        self.match = None
    def __call__(self, match): 
        self.match = match
        return ''

def re_subm(pat, repl, string):
    """
    Like re.sub, but returns the replacement _and_ the match object.
    
        >>> t, m = re_subm('g(oo+)fball', r'f\\1lish', 'goooooofball')
        >>> t
        'foooooolish'
        >>> m.groups()
        ('oooooo',)
    """
    compiled_pat = re_compile(pat)
    proxy = _re_subm_proxy()
    compiled_pat.sub(proxy.__call__, string)
    return compiled_pat.sub(repl, string), proxy.match

def group(seq, size): 
    """
    Returns an iterator over a series of lists of length size from iterable.

        >>> list(group([1,2,3,4], 2))
        [[1, 2], [3, 4]]
    """
    if not hasattr(seq, 'next'):  
        seq = iter(seq)
    while True: 
        yield [seq.next() for i in xrange(size)]

def uniq(seq):
   """
   Removes duplicate elements from a list.

       >>> uniq([1,2,3,1,4,5,6])
       [1, 2, 3, 4, 5, 6]
   """
   seen = set()
   result = []
   for item in seq:
       if item in seen: continue
       seen.add(item)
       result.append(item)
   return result

def iterview(x):
   """
   Takes an iterable `x` and returns an iterator over it
   which prints its progress to stderr as it iterates through.
   """
   WIDTH = 70

   def plainformat(n, lenx):
       return '%5.1f%% (%*d/%d)' % ((float(n)/lenx)*100, len(str(lenx)), n, lenx)

   def bars(size, n, lenx):
       val = int((float(n)*size)/lenx + 0.5)
       if size - val:
           spacing = ">" + (" "*(size-val))[1:]
       else:
           spacing = ""
       return "[%s%s]" % ("="*val, spacing)

   def eta(elapsed, n, lenx):
       if n == 0:
           return '--:--:--'
       if n == lenx:
           secs = int(elapsed)
       else:
           secs = int((elapsed/n) * (lenx-n))
       mins, secs = divmod(secs, 60)
       hrs, mins = divmod(mins, 60)

       return '%02d:%02d:%02d' % (hrs, mins, secs)

   def format(starttime, n, lenx):
       out = plainformat(n, lenx) + ' '
       if n == lenx:
           end = '     '
       else:
           end = ' ETA '
       end += eta(time.time() - starttime, n, lenx)
       out += bars(WIDTH - len(out) - len(end), n, lenx)
       out += end
       return out

   starttime = time.time()
   lenx = len(x)
   for n, y in enumerate(x):
       sys.stderr.write('\r' + format(starttime, n, lenx))
       yield y
   sys.stderr.write('\r' + format(starttime, n+1, lenx) + '\n')

class IterBetter:
    """
    Returns an object that can be used as an iterator 
    but can also be used via __getitem__ (although it 
    cannot go backwards -- that is, you cannot request 
    `iterbetter[0]` after requesting `iterbetter[1]`).
    
        >>> import itertools
        >>> c = iterbetter(itertools.count())
        >>> c[1]
        1
        >>> c[5]
        5
        >>> c[3]
        Traceback (most recent call last):
            ...
        IndexError: already passed 3
    """
    def __init__(self, iterator): 
        self.i, self.c = iterator, 0
    def __iter__(self): 
        while 1:    
            yield self.i.next()
            self.c += 1
    def __getitem__(self, i):
        #todo: slices
        if i < self.c: 
            raise IndexError, "already passed "+str(i)
        try:
            while i > self.c: 
                self.i.next()
                self.c += 1
            # now self.c == i
            self.c += 1
            return self.i.next()
        except StopIteration: 
            raise IndexError, str(i)
iterbetter = IterBetter

def dictreverse(mapping):
    """
    Returns a new dictionary with keys and values swapped.
    
        >>> dictreverse({1: 2, 3: 4})
        {2: 1, 4: 3}
    """
    return dict([(value, key) for (key, value) in mapping.iteritems()])

def dictfind(dictionary, element):
    """
    Returns a key whose value in `dictionary` is `element` 
    or, if none exists, None.
    
        >>> d = {1:2, 3:4}
        >>> dictfind(d, 4)
        3
        >>> dictfind(d, 5)
    """
    for (key, value) in dictionary.iteritems():
        if element is value: 
            return key

def dictfindall(dictionary, element):
    """
    Returns the keys whose values in `dictionary` are `element`
    or, if none exists, [].
    
        >>> d = {1:4, 3:4}
        >>> dictfindall(d, 4)
        [1, 3]
        >>> dictfindall(d, 5)
        []
    """
    res = []
    for (key, value) in dictionary.iteritems():
        if element is value:
            res.append(key)
    return res

def dictincr(dictionary, element):
    """
    Increments `element` in `dictionary`, 
    setting it to one if it doesn't exist.
    
        >>> d = {1:2, 3:4}
        >>> dictincr(d, 1)
        3
        >>> d[1]
        3
        >>> dictincr(d, 5)
        1
        >>> d[5]
        1
    """
    dictionary.setdefault(element, 0)
    dictionary[element] += 1
    return dictionary[element]

def dictadd(*dicts):
    """
    Returns a dictionary consisting of the keys in the argument dictionaries.
    If they share a key, the value from the last argument is used.
    
        >>> dictadd({1: 0, 2: 0}, {2: 1, 3: 1})
        {1: 0, 2: 1, 3: 1}
    """
    result = {}
    for dct in dicts:
        result.update(dct)
    return result

def listget(lst, ind, default=None):
    """
    Returns `lst[ind]` if it exists, `default` otherwise.
    
        >>> listget(['a'], 0)
        'a'
        >>> listget(['a'], 1)
        >>> listget(['a'], 1, 'b')
        'b'
    """
    if len(lst)-1 < ind: 
        return default
    return lst[ind]

def intget(integer, default=None):
    """
    Returns `integer` as an int or `default` if it can't.
    
        >>> intget('3')
        3
        >>> intget('3a')
        >>> intget('3a', 0)
        0
    """
    try:
        return int(integer)
    except (TypeError, ValueError):
        return default

def datestr(then, now=None):
    """
    Converts a (UTC) datetime object to a nice string representation.
    
        >>> from datetime import datetime, timedelta
        >>> d = datetime(1970, 5, 1)
        >>> datestr(d, now=d)
        '0 microseconds ago'
        >>> for t, v in {
        ...   timedelta(microseconds=1): '1 microsecond ago',
        ...   timedelta(microseconds=2): '2 microseconds ago',
        ...   -timedelta(microseconds=1): '1 microsecond from now',
        ...   -timedelta(microseconds=2): '2 microseconds from now',
        ...   timedelta(microseconds=2000): '2 milliseconds ago',
        ...   timedelta(seconds=2): '2 seconds ago',
        ...   timedelta(seconds=2*60): '2 minutes ago',
        ...   timedelta(seconds=2*60*60): '2 hours ago',
        ...   timedelta(days=2): '2 days ago',
        ... }.iteritems():
        ...     assert datestr(d, now=d+t) == v
        >>> datestr(datetime(1970, 1, 1), now=d)
        'January  1'
        >>> datestr(datetime(1969, 1, 1), now=d)
        'January  1, 1969'
        >>> datestr(datetime(1970, 6, 1), now=d)
        'June  1, 1970'
        >>> datestr(None)
        ''
    """
    def agohence(n, what, divisor=None):
        if divisor: n = n // divisor

        out = str(abs(n)) + ' ' + what       # '2 day'
        if abs(n) != 1: out += 's'           # '2 days'
        out += ' '                           # '2 days '
        if n < 0:
            out += 'from now'
        else:
            out += 'ago'
        return out                           # '2 days ago'

    oneday = 24 * 60 * 60

    if not then: return ""
    if not now: now = datetime.datetime.utcnow()
    if type(now).__name__ == "DateTime":
        now = datetime.datetime.fromtimestamp(now)
    if type(then).__name__ == "DateTime":
        then = datetime.datetime.fromtimestamp(then)
    elif type(then).__name__ == "date":
        then = datetime.datetime(then.year, then.month, then.day)

    delta = now - then
    deltaseconds = int(delta.days * oneday + delta.seconds + delta.microseconds * 1e-06)
    deltadays = abs(deltaseconds) // oneday
    if deltaseconds < 0: deltadays *= -1 # fix for oddity of floor

    if deltadays:
        if abs(deltadays) < 4:
            return agohence(deltadays, 'day')

        out = then.strftime('%B %e') # e.g. 'June 13'
        if then.year != now.year or deltadays < 0:
            out += ', %s' % then.year
        return out

    if int(deltaseconds):
        if abs(deltaseconds) > (60 * 60):
            return agohence(deltaseconds, 'hour', 60 * 60)
        elif abs(deltaseconds) > 60:
            return agohence(deltaseconds, 'minute', 60)
        else:
            return agohence(deltaseconds, 'second')

    deltamicroseconds = delta.microseconds
    if delta.days: deltamicroseconds = int(delta.microseconds - 1e6) # datetime oddity
    if abs(deltamicroseconds) > 1000:
        return agohence(deltamicroseconds, 'millisecond', 1000)

    return agohence(deltamicroseconds, 'microsecond')

def numify(string):
    """
    Removes all non-digit characters from `string`.
    
        >>> numify('800-555-1212')
        '8005551212'
        >>> numify('800.555.1212')
        '8005551212'
    
    """
    return ''.join([c for c in str(string) if c.isdigit()])

def denumify(string, pattern):
    """
    Formats `string` according to `pattern`, where the letter X gets replaced
    by characters from `string`.
    
        >>> denumify("8005551212", "(XXX) XXX-XXXX")
        '(800) 555-1212'
    
    """
    out = []
    for c in pattern:
        if c == "X":
            out.append(string[0])
            string = string[1:]
        else:
            out.append(c)
    return ''.join(out)

def commify(n):
    """
    Add commas to an integer `n`.

        >>> commify(1)
        '1'
        >>> commify(123)
        '123'
        >>> commify(1234)
        '1,234'
        >>> commify(1234567890)
        '1,234,567,890'
        >>> commify(123.0)
        '123.0'
        >>> commify(1234.5)
        '1,234.5'
        >>> commify(1234.56789)
        '1,234.56789'
        >>> commify('%.2f' % 1234.5)
        '1,234.50'
        >>> commify(None)
        >>>

    """
    if n is None: return None
    n = str(n)
    if '.' in n:
        dollars, cents = n.split('.')
    else:
        dollars, cents = n, None

    r = []
    for i, c in enumerate(str(dollars)[::-1]):
        if i and (not (i % 3)):
            r.insert(0, ',')
        r.insert(0, c)
    out = ''.join(r)
    if cents:
        out += '.' + cents
    return out

def dateify(datestring):
    """
    Formats a numified `datestring` properly.
    """
    return denumify(datestring, "XXXX-XX-XX XX:XX:XX")


def nthstr(n):
    """
    Formats an ordinal.
    Doesn't handle negative numbers.

        >>> nthstr(1)
        '1st'
        >>> nthstr(0)
        '0th'
        >>> [nthstr(x) for x in [2, 3, 4, 5, 10, 11, 12, 13, 14, 15]]
        ['2nd', '3rd', '4th', '5th', '10th', '11th', '12th', '13th', '14th', '15th']
        >>> [nthstr(x) for x in [91, 92, 93, 94, 99, 100, 101, 102]]
        ['91st', '92nd', '93rd', '94th', '99th', '100th', '101st', '102nd']
        >>> [nthstr(x) for x in [111, 112, 113, 114, 115]]
        ['111th', '112th', '113th', '114th', '115th']

    """
    
    assert n >= 0
    if n % 100 in [11, 12, 13]: return '%sth' % n
    return {1: '%sst', 2: '%snd', 3: '%srd'}.get(n % 10, '%sth') % n

def cond(predicate, consequence, alternative=None):
    """
    Function replacement for if-else to use in expressions.
        
        >>> x = 2
        >>> cond(x % 2 == 0, "even", "odd")
        'even'
        >>> cond(x % 2 == 0, "even", "odd") + '_row'
        'even_row'
    """
    if predicate:
        return consequence
    else:
        return alternative

class CaptureStdout:
    """
    Captures everything `func` prints to stdout and returns it instead.
    
        >>> def idiot():
        ...     print "foo"
        >>> capturestdout(idiot)()
        'foo\\n'
    
    **WARNING:** Not threadsafe!
    """
    def __init__(self, func): 
        self.func = func
    def __call__(self, *args, **keywords):
        from cStringIO import StringIO
        # Not threadsafe!
        out = StringIO()
        oldstdout = sys.stdout
        sys.stdout = out
        try: 
            self.func(*args, **keywords)
        finally: 
            sys.stdout = oldstdout
        return out.getvalue()

capturestdout = CaptureStdout

class Profile:
    """
    Profiles `func` and returns a tuple containing its output
    and a string with human-readable profiling information.
        
        >>> import time
        >>> out, inf = profile(time.sleep)(.001)
        >>> out
        >>> inf[:10].strip()
        'took 0.0'
    """
    def __init__(self, func): 
        self.func = func
    def __call__(self, *args): ##, **kw):   kw unused
        import hotshot, hotshot.stats, tempfile ##, time already imported
        temp = tempfile.NamedTemporaryFile()
        prof = hotshot.Profile(temp.name)

        stime = time.time()
        result = prof.runcall(self.func, *args)
        stime = time.time() - stime
        prof.close()

        import cStringIO
        out = cStringIO.StringIO()
        stats = hotshot.stats.load(temp.name)
        stats.stream = out
        stats.strip_dirs()
        stats.sort_stats('time', 'calls')
        stats.print_stats(40)
        stats.print_callers()

        x =  '\n\ntook '+ str(stime) + ' seconds\n'
        x += out.getvalue()

        return result, x

profile = Profile


import traceback
# hack for compatibility with Python 2.3:
if not hasattr(traceback, 'format_exc'):
    from cStringIO import StringIO
    def format_exc(limit=None):
        strbuf = StringIO()
        traceback.print_exc(limit, strbuf)
        return strbuf.getvalue()
    traceback.format_exc = format_exc

def tryall(context, prefix=None):
    """
    Tries a series of functions and prints their results. 
    `context` is a dictionary mapping names to values; 
    the value will only be tried if it's callable.
    
        >>> tryall(dict(j=lambda: True))
        j: True
        ----------------------------------------
        results:
           True: 1

    For example, you might have a file `test/stuff.py` 
    with a series of functions testing various things in it. 
    At the bottom, have a line:

        if __name__ == "__main__": tryall(globals())

    Then you can run `python test/stuff.py` and get the results of 
    all the tests.
    """
    context = context.copy() # vars() would update
    results = {}
    for (key, value) in context.iteritems():
        if not hasattr(value, '__call__'): 
            continue
        if prefix and not key.startswith(prefix): 
            continue
        print key + ':',
        try:
            r = value()
            dictincr(results, r)
            print r
        except:
            print 'ERROR'
            dictincr(results, 'ERROR')
            print '   ' + '\n   '.join(traceback.format_exc().split('\n'))
        
    print '-'*40
    print 'results:'
    for (key, value) in results.iteritems():
        print ' '*2, str(key)+':', value
        
class ThreadedDict:
    """
    Thread local storage.
    
        >>> d = ThreadedDict()
        >>> d.x = 1
        >>> d.x
        1
        >>> import threading
        >>> def f(): d.x = 2
        ...
        >>> t = threading.Thread(target=f)
        >>> t.start()
        >>> t.join()
        >>> d.x
        1
    """
    def __getattr__(self, key):
        return getattr(self._getd(), key)

    def __setattr__(self, key, value):
        return setattr(self._getd(), key, value)

    def __delattr__(self, key):
        return delattr(self._getd(), key)

    def __hash__(self): 
        return id(self)

    def _getd(self):
        t = threading.currentThread()
        if not hasattr(t, '_d'):
            # using __dict__ of thread as thread local storage
            t._d = {}

        # there could be multiple instances of ThreadedDict.
        # use self as key
        if self not in t._d:
            t._d[self] = storage()
        return t._d[self]

threadeddict = ThreadedDict

def autoassign(self, locals):
    """
    Automatically assigns local variables to `self`.
    
        >>> self = storage()
        >>> autoassign(self, dict(a=1, b=2))
        >>> self
        <Storage {'a': 1, 'b': 2}>
    
    Generally used in `__init__` methods, as in:

        def __init__(self, foo, bar, baz=1): autoassign(self, locals())
    """
    for (key, value) in locals.iteritems():
        if key == 'self': 
            continue
        setattr(self, key, value)

def to36(q):
    """
    Converts an integer to base 36 (a useful scheme for human-sayable IDs).
    
        >>> to36(35)
        'z'
        >>> to36(119292)
        '2k1o'
        >>> int(to36(939387374), 36)
        939387374
        >>> to36(0)
        '0'
        >>> to36(-393)
        Traceback (most recent call last):
            ... 
        ValueError: must supply a positive integer
    
    """
    if q < 0: raise ValueError, "must supply a positive integer"
    letters = "0123456789abcdefghijklmnopqrstuvwxyz"
    converted = []
    while q != 0:
        q, r = divmod(q, 36)
        converted.insert(0, letters[r])
    return "".join(converted) or '0'


r_url = re_compile('(?<!\()(http://(\S+))')
def safemarkdown(text):
    """
    Converts text to HTML following the rules of Markdown, but blocking any
    outside HTML input, so that only the things supported by Markdown
    can be used. Also converts raw URLs to links.

    (requires [markdown.py](http://webpy.org/markdown.py))
    """
    from markdown import markdown
    if text:
        text = text.replace('<', '&lt;')
        # TODO: automatically get page title?
        text = r_url.sub(r'<\1>', text)
        text = markdown(text)
        return text

def sendmail(from_address, to_address, subject, message, headers=None, **kw):
    """
    Sends the email message `message` with mail and envelope headers
    for from `from_address_` to `to_address` with `subject`. 
    Additional email headers can be specified with the dictionary 
    `headers.

    If `web.config.smtp_server` is set, it will send the message
    to that SMTP server. Otherwise it will look for 
    `/usr/sbin/sendmail`, the typical location for the sendmail-style
    binary. To use sendmail from a different path, set `web.config.sendmail_path`.
    """
    try:
        import webapi
    except ImportError:
        webapi = Storage(config=Storage())
    
    if headers is None: headers = {}
    
    cc = kw.get('cc', [])
    bcc = kw.get('bcc', [])
    
    def listify(x):
        if not isinstance(x, list):
            return [safestr(x)]
        else:
            return [safestr(a) for a in x]

    from_address = safestr(from_address)

    to_address = listify(to_address)
    cc = listify(cc)
    bcc = listify(bcc)

    recipients = to_address + cc + bcc
    
    headers = dictadd({
      'MIME-Version': '1.0',
      'Content-Type': 'text/plain; charset=UTF-8',
      'Content-Disposition': 'inline',
      'From': from_address,
      'To': ", ".join(to_address),
      'Subject': subject
    }, headers)

    if cc:
        headers['Cc'] = ", ".join(cc)
    
    import email.Utils
    from_address = email.Utils.parseaddr(from_address)[1]
    recipients = [email.Utils.parseaddr(r)[1] for r in recipients]
    message = ('\n'.join([safestr('%s: %s' % x) for x in headers.iteritems()])
      + "\n\n" +  safestr(message))

    if webapi.config.get('smtp_server'):
        server = webapi.config.get('smtp_server')
        port = webapi.config.get('smtp_port', 0)
        username = webapi.config.get('smtp_username') 
        password = webapi.config.get('smtp_password')
        debug_level = webapi.config.get('smtp_debuglevel', None)
        starttls = webapi.config.get('smtp_starttls', False)

        import smtplib
        smtpserver = smtplib.SMTP(server, port)

        if debug_level:
            smtpserver.set_debuglevel(debug_level)

        if starttls:
            smtpserver.ehlo()
            smtpserver.starttls()
            smtpserver.ehlo()

        if username and password:
            smtpserver.login(username, password)

        smtpserver.sendmail(from_address, recipients, message)
        smtpserver.quit()
    else:
        sendmail = webapi.config.get('sendmail_path', '/usr/sbin/sendmail')
        
        assert not from_address.startswith('-'), 'security'
        for r in recipients:
            assert not r.startswith('-'), 'security'
                
        cmd = [sendmail, '-f', from_address] + recipients

        if subprocess:
            p = subprocess.Popen(cmd, stdin=subprocess.PIPE)
            p.stdin.write(message)
            p.stdin.close()
            p.wait()
        else:
            import os
            i, o = os.popen2(cmd)
            i.write(message)
            i.close()
            o.close()
            del i, o

if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = webapi
"""
Web API (wrapper around WSGI)
(from web.py)
"""

__all__ = [
    "config",
    "header", "debug",
    "input", "data",
    "setcookie", "cookies",
    "ctx", 
    "HTTPError", 

    # 200, 201, 202
    "OK", "Created", "Accepted",    
    "ok", "created", "accepted",
    
    # 301, 302, 303, 304, 407
    "Redirect", "Found", "SeeOther", "NotModified", "TempRedirect", 
    "redirect", "found", "seeother", "notmodified", "tempredirect",

    # 400, 401, 403, 404, 405, 406, 409, 410, 412
    "BadRequest", "Unauthorized", "Forbidden", "NoMethod", "NotFound", "NotAcceptable", "Conflict", "Gone", "PreconditionFailed",
    "badrequest", "unauthorized", "forbidden", "nomethod", "notfound", "notacceptable", "conflict", "gone", "preconditionfailed",

    # 500
    "InternalError", 
    "internalerror",
]

import sys, cgi, Cookie, pprint, urlparse, urllib
from utils import storage, storify, threadeddict, dictadd, intget, utf8

config = storage()
config.__doc__ = """
A configuration object for various aspects of web.py.

`debug`
   : when True, enables reloading, disabled template caching and sets internalerror to debugerror.
"""

class HTTPError(Exception):
    def __init__(self, status, headers={}, data=""):
        ctx.status = status
        for k, v in headers.items():
            header(k, v)
        self.data = data
        Exception.__init__(self, status)
        
def _status_code(status, data=None, classname=None, docstring=None):
    if data is None:
        data = status.split(" ", 1)[1]
    classname = status.split(" ", 1)[1].replace(' ', '') # 304 Not Modified -> NotModified    
    docstring = docstring or '`%s` status' % status

    def __init__(self, data=data, headers={}):
        HTTPError.__init__(self, status, headers, data)
        
    # trick to create class dynamically with dynamic docstring.
    return type(classname, (HTTPError, object), {
        '__doc__': docstring,
        '__init__': __init__
    })

ok = OK = _status_code("200 OK", data="")
created = Created = _status_code("201 Created")
accepted = Accepted = _status_code("202 Accepted")

class Redirect(HTTPError):
    """A `301 Moved Permanently` redirect."""
    def __init__(self, url, status='301 Moved Permanently', absolute=False):
        """
        Returns a `status` redirect to the new URL. 
        `url` is joined with the base URL so that things like 
        `redirect("about") will work properly.
        """
        newloc = urlparse.urljoin(ctx.path, url)

        if newloc.startswith('/'):
            if absolute:
                home = ctx.realhome
            else:
                home = ctx.home
            newloc = home + newloc

        headers = {
            'Content-Type': 'text/html',
            'Location': newloc
        }
        HTTPError.__init__(self, status, headers, "")

redirect = Redirect

class Found(Redirect):
    """A `302 Found` redirect."""
    def __init__(self, url, absolute=False):
        Redirect.__init__(self, url, '302 Found', absolute=absolute)

found = Found

class SeeOther(Redirect):
    """A `303 See Other` redirect."""
    def __init__(self, url, absolute=False):
        Redirect.__init__(self, url, '303 See Other', absolute=absolute)
    
seeother = SeeOther

class NotModified(HTTPError):
    """A `304 Not Modified` status."""
    def __init__(self):
        HTTPError.__init__(self, "304 Not Modified")

notmodified = NotModified

class TempRedirect(Redirect):
    """A `307 Temporary Redirect` redirect."""
    def __init__(self, url, absolute=False):
        Redirect.__init__(self, url, '307 Temporary Redirect', absolute=absolute)

tempredirect = TempRedirect

class BadRequest(HTTPError):
    """`400 Bad Request` error."""
    message = "bad request"
    def __init__(self):
        status = "400 Bad Request"
        headers = {'Content-Type': 'text/html'}
        HTTPError.__init__(self, status, headers, self.message)

badrequest = BadRequest

class _NotFound(HTTPError):
    """`404 Not Found` error."""
    message = "not found"
    def __init__(self, message=None):
        status = '404 Not Found'
        headers = {'Content-Type': 'text/html'}
        HTTPError.__init__(self, status, headers, message or self.message)

def NotFound(message=None):
    """Returns HTTPError with '404 Not Found' error from the active application.
    """
    if message:
        return _NotFound(message)
    elif ctx.get('app_stack'):
        return ctx.app_stack[-1].notfound()
    else:
        return _NotFound()

notfound = NotFound

unauthorized = Unauthorized = _status_code("401 Unauthorized")
forbidden = Forbidden = _status_code("403 Forbidden")
notacceptable = NotAcceptable = _status_code("406 Not Acceptable")
conflict = Conflict = _status_code("409 Conflict")
preconditionfailed = PreconditionFailed = _status_code("412 Precondition Failed")

class NoMethod(HTTPError):
    """A `405 Method Not Allowed` error."""
    def __init__(self, cls=None):
        status = '405 Method Not Allowed'
        headers = {}
        headers['Content-Type'] = 'text/html'
        
        methods = ['GET', 'HEAD', 'POST', 'PUT', 'DELETE']
        if cls:
            methods = [method for method in methods if hasattr(cls, method)]

        headers['Allow'] = ', '.join(methods)
        data = None
        HTTPError.__init__(self, status, headers, data)
        
nomethod = NoMethod

class Gone(HTTPError):
    """`410 Gone` error."""
    message = "gone"
    def __init__(self):
        status = '410 Gone'
        headers = {'Content-Type': 'text/html'}
        HTTPError.__init__(self, status, headers, self.message)

gone = Gone

class _InternalError(HTTPError):
    """500 Internal Server Error`."""
    message = "internal server error"
    
    def __init__(self, message=None):
        status = '500 Internal Server Error'
        headers = {'Content-Type': 'text/html'}
        HTTPError.__init__(self, status, headers, message or self.message)

def InternalError(message=None):
    """Returns HTTPError with '500 internal error' error from the active application.
    """
    if message:
        return _InternalError(message)
    elif ctx.get('app_stack'):
        return ctx.app_stack[-1].internalerror()
    else:
        return _InternalError()

internalerror = InternalError

def header(hdr, value, unique=False):
    """
    Adds the header `hdr: value` with the response.
    
    If `unique` is True and a header with that name already exists,
    it doesn't add a new one. 
    """
    hdr, value = utf8(hdr), utf8(value)
    # protection against HTTP response splitting attack
    if '\n' in hdr or '\r' in hdr or '\n' in value or '\r' in value:
        raise ValueError, 'invalid characters in header'
        
    if unique is True:
        for h, v in ctx.headers:
            if h.lower() == hdr.lower(): return
    
    ctx.headers.append((hdr, value))

def input(*requireds, **defaults):
    """
    Returns a `storage` object with the GET and POST arguments. 
    See `storify` for how `requireds` and `defaults` work.
    """
    from cStringIO import StringIO
    def dictify(fs): 
        # hack to make web.input work with enctype='text/plain.
        if fs.list is None:
            fs.list = [] 

        return dict([(k, fs[k]) for k in fs.keys()])
    
    _method = defaults.pop('_method', 'both')
    
    e = ctx.env.copy()
    a = b = {}
    
    if _method.lower() in ['both', 'post', 'put']:
        if e['REQUEST_METHOD'] in ['POST', 'PUT']:
            if e.get('CONTENT_TYPE', '').lower().startswith('multipart/'):
                # since wsgi.input is directly passed to cgi.FieldStorage, 
                # it can not be called multiple times. Saving the FieldStorage
                # object in ctx to allow calling web.input multiple times.
                a = ctx.get('_fieldstorage')
                if not a:
                    fp = e['wsgi.input']
                    a = cgi.FieldStorage(fp=fp, environ=e, keep_blank_values=1)
                    ctx._fieldstorage = a
            else:
                fp = StringIO(data())
                a = cgi.FieldStorage(fp=fp, environ=e, keep_blank_values=1)
            a = dictify(a)

    if _method.lower() in ['both', 'get']:
        e['REQUEST_METHOD'] = 'GET'
        b = dictify(cgi.FieldStorage(environ=e, keep_blank_values=1))

    out = dictadd(b, a)
    try:
        defaults.setdefault('_unicode', True) # force unicode conversion by default.
        return storify(out, *requireds, **defaults)
    except KeyError:
        raise badrequest()

def data():
    """Returns the data sent with the request."""
    if 'data' not in ctx:
        cl = intget(ctx.env.get('CONTENT_LENGTH'), 0)
        ctx.data = ctx.env['wsgi.input'].read(cl)
    return ctx.data

def setcookie(name, value, expires="", domain=None, secure=False):
    """Sets a cookie."""
    if expires < 0: 
        expires = -1000000000 
    kargs = {'expires': expires, 'path':'/'}
    if domain: 
        kargs['domain'] = domain
    if secure:
        kargs['secure'] = secure
    # @@ should we limit cookies to a different path?
    cookie = Cookie.SimpleCookie()
    cookie[name] = urllib.quote(utf8(value))
    for key, val in kargs.iteritems(): 
        cookie[name][key] = val
    header('Set-Cookie', cookie.items()[0][1].OutputString())

def cookies(*requireds, **defaults):
    """
    Returns a `storage` object with all the cookies in it.
    See `storify` for how `requireds` and `defaults` work.
    """
    cookie = Cookie.SimpleCookie()
    cookie.load(ctx.env.get('HTTP_COOKIE', ''))
    try:
        d = storify(cookie, *requireds, **defaults)
        for k, v in d.items():
            d[k] = v and urllib.unquote(v)
        return d
    except KeyError:
        badrequest()
        raise StopIteration

def debug(*args):
    """
    Prints a prettyprinted version of `args` to stderr.
    """
    try: 
        out = ctx.environ['wsgi.errors']
    except: 
        out = sys.stderr
    for arg in args:
        print >> out, pprint.pformat(arg)
    return ''

def _debugwrite(x):
    try: 
        out = ctx.environ['wsgi.errors']
    except: 
        out = sys.stderr
    out.write(x)
debug.write = _debugwrite

ctx = context = threadeddict()

ctx.__doc__ = """
A `storage` object containing various information about the request:
  
`environ` (aka `env`)
   : A dictionary containing the standard WSGI environment variables.

`host`
   : The domain (`Host` header) requested by the user.

`home`
   : The base path for the application.

`ip`
   : The IP address of the requester.

`method`
   : The HTTP method used.

`path`
   : The path request.
   
`query`
   : If there are no query arguments, the empty string. Otherwise, a `?` followed
     by the query string.

`fullpath`
   : The full path requested, including query arguments (`== path + query`).

### Response Data

`status` (default: "200 OK")
   : The status code to be used in the response.

`headers`
   : A list of 2-tuples to be used in the response.

`output`
   : A string to be used as the response.
"""

########NEW FILE########
__FILENAME__ = webopenid
"""openid.py: an openid library for web.py

Notes:

 - This will create a file called .openid_secret_key in the 
   current directory with your secret key in it. If someone 
   has access to this file they can log in as any user. And 
   if the app can't find this file for any reason (e.g. you 
   moved the app somewhere else) then each currently logged 
   in user will get logged out.

 - State must be maintained through the entire auth process 
   -- this means that if you have multiple web.py processes 
   serving one set of URLs or if you restart your app often 
   then log ins will fail. You have to replace sessions and 
   store for things to work.

 - We set cookies starting with "openid_".

"""

import os
import random
import hmac
import __init__ as web
import openid.consumer.consumer
import openid.store.memstore

sessions = {}
store = openid.store.memstore.MemoryStore()

def _secret():
    try:
        secret = file('.openid_secret_key').read()
    except IOError:
        # file doesn't exist
        secret = os.urandom(20)
        file('.openid_secret_key', 'w').write(secret)
    return secret

def _hmac(identity_url):
    return hmac.new(_secret(), identity_url).hexdigest()

def _random_session():
    n = random.random()
    while n in sessions:
        n = random.random()
    n = str(n)
    return n

def status():
    oid_hash = web.cookies().get('openid_identity_hash', '').split(',', 1)
    if len(oid_hash) > 1:
        oid_hash, identity_url = oid_hash
        if oid_hash == _hmac(identity_url):
            return identity_url
    return None

def form(openid_loc):
    oid = status()
    if oid:
        return '''
        <form method="post" action="%s">
          <img src="http://openid.net/login-bg.gif" alt="OpenID" />
          <strong>%s</strong>
          <input type="hidden" name="action" value="logout" />
          <input type="hidden" name="return_to" value="%s" />
          <button type="submit">log out</button>
        </form>''' % (openid_loc, oid, web.ctx.fullpath)
    else:
        return '''
        <form method="post" action="%s">
          <input type="text" name="openid" value="" 
            style="background: url(http://openid.net/login-bg.gif) no-repeat; padding-left: 18px; background-position: 0 50%%;" />
          <input type="hidden" name="return_to" value="%s" />
          <button type="submit">log in</button>
        </form>''' % (openid_loc, web.ctx.fullpath)

def logout():
    web.setcookie('openid_identity_hash', '', expires=-1)

class host:
    def POST(self):
        # unlike the usual scheme of things, the POST is actually called
        # first here
        i = web.input(return_to='/')
        if i.get('action') == 'logout':
            logout()
            return web.redirect(i.return_to)

        i = web.input('openid', return_to='/')

        n = _random_session()
        sessions[n] = {'webpy_return_to': i.return_to}
        
        c = openid.consumer.consumer.Consumer(sessions[n], store)
        a = c.begin(i.openid)
        f = a.redirectURL(web.ctx.home, web.ctx.home + web.ctx.fullpath)

        web.setcookie('openid_session_id', n)
        return web.redirect(f)

    def GET(self):
        n = web.cookies('openid_session_id').openid_session_id
        web.setcookie('openid_session_id', '', expires=-1)
        return_to = sessions[n]['webpy_return_to']

        c = openid.consumer.consumer.Consumer(sessions[n], store)
        a = c.complete(web.input(), web.ctx.home + web.ctx.fullpath)

        if a.status.lower() == 'success':
            web.setcookie('openid_identity_hash', _hmac(a.identity_url) + ',' + a.identity_url)

        del sessions[n]
        return web.redirect(return_to)

########NEW FILE########
__FILENAME__ = wsgi
"""
WSGI Utilities
(from web.py)
"""

import os, sys

import http
import webapi as web
from utils import listget
from net import validaddr, validip
import httpserver
    
def runfcgi(func, addr=('localhost', 8000)):
    """Runs a WSGI function as a FastCGI server."""
    import flup.server.fcgi as flups
    return flups.WSGIServer(func, multiplexed=True, bindAddress=addr).run()

def runscgi(func, addr=('localhost', 4000)):
    """Runs a WSGI function as an SCGI server."""
    import flup.server.scgi as flups
    return flups.WSGIServer(func, bindAddress=addr).run()

def runwsgi(func):
    """
    Runs a WSGI-compatible `func` using FCGI, SCGI, or a simple web server,
    as appropriate based on context and `sys.argv`.
    """
    
    if os.environ.has_key('SERVER_SOFTWARE'): # cgi
        os.environ['FCGI_FORCE_CGI'] = 'Y'

    if (os.environ.has_key('PHP_FCGI_CHILDREN') #lighttpd fastcgi
      or os.environ.has_key('SERVER_SOFTWARE')):
        return runfcgi(func, None)
    
    if 'fcgi' in sys.argv or 'fastcgi' in sys.argv:
        args = sys.argv[1:]
        if 'fastcgi' in args: args.remove('fastcgi')
        elif 'fcgi' in args: args.remove('fcgi')
        if args:
            return runfcgi(func, validaddr(args[0]))
        else:
            return runfcgi(func, None)
    
    if 'scgi' in sys.argv:
        args = sys.argv[1:]
        args.remove('scgi')
        if args:
            return runscgi(func, validaddr(args[0]))
        else:
            return runscgi(func)
    
    return httpserver.runsimple(func, validip(listget(sys.argv, 1, '')))
    
def _is_dev_mode():
    # quick hack to check if the program is running in dev mode.
    if os.environ.has_key('SERVER_SOFTWARE') \
        or os.environ.has_key('PHP_FCGI_CHILDREN') \
        or 'fcgi' in sys.argv or 'fastcgi' in sys.argv \
        or 'mod_wsgi' in sys.argv:
            return False
    return True

# When running the builtin-server, enable debug mode if not already set.
web.config.setdefault('debug', _is_dev_mode())

########NEW FILE########
__FILENAME__ = datastructures
#!/usr/bin/env python
#
# Copyright (C) 2007 Lemur Consulting Ltd
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
r"""datastructures.py: Datastructures for search engine core.

"""
__docformat__ = "restructuredtext en"

import errors
from replaylog import log
import xapian
import cPickle

class Field(object):
    # Use __slots__ because we're going to have very many Field objects in
    # typical usage.
    __slots__ = 'name', 'value'

    def __init__(self, name, value):
        self.name = name
        self.value = value

    def __repr__(self):
        return 'Field(%r, %r)' % (self.name, self.value)

class UnprocessedDocument(object):
    """A unprocessed document to be passed to the indexer.

    This represents an item to be processed and stored in the search engine.
    Each document will be processed by the indexer to generate a
    ProcessedDocument, which can then be stored in the search engine index.

    Note that some information in an UnprocessedDocument will not be
    represented in the ProcessedDocument: therefore, it is not possible to
    retrieve an UnprocessedDocument from the search engine index.

    An unprocessed document is a simple container with two attributes:

     - `fields` is a list of Field objects, or an iterator returning Field
       objects.
     - `id` is a string holding a unique identifier for the document (or
       None to get the database to allocate a unique identifier automatically
       when the document is added).

    """

    __slots__ = 'id', 'fields',
    def __init__(self, id=None, fields=None):
        self.id = id
        if fields is None:
            self.fields = []
        else:
            self.fields = fields

    def __repr__(self):
        return 'UnprocessedDocument(%r, %r)' % (self.id, self.fields)

class ProcessedDocument(object):
    """A processed document, as stored in the index.

    This represents an item which is ready to be stored in the search engine,
    or which has been returned by the search engine.

    """

    __slots__ = '_doc', '_fieldmappings', '_data',
    def __init__(self, fieldmappings, xapdoc=None):
        """Create a ProcessedDocument.

        `fieldmappings` is the configuration from a database connection used lookup
        the configuration to use to store each field.
    
        If supplied, `xapdoc` is a Xapian document to store in the processed
        document.  Otherwise, a new Xapian document is created.

        """
        if xapdoc is None:
            self._doc = log(xapian.Document)
        else:
            self._doc = xapdoc
        self._fieldmappings = fieldmappings
        self._data = None

    def add_term(self, field, term, wdfinc=1, positions=None):
        """Add a term to the document.

        Terms are the main unit of information used for performing searches.

        - `field` is the field to add the term to.
        - `term` is the term to add.
        - `wdfinc` is the value to increase the within-document-frequency
          measure for the term by.
        - `positions` is the positional information to add for the term.
          This may be None to indicate that there is no positional information,
          or may be an integer to specify one position, or may be a sequence of
          integers to specify several positions.  (Note that the wdf is not
          increased automatically for each position: if you add a term at 7
          positions, and the wdfinc value is 2, the total wdf for the term will
          only be increased by 2, not by 14.)

        """
        prefix = self._fieldmappings.get_prefix(field)
        if len(term) > 0:
            # We use the following check, rather than "isupper()" to ensure
            # that we match the check performed by the queryparser, regardless
            # of our locale.
            if ord(term[0]) >= ord('A') and ord(term[0]) <= ord('Z'):
                prefix = prefix + ':'

        # Note - xapian currently restricts term lengths to about 248
        # characters - except that zero bytes are encoded in two bytes, so
        # in practice a term of length 125 characters could be too long.
        # Xapian will give an error when commit() is called after such
        # documents have been added to the database.
        # As a simple workaround, we give an error here for terms over 220
        # characters, which will catch most occurrences of the error early.
        #
        # In future, it might be good to change to a hashing scheme in this
        # situation (or for terms over, say, 64 characters), where the
        # characters after position 64 are hashed (we obviously need to do this
        # hashing at search time, too).
        if len(prefix + term) > 220:
            raise errors.IndexerError("Field %r is too long: maximum length "
                                       "220 - was %d (%r)" %
                                       (field, len(prefix + term),
                                        prefix + term))

        if positions is None:
            self._doc.add_term(prefix + term, wdfinc)
        elif isinstance(positions, int):
            self._doc.add_posting(prefix + term, positions, wdfinc)
        else:
            self._doc.add_term(prefix + term, wdfinc)
            for pos in positions:
                self._doc.add_posting(prefix + term, pos, 0)

    def add_value(self, field, value, purpose=''):
        """Add a value to the document.

        Values are additional units of information used when performing
        searches.  Note that values are _not_ intended to be used to store
        information for display in the search results - use the document data
        for that.  The intention is that as little information as possible is
        stored in values, so that they can be accessed as quickly as possible
        during the search operation.
        
        Unlike terms, each document may have at most one value in each field
        (whereas there may be an arbitrary number of terms in a given field).
        If an attempt to add multiple values to a single field is made, only
        the last value added will be stored.

        """
        slot = self._fieldmappings.get_slot(field, purpose)
        self._doc.add_value(slot, value)

    def get_value(self, field, purpose=''):
        """Get a value from the document.

        """
        slot = self._fieldmappings.get_slot(field, purpose)
        return self._doc.get_value(slot)

    def prepare(self):
        """Prepare the document for adding to a xapian database.

        This updates the internal xapian document with any changes which have
        been made, and then returns it.

        """
        if self._data is not None:
            self._doc.set_data(cPickle.dumps(self._data, 2))
            self._data = None
        return self._doc

    def _get_data(self):
        if self._data is None:
            rawdata = self._doc.get_data()
            if rawdata == '':
                self._data = {}
            else:
                self._data = cPickle.loads(rawdata)
        return self._data
    def _set_data(self, data):
        if not isinstance(data, dict):
            raise TypeError("Cannot set data to any type other than a dict")
        self._data = data
    data = property(_get_data, _set_data, doc=
    """The data stored in this processed document.

    This data is a dictionary of entries, where the key is a fieldname, and the
    value is a list of strings.

    """)

    def _get_id(self):
        tl = self._doc.termlist()
        try:
            term = tl.skip_to('Q').term
            if len(term) == 0 or term[0] != 'Q':
                return None
        except StopIteration:
            return None
        return term[1:]
    def _set_id(self, id):
        tl = self._doc.termlist()
        try:
            term = tl.skip_to('Q').term
        except StopIteration:
            term = ''
        if len(term) != 0 and term[0] == 'Q':
            self._doc.remove_term(term)
        if id is not None:
            self._doc.add_term('Q' + id, 0)
    id = property(_get_id, _set_id, doc=
    """The unique ID for this document.

    """)

    def __repr__(self):
        return '<ProcessedDocument(%r)>' % (self.id)

if __name__ == '__main__':
    import doctest, sys
    doctest.testmod (sys.modules[__name__])

########NEW FILE########
__FILENAME__ = errors
#!/usr/bin/env python
#
# Copyright (C) 2007 Lemur Consulting Ltd
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
r"""errors.py: Exceptions for the search engine core.

"""
__docformat__ = "restructuredtext en"

class SearchEngineError(Exception):
    r"""Base class for exceptions thrown by the search engine.

    Any errors generated by xappy itself, or by xapian, will be instances of
    this class or its subclasses.

    """

class IndexerError(SearchEngineError):
    r"""Class used to report errors relating to the indexing API.

    """

class SearchError(SearchEngineError):
    r"""Class used to report errors relating to the search API.

    """


class XapianError(SearchEngineError):
    r"""Base class for exceptions thrown by the xapian.

    Any errors generated by xapian will be instances of this class or its
    subclasses.

    """

def _rebase_xapian_exceptions():
    """Add new base classes for all the xapian exceptions.

    """
    import xapian
    for name in (
                 'AssertionError',
                 'DatabaseCorruptError',
                 'DatabaseCreateError',
                 'DatabaseError',
                 'DatabaseLockError',
                 'DatabaseModifiedError',
                 'DatabaseOpeningError',
                 'DatabaseVersionError',
                 'DocNotFoundError',
                 # We skip 'Error' because it inherits directly from exception
                 # and this causes problems with method resolution order.
                 # However, we probably don't need it anyway, because it's
                 # just a base class, and shouldn't ever actually be raised.
                 # Users can catch xappy.XapianError instead.
                 'FeatureUnavailableError',
                 'InternalError',
                 'InvalidArgumentError',
                 'InvalidOperationError',
                 'LogicError',
                 'NetworkError',
                 'NetworkTimeoutError',
                 'QueryParserError',
                 'RangeError',
                 'RuntimeError',
                 'UnimplementedError',
                 ):
        xapian_exception = getattr(xapian, name, None)
        if xapian_exception is not None:
            xapian_exception.__bases__ += (XapianError, )
            globals()['Xapian' + name] = xapian_exception

_rebase_xapian_exceptions()

########NEW FILE########
__FILENAME__ = fieldactions
#!/usr/bin/env python
#
# Copyright (C) 2007 Lemur Consulting Ltd
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
r"""fieldactions.py: Definitions and implementations of field actions.

"""
__docformat__ = "restructuredtext en"

import _checkxapian
import errors
import marshall
from replaylog import log
import xapian
import parsedate

def _act_store_content(fieldname, doc, value, context):
    """Perform the STORE_CONTENT action.
    
    """
    try:
        fielddata = doc.data[fieldname]
    except KeyError:
        fielddata = []
        doc.data[fieldname] = fielddata
    fielddata.append(value)

def _act_index_exact(fieldname, doc, value, context):
    """Perform the INDEX_EXACT action.
    
    """
    doc.add_term(fieldname, value, 0)

def _act_tag(fieldname, doc, value, context):
    """Perform the TAG action.
    
    """
    doc.add_term(fieldname, value.lower(), 0)

def _act_facet(fieldname, doc, value, context, type=None):
    """Perform the FACET action.
    
    """
    if type is None or type == 'string':
        value = value.lower()
        doc.add_term(fieldname, value, 0)
        serialiser = log(xapian.StringListSerialiser,
                          doc.get_value(fieldname, 'facet'))
        serialiser.append(value)
        doc.add_value(fieldname, serialiser.get(), 'facet')
    else:
        marshaller = SortableMarshaller()
        fn = marshaller.get_marshall_function(fieldname, type)
        doc.add_value(fieldname, fn(fieldname, value), 'facet')

def _act_index_freetext(fieldname, doc, value, context, weight=1, 
                        language=None, stop=None, spell=False,
                        nopos=False,
                        allow_field_specific=True,
                        search_by_default=True):
    """Perform the INDEX_FREETEXT action.
    
    """
    termgen = log(xapian.TermGenerator)
    if language is not None:
        termgen.set_stemmer(log(xapian.Stem, language))
        
    if stop is not None:
        stopper = log(xapian.SimpleStopper)
        for term in stop:
            stopper.add (term)
        termgen.set_stopper (stopper)

    if spell:
        termgen.set_database(context.index)
        termgen.set_flags(termgen.FLAG_SPELLING)
    
    termgen.set_document(doc._doc)

    if search_by_default:
        termgen.set_termpos(context.current_position)
        # Store a copy of the field without a prefix, for non-field-specific
        # searches.
        if nopos:
            termgen.index_text_without_positions(value, weight, '')
        else:
            termgen.index_text(value, weight, '')

    if allow_field_specific:
        # Store a second copy of the term with a prefix, for field-specific
        # searches.
        prefix = doc._fieldmappings.get_prefix(fieldname)
        if len(prefix) != 0:
            termgen.set_termpos(context.current_position)
            if nopos:
                termgen.index_text_without_positions(value, weight, prefix)
            else:
                termgen.index_text(value, weight, prefix)

    # Add a gap between each field instance, so that phrase searches don't
    # match across instances.
    termgen.increase_termpos(10)
    context.current_position = termgen.get_termpos()

class SortableMarshaller(object):
    """Implementation of marshalling for sortable values.

    """
    def __init__(self, indexing=True):
        if indexing:
            self._err = errors.IndexerError
        else:
            self._err = errors.SearchError

    def marshall_string(self, fieldname, value):
        """Marshall a value for sorting in lexicograpical order.

        This returns the input as the output, since strings already sort in
        lexicographical order.

        """
        return value

    def marshall_float(self, fieldname, value):
        """Marshall a value for sorting as a floating point value.

        """
        # convert the value to a float
        try:
            value = float(value)
        except ValueError:
            raise self._err("Value supplied to field %r must be a "
                            "valid floating point number: was %r" %
                            (fieldname, value))
        return marshall.float_to_string(value)

    def marshall_date(self, fieldname, value):
        """Marshall a value for sorting as a date.

        """
        try:
            value = parsedate.date_from_string(value)
        except ValueError, e:
            raise self._err("Value supplied to field %r must be a "
                            "valid date: was %r: error is '%s'" %
                            (fieldname, value, str(e)))
        return marshall.date_to_string(value)

    def get_marshall_function(self, fieldname, sorttype):
        """Get a function used to marshall values of a given sorttype.

        """
        try:
            return {
                None: self.marshall_string,
                'string': self.marshall_string,
                'float': self.marshall_float,
                'date': self.marshall_date,
            }[sorttype]
        except KeyError:
            raise self._err("Unknown sort type %r for field %r" %
                            (sorttype, fieldname))


def _act_sort_and_collapse(fieldname, doc, value, context, type=None):
    """Perform the SORTABLE action.

    """
    marshaller = SortableMarshaller()
    fn = marshaller.get_marshall_function(fieldname, type)
    value = fn(fieldname, value)
    doc.add_value(fieldname, value, 'collsort')

class ActionContext(object):
    """The context in which an action is performed.

    This is just used to pass term generators, word positions, and the like
    around.

    """
    def __init__(self, index):
        self.current_language = None
        self.current_position = 0
        self.index = index

class FieldActions(object):
    """An object describing the actions to be performed on a field.

    The supported actions are:
    
    - `STORE_CONTENT`: store the unprocessed content of the field in the search
      engine database.  All fields which need to be displayed or used when
      displaying the search results need to be given this action.

    - `INDEX_EXACT`: index the exact content of the field as a single search
      term.  Fields whose contents need to be searchable as an "exact match"
      need to be given this action.

    - `INDEX_FREETEXT`: index the content of this field as text.  The content
      will be split into terms, allowing free text searching of the field.  Four
      optional parameters may be supplied:

      - 'weight' is a multiplier to apply to the importance of the field.  This
        must be an integer, and the default value is 1.
      - 'language' is the language to use when processing the field.  This can
        be expressed as an ISO 2-letter language code.  The supported languages
        are those supported by the xapian core in use.
      - 'stop' is an iterable of stopwords to filter out of the generated
        terms.  Note that due to Xapian design, only non-positional terms are
        affected, so this is of limited use.
      - 'spell' is a boolean flag - if true, the contents of the field will be
        used for spelling correction.
      - 'nopos' is a boolean flag - if true, positional information is not
        stored.
      - 'allow_field_specific' is a boolean flag - if False, prevents terms with the field
        prefix being generated.  This means that searches specific to this
        field will not work, and thus should only be used when only non-field
        specific searches are desired.  Defaults to True.
      - 'search_by_default' is a boolean flag - if False, the field will not be
        searched by non-field specific searches.  If True, or omitted, the
        field will be included in searches for non field-specific searches.

    - `SORTABLE`: index the content of the field such that it can be used to
      sort result sets.  It also allows result sets to be restricted to those
      documents with a field values in a given range.  One optional parameter
      may be supplied:

      - 'type' is a value indicating how to sort the field.  It has several
        possible values:

        - 'string' - sort in lexicographic (ie, alphabetical) order.
          This is the default, used if no type is set.
        - 'float' - treat the values as (decimal representations of) floating
          point numbers, and sort in numerical order.  The values in the field
          must be valid floating point numbers (according to Python's float()
          function).
        - 'date' - sort in date order.  The values must be valid dates (either
          Python datetime.date objects, or ISO 8601 format (ie, YYYYMMDD or
          YYYY-MM-DD).

    - `COLLAPSE`: index the content of the field such that it can be used to
      "collapse" result sets, such that only the highest result with each value
      of the field will be returned.

    - `TAG`: the field contains tags; these are strings, which will be matched
      in a case insensitive way, but otherwise must be exact matches.  Tag
      fields can be searched for by making an explict query (ie, using
      query_field(), but not with query_parse()).  A list of the most frequent
      tags in a result set can also be accessed easily.

    - `FACET`: the field represents a classification facet; these are strings
      which will be matched exactly, but a list of all the facets present in
      the result set can also be accessed easily - in addition, a suitable
      subset of the facets, and a selection of the facet values, present in the
      result set can be calculated.  One optional parameter may be supplied:

      - 'type' is a value indicating the type of facet contained in the field:

        - 'string' - the facet values are exact binary strings.
        - 'float' - the facet values are floating point numbers.

    """

    # See the class docstring for the meanings of the following constants.
    STORE_CONTENT = 1
    INDEX_EXACT = 2
    INDEX_FREETEXT = 3
    SORTABLE = 4 
    COLLAPSE = 5
    TAG = 6
    FACET = 7

    # Sorting and collapsing store the data in a value, but the format depends
    # on the sort type.  Easiest way to implement is to treat them as the same
    # action.
    SORT_AND_COLLAPSE = -1

    _unsupported_actions = []

    if 'tags' in _checkxapian.missing_features:
        _unsupported_actions.append(TAG)
    if 'facets' in _checkxapian.missing_features:
        _unsupported_actions.append(FACET)

    def __init__(self, fieldname):
        # Dictionary of actions, keyed by type.
        self._actions = {}
        self._fieldname = fieldname

    def add(self, field_mappings, action, **kwargs):
        """Add an action to perform on a field.

        """
        if action in self._unsupported_actions:
            raise errors.IndexerError("Action unsupported with this release of xapian")

        if action not in (FieldActions.STORE_CONTENT,
                          FieldActions.INDEX_EXACT,
                          FieldActions.INDEX_FREETEXT,
                          FieldActions.SORTABLE,
                          FieldActions.COLLAPSE,
                          FieldActions.TAG,
                          FieldActions.FACET,
                         ):
            raise errors.IndexerError("Unknown field action: %r" % action)

        info = self._action_info[action]

        # Check parameter names
        for key in kwargs.keys():
            if key not in info[1]:
                raise errors.IndexerError("Unknown parameter name for action %r: %r" % (info[0], key))

        # Fields cannot be indexed both with "EXACT" and "FREETEXT": whilst we
        # could implement this, the query parser wouldn't know what to do with
        # searches.
        if action == FieldActions.INDEX_EXACT:
            if FieldActions.INDEX_FREETEXT in self._actions:
                raise errors.IndexerError("Field %r is already marked for indexing "
                                   "as free text: cannot mark for indexing "
                                   "as exact text as well" % self._fieldname)
        if action == FieldActions.INDEX_FREETEXT:
            if FieldActions.INDEX_EXACT in self._actions:
                raise errors.IndexerError("Field %r is already marked for indexing "
                                   "as exact text: cannot mark for indexing "
                                   "as free text as well" % self._fieldname)

        # Fields cannot be indexed as more than one type for "SORTABLE": to
        # implement this, we'd need to use a different prefix for each sortable
        # type, but even then the search end wouldn't know what to sort on when
        # searching.  Also, if they're indexed as "COLLAPSE", the value must be
        # stored in the right format for the type "SORTABLE".
        if action == FieldActions.SORTABLE or action == FieldActions.COLLAPSE:
            if action == FieldActions.COLLAPSE:
                sorttype = None
            else:
                try:
                    sorttype = kwargs['type']
                except KeyError:
                    sorttype = 'string'
            kwargs['type'] = sorttype
            action = FieldActions.SORT_AND_COLLAPSE

            try:
                oldsortactions = self._actions[FieldActions.SORT_AND_COLLAPSE]
            except KeyError:
                oldsortactions = ()

            if len(oldsortactions) > 0:
                for oldsortaction in oldsortactions:
                    oldsorttype = oldsortaction['type']

                if sorttype == oldsorttype or oldsorttype is None:
                    # Use new type
                    self._actions[action] = []
                elif sorttype is None:
                    # Use old type
                    return
                else:
                    raise errors.IndexerError("Field %r is already marked for "
                                               "sorting, with a different "
                                               "sort type" % self._fieldname)

        if 'prefix' in info[3]:
            field_mappings.add_prefix(self._fieldname)
        if 'slot' in info[3]:
            purposes = info[3]['slot']
            if isinstance(purposes, basestring):
                field_mappings.add_slot(self._fieldname, purposes)
            else:
                slotnum = None
                for purpose in purposes:
                    slotnum = field_mappings.get_slot(self._fieldname, purpose)
                    if slotnum is not None:
                        break
                for purpose in purposes:
                    field_mappings.add_slot(self._fieldname, purpose, slotnum=slotnum)

        # Make an entry for the action
        if action not in self._actions:
            self._actions[action] = []

        # Check for repetitions of actions
        for old_action in self._actions[action]:
            if old_action == kwargs:
                return

        # Append the action to the list of actions
        self._actions[action].append(kwargs)

    def perform(self, doc, value, context):
        """Perform the actions on the field.

        - `doc` is a ProcessedDocument to store the result of the actions in.
        - `value` is a string holding the value of the field.
        - `context` is an ActionContext object used to keep state in.

        """
        for type, actionlist in self._actions.iteritems():
            info = self._action_info[type]            
            for kwargs in actionlist:
                info[2](self._fieldname, doc, value, context, **kwargs)

    _action_info = {
        STORE_CONTENT: ('STORE_CONTENT', (), _act_store_content, {}, ),
        INDEX_EXACT: ('INDEX_EXACT', (), _act_index_exact, {'prefix': True}, ),
        INDEX_FREETEXT: ('INDEX_FREETEXT', ('weight', 'language', 'stop', 'spell', 'nopos', 'allow_field_specific', 'search_by_default', ), 
            _act_index_freetext, {'prefix': True, }, ),
        SORTABLE: ('SORTABLE', ('type', ), None, {'slot': 'collsort',}, ),
        COLLAPSE: ('COLLAPSE', (), None, {'slot': 'collsort',}, ),
        TAG: ('TAG', (), _act_tag, {'prefix': True,}, ),
        FACET: ('FACET', ('type', ), _act_facet, {'prefix': True, 'slot': 'facet',}, ),

        SORT_AND_COLLAPSE: ('SORT_AND_COLLAPSE', ('type', ), _act_sort_and_collapse, {'slot': 'collsort',}, ),
    }

if __name__ == '__main__':
    import doctest, sys
    doctest.testmod (sys.modules[__name__])

########NEW FILE########
__FILENAME__ = fieldmappings
#!/usr/bin/env python
#
# Copyright (C) 2007 Lemur Consulting Ltd
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
r"""fieldmappings.py: Mappings from field names to term prefixes, etc.

"""
__docformat__ = "restructuredtext en"

import cPickle as _cPickle

class FieldMappings(object):
    """Mappings from field names to term prefixes, slot values, etc.

    The following mappings are maintained:

    - a mapping from field name to the string prefix to insert at the start of
      terms.
    - a mapping from field name to the slot numbers to store the field contents
      in.

    """
    __slots__ = '_prefixes', '_prefixcount', '_slots', '_slotcount', 

    def __init__(self, serialised=None):
        """Create a new field mapping object, or unserialise a saved one.

        """
        if serialised is not None:
            (self._prefixes, self._prefixcount,
             self._slots, self._slotcount) = _cPickle.loads(serialised)
        else:
            self._prefixes = {}
            self._prefixcount = 0
            self._slots = {}
            self._slotcount = 0

    def _genPrefix(self):
        """Generate a previously unused prefix.

        Prefixes are uppercase letters, and start with 'X' (this is a Xapian
        convention, for compatibility with other Xapian tools: other starting
        letters are reserved for special meanings):

        >>> maps = FieldMappings()
        >>> maps._genPrefix()
        'XA'
        >>> maps._genPrefix()
        'XB'
        >>> [maps._genPrefix() for i in xrange(60)]
        ['XC', 'XD', 'XE', 'XF', 'XG', 'XH', 'XI', 'XJ', 'XK', 'XL', 'XM', 'XN', 'XO', 'XP', 'XQ', 'XR', 'XS', 'XT', 'XU', 'XV', 'XW', 'XX', 'XY', 'XZ', 'XAA', 'XBA', 'XCA', 'XDA', 'XEA', 'XFA', 'XGA', 'XHA', 'XIA', 'XJA', 'XKA', 'XLA', 'XMA', 'XNA', 'XOA', 'XPA', 'XQA', 'XRA', 'XSA', 'XTA', 'XUA', 'XVA', 'XWA', 'XXA', 'XYA', 'XZA', 'XAB', 'XBB', 'XCB', 'XDB', 'XEB', 'XFB', 'XGB', 'XHB', 'XIB', 'XJB']
        >>> maps = FieldMappings()
        >>> [maps._genPrefix() for i in xrange(27*26 + 5)][-10:]
        ['XVZ', 'XWZ', 'XXZ', 'XYZ', 'XZZ', 'XAAA', 'XBAA', 'XCAA', 'XDAA', 'XEAA']
        """
        res = []
        self._prefixcount += 1
        num = self._prefixcount
        while num != 0:
            ch = (num - 1) % 26
            res.append(chr(ch + ord('A')))
            num -= ch
            num = num // 26
        return 'X' + ''.join(res)

    def get_fieldname_from_prefix(self, prefix):
        """Get a fieldname from a prefix.

        If the prefix is not found, return None.

        """
        for key, val in self._prefixes.iteritems():
            if val == prefix:
                return key
        return None

    def get_prefix(self, fieldname):
        """Get the prefix used for a given field name.

        """
        return self._prefixes[fieldname]

    def get_slot(self, fieldname, purpose):
        """Get the slot number used for a given field name and purpose.

        """
        return self._slots[(fieldname, purpose)]

    def add_prefix(self, fieldname):
        """Allocate a prefix for the given field.

        If a prefix is already allocated for this field, this has no effect.

        """
        if fieldname in self._prefixes:
            return
        self._prefixes[fieldname] = self._genPrefix()

    def add_slot(self, fieldname, purpose, slotnum=None):
        """Allocate a slot number for the given field and purpose.

        If a slot number is already allocated for this field and purpose, this
        has no effect.

        Returns the slot number allocated for the field and purpose (whether
        newly allocated, or previously allocated).

        If `slotnum` is supplied, the number contained in it is used to
        allocate the new slot, instead of allocating a new number.  No checks
        will be made to ensure that the slot number doesn't collide with
        existing (or later allocated) numbers: the main purpose of this
        parameter is to share allocations - ie, to collide deliberately.

        """
        try:
            return self._slots[(fieldname, purpose)]
        except KeyError:
            pass

        if slotnum is None:
            self._slots[(fieldname, purpose)] = self._slotcount
            self._slotcount += 1
            return self._slotcount - 1
        else:
            self._slots[(fieldname, purpose)] = slotnum
            return slotnum

    def serialise(self):
        """Serialise the field mappings to a string.

        This can be unserialised by passing the result of this method to the
        constructor of a new FieldMappings object.

        """
        return _cPickle.dumps((self._prefixes,
                               self._prefixcount,
                               self._slots,
                               self._slotcount,
                              ), 2)

########NEW FILE########
__FILENAME__ = highlight
#!/usr/bin/env python
#
# Copyright (C) 2007 Lemur Consulting Ltd
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
r"""highlight.py: Highlight and summarise text.

"""
__docformat__ = "restructuredtext en"

import re
import xapian

class Highlighter(object):
    """Class for highlighting text and creating contextual summaries.

    >>> hl = Highlighter("en")
    >>> hl.makeSample('Hello world.', ['world'])
    'Hello world.'
    >>> hl.highlight('Hello world', ['world'], ('<', '>'))
    'Hello <world>'

    """

    # split string into words, spaces, punctuation and markup tags
    _split_re = re.compile(r'<\w+[^>]*>|</\w+>|[\w\']+|\s+|[^\w\'\s<>/]+')

    def __init__(self, language_code='en', stemmer=None):
        """Create a new highlighter for the specified language.

        """
        if stemmer is not None:
            self.stem = stemmer
        else:
            self.stem = xapian.Stem(language_code)

    def _split_text(self, text, strip_tags=False):
        """Split some text into words and non-words.

        - `text` is the text to process.  It may be a unicode object or a utf-8
          encoded simple string.
        - `strip_tags` is a flag - False to keep tags, True to strip all tags
          from the output.

        Returns a list of utf-8 encoded simple strings.

        """
        if isinstance(text, unicode):
            text = text.encode('utf-8')

        words = self._split_re.findall(text)
        if strip_tags:
            return [w for w in words if w[0] != '<']
        else:
            return words

    def _strip_prefix(self, term):
        """Strip the prefix off a term.

        Prefixes are any initial capital letters, with the exception that R always
        ends a prefix, even if followed by capital letters.

        >>> hl = Highlighter("en")
        >>> print hl._strip_prefix('hello')
        hello
        >>> print hl._strip_prefix('Rhello')
        hello
        >>> print hl._strip_prefix('XARHello')
        Hello
        >>> print hl._strip_prefix('XAhello')
        hello
        >>> print hl._strip_prefix('XAh')
        h
        >>> print hl._strip_prefix('XA')
        <BLANKLINE>

        """
        for p in xrange(len(term)):
            if term[p].islower():
                return term[p:]
            elif term[p] == 'R':
                return term[p+1:]
        return ''

    def _query_to_stemmed_words(self, query):
        """Convert a query to a list of stemmed words.

        - `query` is the query to parse: it may be xapian.Query object, or a
          sequence of terms.

        """
        if isinstance(query, xapian.Query):
            return [self._strip_prefix(t) for t in query]
        else:
            return [self.stem(q.lower()) for q in query]


    def makeSample(self, text, query, maxlen=600, hl=None):
        """Make a contextual summary from the supplied text.

        This basically works by splitting the text into phrases, counting the query
        terms in each, and keeping those with the most.

        Any markup tags in the text will be stripped.

        `text` is the source text to summarise.
        `query` is either a Xapian query object or a list of (unstemmed) term strings.
        `maxlen` is the maximum length of the generated summary.
        `hl` is a pair of strings to insert around highlighted terms, e.g. ('<b>', '</b>')

        """

        # coerce maxlen into an int, otherwise truncation doesn't happen
        maxlen = int(maxlen)

        words = self._split_text(text, True)
        terms = self._query_to_stemmed_words(query)
        
        # build blocks delimited by puncuation, and count matching words in each block
        # blocks[n] is a block [firstword, endword, charcount, termcount, selected]
        blocks = []
        start = end = count = blockchars = 0

        while end < len(words):
            blockchars += len(words[end])
            if words[end].isalnum():
                if self.stem(words[end].lower()) in terms:
                    count += 1
                end += 1
            elif words[end] in ',.;:?!\n':
                end += 1
                blocks.append([start, end, blockchars, count, False])
                start = end
                blockchars = 0
                count = 0
            else:
                end += 1
        if start != end:
            blocks.append([start, end, blockchars, count, False])
        if len(blocks) == 0:
            return ''

        # select high-scoring blocks first, down to zero-scoring
        chars = 0
        for count in xrange(3, -1, -1):
            for b in blocks:
                if b[3] >= count:
                    b[4] = True
                    chars += b[2]
                    if chars >= maxlen: break
            if chars >= maxlen: break

        # assemble summary
        words2 = []
        lastblock = -1
        for i, b in enumerate(blocks):
            if b[4]:
                if i != lastblock + 1:
                    words2.append('..')
                words2.extend(words[b[0]:b[1]])
                lastblock = i

        if not blocks[-1][4]:
            words2.append('..')

        # trim down to maxlen
        l = 0
        for i in xrange (len (words2)):
            l += len (words2[i])
            if l >= maxlen:
                words2[i:] = ['..']
                break

        if hl is None:
            return ''.join(words2)
        else:
            return self._hl(words2, terms, hl)

    def highlight(self, text, query, hl, strip_tags=False):
        """Add highlights (string prefix/postfix) to a string.

        `text` is the source to highlight.
        `query` is either a Xapian query object or a list of (unstemmed) term strings.
        `hl` is a pair of highlight strings, e.g. ('<i>', '</i>')
        `strip_tags` strips HTML markout iff True

        >>> hl = Highlighter()
        >>> qp = xapian.QueryParser()
        >>> q = qp.parse_query('cat dog')
        >>> tags = ('[[', ']]')
        >>> hl.highlight('The cat went Dogging; but was <i>dog tired</i>.', q, tags)
        'The [[cat]] went [[Dogging]]; but was <i>[[dog]] tired</i>.'

        """
        words = self._split_text(text, strip_tags)
        terms = self._query_to_stemmed_words(query)
        return self._hl(words, terms, hl)

    def _hl(self, words, terms, hl):
        """Add highlights to a list of words.
        
        `words` is the list of words and non-words to be highlighted..
        `terms` is the list of stemmed words to look for.

        """
        for i, w in enumerate(words):
            # HACK - more forgiving about stemmed terms 
            wl = w.lower()
            if wl in terms or self.stem (wl) in terms:
                words[i] = ''.join((hl[0], w, hl[1]))

        return ''.join(words)


__test__ = {
    'no_punc': r'''

    Test the highlighter's behaviour when there is no punctuation in the sample
    text (regression test - used to return no output):
    >>> hl = Highlighter("en")
    >>> hl.makeSample('Hello world', ['world'])
    'Hello world'

    ''',

    'stem_levels': r'''

    Test highlighting of words, and how it works with stemming:
    >>> hl = Highlighter("en")

    # "word" and "wording" stem to "word", so the following 4 calls all return
    # the same thing
    >>> hl.makeSample('Hello. word. wording. wordinging.', ['word'], hl='<>')
    'Hello. <word>. <wording>. wordinging.'
    >>> hl.highlight('Hello. word. wording. wordinging.', ['word'], '<>')
    'Hello. <word>. <wording>. wordinging.'
    >>> hl.makeSample('Hello. word. wording. wordinging.', ['wording'], hl='<>')
    'Hello. <word>. <wording>. wordinging.'
    >>> hl.highlight('Hello. word. wording. wordinging.', ['wording'], '<>')
    'Hello. <word>. <wording>. wordinging.'

    # "wordinging" stems to "wording", so only the last two words are
    # highlighted for this one.
    >>> hl.makeSample('Hello. word. wording. wordinging.', ['wordinging'], hl='<>')
    'Hello. word. <wording>. <wordinging>.'
    >>> hl.highlight('Hello. word. wording. wordinging.', ['wordinging'], '<>')
    'Hello. word. <wording>. <wordinging>.'
    ''',

    'supplied_stemmer': r'''

    Test behaviour if we pass in our own stemmer:
    >>> stem = xapian.Stem('en')
    >>> hl = Highlighter(stemmer=stem)
    >>> hl.highlight('Hello. word. wording. wordinging.', ['word'], '<>')
    'Hello. <word>. <wording>. wordinging.'

    ''',

    'unicode': r'''

    Test behaviour if we pass in unicode input:
    >>> hl = Highlighter('en')
    >>> hl.highlight(u'Hello\xf3. word. wording. wordinging.', ['word'], '<>')
    'Hello\xc3\xb3. <word>. <wording>. wordinging.'

    ''',

    'no_sample': r'''

    Test behaviour if we pass in unicode input:
    >>> hl = Highlighter('en')
    >>> hl.makeSample(u'', ['word'])
    ''

    ''',

    'short_samples': r'''

    >>> hl = Highlighter('en')
    >>> hl.makeSample("A boring start.  Hello world indeed.  A boring end.", ['hello'], 20, ('<', '>'))
    '..  <Hello> world ..'
    >>> hl.makeSample("A boring start.  Hello world indeed.  A boring end.", ['hello'], 40, ('<', '>'))
    'A boring start.  <Hello> world indeed...'
    >>> hl.makeSample("A boring start.  Hello world indeed.  A boring end.", ['boring'], 40, ('<', '>'))
    'A <boring> start...  A <boring> end.'

    ''',

    'apostrophes': r'''

    >>> hl = Highlighter('en')
    >>> hl.makeSample("A boring start.  Hello world's indeed.  A boring end.", ['world'], 40, ('<', '>'))
    "A boring start.  Hello <world's> indeed..."

    ''',

}

if __name__ == '__main__':
    import doctest, sys
    doctest.testmod (sys.modules[__name__])

########NEW FILE########
__FILENAME__ = indexerconnection
#!/usr/bin/env python
#
# Copyright (C) 2007 Lemur Consulting Ltd
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
r"""indexerconnection.py: A connection to the search engine for indexing.

"""
__docformat__ = "restructuredtext en"

import _checkxapian
import cPickle
import xapian

from datastructures import *
import errors
from fieldactions import *
import fieldmappings
import memutils
from replaylog import log

class IndexerConnection(object):
    """A connection to the search engine for indexing.

    """

    def __init__(self, indexpath):
        """Create a new connection to the index.

        There may only be one indexer connection for a particular database open
        at a given time.  Therefore, if a connection to the database is already
        open, this will raise a xapian.DatabaseLockError.

        If the database doesn't already exist, it will be created.

        """
        self._index = log(xapian.WritableDatabase, indexpath, xapian.DB_CREATE_OR_OPEN)
        self._indexpath = indexpath

        # Read existing actions.
        self._field_actions = {}
        self._field_mappings = fieldmappings.FieldMappings()
        self._facet_hierarchy = {}
        self._facet_query_table = {}
        self._next_docid = 0
        self._config_modified = False
        self._load_config()

        # Set management of the memory used.
        # This can be removed once Xapian implements this itself.
        self._mem_buffered = 0
        self.set_max_mem_use()

    def set_max_mem_use(self, max_mem=None, max_mem_proportion=None):
        """Set the maximum memory to use.

        This call allows the amount of memory to use to buffer changes to be
        set.  This will affect the speed of indexing, but should not result in
        other changes to the indexing.

        Note: this is an approximate measure - the actual amount of memory used
        max exceed the specified amount.  Also, note that future versions of
        xapian are likely to implement this differently, so this setting may be
        entirely ignored.

        The absolute amount of memory to use (in bytes) may be set by setting
        max_mem.  Alternatively, the proportion of the available memory may be
        set by setting max_mem_proportion (this should be a value between 0 and
        1).

        Setting too low a value will result in excessive flushing, and very
        slow indexing.  Setting too high a value will result in excessive
        buffering, leading to swapping, and very slow indexing.

        A reasonable default for max_mem_proportion for a system which is
        dedicated to indexing is probably 0.5: if other tasks are also being
        performed on the system, the value should be lowered.

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        if max_mem is not None and max_mem_proportion is not None:
            raise errors.IndexerError("Only one of max_mem and "
                                       "max_mem_proportion may be specified")

        if max_mem is None and max_mem_proportion is None:
            self._max_mem = None

        if max_mem_proportion is not None:
            physmem = memutils.get_physical_memory()
            if physmem is not None:
                max_mem = int(physmem * max_mem_proportion)

        self._max_mem = max_mem

    def _store_config(self):
        """Store the configuration for the database.

        Currently, this stores the configuration in a file in the database
        directory, so changes to it are not protected by transactions.  When
        support is available in xapian for storing metadata associated with
        databases. this will be used instead of a file.

        """
        assert self._index is not None

        config_str = cPickle.dumps((
                                     self._field_actions,
                                     self._field_mappings.serialise(),
                                     self._facet_hierarchy,
                                     self._facet_query_table,
                                     self._next_docid,
                                    ), 2)
        log(self._index.set_metadata, '_xappy_config', config_str)

        self._config_modified = False

    def _load_config(self):
        """Load the configuration for the database.

        """
        assert self._index is not None

        config_str = log(self._index.get_metadata, '_xappy_config')
        if len(config_str) == 0:
            return

        try:
            (self._field_actions, mappings, self._facet_hierarchy, self._facet_query_table, self._next_docid) = cPickle.loads(config_str)
        except ValueError:
            # Backwards compatibility - configuration used to lack _facet_hierarchy and _facet_query_table
            (self._field_actions, mappings, self._next_docid) = cPickle.loads(config_str)
            self._facet_hierarchy = {}
            self._facet_query_table = {}
        self._field_mappings = fieldmappings.FieldMappings(mappings)

        self._config_modified = False

    def _allocate_id(self):
        """Allocate a new ID.

        """
        while True:
            idstr = "%x" % self._next_docid
            self._next_docid += 1
            if not self._index.term_exists('Q' + idstr):
                break
        self._config_modified = True
        return idstr

    def add_field_action(self, fieldname, fieldtype, **kwargs):
        """Add an action to be performed on a field.

        Note that this change to the configuration will not be preserved on
        disk until the next call to flush().

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        if fieldname in self._field_actions:
            actions = self._field_actions[fieldname]
        else:
            actions = FieldActions(fieldname)
            self._field_actions[fieldname] = actions
        actions.add(self._field_mappings, fieldtype, **kwargs)
        self._config_modified = True

    def clear_field_actions(self, fieldname):
        """Clear all actions for the specified field.

        This does not report an error if there are already no actions for the
        specified field.

        Note that this change to the configuration will not be preserved on
        disk until the next call to flush().

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        if fieldname in self._field_actions:
            del self._field_actions[fieldname]
            self._config_modified = True

    def get_fields_with_actions(self):
        """Get a list of field names which have actions defined.

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        return self._field_actions.keys()

    def process(self, document):
        """Process an UnprocessedDocument with the settings in this database.

        The resulting ProcessedDocument is returned.

        Note that this processing will be automatically performed if an
        UnprocessedDocument is supplied to the add() or replace() methods of
        IndexerConnection.  This method is exposed to allow the processing to
        be performed separately, which may be desirable if you wish to manually
        modify the processed document before adding it to the database, or if
        you want to split processing of documents from adding documents to the
        database for performance reasons.

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        result = ProcessedDocument(self._field_mappings)
        result.id = document.id
        context = ActionContext(self._index)

        for field in document.fields:
            try:
                actions = self._field_actions[field.name]
            except KeyError:
                # If no actions are defined, just ignore the field.
                continue
            actions.perform(result, field.value, context)

        return result

    def _get_bytes_used_by_doc_terms(self, xapdoc):
        """Get an estimate of the bytes used by the terms in a document.

        (This is a very rough estimate.)

        """
        count = 0
        for item in xapdoc.termlist():
            # The term may also be stored in the spelling correction table, so
            # double the amount used.
            count += len(item.term) * 2

            # Add a few more bytes for holding the wdf, and other bits and
            # pieces.
            count += 8

        # Empirical observations indicate that about 5 times as much memory as
        # the above calculation predicts is used for buffering in practice.
        return count * 5

    def add(self, document):
        """Add a new document to the search engine index.

        If the document has a id set, and the id already exists in
        the database, an exception will be raised.  Use the replace() method
        instead if you wish to overwrite documents.

        Returns the id of the newly added document (making up a new
        unique ID if no id was set).

        The supplied document may be an instance of UnprocessedDocument, or an
        instance of ProcessedDocument.

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        if not hasattr(document, '_doc'):
            # It's not a processed document.
            document = self.process(document)

        # Ensure that we have a id
        orig_id = document.id
        if orig_id is None:
            id = self._allocate_id()
            document.id = id
        else:
            id = orig_id
            if self._index.term_exists('Q' + id):
                raise errors.IndexerError("Document ID of document supplied to add() is not unique.")
            
        # Add the document.
        xapdoc = document.prepare()
        self._index.add_document(xapdoc)

        if self._max_mem is not None:
            self._mem_buffered += self._get_bytes_used_by_doc_terms(xapdoc)
            if self._mem_buffered > self._max_mem:
                self.flush()

        if id is not orig_id:
            document.id = orig_id
        return id

    def replace(self, document):
        """Replace a document in the search engine index.

        If the document does not have a id set, an exception will be
        raised.

        If the document has a id set, and the id does not already
        exist in the database, this method will have the same effect as add().

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        if not hasattr(document, '_doc'):
            # It's not a processed document.
            document = self.process(document)

        # Ensure that we have a id
        id = document.id
        if id is None:
            raise errors.IndexerError("No document ID set for document supplied to replace().")

        xapdoc = document.prepare()
        self._index.replace_document('Q' + id, xapdoc)

        if self._max_mem is not None:
            self._mem_buffered += self._get_bytes_used_by_doc_terms(xapdoc)
            if self._mem_buffered > self._max_mem:
                self.flush()

    def _make_synonym_key(self, original, field):
        """Make a synonym key (ie, the term or group of terms to store in
        xapian).

        """
        if field is not None:
            prefix = self._field_mappings.get_prefix(field)
        else:
            prefix = ''
        original = original.lower()
        # Add the prefix to the start of each word.
        return ' '.join((prefix + word for word in original.split(' ')))

    def add_synonym(self, original, synonym, field=None,
                    original_field=None, synonym_field=None):
        """Add a synonym to the index.

         - `original` is the word or words which will be synonym expanded in
           searches (if multiple words are specified, each word should be
           separated by a single space).
         - `synonym` is a synonym for `original`.
         - `field` is the field which the synonym is specific to.  If no field
           is specified, the synonym will be used for searches which are not
           specific to any particular field.

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        if original_field is None:
            original_field = field
        if synonym_field is None:
            synonym_field = field
        key = self._make_synonym_key(original, original_field)
        # FIXME - this only works for exact fields which have no upper case
        # characters, or single words
        value = self._make_synonym_key(synonym, synonym_field)
        self._index.add_synonym(key, value)

    def remove_synonym(self, original, synonym, field=None):
        """Remove a synonym from the index.

         - `original` is the word or words which will be synonym expanded in
           searches (if multiple words are specified, each word should be
           separated by a single space).
         - `synonym` is a synonym for `original`.
         - `field` is the field which this synonym is specific to.  If no field
           is specified, the synonym will be used for searches which are not
           specific to any particular field.

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        key = self._make_synonym_key(original, field)
        self._index.remove_synonym(key, synonym.lower())

    def clear_synonyms(self, original, field=None):
        """Remove all synonyms for a word (or phrase).

         - `field` is the field which this synonym is specific to.  If no field
           is specified, the synonym will be used for searches which are not
           specific to any particular field.

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        key = self._make_synonym_key(original, field)
        self._index.clear_synonyms(key)

    def _assert_facet(self, facet):
        """Raise an error if facet is not a declared facet field.

        """
        for action in self._field_actions[facet]._actions:
            if action == FieldActions.FACET:
                return
        raise errors.IndexerError("Field %r is not indexed as a facet" % facet)

    def add_subfacet(self, subfacet, facet):
        """Add a subfacet-facet relationship to the facet hierarchy.
        
        Any existing relationship for that subfacet is replaced.

        Raises a KeyError if either facet or subfacet is not a field,
        and an IndexerError if either facet or subfacet is not a facet field.
        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        self._assert_facet(facet)
        self._assert_facet(subfacet)
        self._facet_hierarchy[subfacet] = facet
        self._config_modified = True

    def remove_subfacet(self, subfacet):
        """Remove any existing facet hierarchy relationship for a subfacet.

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        if subfacet in self._facet_hierarchy:
            del self._facet_hierarchy[subfacet]
            self._config_modified = True

    def get_subfacets(self, facet):
        """Get a list of subfacets of a facet.

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        return [k for k, v in self._facet_hierarchy.iteritems() if v == facet] 

    FacetQueryType_Preferred = 1;
    FacetQueryType_Never = 2;
    def set_facet_for_query_type(self, query_type, facet, association):
        """Set the association between a query type and a facet.

        The value of `association` must be one of
        IndexerConnection.FacetQueryType_Preferred,
        IndexerConnection.FacetQueryType_Never or None. A value of None removes
        any previously set association.

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        if query_type is None:
            raise errors.IndexerError("Cannot set query type information for None")
        self._assert_facet(facet)
        if query_type not in self._facet_query_table:
            self._facet_query_table[query_type] = {}
        if association is None:
            if facet in self._facet_query_table[query_type]:
                del self._facet_query_table[query_type][facet]
        else:
            self._facet_query_table[query_type][facet] = association;
        if self._facet_query_table[query_type] == {}:
            del self._facet_query_table[query_type]
        self._config_modified = True

    def get_facets_for_query_type(self, query_type, association):
        """Get the set of facets associated with a query type.

        Only those facets associated with the query type in the specified
        manner are returned; `association` must be one of
        IndexerConnection.FacetQueryType_Preferred or
        IndexerConnection.FacetQueryType_Never.

        If the query type has no facets associated with it, None is returned.

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        if query_type not in self._facet_query_table:
            return None
        facet_dict = self._facet_query_table[query_type]
        return set([facet for facet, assoc in facet_dict.iteritems() if assoc == association])

    def set_metadata(self, key, value):
        """Set an item of metadata stored in the connection.

        The value supplied will be returned by subsequent calls to
        get_metadata() which use the same key.

        Keys with a leading underscore are reserved for internal use - you
        should not use such keys unless you really know what you are doing.

        This will store the value supplied in the database.  It will not be
        visible to readers (ie, search connections) until after the next flush.

        The key is limited to about 200 characters (the same length as a term
        is limited to).  The value can be several megabytes in size.

        To remove an item of metadata, simply call this with a `value`
        parameter containing an empty string.

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        if not hasattr(self._index, 'set_metadata'):
            raise errors.IndexerError("Version of xapian in use does not support metadata")
        log(self._index.set_metadata, key, value)

    def get_metadata(self, key):
        """Get an item of metadata stored in the connection.

        This returns a value stored by a previous call to set_metadata.

        If the value is not found, this will return the empty string.

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        if not hasattr(self._index, 'get_metadata'):
            raise errors.IndexerError("Version of xapian in use does not support metadata")
        return log(self._index.get_metadata, key)

    def delete(self, id):
        """Delete a document from the search engine index.

        If the id does not already exist in the database, this method
        will have no effect (and will not report an error).

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        self._index.delete_document('Q' + id)

    def flush(self):
        """Apply recent changes to the database.

        If an exception occurs, any changes since the last call to flush() may
        be lost.

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        if self._config_modified:
            self._store_config()
        self._index.flush()
        self._mem_buffered = 0

    def close(self):
        """Close the connection to the database.

        It is important to call this method before allowing the class to be
        garbage collected, because it will ensure that any un-flushed changes
        will be flushed.  It also ensures that the connection is cleaned up
        promptly.

        No other methods may be called on the connection after this has been
        called.  (It is permissible to call close() multiple times, but
        only the first call will have any effect.)

        If an exception occurs, the database will be closed, but changes since
        the last call to flush may be lost.

        """
        if self._index is None:
            return
        try:
            self.flush()
        finally:
            # There is currently no "close()" method for xapian databases, so
            # we have to rely on the garbage collector.  Since we never copy
            # the _index property out of this class, there should be no cycles,
            # so the standard python implementation should garbage collect
            # _index straight away.  A close() method is planned to be added to
            # xapian at some point - when it is, we should call it here to make
            # the code more robust.
            self._index = None
            self._indexpath = None
            self._field_actions = None
            self._config_modified = False

    def get_doccount(self):
        """Count the number of documents in the database.

        This count will include documents which have been added or removed but
        not yet flushed().

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        return self._index.get_doccount()

    def iterids(self):
        """Get an iterator which returns all the ids in the database.

        The unqiue_ids are currently returned in binary lexicographical sort
        order, but this should not be relied on.

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        return PrefixedTermIter('Q', self._index.allterms())

    def get_document(self, id):
        """Get the document with the specified unique ID.

        Raises a KeyError if there is no such document.  Otherwise, it returns
        a ProcessedDocument.

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        postlist = self._index.postlist('Q' + id)
        try:
            plitem = postlist.next()
        except StopIteration:
            # Unique ID not found
            raise KeyError('Unique ID %r not found' % id)
        try:
            postlist.next()
            raise errors.IndexerError("Multiple documents " #pragma: no cover
                                       "found with same unique ID")
        except StopIteration:
            # Only one instance of the unique ID found, as it should be.
            pass

        result = ProcessedDocument(self._field_mappings)
        result.id = id
        result._doc = self._index.get_document(plitem.docid)
        return result

    def iter_synonyms(self, prefix=""):
        """Get an iterator over the synonyms.

         - `prefix`: if specified, only synonym keys with this prefix will be
           returned.

        The iterator returns 2-tuples, in which the first item is the key (ie,
        a 2-tuple holding the term or terms which will be synonym expanded,
        followed by the fieldname specified (or None if no fieldname)), and the
        second item is a tuple of strings holding the synonyms for the first
        item.

        These return values are suitable for the dict() builtin, so you can
        write things like:

         >>> conn = IndexerConnection('foo')
         >>> conn.add_synonym('foo', 'bar')
         >>> conn.add_synonym('foo bar', 'baz')
         >>> conn.add_synonym('foo bar', 'foo baz')
         >>> dict(conn.iter_synonyms())
         {('foo', None): ('bar',), ('foo bar', None): ('baz', 'foo baz')}

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        return SynonymIter(self._index, self._field_mappings, prefix)

    def iter_subfacets(self):
        """Get an iterator over the facet hierarchy.

        The iterator returns 2-tuples, in which the first item is the
        subfacet and the second item is its parent facet.

        The return values are suitable for the dict() builtin, for example:

         >>> conn = IndexerConnection('db')
         >>> conn.add_field_action('foo', FieldActions.FACET)
         >>> conn.add_field_action('bar', FieldActions.FACET)
         >>> conn.add_field_action('baz', FieldActions.FACET)
         >>> conn.add_subfacet('foo', 'bar')
         >>> conn.add_subfacet('baz', 'bar')
         >>> dict(conn.iter_subfacets())
         {'foo': 'bar', 'baz': 'bar'}

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        if 'facets' in _checkxapian.missing_features:
            raise errors.IndexerError("Facets unsupported with this release of xapian")
        return self._facet_hierarchy.iteritems()

    def iter_facet_query_types(self, association):
        """Get an iterator over query types and their associated facets.

        Only facets associated with the query types in the specified manner
        are returned; `association` must be one of IndexerConnection.FacetQueryType_Preferred
        or IndexerConnection.FacetQueryType_Never.

        The iterator returns 2-tuples, in which the first item is the query
        type and the second item is the associated set of facets.

        The return values are suitable for the dict() builtin, for example:

         >>> conn = IndexerConnection('db')
         >>> conn.add_field_action('foo', FieldActions.FACET)
         >>> conn.add_field_action('bar', FieldActions.FACET)
         >>> conn.add_field_action('baz', FieldActions.FACET)
         >>> conn.set_facet_for_query_type('type1', 'foo', conn.FacetQueryType_Preferred)
         >>> conn.set_facet_for_query_type('type1', 'bar', conn.FacetQueryType_Never)
         >>> conn.set_facet_for_query_type('type1', 'baz', conn.FacetQueryType_Never)
         >>> conn.set_facet_for_query_type('type2', 'bar', conn.FacetQueryType_Preferred)
         >>> dict(conn.iter_facet_query_types(conn.FacetQueryType_Preferred))
         {'type1': set(['foo']), 'type2': set(['bar'])}
         >>> dict(conn.iter_facet_query_types(conn.FacetQueryType_Never))
         {'type1': set(['bar', 'baz'])}

        """
        if self._index is None:
            raise errors.IndexerError("IndexerConnection has been closed")
        if 'facets' in _checkxapian.missing_features:
            raise errors.IndexerError("Facets unsupported with this release of xapian")
        return FacetQueryTypeIter(self._facet_query_table, association)

class PrefixedTermIter(object):
    """Iterate through all the terms with a given prefix.

    """
    def __init__(self, prefix, termiter):
        """Initialise the prefixed term iterator.

        - `prefix` is the prefix to return terms for.
        - `termiter` is a xapian TermIterator, which should be at its start.

        """

        # The algorithm used in next() currently only works for single
        # character prefixes, so assert that the prefix is single character.
        # To deal with multicharacter prefixes, we need to check for terms
        # which have a starting prefix equal to that given, but then have a
        # following uppercase alphabetic character, indicating that the actual
        # prefix is longer than the target prefix.  We then need to skip over
        # these.  Not too hard to implement, but we don't need it yet.
        assert(len(prefix) == 1)

        self._started = False
        self._prefix = prefix
        self._prefixlen = len(prefix)
        self._termiter = termiter

    def __iter__(self):
        return self

    def next(self):
        """Get the next term with the specified prefix.

        """
        if not self._started:
            term = self._termiter.skip_to(self._prefix).term
            self._started = True
        else:
            term = self._termiter.next().term
        if len(term) < self._prefixlen or term[:self._prefixlen] != self._prefix:
            raise StopIteration
        return term[self._prefixlen:]


class SynonymIter(object):
    """Iterate through a list of synonyms.

    """
    def __init__(self, index, field_mappings, prefix):
        """Initialise the synonym iterator.

         - `index` is the index to get the synonyms from.
         - `field_mappings` is the FieldMappings object for the iterator.
         - `prefix` is the prefix to restrict the returned synonyms to.

        """
        self._index = index
        self._field_mappings = field_mappings
        self._syniter = self._index.synonym_keys(prefix)

    def __iter__(self):
        return self

    def next(self):
        """Get the next synonym.

        """
        synkey = self._syniter.next()
        pos = 0
        for char in synkey:
            if char.isupper(): pos += 1
            else: break
        if pos == 0:
            fieldname = None
            terms = synkey
        else:
            prefix = synkey[:pos]
            fieldname = self._field_mappings.get_fieldname_from_prefix(prefix)
            terms = ' '.join((term[pos:] for term in synkey.split(' ')))
        synval = tuple(self._index.synonyms(synkey))
        return ((terms, fieldname), synval)

class FacetQueryTypeIter(object):
    """Iterate through all the query types and their associated facets.

    """
    def __init__(self, facet_query_table, association):
        """Initialise the query type facet iterator.

        Only facets associated with each query type in the specified
        manner are returned (`association` must be one of
        IndexerConnection.FacetQueryType_Preferred or
        IndexerConnection.FacetQueryType_Never).

        """
        self._table_iter = facet_query_table.iteritems()
        self._association = association

    def __iter__(self):
        return self

    def next(self):
        """Get the next (query type, facet set) 2-tuple.

        """
        query_type, facet_dict = self._table_iter.next()
        facet_list = [facet for facet, association in facet_dict.iteritems() if association == self._association]
        if len(facet_list) == 0:
            return self.next()
        return (query_type, set(facet_list))

if __name__ == '__main__':
    import doctest, sys
    doctest.testmod (sys.modules[__name__])

########NEW FILE########
__FILENAME__ = marshall
#!/usr/bin/env python
#
# Copyright (C) 2007 Lemur Consulting Ltd
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
r"""marshall.py: Marshal values into strings

"""
__docformat__ = "restructuredtext en"

import math
import xapian
from replaylog import log as _log

def float_to_string(value):
    """Marshall a floating point number to a string which sorts in the
    appropriate manner.

    """
    return _log(xapian.sortable_serialise, value)

def date_to_string(date):
    """Marshall a date to a string which sorts in the appropriate manner.

    """
    return '%04d%02d%02d' % (date.year, date.month, date.day)

########NEW FILE########
__FILENAME__ = memutils
#!/usr/bin/env python
#
# Copyright (C) 2007 Lemur Consulting Ltd
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
r"""memutils.py: Memory handling utilities.

"""
__docformat__ = "restructuredtext en"

import os

def _get_physical_mem_sysconf():
    """Try getting a value for the physical memory using os.sysconf().

    Returns None if no value can be obtained - otherwise, returns a value in
    bytes.

    """
    if getattr(os, 'sysconf', None) is None:
        return None

    try:
        pagesize = os.sysconf('SC_PAGESIZE')
    except ValueError:
        try:
            pagesize = os.sysconf('SC_PAGE_SIZE')
        except ValueError:
            return None

    try:
        pagecount = os.sysconf('SC_PHYS_PAGES')
    except ValueError:
        return None

    return pagesize * pagecount

def _get_physical_mem_win32():
    """Try getting a value for the physical memory using GlobalMemoryStatus.

    This is a windows specific method.  Returns None if no value can be
    obtained (eg, not running on windows) - otherwise, returns a value in
    bytes.

    """
    try:
        import ctypes
        import ctypes.wintypes as wintypes
    except ValueError:
        return None
    
    class MEMORYSTATUS(wintypes.Structure):
        _fields_ = [
            ('dwLength', wintypes.DWORD),
            ('dwMemoryLoad', wintypes.DWORD),
            ('dwTotalPhys', wintypes.DWORD),
            ('dwAvailPhys', wintypes.DWORD),
            ('dwTotalPageFile', wintypes.DWORD),
            ('dwAvailPageFile', wintypes.DWORD),
            ('dwTotalVirtual', wintypes.DWORD),
            ('dwAvailVirtual', wintypes.DWORD),
        ]

    m = MEMORYSTATUS()
    wintypes.windll.kernel32.GlobalMemoryStatus(wintypes.byref(m))
    return m.dwTotalPhys

def get_physical_memory():
    """Get the amount of physical memory in the system, in bytes.

    If this can't be obtained, returns None.

    """
    result = _get_physical_mem_sysconf()
    if result is not None:
        return result
    return _get_physical_mem_win32()

########NEW FILE########
__FILENAME__ = parsedate
#!/usr/bin/env python
#
# Copyright (C) 2007 Lemur Consulting Ltd
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
r"""parsedate.py: Parse date strings.

"""
__docformat__ = "restructuredtext en"

import datetime
import re

yyyymmdd_re = re.compile(r'(?P<year>[0-9]{4})(?P<month>[0-9]{2})(?P<day>[0-9]{2})$')
yyyy_mm_dd_re = re.compile(r'(?P<year>[0-9]{4})([-/.])(?P<month>[0-9]{2})\2(?P<day>[0-9]{2})$')

def date_from_string(value):
    """Parse a string into a date.

    If the value supplied is already a date-like object (ie, has 'year',
    'month' and 'day' attributes), it is returned without processing.

    Supported date formats are:

     - YYYYMMDD
     - YYYY-MM-DD 
     - YYYY/MM/DD 
     - YYYY.MM.DD 

    """
    if (hasattr(value, 'year')
        and hasattr(value, 'month')
        and hasattr(value, 'day')):
        return value

    mg = yyyymmdd_re.match(value)
    if mg is None:
        mg = yyyy_mm_dd_re.match(value)

    if mg is not None:
        year, month, day = (int(i) for i in mg.group('year', 'month', 'day'))
        return datetime.date(year, month, day)

    raise ValueError('Unrecognised date format')

########NEW FILE########
__FILENAME__ = replaylog
#!/usr/bin/env python
#
# Copyright (C) 2007 Lemur Consulting Ltd
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
r"""replaylog.py: Log all xapian calls to a file, so that they can be replayed.

"""
__docformat__ = "restructuredtext en"

import datetime
import sys
import thread
import threading
import time
import traceback
import types
import weakref
import xapian

from pprint import pprint

# The logger in use.
_replay_log = None

# True if a replay log has ever been in use since import time.
_had_replay_log = False

class NotifyingDeleteObject(object):
    """An wrapping for an object which calls a callback when its deleted.

    Note that the callback will be called from a __del__ method, so shouldn't
    raise any exceptions, and probably shouldn't make new references to the
    object supplied to it.

    """
    def __init__(self, obj, callback):
        self.obj = obj
        self.callback = callback

    def __del__(self):
        self.callback(self.obj)

class ReplayLog(object):
    """Log of xapian calls, to be replayed.

    """

    def __init__(self, logpath):
        """Create a new replay log.

        """
        # Mutex used to protect all access to _fd
        self._fd_mutex = threading.Lock()
        self._fd = file(logpath, 'wb')

        # Mutex used to protect all access to members other than _fd
        self._mutex = threading.Lock()
        self._next_call = 1

        self._next_thread = 0
        self._thread_ids = {}

        self._objs = weakref.WeakKeyDictionary()
        self._next_num = 1

        self._xapian_classes = {}
        self._xapian_functions = {}
        self._xapian_methods = {}
        for name in dir(xapian):
            item = getattr(xapian, name)
            has_members = False
            for membername in dir(item):
                member = getattr(item, membername)
                if isinstance(member, types.MethodType):
                    self._xapian_methods[member.im_func] = (name, membername)
                    has_members = True
            if has_members:
                self._xapian_classes[item] = name
            if isinstance(item, types.BuiltinFunctionType):
                self._xapian_functions[item] = name

    def _get_obj_num(self, obj, maybe_new):
        """Get the number associated with an object.

        If maybe_new is False, a value of 0 will be supplied if the object
        hasn't already been seen.  Otherwise, a new (and previously unused)
        value will be allocated to the object.

        The mutex should be held when this is called.

        """
        try:
            num = self._objs[obj]
            return num.obj
        except KeyError:
            pass

        if not maybe_new:
            return 0

        self._objs[obj] = NotifyingDeleteObject(self._next_num, self._obj_gone)
        self._next_num += 1
        return self._next_num - 1

    def _is_xap_obj(self, obj):
        """Return True iff an object is an instance of a xapian object.

        (Also returns true if the object is an instance of a subclass of a
        xapian object.)

        The mutex should be held when this is called.

        """
        # Check for xapian classes.
        classname = self._xapian_classes.get(type(obj), None)
        if classname is not None:
            return True
        # Check for subclasses of xapian classes.
        for classobj, classname in self._xapian_classes.iteritems():
            if isinstance(obj, classobj):
                return True
        # Not a xapian class or subclass.
        return False

    def _get_xap_name(self, obj, maybe_new=False):
        """Get the name of a xapian class or method.

        The mutex should be held when this is called.

        """
        # Check if it's a xapian class, or subclass.
        if isinstance(obj, types.TypeType):
            classname = self._xapian_classes.get(obj, None)
            if classname is not None:
                return classname

            for classobj, classname in self._xapian_classes.iteritems():
                if issubclass(obj, classobj):
                    return "subclassof_%s" % (classname, )

            return None

        # Check if it's a xapian function.
        if isinstance(obj, types.BuiltinFunctionType):
            funcname = self._xapian_functions.get(obj, None)
            if funcname is not None:
                return funcname

        # Check if it's a proxied object.
        if isinstance(obj, LoggedProxy):
            classname = self._xapian_classes.get(obj.__class__, None)
            if classname is not None:
                objnum = self._get_obj_num(obj, maybe_new=maybe_new)
                return "%s#%d" % (classname, objnum)

        # Check if it's a proxied method.
        if isinstance(obj, LoggedProxyMethod):
            classname, methodname = self._xapian_methods[obj.real.im_func]
            objnum = self._get_obj_num(obj.proxyobj, maybe_new=maybe_new)
            return "%s#%d.%s" % (classname, objnum, methodname)

        # Check if it's a subclass of a xapian class.  Note: this will only
        # pick up subclasses, because the original classes are filtered out
        # higher up.
        for classobj, classname in self._xapian_classes.iteritems():
            if isinstance(obj, classobj):
                objnum = self._get_obj_num(obj, maybe_new=maybe_new)
                return "subclassof_%s#%d" % (classname, objnum)

        return None

    def _log(self, msg):
        self._fd_mutex.acquire()
        try:
#            msg = '%s,%s' % (
#                datetime.datetime.fromtimestamp(time.time()).isoformat(),
#                msg,
#            )
            self._fd.write(msg)
            self._fd.flush()
        finally:
            self._fd_mutex.release()

    def _repr_arg(self, arg):
        """Return a representation of an argument.

        The mutex should be held when this is called.

        """

        xapargname = self._get_xap_name(arg)
        if xapargname is not None:
            return xapargname

        if isinstance(arg, basestring):
            if isinstance(arg, unicode):
                arg = arg.encode('utf-8')
            return 'str(%d,%s)' % (len(arg), arg)

        if isinstance(arg, long):
            try:
                arg = int(arg)
            except OverFlowError:
                pass

        if isinstance(arg, long):
            return 'long(%d)' % arg

        if isinstance(arg, int):
            return 'int(%d)' % arg

        if isinstance(arg, float):
            return 'float(%f)' % arg

        if arg is None:
            return 'None'

        if hasattr(arg, '__iter__'):
            seq = []
            for item in arg:
                seq.append(self._repr_arg(item))
            return 'list(%s)' % ','.join(seq)

        return 'UNKNOWN:' + str(arg)

    def _repr_args(self, args):
        """Return a representation of a list of arguments.

        The mutex should be held when this is called.

        """
        logargs = []
        for arg in args:
            logargs.append(self._repr_arg(arg))
        return ','.join(logargs)

    def _get_call_id(self):
        """Get an ID string for a call.

        The mutex should be held when this is called.

        """
        call_num = self._next_call
        self._next_call += 1

        thread_id = thread.get_ident()
        try:
            thread_num = self._thread_ids[thread_id]
        except KeyError:
            thread_num = self._next_thread
            self._thread_ids[thread_id] = thread_num
            self._next_thread += 1

        if thread_num is 0:
            return "%s" % call_num
        return "%dT%d" % (call_num, thread_num)

    def log_call(self, call, *args):
        """Add a log message about a call.

        Returns a number for the call, so it can be tied to a particular
        result.

        """
        self._mutex.acquire()
        try:
            logargs = self._repr_args(args)
            xapobjname = self._get_xap_name(call)
            call_id = self._get_call_id()
        finally:
            self._mutex.release()

        if xapobjname is not None:
            self._log("CALL%s:%s(%s)\n" % (call_id, xapobjname, logargs))
        else:
            self._log("CALL%s:UNKNOWN:%r(%s)\n" % (call_id, call, logargs))
        return call_id

    def log_except(self, (etype, value, tb), call_id):
        """Log an exception which has occurred.

        """
        # No access to an members, so no need to acquire mutex.
        exc = traceback.format_exception_only(etype, value)
        self._log("EXCEPT%s:%s\n" % (call_id, ''.join(exc).strip()))

    def log_retval(self, ret, call_id):
        """Log a return value.

        """
        if ret is None:
            self._log("RET%s:None\n" % call_id)
            return

        self._mutex.acquire()
        try:
            # If it's a xapian object, return a proxy for it.
            if self._is_xap_obj(ret):
                ret = LoggedProxy(ret)
                xapobjname = self._get_xap_name(ret, maybe_new=True)
            msg = "RET%s:%s\n" % (call_id, self._repr_arg(ret))
        finally:
            self._mutex.release()

        # Not a xapian object - just return it.
        self._log(msg)
        return ret

    def _obj_gone(self, num):
        """Log that an object has been deleted.

        """
        self._log('DEL:#%d\n' % num)

class LoggedProxy(object):
    """A proxy for a xapian object, which logs all calls made on the object.

    """
    def __init__(self, obj):
        self.__obj = obj

    def __getattribute__(self, name):
        obj = object.__getattribute__(self, '_LoggedProxy__obj')
        if name == '__obj':
            return obj
        real = getattr(obj, name)
        if not isinstance(real, types.MethodType):
            return real
        return LoggedProxyMethod(real, self)

    def __iter__(self):
        obj = object.__getattribute__(self, '_LoggedProxy__obj')
        return obj.__iter__()

    def __len__(self):
        obj = object.__getattribute__(self, '_LoggedProxy__obj')
        return obj.__len__()

    def __repr__(self):
        obj = object.__getattribute__(self, '_LoggedProxy__obj')
        return '<LoggedProxy of %s >' % obj.__repr__()

    def __str__(self):
        obj = object.__getattribute__(self, '_LoggedProxy__obj')
        return obj.__str__()

class LoggedProxyMethod(object):
    """A proxy for a xapian method, which logs all calls made on the method.

    """
    def __init__(self, real, proxyobj):
        """Make a proxy for the method.

        """
        self.real = real
        self.proxyobj = proxyobj

    def __call__(self, *args):
        """Call the proxied method, logging the call.

        """
        return log(self, *args)

def set_replay_path(logpath):
    """Set the path for the replay log.

    """
    global _replay_log
    global _had_replay_log
    if logpath is None:
        _replay_log = None
    else:
        _had_replay_log = True
        _replay_log = ReplayLog(logpath)

def _unproxy_call_and_args(call, args):
    """Convert a call and list of arguments to unproxied form.

    """
    if isinstance(call, LoggedProxyMethod):
        realcall = call.real
    else:
        realcall = call

    realargs = []
    for arg in args:
        if isinstance(arg, LoggedProxy):
            arg = arg.__obj
        realargs.append(arg)

    return realcall, realargs

def log(call, *args):
    """Make a call to xapian, and log it.

    """
    # If we've never had a replay log in force, no need to unproxy objects.
    global _had_replay_log
    if not _had_replay_log:
        return call(*args)

    # Get unproxied versions of the call and arguments.
    realcall, realargs = _unproxy_call_and_args(call, args)

    # If we have no replay log currently, just do the call.
    global _replay_log
    replay_log = _replay_log
    if replay_log is None:
        return realcall(*realargs)

    # We have a replay log: do a logged version of the call.
    call_id = replay_log.log_call(call, *args)
    try:
        ret = realcall(*realargs)
    except:
        replay_log.log_except(sys.exc_info(), call_id)
        raise
    return replay_log.log_retval(ret, call_id)

#set_replay_path('replay.log')

########NEW FILE########
__FILENAME__ = schema
#!/usr/bin/env python
#
# Copyright (C) 2008 Lemur Consulting Ltd
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
r"""schema.py: xdefinitions and implementations of field actions.

"""
__docformat__ = "restructuredtext en"

import errors as _errors
from replaylog import log as _log
import parsedate as _parsedate

class Schema(object):
    def __init__(self):
        pass

if __name__ == '__main__':
    import doctest, sys
    doctest.testmod (sys.modules[__name__])

########NEW FILE########
__FILENAME__ = searchconnection
#!/usr/bin/env python
#
# Copyright (C) 2007 Lemur Consulting Ltd
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
r"""searchconnection.py: A connection to the search engine for searching.

"""
__docformat__ = "restructuredtext en"

import _checkxapian
import os as _os
import cPickle as _cPickle
import math

import xapian as _xapian
from datastructures import *
from fieldactions import *
import fieldmappings as _fieldmappings
import highlight as _highlight 
import errors as _errors
import indexerconnection as _indexerconnection
import re as _re
from replaylog import log as _log

class SearchResult(ProcessedDocument):
    """A result from a search.

    As well as being a ProcessedDocument representing the document in the
    database, the result has several members which may be used to get
    information about how well the document matches the search:

     - `rank`: The rank of the document in the search results, starting at 0
       (ie, 0 is the "top" result, 1 is the second result, etc).

     - `weight`: A floating point number indicating the weight of the result
       document.  The value is only meaningful relative to other results for a
       given search - a different search, or the same search with a different
       database, may give an entirely different scale to the weights.  This
       should not usually be displayed to users, but may be useful if trying to
       perform advanced reweighting operations on search results.

     - `percent`: A percentage value for the weight of a document.  This is
       just a rescaled form of the `weight` member.  It doesn't represent any
       kind of probability value; the only real meaning of the numbers is that,
       within a single set of results, a document with a higher percentage
       corresponds to a better match.  Because the percentage doesn't really
       represent a probability, or a confidence value, it is probably unhelpful
       to display it to most users, since they tend to place an over emphasis
       on its meaning.  However, it is included because it may be useful
       occasionally.

    """
    def __init__(self, msetitem, results):
        ProcessedDocument.__init__(self, results._fieldmappings, msetitem.document)
        self.rank = msetitem.rank
        self.weight = msetitem.weight
        self.percent = msetitem.percent
        self._results = results

    def _get_language(self, field):
        """Get the language that should be used for a given field.

        Raises a KeyError if the field is not known.

        """
        actions = self._results._conn._field_actions[field]._actions
        for action, kwargslist in actions.iteritems():
            if action == FieldActions.INDEX_FREETEXT:
                for kwargs in kwargslist:
                    try:
                        return kwargs['language']
                    except KeyError:
                        pass
        return 'none'

    def summarise(self, field, maxlen=600, hl=('<b>', '</b>'), query=None):
        """Return a summarised version of the field specified.

        This will return a summary of the contents of the field stored in the
        search result, with words which match the query highlighted.

        The maximum length of the summary (in characters) may be set using the
        maxlen parameter.

        The return value will be a string holding the summary, with
        highlighting applied.  If there are multiple instances of the field in
        the document, the instances will be joined with a newline character.
        
        To turn off highlighting, set hl to None.  Each highlight will consist
        of the first entry in the `hl` list being placed before the word, and
        the second entry in the `hl` list being placed after the word.

        Any XML or HTML style markup tags in the field will be stripped before
        the summarisation algorithm is applied.

        If `query` is supplied, it should contain a Query object, as returned
        from SearchConnection.query_parse() or related methods, which will be
        used as the basis of the summarisation and highlighting rather than the
        query which was used for the search.

        Raises KeyError if the field is not known.

        """
        highlighter = _highlight.Highlighter(language_code=self._get_language(field))
        field = self.data[field]
        results = []
        text = '\n'.join(field)
        if query is None:
            query = self._results._query
        return highlighter.makeSample(text, query, maxlen, hl)

    def highlight(self, field, hl=('<b>', '</b>'), strip_tags=False, query=None):
        """Return a highlighted version of the field specified.

        This will return all the contents of the field stored in the search
        result, with words which match the query highlighted.

        The return value will be a list of strings (corresponding to the list
        of strings which is the raw field data).

        Each highlight will consist of the first entry in the `hl` list being
        placed before the word, and the second entry in the `hl` list being
        placed after the word.

        If `strip_tags` is True, any XML or HTML style markup tags in the field
        will be stripped before highlighting is applied.

        If `query` is supplied, it should contain a Query object, as returned
        from SearchConnection.query_parse() or related methods, which will be
        used as the basis of the summarisation and highlighting rather than the
        query which was used for the search.

        Raises KeyError if the field is not known.

        """
        highlighter = _highlight.Highlighter(language_code=self._get_language(field))
        field = self.data[field]
        results = []
        if query is None:
            query = self._results._query
        for text in field:
            results.append(highlighter.highlight(text, query, hl, strip_tags))
        return results

    def __repr__(self):
        return ('<SearchResult(rank=%d, id=%r, data=%r)>' %
                (self.rank, self.id, self.data))


class SearchResultIter(object):
    """An iterator over a set of results from a search.

    """
    def __init__(self, results, order):
        self._results = results
        self._order = order
        if self._order is None:
            self._iter = iter(results._mset)
        else:
            self._iter = iter(self._order)

    def next(self):
        if self._order is None:
            msetitem = self._iter.next()
        else:
            index = self._iter.next()
            msetitem = self._results._mset.get_hit(index)
        return SearchResult(msetitem, self._results)


def _get_significant_digits(value, lower, upper):
    """Get the significant digits of value which are constrained by the
    (inclusive) lower and upper bounds.

    If there are no significant digits which are definitely within the
    bounds, exactly one significant digit will be returned in the result.

    >>> _get_significant_digits(15,15,15)
    15
    >>> _get_significant_digits(15,15,17)
    20
    >>> _get_significant_digits(4777,208,6000)
    5000
    >>> _get_significant_digits(4777,4755,4790)
    4800
    >>> _get_significant_digits(4707,4695,4710)
    4700
    >>> _get_significant_digits(4719,4717,4727)
    4720
    >>> _get_significant_digits(0,0,0)
    0
    >>> _get_significant_digits(9,9,10)
    9
    >>> _get_significant_digits(9,9,100)
    9

    """
    assert(lower <= value)
    assert(value <= upper)
    diff = upper - lower

    # Get the first power of 10 greater than the difference.
    # This corresponds to the magnitude of the smallest significant digit.
    if diff == 0:
        pos_pow_10 = 1
    else:
        pos_pow_10 = int(10 ** math.ceil(math.log10(diff)))

    # Special case for situation where we don't have any significant digits:
    # get the magnitude of the most significant digit in value.
    if pos_pow_10 > value:
        if value == 0:
            pos_pow_10 = 1
        else:
            pos_pow_10 = int(10 ** math.floor(math.log10(value)))

    # Return the value, rounded to the nearest multiple of pos_pow_10
    return ((value + pos_pow_10 // 2) // pos_pow_10) * pos_pow_10

class SearchResults(object):
    """A set of results of a search.

    """
    def __init__(self, conn, enq, query, mset, fieldmappings, tagspy,
                 tagfields, facetspy, facetfields, facethierarchy,
                 facetassocs):
        self._conn = conn
        self._enq = enq
        self._query = query
        self._mset = mset
        self._mset_order = None
        self._fieldmappings = fieldmappings
        self._tagspy = tagspy
        if tagfields is None:
            self._tagfields = None
        else:
            self._tagfields = set(tagfields)
        self._facetspy = facetspy
        self._facetfields = facetfields
        self._facethierarchy = facethierarchy
        self._facetassocs = facetassocs
        self._numeric_ranges_built = {}

    def _cluster(self, num_clusters, maxdocs, fields=None):
        """Cluster results based on similarity.

        Note: this method is experimental, and will probably disappear or
        change in the future.

        The number of clusters is specified by num_clusters: unless there are
        too few results, there will be exaclty this number of clusters in the
        result.

        """
        clusterer = _xapian.ClusterSingleLink()
        xapclusters = _xapian.ClusterAssignments()
        docsim = _xapian.DocSimCosine()
        source = _xapian.MSetDocumentSource(self._mset, maxdocs)

        if fields is None:
            clusterer.cluster(self._conn._index, xapclusters, docsim, source, num_clusters)
        else:
            decider = self._make_expand_decider(fields)
            clusterer.cluster(self._conn._index, xapclusters, docsim, source, decider, num_clusters)

        newid = 0
        idmap = {}
        clusters = {}
        for item in self._mset:
            docid = item.docid
            clusterid = xapclusters.cluster(docid)
            if clusterid not in idmap:
                idmap[clusterid] = newid
                newid += 1
            clusterid = idmap[clusterid]
            if clusterid not in clusters:
                clusters[clusterid] = []
            clusters[clusterid].append(item.rank)
        return clusters

    def _reorder_by_clusters(self, clusters):
        """Reorder the mset based on some clusters.

        """
        if self.startrank != 0:
            raise _errors.SearchError("startrank must be zero to reorder by clusters")
        reordered = False
        tophits = []
        nottophits = []

        clusterstarts = dict(((c[0], None) for c in clusters.itervalues()))
        for i in xrange(self.endrank):
            if i in clusterstarts:
                tophits.append(i)
            else:
                nottophits.append(i)
        self._mset_order = tophits
        self._mset_order.extend(nottophits)

    def _make_expand_decider(self, fields):
        """Make an expand decider which accepts only terms in the specified
        field.

        """
        prefixes = {}
        if isinstance(fields, basestring):
            fields = [fields]
        for field in fields:
            try:
                actions = self._conn._field_actions[field]._actions
            except KeyError:
                continue
            for action, kwargslist in actions.iteritems():
                if action == FieldActions.INDEX_FREETEXT:
                    prefix = self._conn._field_mappings.get_prefix(field)
                    prefixes[prefix] = None
                    prefixes['Z' + prefix] = None
                if action in (FieldActions.INDEX_EXACT,
                              FieldActions.TAG,
                              FieldActions.FACET,):
                    prefix = self._conn._field_mappings.get_prefix(field)
                    prefixes[prefix] = None
        prefix_re = _re.compile('|'.join([_re.escape(x) + '[^A-Z]' for x in prefixes.keys()]))
        class decider(_xapian.ExpandDecider):
            def __call__(self, term):
                return prefix_re.match(term) is not None
        return decider()

    def _reorder_by_similarity(self, count, maxcount, max_similarity,
                               fields=None):
        """Reorder results based on similarity.

        The top `count` documents will be chosen such that they are relatively
        dissimilar.  `maxcount` documents will be considered for moving around,
        and `max_similarity` is a value between 0 and 1 indicating the maximum
        similarity to the previous document before a document is moved down the
        result set.

        Note: this method is experimental, and will probably disappear or
        change in the future.

        """
        if self.startrank != 0:
            raise _errors.SearchError("startrank must be zero to reorder by similiarity")
        ds = _xapian.DocSimCosine()
        ds.set_termfreqsource(_xapian.DatabaseTermFreqSource(self._conn._index))

        if fields is not None:
            ds.set_expand_decider(self._make_expand_decider(fields))

        tophits = []
        nottophits = []
        full = False
        reordered = False

        sim_count = 0
        new_order = []
        end = min(self.endrank, maxcount)
        for i in xrange(end):
            if full:
                new_order.append(i)
                continue
            hit = self._mset.get_hit(i)
            if len(tophits) == 0:
                tophits.append(hit)
                continue

            # Compare each incoming hit to tophits
            maxsim = 0.0
            for tophit in tophits[-1:]:
                sim_count += 1
                sim = ds.similarity(hit.document, tophit.document)
                if sim > maxsim:
                    maxsim = sim

            # If it's not similar to an existing hit, add to tophits.
            if maxsim < max_similarity:
                tophits.append(hit)
            else:
                nottophits.append(hit)
                reordered = True

            # If we're full of hits, append to the end.
            if len(tophits) >= count:
                for hit in tophits:
                    new_order.append(hit.rank)
                for hit in nottophits:
                    new_order.append(hit.rank)
                full = True
        if not full:
            for hit in tophits:
                new_order.append(hit.rank)
            for hit in nottophits:
                new_order.append(hit.rank)
        if end != self.endrank:
            new_order.extend(range(end, self.endrank))
        assert len(new_order) == self.endrank
        if reordered:
            self._mset_order = new_order
        else:
            assert new_order == range(self.endrank)

    def __repr__(self):
        return ("<SearchResults(startrank=%d, "
                "endrank=%d, "
                "more_matches=%s, "
                "matches_lower_bound=%d, "
                "matches_upper_bound=%d, "
                "matches_estimated=%d, "
                "estimate_is_exact=%s)>" %
                (
                 self.startrank,
                 self.endrank,
                 self.more_matches,
                 self.matches_lower_bound,
                 self.matches_upper_bound,
                 self.matches_estimated,
                 self.estimate_is_exact,
                ))

    def _get_more_matches(self):
        # This check relies on us having asked for at least one more result
        # than retrieved to be checked.
        return (self.matches_lower_bound > self.endrank)
    more_matches = property(_get_more_matches, doc=
    """Check whether there are further matches after those in this result set.

    """)

    def _get_startrank(self):
        return self._mset.get_firstitem()
    startrank = property(_get_startrank, doc=
    """Get the rank of the first item in the search results.

    This corresponds to the "startrank" parameter passed to the search() method.

    """)

    def _get_endrank(self):
        return self._mset.get_firstitem() + len(self._mset)
    endrank = property(_get_endrank, doc=
    """Get the rank of the item after the end of the search results.

    If there are sufficient results in the index, this corresponds to the
    "endrank" parameter passed to the search() method.

    """)

    def _get_lower_bound(self):
        return self._mset.get_matches_lower_bound()
    matches_lower_bound = property(_get_lower_bound, doc=
    """Get a lower bound on the total number of matching documents.

    """)

    def _get_upper_bound(self):
        return self._mset.get_matches_upper_bound()
    matches_upper_bound = property(_get_upper_bound, doc=
    """Get an upper bound on the total number of matching documents.

    """)

    def _get_human_readable_estimate(self):
        lower = self._mset.get_matches_lower_bound()
        upper = self._mset.get_matches_upper_bound()
        est = self._mset.get_matches_estimated()
        return _get_significant_digits(est, lower, upper)
    matches_human_readable_estimate = property(_get_human_readable_estimate,
                                               doc=
    """Get a human readable estimate of the number of matching documents.

    This consists of the value returned by the "matches_estimated" property,
    rounded to an appropriate number of significant digits (as determined by
    the values of the "matches_lower_bound" and "matches_upper_bound"
    properties).

    """)

    def _get_estimated(self):
        return self._mset.get_matches_estimated()
    matches_estimated = property(_get_estimated, doc=
    """Get an estimate for the total number of matching documents.

    """)

    def _estimate_is_exact(self):
        return self._mset.get_matches_lower_bound() == \
               self._mset.get_matches_upper_bound()
    estimate_is_exact = property(_estimate_is_exact, doc=
    """Check whether the estimated number of matching documents is exact.

    If this returns true, the estimate given by the `matches_estimated`
    property is guaranteed to be correct.

    If this returns false, it is possible that the actual number of matching
    documents is different from the number given by the `matches_estimated`
    property.

    """)

    def get_hit(self, index):
        """Get the hit with a given index.

        """
        if self._mset_order is None:
            msetitem = self._mset.get_hit(index)
        else:
            msetitem = self._mset.get_hit(self._mset_order[index])
        return SearchResult(msetitem, self)
    __getitem__ = get_hit

    def __iter__(self):
        """Get an iterator over the hits in the search result.

        The iterator returns the results in increasing order of rank.

        """
        return SearchResultIter(self, self._mset_order)

    def __len__(self):
        """Get the number of hits in the search result.

        Note that this is not (usually) the number of matching documents for
        the search.  If startrank is non-zero, it's not even the rank of the
        last document in the search result.  It's simply the number of hits
        stored in the search result.

        It is, however, the number of items returned by the iterator produced
        by calling iter() on this SearchResults object.

        """
        return len(self._mset)

    def get_top_tags(self, field, maxtags):
        """Get the most frequent tags in a given field.

         - `field` - the field to get tags for.  This must have been specified
           in the "gettags" argument of the search() call.
         - `maxtags` - the maximum number of tags to return.

        Returns a sequence of 2-item tuples, in which the first item in the
        tuple is the tag, and the second is the frequency of the tag in the
        matches seen (as an integer).

        """
        if 'tags' in _checkxapian.missing_features:
            raise errors.SearchError("Tags unsupported with this release of xapian")
        if self._tagspy is None or field not in self._tagfields:
            raise _errors.SearchError("Field %r was not specified for getting tags" % field)
        prefix = self._conn._field_mappings.get_prefix(field)
        return self._tagspy.get_top_terms(prefix, maxtags)

    def get_suggested_facets(self, maxfacets=5, desired_num_of_categories=7,
                             required_facets=None):
        """Get a suggested set of facets, to present to the user.

        This returns a list, in descending order of the usefulness of the
        facet, in which each item is a tuple holding:

         - fieldname of facet.
         - sequence of 2-tuples holding the suggested values or ranges for that
           field:

           For facets of type 'string', the first item in the 2-tuple will
           simply be the string supplied when the facet value was added to its
           document.  For facets of type 'float', it will be a 2-tuple, holding
           floats giving the start and end of the suggested value range.

           The second item in the 2-tuple will be the frequency of the facet
           value or range in the result set.

        If required_facets is not None, it must be a field name, or a sequence
        of field names.  Any field names mentioned in required_facets will be
        returned if there are any facet values at all in the search results for
        that field.  The facet will only be omitted if there are no facet
        values at all for the field.

        The value of maxfacets will be respected as far as possible; the
        exception is that if there are too many fields listed in
        required_facets with at least one value in the search results, extra
        facets will be returned (ie, obeying the required_facets parameter is
        considered more important than the maxfacets parameter).

        If facet_hierarchy was indicated when search() was called, and the
        query included facets, then only subfacets of those query facets and
        top-level facets will be included in the returned list. Furthermore
        top-level facets will only be returned if there are remaining places
        in the list after it has been filled with subfacets. Note that
        required_facets is still respected regardless of the facet hierarchy.

        If a query type was specified when search() was called, and the query
        included facets, then facets with an association of Never to the
        query type are never returned, even if mentioned in required_facets.
        Facets with an association of Preferred are listed before others in
        the returned list.

        """
        if 'facets' in _checkxapian.missing_features:
            raise errors.SearchError("Facets unsupported with this release of xapian")
        if self._facetspy is None:
            raise _errors.SearchError("Facet selection wasn't enabled when the search was run")
        if isinstance(required_facets, basestring):
            required_facets = [required_facets]
        scores = []
        facettypes = {}
        for field, slot, kwargslist in self._facetfields:
            type = None
            for kwargs in kwargslist:
                type = kwargs.get('type', None)
                if type is not None: break
            if type is None: type = 'string'

            if type == 'float':
                if field not in self._numeric_ranges_built:
                    self._facetspy.build_numeric_ranges(slot, desired_num_of_categories)
                    self._numeric_ranges_built[field] = None
            facettypes[field] = type
            score = self._facetspy.score_categorisation(slot, desired_num_of_categories)
            scores.append((score, field, slot))

        # Sort on whether facet is top-level ahead of score (use subfacets first),
        # and on whether facet is preferred for the query type ahead of anything else
        if self._facethierarchy:
            # Note, tuple[-2] is the value of 'field' in a scores tuple
            scores = [(tuple[-2] not in self._facethierarchy,) + tuple for tuple in scores]
        if self._facetassocs:
            preferred = _indexerconnection.IndexerConnection.FacetQueryType_Preferred
            scores = [(self._facetassocs.get(tuple[-2]) != preferred,) + tuple for tuple in scores]
        scores.sort()
        if self._facethierarchy:
            index = 1
        else:
            index = 0
        if self._facetassocs:
            index += 1
        if index > 0:
            scores = [tuple[index:] for tuple in scores]

        results = []
        required_results = []
        for score, field, slot in scores:
            # Check if the facet is required
            required = False
            if required_facets is not None:
                required = field in required_facets

            # If we've got enough facets, and the field isn't required, skip it
            if not required and len(results) + len(required_results) >= maxfacets:
                continue

            # Get the values
            values = self._facetspy.get_values_as_dict(slot)
            if field in self._numeric_ranges_built:
                if '' in values:
                    del values['']

            # Required facets must occur at least once, other facets must occur
            # at least twice.
            if required:
                if len(values) < 1:
                    continue
            else:
                if len(values) <= 1:
                    continue

            newvalues = []
            if facettypes[field] == 'float':
                # Convert numbers to python numbers, and number ranges to a
                # python tuple of two numbers.
                for value, frequency in values.iteritems():
                    if len(value) <= 9:
                        value1 = _log(_xapian.sortable_unserialise, value)
                        value2 = value1
                    else:
                        value1 = _log(_xapian.sortable_unserialise, value[:9])
                        value2 = _log(_xapian.sortable_unserialise, value[9:])
                    newvalues.append(((value1, value2), frequency))
            else:
                for value, frequency in values.iteritems():
                    newvalues.append((value, frequency))

            newvalues.sort()
            if required:
                required_results.append((score, field, newvalues))
            else:
                results.append((score, field, newvalues))

        # Throw away any excess results if we have more required_results to
        # insert.
        maxfacets = maxfacets - len(required_results)
        if maxfacets <= 0:
            results = required_results
        else:
            results = results[:maxfacets]
            results.extend(required_results)
            results.sort()

        # Throw away the scores because they're not meaningful outside this
        # algorithm.
        results = [(field, newvalues) for (score, field, newvalues) in results]
        return results


class SearchConnection(object):
    """A connection to the search engine for searching.

    The connection will access a view of the database.

    """
    _qp_flags_base = _xapian.QueryParser.FLAG_LOVEHATE
    _qp_flags_phrase = _xapian.QueryParser.FLAG_PHRASE
    _qp_flags_synonym = (_xapian.QueryParser.FLAG_AUTO_SYNONYMS |
                         _xapian.QueryParser.FLAG_AUTO_MULTIWORD_SYNONYMS)
    _qp_flags_bool = _xapian.QueryParser.FLAG_BOOLEAN

    _index = None

    def __init__(self, indexpath):
        """Create a new connection to the index for searching.

        There may only an arbitrary number of search connections for a
        particular database open at a given time (regardless of whether there
        is a connection for indexing open as well).

        If the database doesn't exist, an exception will be raised.

        """
        self._index = _log(_xapian.Database, indexpath)
        self._indexpath = indexpath

        # Read the actions.
        self._load_config()

        self._close_handlers = []

    def __del__(self):
        self.close()

    def append_close_handler(self, handler, userdata=None):
        """Append a callback to the list of close handlers.

        These will be called when the SearchConnection is closed.  This happens
        when the close() method is called, or when the SearchConnection object
        is deleted.  The callback will be passed two arguments: the path to the
        SearchConnection object, and the userdata supplied to this method.

        The handlers will be called in the order in which they were added.

        The handlers will be called after the connection has been closed, so
        cannot prevent it closing: their return value will be ignored.  In
        addition, they should not raise any exceptions.

        """
        self._close_handlers.append((handler, userdata))

    def _get_sort_type(self, field):
        """Get the sort type that should be used for a given field.

        """
        try:
            actions = self._field_actions[field]._actions
        except KeyError:
            actions = {}
        for action, kwargslist in actions.iteritems():
            if action == FieldActions.SORT_AND_COLLAPSE:
                for kwargs in kwargslist:
                    return kwargs['type']

    def _load_config(self):
        """Load the configuration for the database.

        """
        # Note: this code is basically duplicated in the IndexerConnection
        # class.  Move it to a shared location.
        assert self._index is not None

        config_str = _log(self._index.get_metadata, '_xappy_config')
        if len(config_str) == 0:
            self._field_actions = {}
            self._field_mappings = _fieldmappings.FieldMappings()
            self._facet_hierarchy = {}
            self._facet_query_table = {}
            return

        try:
            (self._field_actions, mappings, self._facet_hierarchy, self._facet_query_table, self._next_docid) = _cPickle.loads(config_str)
        except ValueError:
            # Backwards compatibility - configuration used to lack _facet_hierarchy and _facet_query_table
            (self._field_actions, mappings, self._next_docid) = _cPickle.loads(config_str)
            self._facet_hierarchy = {}
            self._facet_query_table = {}
        self._field_mappings = _fieldmappings.FieldMappings(mappings)

    def reopen(self):
        """Reopen the connection.

        This updates the revision of the index which the connection references
        to the latest flushed revision.

        """
        if self._index is None:
            raise _errors.SearchError("SearchConnection has been closed")
        self._index.reopen()
        # Re-read the actions.
        self._load_config()
        
    def close(self):
        """Close the connection to the database.

        It is important to call this method before allowing the class to be
        garbage collected to ensure that the connection is cleaned up promptly.

        No other methods may be called on the connection after this has been
        called.  (It is permissible to call close() multiple times, but
        only the first call will have any effect.)

        If an exception occurs, the database will be closed, but changes since
        the last call to flush may be lost.

        """
        if self._index is None:
            return

        # Remember the index path
        indexpath = self._indexpath

        # There is currently no "close()" method for xapian databases, so
        # we have to rely on the garbage collector.  Since we never copy
        # the _index property out of this class, there should be no cycles,
        # so the standard python implementation should garbage collect
        # _index straight away.  A close() method is planned to be added to
        # xapian at some point - when it is, we should call it here to make
        # the code more robust.
        self._index = None
        self._indexpath = None
        self._field_actions = None
        self._field_mappings = None

        # Call the close handlers.
        for handler, userdata in self._close_handlers:
            try:
                handler(indexpath, userdata)
            except Exception, e:
                import sys, traceback
                print >>sys.stderr, "WARNING: unhandled exception in handler called by SearchConnection.close(): %s" % traceback.format_exception_only(type(e), e)

    def get_doccount(self):
        """Count the number of documents in the database.

        This count will include documents which have been added or removed but
        not yet flushed().

        """
        if self._index is None:
            raise _errors.SearchError("SearchConnection has been closed")
        return self._index.get_doccount()

    OP_AND = _xapian.Query.OP_AND
    OP_OR = _xapian.Query.OP_OR
    def query_composite(self, operator, queries):
        """Build a composite query from a list of queries.

        The queries are combined with the supplied operator, which is either
        SearchConnection.OP_AND or SearchConnection.OP_OR.

        """
        if self._index is None:
            raise _errors.SearchError("SearchConnection has been closed")
        return _log(_xapian.Query, operator, list(queries))

    def query_multweight(self, query, multiplier):
        """Build a query which modifies the weights of a subquery.

        This produces a query which returns the same documents as the subquery,
        and in the same order, but with the weights assigned to each document
        multiplied by the value of "multiplier".  "multiplier" may be any floating
        point value, but negative values will be clipped to 0, since Xapian
        doesn't support negative weights.

        This can be useful when producing queries to be combined with
        query_composite, because it allows the relative importance of parts of
        the query to be adjusted.

        """
        return _log(_xapian.Query, _xapian.Query.OP_SCALE_WEIGHT, query, multiplier)

    def query_filter(self, query, filter, exclude=False):
        """Filter a query with another query.

        If exclude is False (or not specified), documents will only match the
        resulting query if they match the both the first and second query: the
        results of the first query are "filtered" to only include those which
        also match the second query.

        If exclude is True, documents will only match the resulting query if
        they match the first query, but not the second query: the results of
        the first query are "filtered" to only include those which do not match
        the second query.
        
        Documents will always be weighted according to only the first query.

        - `query`: The query to filter.
        - `filter`: The filter to apply to the query.
        - `exclude`: If True, the sense of the filter is reversed - only
          documents which do not match the second query will be returned. 

        """
        if self._index is None:
            raise _errors.SearchError("SearchConnection has been closed")
        if not isinstance(filter, _xapian.Query):
            raise _errors.SearchError("Filter must be a Xapian Query object")
        if exclude:
            return _log(_xapian.Query, _xapian.Query.OP_AND_NOT, query, filter)
        else:
            return _log(_xapian.Query, _xapian.Query.OP_FILTER, query, filter)

    def query_adjust(self, primary, secondary):
        """Adjust the weights of one query with a secondary query.

        Documents will be returned from the resulting query if and only if they
        match the primary query (specified by the "primary" parameter).
        However, the weights (and hence, the relevance rankings) of the
        documents will be adjusted by adding weights from the secondary query
        (specified by the "secondary" parameter).

        """
        if self._index is None:
            raise _errors.SearchError("SearchConnection has been closed")
        return _log(_xapian.Query, _xapian.Query.OP_AND_MAYBE, primary, secondary)

    def query_range(self, field, begin, end):
        """Create a query for a range search.
        
        This creates a query which matches only those documents which have a
        field value in the specified range.

        Begin and end must be appropriate values for the field, according to
        the 'type' parameter supplied to the SORTABLE action for the field.

        The begin and end values are both inclusive - any documents with a
        value equal to begin or end will be returned (unless end is less than
        begin, in which case no documents will be returned).

        Begin or end may be set to None in order to create an open-ended
        range.  (They may also both be set to None, which will generate a query
        which matches all documents containing any value for the field.)

        """
        if self._index is None:
            raise _errors.SearchError("SearchConnection has been closed")

        if begin is None and end is None:
            # Return a "match everything" query
            return _log(_xapian.Query, '')

        try:
            slot = self._field_mappings.get_slot(field, 'collsort')
        except KeyError:
            # Return a "match nothing" query
            return _log(_xapian.Query)

        sorttype = self._get_sort_type(field)
        marshaller = SortableMarshaller(False)
        fn = marshaller.get_marshall_function(field, sorttype)

        if begin is not None:
            begin = fn(field, begin)
        if end is not None:
            end = fn(field, end)

        if begin is None:
            return _log(_xapian.Query, _xapian.Query.OP_VALUE_LE, slot, end)

        if end is None:
            return _log(_xapian.Query, _xapian.Query.OP_VALUE_GE, slot, begin)

        return _log(_xapian.Query, _xapian.Query.OP_VALUE_RANGE, slot, begin, end)

    def query_facet(self, field, val):
        """Create a query for a facet value.
        
        This creates a query which matches only those documents which have a
        facet value in the specified range.

        For a numeric range facet, val should be a tuple holding the start and
        end of the range, or a comma separated string holding two floating
        point values.  For other facets, val should be the value to look
        for.

        The start and end values are both inclusive - any documents with a
        value equal to start or end will be returned (unless end is less than
        start, in which case no documents will be returned).

        """
        if self._index is None:
            raise _errors.SearchError("SearchConnection has been closed")
        if 'facets' in _checkxapian.missing_features:
            raise errors.SearchError("Facets unsupported with this release of xapian")

        try:
            actions = self._field_actions[field]._actions
        except KeyError:
            actions = {}
        facettype = None
        for action, kwargslist in actions.iteritems():
            if action == FieldActions.FACET:
                for kwargs in kwargslist:
                    facettype = kwargs.get('type', None)
                    if facettype is not None:
                        break
            if facettype is not None:
                break

        if facettype == 'float':
            if isinstance(val, basestring):
                val = [float(v) for v in val.split(',', 2)]
            assert(len(val) == 2)
            try:
                slot = self._field_mappings.get_slot(field, 'facet')
            except KeyError:
                return _log(_xapian.Query)
            # FIXME - check that sorttype == self._get_sort_type(field)
            sorttype = 'float'
            marshaller = SortableMarshaller(False)
            fn = marshaller.get_marshall_function(field, sorttype)
            begin = fn(field, val[0])
            end = fn(field, val[1])
            return _log(_xapian.Query, _xapian.Query.OP_VALUE_RANGE, slot, begin, end)
        else:
            assert(facettype == 'string' or facettype is None)
            prefix = self._field_mappings.get_prefix(field)
            return _log(_xapian.Query, prefix + val.lower())


    def _prepare_queryparser(self, allow, deny, default_op, default_allow,
                             default_deny):
        """Prepare (and return) a query parser using the specified fields and
        operator.

        """
        if self._index is None:
            raise _errors.SearchError("SearchConnection has been closed")

        if isinstance(allow, basestring):
            allow = (allow, )
        if isinstance(deny, basestring):
            deny = (deny, )
        if allow is not None and len(allow) == 0:
            allow = None
        if deny is not None and len(deny) == 0:
            deny = None
        if allow is not None and deny is not None:
            raise _errors.SearchError("Cannot specify both `allow` and `deny` "
                                      "(got %r and %r)" % (allow, deny))

        if isinstance(default_allow, basestring):
            default_allow = (default_allow, )
        if isinstance(default_deny, basestring):
            default_deny = (default_deny, )
        if default_allow is not None and len(default_allow) == 0:
            default_allow = None
        if default_deny is not None and len(default_deny) == 0:
            default_deny = None
        if default_allow is not None and default_deny is not None:
            raise _errors.SearchError("Cannot specify both `default_allow` and `default_deny` "
                                      "(got %r and %r)" % (default_allow, default_deny))

        qp = _log(_xapian.QueryParser)
        qp.set_database(self._index)
        qp.set_default_op(default_op)

        if allow is None:
            allow = [key for key in self._field_actions]
        if deny is not None:
            allow = [key for key in allow if key not in deny]

        for field in allow:
            try:
                actions = self._field_actions[field]._actions
            except KeyError:
                actions = {}
            for action, kwargslist in actions.iteritems():
                if action == FieldActions.INDEX_EXACT:
                    # FIXME - need patched version of xapian to add exact prefixes
                    #qp.add_exact_prefix(field, self._field_mappings.get_prefix(field))
                    qp.add_prefix(field, self._field_mappings.get_prefix(field))
                if action == FieldActions.INDEX_FREETEXT:
                    allow_field_specific = True
                    for kwargs in kwargslist:
                        allow_field_specific = allow_field_specific or kwargs.get('allow_field_specific', True)
                    if not allow_field_specific:
                        continue
                    qp.add_prefix(field, self._field_mappings.get_prefix(field))
                    for kwargs in kwargslist:
                        try:
                            lang = kwargs['language']
                            my_stemmer = _log(_xapian.Stem, lang)
                            qp.my_stemmer = my_stemmer
                            qp.set_stemmer(my_stemmer)
                            qp.set_stemming_strategy(qp.STEM_SOME)
                        except KeyError:
                            pass

        if default_allow is not None or default_deny is not None:
            if default_allow is None:
                default_allow = [key for key in self._field_actions]
            if default_deny is not None:
                default_allow = [key for key in default_allow if key not in default_deny]
            for field in default_allow:
                try:
                    actions = self._field_actions[field]._actions
                except KeyError:
                    actions = {}
                for action, kwargslist in actions.iteritems():
                    if action == FieldActions.INDEX_FREETEXT:
                        qp.add_prefix('', self._field_mappings.get_prefix(field))
                        # FIXME - set stemming options for the default prefix

        return qp

    def _query_parse_with_prefix(self, qp, string, flags, prefix):
        """Parse a query, with an optional prefix.

        """
        if prefix is None:
            return qp.parse_query(string, flags)
        else:
            return qp.parse_query(string, flags, prefix)

    def _query_parse_with_fallback(self, qp, string, prefix=None):
        """Parse a query with various flags.
        
        If the initial boolean pass fails, fall back to not using boolean
        operators.

        """
        try:
            q1 = self._query_parse_with_prefix(qp, string,
                                               self._qp_flags_base |
                                               self._qp_flags_phrase |
                                               self._qp_flags_synonym |
                                               self._qp_flags_bool,
                                               prefix)
        except _xapian.QueryParserError, e:
            # If we got a parse error, retry without boolean operators (since
            # these are the usual cause of the parse error).
            q1 = self._query_parse_with_prefix(qp, string,
                                               self._qp_flags_base |
                                               self._qp_flags_phrase |
                                               self._qp_flags_synonym,
                                               prefix)

        qp.set_stemming_strategy(qp.STEM_NONE)
        try:
            q2 = self._query_parse_with_prefix(qp, string,
                                               self._qp_flags_base |
                                               self._qp_flags_bool,
                                               prefix)
        except _xapian.QueryParserError, e:
            # If we got a parse error, retry without boolean operators (since
            # these are the usual cause of the parse error).
            q2 = self._query_parse_with_prefix(qp, string,
                                               self._qp_flags_base,
                                               prefix)

        return _log(_xapian.Query, _xapian.Query.OP_AND_MAYBE, q1, q2)

    def query_parse(self, string, allow=None, deny=None, default_op=OP_AND,
                    default_allow=None, default_deny=None):
        """Parse a query string.

        This is intended for parsing queries entered by a user.  If you wish to
        combine structured queries, it is generally better to use the other
        query building methods, such as `query_composite` (though you may wish
        to create parts of the query to combine with such methods with this
        method).

        The string passed to this method can have various operators in it.  In
        particular, it may contain field specifiers (ie, field names, followed
        by a colon, followed by some text to search for in that field).  For
        example, if "author" is a field in the database, the search string
        could contain "author:richard", and this would be interpreted as
        "search for richard in the author field".  By default, any fields in
        the database which are indexed with INDEX_EXACT or INDEX_FREETEXT will
        be available for field specific searching in this way - however, this
        can be modified using the "allow" or "deny" parameters, and also by the
        allow_field_specific tag on INDEX_FREETEXT fields.

        Any text which isn't prefixed by a field specifier is used to search
        the "default set" of fields.  By default, this is the full set of
        fields in the database which are indexed with INDEX_FREETEXT and for
        which the search_by_default flag set (ie, if the text is found in any
        of those fields, the query will match).  However, this may be modified
        with the "default_allow" and "default_deny" parameters.  (Note that
        fields which are indexed with INDEX_EXACT aren't allowed to be used in
        the default list of fields.)

        - `string`: The string to parse.
        - `allow`: A list of fields to allow in the query.
        - `deny`: A list of fields not to allow in the query.
        - `default_op`: The default operator to combine query terms with.
        - `default_allow`: A list of fields to search for by default.
        - `default_deny`: A list of fields not to search for by default.

        Only one of `allow` and `deny` may be specified.

        Only one of `default_allow` and `default_deny` may be specified.

        If any of the entries in `allow` are not present in the configuration
        for the database, or are not specified for indexing (either as
        INDEX_EXACT or INDEX_FREETEXT), they will be ignored.  If any of the
        entries in `deny` are not present in the configuration for the
        database, they will be ignored.

        Returns a Query object, which may be passed to the search() method, or
        combined with other queries.

        """
        qp = self._prepare_queryparser(allow, deny, default_op, default_allow,
                                       default_deny)
        return self._query_parse_with_fallback(qp, string)

    def query_field(self, field, value, default_op=OP_AND):
        """A query for a single field.

        """
        if self._index is None:
            raise _errors.SearchError("SearchConnection has been closed")
        try:
            actions = self._field_actions[field]._actions
        except KeyError:
            actions = {}

        # need to check on field type, and stem / split as appropriate
        for action, kwargslist in actions.iteritems():
            if action in (FieldActions.INDEX_EXACT,
                          FieldActions.TAG,
                          FieldActions.FACET,):
                prefix = self._field_mappings.get_prefix(field)
                if len(value) > 0:
                    chval = ord(value[0])
                    if chval >= ord('A') and chval <= ord('Z'):
                        prefix = prefix + ':'
                return _log(_xapian.Query, prefix + value)
            if action == FieldActions.INDEX_FREETEXT:
                qp = _log(_xapian.QueryParser)
                qp.set_default_op(default_op)
                prefix = self._field_mappings.get_prefix(field)
                for kwargs in kwargslist:
                    try:
                        lang = kwargs['language']
                        qp.set_stemmer(_log(_xapian.Stem, lang))
                        qp.set_stemming_strategy(qp.STEM_SOME)
                    except KeyError:
                        pass
                return self._query_parse_with_fallback(qp, value, prefix)

        return _log(_xapian.Query)

    def query_similar(self, ids, allow=None, deny=None, simterms=10):
        """Get a query which returns documents which are similar to others.

        The list of document IDs to base the similarity search on is given in
        `ids`.  This should be an iterable, holding a list of strings.  If
        any of the supplied IDs cannot be found in the database, they will be
        ignored.  (If no IDs can be found in the database, the resulting query
        will not match any documents.)

        By default, all fields which have been indexed for freetext searching
        will be used for the similarity calculation.  The list of fields used
        for this can be customised using the `allow` and `deny` parameters
        (only one of which may be specified):

        - `allow`: A list of fields to base the similarity calculation on.
        - `deny`: A list of fields not to base the similarity calculation on.
        - `simterms`: Number of terms to use for the similarity calculation.

        For convenience, any of `ids`, `allow`, or `deny` may be strings, which
        will be treated the same as a list of length 1.

        Regardless of the setting of `allow` and `deny`, only fields which have
        been indexed for freetext searching will be used for the similarity
        measure - all other fields will always be ignored for this purpose.

        """
        eterms, prefixes = self._get_eterms(ids, allow, deny, simterms)

        # Use the "elite set" operator, which chooses the terms with the
        # highest query weight to use.
        q = _log(_xapian.Query, _xapian.Query.OP_ELITE_SET, eterms, simterms)
        return q

    def significant_terms(self, ids, maxterms=10, allow=None, deny=None):
        """Get a set of "significant" terms for a document, or documents.

        This has a similar interface to query_similar(): it takes a list of
        ids, and an optional specification of a set of fields to consider.
        Instead of returning a query, it returns a list of terms from the
        document (or documents), which appear "significant".  Roughly,
        in this situation significant means that the terms occur more
        frequently in the specified document than in the rest of the corpus.

        The list is in decreasing order of "significance".

        By default, all terms related to fields which have been indexed for
        freetext searching will be considered for the list of significant
        terms.  The list of fields used for this can be customised using the
        `allow` and `deny` parameters (only one of which may be specified):

        - `allow`: A list of fields to consider.
        - `deny`: A list of fields not to consider.

        For convenience, any of `ids`, `allow`, or `deny` may be strings, which
        will be treated the same as a list of length 1.

        Regardless of the setting of `allow` and `deny`, only fields which have
        been indexed for freetext searching will be considered - all other
        fields will always be ignored for this purpose.

        The maximum number of terms to return may be specified by the maxterms
        parameter.

        """
        eterms, prefixes = self._get_eterms(ids, allow, deny, maxterms)
        terms = []
        for term in eterms:
            pos = 0
            for char in term:
                if not char.isupper():
                    break
                pos += 1
            field = prefixes[term[:pos]]
            value = term[pos:]
            terms.append((field, value))
        return terms

    def _get_eterms(self, ids, allow, deny, simterms):
        """Get a set of terms for an expand

        """
        if self._index is None:
            raise _errors.SearchError("SearchConnection has been closed")
        if allow is not None and deny is not None:
            raise _errors.SearchError("Cannot specify both `allow` and `deny`")

        if isinstance(ids, basestring):
            ids = (ids, )
        if isinstance(allow, basestring):
            allow = (allow, )
        if isinstance(deny, basestring):
            deny = (deny, )

        # Set "allow" to contain a list of all the fields to use.
        if allow is None:
            allow = [key for key in self._field_actions]
        if deny is not None:
            allow = [key for key in allow if key not in deny]

        # Set "prefixes" to contain a list of all the prefixes to use.
        prefixes = {}
        for field in allow:
            try:
                actions = self._field_actions[field]._actions
            except KeyError:
                actions = {}
            for action, kwargslist in actions.iteritems():
                if action == FieldActions.INDEX_FREETEXT:
                    prefixes[self._field_mappings.get_prefix(field)] = field

        # Repeat the expand until we don't get a DatabaseModifiedError
        while True:
            try:
                eterms = self._perform_expand(ids, prefixes, simterms)
                break;
            except _xapian.DatabaseModifiedError, e:
                self.reopen()
        return eterms, prefixes

    class ExpandDecider(_xapian.ExpandDecider):
        def __init__(self, prefixes):
            _xapian.ExpandDecider.__init__(self)
            self._prefixes = prefixes

        def __call__(self, term):
            pos = 0
            for char in term:
                if not char.isupper():
                    break
                pos += 1
            if term[:pos] in self._prefixes:
                return True
            return False

    def _perform_expand(self, ids, prefixes, simterms):
        """Perform an expand operation to get the terms for a similarity
        search, given a set of ids (and a set of prefixes to restrict the
        similarity operation to).

        """
        # Set idquery to be a query which returns the documents listed in
        # "ids".
        idquery = _log(_xapian.Query, _xapian.Query.OP_OR, ['Q' + id for id in ids])

        enq = _log(_xapian.Enquire, self._index)
        enq.set_query(idquery)
        rset = _log(_xapian.RSet)
        for id in ids:
            pl = self._index.postlist('Q' + id)
            try:
                xapid = pl.next()
                rset.add_document(xapid.docid)
            except StopIteration:
                pass

        expanddecider = _log(self.ExpandDecider, prefixes)
        eset = enq.get_eset(simterms, rset, 0, 1.0, expanddecider)
        return [term.term for term in eset]

    def query_all(self):
        """A query which matches all the documents in the database.

        """
        return _log(_xapian.Query, '')

    def query_none(self):
        """A query which matches no documents in the database.

        This may be useful as a placeholder in various situations.

        """
        return _log(_xapian.Query)

    def spell_correct(self, querystr, allow=None, deny=None, default_op=OP_AND,
                      default_allow=None, default_deny=None):
        """Correct a query spelling.

        This returns a version of the query string with any misspelt words
        corrected.

        - `allow`: A list of fields to allow in the query.
        - `deny`: A list of fields not to allow in the query.
        - `default_op`: The default operator to combine query terms with.
        - `default_allow`: A list of fields to search for by default.
        - `default_deny`: A list of fields not to search for by default.

        Only one of `allow` and `deny` may be specified.

        Only one of `default_allow` and `default_deny` may be specified.

        If any of the entries in `allow` are not present in the configuration
        for the database, or are not specified for indexing (either as
        INDEX_EXACT or INDEX_FREETEXT), they will be ignored.  If any of the
        entries in `deny` are not present in the configuration for the
        database, they will be ignored.

        Note that it is possible that the resulting spell-corrected query will
        still match no documents - the user should usually check that some
        documents are matched by the corrected query before suggesting it to
        users.

        """
        qp = self._prepare_queryparser(allow, deny, default_op, default_allow,
                                       default_deny)
        try:
            qp.parse_query(querystr,
                           self._qp_flags_base |
                           self._qp_flags_phrase |
                           self._qp_flags_synonym |
                           self._qp_flags_bool |
                           qp.FLAG_SPELLING_CORRECTION)
        except _xapian.QueryParserError:
            qp.parse_query(querystr,
                           self._qp_flags_base |
                           self._qp_flags_phrase |
                           self._qp_flags_synonym |
                           qp.FLAG_SPELLING_CORRECTION)
        corrected = qp.get_corrected_query_string()
        if len(corrected) == 0:
            if isinstance(querystr, unicode):
                # Encode as UTF-8 for consistency - this happens automatically
                # to values passed to Xapian.
                return querystr.encode('utf-8')
            return querystr
        return corrected

    def can_collapse_on(self, field):
        """Check if this database supports collapsing on a specified field.

        """
        if self._index is None:
            raise _errors.SearchError("SearchConnection has been closed")
        try:
            self._field_mappings.get_slot(field, 'collsort')
        except KeyError:
            return False
        return True

    def can_sort_on(self, field):
        """Check if this database supports sorting on a specified field.

        """
        if self._index is None:
            raise _errors.SearchError("SearchConnection has been closed")
        try:
            self._field_mappings.get_slot(field, 'collsort')
        except KeyError:
            return False
        return True
        
    def _get_prefix_from_term(self, term):
        """Get the prefix of a term.
   
        Prefixes are any initial capital letters, with the exception that R always
        ends a prefix, even if followed by capital letters.
        
        """
        for p in xrange(len(term)):
            if term[p].islower():
                return term[:p]
            elif term[p] == 'R':
                return term[:p+1]
        return term

    def _facet_query_never(self, facet, query_type):
        """Check if a facet must never be returned by a particular query type.

        Returns True if the facet must never be returned.

        Returns False if the facet may be returned - either becuase there is no
        entry for the query type, or because the entry is not
        FacetQueryType_Never.

        """
        if query_type is None:
            return False
        if query_type not in self._facet_query_table:
            return False
        if facet not in self._facet_query_table[query_type]:
            return False
        return self._facet_query_table[query_type][facet] == _indexerconnection.IndexerConnection.FacetQueryType_Never

    def search(self, query, startrank, endrank,
               checkatleast=0, sortby=None, collapse=None,
               gettags=None,
               getfacets=None, allowfacets=None, denyfacets=None, usesubfacets=None,
               percentcutoff=None, weightcutoff=None,
               query_type=None):
        """Perform a search, for documents matching a query.

        - `query` is the query to perform.
        - `startrank` is the rank of the start of the range of matching
          documents to return (ie, the result with this rank will be returned).
          ranks start at 0, which represents the "best" matching document.
        - `endrank` is the rank at the end of the range of matching documents
          to return.  This is exclusive, so the result with this rank will not
          be returned.
        - `checkatleast` is the minimum number of results to check for: the
          estimate of the total number of matches will always be exact if
          the number of matches is less than `checkatleast`.  A value of ``-1``
          can be specified for the checkatleast parameter - this has the
          special meaning of "check all matches", and is equivalent to passing
          the result of get_doccount().
        - `sortby` is the name of a field to sort by.  It may be preceded by a
          '+' or a '-' to indicate ascending or descending order
          (respectively).  If the first character is neither '+' or '-', the
          sort will be in ascending order.
        - `collapse` is the name of a field to collapse the result documents
          on.  If this is specified, there will be at most one result in the
          result set for each value of the field.
        - `gettags` is the name of a field to count tag occurrences in, or a
          list of fields to do so.
        - `getfacets` is a boolean - if True, the matching documents will be
          examined to build up a list of the facet values contained in them.
        - `allowfacets` is a list of the fieldnames of facets to consider.
        - `denyfacets` is a list of fieldnames of facets which will not be
          considered.
        - `usesubfacets` is a boolean - if True, only top-level facets and
          subfacets of facets appearing in the query are considered (taking
          precedence over `allowfacets` and `denyfacets`).
        - `percentcutoff` is the minimum percentage a result must have to be
          returned.
        - `weightcutoff` is the minimum weight a result must have to be
          returned.
        - `query_type` is a value indicating the type of query being
          performed. If not None, the value is used to influence which facets
          are be returned by the get_suggested_facets() function. If the
          value of `getfacets` is False, it has no effect.

        If neither 'allowfacets' or 'denyfacets' is specified, all fields
        holding facets will be considered (but see 'usesubfacets').

        """
        if self._index is None:
            raise _errors.SearchError("SearchConnection has been closed")
        if 'facets' in _checkxapian.missing_features:
            if getfacets is not None or \
               allowfacets is not None or \
               denyfacets is not None or \
               usesubfacets is not None or \
               query_type is not None:
                raise errors.SearchError("Facets unsupported with this release of xapian")
        if 'tags' in _checkxapian.missing_features:
            if gettags is not None:
                raise errors.SearchError("Tags unsupported with this release of xapian")
        if checkatleast == -1:
            checkatleast = self._index.get_doccount()

        enq = _log(_xapian.Enquire, self._index)
        enq.set_query(query)

        if sortby is not None:
            asc = True
            if sortby[0] == '-':
                asc = False
                sortby = sortby[1:]
            elif sortby[0] == '+':
                sortby = sortby[1:]

            try:
                slotnum = self._field_mappings.get_slot(sortby, 'collsort')
            except KeyError:
                raise _errors.SearchError("Field %r was not indexed for sorting" % sortby)

            # Note: we invert the "asc" parameter, because xapian treats
            # "ascending" as meaning "higher values are better"; in other
            # words, it considers "ascending" to mean return results in
            # descending order.
            enq.set_sort_by_value_then_relevance(slotnum, not asc)

        if collapse is not None:
            try:
                slotnum = self._field_mappings.get_slot(collapse, 'collsort')
            except KeyError:
                raise _errors.SearchError("Field %r was not indexed for collapsing" % collapse)
            enq.set_collapse_key(slotnum)

        maxitems = max(endrank - startrank, 0)
        # Always check for at least one more result, so we can report whether
        # there are more matches.
        checkatleast = max(checkatleast, endrank + 1)

        # Build the matchspy.
        matchspies = []

        # First, add a matchspy for any gettags fields
        if isinstance(gettags, basestring):
            if len(gettags) != 0:
                gettags = [gettags]
        tagspy = None
        if gettags is not None and len(gettags) != 0:
            tagspy = _log(_xapian.TermCountMatchSpy)
            for field in gettags:
                try:
                    prefix = self._field_mappings.get_prefix(field)
                    tagspy.add_prefix(prefix)
                except KeyError:
                    raise _errors.SearchError("Field %r was not indexed for tagging" % field)
            matchspies.append(tagspy)


        # add a matchspy for facet selection here.
        facetspy = None
        facetfields = []
        if getfacets:
            if allowfacets is not None and denyfacets is not None:
                raise _errors.SearchError("Cannot specify both `allowfacets` and `denyfacets`")
            if allowfacets is None:
                allowfacets = [key for key in self._field_actions]
            if denyfacets is not None:
                allowfacets = [key for key in allowfacets if key not in denyfacets]

            # include None in queryfacets so a top-level facet will
            # satisfy self._facet_hierarchy.get(field) in queryfacets
            # (i.e. always include top-level facets)
            queryfacets = set([None])
            if usesubfacets:
                # add facets used in the query to queryfacets
                termsiter = query.get_terms_begin()
                termsend = query.get_terms_end()
                while termsiter != termsend:
                    prefix = self._get_prefix_from_term(termsiter.get_term())
                    field = self._field_mappings.get_fieldname_from_prefix(prefix)
                    if field and FieldActions.FACET in self._field_actions[field]._actions:
                        queryfacets.add(field)
                    termsiter.next()

            for field in allowfacets:
                try:
                    actions = self._field_actions[field]._actions
                except KeyError:
                    actions = {}
                for action, kwargslist in actions.iteritems():
                    if action == FieldActions.FACET:
                        # filter out non-top-level facets that aren't subfacets
                        # of a facet in the query
                        if usesubfacets and self._facet_hierarchy.get(field) not in queryfacets:
                            continue
                        # filter out facets that should never be returned for the query type
                        if self._facet_query_never(field, query_type):
                            continue
                        slot = self._field_mappings.get_slot(field, 'facet')
                        if facetspy is None:
                            facetspy = _log(_xapian.CategorySelectMatchSpy)
                        facettype = None
                        for kwargs in kwargslist:
                            facettype = kwargs.get('type', None)
                            if facettype is not None:
                                break
                        if facettype is None or facettype == 'string':
                            facetspy.add_slot(slot, True)
                        else:
                            facetspy.add_slot(slot)
                        facetfields.append((field, slot, kwargslist))

            if facetspy is None:
                # Set facetspy to False, to distinguish from no facet
                # calculation being performed.  (This will prevent an
                # error being thrown when the list of suggested facets is
                # requested - instead, an empty list will be returned.)
                facetspy = False
            else:
                matchspies.append(facetspy)


        # Finally, build a single matchspy to pass to get_mset().
        if len(matchspies) == 0:
            matchspy = None
        elif len(matchspies) == 1:
            matchspy = matchspies[0]
        else:
            matchspy = _log(_xapian.MultipleMatchDecider)
            for spy in matchspies:
                matchspy.append(spy)

        enq.set_docid_order(enq.DONT_CARE)

        # Set percentage and weight cutoffs
        if percentcutoff is not None or weightcutoff is not None:
            if percentcutoff is None:
                percentcutoff = 0
            if weightcutoff is None:
                weightcutoff = 0
            enq.set_cutoff(percentcutoff, weightcutoff)

        # Repeat the search until we don't get a DatabaseModifiedError
        while True:
            try:
                if matchspy is None:
                    mset = enq.get_mset(startrank, maxitems, checkatleast)
                else:
                    mset = enq.get_mset(startrank, maxitems, checkatleast,
                                        None, None, matchspy)
                break
            except _xapian.DatabaseModifiedError, e:
                self.reopen()
        facet_hierarchy = None
        if usesubfacets:
            facet_hierarchy = self._facet_hierarchy
            
        return SearchResults(self, enq, query, mset, self._field_mappings,
                             tagspy, gettags, facetspy, facetfields,
                             facet_hierarchy,
                             self._facet_query_table.get(query_type))

    def iterids(self):
        """Get an iterator which returns all the ids in the database.

        The unqiue_ids are currently returned in binary lexicographical sort
        order, but this should not be relied on.

        Note that the iterator returned by this method may raise a
        xapian.DatabaseModifiedError exception if modifications are committed
        to the database while the iteration is in progress.  If this happens,
        the search connection must be reopened (by calling reopen) and the
        iteration restarted.

        """
        if self._index is None:
            raise _errors.SearchError("SearchConnection has been closed")
        return _indexerconnection.PrefixedTermIter('Q', self._index.allterms())

    def get_document(self, id):
        """Get the document with the specified unique ID.

        Raises a KeyError if there is no such document.  Otherwise, it returns
        a ProcessedDocument.

        """
        if self._index is None:
            raise _errors.SearchError("SearchConnection has been closed")
        while True:
            try:
                postlist = self._index.postlist('Q' + id)
                try:
                    plitem = postlist.next()
                except StopIteration:
                    # Unique ID not found
                    raise KeyError('Unique ID %r not found' % id)
                try:
                    postlist.next()
                    raise _errors.IndexerError("Multiple documents " #pragma: no cover
                                               "found with same unique ID")
                except StopIteration:
                    # Only one instance of the unique ID found, as it should be.
                    pass

                result = ProcessedDocument(self._field_mappings)
                result.id = id
                result._doc = self._index.get_document(plitem.docid)
                return result
            except _xapian.DatabaseModifiedError, e:
                self.reopen()

    def iter_synonyms(self, prefix=""):
        """Get an iterator over the synonyms.

         - `prefix`: if specified, only synonym keys with this prefix will be
           returned.

        The iterator returns 2-tuples, in which the first item is the key (ie,
        a 2-tuple holding the term or terms which will be synonym expanded,
        followed by the fieldname specified (or None if no fieldname)), and the
        second item is a tuple of strings holding the synonyms for the first
        item.

        These return values are suitable for the dict() builtin, so you can
        write things like:

         >>> conn = _indexerconnection.IndexerConnection('foo')
         >>> conn.add_synonym('foo', 'bar')
         >>> conn.add_synonym('foo bar', 'baz')
         >>> conn.add_synonym('foo bar', 'foo baz')
         >>> conn.flush()
         >>> conn = SearchConnection('foo')
         >>> dict(conn.iter_synonyms())
         {('foo', None): ('bar',), ('foo bar', None): ('baz', 'foo baz')}

        """
        if self._index is None:
            raise _errors.SearchError("SearchConnection has been closed")
        return _indexerconnection.SynonymIter(self._index, self._field_mappings, prefix)

    def get_metadata(self, key):
        """Get an item of metadata stored in the connection.

        This returns a value stored by a previous call to
        IndexerConnection.set_metadata.

        If the value is not found, this will return the empty string.

        """
        if self._index is None:
            raise _errors.IndexerError("SearchConnection has been closed")
        if not hasattr(self._index, 'get_metadata'):
            raise _errors.IndexerError("Version of xapian in use does not support metadata")
        return _log(self._index.get_metadata, key)

if __name__ == '__main__':
    import doctest, sys
    doctest.testmod (sys.modules[__name__])

########NEW FILE########
__FILENAME__ = _checkxapian
# Copyright (C) 2008 Lemur Consulting Ltd
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
r"""_checkxapian.py: Check the version of xapian used.

Raises an ImportError on import if the version used is too old to be used at
all.

"""
__docformat__ = "restructuredtext en"

# The minimum version of xapian required to work at all.
min_xapian_version = (1, 0, 6)

# Dictionary of features we can't support do to them being missing from the
# available version of xapian.
missing_features = {}

import xapian

versions = xapian.major_version(), xapian.minor_version(), xapian.revision()


if versions < min_xapian_version:
    raise ImportError("""
        Xapian Python bindings installed, but need at least version %d.%d.%d - got %s
        """.strip() % tuple(list(min_xapian_version) + [xapian.version_string()]))

if not hasattr(xapian, 'TermCountMatchSpy'):
    missing_features['tags'] = 1
if not hasattr(xapian, 'CategorySelectMatchSpy'):
    missing_features['facets'] = 1

########NEW FILE########
__FILENAME__ = xmltramp
"""xmltramp: Make XML documents easily accessible."""

__version__ = "2.18"
__author__ = "Aaron Swartz"
__credits__ = "Many thanks to pjz, bitsko, and DanC."
__copyright__ = "(C) 2003-2006 Aaron Swartz. GNU GPL 2."

if not hasattr(__builtins__, 'True'): True, False = 1, 0
def isstr(f): return isinstance(f, type('')) or isinstance(f, type(u''))
def islst(f): return isinstance(f, type(())) or isinstance(f, type([]))

empty = {'http://www.w3.org/1999/xhtml': ['img', 'br', 'hr', 'meta', 'link', 'base', 'param', 'input', 'col', 'area']}

def quote(x, elt=True):
	if elt and '<' in x and len(x) > 24 and x.find(']]>') == -1: return "<![CDATA["+x+"]]>"
	else: x = x.replace('&', '&amp;').replace('<', '&lt;').replace(']]>', ']]&gt;')
	if not elt: x = x.replace('"', '&quot;')
	return x

class Element:
	def __init__(self, name, attrs=None, children=None, prefixes=None):
		if islst(name) and name[0] == None: name = name[1]
		if attrs:
			na = {}
			for k in attrs.keys():
				if islst(k) and k[0] == None: na[k[1]] = attrs[k]
				else: na[k] = attrs[k]
			attrs = na
		
		self._name = name
		self._attrs = attrs or {}
		self._dir = children or []
		
		prefixes = prefixes or {}
		self._prefixes = dict(zip(prefixes.values(), prefixes.keys()))
		
		if prefixes: self._dNS = prefixes.get(None, None)
		else: self._dNS = None
	
	def __repr__(self, recursive=0, multiline=0, inprefixes=None):
		def qname(name, inprefixes): 
			if islst(name):
				if inprefixes[name[0]] is not None:
					return inprefixes[name[0]]+':'+name[1]
				else:
					return name[1]
			else:
				return name
		
		def arep(a, inprefixes, addns=1):
			out = ''

			for p in self._prefixes.keys():
				if not p in inprefixes.keys():
					if addns: out += ' xmlns'
					if addns and self._prefixes[p]: out += ':'+self._prefixes[p]
					if addns: out += '="'+quote(p, False)+'"'
					inprefixes[p] = self._prefixes[p]
			
			for k in a.keys():
				out += ' ' + qname(k, inprefixes)+ '="' + quote(a[k], False) + '"'
			
			return out
		
		inprefixes = inprefixes or {u'http://www.w3.org/XML/1998/namespace':'xml'}
		
		# need to call first to set inprefixes:
		attributes = arep(self._attrs, inprefixes, recursive) 
		out = '<' + qname(self._name, inprefixes)  + attributes 
		
		if not self._dir and (self._name[0] in empty.keys() 
		  and self._name[1] in empty[self._name[0]]):
			out += ' />'
			return out
		
		out += '>'

		if recursive:
			content = 0
			for x in self._dir: 
				if isinstance(x, Element): content = 1
				
			pad = '\n' + ('\t' * recursive)
			for x in self._dir:
				if multiline and content: out +=  pad 
				if isstr(x): out += quote(x)
				elif isinstance(x, Element):
					out += x.__repr__(recursive+1, multiline, inprefixes.copy())
				else:
					raise TypeError, "I wasn't expecting "+`x`+"."
			if multiline and content: out += '\n' + ('\t' * (recursive-1))
		else:
			if self._dir: out += '...'
		
		out += '</'+qname(self._name, inprefixes)+'>'
			
		return out
	
	def __unicode__(self):
		text = ''
		for x in self._dir:
			text += unicode(x)
		return ' '.join(text.split())
		
	def __str__(self):
		return self.__unicode__().encode('utf-8')
	
	def __getattr__(self, n):
		if n[0] == '_': raise AttributeError, "Use foo['"+n+"'] to access the child element."
		if self._dNS: n = (self._dNS, n)
		for x in self._dir:
			if isinstance(x, Element) and x._name == n: return x
		raise AttributeError, 'No child element named %s' % repr(n)
		
	def __hasattr__(self, n):
		for x in self._dir:
			if isinstance(x, Element) and x._name == n: return True
		return False
		
 	def __setattr__(self, n, v):
		if n[0] == '_': self.__dict__[n] = v
		else: self[n] = v
 

	def __getitem__(self, n):
		if isinstance(n, type(0)): # d[1] == d._dir[1]
			return self._dir[n]
		elif isinstance(n, slice(0).__class__):
			# numerical slices
			if isinstance(n.start, type(0)): return self._dir[n.start:n.stop]
			
			# d['foo':] == all <foo>s
			n = n.start
			if self._dNS and not islst(n): n = (self._dNS, n)
			out = []
			for x in self._dir:
				if isinstance(x, Element) and x._name == n: out.append(x) 
			return out
		else: # d['foo'] == first <foo>
			if self._dNS and not islst(n): n = (self._dNS, n)
			for x in self._dir:
				if isinstance(x, Element) and x._name == n: return x
			raise KeyError, n
	
	def __setitem__(self, n, v):
		if isinstance(n, type(0)): # d[1]
			self._dir[n] = v
		elif isinstance(n, slice(0).__class__):
			# d['foo':] adds a new foo
			n = n.start
			if self._dNS and not islst(n): n = (self._dNS, n)

			nv = Element(n)
			self._dir.append(nv)
			
		else: # d["foo"] replaces first <foo> and dels rest
			if self._dNS and not islst(n): n = (self._dNS, n)

			nv = Element(n); nv._dir.append(v)
			replaced = False

			todel = []
			for i in range(len(self)):
				if self[i]._name == n:
					if replaced:
						todel.append(i)
					else:
						self[i] = nv
						replaced = True
			if not replaced: self._dir.append(nv)
			for i in todel: del self[i]

	def __delitem__(self, n):
		if isinstance(n, type(0)): del self._dir[n]
		elif isinstance(n, slice(0).__class__):
			# delete all <foo>s
			n = n.start
			if self._dNS and not islst(n): n = (self._dNS, n)
			
			for i in range(len(self)):
				if self[i]._name == n: del self[i]
		else:
			# delete first foo
			for i in range(len(self)):
				if self[i]._name == n: del self[i]
				break
	
	def __call__(self, *_pos, **_set): 
		if _set:
			for k in _set.keys(): self._attrs[k] = _set[k]
		if len(_pos) > 1:
			for i in range(0, len(_pos), 2):
				self._attrs[_pos[i]] = _pos[i+1]
		if len(_pos) == 1:
			return self._attrs[_pos[0]]
		if len(_pos) == 0:
			return self._attrs

	def __len__(self): return len(self._dir)

class Namespace:
	def __init__(self, uri): self.__uri = uri
	def __getattr__(self, n): return (self.__uri, n)
	def __getitem__(self, n): return (self.__uri, n)

from xml.sax.handler import EntityResolver, DTDHandler, ContentHandler, ErrorHandler

class Seeder(EntityResolver, DTDHandler, ContentHandler, ErrorHandler):
	def __init__(self):
		self.stack = []
		self.ch = ''
		self.prefixes = {}
		ContentHandler.__init__(self)
		
	def startPrefixMapping(self, prefix, uri):
		if not self.prefixes.has_key(prefix): self.prefixes[prefix] = []
		self.prefixes[prefix].append(uri)
	def endPrefixMapping(self, prefix):
		self.prefixes[prefix].pop()
	
	def startElementNS(self, name, qname, attrs):
		ch = self.ch; self.ch = ''	
		if ch and not ch.isspace(): self.stack[-1]._dir.append(ch)

		attrs = dict(attrs)
		newprefixes = {}
		for k in self.prefixes.keys(): newprefixes[k] = self.prefixes[k][-1]
		
		self.stack.append(Element(name, attrs, prefixes=newprefixes.copy()))
	
	def characters(self, ch):
		self.ch += ch
	
	def endElementNS(self, name, qname):
		ch = self.ch; self.ch = ''
		if ch and not ch.isspace(): self.stack[-1]._dir.append(ch)
	
		element = self.stack.pop()
		if self.stack:
			self.stack[-1]._dir.append(element)
		else:
			self.result = element

from xml.sax import make_parser
from xml.sax.handler import feature_namespaces

def seed(fileobj):
	seeder = Seeder()
	parser = make_parser()
	parser.setFeature(feature_namespaces, 1)
	parser.setContentHandler(seeder)
	parser.parse(fileobj)
	return seeder.result

def parse(text):
	from StringIO import StringIO
	return seed(StringIO(text))

def load(url): 
	import urllib
	return seed(urllib.urlopen(url))

def unittest():
	parse('<doc>a<baz>f<b>o</b>ob<b>a</b>r</baz>a</doc>').__repr__(1,1) == \
	  '<doc>\n\ta<baz>\n\t\tf<b>o</b>ob<b>a</b>r\n\t</baz>a\n</doc>'
	
	assert str(parse("<doc />")) == ""
	assert str(parse("<doc>I <b>love</b> you.</doc>")) == "I love you."
	assert parse("<doc>\nmom\nwow\n</doc>")[0].strip() == "mom\nwow"
	assert str(parse('<bing>  <bang> <bong>center</bong> </bang>  </bing>')) == "center"
	assert str(parse('<doc>\xcf\x80</doc>')) == '\xcf\x80'
	
	d = Element('foo', attrs={'foo':'bar'}, children=['hit with a', Element('bar'), Element('bar')])
	
	try: 
		d._doesnotexist
		raise "ExpectedError", "but found success. Damn."
	except AttributeError: pass
	assert d.bar._name == 'bar'
	try:
		d.doesnotexist
		raise "ExpectedError", "but found success. Damn."
	except AttributeError: pass
	
	assert hasattr(d, 'bar') == True
	
	assert d('foo') == 'bar'
	d(silly='yes')
	assert d('silly') == 'yes'
	assert d() == d._attrs
	
	assert d[0] == 'hit with a'
	d[0] = 'ice cream'
	assert d[0] == 'ice cream'
	del d[0]
	assert d[0]._name == "bar"
	assert len(d[:]) == len(d._dir)
	assert len(d[1:]) == len(d._dir) - 1
	assert len(d['bar':]) == 2
	d['bar':] = 'baz'
	assert len(d['bar':]) == 3
	assert d['bar']._name == 'bar'
	
	d = Element('foo')
	
	doc = Namespace("http://example.org/bar")
	bbc = Namespace("http://example.org/bbc")
	dc = Namespace("http://purl.org/dc/elements/1.1/")
	d = parse("""<doc version="2.7182818284590451"
	  xmlns="http://example.org/bar" 
	  xmlns:dc="http://purl.org/dc/elements/1.1/"
	  xmlns:bbc="http://example.org/bbc">
		<author>John Polk and John Palfrey</author>
		<dc:creator>John Polk</dc:creator>
		<dc:creator>John Palfrey</dc:creator>
		<bbc:show bbc:station="4">Buffy</bbc:show>
	</doc>""")

	assert repr(d) == '<doc version="2.7182818284590451">...</doc>'
	assert d.__repr__(1) == '<doc xmlns:bbc="http://example.org/bbc" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns="http://example.org/bar" version="2.7182818284590451"><author>John Polk and John Palfrey</author><dc:creator>John Polk</dc:creator><dc:creator>John Palfrey</dc:creator><bbc:show bbc:station="4">Buffy</bbc:show></doc>'
	assert d.__repr__(1,1) == '<doc xmlns:bbc="http://example.org/bbc" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns="http://example.org/bar" version="2.7182818284590451">\n\t<author>John Polk and John Palfrey</author>\n\t<dc:creator>John Polk</dc:creator>\n\t<dc:creator>John Palfrey</dc:creator>\n\t<bbc:show bbc:station="4">Buffy</bbc:show>\n</doc>'

	assert repr(parse("<doc xml:lang='en' />")) == '<doc xml:lang="en"></doc>'

	assert str(d.author) == str(d['author']) == "John Polk and John Palfrey"
	assert d.author._name == doc.author
	assert str(d[dc.creator]) == "John Polk"
	assert d[dc.creator]._name == dc.creator
	assert str(d[dc.creator:][1]) == "John Palfrey"
	d[dc.creator] = "Me!!!"
	assert str(d[dc.creator]) == "Me!!!"
	assert len(d[dc.creator:]) == 1
	d[dc.creator:] = "You!!!"
	assert len(d[dc.creator:]) == 2
	
	assert d[bbc.show](bbc.station) == "4"
	d[bbc.show](bbc.station, "5")
	assert d[bbc.show](bbc.station) == "5"

	e = Element('e')
	e.c = '<img src="foo">'
	assert e.__repr__(1) == '<e><c>&lt;img src="foo"></c></e>'
	e.c = '2 > 4'
	assert e.__repr__(1) == '<e><c>2 > 4</c></e>'
	e.c = 'CDATA sections are <em>closed</em> with ]]>.'
	assert e.__repr__(1) == '<e><c>CDATA sections are &lt;em>closed&lt;/em> with ]]&gt;.</c></e>'
	e.c = parse('<div xmlns="http://www.w3.org/1999/xhtml">i<br /><span></span>love<br />you</div>')
	assert e.__repr__(1) == '<e><c><div xmlns="http://www.w3.org/1999/xhtml">i<br /><span></span>love<br />you</div></c></e>'	
	
	e = Element('e')
	e('c', 'that "sucks"')
	assert e.__repr__(1) == '<e c="that &quot;sucks&quot;"></e>'

	
	assert quote("]]>") == "]]&gt;"
	assert quote('< dkdkdsd dkd sksdksdfsd fsdfdsf]]> kfdfkg >') == '&lt; dkdkdsd dkd sksdksdfsd fsdfdsf]]&gt; kfdfkg >'
	
	assert parse('<x a="&lt;"></x>').__repr__(1) == '<x a="&lt;"></x>'
	assert parse('<a xmlns="http://a"><b xmlns="http://b"/></a>').__repr__(1) == '<a xmlns="http://a"><b xmlns="http://b"></b></a>'
	
if __name__ == '__main__': unittest()

########NEW FILE########
__FILENAME__ = zipfile
"""
Read and write ZIP files.
"""
import struct, os, time, sys, shutil
import binascii, cStringIO

try:
    import zlib # We may need its compression method
    crc32 = zlib.crc32
except ImportError:
    zlib = None
    crc32 = binascii.crc32

__all__ = ["BadZipfile", "error", "ZIP_STORED", "ZIP_DEFLATED", "is_zipfile",
           "ZipInfo", "ZipFile", "PyZipFile", "LargeZipFile" ]

class BadZipfile(Exception):
    pass


class LargeZipFile(Exception):
    """
    Raised when writing a zipfile, the zipfile requires ZIP64 extensions
    and those extensions are disabled.
    """

error = BadZipfile      # The exception raised by this module

ZIP64_LIMIT= (1 << 31) - 1
ZIP_FILECOUNT_LIMIT = 1 << 16
ZIP_MAX_COMMENT = (1 << 16) - 1

# constants for Zip file compression methods
ZIP_STORED = 0
ZIP_DEFLATED = 8
# Other ZIP compression methods not supported

# Below are some formats and associated data for reading/writing headers using
# the struct module.  The names and structures of headers/records are those used
# in the PKWARE description of the ZIP file format:
#     http://www.pkware.com/documents/casestudies/APPNOTE.TXT
# (URL valid as of January 2008)

# The "end of central directory" structure, magic number, size, and indices
# (section V.I in the format document)
structEndArchive = "<4s4H2LH"
stringEndArchive = "PK\005\006"
sizeEndCentDir = struct.calcsize(structEndArchive)

_ECD_SIGNATURE = 0
_ECD_DISK_NUMBER = 1
_ECD_DISK_START = 2
_ECD_ENTRIES_THIS_DISK = 3
_ECD_ENTRIES_TOTAL = 4
_ECD_SIZE = 5
_ECD_OFFSET = 6
_ECD_COMMENT_SIZE = 7
# These last two indices are not part of the structure as defined in the
# spec, but they are used internally by this module as a convenience
_ECD_COMMENT = 8
_ECD_LOCATION = 9

# The "central directory" structure, magic number, size, and indices
# of entries in the structure (section V.F in the format document)
structCentralDir = "<4s4B4HL2L5H2L"
stringCentralDir = "PK\001\002"
sizeCentralDir = struct.calcsize(structCentralDir)

# indexes of entries in the central directory structure
_CD_SIGNATURE = 0
_CD_CREATE_VERSION = 1
_CD_CREATE_SYSTEM = 2
_CD_EXTRACT_VERSION = 3
_CD_EXTRACT_SYSTEM = 4
_CD_FLAG_BITS = 5
_CD_COMPRESS_TYPE = 6
_CD_TIME = 7
_CD_DATE = 8
_CD_CRC = 9
_CD_COMPRESSED_SIZE = 10
_CD_UNCOMPRESSED_SIZE = 11
_CD_FILENAME_LENGTH = 12
_CD_EXTRA_FIELD_LENGTH = 13
_CD_COMMENT_LENGTH = 14
_CD_DISK_NUMBER_START = 15
_CD_INTERNAL_FILE_ATTRIBUTES = 16
_CD_EXTERNAL_FILE_ATTRIBUTES = 17
_CD_LOCAL_HEADER_OFFSET = 18

# The "local file header" structure, magic number, size, and indices
# (section V.A in the format document)
structFileHeader = "<4s2B4HL2L2H"
stringFileHeader = "PK\003\004"
sizeFileHeader = struct.calcsize(structFileHeader)

_FH_SIGNATURE = 0
_FH_EXTRACT_VERSION = 1
_FH_EXTRACT_SYSTEM = 2
_FH_GENERAL_PURPOSE_FLAG_BITS = 3
_FH_COMPRESSION_METHOD = 4
_FH_LAST_MOD_TIME = 5
_FH_LAST_MOD_DATE = 6
_FH_CRC = 7
_FH_COMPRESSED_SIZE = 8
_FH_UNCOMPRESSED_SIZE = 9
_FH_FILENAME_LENGTH = 10
_FH_EXTRA_FIELD_LENGTH = 11

# The "Zip64 end of central directory locator" structure, magic number, and size
structEndArchive64Locator = "<4sLQL"
stringEndArchive64Locator = "PK\x06\x07"
sizeEndCentDir64Locator = struct.calcsize(structEndArchive64Locator)

# The "Zip64 end of central directory" record, magic number, size, and indices
# (section V.G in the format document)
structEndArchive64 = "<4sQ2H2L4Q"
stringEndArchive64 = "PK\x06\x06"
sizeEndCentDir64 = struct.calcsize(structEndArchive64)

_CD64_SIGNATURE = 0
_CD64_DIRECTORY_RECSIZE = 1
_CD64_CREATE_VERSION = 2
_CD64_EXTRACT_VERSION = 3
_CD64_DISK_NUMBER = 4
_CD64_DISK_NUMBER_START = 5
_CD64_NUMBER_ENTRIES_THIS_DISK = 6
_CD64_NUMBER_ENTRIES_TOTAL = 7
_CD64_DIRECTORY_SIZE = 8
_CD64_OFFSET_START_CENTDIR = 9

def is_zipfile(filename):
    """Quickly see if file is a ZIP file by checking the magic number."""
    try:
        fpin = open(filename, "rb")
        endrec = _EndRecData(fpin)
        fpin.close()
        if endrec:
            return True                 # file has correct magic number
    except IOError:
        pass
    return False

def _EndRecData64(fpin, offset, endrec):
    """
    Read the ZIP64 end-of-archive records and use that to update endrec
    """
    fpin.seek(offset - sizeEndCentDir64Locator, 2)
    data = fpin.read(sizeEndCentDir64Locator)
    sig, diskno, reloff, disks = struct.unpack(structEndArchive64Locator, data)
    if sig != stringEndArchive64Locator:
        return endrec

    if diskno != 0 or disks != 1:
        raise BadZipfile("zipfiles that span multiple disks are not supported")

    # Assume no 'zip64 extensible data'
    fpin.seek(offset - sizeEndCentDir64Locator - sizeEndCentDir64, 2)
    data = fpin.read(sizeEndCentDir64)
    sig, sz, create_version, read_version, disk_num, disk_dir, \
            dircount, dircount2, dirsize, diroffset = \
            struct.unpack(structEndArchive64, data)
    if sig != stringEndArchive64:
        return endrec

    # Update the original endrec using data from the ZIP64 record
    endrec[_ECD_SIGNATURE] = sig
    endrec[_ECD_DISK_NUMBER] = disk_num
    endrec[_ECD_DISK_START] = disk_dir
    endrec[_ECD_ENTRIES_THIS_DISK] = dircount
    endrec[_ECD_ENTRIES_TOTAL] = dircount2
    endrec[_ECD_SIZE] = dirsize
    endrec[_ECD_OFFSET] = diroffset
    return endrec


def _EndRecData(fpin):
    """Return data from the "End of Central Directory" record, or None.

    The data is a list of the nine items in the ZIP "End of central dir"
    record followed by a tenth item, the file seek offset of this record."""

    # Determine file size
    fpin.seek(0, 2)
    filesize = fpin.tell()

    # Check to see if this is ZIP file with no archive comment (the
    # "end of central directory" structure should be the last item in the
    # file if this is the case).
    fpin.seek(-sizeEndCentDir, 2)
    data = fpin.read()
    if data[0:4] == stringEndArchive and data[-2:] == "\000\000":
        # the signature is correct and there's no comment, unpack structure
        endrec = struct.unpack(structEndArchive, data)
        endrec=list(endrec)

        # Append a blank comment and record start offset
        endrec.append("")
        endrec.append(filesize - sizeEndCentDir)
        if endrec[_ECD_OFFSET] == 0xffffffff:
            # the value for the "offset of the start of the central directory"
            # indicates that there is a "Zip64 end of central directory"
            # structure present, so go look for it
            return _EndRecData64(fpin, -sizeEndCentDir, endrec)

        return endrec

    # Either this is not a ZIP file, or it is a ZIP file with an archive
    # comment.  Search the end of the file for the "end of central directory"
    # record signature. The comment is the last item in the ZIP file and may be
    # up to 64K long.  It is assumed that the "end of central directory" magic
    # number does not appear in the comment.
    maxCommentStart = max(filesize - (1 << 16) - sizeEndCentDir, 0)
    fpin.seek(maxCommentStart, 0)
    data = fpin.read()
    start = data.rfind(stringEndArchive)
    if start >= 0:
        # found the magic number; attempt to unpack and interpret
        recData = data[start:start+sizeEndCentDir]
        endrec = list(struct.unpack(structEndArchive, recData))
        comment = data[start+sizeEndCentDir:]
        # check that comment length is correct
        if endrec[_ECD_COMMENT_SIZE] == len(comment):
            # Append the archive comment and start offset
            endrec.append(comment)
            endrec.append(maxCommentStart + start)
            if endrec[_ECD_OFFSET] == 0xffffffff:
                # There is apparently a "Zip64 end of central directory"
                # structure present, so go look for it
                return _EndRecData64(fpin, start - filesize, endrec)
            return endrec

    # Unable to find a valid end of central directory structure
    return


class ZipInfo (object):
    """Class with attributes describing each file in the ZIP archive."""

    __slots__ = (
            'orig_filename',
            'filename',
            'date_time',
            'compress_type',
            'comment',
            'extra',
            'create_system',
            'create_version',
            'extract_version',
            'reserved',
            'flag_bits',
            'volume',
            'internal_attr',
            'external_attr',
            'header_offset',
            'CRC',
            'compress_size',
            'file_size',
            '_raw_time',
        )

    def __init__(self, filename="NoName", date_time=(1980,1,1,0,0,0)):
        self.orig_filename = filename   # Original file name in archive

        # Terminate the file name at the first null byte.  Null bytes in file
        # names are used as tricks by viruses in archives.
        null_byte = filename.find(chr(0))
        if null_byte >= 0:
            filename = filename[0:null_byte]
        # This is used to ensure paths in generated ZIP files always use
        # forward slashes as the directory separator, as required by the
        # ZIP format specification.
        if os.sep != "/" and os.sep in filename:
            filename = filename.replace(os.sep, "/")

        self.filename = filename        # Normalized file name
        self.date_time = date_time      # year, month, day, hour, min, sec
        # Standard values:
        self.compress_type = ZIP_STORED # Type of compression for the file
        self.comment = ""               # Comment for each file
        self.extra = ""                 # ZIP extra data
        if sys.platform == 'win32':
            self.create_system = 0          # System which created ZIP archive
        else:
            # Assume everything else is unix-y
            self.create_system = 3          # System which created ZIP archive
        self.create_version = 20        # Version which created ZIP archive
        self.extract_version = 20       # Version needed to extract archive
        self.reserved = 0               # Must be zero
        self.flag_bits = 0              # ZIP flag bits
        self.volume = 0                 # Volume number of file header
        self.internal_attr = 0          # Internal attributes
        self.external_attr = 0          # External file attributes
        # Other attributes are set by class ZipFile:
        # header_offset         Byte offset to the file header
        # CRC                   CRC-32 of the uncompressed file
        # compress_size         Size of the compressed file
        # file_size             Size of the uncompressed file

    def FileHeader(self):
        """Return the per-file header as a string."""
        dt = self.date_time
        dosdate = (dt[0] - 1980) << 9 | dt[1] << 5 | dt[2]
        dostime = dt[3] << 11 | dt[4] << 5 | (dt[5] // 2)
        if self.flag_bits & 0x08:
            # Set these to zero because we write them after the file data
            CRC = compress_size = file_size = 0
        else:
            CRC = self.CRC
            compress_size = self.compress_size
            file_size = self.file_size

        extra = self.extra

        if file_size > ZIP64_LIMIT or compress_size > ZIP64_LIMIT:
            # File is larger than what fits into a 4 byte integer,
            # fall back to the ZIP64 extension
            fmt = '<HHQQ'
            extra = extra + struct.pack(fmt,
                    1, struct.calcsize(fmt)-4, file_size, compress_size)
            file_size = 0xffffffff
            compress_size = 0xffffffff
            self.extract_version = max(45, self.extract_version)
            self.create_version = max(45, self.extract_version)

        filename, flag_bits = self._encodeFilenameFlags()
        header = struct.pack(structFileHeader, stringFileHeader,
                 self.extract_version, self.reserved, flag_bits,
                 self.compress_type, dostime, dosdate, CRC,
                 compress_size, file_size,
                 len(filename), len(extra))
        return header + filename + extra

    def _encodeFilenameFlags(self):
        if isinstance(self.filename, unicode):
            try:
                return self.filename.encode('ascii'), self.flag_bits
            except UnicodeEncodeError:
                return self.filename.encode('utf-8'), self.flag_bits | 0x800
        else:
            return self.filename, self.flag_bits

    def _decodeFilename(self):
        if self.flag_bits & 0x800:
            return self.filename.decode('utf-8')
        else:
            return self.filename

    def _decodeExtra(self):
        # Try to decode the extra field.
        extra = self.extra
        unpack = struct.unpack
        while extra:
            tp, ln = unpack('<HH', extra[:4])
            if tp == 1:
                if ln >= 24:
                    counts = unpack('<QQQ', extra[4:28])
                elif ln == 16:
                    counts = unpack('<QQ', extra[4:20])
                elif ln == 8:
                    counts = unpack('<Q', extra[4:12])
                elif ln == 0:
                    counts = ()
                else:
                    raise RuntimeError, "Corrupt extra field %s"%(ln,)

                idx = 0

                # ZIP64 extension (large files and/or large archives)
                if self.file_size in (0xffffffffffffffffL, 0xffffffffL):
                    self.file_size = counts[idx]
                    idx += 1

                if self.compress_size == 0xFFFFFFFFL:
                    self.compress_size = counts[idx]
                    idx += 1

                if self.header_offset == 0xffffffffL:
                    old = self.header_offset
                    self.header_offset = counts[idx]
                    idx+=1

            extra = extra[ln+4:]


class _ZipDecrypter:
    """Class to handle decryption of files stored within a ZIP archive.

    ZIP supports a password-based form of encryption. Even though known
    plaintext attacks have been found against it, it is still useful
    to be able to get data out of such a file.

    Usage:
        zd = _ZipDecrypter(mypwd)
        plain_char = zd(cypher_char)
        plain_text = map(zd, cypher_text)
    """

    def _GenerateCRCTable():
        """Generate a CRC-32 table.

        ZIP encryption uses the CRC32 one-byte primitive for scrambling some
        internal keys. We noticed that a direct implementation is faster than
        relying on binascii.crc32().
        """
        poly = 0xedb88320
        table = [0] * 256
        for i in range(256):
            crc = i
            for j in range(8):
                if crc & 1:
                    crc = ((crc >> 1) & 0x7FFFFFFF) ^ poly
                else:
                    crc = ((crc >> 1) & 0x7FFFFFFF)
            table[i] = crc
        return table
    crctable = _GenerateCRCTable()

    def _crc32(self, ch, crc):
        """Compute the CRC32 primitive on one byte."""
        return ((crc >> 8) & 0xffffff) ^ self.crctable[(crc ^ ord(ch)) & 0xff]

    def __init__(self, pwd):
        self.key0 = 305419896
        self.key1 = 591751049
        self.key2 = 878082192
        for p in pwd:
            self._UpdateKeys(p)

    def _UpdateKeys(self, c):
        self.key0 = self._crc32(c, self.key0)
        self.key1 = (self.key1 + (self.key0 & 255)) & 4294967295
        self.key1 = (self.key1 * 134775813 + 1) & 4294967295
        self.key2 = self._crc32(chr((self.key1 >> 24) & 255), self.key2)

    def __call__(self, c):
        """Decrypt a single character."""
        c = ord(c)
        k = self.key2 | 2
        c = c ^ (((k * (k^1)) >> 8) & 255)
        c = chr(c)
        self._UpdateKeys(c)
        return c

class ZipExtFile:
    """File-like object for reading an archive member.
       Is returned by ZipFile.open().
    """

    def __init__(self, fileobj, zipinfo, decrypt=None):
        self.fileobj = fileobj
        self.decrypter = decrypt
        self.bytes_read = 0L
        self.rawbuffer = ''
        self.readbuffer = ''
        self.linebuffer = ''
        self.eof = False
        self.univ_newlines = False
        self.nlSeps = ("\n", )
        self.lastdiscard = ''

        self.compress_type = zipinfo.compress_type
        self.compress_size = zipinfo.compress_size

        self.closed  = False
        self.mode    = "r"
        self.name = zipinfo.filename

        # read from compressed files in 64k blocks
        self.compreadsize = 64*1024
        if self.compress_type == ZIP_DEFLATED:
            self.dc = zlib.decompressobj(-15)

    def set_univ_newlines(self, univ_newlines):
        self.univ_newlines = univ_newlines

        # pick line separator char(s) based on universal newlines flag
        self.nlSeps = ("\n", )
        if self.univ_newlines:
            self.nlSeps = ("\r\n", "\r", "\n")

    def __iter__(self):
        return self

    def next(self):
        nextline = self.readline()
        if not nextline:
            raise StopIteration()

        return nextline

    def close(self):
        self.closed = True

    def _checkfornewline(self):
        nl, nllen = -1, -1
        if self.linebuffer:
            # ugly check for cases where half of an \r\n pair was
            # read on the last pass, and the \r was discarded.  In this
            # case we just throw away the \n at the start of the buffer.
            if (self.lastdiscard, self.linebuffer[0]) == ('\r','\n'):
                self.linebuffer = self.linebuffer[1:]

            for sep in self.nlSeps:
                nl = self.linebuffer.find(sep)
                if nl >= 0:
                    nllen = len(sep)
                    return nl, nllen

        return nl, nllen

    def readline(self, size = -1):
        """Read a line with approx. size. If size is negative,
           read a whole line.
        """
        if size < 0:
            size = sys.maxint
        elif size == 0:
            return ''

        # check for a newline already in buffer
        nl, nllen = self._checkfornewline()

        if nl >= 0:
            # the next line was already in the buffer
            nl = min(nl, size)
        else:
            # no line break in buffer - try to read more
            size -= len(self.linebuffer)
            while nl < 0 and size > 0:
                buf = self.read(min(size, 100))
                if not buf:
                    break
                self.linebuffer += buf
                size -= len(buf)

                # check for a newline in buffer
                nl, nllen = self._checkfornewline()

            # we either ran out of bytes in the file, or
            # met the specified size limit without finding a newline,
            # so return current buffer
            if nl < 0:
                s = self.linebuffer
                self.linebuffer = ''
                return s

        buf = self.linebuffer[:nl]
        self.lastdiscard = self.linebuffer[nl:nl + nllen]
        self.linebuffer = self.linebuffer[nl + nllen:]

        # line is always returned with \n as newline char (except possibly
        # for a final incomplete line in the file, which is handled above).
        return buf + "\n"

    def readlines(self, sizehint = -1):
        """Return a list with all (following) lines. The sizehint parameter
        is ignored in this implementation.
        """
        result = []
        while True:
            line = self.readline()
            if not line: break
            result.append(line)
        return result

    def read(self, size = None):
        # act like file() obj and return empty string if size is 0
        if size == 0:
            return ''

        # determine read size
        bytesToRead = self.compress_size - self.bytes_read

        # adjust read size for encrypted files since the first 12 bytes
        # are for the encryption/password information
        if self.decrypter is not None:
            bytesToRead -= 12

        if size is not None and size >= 0:
            if self.compress_type == ZIP_STORED:
                lr = len(self.readbuffer)
                bytesToRead = min(bytesToRead, size - lr)
            elif self.compress_type == ZIP_DEFLATED:
                if len(self.readbuffer) > size:
                    # the user has requested fewer bytes than we've already
                    # pulled through the decompressor; don't read any more
                    bytesToRead = 0
                else:
                    # user will use up the buffer, so read some more
                    lr = len(self.rawbuffer)
                    bytesToRead = min(bytesToRead, self.compreadsize - lr)

        # avoid reading past end of file contents
        if bytesToRead + self.bytes_read > self.compress_size:
            bytesToRead = self.compress_size - self.bytes_read

        # try to read from file (if necessary)
        if bytesToRead > 0:
            bytes = self.fileobj.read(bytesToRead)
            self.bytes_read += len(bytes)
            self.rawbuffer += bytes

            # handle contents of raw buffer
            if self.rawbuffer:
                newdata = self.rawbuffer
                self.rawbuffer = ''

                # decrypt new data if we were given an object to handle that
                if newdata and self.decrypter is not None:
                    newdata = ''.join(map(self.decrypter, newdata))

                # decompress newly read data if necessary
                if newdata and self.compress_type == ZIP_DEFLATED:
                    newdata = self.dc.decompress(newdata)
                    self.rawbuffer = self.dc.unconsumed_tail
                    if self.eof and len(self.rawbuffer) == 0:
                        # we're out of raw bytes (both from the file and
                        # the local buffer); flush just to make sure the
                        # decompressor is done
                        newdata += self.dc.flush()
                        # prevent decompressor from being used again
                        self.dc = None

                self.readbuffer += newdata


        # return what the user asked for
        if size is None or len(self.readbuffer) <= size:
            bytes = self.readbuffer
            self.readbuffer = ''
        else:
            bytes = self.readbuffer[:size]
            self.readbuffer = self.readbuffer[size:]

        return bytes


class ZipFile:
    """ Class with methods to open, read, write, close, list zip files.

    z = ZipFile(file, mode="r", compression=ZIP_STORED, allowZip64=False)

    file: Either the path to the file, or a file-like object.
          If it is a path, the file will be opened and closed by ZipFile.
    mode: The mode can be either read "r", write "w" or append "a".
    compression: ZIP_STORED (no compression) or ZIP_DEFLATED (requires zlib).
    allowZip64: if True ZipFile will create files with ZIP64 extensions when
                needed, otherwise it will raise an exception when this would
                be necessary.

    """

    fp = None                   # Set here since __del__ checks it

    def __init__(self, file, mode="r", compression=ZIP_STORED, allowZip64=False):
        """Open the ZIP file with mode read "r", write "w" or append "a"."""
        if mode not in ("r", "w", "a"):
            raise RuntimeError('ZipFile() requires mode "r", "w", or "a"')

        if compression == ZIP_STORED:
            pass
        elif compression == ZIP_DEFLATED:
            if not zlib:
                raise RuntimeError,\
                      "Compression requires the (missing) zlib module"
        else:
            raise RuntimeError, "That compression method is not supported"

        self._allowZip64 = allowZip64
        self._didModify = False
        self.debug = 0  # Level of printing: 0 through 3
        self.NameToInfo = {}    # Find file info given name
        self.filelist = []      # List of ZipInfo instances for archive
        self.compression = compression  # Method of compression
        self.mode = key = mode.replace('b', '')[0]
        self.pwd = None
        self.comment = ''

        # Check if we were passed a file-like object
        if isinstance(file, basestring):
            self._filePassed = 0
            self.filename = file
            modeDict = {'r' : 'rb', 'w': 'wb', 'a' : 'r+b'}
            try:
                self.fp = open(file, modeDict[mode])
            except IOError:
                if mode == 'a':
                    mode = key = 'w'
                    self.fp = open(file, modeDict[mode])
                else:
                    raise
        else:
            self._filePassed = 1
            self.fp = file
            self.filename = getattr(file, 'name', None)

        if key == 'r':
            self._GetContents()
        elif key == 'w':
            pass
        elif key == 'a':
            try:                        # See if file is a zip file
                self._RealGetContents()
                # seek to start of directory and overwrite
                self.fp.seek(self.start_dir, 0)
            except BadZipfile:          # file is not a zip file, just append
                self.fp.seek(0, 2)
        else:
            if not self._filePassed:
                self.fp.close()
                self.fp = None
            raise RuntimeError, 'Mode must be "r", "w" or "a"'

    def _GetContents(self):
        """Read the directory, making sure we close the file if the format
        is bad."""
        try:
            self._RealGetContents()
        except BadZipfile:
            if not self._filePassed:
                self.fp.close()
                self.fp = None
            raise

    def _RealGetContents(self):
        """Read in the table of contents for the ZIP file."""
        fp = self.fp
        endrec = _EndRecData(fp)
        if not endrec:
            raise BadZipfile, "File is not a zip file"
        if self.debug > 1:
            print endrec
        size_cd = endrec[_ECD_SIZE]             # bytes in central directory
        offset_cd = endrec[_ECD_OFFSET]         # offset of central directory
        self.comment = endrec[_ECD_COMMENT]     # archive comment

        # "concat" is zero, unless zip was concatenated to another file
        concat = endrec[_ECD_LOCATION] - size_cd - offset_cd
        if endrec[_ECD_SIGNATURE] == stringEndArchive64:
            # If Zip64 extension structures are present, account for them
            concat -= (sizeEndCentDir64 + sizeEndCentDir64Locator)

        if self.debug > 2:
            inferred = concat + offset_cd
            print "given, inferred, offset", offset_cd, inferred, concat
        # self.start_dir:  Position of start of central directory
        self.start_dir = offset_cd + concat
        fp.seek(self.start_dir, 0)
        data = fp.read(size_cd)
        fp = cStringIO.StringIO(data)
        total = 0
        while total < size_cd:
            centdir = fp.read(sizeCentralDir)
            if centdir[0:4] != stringCentralDir:
                raise BadZipfile, "Bad magic number for central directory"
            centdir = struct.unpack(structCentralDir, centdir)
            if self.debug > 2:
                print centdir
            filename = fp.read(centdir[_CD_FILENAME_LENGTH])
            # Create ZipInfo instance to store file information
            x = ZipInfo(filename)
            x.extra = fp.read(centdir[_CD_EXTRA_FIELD_LENGTH])
            x.comment = fp.read(centdir[_CD_COMMENT_LENGTH])
            x.header_offset = centdir[_CD_LOCAL_HEADER_OFFSET]
            (x.create_version, x.create_system, x.extract_version, x.reserved,
                x.flag_bits, x.compress_type, t, d,
                x.CRC, x.compress_size, x.file_size) = centdir[1:12]
            x.volume, x.internal_attr, x.external_attr = centdir[15:18]
            # Convert date/time code to (year, month, day, hour, min, sec)
            x._raw_time = t
            x.date_time = ( (d>>9)+1980, (d>>5)&0xF, d&0x1F,
                                     t>>11, (t>>5)&0x3F, (t&0x1F) * 2 )

            x._decodeExtra()
            x.header_offset = x.header_offset + concat
            x.filename = x._decodeFilename()
            self.filelist.append(x)
            self.NameToInfo[x.filename] = x

            # update total bytes read from central directory
            total = (total + sizeCentralDir + centdir[_CD_FILENAME_LENGTH]
                     + centdir[_CD_EXTRA_FIELD_LENGTH]
                     + centdir[_CD_COMMENT_LENGTH])

            if self.debug > 2:
                print "total", total


    def namelist(self):
        """Return a list of file names in the archive."""
        l = []
        for data in self.filelist:
            l.append(data.filename)
        return l

    def infolist(self):
        """Return a list of class ZipInfo instances for files in the
        archive."""
        return self.filelist

    def printdir(self):
        """Print a table of contents for the zip file."""
        print "%-46s %19s %12s" % ("File Name", "Modified    ", "Size")
        for zinfo in self.filelist:
            date = "%d-%02d-%02d %02d:%02d:%02d" % zinfo.date_time[:6]
            print "%-46s %s %12d" % (zinfo.filename, date, zinfo.file_size)

    def testzip(self):
        """Read all the files and check the CRC."""
        chunk_size = 2 ** 20
        for zinfo in self.filelist:
            try:
                # Read by chunks, to avoid an OverflowError or a
                # MemoryError with very large embedded files.
                f = self.open(zinfo.filename, "r")
                while f.read(chunk_size):     # Check CRC-32
                    pass
            except BadZipfile:
                return zinfo.filename

    def getinfo(self, name):
        """Return the instance of ZipInfo given 'name'."""
        info = self.NameToInfo.get(name)
        if info is None:
            raise KeyError(
                'There is no item named %r in the archive' % name)

        return info

    def setpassword(self, pwd):
        """Set default password for encrypted files."""
        self.pwd = pwd

    def read(self, name, pwd=None):
        """Return file bytes (as a string) for name."""
        return self.open(name, "r", pwd).read()

    def open(self, name, mode="r", pwd=None):
        """Return file-like object for 'name'."""
        if mode not in ("r", "U", "rU"):
            raise RuntimeError, 'open() requires mode "r", "U", or "rU"'
        if not self.fp:
            raise RuntimeError, \
                  "Attempt to read ZIP archive that was already closed"

        # Only open a new file for instances where we were not
        # given a file object in the constructor
        if self._filePassed:
            zef_file = self.fp
        else:
            zef_file = open(self.filename, 'rb')

        # Make sure we have an info object
        if isinstance(name, ZipInfo):
            # 'name' is already an info object
            zinfo = name
        else:
            # Get info object for name
            zinfo = self.getinfo(name)

        zef_file.seek(zinfo.header_offset, 0)

        # Skip the file header:
        fheader = zef_file.read(sizeFileHeader)
        if fheader[0:4] != stringFileHeader:
            raise BadZipfile, "Bad magic number for file header"

        fheader = struct.unpack(structFileHeader, fheader)
        fname = zef_file.read(fheader[_FH_FILENAME_LENGTH])
        if fheader[_FH_EXTRA_FIELD_LENGTH]:
            zef_file.read(fheader[_FH_EXTRA_FIELD_LENGTH])

        if fname != zinfo.orig_filename:
            raise BadZipfile, \
                      'File name in directory "%s" and header "%s" differ.' % (
                          zinfo.orig_filename, fname)

        # check for encrypted flag & handle password
        is_encrypted = zinfo.flag_bits & 0x1
        zd = None
        if is_encrypted:
            if not pwd:
                pwd = self.pwd
            if not pwd:
                raise RuntimeError, "File %s is encrypted, " \
                      "password required for extraction" % name

            zd = _ZipDecrypter(pwd)
            # The first 12 bytes in the cypher stream is an encryption header
            #  used to strengthen the algorithm. The first 11 bytes are
            #  completely random, while the 12th contains the MSB of the CRC,
            #  or the MSB of the file time depending on the header type
            #  and is used to check the correctness of the password.
            bytes = zef_file.read(12)
            h = map(zd, bytes[0:12])
            if zinfo.flag_bits & 0x8:
                # compare against the file type from extended local headers
                check_byte = (zinfo._raw_time >> 8) & 0xff
            else:
                # compare against the CRC otherwise
                check_byte = (zinfo.CRC >> 24) & 0xff
            if ord(h[11]) != check_byte:
                raise RuntimeError("Bad password for file", name)

        # build and return a ZipExtFile
        if zd is None:
            zef = ZipExtFile(zef_file, zinfo)
        else:
            zef = ZipExtFile(zef_file, zinfo, zd)

        # set universal newlines on ZipExtFile if necessary
        if "U" in mode:
            zef.set_univ_newlines(True)
        return zef

    def extract(self, member, path=None, pwd=None):
        """Extract a member from the archive to the current working directory,
           using its full name. Its file information is extracted as accurately
           as possible. `member' may be a filename or a ZipInfo object. You can
           specify a different directory using `path'.
        """
        if not isinstance(member, ZipInfo):
            member = self.getinfo(member)

        if path is None:
            path = os.getcwd()

        return self._extract_member(member, path, pwd)

    def extractall(self, path=None, members=None, pwd=None):
        """Extract all members from the archive to the current working
           directory. `path' specifies a different directory to extract to.
           `members' is optional and must be a subset of the list returned
           by namelist().
        """
        if members is None:
            members = self.namelist()

        for zipinfo in members:
            self.extract(zipinfo, path, pwd)

    def _extract_member(self, member, targetpath, pwd):
        """Extract the ZipInfo object 'member' to a physical
           file on the path targetpath.
        """
        # build the destination pathname, replacing
        # forward slashes to platform specific separators.
        if targetpath[-1:] == "/":
            targetpath = targetpath[:-1]

        # don't include leading "/" from file name if present
        if os.path.isabs(member.filename):
            targetpath = os.path.join(targetpath, member.filename[1:])
        else:
            targetpath = os.path.join(targetpath, member.filename)

        targetpath = os.path.normpath(targetpath)

        # Create all upper directories if necessary.
        upperdirs = os.path.dirname(targetpath)
        if upperdirs and not os.path.exists(upperdirs):
            os.makedirs(upperdirs)

        source = self.open(member, pwd=pwd)
        target = file(targetpath, "wb")
        shutil.copyfileobj(source, target)
        source.close()
        target.close()

        return targetpath

    def _writecheck(self, zinfo):
        """Check for errors before writing a file to the archive."""
        if zinfo.filename in self.NameToInfo:
            if self.debug:      # Warning for duplicate names
                print "Duplicate name:", zinfo.filename
        if self.mode not in ("w", "a"):
            raise RuntimeError, 'write() requires mode "w" or "a"'
        if not self.fp:
            raise RuntimeError, \
                  "Attempt to write ZIP archive that was already closed"
        if zinfo.compress_type == ZIP_DEFLATED and not zlib:
            raise RuntimeError, \
                  "Compression requires the (missing) zlib module"
        if zinfo.compress_type not in (ZIP_STORED, ZIP_DEFLATED):
            raise RuntimeError, \
                  "That compression method is not supported"
        if zinfo.file_size > ZIP64_LIMIT:
            if not self._allowZip64:
                raise LargeZipFile("Filesize would require ZIP64 extensions")
        if zinfo.header_offset > ZIP64_LIMIT:
            if not self._allowZip64:
                raise LargeZipFile("Zipfile size would require ZIP64 extensions")

    def write(self, filename, arcname=None, compress_type=None):
        """Put the bytes from filename into the archive under the name
        arcname."""
        if not self.fp:
            raise RuntimeError(
                  "Attempt to write to ZIP archive that was already closed")

        st = os.stat(filename)
        mtime = time.localtime(st.st_mtime)
        date_time = mtime[0:6]
        # Create ZipInfo instance to store file information
        if arcname is None:
            arcname = filename
        arcname = os.path.normpath(os.path.splitdrive(arcname)[1])
        while arcname[0] in (os.sep, os.altsep):
            arcname = arcname[1:]
        zinfo = ZipInfo(arcname, date_time)
        zinfo.external_attr = (st[0] & 0xFFFF) << 16L      # Unix attributes
        if compress_type is None:
            zinfo.compress_type = self.compression
        else:
            zinfo.compress_type = compress_type

        zinfo.file_size = st.st_size
        zinfo.flag_bits = 0x00
        zinfo.header_offset = self.fp.tell()    # Start of header bytes

        self._writecheck(zinfo)
        self._didModify = True
        fp = open(filename, "rb")
        # Must overwrite CRC and sizes with correct data later
        zinfo.CRC = CRC = 0
        zinfo.compress_size = compress_size = 0
        zinfo.file_size = file_size = 0
        self.fp.write(zinfo.FileHeader())
        if zinfo.compress_type == ZIP_DEFLATED:
            cmpr = zlib.compressobj(zlib.Z_DEFAULT_COMPRESSION,
                 zlib.DEFLATED, -15)
        else:
            cmpr = None
        while 1:
            buf = fp.read(1024 * 8)
            if not buf:
                break
            file_size = file_size + len(buf)
            CRC = crc32(buf, CRC) & 0xffffffff
            if cmpr:
                buf = cmpr.compress(buf)
                compress_size = compress_size + len(buf)
            self.fp.write(buf)
        fp.close()
        if cmpr:
            buf = cmpr.flush()
            compress_size = compress_size + len(buf)
            self.fp.write(buf)
            zinfo.compress_size = compress_size
        else:
            zinfo.compress_size = file_size
        zinfo.CRC = CRC
        zinfo.file_size = file_size
        # Seek backwards and write CRC and file sizes
        position = self.fp.tell()       # Preserve current position in file
        self.fp.seek(zinfo.header_offset + 14, 0)
        self.fp.write(struct.pack("<LLL", zinfo.CRC, zinfo.compress_size,
              zinfo.file_size))
        self.fp.seek(position, 0)
        self.filelist.append(zinfo)
        self.NameToInfo[zinfo.filename] = zinfo

    def writestr(self, zinfo_or_arcname, bytes):
        """Write a file into the archive.  The contents is the string
        'bytes'.  'zinfo_or_arcname' is either a ZipInfo instance or
        the name of the file in the archive."""
        if not isinstance(zinfo_or_arcname, ZipInfo):
            zinfo = ZipInfo(filename=zinfo_or_arcname,
                            date_time=time.localtime(time.time())[:6])
            zinfo.compress_type = self.compression
            zinfo.external_attr = 0600 << 16
        else:
            zinfo = zinfo_or_arcname

        if not self.fp:
            raise RuntimeError(
                  "Attempt to write to ZIP archive that was already closed")

        zinfo.file_size = len(bytes)            # Uncompressed size
        zinfo.header_offset = self.fp.tell()    # Start of header bytes
        self._writecheck(zinfo)
        self._didModify = True
        zinfo.CRC = crc32(bytes) & 0xffffffff       # CRC-32 checksum
        if zinfo.compress_type == ZIP_DEFLATED:
            co = zlib.compressobj(zlib.Z_DEFAULT_COMPRESSION,
                 zlib.DEFLATED, -15)
            bytes = co.compress(bytes) + co.flush()
            zinfo.compress_size = len(bytes)    # Compressed size
        else:
            zinfo.compress_size = zinfo.file_size
        zinfo.header_offset = self.fp.tell()    # Start of header bytes
        self.fp.write(zinfo.FileHeader())
        self.fp.write(bytes)
        self.fp.flush()
        if zinfo.flag_bits & 0x08:
            # Write CRC and file sizes after the file data
            self.fp.write(struct.pack("<lLL", zinfo.CRC, zinfo.compress_size,
                  zinfo.file_size))
        self.filelist.append(zinfo)
        self.NameToInfo[zinfo.filename] = zinfo

    def __del__(self):
        """Call the "close()" method in case the user forgot."""
        self.close()

    def close(self):
        """Close the file, and for mode "w" and "a" write the ending
        records."""
        if self.fp is None:
            return

        if self.mode in ("w", "a") and self._didModify: # write ending records
            count = 0
            pos1 = self.fp.tell()
            for zinfo in self.filelist:         # write central directory
                count = count + 1
                dt = zinfo.date_time
                dosdate = (dt[0] - 1980) << 9 | dt[1] << 5 | dt[2]
                dostime = dt[3] << 11 | dt[4] << 5 | (dt[5] // 2)
                extra = []
                if zinfo.file_size > ZIP64_LIMIT \
                        or zinfo.compress_size > ZIP64_LIMIT:
                    extra.append(zinfo.file_size)
                    extra.append(zinfo.compress_size)
                    file_size = 0xffffffff
                    compress_size = 0xffffffff
                else:
                    file_size = zinfo.file_size
                    compress_size = zinfo.compress_size

                if zinfo.header_offset > ZIP64_LIMIT:
                    extra.append(zinfo.header_offset)
                    header_offset = 0xffffffffL
                else:
                    header_offset = zinfo.header_offset

                extra_data = zinfo.extra
                if extra:
                    # Append a ZIP64 field to the extra's
                    extra_data = struct.pack(
                            '<HH' + 'Q'*len(extra),
                            1, 8*len(extra), *extra) + extra_data

                    extract_version = max(45, zinfo.extract_version)
                    create_version = max(45, zinfo.create_version)
                else:
                    extract_version = zinfo.extract_version
                    create_version = zinfo.create_version

                try:
                    filename, flag_bits = zinfo._encodeFilenameFlags()
                    centdir = struct.pack(structCentralDir,
                     stringCentralDir, create_version,
                     zinfo.create_system, extract_version, zinfo.reserved,
                     flag_bits, zinfo.compress_type, dostime, dosdate,
                     zinfo.CRC, compress_size, file_size,
                     len(filename), len(extra_data), len(zinfo.comment),
                     0, zinfo.internal_attr, zinfo.external_attr,
                     header_offset)
                except DeprecationWarning:
                    print >>sys.stderr, (structCentralDir,
                     stringCentralDir, create_version,
                     zinfo.create_system, extract_version, zinfo.reserved,
                     zinfo.flag_bits, zinfo.compress_type, dostime, dosdate,
                     zinfo.CRC, compress_size, file_size,
                     len(zinfo.filename), len(extra_data), len(zinfo.comment),
                     0, zinfo.internal_attr, zinfo.external_attr,
                     header_offset)
                    raise
                self.fp.write(centdir)
                self.fp.write(filename)
                self.fp.write(extra_data)
                self.fp.write(zinfo.comment)

            pos2 = self.fp.tell()
            # Write end-of-zip-archive record
            centDirOffset = pos1
            if pos1 > ZIP64_LIMIT:
                # Need to write the ZIP64 end-of-archive records
                zip64endrec = struct.pack(
                        structEndArchive64, stringEndArchive64,
                        44, 45, 45, 0, 0, count, count, pos2 - pos1, pos1)
                self.fp.write(zip64endrec)

                zip64locrec = struct.pack(
                        structEndArchive64Locator,
                        stringEndArchive64Locator, 0, pos2, 1)
                self.fp.write(zip64locrec)
                centDirOffset = 0xFFFFFFFF

            # check for valid comment length
            if len(self.comment) >= ZIP_MAX_COMMENT:
                if self.debug > 0:
                    msg = 'Archive comment is too long; truncating to %d bytes' \
                          % ZIP_MAX_COMMENT
                self.comment = self.comment[:ZIP_MAX_COMMENT]

            endrec = struct.pack(structEndArchive, stringEndArchive,
                                 0, 0, count % ZIP_FILECOUNT_LIMIT,
                                 count % ZIP_FILECOUNT_LIMIT, pos2 - pos1,
                                 centDirOffset, len(self.comment))
            self.fp.write(endrec)
            self.fp.write(self.comment)
            self.fp.flush()

        if not self._filePassed:
            self.fp.close()
        self.fp = None


class PyZipFile(ZipFile):
    """Class to create ZIP archives with Python library files and packages."""

    def writepy(self, pathname, basename = ""):
        """Add all files from "pathname" to the ZIP archive.

        If pathname is a package directory, search the directory and
        all package subdirectories recursively for all *.py and enter
        the modules into the archive.  If pathname is a plain
        directory, listdir *.py and enter all modules.  Else, pathname
        must be a Python *.py file and the module will be put into the
        archive.  Added modules are always module.pyo or module.pyc.
        This method will compile the module.py into module.pyc if
        necessary.
        """
        dir, name = os.path.split(pathname)
        if os.path.isdir(pathname):
            initname = os.path.join(pathname, "__init__.py")
            if os.path.isfile(initname):
                # This is a package directory, add it
                if basename:
                    basename = "%s/%s" % (basename, name)
                else:
                    basename = name
                if self.debug:
                    print "Adding package in", pathname, "as", basename
                fname, arcname = self._get_codename(initname[0:-3], basename)
                if self.debug:
                    print "Adding", arcname
                self.write(fname, arcname)
                dirlist = os.listdir(pathname)
                dirlist.remove("__init__.py")
                # Add all *.py files and package subdirectories
                for filename in dirlist:
                    path = os.path.join(pathname, filename)
                    root, ext = os.path.splitext(filename)
                    if os.path.isdir(path):
                        if os.path.isfile(os.path.join(path, "__init__.py")):
                            # This is a package directory, add it
                            self.writepy(path, basename)  # Recursive call
                    elif ext == ".py":
                        fname, arcname = self._get_codename(path[0:-3],
                                         basename)
                        if self.debug:
                            print "Adding", arcname
                        self.write(fname, arcname)
            else:
                # This is NOT a package directory, add its files at top level
                if self.debug:
                    print "Adding files from directory", pathname
                for filename in os.listdir(pathname):
                    path = os.path.join(pathname, filename)
                    root, ext = os.path.splitext(filename)
                    if ext == ".py":
                        fname, arcname = self._get_codename(path[0:-3],
                                         basename)
                        if self.debug:
                            print "Adding", arcname
                        self.write(fname, arcname)
        else:
            if pathname[-3:] != ".py":
                raise RuntimeError, \
                      'Files added with writepy() must end with ".py"'
            fname, arcname = self._get_codename(pathname[0:-3], basename)
            if self.debug:
                print "Adding file", arcname
            self.write(fname, arcname)

    def _get_codename(self, pathname, basename):
        """Return (filename, archivename) for the path.

        Given a module name path, return the correct file path and
        archive name, compiling if necessary.  For example, given
        /python/lib/string, return (/python/lib/string.pyc, string).
        """
        file_py  = pathname + ".py"
        file_pyc = pathname + ".pyc"
        file_pyo = pathname + ".pyo"
        if os.path.isfile(file_pyo) and \
                            os.stat(file_pyo).st_mtime >= os.stat(file_py).st_mtime:
            fname = file_pyo    # Use .pyo file
        elif not os.path.isfile(file_pyc) or \
             os.stat(file_pyc).st_mtime < os.stat(file_py).st_mtime:
            import py_compile
            if self.debug:
                print "Compiling", file_py
            try:
                py_compile.compile(file_py, file_pyc, None, True)
            except py_compile.PyCompileError,err:
                print err.msg
            fname = file_pyc
        else:
            fname = file_pyc
        archivename = os.path.split(fname)[1]
        if basename:
            archivename = "%s/%s" % (basename, archivename)
        return (fname, archivename)


def main(args = None):
    import textwrap
    USAGE=textwrap.dedent("""\
        Usage:
            zipfile.py -l zipfile.zip        # Show listing of a zipfile
            zipfile.py -t zipfile.zip        # Test if a zipfile is valid
            zipfile.py -e zipfile.zip target # Extract zipfile into target dir
            zipfile.py -c zipfile.zip src ... # Create zipfile from sources
        """)
    if args is None:
        args = sys.argv[1:]

    if not args or args[0] not in ('-l', '-c', '-e', '-t'):
        print USAGE
        sys.exit(1)

    if args[0] == '-l':
        if len(args) != 2:
            print USAGE
            sys.exit(1)
        zf = ZipFile(args[1], 'r')
        zf.printdir()
        zf.close()

    elif args[0] == '-t':
        if len(args) != 2:
            print USAGE
            sys.exit(1)
        zf = ZipFile(args[1], 'r')
        zf.testzip()
        print "Done testing"

    elif args[0] == '-e':
        if len(args) != 3:
            print USAGE
            sys.exit(1)

        zf = ZipFile(args[1], 'r')
        out = args[2]
        for path in zf.namelist():
            if path.startswith('./'):
                tgt = os.path.join(out, path[2:])
            else:
                tgt = os.path.join(out, path)

            tgtdir = os.path.dirname(tgt)
            if not os.path.exists(tgtdir):
                os.makedirs(tgtdir)
            fp = open(tgt, 'wb')
            fp.write(zf.read(path))
            fp.close()
        zf.close()

    elif args[0] == '-c':
        if len(args) < 3:
            print USAGE
            sys.exit(1)

        def addToZip(zf, path, zippath):
            if os.path.isfile(path):
                zf.write(path, zippath, ZIP_DEFLATED)
            elif os.path.isdir(path):
                for nm in os.listdir(path):
                    addToZip(zf,
                            os.path.join(path, nm), os.path.join(zippath, nm))
            # else: ignore

        zf = ZipFile(args[1], 'w', allowZip64=True)
        for src in args[2:]:
            addToZip(zf, src, os.path.basename(src))

        zf.close()

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = webapp
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import re, sys, urllib
import web

from utils import zip2rep, simplegraphs, apipublish, users, writerep, se, wyrapp, api, helpers
import blog
import petition
import settings
from settings import db, render, production_mode
import schema
import config
import os, simplejson, cPickle as pickle

if not production_mode:
	web.config.debug = True
	db.printing = True

options = r'(?:\.(html|xml|rdf|n3|json))'
urls = (
  r'/', 'index',
  r'/us/(?:index%s)?' % options, 'find',
  r'/us/([A-Z][A-Z])', 'redistrict',
  r'/us/([a-z][a-z])%s?' % options, 'state',
  r'/us/([A-Z][A-Z]-\d+)', 'redistrict',
  r'/us/([a-z][a-z]-\d+)%s?' % options, 'district',
  r'/(us|p)/by/(.*)/distribution\.png', 'sparkdist',
  r'/(us|p)/by/(.*)', 'dproperty',
  r'/p/(.*?)/lobby', 'politician_lobby',
  r'/p/(.*?)/earmarks', 'politician_earmarks',
  r'/p/(.*?)/introduced', 'politician_introduced',
  r'/p/(.*?)/groups', 'politician_groups',
  r'/p/(.*?)/contribs', 'politician_contribs',
  r'/p/(.*?)/contrib-employers', 'politician_contrib_employers',
  r'/p/(.*?)/(\d+)', 'politician_group',
  r'/p/(.*?)%s?' % options, 'politician',
  r'/e/(.*?)%s?' % options, 'earmark',
  r'/b/(.*?)%s?' % options, 'bill',
  #r'/h/', 'handshakes',
  r'/contrib/(distribution\.png|)', 'contributions',
  r'/contrib/(\d+)/(.*?)' , 'contributor',
  r'/occupation/(.*?)/candidates' , 'occupation_candidates',
  r'/occupation/(.*?)/committees' , 'occupation_committees',
  r'/occupation/(.*?)' , 'occupation',
  r'/empl/(.*?)%s?' % options, 'employer',
  r'/r/us/(.*?)%s?' % options, 'roll',
  r'/lob/c/?(.*?)', 'lob_contrib',
  r'/lob/f/?(.*?)', 'lob_filing',
  r'/lob/o/?(.*?)', 'lob_org',
  r'/lob/pa/?(.*?)', 'lob_pac',
  r'/lob/pe/?(.*?)', 'lob_person',
  r'/ein/(\d+)(/.*)?', 'ein',
  r'/writerep', wyrapp.app,
  r'/api', api.app,
  r'/about(/?)', 'about',
  r'/about/team', 'aboutteam',
  r'/about/help', 'abouthelp',
  r'/about/api', 'aboutapi',
  r'/about/feedback', 'feedback',
  r'/thanks', 'email_thanks',
  r'/contribute(/?)', 'contribute',
  r'/blog', blog.app,
  r'/share', 'petition.share',
  r'/bbauth/', 'contacts.auth_yahoo',
  r'/authsub', 'contacts.auth_google',
  r'/auth/msn', 'contacts.auth_msn',
  r'/code/(.*)', 'code',
  r'/c', petition.app,
  r'/u', users.app,
  r'/static/(.*)', 'static',
  r'/data/(.*)', 'static',
  r'/robots.txt', 'robotstxt'
)

class robotstxt:
    def GET(self): return file('static/robots.txt').read()

class code:
    def GET(self, x): raise web.seeother('https://github.com/aaronsw/watchdog')

class static:
    def GET(self, p):
        raise web.seeother('http://static.watchdog.net/' + p)

class index:
    def GET(self):
        return render.index()

class about:
    def GET(self, endslash=None):
        if not endslash: raise web.seeother('/about/')
        return render.about()

class aboutapi:
    def GET(self):
        return render.about_api()

class aboutteam:
    def GET(self):
        return render.about_team()

class abouthelp:
    def GET(self):
        return render.about_help()

class contribute:
    def GET(self, endslash=None):
        if not endslash: raise web.seeother('/contribute/')
        return render.contribute()

class feedback:
    def GET(self):
        return render.feedback()

    #def POST(self):
    #    i = web.input(email='info@watchdog.net')
    #    web.sendmail('Feedback <%s>' % i.email, 'Watchdog <info@watchdog.net>',
    #      'watchdog.net feedback',
    #      i.content +'\n\n' + web.ctx.ip)
    #
    #    return render.feedback_thanks()

class email_thanks:
    def GET(self):
        i = web.input(url='/')
        return render.email_thanks(i.url)

class find:
    def GET(self, format=None):
        i = web.input(address=None)
        pzip5 = re.compile(r'\d{5}')
        pzip4 = re.compile(r'\d{5}-\d{4}')
        pdist = re.compile(r'[a-zA-Z]{2}\-\d{2}')
        
        dists = None
        if not i.get('q'):
            i.q = i.get('zip')

        if i.q:
            if pzip4.match(i.q):
                zip, plus4 = i.q.split('-')
                dists = [x.district_id for x in
                  db.select('zip4', where='zip=$zip and plus4=$plus4', vars=locals())]
            
            elif pzip5.match(i.q):
                try:
                    dists = zip2rep.zip2dist(i.q, i.address)
                except zip2rep.BadAddress:
                    return render.find_badaddr(i.q, i.address)
            
            if dists:
                d_dists = list(schema.District.select(where=web.sqlors('name=', dists)))
                out = apipublish.publish(d_dists, format)
                if out: return out

                if len(dists) == 1:
                    raise web.seeother('/us/%s' % dists[0].lower())
                elif len(dists) == 0:
                    return render.find_none(i.q)
                else:
                    return render.find_multi(i.q, d_dists)

            if pdist.match(i.q):
                raise web.seeother('/us/%s' % i.q)
            
            results = se.query(i.q)
            reps = schema.Politician.select(where=web.sqlors('id=', results))
            if len(reps) > 1:
                return render.find_multi_reps(reps, congress_ranges)
            else:
                try:
                    rep = reps[0]
                    web.seeother('/p/%s' % rep.id)
                except IndexError:
                    raise web.notfound()

        else:
            index = list(schema.District.select(order='name asc'))
            for i in index:
                i.politician = list(db.select('curr_politician', where='district_id = $i.name', vars=locals()))
            out = apipublish.publish(index, format)
            if out: return out

            return render.districtlist(index)

class state:
    def index(self):
            return ('/us/%s' % (s.code) for s in db.select('state', what='code'))

    def GET(self, state, format=None):
        try:
            state = schema.State.where(code=state.upper())[0]
            state.senators = db.select('curr_politician', where='district_id = $state.code', vars=locals())
        except IndexError:
            raise web.notfound()

        out = apipublish.publish([state], format)
        if out: return out

        return render.state(state)

class redistrict:
    def GET(self, district):
        return web.seeother('/us/' + district.lower())

class district:
    def index(self):
        return ('/us/%s' % (d.name) for d in db.select('district', what='name'))

    def GET(self, district, format=None):
        try:
            d = schema.District.where(name=district.upper())[0]
            d.politician = list(db.select('curr_politician', where='district_id = $d.name', vars=locals()))[0]
        except IndexError:
            raise web.notfound()

        out = apipublish.publish([d], format)
        if out: return out
        
        return render.district(d, sparkpos)

def group_politician_similarity(politician_id, qmin=None):
    """Find the interest groups that vote most like a politician."""
    query_min = lambda mintotal, politician_id=politician_id: db.select(
      'group_politician_similarity'
      ' JOIN interest_group ON (interest_group.id = group_id)',
      what='*, cast(agreed as float)/total as agreement',
      where='total >= $mintotal AND politician_id=$politician_id ',
      vars=locals()).list()

    if qmin:
        q = query_min(qmin)
    else:
        q = query_min(5)
        if not q:
            q = query_min(3)
            if not q:
                q = query_min(1)

    q.sort(lambda x, y: cmp(x.agreement, y.agreement), reverse=True)
    return q

def politician_contributors(polid, limit=None):
    query = """SELECT cn.name, cn.zip, 
            min(cn.sent) as from_date, max(cn.sent) as to_date, 
            sum(cn.amount) as amt FROM committee cm, politician_fec_ids pfi, 
            politician p, contribution cn WHERE cn.recipient_id = cm.id 
            AND cm.candidate_id = pfi.fec_id AND pfi.politician_id = p.id 
            AND p.id = $polid AND cn.employer_stem != '' GROUP BY cn.name, cn.zip 
            ORDER BY amt DESC"""
    if limit: query = query + ' LIMIT %d' % limit
    return db.query(query, vars=locals())

def politician_contributor_employers(polid, limit=None):
    query = """SELECT cn.employer_stem, 
            sum(cn.amount) as amt FROM committee cm, politician_fec_ids pfi, 
            politician p, contribution cn WHERE cn.recipient_id = cm.id 
            AND cm.candidate_id = pfi.fec_id AND pfi.politician_id = p.id 
            AND p.id = $polid AND cn.employer_stem != '' GROUP BY cn.employer_stem 
            ORDER BY amt DESC"""
    if limit: query = query + ' LIMIT %d' % limit
    return db.query(query, vars=locals())

def candidates_by_occupation(occupation, limit=None):
    query = """SELECT sum(amount) AS amt, p.firstname, 
            p.lastname, p.id as polid, p.party FROM contribution cn, 
            committee cm, politician_fec_ids pfi, politician p 
            WHERE cn.recipient_id = cm.id AND cm.candidate_id = pfi.fec_id 
            AND pfi.politician_id = p.id 
            AND lower(cn.occupation) = lower($occupation)
            GROUP BY polid, p.lastname, p.firstname, p.party 
            ORDER BY amt DESC"""
    if limit: query = query + ' LIMIT %d' % limit
    return db.query(query, vars=locals())

def committees_by_occupation(occupation, limit=None):
    query = """SELECT sum(amount) AS amt, cm.id, cm.name
            FROM contribution cn, committee cm 
            WHERE cn.recipient_id = cm.id 
            AND lower(cn.occupation) = lower($occupation)
            GROUP BY cm.id, cm.name
            ORDER BY amt DESC"""
    if limit: query = query + " LIMIT %d" % limit
    return db.query(query, vars=locals())

def politician_lob_contributions(polid, page, limit):
    return db.select(['lob_organization', 'lob_filing', 'lob_contribution', 'lob_person'], 
                where="politician_id = $polid and lob_filing.id = filing_id and lob_organization.id = org_id and lob_person.id = lobbyist_id", 
                order='amount desc', limit=limit, offset=page*limit,
                vars=locals())


def bill_list(format, page=0, limit=50):
    bills = schema.Bill.select(limit=limit, offset=page*limit, order='session desc, introduced desc, number desc')

    out = apipublish.publish(bills, format)
    if out: return out
    #@@ add link to next page

    return render.bill_list(bills, limit)

class roll:
    def index(self):
        return ('/r/us/%s' % (r.id) for r in db.select('roll', what='id'))

    def GET(self, roll_id, format=None):
        try:
            b = schema.Roll.where(id=roll_id)[0]
            votes = schema.Vote.where(roll_id=b.id)
        except IndexError:
            raise web.notfound()

        out = apipublish.publish([b], format)
        if out: return out
        
        def votepct(pvotes):
            s = (float(pvotes.get(1, 0)) / sum(pvotes.values()))
            return str(s * 100)[:4].rstrip('.') + '%'

        return render.roll(b, votes, votepct)

class bill:
    def index(self):
        return ('/b/%s' % (b.id) for b in db.select('bill', what='id'))

    def GET(self, bill_id, format=None):
        if bill_id == "" or bill_id == "index":
            i = web.input(page=0)
            return bill_list(format, int(i.page))
        
        try:
            b = schema.Bill.where(id=bill_id)[0]
        except IndexError:
            raise web.notfound()
        
        out = apipublish.publish([b], format)
        if out: return out
        
        return render.bill(b)
        
class contributor:
    def index(self):
        def format(name):
            names = name.lower().split(', ')
            if len(names) > 1:
                return '_'.join(names[1].split() + [names[0]])
            return urllib.quote(name)
        return ('/contrib/%s/%s' % (c.zip,  format(c.name)) \
                    for c in db.select('contribution', what='zip, name'))

    def GET(self, zipcode, name):
        names = name.lower().replace('_', ' ').split(' ')
        if len(names) > 1: name = names[-1]+', '+' '.join(names[:-1])
        else: name = names[0]
        candidates = list(db.query("""SELECT count(*) AS how_many, 
            sum(amount) AS how_much, p.firstname, p.lastname, 
            cm.name AS committee, cm.id as committee_id, occupation, 
            employer_stem, employer, p.id as polid ,
            min(cn.sent) as from_date, max(cn.sent) as to_date 
            FROM contribution cn, committee cm, politician_fec_ids pfi, 
            politician p WHERE cn.recipient_id = cm.id 
            AND cm.candidate_id = pfi.fec_id AND pfi.politician_id = p.id 
            AND lower(cn.name) = $name AND cn.zip = $zipcode 
            GROUP BY cm.id, cm.name, p.lastname, p.firstname, cn.occupation, 
            cn.employer_stem, cn.employer, p.id ORDER BY lower(cn.employer_stem), 
            lower(occupation), to_date DESC, how_much DESC""", vars=locals()))
        committees = list(db.query("""SELECT count(*) AS how_many, 
            sum(amount) AS how_much, cm.name, cm.id, occupation, 
            employer_stem, employer, max(cn.sent) as to_date, min(cn.sent) as from_date 
            FROM contribution cn, committee cm WHERE cn.recipient_id = cm.id 
            AND lower(cn.name) = $name AND cn.zip = $zipcode 
            GROUP BY cm.id, cm.name, cn.occupation, cn.employer_stem, cn.employer
            ORDER BY lower(cn.employer_stem), 
            lower(occupation), to_date DESC, how_much DESC""", vars=locals()))
        return render.contributor(candidates, committees, zipcode, name)

class occupation:
    def index(self):
            #/occupation/<occupation>, /occupation/<occupation>/candidates, /occupation/<occupation>/committees
        occupations = (c.occupation.lower() \
                        for c in db.query('select distinct occupation from contribution'))
        return (('/occupation/%s' % urllib.quote(o), '/occupation/%s/candidates' % urllib.quote(o), '/occupation/%s/committees' % urllib.quote(o))  \
                    for o in occupations if o)

    def GET(self, occupation):
        if occupation != occupation.lower(): raise web.seeother('/occupation/%s' % occupation.lower())
        if os.path.exists(config.cache_dir + '/occupation/' + occupation):
            candidates, committees = pickle.load(file(config.cache_dir + '/occupation/' + occupation))
            candidates, committees = candidates[:5], committees[:5]
        else:
            candidates = candidates_by_occupation(occupation, 5)
            committees = committees_by_occupation(occupation, 5)
        return render.occupation(candidates, committees, occupation) 

def cache_occupation(occupation):
    candidates = list(candidates_by_occupation(occupation))
    committees = list(committees_by_occupation(occupation))
    pickle.dump((candidates, committees), file(config.cache_dir + '/occupation/' + occupation, 'w'))
    
class occupation_candidates:
    #index done in occupation
    def GET(self, occupation):
        if os.path.exists(config.cache_dir + '/occupation/' + occupation):
             candidates = pickle.load(file(config.cache_dir + '/occupation/' + occupation))[0]
        else:
             candidates = candidates_by_occupation(occupation)
        return render.occupation_candidates(candidates, occupation)     

class occupation_committees:
    #index done in occupation
    def GET(self, occupation):
        if os.path.exists(config.cache_dir + '/occupation/' + occupation):
             committees = pickle.load(file(config.cache_dir + '/occupation/' + occupation))[1]
        else:
             committees = committees_by_occupation(occupation)
        return render.occupation_committees(committees, occupation)     

class contributions:
    """from a corp to a pol"""
    def index(self):
        return ('/contrib/?from=%s&to=%s' % (urllib.quote(c.frm), urllib.quote(c.to)) \
                    for c in db.query("""SELECT cn.employer_stem as frm, p.id as to
                            FROM contribution cn, committee cm, politician_fec_ids pfi, politician p 
                            WHERE cn.recipient_id = cm.id AND cm.candidate_id = pfi.fec_id 
                            AND pfi.politician_id = p.id"""))

    def GET(self, img=None):
        i = web.input()
        frm, to = i.get('from', ''), i.get('to', '')
        if frm and to:
            contributions = db.query("""SELECT sum(amount) AS amount,
                p.firstname, p.lastname, p.id as polid,
                employer_stem as employer, date_part('year', sent) as year
                FROM contribution cn, committee cm, politician_fec_ids pfi, politician p 
                WHERE cn.recipient_id = cm.id AND cm.candidate_id = pfi.fec_id 
                AND pfi.politician_id = p.id AND cn.employer_stem = $frm AND p.id=$to 
                GROUP BY year, p.lastname, p.firstname, cn.employer_stem, p.id
                ORDER BY year""", vars=locals()).list()
            
            if img:
                points = [c.amount for c in contributions]
                web.header('Content-Type', 'image/png')
                return simplegraphs.sparkline(points, i.get('point', 0))
            return render.contributions(frm, to, contributions)
        else:
            raise web.notfound()
  
class employer:
    def index(self):
        #'/empl/(.*?)%s?'
        return ('/empl/%s' % (urllib.quote(c.employer_stem)) \
                    for c in db.query('select distinct(employer_stem) from contribution'))

    def GET(self, corp_id, format=None):
        if corp_id == '': raise web.notfound()
        corp_id = corp_id.lower().replace('_', ' ')
        contributions = db.query("""SELECT count(*) as how_many, 
            sum(amount) as how_much, p.firstname, p.lastname, p.id as polid
            FROM contribution cn, committee cm, politician_fec_ids pfi, 
            politician p WHERE cn.recipient_id = cm.id 
            AND cm.candidate_id = pfi.fec_id AND pfi.politician_id = p.id 
            AND lower(cn.employer_stem) = $corp_id
            GROUP BY p.lastname, p.firstname, p.id 
            ORDER BY how_much DESC""", vars=locals())
        total_num = db.select('contribution', 
                            what='count(*)',
                            where='lower(employer_stem)=lower($corp_id)', 
                            vars=locals())[0].count
        return render.employer(contributions, corp_id, total_num)

def earmark_list(format, page=0, limit=50):
    earmarks = schema.Earmark.select(limit=limit, offset=page*limit, order='id')

    out = apipublish.publish(earmarks, format)
    if out: return out
    return render.earmark_list(earmarks, limit)

def earmark_pol_list(pol_id, format, page=0, limit=50):
    earmarks = db.select(['earmark_sponsor', 'earmark'], what='earmark.*', 
            where='politician_id = $pol_id AND earmark_id=earmark.id', 
            order='final_amt desc', vars=locals()).list()
    for e in earmarks:
        p = schema.Politician.where(id=pol_id)[0]
        e.sponsor_name = '%s %s' % (p.title, p.name)
    if not earmarks:
        # @@TODO: something better here. 
        raise web.notfound()
    out = apipublish.publish(earmarks, format)
    if out: return out
    return render.earmark_list(earmarks, limit)

class politician_earmarks:
    def index(self):
        #/p/(.*?)/earmarks
        return ('/p/%s/earmarks' % (e.politician_id) \
                    for e in db.query('select distinct(politician_id) from earmark_sponsor'))

    def GET(self, polid, format=None):
        try:
            em = schema.Politician.where(id=polid)[0]
        except IndexError:
            raise web.notfound()
        return earmark_pol_list(polid, format)

class earmark:
    def index(self):
        #/e/(.*?)%s
        return ('/e/%s' % (e.id) for e in db.select('earmark', what='id'))

    def GET(self, earmark_id, format=None):
        # No earmark id, show list
        if earmark_id == "" or earmark_id == "index":
            # Show earmark list
            i = web.input(page=0)
            return earmark_list(format, int(i.page))
        # Display the specific earmark
        try:
            em = schema.Earmark.where(id=int(earmark_id))[0]
        except IndexError:
            raise web.notfound()
        except ValueError:
            raise web.notfound()
        return render.earmark(em)


class politician:
    def index(self):
        return ('/p/%s' % (p.id) for p in db.select('politician', what='id'))

    def GET(self, polid, format=None):
        if polid != polid.lower():
            raise web.seeother('/p/' + polid.lower())
        
        i = web.input()
        idlookup = False
        for k in ['votesmartid', 'bioguideid', 'opensecretsid', 'govtrackid']:
            if i.get(k):
                idlookup = True
                ps = schema.Politician.where(**{k: i[k]})
                if ps: raise web.seeother('/p/' + ps[0].id)

        if idlookup:
            # we were looking up by ID but nothing matched
            raise web.notfound()

        if polid == "" or polid == "index":
            polids = tuple(x.id for x in db.query('select id from curr_politician'))
            p = schema.Politician.select(where='id in $polids', order='district_id asc', vars=locals())
            out = apipublish.publish(p, format)
            if out: return out

            return render.pollist(p)

        try:
            p = schema.Politician.where(id=polid)[0]
        except IndexError:
            raise web.notfound()

        #@@move into schema
        p.fec_ids = [x.fec_id for x in db.select('politician_fec_ids', what='fec_id',
          where='politician_id=$polid', vars=locals())]

        p.related_groups = group_politician_similarity(polid)
        p.contributors = politician_contributors(polid, 5)
        p.contributor_employers = politician_contributor_employers(polid, 5)
        p.lob_contribs = politician_lob_contributions(polid, 0, 5)
        p.capitolwords = p.bioguideid and get_capitolwords(p.bioguideid)
        out = apipublish.publish([p], format)
        if out: return out

        return render.politician(p, sparkpos)

def get_capitolwords(bioguideid):
    capitolwords_path = 'data/crawl/capitolwords'
    fn = "%s/%s.json" % (capitolwords_path, bioguideid)
    if os.path.isfile(fn):
        words = simplejson.load(file(fn))
        return [web.storage(w) for w in words]
    
class politician_lobby:
    def index(self):
        #/p/(.*?)/lobby
        return ('/p/%s/lobby' % (p.politician_id) 
                    for p in db.query('select distinct(politician_id) from lob_contribution'))

    def GET(self, polid, format=None):
        limit = 50
        page = int(web.input(page=0).page)
        #c = schema.lob_contribution.select(where='politician_id=$polid', limit=limit, offset=page*limit, order='amount desc', vars=locals())
        a = db.select(['lob_filing', 'lob_contribution'], 
                what='SUM(amount)',
                where="politician_id = $polid AND lob_filing.id = filing_id",
                vars=locals())[0].sum
        c = politician_lob_contributions(polid, page, limit)
        return render.politician_lobby(c, a, limit)

class lob_filing:
    def index(self):
        #/lob/f/?(.*?)
        return ('/lob/f/%s' % (f.id) 
                    for f in db.query('select distinct(id) from lob_filing'))

    def GET(self, filing_id):
        limit = 50
        try:
            filing_id = int(filing_id or 0)
            page = int(web.input(page=0).page)
        except ValueError:
            raise web.notfound()
        if filing_id:
            f = schema.lob_filing.select(where='id=$filing_id', limit=limit, offset=page*limit, vars=locals())
        else:
            f = schema.lob_filing.select(limit=limit, offset=page*limit)

        if not f: raise web.notfound()
        return render.lob_filings(f, limit)

class lob_contrib:
    def index(self):
        #/lob/c/?(.*?)
        return ('/lob/c/%s' %(l.filing_id) \
                for l in db.select('lob_contribution', what='filing_id'))

    def GET(self, filing_id):
        limit = 50
        page = int(web.input(page=0).page)
        if filing_id:
            c = schema.lob_contribution.select(where='filing_id=$filing_id', limit=limit, offset=page*limit, order='amount desc', vars=locals())
        else:
            c = schema.lob_contribution.select(limit=limit, offset=page*limit, order='amount desc')

        if not c: raise web.notfound()
        return render.lob_contributions(c, limit)

class lob_pac:
    def index(self):
        #/lob/pa/?(.*?)
        return ('/lob/pa/%s' % (pac.id) for pac in db.select('lob_pac', what='id'))
        
    def GET(self, pac_id):
        limit = 50
        i = web.input(page=0)
        page = int(i.page)
        if 'filing_id' in i:
            p = [x.pac for x in schema.lob_pac_filings.select(where='filing_id=$i.filing_id',limit=limit, offset=page*limit, vars=locals())]
        elif pac_id:
            p = schema.lob_pac.select(where='id=$pac_id',limit=limit, offset=page*limit, vars=locals())
        else:
            p = schema.lob_pac.select(limit=limit, offset=page*limit)

        if not p: raise web.notfound()
        return render.lob_pacs(p, limit)

class lob_org:
    def index(self):
        #/lob/o/?(.*?)
        return ('/lob/o/%s' % (l.id) for l in db.select('lob_organization', what='id'))

    def GET(self, org_id):
        limit = 50
        i = web.input(page=0)
        page = int(i.page)
        if org_id:
            o = schema.lob_organization.select(where='id=$org_id', limit=limit, offset=page*limit, order='name asc', vars=locals())
        else:
            o = schema.lob_organization.select(limit=limit, offset=page*limit, order='name asc')

        if not o: raise web.notfound()
        return render.lob_orgs(o,limit)

class lob_person:
    def index(self):
        #/lob/pe/?(.*?)
        return ('/lob/pe/%s' % (p.id) for p in db.select('lob_person'))

    def GET(self, person_id):
        limit = 50
        i = web.input(page=0)
        page = int(i.page)
        if person_id:
            p = schema.lob_person.select(where='id=$person_id', limit=limit, offset=page*limit, order='lastname asc', vars=locals())
        else:
            p = schema.lob_person.select(limit=limit, offset=page*limit, order='lastname asc')

        if not p: raise web.notfound()
        return render.lob_person(p, limit)

class ein:
    def index(self):
        #/ein/\d+/.*
        return ('/ein/%s/%s' % (p.ein, helpers.urlify(p.primary_name))
                for p in db.query('select ein, primary_name from exempt_org'))
    
    def GET(self, ein, slug=None):
        try:
            p = schema.Exempt_Org.select(where='ein=$ein', vars=locals())[0]
        except IndexError:
            raise web.notfound()
        if slug != '/' + helpers.urlify(p.primary_name):
            raise web.redirect('/ein/%s/%s' % (ein, helpers.urlify(p.primary_name)))
        return render.exempt_org(p, helpers.eo_codes)

class politician_introduced:
    def index(self):
        #/p/(.*?)/introduced
        return ('/p/%s/introduced' % (p.sponsor_id) \
                for p in db.query('select distinct(sponsor_id) from bill'))

    def GET(self, politician_id):
        try:
            pol = schema.Politician.where(id=politician_id)[0]
        except IndexError:
            raise web.notfound()
        return render.politician_introduced(pol)

class politician_groups:
    def index(self):
        #/p/(.*?)/groups
        return ('/p/%s/groups' % (p.politician_id) \
                for p in db.query('select distinct(politician_id) from group_politician_similarity'))

    def GET(self, politician_id):
        related = group_politician_similarity(politician_id, qmin=1)
        try:
            pol = schema.Politician.where(id=politician_id)[0]
        except IndexError:
            raise web.notfound()
        return render.politician_groups(pol, related)

class politician_contribs:
    def index(self):
        #'/p/(.*?)/contribs'
        return ('/p/%s/contribs' % (p.politician_id) \
                for p in db.select('politician_fec_ids', what='politician_id'))

    def GET(self, polid):
        try:
            pol = schema.Politician.where(id=polid)[0]
        except IndexError:
            raise web.notfound()
        contribs = politician_contributors(polid)
        return render.politician_contribs(pol, contribs)

class politician_contrib_employers:
    def index(self):
        #'/p/(.*?)/contrib-employers'
        return ('/p/%s/contrib-employers' % (p.politician_id) \
                for p in db.select('politician_fec_ids', what='politician_id'))

    def GET(self, polid):
        try:
            pol = schema.Politician.where(id=polid)[0]
        except IndexError:
            raise web.notfound()
        contribs = politician_contributor_employers(polid)
        return render.politician_contrib_employers(pol, contribs)

class politician_group:
    def index(self):
        #/p/(.*?)/(\d+)
        result = db.select(['interest_group_bill_support', 'position'], 
                what='politician_id, group_id',
                where='interest_group_bill_support.bill_id = position.bill_id')
        return ('/p/%s/%s' % (r.politician_id, r.group_id) for r in result)
        
    def GET(self, politician_id, group_id):
        votes = db.select(['position', 'interest_group_bill_support', 'bill'],
          where="interest_group_bill_support.bill_id = position.bill_id AND "
                 "position.bill_id = bill.id AND "
                "politician_id = $politician_id AND group_id = $group_id",
         order='vote = support desc',
          vars=locals())
        
        pol = schema.Politician.where(id=politician_id)
        group = schema.Interest_Group.where(id=group_id)
        if not (pol and group):
            raise web.notfound()
        return render.politician_group(votes, pol[0], group[0].longname)


r_safeproperty = re.compile('^[a-z0-9_]+$')
table_map = {'us': 'district', 'p': 'politician'}

def namesmap():
    d = {}
    cols = [cname for cname, c in schema.Politician.columns.items() if c.sql_type in ('real', 'int')]
    prefix_map = dict(n_='number of <>', pct_='money from <>', amt_='amount of <>')
    for c in cols:
        for prefix in prefix_map:
            if c.startswith(prefix):
                x = web.lstrips(c, prefix).split('_')
                if x[0] in ('earmark', 'vote', 'bill', 'smalldonor'): x[0] += 's' #make plural
                if c == 'pct_spent':
                    d[c] = 'money spent'
                else:
                    d[c] = prefix_map[prefix].replace('<>', ' '.join(x))
    for c in cols:
        if c not in d:
            d[c] = c.replace('_', ' ')
    return d

class dproperty:
    def index(self):
        def get_number_columns(table):
            return [cname for cname, c in table.columns.iteritems() if c.sql_type in ('int', 'real')]

        for prefix, table in table_map.iteritems():
            table = getattr(schema, table.title())
            yield ('/%s/by/%s' % (prefix, col) for col in get_number_columns(table))

    def GET(self, table, what):
        try:
            table = table_map[table]
        except KeyError:
            raise web.notfound()
        if not r_safeproperty.match(what): raise web.notfound()

        #if `what` is not there in the `table` (provide available options rather than 404???)
        try:
            maxnum = float(db.select(table,
                                 what='max(%s) as m' % what,
                                 vars=locals())[0].m)
        except:
            raise web.notfound()

        items = db.select(table,
                          what="*, 100*(%s/$maxnum) as pct" % what,
                          order='%s desc' % what,
                          where='%s is not null' % what,
                          vars=locals()).list()
        for item in items:
            if table == 'district':
                item.id = 'd' + item.name
                item.path = '/us/' + item.name.lower()
            elif table == 'politician':
                state = '-'+item.district_id.split('-')[0] if item.district_id else ''
                item.name = '%s %s (%s%s)' % (item.firstname, item.lastname,
                  (item.party or 'I')[0], state)
                item.path = '/p/' + item.id
        return render.dproperty(items, what, namesmap().get(what))

def sparkpos(table, what, id):
    if table == 'district':
        id_col = 'name'
        id = id.upper()
    elif table == 'politician':
        id_col= 'id'
    else: return 0
    assert table in table_map.values()
    if not r_safeproperty.match(what): raise web.notfound()
    
    item = db.query("select count(*) as position from %(table)s, \
      (select * from %(table)s where %(id_col)s=$id) as a \
      where %(table)s.%(what)s > a.%(what)s" % 
      {'table':table, 'what':what, 'id_col':id_col}, vars={'id': id})[0]
    return item.position + 1 # '#1' looks better than '#0'

class sparkdist:
    def GET(self, table, what):
        try:
            table = table_map[table]
        except KeyError:
            raise web.notfound()
        if not r_safeproperty.match(what): raise web.notfound()

        inp = web.input(point=None)
        points = db.select(table, what=what, order=what+' asc', where=what+' is not null')
        points = [x[what] for x in points.list()]

        web.header('Content-Type', 'image/png')
        return simplegraphs.sparkline(points, inp.point)

class handshakes:
    def index(self):
        return ('/h/')
    def GET(self):
        handshakes = schema.Handshakes.select(order='year desc, pol2corp+corp2pol desc')
        return render.handshakes(handshakes)

app = web.application(urls, globals())
def notfound():
    web.ctx.status = '404 Not Found'
    return web.notfound(getattr(render, '404')())

def internalerror():
    return web.internalerror(file('templates/500.html').read())

def and_join(phrases):
    """Format a list of phrases as an English list.

    This must already exist in web.py but I can't find it."""
    phrases = list(phrases)
    assert len(phrases) > 0          # caller should special-case this
    if len(phrases) == 1:
        return phrases[0]
    elif len(phrases) == 2:
        return ' and '.join(phrases)
    else:
        return ', '.join(phrases[:-1] + ['and ' + phrases[-1]])

def pluralize(noun, plural, number):
    """Inflect a noun for number."""
    if number == 1:
        return noun
    else:
        return plural

def divide_into_ranges(ints):
    """Summarize a sorted sequence of ints as a sequence of contiguous ranges.

    Return value is a list of lists of the form `[start, stop]`, where `stop`
    is one more than the last item in the range.
    """
    rv = []

    for item in ints:
        if len(rv) == 0:
            rv.append([item, item+1])
        elif item == rv[-1][1]:
            rv[-1][1] += 1
        else:
            rv.append([item, item+1])

    return rv

def congress_ranges(congresses):
    """Format a list of Congress ordinal numbers
    as a coalesced English sequence of ranges."""

    nthstr = web.utils.nthstr

    if not congresses:
        return "no known Congresses"

    ranges = divide_into_ranges(congresses)
    phrases = []
    for start, stop in ranges:
        if stop == start + 1:
            phrases.append(nthstr(start))
        elif stop == start + 2:
            phrases.append(nthstr(start))
            phrases.append(nthstr(start+1))
        else:
            phrases.append('%s%s' % (nthstr(start), nthstr(stop-1)))

    return "the %s %s" % (and_join(phrases),
                          pluralize("Congress", "Congresses", len(congresses)))

app.notfound = notfound
if production_mode:
    pass#app.internalerror = web.emailerrors(config.send_errors_to, internalerror)
wsgiapp = app.wsgifunc()
if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == 'cache':
        cache_occupation(sys.argv[2])
    else:
        app.run()

########NEW FILE########
__FILENAME__ = webapp_test
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"Unit tests for code in webapp.py."
import re, time, urllib, pprint, StringIO
import simplejson
import web
from utils import rdftramp
import webapp
import cgitb
cgitb.enable(format='text')

def ok(a, b): assert a == b, (a, b)
def ok_re(a, b): assert re.search(b, a), (a, b)
def ok_items(actual, expected):
    "Check specified keys in a dict."
    for k, v in expected.items(): ok(actual[k], v)

defaultns = rdftramp.Namespace('http://watchdog.net/about/api#')
def ok_graph(actual, expected, ns=defaultns):
    for k, v in expected.items():
        if k == '_type':
            k = rdftramp.rdf.type
            v = ns[v]
        else:
            k = ns[k]
        if isinstance(v, str) and v.startswith('http://'):
            v = rdftramp.URI(v)
        ok(actual[k], v)

def time_thunk(thunk):
    start = time.time()
    rv = thunk()
    return time.time() - start, rv

def json(path):
    "Get and decode JSON for a certain path."
    resp = webapp.app.request(path + '.json')
    ok(resp.status[:3], '200')
    ok(resp.headers['Content-Type'], 'application/json')
    return simplejson.loads(resp.data)

#@@ more places should use this
def html(path):
    resp = webapp.app.request(path, headers={ 'Accept': 'text/html' })
    ok(resp.status[:3], '200')
    assert resp.headers['Content-Type'].startswith('text/html')
    return resp.data

def test_find():
    "Test /us/."
    headers = {'Accept': 'text/html'}
    ok(webapp.app.request('/', headers=headers).status[:3], '200')

    # A ZIP code within a single congressional district.
    resp = webapp.app.request('/us/?zip=94070', headers=headers)
    ok(resp.status[:3], '303')
    ok(resp.headers.get('Location'), 'http://0.0.0.0:8080/us/ca-12')

    # A ZIP code in Indiana that crosses three districts.
    in_zip = html('/us/?zip=46131')
    ok_re(in_zip, '/us/in-04')
    ok_re(in_zip, 'Steve Buyer')   # rep for IN-04 at the moment
    ok_re(in_zip, '/us/in-05')
    ok_re(in_zip, '/us/in-06')
    assert '/us/in-07' not in in_zip, in_zip
    
    # Test for LEFT OUTER JOIN: district row with no corresponding politician row.
    ok_re(html('/us/?zip=70072'), '/us/la-01')       # no rep at the moment

    # Test for /us/ listing of all the districts and reps.
    # Takes 9-12 seconds on my machine, I think because it's
    # retrieving the district outline data from MySQL.  Takes nearly
    # 1s on watchdog.net.
    reqtime, resp = time_thunk(lambda: webapp.app.request('/us/',
                                               headers=headers))
    print "took %.3f sec to get /us/" % reqtime
    ok(resp.status[:3], '200')
    ok_re(resp.data, '/us/in-04')
    ok_re(resp.data, 'Stephen E. Buyer')
    ok_re(resp.data, '/us/la-01')       # LEFT OUTER JOIN test
    assert '(Rep.  )' not in resp.data

    # JSON of /us/ --- very minimal test
    index = json('/us/index')
    ok(len(index), len(list(webapp.db.select('district'))))

def test_state():
    "Test state pages such as /us/nm.html."
    nm = html('/us/nm')
    ok_re(nm, 'href="/us/nm-01"')
    assert '/us/NM-01' not in nm # the uppercase URLs aren't canonical
    ok_re(nm, 'href="/us/nm-02"')
    ok_re(nm, 'href="/us/nm-03"')
    assert '/us/nm-04' not in nm

    # JSON
    resp = webapp.app.request('/us/nm.json')
    ok(resp.status[:3], '200')
    # Copied and pasted from current output; hope it's right.  See
    # below about perils of writing unit tests afterwards.
    nm = simplejson.loads(resp.data)
    ok(len(nm), 1)
    ok_items(nm[0],
        {     'code': 'NM',
          'fipscode': '35',
              'name': 'New Mexico',
            'status': 'state',
             '_type': 'State',
               'uri': 'http://watchdog.net/us/nm#it',
         'wikipedia': 'http://en.wikipedia.org/wiki/New_Mexico',
         'districts': ['http://watchdog.net/us/nm#it', 
                       'http://watchdog.net/us/nm-01#it', 
                       'http://watchdog.net/us/nm-02#it', 
                       'http://watchdog.net/us/nm-03#it']
        })

    # JSON obtained with Accept header.
    rsp2 = webapp.app.request('/us/nm', headers={'Accept': 'application/json'})
    ok(rsp2.data, resp.data)

    #@@ I'd write an N3 test but I'm too sleepy to Google up an N3
    # parser right now.

def test_district():
    "Test district pages such as /us/nm-02."
    nm_02 = html('/us/nm-02')
    ok_re(nm_02, r'69,598\s+sq\. mi\.')  # the district's area
    ok_re(nm_02, 'href=".*/us/nm"')

    # JSON
    (district,) = json('/us/nm-02')
    # I hope these are right.  I just copied them from the current
    # output --- this is a problem with doing unit tests after the
    # fact.  I omitted floating-point numbers (poverty_pct,
    # center_lat, center_lng) and the outline.
    ok_items(district, dict(
        almanac = 'http://nationaljournal.com/pubs/almanac/2008/people/nm/rep_nm02.htm',
        area_sqmi = 69598,
        cook_index = 'R+6',
        est_population = 625204,
        est_population_year = 2005,
        median_income = 29269,
        name = 'NM-02',
        state = 'http://watchdog.net/us/nm#it',
        _type = 'District',
        uri = 'http://watchdog.net/us/nm-02#it',
        wikipedia = "http://en.wikipedia.org/wiki/New_Mexico's_2nd_congressional_district",
        zoom_level = 6,
        voting = True,
    ))

def test_politician():
    (henry,) = json('/p/henry_waxman')  # unpack single item
    henry_dict = dict(
        bioguideid = 'W000215',
        #birthday = '1939-09-12', #@@ looks like at some point we
        # decided to change the format of dates in our JSON? Why?
        district = 'http://watchdog.net/us/ca-30#it',
        firstname = 'Henry',
        gender = 'M',
        lastname = 'Waxman',
        middlename = 'A.',
        officeurl = 'http://www.henrywaxman.house.gov',
        opensecretsid = 'N00001861',
        #party = 'Democrat',             # Democratic?
        photo_credit_text = 'Congressional Biographical Directory',
        photo_credit_url =
            'http://bioguide.congress.gov/scripts/bibdisplay.pl?index=W000215',
        religion = 'Jewish',
        _type = 'Politician',
        uri = 'http://watchdog.net/p/henry_waxman#it',
        wikipedia = 'http://en.wikipedia.org/wiki/Henry_Waxman',
        words_per_speech = 1445,
        n_speeches = 8
    )
    ok_items(henry, henry_dict)
    henry_photo_path = '/data/crawl/house/photos/W000215.jpg'
    ok(henry['photo_path'], henry_photo_path)
    henry_govtrackid = '400425'
    ok(henry['govtrackid'], henry_govtrackid)

    #ratings = henry['interest_group_rating']
    #assert dict(year=2006,
    #            groupname='ITIC',
    #            longname='Information Technology Industry Council',
    #            rating=43) in ratings, ratings
    #assert dict(year=2005,
    #            groupname='COC',
    #            longname='Chamber of Commerce of the United States',
    #            rating=38) in ratings, ratings

    #@@ This takes too long now.  I havent looked into why.
    if False:
        reqtime, listing = time_thunk(lambda: json('/p/index'))
        print "took %.3f sec to get /p/index.json" % reqtime
        young = [x for x in listing if 
          x['uri'] == 'http://watchdog.net/p/don_young#it'][0]
        ok_items(young, dict(
            district = 'http://watchdog.net/us/ak-00',
            _type = 'Politician',
            uri = 'http://watchdog.net/p/don_young#it',
            wikipedia = 'http://en.wikipedia.org/wiki/Don_Young'
        ))
        ok(listing[-1]['district'], 'http://watchdog.net/us/wy-00')
    
    henry_uri = henry_dict.pop('uri')
    g = rdftramp.Graph()
    g.parse(StringIO.StringIO(webapp.app.request('/p/henry_waxman.n3').data),
            format='n3')
    thing = rdftramp.Thing(rdftramp.URI(henry_uri), g)
    ok_graph(thing, henry_dict)
    #@@ ideally this should be the correct URL, not this file:/// horror
    # (hopefully its a correct relative URL in the HTTP interface)
    ok_graph(thing, dict(photo_path=rdftramp.URI('file://' + henry_photo_path)))
    ok_graph(thing, dict(
        govtrackid = 'http://www.govtrack.us/congress/person.xpd?id=' +
        henry_govtrackid))

    g2 = rdftramp.Graph()
    g2.parse(StringIO.StringIO(webapp.app.request('/p/henry_waxman.xml').data),
             format='xml')
    ok_graph(rdftramp.Thing(rdftramp.URI(henry_uri), g2), henry_dict)
    
    # rdflib doesn't support graph equivalence?!
    # http://code.google.com/p/rdflib/issues/detail?id=24
    #assert g == g2
    #@@ probably should test interest group ratings...
    
    ok_re(webapp.app.request('/p/eleanor_holmes_norton').data, "Eleanor Holmes Norton")
    

def test_dproperty():
    page = html('/us/by/est_population')
    montana = re.search('(?s)<li id="dMT-00">(.*)</li>', page)
    assert montana is not None, page
    montana = montana.group(1)
    ok_re(montana, 'href="/us/mt-00">')

def test_blog():
    html('/blog/')

def test_congress_ranges():
    ok(webapp.pluralize('Congress', 'Congresses', 0),  'Congresses')
    ok(webapp.pluralize('Congress', 'Congresses', 1),  'Congress')
    ok(webapp.pluralize('Congress', 'Congresses', 2),  'Congresses')
    ok(webapp.pluralize('Congress', 'Congresses', 20), 'Congresses')

    ok(webapp.congress_ranges([]), "no known Congresses")
    ok(webapp.congress_ranges([104]), "the 104th Congress")
    ok(webapp.congress_ranges([102]), "the 102nd Congress")
    ok(webapp.congress_ranges([102, 103]), "the 102nd and 103rd Congresses")
    ok(webapp.congress_ranges([102, 104, 105]),
       "the 102nd, 104th, and 105th Congresses")
    ok(webapp.congress_ranges([100, 102, 104, 105]),
       "the 100th, 102nd, 104th, and 105th Congresses")
    ok(webapp.congress_ranges([102, 103, 104]), "the 102nd104th Congresses")
    ok(webapp.congress_ranges([86, 87, 88, 90, 91, 92]),
       "the 86th88th and 90th92nd Congresses")
    ok(webapp.congress_ranges([86, 87, 88, 90, 91, 92, 94, 95, 96]),
       "the 86th88th, 90th92nd, and 94th96th Congresses")

def test_webapp():
    "Test the actual watchdog.net webapp.app app."
    test_state()
    test_district()
    test_politician()
    test_dproperty()
    test_blog()
    test_find()                         # slow

def main():
    test_congress_ranges()
    test_webapp()


if __name__ == '__main__': main()


########NEW FILE########

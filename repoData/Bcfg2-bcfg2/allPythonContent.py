__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Bcfg2 documentation build configuration file, created by
# sphinx-quickstart on Sun Dec 13 12:10:30 2009.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import os
import re
import sys
import time

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.insert(0, os.path.abspath('../src/lib'))
sys.path.insert(0, os.path.abspath('..'))
sys.path.insert(0, os.path.abspath('exts'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.doctest',
              'sphinx.ext.intersphinx', 'sphinx.ext.viewcode',
              'xmlschema']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# Path to XML schemas
xmlschema_path = "../schemas"

# The suffix of source filenames.
source_suffix = '.txt'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
#master_doc = 'contents'
master_doc = 'index'

# General information about the project.
# py3k compatibility
if sys.hexversion >= 0x03000000:
    project = 'Bcfg2'
    copyright = '2009-%s, Narayan Desai' % time.strftime('%Y')
else:
    project = u'Bcfg2'
    copyright = u'2009-%s, Narayan Desai' % time.strftime('%Y')

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '1.3'
# The full version, including alpha/beta/rc tags.
release = '1.3.4'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
html_theme_options = {
    "collapsiblesidebar": "true"
}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
html_favicon = 'favicon.ico'

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
html_sidebars = {
    'index': 'indexsidebar.html'
}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'Bcfg2doc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
# py3k compatibility
if sys.hexversion >= 0x03000000:
    latex_documents = [
      ('index', 'Bcfg2.tex', 'Bcfg2 Documentation',
       'Narayan Desai et al.', 'manual'),
    ]
else:
    latex_documents = [
      ('index', 'Bcfg2.tex', u'Bcfg2 Documentation',
       u'Narayan Desai et al.', 'manual'),
    ]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('man/bcfg2', 'bcfg2', 'Bcfg2 client tool', [], 1),
    ('man/bcfg2-admin', 'bcfg2-admin',
     'Perform repository administration tasks', [], 8),
    ('man/bcfg2-build-reports', 'bcfg2-build-reports',
     'Generate state reports for Bcfg2 clients', [], 8),
    ('man/bcfg2.conf', 'bcfg2.conf',
     'Configuration parameters for Bcfg2', [], 5),
    ('man/bcfg2-crypt', 'bcfg2-crypt',
     'Bcfg2 encryption and decryption utility', [], 8),
    ('man/bcfg2-info', 'bcfg2-info',
     'Creates a local version of the Bcfg2 server core for state observation',
     [], 8),
    ('man/bcfg2-lint', 'bcfg2-lint',
     'Check Bcfg2 specification for validity, common mistakes, and style',
     [], 8),
    ('man/bcfg2-lint.conf', 'bcfg2-lint.conf',
     'Configuration parameters for bcfg2-lint', [], 5),
    ('man/bcfg2-report-collector', 'bcfg2-report-collector',
     'Reports collection daemon', [], 8),
    ('man/bcfg2-reports', 'bcfg2-reports',
     'Query reporting system for client status', [], 8),
    ('man/bcfg2-server', 'bcfg2-server',
     'Server for client configuration specifications', [], 8),
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'Bcfg2', u'Bcfg2 Documentation',
   u'Narayan Desai', 'Bcfg2', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'


# autodoc settings
autodoc_default_flags = ['members', 'show-inheritance']
autoclass_content = "both"

private_re = re.compile(r'^\s*\.\.\s*private-include:\s*(.+)$')

private_include = []


def skip_member_from_docstring(app, what, name, obj, skip, options):
    """ since sphinx 1.0 autodoc doesn't support the :private-members:
    directive, this function allows you to specify
    ``.. private-include: <name>[,<name,...]`` in the docstring of a
    class containing a private method, etc., to ensure that it's
    included. Due to a bug in Sphinx, this doesn't work for attributes
    -- only things that actually have docstrings.  If you want to
    include private attributes, you have to explicitly include them,
    either with :members:, or by putting :autoattribute: in the
    __init__ docstring for a class or module docstring."""
    global private_include
    if name == '__doc__':
        private_include = []
        if not obj:
            return None
        for line in obj.splitlines():
            match = private_re.match(line)
            if match:
                private_include.extend(re.split(r',\s*', match.group(1)))
        return None

    if not skip:
        return None

    if name in private_include:
        return False
    return None


def setup(app):
    app.connect('autodoc-skip-member', skip_member_from_docstring)

# intersphinx settings

# generate intersphinx mappings for all versions of python we support;
# the default will be the version of python this is built with.
# Python only started using sphinx in 2.6, so we won't have docs for
# 2.4 or 2.5.  These are in reverse order, since sphinx seems to look
# in the last mapping first.

def check_object_path(key, url, path):
    if os.path.isfile(path):
        return {key: (url, path)}
    else:
        return {key: (url, None)}

intersphinx_mapping = {}
intersphinx_mapping.update(\
    check_object_path('mock',
                      'http://www.voidspace.org.uk/python/mock',
                      '/usr/share/doc/python-mock-doc/html/objects.inv'))
intersphinx_mapping.update(\
    check_object_path('cherrypy',
                      'http://docs.cherrypy.org/stable',
                      'intersphinx/cherrypy/objects.inv'))

versions = ["3.2", "2.7", "2.6"]
cur_version = '.'.join(str(v) for v in sys.version_info[0:2])

for pyver in versions:
    if pyver == cur_version:
        key = 'py'
    else:
        key = 'py' + pyver.replace(".", "")
    intersphinx_mapping.update(\
        check_object_path(key,
                          'http://docs.python.org/%s' % pyver,
                          '/usr/share/doc/python'
                            + '.'.join([str(x) for x in sys.version_info[0:2]])
                            + '/html/objects.inv'))

########NEW FILE########
__FILENAME__ = xmlschema
""" Sphinx extension to generate documention from XML schemas.  Known
to be woefully imcomplete, probably buggy, terrible error handling,
but it *works* for the subset of XML schema we use in Bcfg2.

Provides the following directives:

* ``.. xml:schema:: <filename>``: Document an XML schema
* ``.. xml:type:: <name>``: Document a complexType or simpleType
* ``.. xml:group:: <name>``: Document an element group
* ``.. xml:attributegroup:: <name>``: Document an attributeGroup
* ``.. xml:element:: <name>``: Document an XML element

Each directive supports the following options:

* ``:namespace: <ns>``: Specify the namespace of the given entity
* ``:nochildren:``: Do not generate documentation for child entities
* ``:noattributegroups:``: Do not generate documentation about
  attribute groups
* ``:nodoc:``: Do not include the documentation included in the entity
  annotation
* ``:notext:``: Do not generate documentation about the text content
  of the entity
* ``:onlyattrs: <attr>,<attr>``: Only generate documentation about the
  comma-separated list of attributes given
* ``:requiredattrs: <attr>,attr>``: Claim that the attributes named in
  the given comma-separated list are required, even if they are not
  flagged as such in the schema.
* ``:linktotype: [<type>,<type>]``: If used as a flag, link to
  documentation on all child types and elements.  If a list is given,
  only link to those types given.  (The default is to generate full
  inline docs for those types.)
* ``:noautodep: [<name>,<name>]``: Do not automatically generate docs
  for any dependent entities.
* ``:inlinetypes: <type>,<type>``: Override a default ``:linktotype:``
  setting for the given types.

Provides the following roles to link to the objects documented above:

* ``:xml:schema:`<name>```: Link to an XML schema
* ``:xml:type:`<name>```: Link to a complexType or simpleType
* ``:xml:group:`<name>```: Link to an element group
* ``:xml:attributegroup:`<name>```: Link to an attributeGroup
* ``:xml:element:`<name>```: Link to an element
* ``:xml:attribute:`<context>:<name>```: Link to the attribute in the
  given context.  The context is the name of the containing object,
  e.g., the parent attributeGroup, element, or complexType.
* ``:xml:datatype:`<name>```: Link to a built-in XML data type.

Note that the entity being linked to does not need to have been
explicitly documented with a directive; e.g., if you document a schema
that contains a complexType, you can link to that type without having
used the ``xml:type::`` directive.

Note also that it's far more reliable to link to a complexType than an
element, since element name collisions are fairly common.  You should
avoid type name collisions whenever possible to maximize usability of
this extension.

There are two configuration items that may be added to conf.py:

* ``xmlschema_path`` gives the base path to all XML schemas.
* ``xmlschema_datatype_url`` gives a string pattern that will be used
  to generate links to built-in XML types.  It must contain a single
  ``%s``, which will be replaced by the name of the type.
"""

import os
import operator
import lxml.etree
from docutils import nodes
from sphinx import addnodes, roles
from docutils.statemachine import ViewList
from docutils.parsers.rst import directives
from sphinx.util.nodes import make_refnode, split_explicit_title, \
    nested_parse_with_titles
from sphinx.util.compat import Directive
from sphinx.domains import ObjType, Domain

try:
    from new import classobj
except ImportError:
    classobj = type

XS = "http://www.w3.org/2001/XMLSchema"
XS_NS = "{%s}" % XS
NSMAP = dict(xs=XS)


def comma_split(opt):
    return opt.split(",")


def flag_or_split(opt):
    try:
        return opt.split(",")
    except AttributeError:
        return True


class _XMLDirective(Directive):
    """ Superclass for the other XML schema directives. """
    required_arguments = 1
    option_spec = dict(namespace=directives.unchanged,
                       nochildren=directives.flag,
                       noattributegroups=directives.flag,
                       nodoc=directives.flag,
                       notext=directives.flag,
                       onlyattrs=comma_split,
                       requiredattrs=comma_split,
                       linktotype=flag_or_split,
                       noautodep=flag_or_split,
                       inlinetypes=comma_split)
    types = []

    def run(self):
        name = self.arguments[0]
        env = self.state.document.settings.env
        reporter = self.state.memo.reporter
        ns_name = self.options.get('namespace')
        try:
            ns_uri = env.xmlschema_namespaces[ns_name]
        except KeyError:
            # URI given as namespace
            ns_uri = ns_name
        etype = None
        for etype in self.types:
            try:
                entity = env.xmlschema_entities[ns_uri][etype][name]
                break
            except KeyError:
                pass
        else:
            reporter.error("No XML %s %s found" %
                           (" or ".join(self.types), name))
            return []
        documentor = XMLDocumentor(entity, env, self.state, name=name,
                                   ns_uri=ns_uri,
                                   include=self.process_include(),
                                   options=self.process_options())
        return documentor.document()

    def process_include(self):
        return dict(children='nochildren' not in self.options,
                    attributegroups='noattributegroups' not in self.options,
                    doc='nodoc' not in self.options,
                    text='notext' not in self.options)

    def process_options(self):
        return dict(onlyattrs=self.options.get('onlyattrs'),
                    requiredattrs=self.options.get('requiredattrs', []),
                    linktotype=self.options.get('linktotype', []),
                    noautodep=self.options.get('noautodep', False),
                    inlinetypes=self.options.get('inlinetypes', []))


def XMLDirective(types):
    class cls(_XMLDirective):
        pass

    cls.__name__ = 'XML%sDirective' % types[0]
    cls.types = types
    return cls


class XMLDocumentor(object):
    def __init__(self, entity, environment, state, name=None, ns_uri=None,
                 parent=None, include=None, options=None):
        self.entity = entity
        self.env = environment
        self.entities = self.env.xmlschema_entities
        self.namespaces = self.env.xmlschema_namespaces
        self.namespaces_by_uri = self.env.xmlschema_namespaces_by_uri
        self.state = state
        self.include = include
        self.options = options
        self.app = self.env.app
        self.reporter = self.state.memo.reporter

        if name is None:
            self.ns_uri = ns_uri
            self.fqname = self.entity.get("name")
            self.ns_name, self.name = self.split_ns(self.fqname)
            if self.ns_uri is None and self.ns_name is not None:
                self.ns_uri = self.namespaces[self.ns_name]
        else:
            self.ns_uri = ns_uri
            self.ns_name = self.namespaces_by_uri[self.ns_uri]
            self.name = name
            if self.ns_name:
                self.fqname = "%s:%s" % (self.ns_name, self.name)
            else:
                self.fqname = name
        self.tname = nodes.strong(self.fqname, self.fqname)
        self.tag = self.entity.tag[len(XS_NS):]
        self.type = tag2type(self.tag)
        self.parent = parent
        if self.parent is None:
            self.dependencies = []
            self.documented = []
        else:
            self.dependencies = self.parent.dependencies
            self.documented = self.parent.documented

    def document(self):
        eid = (self.tag, self.fqname)
        if eid in self.documented:
            return [build_paragraph(get_xref(self.tag, eid[1]))]
        else:
            self.documented.append(eid)

        rv = [self.target_node(self.tag, self.ns_name, self.name)]

        data = addnodes.desc(objtype=self.tag)
        targetid = get_target_id(self.tag, self.ns_name, self.name)
        header = addnodes.desc_signature('', '',
                                         first=True,
                                         ids=[targetid])

        if self.include['doc']:
            header.extend([nodes.emphasis(self.tag, self.tag),
                          text(" "), self.tname])
            data.append(header)
        contents = nodes.definition()
        if self.include['doc']:
            contents.append(self.get_doc(self.entity))
        contents.extend(getattr(self, "document_%s" % self.tag)())
        data.append(contents)
        rv.append(data)

        if self.parent is None:
            # avoid adding duplicate dependencies
            added = [(self.type, self.name)]
            for typ, name, entity in self.dependencies:
                if not name:
                    name = entity.get('name')
                if (typ, name) in added:
                    continue
                ns_name, name = self.split_ns(name)
                ns_uri = self.namespaces[ns_name]
                if not entity:
                    try:
                        entity = self.entities[ns_uri][typ][name]
                    except KeyError:
                        self.app.warn("Dependency %s not found in schemas" %
                                      get_target_id(typ, ns_name, name))
                        continue
                doc = self.get_documentor(entity, name=name, ns_uri=ns_uri)
                rv.extend(doc.document())
                added.append((typ, name))
        return rv

    def document_schema(self):
        try:
            element = self.entity.xpath("xs:element", namespaces=NSMAP)[0]
            ns, name = self.split_ns(element.get("name"))
            doc = self.get_documentor(element, name=name,
                                      ns_uri=self.namespaces[ns])
            return doc.document()
        except IndexError:
            # no top-level element or group -- just a list of
            # (abstract) complexTypes?
            rv = []
            for ctype in self.entity.xpath("xs:complexType", namespaces=NSMAP):
                ns, name = self.split_ns(ctype.get("name"))
                doc = self.get_documentor(ctype, name=name,
                                          ns_uri=self.namespaces[ns])
                rv.extend(doc.document())
            return rv

    def document_group(self):
        rv = nodes.definition_list()
        try:
            (children, groups) = \
                self.get_child_elements(self.entity, nodeclass=nodes.paragraph)
        except TypeError:
            return [build_paragraph(nodes.strong("Any", "Any"),
                                    " arbitrary element allowed")]

        append_node(rv, nodes.term, text("Elements:"))
        append_node(rv, nodes.definition, *children)
        if len(groups):
            append_node(rv, nodes.term, text("Element groups:"))
            append_node(rv, nodes.definition, *groups)
        return rv

    def document_element(self):
        fqtype = self.entity.get("type")
        if fqtype:
            (etype_ns, etype) = self.split_ns(fqtype)
            ns_uri = self.get_namespace_uri(etype_ns)
            values = self.get_values_from_type()
            if values != "Any":
                return [build_paragraph(
                        self.tname,
                        " takes only text content, which may be the ",
                        "following values: ",
                        values)]
            elif etype in self.entities[ns_uri]["complexType"]:
                if ((self.options['linktotype'] is True or
                     self.name in self.options['linktotype'] or
                     etype in self.options['linktotype'] or
                     fqtype in self.options['linktotype']) and
                    self.name not in self.options['inlinetypes'] and
                    etype not in self.options['inlinetypes']):
                    self.add_dep('complexType', fqtype, None)
                    return [build_paragraph("Type: ",
                                            get_xref("type", fqtype))]

                typespec = self.entities[ns_uri]["complexType"][etype]
                doc = self.get_documentor(typespec,
                                          name=self.entity.get("name"))
                rv = [self.target_node("complexType", etype_ns, etype)]
                if self.include['doc'] and not self.get_doc(self.entity):
                    rv.append(self.get_doc(typespec))
                rv.extend(doc.document_complexType())
                return rv
            else:
                self.reporter.error("Unknown element type %s" % fqtype)
                return []
        else:
            rv = []
            typespec = self.entity.xpath("xs:complexType", namespaces=NSMAP)[0]
            if self.include['doc'] and not self.get_doc(self.entity):
                rv.append(self.get_doc(typespec))
            if typespec is not None:
                rv = [self.target_node("complexType", self.ns_name, self.name)]
                doc = self.get_documentor(typespec)
                rv.extend(doc.document_complexType())
            return rv

    def document_complexType(self):
        rv = nodes.definition_list()

        try:
            content = self.entity.xpath("xs:simpleContent",
                                        namespaces=NSMAP)[0]
            base = content.xpath("xs:extension|xs:restriction",
                                 namespaces=NSMAP)[0]
            attr_container = base
        except IndexError:
            base = None
            attr_container = self.entity

        ##### ATTRIBUTES #####
        table, tbody = self.get_attr_table()
        attrs = self.get_attrs(attr_container)
        if attrs:
            tbody.extend(attrs)

        foreign_attr_groups = nodes.bullet_list()
        for agroup in attr_container.xpath("xs:attributeGroup",
                                           namespaces=NSMAP):
            # if the attribute group is in another namespace, just
            # link to it
            ns, name = self.split_ns(agroup.get('ref'))
            if ns != self.ns_name:
                append_node(
                    foreign_attr_groups,
                    nodes.list_item,
                    build_paragraph(get_xref(tag2type("attributeGroup"),
                                             ":".join([ns, name]))))
            else:
                tbody.extend(self.get_attrs(
                        self.entities['attributeGroup'][name]))

        if len(tbody):
            append_node(rv, nodes.term, text("Attributes:"))
            append_node(rv, nodes.definition, table)
        if self.include['attributegroups'] and len(foreign_attr_groups):
            append_node(rv, nodes.term, text("Attribute groups:"))
            append_node(rv, nodes.definition, foreign_attr_groups)

        ##### ELEMENTS #####
        if self.include['children']:
            # todo: distinguish between elements that may occur and
            # elements that must occur
            try:
                (children, groups) = self.get_child_elements(self.entity)
            except TypeError:
                children = None
                groups = None
                rv.append(build_paragraph(nodes.strong("Any", "Any"),
                                          " arbitrary child elements allowed"))
            if children:
                append_node(rv, nodes.term, text("Child elements:"))
                append_node(rv, nodes.definition,
                            build_node(nodes.bullet_list, *children))

            if groups:
                append_node(rv, nodes.term, text("Element groups:"))
                append_node(rv, nodes.definition, *groups)

        ##### TEXT CONTENT #####
        if self.include['text']:
            if self.entity.get("mixed", "false").lower() == "true":
                append_node(rv, nodes.term, text("Text content:"))
                append_node(rv, nodes.definition,
                            build_paragraph(self.get_values_from_simpletype()))
            elif base is not None:
                append_node(rv, nodes.term, text("Text content:"))
                append_node(
                    rv, nodes.definition,
                    build_paragraph(self.get_values_from_simpletype(content)))

        return [rv]

    def document_attributeGroup(self):
        attrs = self.get_attrs(self.entity)
        if attrs:
            table, tbody = self.get_attr_table()
            tbody.extend(attrs)
            return [table]
        else:
            return []

    def get_attr_table(self):
        atable = nodes.table()
        atgroup = build_node(nodes.tgroup('', cols=5),
                             nodes.colspec(colwidth=10),
                             nodes.colspec(colwidth=50),
                             nodes.colspec(colwidth=20),
                             nodes.colspec(colwidth=10),
                             nodes.colspec(colwidth=10),
                             nodes.thead('',
                                         build_table_row("Name", "Description",
                                                         "Values", "Required",
                                                         "Default")))
        atable.append(atgroup)
        atable_body = nodes.tbody()
        atgroup.append(atable_body)
        return (atable, atable_body)

    def get_child_elements(self, el, nodeclass=None):
        """ returns a tuple of (child element nodes, element group
        nodes).  HOWEVER, if _any_ child is allowed, returns True. """
        children = []
        groups = []
        if nodeclass is None:
            nodeclass = nodes.list_item

        if el.xpath("xs:any", namespaces=NSMAP):
            return True

        for child in el.xpath("xs:element", namespaces=NSMAP):
            node = nodeclass()
            if child.get('ref'):
                node.append(build_paragraph(get_xref('element',
                                                     child.get('ref'))))
            else:
                # child element given inline
                doc = self.get_documentor(child, name=child.get('name'))
                node.extend(doc.document())
            children.append(node)

        for group in el.xpath("xs:group", namespaces=NSMAP):
            if group.get('ref'):
                name = group.get('ref')
                node = nodeclass()
                node.append(build_paragraph(get_xref('group', name)))
                self.add_dep('group', name, None)
                groups.append(node)
            else:
                rv = self.get_child_elements(group, nodeclass=nodeclass)
                try:
                    children.extend(rv[0])
                    groups.extend(rv[1])
                except TypeError:
                    return rv

        for container in el.xpath("xs:all|xs:choice|xs:sequence",
                                  namespaces=NSMAP):
            rv = self.get_child_elements(container, nodeclass=nodeclass)
            try:
                children.extend(rv[0])
                groups.extend(rv[1])
            except TypeError:
                return rv
        return (children, groups)

    def get_documentor(self, entity, name=None, ns_uri=None):
        if name is None:
            name = self.name
        if ns_uri is None:
            ns_uri = self.ns_uri
        return XMLDocumentor(entity, self.env, self.state, name=name,
                             ns_uri=ns_uri, parent=self, options=self.options,
                             include=self.include)

    def get_attrs(self, el, context=None):
        cnode = el
        while context is None and cnode is not None:
            context = cnode.get('name')
            cnode = cnode.getparent()

        rows = []
        for attr in el.xpath("xs:attribute[@name]", namespaces=NSMAP):
            name = attr.get("name")
            if self.ns_name:
                fqname = "%s:%s" % (self.ns_name, name)
            else:
                fqname = name
            if (self.options['onlyattrs'] and
                name not in self.options['onlyattrs'] and
                fqname not in self.options['onlyattrs']):
                continue
            tag = attr.tag[len(XS_NS):]
            row = [build_paragraph(self.target_node(tag, self.ns_name, context,
                                                    name),
                                   nodes.literal(fqname, fqname))]
            row.append(self.get_doc(attr))
            if attr.get("type") is not None:
                row.append(build_paragraph(
                        self.get_values_from_type(entity=attr)))
            else:
                try:
                    atype = attr.xpath("xs:simpleType", namespaces=NSMAP)[0]
                    row.append(self.get_values_from_simpletype(atype))
                except IndexError:
                    # todo: warn about no type found
                    pass
            reqd = 0
            if (name in self.options['requiredattrs'] or
                attr.get("use", "optional") == "required"):
                row.append("Yes")
                reqd = 1
            else:
                row.append("No")
            default = attr.get("default")
            if default is None:
                row.append("None")
            else:
                row.append(nodes.literal(default, default))
            # we record name and required separately to make sorting
            # easier
            rows.append((name, reqd, build_table_row(*row)))
        rows.sort(key=operator.itemgetter(0))
        rows.sort(key=operator.itemgetter(1), reverse=True)
        if not self.options['onlyattrs'] or '*' in self.options['onlyattrs']:
            try:
                anyattr = el.xpath("xs:anyAttribute", namespaces=NSMAP)[0]
                rows.append((None, None,
                             build_table_row('*', self.get_doc(anyattr),
                                             "Any", "No", "None")))
            except IndexError:
                pass
        return [r[2] for r in rows]

    def get_values_from_type(self, entity=None, typeattr='type'):
        if entity is None:
            entity = self.entity
        ns_name, name = self.split_ns(entity.get(typeattr))
        ns_uri = self.get_namespace_uri(ns_name, entity=entity)
        if ns_uri == XS:
            return self.get_builtin_type(name)
        elif name in self.entities[ns_uri]['simpleType']:
            return self.get_values_from_simpletype(
                self.entities[ns_uri]['simpleType'][name])
        else:
            return "Any"

    def get_builtin_type(self, vtype):
        if vtype == "boolean":
            return get_value_list(["true", "false"])
        else:
            return get_datatype_ref(vtype, vtype,
                                    self.app.config.xmlschema_datatype_url)

    def get_doc(self, el):
        try:
            return self.parse(el.xpath("xs:annotation/xs:documentation",
                                       namespaces=NSMAP)[0].text)
        except IndexError:
            return build_paragraph('')

    def parse(self, text):
        node = nodes.paragraph()
        vl = ViewList()
        for line in text.splitlines():
            vl.append(line, '<xmlschema>')
        nested_parse_with_titles(self.state, vl, node)
        try:
            return node[0]
        except IndexError:
            return build_paragraph(text)

    def split_ns(self, name):
        try:
            (ns, name) = name.split(":")
        except ValueError:
            ns = self.ns_name
        return (ns, name)

    def get_values_from_simpletype(self, entity=None):
        if entity is None:
            entity = self.entity
        # todo: xs:union, xs:list
        try:
            restriction = entity.xpath("xs:restriction|xs:extension",
                                       namespaces=NSMAP)[0]
        except IndexError:
            return "Any"
        doc = self.get_doc(restriction)
        if len(doc) == 1 and len(doc[0]) == 0:
            # if get_doc returns a paragraph node with an empty Text
            # node
            enum = [e.get("value")
                    for e in restriction.xpath("xs:enumeration",
                                               namespaces=NSMAP)]
            if len(enum):
                return get_value_list(enum)
            else:
                return self.get_values_from_type(entity=restriction,
                                                 typeattr='base')
        else:
            return doc

    def add_dep(self, typ, name, entity):
        try:
            if name in self.options['noautodep']:
                return
        except TypeError:
            if self.options['noautodep']:
                return
        self.dependencies.append((typ, name, entity))

    def target_node(self, tag, ns, *extra):
        targetid = get_target_id(tag, ns, *extra)
        fqname = targetid[len(tag) + 1:]
        rv = nodes.target('', '', ids=[targetid])
        self.add_domain_data(tag2type(tag), fqname,
                             (self.env.docname, targetid))
        return rv

    def add_domain_data(self, typ, key, data):
        if key not in self.env.domaindata['xml'][typ]:
            self.env.domaindata['xml'][typ][key] = data

    def get_namespace_uri(self, ns_name, entity=None):
        if entity is None:
            entity = self.entity
        xs_ns = get_xs_ns(entity)
        if ns_name == xs_ns:
            return XS
        else:
            return self.namespaces[ns_name]


def tag2type(tag):
    if tag in ['complexType', 'simpleType']:
        return 'type'
    elif tag == 'attributeGroup':
        return 'attributegroup'
    return tag


def text(txt):
    return nodes.Text(txt, txt)


def append_node(parent, cls_or_node, *contents):
    parent.append(build_node(cls_or_node, *contents))


def build_node(cls_or_node, *contents):
    if isinstance(cls_or_node, (type, classobj)):
        rv = cls_or_node()
    else:
        rv = cls_or_node
    rv.extend(contents)
    return rv


def get_xref(typ, target, title=None):
    if title is None:
        title = target
    ref = addnodes.pending_xref(title,
                                reftype=typ,
                                refdomain="xml",
                                reftarget=target)
    ref.append(nodes.literal(title, title))
    return ref


def build_table_row(*vals):
    rv = nodes.row('')
    for val in vals:
        if isinstance(val, nodes.Node):
            node = val
        else:
            node = nodes.paragraph(val, val)
        rv.append(nodes.entry(node, node))
    return rv


def build_paragraph(*args):
    """ convenience method to build a paragraph node """
    rv = nodes.paragraph()
    for content in args:
        if isinstance(content, nodes.Node):
            rv.append(content)
        else:
            rv.append(text(content))
    return rv


def get_target_id(etype, ns_name, *extra):
    if ns_name:
        return ":".join([etype, ns_name] + list(extra))
    else:
        return ":".join([etype] + list(extra))


def get_value_list(vals):
    rv = nodes.paragraph()
    if vals:
        rv.append(nodes.literal(vals[0], vals[0]))
        for i in range(1, len(vals)):
            rv.append(text(" | "))
            rv.append(nodes.literal(vals[i], vals[i]))
    return rv


def get_xs_ns(el):
    return get_namespace_name(el, XS)


def get_namespace_name(el, ns_uri):
    for name, ns in el.nsmap.items():
        if ns == ns_uri:
            return name
    return None


def get_datatype_ref(title, target, baseurl):
    return build_node(nodes.reference('', '', refuri=baseurl % target),
                      nodes.literal(title, title))


class XMLDatatypeRole(object):
    def __init__(self, baseurl):
        self.baseurl = baseurl

    def __call__(self, name, rawtext, text, lineno, inliner, options={},
                 content=[]):
        has_explicit_title, title, target = split_explicit_title(text)
        return [get_datatype_ref(title, target, self.baseurl)], []


class XMLXRefRole(roles.XRefRole):
    def __init__(self, typ, **kwargs):
        roles.XRefRole.__init__(self, **kwargs)
        self.type = typ

    def process_link(self, env, refnode, has_explicit_title, title, target):
        if (self.type == 'attribute' and
            not has_explicit_title and
            ':' in title):
            title = title.split(':')[-1]
        return roles.XRefRole.process_link(self, env, refnode,
                                           has_explicit_title, title, target)


class XMLDomain(Domain):
    name = "xml"
    label = "XML"

    types = dict(schema=['schema'],
                 type=['complexType', 'simpleType'],
                 group=['group'],
                 attributegroup=['attributeGroup'],
                 element=['element'],
                 attribute=None)

    object_types = dict([(t, ObjType("XML %s" % t.title(), t))
                         for t in types.keys()])
    directives = dict([(t, XMLDirective(h))
                        for t, h in types.items() if h is not None])
    roles = dict([(t, XMLXRefRole(t)) for t in types.keys()])
    dangling_warnings = dict([(t, "unknown XML %s: %%(target)s" % t)
                              for t in types.keys()])
    initial_data = dict([(t, dict()) for t in types.keys()])
    data_version = 3

    def clear_doc(self, docname):
        to_del = []
        for dtype in self.types.keys():
            for key, (doc, _) in self.data[dtype].iteritems():
                if doc == docname:
                    to_del.append((dtype, key))
        for dtype, key in to_del:
            del self.data[dtype][key]

    def resolve_xref(self, env, fromdocname, builder, typ, target, node,
                     contnode):
        if typ in ['complexType', 'simpleType']:
            typ = 'type'
        if target in self.data[typ]:
            docname, labelid = self.data[typ][target]
        else:
            return None
        return make_refnode(builder, fromdocname, docname,
                            labelid, contnode)

    def get_objects(self):
        for dtype in self.types.keys():
            for name, (docname, tgtid) in self.data[dtype].iteritems():
                yield (name, name, dtype, docname, tgtid,
                       self.object_types[dtype].attrs['searchprio'])


def setup(app):
    app.add_config_value('xmlschema_path', '.', False)
    app.add_config_value('xmlschema_datatype_url',
                         'http://www.w3.org/TR/xmlschema-2/#%s', False)
    app.add_domain(XMLDomain)
    app.connect('builder-inited', load_xml_schemas)
    app.connect('builder-inited', add_xml_datatype_role)


def add_xml_datatype_role(app):
    app.add_role_to_domain('xml', 'datatype',
                           XMLDatatypeRole(app.config.xmlschema_datatype_url))


def load_xml_schemas(app):
    entities = dict()
    entities[None] = dict(schema=dict(),
                          group=dict(),
                          attributeGroup=dict(),
                          element=dict(),
                          simpleType=dict(),
                          complexType=dict())
    namespaces = dict()
    namespaces_by_uri = dict()
    schemapath = os.path.abspath(os.path.join(app.builder.env.srcdir,
                                              app.config.xmlschema_path))
    for root, _, files in os.walk(schemapath):
        for fname in files:
            if not fname.endswith(".xsd"):
                continue
            path = os.path.join(root, fname)
            relpath = path[len(schemapath):].strip("/")
            schema = lxml.etree.parse(path).getroot()

            ns = schema.get("targetNamespace")
            ns_name = get_namespace_name(schema, ns)
            if ns_name not in namespaces:
                namespaces[ns_name] = ns
            if ns not in namespaces_by_uri:
                namespaces_by_uri[ns] = ns_name

            if ns not in entities:
                entities[ns] = dict(schema=dict(),
                                    group=dict(),
                                    attributeGroup=dict(),
                                    element=dict(),
                                    simpleType=dict(),
                                    complexType=dict())
            # schemas don't require namespaces to be identified
            # uniquely, but we let the user identify them either with
            # or without the namespace
            entities[None]['schema'][relpath] = schema
            entities[ns]['schema'][relpath] = schema
            for entity in schema.xpath("//xs:*[@name]", namespaces=NSMAP):
                tag = entity.tag[len(XS_NS):]
                if tag in entities[ns]:
                    entities[ns][tag][entity.get("name")] = entity
    app.builder.env.xmlschema_namespaces = namespaces
    app.builder.env.xmlschema_namespaces_by_uri = namespaces_by_uri
    app.builder.env.xmlschema_entities = entities

########NEW FILE########
__FILENAME__ = include
""" IncludeHelper makes it easier to include group- and host-specific
files in a template.

Synopsis:

  {% python
  import os
  include = metadata.TemplateHelper['include']
  custom = include.IncludeHelper(metadata, path).files(os.path.basename(name))
  %}\
  {% for file in custom %}\

  ########## Start ${include.describe_specificity(file)} ##########
  {% include ${file} %}
  ########## End ${include.describe_specificity(file)} ##########
  {% end %}\

This would let you include files with the same base name; e.g. in a
template for ''foo.conf'', the include files would be called
''foo.conf.G_<group>.genshi_include''.  If a template needs to include
different files in different places, you can do that like so:

  inc = metadata.TemplateHelper['include'].IncludeHelper(metadata, path)
  custom_bar = inc.files("bar")
  custom_baz = inc.files("baz")

This would result in two different sets of custom files being used,
one drawn from ''bar.conf.G_<group>.genshi_include'' and the other
from ''baz.conf.G_<group>.genshi_include''.

"""

import os
import re

__export__ = ["IncludeHelper", "get_specificity", "describe_specificity"]


class IncludeHelper(object):
    def __init__(self, metadata, path):
        """ Constructor.

        The template path can be found in the ''path'' variable that
        is set for all Genshi templates. """
        self.metadata = metadata
        self.path = path

    def get_basedir(self):
        return os.path.dirname(self.path)

    def files(self, fname, groups=None):
        """ Return a list of files to include for this host.  Files
        are found in the template directory based on the following
        patterns:

          * ''<prefix>.H_<hostname>.genshi_include'': Host-specific files
          * ''<prefix>.G_<group>.genshi_include'': Group-specific files
          * ''<prefix>.genshi_include'': Non-specific includes

        Note that there is no numeric priority on the group-specific
        files; all matching files are returned by
        ``IncludeHelper.files()``.  If you wish to only include files
        for a subset of groups, pass the ``groups`` keyword argument.
        Host-specific files are always included in the return
        value. """
        files = []
        hostfile = os.path.join(self.get_basedir(),
                                "%s.H_%s.genshi_include" %
                                (fname, self.metadata.hostname))
        if os.path.isfile(hostfile):
            files.append(hostfile)

        allfile = os.path.join(self.get_basedir(), "%s.genshi_include" % fname)
        if os.path.isfile(allfile):
            files.append(allfile)

        if groups is None:
            groups = sorted(self.metadata.groups)

        for group in groups:
            filename = os.path.join(self.get_basedir(),
                                    "%s.G_%s.genshi_include" % (fname, group))
            if os.path.isfile(filename):
                files.append(filename)

        return files


SPECIFICITY_RE = re.compile(r'(G|H)_(.*)\.genshi_include')


def get_specificity(fname):
    """ Get a tuple of (<type>, <parameter>) describing the
    specificity of the given file.  Specificity types are "host",
    "group", or "all".  The parameter will be either a hostname, a
    group name, or None (for "all"). """
    match = SPECIFICITY_RE.search(fname)
    if match:
        if match.group(1) == "G":
            stype = "group"
        else:
            stype = "host"
        return (stype, match.group(2))
    return ("all", None)


def describe_specificity(fname):
    """ Get a string describing the specificity of the given file """
    (stype, param) = get_specificity(fname)
    if stype != "all":
        return "%s-specific configs for %s" % (stype, param)
    else:
        return "Generic configs for all clients"

########NEW FILE########
__FILENAME__ = Cache
""" An implementation of a simple memory-backed cache. Right now this
doesn't provide many features, but more (time-based expiration, etc.)
can be added as necessary. """


class Cache(dict):
    """ an implementation of a simple memory-backed cache """

    def expire(self, key=None):
        """ expire all items, or a specific item, from the cache """
        if key is None:
            self.clear()
        elif key in self:
            del self[key]

########NEW FILE########
__FILENAME__ = Client
""" The main Bcfg2 client class """

import os
import sys
import stat
import time
import fcntl
import socket
import logging
import tempfile
import Bcfg2.Proxy
import Bcfg2.Logger
import Bcfg2.Options
import Bcfg2.Client.XML
import Bcfg2.Client.Frame
import Bcfg2.Client.Tools
from Bcfg2.Utils import locked, Executor
from Bcfg2.Compat import xmlrpclib
from Bcfg2.version import __version__


class Client(object):
    """ The main Bcfg2 client class """

    def __init__(self, setup):
        self.toolset = None
        self.tools = None
        self.config = None
        self._proxy = None
        self.setup = setup

        if self.setup['debug']:
            level = logging.DEBUG
        elif self.setup['verbose']:
            level = logging.INFO
        else:
            level = logging.WARNING
        Bcfg2.Logger.setup_logging('bcfg2',
                                   to_syslog=self.setup['syslog'],
                                   level=level,
                                   to_file=self.setup['logging'])
        self.logger = logging.getLogger('bcfg2')
        self.logger.debug(self.setup)

        self.cmd = Executor(self.setup['command_timeout'])

        if self.setup['bundle_quick']:
            if not self.setup['bundle'] and not self.setup['skipbundle']:
                self.logger.error("-Q option requires -b or -B")
                raise SystemExit(1)
            elif self.setup['remove']:
                self.logger.error("-Q option incompatible with -r")
                raise SystemExit(1)
        if 'drivers' in self.setup and self.setup['drivers'] == 'help':
            self.logger.info("The following drivers are available:")
            self.logger.info(Bcfg2.Client.Tools.drivers)
            raise SystemExit(0)
        if self.setup['remove'] and 'services' in self.setup['remove'].lower():
            self.logger.error("Service removal is nonsensical; "
                              "removed services will only be disabled")
        if (self.setup['remove'] and
            self.setup['remove'].lower() not in ['all', 'services', 'packages',
                                                 'users']):
            self.logger.error("Got unknown argument %s for -r" %
                              self.setup['remove'])
        if self.setup["file"] and self.setup["cache"]:
            print("cannot use -f and -c together")
            raise SystemExit(1)
        if not self.setup['server'].startswith('https://'):
            self.setup['server'] = 'https://' + self.setup['server']

    def _probe_failure(self, probename, msg):
        """ handle failure of a probe in the way the user wants us to
        (exit or continue) """
        message = "Failed to execute probe %s: %s" % (probename, msg)
        if self.setup['probe_exit']:
            self.fatal_error(message)
        else:
            self.logger.error(message)

    def run_probe(self, probe):
        """Execute probe."""
        name = probe.get('name')
        self.logger.info("Running probe %s" % name)
        ret = Bcfg2.Client.XML.Element("probe-data",
                                       name=name,
                                       source=probe.get('source'))
        try:
            scripthandle, scriptname = tempfile.mkstemp()
            script = os.fdopen(scripthandle, 'w')
            try:
                script.write("#!%s\n" %
                             (probe.attrib.get('interpreter', '/bin/sh')))
                if sys.hexversion >= 0x03000000:
                    script.write(probe.text)
                else:
                    script.write(probe.text.encode('utf-8'))
                script.close()
                os.chmod(scriptname,
                         stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH |
                         stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH |
                         stat.S_IWUSR)  # 0755
                rv = self.cmd.run(scriptname, timeout=self.setup['timeout'])
                if rv.stderr:
                    self.logger.warning("Probe %s has error output: %s" %
                                        (name, rv.stderr))
                if not rv.success:
                    self._probe_failure(name, "Return value %s" % rv)
                self.logger.info("Probe %s has result:" % name)
                self.logger.info(rv.stdout)
                if sys.hexversion >= 0x03000000:
                    ret.text = rv.stdout
                else:
                    ret.text = rv.stdout.decode('utf-8')
            finally:
                os.unlink(scriptname)
        except SystemExit:
            raise
        except:
            self._probe_failure(name, sys.exc_info()[1])
        return ret

    def fatal_error(self, message):
        """Signal a fatal error."""
        self.logger.error("Fatal error: %s" % (message))
        raise SystemExit(1)

    @property
    def proxy(self):
        """ get an XML-RPC proxy to the server """
        if self._proxy is None:
            self._proxy = Bcfg2.Proxy.ComponentProxy(
                self.setup['server'],
                self.setup['user'],
                self.setup['password'],
                key=self.setup['key'],
                cert=self.setup['certificate'],
                ca=self.setup['ca'],
                allowedServerCNs=self.setup['serverCN'],
                timeout=self.setup['timeout'],
                retries=int(self.setup['retries']),
                delay=int(self.setup['retry_delay']))
        return self._proxy

    def run_probes(self, times=None):
        """ run probes and upload probe data """
        if times is None:
            times = dict()

        try:
            probes = Bcfg2.Client.XML.XML(str(self.proxy.GetProbes()))
        except (Bcfg2.Proxy.ProxyError,
                Bcfg2.Proxy.CertificateError,
                socket.gaierror,
                socket.error):
            err = sys.exc_info()[1]
            self.fatal_error("Failed to download probes from bcfg2: %s" % err)
        except Bcfg2.Client.XML.ParseError:
            err = sys.exc_info()[1]
            self.fatal_error("Server returned invalid probe requests: %s" %
                             err)

        times['probe_download'] = time.time()

        # execute probes
        probedata = Bcfg2.Client.XML.Element("ProbeData")
        for probe in probes.findall(".//probe"):
            probedata.append(self.run_probe(probe))

        if len(probes.findall(".//probe")) > 0:
            try:
                # upload probe responses
                self.proxy.RecvProbeData(
                    Bcfg2.Client.XML.tostring(
                        probedata,
                        xml_declaration=False).decode('utf-8'))
            except Bcfg2.Proxy.ProxyError:
                err = sys.exc_info()[1]
                self.fatal_error("Failed to upload probe data: %s" % err)

        times['probe_upload'] = time.time()

    def get_config(self, times=None):
        """ load the configuration, either from the cached
        configuration file (-f), or from the server """
        if times is None:
            times = dict()

        if self.setup['file']:
            # read config from file
            try:
                self.logger.debug("Reading cached configuration from %s" %
                                  self.setup['file'])
                return open(self.setup['file'], 'r').read()
            except IOError:
                self.fatal_error("Failed to read cached configuration from: %s"
                                 % (self.setup['file']))
        else:
            # retrieve config from server
            if self.setup['profile']:
                try:
                    self.proxy.AssertProfile(self.setup['profile'])
                except Bcfg2.Proxy.ProxyError:
                    err = sys.exc_info()[1]
                    self.fatal_error("Failed to set client profile: %s" % err)

            try:
                self.proxy.DeclareVersion(__version__)
            except xmlrpclib.Fault:
                err = sys.exc_info()[1]
                if (err.faultCode == xmlrpclib.METHOD_NOT_FOUND or
                    (err.faultCode == 7 and
                     err.faultString.startswith("Unknown method"))):
                    self.logger.debug("Server does not support declaring "
                                      "client version")
                else:
                    self.logger.error("Failed to declare version: %s" % err)
            except (Bcfg2.Proxy.ProxyError,
                    Bcfg2.Proxy.CertificateError,
                    socket.gaierror,
                    socket.error):
                err = sys.exc_info()[1]
                self.logger.error("Failed to declare version: %s" % err)

            self.run_probes(times=times)

            if self.setup['decision'] in ['whitelist', 'blacklist']:
                try:
                    self.setup['decision_list'] = \
                        self.proxy.GetDecisionList(self.setup['decision'])
                    self.logger.info("Got decision list from server:")
                    self.logger.info(self.setup['decision_list'])
                except Bcfg2.Proxy.ProxyError:
                    err = sys.exc_info()[1]
                    self.fatal_error("Failed to get decision list: %s" % err)

            try:
                rawconfig = self.proxy.GetConfig().encode('utf-8')
            except Bcfg2.Proxy.ProxyError:
                err = sys.exc_info()[1]
                self.fatal_error("Failed to download configuration from "
                                 "Bcfg2: %s" % err)

            times['config_download'] = time.time()
        return rawconfig

    def run(self):
        """Perform client execution phase."""
        times = {}

        # begin configuration
        times['start'] = time.time()

        self.logger.info("Starting Bcfg2 client run at %s" % times['start'])

        rawconfig = self.get_config(times=times).decode('utf-8')

        if self.setup['cache']:
            try:
                open(self.setup['cache'], 'w').write(rawconfig)
                os.chmod(self.setup['cache'], 33152)
            except IOError:
                self.logger.warning("Failed to write config cache file %s" %
                                    (self.setup['cache']))
            times['caching'] = time.time()

        try:
            self.config = Bcfg2.Client.XML.XML(rawconfig)
        except Bcfg2.Client.XML.ParseError:
            syntax_error = sys.exc_info()[1]
            self.fatal_error("The configuration could not be parsed: %s" %
                             syntax_error)

        times['config_parse'] = time.time()

        if self.config.tag == 'error':
            self.fatal_error("Server error: %s" % (self.config.text))
            return(1)

        if self.setup['bundle_quick']:
            newconfig = Bcfg2.Client.XML.XML('<Configuration/>')
            for bundle in self.config.getchildren():
                if (bundle.tag == 'Bundle' and
                    ((self.setup['bundle'] and
                      bundle.get('name') in self.setup['bundle']) or
                     (self.setup['skipbundle'] and
                      bundle.get('name') not in self.setup['skipbundle']))):
                    newconfig.append(bundle)
            self.config = newconfig

        self.tools = Bcfg2.Client.Frame.Frame(self.config,
                                              self.setup,
                                              times, self.setup['drivers'],
                                              self.setup['dryrun'])

        if not self.setup['omit_lock_check']:
            # check lock here
            try:
                lockfile = open(self.setup['lockfile'], 'w')
                if locked(lockfile.fileno()):
                    self.fatal_error("Another instance of Bcfg2 is running. "
                                     "If you want to bypass the check, run "
                                     "with the %s option" %
                                     Bcfg2.Options.OMIT_LOCK_CHECK.cmd)
            except SystemExit:
                raise
            except:
                lockfile = None
                self.logger.error("Failed to open lockfile %s: %s" %
                                  (self.setup['lockfile'], sys.exc_info()[1]))

        # execute the configuration
        self.tools.Execute()

        if not self.setup['omit_lock_check']:
            # unlock here
            if lockfile:
                try:
                    fcntl.lockf(lockfile.fileno(), fcntl.LOCK_UN)
                    os.remove(self.setup['lockfile'])
                except OSError:
                    self.logger.error("Failed to unlock lockfile %s" %
                                      lockfile.name)

        if not self.setup['file'] and not self.setup['bundle_quick']:
            # upload statistics
            feedback = self.tools.GenerateStats()

            try:
                self.proxy.RecvStats(
                    Bcfg2.Client.XML.tostring(
                        feedback,
                        xml_declaration=False).decode('utf-8'))
            except Bcfg2.Proxy.ProxyError:
                err = sys.exc_info()[1]
                self.logger.error("Failed to upload configuration statistics: "
                                  "%s" % err)
                raise SystemExit(2)

        self.logger.info("Finished Bcfg2 client run at %s" % time.time())

########NEW FILE########
__FILENAME__ = Frame
""" Frame is the Client Framework that verifies and installs entries,
and generates statistics. """

import copy
import time
import fnmatch
import logging
import Bcfg2.Client.Tools
from Bcfg2.Client import prompt
from Bcfg2.Compat import any, all  # pylint: disable=W0622


def matches_entry(entryspec, entry):
    """ Determine if the Decisions-style entry specification matches
    the entry.  Both are tuples of (tag, name).  The entryspec can
    handle the wildcard * in either position. """
    if entryspec == entry:
        return True
    return all(fnmatch.fnmatch(entry[i], entryspec[i]) for i in [0, 1])


def matches_white_list(entry, whitelist):
    """ Return True if (<entry tag>, <entry name>) is in the given
    whitelist. """
    return any(matches_entry(we, (entry.tag, entry.get('name')))
               for we in whitelist)


def passes_black_list(entry, blacklist):
    """ Return True if (<entry tag>, <entry name>) is not in the given
    blacklist. """
    return not any(matches_entry(be, (entry.tag, entry.get('name')))
                   for be in blacklist)


# pylint: disable=W0702
# in frame we frequently want to catch all exceptions, regardless of
# type, so disable the pylint rule that catches that.


class Frame(object):
    """Frame is the container for all Tool objects and state information."""

    def __init__(self, config, setup, times, drivers, dryrun):
        self.config = config
        self.times = times
        self.dryrun = dryrun
        self.times['initialization'] = time.time()
        self.setup = setup
        self.tools = []
        self.states = {}
        self.whitelist = []
        self.blacklist = []
        self.removal = []
        self.logger = logging.getLogger(__name__)
        for driver in drivers[:]:
            if (driver not in Bcfg2.Client.Tools.drivers and
                isinstance(driver, str)):
                self.logger.error("Tool driver %s is not available" % driver)
                drivers.remove(driver)

        tclass = {}
        for tool in drivers:
            if not isinstance(tool, str):
                tclass[time.time()] = tool
            tool_class = "Bcfg2.Client.Tools.%s" % tool
            try:
                tclass[tool] = getattr(__import__(tool_class, globals(),
                                                  locals(), ['*']),
                                       tool)
            except ImportError:
                continue
            except:
                self.logger.error("Tool %s unexpectedly failed to load" % tool,
                                  exc_info=1)

        for tool in list(tclass.values()):
            try:
                self.tools.append(tool(self.logger, setup, config))
            except Bcfg2.Client.Tools.ToolInstantiationError:
                continue
            except:
                self.logger.error("Failed to instantiate tool %s" % tool,
                                  exc_info=1)

        for tool in self.tools[:]:
            for conflict in getattr(tool, 'conflicts', []):
                for item in self.tools:
                    if item.name == conflict:
                        self.tools.remove(item)

        self.logger.info("Loaded tool drivers:")
        self.logger.info([tool.name for tool in self.tools])

        deprecated = [tool.name for tool in self.tools if tool.deprecated]
        if deprecated:
            self.logger.warning("Loaded deprecated tool drivers:")
            self.logger.warning(deprecated)
        experimental = [tool.name for tool in self.tools if tool.experimental]
        if experimental:
            self.logger.info("Loaded experimental tool drivers:")
            self.logger.info(experimental)

        # find entries not handled by any tools
        self.unhandled = [entry for struct in config
                          for entry in struct
                          if entry not in self.handled]

        if self.unhandled:
            self.logger.error("The following entries are not handled by any "
                              "tool:")
            for entry in self.unhandled:
                self.logger.error("%s:%s:%s" % (entry.tag, entry.get('type'),
                                                entry.get('name')))

        self.find_dups(config)

        pkgs = [(entry.get('name'), entry.get('origin'))
                for struct in config
                for entry in struct
                if entry.tag == 'Package']
        if pkgs:
            self.logger.debug("The following packages are specified in bcfg2:")
            self.logger.debug([pkg[0] for pkg in pkgs if pkg[1] is None])
            self.logger.debug("The following packages are prereqs added by "
                              "Packages:")
            self.logger.debug([pkg[0] for pkg in pkgs if pkg[1] == 'Packages'])

    def find_dups(self, config):
        """ Find duplicate entries and warn about them """
        entries = dict()
        for struct in config:
            for entry in struct:
                for tool in self.tools:
                    if tool.handlesEntry(entry):
                        pkey = tool.primarykey(entry)
                        if pkey in entries:
                            entries[pkey] += 1
                        else:
                            entries[pkey] = 1
        multi = [e for e, c in entries.items() if c > 1]
        if multi:
            self.logger.debug("The following entries are included multiple "
                              "times:")
            for entry in multi:
                self.logger.debug(entry)

    def promptFilter(self, msg, entries):
        """Filter a supplied list based on user input."""
        ret = []
        entries.sort(key=lambda e: e.tag + ":" + e.get('name'))
        for entry in entries[:]:
            if entry in self.unhandled:
                # don't prompt for entries that can't be installed
                continue
            if 'qtext' in entry.attrib:
                iprompt = entry.get('qtext')
            else:
                iprompt = msg % (entry.tag, entry.get('name'))
            if prompt(iprompt):
                ret.append(entry)
        return ret

    def __getattr__(self, name):
        if name in ['extra', 'handled', 'modified', '__important__']:
            ret = []
            for tool in self.tools:
                ret += getattr(tool, name)
            return ret
        elif name in self.__dict__:
            return self.__dict__[name]
        raise AttributeError(name)

    def InstallImportant(self):
        """Install important entries

        We also process the decision mode stuff here because we want to prevent
        non-whitelisted/blacklisted 'important' entries from being installed
        prior to determining the decision mode on the client.
        """
        # Need to process decision stuff early so that dryrun mode
        # works with it
        self.whitelist = [entry for entry in self.states
                          if not self.states[entry]]
        if not self.setup['file']:
            if self.setup['decision'] == 'whitelist':
                dwl = self.setup['decision_list']
                w_to_rem = [e for e in self.whitelist
                            if not matches_white_list(e, dwl)]
                if w_to_rem:
                    self.logger.info("In whitelist mode: "
                                     "suppressing installation of:")
                    self.logger.info(["%s:%s" % (e.tag, e.get('name'))
                                      for e in w_to_rem])
                    self.whitelist = [x for x in self.whitelist
                                      if x not in w_to_rem]
            elif self.setup['decision'] == 'blacklist':
                b_to_rem = \
                    [e for e in self.whitelist
                     if not passes_black_list(e, self.setup['decision_list'])]
                if b_to_rem:
                    self.logger.info("In blacklist mode: "
                                     "suppressing installation of:")
                    self.logger.info(["%s:%s" % (e.tag, e.get('name'))
                                      for e in b_to_rem])
                    self.whitelist = [x for x in self.whitelist
                                      if x not in b_to_rem]

        # take care of important entries first
        if not self.dryrun:
            parent_map = dict((c, p)
                              for p in self.config.getiterator()
                              for c in p)
            for cfile in self.config.findall(".//Path"):
                if (cfile.get('name') not in self.__important__ or
                    cfile.get('type') != 'file' or
                    cfile not in self.whitelist):
                    continue
                parent = parent_map[cfile]
                if ((parent.tag == "Bundle" and
                     ((self.setup['bundle'] and
                       parent.get("name") not in self.setup['bundle']) or
                      (self.setup['skipbundle'] and
                       parent.get("name") in self.setup['skipbundle']))) or
                    (parent.tag == "Independent" and
                     (self.setup['bundle'] or self.setup['skipindep']))):
                    continue
                tools = [t for t in self.tools
                         if t.handlesEntry(cfile) and t.canVerify(cfile)]
                if tools:
                    if (self.setup['interactive'] and not
                        self.promptFilter("Install %s: %s? (y/N):", [cfile])):
                        self.whitelist.remove(cfile)
                        continue
                    try:
                        self.states[cfile] = tools[0].InstallPath(cfile)
                        if self.states[cfile]:
                            tools[0].modified.append(cfile)
                    except:
                        self.logger.error("Unexpected tool failure",
                                          exc_info=1)
                    cfile.set('qtext', '')
                    if tools[0].VerifyPath(cfile, []):
                        self.whitelist.remove(cfile)

    def Inventory(self):
        """
           Verify all entries,
           find extra entries,
           and build up workqueues

        """
        # initialize all states
        for struct in self.config.getchildren():
            for entry in struct.getchildren():
                self.states[entry] = False
        for tool in self.tools:
            try:
                tool.Inventory(self.states)
            except:
                self.logger.error("%s.Inventory() call failed:" % tool.name,
                                  exc_info=1)

    def Decide(self):  # pylint: disable=R0912
        """Set self.whitelist based on user interaction."""
        iprompt = "Install %s: %s? (y/N): "
        rprompt = "Remove %s: %s? (y/N): "
        if self.setup['remove']:
            if self.setup['remove'] == 'all':
                self.removal = self.extra
            elif self.setup['remove'].lower() == 'services':
                self.removal = [entry for entry in self.extra
                                if entry.tag == 'Service']
            elif self.setup['remove'].lower() == 'packages':
                self.removal = [entry for entry in self.extra
                                if entry.tag == 'Package']
            elif self.setup['remove'].lower() == 'users':
                self.removal = [entry for entry in self.extra
                                if entry.tag in ['POSIXUser', 'POSIXGroup']]

        candidates = [entry for entry in self.states
                      if not self.states[entry]]

        if self.dryrun:
            if self.whitelist:
                self.logger.info("In dryrun mode: "
                                 "suppressing entry installation for:")
                self.logger.info(["%s:%s" % (entry.tag, entry.get('name'))
                                  for entry in self.whitelist])
                self.whitelist = []
            if self.removal:
                self.logger.info("In dryrun mode: "
                                 "suppressing entry removal for:")
                self.logger.info(["%s:%s" % (entry.tag, entry.get('name'))
                                  for entry in self.removal])
            self.removal = []

        # Here is where most of the work goes
        # first perform bundle filtering
        all_bundle_names = [b.get('name')
                            for b in self.config.findall('./Bundle')]
        bundles = self.config.getchildren()
        if self.setup['bundle']:
            # warn if non-existent bundle given
            for bundle in self.setup['bundle']:
                if bundle not in all_bundle_names:
                    self.logger.info("Warning: Bundle %s not found" % bundle)
            bundles = [b for b in bundles
                       if b.get('name') in self.setup['bundle']]
        elif self.setup['indep']:
            bundles = [b for b in bundles if b.tag != 'Bundle']
        if self.setup['skipbundle']:
            # warn if non-existent bundle given
            if not self.setup['bundle_quick']:
                for bundle in self.setup['skipbundle']:
                    if bundle not in all_bundle_names:
                        self.logger.info("Warning: Bundle %s not found" %
                                         bundle)
            bundles = [b for b in bundles
                       if b.get('name') not in self.setup['skipbundle']]
        if self.setup['skipindep']:
            bundles = [b for b in bundles if b.tag == 'Bundle']

        self.whitelist = [e for e in self.whitelist
                          if any(e in b for b in bundles)]

        # first process prereq actions
        for bundle in bundles[:]:
            if bundle.tag != 'Bundle':
                continue
            bmodified = len([item for item in bundle
                             if item in self.whitelist or
                             item in self.modified])
            actions = [a for a in bundle.findall('./Action')
                       if (a.get('timing') != 'post' and
                           (bmodified or a.get('when') == 'always'))]
            # now we process all "pre" and "both" actions that are either
            # always or the bundle has been modified
            if self.setup['interactive']:
                self.promptFilter(iprompt, actions)
            self.DispatchInstallCalls(actions)

            # need to test to fail entries in whitelist
            if False in [self.states[a] for a in actions]:
                # then display bundles forced off with entries
                self.logger.info("Bundle %s failed prerequisite action" %
                                 (bundle.get('name')))
                bundles.remove(bundle)
                b_to_remv = [ent for ent in self.whitelist if ent in bundle]
                if b_to_remv:
                    self.logger.info("Not installing entries from Bundle %s" %
                                     (bundle.get('name')))
                    self.logger.info(["%s:%s" % (e.tag, e.get('name'))
                                      for e in b_to_remv])
                    for ent in b_to_remv:
                        self.whitelist.remove(ent)

        self.logger.debug("Installing entries in the following bundle(s):")
        self.logger.debug("  %s" % ", ".join(b.get("name") for b in bundles
                                             if b.get("name")))

        if self.setup['interactive']:
            self.whitelist = self.promptFilter(iprompt, self.whitelist)
            self.removal = self.promptFilter(rprompt, self.removal)

        for entry in candidates:
            if entry not in self.whitelist:
                self.blacklist.append(entry)

    def DispatchInstallCalls(self, entries):
        """Dispatch install calls to underlying tools."""
        for tool in self.tools:
            handled = [entry for entry in entries if tool.canInstall(entry)]
            if not handled:
                continue
            try:
                tool.Install(handled, self.states)
            except:
                self.logger.error("%s.Install() call failed:" % tool.name,
                                  exc_info=1)

    def Install(self):
        """Install all entries."""
        self.DispatchInstallCalls(self.whitelist)
        mods = self.modified
        mbundles = [struct for struct in self.config.findall('Bundle')
                    if any(True for mod in mods if mod in struct)]

        if self.modified:
            # Handle Bundle interdeps
            if mbundles:
                self.logger.info("The Following Bundles have been modified:")
                self.logger.info([mbun.get('name') for mbun in mbundles])
            tbm = [(t, b) for t in self.tools for b in mbundles]
            for tool, bundle in tbm:
                try:
                    tool.Inventory(self.states, [bundle])
                except:
                    self.logger.error("%s.Inventory() call failed:" %
                                      tool.name,
                                      exc_info=1)
            clobbered = [entry for bundle in mbundles for entry in bundle
                         if (not self.states[entry] and
                             entry not in self.blacklist)]
            if clobbered:
                self.logger.debug("Found clobbered entries:")
                self.logger.debug(["%s:%s" % (entry.tag, entry.get('name'))
                                   for entry in clobbered])
                if not self.setup['interactive']:
                    self.DispatchInstallCalls(clobbered)

        for bundle in self.config.findall('.//Bundle'):
            if (self.setup['bundle'] and
                bundle.get('name') not in self.setup['bundle']):
                # prune out unspecified bundles when running with -b
                continue
            if bundle in mbundles:
                self.logger.debug("Bundle %s was modified" %
                                  bundle.get('name'))
                func = "BundleUpdated"
            else:
                self.logger.debug("Bundle %s was not modified" %
                                  bundle.get('name'))
                func = "BundleNotUpdated"
            for tool in self.tools:
                try:
                    getattr(tool, func)(bundle, self.states)
                except:
                    self.logger.error("%s.%s() call failed:" %
                                      (tool.name, func), exc_info=1)

    def Remove(self):
        """Remove extra entries."""
        for tool in self.tools:
            extras = [entry for entry in self.removal
                      if tool.handlesEntry(entry)]
            if extras:
                try:
                    tool.Remove(extras)
                except:
                    self.logger.error("%s.Remove() failed" % tool.name,
                                      exc_info=1)

    def CondDisplayState(self, phase):
        """Conditionally print tracing information."""
        self.logger.info('Phase: %s' % phase)
        self.logger.info('Correct entries:        %d' %
                         list(self.states.values()).count(True))
        self.logger.info('Incorrect entries:      %d' %
                         list(self.states.values()).count(False))
        if phase == 'final' and list(self.states.values()).count(False):
            for entry in sorted(self.states.keys(), key=lambda e: e.tag + ":" +
                                e.get('name')):
                if not self.states[entry]:
                    etype = entry.get('type')
                    if etype:
                        self.logger.info("%s:%s:%s" % (entry.tag, etype,
                                                       entry.get('name')))
                    else:
                        self.logger.info("%s:%s" % (entry.tag,
                                                    entry.get('name')))
        self.logger.info('Total managed entries:  %d' %
                         len(list(self.states.values())))
        self.logger.info('Unmanaged entries:      %d' % len(self.extra))
        if phase == 'final' and self.setup['extra']:
            for entry in sorted(self.extra, key=lambda e: e.tag + ":" +
                                e.get('name')):
                etype = entry.get('type')
                if etype:
                    self.logger.info("%s:%s:%s" % (entry.tag, etype,
                                                   entry.get('name')))
                else:
                    self.logger.info("%s:%s" % (entry.tag,
                                                entry.get('name')))

        if ((list(self.states.values()).count(False) == 0) and not self.extra):
            self.logger.info('All entries correct.')

    def ReInventory(self):
        """Recheck everything."""
        if not self.dryrun and self.setup['kevlar']:
            self.logger.info("Rechecking system inventory")
            self.Inventory()

    def Execute(self):
        """Run all methods."""
        self.Inventory()
        self.times['inventory'] = time.time()
        self.CondDisplayState('initial')
        self.InstallImportant()
        self.Decide()
        self.Install()
        self.times['install'] = time.time()
        self.Remove()
        self.times['remove'] = time.time()
        if self.modified:
            self.ReInventory()
            self.times['reinventory'] = time.time()
        self.times['finished'] = time.time()
        self.CondDisplayState('final')

    def GenerateStats(self):
        """Generate XML summary of execution statistics."""
        feedback = Bcfg2.Client.XML.Element("upload-statistics")
        stats = Bcfg2.Client.XML.SubElement(
            feedback,
            'Statistics',
            total=str(len(self.states)),
            version='2.0',
            revision=self.config.get('revision', '-1'))
        good_entries = [key for key, val in list(self.states.items()) if val]
        good = len(good_entries)
        stats.set('good', str(good))
        if any(not val for val in list(self.states.values())):
            stats.set('state', 'dirty')
        else:
            stats.set('state', 'clean')

        # List bad elements of the configuration
        for (data, ename) in [(self.modified, 'Modified'),
                              (self.extra, "Extra"),
                              (good_entries, "Good"),
                              ([entry for entry in self.states
                                if not self.states[entry]], "Bad")]:
            container = Bcfg2.Client.XML.SubElement(stats, ename)
            for item in data:
                item.set('qtext', '')
                container.append(copy.deepcopy(item))
                item.text = None

        timeinfo = Bcfg2.Client.XML.Element("OpStamps")
        feedback.append(stats)
        for (event, timestamp) in list(self.times.items()):
            timeinfo.set(event, str(timestamp))
        stats.append(timeinfo)
        return feedback

########NEW FILE########
__FILENAME__ = Action
"""Action driver"""

import os
import sys
import select
import Bcfg2.Client.Tools
from Bcfg2.Client.Frame import matches_white_list, passes_black_list
from Bcfg2.Compat import input  # pylint: disable=W0622


class Action(Bcfg2.Client.Tools.Tool):
    """Implement Actions"""
    name = 'Action'
    __handles__ = [('PostInstall', None), ('Action', None)]
    __req__ = {'PostInstall': ['name'],
               'Action': ['name', 'timing', 'when', 'command', 'status']}

    def _action_allowed(self, action):
        """ Return true if the given action is allowed to be run by
        the whitelist or blacklist """
        if self.setup['decision'] == 'whitelist' and \
           not matches_white_list(action, self.setup['decision_list']):
            self.logger.info("In whitelist mode: suppressing Action: %s" %
                             action.get('name'))
            return False
        if self.setup['decision'] == 'blacklist' and \
           not passes_black_list(action, self.setup['decision_list']):
            self.logger.info("In blacklist mode: suppressing Action: %s" %
                             action.get('name'))
            return False
        return True

    def RunAction(self, entry):
        """This method handles command execution and status return."""
        shell = False
        shell_string = ''
        if entry.get('shell', 'false') == 'true':
            shell = True
            shell_string = '(in shell) '

        if not self.setup['dryrun']:
            if self.setup['interactive']:
                prompt = ('Run Action %s%s, %s: (y/N): ' %
                          (shell_string, entry.get('name'),
                           entry.get('command')))
                # flush input buffer
                while len(select.select([sys.stdin.fileno()], [], [],
                                        0.0)[0]) > 0:
                    os.read(sys.stdin.fileno(), 4096)
                ans = input(prompt)
                if ans not in ['y', 'Y']:
                    return False
            if self.setup['servicemode'] == 'build':
                if entry.get('build', 'true') == 'false':
                    self.logger.debug("Action: Deferring execution of %s due "
                                      "to build mode" % entry.get('command'))
                    return False
            self.logger.debug("Running Action %s %s" %
                              (shell_string, entry.get('name')))
            rv = self.cmd.run(entry.get('command'), shell=shell)
            self.logger.debug("Action: %s got return code %s" %
                              (entry.get('command'), rv.retval))
            entry.set('rc', str(rv.retval))
            return entry.get('status', 'check') == 'ignore' or rv.success
        else:
            self.logger.debug("In dryrun mode: not running action: %s" %
                              (entry.get('name')))
            return False

    def VerifyAction(self, dummy, _):
        """Actions always verify true."""
        return True

    def VerifyPostInstall(self, dummy, _):
        """Actions always verify true."""
        return True

    def InstallAction(self, entry):
        """Run actions as pre-checks for bundle installation."""
        if entry.get('timing') != 'post':
            return self.RunAction(entry)
        return True

    def InstallPostInstall(self, entry):
        """ Install a deprecated PostInstall entry """
        self.logger.warning("Installing deprecated PostInstall entry %s" %
                            entry.get("name"))
        return self.InstallAction(entry)

    def BundleUpdated(self, bundle, states):
        """Run postinstalls when bundles have been updated."""
        for postinst in bundle.findall("PostInstall"):
            if not self._action_allowed(postinst):
                continue
            self.cmd.run(postinst.get('name'))
        for action in bundle.findall("Action"):
            if action.get('timing') in ['post', 'both']:
                if not self._action_allowed(action):
                    continue
                states[action] = self.RunAction(action)

    def BundleNotUpdated(self, bundle, states):
        """Run Actions when bundles have not been updated."""
        for action in bundle.findall("Action"):
            if action.get('timing') in ['post', 'both'] and \
               action.get('when') != 'modified':
                if not self._action_allowed(action):
                    continue
                states[action] = self.RunAction(action)

########NEW FILE########
__FILENAME__ = APK
"""This provides Bcfg2 support for Alpine Linux APK packages."""

import Bcfg2.Client.Tools


class APK(Bcfg2.Client.Tools.PkgTool):
    """Support for Apk packages."""
    name = 'APK'
    __execs__ = ["/sbin/apk"]
    __handles__ = [('Package', 'apk')]
    __req__ = {'Package': ['name', 'version']}
    pkgtype = 'apk'
    pkgtool = ("/sbin/apk add %s", ("%s", ["name"]))

    def __init__(self, logger, setup, config):
        Bcfg2.Client.Tools.PkgTool.__init__(self, logger, setup, config)
        self.installed = {}
        self.RefreshPackages()

    def RefreshPackages(self):
        """Refresh memory hashes of packages."""
        names = self.cmd.run("/sbin/apk info").stdout.splitlines()
        nameversions = self.cmd.run("/sbin/apk info -v").stdout.splitlines()
        for pkg in zip(names, nameversions):
            pkgname = pkg[0]
            version = pkg[1][len(pkgname) + 1:]
            self.logger.debug(" pkgname: %s" % pkgname)
            self.logger.debug(" version: %s" % version)
            self.installed[pkgname] = version

    def VerifyPackage(self, entry, _):
        """Verify Package status for entry."""
        if 'version' not in entry.attrib:
            self.logger.info("Cannot verify unversioned package %s" %
                             entry.attrib['name'])
            return False

        if entry.attrib['name'] in self.installed:
            if entry.attrib['version'] in \
                    ['auto', self.installed[entry.attrib['name']]]:
                # if (not self.setup['quick'] and
                #     entry.get('verify', 'true') == 'true'):
                # FIXME: Does APK have any sort of verification mechanism?
                return True
            else:
                self.logger.info(" pkg %s at version %s, not %s" %
                                 (entry.attrib['name'],
                                  self.installed[entry.attrib['name']],
                                  entry.attrib['version']))
                entry.set('current_version', self.installed[entry.get('name')])
                return False
        entry.set('current_exists', 'false')
        return False

    def Remove(self, packages):
        """Remove extra packages."""
        names = [pkg.get('name') for pkg in packages]
        self.logger.info("Removing packages: %s" % " ".join(names))
        self.cmd.run("/sbin/apk del %s" % " ".join(names))
        self.RefreshPackages()
        self.extra = self.FindExtra()

########NEW FILE########
__FILENAME__ = APT
"""This is the Bcfg2 support for apt-get."""

# suppress apt API warnings
import warnings
warnings.filterwarnings("ignore", "apt API not stable yet",
                        FutureWarning)
import apt.cache
import os
import Bcfg2.Client.Tools

class APT(Bcfg2.Client.Tools.Tool):
    """The Debian toolset implements package and service operations and inherits
    the rest from Toolset.Toolset.

    """
    name = 'APT'
    __execs__ = []
    __handles__ = [('Package', 'deb'), ('Path', 'ignore')]
    __req__ = {'Package': ['name', 'version'], 'Path': ['type']}

    def __init__(self, logger, setup, config):
        Bcfg2.Client.Tools.Tool.__init__(self, logger, setup, config)

        self.install_path = setup.get('apt_install_path', '/usr')
        self.var_path = setup.get('apt_var_path', '/var')
        self.etc_path = setup.get('apt_etc_path', '/etc')
        self.debsums = '%s/bin/debsums' % self.install_path
        self.aptget = '%s/bin/apt-get' % self.install_path
        self.dpkg = '%s/bin/dpkg' % self.install_path
        self.__execs__ = [self.debsums, self.aptget, self.dpkg]

        path_entries = os.environ['PATH'].split(':')
        for reqdir in ['/sbin', '/usr/sbin']:
            if reqdir not in path_entries:
                os.environ['PATH'] = os.environ['PATH'] + ':' + reqdir
        self.pkgcmd = '%s ' % self.aptget + \
                      '-o DPkg::Options::=--force-confold ' + \
                      '-o DPkg::Options::=--force-confmiss ' + \
                      '--reinstall ' + \
                      '--force-yes '
        if not self.setup['debug']:
            self.pkgcmd += '-q=2 '
        self.pkgcmd += '-y install %s'
        self.ignores = [entry.get('name') for struct in config \
                        for entry in struct \
                        if entry.tag == 'Path' and \
                        entry.get('type') == 'ignore']
        self.__important__ = self.__important__ + \
                             ["%s/cache/debconf/config.dat" % self.var_path,
                              "%s/cache/debconf/templates.dat" % self.var_path,
                              '/etc/passwd', '/etc/group',
                              '%s/apt/apt.conf' % self.etc_path,
                              '%s/dpkg/dpkg.cfg' % self.etc_path] + \
                             [entry.get('name') for struct in config for entry in struct \
                              if entry.tag == 'Path' and \
                              entry.get('name').startswith('%s/apt/sources.list' % self.etc_path)]
        self.nonexistent = [entry.get('name') for struct in config for entry in struct \
                              if entry.tag == 'Path' and entry.get('type') == 'nonexistent']
        os.environ["DEBIAN_FRONTEND"] = 'noninteractive'
        self.actions = {}
        if self.setup['kevlar'] and not self.setup['dryrun']:
            self.cmd.run("%s --force-confold --configure --pending" %
                         self.dpkg)
            self.cmd.run("%s clean" % self.aptget)
            try:
                self.pkg_cache = apt.cache.Cache()
            except SystemError:
                e = sys.exc_info()[1]
                self.logger.info("Failed to initialize APT cache: %s" % e)
                raise Bcfg2.Client.Tools.ToolInstantiationError
            self.pkg_cache.update()
        self.pkg_cache = apt.cache.Cache()
        if 'req_reinstall_pkgs' in dir(self.pkg_cache):
            self._newapi = True
        else:
            self._newapi = False

    def FindExtra(self):
        """Find extra packages."""
        packages = [entry.get('name') for entry in self.getSupportedEntries()]
        if self._newapi:
            extras = [(p.name, p.installed.version) for p in self.pkg_cache
                      if p.is_installed and p.name not in packages]
        else:
            extras = [(p.name, p.installedVersion) for p in self.pkg_cache
                      if p.isInstalled and p.name not in packages]
        return [Bcfg2.Client.XML.Element('Package', name=name, \
                                         type='deb', version=version) \
                                         for (name, version) in extras]

    def VerifyDebsums(self, entry, modlist):
        output = \
            self.cmd.run("%s -as %s" %
                         (self.debsums, entry.get('name'))).stdout.splitlines()
        if len(output) == 1 and "no md5sums for" in output[0]:
            self.logger.info("Package %s has no md5sums. Cannot verify" % \
                             entry.get('name'))
            entry.set('qtext',
                      "Reinstall Package %s-%s to setup md5sums? (y/N) " %
                      (entry.get('name'), entry.get('version')))
            return False
        files = []
        for item in output:
            if "checksum mismatch" in item:
                files.append(item.split()[-1])
            elif "changed file" in item:
                files.append(item.split()[3])
            elif "can't open" in item:
                if item.split()[5] not in self.nonexistent:
                    files.append(item.split()[5])
            elif "missing file" in item and \
                 item.split()[3] in self.nonexistent:
                # these files should not exist
                continue
            elif "is not installed" in item or "missing file" in item:
                self.logger.error("Package %s is not fully installed" \
                                  % entry.get('name'))
            else:
                self.logger.error("Got Unsupported pattern %s from debsums" \
                                  % item)
                files.append(item)
        files = list(set(files) - set(self.ignores))
        # We check if there is file in the checksum to do
        if files:
            # if files are found there we try to be sure our modlist is sane
            # with erroneous symlinks
            modlist = [os.path.realpath(filename) for filename in modlist]
            bad = [filename for filename in files if filename not in modlist]
            if bad:
                self.logger.debug("It is suggested that you either manage these "
                                  "files, revert the changes, or ignore false "
                                  "failures:")
                self.logger.info("Package %s failed validation. Bad files are:" % \
                                 entry.get('name'))
                self.logger.info(bad)
                entry.set('qtext',
                          "Reinstall Package %s-%s to fix failing files? (y/N) " % \
                          (entry.get('name'), entry.get('version')))
                return False
        return True

    def VerifyPackage(self, entry, modlist, checksums=True):
        """Verify package for entry."""
        if not 'version' in entry.attrib:
            self.logger.info("Cannot verify unversioned package %s" %
                             (entry.attrib['name']))
            return False
        pkgname = entry.get('name')
        if self.pkg_cache.has_key(pkgname):
            if self._newapi:
                is_installed = self.pkg_cache[pkgname].is_installed
            else:
                is_installed = self.pkg_cache[pkgname].isInstalled
        if not self.pkg_cache.has_key(pkgname) or not is_installed:
            self.logger.info("Package %s not installed" % (entry.get('name')))
            entry.set('current_exists', 'false')
            return False

        pkg = self.pkg_cache[pkgname]
        if self._newapi:
            installed_version = pkg.installed.version
            candidate_version = pkg.candidate.version
        else:
            installed_version = pkg.installedVersion
            candidate_version = pkg.candidateVersion
        if entry.get('version') == 'auto':
            if self._newapi:
                is_upgradable = self.pkg_cache._depcache.is_upgradable(pkg._pkg)
            else:
                is_upgradable = self.pkg_cache._depcache.IsUpgradable(pkg._pkg)
            if is_upgradable:
                desiredVersion = candidate_version
            else:
                desiredVersion = installed_version
        elif entry.get('version') == 'any':
            desiredVersion = installed_version
        else:
            desiredVersion = entry.get('version')
        if desiredVersion != installed_version:
            entry.set('current_version', installed_version)
            entry.set('qtext', "Modify Package %s (%s -> %s)? (y/N) " % \
                      (entry.get('name'), entry.get('current_version'),
                       desiredVersion))
            return False
        else:
            # version matches
            if not self.setup['quick'] and entry.get('verify', 'true') == 'true' \
                   and checksums:
                pkgsums = self.VerifyDebsums(entry, modlist)
                return pkgsums
            return True

    def Remove(self, packages):
        """Deal with extra configuration detected."""
        pkgnames = " ".join([pkg.get('name') for pkg in packages])
        self.pkg_cache = apt.cache.Cache()
        if len(packages) > 0:
            self.logger.info('Removing packages:')
            self.logger.info(pkgnames)
            for pkg in pkgnames.split(" "):
                try:
                    if self._newapi:
                        self.pkg_cache[pkg].mark_delete(purge=True)
                    else:
                        self.pkg_cache[pkg].markDelete(purge=True)
                except:
                    if self._newapi:
                        self.pkg_cache[pkg].mark_delete()
                    else:
                        self.pkg_cache[pkg].markDelete()
            try:
                self.pkg_cache.commit()
            except SystemExit:
                # thank you python-apt 0.6
                pass
            self.pkg_cache = apt.cache.Cache()
            self.modified += packages
            self.extra = self.FindExtra()

    def Install(self, packages, states):
        # it looks like you can't install arbitrary versions of software
        # out of the pkg cache, we will still need to call apt-get
        ipkgs = []
        bad_pkgs = []
        for pkg in packages:
            if not self.pkg_cache.has_key(pkg.get('name')):
                self.logger.error("APT has no information about package %s" % (pkg.get('name')))
                continue
            if pkg.get('version') in ['auto', 'any']:
                if self._newapi:
                    try:
                        ipkgs.append("%s=%s" % (pkg.get('name'),
                                                self.pkg_cache[pkg.get('name')].candidate.version))
                    except AttributeError:
                        self.logger.error("Failed to find %s in apt package cache" %
                                          pkg.get('name'))
                        continue
                else:
                    ipkgs.append("%s=%s" % (pkg.get('name'),
                                            self.pkg_cache[pkg.get('name')].candidateVersion))
                continue
            if self._newapi:
                avail_vers = [x.ver_str for x in \
                              self.pkg_cache[pkg.get('name')]._pkg.version_list]
            else:
                avail_vers = [x.VerStr for x in \
                              self.pkg_cache[pkg.get('name')]._pkg.VersionList]
            if pkg.get('version') in avail_vers:
                ipkgs.append("%s=%s" % (pkg.get('name'), pkg.get('version')))
                continue
            else:
                self.logger.error("Package %s: desired version %s not in %s" \
                                  % (pkg.get('name'), pkg.get('version'),
                                     avail_vers))
            bad_pkgs.append(pkg.get('name'))
        if bad_pkgs:
            self.logger.error("Cannot find correct versions of packages:")
            self.logger.error(bad_pkgs)
        if not ipkgs:
            return
        if not self.cmd.run(self.pkgcmd % (" ".join(ipkgs))):
            self.logger.error("APT command failed")
        self.pkg_cache = apt.cache.Cache()
        self.extra = self.FindExtra()
        for package in packages:
            states[package] = self.VerifyPackage(package, [], checksums=False)
            if states[package]:
                self.modified.append(package)

    def VerifyPath(self, entry, _):
        """Do nothing here since we only verify Path type=ignore."""
        return True

########NEW FILE########
__FILENAME__ = Blast
"""This provides Bcfg2 support for Blastwave."""

import tempfile
import Bcfg2.Client.Tools.SYSV


class Blast(Bcfg2.Client.Tools.SYSV.SYSV):
    """Support for Blastwave packages."""
    pkgtype = 'blast'
    pkgtool = ("/opt/csw/bin/pkg-get install %s", ("%s", ["bname"]))
    name = 'Blast'
    __execs__ = ['/opt/csw/bin/pkg-get', "/usr/bin/pkginfo"]
    __handles__ = [('Package', 'blast')]
    __req__ = {'Package': ['name', 'version', 'bname']}

    def __init__(self, logger, setup, config):
        # dont use the sysv constructor
        Bcfg2.Client.Tools.PkgTool.__init__(self, logger, setup, config)
        noaskfile = tempfile.NamedTemporaryFile()
        self.noaskname = noaskfile.name
        try:
            noaskfile.write(Bcfg2.Client.Tools.SYSV.noask)
        except:
            pass

    # VerifyPackage comes from Bcfg2.Client.Tools.SYSV
    # Install comes from Bcfg2.Client.Tools.PkgTool
    # Extra comes from Bcfg2.Client.Tools.Tool
    # Remove comes from Bcfg2.Client.Tools.SYSV
    def FindExtra(self):
        """Pass through to null FindExtra call."""
        return []

########NEW FILE########
__FILENAME__ = Chkconfig
# This is the bcfg2 support for chkconfig

"""This is chkconfig support."""

import os

import Bcfg2.Client.Tools
import Bcfg2.Client.XML


class Chkconfig(Bcfg2.Client.Tools.SvcTool):
    """Chkconfig support for Bcfg2."""
    name = 'Chkconfig'
    __execs__ = ['/sbin/chkconfig']
    __handles__ = [('Service', 'chkconfig')]
    __req__ = {'Service': ['name', 'status']}
    os.environ['LC_ALL'] = 'C'

    def get_svc_command(self, service, action):
        return "/sbin/service %s %s" % (service.get('name'), action)

    def verify_bootstatus(self, entry, bootstatus):
        """Verify bootstatus for entry."""
        rv = self.cmd.run("/sbin/chkconfig --list %s " % entry.get('name'))
        if rv.success:
            srvdata = rv.stdout.splitlines()[0].split()
        else:
            # service not installed
            entry.set('current_bootstatus', 'service not installed')
            return False

        if len(srvdata) == 2:
            # This is an xinetd service
            if bootstatus == srvdata[1]:
                return True
            else:
                entry.set('current_bootstatus', srvdata[1])
                return False

        try:
            onlevels = [level.split(':')[0] for level in srvdata[1:]
                        if level.split(':')[1] == 'on']
        except IndexError:
            onlevels = []

        if bootstatus == 'on':
            current_bootstatus = (len(onlevels) > 0)
        else:
            current_bootstatus = (len(onlevels) == 0)
        return current_bootstatus

    def VerifyService(self, entry, _):
        """Verify Service status for entry."""
        entry.set('target_status', entry.get('status'))  # for reporting
        bootstatus = self.get_bootstatus(entry)
        if bootstatus is None:
            return True
        current_bootstatus = self.verify_bootstatus(entry, bootstatus)

        if entry.get('status') == 'ignore':
            # 'ignore' should verify
            current_svcstatus = True
            svcstatus = True
        else:
            svcstatus = self.check_service(entry)
            if entry.get('status') == 'on':
                if svcstatus:
                    current_svcstatus = True
                else:
                    current_svcstatus = False
            elif entry.get('status') == 'off':
                if svcstatus:
                    current_svcstatus = False
                else:
                    current_svcstatus = True

        if svcstatus:
            entry.set('current_status', 'on')
        else:
            entry.set('current_status', 'off')

        return current_bootstatus and current_svcstatus

    def InstallService(self, entry):
        """Install Service entry."""
        self.cmd.run("/sbin/chkconfig --add %s" % (entry.get('name')))
        self.logger.info("Installing Service %s" % (entry.get('name')))
        bootstatus = self.get_bootstatus(entry)
        if bootstatus is not None:
            if bootstatus == 'on':
                # make sure service is enabled on boot
                bootcmd = '/sbin/chkconfig %s %s' % \
                          (entry.get('name'), bootstatus)
            elif bootstatus == 'off':
                # make sure service is disabled on boot
                bootcmd = '/sbin/chkconfig %s %s' % (entry.get('name'),
                                                     bootstatus)
            bootcmdrv = self.cmd.run(bootcmd).success
            if self.setup['servicemode'] == 'disabled':
                # 'disabled' means we don't attempt to modify running svcs
                return bootcmdrv
            buildmode = self.setup['servicemode'] == 'build'
            if (entry.get('status') == 'on' and not buildmode) and \
               entry.get('current_status') == 'off':
                svccmdrv = self.start_service(entry)
            elif (entry.get('status') == 'off' or buildmode) and \
                    entry.get('current_status') == 'on':
                svccmdrv = self.stop_service(entry)
            else:
                svccmdrv = True  # ignore status attribute
            return bootcmdrv and svccmdrv
        else:
            # when bootstatus is 'None', status == 'ignore'
            return True

    def FindExtra(self):
        """Locate extra chkconfig Services."""
        allsrv = [line.split()[0]
                  for line in
                  self.cmd.run("/sbin/chkconfig --list").stdout.splitlines()
                  if ":on" in line]
        self.logger.debug('Found active services:')
        self.logger.debug(allsrv)
        specified = [srv.get('name') for srv in self.getSupportedEntries()]
        return [Bcfg2.Client.XML.Element('Service', type='chkconfig',
                                         name=name)
                for name in allsrv if name not in specified]

########NEW FILE########
__FILENAME__ = DebInit
"""Debian Init Support for Bcfg2"""

import glob
import os
import re
import Bcfg2.Client.Tools

# Debian squeeze and beyond uses a dependecy based boot sequence
DEBIAN_OLD_STYLE_BOOT_SEQUENCE = ('etch', '4.0', 'lenny')


class DebInit(Bcfg2.Client.Tools.SvcTool):
    """Debian Service Support for Bcfg2."""
    name = 'DebInit'
    __execs__ = ['/usr/sbin/update-rc.d', '/usr/sbin/invoke-rc.d']
    __handles__ = [('Service', 'deb')]
    __req__ = {'Service': ['name', 'status']}
    svcre = \
        re.compile(r'/etc/.*/(?P<action>[SK])(?P<sequence>\d+)(?P<name>\S+)')

    def get_svc_command(self, service, action):
        return '/usr/sbin/invoke-rc.d %s %s' % (service.get('name'), action)

    def verify_bootstatus(self, entry, bootstatus):
        """Verify bootstatus for entry."""
        rawfiles = glob.glob("/etc/rc*.d/[SK]*%s" % (entry.get('name')))
        files = []

        try:
            deb_version = open('/etc/debian_version').read().split('/', 1)[0]
        except IOError:
            deb_version = 'unknown'

        if entry.get('sequence'):
            if (deb_version in DEBIAN_OLD_STYLE_BOOT_SEQUENCE or
                deb_version.startswith('5') or
                os.path.exists('/etc/init.d/.legacy-bootordering')):
                start_sequence = int(entry.get('sequence'))
                kill_sequence = 100 - start_sequence
            else:
                start_sequence = None
                self.logger.warning("Your debian version boot sequence is "
                                    "dependency based \"sequence\" attribute "
                                    "will be ignored.")
        else:
            start_sequence = None

        for filename in rawfiles:
            match = self.svcre.match(filename)
            if not match:
                self.logger.error("Failed to match file: %s" % filename)
                continue
            if match.group('name') == entry.get('name'):
                files.append(filename)
        if bootstatus == 'off':
            if files:
                entry.set('current_bootstatus', 'on')
                return False
            else:
                return True
        elif files:
            if start_sequence:
                for filename in files:
                    match = self.svcre.match(filename)
                    file_sequence = int(match.group('sequence'))
                    if ((match.group('action') == 'S' and
                         file_sequence != start_sequence) or
                        (match.group('action') == 'K' and
                         file_sequence != kill_sequence)):
                        return False
            return True
        else:
            entry.set('current_bootstatus', 'off')
            return False

    def VerifyService(self, entry, _):
        """Verify Service status for entry."""
        entry.set('target_status', entry.get('status'))  # for reporting
        bootstatus = self.get_bootstatus(entry)
        if bootstatus is None:
            return True
        current_bootstatus = self.verify_bootstatus(entry, bootstatus)

        if entry.get('status') == 'ignore':
            # 'ignore' should verify
            current_svcstatus = True
            svcstatus = True
        else:
            svcstatus = self.check_service(entry)
            if entry.get('status') == 'on':
                if svcstatus:
                    current_svcstatus = True
                else:
                    current_svcstatus = False
            elif entry.get('status') == 'off':
                if svcstatus:
                    current_svcstatus = False
                else:
                    current_svcstatus = True

        if svcstatus:
            entry.set('current_status', 'on')
        else:
            entry.set('current_status', 'off')

        return current_bootstatus and current_svcstatus

    def InstallService(self, entry):
        """Install Service entry."""
        self.logger.info("Installing Service %s" % (entry.get('name')))
        bootstatus = self.get_bootstatus(entry)

        # check if init script exists
        try:
            os.stat('/etc/init.d/%s' % entry.get('name'))
        except OSError:
            self.logger.debug("Init script for service %s does not exist" %
                              entry.get('name'))
            return False

        if bootstatus is not None:
            seqcmdrv = True
            if bootstatus == 'on':
                # make sure service is enabled on boot
                bootcmd = '/usr/sbin/update-rc.d %s defaults' % \
                          entry.get('name')
                if entry.get('sequence'):
                    seqcmd = '/usr/sbin/update-rc.d -f %s remove' % \
                             entry.get('name')
                    seqcmdrv = self.cmd.run(seqcmd)
                    start_sequence = int(entry.get('sequence'))
                    kill_sequence = 100 - start_sequence
                    bootcmd = '%s %d %d' % (bootcmd, start_sequence,
                                            kill_sequence)
            elif bootstatus == 'off':
                # make sure service is disabled on boot
                bootcmd = '/usr/sbin/update-rc.d -f %s remove' % \
                          entry.get('name')
            bootcmdrv = self.cmd.run(bootcmd)
            if self.setup['servicemode'] == 'disabled':
                # 'disabled' means we don't attempt to modify running svcs
                return bootcmdrv and seqcmdrv
            buildmode = self.setup['servicemode'] == 'build'
            if (entry.get('status') == 'on' and not buildmode) and \
               entry.get('current_status') == 'off':
                svccmdrv = self.start_service(entry)
            elif (entry.get('status') == 'off' or buildmode) and \
                    entry.get('current_status') == 'on':
                svccmdrv = self.stop_service(entry)
            else:
                svccmdrv = True  # ignore status attribute
            return bootcmdrv and svccmdrv and seqcmdrv
        else:
            # when bootstatus is 'None', status == 'ignore'
            return True

    def FindExtra(self):
        """Find Extra Debian Service entries."""
        specified = [entry.get('name') for entry in self.getSupportedEntries()]
        extra = set()
        for fname in glob.glob("/etc/rc[12345].d/S*"):
            name = self.svcre.match(fname).group('name')
            if name not in specified:
                extra.add(name)
        return [Bcfg2.Client.XML.Element('Service', name=name, type='deb')
                for name in list(extra)]

    def Remove(self, _):
        """Remove extra service entries."""
        # Extra service removal is nonsensical
        # Extra services need to be reflected in the config
        return

########NEW FILE########
__FILENAME__ = Encap
"""Bcfg2 Support for Encap Packages"""

import glob
import re
import Bcfg2.Client.Tools


class Encap(Bcfg2.Client.Tools.PkgTool):
    """Support for Encap packages."""
    name = 'Encap'
    __execs__ = ['/usr/local/bin/epkg']
    __handles__ = [('Package', 'encap')]
    __req__ = {'Package': ['version', 'url']}
    pkgtype = 'encap'
    pkgtool = ("/usr/local/bin/epkg -l -f -q %s", ("%s", ["url"]))
    splitter = re.compile(r'.*/(?P<name>[\w-]+)\-(?P<version>[\w\.+-]+)')

    def RefreshPackages(self):
        """Try to find encap packages."""
        self.installed = {}
        for pkg in glob.glob("/usr/local/encap/*"):
            match = self.splitter.match(pkg)
            if match:
                self.installed[match.group('name')] = match.group('version')
            else:
                print("Failed to split name %s" % pkg)
        self.logger.debug("Encap: RefreshPackages: self.installed.keys() are:")
        self.logger.debug("%s" % list(self.installed.keys()))

    def VerifyPackage(self, entry, _):
        """Verify Package status for entry."""
        if not entry.get('version'):
            self.logger.info("Insufficient information of Package %s; "
                             "cannot Verify" % entry.get('name'))
            return False
        success = self.cmd.run("/usr/local/bin/epkg -q -S -k %s-%s" %
                               (entry.get('name'),
                                entry.get('version'))).success
        if not success:
            self.logger.debug("Package %s version incorrect" %
                              entry.get('name'))
        return success

    def Remove(self, packages):
        """Deal with extra configuration detected."""
        names = " ".join([pkg.get('name') for pkg in packages])
        self.logger.info("Removing packages: %s" % (names))
        self.cmd.run("/usr/local/bin/epkg -l -q -r %s" % (names))
        self.RefreshPackages()
        self.extra = self.FindExtra()

########NEW FILE########
__FILENAME__ = FreeBSDInit
"""FreeBSD Init Support for Bcfg2."""
__revision__ = '$Rev$'

# TODO
# - hardcoded path to ports rc.d
# - doesn't know about /etc/rc.d/

import os
import Bcfg2.Client.Tools


class FreeBSDInit(Bcfg2.Client.Tools.SvcTool):
    """FreeBSD service support for Bcfg2."""
    name = 'FreeBSDInit'
    __handles__ = [('Service', 'freebsd')]
    __req__ = {'Service': ['name', 'status']}

    def __init__(self, logger, cfg, setup):
        Bcfg2.Client.Tools.Tool.__init__(self, logger, cfg, setup)
        if os.uname()[0] != 'FreeBSD':
            raise Bcfg2.Client.Tools.ToolInstantiationError

    def VerifyService(self, entry, _):
        return True

    def get_svc_command(self, service, action):
        return "/usr/local/etc/rc.d/%s %s" % (service.get('name'), action)

########NEW FILE########
__FILENAME__ = FreeBSDPackage
"""This is the Bcfg2 tool for the FreeBSD package system."""

# TODO
# - actual package installation
# - verification of package files

import re
import Bcfg2.Client.Tools


class FreeBSDPackage(Bcfg2.Client.Tools.PkgTool):
    """The FreeBSD toolset implements package operations and inherits
    the rest from Toolset.Toolset."""
    name = 'FreeBSDPackage'
    __execs__ = ['/usr/sbin/pkg_add', '/usr/sbin/pkg_info']
    __handles__ = [('Package', 'freebsdpkg')]
    __req__ = {'Package': ['name', 'version']}
    pkgtool = ('/usr/sbin/pkg_add -r %s', ('%s-%s', ['name', 'version']))
    pkgtype = 'freebsdpkg'

    def RefreshPackages(self):
        self.installed = {}
        packages = self.cmd.run("/usr/sbin/pkg_info -a -E").stdout.splitlines()
        pattern = re.compile(r'(.*)-(\d.*)')
        for pkg in packages:
            if pattern.match(pkg):
                name = pattern.match(pkg).group(1)
                version = pattern.match(pkg).group(2)
                self.installed[name] = version

    def VerifyPackage(self, entry, _):
        if 'version' not in entry.attrib:
            self.logger.info("Cannot verify unversioned package %s" %
                             entry.attrib['name'])
            return False
        if entry.attrib['name'] in self.installed:
            if self.installed[entry.attrib['name']] == entry.attrib['version']:
                # TODO: verfification
                return True
            else:
                entry.set('current_version', self.installed[entry.get('name')])
                return False

        self.logger.info("Package %s not installed" % (entry.get('name')))
        entry.set('current_exists', 'false')
        return False

########NEW FILE########
__FILENAME__ = IPS
"""This is the Bcfg2 support for OpenSolaris packages."""

import pkg.client.image as image
import pkg.client.progress as progress

import Bcfg2.Client.Tools


class IPS(Bcfg2.Client.Tools.PkgTool):
    """The IPS driver implements OpenSolaris package operations."""
    name = 'IPS'
    pkgtype = 'ips'
    conflicts = ['SYSV']
    __handles__ = [('Package', 'ips')]
    __req__ = {'Package': ['name', 'version']}
    pkgtool = ('pkg install --no-refresh %s', ('%s', ['name']))

    def __init__(self, logger, setup, cfg):
        self.installed = {}
        self.pending_upgrades = set()
        self.image = image.Image()
        self.image.find_root('/', False)
        self.image.load_config()
        Bcfg2.Client.Tools.PkgTool.__init__(self, logger, setup, cfg)
        self.cfg = cfg

    def RefreshPackages(self):
        self.installed = dict()
        self.image.history.operation_name = "list"
        self.image.load_catalogs(progress.NullProgressTracker())
        for (pfmri, pinfo) in self.image.inventory([], False):
            pname = pfmri.pkg_name
            pversion = pfmri.version.get_short_version()
            self.installed[pname] = pversion
            if pinfo['upgradable']:
                self.pending_upgrades.add(pname)

    def VerifyPackage(self, entry, _):
        """Verify package for entry."""
        pname = entry.get('name')
        if 'version' not in entry.attrib:
            self.logger.info("Cannot verify unversioned package %s" % (pname))
            return False
        if pname not in self.installed:
            self.logger.debug("IPS: Package %s not installed" % pname)
            return False
        if entry.get('version') == 'auto':
            if pname in self.pending_upgrades:
                return False
        elif entry.get('version') == 'any':
            pass
        else:
            if entry.get('version') != self.installed[pname]:
                self.logger.debug("IPS: Package %s: have %s want %s" %
                                  (pname, self.installed[pname],
                                   entry.get('version')))
                return False

        # need to implement pkg chksum validation
        return True

########NEW FILE########
__FILENAME__ = launchd
"""launchd support for Bcfg2."""

import os
import Bcfg2.Client.Tools


class launchd(Bcfg2.Client.Tools.Tool):  # pylint: disable=C0103
    """Support for Mac OS X launchd services.  Currently requires the
    path to the plist to load/unload, and Name is acually a
    reverse-fqdn (or the label)."""
    __handles__ = [('Service', 'launchd')]
    __execs__ = ['/bin/launchctl', '/usr/bin/defaults']
    __req__ = {'Service': ['name', 'status']}

    def __init__(self, logger, setup, config):
        Bcfg2.Client.Tools.Tool.__init__(self, logger, setup, config)

        # Locate plist file that provides given reverse-fqdn name:
        #
        # * ``/Library/LaunchAgents``: Per-user agents provided by the
        #   administrator.
        # * ``/Library/LaunchDaemons``: System-wide daemons provided
        #   by the administrator.
        # * ``/System/Library/LaunchAgents``: Mac OS X per-user
        #   agents.
        # * ``/System/Library/LaunchDaemons``: Mac OS X system-wide
        #   daemons.
        plist_locations = ["/Library/LaunchDaemons",
                           "/System/Library/LaunchDaemons"]
        self.plist_mapping = {}
        for directory in plist_locations:
            for daemon in os.listdir(directory):
                if daemon.endswith(".plist"):
                    daemon = daemon[:-6]
                dpath = os.path.join(directory, daemon)
                rv = self.cmd.run(['defaults', 'read', dpath, 'Label'])
                if rv.success:
                    label = rv.stdout.splitlines()[0]
                    self.plist_mapping[label] = dpath
                else:
                    self.logger.warning("Could not get label from %s" % dpath)

    def FindPlist(self, entry):
        """ Find the location of the plist file for the given entry """
        return self.plist_mapping.get(entry.get('name'), None)

    def os_version(self):
        """ Determine the OS version """
        rv = self.cmd.run('sw_vers')
        if rv:
            for line in rv.stdout.splitlines():
                if line.startswith("ProductVersion"):
                    return line.split()[-1]
        else:
            return ''

    def VerifyService(self, entry, _):
        """Verify launchd service entry."""
        if entry.get('status') == 'ignore':
            return True

        try:
            services = self.cmd.run("/bin/launchctl list").stdout.splitlines()
        except IndexError:
            # happens when no services are running (should be never)
            services = []
        # launchctl output changed in 10.5
        # It is now three columns, with the last
        # column being the name of the # service
        if int(self.os_version().split('.')[1]) >= 5:
            services = [s.split()[-1] for s in services]
        if entry.get('name') in services:
            # doesn't check if non-spawning services are Started
            return entry.get('status') == 'on'
        else:
            self.logger.debug("Launchd: Didn't find service Loaded "
                              "(launchd running under same user as bcfg)")
            return entry.get('status') == 'off'

        try:
            # Perhaps add the "-w" flag to load and
            # unload to modify the file itself!
            self.cmd.run("/bin/launchctl load -w %s" % self.FindPlist(entry))
        except IndexError:
            return 'on'
        return False

    def InstallService(self, entry):
        """Enable or disable launchd item."""
        name = entry.get('name')
        if entry.get('status') == 'on':
            self.logger.error("Installing service %s" % name)
            self.cmd.run("/bin/launchctl load -w %s" % self.FindPlist(entry))
            return self.cmd.run("/bin/launchctl start %s" % name).success
        else:
            self.logger.error("Uninstalling service %s" % name)
            self.cmd.run("/bin/launchctl stop %s" % name)
            return self.cmd.run("/bin/launchctl unload -w %s" %
                                self.FindPlist(entry)).success

    def Remove(self, svcs):
        """Remove Extra launchd entries."""
        pass

    def FindExtra(self):
        """Find Extra launchd services."""
        try:
            allsrv = self.cmd.run("/bin/launchctl list").stdout.splitlines()
        except IndexError:
            allsrv = []

        for entry in self.getSupportedEntries():
            svc = entry.get("name")
            if svc in allsrv:
                allsrv.remove(svc)
        return [Bcfg2.Client.XML.Element("Service", type='launchd', name=name,
                                         status='on')
                for name in allsrv]

    def BundleUpdated(self, bundle, states):
        """Reload launchd plist."""
        for entry in [entry for entry in bundle if self.handlesEntry(entry)]:
            if not self.canInstall(entry):
                self.logger.error("Insufficient information to restart "
                                  "service %s" % entry.get('name'))
            else:
                name = entry.get('name')
                if entry.get('status') == 'on' and self.FindPlist(entry):
                    self.logger.info("Reloading launchd service %s" % name)
                    # stop?
                    self.cmd.run("/bin/launchctl stop %s" % name)
                    # what if it disappeared? how do we stop services
                    # that are currently running but the plist disappeared?!
                    self.cmd.run("/bin/launchctl unload -w %s" %
                                 (self.FindPlist(entry)))
                    self.cmd.run("/bin/launchctl load -w %s" %
                                 (self.FindPlist(entry)))
                    self.cmd.run("/bin/launchctl start %s" % name)
                else:
                    # only if necessary....
                    self.cmd.run("/bin/launchctl stop %s" % name)
                    self.cmd.run("/bin/launchctl unload -w %s" %
                                 (self.FindPlist(entry)))

########NEW FILE########
__FILENAME__ = MacPorts
"""This provides Bcfg2 support for macports packages."""

import Bcfg2.Client.Tools


class MacPorts(Bcfg2.Client.Tools.PkgTool):
    """macports package support."""
    name = 'MacPorts'
    __execs__ = ["/opt/local/bin/port"]
    __handles__ = [('Package', 'macport')]
    __req__ = {'Package': ['name', 'version']}
    pkgtype = 'macport'
    pkgtool = ('/opt/local/bin/port install %s', ('%s', ['name']))

    def __init__(self, logger, setup, config):
        Bcfg2.Client.Tools.PkgTool.__init__(self, logger, setup, config)
        self.installed = {}
        self.RefreshPackages()

    def RefreshPackages(self):
        """Refresh memory hashes of packages."""
        pkgcache = self.cmd.run(["/opt/local/bin/port",
                                 "installed"]).stdout.splitlines()
        self.installed = {}
        for pkg in pkgcache:
            if pkg.startswith("Warning:"):
                continue
            if pkg.startswith("The following ports are currently installed"):
                continue
            if pkg.startswith("No ports are installed"):
                return
            pkgname = pkg.split('@')[0].strip()
            version = pkg.split('@')[1].split(' ')[0]
            self.logger.info(" pkgname: %s version: %s" % (pkgname, version))
            self.installed[pkgname] = version

    def VerifyPackage(self, entry, _):
        """Verify Package status for entry."""
        if 'version' not in entry.attrib:
            self.logger.info("Cannot verify unversioned package %s" %
                             entry.attrib['name'])
            return False

        if entry.attrib['name'] in self.installed:
            if (self.installed[entry.attrib['name']] == entry.attrib['version']
                or entry.attrib['version'] == 'any'):
                # if (not self.setup['quick'] and
                #     entry.get('verify', 'true') == 'true'):
                # FIXME: We should be able to check this once
                #        http://trac.macports.org/ticket/15709 is implemented
                return True
            else:
                self.logger.info("  %s: Wrong version installed.  "
                                 "Want %s, but have %s" %
                                 (entry.get("name"),
                                  entry.get("version"),
                                  self.installed[entry.get("name")],
                                  ))

                entry.set('current_version', self.installed[entry.get('name')])
                return False
        entry.set('current_exists', 'false')
        return False

    def Remove(self, packages):
        """Remove extra packages."""
        names = [pkg.get('name') for pkg in packages]
        self.logger.info("Removing packages: %s" % " ".join(names))
        self.cmd.run("/opt/local/bin/port uninstall %s" %
                     " ".join(names))
        self.RefreshPackages()
        self.extra = self.FindExtra()

########NEW FILE########
__FILENAME__ = OpenCSW
# This is the bcfg2 support for opencsw packages (pkgutil)
"""This provides Bcfg2 support for OpenCSW packages."""

import tempfile
import Bcfg2.Client.Tools.SYSV


class OpenCSW(Bcfg2.Client.Tools.SYSV.SYSV):
    """Support for OpenCSW packages."""
    pkgtype = 'opencsw'
    pkgtool = ("/opt/csw/bin/pkgutil -y -i %s", ("%s", ["bname"]))
    name = 'OpenCSW'
    __execs__ = ['/opt/csw/bin/pkgutil', "/usr/bin/pkginfo"]
    __handles__ = [('Package', 'opencsw')]
    __req__ = {'Package': ['name', 'version', 'bname']}

    def __init__(self, logger, setup, config):
        # dont use the sysv constructor
        Bcfg2.Client.Tools.PkgTool.__init__(self, logger, setup, config)
        noaskfile = tempfile.NamedTemporaryFile()
        self.noaskname = noaskfile.name
        try:
            noaskfile.write(Bcfg2.Client.Tools.SYSV.noask)
        except:
            pass

    # VerifyPackage comes from Bcfg2.Client.Tools.SYSV
    # Install comes from Bcfg2.Client.Tools.PkgTool
    # Extra comes from Bcfg2.Client.Tools.Tool
    # Remove comes from Bcfg2.Client.Tools.SYSV
    def FindExtra(self):
        """Pass through to null FindExtra call."""
        return []

########NEW FILE########
__FILENAME__ = Pacman
"""This is the bcfg2 support for pacman"""

import sys
import Bcfg2.Client.Tools


class Pacman(Bcfg2.Client.Tools.PkgTool):
    '''Archlinux package support'''
    name = 'Pacman'
    __execs__ = ["/usr/bin/pacman"]
    __handles__ = [('Package', 'pacman')]
    __req__ = {'Package': ['name', 'version']}
    pkgtype = 'pacman'
    pkgtool = ("/usr/bin/pacman --needed --noconfirm --noprogressbar")

    def __init__(self, logger, setup, config):
        Bcfg2.Client.Tools.PkgTool.__init__(self, logger, setup, config)
        self.installed = {}
        self.RefreshPackages()

    def RefreshPackages(self):
        '''Refresh memory hashes of packages'''
        self.installed = {}
        for pkg in self.cmd.run("/usr/bin/pacman -Q").stdout.splitlines():
            pkgname = pkg.split(' ')[0].strip()
            version = pkg.split(' ')[1].strip()
            self.installed[pkgname] = version

    def VerifyPackage(self, entry, _):
        '''Verify Package status for entry'''

        self.logger.info("VerifyPackage: %s : %s" % (entry.get('name'),
                                                     entry.get('version')))

        if 'version' not in entry.attrib:
            self.logger.info("Cannot verify unversioned package %s" %
                             entry.attrib['name'])
            return False

        if entry.attrib['name'] in self.installed:
            if entry.attrib['version'] == 'auto':
                return True
            elif self.installed[entry.attrib['name']] == \
                    entry.attrib['version']:
                # if (not self.setup['quick'] and
                #     entry.get('verify', 'true') == 'true'):
                # FIXME: need to figure out if pacman
                #        allows you to verify packages
                return True
            else:
                entry.set('current_version', self.installed[entry.get('name')])
                self.logger.info("attribname: %s" % (entry.attrib['name']))
                self.logger.info("attribname: %s" % (entry.attrib['name']))
                return False
        entry.set('current_exists', 'false')
        self.logger.info("attribname: %s" % (entry.attrib['name']))
        return False

    def Remove(self, packages):
        '''Remove extra packages'''
        names = [pkg.get('name') for pkg in packages]
        self.logger.info("Removing packages: %s" % " ".join(names))
        self.cmd.run("%s --noconfirm --noprogressbar -R %s" %
                     (self.pkgtool, " ".join(names)))
        self.RefreshPackages()
        self.extra = self.FindExtra()

    def Install(self, packages, states):
        '''
        Pacman Install
        '''
        pkgline = ""
        for pkg in packages:
            pkgline += " " + pkg.get('name')

        self.logger.info("packages : " + pkgline)

        try:
            self.logger.debug("Running : %s -S %s" % (self.pkgtool, pkgline))
            self.cmd.run("%s -S %s" % (self.pkgtool, pkgline))
        except:  # pylint: disable=W0702
            err = sys.exc_info()[1]
            self.logger.error("Error occurred during installation: %s" % err)

########NEW FILE########
__FILENAME__ = Portage
"""This is the Bcfg2 tool for the Gentoo Portage system."""

import re
import Bcfg2.Client.Tools


class Portage(Bcfg2.Client.Tools.PkgTool):
    """The Gentoo toolset implements package and service operations and
    inherits the rest from Toolset.Toolset."""
    name = 'Portage'
    __execs__ = ['/usr/bin/emerge', '/usr/bin/equery']
    __handles__ = [('Package', 'ebuild')]
    __req__ = {'Package': ['name', 'version']}
    pkgtype = 'ebuild'
    # requires a working PORTAGE_BINHOST in make.conf
    _binpkgtool = ('emerge --getbinpkgonly %s', ('=%s-%s', ['name',
                                                            'version']))
    pkgtool = ('emerge %s', ('=%s-%s', ['name', 'version']))

    def __init__(self, logger, cfg, setup):
        self._initialised = False
        Bcfg2.Client.Tools.PkgTool.__init__(self, logger, cfg, setup)
        self._initialised = True
        self.__important__ = self.__important__ + ['/etc/make.conf']
        self._pkg_pattern = re.compile(r'(.*)-(\d.*)')
        self._ebuild_pattern = re.compile('(ebuild|binary)')
        self.cfg = cfg
        self.installed = {}
        self._binpkgonly = self.setup.get('portage_binpkgonly', False)
        if self._binpkgonly:
            self.pkgtool = self._binpkgtool
        self.RefreshPackages()

    def RefreshPackages(self):
        """Refresh memory hashes of packages."""
        if not self._initialised:
            return
        self.logger.info('Getting list of installed packages')
        self.installed = {}
        for pkg in self.cmd.run(["equery", "-q",
                                 "list", "*"]).stdout.splitlines():
            if self._pkg_pattern.match(pkg):
                name = self._pkg_pattern.match(pkg).group(1)
                version = self._pkg_pattern.match(pkg).group(2)
                self.installed[name] = version
            else:
                self.logger.info("Failed to parse pkg name %s" % pkg)

    def VerifyPackage(self, entry, modlist):
        """Verify package for entry."""
        if 'version' not in entry.attrib:
            self.logger.info("Cannot verify unversioned package %s" %
                             (entry.get('name')))
            return False

        if not (entry.get('name') in self.installed):
            # Can't verify package that isn't installed
            entry.set('current_exists', 'false')
            return False

        # get the installed version
        version = self.installed[entry.get('name')]
        entry.set('current_version', version)

        if not self.setup['quick']:
            if ('verify' not in entry.attrib or
                entry.get('verify').lower() == 'true'):

                # Check the package if:
                # - Not running in quick mode
                # - No verify option is specified in the literal configuration
                #    OR
                # - Verify option is specified and is true

                self.logger.debug('Running equery check on %s' %
                                  entry.get('name'))
                for line in self.cmd.run(
                    ["/usr/bin/equery", "-N", "check",
                     '=%s-%s' % (entry.get('name'),
                                 entry.get('version'))]).stdout.splitlines():
                    if '!!!' in line and line.split()[1] not in modlist:
                        return False

        # By now the package must be in one of the following states:
        # - Not require checking
        # - Have no files modified at all
        # - Have modified files in the modlist only
        if self.installed[entry.get('name')] == version:
            # Specified package version is installed
            # Specified package version may be any in literal configuration
            return True

        # Something got skipped. Indicates a bug
        return False

    def Remove(self, packages):
        """Deal with extra configuration detected."""
        pkgnames = " ".join([pkg.get('name') for pkg in packages])
        if len(packages) > 0:
            self.logger.info('Removing packages:')
            self.logger.info(pkgnames)
            self.cmd.run("emerge --unmerge --quiet %s" %
                         " ".join(pkgnames.split(' ')))
            self.RefreshPackages()
            self.extra = self.FindExtra()

########NEW FILE########
__FILENAME__ = Augeas
""" Augeas driver """

import sys
import Bcfg2.Client.XML
from augeas import Augeas
from Bcfg2.Client.Tools.POSIX.base import POSIXTool
from Bcfg2.Client.Tools.POSIX.File import POSIXFile


class AugeasCommand(object):
    """ Base class for all Augeas command objects """

    def __init__(self, command, augeas_obj, logger):
        self._augeas = augeas_obj
        self.command = command
        self.entry = self.command.getparent()
        self.logger = logger

    def get_path(self, attr="path"):
        """ Get a fully qualified path from the name of the parent entry and
        the path given in this command tag.

        @param attr: The attribute to get the relative path from
        @type attr: string
        @returns: string - the fully qualified Augeas path

        """
        return "/files/%s/%s" % (self.entry.get("name").strip("/"),
                                 self.command.get(attr).lstrip("/"))

    def _exists(self, path):
        """ Return True if a path exists in Augeas, False otherwise.

        Note that a False return can mean many things: A file that
        doesn't exist, a node within the file that doesn't exist, no
        lens to parse the file, etc. """
        return len(self._augeas.match(path)) > 1

    def _verify_exists(self, path=None):
        """ Verify that the given path exists, with friendly debug
        logging.

        @param path: The path to verify existence of.  Defaults to the
                     result of
                     :func:`Bcfg2.Client.Tools.POSIX.Augeas.AugeasCommand.getpath`.
        @type path: string
        @returns: bool - Whether or not the path exists
        """
        if path is None:
            path = self.get_path()
        self.logger.debug("Augeas: Verifying that '%s' exists" % path)
        return self._exists(path)

    def _verify_not_exists(self, path=None):
        """ Verify that the given path does not exist, with friendly
        debug logging.

        @param path: The path to verify existence of.  Defaults to the
                     result of
                     :func:`Bcfg2.Client.Tools.POSIX.Augeas.AugeasCommand.getpath`.
        @type path: string
        @returns: bool - Whether or not the path does not exist.
                  (I.e., True if it does not exist, False if it does
                  exist.)
        """
        if path is None:
            path = self.get_path()
        self.logger.debug("Augeas: Verifying that '%s' does not exist" % path)
        return not self._exists(path)

    def _verify_set(self, expected, path=None):
        """ Verify that the given path is set to the given value, with
        friendly debug logging.

        @param expected: The expected value of the node.
        @param path: The path to verify existence of.  Defaults to the
                     result of
                     :func:`Bcfg2.Client.Tools.POSIX.Augeas.AugeasCommand.getpath`.
        @type path: string
        @returns: bool - Whether or not the path matches the expected value.

        """
        if path is None:
            path = self.get_path()
        self.logger.debug("Augeas: Verifying '%s' == '%s'" % (path, expected))
        actual = self._augeas.get(path)
        if actual == expected:
            return True
        else:
            self.logger.debug("Augeas: '%s' failed verification: '%s' != '%s'"
                              % (path, actual, expected))
            return False

    def __str__(self):
        return Bcfg2.Client.XML.tostring(self.command)

    def verify(self):
        """ Verify that the command has been applied. """
        raise NotImplementedError

    def install(self):
        """ Run the command. """
        raise NotImplementedError


class Remove(AugeasCommand):
    """ Augeas ``rm`` command """
    def verify(self):
        return self._verify_not_exists()

    def install(self):
        self.logger.debug("Augeas: Removing %s" % self.get_path())
        return self._augeas.remove(self.get_path())


class Move(AugeasCommand):
    """ Augeas ``move`` command """
    def __init__(self, command, augeas_obj, logger):
        AugeasCommand.__init__(self, command, augeas_obj, logger)
        self.source = self.get_path("source")
        self.dest = self.get_path("destination")

    def verify(self):
        return (self._verify_not_exists(self.source),
                self._verify_exists(self.dest))

    def install(self):
        self.logger.debug("Augeas: Moving %s to %s" % (self.source, self.dest))
        return self._augeas.move(self.source, self.dest)


class Set(AugeasCommand):
    """ Augeas ``set`` command """
    def __init__(self, command, augeas_obj, logger):
        AugeasCommand.__init__(self, command, augeas_obj, logger)
        self.value = self.command.get("value")

    def verify(self):
        return self._verify_set(self.value)

    def install(self):
        self.logger.debug("Augeas: Setting %s to %s" % (self.get_path(),
                                                        self.value))
        return self._augeas.set(self.get_path(), self.value)


class Clear(Set):
    """ Augeas ``clear`` command """
    def __init__(self, command, augeas_obj, logger):
        Set.__init__(self, command, augeas_obj, logger)
        self.value = None


class SetMulti(AugeasCommand):
    """ Augeas ``setm`` command """
    def __init__(self, command, augeas_obj, logger):
        AugeasCommand.__init__(self, command, augeas_obj, logger)
        self.sub = self.command.get("sub")
        self.value = self.command.get("value")
        self.base = self.get_path("base")

    def verify(self):
        return all(self._verify_set(self.value,
                                    path="%s/%s" % (path, self.sub))
                   for path in self._augeas.match(self.base))

    def install(self):
        return self._augeas.setm(self.base, self.sub, self.value)


class Insert(AugeasCommand):
    """ Augeas ``ins`` command """
    def __init__(self, command, augeas_obj, logger):
        AugeasCommand.__init__(self, command, augeas_obj, logger)
        self.label = self.command.get("label")
        self.where = self.command.get("where", "before")
        self.before = self.where == "before"

    def verify(self):
        return self._verify_exists("%s/../%s" % (self.get_path(), self.label))

    def install(self):
        self.logger.debug("Augeas: Inserting new %s %s %s" %
                          (self.label, self.where, self.get_path()))
        return self._augeas.insert(self.get_path(), self.label, self.before)


class POSIXAugeas(POSIXTool):
    """ Handle <Path type='augeas'...> entries.  See
    :ref:`client-tools-augeas`. """
    __req__ = ['name', 'mode', 'owner', 'group']

    def __init__(self, logger, setup, config):
        POSIXTool.__init__(self, logger, setup, config)
        self._augeas = dict()
        # file tool for setting initial values of files that don't
        # exist
        self.filetool = POSIXFile(logger, setup, config)

    def get_augeas(self, entry):
        """ Get an augeas object for the given entry. """
        if entry.get("name") not in self._augeas:
            aug = Augeas()
            if entry.get("lens"):
                self.logger.debug("Augeas: Adding %s to include path for %s" %
                                  (entry.get("name"), entry.get("lens")))
                incl = "/augeas/load/%s/incl" % entry.get("lens")
                ilen = len(aug.match(incl))
                if ilen == 0:
                    self.logger.error("Augeas: Lens %s does not exist" %
                                      entry.get("lens"))
                else:
                    aug.set("%s[%s]" % (incl, ilen + 1), entry.get("name"))
                    aug.load()
            self._augeas[entry.get("name")] = aug
        return self._augeas[entry.get("name")]

    def fully_specified(self, entry):
        return len(entry.getchildren()) != 0

    def get_commands(self, entry):
        """ Get a list of commands to verify or install.

        @param entry: The entry to get commands from.
        @type entry: lxml.etree._Element
        @param unverified: Only get commands that failed verification.
        @type unverified: bool
        @returns: list of
                  :class:`Bcfg2.Client.Tools.POSIX.Augeas.AugeasCommand`
                  objects representing the commands.
        """
        rv = []
        for cmd in entry.iterchildren():
            if cmd.tag == "Initial":
                continue
            if cmd.tag in globals():
                rv.append(globals()[cmd.tag](cmd, self.get_augeas(entry),
                                             self.logger))
            else:
                err = "Augeas: Unknown command %s in %s" % (cmd.tag,
                                                            entry.get("name"))
                self.logger.error(err)
                entry.set('qtext', "\n".join([entry.get('qtext', ''), err]))
        return rv

    def verify(self, entry, modlist):
        rv = True
        for cmd in self.get_commands(entry):
            try:
                if not cmd.verify():
                    err = "Augeas: Command has not been applied to %s: %s" % \
                          (entry.get("name"), cmd)
                    self.logger.debug(err)
                    entry.set('qtext', "\n".join([entry.get('qtext', ''),
                                                  err]))
                    rv = False
                    cmd.command.set("verified", "false")
                else:
                    cmd.command.set("verified", "true")
            except:  # pylint: disable=W0702
                err = "Augeas: Unexpected error verifying %s: %s: %s" % \
                      (entry.get("name"), cmd, sys.exc_info()[1])
                self.logger.error(err)
                entry.set('qtext', "\n".join([entry.get('qtext', ''), err]))
                rv = False
                cmd.command.set("verified", "false")
        return POSIXTool.verify(self, entry, modlist) and rv

    def install(self, entry):
        rv = True
        if entry.get("current_exists", "true") == "false":
            initial = entry.find("Initial")
            if initial is not None:
                self.logger.debug("Augeas: Setting initial data for %s" %
                                  entry.get("name"))
                file_entry = Bcfg2.Client.XML.Element("Path",
                                                      **dict(entry.attrib))
                file_entry.text = initial.text
                self.filetool.install(file_entry)
                # re-parse the file
                self.get_augeas(entry).load()
        for cmd in self.get_commands(entry):
            try:
                cmd.install()
            except:  # pylint: disable=W0702
                self.logger.error(
                    "Failure running Augeas command on %s: %s: %s" %
                    (entry.get("name"), cmd, sys.exc_info()[1]))
                rv = False
        try:
            self.get_augeas(entry).save()
        except:  # pylint: disable=W0702
            self.logger.error("Failure saving Augeas changes to %s: %s" %
                              (entry.get("name"), sys.exc_info()[1]))
            rv = False
        return POSIXTool.install(self, entry) and rv

########NEW FILE########
__FILENAME__ = base
""" Base class for tools that handle POSIX (Path) entries """

import os
import sys
import pwd
import grp
import stat
import copy
import shutil
import Bcfg2.Client.Tools
import Bcfg2.Client.XML
from Bcfg2.Compat import oct_mode

try:
    import selinux
    HAS_SELINUX = selinux.is_selinux_enabled()
except ImportError:
    HAS_SELINUX = False

try:
    import posix1e
    HAS_ACLS = True

    # map between permissions characters and numeric ACL constants
    ACL_MAP = dict(r=posix1e.ACL_READ,
                   w=posix1e.ACL_WRITE,
                   x=posix1e.ACL_EXECUTE)
except ImportError:
    HAS_ACLS = False
    ACL_MAP = dict(r=4, w=2, x=1)

# map between dev_type attribute and stat constants
device_map = dict(block=stat.S_IFBLK,  # pylint: disable=C0103
                  char=stat.S_IFCHR,
                  fifo=stat.S_IFIFO)


class POSIXTool(Bcfg2.Client.Tools.Tool):
    """ Base class for tools that handle POSIX (Path) entries """
    def fully_specified(self, entry):  # pylint: disable=W0613
        """ return True if the entry is fully specified """
        # checking is done by __req__
        return True

    def verify(self, entry, modlist):  # pylint: disable=W0613
        """ return True if the entry is correct on disk """
        if not self._verify_metadata(entry):
            return False
        if entry.get('recursive', 'false').lower() == 'true':
            # verify ownership information recursively
            for root, dirs, files in os.walk(entry.get('name')):
                for path in dirs + files:
                    if not self._verify_metadata(entry,
                                                 path=os.path.join(root,
                                                                   path)):
                        return False
        return True

    def install(self, entry):
        """ Install the given entry.  Return True on success. """
        rv = True
        rv &= self._set_perms(entry)
        if entry.get('recursive', 'false').lower() == 'true':
            # set metadata recursively
            for root, dirs, files in os.walk(entry.get('name')):
                for path in dirs + files:
                    rv &= self._set_perms(entry, path=os.path.join(root, path))
        return rv

    def _remove(self, entry, recursive=True):
        """ Remove a Path entry, whatever that takes """
        if os.path.islink(entry.get('name')):
            os.unlink(entry.get('name'))
        elif os.path.isdir(entry.get('name')):
            if recursive:
                shutil.rmtree(entry.get('name'))
            else:
                os.rmdir(entry.get('name'))
        else:
            os.unlink(entry.get('name'))

    def _exists(self, entry, remove=False):
        """ check for existing paths and optionally remove them.  if
        the path exists, return the lstat of it """
        try:
            ondisk = os.lstat(entry.get('name'))
            if remove:
                try:
                    self._remove(entry)
                    return None
                except OSError:
                    err = sys.exc_info()[1]
                    self.logger.warning('POSIX: Failed to unlink %s: %s' %
                                        (entry.get('name'), err))
                    return ondisk  # probably still exists
            else:
                return ondisk
        except OSError:
            return None

    def _set_perms(self, entry, path=None):
        """ set permissions on the given entry, or on the given path
        according to the given entry """
        if path is None:
            path = entry.get("name")

        rv = True
        if entry.get("owner") and entry.get("group"):
            try:
                self.logger.debug("POSIX: Setting ownership of %s to %s:%s" %
                                  (path,
                                   self._norm_entry_uid(entry),
                                   self._norm_entry_gid(entry)))
                os.chown(path, self._norm_entry_uid(entry),
                         self._norm_entry_gid(entry))
            except KeyError:
                self.logger.error('POSIX: Failed to change ownership of %s' %
                                  path)
                rv = False
                os.chown(path, 0, 0)
            except OSError:
                self.logger.error('POSIX: Failed to change ownership of %s' %
                                  path)
                rv = False

        if entry.get("mode"):
            wanted_mode = int(entry.get('mode'), 8)
            if entry.get('dev_type'):
                wanted_mode |= device_map[entry.get('dev_type')]
            try:
                self.logger.debug("POSIX: Setting mode on %s to %s" %
                                  (path, oct_mode(wanted_mode)))
                os.chmod(path, wanted_mode)
            except (OSError, KeyError):
                self.logger.error('POSIX: Failed to change mode on %s' %
                                  path)
                rv = False

        if entry.get('mtime'):
            try:
                os.utime(entry.get('name'), (int(entry.get('mtime')),
                                             int(entry.get('mtime'))))
            except OSError:
                self.logger.error("POSIX: Failed to set mtime of %s" % path)
                rv = False

        rv &= self._set_secontext(entry, path=path)
        rv &= self._set_acls(entry, path=path)
        return rv

    def _apply_acl(self, acl, path, atype=None):
        """ Apply the given ACL to the given path """
        if atype is None:
            # the default value for atype is set this way (rather than
            # in the argument list) because posix1e libs may not be
            # installed, and this code is executed at run-time (and
            # thus will never be reached if ACLs aren't supported),
            # but argument lists are parsed at compile-time
            atype = posix1e.ACL_TYPE_ACCESS
        if atype == posix1e.ACL_TYPE_ACCESS:
            atype_str = "access"
        else:
            atype_str = "default"
        if acl.valid():
            self.logger.debug("POSIX: Applying %s ACL to %s:" % (atype_str,
                                                                 path))
            for line in str(acl).splitlines():
                self.logger.debug("  " + line)
            try:
                acl.applyto(path, atype)
                return True
            except OSError:
                err = sys.exc_info()[1]
                self.logger.error("POSIX: Failed to set ACLs on %s: %s" %
                                  (path, err))
                return False
        else:
            self.logger.warning("POSIX: %s ACL created for %s was invalid:"
                                % (atype_str.title(), path))
            for line in str(acl).splitlines():
                self.logger.warning("  " + line)
            return False

    def _set_acls(self, entry, path=None):  # pylint: disable=R0912
        """ set POSIX ACLs on the file on disk according to the config """
        if not HAS_ACLS:
            if entry.findall("ACL"):
                self.logger.debug("POSIX: ACLs listed for %s but no pylibacl "
                                  "library installed" % entry.get('name'))
            return True
        acls = self._list_entry_acls(entry)

        if path is None:
            path = entry.get("name")

        try:
            acl = posix1e.ACL(file=path)
        except IOError:
            err = sys.exc_info()[1]
            if err.errno == 95:
                # fs is mounted noacl
                if acls:
                    self.logger.error("POSIX: Cannot set ACLs on filesystem "
                                      "mounted without ACL support: %s" % path)
                else:
                    # no ACLs on the entry, no ACLs on the filesystem.
                    # all is well in the world.
                    return True
            else:
                self.logger.error("POSIX: Error getting current ACLS on %s: %s"
                                  % (path, err))
            return False
        # clear ACLs out so we start fresh -- way easier than trying
        # to add/remove/modify ACLs
        for aclentry in acl:
            if aclentry.tag_type in [posix1e.ACL_USER, posix1e.ACL_GROUP]:
                acl.delete_entry(aclentry)
        if os.path.isdir(path):
            defacl = posix1e.ACL(filedef=path)
            for aclentry in defacl:
                if aclentry.tag_type in [posix1e.ACL_USER,
                                         posix1e.ACL_USER_OBJ,
                                         posix1e.ACL_GROUP,
                                         posix1e.ACL_GROUP_OBJ,
                                         posix1e.ACL_OTHER]:
                    defacl.delete_entry(aclentry)
        else:
            defacl = None

        if not acls:
            self.logger.debug("POSIX: Removed ACLs from %s" %
                              entry.get("name"))
            return True

        for aclkey, perms in acls.items():
            atype, scope, qualifier = aclkey
            if atype == "default":
                if defacl is None:
                    self.logger.warning("POSIX: Cannot set default ACLs on "
                                        "non-directory %s" % path)
                    continue
                aclentry = posix1e.Entry(defacl)
            else:
                aclentry = posix1e.Entry(acl)
            for perm in ACL_MAP.values():
                if perm & perms:
                    aclentry.permset.add(perm)
            aclentry.tag_type = scope
            try:
                if scope == posix1e.ACL_USER:
                    scopename = "user"
                    if qualifier:
                        aclentry.qualifier = self._norm_uid(qualifier)
                    else:
                        aclentry.tag_type = posix1e.ACL_USER_OBJ
                elif scope == posix1e.ACL_GROUP:
                    scopename = "group"
                    if qualifier:
                        aclentry.qualifier = self._norm_gid(qualifier)
                    else:
                        aclentry.tag_type = posix1e.ACL_GROUP_OBJ
            except (OSError, KeyError):
                err = sys.exc_info()[1]
                self.logger.error("POSIX: Could not resolve %s %s: %s" %
                                  (scopename, qualifier, err))
                continue
        acl.calc_mask()

        rv = self._apply_acl(acl, path)
        if defacl:
            defacl.calc_mask()
            rv &= self._apply_acl(defacl, path, posix1e.ACL_TYPE_DEFAULT)
        return rv

    def _set_secontext(self, entry, path=None):
        """ set the SELinux context of the file on disk according to the
        config"""
        if not HAS_SELINUX:
            return True

        if path is None:
            path = entry.get("name")
        context = entry.get("secontext")
        if not context:
            # no context listed
            return True

        if context == '__default__':
            try:
                selinux.restorecon(path)
                rv = True
            except OSError:
                err = sys.exc_info()[1]
                self.logger.error("POSIX: Failed to restore SELinux context "
                                  "for %s: %s" % (path, err))
                rv = False
        else:
            try:
                rv = selinux.lsetfilecon(path, context) == 0
            except OSError:
                err = sys.exc_info()[1]
                self.logger.error("POSIX: Failed to restore SELinux context "
                                  "for %s: %s" % (path, err))
                rv = False
        return rv

    def _norm_gid(self, gid):
        """ This takes a group name or gid and returns the
        corresponding gid. """
        try:
            return int(gid)
        except ValueError:
            return int(grp.getgrnam(gid)[2])

    def _norm_entry_gid(self, entry):
        """ Given an entry, return the GID number of the desired group """
        try:
            return self._norm_gid(entry.get('group'))
        except (OSError, KeyError):
            err = sys.exc_info()[1]
            self.logger.error('POSIX: GID normalization failed for %s on %s: '
                              '%s' % (entry.get('group'),
                                      entry.get('name'),
                                      err))
            return 0

    def _norm_uid(self, uid):
        """ This takes a username or uid and returns the
        corresponding uid. """
        try:
            return int(uid)
        except ValueError:
            return int(pwd.getpwnam(uid)[2])

    def _norm_entry_uid(self, entry):
        """ Given an entry, return the UID number of the desired owner """
        try:
            return self._norm_uid(entry.get("owner"))
        except (OSError, KeyError):
            err = sys.exc_info()[1]
            self.logger.error('POSIX: UID normalization failed for %s on %s: '
                              '%s' % (entry.get('owner'),
                                      entry.get('name'),
                                      err))
            return 0

    def _norm_acl_perms(self, perms):
        """ takes a representation of an ACL permset and returns a digit
        representing the permissions entailed by it.  representations can
        either be a single octal digit, a string of up to three 'r',
        'w', 'x', or '-' characters, or a posix1e.Permset object"""
        if perms is None:
            return 0
        elif hasattr(perms, 'test'):
            # Permset object
            return sum([p for p in ACL_MAP.values()
                        if perms.test(p)])

        try:
            # single octal digit
            rv = int(perms)
            if rv >= 0 and rv < 8:
                return rv
            else:
                self.logger.error("POSIX: Permissions digit out of range in "
                                  "ACL: %s" % perms)
                return 0
        except ValueError:
            # couldn't be converted to an int; process as a string
            if len(perms) > 3:
                self.logger.error("POSIX: Permissions string too long in ACL: "
                                  "%s" % perms)
                return 0
            rv = 0
            for char in perms:
                if char == '-':
                    continue
                elif char not in ACL_MAP:
                    self.logger.warning("POSIX: Unknown permissions character "
                                        "in ACL: %s" % char)
                elif rv & ACL_MAP[char]:
                    self.logger.warning("POSIX: Duplicate permissions "
                                        "character in ACL: %s" % perms)
                else:
                    rv |= ACL_MAP[char]
            return rv

    def _acl2string(self, aclkey, perms):
        """ Get a string representation of the given ACL.  aclkey must
        be a tuple of (<acl type>, <acl scope>, <qualifier>) """
        atype, scope, qualifier = aclkey
        if not qualifier:
            qualifier = ''
        acl_str = []
        if atype == 'default':
            acl_str.append(atype)
        if scope == posix1e.ACL_USER or scope == posix1e.ACL_USER_OBJ:
            acl_str.append("user")
        elif scope == posix1e.ACL_GROUP or scope == posix1e.ACL_GROUP_OBJ:
            acl_str.append("group")
        elif scope == posix1e.ACL_OTHER:
            acl_str.append("other")
        acl_str.append(qualifier)
        acl_str.append(self._acl_perm2string(perms))
        return ":".join(acl_str)

    def _acl_perm2string(self, perm):
        """ Turn an octal permissions integer into a string suitable
        for use with ACLs """
        rv = []
        for char in 'rwx':
            if ACL_MAP[char] & perm:
                rv.append(char)
            else:
                rv.append('-')
        return ''.join(rv)

    def _gather_data(self, path):
        """ Get data on the existing state of <path> -- e.g., whether
        or not it exists, owner, group, permissions, etc. """
        try:
            ondisk = os.lstat(path)
        except OSError:
            self.logger.debug("POSIX: %s does not exist" % path)
            return (False, None, None, None, None, None)

        try:
            owner = str(ondisk[stat.ST_UID])
        except OSError:
            err = sys.exc_info()[1]
            self.logger.debug("POSIX: Could not get current owner of %s: %s" %
                              (path, err))
            owner = None
        except KeyError:
            self.logger.error('POSIX: User resolution failed for %s' % path)
            owner = None

        try:
            group = str(ondisk[stat.ST_GID])
        except (OSError, KeyError):
            err = sys.exc_info()[1]
            self.logger.debug("POSIX: Could not get current group of %s: %s" %
                              (path, err))
            group = None
        except KeyError:
            self.logger.error('POSIX: Group resolution failed for %s' % path)
            group = None

        try:
            mode = oct_mode(ondisk[stat.ST_MODE])[-4:]
        except (OSError, KeyError, TypeError):
            err = sys.exc_info()[1]
            self.logger.debug("POSIX: Could not get current permissions of "
                              "%s: %s" % (path, err))
            mode = None

        if HAS_SELINUX:
            try:
                secontext = selinux.lgetfilecon(path)[1].split(":")[2]
            except (OSError, KeyError):
                err = sys.exc_info()[1]
                self.logger.debug("POSIX: Could not get current SELinux "
                                  "context of %s: %s" % (path, err))
                secontext = None
        else:
            secontext = None

        if HAS_ACLS and not stat.S_ISLNK(ondisk[stat.ST_MODE]):
            acls = self._list_file_acls(path)
        else:
            acls = None
        return (ondisk, owner, group, mode, secontext, acls)

    def _verify_metadata(self, entry, path=None):  # pylint: disable=R0912
        """ generic method to verify mode, owner, group, secontext, acls,
        and mtime """
        # allow setting an alternate path for recursive permissions checking
        if path is None:
            path = entry.get('name')
        attrib = dict()
        ondisk, attrib['current_owner'], attrib['current_group'], \
            attrib['current_mode'], attrib['current_secontext'] = \
            self._gather_data(path)[0:5]

        if not ondisk:
            entry.set('current_exists', 'false')
            return False

        # we conditionally verify every bit of metadata only if it's
        # specified on the entry.  consequently, canVerify() and
        # fully_specified() are preconditions of _verify_metadata(),
        # since they will ensure that everything that needs to be
        # specified actually is.  this lets us gracefully handle
        # symlink and hardlink entries, which have SELinux contexts
        # but not other permissions, optional secontext and mtime
        # attrs, and so on.
        wanted_owner, wanted_group, wanted_mode, mtime = None, None, None, -1
        if entry.get('mtime', '-1') != '-1':
            mtime = str(ondisk[stat.ST_MTIME])
        if entry.get("owner"):
            wanted_owner = str(self._norm_entry_uid(entry))
        if entry.get("group"):
            wanted_group = str(self._norm_entry_gid(entry))
        if entry.get("mode"):
            while len(entry.get('mode', '')) < 4:
                entry.set('mode', '0' + entry.get('mode', ''))
            wanted_mode = int(entry.get('mode'), 8)

        errors = []
        if wanted_owner and attrib['current_owner'] != wanted_owner:
            errors.append("Owner for path %s is incorrect. "
                          "Current owner is %s but should be %s" %
                          (path, attrib['current_owner'], entry.get('owner')))

        if wanted_group and attrib['current_group'] != wanted_group:
            errors.append("Group for path %s is incorrect. "
                          "Current group is %s but should be %s" %
                          (path, attrib['current_group'], entry.get('group')))

        if (wanted_mode and
            oct_mode(int(attrib['current_mode'], 8)) != oct_mode(wanted_mode)):
            errors.append("Permissions for path %s are incorrect. "
                          "Current permissions are %s but should be %s" %
                          (path, attrib['current_mode'], entry.get('mode')))

        if entry.get('mtime'):
            attrib['current_mtime'] = mtime
            if mtime != entry.get('mtime', '-1'):
                errors.append("mtime for path %s is incorrect. "
                              "Current mtime is %s but should be %s" %
                              (path, mtime, entry.get('mtime')))

        if HAS_SELINUX:
            wanted_secontext = None
            if entry.get("secontext") == "__default__":
                try:
                    wanted_secontext = \
                        selinux.matchpathcon(
                            path, ondisk[stat.ST_MODE])[1].split(":")[2]
                except OSError:
                    errors.append("%s has no default SELinux context" %
                                  entry.get("name"))
            else:
                wanted_secontext = entry.get("secontext")
            if (wanted_secontext and
                attrib['current_secontext'] != wanted_secontext):
                errors.append("SELinux context for path %s is incorrect. "
                              "Current context is %s but should be %s" %
                              (path, attrib['current_secontext'],
                               wanted_secontext))

        if errors:
            for error in errors:
                self.logger.debug("POSIX: " + error)
            entry.set('qtext', "\n".join([entry.get('qtext', '')] + errors))
        if path == entry.get("name"):
            for attr, val in attrib.items():
                if val is not None:
                    entry.set(attr, str(val))

        return self._verify_acls(entry, path=path) and len(errors) == 0

    def _list_entry_acls(self, entry):
        """ Given an entry, get a dict of POSIX ACLs described in that
        entry. """
        wanted = dict()
        for acl in entry.findall("ACL"):
            if acl.get("scope") == "user":
                if acl.get("user"):
                    scope = posix1e.ACL_USER
                else:
                    scope = posix1e.ACL_USER_OBJ
            elif acl.get("scope") == "group":
                if acl.get("group"):
                    scope = posix1e.ACL_GROUP
                else:
                    scope = posix1e.ACL_GROUP_OBJ
            elif acl.get("scope") == "other":
                scope = posix1e.ACL_OTHER
            else:
                self.logger.error("POSIX: Unknown ACL scope %s" %
                                  acl.get("scope"))
                continue
            if acl.get('perms') is None:
                self.logger.error("POSIX: No permissions set for ACL: %s" %
                                  Bcfg2.Client.XML.tostring(acl))
                continue
            qual = acl.get(acl.get("scope"))
            if not qual:
                qual = ''
            wanted[(acl.get("type"), scope, qual)] = \
                self._norm_acl_perms(acl.get('perms'))
        return wanted

    def _list_file_acls(self, path):
        """ Given a path, get a dict of existing POSIX ACLs on that
        path.  The dict keys are a tuple of (<acl type (access or
        default)>, <acl scope (user or group)>, <acl qualifer (the
        user or group it applies to)>.  values are the permissions of
        the described ACL. """
        def _process_acl(acl, atype):
            """ Given an ACL object, process it appropriately and add
            it to the return value """
            try:
                qual = ''
                if acl.tag_type == posix1e.ACL_USER:
                    qual = pwd.getpwuid(acl.qualifier)[0]
                elif acl.tag_type == posix1e.ACL_GROUP:
                    qual = grp.getgrgid(acl.qualifier)[0]
                elif atype == "access" or acl.tag_type == posix1e.ACL_MASK:
                    return
            except (OSError, KeyError):
                err = sys.exc_info()[1]
                self.logger.error("POSIX: Lookup of %s %s failed: %s" %
                                  (atype, acl.qualifier, err))
                qual = acl.qualifier
            existing[(atype, acl.tag_type, qual)] = \
                self._norm_acl_perms(acl.permset)

        existing = dict()
        try:
            for acl in posix1e.ACL(file=path):
                _process_acl(acl, "access")
        except IOError:
            err = sys.exc_info()[1]
            if err.errno == 95:
                # fs is mounted noacl
                self.logger.debug("POSIX: Filesystem mounted without ACL "
                                  "support: %s" % path)
            else:
                self.logger.error("POSIX: Error getting current ACLS on %s: %s"
                                  % (path, err))
            return existing

        if os.path.isdir(path):
            for acl in posix1e.ACL(filedef=path):
                _process_acl(acl, "default")
        return existing

    def _verify_acls(self, entry, path=None):  # pylint: disable=R0912
        """ verify POSIX ACLs on the given entry.  return True if all
        ACLS are correct, false otherwise """
        def _verify_acl(aclkey, perms):
            """ Given ACL data, process it appropriately and add it to
            missing or wrong lists if appropriate """
            if aclkey not in existing:
                missing.append(self._acl2string(aclkey, perms))
            elif existing[aclkey] != perms:
                wrong.append((self._acl2string(aclkey, perms),
                              self._acl2string(aclkey, existing[aclkey])))
            if path == entry.get("name"):
                atype, scope, qual = aclkey
                aclentry = Bcfg2.Client.XML.Element("ACL", type=atype,
                                                    perms=str(perms))
                if (scope == posix1e.ACL_USER or
                    scope == posix1e.ACL_USER_OBJ):
                    aclentry.set("scope", "user")
                elif (scope == posix1e.ACL_GROUP or
                      scope == posix1e.ACL_GROUP_OBJ):
                    aclentry.set("scope", "group")
                elif scope == posix1e.ACL_OTHER:
                    aclentry.set("scope", "other")
                else:
                    self.logger.debug("POSIX: Unknown ACL scope %s on %s" %
                                      (scope, path))
                    return

                if scope != posix1e.ACL_OTHER:
                    aclentry.set(aclentry.get("scope"), qual)
                entry.append(aclentry)

        if not HAS_ACLS:
            if entry.findall("ACL"):
                self.logger.debug("POSIX: ACLs listed for %s but no pylibacl "
                                  "library installed" % entry.get('name'))
            return True

        if path is None:
            path = entry.get("name")

        # create lists of normalized representations of the ACLs we want
        # and the ACLs we have.  this will make them easier to compare
        # than trying to mine that data out of the ACL objects and XML
        # objects and compare it at the same time.
        wanted = self._list_entry_acls(entry)
        existing = self._list_file_acls(path)

        missing = []
        extra = []
        wrong = []
        for aclkey, perms in wanted.items():
            _verify_acl(aclkey, perms)

        for aclkey, perms in existing.items():
            if aclkey not in wanted:
                extra.append(self._acl2string(aclkey, perms))

        msg = []
        if missing:
            msg.append("%s ACLs are missing: %s" % (len(missing),
                                                    ", ".join(missing)))
        if wrong:
            msg.append("%s ACLs are wrong: %s" %
                       (len(wrong),
                        "; ".join(["%s should be %s" % (e, w)
                                   for w, e in wrong])))
        if extra:
            msg.append("%s extra ACLs: %s" % (len(extra), ", ".join(extra)))

        if msg:
            msg.insert(0, "POSIX: ACLs for %s are incorrect." % path)
            self.logger.debug(msg[0])
            for line in msg[1:]:
                self.logger.debug("  " + line)
            entry.set('qtext', "\n".join([entry.get("qtext", '')] + msg))
            return False
        return True

    def _makedirs(self, entry, path=None):
        """ os.makedirs helpfully creates all parent directories for
        us, but it sets permissions according to umask, which is
        probably wrong.  we need to find out which directories were
        created and try to set permissions on those
        (http://trac.mcs.anl.gov/projects/bcfg2/ticket/1125 and
        http://trac.mcs.anl.gov/projects/bcfg2/ticket/1134) """
        created = []
        if path is None:
            path = entry.get("name")
        cur = path
        while cur and cur != '/':
            if not os.path.exists(cur):
                created.append(cur)
            cur = os.path.dirname(cur)
        rv = True
        try:
            os.makedirs(path)
        except OSError:
            err = sys.exc_info()[1]
            self.logger.error('POSIX: Failed to create directory %s: %s' %
                              (path, err))
            rv = False

        # set auto-created directories to mode 755 and use best effort for
        # permissions.  If you need something else, you should specify it in
        # your config.
        tmpentry = copy.deepcopy(entry)
        tmpentry.set('mode', '0755')
        for acl in tmpentry.findall('ACL'):
            acl.set('perms',
                    oct_mode(self._norm_acl_perms(acl.get('perms')) |
                             ACL_MAP['x']))
        for cpath in created:
            self._set_perms(tmpentry, path=cpath)
        return rv


class POSIXLinkTool(POSIXTool):
    """ Base handler for link (symbolic and hard) entries """
    __req__ = ['name', 'to']
    __linktype__ = None

    def verify(self, entry, modlist):
        rv = True

        try:
            if not self._verify(entry):
                msg = "%s %s is incorrect" % (self.__linktype__.title(),
                                              entry.get('name'))
                self.logger.debug("POSIX: " + msg)
                entry.set('qtext', "\n".join([entry.get('qtext', ''), msg]))
                rv = False
        except OSError:
            self.logger.debug("POSIX: %s %s does not exist" %
                              (entry.tag, entry.get("name")))
            entry.set('current_exists', 'false')
            return False

        return POSIXTool.verify(self, entry, modlist) and rv

    def _verify(self, entry):
        """ perform actual verification of the link entry """
        raise NotImplementedError

    def install(self, entry):
        ondisk = self._exists(entry, remove=True)
        if ondisk:
            self.logger.info("POSIX: %s %s cleanup failed" %
                             (self.__linktype__.title(), entry.get('name')))
        try:
            self._link(entry)
            rv = True
        except OSError:
            err = sys.exc_info()[1]
            self.logger.error("POSIX: Failed to create %s %s to %s: %s" %
                              (self.__linktype__, entry.get('name'),
                               entry.get('to'), err))
            rv = False
        return POSIXTool.install(self, entry) and rv

    def _link(self, entry):
        """ create the link """
        raise NotImplementedError

########NEW FILE########
__FILENAME__ = Device
""" Handle <Path type='nonexistent' ...> entries """

import os
import sys
from Bcfg2.Client.Tools.POSIX.base import POSIXTool, device_map


class POSIXDevice(POSIXTool):
    """ Handle <Path type='nonexistent' ...> entries """
    __req__ = ['name', 'dev_type', 'mode', 'owner', 'group']

    def fully_specified(self, entry):
        if entry.get('dev_type') in ['block', 'char']:
            # check if major/minor are properly specified
            if (entry.get('major') is None or
                entry.get('minor') is None):
                return False
        return True

    def verify(self, entry, modlist):
        """Verify device entry."""
        ondisk = self._exists(entry)
        if not ondisk:
            return False

        # attempt to verify device properties as specified in config
        rv = True
        dev_type = entry.get('dev_type')
        if dev_type in ['block', 'char']:
            major = int(entry.get('major'))
            minor = int(entry.get('minor'))
            if major != os.major(ondisk.st_rdev):
                msg = ("Major number for device %s is incorrect. "
                       "Current major is %s but should be %s" %
                       (entry.get("name"), os.major(ondisk.st_rdev), major))
                self.logger.debug('POSIX: ' + msg)
                entry.set('qtext', entry.get('qtext', '') + "\n" + msg)
                rv = False

            if minor != os.minor(ondisk.st_rdev):
                msg = ("Minor number for device %s is incorrect. "
                       "Current minor is %s but should be %s" %
                       (entry.get("name"), os.minor(ondisk.st_rdev), minor))
                self.logger.debug('POSIX: ' + msg)
                entry.set('qtext', entry.get('qtext', '') + "\n" + msg)
                rv = False
        return POSIXTool.verify(self, entry, modlist) and rv

    def install(self, entry):
        if not self._exists(entry, remove=True):
            try:
                dev_type = entry.get('dev_type')
                mode = device_map[dev_type] | int(entry.get('mode'), 8)
                if dev_type in ['block', 'char']:
                    major = int(entry.get('major'))
                    minor = int(entry.get('minor'))
                    device = os.makedev(major, minor)
                    os.mknod(entry.get('name'), mode, device)
                else:
                    os.mknod(entry.get('name'), mode)
            except (KeyError, OSError, ValueError):
                err = sys.exc_info()[1]
                self.logger.error('POSIX: Failed to install %s: %s' %
                                  (entry.get('name'), err))
                return False
        return POSIXTool.install(self, entry)

########NEW FILE########
__FILENAME__ = Directory
""" Handle <Path type='directory' ...> entries """

import os
import sys
import stat
import Bcfg2.Client.XML
from Bcfg2.Client.Tools.POSIX.base import POSIXTool


class POSIXDirectory(POSIXTool):
    """ Handle <Path type='directory' ...> entries """
    __req__ = ['name', 'mode', 'owner', 'group']

    def verify(self, entry, modlist):
        ondisk = self._exists(entry)
        if not ondisk:
            return False

        if not stat.S_ISDIR(ondisk[stat.ST_MODE]):
            self.logger.info("POSIX: %s is not a directory" %
                             entry.get('name'))
            return False

        prune = True
        if entry.get('prune', 'false').lower() == 'true':
            # check for any extra entries when prune='true' attribute is set
            try:
                extras = [os.path.join(entry.get('name'), ent)
                          for ent in os.listdir(entry.get('name'))
                          if os.path.join(entry.get('name'),
                                          ent) not in modlist]
                if extras:
                    prune = False
                    msg = "Directory %s contains extra entries: %s" % \
                        (entry.get('name'), "; ".join(extras))
                    self.logger.info("POSIX: " + msg)
                    entry.set('qtext', entry.get('qtext', '') + '\n' + msg)
                    for extra in extras:
                        Bcfg2.Client.XML.SubElement(entry, 'Prune', name=extra)
            except OSError:
                prune = True

        return POSIXTool.verify(self, entry, modlist) and prune

    def install(self, entry):
        """Install directory entries."""
        fmode = self._exists(entry)

        if fmode and not stat.S_ISDIR(fmode[stat.ST_MODE]):
            self.logger.info("POSIX: Found a non-directory entry at %s, "
                             "removing" % entry.get('name'))
            try:
                os.unlink(entry.get('name'))
                fmode = False
            except OSError:
                err = sys.exc_info()[1]
                self.logger.error("POSIX: Failed to unlink %s: %s" %
                                  (entry.get('name'), err))
                return False
        elif fmode:
            self.logger.debug("POSIX: Found a pre-existing directory at %s" %
                              entry.get('name'))

        rv = True
        if not fmode:
            rv &= self._makedirs(entry)

        if entry.get('prune', 'false') == 'true':
            for pent in entry.findall('Prune'):
                pname = pent.get('name')
                try:
                    self.logger.debug("POSIX: Removing %s" % pname)
                    self._remove(pent)
                except OSError:
                    err = sys.exc_info()[1]
                    self.logger.error("POSIX: Failed to unlink %s: %s" %
                                      (pname, err))
                    rv = False
        return POSIXTool.install(self, entry) and rv

########NEW FILE########
__FILENAME__ = File
""" Handle <Path type='file' ...> entries """

import os
import sys
import stat
import time
import difflib
import tempfile
from Bcfg2.Client.Tools.POSIX.base import POSIXTool
from Bcfg2.Compat import unicode, b64encode, b64decode  # pylint: disable=W0622


class POSIXFile(POSIXTool):
    """ Handle <Path type='file' ...> entries """
    __req__ = ['name', 'mode', 'owner', 'group']

    def fully_specified(self, entry):
        return entry.text is not None or entry.get('empty', 'false') == 'true'

    def _is_string(self, strng, encoding):
        """ Returns true if the string contains no ASCII control
        characters and can be decoded from the specified encoding. """
        for char in strng:
            if ord(char) < 9 or ord(char) > 13 and ord(char) < 32:
                return False
        if not hasattr(strng, "decode"):
            # py3k
            return True
        try:
            strng.decode(encoding)
            return True
        except:  # pylint: disable=W0702
            return False

    def _get_data(self, entry):
        """ Get a tuple of (<file data>, <is binary>) for the given entry """
        is_binary = entry.get('encoding', 'ascii') == 'base64'
        if entry.get('empty', 'false') == 'true' or not entry.text:
            tempdata = ''
        elif is_binary:
            tempdata = b64decode(entry.text)
        else:
            tempdata = entry.text
            if isinstance(tempdata, unicode) and unicode != str:
                try:
                    tempdata = tempdata.encode(self.setup['encoding'])
                except UnicodeEncodeError:
                    err = sys.exc_info()[1]
                    self.logger.error("POSIX: Error encoding file %s: %s" %
                                      (entry.get('name'), err))
        return (tempdata, is_binary)

    def verify(self, entry, modlist):
        ondisk = self._exists(entry)
        tempdata, is_binary = self._get_data(entry)
        if isinstance(tempdata, str) and str != unicode:
            tempdatasize = len(tempdata)
        else:
            tempdatasize = len(tempdata.encode(self.setup['encoding']))

        different = False
        content = None
        if not ondisk:
            # first, see if the target file exists at all; if not,
            # they're clearly different
            different = True
            content = ""
        elif tempdatasize != ondisk[stat.ST_SIZE]:
            # next, see if the size of the target file is different
            # from the size of the desired content
            different = True
        else:
            # finally, read in the target file and compare them
            # directly. comparison could be done with a checksum,
            # which might be faster for big binary files, but slower
            # for everything else
            try:
                content = open(entry.get('name')).read()
            except UnicodeDecodeError:
                content = open(entry.get('name'),
                               encoding=self.setup['encoding']).read()
            except IOError:
                self.logger.error("POSIX: Failed to read %s: %s" %
                                  (entry.get("name"), sys.exc_info()[1]))
                return False
            different = content != tempdata

        if different:
            self.logger.debug("POSIX: %s has incorrect contents" %
                              entry.get("name"))
            self._get_diffs(
                entry, interactive=self.setup['interactive'],
                sensitive=entry.get('sensitive', 'false').lower() == 'true',
                is_binary=is_binary, content=content)
        return POSIXTool.verify(self, entry, modlist) and not different

    def _write_tmpfile(self, entry):
        """ Write the file data to a temp file """
        filedata = self._get_data(entry)[0]
        # get a temp file to write to that is in the same directory as
        # the existing file in order to preserve any permissions
        # protections on that directory, and also to avoid issues with
        # /tmp set nosetuid while creating files that are supposed to
        # be setuid
        try:
            (newfd, newfile) = \
                tempfile.mkstemp(prefix=os.path.basename(entry.get("name")),
                                 dir=os.path.dirname(entry.get("name")))
        except OSError:
            err = sys.exc_info()[1]
            self.logger.error("POSIX: Failed to create temp file in %s: %s" %
                              (os.path.dirname(entry.get('name')), err))
            return False
        try:
            if isinstance(filedata, str) and str != unicode:
                os.fdopen(newfd, 'w').write(filedata)
            else:
                os.fdopen(newfd, 'wb').write(
                    filedata.encode(self.setup['encoding']))
        except (OSError, IOError):
            err = sys.exc_info()[1]
            self.logger.error("POSIX: Failed to open temp file %s for writing "
                              "%s: %s" %
                              (newfile, entry.get("name"), err))
            return False
        return newfile

    def _rename_tmpfile(self, newfile, entry):
        """ Rename the given file to the appropriate filename for entry """
        try:
            os.rename(newfile, entry.get('name'))
            return True
        except OSError:
            err = sys.exc_info()[1]
            self.logger.error("POSIX: Failed to rename temp file %s to %s: %s"
                              % (newfile, entry.get('name'), err))
            try:
                os.unlink(newfile)
            except OSError:
                err = sys.exc_info()[1]
                self.logger.error("POSIX: Could not remove temp file %s: %s" %
                                  (newfile, err))
            return False

    def install(self, entry):
        """Install device entries."""
        if not os.path.exists(os.path.dirname(entry.get('name'))):
            if not self._makedirs(entry,
                                  path=os.path.dirname(entry.get('name'))):
                return False
        newfile = self._write_tmpfile(entry)
        if not newfile:
            return False
        rv = self._set_perms(entry, path=newfile)
        if not self._rename_tmpfile(newfile, entry):
            return False

        return POSIXTool.install(self, entry) and rv

    def _get_diffs(self, entry, interactive=False,  # pylint: disable=R0912
                   sensitive=False, is_binary=False, content=None):
        """ generate the necessary diffs for entry """
        if not interactive and sensitive:
            return

        prompt = [entry.get('qtext', '')]
        attrs = dict()
        if content is None:
            # it's possible that we figured out the files are
            # different without reading in the local file.  if the
            # supplied version of the file is not binary, we now have
            # to read in the local file to figure out if _it_ is
            # binary, and either include that fact or the diff in our
            # prompts for -I and the reports
            try:
                content = open(entry.get('name')).read()
            except UnicodeDecodeError:
                content = open(entry.get('name'), encoding='utf-8').read()
            except IOError:
                self.logger.error("POSIX: Failed to read %s: %s" %
                                  (entry.get("name"), sys.exc_info()[1]))
                return False
        if not is_binary:
            is_binary |= not self._is_string(content, self.setup['encoding'])
        if is_binary:
            # don't compute diffs if the file is binary
            prompt.append('Binary file, no printable diff')
            attrs['current_bfile'] = b64encode(content)
        else:
            if interactive:
                diff = self._diff(content, self._get_data(entry)[0],
                                  difflib.unified_diff,
                                  filename=entry.get("name"))
                if diff:
                    udiff = '\n'.join(l.rstrip('\n') for l in diff)
                    if hasattr(udiff, "decode"):
                        udiff = udiff.decode(self.setup['encoding'])
                    try:
                        prompt.append(udiff)
                    except UnicodeEncodeError:
                        prompt.append("Could not encode diff")
                elif entry.get("empty", "true"):
                    # the file doesn't exist on disk, but there's no
                    # expected content
                    prompt.append("%s does not exist" % entry.get("name"))
                else:
                    prompt.append("Diff took too long to compute, no "
                                  "printable diff")
            if not sensitive:
                diff = self._diff(content, self._get_data(entry)[0],
                                  difflib.ndiff, filename=entry.get("name"))
                if diff:
                    attrs["current_bdiff"] = b64encode("\n".join(diff))
                else:
                    attrs['current_bfile'] = b64encode(content)
        if interactive:
            entry.set("qtext", "\n".join(prompt))
        if not sensitive:
            for attr, val in attrs.items():
                entry.set(attr, val)

    def _diff(self, content1, content2, difffunc, filename=None):
        """ Return a diff of the two strings, as produced by difffunc.
        warns after 5 seconds and times out after 30 seconds. """
        rv = []
        start = time.time()
        longtime = False
        for diffline in difffunc(content1.split('\n'),
                                 content2.split('\n')):
            now = time.time()
            rv.append(diffline)
            if now - start > 5 and not longtime:
                if filename:
                    self.logger.info("POSIX: Diff of %s taking a long time" %
                                     filename)
                else:
                    self.logger.info("POSIX: Diff taking a long time")
                longtime = True
            elif now - start > 30:
                if filename:
                    self.logger.error("POSIX: Diff of %s took too long; "
                                      "giving up" % filename)
                else:
                    self.logger.error("POSIX: Diff took too long; giving up")
                return False
        return rv

########NEW FILE########
__FILENAME__ = Hardlink
""" Handle <Path type="hardlink" ...> entries """

import os
from Bcfg2.Client.Tools.POSIX.base import POSIXLinkTool


class POSIXHardlink(POSIXLinkTool):
    """ Handle <Path type="hardlink" ...> entries """
    __linktype__ = "hardlink"

    def _verify(self, entry):
        return os.path.samefile(entry.get('name'), entry.get('to'))

    def _link(self, entry):
        return os.link(entry.get('to'), entry.get('name'))

########NEW FILE########
__FILENAME__ = Nonexistent
""" Handle <Path type='nonexistent' ...> entries """

import os
import sys
from Bcfg2.Client.Tools.POSIX.base import POSIXTool


class POSIXNonexistent(POSIXTool):
    """ Handle <Path type='nonexistent' ...> entries """
    __req__ = ['name']

    def verify(self, entry, _):
        if os.path.lexists(entry.get('name')):
            self.logger.debug("POSIX: %s exists but should not" %
                              entry.get("name"))
            return False
        return True

    def install(self, entry):
        ename = entry.get('name')
        recursive = entry.get('recursive', '').lower() == 'true'
        if recursive:
            # ensure that configuration spec is consistent first
            for struct in self.config.getchildren():
                for el in struct.getchildren():
                    if (el.tag == 'Path' and
                        el.get('type') != 'nonexistent' and
                        el.get('name').startswith(ename)):
                        self.logger.error('POSIX: Not removing %s. One or '
                                          'more files in this directory are '
                                          'specified in your configuration.' %
                                          ename)
                        return False
        try:
            self._remove(entry, recursive=recursive)
            return True
        except OSError:
            err = sys.exc_info()[1]
            self.logger.error('POSIX: Failed to remove %s: %s' % (ename, err))
            return False

########NEW FILE########
__FILENAME__ = Permissions
""" Handle <Path type='permissions' ...> entries """

from Bcfg2.Client.Tools.POSIX.base import POSIXTool


class POSIXPermissions(POSIXTool):
    """ Handle <Path type='permissions' ...> entries """
    __req__ = ['name', 'mode', 'owner', 'group']

########NEW FILE########
__FILENAME__ = Symlink
""" Handle <Path type="symlink" ...> entries """

import os
from Bcfg2.Client.Tools.POSIX.base import POSIXLinkTool


class POSIXSymlink(POSIXLinkTool):
    """ Handle <Path type="symlink" ...> entries """
    __linktype__ = "symlink"

    def _verify(self, entry):
        sloc = os.readlink(entry.get('name'))
        if sloc != entry.get('to'):
            entry.set('current_to', sloc)
            return False
        return True

    def _link(self, entry):
        return os.symlink(entry.get('to'), entry.get('name'))

########NEW FILE########
__FILENAME__ = POSIXUsers
""" A tool to handle creating users and groups with useradd/mod/del
and groupadd/mod/del """

import pwd
import grp
import Bcfg2.Client.XML
import Bcfg2.Client.Tools
from Bcfg2.Utils import PackedDigitRange


class POSIXUsers(Bcfg2.Client.Tools.Tool):
    """ A tool to handle creating users and groups with
    useradd/mod/del and groupadd/mod/del """
    __execs__ = ['/usr/sbin/useradd', '/usr/sbin/usermod', '/usr/sbin/userdel',
                 '/usr/sbin/groupadd', '/usr/sbin/groupmod',
                 '/usr/sbin/groupdel']
    __handles__ = [('POSIXUser', None),
                   ('POSIXGroup', None)]
    __req__ = dict(POSIXUser=['name'],
                   POSIXGroup=['name'])
    experimental = True

    #: A mapping of XML entry attributes to the indexes of
    #: corresponding values in the get{pw|gr}all data structures
    attr_mapping = dict(POSIXUser=dict(name=0, uid=2, gecos=4, home=5,
                                       shell=6),
                        POSIXGroup=dict(name=0, gid=2))

    #: A mapping that describes the attribute name of the id of a given
    #: user or group
    id_mapping = dict(POSIXUser="uid", POSIXGroup="gid")

    def __init__(self, logger, setup, config):
        Bcfg2.Client.Tools.Tool.__init__(self, logger, setup, config)
        self.set_defaults = dict(POSIXUser=self.populate_user_entry,
                                 POSIXGroup=lambda g: g)
        self._existing = None
        self._whitelist = dict(POSIXUser=None, POSIXGroup=None)
        self._blacklist = dict(POSIXUser=None, POSIXGroup=None)
        if self.setup['posix_uid_whitelist']:
            self._whitelist['POSIXUser'] = \
                PackedDigitRange(*self.setup['posix_uid_whitelist'])
        else:
            self._blacklist['POSIXUser'] = \
                PackedDigitRange(*self.setup['posix_uid_blacklist'])
        if self.setup['posix_gid_whitelist']:
            self._whitelist['POSIXGroup'] = \
                PackedDigitRange(*self.setup['posix_gid_whitelist'])
        else:
            self._blacklist['POSIXGroup'] = \
                PackedDigitRange(*self.setup['posix_gid_blacklist'])

    @property
    def existing(self):
        """ Get a dict of existing users and groups """
        if self._existing is None:
            self._existing = dict(POSIXUser=dict([(u[0], u)
                                                  for u in pwd.getpwall()]),
                                  POSIXGroup=dict([(g[0], g)
                                                   for g in grp.getgrall()]))
        return self._existing

    def _in_managed_range(self, tag, eid):
        """ Check if the given uid or gid is in the appropriate
        managed range.  This means that either a) a whitelist is
        defined, and the uid/gid is in that whitelist; or b) no
        whitelist is defined, and the uid/gid is not in the
        blacklist. """
        if self._whitelist[tag] is None:
            return eid not in self._blacklist[tag]
        else:
            return eid in self._whitelist[tag]

    def canInstall(self, entry):
        if not Bcfg2.Client.Tools.Tool.canInstall(self, entry):
            return False
        eid = entry.get(self.id_mapping[entry.tag])
        if eid is not None and not self._in_managed_range(entry.tag, eid):
            if self._whitelist[entry.tag] is not None:
                err = "not in whitelist"
            else:  # blacklisted
                err = "in blacklist"
            self.logger.debug("%s: %s %s %s: %s" %
                              (self.primarykey(entry), err,
                               self.id_mapping[entry.tag], eid,
                               self._blacklist[entry.tag]))
            return False
        return True

    def Inventory(self, states, structures=None):
        if not structures:
            structures = self.config.getchildren()
        # we calculate a list of all POSIXUser and POSIXGroup entries,
        # and then add POSIXGroup entries that are required to create
        # the primary group for each user to the structures.  this is
        # sneaky and possibly evil, but it works great.
        groups = []
        for struct in structures:
            groups.extend([e.get("name")
                           for e in struct.findall("POSIXGroup")])
        for struct in structures:
            for entry in struct.findall("POSIXUser"):
                group = self.set_defaults[entry.tag](entry).get('group')
                if group and group not in groups:
                    self.logger.debug("POSIXUsers: Adding POSIXGroup entry "
                                      "'%s' for user '%s'" %
                                      (group, entry.get("name")))
                    struct.append(Bcfg2.Client.XML.Element("POSIXGroup",
                                                           name=group))
        return Bcfg2.Client.Tools.Tool.Inventory(self, states, structures)

    def FindExtra(self):
        extra = []
        for handles in self.__handles__:
            tag = handles[0]
            specified = []
            for entry in self.getSupportedEntries():
                if entry.tag == tag:
                    specified.append(entry.get("name"))
            for name, data in self.existing[tag].items():
                eid = data[self.attr_mapping[tag][self.id_mapping[tag]]]
                if name not in specified and self._in_managed_range(tag, eid):
                    extra.append(Bcfg2.Client.XML.Element(tag, name=name))

        return extra

    def populate_user_entry(self, entry):
        """ Given a POSIXUser entry, set all of the 'missing' attributes
        with their defaults """
        defaults = dict(group=entry.get('name'),
                        gecos=entry.get('name'),
                        shell='/bin/bash')
        if entry.get('name') == 'root':
            defaults['home'] = '/root'
        else:
            defaults['home'] = '/home/%s' % entry.get('name')
        for key, val in defaults.items():
            if entry.get(key) is None:
                entry.set(key, val)
        if entry.get('group') in self.existing['POSIXGroup']:
            entry.set('gid',
                      str(self.existing['POSIXGroup'][entry.get('group')][2]))
        return entry

    def user_supplementary_groups(self, entry):
        """ Get a list of supplmentary groups that the user in the
        given entry is a member of """
        return [g for g in self.existing['POSIXGroup'].values()
                if entry.get("name") in g[3] and g[0] != entry.get("group")
                and self._in_managed_range('POSIXGroup', g[2])]

    def VerifyPOSIXUser(self, entry, _):
        """ Verify a POSIXUser entry """
        rv = self._verify(self.populate_user_entry(entry))
        if entry.get("current_exists", "true") == "true":
            # verify supplemental groups
            actual = [g[0] for g in self.user_supplementary_groups(entry)]
            expected = [e.get("group", e.text).strip()
                        for e in entry.findall("MemberOf")]
            if set(expected) != set(actual):
                entry.set('qtext',
                          "\n".join([entry.get('qtext', '')] +
                                    ["%s %s has incorrect supplemental group "
                                     "membership. Currently: %s. Should be: %s"
                                     % (entry.tag, entry.get("name"),
                                        actual, expected)]))
                rv = False
        if self.setup['interactive'] and not rv:
            entry.set('qtext',
                      '%s\nInstall %s %s: (y/N) ' %
                      (entry.get('qtext', ''), entry.tag, entry.get('name')))
        return rv

    def VerifyPOSIXGroup(self, entry, _):
        """ Verify a POSIXGroup entry """
        rv = self._verify(entry)
        if self.setup['interactive'] and not rv:
            entry.set('qtext',
                      '%s\nInstall %s %s: (y/N) ' %
                      (entry.get('qtext', ''), entry.tag, entry.get('name')))
        return rv

    def _verify(self, entry):
        """ Perform most of the actual work of verification """
        errors = []
        if entry.get("name") not in self.existing[entry.tag]:
            entry.set('current_exists', 'false')
            errors.append("%s %s does not exist" % (entry.tag,
                                                    entry.get("name")))
        else:
            for attr, idx in self.attr_mapping[entry.tag].items():
                val = str(self.existing[entry.tag][entry.get("name")][idx])
                entry.set("current_%s" %
                          attr, val.decode(self.setup['encoding']))
                if attr in ["uid", "gid"]:
                    if entry.get(attr) is None:
                        # no uid/gid specified, so we let the tool
                        # automatically determine one -- i.e., it always
                        # verifies
                        continue
                entval = entry.get(attr)
                if not isinstance(entval, str):
                    entval = entval.encode('utf-8')
                if val != entval:
                    errors.append("%s for %s %s is incorrect.  Current %s is "
                                  "%s, but should be %s" %
                                  (attr.title(), entry.tag, entry.get("name"),
                                   attr, val, entry.get(attr)))

        if errors:
            for error in errors:
                self.logger.debug("%s: %s" % (self.name, error))
            entry.set('qtext', "\n".join([entry.get('qtext', '')] + errors))
        return len(errors) == 0

    def Install(self, entries, states):
        for entry in entries:
            # install groups first, so that all groups exist for
            # users that might need them
            if entry.tag == 'POSIXGroup':
                states[entry] = self._install(entry)
        for entry in entries:
            if entry.tag == 'POSIXUser':
                states[entry] = self._install(entry)
        self._existing = None

    def _install(self, entry):
        """ add or modify a user or group using the appropriate command """
        if entry.get("name") not in self.existing[entry.tag]:
            action = "add"
        else:
            action = "mod"
        rv = self.cmd.run(self._get_cmd(action,
                                        self.set_defaults[entry.tag](entry)))
        if rv.success:
            self.modified.append(entry)
        else:
            self.logger.error("POSIXUsers: Error creating %s %s: %s" %
                              (entry.tag, entry.get("name"), rv.error))
        return rv.success

    def _get_cmd(self, action, entry):
        """ Get a command to perform the appropriate action (add, mod,
        del) on the given entry.  The command is always the same; we
        set all attributes on a given user or group when modifying it
        rather than checking which ones need to be changed.  This
        makes things fail as a unit (e.g., if a user is logged in, you
        can't change its home dir, but you could change its GECOS, but
        the whole operation fails), but it also makes this function a
        lot, lot easier and simpler."""
        cmd = ["/usr/sbin/%s%s" % (entry.tag[5:].lower(), action)]
        if action != 'del':
            if entry.tag == 'POSIXGroup':
                if entry.get('gid'):
                    cmd.extend(['-g', entry.get('gid')])
            elif entry.tag == 'POSIXUser':
                if entry.get('uid'):
                    cmd.extend(['-u', entry.get('uid')])
                cmd.extend(['-g', entry.get('group')])
                extras = [e.get("group", e.text).strip()
                          for e in entry.findall("MemberOf")]
                if extras:
                    cmd.extend(['-G', ",".join(extras)])
                cmd.extend(['-d', entry.get('home')])
                cmd.extend(['-s', entry.get('shell')])
                cmd.extend(['-c', entry.get('gecos')])
        cmd.append(entry.get('name'))
        return cmd

    def Remove(self, entries):
        for entry in entries:
            # remove users first, so that all users have been removed
            # from groups before we remove them
            if entry.tag == 'POSIXUser':
                self._remove(entry)
        for entry in entries:
            if entry.tag == 'POSIXGroup':
                try:
                    grp.getgrnam(entry.get("name"))
                    self._remove(entry)
                except KeyError:
                    # at least some versions of userdel automatically
                    # remove the primary group for a user if the group
                    # name is the same as the username, and no other
                    # users are in the group
                    self.logger.info("POSIXUsers: Group %s does not exist. "
                                     "It may have already been removed when "
                                     "its users were deleted" %
                                     entry.get("name"))
        self._existing = None
        self.extra = self.FindExtra()

    def _remove(self, entry):
        """ Remove an entry """
        rv = self.cmd.run(self._get_cmd("del", entry))
        if not rv.success:
            self.logger.error("POSIXUsers: Error deleting %s %s: %s" %
                              (entry.tag, entry.get("name"), rv.error))
        return rv.success

########NEW FILE########
__FILENAME__ = RcUpdate
"""This is rc-update support."""

import os
import Bcfg2.Client.Tools
import Bcfg2.Client.XML


class RcUpdate(Bcfg2.Client.Tools.SvcTool):
    """RcUpdate support for Bcfg2."""
    name = 'RcUpdate'
    __execs__ = ['/sbin/rc-update', '/bin/rc-status']
    __handles__ = [('Service', 'rc-update')]
    __req__ = {'Service': ['name', 'status']}

    def get_enabled_svcs(self):
        """
        Return a list of all enabled services.
        """
        return [line.split()[0]
                for line in self.cmd.run(['/bin/rc-status',
                                          '-s']).stdout.splitlines()
                if 'started' in line]

    def get_default_svcs(self):
        """Return a list of services in the 'default' runlevel."""
        return [line.split()[0]
                for line in self.cmd.run(['/sbin/rc-update',
                                          'show']).stdout.splitlines()
                if 'default' in line]

    def verify_bootstatus(self, entry, bootstatus):
        """Verify bootstatus for entry."""
        # get a list of all started services
        allsrv = self.get_default_svcs()
        # set current_bootstatus attribute
        if entry.get('name') in allsrv:
            entry.set('current_bootstatus', 'on')
        else:
            entry.set('current_bootstatus', 'off')
        if bootstatus == 'on':
            return entry.get('name') in allsrv
        else:
            return entry.get('name') not in allsrv

    def VerifyService(self, entry, _):
        """
        Verify Service status for entry.
        Assumes we run in the "default" runlevel.

        """
        entry.set('target_status', entry.get('status'))  # for reporting
        bootstatus = self.get_bootstatus(entry)
        if bootstatus is None:
            return True
        current_bootstatus = self.verify_bootstatus(entry, bootstatus)

        # check if init script exists
        try:
            os.stat('/etc/init.d/%s' % entry.get('name'))
        except OSError:
            self.logger.debug('Init script for service %s does not exist' %
                              entry.get('name'))
            return False

        if entry.get('status') == 'ignore':
            # 'ignore' should verify
            current_svcstatus = True
            svcstatus = True
        else:
            svcstatus = self.check_service(entry)
            if entry.get('status') == 'on':
                if svcstatus:
                    current_svcstatus = True
                else:
                    current_svcstatus = False
            elif entry.get('status') == 'off':
                if svcstatus:
                    current_svcstatus = False
                else:
                    current_svcstatus = True

        if svcstatus:
            entry.set('current_status', 'on')
        else:
            entry.set('current_status', 'off')

        return current_bootstatus and current_svcstatus

    def InstallService(self, entry):
        """Install Service entry."""
        self.logger.info('Installing Service %s' % entry.get('name'))
        bootstatus = self.get_bootstatus(entry)
        if bootstatus is not None:
            if bootstatus == 'on':
                # make sure service is enabled on boot
                bootcmd = '/sbin/rc-update add %s default'
            elif bootstatus == 'off':
                # make sure service is disabled on boot
                bootcmd = '/sbin/rc-update del %s default'
            bootcmdrv = self.cmd.run(bootcmd % entry.get('name')).success
            if self.setup['servicemode'] == 'disabled':
                # 'disabled' means we don't attempt to modify running svcs
                return bootcmdrv
            buildmode = self.setup['servicemode'] == 'build'
            if (entry.get('status') == 'on' and not buildmode) and \
               entry.get('current_status') == 'off':
                svccmdrv = self.start_service(entry)
            elif (entry.get('status') == 'off' or buildmode) and \
                    entry.get('current_status') == 'on':
                svccmdrv = self.stop_service(entry)
            else:
                svccmdrv = True  # ignore status attribute
            return bootcmdrv and svccmdrv
        else:
            # when bootstatus is 'None', status == 'ignore'
            return True

    def FindExtra(self):
        """Locate extra rc-update services."""
        allsrv = self.get_enabled_svcs()
        self.logger.debug('Found active services:')
        self.logger.debug(allsrv)
        specified = [srv.get('name') for srv in self.getSupportedEntries()]
        return [Bcfg2.Client.XML.Element('Service', type='rc-update',
                                         name=name)
                for name in allsrv if name not in specified]

########NEW FILE########
__FILENAME__ = RPM
"""Bcfg2 Support for RPMS"""

import os.path
import rpm
import rpmtools
import Bcfg2.Client.Tools

class RPM(Bcfg2.Client.Tools.PkgTool):
    """Support for RPM packages."""
    __execs__ = ['/bin/rpm', '/var/lib/rpm']
    __handles__ = [('Package', 'rpm')]

    __req__ = {'Package': ['name', 'version']}
    __ireq__ = {'Package': ['url']}

    __new_req__ = {'Package': ['name'],
                   'Instance': ['version', 'release', 'arch']}
    __new_ireq__ = {'Package': ['uri'], \
                    'Instance': ['simplefile']}

    __gpg_req__ = {'Package': ['name', 'version']}
    __gpg_ireq__ = {'Package': ['name', 'version']}

    __new_gpg_req__ = {'Package': ['name'],
                       'Instance': ['version', 'release']}
    __new_gpg_ireq__ = {'Package': ['name'],
                        'Instance': ['version', 'release']}

    conflicts = ['RPMng']

    pkgtype = 'rpm'
    pkgtool = ("rpm --oldpackage --replacepkgs --quiet -U %s", ("%s", ["url"]))

    def __init__(self, logger, setup, config):
        Bcfg2.Client.Tools.PkgTool.__init__(self, logger, setup, config)

        # create a global ignore list used when ignoring particular
        # files during package verification
        self.ignores = [entry.get('name') for struct in config for entry in struct \
                        if entry.get('type') == 'ignore']
        self.instance_status = {}
        self.extra_instances = []
        self.modlists = {}
        self.gpg_keyids = self.getinstalledgpg()

        opt_prefix = self.name.lower()
        self.installOnlyPkgs = self.setup["%s_installonly" % opt_prefix]
        if 'gpg-pubkey' not in self.installOnlyPkgs:
            self.installOnlyPkgs.append('gpg-pubkey')
        self.erase_flags = self.setup['%s_erase_flags' % opt_prefix]
        self.pkg_checks = self.setup['%s_pkg_checks' % opt_prefix]
        self.pkg_verify = self.setup['%s_pkg_verify' % opt_prefix]
        self.installed_action = self.setup['%s_installed_action' % opt_prefix]
        self.version_fail_action = self.setup['%s_version_fail_action' %
                                              opt_prefix]
        self.verify_fail_action = self.setup['%s_verify_fail_action' %
                                             opt_prefix]
        self.verify_flags = self.setup['%s_verify_flags' % opt_prefix]
        if '' in self.verify_flags:
            self.verify_flags.remove('')

        self.logger.debug('%s: installOnlyPackages = %s' %
                          (self.name, self.installOnlyPkgs))
        self.logger.debug('%s: erase_flags = %s' %
                          (self.name, self.erase_flags))
        self.logger.debug('%s: pkg_checks = %s' %
                          (self.name, self.pkg_checks))
        self.logger.debug('%s: pkg_verify = %s' %
                          (self.name, self.pkg_verify))
        self.logger.debug('%s: installed_action = %s' %
                          (self.name, self.installed_action))
        self.logger.debug('%s: version_fail_action = %s' %
                          (self.name, self.version_fail_action))
        self.logger.debug('%s: verify_fail_action = %s' %
                          (self.name, self.verify_fail_action))
        self.logger.debug('%s: verify_flags = %s' %
                          (self.name, self.verify_flags))

        # Force a re- prelink of all packages if prelink exists.
        # Many, if not most package verifies can be caused by out of
        # date prelinking.
        if os.path.isfile('/usr/sbin/prelink') and not self.setup['dryrun']:
            rv = self.cmd.run('/usr/sbin/prelink -a -mR')
            if rv.success:
                self.logger.debug('Pre-emptive prelink succeeded')
            else:
                # FIXME : this is dumb - what if the output is huge?
                self.logger.error('Pre-emptive prelink failed: %s' % rv.error)

    def RefreshPackages(self):
        """
            Creates self.installed{} which is a dict of installed packages.

            The dict items are lists of nevra dicts.  This loosely matches the
            config from the server and what rpmtools uses to specify pacakges.

            e.g.

            self.installed['foo'] = [ {'name':'foo', 'epoch':None,
                                       'version':'1', 'release':2,
                                       'arch':'i386'},
                                      {'name':'foo', 'epoch':None,
                                       'version':'1', 'release':2,
                                       'arch':'x86_64'} ]
        """
        self.installed = {}
        refresh_ts = rpmtools.rpmtransactionset()
        # Don't bother with signature checks at this stage. The GPG keys might
        # not be installed.
        refresh_ts.setVSFlags(rpm._RPMVSF_NODIGESTS|rpm._RPMVSF_NOSIGNATURES)
        for nevra in rpmtools.rpmpackagelist(refresh_ts):
            self.installed.setdefault(nevra['name'], []).append(nevra)
        if self.setup['debug']:
            print("The following package instances are installed:")
            for name, instances in list(self.installed.items()):
                self.logger.debug("    " + name)
                for inst in instances:
                    self.logger.debug("        %s" %self.str_evra(inst))
        refresh_ts.closeDB()
        del refresh_ts

    def VerifyPackage(self, entry, modlist, pinned_version=None):
        """
            Verify Package status for entry.
            Performs the following:
                - Checks for the presence of required Package Instances.
                - Compares the evra 'version' info against self.installed{}.
                - RPM level package verify (rpm --verify).
                - Checks for the presence of unrequired package instances.

            Produces the following dict and list for RPM.Install() to use:
              For installs/upgrades/fixes of required instances:
                instance_status = { <Instance Element Object>:
                                       { 'installed': True|False,
                                         'version_fail': True|False,
                                         'verify_fail': True|False,
                                         'pkg': <Package Element Object>,
                                         'modlist': [ <filename>, ... ],
                                         'verify' : [ <rpm --verify results> ]
                                       }, ......
                                  }

              For deletions of unrequired instances:
                extra_instances = [ <Package Element Object>, ..... ]

              Constructs the text prompts for interactive mode.
        """
        instances = [inst for inst in entry if inst.tag == 'Instance' or inst.tag == 'Package']
        if instances == []:
            # We have an old style no Instance entry. Convert it to new style.
            instance = Bcfg2.Client.XML.SubElement(entry, 'Package')
            for attrib in list(entry.attrib.keys()):
                instance.attrib[attrib] = entry.attrib[attrib]
            if (self.pkg_checks and
                entry.get('pkg_checks', 'true').lower() == 'true'):
                if 'any' in [entry.get('version'), pinned_version]:
                    version, release = 'any', 'any'
                elif entry.get('version') == 'auto':
                    if pinned_version != None:
                        version, release = pinned_version.split('-')
                    else:
                        return False
                else:
                    version, release = entry.get('version').split('-')
                instance.set('version', version)
                instance.set('release', release)
                if entry.get('verify', 'true') == 'false':
                    instance.set('verify', 'false')
            instances = [ instance ]

        self.logger.debug("Verifying package instances for %s" % entry.get('name'))
        package_fail = False
        qtext_versions = ''

        if entry.get('name') in self.installed:
            # There is at least one instance installed.
            if (self.pkg_checks and
                entry.get('pkg_checks', 'true').lower() == 'true'):
                rpmTs = rpm.TransactionSet()
                rpmHeader = None
                for h in rpmTs.dbMatch(rpm.RPMTAG_NAME, entry.get('name')):
                    if rpmHeader is None or rpm.versionCompare(h, rpmHeader) > 0:
                        rpmHeader = h
                rpmProvides = [ h['provides'] for h in \
                            rpmTs.dbMatch(rpm.RPMTAG_NAME, entry.get('name')) ]
                rpmIntersection = set(rpmHeader['provides']) & \
                                  set(self.installOnlyPkgs)
                if len(rpmIntersection) > 0:
                    # Packages that should only be installed or removed.
                    # e.g. kernels.
                    self.logger.debug("        Install only package.")
                    for inst in instances:
                        self.instance_status.setdefault(inst, {})['installed'] = False
                        self.instance_status[inst]['version_fail'] = False
                        if inst.tag == 'Package' and len(self.installed[entry.get('name')]) > 1:
                            self.logger.error("WARNING: Multiple instances of package %s are installed." % \
                                              (entry.get('name')))
                        for pkg in self.installed[entry.get('name')]:
                            if inst.get('version') == 'any' or self.pkg_vr_equal(inst, pkg) \
                               or self.inst_evra_equal(inst, pkg):
                                if inst.get('version') == 'any':
                                    self.logger.error("got any version")
                                self.logger.debug("        %s" % self.str_evra(inst))
                                self.instance_status[inst]['installed'] = True

                                if (self.pkg_verify and
                                    inst.get('pkg_verify', 'true').lower() == 'true'):
                                    flags = inst.get('verify_flags', '').split(',') + self.verify_flags
                                    if pkg.get('gpgkeyid', '')[-8:] not in self.gpg_keyids and \
                                       entry.get('name') != 'gpg-pubkey':
                                        flags += ['nosignature', 'nodigest']
                                        self.logger.debug('WARNING: Package %s %s requires GPG Public key with ID %s'\
                                                           % (pkg.get('name'), self.str_evra(pkg), \
                                                              pkg.get('gpgkeyid', '')))
                                        self.logger.debug('         Disabling signature check.')

                                    if self.setup.get('quick', False):
                                        if rpmtools.prelink_exists:
                                            flags += ['nomd5', 'nosize']
                                        else:
                                            flags += ['nomd5']
                                    self.logger.debug("        verify_flags = %s" % flags)

                                    if inst.get('verify', 'true') == 'false':
                                        self.instance_status[inst]['verify'] = None
                                    else:
                                        vp_ts = rpmtools.rpmtransactionset()
                                        self.instance_status[inst]['verify'] = \
                                                                             rpmtools.rpm_verify( vp_ts, pkg, flags)
                                        vp_ts.closeDB()
                                        del vp_ts

                        if self.instance_status[inst]['installed'] == False:
                            self.logger.info("        Package %s %s not installed." % \
                                         (entry.get('name'), self.str_evra(inst)))

                            qtext_versions = qtext_versions + 'I(%s) ' % self.str_evra(inst)
                            entry.set('current_exists', 'false')
                else:
                    # Normal Packages that can be upgraded.
                    for inst in instances:
                        self.instance_status.setdefault(inst, {})['installed'] = False
                        self.instance_status[inst]['version_fail'] = False

                        # Only installed packages with the same architecture are
                        # relevant.
                        if inst.get('arch', None) == None:
                            arch_match = self.installed[entry.get('name')]
                        else:
                            arch_match = [pkg for pkg in self.installed[entry.get('name')] \
                                              if pkg.get('arch', None) == inst.get('arch', None)]

                        if len(arch_match) > 1:
                            self.logger.error("Multiple instances of package %s installed with the same achitecture." % \
                                                  (entry.get('name')))
                        elif len(arch_match) == 1:
                            # There is only one installed like there should be.
                            # Check that it is the right version.
                            for pkg in arch_match:
                                if inst.get('version') == 'any' or self.pkg_vr_equal(inst, pkg) or \
                                       self.inst_evra_equal(inst, pkg):
                                    self.logger.debug("        %s" % self.str_evra(inst))
                                    self.instance_status[inst]['installed'] = True

                                    if (self.pkg_verify and
                                        inst.get('pkg_verify', 'true').lower() == 'true'):
                                        flags = inst.get('verify_flags', '').split(',') + self.verify_flags
                                        if pkg.get('gpgkeyid', '')[-8:] not in self.gpg_keyids and \
                                           'nosignature' not in flags:
                                            flags += ['nosignature', 'nodigest']
                                            self.logger.info('WARNING: Package %s %s requires GPG Public key with ID %s'\
                                                         % (pkg.get('name'), self.str_evra(pkg), \
                                                            pkg.get('gpgkeyid', '')))
                                            self.logger.info('         Disabling signature check.')

                                        if self.setup.get('quick', False):
                                            if rpmtools.prelink_exists:
                                                flags += ['nomd5', 'nosize']
                                            else:
                                                flags += ['nomd5']
                                        self.logger.debug("        verify_flags = %s" % flags)

                                        if inst.get('verify', 'true') == 'false':
                                            self.instance_status[inst]['verify'] = None
                                        else:
                                            vp_ts = rpmtools.rpmtransactionset()
                                            self.instance_status[inst]['verify'] = \
                                                                                 rpmtools.rpm_verify( vp_ts, pkg, flags )
                                            vp_ts.closeDB()
                                            del vp_ts

                                else:
                                    # Wrong version installed.
                                    self.instance_status[inst]['version_fail'] = True
                                    self.logger.info("        Wrong version installed.  Want %s, but have %s"\
                                                    % (self.str_evra(inst), self.str_evra(pkg)))

                                    qtext_versions = qtext_versions + 'U(%s -> %s) ' % \
                                                          (self.str_evra(pkg), self.str_evra(inst))
                        elif len(arch_match) == 0:
                            # This instance is not installed.
                            self.instance_status[inst]['installed'] = False
                            self.logger.info("        %s is not installed." % self.str_evra(inst))
                            qtext_versions = qtext_versions + 'I(%s) ' % self.str_evra(inst)

                # Check the rpm verify results.
                for inst in instances:
                    instance_fail = False
                    # Dump the rpm verify results.
                    #****Write something to format this nicely.*****
                    if self.setup['debug'] and self.instance_status[inst].get('verify', None):
                        self.logger.debug(self.instance_status[inst]['verify'])

                    self.instance_status[inst]['verify_fail'] = False
                    if self.instance_status[inst].get('verify', None):
                        if len(self.instance_status[inst].get('verify')) > 1:
                            self.logger.info("WARNING: Verification of more than one package instance.")

                        for result in self.instance_status[inst]['verify']:

                            # Check header results
                            if result.get('hdr', None):
                                instance_fail = True
                                self.instance_status[inst]['verify_fail'] = True

                            # Check dependency results
                            if result.get('deps', None):
                                instance_fail = True
                                self.instance_status[inst]['verify_fail'] = True

                            # Check the rpm verify file results against the modlist
                            # and entry and per Instance Ignores.
                            ignores = [ig.get('name') for ig in entry.findall('Ignore')] + \
                                      [ig.get('name') for ig in inst.findall('Ignore')] + \
                                      self.ignores
                            for file_result in result.get('files', []):
                                if file_result[-1] not in modlist + ignores:
                                    instance_fail = True
                                    self.instance_status[inst]['verify_fail'] = True
                                else:
                                    self.logger.debug("        Modlist/Ignore match: %s" % \
                                                                                 (file_result[-1]))

                        if instance_fail == True:
                            self.logger.debug("*** Instance %s failed RPM verification ***" % \
                                              self.str_evra(inst))
                            qtext_versions = qtext_versions + 'R(%s) ' % self.str_evra(inst)
                            self.modlists[entry] = modlist

                            # Attach status structure for return to server for reporting.
                            inst.set('verify_status', str(self.instance_status[inst]))

                    if self.instance_status[inst]['installed'] == False or \
                       self.instance_status[inst].get('version_fail', False)== True or \
                       self.instance_status[inst].get('verify_fail', False) == True:
                        package_fail = True
                        self.instance_status[inst]['pkg'] = entry
                        self.modlists[entry] = modlist

                # Find Installed Instances that are not in the Config.
                extra_installed = self.FindExtraInstances(entry, self.installed[entry.get('name')])
                if extra_installed != None:
                    package_fail = True
                    self.extra_instances.append(extra_installed)
                    for inst in extra_installed.findall('Instance'):
                        qtext_versions = qtext_versions + 'D(%s) ' % self.str_evra(inst)
                    self.logger.debug("Found Extra Instances %s" % qtext_versions)

                if package_fail == True:
                    self.logger.info("        Package %s failed verification." % \
                                                              (entry.get('name')))
                    qtext = 'Install/Upgrade/delete Package %s instance(s) - %s (y/N) ' % \
                                                  (entry.get('name'), qtext_versions)
                    entry.set('qtext', qtext)

                    bcfg2_versions = ''
                    for bcfg2_inst in [inst for inst in instances if inst.tag == 'Instance']:
                        bcfg2_versions = bcfg2_versions + '(%s) ' % self.str_evra(bcfg2_inst)
                    if bcfg2_versions != '':
                        entry.set('version', bcfg2_versions)
                    installed_versions = ''

                    for installed_inst in self.installed[entry.get('name')]:
                        installed_versions = installed_versions + '(%s) ' % \
                                                                      self.str_evra(installed_inst)

                    entry.set('current_version', installed_versions)
                    return False

        else:
            # There are no Instances of this package installed.
            self.logger.debug("Package %s has no instances installed" % (entry.get('name')))
            entry.set('current_exists', 'false')
            bcfg2_versions = ''
            for inst in instances:
                qtext_versions = qtext_versions + 'I(%s) ' % self.str_evra(inst)
                self.instance_status.setdefault(inst, {})['installed'] = False
                self.modlists[entry] = modlist
                self.instance_status[inst]['pkg'] = entry
                if inst.tag == 'Instance':
                    bcfg2_versions = bcfg2_versions + '(%s) ' % self.str_evra(inst)
            if bcfg2_versions != '':
                entry.set('version', bcfg2_versions)
            entry.set('qtext', "Install Package %s Instance(s) %s? (y/N) " % \
                      (entry.get('name'), qtext_versions))

            return False
        return True

    def Remove(self, packages):
        """
           Remove specified entries.

           packages is a list of Package Entries with Instances generated
           by FindExtra().

        """
        self.logger.debug('Running RPM.Remove()')

        pkgspec_list = []
        for pkg in packages:
            for inst in pkg:
                if pkg.get('name') != 'gpg-pubkey':
                    pkgspec = { 'name':pkg.get('name'),
                            'epoch':inst.get('epoch', None),
                            'version':inst.get('version'),
                            'release':inst.get('release'),
                            'arch':inst.get('arch') }
                    pkgspec_list.append(pkgspec)
                else:
                    pkgspec = { 'name':pkg.get('name'),
                            'version':inst.get('version'),
                            'release':inst.get('release')}
                    self.logger.info("WARNING: gpg-pubkey package not in configuration %s %s"\
                                                 % (pkgspec.get('name'), self.str_evra(pkgspec)))
                    self.logger.info("         This package will be deleted in a future version of the RPM driver.")
                #pkgspec_list.append(pkg_spec)

        erase_results = rpmtools.rpm_erase(pkgspec_list, self.erase_flags)
        if erase_results == []:
            self.modified += packages
            for pkg in pkgspec_list:
                self.logger.info("Deleted %s %s" % (pkg.get('name'), self.str_evra(pkg)))
        else:
            self.logger.info("Bulk erase failed with errors:")
            self.logger.debug("Erase results = %s" % erase_results)
            self.logger.info("Attempting individual erase for each package.")
            pkgspec_list = []
            for pkg in packages:
                pkg_modified = False
                for inst in pkg:
                    if pkg.get('name') != 'gpg-pubkey':
                        pkgspec = { 'name':pkg.get('name'),
                                'epoch':inst.get('epoch', None),
                                'version':inst.get('version'),
                                'release':inst.get('release'),
                                'arch':inst.get('arch') }
                        pkgspec_list.append(pkgspec)
                    else:
                        pkgspec = { 'name':pkg.get('name'),
                                'version':inst.get('version'),
                                'release':inst.get('release')}
                        self.logger.info("WARNING: gpg-pubkey package not in configuration %s %s"\
                                                   % (pkgspec.get('name'), self.str_evra(pkgspec)))
                        self.logger.info("         This package will be deleted in a future version of the RPM driver.")
                        continue # Don't delete the gpg-pubkey packages for now.
                    erase_results = rpmtools.rpm_erase([pkgspec], self.erase_flags)
                    if erase_results == []:
                        pkg_modified = True
                        self.logger.info("Deleted %s %s" % \
                                                   (pkgspec.get('name'), self.str_evra(pkgspec)))
                    else:
                        self.logger.error("unable to delete %s %s" % \
                                                   (pkgspec.get('name'), self.str_evra(pkgspec)))
                        self.logger.debug("Failure = %s" % erase_results)
                if pkg_modified == True:
                    self.modified.append(pkg)

        self.RefreshPackages()
        self.extra = self.FindExtra()

    def FixInstance(self, instance, inst_status):
        """
           Control if a reinstall of a package happens or not based on the
           results from RPM.VerifyPackage().

           Return True to reinstall, False to not reintstall.

        """
        fix = False

        if inst_status.get('installed', False) == False:
            if instance.get('installed_action', 'install') == "install" and \
               self.installed_action == "install":
                fix = True
            else:
                self.logger.debug('Installed Action for %s %s is to not install' % \
                                  (inst_status.get('pkg').get('name'),
                                   self.str_evra(instance)))

        elif inst_status.get('version_fail', False) == True:
            if instance.get('version_fail_action', 'upgrade') == "upgrade" and \
                self.version_fail_action == "upgrade":
                fix = True
            else:
                self.logger.debug('Version Fail Action for %s %s is to not upgrade' % \
                                  (inst_status.get('pkg').get('name'),
                                   self.str_evra(instance)))

        elif inst_status.get('verify_fail', False) == True and self.name == "RPM":
            # yum can't reinstall packages so only do this for rpm.
            if instance.get('verify_fail_action', 'reinstall') == "reinstall" and \
               self.verify_fail_action == "reinstall":
                for inst in inst_status.get('verify'):
                    # This needs to be a for loop rather than a straight get()
                    # because the underlying routines handle multiple packages
                    # and return a list of results.
                    self.logger.debug('reinstall_check: %s %s:%s-%s.%s' % inst.get('nevra'))

                    if inst.get("hdr", False):
                        fix = True

                    elif inst.get('files', False):
                        # Parse rpm verify file results
                        for file_result in inst.get('files', []):
                            self.logger.debug('reinstall_check: file: %s' % file_result)
                            if file_result[-2] != 'c':
                                fix = True
                                break

                    # Shouldn't really need this, but included for clarity.
                    elif inst.get("deps", False):
                        fix = False
            else:
                self.logger.debug('Verify Fail Action for %s %s is to not reinstall' % \
                                                     (inst_status.get('pkg').get('name'),
                                                      self.str_evra(instance)))

        return fix

    def Install(self, packages, states):
        """
           Try and fix everything that RPM.VerifyPackages() found wrong for
           each Package Entry.  This can result in individual RPMs being
           installed (for the first time), reinstalled, deleted, downgraded
           or upgraded.

           packages is a list of Package Elements that has
               states[<Package Element>] == False

           The following effects occur:
           - states{} is conditionally updated for each package.
           - self.installed{} is rebuilt, possibly multiple times.
           - self.instance_statusi{} is conditionally updated for each instance
             of a package.
           - Each package will be added to self.modified[] if its states{}
             entry is set to True.

        """
        self.logger.info('Runing RPM.Install()')

        install_only_pkgs = []
        gpg_keys = []
        upgrade_pkgs = []

        # Remove extra instances.
        # Can not reverify because we don't have a package entry.
        if len(self.extra_instances) > 0:
            if (self.setup.get('remove') == 'all' or \
                self.setup.get('remove') == 'packages') and\
                not self.setup.get('dryrun'):
                self.Remove(self.extra_instances)
            else:
                self.logger.info("The following extra package instances will be removed by the '-r' option:")
                for pkg in self.extra_instances:
                    for inst in pkg:
                        self.logger.info("    %s %s" % (pkg.get('name'), self.str_evra(inst)))

        # Figure out which instances of the packages actually need something
        # doing to them and place in the appropriate work 'queue'.
        for pkg in packages:
            for inst in [instn for instn in pkg if instn.tag \
                         in ['Instance', 'Package']]:
                if self.FixInstance(inst, self.instance_status[inst]):
                    if pkg.get('name') == 'gpg-pubkey':
                        gpg_keys.append(inst)
                    elif pkg.get('name') in self.installOnlyPkgs:
                        install_only_pkgs.append(inst)
                    else:
                        upgrade_pkgs.append(inst)

        # Fix installOnlyPackages
        if len(install_only_pkgs) > 0:
            self.logger.info("Attempting to install 'install only packages'")
            install_args = \
                " ".join(os.path.join(self.instance_status[inst].get('pkg').get('uri'),
                                      inst.get('simplefile'))
                         for inst in install_only_pkgs)
            if self.cmd.run("rpm --install --quiet --oldpackage --replacepkgs "
                            "%s" % install_args):
                # The rpm command succeeded.  All packages installed.
                self.logger.info("Single Pass for InstallOnlyPkgs Succeded")
                self.RefreshPackages()
            else:
                # The rpm command failed.  No packages installed.
                # Try installing instances individually.
                self.logger.error("Single Pass for InstallOnlyPackages Failed")
                installed_instances = []
                for inst in install_only_pkgs:
                    install_args = \
                        os.path.join(self.instance_status[inst].get('pkg').get('uri'),
                                     inst.get('simplefile'))
                    if self.cmd.run("rpm --install --quiet --oldpackage "
                                    "--replacepkgs %s" % install_args):
                        installed_instances.append(inst)
                    else:
                        self.logger.debug("InstallOnlyPackage %s %s would not install." % \
                                              (self.instance_status[inst].get('pkg').get('name'), \
                                               self.str_evra(inst)))

                install_pkg_set = set([self.instance_status[inst].get('pkg') \
                                                      for inst in install_only_pkgs])
                self.RefreshPackages()

        # Install GPG keys.
        if len(gpg_keys) > 0:
            for inst in gpg_keys:
                self.logger.info("Installing GPG keys.")
                key_arg = os.path.join(self.instance_status[inst].get('pkg').get('uri'), \
                                                     inst.get('simplefile'))
                if not self.cmd.run("rpm --import %s" % key_arg):
                    self.logger.debug("Unable to install %s-%s" %
                                      (self.instance_status[inst].get('pkg').get('name'),
                                       self.str_evra(inst)))
                else:
                    self.logger.debug("Installed %s-%s-%s" %
                                      (self.instance_status[inst].get('pkg').get('name'),
                                       inst.get('version'),
                                       inst.get('release')))
            self.RefreshPackages()
            self.gpg_keyids = self.getinstalledgpg()
            pkg = self.instance_status[gpg_keys[0]].get('pkg')
            states[pkg] = self.VerifyPackage(pkg, [])

        # Fix upgradeable packages.
        if len(upgrade_pkgs) > 0:
            self.logger.info("Attempting to upgrade packages")
            upgrade_args = " ".join([os.path.join(self.instance_status[inst].get('pkg').get('uri'), \
                                                  inst.get('simplefile')) \
                                           for inst in upgrade_pkgs])
            if self.cmd.run("rpm --upgrade --quiet --oldpackage --replacepkgs "
                            "%s" % upgrade_args):
                # The rpm command succeeded.  All packages upgraded.
                self.logger.info("Single Pass for Upgraded Packages Succeded")
                upgrade_pkg_set = set([self.instance_status[inst].get('pkg')
                                       for inst in upgrade_pkgs])
                self.RefreshPackages()
            else:
                # The rpm command failed.  No packages upgraded.
                # Try upgrading instances individually.
                self.logger.error("Single Pass for Upgrading Packages Failed")
                upgraded_instances = []
                for inst in upgrade_pkgs:
                    upgrade_args = os.path.join(self.instance_status[inst].get('pkg').get('uri'), \
                                                     inst.get('simplefile'))
                    #self.logger.debug("rpm --upgrade --quiet --oldpackage --replacepkgs %s" % \
                    #                                                      upgrade_args)
                    if self.cmd.run("rpm --upgrade --quiet --oldpackage "
                                    "--replacepkgs %s" % upgrade_args):
                        upgraded_instances.append(inst)
                    else:
                        self.logger.debug("Package %s %s would not upgrade." %
                                          (self.instance_status[inst].get('pkg').get('name'),
                                           self.str_evra(inst)))

                upgrade_pkg_set = set([self.instance_status[inst].get('pkg') \
                                                      for inst in upgrade_pkgs])
                self.RefreshPackages()

        if not self.setup['kevlar']:
            for pkg_entry in packages:
                self.logger.debug("Reverifying Failed Package %s" % (pkg_entry.get('name')))
                states[pkg_entry] = self.VerifyPackage(pkg_entry, \
                                                       self.modlists.get(pkg_entry, []))

        for entry in [ent for ent in packages if states[ent]]:
            self.modified.append(entry)

    def canInstall(self, entry):
        """Test if entry has enough information to be installed."""
        if not self.handlesEntry(entry):
            return False

        if 'failure' in entry.attrib:
            self.logger.error("Cannot install entry %s:%s with bind failure" % \
                              (entry.tag, entry.get('name')))
            return False


        instances = entry.findall('Instance')

        # If the entry wasn't verifiable, then we really don't want to try and fix something
        # that we don't know is broken.
        if not self.canVerify(entry):
            self.logger.debug("WARNING: Package %s was not verifiable, not passing to Install()" \
                                           % entry.get('name'))
            return False

        if not instances:
            # Old non Instance format, unmodified.
            if entry.get('name') == 'gpg-pubkey':
                # gpg-pubkey packages aren't really pacakges, so we have to do
                # something a little different.
                # Check that the Package Level has what we need for verification.
                if [attr for attr in self.__gpg_ireq__[entry.tag] if attr not in entry.attrib]:
                    self.logger.error("Incomplete information for entry %s:%s; cannot install" \
                                      % (entry.tag, entry.get('name')))
                    return False
            else:
                if [attr for attr in self.__ireq__[entry.tag] if attr not in entry.attrib]:
                    self.logger.error("Incomplete information for entry %s:%s; cannot install" \
                                      % (entry.tag, entry.get('name')))
                    return False
        else:
            if entry.get('name') == 'gpg-pubkey':
                # gpg-pubkey packages aren't really pacakges, so we have to do
                # something a little different.
                # Check that the Package Level has what we need for verification.
                if [attr for attr in self.__new_gpg_ireq__[entry.tag] if attr not in entry.attrib]:
                    self.logger.error("Incomplete information for entry %s:%s; cannot install" \
                                      % (entry.tag, entry.get('name')))
                    return False
                # Check that the Instance Level has what we need for verification.
                for inst in instances:
                    if [attr for attr in self.__new_gpg_ireq__[inst.tag] \
                                 if attr not in inst.attrib]:
                        self.logger.error("Incomplete information for entry %s:%s; cannot install"\
                                          % (inst.tag, entry.get('name')))
                        return False
            else:
                # New format with Instances.
                # Check that the Package Level has what we need for verification.
                if [attr for attr in self.__new_ireq__[entry.tag] if attr not in entry.attrib]:
                    self.logger.error("Incomplete information for entry %s:%s; cannot install" \
                                      % (entry.tag, entry.get('name')))
                    self.logger.error("             Required attributes that may not be present are %s" \
                                      % (self.__new_ireq__[entry.tag]))
                    return False
                # Check that the Instance Level has what we need for verification.
                for inst in instances:
                    if inst.tag == 'Instance':
                        if [attr for attr in self.__new_ireq__[inst.tag] \
                                     if attr not in inst.attrib]:
                            self.logger.error("Incomplete information for %s of package %s; cannot install" \
                                              % (inst.tag, entry.get('name')))
                            self.logger.error("         Required attributes that may not be present are %s" \
                                              % (self.__new_ireq__[inst.tag]))
                            return False
        return True

    def canVerify(self, entry):
        """
            Test if entry has enough information to be verified.

            Three types of entries are checked.
               Old style Package
               New style Package with Instances
               pgp-pubkey packages

           Also the old style entries get modified after the first
           VerifyPackage() run, so there needs to be a second test.

        """
        if not self.handlesEntry(entry):
            return False

        if 'failure' in entry.attrib:
            self.logger.error("Entry %s:%s reports bind failure: %s" % \
                              (entry.tag, entry.get('name'), entry.get('failure')))
            return False

        # We don't want to do any checks so we don't care what the entry has in it.
        if (not self.pkg_checks or
            entry.get('pkg_checks', 'true').lower() == 'false'):
            return True

        instances = entry.findall('Instance')

        if not instances:
            # Old non Instance format, unmodified.
            if entry.get('name') == 'gpg-pubkey':
                # gpg-pubkey packages aren't really pacakges, so we have to do
                # something a little different.
                # Check that the Package Level has what we need for verification.
                if [attr for attr in self.__gpg_req__[entry.tag] if attr not in entry.attrib]:
                    self.logger.error("Incomplete information for entry %s:%s; cannot verify" \
                                      % (entry.tag, entry.get('name')))
                    return False
            elif entry.tag == 'Path' and entry.get('type') == 'ignore':
                # ignored Paths are only relevant during failed package
                # verification
                pass
            else:
                if [attr for attr in self.__req__[entry.tag] if attr not in entry.attrib]:
                    self.logger.error("Incomplete information for entry %s:%s; cannot verify" \
                                      % (entry.tag, entry.get('name')))
                    return False
        else:
            if entry.get('name') == 'gpg-pubkey':
                # gpg-pubkey packages aren't really pacakges, so we have to do
                # something a little different.
                # Check that the Package Level has what we need for verification.
                if [attr for attr in self.__new_gpg_req__[entry.tag] if attr not in entry.attrib]:
                    self.logger.error("Incomplete information for entry %s:%s; cannot verify" \
                                      % (entry.tag, entry.get('name')))
                    return False
                # Check that the Instance Level has what we need for verification.
                for inst in instances:
                    if [attr for attr in self.__new_gpg_req__[inst.tag] \
                                 if attr not in inst.attrib]:
                        self.logger.error("Incomplete information for entry %s:%s; cannot verify" \
                                          % (inst.tag, inst.get('name')))
                        return False
            else:
                # New format with Instances, or old style modified.
                # Check that the Package Level has what we need for verification.
                if [attr for attr in self.__new_req__[entry.tag] if attr not in entry.attrib]:
                    self.logger.error("Incomplete information for entry %s:%s; cannot verify" \
                                      % (entry.tag, entry.get('name')))
                    return False
                # Check that the Instance Level has what we need for verification.
                for inst in instances:
                    if inst.tag == 'Instance':
                        if [attr for attr in self.__new_req__[inst.tag] \
                                     if attr not in inst.attrib]:
                            self.logger.error("Incomplete information for entry %s:%s; cannot verify" \
                                              % (inst.tag, inst.get('name')))
                            return False
        return True

    def FindExtra(self):
        """Find extra packages."""
        packages = [entry.get('name') for entry in self.getSupportedEntries()]
        extras = []

        for (name, instances) in list(self.installed.items()):
            if name not in packages:
                extra_entry = Bcfg2.Client.XML.Element('Package', name=name, type=self.pkgtype)
                for installed_inst in instances:
                    if self.setup['extra']:
                        self.logger.info("Extra Package %s %s." % \
                                         (name, self.str_evra(installed_inst)))
                    tmp_entry = Bcfg2.Client.XML.SubElement(extra_entry, 'Instance', \
                                     version = installed_inst.get('version'), \
                                     release = installed_inst.get('release'))
                    if installed_inst.get('epoch', None) != None:
                        tmp_entry.set('epoch', str(installed_inst.get('epoch')))
                    if installed_inst.get('arch', None) != None:
                        tmp_entry.set('arch', installed_inst.get('arch'))
                extras.append(extra_entry)
        return extras


    def FindExtraInstances(self, pkg_entry, installed_entry):
        """
            Check for installed instances that are not in the config.
            Return a Package Entry with Instances to remove, or None if there
            are no Instances to remove.

        """
        name = pkg_entry.get('name')
        extra_entry = Bcfg2.Client.XML.Element('Package', name=name, type=self.pkgtype)
        instances = [inst for inst in pkg_entry if inst.tag == 'Instance' or inst.tag == 'Package']
        if name in self.installOnlyPkgs:
            for installed_inst in installed_entry:
                not_found = True
                for inst in instances:
                    if self.pkg_vr_equal(inst, installed_inst) or \
                       self.inst_evra_equal(inst, installed_inst):
                        not_found = False
                        break
                if not_found == True:
                    # Extra package.
                    self.logger.info("Extra InstallOnlyPackage %s %s." % \
                                     (name, self.str_evra(installed_inst)))
                    tmp_entry = Bcfg2.Client.XML.SubElement(extra_entry, 'Instance', \
                                     version = installed_inst.get('version'), \
                                     release = installed_inst.get('release'))
                    if installed_inst.get('epoch', None) != None:
                        tmp_entry.set('epoch', str(installed_inst.get('epoch')))
                    if installed_inst.get('arch', None) != None:
                        tmp_entry.set('arch', installed_inst.get('arch'))
        else:
            # Normal package, only check arch.
            for installed_inst in installed_entry:
                not_found = True
                for inst in instances:
                    if installed_inst.get('arch', None) == inst.get('arch', None) or\
                       inst.tag == 'Package':
                        not_found = False
                        break
                if not_found:
                    self.logger.info("Extra Normal Package Instance %s %s" % \
                                     (name, self.str_evra(installed_inst)))
                    tmp_entry = Bcfg2.Client.XML.SubElement(extra_entry, 'Instance', \
                                     version = installed_inst.get('version'), \
                                     release = installed_inst.get('release'))
                    if installed_inst.get('epoch', None) != None:
                        tmp_entry.set('epoch', str(installed_inst.get('epoch')))
                    if installed_inst.get('arch', None) != None:
                        tmp_entry.set('arch', installed_inst.get('arch'))

        if len(extra_entry) == 0:
            extra_entry = None

        return extra_entry

    def str_evra(self, instance):
        """Convert evra dict entries to a string."""
        if instance.get('epoch', '*') in ['*', None]:
            return '%s-%s.%s' % (instance.get('version', '*'),
                                 instance.get('release', '*'),
                                 instance.get('arch', '*'))
        else:
            return '%s:%s-%s.%s' % (instance.get('epoch', '*'),
                                    instance.get('version', '*'),
                                    instance.get('release', '*'),
                                    instance.get('arch', '*'))

    def pkg_vr_equal(self, config_entry, installed_entry):
        '''
            Compare old style entry to installed entry.  Which means ignore
            the epoch and arch.
        '''
        if (config_entry.tag == 'Package' and \
            config_entry.get('version') == installed_entry.get('version') and \
            config_entry.get('release') == installed_entry.get('release')):
            return True
        else:
            return False

    def inst_evra_equal(self, config_entry, installed_entry):
        """Compare new style instance to installed entry."""

        if config_entry.get('epoch', None) != None:
            epoch = int(config_entry.get('epoch'))
        else:
            epoch = None

        if (config_entry.tag == 'Instance' and \
           (epoch == installed_entry.get('epoch', 0) or \
               (epoch == 0 and installed_entry.get('epoch', 0) == None) or \
               (epoch == None and installed_entry.get('epoch', 0) == 0)) and \
           config_entry.get('version') == installed_entry.get('version') and \
           config_entry.get('release') == installed_entry.get('release') and \
           config_entry.get('arch', None) == installed_entry.get('arch', None)):
            return True
        else:
            return False

    def getinstalledgpg(self):
        """
           Create a list of installed GPG key IDs.

           The pgp-pubkey package version is the least significant 4 bytes
           (big-endian) of the key ID which is good enough for our purposes.

        """
        init_ts = rpmtools.rpmtransactionset()
        init_ts.setVSFlags(rpm._RPMVSF_NODIGESTS|rpm._RPMVSF_NOSIGNATURES)
        gpg_hdrs = rpmtools.getheadersbykeyword(init_ts, **{'name':'gpg-pubkey'})
        keyids = [ header[rpm.RPMTAG_VERSION] for header in gpg_hdrs]
        keyids.append('None')
        init_ts.closeDB()
        del init_ts
        return keyids

    def VerifyPath(self, entry, _):
        """
           We don't do anything here since all
           Paths are processed in __init__
        """
        return True

########NEW FILE########
__FILENAME__ = RPMng
""" RPM driver called 'RPMng' for backwards compat """

from Bcfg2.Client.Tools.RPM import RPM


class RPMng(RPM):
    """ RPM driver called 'RPMng' for backwards compat """
    deprecated = True
    name = "RPM"

########NEW FILE########
__FILENAME__ = rpmtools
#!/usr/bin/env python
"""
    Module that uses rpm-python to implement the following rpm
    functionality for the bcfg2 RPM and YUM client drivers:

        rpm -qa
        rpm --verify
        rpm --erase

    The code closely follows the rpm C code.

    The code was written to be used in the bcfg2 RPM/YUM drivers.

    Some command line options have been provided to assist with
    testing and development, but the output isn't pretty and looks
    nothing like rpm output.

    Run 'rpmtools' -h for the options.

"""

import grp
import optparse
import os
import pwd
import rpm
import stat
import sys
if sys.version_info >= (2, 5):
    import hashlib
    py24compat = False
else:
    # FIXME: Remove when client python dep is 2.5 or greater
    py24compat = True
    import md5

# Determine what prelink tools we have available.
# The isprelink module is a python extension that examines the ELF headers
# to see if the file has been prelinked.  If it is not present a lot of files
# are unnecessarily run through the prelink command.
try:
    from isprelink import *
    isprelink_imported = True
except ImportError:
    isprelink_imported = False

# If the prelink command is installed on the system then we need to do
# prelink -y on files.
if os.access('/usr/sbin/prelink', os.X_OK):
    prelink_exists = True
else:
    prelink_exists = False

# If we don't have isprelink then we will use the prelink configuration file to
# filter what we have to put through prelink -y.
import re
blacklist = []
whitelist = []
try:
    f = open('/etc/prelink.conf', mode='r')
    for line in f:
        if line.startswith('#'):
            continue
        option, pattern = line.split()
        if pattern.startswith('*.'):
            pattern = pattern.replace('*.', '\.')
            pattern += '$'
        elif pattern.startswith('/'):
            pattern = '^' + pattern
        if option == '-b':
            blacklist.append(pattern)
        elif option == '-l':
            whitelist.append(pattern)
    f.close()
except IOError:
    pass

blacklist_re = re.compile('|'.join(blacklist))
whitelist_re = re.compile('|'.join(whitelist))

# Flags that are not defined in rpm-python.
# They are defined in lib/rpmcli.h
# Bit(s) for verifyFile() attributes.
#
RPMVERIFY_NONE            = 0                      #  /*!< */
RPMVERIFY_MD5             = 1          # 1 << 0    #  /*!< from %verify(md5) */
RPMVERIFY_FILESIZE        = 2          # 1 << 1    #  /*!< from %verify(size) */
RPMVERIFY_LINKTO          = 4          # 1 << 2    #  /*!< from %verify(link) */
RPMVERIFY_USER            = 8          # 1 << 3    #  /*!< from %verify(user) */
RPMVERIFY_GROUP           = 16         # 1 << 4    #  /*!< from %verify(group) */
RPMVERIFY_MTIME           = 32         # 1 << 5    #  /*!< from %verify(mtime) */
RPMVERIFY_MODE            = 64         # 1 << 6    #  /*!< from %verify(mode) */
RPMVERIFY_RDEV            = 128        # 1 << 7    #  /*!< from %verify(rdev) */
RPMVERIFY_CONTEXTS        = 32768      # (1 << 15) #  /*!< from --nocontexts */
RPMVERIFY_READLINKFAIL    = 268435456  # (1 << 28) #  /*!< readlink failed */
RPMVERIFY_READFAIL        = 536870912  # (1 << 29) #  /*!< file read failed */
RPMVERIFY_LSTATFAIL       = 1073741824 # (1 << 30) #  /*!< lstat failed */
RPMVERIFY_LGETFILECONFAIL = 2147483648 # (1 << 31) #  /*!< lgetfilecon failed */

RPMVERIFY_FAILURES =    \
 (RPMVERIFY_LSTATFAIL|RPMVERIFY_READFAIL|RPMVERIFY_READLINKFAIL| \
  RPMVERIFY_LGETFILECONFAIL)

# Bit(s) to control rpm_verify() operation.
#
VERIFY_DEFAULT       = 0,       #  /*!< */
VERIFY_MD5           = 1 << 0   #  /*!< from --nomd5 */
VERIFY_SIZE          = 1 << 1   #  /*!< from --nosize */
VERIFY_LINKTO        = 1 << 2   #  /*!< from --nolinkto */
VERIFY_USER          = 1 << 3   #  /*!< from --nouser */
VERIFY_GROUP         = 1 << 4   #  /*!< from --nogroup */
VERIFY_MTIME         = 1 << 5   #  /*!< from --nomtime */
VERIFY_MODE          = 1 << 6   #  /*!< from --nomode */
VERIFY_RDEV          = 1 << 7   #  /*!< from --nodev */
#        /* bits 8-14 unused, reserved for rpmVerifyAttrs */
VERIFY_CONTEXTS      = 1 << 15  #  /*!< verify: from --nocontexts */
VERIFY_FILES         = 1 << 16  #  /*!< verify: from --nofiles */
VERIFY_DEPS          = 1 << 17  #  /*!< verify: from --nodeps */
VERIFY_SCRIPT        = 1 << 18  #  /*!< verify: from --noscripts */
VERIFY_DIGEST        = 1 << 19  #  /*!< verify: from --nodigest */
VERIFY_SIGNATURE     = 1 << 20  #  /*!< verify: from --nosignature */
VERIFY_PATCHES       = 1 << 21  #  /*!< verify: from --nopatches */
VERIFY_HDRCHK        = 1 << 22  #  /*!< verify: from --nohdrchk */
VERIFY_FOR_LIST      = 1 << 23  #  /*!< query:  from --list */
VERIFY_FOR_STATE     = 1 << 24  #  /*!< query:  from --state */
VERIFY_FOR_DOCS      = 1 << 25  #  /*!< query:  from --docfiles */
VERIFY_FOR_CONFIG    = 1 << 26  #  /*!< query:  from --configfiles */
VERIFY_FOR_DUMPFILES = 1 << 27  #  /*!< query:  from --dump */
#        /* bits 28-31 used in rpmVerifyAttrs */

# Comes from C cource.  lib/rpmcli.h
VERIFY_ATTRS =   \
  (VERIFY_MD5 | VERIFY_SIZE | VERIFY_LINKTO | VERIFY_USER | VERIFY_GROUP | \
   VERIFY_MTIME | VERIFY_MODE | VERIFY_RDEV | VERIFY_CONTEXTS)

VERIFY_ALL =     \
  (VERIFY_ATTRS | VERIFY_FILES | VERIFY_DEPS | VERIFY_SCRIPT | VERIFY_DIGEST |\
   VERIFY_SIGNATURE | VERIFY_HDRCHK)


# Some masks for what checks to NOT do on these file types.
# The C code actiually resets these up for every file.
DIR_FLAGS = ~(RPMVERIFY_MD5 | RPMVERIFY_FILESIZE | RPMVERIFY_MTIME | \
              RPMVERIFY_LINKTO)

# These file types all have the same mask, but hopefully this will make the
# code more readable.
FIFO_FLAGS = CHR_FLAGS = BLK_FLAGS = GHOST_FLAGS = DIR_FLAGS

LINK_FLAGS = ~(RPMVERIFY_MD5 | RPMVERIFY_FILESIZE | RPMVERIFY_MTIME | \
               RPMVERIFY_MODE | RPMVERIFY_USER | RPMVERIFY_GROUP)

REG_FLAGS = ~(RPMVERIFY_LINKTO)


def s_isdev(mode):
    """
        Check to see if a file is a device.

    """
    return stat.S_ISBLK(mode) | stat.S_ISCHR(mode)

def rpmpackagelist(rts):
    """
        Equivalent of rpm -qa.  Intended for RefreshPackages() in the RPM Driver.
        Requires rpmtransactionset() to be run first to get a ts.
        Returns a list of pkgspec dicts.

        e.g. [ {'name':'foo', 'epoch':'20', 'version':'1.2', 'release':'5', 'arch':'x86_64' },
               {'name':'bar', 'epoch':'10', 'version':'5.2', 'release':'2', 'arch':'x86_64' } ]

    """
    return [{'name':header[rpm.RPMTAG_NAME],
             'epoch':header[rpm.RPMTAG_EPOCH],
             'version':header[rpm.RPMTAG_VERSION],
             'release':header[rpm.RPMTAG_RELEASE],
             'arch':header[rpm.RPMTAG_ARCH],
             'gpgkeyid':header.sprintf("%|SIGGPG?{%{SIGGPG:pgpsig}}:{None}|").split()[-1]}
             for header in rts.dbMatch()]

def getindexbykeyword(index_ts, **kwargs):
    """
        Return list of indexs from the rpmdb matching keywords
        ex: getHeadersByKeyword(name='foo', version='1', release='1')

        Can be passed any structure that can be indexed by the pkgspec
        keyswords as other keys are filtered out.

    """
    lst = []
    name = kwargs.get('name')
    if name:
        index_mi = index_ts.dbMatch(rpm.RPMTAG_NAME, name)
    else:
        index_mi = index_ts.dbMatch()

    if 'epoch' in kwargs:
        if kwargs['epoch'] != None and kwargs['epoch'] != 'None':
            kwargs['epoch'] = int(kwargs['epoch'])
        else:
            del(kwargs['epoch'])

    keywords = [key for key in list(kwargs.keys()) \
                         if key in ('name', 'epoch', 'version', 'release', 'arch')]
    keywords_len = len(keywords)
    for hdr in index_mi:
        match = 0
        for keyword in keywords:
            if hdr[keyword] == kwargs[keyword]:
                match += 1
        if match == keywords_len:
            lst.append(index_mi.instance())
    del index_mi
    return lst

def getheadersbykeyword(header_ts, **kwargs):
    """
        Borrowed parts of this from from Yum.  Need to fix it though.
        Epoch is not handled right.

        Return list of headers from the rpmdb matching keywords
        ex: getHeadersByKeyword(name='foo', version='1', release='1')

        Can be passed any structure that can be indexed by the pkgspec
        keyswords as other keys are filtered out.

    """
    lst = []
    name = kwargs.get('name')
    if name:
        header_mi = header_ts.dbMatch(rpm.RPMTAG_NAME, name)
    else:
        header_mi = header_ts.dbMatch()

    if 'epoch' in kwargs:
        if kwargs['epoch'] != None and kwargs['epoch'] != 'None':
            kwargs['epoch'] = int(kwargs['epoch'])
        else:
            del(kwargs['epoch'])

    keywords = [key for key in list(kwargs.keys()) \
                         if key in ('name', 'epoch', 'version', 'release', 'arch')]
    keywords_len = len(keywords)
    for hdr in header_mi:
        match = 0
        for keyword in keywords:
            if hdr[keyword] == kwargs[keyword]:
                match += 1
        if match == keywords_len:
            lst.append(hdr)
    del header_mi
    return lst

def prelink_md5_check(filename):
    """
        Checks if a file is prelinked.  If it is run it through prelink -y
        to get the unprelinked md5 and file size.

        Return 0 if the file was not prelinked, otherwise return the file size.
        Always return the md5.

    """
    prelink = False
    try:
        plf = open(filename, "rb")
    except IOError:
        return False, 0

    if prelink_exists:
        if isprelink_imported:
            plfd = plf.fileno()
            if isprelink(plfd):
                plf.close()
                cmd = '/usr/sbin/prelink -y %s 2> /dev/null' \
                                            % (re.escape(filename))
                plf = os.popen(cmd, 'rb')
                prelink = True
        elif whitelist_re.search(filename) and not blacklist_re.search(filename):
            plf.close()
            cmd = '/usr/sbin/prelink -y %s 2> /dev/null' \
                                        % (re.escape(filename))
            plf = os.popen(cmd, 'rb')
            prelink = True

    fsize = 0
    if py24compat:
        chksum = md5.new()
    else:
        chksum = hashlib.md5()
    while 1:
        data = plf.read()
        if not data:
            break
        fsize += len(data)
        chksum.update(data)
    plf.close()
    file_md5 = chksum.hexdigest()
    if prelink:
        return file_md5, fsize
    else:
        return file_md5, 0

def prelink_size_check(filename):
    """
       This check is only done if the prelink_md5_check() is not done first.

       Checks if a file is prelinked.  If it is run it through prelink -y
       to get the unprelinked file size.

       Return 0 if the file was not prelinked, otherwise return the file size.

    """
    fsize = 0
    try:
        plf = open(filename, "rb")
    except IOError:
        return False

    if prelink_exists:
        if isprelink_imported:
            plfd = plf.fileno()
            if isprelink(plfd):
                plf.close()
                cmd = '/usr/sbin/prelink -y %s 2> /dev/null' \
                                            % (re.escape(filename))
                plf = os.popen(cmd, 'rb')

                while 1:
                    data = plf.read()
                    if not data:
                        break
                    fsize += len(data)

        elif whitelist_re.search(filename) and not blacklist_re.search(filename):
            plf.close()
            cmd = '/usr/sbin/prelink -y %s 2> /dev/null' \
                                        % (re.escape(filename))
            plf = os.popen(cmd, 'rb')

            while 1:
                data = plf.read()
                if not data:
                    break
                fsize += len(data)

    plf.close()

    return fsize

def debug_verify_flags(vflags):
    """
        Decodes the verify flags bits.
    """
    if vflags & RPMVERIFY_MD5:
        print('RPMVERIFY_MD5')
    if vflags & RPMVERIFY_FILESIZE:
        print('RPMVERIFY_FILESIZE')
    if vflags & RPMVERIFY_LINKTO:
        print('RPMVERIFY_LINKTO')
    if vflags & RPMVERIFY_USER:
        print('RPMVERIFY_USER')
    if vflags & RPMVERIFY_GROUP:
        print('RPMVERIFY_GROUP')
    if vflags & RPMVERIFY_MTIME:
        print('RPMVERIFY_MTIME')
    if vflags & RPMVERIFY_MODE:
        print('RPMVERIFY_MODE')
    if vflags & RPMVERIFY_RDEV:
        print('RPMVERIFY_RDEV')
    if vflags & RPMVERIFY_CONTEXTS:
        print('RPMVERIFY_CONTEXTS')
    if vflags & RPMVERIFY_READLINKFAIL:
        print('RPMVERIFY_READLINKFAIL')
    if vflags & RPMVERIFY_READFAIL:
        print('RPMVERIFY_READFAIL')
    if vflags & RPMVERIFY_LSTATFAIL:
        print('RPMVERIFY_LSTATFAIL')
    if vflags & RPMVERIFY_LGETFILECONFAIL:
        print('RPMVERIFY_LGETFILECONFAIL')

def debug_file_flags(fflags):
    """
        Decodes the file flags bits.
    """
    if fflags & rpm.RPMFILE_CONFIG:
        print('rpm.RPMFILE_CONFIG')

    if fflags & rpm.RPMFILE_DOC:
        print('rpm.RPMFILE_DOC')

    if fflags & rpm.RPMFILE_ICON:
        print('rpm.RPMFILE_ICON')

    if fflags & rpm.RPMFILE_MISSINGOK:
        print('rpm.RPMFILE_MISSINGOK')

    if fflags & rpm.RPMFILE_NOREPLACE:
        print('rpm.RPMFILE_NOREPLACE')

    if fflags & rpm.RPMFILE_GHOST:
        print('rpm.RPMFILE_GHOST')

    if fflags & rpm.RPMFILE_LICENSE:
        print('rpm.RPMFILE_LICENSE')

    if fflags & rpm.RPMFILE_README:
        print('rpm.RPMFILE_README')

    if fflags & rpm.RPMFILE_EXCLUDE:
        print('rpm.RPMFILE_EXLUDE')

    if fflags & rpm.RPMFILE_UNPATCHED:
        print('rpm.RPMFILE_UNPATCHED')

    if fflags & rpm.RPMFILE_PUBKEY:
        print('rpm.RPMFILE_PUBKEY')

def rpm_verify_file(fileinfo, rpmlinktos, omitmask):
    """
        Verify all the files in a package.

        Returns a list of error flags, the file type and file name.  The list
        entries are strings that are the same as the labels for the bitwise
        flags used in the C code.

    """
    (fname, fsize, fmode, fmtime, fflags, frdev, finode, fnlink, fstate, \
            vflags, fuser, fgroup, fmd5) = fileinfo

    # 1. rpmtsRootDir stuff.  What does it do and where to I get it from?

    file_results = []
    flags = vflags

    # Check to see if the file was installed - if not pretend all is ok.
    # This is what the rpm C code does!
    if fstate != rpm.RPMFILE_STATE_NORMAL:
        return file_results

    # Get the installed files stats
    try:
        lstat = os.lstat(fname)
    except OSError:
        if not (fflags & (rpm.RPMFILE_MISSINGOK|rpm.RPMFILE_GHOST)):
            file_results.append('RPMVERIFY_LSTATFAIL')
            #file_results.append(fname)
        return file_results

    # 5. Contexts?  SELinux stuff?

    # Setup what checks to do.  This is straight out of the C code.
    if stat.S_ISDIR(lstat.st_mode):
        flags &= DIR_FLAGS
    elif stat.S_ISLNK(lstat.st_mode):
        flags &= LINK_FLAGS
    elif stat.S_ISFIFO(lstat.st_mode):
        flags &= FIFO_FLAGS
    elif stat.S_ISCHR(lstat.st_mode):
        flags &= CHR_FLAGS
    elif stat.S_ISBLK(lstat.st_mode):
        flags &= BLK_FLAGS
    else:
        flags &= REG_FLAGS

    if (fflags & rpm.RPMFILE_GHOST):
        flags &= GHOST_FLAGS

    flags &= ~(omitmask | RPMVERIFY_FAILURES)

    # 8. SELinux stuff.

    prelink_size = 0
    if flags & RPMVERIFY_MD5:
        prelink_md5, prelink_size = prelink_md5_check(fname)
        if prelink_md5 == False:
            file_results.append('RPMVERIFY_MD5')
            file_results.append('RPMVERIFY_READFAIL')
        elif  prelink_md5 != fmd5:
            file_results.append('RPMVERIFY_MD5')

    if flags & RPMVERIFY_LINKTO:
        linkto = os.readlink(fname)
        if not linkto:
            file_results.append('RPMVERIFY_READLINKFAIL')
            file_results.append('RPMVERIFY_LINKTO')
        else:
            if len(rpmlinktos) == 0  or linkto != rpmlinktos:
                file_results.append('RPMVERIFY_LINKTO')

    if flags & RPMVERIFY_FILESIZE:
        if not (flags & RPMVERIFY_MD5): # prelink check hasn't been done.
            prelink_size = prelink_size_check(fname)
        if (prelink_size != 0):         # This is a prelinked file.
            if (prelink_size != fsize):
                file_results.append('RPMVERIFY_FILESIZE')
        elif lstat.st_size != fsize:    # It wasn't a prelinked file.
            file_results.append('RPMVERIFY_FILESIZE')

    if flags & RPMVERIFY_MODE:
        metamode = fmode
        filemode = lstat.st_mode

        # Comparing the type of %ghost files is meaningless, but perms are ok.
        if fflags & rpm.RPMFILE_GHOST:
            metamode &= ~0xf000
            filemode &= ~0xf000

        if (stat.S_IFMT(metamode) != stat.S_IFMT(filemode)) or \
           (stat.S_IMODE(metamode) != stat.S_IMODE(filemode)):
            file_results.append('RPMVERIFY_MODE')

    if flags & RPMVERIFY_RDEV:
        if (stat.S_ISCHR(fmode) != stat.S_ISCHR(lstat.st_mode) or
            stat.S_ISBLK(fmode) != stat.S_ISBLK(lstat.st_mode)):
            file_results.append('RPMVERIFY_RDEV')
        elif (s_isdev(fmode) & s_isdev(lstat.st_mode)):
            st_rdev = lstat.st_rdev
            if frdev != st_rdev:
                file_results.append('RPMVERIFY_RDEV')

    if flags & RPMVERIFY_MTIME:
        if lstat.st_mtime != fmtime:
            file_results.append('RPMVERIFY_MTIME')

    if flags & RPMVERIFY_USER:
        try:
            user = pwd.getpwuid(lstat.st_uid)[0]
        except KeyError:
            user = None
        if not user or not fuser or (user != fuser):
            file_results.append('RPMVERIFY_USER')

    if flags & RPMVERIFY_GROUP:
        try:
            group = grp.getgrgid(lstat.st_gid)[0]
        except KeyError:
            group = None
        if not group or not fgroup or (group != fgroup):
            file_results.append('RPMVERIFY_GROUP')

    return file_results

def rpm_verify_dependencies(header):
    """
        Check package dependencies. Header is an rpm.hdr.

        Don't like opening another ts to do this, but
        it was the only way I could find of clearing the ts
        out.

        Have asked on the rpm-maint list on how to do
        this the right way (28 Feb 2007).

        ts.check() returns:

        ((name, version, release), (reqname, reqversion), \
            flags, suggest, sense)

    """
    _ts1 = rpmtransactionset()
    _ts1.addInstall(header, 'Dep Check', 'i')
    dep_errors = _ts1.check()
    _ts1.closeDB()
    return dep_errors

def rpm_verify_package(vp_ts, header, verify_options):
    """
        Verify a single package specified by header.  Header is an rpm.hdr.

        If errors are found it returns a dictionary of errors.

    """
    # Set some transaction level flags.
    vsflags = 0
    if 'nodigest' in verify_options:
        vsflags |= rpm._RPMVSF_NODIGESTS
    if 'nosignature' in verify_options:
        vsflags |= rpm._RPMVSF_NOSIGNATURES
    ovsflags = vp_ts.setVSFlags(vsflags)

    # Map from the Python options to the rpm bitwise flags.
    omitmask = 0

    if 'nolinkto' in verify_options:
        omitmask |= VERIFY_LINKTO
    if 'nomd5' in verify_options:
        omitmask |= VERIFY_MD5
    if 'nosize' in verify_options:
        omitmask |= VERIFY_SIZE
    if 'nouser' in verify_options:
        omitmask |= VERIFY_USER
    if 'nogroup' in verify_options:
        omitmask |= VERIFY_GROUP
    if 'nomtime' in verify_options:
        omitmask |= VERIFY_MTIME
    if 'nomode' in verify_options:
        omitmask |= VERIFY_MODE
    if 'nordev' in verify_options:
        omitmask |= VERIFY_RDEV

    omitmask = ((~omitmask & VERIFY_ATTRS) ^ VERIFY_ATTRS)

    package_results = {}

    # Check Signatures and Digests.
    # No idea what this might return.  Need to break something to see.
    # Setting the vsflags above determines what gets checked in the header.
    hdr_stat = vp_ts.hdrCheck(header.unload())
    if hdr_stat:
        package_results['hdr'] = hdr_stat

    # Check Package Depencies.
    if 'nodeps' not in verify_options:
        dep_stat = rpm_verify_dependencies(header)
        if dep_stat:
            package_results['deps'] = dep_stat

    # Check all the package files.
    if 'nofiles' not in verify_options:
        vp_fi = header.fiFromHeader()
        for fileinfo in vp_fi:
            # Do not bother doing anything with ghost files.
            # This is what RPM does.
            if fileinfo[4] & rpm.RPMFILE_GHOST:
                continue

            # This is only needed because of an inconsistency in the
            # rpm.fi interface.
            linktos = vp_fi.FLink()

            file_stat = rpm_verify_file(fileinfo, linktos, omitmask)

            #if len(file_stat) > 0 or options.verbose:
            if len(file_stat) > 0:
                fflags = fileinfo[4]
                if fflags & rpm.RPMFILE_CONFIG:
                    file_stat.append('c')
                elif fflags & rpm.RPMFILE_DOC:
                    file_stat.append('d')
                elif fflags & rpm.RPMFILE_GHOST:
                    file_stat.append('g')
                elif fflags & rpm.RPMFILE_LICENSE:
                    file_stat.append('l')
                elif fflags & rpm.RPMFILE_PUBKEY:
                    file_stat.append('P')
                elif fflags & rpm.RPMFILE_README:
                    file_stat.append('r')
                else:
                    file_stat.append(' ')

                file_stat.append(fileinfo[0]) # The filename.
                package_results.setdefault('files', []).append(file_stat)

    # Run the verify script if there is one.
    # Do we want this?
    #if 'noscripts' not in verify_options:
    #    script_stat = rpmVerifyscript()
    #    if script_stat:
    #        package_results['script'] = script_stat

    # If there have been any errors, add the package nevra to the result.
    if len(package_results) > 0:
        package_results.setdefault('nevra', (header[rpm.RPMTAG_NAME], \
                                             header[rpm.RPMTAG_EPOCH], \
                                             header[rpm.RPMTAG_VERSION], \
                                             header[rpm.RPMTAG_RELEASE], \
                                             header[rpm.RPMTAG_ARCH]))
    else:
        package_results = None

    # Put things back the way we found them.
    vsflags = vp_ts.setVSFlags(ovsflags)

    return package_results

def rpm_verify(verify_ts, verify_pkgspec, verify_options=[]):
    """
       Requires rpmtransactionset() to be run first to get a ts.

       pkgspec is a dict specifying the package
       e.g.:
           For a single package
           { name='foo', epoch='20', version='1', release='1', arch='x86_64'}

           For all packages
           {}

       Or any combination of keywords to select one or more packages to verify.

       options is a list of 'rpm --verify' options. Default is to check everything.
       e.g.:
           [ 'nodeps', 'nodigest', 'nofiles', 'noscripts', 'nosignature',
             'nolinkto' 'nomd5', 'nosize', 'nouser', 'nogroup', 'nomtime',
             'nomode', 'nordev' ]

       Returns a list.  One list entry per package.  Each list entry is a
       dictionary.  Dict keys are 'files', 'deps', 'nevra' and 'hdr'.
       Entries only get added for the failures. If nothing failed, None is
       returned.

       Its all a bit messy and probably needs reviewing.

       [ { 'hdr': [???],
           'deps: [((name, version, release), (reqname, reqversion),
                    flags, suggest, sense), .... ]
           'files': [ ['filename1', 'RPMVERIFY_GROUP', 'RPMVERIFY_USER' ],
                      ['filename2', 'RPMVERFIY_LSTATFAIL']]
           'nevra': ['name1', 'epoch1', 'version1', 'release1', 'arch1'] }
         { 'hdr': [???],
           'deps: [((name, version, release), (reqname, reqversion),
                    flags, suggest, sense), .... ]
           'files': [ ['filename', 'RPMVERIFY_GROUP', 'RPMVERIFY_USER" ],
                      ['filename2', 'RPMVERFIY_LSTATFAIL']]
           'nevra': ['name2', 'epoch2', 'version2', 'release2', 'arch2'] } ]

    """
    verify_results = []
    headers = getheadersbykeyword(verify_ts, **verify_pkgspec)
    for header in headers:
        result = rpm_verify_package(verify_ts, header, verify_options)
        if result:
            verify_results.append(result)

    return verify_results

def rpmtransactionset():
    """
        A simple wrapper for rpm.TransactionSet() to keep everthiing together.
        Might use it to set some ts level flags later.

    """
    ts = rpm.TransactionSet()
    return ts

class Rpmtscallback(object):
    """
        Callback for ts.run().  Used for adding, upgrading and removing packages.
        Starting with all possible reasons codes, but bcfg2 will probably only
        make use of a few of them.

        Mostly just printing stuff at the moment to understand how the callback
        is used.

    """
    def __init__(self):
        self.fdnos = {}

    def callback(self, reason, amount, total, key, client_data):
        """
            Generic rpmts call back.
        """
        if   reason == rpm.RPMCALLBACK_INST_OPEN_FILE:
            pass
        elif reason == rpm.RPMCALLBACK_INST_CLOSE_FILE:
            pass
        elif reason == rpm.RPMCALLBACK_INST_START:
            pass
        elif reason == rpm.RPMCALLBACK_TRANS_PROGRESS or \
             reason == rpm.RPMCALLBACK_INST_PROGRESS:
            pass
            #       rpm.RPMCALLBACK_INST_PROGRESS'
        elif reason == rpm.RPMCALLBACK_TRANS_START:
            pass
        elif reason == rpm.RPMCALLBACK_TRANS_STOP:
            pass
        elif reason == rpm.RPMCALLBACK_REPACKAGE_START:
            pass
        elif reason == rpm.RPMCALLBACK_REPACKAGE_PROGRESS:
            pass
        elif reason == rpm.RPMCALLBACK_REPACKAGE_STOP:
            pass
        elif reason == rpm.RPMCALLBACK_UNINST_PROGRESS:
            pass
        elif reason == rpm.RPMCALLBACK_UNINST_START:
            pass
        elif reason == rpm.RPMCALLBACK_UNINST_STOP:
            pass
            # How do we get at this?
            # RPM.modified += key
        elif reason == rpm.RPMCALLBACK_UNPACK_ERROR:
            pass
        elif reason == rpm.RPMCALLBACK_CPIO_ERROR:
            pass
        elif reason == rpm.RPMCALLBACK_UNKNOWN:
            pass
        else:
            print('ERROR - Fell through callBack')


def rpm_erase(erase_pkgspecs, erase_flags):
    """
       pkgspecs is a list of pkgspec dicts specifying packages
       e.g.:
           For a single package
           { name='foo', epoch='20', version='1', release='1', arch='x86_64'}

    """
    erase_ts_flags = 0
    if 'noscripts' in erase_flags:
        erase_ts_flags |= rpm.RPMTRANS_FLAG_NOSCRIPTS
    if 'notriggers' in erase_flags:
        erase_ts_flags |= rpm.RPMTRANS_FLAG_NOTRIGGERS
    if 'repackage' in erase_flags:
        erase_ts_flags |= rpm.RPMTRANS_FLAG_REPACKAGE

    erase_ts = rpmtransactionset()
    erase_ts.setFlags(erase_ts_flags)

    for pkgspec in erase_pkgspecs:
        idx_list = getindexbykeyword(erase_ts, **pkgspec)
        if len(idx_list) > 1 and not 'allmatches' in erase_flags:
            #pass
            print('ERROR - Multiple package match for erase', pkgspec)
        else:
            for idx in idx_list:
                erase_ts.addErase(idx)

    #for te in erase_ts:

    erase_problems = []
    if 'nodeps' not in erase_flags:
        erase_problems = erase_ts.check()

    if erase_problems == []:
        erase_ts.order()
        erase_callback = Rpmtscallback()
        erase_ts.run(erase_callback.callback, 'Erase')
    #else:

    erase_ts.closeDB()
    del erase_ts
    return erase_problems

def display_verify_file(file_results):
    '''
        Display file results similar to rpm --verify.
    '''
    filename = file_results[-1]
    filetype = file_results[-2]

    result_string = ''

    if 'RPMVERIFY_LSTATFAIL' in file_results:
        result_string = 'missing '
    else:
        if 'RPMVERIFY_FILESIZE' in file_results:
            result_string = result_string + 'S'
        else:
            result_string = result_string + '.'

        if 'RPMVERIFY_MODE' in file_results:
            result_string = result_string + 'M'
        else:
            result_string = result_string + '.'

        if 'RPMVERIFY_MD5' in file_results:
            if 'RPMVERIFY_READFAIL' in file_results:
                result_string = result_string + '?'
            else:
                result_string = result_string + '5'
        else:
            result_string = result_string + '.'

        if 'RPMVERIFY_RDEV' in file_results:
            result_string = result_string + 'D'
        else:
            result_string = result_string + '.'

        if 'RPMVERIFY_LINKTO' in file_results:
            if 'RPMVERIFY_READLINKFAIL' in file_results:
                result_string = result_string + '?'
            else:
                result_string = result_string + 'L'
        else:
            result_string = result_string + '.'

        if 'RPMVERIFY_USER' in file_results:
            result_string = result_string + 'U'
        else:
            result_string = result_string + '.'

        if 'RPMVERIFY_GROUP' in file_results:
            result_string = result_string + 'G'
        else:
            result_string = result_string + '.'

        if 'RPMVERIFY_MTIME' in file_results:
            result_string = result_string + 'T'
        else:
            result_string = result_string + '.'

    print(result_string + '  ' + filetype + ' ' + filename)
    sys.stdout.flush()

#===============================================================================
# Some options and output to assist with development and testing.
# These are not intended for normal use.
if __name__ == "__main__":

    p = optparse.OptionParser()

    p.add_option('--name', action='store', \
                 default=None, \
                 help='''Package name to verify.

                         ******************************************
                         NOT SPECIFYING A NAME MEANS 'ALL' PACKAGES.
                         ******************************************

                         The specified operation will be carried out on  all
                         instances of packages that match the package specification
                         (name, epoch, version, release, arch).''')

    p.add_option('--epoch', action='store', \
                 default=None, \
                 help='''Package epoch.''')

    p.add_option('--version', action='store', \
                 default=None, \
                 help='''Package version.''')

    p.add_option('--release', action='store', \
                 default=None, \
                 help='''Package release.''')

    p.add_option('--arch', action='store', \
                 default=None, \
                 help='''Package arch.''')

    p.add_option('--erase', '-e', action='store_true', \
                 default=None, \
                 help='''****************************************************
                         REMOVE PACKAGES.  THERE ARE NO WARNINGS.  MULTIPLE
                         PACKAGES WILL BE REMOVED IF A FULL PACKAGE SPEC IS NOT
                         GIVEN. E.G. IF JUST A NAME IS GIVEN ALL INSTALLED
                         INSTANCES OF THAT PACKAGE WILL BE REMOVED PROVIDED
                         DEPENDENCY CHECKS PASS.  IF JUST AN EPOCH IS GIVEN
                         ALL PACKAGE INSTANCES WITH THAT EPOCH WILL BE REMOVED.
                         ****************************************************''')

    p.add_option('--list', '-l', action='store_true', \
                 help='''List package identity info. rpm -qa ish equivalent
                         intended for use in RefreshPackages().''')

    p.add_option('--verify', action='store_true', \
                 help='''Verify Package(s).  Output is only produced after all
                         packages has been verified. Be patient.''')

    p.add_option('--verbose', '-v', action='store_true', \
                 help='''Verbose output for --verify option.  Output is the
                         same as rpm -v --verify.''')

    p.add_option('--nodeps', action='store_true', \
                 default=False, \
                 help='Do not do dependency testing.')

    p.add_option('--nodigest', action='store_true', \
                 help='Do not check package digests.')

    p.add_option('--nofiles', action='store_true', \
                 help='Do not do file checks.')

    p.add_option('--noscripts', action='store_true', \
                 help='Do not run verification scripts.')

    p.add_option('--nosignature', action='store_true', \
                 help='Do not do package signature verification.')

    p.add_option('--nolinkto', action='store_true', \
                 help='Do not do symlink tests.')

    p.add_option('--nomd5', action='store_true', \
                 help='''Do not do MD5 checksums on files.  Note that this does
                                            not work for prelink files yet.''')

    p.add_option('--nosize', action='store_true', \
                 help='''Do not do file size tests. Note that this does not work
                                            for prelink files yet.''')

    p.add_option('--nouser', action='store_true', \
                 help='Do not check file user ownership.')

    p.add_option('--nogroup', action='store_true', \
                 help='Do not check file group ownership.')

    p.add_option('--nomtime', action='store_true', \
                 help='Do not check file modification times.')

    p.add_option('--nomode', action='store_true', \
                 help='Do not check file modes (permissions).')

    p.add_option('--nordev', action='store_true', \
                 help='Do not check device node.')

    p.add_option('--notriggers', action='store_true', \
                 help='Do not do not generate triggers on erase.')

    p.add_option('--repackage', action='store_true', \
                 help='''Do repackage on erase.i Packages are put
                                            in /var/spool/repackage.''')

    p.add_option('--allmatches', action='store_true', \
                 help='''Remove all package instances that match the
                         pkgspec.

                         ***************************************************
                         NO WARNINGS ARE GIVEN.  IF THERE IS NO PACKAGE SPEC
                         THAT MEANS ALL PACKAGES!!!!
                         ***************************************************''')

    options, arguments = p.parse_args()

    pkgspec = {}
    rpm_options = []

    if options.nodeps:
        rpm_options.append('nodeps')

    if options.nodigest:
        rpm_options.append('nodigest')

    if options.nofiles:
        rpm_options.append('nofiles')

    if options.noscripts:
        rpm_options.append('noscripts')

    if options.nosignature:
        rpm_options.append('nosignature')

    if options.nolinkto:
        rpm_options.append('nolinkto')

    if options.nomd5:
        rpm_options.append('nomd5')

    if options.nosize:
        rpm_options.append('nosize')

    if options.nouser:
        rpm_options.append('nouser')

    if options.nogroup:
        rpm_options.append('nogroup')

    if options.nomtime:
        rpm_options.append('nomtime')

    if options.nomode:
        rpm_options.append('nomode')

    if options.nordev:
        rpm_options.append('nordev')

    if options.repackage:
        rpm_options.append('repackage')

    if options.allmatches:
        rpm_options.append('allmatches')

    main_ts = rpmtransactionset()

    cmdline_pkgspec = {}
    if options.name != 'all':
        if options.name:
            cmdline_pkgspec['name'] = str(options.name)
        if options.epoch:
            cmdline_pkgspec['epoch'] = str(options.epoch)
        if options.version:
            cmdline_pkgspec['version'] = str(options.version)
        if options.release:
            cmdline_pkgspec['release'] = str(options.release)
        if options.arch:
            cmdline_pkgspec['arch'] = str(options.arch)

    if options.verify:
        results = rpm_verify(main_ts, cmdline_pkgspec, rpm_options)
        for r in results:
            files = r.get('files', '')
            for f in files:
                display_verify_file(f)

    elif options.list:
        for p in rpmpackagelist(main_ts):
            print(p)

    elif options.erase:
        if options.name:
            rpm_erase([cmdline_pkgspec], rpm_options)
        else:
            print('You must specify the "--name" option')

########NEW FILE########
__FILENAME__ = SELinux
""" Classes for SELinux entry support """

import os
import re
import sys
import copy
import glob
import struct
import socket
import selinux
import seobject
import Bcfg2.Client.XML
import Bcfg2.Client.Tools
from Bcfg2.Client.Tools.POSIX.File import POSIXFile
from Bcfg2.Compat import long  # pylint: disable=W0622


def pack128(int_val):
    """ pack a 128-bit integer in big-endian format """
    max_word_size = 2 ** 32 - 1

    if int_val <= max_word_size:
        return struct.pack('>L', int_val)

    words = []
    for i in range(4):  # pylint: disable=W0612
        word = int_val & max_word_size
        words.append(int(word))
        int_val >>= 32
    words.reverse()
    return struct.pack('>4I', *words)


def netmask_itoa(netmask, proto="ipv4"):
    """ convert an integer netmask (e.g., /16) to dotted-quad
    notation (255.255.0.0) or IPv6 prefix notation (ffff::) """
    if proto == "ipv4":
        size = 32
        family = socket.AF_INET
    else:  # ipv6
        size = 128
        family = socket.AF_INET6
    try:
        netmask = int(netmask)
    except ValueError:
        return netmask

    if netmask > size:
        raise ValueError("Netmask too large: %s" % netmask)

    res = long(0)
    for i in range(netmask):
        res |= 1 << (size - i - 1)
    netmask = socket.inet_ntop(family, pack128(res))
    return netmask


class SELinux(Bcfg2.Client.Tools.Tool):
    """ SELinux entry support """
    name = 'SELinux'
    __handles__ = [('SEBoolean', None),
                   ('SEFcontext', None),
                   ('SEInterface', None),
                   ('SELogin', None),
                   ('SEModule', None),
                   ('SENode', None),
                   ('SEPermissive', None),
                   ('SEPort', None),
                   ('SEUser', None)]
    __req__ = dict(SEBoolean=['name', 'value'],
                   SEFcontext=['name', 'selinuxtype'],
                   SEInterface=['name', 'selinuxtype'],
                   SELogin=['name', 'selinuxuser'],
                   SEModule=['name'],
                   SENode=['name', 'selinuxtype', 'proto'],
                   SEPermissive=['name'],
                   SEPort=['name', 'selinuxtype'],
                   SEUser=['name', 'roles', 'prefix'])

    def __init__(self, logger, setup, config):
        Bcfg2.Client.Tools.Tool.__init__(self, logger, setup, config)
        self.handlers = {}
        for handler in self.__handles__:
            etype = handler[0]
            self.handlers[etype] = \
                globals()["SELinux%sHandler" % etype.title()](self, logger,
                                                              setup, config)
        self.txn = False
        self.post_txn_queue = []

    def __getattr__(self, attr):
        if attr.startswith("VerifySE"):
            return self.GenericSEVerify
        elif attr.startswith("InstallSE"):
            return self.GenericSEInstall
        # there's no need for an else here, because python checks for
        # an attribute in the "normal" ways first.  i.e., if self.txn
        # is used, __getattr__() is never called because txn exists as
        # a "normal" attribute of this object.  See
        # http://docs.python.org/2/reference/datamodel.html#object.__getattr__
        # for details

    def BundleUpdated(self, _, states):
        for handler in self.handlers.values():
            handler.BundleUpdated(states)

    def FindExtra(self):
        extra = []
        for handler in self.handlers.values():
            extra.extend(handler.FindExtra())
        return extra

    def canInstall(self, entry):
        return (Bcfg2.Client.Tools.Tool.canInstall(self, entry) and
                self.handlers[entry.tag].canInstall(entry))

    def primarykey(self, entry):
        """ return a string that should be unique amongst all entries
        in the specification """
        return self.handlers[entry.tag].primarykey(entry)

    def Install(self, entries, states):
        # start a transaction
        semanage = seobject.semanageRecords("")
        if hasattr(semanage, "start"):
            self.logger.debug("Starting SELinux transaction")
            semanage.start()
            self.txn = True
        else:
            self.logger.debug("SELinux transactions not supported; this may "
                              "slow things down considerably")
        Bcfg2.Client.Tools.Tool.Install(self, entries, states)
        if hasattr(semanage, "finish"):
            self.logger.debug("Committing SELinux transaction")
            semanage.finish()
            self.txn = False
            for func, arg, kwargs in self.post_txn_queue:
                states[arg] = func(*arg, **kwargs)

    def GenericSEInstall(self, entry):
        """Dispatch install to the proper method according to entry tag"""
        return self.handlers[entry.tag].Install(entry)

    def GenericSEVerify(self, entry, _):
        """Dispatch verify to the proper method according to entry tag"""
        rv = self.handlers[entry.tag].Verify(entry)
        if entry.get('qtext') and self.setup['interactive']:
            entry.set('qtext',
                      '%s\nInstall %s: (y/N) ' %
                      (entry.get('qtext'),
                       self.handlers[entry.tag].tostring(entry)))
        return rv

    def Remove(self, entries):
        """Dispatch verify to the proper removal
        method according to entry tag"""
        # sort by type
        types = list()
        for entry in entries:
            if entry.tag not in types:
                types.append(entry.tag)

        for etype in types:
            self.handlers[etype].Remove([e for e in entries
                                         if e.tag == etype])


class SELinuxEntryHandler(object):
    """ Generic handler for all SELinux entries """
    etype = None
    key_format = ("name",)
    value_format = ()
    str_format = '%(name)s'
    custom_re = re.compile(r' (?P<name>\S+)$')
    custom_format = None

    def __init__(self, tool, logger, setup, config):
        self.tool = tool
        self.logger = logger
        self.setup = setup
        self.config = config
        self._records = None
        self._all = None
        if not self.custom_format:
            self.custom_format = self.key_format

    @property
    def records(self):
        """ return the records object for this entry type """
        if self._records is None:
            self._records = getattr(seobject, "%sRecords" % self.etype)("")
        return self._records

    @property
    def all_records(self):
        """ get a dict of all defined records for this entry type """
        if self._all is None:
            self._all = self.records.get_all()
        return self._all

    @property
    def custom_records(self):
        """ try to get a dict of all customized records for this entry
        type, if the records object supports the customized() method
        """
        if hasattr(self.records, "customized") and self.custom_re:
            rv = dict()
            for key in self.custom_keys:
                if key in self.all_records:
                    rv[key] = self.all_records[key]
                else:
                    self.logger.warning("SELinux %s %s customized, but no "
                                        "record found. This may indicate an "
                                        "error in your SELinux policy." %
                                        (self.etype, key))
            return rv
        else:
            # ValueError is really a pretty dumb exception to raise,
            # but that's what the seobject customized() method raises
            # if it's defined but not implemented.  yeah, i know, wtf.
            raise ValueError("custom_records")

    @property
    def custom_keys(self):
        """ get a list of keys for selinux records of this entry type
        that have been customized """
        keys = []
        for cmd in self.records.customized():
            match = self.custom_re.search(cmd)
            if match:
                if (len(self.custom_format) == 1 and
                    self.custom_format[0] == "name"):
                    keys.append(match.group("name"))
                else:
                    keys.append(tuple([match.group(k)
                                       for k in self.custom_format]))
        return keys

    def tostring(self, entry):
        """ transform an XML SELinux entry into a human-readable
        string """
        return self.str_format % entry.attrib

    def keytostring(self, key):
        """ transform a SELinux record key into a human-readable
        string """
        return self.str_format % self._key2attrs(key)

    def _key(self, entry):
        """ Generate an SELinux record key from an XML SELinux entry """
        if len(self.key_format) == 1 and self.key_format[0] == "name":
            return entry.get("name")
        else:
            rv = []
            for key in self.key_format:
                rv.append(entry.get(key))
            return tuple(rv)

    def _key2attrs(self, key):
        """ Generate an XML attribute dict from an SELinux record key """
        if isinstance(key, tuple):
            rv = dict((self.key_format[i], key[i])
                      for i in range(len(self.key_format))
                      if self.key_format[i])
        else:
            rv = dict(name=key)
        if self.value_format:
            vals = self.all_records[key]
            rv.update(dict((self.value_format[i], vals[i])
                           for i in range(len(self.value_format))
                           if self.value_format[i]))
        return rv

    def key2entry(self, key):
        """ Generate an XML entry from an SELinux record key """
        attrs = self._key2attrs(key)
        return Bcfg2.Client.XML.Element("SE%s" % self.etype.title(), **attrs)

    def _args(self, entry, method):
        """ Get the argument list for invoking _modify or _add, or
        _delete methods """
        if hasattr(self, "_%sargs" % method):
            return getattr(self, "_%sargs" % method)(entry)
        elif hasattr(self, "_defaultargs"):
            # default args
            return self._defaultargs(entry)  # pylint: disable=E1101
        else:
            raise NotImplementedError

    def _deleteargs(self, entry):
        """ Get the argument list for invoking delete methods """
        return (self._key(entry))

    def canInstall(self, entry):
        """ return True if this entry is complete and can be installed """
        return bool(self._key(entry))

    def primarykey(self, entry):
        """ return a string that should be unique amongst all entries
        in the specification.  some entry types are not universally
        disambiguated by tag:type:name alone """
        return ":".join([entry.tag, entry.get("name")])

    def exists(self, entry):
        """ return True if the entry already exists in the record list """
        if self._key(entry) not in self.all_records:
            self.logger.debug("SELinux %s %s does not exist" %
                              (self.etype, self.tostring(entry)))
            return False
        return True

    def Verify(self, entry):
        """ verify that the entry is correct on the client system """
        if not self.exists(entry):
            entry.set('current_exists', 'false')
            return False

        errors = []
        current_attrs = self._key2attrs(self._key(entry))
        desired_attrs = entry.attrib
        for attr in self.value_format:
            if not attr:
                continue
            if current_attrs[attr] != desired_attrs[attr]:
                entry.set('current_%s' % attr, current_attrs[attr])
                errors.append("%s %s has wrong %s: %s, should be %s" %
                              (entry.tag, entry.get('name'), attr,
                               current_attrs[attr], desired_attrs[attr]))

        if errors:
            for error in errors:
                self.logger.debug(error)
            entry.set('qtext', "\n".join([entry.get('qtext', '')] + errors))
            return False
        else:
            return True

    def Install(self, entry, method=None):
        """ install the entry on the client system """
        if not method:
            if self.exists(entry):
                method = "modify"
            else:
                method = "add"
        self.logger.debug("%s SELinux %s %s" %
                          (method.title(), self.etype, self.tostring(entry)))

        try:
            getattr(self.records, method)(*self._args(entry, method))
            self._all = None
            return True
        except ValueError:
            err = sys.exc_info()[1]
            self.logger.info("Failed to %s SELinux %s %s: %s" %
                             (method, self.etype, self.tostring(entry), err))
            return False

    def Remove(self, entries):
        """ remove the entry from the client system """
        for entry in entries:
            try:
                self.records.delete(*self._args(entry, "delete"))
                self._all = None
            except ValueError:
                err = sys.exc_info()[1]
                self.logger.info("Failed to remove SELinux %s %s: %s" %
                                 (self.etype, self.tostring(entry), err))

    def FindExtra(self):
        """ find extra entries of this entry type """
        specified = [self._key(e)
                     for e in self.tool.getSupportedEntries()
                     if e.tag == "SE%s" % self.etype.title()]
        try:
            records = self.custom_records
        except ValueError:
            records = self.all_records
        return [self.key2entry(key)
                for key in records.keys()
                if key not in specified]

    def BundleUpdated(self, states):
        """ perform any additional magic tasks that need to be run
        when a bundle is updated """
        pass


class SELinuxSebooleanHandler(SELinuxEntryHandler):
    """ handle SELinux boolean entries """
    etype = "boolean"
    value_format = ("value",)

    @property
    def all_records(self):
        # older versions of selinux return a single 0/1 value for each
        # bool, while newer versions return a list of three 0/1 values
        # representing various states. we don't care about the latter
        # two values, but it's easier to coerce the older format into
        # the newer format as far as interoperation with the rest of
        # SELinuxEntryHandler goes
        rv = SELinuxEntryHandler.all_records.fget(self)
        if rv.values()[0] in [0, 1]:
            for key, val in rv.items():
                rv[key] = [val, val, val]
        return rv

    def _key2attrs(self, key):
        rv = SELinuxEntryHandler._key2attrs(self, key)
        status = self.all_records[key][0]
        if status:
            rv['value'] = "on"
        else:
            rv['value'] = "off"
        return rv

    def _defaultargs(self, entry):
        """ argument list for adding, modifying and deleting entries """
        # the only values recognized by both new and old versions of
        # selinux are the strings "0" and "1".  old selinux accepts
        # ints or bools as well, new selinux accepts "on"/"off"
        if entry.get("value").lower() == "on":
            value = "1"
        else:
            value = "0"
        return (entry.get("name"), value)

    def canInstall(self, entry):
        if entry.get("value").lower() not in ["on", "off"]:
            self.logger.debug("SELinux %s %s has a bad value: %s" %
                              (self.etype, self.tostring(entry),
                               entry.get("value")))
            return False
        return (self.exists(entry) and
                SELinuxEntryHandler.canInstall(self, entry))


class SELinuxSeportHandler(SELinuxEntryHandler):
    """ handle SELinux port entries """
    etype = "port"
    value_format = ('selinuxtype', None)
    custom_re = re.compile(r'-p (?P<proto>tcp|udp).*? '
                           r'(?P<start>\d+)(?:-(?P<end>\d+))?$')

    @property
    def custom_keys(self):
        keys = []
        for cmd in self.records.customized():
            match = self.custom_re.search(cmd)
            if match:
                if match.group('end'):
                    keys.append((int(match.group('start')),
                                 int(match.group('end')),
                                 match.group('proto')))
                else:
                    keys.append((int(match.group('start')),
                                 int(match.group('start')),
                                 match.group('proto')))
        return keys

    @property
    def all_records(self):
        if self._all is None:
            # older versions of selinux use (startport, endport) as
            # they key for the ports.get_all() dict, and (type, proto,
            # level) as the value; this is obviously broken, so newer
            # versions use (startport, endport, proto) as the key, and
            # (type, level) as the value.  abstracting around this
            # sucks.
            ports = self.records.get_all()
            if len(ports.keys()[0]) == 3:
                self._all = ports
            else:
                # uglist list comprehension ever?
                self._all = dict([((k[0], k[1], v[1]), (v[0], v[2]))
                                  for k, v in ports.items()])
        return self._all

    def _key(self, entry):
        try:
            (port, proto) = entry.get("name").split("/")
        except ValueError:
            self.logger.error("Invalid SELinux node %s: no protocol specified"
                              % entry.get("name"))
            return
        if "-" in port:
            start, end = port.split("-")
        else:
            start = port
            end = port
        return (int(start), int(end), proto)

    def _key2attrs(self, key):
        if key[0] == key[1]:
            port = str(key[0])
        else:
            port = "%s-%s" % (key[0], key[1])
        vals = self.all_records[key]
        return dict(name="%s/%s" % (port, key[2]), selinuxtype=vals[0])

    def _defaultargs(self, entry):
        """ argument list for adding and modifying entries """
        (port, proto) = entry.get("name").split("/")
        return (port, proto, entry.get("mlsrange", ""),
                entry.get("selinuxtype"))

    def _deleteargs(self, entry):
        return tuple(entry.get("name").split("/"))


class SELinuxSefcontextHandler(SELinuxEntryHandler):
    """ handle SELinux file context entries """

    etype = "fcontext"
    key_format = ("name", "filetype")
    value_format = (None, None, "selinuxtype", None)
    filetypeargs = dict(all="",
                        regular="--",
                        directory="-d",
                        symlink="-l",
                        pipe="-p",
                        socket="-s",
                        block="-b",
                        char="-c",
                        door="-D")
    filetypenames = dict(all="all files",
                         regular="regular file",
                         directory="directory",
                         symlink="symbolic link",
                         pipe="named pipe",
                         socket="socket",
                         block="block device",
                         char="character device",
                         door="door")
    filetypeattrs = dict([v, k] for k, v in filetypenames.iteritems())
    custom_re = re.compile(r'-f \'(?P<filetype>[a-z ]+)\'.*? \'(?P<name>.*)\'')

    @property
    def all_records(self):
        if self._all is None:
            # on older selinux, fcontextRecords.get_all() returns a
            # list of tuples of (filespec, filetype, seuser, serole,
            # setype, level); on newer selinux, get_all() returns a
            # dict of (filespec, filetype) => (seuser, serole, setype,
            # level).
            fcontexts = self.records.get_all()
            if isinstance(fcontexts, dict):
                self._all = fcontexts
            else:
                self._all = dict([(f[0:2], f[2:]) for f in fcontexts])
        return self._all

    def _key(self, entry):
        ftype = entry.get("filetype", "all")
        return (entry.get("name"),
                self.filetypenames.get(ftype, ftype))

    def _key2attrs(self, key):
        rv = dict(name=key[0], filetype=self.filetypeattrs[key[1]])
        vals = self.all_records[key]
        # in older versions of selinux, an fcontext with no selinux
        # type is the single value None; in newer versions, it's a
        # tuple whose 0th (and only) value is None.
        if vals and vals[0]:
            rv["selinuxtype"] = vals[2]
        else:
            rv["selinuxtype"] = "<<none>>"
        return rv

    def canInstall(self, entry):
        return (entry.get("filetype", "all") in self.filetypeargs and
                SELinuxEntryHandler.canInstall(self, entry))

    def _defaultargs(self, entry):
        """ argument list for adding, modifying, and deleting entries """
        return (entry.get("name"), entry.get("selinuxtype"),
                self.filetypeargs[entry.get("filetype", "all")],
                entry.get("mlsrange", ""), '')

    def primarykey(self, entry):
        return ":".join([entry.tag, entry.get("name"),
                         entry.get("filetype", "all")])


class SELinuxSenodeHandler(SELinuxEntryHandler):
    """ handle SELinux node entries """

    etype = "node"
    value_format = (None, None, "selinuxtype", None)
    str_format = '%(name)s (%(proto)s)'
    custom_re = re.compile(r'-M (?P<netmask>\S+).*?'
                           r'-p (?P<proto>ipv\d).*? (?P<addr>\S+)$')
    custom_format = ('addr', 'netmask', 'proto')

    def _key(self, entry):
        try:
            (addr, netmask) = entry.get("name").split("/")
        except ValueError:
            self.logger.error("Invalid SELinux node %s: no netmask specified" %
                              entry.get("name"))
            return
        netmask = netmask_itoa(netmask, proto=entry.get("proto"))
        return (addr, netmask, entry.get("proto"))

    def _key2attrs(self, key):
        vals = self.all_records[key]
        return dict(name="%s/%s" % (key[0], key[1]), proto=key[2],
                    selinuxtype=vals[2])

    def _defaultargs(self, entry):
        """ argument list for adding, modifying, and deleting entries """
        (addr, netmask) = entry.get("name").split("/")
        return (addr, netmask, entry.get("proto"), entry.get("mlsrange", ""),
                entry.get("selinuxtype"))


class SELinuxSeloginHandler(SELinuxEntryHandler):
    """ handle SELinux login entries """

    etype = "login"
    value_format = ("selinuxuser", None)

    def _defaultargs(self, entry):
        """ argument list for adding, modifying, and deleting entries """
        return (entry.get("name"), entry.get("selinuxuser"),
                entry.get("mlsrange", ""))


class SELinuxSeuserHandler(SELinuxEntryHandler):
    """ handle SELinux user entries """

    etype = "user"
    value_format = ("prefix", None, None, "roles")

    def __init__(self, tool, logger, setup, config):
        SELinuxEntryHandler.__init__(self, tool, logger, setup, config)
        self.needs_prefix = False

    @property
    def records(self):
        if self._records is None:
            self._records = seobject.seluserRecords()
        return self._records

    def Install(self, entry, method=None):
        # in older versions of selinux, modify() is broken if you
        # provide a prefix _at all_, so we try to avoid giving the
        # prefix.  however, in newer versions, prefix is _required_,
        # so we a) try without a prefix; b) catch TypeError, which
        # indicates that we had the wrong number of args (ValueError
        # is thrown by the bug in older versions of selinux); and c)
        # try with prefix.
        try:
            SELinuxEntryHandler.Install(self, entry, method=method)
        except TypeError:
            self.needs_prefix = True
            SELinuxEntryHandler.Install(self, entry, method=method)

    def _defaultargs(self, entry):
        """ argument list for adding, modifying, and deleting entries """
        # in older versions of selinux, modify() is broken if you
        # provide a prefix _at all_, so we try to avoid giving the
        # prefix.  see the comment in Install() above for more
        # details.
        rv = [entry.get("name"),
              entry.get("roles", "").replace(" ", ",").split(","),
              '', entry.get("mlsrange", "")]
        if self.needs_prefix:
            rv.append(entry.get("prefix"))
        else:
            key = self._key(entry)
            if key in self.all_records:
                attrs = self._key2attrs(key)
                if attrs['prefix'] != entry.get("prefix"):
                    rv.append(entry.get("prefix"))
        return tuple(rv)


class SELinuxSeinterfaceHandler(SELinuxEntryHandler):
    """ handle SELinux interface entries """

    etype = "interface"
    value_format = (None, None, "selinuxtype", None)

    def _defaultargs(self, entry):
        """ argument list for adding, modifying, and deleting entries """
        return (entry.get("name"), entry.get("mlsrange", ""),
                entry.get("selinuxtype"))


class SELinuxSepermissiveHandler(SELinuxEntryHandler):
    """ handle SELinux permissive domain entries """

    etype = "permissive"

    @property
    def records(self):
        try:
            return SELinuxEntryHandler.records.fget(self)
        except AttributeError:
            self.logger.info("Permissive domains not supported by this "
                             "version of SELinux")
            self._records = None
            return self._records

    @property
    def all_records(self):
        if self._all is None:
            if self.records is None:
                self._all = dict()
            else:
                # permissiveRecords.get_all() returns a list, so we just
                # make it into a dict so that the rest of
                # SELinuxEntryHandler works
                self._all = dict([(d, d) for d in self.records.get_all()])
        return self._all

    def _defaultargs(self, entry):
        """ argument list for adding, modifying, and deleting entries """
        return (entry.get("name"),)


class SELinuxSemoduleHandler(SELinuxEntryHandler):
    """ handle SELinux module entries """

    etype = "module"
    value_format = (None, "disabled")

    def __init__(self, tool, logger, setup, config):
        SELinuxEntryHandler.__init__(self, tool, logger, setup, config)
        self.filetool = POSIXFile(logger, setup, config)
        try:
            self.setype = selinux.selinux_getpolicytype()[1]
        except IndexError:
            self.logger.error("Unable to determine SELinux policy type")
            self.setype = None

    @property
    def all_records(self):
        if self._all is None:
            try:
                # we get a list of tuples back; coerce it into a dict
                self._all = dict([(m[0], (m[1], m[2]))
                                  for m in self.records.get_all()])
            except AttributeError:
                # early versions of seobject don't have moduleRecords,
                # so we parse the output of `semodule` >_<
                self._all = dict()
                self.logger.debug("SELinux: Getting modules from semodule")
                try:
                    rv = self.tool.cmd.run(['semodule', '-l'])
                except OSError:
                    # semanage failed; probably not in $PATH.  try to
                    # get the list of modules from the filesystem
                    err = sys.exc_info()[1]
                    self.logger.debug("SELinux: Failed to run semodule: %s" %
                                      err)
                    self._all.update(self._all_records_from_filesystem())
                else:
                    if rv.success:
                        # ran semodule successfully
                        for line in rv.stdout.splitlines():
                            mod, version = line.split()
                            self._all[mod] = (version, 1)

                        # get other (disabled) modules from the filesystem
                        for mod in self._all_records_from_filesystem().keys():
                            if mod not in self._all:
                                self._all[mod] = ('', 0)
                    else:
                        self.logger.error("SELinux: Failed to run semodule: %s"
                                          % rv.error)
                        self._all.update(self._all_records_from_filesystem())
        return self._all

    def _all_records_from_filesystem(self):
        """ the seobject API doesn't support modules and semodule is
        broken or missing, so just list modules on the filesystem.
        this is terrible. """
        self.logger.debug("SELinux: Getting modules from filesystem")
        rv = dict()
        for mod in glob.glob(os.path.join("/usr/share/selinux", self.setype,
                                          "*.pp")):
            rv[os.path.basename(mod)[:-3]] = ('', 1)
        return rv

    def _key(self, entry):
        name = entry.get("name").lstrip("/")
        if name.endswith(".pp"):
            return name[:-3]
        else:
            return name

    def _key2attrs(self, key):
        rv = SELinuxEntryHandler._key2attrs(self, key)
        status = self.all_records[key][1]
        if status:
            rv['disabled'] = "false"
        else:
            rv['disabled'] = "true"
        return rv

    def _filepath(self, entry):
        """ get the path to the .pp module file for this module entry
        """
        return os.path.join("/usr/share/selinux", self.setype,
                            entry.get("name") + '.pp')

    def _pathentry(self, entry):
        """ Get an XML Path entry based on this SELinux module entry,
        suitable for installing the module .pp file itself to the
        filesystem """
        pathentry = copy.deepcopy(entry)
        pathentry.set("name", self._filepath(pathentry))
        pathentry.set("mode", "0644")
        pathentry.set("owner", "root")
        pathentry.set("group", "root")
        pathentry.set("secontext", "__default__")
        return pathentry

    def Verify(self, entry):
        if not entry.get("disabled"):
            entry.set("disabled", "false")
        return (SELinuxEntryHandler.Verify(self, entry) and
                self.filetool.verify(self._pathentry(entry), []))

    def canInstall(self, entry):
        return (entry.text and self.setype and
                SELinuxEntryHandler.canInstall(self, entry))

    def Install(self, entry, _=None):
        if not self.filetool.install(self._pathentry(entry)):
            return False
        if hasattr(seobject, 'moduleRecords'):
            # if seobject has the moduleRecords attribute, install the
            # module using the seobject library
            return self._install_seobject(entry)
        else:
            # seobject doesn't have the moduleRecords attribute, so
            # install the module using `semodule`
            self.logger.debug("Installing %s using semodule" %
                              entry.get("name"))
            self._all = None
            return self._install_semodule(entry)

    def _install_seobject(self, entry):
        """ Install an SELinux module using the seobject library """
        try:
            if not SELinuxEntryHandler.Install(self, entry):
                return False
        except NameError:
            # some versions of selinux have a bug in seobject that
            # makes modify() calls fail.  add() seems to have the same
            # effect as modify, but without the bug
            if self.exists(entry):
                if not SELinuxEntryHandler.Install(self, entry, method="add"):
                    return False

        if entry.get("disabled", "false").lower() == "true":
            method = "disable"
        else:
            method = "enable"
        return SELinuxEntryHandler.Install(self, entry, method=method)

    def _install_semodule(self, entry, fromqueue=False):
        """ Install an SELinux module using the semodule command """
        if fromqueue:
            self.logger.debug("Installing SELinux module %s from "
                              "post-transaction queue" % entry.get("name"))
        elif self.tool.txn:
            # we've started a transaction, so if we run semodule -i
            # then it'll fail with lock errors.  so we add this
            # installation to a queue to be run after the transaction
            # is closed.
            self.logger.debug("Waiting to install SELinux module %s until "
                              "SELinux transaction is finished" %
                              entry.get('name'))
            self.tool.post_txn_queue.append((self._install_semodule,
                                             (entry,),
                                             dict(fromqueue=True)))
            return False
        self.logger.debug("Install SELinux module %s with semodule -i %s" %
                          (entry.get('name'), self._filepath(entry)))
        try:
            rv = self.tool.cmd.run(['semodule', '-i', self._filepath(entry)])
        except OSError:
            err = sys.exc_info()[1]
            self.logger.error("Failed to install SELinux module %s with "
                              "semodule: %s" % (entry.get("name"), err))
            return False
        if rv.success:
            if entry.get("disabled", "false").lower() == "true":
                self.logger.warning("SELinux: Cannot disable modules with "
                                    "semodule")
                return False
            else:
                return True
        else:
            self.logger.error("Failed to install SELinux module %s with "
                              "semodule: %s" % (entry.get("name"), rv.error))
            return False

    def _addargs(self, entry):
        """ argument list for adding entries """
        return (self._filepath(entry),)

    def _defaultargs(self, entry):
        """ argument list for modifying and deleting entries """
        return (entry.get("name"),)

    def FindExtra(self):
        specified = [self._key(e)
                     for e in self.tool.getSupportedEntries()]
        rv = []
        for module in self._all_records_from_filesystem().keys():
            if module not in specified:
                rv.append(self.key2entry(module))
        return rv

########NEW FILE########
__FILENAME__ = SMF
"""SMF support for Bcfg2"""

import glob
import os

import Bcfg2.Client.Tools


class SMF(Bcfg2.Client.Tools.SvcTool):
    """Support for Solaris SMF Services."""
    __handles__ = [('Service', 'smf')]
    __execs__ = ['/usr/sbin/svcadm', '/usr/bin/svcs']
    __req__ = {'Service': ['name', 'status', 'FMRI']}

    def get_svc_command(self, service, action):
        if service.get('type') == 'lrc':
            return Bcfg2.Client.Tools.SvcTool.get_svc_command(self,
                                                              service, action)
        if action == 'stop':
            return "/usr/sbin/svcadm disable %s" % (service.get('FMRI'))
        elif action == 'restart':
            return "/usr/sbin/svcadm restart %s" % (service.get('FMRI'))
        elif action == 'start':
            return "/usr/sbin/svcadm enable %s" % (service.get('FMRI'))

    def GetFMRI(self, entry):
        """Perform FMRI resolution for service."""
        if 'FMRI' not in entry.attrib:
            rv = self.cmd.run(["/usr/bin/svcs", "-H", "-o", "FMRI",
                               entry.get('name')])
            if rv.success:
                entry.set('FMRI', rv.stdout.splitlines()[0])
            else:
                self.logger.info('Failed to locate FMRI for service %s' %
                                 entry.get('name'))
            return rv.success
        return True

    def VerifyService(self, entry, _):
        """Verify SMF Service entry."""
        if not self.GetFMRI(entry):
            self.logger.error("smf service %s doesn't have FMRI set" %
                              entry.get('name'))
            return False
        if entry.get('FMRI').startswith('lrc'):
            filename = entry.get('FMRI').split('/')[-1]
            # this is a legacy service
            gname = "/etc/rc*.d/%s" % filename
            files = glob.glob(gname.replace('_', '.'))
            if files:
                self.logger.debug("Matched %s with %s" %
                                  (entry.get("FMRI"), ":".join(files)))
                return entry.get('status') == 'on'
            else:
                self.logger.debug("No service matching %s" %
                                  entry.get("FMRI"))
                return entry.get('status') == 'off'
        try:
            srvdata = \
                self.cmd.run("/usr/bin/svcs -H -o STA %s" %
                             entry.get('FMRI')).stdout.splitlines()[0].split()
        except IndexError:
            # Occurs when no lines are returned (service not installed)
            return False

        entry.set('current_status', srvdata[0])
        if entry.get('status') == 'on':
            return srvdata[0] == 'ON'
        else:
            return srvdata[0] in ['OFF', 'UN', 'MNT', 'DIS', 'DGD']

    def InstallService(self, entry):
        """Install SMF Service entry."""
        self.logger.info("Installing Service %s" % (entry.get('name')))
        if entry.get('status') == 'off':
            if entry.get("FMRI").startswith('lrc'):
                try:
                    loc = entry.get("FMRI")[4:].replace('_', '.')
                    self.logger.debug("Renaming file %s to %s" %
                                      (loc, loc.replace('/S', '/DISABLED.S')))
                    os.rename(loc, loc.replace('/S', '/DISABLED.S'))
                    return True
                except OSError:
                    self.logger.error("Failed to rename init script %s" % loc)
                    return False
            else:
                return self.cmd.run("/usr/sbin/svcadm disable %s" %
                                    entry.get('FMRI')).success
        elif entry.get('FMRI').startswith('lrc'):
            loc = entry.get("FMRI")[4:].replace('_', '.')
            try:
                os.stat(loc.replace('/S', '/Disabled.'))
                self.logger.debug("Renaming file %s to %s" %
                                  (loc.replace('/S', '/DISABLED.S'), loc))
                os.rename(loc.replace('/S', '/DISABLED.S'), loc)
                return True
            except OSError:
                self.logger.debug("Failed to rename %s to %s" %
                                  (loc.replace('/S', '/DISABLED.S'), loc))
                return False
        else:
            srvdata = \
                self.cmd.run("/usr/bin/svcs -H -o STA %s" %
                             entry.get('FMRI'))[1].splitlines()[0].split()
            if srvdata[0] == 'MNT':
                cmdarg = 'clear'
            else:
                cmdarg = 'enable'
            return self.cmd.run("/usr/sbin/svcadm %s -r %s" %
                                (cmdarg, entry.get('FMRI'))).success

    def Remove(self, svcs):
        """Remove Extra SMF entries."""
        # Extra service entry removal is nonsensical
        # Extra service entries should be reflected in config, even if disabled
        pass

    def FindExtra(self):
        """Find Extra SMF Services."""
        allsrv = []
        for srvc in self.cmd.run(["/usr/bin/svcs", "-a", "-H",
                                  "-o", "FMRI,STATE"]).stdout.splitlines():
            name, version = srvc.split()
            if version != 'disabled':
                allsrv.append(name)

        for svc in self.getSupportedEntries():
            if svc.get("FMRI") in allsrv:
                allsrv.remove(svc.get('FMRI'))
        return [Bcfg2.Client.XML.Element("Service", type='smf', name=name)
                for name in allsrv]

########NEW FILE########
__FILENAME__ = Systemd
# This is the bcfg2 support for systemd

"""This is systemd support."""

import Bcfg2.Client.Tools
import Bcfg2.Client.XML


class Systemd(Bcfg2.Client.Tools.SvcTool):
    """Systemd support for Bcfg2."""
    name = 'Systemd'
    __execs__ = ['/bin/systemctl']
    __handles__ = [('Service', 'systemd')]
    __req__ = {'Service': ['name', 'status']}

    def get_svc_command(self, service, action):
        return "/bin/systemctl %s %s.service" % (action, service.get('name'))

    def VerifyService(self, entry, _):
        """Verify Service status for entry."""
        if entry.get('status') == 'ignore':
            return True

        cmd = "/bin/systemctl status %s.service " % (entry.get('name'))
        rv = self.cmd.run(cmd)

        if 'Loaded: error' in rv.stdout:
            entry.set('current_status', 'off')
            return False
        elif 'Active: active' in rv.stdout:
            entry.set('current_status', 'on')
            return entry.get('status') == 'on'
        else:
            entry.set('current_status', 'off')
            return entry.get('status') == 'off'

    def InstallService(self, entry):
        """Install Service entry."""
        if entry.get('status') == 'on':
            rv = self.cmd.run(self.get_svc_command(entry, 'enable')).success
            rv &= self.cmd.run(self.get_svc_command(entry, 'start')).success
        else:
            rv = self.cmd.run(self.get_svc_command(entry, 'stop')).success
            rv &= self.cmd.run(self.get_svc_command(entry, 'disable')).success

        return rv

########NEW FILE########
__FILENAME__ = SYSV
"""This provides bcfg2 support for Solaris SYSV packages."""

import tempfile
from Bcfg2.Compat import any  # pylint: disable=W0622
import Bcfg2.Client.Tools
import Bcfg2.Client.XML

# pylint: disable=C0103
noask = '''
mail=
instance=overwrite
partial=nocheck
runlevel=nocheck
idepend=nocheck
rdepend=nocheck
space=ask
setuid=nocheck
conflict=nocheck
action=nocheck
basedir=default
'''
# pylint: enable=C0103


class SYSV(Bcfg2.Client.Tools.PkgTool):
    """Solaris SYSV package support."""
    __execs__ = ["/usr/sbin/pkgadd", "/usr/bin/pkginfo"]
    __handles__ = [('Package', 'sysv')]
    __req__ = {'Package': ['name', 'version']}
    __ireq__ = {'Package': ['name', 'url', 'version']}
    name = 'SYSV'
    pkgtype = 'sysv'
    pkgtool = ("/usr/sbin/pkgadd %s -n -d %%s", (('%s %s', ['url', 'name'])))

    def __init__(self, logger, setup, config):
        Bcfg2.Client.Tools.PkgTool.__init__(self, logger, setup, config)
        # noaskfile needs to live beyond __init__ otherwise file is removed
        self.noaskfile = tempfile.NamedTemporaryFile()
        self.noaskname = self.noaskfile.name
        try:
            self.noaskfile.write(noask)
            # flush admin file contents to disk
            self.noaskfile.flush()
            self.pkgtool = (self.pkgtool[0] % ("-a %s" % (self.noaskname)),
                            self.pkgtool[1])
        except:  # pylint: disable=W0702
            self.pkgtool = (self.pkgtool[0] % "", self.pkgtool[1])

    def RefreshPackages(self):
        """Refresh memory hashes of packages."""
        self.installed = {}
        # Build list of packages
        lines = self.cmd.run("/usr/bin/pkginfo -x").stdout.splitlines()
        while lines:
            # Splitting on whitespace means that packages with spaces in
            # their version numbers don't work right.  Found this with
            # IBM TSM software with package versions like
            #           "Version 6 Release 1 Level 0.0"
            # Should probably be done with a regex but this works.
            version = lines.pop().split(') ')[1]
            pkg = lines.pop().split()[0]
            self.installed[pkg] = version

    def VerifyPackage(self, entry, modlist):
        """Verify Package status for entry."""
        desired_version = entry.get('version')
        if desired_version == 'any':
            desired_version = self.installed.get(entry.get('name'),
                                                 desired_version)

        if not self.cmd.run(["/usr/bin/pkginfo", "-q", "-v",
                             desired_version, entry.get('name')]):
            if entry.get('name') in self.installed:
                self.logger.debug("Package %s version incorrect: "
                                  "have %s want %s" %
                                  (entry.get('name'),
                                   self.installed[entry.get('name')],
                                   desired_version))
            else:
                self.logger.debug("Package %s not installed" %
                                  entry.get("name"))
        else:
            if (self.setup['quick'] or
                entry.attrib.get('verify', 'true') == 'false'):
                return True
            rv = self.cmd.run("/usr/sbin/pkgchk -n %s" % entry.get('name'))
            if rv.success:
                return True
            else:
                output = [line for line in rv.stdout.splitlines()
                          if line[:5] == 'ERROR']
                if any(name for name in output
                       if name.split()[-1] not in modlist):
                    self.logger.debug("Package %s content verification failed"
                                      % entry.get('name'))
                else:
                    return True
        return False

    def Remove(self, packages):
        """Remove specified Sysv packages."""
        names = [pkg.get('name') for pkg in packages]
        self.logger.info("Removing packages: %s" % (names))
        self.cmd.run("/usr/sbin/pkgrm -a %s -n %s" %
                     (self.noaskname, names))
        self.RefreshPackages()
        self.extra = self.FindExtra()

########NEW FILE########
__FILENAME__ = Upstart
"""Upstart support for Bcfg2."""

import glob
import re

import Bcfg2.Client.Tools
import Bcfg2.Client.XML


class Upstart(Bcfg2.Client.Tools.SvcTool):
    """Upstart service support for Bcfg2."""
    name = 'Upstart'
    __execs__ = ['/lib/init/upstart-job',
                 '/sbin/initctl',
                 '/usr/sbin/service']
    __handles__ = [('Service', 'upstart')]
    __req__ = {'Service': ['name', 'status']}
    svcre = re.compile("/etc/init/(?P<name>.*).conf")

    def get_svc_command(self, service, action):
        return "/usr/sbin/service %s %s" % (service.get('name'), action)

    def VerifyService(self, entry, _):
        """Verify Service status for entry

           Verifying whether or not the service is enabled can be done
           at the file level with upstart using the contents of
           /etc/init/servicename.conf. All we need to do is make sure
           the service is running when it should be.
        """

        if entry.get('status') == 'ignore':
            return True

        if entry.get('parameters'):
            params = entry.get('parameters')
        else:
            params = ''

        try:
            output = self.cmd.run('/usr/sbin/service %s status %s' %
                                  (entry.get('name'),
                                   params)).stdout.splitlines()[0]
        except IndexError:
            self.logger.error("Service %s not an Upstart service" %
                              entry.get('name'))
            return False

        match = re.compile(r'%s( \(.*\))? (start|stop)/(running|waiting)' %
                           entry.get('name')).match(output)
        if match is None:
            # service does not exist
            entry.set('current_status', 'off')
            status = False
        elif match.group(3) == 'running':
            # service is running
            entry.set('current_status', 'on')
            if entry.get('status') == 'off':
                status = False
            else:
                status = True
        else:
            # service is not running
            entry.set('current_status', 'off')
            if entry.get('status') == 'on':
                status = False
            else:
                status = True

        return status

    def InstallService(self, entry):
        """Install Service for entry."""
        if entry.get('status') == 'on':
            cmd = "start"
        elif entry.get('status') == 'off':
            cmd = "stop"
        return self.cmd.run(self.get_svc_command(entry, cmd)).success

    def FindExtra(self):
        """Locate extra Upstart services."""
        specified = [entry.get('name') for entry in self.getSupportedEntries()]
        extra = []
        for fname in glob.glob("/etc/init/*.conf"):
            if self.svcre.match(fname).group('name') not in specified:
                extra.append(self.svcre.match(fname).group('name'))
        return [Bcfg2.Client.XML.Element('Service', type='upstart', name=name)
                for name in extra]

########NEW FILE########
__FILENAME__ = VCS
"""VCS support."""

# TODO:
#   * add svn support
#   * integrate properly with reports
missing = []

import errno
import os
import shutil
import sys
import stat

# python-dulwich git imports
try:
    import dulwich
    import dulwich.index
    from dulwich.errors import NotGitRepository
except ImportError:
    missing.append('git')
# subversion import
try:
    import pysvn
except ImportError:
    missing.append('svn')

import Bcfg2.Client.Tools


def cleanup_mode(mode):
    """Cleanup a mode value.

    This will return a mode that can be stored in a tree object.

    :param mode: Mode to clean up.
    """
    if stat.S_ISLNK(mode):
        return stat.S_IFLNK
    elif stat.S_ISDIR(mode):
        return stat.S_IFDIR
    elif dulwich.index.S_ISGITLINK(mode):
        return dulwich.index.S_IFGITLINK
    ret = stat.S_IFREG | int('644', 8)
    ret |= (mode & int('111', 8))
    return ret


def index_entry_from_stat(stat_val, hex_sha, flags, mode=None):
    """Create a new index entry from a stat value.

    :param stat_val: POSIX stat_result instance
    :param hex_sha: Hex sha of the object
    :param flags: Index flags
    """
    if mode is None:
        mode = cleanup_mode(stat_val.st_mode)
    return (stat_val.st_ctime, stat_val.st_mtime, stat_val.st_dev,
            stat_val.st_ino, mode, stat_val.st_uid,
            stat_val.st_gid, stat_val.st_size, hex_sha, flags)


class VCS(Bcfg2.Client.Tools.Tool):
    """VCS support."""
    __handles__ = [('Path', 'vcs')]
    __req__ = {'Path': ['name',
                        'type',
                        'vcstype',
                        'sourceurl',
                        'revision']}

    def git_write_index(self, entry):
        """Write the git index"""
        pass

    def Verifygit(self, entry, _):
        """Verify git repositories"""
        try:
            repo = dulwich.repo.Repo(entry.get('name'))
        except NotGitRepository:
            self.logger.info("Repository %s does not exist" %
                             entry.get('name'))
            return False

        try:
            expected_rev = entry.get('revision')
            cur_rev = repo.head()
        except:
            return False

        try:
            client, path = dulwich.client.get_transport_and_path(entry.get('sourceurl'))
            remote_refs = client.fetch_pack(path, (lambda x: None), None, None, None)
            if expected_rev in remote_refs:
                expected_rev = remote_refs[expected_rev]
        except:
            pass

        if cur_rev != expected_rev:
            self.logger.info("At revision %s need to go to revision %s" %
                             (cur_rev.strip(), expected_rev.strip()))
            return False

        return True

    def Installgit(self, entry):
        """Checkout contents from a git repository"""
        destname = entry.get('name')
        if os.path.lexists(destname):
            # remove incorrect contents
            try:
                if os.path.isdir(destname):
                    shutil.rmtree(destname)
                else:
                    os.remove(destname)
            except OSError:
                self.logger.info('Failed to remove %s' %
                                 destname)
                return False

        dulwich.file.ensure_dir_exists(destname)
        destr = dulwich.repo.Repo.init(destname)
        cl, host_path = dulwich.client.get_transport_and_path(entry.get('sourceurl'))
        remote_refs = cl.fetch(host_path,
                               destr,
                               determine_wants=destr.object_store.determine_wants_all,
                               progress=sys.stdout.write)

        if entry.get('revision') in remote_refs:
            destr.refs['HEAD'] = remote_refs[entry.get('revision')]
        else:
            destr.refs['HEAD'] = entry.get('revision')

        dtree = destr['HEAD'].tree
        index = dulwich.index.Index(destr.index_path())
        for fname, mode, sha in destr.object_store.iter_tree_contents(dtree):
            full_path = os.path.join(destname, fname)
            dulwich.file.ensure_dir_exists(os.path.dirname(full_path))

            if stat.S_ISLNK(mode):
                src_path = destr[sha].as_raw_string()
                try:
                    os.symlink(src_path, full_path)
                except OSError:
                    e = sys.exc_info()[1]
                    if e.errno == errno.EEXIST:
                        os.unlink(full_path)
                        os.symlink(src_path, full_path)
                    else:
                        raise
            else:
                file = open(full_path, 'wb')
                file.write(destr[sha].as_raw_string())
                file.close()
                os.chmod(full_path, mode)

            st = os.lstat(full_path)
            index[fname] = index_entry_from_stat(st, sha, 0)

        index.write()
        return True

    def Verifysvn(self, entry, _):
        """Verify svn repositories"""
        # pylint: disable=E1101
        headrev = pysvn.Revision(pysvn.opt_revision_kind.head)
        # pylint: enable=E1101
        client = pysvn.Client()
        try:
            cur_rev = str(client.info(entry.get('name')).revision.number)
            server = client.info2(entry.get('sourceurl'), headrev,
                                  recurse=False)
            if server:
                server_rev = str(server[0][1].rev.number)
        except:
            self.logger.info("Repository %s does not exist" %
                             entry.get('name'))
            return False

        if entry.get('revision') == 'latest' and cur_rev == server_rev:
            return True

        if cur_rev != entry.get('revision'):
            self.logger.info("At revision %s need to go to revision %s" %
                             (cur_rev, entry.get('revision')))
            return False

        return True

    def Installsvn(self, entry):
        """Checkout contents from a svn repository"""
        # pylint: disable=E1101
        client = pysvn.Client()
        try:
            client.update(entry.get('name'), recurse=True)
        except pysvn.ClientError:
            self.logger.error("Failed to update repository", exc_info=1)
            return False
        return True
        # pylint: enable=E1101

    def VerifyPath(self, entry, _):
        vcs = entry.get('vcstype')
        if vcs in missing:
            self.logger.error("Missing %s python libraries. Cannot verify" %
                              vcs)
            return False
        ret = getattr(self, 'Verify%s' % vcs)
        return ret(entry, _)

    def InstallPath(self, entry):
        vcs = entry.get('vcstype')
        if vcs in missing:
            self.logger.error("Missing %s python libraries. "
                              "Unable to install" % vcs)
            return False
        ret = getattr(self, 'Install%s' % vcs)
        return ret(entry)

########NEW FILE########
__FILENAME__ = YUM
"""This provides bcfg2 support for yum."""

import copy
import os.path
import sys
import yum
import yum.packages
import yum.rpmtrans
import yum.callbacks
import yum.Errors
import yum.misc
import rpmUtils.arch
import Bcfg2.Client.XML
import Bcfg2.Client.Tools


def build_yname(pkgname, inst):
    """Build yum appropriate package name."""
    rv = {}
    if isinstance(inst, yum.packages.PackageObject):
        for i in ['name', 'epoch', 'version', 'release', 'arch']:
            rv[i] = getattr(inst, i)
    else:
        rv['name'] = pkgname
        if inst.get('version') != 'any':
            rv['version'] = inst.get('version')
        if inst.get('epoch', False):
            rv['epoch'] = inst.get('epoch')
        if inst.get('release', False) and inst.get('release') != 'any':
            rv['release'] = inst.get('release')
        if inst.get('arch', False) and inst.get('arch') != 'any':
            rv['arch'] = inst.get('arch')
    return rv


def short_yname(nevra):
    """ given a nevra dict, get a dict of options to pass to functions
    like yum.YumBase.rpmdb.searchNevra(), which expect short names
    (e.g., "rel" instead of "release") """
    rv = nevra.copy()
    if 'version' in rv:
        rv['ver'] = rv['version']
        del rv['version']
    if 'release' in rv:
        rv['rel'] = rv['release']
        del rv['release']
    return rv


def nevra2string(pkg):
    """ convert a yum package object or nevra dict to a friendly
    human-readable string """
    if isinstance(pkg, yum.packages.PackageObject):
        return str(pkg)
    else:
        ret = []
        for attr, fmt in [('epoch', '%s:'), ('name', '%s'), ('version', '-%s'),
                          ('release', '-%s'), ('arch', '.%s')]:
            if attr in pkg:
                ret.append(fmt % pkg[attr])
        return "".join(ret)


class RPMDisplay(yum.rpmtrans.RPMBaseCallback):
    """We subclass the default RPM transaction callback so that we
       can control Yum's verbosity and pipe it through the right logger."""

    def __init__(self, logger):
        yum.rpmtrans.RPMBaseCallback.__init__(self)
        # we want to log events to *both* the Bcfg2 logger (which goes
        # to stderr or syslog or wherever the user wants it to go)
        # *and* the yum file logger, which will go to yum.log (ticket
        # #1103)
        self.bcfg2_logger = logger
        self.state = None
        self.package = None

    def event(self, package, action, te_current, te_total,
              ts_current, ts_total):
        """
        @param package: A yum package object or simple string of a package name
        @param action: A yum.constant transaction set state or in the obscure
                       rpm repackage case it could be the string 'repackaging'
        @param te_current: Current number of bytes processed in the transaction
                           element being processed
        @param te_total: Total number of bytes in the transaction element being
                         processed
        @param ts_current: number of processes completed in whole transaction
        @param ts_total: total number of processes in the transaction.
        """

        if self.package != str(package) or action != self.state:
            self.bcfg2_logger.info("%s: %s" % (self.action[action], package))
            self.state = action
            self.package = str(package)

    def scriptout(self, package, msgs):
        """Handle output from package scripts."""

        if msgs:
            msg = "%s: %s" % (package, msgs)
            self.bcfg2_logger.debug(msg)

    def errorlog(self, msg):
        """Deal with error reporting."""
        self.bcfg2_logger.error(msg)


class YumDisplay(yum.callbacks.ProcessTransBaseCallback):
    """Class to handle display of what step we are in the Yum transaction
       such as downloading packages, etc."""

    def __init__(self, logger):
        yum.callbacks.ProcessTransBaseCallback.__init__(self)
        self.logger = logger


class YUM(Bcfg2.Client.Tools.PkgTool):
    """Support for Yum packages."""
    pkgtype = 'yum'
    __execs__ = []
    __handles__ = [('Package', 'yum'),
                   ('Package', 'rpm'),
                   ('Path', 'ignore')]

    __req__ = {'Package': ['type'],
               'Path': ['type']}

    conflicts = ['YUM24', 'RPM', 'RPMng', 'YUMng']

    def __init__(self, logger, setup, config):
        self.yumbase = self._loadYumBase(setup=setup, logger=logger)
        Bcfg2.Client.Tools.PkgTool.__init__(self, logger, setup, config)
        self.ignores = []
        for struct in config:
            self.ignores.extend([entry.get('name')
                                 for entry in struct
                                 if (entry.tag == 'Path' and
                                     entry.get('type') == 'ignore')])
        self.instance_status = {}
        self.extra_instances = []
        self.modlists = {}
        for struct in config:
            self.__important__.extend(
                [entry.get('name')
                 for entry in struct
                 if (entry.tag == 'Path' and
                     (entry.get('name').startswith('/etc/yum.d') or
                      entry.get('name').startswith('/etc/yum.repos.d')) or
                     entry.get('name') == '/etc/yum.conf')])
        self.yum_avail = dict()
        self.yum_installed = dict()
        self.verify_cache = dict()

        yup = self.yumbase.doPackageLists(pkgnarrow='updates')
        if hasattr(self.yumbase.rpmdb, 'pkglist'):
            yinst = self.yumbase.rpmdb.pkglist
        else:
            yinst = self.yumbase.rpmdb.getPkgList()
        for dest, source in [(self.yum_avail, yup.updates),
                             (self.yum_installed, yinst)]:
            for pkg in source:
                if dest is self.yum_avail:
                    pname = pkg.name
                    data = [(pkg.arch, (pkg.epoch, pkg.version, pkg.release))]
                else:
                    pname = pkg[0]
                    data = [(pkg[1], (pkg[2], pkg[3], pkg[4]))]
                if pname in dest:
                    dest[pname].update(data)
                else:
                    dest[pname] = dict(data)

        # Process the Yum section from the config file.  These are all
        # boolean flags, either we do stuff or we don't
        self.pkg_checks = self.setup["yum_pkg_checks"]
        self.pkg_verify = self.setup["yum_pkg_verify"]
        self.do_install = self.setup["yum_installed_action"] == "install"
        self.do_upgrade = self.setup["yum_version_fail_action"] == "upgrade"
        self.do_reinst = self.setup["yum_verify_fail_action"] == "reinstall"
        self.verify_flags = self.setup["yum_verify_flags"]

        self.installonlypkgs = self.yumbase.conf.installonlypkgs
        if 'gpg-pubkey' not in self.installonlypkgs:
            self.installonlypkgs.append('gpg-pubkey')

        self.logger.debug("Yum: Install missing: %s" % self.do_install)
        self.logger.debug("Yum: pkg_checks: %s" % self.pkg_checks)
        self.logger.debug("Yum: pkg_verify: %s" % self.pkg_verify)
        self.logger.debug("Yum: Upgrade on version fail: %s" % self.do_upgrade)
        self.logger.debug("Yum: Reinstall on verify fail: %s" % self.do_reinst)
        self.logger.debug("Yum: installonlypkgs: %s" % self.installonlypkgs)
        self.logger.debug("Yum: verify_flags: %s" % self.verify_flags)

    def _loadYumBase(self, setup=None, logger=None):
        ''' this may be called before PkgTool.__init__() is called on
        this object (when the YUM object is first instantiated;
        PkgTool.__init__() calls RefreshPackages(), which requires a
        YumBase object already exist), or after __init__() has
        completed, when we reload the yum config before installing
        packages. Consequently, we support both methods by allowing
        setup and logger, the only object properties we use in this
        function, to be passed as keyword arguments or to be omitted
        and drawn from the object itself.'''
        rv = yum.YumBase()  # pylint: disable=C0103

        if setup is None:
            setup = self.setup
        if logger is None:
            logger = self.logger

        if setup['debug']:
            debuglevel = 3
        elif setup['verbose']:
            debuglevel = 2
        else:
            debuglevel = 0

        # pylint: disable=E1121,W0212
        try:
            rv.preconf.debuglevel = debuglevel
            rv._getConfig()
        except AttributeError:
            rv._getConfig(self.yumbase.conf.config_file_path,
                          debuglevel=debuglevel)
        # pylint: enable=E1121,W0212

        try:
            rv.doConfigSetup()
            rv.doTsSetup()
            rv.doRpmDBSetup()
        except yum.Errors.RepoError:
            logger.error("YUM Repository error: %s" % sys.exc_info()[1])
            raise Bcfg2.Client.Tools.ToolInstantiationError
        except Exception:
            logger.error("Yum error: %s" % sys.exc_info()[1])
            raise Bcfg2.Client.Tools.ToolInstantiationError
        return rv

    def _fixAutoVersion(self, entry):
        """ handle entries with version="auto" by setting the version
        to the newest available """
        # old style entry; synthesize Instances from current installed
        if (entry.get('name') not in self.yum_installed and
            entry.get('name') not in self.yum_avail):
            # new entry; fall back to default
            entry.set('version', 'any')
        else:
            data = copy.copy(self.yum_installed[entry.get('name')])
            if entry.get('name') in self.yum_avail:
                # installed but out of date
                data.update(self.yum_avail[entry.get('name')])
            for (arch, (epoch, vers, rel)) in list(data.items()):
                inst = Bcfg2.Client.XML.SubElement(entry, "Instance",
                                                   name=entry.get('name'),
                                                   version=vers, arch=arch,
                                                   release=rel, epoch=epoch)
                if 'verify_flags' in entry.attrib:
                    inst.set('verify_flags', entry.get('verify_flags'))
                if 'verify' in entry.attrib:
                    inst.set('verify', entry.get('verify'))

    def _buildInstances(self, entry):
        """ get a list of all instances of the package from the given
        entry.  converts from a Package entry without any Instance
        tags as necessary """
        instances = [inst for inst in entry
                     if inst.tag == 'Instance' or inst.tag == 'Package']

        # Uniquify instances.  Cases where duplicates are returned.
        # However, the elements aren't comparable.

        if instances == []:
            # We have an old style no Instance entry. Convert it to new style.
            instance = Bcfg2.Client.XML.SubElement(entry, 'Package')
            for attrib in list(entry.attrib.keys()):
                instance.attrib[attrib] = entry.attrib[attrib]
            instances = [instance]

        return instances

    def _getGPGKeysAsPackages(self):
        """Return a list of the GPG RPM signing keys installed on the
           system as a list of Package Objects."""

        # GPG keys existing in the RPMDB have numbered days
        # and newer Yum versions will not return information about them
        if hasattr(self.yumbase.rpmdb, 'returnGPGPubkeyPackages'):
            return self.yumbase.rpmdb.returnGPGPubkeyPackages()
        return self.yumbase.rpmdb.searchNevra(name='gpg-pubkey')

    def missing_attrs(self, entry):
        """ Implementing from superclass to check for existence of either
        name or group attribute for Package entry in the case of a YUM
        group. """
        missing = Bcfg2.Client.Tools.PkgTool.missing_attrs(self, entry)

        if (entry.get('name', None) is None and
            entry.get('group', None) is None):
            missing += ['name', 'group']
        return missing

    def _verifyHelper(self, pkg_obj):
        """ _verifyHelper primarly deals with a yum bug where the
        pkg_obj.verify() method does not properly take into count multilib
        sharing of files.  Neither does RPM proper, really....it just
        ignores the problem. """
        def verify(pkg):
            """ helper to perform the verify according to the best
            options for whatever version of the API we're
            using. Disabling file checksums is a new feature yum
            3.2.17-ish """
            try:
                return pkg.verify(fast=self.setup.get('quick', False))
            except TypeError:
                # Older Yum API
                return pkg.verify()

        key = (pkg_obj.name, pkg_obj.epoch, pkg_obj.version, pkg_obj.release,
               pkg_obj.arch)
        if key in self.verify_cache:
            results = self.verify_cache[key]
        else:
            results = verify(pkg_obj)
            self.verify_cache[key] = results
        if not rpmUtils.arch.isMultiLibArch():
            return results

        # Okay deal with a buggy yum multilib and verify. first find
        # all arches of pkg
        packages = self.yumbase.rpmdb.searchNevra(name=pkg_obj.name,
                                                  epoch=pkg_obj.epoch,
                                                  ver=pkg_obj.version,
                                                  rel=pkg_obj.release)
        if len(packages) == 1:
            return results  # No mathcing multilib packages

        # Will be the list of common fnames
        files = set(pkg_obj.returnFileEntries())
        common = {}
        for pkg in packages:
            if pkg != pkg_obj:
                files = files & set(pkg.returnFileEntries())
        for pkg in packages:
            key = (pkg.name, pkg.epoch, pkg.version, pkg.release, pkg.arch)
            self.logger.debug("Multilib Verify: comparing %s to %s" %
                              (pkg_obj, pkg))
            if key not in self.verify_cache:
                self.verify_cache[key] = verify(pkg)
            for fname in list(self.verify_cache[key].keys()):
                # file problems must exist in ALL multilib packages to be real
                if fname in files:
                    common[fname] = common.get(fname, 0) + 1

        flag = len(packages) - 1
        for fname, i in list(common.items()):
            if i == flag:
                # this fname had verify problems in all but one of the multilib
                # packages.  That means its correct in the package that's
                # "on top."  Therefore, this is a fake verify problem.
                if fname in results:
                    del results[fname]

        return results

    def RefreshPackages(self):
        """
            Creates self.installed{} which is a dict of installed packages.

            The dict items are lists of nevra dicts.  This loosely matches the
            config from the server and what rpmtools uses to specify pacakges.

            e.g.

            self.installed['foo'] = [ {'name':'foo', 'epoch':None,
                                       'version':'1', 'release':2,
                                       'arch':'i386'},
                                      {'name':'foo', 'epoch':None,
                                       'version':'1', 'release':2,
                                       'arch':'x86_64'} ]
        """

        self.installed = {}
        packages = self._getGPGKeysAsPackages() + \
            self.yumbase.rpmdb.returnPackages()
        for pkg in packages:
            pattrs = {}
            for i in ['name', 'epoch', 'version', 'release', 'arch']:
                if i == 'arch' and getattr(pkg, i) is None:
                    pattrs[i] = 'noarch'
                elif i == 'epoch' and getattr(pkg, i) is None:
                    pattrs[i] = '0'
                else:
                    pattrs[i] = getattr(pkg, i)
            self.installed.setdefault(pkg.name, []).append(pattrs)

    # pylint: disable=R0914,R0912,R0915
    def VerifyPackage(self, entry, modlist):
        """ Verify Package status for entry.
        Performs the following:
        * Checks for the presence of required Package Instances.
        * Compares the evra 'version' info against self.installed{}.
        * RPM level package verify (rpm --verify).
        * Checks for the presence of unrequired package instances.

        Produces the following dict and list for Yum.Install() to use:
        * For installs/upgrades/fixes of required instances::
            instance_status = { <Instance Element Object>:
                                       { 'installed': True|False,
                                         'version_fail': True|False,
                                         'verify_fail': True|False,
                                         'pkg': <Package Element Object>,
                                         'modlist': [ <filename>, ... ],
                                         'verify' : [ <rpm --verify results> ]
                                       }, ......
                                  }

        * For deletions of unrequired instances::
            extra_instances = [ <Package Element Object>, ..... ]

        Constructs the text prompts for interactive mode. """
        if entry.get('version', False) == 'auto':
            self._fixAutoVersion(entry)

        if entry.get('group'):
            self.logger.debug("Verifying packages for group %s" %
                              entry.get('group'))
        else:
            self.logger.debug("Verifying package instances for %s" %
                              entry.get('name'))

        self.verify_cache = dict()  # Used for checking multilib packages
        self.modlists[entry] = modlist
        instances = self._buildInstances(entry)
        pkg_cache = []
        package_fail = False
        qtext_versions = []
        virt_pkg = False
        pkg_checks = (self.pkg_checks and
                      entry.get('pkg_checks', 'true').lower() == 'true')
        pkg_verify = (self.pkg_verify and
                      entry.get('pkg_verify', 'true').lower() == 'true')
        yum_group = False

        if entry.get('name') == 'gpg-pubkey':
            all_pkg_objs = self._getGPGKeysAsPackages()
            pkg_verify = False  # No files here to verify
        elif entry.get('group'):
            entry.set('name', 'group:%s' % entry.get('group'))
            yum_group = True
            all_pkg_objs = []
            instances = []
            if self.yumbase.comps.has_group(entry.get('group')):
                group = self.yumbase.comps.return_group(entry.get('group'))
                group_packages = [p
                                  for p, d in group.mandatory_packages.items()
                                  if d]
                group_type = entry.get('choose', 'default')
                if group_type in ['default', 'optional', 'all']:
                    group_packages += [
                        p for p, d in group.default_packages.items()
                        if d]
                if group_type in ['optional', 'all']:
                    group_packages += [
                        p for p, d in group.optional_packages.items()
                        if d]
                if len(group_packages) == 0:
                    self.logger.error("No packages found for group %s" %
                                      entry.get("group"))
                for pkg in group_packages:
                    # create package instances for each package in yum group
                    instance = Bcfg2.Client.XML.SubElement(entry, 'Package')
                    instance.attrib['name'] = pkg
                    instance.attrib['type'] = 'yum'
                    try:
                        newest = \
                            self.yumbase.pkgSack.returnNewestByName(pkg)[0]
                        instance.attrib['version'] = newest['version']
                        instance.attrib['epoch'] = newest['epoch']
                        instance.attrib['release'] = newest['release']
                    except:  # pylint: disable=W0702
                        self.logger.info("Error finding newest package "
                                         "for %s" %
                                         pkg)
                        instance.attrib['version'] = 'any'
                    instances.append(instance)
            else:
                self.logger.error("Group not found: %s" % entry.get("group"))
        else:
            all_pkg_objs = \
                self.yumbase.rpmdb.searchNevra(name=entry.get('name'))
        if len(all_pkg_objs) == 0 and yum_group is not True:
            # Some sort of virtual capability?  Try to resolve it
            all_pkg_objs = self.yumbase.rpmdb.searchProvides(entry.get('name'))
            if len(all_pkg_objs) > 0:
                virt_pkg = True
                self.logger.info("%s appears to be provided by:" %
                                 entry.get('name'))
                for pkg in all_pkg_objs:
                    self.logger.info("  %s" % pkg)

        for inst in instances:
            if yum_group:
                # the entry is not the name of the package
                nevra = build_yname(inst.get('name'), inst)
                all_pkg_objs = \
                    self.yumbase.rpmdb.searchNevra(name=inst.get('name'))
            else:
                nevra = build_yname(entry.get('name'), inst)
            if nevra in pkg_cache:
                continue  # Ignore duplicate instances
            else:
                pkg_cache.append(nevra)

            self.logger.debug("Verifying: %s" % nevra2string(nevra))

            # Set some defaults here
            stat = self.instance_status.setdefault(inst, {})
            stat['installed'] = True
            stat['version_fail'] = False
            stat['verify'] = {}
            stat['verify_fail'] = False
            if yum_group:
                stat['pkg'] = inst
            else:
                stat['pkg'] = entry
            stat['modlist'] = modlist
            if inst.get('verify_flags'):
                # this splits on either space or comma
                verify_flags = \
                    inst.get('verify_flags').lower().replace(' ',
                                                             ',').split(',')
            else:
                verify_flags = self.verify_flags

            if 'arch' in nevra:
                # If arch is specified use it to select the package
                pkg_objs = [p for p in all_pkg_objs if p.arch == nevra['arch']]
            else:
                pkg_objs = all_pkg_objs
            if len(pkg_objs) == 0:
                # Package (name, arch) not installed
                entry.set('current_exists', 'false')
                self.logger.debug("  %s is not installed" %
                                  nevra2string(nevra))
                stat['installed'] = False
                package_fail = True
                qtext_versions.append("I(%s)" % nevra)
                continue

            if not pkg_checks:
                continue

            # Check EVR
            if virt_pkg:
                # we need to make sure that the version of the symbol
                # provided matches the one required in the
                # configuration
                vlist = []
                for attr in ["epoch", "version", "release"]:
                    vlist.append(nevra.get(attr))
                if tuple(vlist) == (None, None, None):
                    # we just require the package name, no particular
                    # version, so just make a copy of all_pkg_objs since every
                    # package that provides this symbol satisfies the
                    # requirement
                    pkg_objs = [po for po in all_pkg_objs]
                else:
                    pkg_objs = [po for po in all_pkg_objs
                                if po.checkPrco('provides',
                                                (nevra["name"], 'EQ',
                                                 tuple(vlist)))]
            elif entry.get('name') == 'gpg-pubkey':
                if 'version' not in nevra:
                    self.logger.warning("Skipping verify: gpg-pubkey without "
                                        "an RPM version")
                    continue
                if 'release' not in nevra:
                    self.logger.warning("Skipping verify: gpg-pubkey without "
                                        "an RPM release")
                    continue
                pkg_objs = [p for p in all_pkg_objs
                            if (p.version == nevra['version']
                                and p.release == nevra['release'])]
            else:
                pkg_objs = self.yumbase.rpmdb.searchNevra(**short_yname(nevra))
            if len(pkg_objs) == 0:
                package_fail = True
                stat['version_fail'] = True
                # Just chose the first pkg for the error message
                current_pkg = all_pkg_objs[0]
                if virt_pkg:
                    provides = \
                        [p for p in current_pkg.provides
                         if p[0] == entry.get("name")][0]
                    current_evr = provides[2]
                    self.logger.info(
                        "  %s: Wrong version installed.  "
                        "Want %s, but %s provides %s" %
                        (entry.get("name"),
                         nevra2string(nevra),
                         nevra2string(current_pkg),
                         yum.misc.prco_tuple_to_string(provides)))
                else:
                    current_evr = (current_pkg.epoch,
                                   current_pkg.version,
                                   current_pkg.release)
                    self.logger.info("  %s: Wrong version installed.  "
                                     "Want %s, but have %s" %
                                     (entry.get("name"),
                                      nevra2string(nevra),
                                      nevra2string(current_pkg)))
                wanted_evr = (nevra.get('epoch', 'any'),
                              nevra.get('version', 'any'),
                              nevra.get('release', 'any'))
                entry.set('current_version', "%s:%s-%s" % current_evr)
                entry.set('version', "%s:%s-%s" % wanted_evr)
                if yum.compareEVR(current_evr, wanted_evr) == 1:
                    entry.set("package_fail_action", "downgrade")
                else:
                    entry.set("package_fail_action", "update")

                qtext_versions.append("U(%s)" % str(all_pkg_objs[0]))
                continue

            if self.setup.get('quick', False):
                # Passed -q on the command line
                continue
            if not (pkg_verify and
                    inst.get('pkg_verify', 'true').lower() == 'true'):
                continue

            # XXX: We ignore GPG sig checking the package as it
            # has nothing to do with the individual file hash/size/etc.
            # GPG checking the package only eaxmines some header/rpmdb
            # wacky-ness, and will not properly detect a compromised rpmdb.
            # Yum's verify routine does not support it for that reaosn.

            if len(pkg_objs) > 1:
                self.logger.debug("  Verify Instance found many packages:")
                for pkg in pkg_objs:
                    self.logger.debug("    %s" % str(pkg))

            try:
                vrfy_result = self._verifyHelper(pkg_objs[0])
            except:  # pylint: disable=W0702
                err = sys.exc_info()[1]
                # Unknown Yum exception
                self.logger.warning("  Verify Exception: %s" % err)
                package_fail = True
                continue

            # Now take out the Yum specific objects / modlists / unproblems
            ignores = [ig.get('name') for ig in entry.findall('Ignore')] + \
                [ig.get('name') for ig in inst.findall('Ignore')] + \
                self.ignores
            for fname, probs in list(vrfy_result.items()):
                if fname in modlist:
                    self.logger.debug("  %s in modlist, skipping" % fname)
                    continue
                if fname in ignores:
                    self.logger.debug("  %s in ignore list, skipping" % fname)
                    continue
                tmp = []
                for prob in probs:
                    if prob.type == 'missing' and os.path.islink(fname):
                        continue
                    elif 'no' + prob.type in verify_flags:
                        continue
                    if prob.type not in ['missingok', 'ghost']:
                        tmp.append((prob.type, prob.message))
                if tmp != []:
                    stat['verify'][fname] = tmp

            if stat['verify'] != {}:
                stat['verify_fail'] = True
                package_fail = True
                self.logger.info("It is suggested that you either manage "
                                 "these files, revert the changes, or ignore "
                                 "false failures:")
                self.logger.info("  Verify Problems: %s" %
                                 stat['pkg'].get('name'))
                for fname, probs in list(stat['verify'].items()):
                    if len(probs) > 1:
                        self.logger.info("    %s" % fname)
                        for prob in probs:
                            self.logger.info("      %s" % prob[1])
                    else:
                        self.logger.info("    %s: %s" % (fname, probs[0]))

        if len(all_pkg_objs) > 0:
            # Is this an install only package?  We just look at the first one
            provides = set([p[0] for p in all_pkg_objs[0].provides] +
                           [all_pkg_objs[0].name])
            install_only = len(set(self.installonlypkgs) & provides) > 0
        else:
            install_only = False

        if virt_pkg or \
           (install_only and not self.setup['kevlar']) or \
           yum_group:
            # virtual capability supplied, we are probably dealing
            # with multiple packages of different names.  This check
            # doesn't make a lot of since in this case.
            # install_only: Yum may clean some of these up itself.
            # Otherwise having multiple instances of install only packages
            # is considered correct
            self.extra_instances = None
        else:
            self.extra_instances = self.FindExtraInstances(entry, all_pkg_objs)
        if self.extra_instances is not None:
            package_fail = True

        return not package_fail
    # pylint: enable=R0914,R0912,R0915

    def FindExtraInstances(self, entry, all_pkg_objs):
        """ Check for installed instances that are not in the
        config. Return a Package Entry with Instances to remove, or
        None if there are no Instances to remove. """
        if len(all_pkg_objs) == 0:
            return None
        name = entry.get('name')
        extra_entry = Bcfg2.Client.XML.Element('Package', name=name,
                                               type=self.pkgtype)
        instances = self._buildInstances(entry)
        pkg_objs = [p for p in all_pkg_objs]  # Shallow copy

        # Algorythm is sensitive to duplicates, check for them
        checked = []
        for inst in instances:
            nevra = build_yname(name, inst)
            pkgs = self.yumbase.rpmdb.searchNevra(**short_yname(nevra))
            if len(pkgs) > 0:
                if pkgs[0] in checked:
                    continue  # We've already taken care of this Instance
                else:
                    checked.append(pkgs[0])
                pkg_objs.remove(pkgs[0])

        for pkg in pkg_objs:
            self.logger.debug("  Extra Instance Found: %s" % str(pkg))
            Bcfg2.Client.XML.SubElement(extra_entry, 'Instance',
                                        epoch=pkg.epoch, name=pkg.name,
                                        version=pkg.version,
                                        release=pkg.release, arch=pkg.arch)

        if pkg_objs == []:
            return None
        else:
            return extra_entry

    def FindExtra(self):
        """Find extra packages."""
        packages = [e.get('name') for e in self.getSupportedEntries()]
        extras = []

        for pkg in list(self.installed.keys()):
            if pkg not in packages:
                entry = Bcfg2.Client.XML.Element('Package', name=pkg,
                                                 type=self.pkgtype)
                for i in self.installed[pkg]:
                    Bcfg2.Client.XML.SubElement(entry, 'Instance',
                                                epoch=i['epoch'],
                                                version=i['version'],
                                                release=i['release'],
                                                arch=i['arch'])

                extras.append(entry)

        return extras

    def _installGPGKey(self, inst, key_file):
        """Examine the GPG keys carefully before installation.  Avoid
           installing duplicate keys.  Returns True on successful install."""

        # RPM Transaction Set
        tset = self.yumbase.rpmdb.readOnlyTS()

        if not os.path.exists(key_file):
            self.logger.debug("GPG Key file %s not installed" % key_file)
            return False

        rawkey = open(key_file).read()
        gpg = yum.misc.getgpgkeyinfo(rawkey)

        ver = yum.misc.keyIdToRPMVer(gpg['keyid'])
        rel = yum.misc.keyIdToRPMVer(gpg['timestamp'])
        if not (ver == inst.get('version') and rel == inst.get('release')):
            self.logger.info("GPG key file %s does not match gpg-pubkey-%s-%s"
                             % (key_file, inst.get('version'),
                                inst.get('release')))
            return False

        if not yum.misc.keyInstalled(tset, gpg['keyid'],
                                     gpg['timestamp']) == 0:
            result = tset.pgpImportPubkey(yum.misc.procgpgkey(rawkey))
        else:
            self.logger.debug("gpg-pubkey-%s-%s already installed" %
                              (inst.get('version'), inst.get('release')))
            return True

        if result != 0:
            self.logger.debug(
                "Unable to install %s-%s" %
                (self.instance_status[inst].get('pkg').get('name'),
                 nevra2string(inst)))
            return False
        else:
            self.logger.debug(
                "Installed %s-%s-%s" %
                (self.instance_status[inst].get('pkg').get('name'),
                 inst.get('version'), inst.get('release')))
            return True

    def _runYumTransaction(self):
        """ run the yum transaction that has already been set up """
        def cleanup():
            """ clean up open stuff when we hit an error """
            self.yumbase.closeRpmDB()
            self.RefreshPackages()

        rpm_display = RPMDisplay(self.logger)
        yum_display = YumDisplay(self.logger)
        # Run the Yum Transaction
        try:
            rescode, restring = self.yumbase.buildTransaction()
        except yum.Errors.YumBaseError:
            err = sys.exc_info()[1]
            self.logger.error("Error building Yum transaction: %s" % err)
            cleanup()
            return

        self.logger.debug("Initial Yum buildTransaction() run said:")
        self.logger.debug("   resultcode: %s, msgs: %s" %
                          (rescode, restring))

        if rescode != 1:
            # Transaction built successfully, run it
            try:
                self.yumbase.processTransaction(callback=yum_display,
                                                rpmDisplay=rpm_display)
                self.logger.info("Single Pass for Install Succeeded")
            except yum.Errors.YumBaseError:
                err = sys.exc_info()[1]
                self.logger.error("Error processing Yum transaction: %s" % err)
                cleanup()
                return
        else:
            # The yum command failed.  No packages installed.
            # Try installing instances individually.
            self.logger.error("Single Pass Install of Packages Failed")
            skip_broken = self.yumbase.conf.skip_broken
            self.yumbase.conf.skip_broken = True
            try:
                rescode, restring = self.yumbase.buildTransaction()
                if rescode != 1:
                    self.yumbase.processTransaction(callback=yum_display,
                                                    rpmDisplay=rpm_display)
                    self.logger.debug(
                        "Second pass install did not install all packages")
                else:
                    self.logger.error("Second pass yum install failed.")
                    self.logger.debug("   %s" % restring)
            except yum.Errors.YumBaseError:
                err = sys.exc_info()[1]
                self.logger.error("Error rerunning Yum transaction: %s" % err)

            self.yumbase.conf.skip_broken = skip_broken

        cleanup()

    def Install(self, packages, states):  # pylint: disable=R0912,R0914,R0915
        """ Try and fix everything that Yum.VerifyPackages() found
        wrong for each Package Entry.  This can result in individual
        RPMs being installed (for the first time), deleted, downgraded
        or upgraded.

        packages is a list of Package Elements that has
        states[<Package Element>] == False

        The following effects occur:
           - states{} is conditionally updated for each package.
           - self.installed{} is rebuilt, possibly multiple times.
           - self.instance_status{} is conditionally updated for each instance
             of a package.
           - Each package will be added to self.modified[] if its states{}
             entry is set to True. """
        self.logger.debug('Running Yum.Install()')

        install_pkgs = []
        gpg_keys = []
        upgrade_pkgs = []
        downgrade_pkgs = []
        reinstall_pkgs = []

        def queue_pkg(pkg, inst, queue):
            """ add a package to the appropriate work queue --
            packages to install, packages to upgrade, etc. """
            if pkg.get('name') == 'gpg-pubkey':
                gpg_keys.append(inst)
            else:
                queue.append(inst)

        # Remove extra instances.
        # Can not reverify because we don't have a package entry.
        if self.extra_instances is not None and len(self.extra_instances) > 0:
            if (self.setup.get('remove') == 'all' or
                self.setup.get('remove') == 'packages'):
                self.Remove(self.extra_instances)
            else:
                self.logger.info("The following extra package instances will "
                                 "be removed by the '-r' option:")
                for pkg in self.extra_instances:
                    for inst in pkg:
                        self.logger.info("    %s %s" %
                                         ((pkg.get('name'),
                                           nevra2string(inst))))

        # Figure out which instances of the packages actually need something
        # doing to them and place in the appropriate work 'queue'.
        for pkg in packages:
            insts = [pinst for pinst in pkg
                     if pinst.tag in ['Instance', 'Package']]
            if insts:
                for inst in insts:
                    if inst not in self.instance_status:
                        self.logger.warning(
                            "  Asked to install/update package never "
                            "verified: %s" %
                            nevra2string(build_yname(pkg.get('name'), inst)))
                        continue
                    status = self.instance_status[inst]
                    if not status.get('installed', False) and self.do_install:
                        queue_pkg(pkg, inst, install_pkgs)
                    elif status.get('version_fail', False) and self.do_upgrade:
                        if pkg.get("package_fail_action") == "downgrade":
                            queue_pkg(pkg, inst, downgrade_pkgs)
                        else:
                            queue_pkg(pkg, inst, upgrade_pkgs)
                    elif status.get('verify_fail', False) and self.do_reinst:
                        queue_pkg(pkg, inst, reinstall_pkgs)
                    else:
                        # Either there was no Install/Version/Verify
                        # task to be done or the user disabled the actions
                        # in the configuration.  XXX Logging for the latter?
                        pass
            else:
                msg = "Yum: Package tag found where Instance expected: %s"
                self.logger.warning(msg % pkg.get('name'))
                queue_pkg(pkg, pkg, install_pkgs)

        # Install GPG keys.
        # Alternatively specify the required keys using 'gpgkey' in the
        # repository definition in yum.conf.  YUM will install the keys
        # automatically.
        if len(gpg_keys) > 0:
            self.logger.info("Installing GPG keys.")
            for inst in gpg_keys:
                if inst.get('simplefile') is None:
                    self.logger.error("GPG key has no simplefile attribute")
                    continue
                key_file = os.path.join(
                    self.instance_status[inst].get('pkg').get('uri'),
                    inst.get('simplefile'))
                self._installGPGKey(inst, key_file)

            self.RefreshPackages()
            pkg = self.instance_status[gpg_keys[0]].get('pkg')
            states[pkg] = self.VerifyPackage(pkg, [])

        # We want to reload all Yum configuration in case we've
        # deployed new .repo files we should consider
        self._loadYumBase()

        # Install packages.
        if len(install_pkgs) > 0:
            self.logger.info("Attempting to install packages")

            for inst in install_pkgs:
                pkg_arg = self.instance_status[inst].get('pkg').get('name')
                self.logger.debug("Installing %s" % pkg_arg)
                try:
                    self.yumbase.install(**build_yname(pkg_arg, inst))
                except yum.Errors.YumBaseError:
                    yume = sys.exc_info()[1]
                    self.logger.error("Error installing package %s: %s" %
                                      (pkg_arg, yume))

        if len(upgrade_pkgs) > 0:
            self.logger.info("Attempting to upgrade packages")

            for inst in upgrade_pkgs:
                pkg_arg = self.instance_status[inst].get('pkg').get('name')
                self.logger.debug("Upgrading %s" % pkg_arg)
                try:
                    self.yumbase.update(**build_yname(pkg_arg, inst))
                except yum.Errors.YumBaseError:
                    yume = sys.exc_info()[1]
                    self.logger.error("Error upgrading package %s: %s" %
                                      (pkg_arg, yume))

        if len(downgrade_pkgs) > 0:
            self.logger.info("Attempting to downgrade packages")

            for inst in downgrade_pkgs:
                pkg_arg = self.instance_status[inst].get('pkg').get('name')
                self.logger.debug("Downgrading %s" % pkg_arg)
                try:
                    self.yumbase.downgrade(**build_yname(pkg_arg, inst))
                except yum.Errors.YumBaseError:
                    yume = sys.exc_info()[1]
                    self.logger.error("Error downgrading package %s: %s" %
                                      (pkg_arg, yume))

        if len(reinstall_pkgs) > 0:
            self.logger.info("Attempting to reinstall packages")
            for inst in reinstall_pkgs:
                pkg_arg = self.instance_status[inst].get('pkg').get('name')
                self.logger.debug("Reinstalling %s" % pkg_arg)
                try:
                    self.yumbase.reinstall(**build_yname(pkg_arg, inst))
                except yum.Errors.YumBaseError:
                    yume = sys.exc_info()[1]
                    self.logger.error("Error reinstalling package %s: %s" %
                                      (pkg_arg, yume))

        self._runYumTransaction()

        if not self.setup['kevlar']:
            for pkg_entry in [p for p in packages if self.canVerify(p)]:
                self.logger.debug("Reverifying Failed Package %s" %
                                  pkg_entry.get('name'))
                states[pkg_entry] = \
                    self.VerifyPackage(pkg_entry,
                                       self.modlists.get(pkg_entry, []))

        for entry in [ent for ent in packages if states[ent]]:
            self.modified.append(entry)

    def Remove(self, packages):
        """
           Remove specified entries.

           packages is a list of Package Entries with Instances generated
           by FindExtra().
        """
        self.logger.debug('Running Yum.Remove()')

        for pkg in packages:
            for inst in pkg:
                nevra = build_yname(pkg.get('name'), inst)
                if pkg.get('name') != 'gpg-pubkey':
                    self.yumbase.remove(**nevra)
                    self.modified.append(pkg)
                else:
                    self.logger.info("WARNING: gpg-pubkey package not in "
                                     "configuration %s %s-%s" %
                                     (nevra['name'], nevra['version'],
                                      nevra['release']))

        self._runYumTransaction()
        self.extra = self.FindExtra()

    def VerifyPath(self, entry, _):  # pylint: disable=W0613
        """Do nothing here since we only verify Path type=ignore"""
        return True

########NEW FILE########
__FILENAME__ = YUM24
"""This provides bcfg2 support for yum."""

import copy
import os.path
import sys
import yum
import Bcfg2.Client.XML
from Bcfg2.Client.Tools.RPM import RPM


def build_yname(pkgname, inst):
    """Build yum appropriate package name."""
    ypname = pkgname
    if inst.get('version') != 'any':
        ypname += '-'
    if inst.get('epoch', False):
        ypname += "%s:" % inst.get('epoch')
    if inst.get('version', False) and inst.get('version') != 'any':
        ypname += "%s" % (inst.get('version'))
    if inst.get('release', False) and inst.get('release') != 'any':
        ypname += "-%s" % (inst.get('release'))
    if inst.get('arch', False) and inst.get('arch') != 'any':
        ypname += ".%s" % (inst.get('arch'))
    return ypname


class YUM24(RPM):
    """Support for Yum packages."""
    pkgtype = 'yum'
    deprecated = True
    __execs__ = ['/usr/bin/yum', '/var/lib/rpm']
    __handles__ = [('Package', 'yum'),
                   ('Package', 'rpm'),
                   ('Path', 'ignore')]

    __req__ = {'Package': ['name', 'version']}
    __ireq__ = {'Package': ['name']}
    #__ireq__ = {'Package': ['name', 'version']}

    __new_req__ = {'Package': ['name'],
                   'Instance': ['version', 'release', 'arch']}
    __new_ireq__ = {'Package': ['name'], \
                    'Instance': []}
    #__new_ireq__ = {'Package': ['name', 'uri'], \
    #                'Instance': ['simplefile', 'version', 'release', 'arch']}

    __gpg_req__ = {'Package': ['name', 'version']}
    __gpg_ireq__ = {'Package': ['name', 'version']}

    __new_gpg_req__ = {'Package': ['name'],
                       'Instance': ['version', 'release']}
    __new_gpg_ireq__ = {'Package': ['name'],
                        'Instance': ['version', 'release']}

    def __init__(self, logger, setup, config):
        RPM.__init__(self, logger, setup, config)
        self.__important__ = self.__important__ + \
                             [entry.get('name') for struct in config \
                              for entry in struct \
                              if entry.tag in ['Path', 'ConfigFile'] and \
                              (entry.get('name').startswith('/etc/yum.d') \
                              or entry.get('name').startswith('/etc/yum.repos.d')) \
                              or entry.get('name') == '/etc/yum.conf']
        self.autodep = setup.get("yum24_autodep")
        self.yum_avail = dict()
        self.yum_installed = dict()
        self.yb = yum.YumBase()
        self.yb.doConfigSetup()
        self.yb.doTsSetup()
        self.yb.doRpmDBSetup()
        yup = self.yb.doPackageLists(pkgnarrow='updates')
        if hasattr(self.yb.rpmdb, 'pkglist'):
            yinst = self.yb.rpmdb.pkglist
        else:
            yinst = self.yb.rpmdb.getPkgList()
        for dest, source in [(self.yum_avail, yup.updates),
                             (self.yum_installed, yinst)]:
            for pkg in source:
                if dest is self.yum_avail:
                    pname = pkg.name
                    data = {pkg.arch: (pkg.epoch, pkg.version, pkg.release)}
                else:
                    pname = pkg[0]
                    if pkg[1] is None:
                        a = 'noarch'
                    else:
                        a = pkg[1]
                    if pkg[2] is None:
                        e = '0'
                    else:
                        e = pkg[2]
                    data = {a: (e, pkg[3], pkg[4])}
                if pname in dest:
                    dest[pname].update(data)
                else:
                    dest[pname] = dict(data)

    def VerifyPackage(self, entry, modlist):
        pinned_version = None
        if entry.get('version', False) == 'auto':
            # old style entry; synthesize Instances from current installed
            if entry.get('name') not in self.yum_installed and \
                   entry.get('name') not in self.yum_avail:
                # new entry; fall back to default
                entry.set('version', 'any')
            else:
                data = copy.copy(self.yum_installed[entry.get('name')])
                if entry.get('name') in self.yum_avail:
                    # installed but out of date
                    data.update(self.yum_avail[entry.get('name')])
                for (arch, (epoch, vers, rel)) in list(data.items()):
                    x = Bcfg2.Client.XML.SubElement(entry, "Instance",
                                                    name=entry.get('name'),
                                                    version=vers, arch=arch,
                                                    release=rel, epoch=epoch)
                    if 'verify_flags' in entry.attrib:
                        x.set('verify_flags', entry.get('verify_flags'))
                    if 'verify' in entry.attrib:
                        x.set('verify', entry.get('verify'))

        if entry.get('type', False) == 'yum':
            # Check for virtual provides or packages.  If we don't have
            # this package use Yum to resolve it to a real package name
            knownPkgs = list(self.yum_installed.keys()) + list(self.yum_avail.keys())
            if entry.get('name') not in knownPkgs:
                # If the package name matches something installed
                # or available the that's the correct package.
                try:
                    pkgDict = dict([(i.name, i) for i in \
                                   self.yb.returnPackagesByDep(entry.get('name'))])
                except yum.Errors.YumBaseError:
                    e = sys.exc_info()[1]
                    self.logger.error('Yum Error Depsolving for %s: %s' % \
                                      (entry.get('name'), str(e)))
                    pkgDict = {}

                if len(pkgDict) > 1:
                    # What do we do with multiple packages?
                    s = "YUM24: returnPackagesByDep(%s) returned many packages"
                    self.logger.info(s % entry.get('name'))
                    s = "YUM24: matching packages: %s"
                    self.logger.info(s % str(list(pkgDict.keys())))
                    pkgs = set(pkgDict.keys()) & set(self.yum_installed.keys())
                    if len(pkgs) > 0:
                        # Virtual packages matches an installed real package
                        pkg = pkgDict[pkgs.pop()]
                        s = "YUM24: chosing: %s" % pkg.name
                        self.logger.info(s)
                    else:
                        # What's the right package?  This will fail verify
                        # and Yum should Do The Right Thing on package install
                        pkg = None
                elif len(pkgDict) == 1:
                    pkg = list(pkgDict.values())[0]
                else:  # len(pkgDict) == 0
                    s = "YUM24: returnPackagesByDep(%s) returned no results"
                    self.logger.info(s % entry.get('name'))
                    pkg = None

                if pkg is not None:
                    s = "YUM24: remapping virtual package %s to %s"
                    self.logger.info(s % (entry.get('name'), pkg.name))
                    entry.set('name', pkg.name)

        return RPM.VerifyPackage(self, entry, modlist)

    def Install(self, packages, states):
        """
           Try and fix everything that YUM24.VerifyPackages() found wrong for
           each Package Entry.  This can result in individual RPMs being
           installed (for the first time), deleted, downgraded
           or upgraded.

           NOTE: YUM can not reinstall a package that it thinks is already
                 installed.

           packages is a list of Package Elements that has
               states[<Package Element>] == False

           The following effects occur:
           - states{} is conditionally updated for each package.
           - self.installed{} is rebuilt, possibly multiple times.
           - self.instance_status{} is conditionally updated for each instance
             of a package.
           - Each package will be added to self.modified[] if its states{}
             entry is set to True.

        """
        self.logger.info('Running YUM24.Install()')

        install_pkgs = []
        gpg_keys = []
        upgrade_pkgs = []

        # Remove extra instances.
        # Can not reverify because we don't have a package entry.
        if len(self.extra_instances) > 0:
            if (self.setup.get('remove') == 'all' or \
                self.setup.get('remove') == 'packages'):
                self.Remove(self.extra_instances)
            else:
                self.logger.info("The following extra package instances will be removed by the '-r' option:")
                for pkg in self.extra_instances:
                    for inst in pkg:
                        self.logger.info("    %s %s" % \
                                         ((pkg.get('name'), self.str_evra(inst))))

        # Figure out which instances of the packages actually need something
        # doing to them and place in the appropriate work 'queue'.
        for pkg in packages:
            insts = [pinst for pinst in pkg \
                     if pinst.tag in ['Instance', 'Package']]
            if insts:
                for inst in insts:
                    if self.FixInstance(inst, self.instance_status[inst]):
                        if self.instance_status[inst].get('installed', False) \
                               == False:
                            if pkg.get('name') == 'gpg-pubkey':
                                gpg_keys.append(inst)
                            else:
                                install_pkgs.append(inst)
                        elif self.instance_status[inst].get('version_fail', \
                                                            False) == True:
                            upgrade_pkgs.append(inst)
            else:
                install_pkgs.append(pkg)

        # Install GPG keys.
        # Alternatively specify the required keys using 'gpgkey' in the
        # repository definition in yum.conf.  YUM will install the keys
        # automatically.
        if len(gpg_keys) > 0:
            for inst in gpg_keys:
                self.logger.info("Installing GPG keys.")
                if inst.get('simplefile') is None:
                    self.logger.error("GPG key has no simplefile attribute")
                    continue
                key_arg = os.path.join(self.instance_status[inst].get('pkg').get('uri'), \
                                       inst.get('simplefile'))
                if self.cmd.run("rpm --import %s" % key_arg).success:
                    self.logger.debug("Unable to install %s-%s" % \
                                      (self.instance_status[inst].get('pkg').get('name'), \
                                       self.str_evra(inst)))
                else:
                    self.logger.debug("Installed %s-%s-%s" % \
                                      (self.instance_status[inst].get('pkg').get('name'), \
                                       inst.get('version'), inst.get('release')))
            self.RefreshPackages()
            self.gpg_keyids = self.getinstalledgpg()
            pkg = self.instance_status[gpg_keys[0]].get('pkg')
            states[pkg] = self.VerifyPackage(pkg, [])

        # Install packages.
        if len(install_pkgs) > 0:
            self.logger.info("Attempting to install packages")

            if self.autodep:
                pkgtool = "/usr/bin/yum -d0 -y install %s"
            else:
                pkgtool = "/usr/bin/yum -d0 install %s"

            install_args = []
            for inst in install_pkgs:
                pkg_arg = self.instance_status[inst].get('pkg').get('name')
                install_args.append(build_yname(pkg_arg, inst))

            if self.cmd.run(pkgtool % " ".join(install_args)).success:
                # The yum command succeeded.  All packages installed.
                self.logger.info("Single Pass for Install Succeeded")
                self.RefreshPackages()
            else:
                # The yum command failed.  No packages installed.
                # Try installing instances individually.
                self.logger.error("Single Pass Install of Packages Failed")
                installed_instances = []
                for inst in install_pkgs:
                    pkg_arg = build_yname(self.instance_status[inst].get('pkg').get('name'), inst)

                    if self.cmd.run(pkgtool % pkg_arg).success:
                        installed_instances.append(inst)
                    else:
                        self.logger.debug("%s %s would not install." %
                                          (self.instance_status[inst].get('pkg').get('name'),
                                               self.str_evra(inst)))
                self.RefreshPackages()

        # Fix upgradeable packages.
        if len(upgrade_pkgs) > 0:
            self.logger.info("Attempting to upgrade packages")

            if self.autodep:
                pkgtool = "/usr/bin/yum -d0 -y update %s"
            else:
                pkgtool = "/usr/bin/yum -d0 update %s"

            upgrade_args = []
            for inst in upgrade_pkgs:
                pkg_arg = build_yname(self.instance_status[inst].get('pkg').get('name'), inst)
                upgrade_args.append(pkg_arg)

            if self.cmd.run(pkgtool % " ".join(upgrade_args)).success:
                # The yum command succeeded.  All packages installed.
                self.logger.info("Single Pass for Install Succeeded")
                self.RefreshPackages()
            else:
                # The yum command failed.  No packages installed.
                # Try installing instances individually.
                self.logger.error("Single Pass Install of Packages Failed")
                installed_instances = []
                for inst in upgrade_pkgs:
                    pkg_arg = build_yname(self.instance_status[inst].get('pkg').get('name'), inst)
                    if self.cmd.run(pkgtool % pkg_arg).success:
                        installed_instances.append(inst)
                    else:
                        self.logger.debug("%s %s would not install." % \
                                              (self.instance_status[inst].get('pkg').get('name'), \
                                               self.str_evra(inst)))

                self.RefreshPackages()

        if not self.setup['kevlar']:
            for pkg_entry in [p for p in packages if self.canVerify(p)]:
                self.logger.debug("Reverifying Failed Package %s" % (pkg_entry.get('name')))
                states[pkg_entry] = self.VerifyPackage(pkg_entry, \
                                                       self.modlists.get(pkg_entry, []))

        for entry in [ent for ent in packages if states[ent]]:
            self.modified.append(entry)

    def Remove(self, packages):
        """
           Remove specified entries.

           packages is a list of Package Entries with Instances generated
           by FindExtra().
        """
        self.logger.debug('Running YUM24.Remove()')

        if self.autodep:
            pkgtool = "/usr/bin/yum -d0 -y erase %s"
        else:
            pkgtool = "/usr/bin/yum -d0 erase %s"

        erase_args = []
        for pkg in packages:
            for inst in pkg:
                if pkg.get('name') != 'gpg-pubkey':
                    pkg_arg = pkg.get('name') + '-'
                    if inst.get('epoch', False):
                        pkg_arg = pkg_arg + inst.get('epoch') + ':'
                    pkg_arg = pkg_arg + inst.get('version') + '-' + inst.get('release')
                    if inst.get('arch', False):
                        pkg_arg = pkg_arg + '.' + inst.get('arch')
                    erase_args.append(pkg_arg)
                else:
                    pkgspec = {'name': pkg.get('name'),
                               'version': inst.get('version'),
                               'release': inst.get('release')}
                    self.logger.info("WARNING: gpg-pubkey package not in configuration %s %s"\
                                                 % (pkgspec.get('name'), self.str_evra(pkgspec)))
                    self.logger.info("         This package will be deleted in a future version of the YUM24 driver.")

        rv = self.cmd.run(pkgtool % " ".join(erase_args))
        if rv.success:
            self.modified += packages
            for pkg in erase_args:
                self.logger.info("Deleted %s" % (pkg))
        else:
            self.logger.info("Bulk erase failed with errors:")
            self.logger.debug("Erase results: %s" % rv.error)
            self.logger.info("Attempting individual erase for each package.")
            for pkg in packages:
                pkg_modified = False
                for inst in pkg:
                    if pkg.get('name') != 'gpg-pubkey':
                        pkg_arg = pkg.get('name') + '-'
                        if 'epoch' in inst.attrib:
                            pkg_arg = pkg_arg + inst.get('epoch') + ':'
                        pkg_arg = pkg_arg + inst.get('version') + '-' + inst.get('release')
                        if 'arch' in inst.attrib:
                            pkg_arg = pkg_arg + '.' + inst.get('arch')
                    else:
                        self.logger.info("WARNING: gpg-pubkey package not in configuration %s %s"\
                                                 % (pkg.get('name'), self.str_evra(pkg)))
                        self.logger.info("         This package will be deleted in a future version of the YUM24 driver.")
                        continue

                    rv = self.cmd.run(self.pkgtool % pkg_arg)
                    if rv.success:
                        pkg_modified = True
                        self.logger.info("Deleted %s" % pkg_arg)
                    else:
                        self.logger.error("Unable to delete %s" % pkg_arg)
                        self.logger.debug("Failure: %s" % rv.error)
                if pkg_modified == True:
                    self.modified.append(pkg)

        self.RefreshPackages()
        self.extra = self.FindExtra()

########NEW FILE########
__FILENAME__ = YUMng
""" YUM driver called 'YUMng' for backwards compat """

from Bcfg2.Client.Tools.YUM import YUM


class YUMng(YUM):
    """ YUM driver called 'YUMng' for backwards compat """
    deprecated = True
    conflicts = ['YUM24', 'RPM', 'RPMng']

########NEW FILE########
__FILENAME__ = XML
'''XML lib compatibility layer for the Bcfg2 client'''

# library will use lxml, then builtin xml.etree, then ElementTree

# pylint: disable=E0611,W0611,W0613,C0103

try:
    from lxml.etree import Element, SubElement, tostring, XMLParser
    from lxml.etree import XMLSyntaxError as ParseError
    from lxml.etree import XML as _XML
    from Bcfg2.Compat import wraps
    driver = 'lxml'

    # libxml2 2.9.0+ doesn't parse 10M+ documents by default:
    # https://mail.gnome.org/archives/commits-list/2012-August/msg00645.html
    try:
        _parser = XMLParser(huge_tree=True)
    except TypeError:
        _parser = XMLParser()

    @wraps(_XML)
    def XML(val, **kwargs):
        """ unicode strings w/encoding declaration are not supported in
        recent lxml.etree, so we try to read XML, and if it fails we try
        encoding the string. """
        kwargs.setdefault('parser', _parser)
        try:
            return _XML(val, **kwargs)
        except ValueError:
            return _XML(val.encode(), **kwargs)
except ImportError:
    # lxml not available
    from xml.parsers.expat import ExpatError as ParseError
    try:
        import xml.etree.ElementTree
        Element = xml.etree.ElementTree.Element
        SubElement = xml.etree.ElementTree.SubElement
        XML = xml.etree.ElementTree.XML

        def tostring(el, encoding=None, xml_declaration=None):
            """ tostring implementation compatible with lxml """
            return xml.etree.ElementTree.tostring(el, encoding=encoding)

        driver = 'etree-py'
    except ImportError:
        try:
            from elementtree.ElementTree import Element, SubElement, XML, \
                tostring
            driver = 'etree'
            import elementtree.ElementTree
            Element = elementtree.ElementTree.Element
            SubElement = elementtree.ElementTree.SubElement
            XML = elementtree.ElementTree.XML

            def tostring(el, encoding=None, xml_declaration=None):
                """ tostring implementation compatible with lxml """
                return elementtree.ElementTree.tostring(el)

        except ImportError:
            print("Failed to load lxml, xml.etree or elementtree.ElementTree")
            print("Cannot continue")
            raise SystemExit(1)

########NEW FILE########
__FILENAME__ = Compat
""" Compatibility imports, mostly for Py3k support, but also for
Python 2.4 and such-like """

###################################################
#                                                 #
#   IF YOU ADD SOMETHING TO THIS FILE, YOU MUST   #
#   DOCUMENT IT IN docs/development/compat.txt    #
#                                                 #
###################################################

import sys

# pylint: disable=E0601,E0602,E0611,W0611,W0622,C0103

try:
    from email.Utils import formatdate
except ImportError:
    from email.utils import formatdate

# urllib imports
try:
    from urllib import quote_plus
    from urlparse import urljoin, urlparse
    from urllib2 import HTTPBasicAuthHandler, \
        HTTPPasswordMgrWithDefaultRealm, build_opener, install_opener, \
        urlopen, HTTPError, URLError
except ImportError:
    from urllib.parse import urljoin, urlparse, quote_plus
    from urllib.request import HTTPBasicAuthHandler, \
        HTTPPasswordMgrWithDefaultRealm, build_opener, install_opener, urlopen
    from urllib.error import HTTPError, URLError

try:
    from cStringIO import StringIO
except ImportError:
    from io import StringIO

try:
    import ConfigParser
except ImportError:
    import configparser as ConfigParser

try:
    import cPickle
except ImportError:
    import pickle as cPickle

try:
    from Queue import Queue, Empty, Full
except ImportError:
    from queue import Queue, Empty, Full

# xmlrpc imports
try:
    import xmlrpclib
    import SimpleXMLRPCServer
except ImportError:
    import xmlrpc.client as xmlrpclib
    import xmlrpc.server as SimpleXMLRPCServer

# socketserver import
try:
    import SocketServer
except ImportError:
    import socketserver as SocketServer

# httplib imports
try:
    import httplib
except ImportError:
    import http.client as httplib

try:
    unicode = unicode
except NameError:
    unicode = str


def u_str(string, encoding=None):
    """ print to file compatibility """
    if sys.hexversion >= 0x03000000:
        return string
    else:
        if encoding is not None:
            return unicode(string, encoding)
        else:
            return unicode(string)

try:
    from functools import wraps
except ImportError:
    def wraps(wrapped):  # pylint: disable=W0613
        """ implementation of functools.wraps() for python 2.4 """
        return lambda f: f


# base64 compat
if sys.hexversion >= 0x03000000:
    from base64 import b64encode as _b64encode, b64decode as _b64decode

    @wraps(_b64encode)
    def b64encode(val, **kwargs):  # pylint: disable=C0111
        try:
            return _b64encode(val, **kwargs)
        except TypeError:
            return _b64encode(val.encode('UTF-8'), **kwargs).decode('UTF-8')

    @wraps(_b64decode)
    def b64decode(val, **kwargs):  # pylint: disable=C0111
        return _b64decode(val.encode('UTF-8'), **kwargs).decode('UTF-8')
else:
    from base64 import b64encode, b64decode

try:
    input = raw_input
except NameError:
    input = input

try:
    reduce = reduce
except NameError:
    from functools import reduce

try:
    from collections import MutableMapping
except ImportError:
    from UserDict import DictMixin as MutableMapping


class CmpMixin(object):
    """ In Py3K, :meth:`object.__cmp__` is no longer magical, so this
    mixin can be used to define the rich comparison operators from
    ``__cmp__`` -- i.e., it makes ``__cmp__`` magical again. """

    def __lt__(self, other):
        return self.__cmp__(other) < 0

    def __gt__(self, other):
        return self.__cmp__(other) > 0

    def __eq__(self, other):
        return self.__cmp__(other) == 0

    def __ne__(self, other):
        return not self.__eq__(other)

    def __ge__(self, other):
        return self.__gt__(other) or self.__eq__(other)

    def __le__(self, other):
        return self.__lt__(other) or self.__eq__(other)

try:
    from pkgutil import walk_packages
except ImportError:
    try:
        from pkgutil import iter_modules
        # iter_modules was added in python 2.5; use it to get an exact
        # re-implementation of walk_packages if possible

        def walk_packages(path=None, prefix='', onerror=None):
            """ Implementation of walk_packages for python 2.5 """
            def seen(path, seenpaths={}):  # pylint: disable=W0102
                """ detect if a path has been 'seen' (i.e., considered
                for inclusion in the generator).  tracks what has been
                seen through the magic of python default arguments """
                if path in seenpaths:
                    return True
                seenpaths[path] = True

            for importer, name, ispkg in iter_modules(path, prefix):
                yield importer, name, ispkg

                if ispkg:
                    try:
                        __import__(name)
                    except ImportError:
                        if onerror is not None:
                            onerror(name)
                    except Exception:
                        if onerror is not None:
                            onerror(name)
                        else:
                            raise
                    else:
                        path = getattr(sys.modules[name], '__path__', [])

                        # don't traverse path items we've seen before
                        path = [p for p in path if not seen(p)]

                        for item in walk_packages(path, name + '.', onerror):
                            yield item
    except ImportError:
        import os

        def walk_packages(path=None, prefix='', onerror=None):
            """ Imperfect, incomplete implementation of
            :func:`pkgutil.walk_packages` for python 2.4. Differences:

            * Requires a full path, not a path relative to something
              in sys.path.  Anywhere we care about that shouldn't be
              an issue.
            * The first element of each tuple is None instead of an
              importer object.
            """
            if path is None:
                path = sys.path
            for mpath in path:
                for fname in os.listdir(mpath):
                    fpath = os.path.join(mpath, fname)
                    if (os.path.isfile(fpath) and fname.endswith(".py") and
                        fname != '__init__.py'):
                        yield None, prefix + fname[:-3], False
                    elif os.path.isdir(fpath):
                        mname = prefix + fname
                        if os.path.exists(os.path.join(fpath, "__init__.py")):
                            yield None, mname, True
                        try:
                            __import__(mname)
                        except ImportError:
                            if onerror is not None:
                                onerror(mname)
                        except Exception:
                            if onerror is not None:
                                onerror(mname)
                            else:
                                raise
                        else:
                            for item in walk_packages([fpath],
                                                      prefix=mname + '.',
                                                      onerror=onerror):
                                yield item


try:
    all = all
    any = any
except NameError:
    def all(iterable):
        """ implementation of builtin all() for python 2.4 """
        for element in iterable:
            if not element:
                return False
        return True

    def any(iterable):
        """ implementation of builtin any() for python 2.4 """
        for element in iterable:
            if element:
                return True
        return False

try:
    from hashlib import md5
except ImportError:
    from md5 import md5


def oct_mode(mode):
    """ Convert a decimal number describing a POSIX permissions mode
    to a string giving the octal mode.  In Python 2, this is a synonym
    for :func:`oct`, but in Python 3 the octal format has changed to
    ``0o000``, which cannot be used as an octal permissions mode, so
    we need to strip the 'o' from the output.  I.e., this function
    acts like the Python 2 :func:`oct` regardless of what version of
    Python is in use.

    :param mode: The decimal mode to convert to octal
    :type mode: int
    :returns: string """
    return oct(mode).replace('o', '')


try:
    long = long
except NameError:
    # longs are just ints in py3k
    long = int


try:
    cmp = cmp
except NameError:
    def cmp(a, b):
        """ Py3k implementation of cmp() """
        return (a > b) - (a < b)

########NEW FILE########
__FILENAME__ = Encryption
""" Bcfg2.Encryption provides a number of convenience methods for
handling encryption in Bcfg2.  See :ref:`server-encryption` for more
details. """

import os
import sys
from M2Crypto import Rand
from M2Crypto.EVP import Cipher, EVPError
from Bcfg2.Compat import StringIO, md5, b64encode, b64decode

#: Constant representing the encryption operation for
#: :class:`M2Crypto.EVP.Cipher`, which uses a simple integer.  This
#: makes our code more readable.
ENCRYPT = 1

#: Constant representing the decryption operation for
#: :class:`M2Crypto.EVP.Cipher`, which uses a simple integer.  This
#: makes our code more readable.
DECRYPT = 0

#: Default cipher algorithm.  To get a full list of valid algorithms,
#: you can run::
#:
#:     openssl list-cipher-algorithms | grep -v ' => ' | \
#:         tr 'A-Z-' 'a-z_' | sort -u
ALGORITHM = "aes_256_cbc"

#: Default initialization vector.  For best security, you should use a
#: unique IV for each message.  :func:`ssl_encrypt` does this in an
#: automated fashion.
IV = r'\0' * 16

#: The config file section encryption options and passphrases are
#: stored in
CFG_SECTION = "encryption"

#: The config option used to store the algorithm
CFG_ALGORITHM = "algorithm"

#: The config option used to store the decryption strictness
CFG_DECRYPT = "decrypt"

Rand.rand_seed(os.urandom(1024))


def _cipher_filter(cipher, instr):
    """ M2Crypto reads and writes file-like objects, so this uses
    StringIO to pass data through it """
    inbuf = StringIO(instr)
    outbuf = StringIO()
    while 1:
        buf = inbuf.read()
        if not buf:
            break
        outbuf.write(cipher.update(buf))
    outbuf.write(cipher.final())
    rv = outbuf.getvalue()
    inbuf.close()
    outbuf.close()
    return rv


def str_encrypt(plaintext, key, iv=IV, algorithm=ALGORITHM, salt=None):
    """ Encrypt a string with a key.  For a higher-level encryption
    interface, see :func:`ssl_encrypt`.

    :param plaintext: The plaintext data to encrypt
    :type plaintext: string
    :param key: The key to encrypt the data with
    :type key: string
    :param iv: The initialization vector
    :type iv: string
    :param algorithm: The cipher algorithm to use
    :type algorithm: string
    :param salt: The salt to use
    :type salt: string
    :returns: string - The decrypted data
    """
    cipher = Cipher(alg=algorithm, key=key, iv=iv, op=ENCRYPT, salt=salt)
    return _cipher_filter(cipher, plaintext)


def str_decrypt(crypted, key, iv=IV, algorithm=ALGORITHM):
    """ Decrypt a string with a key.  For a higher-level decryption
    interface, see :func:`ssl_decrypt`.

    :param crypted: The raw binary encrypted data
    :type crypted: string
    :param key: The encryption key to decrypt with
    :type key: string
    :param iv: The initialization vector
    :type iv: string
    :param algorithm: The cipher algorithm to use
    :type algorithm: string
    :returns: string - The decrypted data
    """
    cipher = Cipher(alg=algorithm, key=key, iv=iv, op=DECRYPT)
    return _cipher_filter(cipher, crypted)


def ssl_decrypt(data, passwd, algorithm=ALGORITHM):
    """ Decrypt openssl-encrypted data.  This can decrypt data
    encrypted by :func:`ssl_encrypt`, or ``openssl enc``.  It performs
    a base64 decode first if the data is base64 encoded, and
    automatically determines the salt and initialization vector (both
    of which are embedded in the encrypted data).

    :param data: The encrypted data (either base64-encoded or raw
                 binary) to decrypt
    :type data: string
    :param passwd: The password to use to decrypt the data
    :type passwd: string
    :param algorithm: The cipher algorithm to use
    :type algorithm: string
    :returns: string - The decrypted data
    """
    # base64-decode the data
    try:
        data = b64decode(data)
    except TypeError:
        # we do not include the data in the error message, because one
        # of the common causes of this is data that claims to be
        # encrypted but is not.  we don't want to include a plaintext
        # secret in the error logs.
        raise TypeError("Could not decode base64 data: %s" %
                        sys.exc_info()[1])
    salt = data[8:16]
    # pylint: disable=E1101,E1121
    hashes = [md5(passwd + salt).digest()]
    for i in range(1, 3):
        hashes.append(md5(hashes[i - 1] + passwd + salt).digest())
    # pylint: enable=E1101,E1121
    key = hashes[0] + hashes[1]
    iv = hashes[2]

    return str_decrypt(data[16:], key=key, iv=iv, algorithm=algorithm)


def ssl_encrypt(plaintext, passwd, algorithm=ALGORITHM, salt=None):
    """ Encrypt data in a format that is openssl compatible.

    :param plaintext: The plaintext data to encrypt
    :type plaintext: string
    :param passwd: The password to use to encrypt the data
    :type passwd: string
    :param algorithm: The cipher algorithm to use
    :type algorithm: string
    :param salt: The salt to use.  If none is provided, one will be
                 randomly generated.
    :type salt: bytes
    :returns: string - The base64-encoded, salted, encrypted string.
              The string includes a trailing newline to make it fully
              compatible with openssl command-line tools.
    """
    if salt is None:
        salt = Rand.rand_bytes(8)

    # pylint: disable=E1101,E1121
    hashes = [md5(passwd + salt).digest()]
    for i in range(1, 3):
        hashes.append(md5(hashes[i - 1] + passwd + salt).digest())
    # pylint: enable=E1101,E1121
    key = hashes[0] + hashes[1]
    iv = hashes[2]

    crypted = str_encrypt(plaintext, key=key, salt=salt, iv=iv,
                          algorithm=algorithm)
    return b64encode("Salted__" + salt + crypted) + "\n"


def get_algorithm(setup):
    """ Get the cipher algorithm from the config file.  This is used
    in case someone uses the OpenSSL algorithm name (e.g.,
    "AES-256-CBC") instead of the M2Crypto name (e.g., "aes_256_cbc"),
    and to handle errors in a sensible way and deduplicate this code.

    :param setup: The Bcfg2 option set to extract passphrases from
    :type setup: Bcfg2.Options.OptionParser
    :returns: dict - a dict of ``<passphrase name>``: ``<passphrase>``
    """
    return setup.cfp.get(CFG_SECTION, CFG_ALGORITHM,
                         default=ALGORITHM).lower().replace("-", "_")


def get_passphrases(setup):
    """ Get all candidate encryption passphrases from the config file.

    :param setup: The Bcfg2 option set to extract passphrases from
    :type setup: Bcfg2.Options.OptionParser
    :returns: dict - a dict of ``<passphrase name>``: ``<passphrase>``
    """
    section = CFG_SECTION
    if setup.cfp.has_section(section):
        return dict([(o, setup.cfp.get(section, o))
                     for o in setup.cfp.options(section)
                     if o not in [CFG_ALGORITHM, CFG_DECRYPT]])
    else:
        return dict()


def bruteforce_decrypt(crypted, passphrases=None, setup=None,
                       algorithm=ALGORITHM):
    """ Convenience method to decrypt the given encrypted string by
    trying the given passphrases or all passphrases (as returned by
    :func:`get_passphrases`) sequentially until one is found that
    works.

    Either ``passphrases`` or ``setup`` must be provided.

    :param crypted: The data to decrypt
    :type crypted: string
    :param passphrases: The passphrases to try.
    :type passphrases: list
    :param setup: A Bcfg2 option set to extract passphrases from
    :type setup: Bcfg2.Options.OptionParser
    :param algorithm: The cipher algorithm to use
    :type algorithm: string
    :returns: string - The decrypted data
    :raises: :class:`M2Crypto.EVP.EVPError`, if the data cannot be decrypted
    """
    if passphrases is None:
        passphrases = get_passphrases(setup).values()
    for passwd in passphrases:
        try:
            return ssl_decrypt(crypted, passwd, algorithm=algorithm)
        except EVPError:
            pass
    raise EVPError("Failed to decrypt")

########NEW FILE########
__FILENAME__ = Logger
"""Bcfg2 logging support"""

import copy
import fcntl
import logging
import logging.handlers
import math
import socket
import struct
import sys
import termios

logging.raiseExceptions = 0


class TermiosFormatter(logging.Formatter):
    """The termios formatter displays output
    in a terminal-sensitive fashion.
    """

    def __init__(self, fmt=None, datefmt=None):
        logging.Formatter.__init__(self, fmt, datefmt)
        if sys.stdout.isatty():
            # now get termios info
            try:
                self.width = struct.unpack('hhhh',
                                           fcntl.ioctl(0,
                                                       termios.TIOCGWINSZ,
                                                       "\000" * 8))[1]
                if self.width == 0:
                    self.width = 80
            except:  # pylint: disable=W0702
                self.width = 80
        else:
            # output to a pipe
            self.width = 32768

    def format(self, record):
        '''format a record for display'''
        returns = []
        line_len = self.width
        if isinstance(record.msg, str):
            for line in record.msg.split('\n'):
                if len(line) <= line_len:
                    returns.append(line)
                else:
                    inner_lines = \
                        int(math.floor(float(len(line)) / line_len)) + 1
                    for msgline in range(inner_lines):
                        returns.append(
                            line[msgline * line_len:(msgline + 1) * line_len])
        elif isinstance(record.msg, list):
            if not record.msg:
                return ''
            record.msg.sort()
            msgwidth = self.width
            col_width = max([len(item) for item in record.msg])
            columns = int(math.floor(float(msgwidth) / (col_width + 2)))
            lines = int(math.ceil(float(len(record.msg)) / columns))
            for lineno in range(lines):
                indices = [idx for idx in [(colNum * lines) + lineno
                                           for colNum in range(columns)]
                           if idx < len(record.msg)]
                retformat = (len(indices) * (" %%-%ds " % col_width))
                returns.append(retformat % tuple([record.msg[idx]
                                                  for idx in indices]))
        else:
            returns.append(str(record.msg))
        if record.exc_info:
            returns.append(self.formatException(record.exc_info))
        return '\n'.join(returns)


class FragmentingSysLogHandler(logging.handlers.SysLogHandler):
    """
       This handler fragments messages into
       chunks smaller than 250 characters
    """

    def __init__(self, procname, path, facility):
        self.procname = procname
        self.unixsocket = False
        logging.handlers.SysLogHandler.__init__(self, path, facility)

    def emit(self, record):
        """Chunk and deliver records."""
        record.name = self.procname
        if isinstance(record.msg, str):
            msgs = []
            error = record.exc_info
            record.exc_info = None
            msgdata = record.msg
            if len(msgdata) == 0:
                return
            while msgdata:
                newrec = copy.copy(record)
                newrec.msg = msgdata[:250]
                msgs.append(newrec)
                msgdata = msgdata[250:]
            msgs[0].exc_info = error
        else:
            msgs = [record]
        for newrec in msgs:
            msg = '<%d>%s\000' % \
                (self.encodePriority(self.facility, newrec.levelname.lower()),
                 self.format(newrec))
            try:
                try:
                    encoded = msg.encode('utf-8')
                except UnicodeDecodeError:
                    encoded = msg
                self.socket.send(encoded)
            except socket.error:
                for i in range(10):  # pylint: disable=W0612
                    try:
                        if isinstance(self.address, tuple):
                            self.socket = socket.socket(socket.AF_INET,
                                                        socket.SOCK_DGRAM)
                            self.socket.connect(self.address)
                        else:
                            self._connect_unixsocket(self.address)
                        break
                    except socket.error:
                        continue
                try:
                    reconn = copy.copy(record)
                    reconn.msg = 'Reconnected to syslog'
                    self.socket.send('<%d>%s\000' %
                                     (self.encodePriority(self.facility,
                                                          logging.WARNING),
                                      self.format(reconn)))
                    self.socket.send(msg)
                except:  # pylint: disable=W0702
                    # If we still fail then drop it.  Running
                    # bcfg2-server as non-root can trigger permission
                    # denied exceptions.
                    pass


def add_console_handler(level=logging.DEBUG):
    """ Add a logging handler that logs at a level to sys.stderr """
    console = logging.StreamHandler(sys.stderr)
    console.setLevel(level)
    # tell the handler to use this format
    console.setFormatter(TermiosFormatter())
    try:
        console.set_name("console")  # pylint: disable=E1101
    except AttributeError:
        console.name = "console"  # pylint: disable=W0201
    logging.root.addHandler(console)


def add_syslog_handler(procname, syslog_facility, level=logging.DEBUG):
    """Add a logging handler that logs as procname to syslog_facility."""
    try:
        try:
            syslog = FragmentingSysLogHandler(procname,
                                              '/dev/log',
                                              syslog_facility)
        except socket.error:
            syslog = FragmentingSysLogHandler(procname,
                                              ('localhost', 514),
                                              syslog_facility)
        try:
            syslog.set_name("syslog")  # pylint: disable=E1101
        except AttributeError:
            syslog.name = "syslog"  # pylint: disable=W0201
        syslog.setLevel(level)
        syslog.setFormatter(
            logging.Formatter('%(name)s[%(process)d]: %(message)s'))
        logging.root.addHandler(syslog)
    except socket.error:
        logging.root.error("Failed to activate syslogging")
    except:
        print("Failed to activate syslogging")


def add_file_handler(to_file, level=logging.DEBUG):
    """Add a logging handler that logs to to_file."""
    filelog = logging.FileHandler(to_file)
    try:
        filelog.set_name("file")  # pylint: disable=E1101
    except AttributeError:
        filelog.name = "file"  # pylint: disable=W0201
    filelog.setLevel(level)
    filelog.setFormatter(
        logging.Formatter('%(asctime)s %(name)s[%(process)d]: %(message)s'))
    logging.root.addHandler(filelog)


def setup_logging(procname, to_console=True, to_syslog=True,
                  syslog_facility='daemon', level=0, to_file=None):
    """Setup logging for Bcfg2 software."""
    if hasattr(logging, 'already_setup'):
        return

    params = []

    if to_console:
        if to_console is True:
            to_console = logging.WARNING
        if level == 0:
            clvl = to_console
        else:
            clvl = min(to_console, level)
        params.append("%s to console" % logging.getLevelName(clvl))
        add_console_handler(clvl)
    if to_syslog:
        if level == 0:
            slvl = logging.INFO
        else:
            slvl = min(level, logging.INFO)
        params.append("%s to syslog" % logging.getLevelName(slvl))
        add_syslog_handler(procname, syslog_facility, level=slvl)
    if to_file is not None:
        params.append("%s to %s" % (logging.getLevelName(level), to_file))
        add_file_handler(to_file, level=level)

    logging.root.setLevel(logging.DEBUG)
    logging.root.debug("Configured logging: %s" % "; ".join(params))
    logging.already_setup = True

########NEW FILE########
__FILENAME__ = manage
#!/usr/bin/env python
from django.core.management import execute_manager
import imp
try:
    imp.find_module('settings') # Assumed to be in the same directory.
except ImportError:
    import sys
    sys.stderr.write("Error: Can't find the file 'settings.py' in the directory containing %r. It appears you've customized things.\nYou'll have to run django-admin.py, passing it your settings module.\n" % __file__)
    sys.exit(1)

import settings

if __name__ == "__main__":
    execute_manager(settings)

########NEW FILE########
__FILENAME__ = Options
"""Option parsing library for utilities."""

import copy
import getopt
import inspect
import os
import re
import shlex
import sys
import grp
import pwd
import Bcfg2.Client.Tools
from Bcfg2.Compat import ConfigParser
from Bcfg2.version import __version__


class OptionFailure(Exception):
    """ raised when malformed Option objects are instantiated """
    pass

DEFAULT_CONFIG_LOCATION = '/etc/bcfg2.conf'
DEFAULT_INSTALL_PREFIX = '/usr'


class DefaultConfigParser(ConfigParser.ConfigParser):
    """ A config parser that can be used to query options with default
    values in the event that the option is not found """

    def __init__(self, *args, **kwargs):
        """Make configuration options case sensitive"""
        ConfigParser.ConfigParser.__init__(self, *args, **kwargs)
        self.optionxform = str

    def get(self, section, option, **kwargs):
        """ convenience method for getting config items """
        default = None
        if 'default' in kwargs:
            default = kwargs['default']
            del kwargs['default']
        try:
            return ConfigParser.ConfigParser.get(self, section, option,
                                                 **kwargs)
        except (ConfigParser.NoSectionError, ConfigParser.NoOptionError):
            if default is not None:
                return default
            else:
                raise

    def getboolean(self, section, option, **kwargs):
        """ convenience method for getting boolean config items """
        default = None
        if 'default' in kwargs:
            default = kwargs['default']
            del kwargs['default']
        try:
            return ConfigParser.ConfigParser.getboolean(self, section,
                                                        option, **kwargs)
        except (ConfigParser.NoSectionError, ConfigParser.NoOptionError,
                ValueError):
            if default is not None:
                return default
            else:
                raise


class Option(object):
    """ a single option, which might be read from the command line,
    environment, or config file """

    # pylint: disable=C0103,R0913
    def __init__(self, desc, default, cmd=None, odesc=False,
                 env=False, cf=False, cook=False, long_arg=False,
                 deprecated_cf=None):
        self.desc = desc
        self.default = default
        self.cmd = cmd
        self.long = long_arg
        if not self.long:
            if cmd and (cmd[0] != '-' or len(cmd) != 2):
                raise OptionFailure("Poorly formed command %s" % cmd)
        elif cmd and not cmd.startswith('--'):
            raise OptionFailure("Poorly formed command %s" % cmd)
        self.odesc = odesc
        self.env = env
        self.cf = cf
        self.deprecated_cf = deprecated_cf
        self.boolean = False
        if not odesc and not cook and isinstance(self.default, bool):
            self.boolean = True
        self.cook = cook
        self.value = None
    # pylint: enable=C0103,R0913

    def get_cooked_value(self, value):
        """ get the value of this option after performing any option
        munging specified in the 'cook' keyword argument to the
        constructor """
        if self.boolean:
            return True
        if self.cook:
            return self.cook(value)
        else:
            return value

    def __str__(self):
        rv = ["%s: " % self.__class__.__name__, self.desc]
        if self.cmd or self.cf:
            rv.append(" (")
        if self.cmd:
            if self.odesc:
                if self.long:
                    rv.append("%s=%s" % (self.cmd, self.odesc))
                else:
                    rv.append("%s %s" % (self.cmd, self.odesc))
            else:
                rv.append("%s" % self.cmd)

        if self.cf:
            if self.cmd:
                rv.append("; ")
            rv.append("[%s].%s" % self.cf)
        if self.cmd or self.cf:
            rv.append(")")
        if hasattr(self, "value"):
            rv.append(": %s" % self.value)
        return "".join(rv)

    def buildHelpMessage(self):
        """ build the help message for this option """
        vals = []
        if not self.cmd:
            return ''
        if self.odesc:
            if self.long:
                vals.append("%s=%s" % (self.cmd, self.odesc))
            else:
                vals.append("%s %s" % (self.cmd, self.odesc))
        else:
            vals.append(self.cmd)
        vals.append(self.desc)
        return "     %-28s %s\n" % tuple(vals)

    def buildGetopt(self):
        """ build a string suitable for describing this short option
        to getopt """
        gstr = ''
        if self.long:
            return gstr
        if self.cmd:
            gstr = self.cmd[1]
            if self.odesc:
                gstr += ':'
        return gstr

    def buildLongGetopt(self):
        """ build a string suitable for describing this long option to
        getopt """
        if self.odesc:
            return self.cmd[2:] + '='
        else:
            return self.cmd[2:]

    def parse(self, opts, rawopts, configparser=None):
        """ parse a single option. try parsing the data out of opts
        (the results of getopt), rawopts (the raw option string), the
        environment, and finally the config parser. either opts or
        rawopts should be provided, but not both """
        if self.cmd and opts:
            # Processing getopted data
            optinfo = [opt[1] for opt in opts if opt[0] == self.cmd]
            if optinfo:
                if optinfo[0]:
                    self.value = self.get_cooked_value(optinfo[0])
                else:
                    self.value = True
                return
        if self.cmd and self.cmd in rawopts:
            if self.odesc:
                data = rawopts[rawopts.index(self.cmd) + 1]
            else:
                data = True
            self.value = self.get_cooked_value(data)
            return
        # No command line option found
        if self.env and self.env in os.environ:
            self.value = self.get_cooked_value(os.environ[self.env])
            return
        if self.cf and configparser:
            try:
                self.value = self.get_cooked_value(configparser.get(*self.cf))
                return
            except (ConfigParser.NoSectionError, ConfigParser.NoOptionError):
                pass
            if self.deprecated_cf:
                try:
                    self.value = self.get_cooked_value(
                        configparser.get(*self.deprecated_cf))
                    print("Warning: [%s] %s is deprecated, use [%s] %s instead"
                          % (self.deprecated_cf[0], self.deprecated_cf[1],
                             self.cf[0], self.cf[1]))
                    return
                except (ConfigParser.NoSectionError,
                        ConfigParser.NoOptionError):
                    pass

        # Default value not cooked
        self.value = self.default


class OptionSet(dict):
    """ a set of Option objects that interfaces with getopt and
    DefaultConfigParser to populate a dict of <option name>:<value>
    """

    def __init__(self, *args, **kwargs):
        dict.__init__(self, *args)
        self.hm = self.buildHelpMessage()  # pylint: disable=C0103
        if 'configfile' in kwargs:
            self.cfile = kwargs['configfile']
        else:
            self.cfile = DEFAULT_CONFIG_LOCATION
        if 'quiet' in kwargs:
            self.quiet = kwargs['quiet']
        else:
            self.quiet = False
        self.cfp = DefaultConfigParser()
        if len(self.cfp.read(self.cfile)) == 0 and not self.quiet:
            # suppress warnings if called from bcfg2-admin init
            caller = inspect.stack()[-1][1].split('/')[-1]
            if caller == 'bcfg2-admin' and len(sys.argv) > 1:
                if sys.argv[1] == 'init':
                    return
            else:
                print("Warning! Unable to read specified configuration file: "
                      "%s" % self.cfile)

    def buildGetopt(self):
        """ build a short option description string suitable for use
        by getopt.getopt """
        return ''.join([opt.buildGetopt() for opt in list(self.values())])

    def buildLongGetopt(self):
        """ build a list of long options suitable for use by
        getopt.getopt """
        return [opt.buildLongGetopt() for opt in list(self.values())
                if opt.long]

    def buildHelpMessage(self):
        """ Build the help mesage for this option set, or use self.hm
        if it is set """
        if hasattr(self, 'hm'):
            return self.hm
        hlist = []  # list of _non-empty_ help messages
        for opt in list(self.values()):
            helpmsg = opt.buildHelpMessage()
            if helpmsg:
                hlist.append(helpmsg)
        return ''.join(hlist)

    def helpExit(self, msg='', code=1):
        """ print help and exit """
        if msg:
            print(msg)
        print("Usage:")
        print(self.buildHelpMessage())
        raise SystemExit(code)

    def versionExit(self, code=0):
        """ print the version of bcfg2 and exit """
        print("%s %s on Python %s" %
              (os.path.basename(sys.argv[0]),
               __version__,
               ".".join(str(v) for v in sys.version_info[0:3])))
        raise SystemExit(code)

    def parse(self, argv, do_getopt=True):
        '''Parse options from command line.'''
        if VERSION not in self.values():
            self['__version__'] = VERSION
        if do_getopt:
            try:
                opts, args = getopt.getopt(argv, self.buildGetopt(),
                                           self.buildLongGetopt())
            except getopt.GetoptError:
                err = sys.exc_info()[1]
                self.helpExit(err)
            if '-h' in argv:
                self.helpExit('', 0)
            if '--version' in argv:
                self.versionExit()
            self['args'] = args
        for key in list(self.keys()):
            if key == 'args':
                continue
            option = self[key]
            if do_getopt:
                option.parse(opts, [], configparser=self.cfp)
            else:
                option.parse([], argv, configparser=self.cfp)
            if hasattr(option, 'value'):
                val = option.value
                self[key] = val
        if "__version__" in self:
            del self['__version__']


def list_split(c_string):
    """ split an option string on commas, optionally surrounded by
    whitespace, returning a list """
    if c_string:
        return re.split(r'\s*,\s*', c_string)
    return []


def list_split_anchored_regex(c_string):
    """ like list_split but split on whitespace and compile each element as
    anchored regex """
    try:
        return [re.compile('^' + x + '$') for x in re.split(r'\s+', c_string)]
    except re.error:
        raise ValueError("Not a list of regexes", c_string)


def colon_split(c_string):
    """ split an option string on colons, returning a list """
    if c_string:
        return c_string.split(r':')
    return []


def dict_split(c_string):
    """ split an option string on commas, optionally surrounded by
    whitespace and split the resulting items again on equals signs,
    returning a dict """
    result = dict()
    if c_string:
        items = re.split(r'\s*,\s*', c_string)
        for item in items:
            if r'=' in item:
                key, value = item.split(r'=', 1)
                try:
                    result[key] = get_bool(value)
                except ValueError:
                    try:
                        result[key] = get_int(value)
                    except ValueError:
                        result[key] = value
            else:
                result[item] = True
    return result


def get_bool(val):
    """ given a string value of a boolean configuration option, return
    an actual bool (True or False) """
    # these values copied from ConfigParser.RawConfigParser.getboolean
    # with the addition of True and False
    truelist = ["1", "yes", "True", "true", "on"]
    falselist = ["0", "no", "False", "false", "off"]
    if val in truelist:
        return True
    elif val in falselist:
        return False
    else:
        raise ValueError("Not a boolean value", val)


def get_int(val):
    """ given a string value of an integer configuration option,
    return an actual int """
    return int(val)


def get_timeout(val):
    """ convert the timeout value into a float or None """
    if val is None:
        return val
    timeout = float(val)  # pass ValueError up the stack
    if timeout <= 0:
        return None
    return timeout


def get_size(value):
    """ Given a number of bytes in a human-readable format (e.g.,
    '512m', '2g'), get the absolute number of bytes as an integer """
    if value == -1:
        return value
    mat = re.match(r'(\d+)([KkMmGg])?', value)
    if not mat:
        raise ValueError("Not a valid size", value)
    rvalue = int(mat.group(1))
    mult = mat.group(2).lower()
    if mult == 'k':
        return rvalue * 1024
    elif mult == 'm':
        return rvalue * 1024 * 1024
    elif mult == 'g':
        return rvalue * 1024 * 1024 * 1024
    else:
        return rvalue


def get_gid(val):
    """ This takes a group name or gid and returns the corresponding
    gid. """
    try:
        return int(val)
    except ValueError:
        return int(grp.getgrnam(val)[2])


def get_uid(val):
    """ This takes a group name or gid and returns the corresponding
    gid. """
    try:
        return int(val)
    except ValueError:
        return int(pwd.getpwnam(val)[2])


# Options accepts keyword argument list with the following values:
#         default:    default value for the option
#         cmd:        command line switch
#         odesc:      option description
#         cf:         tuple containing section/option
#         cook:       method for parsing option
#         long_arg:   (True|False) specifies whether cmd is a long argument

# General options
CFILE = \
    Option('Specify configuration file',
           default=DEFAULT_CONFIG_LOCATION,
           cmd='-C',
           odesc='<conffile>',
           env="BCFG2_CONFIG")
LOCKFILE = \
    Option('Specify lockfile',
           default='/var/lock/bcfg2.run',
           odesc='<Path to lockfile>',
           cf=('components', 'lockfile'))
HELP = \
    Option('Print this usage message',
           default=False,
           cmd='-h')
VERSION = \
    Option('Print the version and exit',
           default=False,
           cmd='--version', long_arg=True)
DAEMON = \
    Option("Daemonize process, storing pid",
           default=None,
           cmd='-D',
           odesc='<pidfile>')
INSTALL_PREFIX = \
    Option('Installation location',
           default=DEFAULT_INSTALL_PREFIX,
           odesc='</path>',
           cf=('server', 'prefix'))
SENDMAIL_PATH = \
    Option('Path to sendmail',
           default='/usr/lib/sendmail',
           cf=('reports', 'sendmailpath'))
INTERACTIVE = \
    Option('Run interactively, prompting the user for each change',
           default=False,
           cmd='-I', )
ENCODING = \
    Option('Encoding of cfg files',
           default='UTF-8',
           cmd='-E',
           odesc='<encoding>',
           cf=('components', 'encoding'))
PARANOID_PATH = \
    Option('Specify path for paranoid file backups',
           default='/var/cache/bcfg2',
           odesc='<paranoid backup path>',
           cf=('paranoid', 'path'))
PARANOID_MAX_COPIES = \
    Option('Specify the number of paranoid copies you want',
           default=1,
           odesc='<max paranoid copies>',
           cf=('paranoid', 'max_copies'))
OMIT_LOCK_CHECK = \
    Option('Omit lock check',
           default=False,
           cmd='-O')
CORE_PROFILE = \
    Option('profile',
           default=False,
           cmd='-p')
SCHEMA_PATH = \
    Option('Path to XML Schema files',
           default='%s/share/bcfg2/schemas' % DEFAULT_INSTALL_PREFIX,
           cmd='--schema',
           odesc='<schema path>',
           cf=('lint', 'schema'),
           long_arg=True)
INTERPRETER = \
    Option("Python interpreter to use",
           default='best',
           cmd="--interpreter",
           odesc='<python|bpython|ipython|best>',
           cf=('bcfg2-info', 'interpreter'),
           long_arg=True)

# Metadata options (mdata section)
MDATA_OWNER = \
    Option('Default Path owner',
           default='root',
           odesc='owner permissions',
           cf=('mdata', 'owner'))
MDATA_GROUP = \
    Option('Default Path group',
           default='root',
           odesc='group permissions',
           cf=('mdata', 'group'))
MDATA_IMPORTANT = \
    Option('Default Path priority (importance)',
           default='False',
           odesc='Important entries are installed first',
           cf=('mdata', 'important'))
MDATA_MODE = \
    Option('Default mode for Path',
           default='644',
           odesc='octal file mode',
           cf=('mdata', 'mode'))
MDATA_SECONTEXT = \
    Option('Default SELinux context',
           default='__default__',
           odesc='SELinux context',
           cf=('mdata', 'secontext'))
MDATA_PARANOID = \
    Option('Default Path paranoid setting',
           default='true',
           odesc='Path paranoid setting',
           cf=('mdata', 'paranoid'))
MDATA_SENSITIVE = \
    Option('Default Path sensitive setting',
           default='false',
           odesc='Path sensitive setting',
           cf=('mdata', 'sensitive'))

# Server options
SERVER_REPOSITORY = \
    Option('Server repository path',
           default='/var/lib/bcfg2',
           cmd='-Q',
           odesc='<repository path>',
           cf=('server', 'repository'))
SERVER_PLUGINS = \
    Option('Server plugin list',
           # default server plugins
           default=['Bundler', 'Cfg', 'Metadata', 'Pkgmgr', 'Rules',
                    'SSHbase'],
           cf=('server', 'plugins'),
           cook=list_split)
SERVER_FILEMONITOR = \
    Option('Server file monitor',
           default='default',
           odesc='File monitoring driver',
           cf=('server', 'filemonitor'))
SERVER_FAM_IGNORE = \
    Option('File globs to ignore',
           default=['*~', '*#', '.#*', '*.swp', '*.swpx', '.*.swx',
                    'SCCS', '.svn', '4913', '.gitignore'],
           cf=('server', 'ignore_files'),
           cook=list_split)
SERVER_FAM_BLOCK = \
    Option('FAM blocks on startup until all events are processed',
           default=False,
           cook=get_bool,
           cf=('server', 'fam_blocking'))
SERVER_LISTEN_ALL = \
    Option('Listen on all interfaces',
           default=False,
           cmd='--listen-all',
           cf=('server', 'listen_all'),
           cook=get_bool,
           long_arg=True)
SERVER_LOCATION = \
    Option('Server Location',
           default='https://localhost:6789',
           cmd='-S',
           odesc='https://server:port',
           cf=('components', 'bcfg2'))
SERVER_KEY = \
    Option('Path to SSL key',
           default="/etc/pki/tls/private/bcfg2.key",
           cmd='--ssl-key',
           odesc='<ssl key>',
           cf=('communication', 'key'),
           long_arg=True)
SERVER_CERT = \
    Option('Path to SSL certificate',
           default="/etc/pki/tls/certs/bcfg2.crt",
           odesc='<ssl cert>',
           cf=('communication', 'certificate'))
SERVER_CA = \
    Option('Path to SSL CA Cert',
           default=None,
           odesc='<ca cert>',
           cf=('communication', 'ca'))
SERVER_PASSWORD = \
    Option('Communication Password',
           default=None,
           cmd='-x',
           odesc='<password>',
           cf=('communication', 'password'))
SERVER_PROTOCOL = \
    Option('Server Protocol',
           default='xmlrpc/ssl',
           cf=('communication', 'protocol'))
SERVER_BACKEND = \
    Option('Server Backend',
           default='best',
           cf=('server', 'backend'))
SERVER_DAEMON_USER = \
    Option('User to run the server daemon as',
           default=0,
           cf=('server', 'user'),
           cook=get_uid)
SERVER_DAEMON_GROUP = \
    Option('Group to run the server daemon as',
           default=0,
           cf=('server', 'group'),
           cook=get_gid)
SERVER_VCS_ROOT = \
    Option('Server VCS repository root',
           default=None,
           odesc='<VCS repository root>',
           cf=('server', 'vcs_root'))
SERVER_UMASK = \
    Option('Server umask',
           default='0077',
           odesc='<Server umask>',
           cf=('server', 'umask'))
SERVER_AUTHENTICATION = \
    Option('Default client authentication method',
           default='cert+password',
           odesc='{cert|bootstrap|cert+password}',
           cf=('communication', 'authentication'))
SERVER_CHILDREN = \
    Option('Spawn this number of children for the multiprocessing core. '
           'By default spawns children equivalent to the number of processors '
           'in the machine.',
           default=None,
           cmd='--children',
           odesc='<children>',
           cf=('server', 'children'),
           cook=get_int,
           long_arg=True)
SERVER_PROBE_ALLOWED_GROUPS = \
    Option('Whitespace-separated list of group names (as regex) to which '
           'probes can assign a client by writing "group:" to stdout.',
           default=[re.compile('.*')],
           cf=('probes', 'allowed_groups'),
           cook=list_split_anchored_regex)

# database options
DB_ENGINE = \
    Option('Database engine',
           default='sqlite3',
           cf=('database', 'engine'),
           deprecated_cf=('statistics', 'database_engine'))
DB_NAME = \
    Option('Database name',
           default=os.path.join(SERVER_REPOSITORY.default, "etc/bcfg2.sqlite"),
           cf=('database', 'name'),
           deprecated_cf=('statistics', 'database_name'))
DB_USER = \
    Option('Database username',
           default=None,
           cf=('database', 'user'),
           deprecated_cf=('statistics', 'database_user'))
DB_PASSWORD = \
    Option('Database password',
           default=None,
           cf=('database', 'password'),
           deprecated_cf=('statistics', 'database_password'))
DB_HOST = \
    Option('Database host',
           default='localhost',
           cf=('database', 'host'),
           deprecated_cf=('statistics', 'database_host'))
DB_PORT = \
    Option('Database port',
           default='',
           cf=('database', 'port'),
           deprecated_cf=('statistics', 'database_port'))
DB_OPTIONS = \
    Option('Database options',
           default=dict(),
           cf=('database', 'options'),
           cook=dict_split)
DB_SCHEMA = \
    Option('Database schema',
           default='public',
           cf=('database', 'schema'))

# Django options
WEB_CFILE = \
    Option('Web interface configuration file',
           default="/etc/bcfg2-web.conf",
           cmd='-W',
           odesc='<conffile>',
           cf=('reporting', 'config'),
           deprecated_cf=('statistics', 'web_prefix'),)
DJANGO_TIME_ZONE = \
    Option('Django timezone',
           default=None,
           cf=('reporting', 'time_zone'),
           deprecated_cf=('statistics', 'web_prefix'),)
DJANGO_DEBUG = \
    Option('Django debug',
           default=None,
           cf=('reporting', 'web_debug'),
           deprecated_cf=('statistics', 'web_prefix'),
           cook=get_bool,)
DJANGO_WEB_PREFIX = \
    Option('Web prefix',
           default=None,
           cf=('reporting', 'web_prefix'),
           deprecated_cf=('statistics', 'web_prefix'),)

# Reporting options
REPORTING_FILE_LIMIT = \
    Option('Reporting file size limit',
           default=get_size('1m'),
           cf=('reporting', 'file_limit'),
           cook=get_size,)

# Reporting options
REPORTING_TRANSPORT = \
    Option('Reporting transport',
           default='DirectStore',
           cf=('reporting', 'transport'),)

# Client options
CLIENT_KEY = \
    Option('Path to SSL key',
           default=None,
           cmd='--ssl-key',
           odesc='<ssl key>',
           cf=('communication', 'key'),
           long_arg=True)
CLIENT_CERT = \
    Option('Path to SSL certificate',
           default=None,
           cmd='--ssl-cert',
           odesc='<ssl cert>',
           cf=('communication', 'certificate'),
           long_arg=True)
CLIENT_CA = \
    Option('Path to SSL CA Cert',
           default=None,
           cmd='--ca-cert',
           odesc='<ca cert>',
           cf=('communication', 'ca'),
           long_arg=True)
CLIENT_SCNS = \
    Option('List of server commonNames',
           default=None,
           cmd='--ssl-cns',
           odesc='<CN1:CN2>',
           cf=('communication', 'serverCommonNames'),
           cook=list_split,
           long_arg=True)
CLIENT_PROFILE = \
    Option('Assert the given profile for the host',
           default=None,
           cmd='-p',
           odesc='<profile>',
           cf=('client', 'profile'))
CLIENT_RETRIES = \
    Option('The number of times to retry network communication',
           default='3',
           cmd='-R',
           odesc='<retry count>',
           cf=('communication', 'retries'))
CLIENT_RETRY_DELAY = \
    Option('The time in seconds to wait between retries',
           default='1',
           cmd='-y',
           odesc='<retry delay>',
           cf=('communication', 'retry_delay'))
CLIENT_DRYRUN = \
    Option('Do not actually change the system',
           default=False,
           cmd='-n')
CLIENT_EXTRA_DISPLAY = \
    Option('enable extra entry output',
           default=False,
           cmd='-e')
CLIENT_PARANOID = \
    Option('Make automatic backups of config files',
           default=False,
           cmd='-P',
           cf=('client', 'paranoid'),
           cook=get_bool)
CLIENT_DRIVERS = \
    Option('Specify tool driver set',
           default=Bcfg2.Client.Tools.default,
           cmd='-D',
           odesc='<driver1,driver2>',
           cf=('client', 'drivers'),
           cook=list_split)
CLIENT_CACHE = \
    Option('Store the configuration in a file',
           default=None,
           cmd='-c',
           odesc='<cache path>')
CLIENT_REMOVE = \
    Option('Force removal of additional configuration items',
           default=None,
           cmd='-r',
           odesc='<entry type|all>')
CLIENT_BUNDLE = \
    Option('Only configure the given bundle(s)',
           default=[],
           cmd='-b',
           odesc='<bundle:bundle>',
           cook=colon_split)
CLIENT_SKIPBUNDLE = \
    Option('Configure everything except the given bundle(s)',
           default=[],
           cmd='-B',
           odesc='<bundle:bundle>',
           cook=colon_split)
CLIENT_BUNDLEQUICK = \
    Option('Only verify/configure the given bundle(s)',
           default=False,
           cmd='-Q')
CLIENT_INDEP = \
    Option('Only configure independent entries, ignore bundles',
           default=False,
           cmd='-z')
CLIENT_SKIPINDEP = \
    Option('Do not configure independent entries',
           default=False,
           cmd='-Z')
CLIENT_KEVLAR = \
    Option('Run in kevlar (bulletproof) mode',
           default=False,
           cmd='-k', )
CLIENT_FILE = \
    Option('Configure from a file rather than querying the server',
           default=None,
           cmd='-f',
           odesc='<specification path>')
CLIENT_QUICK = \
    Option('Disable some checksum verification',
           default=False,
           cmd='-q')
CLIENT_USER = \
    Option('The user to provide for authentication',
           default='root',
           cmd='-u',
           odesc='<user>',
           cf=('communication', 'user'))
CLIENT_SERVICE_MODE = \
    Option('Set client service mode',
           default='default',
           cmd='-s',
           odesc='<default|disabled|build>')
CLIENT_TIMEOUT = \
    Option('Set the client XML-RPC timeout',
           default=90,
           cmd='-t',
           odesc='<timeout>',
           cf=('communication', 'timeout'))
CLIENT_DLIST = \
    Option('Run client in server decision list mode',
           default='none',
           cmd='-l',
           odesc='<whitelist|blacklist|none>',
           cf=('client', 'decision'))
CLIENT_DECISION_LIST = \
    Option('Decision List',
           default=False,
           cmd='--decision-list',
           odesc='<file>',
           long_arg=True)
CLIENT_EXIT_ON_PROBE_FAILURE = \
    Option("The client should exit if a probe fails",
           default=True,
           cmd='--exit-on-probe-failure',
           long_arg=True,
           cf=('client', 'exit_on_probe_failure'),
           cook=get_bool)
CLIENT_PROBE_TIMEOUT = \
    Option("Timeout when running client probes",
           default=None,
           cf=('client', 'probe_timeout'),
           cook=get_timeout)
CLIENT_COMMAND_TIMEOUT = \
    Option("Timeout when client runs other external commands (not probes)",
           default=None,
           cf=('client', 'command_timeout'),
           cook=get_timeout)

# bcfg2-test and bcfg2-lint options
TEST_NOSEOPTS = \
    Option('Options to pass to nosetests. Only honored with --children 0',
           default=[],
           cmd='--nose-options',
           odesc='<opts>',
           cf=('bcfg2_test', 'nose_options'),
           cook=shlex.split,
           long_arg=True)
TEST_IGNORE = \
    Option('Ignore these entries if they fail to build.',
           default=[],
           cmd='--ignore',
           odesc='<Type>:<name>,<Type>:<name>',
           cf=('bcfg2_test', 'ignore_entries'),
           cook=list_split,
           long_arg=True)
TEST_CHILDREN = \
    Option('Spawn this number of children for bcfg2-test (python 2.6+)',
           default=0,
           cmd='--children',
           odesc='<children>',
           cf=('bcfg2_test', 'children'),
           cook=get_int,
           long_arg=True)
TEST_XUNIT = \
    Option('Output an XUnit result file with --children',
           default=None,
           cmd='--xunit',
           odesc='<xunit file>',
           cf=('bcfg2_test', 'xunit'),
           long_arg=True)
LINT_CONFIG = \
    Option('Specify bcfg2-lint configuration file',
           default='/etc/bcfg2-lint.conf',
           cmd='--lint-config',
           odesc='<conffile>',
           long_arg=True)
LINT_PLUGINS = \
    Option('bcfg2-lint plugin list',
           default=None,  # default is Bcfg2.Server.Lint.__all__
           cf=('lint', 'plugins'),
           cook=list_split)
LINT_SHOW_ERRORS = \
    Option('Show error handling',
           default=False,
           cmd='--list-errors',
           long_arg=True)
LINT_FILES_ON_STDIN = \
    Option('Operate on a list of files supplied on stdin',
           default=False,
           cmd='--stdin',
           long_arg=True)

# individual client tool options
CLIENT_APT_TOOLS_INSTALL_PATH = \
    Option('Apt tools install path',
           default='/usr',
           cf=('APT', 'install_path'))
CLIENT_APT_TOOLS_VAR_PATH = \
    Option('Apt tools var path',
           default='/var',
           cf=('APT', 'var_path'))
CLIENT_SYSTEM_ETC_PATH = \
    Option('System etc path',
           default='/etc',
           cf=('APT', 'etc_path'))
CLIENT_PORTAGE_BINPKGONLY = \
    Option('Portage binary packages only',
           default=False,
           cf=('Portage', 'binpkgonly'),
           cook=get_bool)
CLIENT_RPM_INSTALLONLY = \
    Option('RPM install-only packages',
           default=['kernel', 'kernel-bigmem', 'kernel-enterprise',
                    'kernel-smp', 'kernel-modules', 'kernel-debug',
                    'kernel-unsupported', 'kernel-devel', 'kernel-source',
                    'kernel-default', 'kernel-largesmp-devel',
                    'kernel-largesmp', 'kernel-xen', 'gpg-pubkey'],
           cf=('RPM', 'installonlypackages'),
           deprecated_cf=('RPMng', 'installonlypackages'),
           cook=list_split)
CLIENT_RPM_PKG_CHECKS = \
    Option("Perform RPM package checks",
           default=True,
           cf=('RPM', 'pkg_checks'),
           deprecated_cf=('RPMng', 'pkg_checks'),
           cook=get_bool)
CLIENT_RPM_PKG_VERIFY = \
    Option("Perform RPM package verify",
           default=True,
           cf=('RPM', 'pkg_verify'),
           deprecated_cf=('RPMng', 'pkg_verify'),
           cook=get_bool)
CLIENT_RPM_INSTALLED_ACTION = \
    Option("RPM installed action",
           default="install",
           cf=('RPM', 'installed_action'),
           deprecated_cf=('RPMng', 'installed_action'))
CLIENT_RPM_ERASE_FLAGS = \
    Option("RPM erase flags",
           default=["allmatches"],
           cf=('RPM', 'erase_flags'),
           deprecated_cf=('RPMng', 'erase_flags'),
           cook=list_split)
CLIENT_RPM_VERSION_FAIL_ACTION = \
    Option("RPM version fail action",
           default="upgrade",
           cf=('RPM', 'version_fail_action'),
           deprecated_cf=('RPMng', 'version_fail_action'))
CLIENT_RPM_VERIFY_FAIL_ACTION = \
    Option("RPM verify fail action",
           default="reinstall",
           cf=('RPM', 'verify_fail_action'),
           deprecated_cf=('RPMng', 'verify_fail_action'))
CLIENT_RPM_VERIFY_FLAGS = \
    Option("RPM verify flags",
           default=[],
           cf=('RPM', 'verify_flags'),
           deprecated_cf=('RPMng', 'verify_flags'),
           cook=list_split)
CLIENT_YUM24_INSTALLONLY = \
    Option('YUM24 install-only packages',
           default=['kernel', 'kernel-bigmem', 'kernel-enterprise',
                    'kernel-smp', 'kernel-modules', 'kernel-debug',
                    'kernel-unsupported', 'kernel-devel', 'kernel-source',
                    'kernel-default', 'kernel-largesmp-devel',
                    'kernel-largesmp', 'kernel-xen', 'gpg-pubkey'],
           cf=('YUM24', 'installonlypackages'),
           cook=list_split)
CLIENT_YUM24_PKG_CHECKS = \
    Option("Perform YUM24 package checks",
           default=True,
           cf=('YUM24', 'pkg_checks'),
           cook=get_bool)
CLIENT_YUM24_PKG_VERIFY = \
    Option("Perform YUM24 package verify",
           default=True,
           cf=('YUM24', 'pkg_verify'),
           cook=get_bool)
CLIENT_YUM24_INSTALLED_ACTION = \
    Option("YUM24 installed action",
           default="install",
           cf=('YUM24', 'installed_action'))
CLIENT_YUM24_ERASE_FLAGS = \
    Option("YUM24 erase flags",
           default=["allmatches"],
           cf=('YUM24', 'erase_flags'),
           cook=list_split)
CLIENT_YUM24_VERSION_FAIL_ACTION = \
    Option("YUM24 version fail action",
           cf=('YUM24', 'version_fail_action'),
           default="upgrade")
CLIENT_YUM24_VERIFY_FAIL_ACTION = \
    Option("YUM24 verify fail action",
           default="reinstall",
           cf=('YUM24', 'verify_fail_action'))
CLIENT_YUM24_VERIFY_FLAGS = \
    Option("YUM24 verify flags",
           default=[],
           cf=('YUM24', 'verify_flags'),
           cook=list_split)
CLIENT_YUM24_AUTODEP = \
    Option("YUM24 autodependency processing",
           default=True,
           cf=('YUM24', 'autodep'),
           cook=get_bool)
CLIENT_YUM_PKG_CHECKS = \
    Option("Perform YUM package checks",
           default=True,
           cf=('YUM', 'pkg_checks'),
           deprecated_cf=('YUMng', 'pkg_checks'),
           cook=get_bool)
CLIENT_YUM_PKG_VERIFY = \
    Option("Perform YUM package verify",
           default=True,
           cf=('YUM', 'pkg_verify'),
           deprecated_cf=('YUMng', 'pkg_verify'),
           cook=get_bool)
CLIENT_YUM_INSTALLED_ACTION = \
    Option("YUM installed action",
           default="install",
           cf=('YUM', 'installed_action'),
           deprecated_cf=('YUMng', 'installed_action'))
CLIENT_YUM_VERSION_FAIL_ACTION = \
    Option("YUM version fail action",
           default="upgrade",
           cf=('YUM', 'version_fail_action'),
           deprecated_cf=('YUMng', 'version_fail_action'))
CLIENT_YUM_VERIFY_FAIL_ACTION = \
    Option("YUM verify fail action",
           default="reinstall",
           cf=('YUM', 'verify_fail_action'),
           deprecated_cf=('YUMng', 'verify_fail_action'))
CLIENT_YUM_VERIFY_FLAGS = \
    Option("YUM verify flags",
           default=[],
           cf=('YUM', 'verify_flags'),
           deprecated_cf=('YUMng', 'verify_flags'),
           cook=list_split)
CLIENT_POSIX_UID_WHITELIST = \
    Option("UID ranges the POSIXUsers tool will manage",
           default=[],
           cf=('POSIXUsers', 'uid_whitelist'),
           cook=list_split)
CLIENT_POSIX_GID_WHITELIST = \
    Option("GID ranges the POSIXUsers tool will manage",
           default=[],
           cf=('POSIXUsers', 'gid_whitelist'),
           cook=list_split)
CLIENT_POSIX_UID_BLACKLIST = \
    Option("UID ranges the POSIXUsers tool will not manage",
           default=[],
           cf=('POSIXUsers', 'uid_blacklist'),
           cook=list_split)
CLIENT_POSIX_GID_BLACKLIST = \
    Option("GID ranges the POSIXUsers tool will not manage",
           default=[],
           cf=('POSIXUsers', 'gid_blacklist'),
           cook=list_split)

# Logging options
LOGGING_FILE_PATH = \
    Option('Set path of file log',
           default=None,
           cmd='-o',
           odesc='<path>',
           cf=('logging', 'path'))
LOGGING_SYSLOG = \
    Option('Log to syslog',
           default=True,
           cook=get_bool,
           cf=('logging', 'syslog'))
DEBUG = \
    Option("Enable debugging output",
           default=False,
           cmd='-d',
           cook=get_bool,
           cf=('logging', 'debug'))
VERBOSE = \
    Option("Enable verbose output",
           default=False,
           cmd='-v',
           cook=get_bool,
           cf=('logging', 'verbose'))
LOG_PERFORMANCE = \
    Option("Periodically log performance statistics",
           default=False,
           cf=('logging', 'performance'))
PERFLOG_INTERVAL = \
    Option("Performance statistics logging interval in seconds",
           default=300.0,
           cook=get_timeout,
           cf=('logging', 'performance_interval'))

# Plugin-specific options
CFG_VALIDATION = \
    Option('Run validation on Cfg files',
           default=True,
           cmd='--cfg-validation',
           cf=('cfg', 'validation'),
           long_arg=True,
           cook=get_bool)

# bcfg2-crypt options
ENCRYPT = \
    Option('Encrypt the specified file',
           default=False,
           cmd='--encrypt',
           long_arg=True)
DECRYPT = \
    Option('Decrypt the specified file',
           default=False,
           cmd='--decrypt',
           long_arg=True)
CRYPT_STDOUT = \
    Option('Decrypt or encrypt the specified file to stdout',
           default=False,
           cmd='--stdout',
           long_arg=True)
CRYPT_PASSPHRASE = \
    Option('Encryption passphrase name',
           default=None,
           cmd='-p',
           odesc='<passphrase>')
CRYPT_XPATH = \
    Option('XPath expression to select elements to encrypt',
           default=None,
           cmd='--xpath',
           odesc='<xpath>',
           long_arg=True)
CRYPT_PROPERTIES = \
    Option('Encrypt the specified file as a Properties file',
           default=False,
           cmd="--properties",
           long_arg=True)
CRYPT_CFG = \
    Option('Encrypt the specified file as a Cfg file',
           default=False,
           cmd="--cfg",
           long_arg=True)
CRYPT_REMOVE = \
    Option('Remove the plaintext file after encrypting',
           default=False,
           cmd="--remove",
           long_arg=True)

# Option groups
CLI_COMMON_OPTIONS = dict(configfile=CFILE,
                          debug=DEBUG,
                          help=HELP,
                          version=VERSION,
                          verbose=VERBOSE,
                          encoding=ENCODING,
                          logging=LOGGING_FILE_PATH,
                          syslog=LOGGING_SYSLOG)

DAEMON_COMMON_OPTIONS = dict(daemon=DAEMON,
                             umask=SERVER_UMASK,
                             listen_all=SERVER_LISTEN_ALL,
                             daemon_uid=SERVER_DAEMON_USER,
                             daemon_gid=SERVER_DAEMON_GROUP)

SERVER_COMMON_OPTIONS = dict(repo=SERVER_REPOSITORY,
                             plugins=SERVER_PLUGINS,
                             password=SERVER_PASSWORD,
                             filemonitor=SERVER_FILEMONITOR,
                             ignore=SERVER_FAM_IGNORE,
                             fam_blocking=SERVER_FAM_BLOCK,
                             location=SERVER_LOCATION,
                             key=SERVER_KEY,
                             cert=SERVER_CERT,
                             ca=SERVER_CA,
                             protocol=SERVER_PROTOCOL,
                             web_configfile=WEB_CFILE,
                             backend=SERVER_BACKEND,
                             vcs_root=SERVER_VCS_ROOT,
                             authentication=SERVER_AUTHENTICATION,
                             perflog=LOG_PERFORMANCE,
                             perflog_interval=PERFLOG_INTERVAL,
                             children=SERVER_CHILDREN,
                             client_timeout=CLIENT_TIMEOUT,
                             probe_allowed_groups=SERVER_PROBE_ALLOWED_GROUPS)

CRYPT_OPTIONS = dict(encrypt=ENCRYPT,
                     decrypt=DECRYPT,
                     crypt_stdout=CRYPT_STDOUT,
                     passphrase=CRYPT_PASSPHRASE,
                     xpath=CRYPT_XPATH,
                     properties=CRYPT_PROPERTIES,
                     cfg=CRYPT_CFG,
                     remove=CRYPT_REMOVE)

DRIVER_OPTIONS = \
    dict(apt_install_path=CLIENT_APT_TOOLS_INSTALL_PATH,
         apt_var_path=CLIENT_APT_TOOLS_VAR_PATH,
         apt_etc_path=CLIENT_SYSTEM_ETC_PATH,
         portage_binpkgonly=CLIENT_PORTAGE_BINPKGONLY,
         rpm_installonly=CLIENT_RPM_INSTALLONLY,
         rpm_pkg_checks=CLIENT_RPM_PKG_CHECKS,
         rpm_pkg_verify=CLIENT_RPM_PKG_VERIFY,
         rpm_installed_action=CLIENT_RPM_INSTALLED_ACTION,
         rpm_erase_flags=CLIENT_RPM_ERASE_FLAGS,
         rpm_version_fail_action=CLIENT_RPM_VERSION_FAIL_ACTION,
         rpm_verify_fail_action=CLIENT_RPM_VERIFY_FAIL_ACTION,
         rpm_verify_flags=CLIENT_RPM_VERIFY_FLAGS,
         yum24_installonly=CLIENT_YUM24_INSTALLONLY,
         yum24_pkg_checks=CLIENT_YUM24_PKG_CHECKS,
         yum24_pkg_verify=CLIENT_YUM24_PKG_VERIFY,
         yum24_installed_action=CLIENT_YUM24_INSTALLED_ACTION,
         yum24_erase_flags=CLIENT_YUM24_ERASE_FLAGS,
         yum24_version_fail_action=CLIENT_YUM24_VERSION_FAIL_ACTION,
         yum24_verify_fail_action=CLIENT_YUM24_VERIFY_FAIL_ACTION,
         yum24_verify_flags=CLIENT_YUM24_VERIFY_FLAGS,
         yum24_autodep=CLIENT_YUM24_AUTODEP,
         yum_pkg_checks=CLIENT_YUM_PKG_CHECKS,
         yum_pkg_verify=CLIENT_YUM_PKG_VERIFY,
         yum_installed_action=CLIENT_YUM_INSTALLED_ACTION,
         yum_version_fail_action=CLIENT_YUM_VERSION_FAIL_ACTION,
         yum_verify_fail_action=CLIENT_YUM_VERIFY_FAIL_ACTION,
         yum_verify_flags=CLIENT_YUM_VERIFY_FLAGS,
         posix_uid_whitelist=CLIENT_POSIX_UID_WHITELIST,
         posix_gid_whitelist=CLIENT_POSIX_GID_WHITELIST,
         posix_uid_blacklist=CLIENT_POSIX_UID_BLACKLIST,
         posix_gid_blacklist=CLIENT_POSIX_GID_BLACKLIST)

CLIENT_COMMON_OPTIONS = \
    dict(extra=CLIENT_EXTRA_DISPLAY,
         quick=CLIENT_QUICK,
         lockfile=LOCKFILE,
         drivers=CLIENT_DRIVERS,
         dryrun=CLIENT_DRYRUN,
         paranoid=CLIENT_PARANOID,
         ppath=PARANOID_PATH,
         max_copies=PARANOID_MAX_COPIES,
         bundle=CLIENT_BUNDLE,
         skipbundle=CLIENT_SKIPBUNDLE,
         bundle_quick=CLIENT_BUNDLEQUICK,
         indep=CLIENT_INDEP,
         skipindep=CLIENT_SKIPINDEP,
         file=CLIENT_FILE,
         interactive=INTERACTIVE,
         cache=CLIENT_CACHE,
         profile=CLIENT_PROFILE,
         remove=CLIENT_REMOVE,
         server=SERVER_LOCATION,
         user=CLIENT_USER,
         password=SERVER_PASSWORD,
         retries=CLIENT_RETRIES,
         retry_delay=CLIENT_RETRY_DELAY,
         kevlar=CLIENT_KEVLAR,
         omit_lock_check=OMIT_LOCK_CHECK,
         decision=CLIENT_DLIST,
         servicemode=CLIENT_SERVICE_MODE,
         key=CLIENT_KEY,
         certificate=CLIENT_CERT,
         ca=CLIENT_CA,
         serverCN=CLIENT_SCNS,
         timeout=CLIENT_TIMEOUT,
         decision_list=CLIENT_DECISION_LIST,
         probe_exit=CLIENT_EXIT_ON_PROBE_FAILURE,
         probe_timeout=CLIENT_PROBE_TIMEOUT,
         command_timeout=CLIENT_COMMAND_TIMEOUT)
CLIENT_COMMON_OPTIONS.update(DRIVER_OPTIONS)
CLIENT_COMMON_OPTIONS.update(CLI_COMMON_OPTIONS)

DATABASE_COMMON_OPTIONS = dict(web_configfile=WEB_CFILE,
                               configfile=CFILE,
                               db_engine=DB_ENGINE,
                               db_name=DB_NAME,
                               db_user=DB_USER,
                               db_password=DB_PASSWORD,
                               db_host=DB_HOST,
                               db_port=DB_PORT,
                               db_options=DB_OPTIONS,
                               db_schema=DB_SCHEMA,
                               time_zone=DJANGO_TIME_ZONE,
                               django_debug=DJANGO_DEBUG,
                               web_prefix=DJANGO_WEB_PREFIX)

REPORTING_COMMON_OPTIONS = dict(reporting_file_limit=REPORTING_FILE_LIMIT,
                                reporting_transport=REPORTING_TRANSPORT)

TEST_COMMON_OPTIONS = dict(noseopts=TEST_NOSEOPTS,
                           test_ignore=TEST_IGNORE,
                           children=TEST_CHILDREN,
                           xunit=TEST_XUNIT,
                           validate=CFG_VALIDATION)

INFO_COMMON_OPTIONS = dict(ppath=PARANOID_PATH,
                           max_copies=PARANOID_MAX_COPIES)
INFO_COMMON_OPTIONS.update(CLI_COMMON_OPTIONS)
INFO_COMMON_OPTIONS.update(SERVER_COMMON_OPTIONS)


class OptionParser(OptionSet):
    """
       OptionParser bootstraps option parsing,
       getting the value of the config file
    """
    def __init__(self, args, argv=None, quiet=False):
        if argv is None:
            argv = sys.argv[1:]
        # the bootstrap is always quiet, since it's running with a
        # default config file and so might produce warnings otherwise
        self.bootstrap = OptionSet([('configfile', CFILE)], quiet=True)
        self.bootstrap.parse(argv, do_getopt=False)
        OptionSet.__init__(self, args, configfile=self.bootstrap['configfile'],
                           quiet=quiet)
        self.optinfo = copy.copy(args)
        # these will be set by parse() and then used by reparse()
        self.argv = []
        self.do_getopt = True

    def reparse(self):
        """ parse the options again, taking any changes (e.g., to the
        config file) into account """
        for key, opt in self.optinfo.items():
            self[key] = opt
        if "args" not in self.optinfo:
            del self['args']
        self.parse(self.argv, self.do_getopt)

    def parse(self, argv, do_getopt=True):
        self.argv = argv
        self.do_getopt = do_getopt
        OptionSet.parse(self, self.argv, do_getopt=self.do_getopt)

    def add_option(self, name, opt):
        """ Add an option to the parser """
        self[name] = opt
        self.optinfo[name] = opt

    def update(self, optdict):
        dict.update(self, optdict)
        self.optinfo.update(optdict)

########NEW FILE########
__FILENAME__ = Proxy
import re
import socket
import logging

# The ssl module is provided by either Python 2.6 or a separate ssl
# package that works on older versions of Python (see
# http://pypi.python.org/pypi/ssl).  If neither can be found, look for
# M2Crypto instead.
try:
    import ssl
    SSL_LIB = 'py26_ssl'
    SSL_ERROR = ssl.SSLError
except ImportError:
    from M2Crypto import SSL
    import M2Crypto.SSL.Checker
    SSL_LIB = 'm2crypto'
    SSL_ERROR = SSL.SSLError

import sys
import time

# Compatibility imports
from Bcfg2.Compat import httplib, xmlrpclib, urlparse, quote_plus

version = sys.version_info[:2]
has_py26 = version >= (2, 6)
has_py32 = version >= (3, 2)

__all__ = ["ComponentProxy",
           "RetryMethod",
           "SSLHTTPConnection",
           "XMLRPCTransport"]


class ProxyError(Exception):
    """ ProxyError provides a consistent reporting interface to
    the various xmlrpclib errors that might arise (mainly
    ProtocolError and Fault) """
    def __init__(self, err):
        msg = None
        if isinstance(err, xmlrpclib.ProtocolError):
            # cut out the password in the URL
            url = re.sub(r'([^:]+):(.*?)@([^@]+:\d+/)', r'\1:******@\3',
                         err.url)
            msg = "XML-RPC Protocol Error for %s: %s (%s)" % (url,
                                                              err.errmsg,
                                                              err.errcode)
        elif isinstance(err, xmlrpclib.Fault):
            msg = "XML-RPC Fault: %s (%s)" % (err.faultString,
                                              err.faultCode)
        else:
            msg = str(err)
        Exception.__init__(self, msg)


class CertificateError(Exception):
    def __init__(self, commonName):
        self.commonName = commonName

    def __str__(self):
        return ("Got unallowed commonName %s from server"
                % self.commonName)


_orig_Method = xmlrpclib._Method

class RetryMethod(xmlrpclib._Method):
    """Method with error handling and retries built in."""
    log = logging.getLogger('xmlrpc')
    max_retries = 3
    retry_delay = 1

    def __call__(self, *args):
        for retry in range(self.max_retries):
            if retry >= self.max_retries - 1:
                final = True
            else:
                final = False
            msg = None
            try:
                return _orig_Method.__call__(self, *args)
            except xmlrpclib.ProtocolError:
                err = sys.exc_info()[1]
                msg = "Server failure: Protocol Error: %s %s" % \
                    (err.errcode, err.errmsg)
            except xmlrpclib.Fault:
                msg = sys.exc_info()[1]
            except socket.error:
                err = sys.exc_info()[1]
                if hasattr(err, 'errno') and err.errno == 336265218:
                    msg = "SSL Key error: %s" % err
                elif hasattr(err, 'errno') and err.errno == 185090050:
                    msg = "SSL CA error: %s" % err
                elif final:
                    msg = "Server failure: %s" % err
            except CertificateError:
                err = sys.exc_info()[1]
                msg = "Got unallowed commonName %s from server" % \
                    err.commonName
            except KeyError:
                err = sys.exc_info()[1]
                msg = "Server disallowed connection: %s" % err
            except ProxyError:
                err = sys.exc_info()[1]
                msg = err
            except:
                etype, err = sys.exc_info()[:2]
                msg = "Unknown failure: %s (%s)" % (err, etype.__name__)
            if msg:
                if final:
                    self.log.error(msg)
                    raise ProxyError(msg)
                else:
                    self.log.info(msg)
                    time.sleep(self.retry_delay)

xmlrpclib._Method = RetryMethod


class SSLHTTPConnection(httplib.HTTPConnection):
    """Extension of HTTPConnection that
    implements SSL and related behaviors.
    """

    logger = logging.getLogger('Bcfg2.Proxy.SSLHTTPConnection')

    def __init__(self, host, port=None, strict=None, timeout=90, key=None,
                 cert=None, ca=None, scns=None, protocol='xmlrpc/ssl'):
        """Initializes the `httplib.HTTPConnection` object and stores security
        parameters

        Parameters
        ----------
        host : string
            Name of host to contact
        port : int, optional
            Port on which to contact the host.  If none is specified,
            the default port of 80 will be used unless the `host`
            string has a port embedded in the form host:port.
        strict : Boolean, optional
            Passed to the `httplib.HTTPConnection` constructor and if
            True, causes the `BadStatusLine` exception to be raised if
            the status line cannot be parsed as a valid HTTP 1.0 or
            1.1 status.
        timeout : int, optional
            Causes blocking operations to timeout after `timeout`
            seconds.
        key : string, optional
            The file system path to the local endpoint's SSL key.  May
            specify the same file as `cert` if using a file that
            contains both.  See
            http://docs.python.org/library/ssl.html#ssl-certificates
            for details.  Required if using xmlrpc/ssl with client
            certificate authentication.
        cert : string, optional
            The file system path to the local endpoint's SSL
            certificate.  May specify the same file as `cert` if using
            a file that contains both.  See
            http://docs.python.org/library/ssl.html#ssl-certificates
            for details.  Required if using xmlrpc/ssl with client
            certificate authentication.
        ca : string, optional
            The file system path to a set of concatenated certificate
            authority certs, which are used to validate certificates
            passed from the other end of the connection.
        scns : array-like, optional
            List of acceptable server commonNames.  The peer cert's
            common name must appear in this list, otherwise the
            connect() call will throw a `CertificateError`.
        protocol : {'xmlrpc/ssl', 'xmlrpc/tlsv1'}, optional
            Communication protocol to use.

        """
        if not has_py26:
            httplib.HTTPConnection.__init__(self, host, port, strict)
        elif not has_py32:
            httplib.HTTPConnection.__init__(self, host, port, strict, timeout)
        else:
            # the strict parameter is deprecated.
            # HTTP 0.9-style "Simple Responses" are not supported anymore.
            httplib.HTTPConnection.__init__(self, host, port, timeout=timeout)
        self.key = key
        self.cert = cert
        self.ca = ca
        self.scns = scns
        self.protocol = protocol
        self.timeout = timeout

    def connect(self):
        """Initiates a connection using previously set attributes."""
        if SSL_LIB == 'py26_ssl':
            self._connect_py26ssl()
        elif SSL_LIB == 'm2crypto':
            self._connect_m2crypto()
        else:
            raise Exception("No SSL module support")

    def _connect_py26ssl(self):
        """Initiates a connection using the ssl module."""
        # check for IPv6
        hostip = socket.getaddrinfo(self.host,
                                    self.port,
                                    socket.AF_UNSPEC,
                                    socket.SOCK_STREAM)[0][4][0]
        if ':' in hostip:
            rawsock = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)
        else:
            rawsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        if self.protocol == 'xmlrpc/ssl':
            ssl_protocol_ver = ssl.PROTOCOL_SSLv23
        elif self.protocol == 'xmlrpc/tlsv1':
            ssl_protocol_ver = ssl.PROTOCOL_TLSv1
        else:
            self.logger.error("Unknown protocol %s" % (self.protocol))
            raise Exception("unknown protocol %s" % self.protocol)
        if self.ca:
            other_side_required = ssl.CERT_REQUIRED
        else:
            other_side_required = ssl.CERT_NONE
            self.logger.warning("No ca is specified. Cannot authenticate the server with SSL.")
        if self.cert and not self.key:
            self.logger.warning("SSL cert specfied, but no key. Cannot authenticate this client with SSL.")
            self.cert = None
        if self.key and not self.cert:
            self.logger.warning("SSL key specfied, but no cert. Cannot authenticate this client with SSL.")
            self.key = None

        rawsock.settimeout(self.timeout)
        self.sock = ssl.SSLSocket(rawsock, cert_reqs=other_side_required,
                                  ca_certs=self.ca, suppress_ragged_eofs=True,
                                  keyfile=self.key, certfile=self.cert,
                                  ssl_version=ssl_protocol_ver)
        self.sock.connect((self.host, self.port))
        peer_cert = self.sock.getpeercert()
        if peer_cert and self.scns:
            scn = [x[0][1] for x in peer_cert['subject'] if x[0][0] == 'commonName'][0]
            if scn not in self.scns:
                raise CertificateError(scn)
        self.sock.closeSocket = True

    def _connect_m2crypto(self):
        """Initiates a connection using the M2Crypto module."""

        if self.protocol == 'xmlrpc/ssl':
            ctx = SSL.Context('sslv23')
        elif self.protocol == 'xmlrpc/tlsv1':
            ctx = SSL.Context('tlsv1')
        else:
            self.logger.error("Unknown protocol %s" % (self.protocol))
            raise Exception("unknown protocol %s" % self.protocol)

        if self.ca:
            # Use the certificate authority to validate the cert
            # presented by the server
            ctx.set_verify(SSL.verify_peer | SSL.verify_fail_if_no_peer_cert, depth=9)
            if ctx.load_verify_locations(self.ca) != 1:
                raise Exception('No CA certs')
        else:
            self.logger.warning("No ca is specified. Cannot authenticate the server with SSL.")

        if self.cert and self.key:
            # A cert/key is defined, use them to support client
            # authentication to the server
            ctx.load_cert(self.cert, self.key)
        elif self.cert:
            self.logger.warning("SSL cert specfied, but no key. Cannot authenticate this client with SSL.")
        elif self.key:
            self.logger.warning("SSL key specfied, but no cert. Cannot authenticate this client with SSL.")

        self.sock = SSL.Connection(ctx)
        if re.match('\\d+\\.\\d+\\.\\d+\\.\\d+', self.host):
            # host is ip address
            try:
                hostname = socket.gethostbyaddr(self.host)[0]
            except:
                # fall back to ip address
                hostname = self.host
        else:
            hostname = self.host
        try:
            self.sock.connect((hostname, self.port))
            # automatically checks cert matches host
        except M2Crypto.SSL.Checker.WrongHost:
            wr = sys.exc_info()[1]
            raise CertificateError(wr)


class XMLRPCTransport(xmlrpclib.Transport):
    def __init__(self, key=None, cert=None, ca=None,
                 scns=None, use_datetime=0, timeout=90):
        if hasattr(xmlrpclib.Transport, '__init__'):
            xmlrpclib.Transport.__init__(self, use_datetime)
        self.key = key
        self.cert = cert
        self.ca = ca
        self.scns = scns
        self.timeout = timeout

    def make_connection(self, host):
        host, self._extra_headers = self.get_host_info(host)[0:2]
        return SSLHTTPConnection(host,
                                 key=self.key,
                                 cert=self.cert,
                                 ca=self.ca,
                                 scns=self.scns,
                                 timeout=self.timeout)

    def request(self, host, handler, request_body, verbose=0):
        """Send request to server and return response."""
        try:
            conn = self.send_request(host, handler, request_body, False)
            response = conn.getresponse()
            errcode = response.status
            errmsg = response.reason
            headers = response.msg
        except (socket.error, SSL_ERROR, httplib.BadStatusLine):
            err = sys.exc_info()[1]
            raise ProxyError(xmlrpclib.ProtocolError(host + handler,
                                                     408,
                                                     str(err),
                                                     self._extra_headers))

        if errcode != 200:
            raise ProxyError(xmlrpclib.ProtocolError(host + handler,
                                                     errcode,
                                                     errmsg,
                                                     headers))

        self.verbose = verbose
        return self.parse_response(response)

    if sys.hexversion < 0x03000000:
        # pylint: disable=E1101
        def send_request(self, host, handler, request_body, debug):
            """ send_request() changed significantly in py3k."""
            conn = self.make_connection(host)
            xmlrpclib.Transport.send_request(self, conn, handler, request_body)
            self.send_host(conn, host)
            self.send_user_agent(conn)
            self.send_content(conn, request_body)
            return conn
        # pylint: enable=E1101


def ComponentProxy(url, user=None, password=None, key=None, cert=None, ca=None,
                   allowedServerCNs=None, timeout=90, retries=3, delay=1):

    """Constructs proxies to components.

    Arguments:
    component_name -- name of the component to connect to

    Additional arguments are passed to the ServerProxy constructor.

    """
    xmlrpclib._Method.max_retries = retries
    xmlrpclib._Method.retry_delay = delay

    if user and password:
        method, path = urlparse(url)[:2]
        newurl = "%s://%s:%s@%s" % (method, quote_plus(user, ''),
                                    quote_plus(password, ''), path)
    else:
        newurl = url
    ssl_trans = XMLRPCTransport(key, cert, ca,
                                allowedServerCNs, timeout=float(timeout))
    return xmlrpclib.ServerProxy(newurl, allow_none=True, transport=ssl_trans)

########NEW FILE########
__FILENAME__ = Collector
import sys
import atexit
import daemon
import logging
import time
import traceback
import threading

# pylint: disable=E0611
from lockfile import LockFailed, LockTimeout
try:
    from lockfile.pidlockfile import PIDLockFile
    from lockfile import Error as PIDFileError
except ImportError:
    from daemon.pidlockfile import PIDLockFile, PIDFileError
# pylint: enable=E0611

import Bcfg2.Logger
from Bcfg2.Reporting.Transport import load_transport_from_config, \
    TransportError, TransportImportError
from Bcfg2.Reporting.Transport.DirectStore import DirectStore
from Bcfg2.Reporting.Storage import load_storage_from_config, \
    StorageError, StorageImportError


class ReportingError(Exception):
    """Generic reporting exception"""
    pass


class ReportingStoreThread(threading.Thread):
    """Thread for calling the storage backend"""
    def __init__(self, interaction, storage, group=None, target=None,
                 name=None, args=(), kwargs=None):
        """Initialize the thread with a reference to the interaction
        as well as the storage engine to use"""
        threading.Thread.__init__(self, group, target, name, args,
                                  kwargs or dict())
        self.interaction = interaction
        self.storage = storage
        self.logger = logging.getLogger('bcfg2-report-collector')

    def run(self):
        """Call the database storage procedure (aka import)"""
        try:
            start = time.time()
            self.storage.import_interaction(self.interaction)
            self.logger.info("Imported interaction for %s in %ss" %
                             (self.interaction.get('hostname', '<unknown>'),
                              time.time() - start))
        except:
            #TODO requeue?
            self.logger.error("Unhandled exception in import thread %s" %
                              traceback.format_exc().splitlines()[-1])


class ReportingCollector(object):
    """The collecting process for reports"""

    def __init__(self, setup):
        """Setup the collector.  This may be called by the daemon or though
        bcfg2-admin"""
        self.setup = setup
        self.datastore = setup['repo']
        self.encoding = setup['encoding']
        self.terminate = None
        self.context = None
        self.children = []
        self.cleanup_threshold = 25

        if setup['debug']:
            level = logging.DEBUG
        elif setup['verbose']:
            level = logging.INFO
        else:
            level = logging.WARNING

        Bcfg2.Logger.setup_logging('bcfg2-report-collector',
                                   to_console=logging.INFO,
                                   to_syslog=setup['syslog'],
                                   to_file=setup['logging'],
                                   level=level)
        self.logger = logging.getLogger('bcfg2-report-collector')

        try:
            self.transport = load_transport_from_config(setup)
            self.storage = load_storage_from_config(setup)
        except TransportError:
            self.logger.error("Failed to load transport: %s" %
                traceback.format_exc().splitlines()[-1])
            raise ReportingError
        except StorageError:
            self.logger.error("Failed to load storage: %s" %
                traceback.format_exc().splitlines()[-1])
            raise ReportingError

        if isinstance(self.transport, DirectStore):
            self.logger.error("DirectStore cannot be used with the collector. "
                              "Use LocalFilesystem instead")
            self.shutdown()
            raise ReportingError

        try:
            self.logger.debug("Validating storage %s" %
                self.storage.__class__.__name__)
            self.storage.validate()
        except:
            self.logger.error("Storage backed %s failed to validate: %s" %
                (self.storage.__class__.__name__,
                    traceback.format_exc().splitlines()[-1]))

    def run(self):
        """Startup the processing and go!"""
        self.terminate = threading.Event()
        atexit.register(self.shutdown)
        self.context = daemon.DaemonContext(detach_process=True)
        iter = 0

        if self.setup['daemon']:
            self.logger.debug("Daemonizing")
            try:
                self.context.pidfile = PIDLockFile(self.setup['daemon'])
                self.context.open()
            except LockFailed:
                self.logger.error("Failed to daemonize: %s" %
                                  sys.exc_info()[1])
                self.shutdown()
                return
            except LockTimeout:
                self.logger.error("Failed to daemonize: "
                                  "Failed to acquire lock on %s" %
                                  self.setup['daemon'])
                self.shutdown()
                return
            except PIDFileError:
                self.logger.error("Error writing pid file: %s" %
                    traceback.format_exc().splitlines()[-1])
                self.shutdown()
                return
            self.logger.info("Starting daemon")

        self.transport.start_monitor(self)

        while not self.terminate.isSet():
            try:
                interaction = self.transport.fetch()
                if not interaction:
                    continue

                store_thread = ReportingStoreThread(interaction, self.storage)
                store_thread.start()
                self.children.append(store_thread)

                iter += 1
                if iter >= self.cleanup_threshold:
                    self.reap_children()
                    iter = 0

            except (SystemExit, KeyboardInterrupt):
                self.logger.info("Shutting down")
                self.shutdown()
            except:
                self.logger.error("Unhandled exception in main loop %s" %
                    traceback.format_exc().splitlines()[-1])

    def shutdown(self):
        """Cleanup and go"""
        if self.terminate:
            # this wil be missing if called from bcfg2-admin
            self.terminate.set()
        if self.transport:
            try:
                self.transport.shutdown()
            except OSError:
                pass
        if self.storage:
            self.storage.shutdown()

    def reap_children(self):
        """Join any non-live threads"""
        newlist = []

        self.logger.debug("Starting reap_children")
        for child in self.children:
            if child.isAlive():
                newlist.append(child)
            else:
                child.join()
                self.logger.debug("Joined child thread %s" % child.getName())
        self.children = newlist

########NEW FILE########
__FILENAME__ = Compat
""" Compatibility imports for Django. """

from django import VERSION
from django.db import transaction

# Django 1.6 deprecated commit_on_success() and introduced atomic() with
# similar semantics.
if VERSION[0] == 1 and VERSION[1] < 6:
    transaction.atomic = transaction.commit_on_success

try:
    # Django < 1.6
    from django.conf.urls.defaults import url, patterns
except ImportError:
    # Django > 1.6
    from django.conf.urls import url, patterns

########NEW FILE########
__FILENAME__ = 0001_initial
# -*- coding: utf-8 -*-
import datetime
from south.db import db
from south.v2 import SchemaMigration
from django.db import models


class Migration(SchemaMigration):

    def forwards(self, orm):
        # Adding model 'Client'
        db.create_table('Reporting_client', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('creation', self.gf('django.db.models.fields.DateTimeField')(auto_now_add=True, blank=True)),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=128)),
            ('current_interaction', self.gf('django.db.models.fields.related.ForeignKey')(blank=True, related_name='parent_client', null=True, to=orm['Reporting.Interaction'])),
            ('expiration', self.gf('django.db.models.fields.DateTimeField')(null=True, blank=True)),
        ))
        db.send_create_signal('Reporting', ['Client'])

        # Adding model 'Interaction'
        db.create_table('Reporting_interaction', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('client', self.gf('django.db.models.fields.related.ForeignKey')(related_name='interactions', to=orm['Reporting.Client'])),
            ('timestamp', self.gf('django.db.models.fields.DateTimeField')(db_index=True)),
            ('state', self.gf('django.db.models.fields.CharField')(max_length=32)),
            ('repo_rev_code', self.gf('django.db.models.fields.CharField')(max_length=64)),
            ('server', self.gf('django.db.models.fields.CharField')(max_length=256)),
            ('good_count', self.gf('django.db.models.fields.IntegerField')()),
            ('total_count', self.gf('django.db.models.fields.IntegerField')()),
            ('bad_count', self.gf('django.db.models.fields.IntegerField')(default=0)),
            ('modified_count', self.gf('django.db.models.fields.IntegerField')(default=0)),
            ('extra_count', self.gf('django.db.models.fields.IntegerField')(default=0)),
            ('profile', self.gf('django.db.models.fields.related.ForeignKey')(related_name='+', to=orm['Reporting.Group'])),
        ))
        db.send_create_signal('Reporting', ['Interaction'])

        # Adding unique constraint on 'Interaction', fields ['client', 'timestamp']
        db.create_unique('Reporting_interaction', ['client_id', 'timestamp'])

        # Adding M2M table for field actions on 'Interaction'
        db.create_table('Reporting_interaction_actions', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('interaction', models.ForeignKey(orm['Reporting.interaction'], null=False)),
            ('actionentry', models.ForeignKey(orm['Reporting.actionentry'], null=False))
        ))
        db.create_unique('Reporting_interaction_actions', ['interaction_id', 'actionentry_id'])

        # Adding M2M table for field packages on 'Interaction'
        db.create_table('Reporting_interaction_packages', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('interaction', models.ForeignKey(orm['Reporting.interaction'], null=False)),
            ('packageentry', models.ForeignKey(orm['Reporting.packageentry'], null=False))
        ))
        db.create_unique('Reporting_interaction_packages', ['interaction_id', 'packageentry_id'])

        # Adding M2M table for field paths on 'Interaction'
        db.create_table('Reporting_interaction_paths', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('interaction', models.ForeignKey(orm['Reporting.interaction'], null=False)),
            ('pathentry', models.ForeignKey(orm['Reporting.pathentry'], null=False))
        ))
        db.create_unique('Reporting_interaction_paths', ['interaction_id', 'pathentry_id'])

        # Adding M2M table for field services on 'Interaction'
        db.create_table('Reporting_interaction_services', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('interaction', models.ForeignKey(orm['Reporting.interaction'], null=False)),
            ('serviceentry', models.ForeignKey(orm['Reporting.serviceentry'], null=False))
        ))
        db.create_unique('Reporting_interaction_services', ['interaction_id', 'serviceentry_id'])

        # Adding M2M table for field failures on 'Interaction'
        db.create_table('Reporting_interaction_failures', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('interaction', models.ForeignKey(orm['Reporting.interaction'], null=False)),
            ('failureentry', models.ForeignKey(orm['Reporting.failureentry'], null=False))
        ))
        db.create_unique('Reporting_interaction_failures', ['interaction_id', 'failureentry_id'])

        # Adding M2M table for field groups on 'Interaction'
        db.create_table('Reporting_interaction_groups', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('interaction', models.ForeignKey(orm['Reporting.interaction'], null=False)),
            ('group', models.ForeignKey(orm['Reporting.group'], null=False))
        ))
        db.create_unique('Reporting_interaction_groups', ['interaction_id', 'group_id'])

        # Adding M2M table for field bundles on 'Interaction'
        db.create_table('Reporting_interaction_bundles', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('interaction', models.ForeignKey(orm['Reporting.interaction'], null=False)),
            ('bundle', models.ForeignKey(orm['Reporting.bundle'], null=False))
        ))
        db.create_unique('Reporting_interaction_bundles', ['interaction_id', 'bundle_id'])

        # Adding model 'Performance'
        db.create_table('Reporting_performance', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('interaction', self.gf('django.db.models.fields.related.ForeignKey')(related_name='performance_items', to=orm['Reporting.Interaction'])),
            ('metric', self.gf('django.db.models.fields.CharField')(max_length=128)),
            ('value', self.gf('django.db.models.fields.DecimalField')(max_digits=32, decimal_places=16)),
        ))
        db.send_create_signal('Reporting', ['Performance'])

        # Adding model 'Group'
        db.create_table('Reporting_group', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('name', self.gf('django.db.models.fields.CharField')(unique=True, max_length=255)),
            ('profile', self.gf('django.db.models.fields.BooleanField')(default=False)),
            ('public', self.gf('django.db.models.fields.BooleanField')(default=False)),
            ('category', self.gf('django.db.models.fields.CharField')(max_length=1024, blank=True)),
            ('comment', self.gf('django.db.models.fields.TextField')(blank=True)),
        ))
        db.send_create_signal('Reporting', ['Group'])

        # Adding M2M table for field groups on 'Group'
        db.create_table('Reporting_group_groups', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('from_group', models.ForeignKey(orm['Reporting.group'], null=False)),
            ('to_group', models.ForeignKey(orm['Reporting.group'], null=False))
        ))
        db.create_unique('Reporting_group_groups', ['from_group_id', 'to_group_id'])

        # Adding M2M table for field bundles on 'Group'
        db.create_table('Reporting_group_bundles', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('group', models.ForeignKey(orm['Reporting.group'], null=False)),
            ('bundle', models.ForeignKey(orm['Reporting.bundle'], null=False))
        ))
        db.create_unique('Reporting_group_bundles', ['group_id', 'bundle_id'])

        # Adding model 'Bundle'
        db.create_table('Reporting_bundle', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('name', self.gf('django.db.models.fields.CharField')(unique=True, max_length=255)),
        ))
        db.send_create_signal('Reporting', ['Bundle'])

        # Adding model 'FilePerms'
        db.create_table('Reporting_fileperms', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('owner', self.gf('django.db.models.fields.CharField')(max_length=128)),
            ('group', self.gf('django.db.models.fields.CharField')(max_length=128)),
            ('perms', self.gf('django.db.models.fields.CharField')(max_length=128)),
        ))
        db.send_create_signal('Reporting', ['FilePerms'])

        # Adding unique constraint on 'FilePerms', fields ['owner', 'group', 'perms']
        db.create_unique('Reporting_fileperms', ['owner', 'group', 'perms'])

        # Adding model 'FileAcl'
        db.create_table('Reporting_fileacl', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=128, db_index=True)),
        ))
        db.send_create_signal('Reporting', ['FileAcl'])

        # Adding model 'FailureEntry'
        db.create_table('Reporting_failureentry', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=128, db_index=True)),
            ('hash_key', self.gf('django.db.models.fields.IntegerField')(db_index=True)),
            ('entry_type', self.gf('django.db.models.fields.CharField')(max_length=128)),
            ('message', self.gf('django.db.models.fields.TextField')()),
        ))
        db.send_create_signal('Reporting', ['FailureEntry'])

        # Adding model 'ActionEntry'
        db.create_table('Reporting_actionentry', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=128, db_index=True)),
            ('hash_key', self.gf('django.db.models.fields.IntegerField')(db_index=True)),
            ('state', self.gf('django.db.models.fields.IntegerField')()),
            ('exists', self.gf('django.db.models.fields.BooleanField')(default=True)),
            ('status', self.gf('django.db.models.fields.CharField')(default='check', max_length=128)),
            ('output', self.gf('django.db.models.fields.IntegerField')(default=0)),
        ))
        db.send_create_signal('Reporting', ['ActionEntry'])

        # Adding model 'PackageEntry'
        db.create_table('Reporting_packageentry', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=128, db_index=True)),
            ('hash_key', self.gf('django.db.models.fields.IntegerField')(db_index=True)),
            ('state', self.gf('django.db.models.fields.IntegerField')()),
            ('exists', self.gf('django.db.models.fields.BooleanField')(default=True)),
            ('target_version', self.gf('django.db.models.fields.CharField')(default='', max_length=1024)),
            ('current_version', self.gf('django.db.models.fields.CharField')(max_length=1024)),
            ('verification_details', self.gf('django.db.models.fields.TextField')(default='')),
        ))
        db.send_create_signal('Reporting', ['PackageEntry'])

        # Adding model 'PathEntry'
        db.create_table('Reporting_pathentry', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=128, db_index=True)),
            ('hash_key', self.gf('django.db.models.fields.IntegerField')(db_index=True)),
            ('state', self.gf('django.db.models.fields.IntegerField')()),
            ('exists', self.gf('django.db.models.fields.BooleanField')(default=True)),
            ('path_type', self.gf('django.db.models.fields.CharField')(max_length=128)),
            ('target_perms', self.gf('django.db.models.fields.related.ForeignKey')(related_name='+', to=orm['Reporting.FilePerms'])),
            ('current_perms', self.gf('django.db.models.fields.related.ForeignKey')(related_name='+', to=orm['Reporting.FilePerms'])),
            ('detail_type', self.gf('django.db.models.fields.IntegerField')(default=0)),
            ('details', self.gf('django.db.models.fields.TextField')(default='')),
        ))
        db.send_create_signal('Reporting', ['PathEntry'])

        # Adding M2M table for field acls on 'PathEntry'
        db.create_table('Reporting_pathentry_acls', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('pathentry', models.ForeignKey(orm['Reporting.pathentry'], null=False)),
            ('fileacl', models.ForeignKey(orm['Reporting.fileacl'], null=False))
        ))
        db.create_unique('Reporting_pathentry_acls', ['pathentry_id', 'fileacl_id'])

        # Adding model 'LinkEntry'
        db.create_table('Reporting_linkentry', (
            ('pathentry_ptr', self.gf('django.db.models.fields.related.OneToOneField')(to=orm['Reporting.PathEntry'], unique=True, primary_key=True)),
            ('target_path', self.gf('django.db.models.fields.CharField')(max_length=1024, blank=True)),
            ('current_path', self.gf('django.db.models.fields.CharField')(max_length=1024, blank=True)),
        ))
        db.send_create_signal('Reporting', ['LinkEntry'])

        # Adding model 'DeviceEntry'
        db.create_table('Reporting_deviceentry', (
            ('pathentry_ptr', self.gf('django.db.models.fields.related.OneToOneField')(to=orm['Reporting.PathEntry'], unique=True, primary_key=True)),
            ('device_type', self.gf('django.db.models.fields.CharField')(max_length=16)),
            ('target_major', self.gf('django.db.models.fields.IntegerField')()),
            ('target_minor', self.gf('django.db.models.fields.IntegerField')()),
            ('current_major', self.gf('django.db.models.fields.IntegerField')()),
            ('current_minor', self.gf('django.db.models.fields.IntegerField')()),
        ))
        db.send_create_signal('Reporting', ['DeviceEntry'])

        # Adding model 'ServiceEntry'
        db.create_table('Reporting_serviceentry', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=128, db_index=True)),
            ('hash_key', self.gf('django.db.models.fields.IntegerField')(db_index=True)),
            ('state', self.gf('django.db.models.fields.IntegerField')()),
            ('exists', self.gf('django.db.models.fields.BooleanField')(default=True)),
            ('target_status', self.gf('django.db.models.fields.CharField')(default='', max_length=128)),
            ('current_status', self.gf('django.db.models.fields.CharField')(default='', max_length=128)),
        ))
        db.send_create_signal('Reporting', ['ServiceEntry'])


    def backwards(self, orm):
        # Removing unique constraint on 'FilePerms', fields ['owner', 'group', 'perms']
        db.delete_unique('Reporting_fileperms', ['owner', 'group', 'perms'])

        # Removing unique constraint on 'Interaction', fields ['client', 'timestamp']
        db.delete_unique('Reporting_interaction', ['client_id', 'timestamp'])

        # Deleting model 'Client'
        db.delete_table('Reporting_client')

        # Deleting model 'Interaction'
        db.delete_table('Reporting_interaction')

        # Removing M2M table for field actions on 'Interaction'
        db.delete_table('Reporting_interaction_actions')

        # Removing M2M table for field packages on 'Interaction'
        db.delete_table('Reporting_interaction_packages')

        # Removing M2M table for field paths on 'Interaction'
        db.delete_table('Reporting_interaction_paths')

        # Removing M2M table for field services on 'Interaction'
        db.delete_table('Reporting_interaction_services')

        # Removing M2M table for field failures on 'Interaction'
        db.delete_table('Reporting_interaction_failures')

        # Removing M2M table for field groups on 'Interaction'
        db.delete_table('Reporting_interaction_groups')

        # Removing M2M table for field bundles on 'Interaction'
        db.delete_table('Reporting_interaction_bundles')

        # Deleting model 'Performance'
        db.delete_table('Reporting_performance')

        # Deleting model 'Group'
        db.delete_table('Reporting_group')

        # Removing M2M table for field groups on 'Group'
        db.delete_table('Reporting_group_groups')

        # Removing M2M table for field bundles on 'Group'
        db.delete_table('Reporting_group_bundles')

        # Deleting model 'Bundle'
        db.delete_table('Reporting_bundle')

        # Deleting model 'FilePerms'
        db.delete_table('Reporting_fileperms')

        # Deleting model 'FileAcl'
        db.delete_table('Reporting_fileacl')

        # Deleting model 'FailureEntry'
        db.delete_table('Reporting_failureentry')

        # Deleting model 'ActionEntry'
        db.delete_table('Reporting_actionentry')

        # Deleting model 'PackageEntry'
        db.delete_table('Reporting_packageentry')

        # Deleting model 'PathEntry'
        db.delete_table('Reporting_pathentry')

        # Removing M2M table for field acls on 'PathEntry'
        db.delete_table('Reporting_pathentry_acls')

        # Deleting model 'LinkEntry'
        db.delete_table('Reporting_linkentry')

        # Deleting model 'DeviceEntry'
        db.delete_table('Reporting_deviceentry')

        # Deleting model 'ServiceEntry'
        db.delete_table('Reporting_serviceentry')


    models = {
        'Reporting.actionentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'ActionEntry'},
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.IntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'output': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'status': ('django.db.models.fields.CharField', [], {'default': "'check'", 'max_length': '128'})
        },
        'Reporting.bundle': {
            'Meta': {'ordering': "('name',)", 'object_name': 'Bundle'},
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '255'})
        },
        'Reporting.client': {
            'Meta': {'object_name': 'Client'},
            'creation': ('django.db.models.fields.DateTimeField', [], {'auto_now_add': 'True', 'blank': 'True'}),
            'current_interaction': ('django.db.models.fields.related.ForeignKey', [], {'blank': 'True', 'related_name': "'parent_client'", 'null': 'True', 'to': "orm['Reporting.Interaction']"}),
            'expiration': ('django.db.models.fields.DateTimeField', [], {'null': 'True', 'blank': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128'})
        },
        'Reporting.deviceentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'DeviceEntry', '_ormbases': ['Reporting.PathEntry']},
            'current_major': ('django.db.models.fields.IntegerField', [], {}),
            'current_minor': ('django.db.models.fields.IntegerField', [], {}),
            'device_type': ('django.db.models.fields.CharField', [], {'max_length': '16'}),
            'pathentry_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': "orm['Reporting.PathEntry']", 'unique': 'True', 'primary_key': 'True'}),
            'target_major': ('django.db.models.fields.IntegerField', [], {}),
            'target_minor': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.failureentry': {
            'Meta': {'object_name': 'FailureEntry'},
            'entry_type': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'hash_key': ('django.db.models.fields.IntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'message': ('django.db.models.fields.TextField', [], {}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'})
        },
        'Reporting.fileacl': {
            'Meta': {'object_name': 'FileAcl'},
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'})
        },
        'Reporting.fileperms': {
            'Meta': {'unique_together': "(('owner', 'group', 'perms'),)", 'object_name': 'FilePerms'},
            'group': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'owner': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'perms': ('django.db.models.fields.CharField', [], {'max_length': '128'})
        },
        'Reporting.group': {
            'Meta': {'ordering': "('name',)", 'object_name': 'Group'},
            'bundles': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Bundle']", 'symmetrical': 'False'}),
            'category': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'blank': 'True'}),
            'comment': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'groups': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Group']", 'symmetrical': 'False'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '255'}),
            'profile': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),
            'public': ('django.db.models.fields.BooleanField', [], {'default': 'False'})
        },
        'Reporting.interaction': {
            'Meta': {'ordering': "['-timestamp']", 'unique_together': "(('client', 'timestamp'),)", 'object_name': 'Interaction'},
            'actions': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.ActionEntry']", 'symmetrical': 'False'}),
            'bad_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'bundles': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Bundle']", 'symmetrical': 'False'}),
            'client': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'interactions'", 'to': "orm['Reporting.Client']"}),
            'extra_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'failures': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.FailureEntry']", 'symmetrical': 'False'}),
            'good_count': ('django.db.models.fields.IntegerField', [], {}),
            'groups': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Group']", 'symmetrical': 'False'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'modified_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'packages': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.PackageEntry']", 'symmetrical': 'False'}),
            'paths': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.PathEntry']", 'symmetrical': 'False'}),
            'profile': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'+'", 'to': "orm['Reporting.Group']"}),
            'repo_rev_code': ('django.db.models.fields.CharField', [], {'max_length': '64'}),
            'server': ('django.db.models.fields.CharField', [], {'max_length': '256'}),
            'services': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.ServiceEntry']", 'symmetrical': 'False'}),
            'state': ('django.db.models.fields.CharField', [], {'max_length': '32'}),
            'timestamp': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True'}),
            'total_count': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.linkentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'LinkEntry', '_ormbases': ['Reporting.PathEntry']},
            'current_path': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'blank': 'True'}),
            'pathentry_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': "orm['Reporting.PathEntry']", 'unique': 'True', 'primary_key': 'True'}),
            'target_path': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'blank': 'True'})
        },
        'Reporting.packageentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'PackageEntry'},
            'current_version': ('django.db.models.fields.CharField', [], {'max_length': '1024'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.IntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'target_version': ('django.db.models.fields.CharField', [], {'default': "''", 'max_length': '1024'}),
            'verification_details': ('django.db.models.fields.TextField', [], {'default': "''"})
        },
        'Reporting.pathentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'PathEntry'},
            'acls': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.FileAcl']", 'symmetrical': 'False'}),
            'current_perms': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'+'", 'to': "orm['Reporting.FilePerms']"}),
            'detail_type': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'details': ('django.db.models.fields.TextField', [], {'default': "''"}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.IntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'path_type': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'target_perms': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'+'", 'to': "orm['Reporting.FilePerms']"})
        },
        'Reporting.performance': {
            'Meta': {'object_name': 'Performance'},
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'interaction': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'performance_items'", 'to': "orm['Reporting.Interaction']"}),
            'metric': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'value': ('django.db.models.fields.DecimalField', [], {'max_digits': '32', 'decimal_places': '16'})
        },
        'Reporting.serviceentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'ServiceEntry'},
            'current_status': ('django.db.models.fields.CharField', [], {'default': "''", 'max_length': '128'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.IntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'target_status': ('django.db.models.fields.CharField', [], {'default': "''", 'max_length': '128'})
        }
    }

    complete_apps = ['Reporting']
########NEW FILE########
__FILENAME__ = 0002_convert_perms_to_mode
# -*- coding: utf-8 -*-
import datetime
from south.db import db
from south.v2 import SchemaMigration
from django.db import models

from Bcfg2 import settings

class Migration(SchemaMigration):

    def forwards(self, orm):
        # Removing unique constraint on 'FilePerms', fields ['owner', 'perms', 'group']
        db.delete_unique('Reporting_fileperms', ['owner', 'perms', 'group'])

        # Renaming field 'FilePerms.perms' to 'FilePerms.mode'
        db.rename_column('Reporting_fileperms', 'perms', 'mode')

        if not settings.DATABASES['default']['ENGINE'] == 'django.db.backends.sqlite3':
            # Adding unique constraint on 'FilePerms', fields ['owner', 'group', 'mode']
            db.create_unique('Reporting_fileperms', ['owner', 'group', 'mode'])


    def backwards(self, orm):
        # Removing unique constraint on 'FilePerms', fields ['owner', 'group', 'mode']
        db.delete_unique('Reporting_fileperms', ['owner', 'group', 'mode'])

        # Renaming field 'FilePerms.mode' to 'FilePerms.perms'
        db.rename_column('Reporting_fileperms', 'mode', 'perms')

        if not settings.DATABASES['default']['ENGINE'] == 'django.db.backends.sqlite3':
            # Adding unique constraint on 'FilePerms', fields ['owner', 'perms', 'group']
            db.create_unique('Reporting_fileperms', ['owner', 'perms', 'group'])


    models = {
        'Reporting.actionentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'ActionEntry'},
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.IntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'output': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'status': ('django.db.models.fields.CharField', [], {'default': "'check'", 'max_length': '128'})
        },
        'Reporting.bundle': {
            'Meta': {'ordering': "('name',)", 'object_name': 'Bundle'},
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '255'})
        },
        'Reporting.client': {
            'Meta': {'object_name': 'Client'},
            'creation': ('django.db.models.fields.DateTimeField', [], {'auto_now_add': 'True', 'blank': 'True'}),
            'current_interaction': ('django.db.models.fields.related.ForeignKey', [], {'blank': 'True', 'related_name': "'parent_client'", 'null': 'True', 'to': "orm['Reporting.Interaction']"}),
            'expiration': ('django.db.models.fields.DateTimeField', [], {'null': 'True', 'blank': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128'})
        },
        'Reporting.deviceentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'DeviceEntry', '_ormbases': ['Reporting.PathEntry']},
            'current_major': ('django.db.models.fields.IntegerField', [], {}),
            'current_minor': ('django.db.models.fields.IntegerField', [], {}),
            'device_type': ('django.db.models.fields.CharField', [], {'max_length': '16'}),
            'pathentry_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': "orm['Reporting.PathEntry']", 'unique': 'True', 'primary_key': 'True'}),
            'target_major': ('django.db.models.fields.IntegerField', [], {}),
            'target_minor': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.failureentry': {
            'Meta': {'object_name': 'FailureEntry'},
            'entry_type': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'hash_key': ('django.db.models.fields.IntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'message': ('django.db.models.fields.TextField', [], {}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'})
        },
        'Reporting.fileacl': {
            'Meta': {'object_name': 'FileAcl'},
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'})
        },
        'Reporting.fileperms': {
            'Meta': {'unique_together': "(('owner', 'group', 'mode'),)", 'object_name': 'FilePerms'},
            'group': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'mode': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'owner': ('django.db.models.fields.CharField', [], {'max_length': '128'})
        },
        'Reporting.group': {
            'Meta': {'ordering': "('name',)", 'object_name': 'Group'},
            'bundles': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Bundle']", 'symmetrical': 'False'}),
            'category': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'blank': 'True'}),
            'comment': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'groups': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Group']", 'symmetrical': 'False'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '255'}),
            'profile': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),
            'public': ('django.db.models.fields.BooleanField', [], {'default': 'False'})
        },
        'Reporting.interaction': {
            'Meta': {'ordering': "['-timestamp']", 'unique_together': "(('client', 'timestamp'),)", 'object_name': 'Interaction'},
            'actions': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.ActionEntry']", 'symmetrical': 'False'}),
            'bad_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'bundles': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Bundle']", 'symmetrical': 'False'}),
            'client': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'interactions'", 'to': "orm['Reporting.Client']"}),
            'extra_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'failures': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.FailureEntry']", 'symmetrical': 'False'}),
            'good_count': ('django.db.models.fields.IntegerField', [], {}),
            'groups': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Group']", 'symmetrical': 'False'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'modified_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'packages': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.PackageEntry']", 'symmetrical': 'False'}),
            'paths': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.PathEntry']", 'symmetrical': 'False'}),
            'profile': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'+'", 'to': "orm['Reporting.Group']"}),
            'repo_rev_code': ('django.db.models.fields.CharField', [], {'max_length': '64'}),
            'server': ('django.db.models.fields.CharField', [], {'max_length': '256'}),
            'services': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.ServiceEntry']", 'symmetrical': 'False'}),
            'state': ('django.db.models.fields.CharField', [], {'max_length': '32'}),
            'timestamp': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True'}),
            'total_count': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.linkentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'LinkEntry', '_ormbases': ['Reporting.PathEntry']},
            'current_path': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'blank': 'True'}),
            'pathentry_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': "orm['Reporting.PathEntry']", 'unique': 'True', 'primary_key': 'True'}),
            'target_path': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'blank': 'True'})
        },
        'Reporting.packageentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'PackageEntry'},
            'current_version': ('django.db.models.fields.CharField', [], {'max_length': '1024'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.IntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'target_version': ('django.db.models.fields.CharField', [], {'default': "''", 'max_length': '1024'}),
            'verification_details': ('django.db.models.fields.TextField', [], {'default': "''"})
        },
        'Reporting.pathentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'PathEntry'},
            'acls': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.FileAcl']", 'symmetrical': 'False'}),
            'current_perms': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'+'", 'to': "orm['Reporting.FilePerms']"}),
            'detail_type': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'details': ('django.db.models.fields.TextField', [], {'default': "''"}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.IntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'path_type': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'target_perms': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'+'", 'to': "orm['Reporting.FilePerms']"})
        },
        'Reporting.performance': {
            'Meta': {'object_name': 'Performance'},
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'interaction': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'performance_items'", 'to': "orm['Reporting.Interaction']"}),
            'metric': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'value': ('django.db.models.fields.DecimalField', [], {'max_digits': '32', 'decimal_places': '16'})
        },
        'Reporting.serviceentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'ServiceEntry'},
            'current_status': ('django.db.models.fields.CharField', [], {'default': "''", 'max_length': '128'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.IntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'target_status': ('django.db.models.fields.CharField', [], {'default': "''", 'max_length': '128'})
        }
    }

    complete_apps = ['Reporting']

########NEW FILE########
__FILENAME__ = 0003_expand_hash_key
# -*- coding: utf-8 -*-
import datetime
from south.db import db
from south.v2 import SchemaMigration
from django.db import models


class Migration(SchemaMigration):

    def forwards(self, orm):

        # Changing field 'FailureEntry.hash_key'
        db.alter_column('Reporting_failureentry', 'hash_key', self.gf('django.db.models.fields.BigIntegerField')())

        # Changing field 'PackageEntry.hash_key'
        db.alter_column('Reporting_packageentry', 'hash_key', self.gf('django.db.models.fields.BigIntegerField')())

        # Changing field 'ServiceEntry.hash_key'
        db.alter_column('Reporting_serviceentry', 'hash_key', self.gf('django.db.models.fields.BigIntegerField')())

        # Changing field 'PathEntry.hash_key'
        db.alter_column('Reporting_pathentry', 'hash_key', self.gf('django.db.models.fields.BigIntegerField')())

        # Changing field 'ActionEntry.hash_key'
        db.alter_column('Reporting_actionentry', 'hash_key', self.gf('django.db.models.fields.BigIntegerField')())

    def backwards(self, orm):

        # Changing field 'FailureEntry.hash_key'
        db.alter_column('Reporting_failureentry', 'hash_key', self.gf('django.db.models.fields.IntegerField')())

        # Changing field 'PackageEntry.hash_key'
        db.alter_column('Reporting_packageentry', 'hash_key', self.gf('django.db.models.fields.IntegerField')())

        # Changing field 'ServiceEntry.hash_key'
        db.alter_column('Reporting_serviceentry', 'hash_key', self.gf('django.db.models.fields.IntegerField')())

        # Changing field 'PathEntry.hash_key'
        db.alter_column('Reporting_pathentry', 'hash_key', self.gf('django.db.models.fields.IntegerField')())

        # Changing field 'ActionEntry.hash_key'
        db.alter_column('Reporting_actionentry', 'hash_key', self.gf('django.db.models.fields.IntegerField')())

    models = {
        'Reporting.actionentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'ActionEntry'},
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'output': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'status': ('django.db.models.fields.CharField', [], {'default': "'check'", 'max_length': '128'})
        },
        'Reporting.bundle': {
            'Meta': {'ordering': "('name',)", 'object_name': 'Bundle'},
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '255'})
        },
        'Reporting.client': {
            'Meta': {'object_name': 'Client'},
            'creation': ('django.db.models.fields.DateTimeField', [], {'auto_now_add': 'True', 'blank': 'True'}),
            'current_interaction': ('django.db.models.fields.related.ForeignKey', [], {'blank': 'True', 'related_name': "'parent_client'", 'null': 'True', 'to': "orm['Reporting.Interaction']"}),
            'expiration': ('django.db.models.fields.DateTimeField', [], {'null': 'True', 'blank': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128'})
        },
        'Reporting.deviceentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'DeviceEntry', '_ormbases': ['Reporting.PathEntry']},
            'current_major': ('django.db.models.fields.IntegerField', [], {}),
            'current_minor': ('django.db.models.fields.IntegerField', [], {}),
            'device_type': ('django.db.models.fields.CharField', [], {'max_length': '16'}),
            'pathentry_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': "orm['Reporting.PathEntry']", 'unique': 'True', 'primary_key': 'True'}),
            'target_major': ('django.db.models.fields.IntegerField', [], {}),
            'target_minor': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.failureentry': {
            'Meta': {'object_name': 'FailureEntry'},
            'entry_type': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'message': ('django.db.models.fields.TextField', [], {}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'})
        },
        'Reporting.fileacl': {
            'Meta': {'object_name': 'FileAcl'},
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'})
        },
        'Reporting.fileperms': {
            'Meta': {'unique_together': "(('owner', 'group', 'mode'),)", 'object_name': 'FilePerms'},
            'group': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'mode': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'owner': ('django.db.models.fields.CharField', [], {'max_length': '128'})
        },
        'Reporting.group': {
            'Meta': {'ordering': "('name',)", 'object_name': 'Group'},
            'bundles': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Bundle']", 'symmetrical': 'False'}),
            'category': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'blank': 'True'}),
            'comment': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'groups': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Group']", 'symmetrical': 'False'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '255'}),
            'profile': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),
            'public': ('django.db.models.fields.BooleanField', [], {'default': 'False'})
        },
        'Reporting.interaction': {
            'Meta': {'ordering': "['-timestamp']", 'unique_together': "(('client', 'timestamp'),)", 'object_name': 'Interaction'},
            'actions': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.ActionEntry']", 'symmetrical': 'False'}),
            'bad_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'bundles': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Bundle']", 'symmetrical': 'False'}),
            'client': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'interactions'", 'to': "orm['Reporting.Client']"}),
            'extra_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'failures': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.FailureEntry']", 'symmetrical': 'False'}),
            'good_count': ('django.db.models.fields.IntegerField', [], {}),
            'groups': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Group']", 'symmetrical': 'False'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'modified_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'packages': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.PackageEntry']", 'symmetrical': 'False'}),
            'paths': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.PathEntry']", 'symmetrical': 'False'}),
            'profile': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'+'", 'to': "orm['Reporting.Group']"}),
            'repo_rev_code': ('django.db.models.fields.CharField', [], {'max_length': '64'}),
            'server': ('django.db.models.fields.CharField', [], {'max_length': '256'}),
            'services': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.ServiceEntry']", 'symmetrical': 'False'}),
            'state': ('django.db.models.fields.CharField', [], {'max_length': '32'}),
            'timestamp': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True'}),
            'total_count': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.linkentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'LinkEntry', '_ormbases': ['Reporting.PathEntry']},
            'current_path': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'blank': 'True'}),
            'pathentry_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': "orm['Reporting.PathEntry']", 'unique': 'True', 'primary_key': 'True'}),
            'target_path': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'blank': 'True'})
        },
        'Reporting.packageentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'PackageEntry'},
            'current_version': ('django.db.models.fields.CharField', [], {'max_length': '1024'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'target_version': ('django.db.models.fields.CharField', [], {'default': "''", 'max_length': '1024'}),
            'verification_details': ('django.db.models.fields.TextField', [], {'default': "''"})
        },
        'Reporting.pathentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'PathEntry'},
            'acls': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.FileAcl']", 'symmetrical': 'False'}),
            'current_perms': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'+'", 'to': "orm['Reporting.FilePerms']"}),
            'detail_type': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'details': ('django.db.models.fields.TextField', [], {'default': "''"}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'path_type': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'target_perms': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'+'", 'to': "orm['Reporting.FilePerms']"})
        },
        'Reporting.performance': {
            'Meta': {'object_name': 'Performance'},
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'interaction': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'performance_items'", 'to': "orm['Reporting.Interaction']"}),
            'metric': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'value': ('django.db.models.fields.DecimalField', [], {'max_digits': '32', 'decimal_places': '16'})
        },
        'Reporting.serviceentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'ServiceEntry'},
            'current_status': ('django.db.models.fields.CharField', [], {'default': "''", 'max_length': '128'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'target_status': ('django.db.models.fields.CharField', [], {'default': "''", 'max_length': '128'})
        }
    }

    complete_apps = ['Reporting']
########NEW FILE########
__FILENAME__ = 0004_profile_can_be_null
# -*- coding: utf-8 -*-
import datetime
from south.db import db
from south.v2 import SchemaMigration
from django.db import models


class Migration(SchemaMigration):

    def forwards(self, orm):

        # Changing field 'Interaction.profile'
        db.alter_column('Reporting_interaction', 'profile_id', self.gf('django.db.models.fields.related.ForeignKey')(null=True, to=orm['Reporting.Group']))

    def backwards(self, orm):

        # User chose to not deal with backwards NULL issues for 'Interaction.profile'
        raise RuntimeError("Cannot reverse this migration. 'Interaction.profile' and its values cannot be restored.")

    models = {
        'Reporting.actionentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'ActionEntry'},
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'output': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'status': ('django.db.models.fields.CharField', [], {'default': "'check'", 'max_length': '128'})
        },
        'Reporting.bundle': {
            'Meta': {'ordering': "('name',)", 'object_name': 'Bundle'},
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '255'})
        },
        'Reporting.client': {
            'Meta': {'object_name': 'Client'},
            'creation': ('django.db.models.fields.DateTimeField', [], {'auto_now_add': 'True', 'blank': 'True'}),
            'current_interaction': ('django.db.models.fields.related.ForeignKey', [], {'blank': 'True', 'related_name': "'parent_client'", 'null': 'True', 'to': "orm['Reporting.Interaction']"}),
            'expiration': ('django.db.models.fields.DateTimeField', [], {'null': 'True', 'blank': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128'})
        },
        'Reporting.deviceentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'DeviceEntry', '_ormbases': ['Reporting.PathEntry']},
            'current_major': ('django.db.models.fields.IntegerField', [], {}),
            'current_minor': ('django.db.models.fields.IntegerField', [], {}),
            'device_type': ('django.db.models.fields.CharField', [], {'max_length': '16'}),
            'pathentry_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': "orm['Reporting.PathEntry']", 'unique': 'True', 'primary_key': 'True'}),
            'target_major': ('django.db.models.fields.IntegerField', [], {}),
            'target_minor': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.failureentry': {
            'Meta': {'object_name': 'FailureEntry'},
            'entry_type': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'message': ('django.db.models.fields.TextField', [], {}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'})
        },
        'Reporting.fileacl': {
            'Meta': {'object_name': 'FileAcl'},
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'})
        },
        'Reporting.fileperms': {
            'Meta': {'unique_together': "(('owner', 'group', 'mode'),)", 'object_name': 'FilePerms'},
            'group': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'mode': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'owner': ('django.db.models.fields.CharField', [], {'max_length': '128'})
        },
        'Reporting.group': {
            'Meta': {'ordering': "('name',)", 'object_name': 'Group'},
            'bundles': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Bundle']", 'symmetrical': 'False'}),
            'category': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'blank': 'True'}),
            'comment': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'groups': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Group']", 'symmetrical': 'False'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '255'}),
            'profile': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),
            'public': ('django.db.models.fields.BooleanField', [], {'default': 'False'})
        },
        'Reporting.interaction': {
            'Meta': {'ordering': "['-timestamp']", 'unique_together': "(('client', 'timestamp'),)", 'object_name': 'Interaction'},
            'actions': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.ActionEntry']", 'symmetrical': 'False'}),
            'bad_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'bundles': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Bundle']", 'symmetrical': 'False'}),
            'client': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'interactions'", 'to': "orm['Reporting.Client']"}),
            'extra_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'failures': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.FailureEntry']", 'symmetrical': 'False'}),
            'good_count': ('django.db.models.fields.IntegerField', [], {}),
            'groups': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Group']", 'symmetrical': 'False'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'modified_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'packages': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.PackageEntry']", 'symmetrical': 'False'}),
            'paths': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.PathEntry']", 'symmetrical': 'False'}),
            'profile': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'+'", 'null': 'True', 'to': "orm['Reporting.Group']"}),
            'repo_rev_code': ('django.db.models.fields.CharField', [], {'max_length': '64'}),
            'server': ('django.db.models.fields.CharField', [], {'max_length': '256'}),
            'services': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.ServiceEntry']", 'symmetrical': 'False'}),
            'state': ('django.db.models.fields.CharField', [], {'max_length': '32'}),
            'timestamp': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True'}),
            'total_count': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.linkentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'LinkEntry', '_ormbases': ['Reporting.PathEntry']},
            'current_path': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'blank': 'True'}),
            'pathentry_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': "orm['Reporting.PathEntry']", 'unique': 'True', 'primary_key': 'True'}),
            'target_path': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'blank': 'True'})
        },
        'Reporting.packageentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'PackageEntry'},
            'current_version': ('django.db.models.fields.CharField', [], {'max_length': '1024'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'target_version': ('django.db.models.fields.CharField', [], {'default': "''", 'max_length': '1024'}),
            'verification_details': ('django.db.models.fields.TextField', [], {'default': "''"})
        },
        'Reporting.pathentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'PathEntry'},
            'acls': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.FileAcl']", 'symmetrical': 'False'}),
            'current_perms': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'+'", 'to': "orm['Reporting.FilePerms']"}),
            'detail_type': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'details': ('django.db.models.fields.TextField', [], {'default': "''"}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'path_type': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'target_perms': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'+'", 'to': "orm['Reporting.FilePerms']"})
        },
        'Reporting.performance': {
            'Meta': {'object_name': 'Performance'},
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'interaction': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'performance_items'", 'to': "orm['Reporting.Interaction']"}),
            'metric': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'value': ('django.db.models.fields.DecimalField', [], {'max_digits': '32', 'decimal_places': '16'})
        },
        'Reporting.serviceentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'ServiceEntry'},
            'current_status': ('django.db.models.fields.CharField', [], {'default': "''", 'max_length': '128'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'target_status': ('django.db.models.fields.CharField', [], {'default': "''", 'max_length': '128'})
        }
    }

    complete_apps = ['Reporting']
########NEW FILE########
__FILENAME__ = 0005_add_selinux_entry_support
# -*- coding: utf-8 -*-
import datetime
from south.db import db
from south.v2 import SchemaMigration
from django.db import models


class Migration(SchemaMigration):

    def forwards(self, orm):
        # Adding model 'SELoginEntry'
        db.create_table('Reporting_seloginentry', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=128, db_index=True)),
            ('hash_key', self.gf('django.db.models.fields.BigIntegerField')(db_index=True)),
            ('state', self.gf('django.db.models.fields.IntegerField')()),
            ('exists', self.gf('django.db.models.fields.BooleanField')(default=True)),
            ('selinuxuser', self.gf('django.db.models.fields.CharField')(max_length=128)),
            ('current_selinuxuser', self.gf('django.db.models.fields.CharField')(max_length=128, null=True)),
        ))
        db.send_create_signal('Reporting', ['SELoginEntry'])

        # Adding model 'SEUserEntry'
        db.create_table('Reporting_seuserentry', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=128, db_index=True)),
            ('hash_key', self.gf('django.db.models.fields.BigIntegerField')(db_index=True)),
            ('state', self.gf('django.db.models.fields.IntegerField')()),
            ('exists', self.gf('django.db.models.fields.BooleanField')(default=True)),
            ('roles', self.gf('django.db.models.fields.CharField')(max_length=128)),
            ('current_roles', self.gf('django.db.models.fields.CharField')(max_length=128, null=True)),
            ('prefix', self.gf('django.db.models.fields.CharField')(max_length=128)),
            ('current_prefix', self.gf('django.db.models.fields.CharField')(max_length=128, null=True)),
        ))
        db.send_create_signal('Reporting', ['SEUserEntry'])

        # Adding model 'SEBooleanEntry'
        db.create_table('Reporting_sebooleanentry', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=128, db_index=True)),
            ('hash_key', self.gf('django.db.models.fields.BigIntegerField')(db_index=True)),
            ('state', self.gf('django.db.models.fields.IntegerField')()),
            ('exists', self.gf('django.db.models.fields.BooleanField')(default=True)),
            ('value', self.gf('django.db.models.fields.BooleanField')(default=True)),
        ))
        db.send_create_signal('Reporting', ['SEBooleanEntry'])

        # Adding model 'SENodeEntry'
        db.create_table('Reporting_senodeentry', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=128, db_index=True)),
            ('hash_key', self.gf('django.db.models.fields.BigIntegerField')(db_index=True)),
            ('state', self.gf('django.db.models.fields.IntegerField')()),
            ('exists', self.gf('django.db.models.fields.BooleanField')(default=True)),
            ('selinuxtype', self.gf('django.db.models.fields.CharField')(max_length=128)),
            ('current_selinuxtype', self.gf('django.db.models.fields.CharField')(max_length=128, null=True)),
            ('proto', self.gf('django.db.models.fields.CharField')(max_length=4)),
        ))
        db.send_create_signal('Reporting', ['SENodeEntry'])

        # Adding model 'SEFcontextEntry'
        db.create_table('Reporting_sefcontextentry', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=128, db_index=True)),
            ('hash_key', self.gf('django.db.models.fields.BigIntegerField')(db_index=True)),
            ('state', self.gf('django.db.models.fields.IntegerField')()),
            ('exists', self.gf('django.db.models.fields.BooleanField')(default=True)),
            ('selinuxtype', self.gf('django.db.models.fields.CharField')(max_length=128)),
            ('current_selinuxtype', self.gf('django.db.models.fields.CharField')(max_length=128, null=True)),
            ('filetype', self.gf('django.db.models.fields.CharField')(max_length=16)),
        ))
        db.send_create_signal('Reporting', ['SEFcontextEntry'])

        # Adding model 'SEInterfaceEntry'
        db.create_table('Reporting_seinterfaceentry', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=128, db_index=True)),
            ('hash_key', self.gf('django.db.models.fields.BigIntegerField')(db_index=True)),
            ('state', self.gf('django.db.models.fields.IntegerField')()),
            ('exists', self.gf('django.db.models.fields.BooleanField')(default=True)),
            ('selinuxtype', self.gf('django.db.models.fields.CharField')(max_length=128)),
            ('current_selinuxtype', self.gf('django.db.models.fields.CharField')(max_length=128, null=True)),
        ))
        db.send_create_signal('Reporting', ['SEInterfaceEntry'])

        # Adding model 'SEPermissiveEntry'
        db.create_table('Reporting_sepermissiveentry', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=128, db_index=True)),
            ('hash_key', self.gf('django.db.models.fields.BigIntegerField')(db_index=True)),
            ('state', self.gf('django.db.models.fields.IntegerField')()),
            ('exists', self.gf('django.db.models.fields.BooleanField')(default=True)),
        ))
        db.send_create_signal('Reporting', ['SEPermissiveEntry'])

        # Adding model 'SEModuleEntry'
        db.create_table('Reporting_semoduleentry', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=128, db_index=True)),
            ('hash_key', self.gf('django.db.models.fields.BigIntegerField')(db_index=True)),
            ('state', self.gf('django.db.models.fields.IntegerField')()),
            ('exists', self.gf('django.db.models.fields.BooleanField')(default=True)),
            ('disabled', self.gf('django.db.models.fields.BooleanField')(default=False)),
            ('current_disabled', self.gf('django.db.models.fields.BooleanField')(default=False)),
        ))
        db.send_create_signal('Reporting', ['SEModuleEntry'])

        # Adding model 'SEPortEntry'
        db.create_table('Reporting_seportentry', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=128, db_index=True)),
            ('hash_key', self.gf('django.db.models.fields.BigIntegerField')(db_index=True)),
            ('state', self.gf('django.db.models.fields.IntegerField')()),
            ('exists', self.gf('django.db.models.fields.BooleanField')(default=True)),
            ('selinuxtype', self.gf('django.db.models.fields.CharField')(max_length=128)),
            ('current_selinuxtype', self.gf('django.db.models.fields.CharField')(max_length=128, null=True)),
        ))
        db.send_create_signal('Reporting', ['SEPortEntry'])

        # Adding M2M table for field sebooleans on 'Interaction'
        db.create_table('Reporting_interaction_sebooleans', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('interaction', models.ForeignKey(orm['Reporting.interaction'], null=False)),
            ('sebooleanentry', models.ForeignKey(orm['Reporting.sebooleanentry'], null=False))
        ))
        db.create_unique('Reporting_interaction_sebooleans', ['interaction_id', 'sebooleanentry_id'])

        # Adding M2M table for field seports on 'Interaction'
        db.create_table('Reporting_interaction_seports', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('interaction', models.ForeignKey(orm['Reporting.interaction'], null=False)),
            ('seportentry', models.ForeignKey(orm['Reporting.seportentry'], null=False))
        ))
        db.create_unique('Reporting_interaction_seports', ['interaction_id', 'seportentry_id'])

        # Adding M2M table for field sefcontexts on 'Interaction'
        db.create_table('Reporting_interaction_sefcontexts', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('interaction', models.ForeignKey(orm['Reporting.interaction'], null=False)),
            ('sefcontextentry', models.ForeignKey(orm['Reporting.sefcontextentry'], null=False))
        ))
        db.create_unique('Reporting_interaction_sefcontexts', ['interaction_id', 'sefcontextentry_id'])

        # Adding M2M table for field senodes on 'Interaction'
        db.create_table('Reporting_interaction_senodes', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('interaction', models.ForeignKey(orm['Reporting.interaction'], null=False)),
            ('senodeentry', models.ForeignKey(orm['Reporting.senodeentry'], null=False))
        ))
        db.create_unique('Reporting_interaction_senodes', ['interaction_id', 'senodeentry_id'])

        # Adding M2M table for field selogins on 'Interaction'
        db.create_table('Reporting_interaction_selogins', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('interaction', models.ForeignKey(orm['Reporting.interaction'], null=False)),
            ('seloginentry', models.ForeignKey(orm['Reporting.seloginentry'], null=False))
        ))
        db.create_unique('Reporting_interaction_selogins', ['interaction_id', 'seloginentry_id'])

        # Adding M2M table for field seusers on 'Interaction'
        db.create_table('Reporting_interaction_seusers', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('interaction', models.ForeignKey(orm['Reporting.interaction'], null=False)),
            ('seuserentry', models.ForeignKey(orm['Reporting.seuserentry'], null=False))
        ))
        db.create_unique('Reporting_interaction_seusers', ['interaction_id', 'seuserentry_id'])

        # Adding M2M table for field seinterfaces on 'Interaction'
        db.create_table('Reporting_interaction_seinterfaces', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('interaction', models.ForeignKey(orm['Reporting.interaction'], null=False)),
            ('seinterfaceentry', models.ForeignKey(orm['Reporting.seinterfaceentry'], null=False))
        ))
        db.create_unique('Reporting_interaction_seinterfaces', ['interaction_id', 'seinterfaceentry_id'])

        # Adding M2M table for field sepermissives on 'Interaction'
        db.create_table('Reporting_interaction_sepermissives', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('interaction', models.ForeignKey(orm['Reporting.interaction'], null=False)),
            ('sepermissiveentry', models.ForeignKey(orm['Reporting.sepermissiveentry'], null=False))
        ))
        db.create_unique('Reporting_interaction_sepermissives', ['interaction_id', 'sepermissiveentry_id'])

        # Adding M2M table for field semodules on 'Interaction'
        db.create_table('Reporting_interaction_semodules', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('interaction', models.ForeignKey(orm['Reporting.interaction'], null=False)),
            ('semoduleentry', models.ForeignKey(orm['Reporting.semoduleentry'], null=False))
        ))
        db.create_unique('Reporting_interaction_semodules', ['interaction_id', 'semoduleentry_id'])


    def backwards(self, orm):
        # Deleting model 'SELoginEntry'
        db.delete_table('Reporting_seloginentry')

        # Deleting model 'SEUserEntry'
        db.delete_table('Reporting_seuserentry')

        # Deleting model 'SEBooleanEntry'
        db.delete_table('Reporting_sebooleanentry')

        # Deleting model 'SENodeEntry'
        db.delete_table('Reporting_senodeentry')

        # Deleting model 'SEFcontextEntry'
        db.delete_table('Reporting_sefcontextentry')

        # Deleting model 'SEInterfaceEntry'
        db.delete_table('Reporting_seinterfaceentry')

        # Deleting model 'SEPermissiveEntry'
        db.delete_table('Reporting_sepermissiveentry')

        # Deleting model 'SEModuleEntry'
        db.delete_table('Reporting_semoduleentry')

        # Deleting model 'SEPortEntry'
        db.delete_table('Reporting_seportentry')

        # Removing M2M table for field sebooleans on 'Interaction'
        db.delete_table('Reporting_interaction_sebooleans')

        # Removing M2M table for field seports on 'Interaction'
        db.delete_table('Reporting_interaction_seports')

        # Removing M2M table for field sefcontexts on 'Interaction'
        db.delete_table('Reporting_interaction_sefcontexts')

        # Removing M2M table for field senodes on 'Interaction'
        db.delete_table('Reporting_interaction_senodes')

        # Removing M2M table for field selogins on 'Interaction'
        db.delete_table('Reporting_interaction_selogins')

        # Removing M2M table for field seusers on 'Interaction'
        db.delete_table('Reporting_interaction_seusers')

        # Removing M2M table for field seinterfaces on 'Interaction'
        db.delete_table('Reporting_interaction_seinterfaces')

        # Removing M2M table for field sepermissives on 'Interaction'
        db.delete_table('Reporting_interaction_sepermissives')

        # Removing M2M table for field semodules on 'Interaction'
        db.delete_table('Reporting_interaction_semodules')


    models = {
        'Reporting.actionentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'ActionEntry'},
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'output': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'status': ('django.db.models.fields.CharField', [], {'default': "'check'", 'max_length': '128'})
        },
        'Reporting.bundle': {
            'Meta': {'ordering': "('name',)", 'object_name': 'Bundle'},
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '255'})
        },
        'Reporting.client': {
            'Meta': {'object_name': 'Client'},
            'creation': ('django.db.models.fields.DateTimeField', [], {'auto_now_add': 'True', 'blank': 'True'}),
            'current_interaction': ('django.db.models.fields.related.ForeignKey', [], {'blank': 'True', 'related_name': "'parent_client'", 'null': 'True', 'to': "orm['Reporting.Interaction']"}),
            'expiration': ('django.db.models.fields.DateTimeField', [], {'null': 'True', 'blank': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128'})
        },
        'Reporting.deviceentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'DeviceEntry', '_ormbases': ['Reporting.PathEntry']},
            'current_major': ('django.db.models.fields.IntegerField', [], {}),
            'current_minor': ('django.db.models.fields.IntegerField', [], {}),
            'device_type': ('django.db.models.fields.CharField', [], {'max_length': '16'}),
            'pathentry_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': "orm['Reporting.PathEntry']", 'unique': 'True', 'primary_key': 'True'}),
            'target_major': ('django.db.models.fields.IntegerField', [], {}),
            'target_minor': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.failureentry': {
            'Meta': {'object_name': 'FailureEntry'},
            'entry_type': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'message': ('django.db.models.fields.TextField', [], {}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'})
        },
        'Reporting.fileacl': {
            'Meta': {'object_name': 'FileAcl'},
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'})
        },
        'Reporting.fileperms': {
            'Meta': {'unique_together': "(('owner', 'group', 'mode'),)", 'object_name': 'FilePerms'},
            'group': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'mode': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'owner': ('django.db.models.fields.CharField', [], {'max_length': '128'})
        },
        'Reporting.group': {
            'Meta': {'ordering': "('name',)", 'object_name': 'Group'},
            'bundles': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Bundle']", 'symmetrical': 'False'}),
            'category': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'blank': 'True'}),
            'comment': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'groups': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Group']", 'symmetrical': 'False'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '255'}),
            'profile': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),
            'public': ('django.db.models.fields.BooleanField', [], {'default': 'False'})
        },
        'Reporting.interaction': {
            'Meta': {'ordering': "['-timestamp']", 'unique_together': "(('client', 'timestamp'),)", 'object_name': 'Interaction'},
            'actions': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.ActionEntry']", 'symmetrical': 'False'}),
            'bad_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'bundles': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Bundle']", 'symmetrical': 'False'}),
            'client': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'interactions'", 'to': "orm['Reporting.Client']"}),
            'extra_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'failures': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.FailureEntry']", 'symmetrical': 'False'}),
            'good_count': ('django.db.models.fields.IntegerField', [], {}),
            'groups': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Group']", 'symmetrical': 'False'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'modified_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'packages': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.PackageEntry']", 'symmetrical': 'False'}),
            'paths': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.PathEntry']", 'symmetrical': 'False'}),
            'profile': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'+'", 'null': 'True', 'to': "orm['Reporting.Group']"}),
            'repo_rev_code': ('django.db.models.fields.CharField', [], {'max_length': '64'}),
            'sebooleans': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.SEBooleanEntry']", 'symmetrical': 'False'}),
            'sefcontexts': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.SEFcontextEntry']", 'symmetrical': 'False'}),
            'seinterfaces': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.SEInterfaceEntry']", 'symmetrical': 'False'}),
            'selogins': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.SELoginEntry']", 'symmetrical': 'False'}),
            'semodules': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.SEModuleEntry']", 'symmetrical': 'False'}),
            'senodes': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.SENodeEntry']", 'symmetrical': 'False'}),
            'sepermissives': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.SEPermissiveEntry']", 'symmetrical': 'False'}),
            'seports': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.SEPortEntry']", 'symmetrical': 'False'}),
            'server': ('django.db.models.fields.CharField', [], {'max_length': '256'}),
            'services': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.ServiceEntry']", 'symmetrical': 'False'}),
            'seusers': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.SEUserEntry']", 'symmetrical': 'False'}),
            'state': ('django.db.models.fields.CharField', [], {'max_length': '32'}),
            'timestamp': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True'}),
            'total_count': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.linkentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'LinkEntry', '_ormbases': ['Reporting.PathEntry']},
            'current_path': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'blank': 'True'}),
            'pathentry_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': "orm['Reporting.PathEntry']", 'unique': 'True', 'primary_key': 'True'}),
            'target_path': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'blank': 'True'})
        },
        'Reporting.packageentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'PackageEntry'},
            'current_version': ('django.db.models.fields.CharField', [], {'max_length': '1024'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'target_version': ('django.db.models.fields.CharField', [], {'default': "''", 'max_length': '1024'}),
            'verification_details': ('django.db.models.fields.TextField', [], {'default': "''"})
        },
        'Reporting.pathentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'PathEntry'},
            'acls': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.FileAcl']", 'symmetrical': 'False'}),
            'current_perms': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'+'", 'to': "orm['Reporting.FilePerms']"}),
            'detail_type': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'details': ('django.db.models.fields.TextField', [], {'default': "''"}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'path_type': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'target_perms': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'+'", 'to': "orm['Reporting.FilePerms']"})
        },
        'Reporting.performance': {
            'Meta': {'object_name': 'Performance'},
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'interaction': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'performance_items'", 'to': "orm['Reporting.Interaction']"}),
            'metric': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'value': ('django.db.models.fields.DecimalField', [], {'max_digits': '32', 'decimal_places': '16'})
        },
        'Reporting.sebooleanentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'SEBooleanEntry'},
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'value': ('django.db.models.fields.BooleanField', [], {'default': 'True'})
        },
        'Reporting.sefcontextentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'SEFcontextEntry'},
            'current_selinuxtype': ('django.db.models.fields.CharField', [], {'max_length': '128', 'null': 'True'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'filetype': ('django.db.models.fields.CharField', [], {'max_length': '16'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'selinuxtype': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'state': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.seinterfaceentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'SEInterfaceEntry'},
            'current_selinuxtype': ('django.db.models.fields.CharField', [], {'max_length': '128', 'null': 'True'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'selinuxtype': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'state': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.seloginentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'SELoginEntry'},
            'current_selinuxuser': ('django.db.models.fields.CharField', [], {'max_length': '128', 'null': 'True'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'selinuxuser': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'state': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.semoduleentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'SEModuleEntry'},
            'current_disabled': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),
            'disabled': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'state': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.senodeentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'SENodeEntry'},
            'current_selinuxtype': ('django.db.models.fields.CharField', [], {'max_length': '128', 'null': 'True'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'proto': ('django.db.models.fields.CharField', [], {'max_length': '4'}),
            'selinuxtype': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'state': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.sepermissiveentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'SEPermissiveEntry'},
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'state': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.seportentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'SEPortEntry'},
            'current_selinuxtype': ('django.db.models.fields.CharField', [], {'max_length': '128', 'null': 'True'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'selinuxtype': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'state': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.serviceentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'ServiceEntry'},
            'current_status': ('django.db.models.fields.CharField', [], {'default': "''", 'max_length': '128'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'target_status': ('django.db.models.fields.CharField', [], {'default': "''", 'max_length': '128'})
        },
        'Reporting.seuserentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'SEUserEntry'},
            'current_prefix': ('django.db.models.fields.CharField', [], {'max_length': '128', 'null': 'True'}),
            'current_roles': ('django.db.models.fields.CharField', [], {'max_length': '128', 'null': 'True'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'prefix': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'roles': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'state': ('django.db.models.fields.IntegerField', [], {})
        }
    }

    complete_apps = ['Reporting']
########NEW FILE########
__FILENAME__ = 0006_add_user_group_entry_support
# -*- coding: utf-8 -*-
import datetime
from south.db import db
from south.v2 import SchemaMigration
from django.db import models


class Migration(SchemaMigration):

    def forwards(self, orm):
        # Adding model 'POSIXGroupEntry'
        db.create_table('Reporting_posixgroupentry', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=128, db_index=True)),
            ('hash_key', self.gf('django.db.models.fields.BigIntegerField')(db_index=True)),
            ('state', self.gf('django.db.models.fields.IntegerField')()),
            ('exists', self.gf('django.db.models.fields.BooleanField')(default=True)),
            ('gid', self.gf('django.db.models.fields.IntegerField')(null=True)),
            ('current_gid', self.gf('django.db.models.fields.IntegerField')(null=True)),
        ))
        db.send_create_signal('Reporting', ['POSIXGroupEntry'])

        # Adding model 'POSIXUserEntry'
        db.create_table('Reporting_posixuserentry', (
            ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
            ('name', self.gf('django.db.models.fields.CharField')(max_length=128, db_index=True)),
            ('hash_key', self.gf('django.db.models.fields.BigIntegerField')(db_index=True)),
            ('state', self.gf('django.db.models.fields.IntegerField')()),
            ('exists', self.gf('django.db.models.fields.BooleanField')(default=True)),
            ('uid', self.gf('django.db.models.fields.IntegerField')(null=True)),
            ('current_uid', self.gf('django.db.models.fields.IntegerField')(null=True)),
            ('group', self.gf('django.db.models.fields.CharField')(max_length=64)),
            ('current_group', self.gf('django.db.models.fields.CharField')(max_length=64, null=True)),
            ('gecos', self.gf('django.db.models.fields.CharField')(max_length=1024)),
            ('current_gecos', self.gf('django.db.models.fields.CharField')(max_length=1024, null=True)),
            ('home', self.gf('django.db.models.fields.CharField')(max_length=1024)),
            ('current_home', self.gf('django.db.models.fields.CharField')(max_length=1024, null=True)),
            ('shell', self.gf('django.db.models.fields.CharField')(default='/bin/bash', max_length=1024)),
            ('current_shell', self.gf('django.db.models.fields.CharField')(max_length=1024, null=True)),
        ))
        db.send_create_signal('Reporting', ['POSIXUserEntry'])

        # Adding M2M table for field posixusers on 'Interaction'
        db.create_table('Reporting_interaction_posixusers', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('interaction', models.ForeignKey(orm['Reporting.interaction'], null=False)),
            ('posixuserentry', models.ForeignKey(orm['Reporting.posixuserentry'], null=False))
        ))
        db.create_unique('Reporting_interaction_posixusers', ['interaction_id', 'posixuserentry_id'])

        # Adding M2M table for field posixgroups on 'Interaction'
        db.create_table('Reporting_interaction_posixgroups', (
            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),
            ('interaction', models.ForeignKey(orm['Reporting.interaction'], null=False)),
            ('posixgroupentry', models.ForeignKey(orm['Reporting.posixgroupentry'], null=False))
        ))
        db.create_unique('Reporting_interaction_posixgroups', ['interaction_id', 'posixgroupentry_id'])


    def backwards(self, orm):
        # Deleting model 'POSIXGroupEntry'
        db.delete_table('Reporting_posixgroupentry')

        # Deleting model 'POSIXUserEntry'
        db.delete_table('Reporting_posixuserentry')

        # Removing M2M table for field posixusers on 'Interaction'
        db.delete_table('Reporting_interaction_posixusers')

        # Removing M2M table for field posixgroups on 'Interaction'
        db.delete_table('Reporting_interaction_posixgroups')


    models = {
        'Reporting.actionentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'ActionEntry'},
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'output': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'status': ('django.db.models.fields.CharField', [], {'default': "'check'", 'max_length': '128'})
        },
        'Reporting.bundle': {
            'Meta': {'ordering': "('name',)", 'object_name': 'Bundle'},
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '255'})
        },
        'Reporting.client': {
            'Meta': {'object_name': 'Client'},
            'creation': ('django.db.models.fields.DateTimeField', [], {'auto_now_add': 'True', 'blank': 'True'}),
            'current_interaction': ('django.db.models.fields.related.ForeignKey', [], {'blank': 'True', 'related_name': "'parent_client'", 'null': 'True', 'to': "orm['Reporting.Interaction']"}),
            'expiration': ('django.db.models.fields.DateTimeField', [], {'null': 'True', 'blank': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128'})
        },
        'Reporting.deviceentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'DeviceEntry', '_ormbases': ['Reporting.PathEntry']},
            'current_major': ('django.db.models.fields.IntegerField', [], {}),
            'current_minor': ('django.db.models.fields.IntegerField', [], {}),
            'device_type': ('django.db.models.fields.CharField', [], {'max_length': '16'}),
            'pathentry_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': "orm['Reporting.PathEntry']", 'unique': 'True', 'primary_key': 'True'}),
            'target_major': ('django.db.models.fields.IntegerField', [], {}),
            'target_minor': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.failureentry': {
            'Meta': {'object_name': 'FailureEntry'},
            'entry_type': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'message': ('django.db.models.fields.TextField', [], {}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'})
        },
        'Reporting.fileacl': {
            'Meta': {'object_name': 'FileAcl'},
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'})
        },
        'Reporting.fileperms': {
            'Meta': {'unique_together': "(('owner', 'group', 'mode'),)", 'object_name': 'FilePerms'},
            'group': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'mode': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'owner': ('django.db.models.fields.CharField', [], {'max_length': '128'})
        },
        'Reporting.group': {
            'Meta': {'ordering': "('name',)", 'object_name': 'Group'},
            'bundles': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Bundle']", 'symmetrical': 'False'}),
            'category': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'blank': 'True'}),
            'comment': ('django.db.models.fields.TextField', [], {'blank': 'True'}),
            'groups': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Group']", 'symmetrical': 'False'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '255'}),
            'profile': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),
            'public': ('django.db.models.fields.BooleanField', [], {'default': 'False'})
        },
        'Reporting.interaction': {
            'Meta': {'ordering': "['-timestamp']", 'unique_together': "(('client', 'timestamp'),)", 'object_name': 'Interaction'},
            'actions': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.ActionEntry']", 'symmetrical': 'False'}),
            'bad_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'bundles': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Bundle']", 'symmetrical': 'False'}),
            'client': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'interactions'", 'to': "orm['Reporting.Client']"}),
            'extra_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'failures': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.FailureEntry']", 'symmetrical': 'False'}),
            'good_count': ('django.db.models.fields.IntegerField', [], {}),
            'groups': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.Group']", 'symmetrical': 'False'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'modified_count': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'packages': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.PackageEntry']", 'symmetrical': 'False'}),
            'paths': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.PathEntry']", 'symmetrical': 'False'}),
            'posixgroups': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.POSIXGroupEntry']", 'symmetrical': 'False'}),
            'posixusers': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.POSIXUserEntry']", 'symmetrical': 'False'}),
            'profile': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'+'", 'null': 'True', 'to': "orm['Reporting.Group']"}),
            'repo_rev_code': ('django.db.models.fields.CharField', [], {'max_length': '64'}),
            'sebooleans': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.SEBooleanEntry']", 'symmetrical': 'False'}),
            'sefcontexts': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.SEFcontextEntry']", 'symmetrical': 'False'}),
            'seinterfaces': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.SEInterfaceEntry']", 'symmetrical': 'False'}),
            'selogins': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.SELoginEntry']", 'symmetrical': 'False'}),
            'semodules': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.SEModuleEntry']", 'symmetrical': 'False'}),
            'senodes': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.SENodeEntry']", 'symmetrical': 'False'}),
            'sepermissives': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.SEPermissiveEntry']", 'symmetrical': 'False'}),
            'seports': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.SEPortEntry']", 'symmetrical': 'False'}),
            'server': ('django.db.models.fields.CharField', [], {'max_length': '256'}),
            'services': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.ServiceEntry']", 'symmetrical': 'False'}),
            'seusers': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.SEUserEntry']", 'symmetrical': 'False'}),
            'state': ('django.db.models.fields.CharField', [], {'max_length': '32'}),
            'timestamp': ('django.db.models.fields.DateTimeField', [], {'db_index': 'True'}),
            'total_count': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.linkentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'LinkEntry', '_ormbases': ['Reporting.PathEntry']},
            'current_path': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'blank': 'True'}),
            'pathentry_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': "orm['Reporting.PathEntry']", 'unique': 'True', 'primary_key': 'True'}),
            'target_path': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'blank': 'True'})
        },
        'Reporting.packageentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'PackageEntry'},
            'current_version': ('django.db.models.fields.CharField', [], {'max_length': '1024'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'target_version': ('django.db.models.fields.CharField', [], {'default': "''", 'max_length': '1024'}),
            'verification_details': ('django.db.models.fields.TextField', [], {'default': "''"})
        },
        'Reporting.pathentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'PathEntry'},
            'acls': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['Reporting.FileAcl']", 'symmetrical': 'False'}),
            'current_perms': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'+'", 'to': "orm['Reporting.FilePerms']"}),
            'detail_type': ('django.db.models.fields.IntegerField', [], {'default': '0'}),
            'details': ('django.db.models.fields.TextField', [], {'default': "''"}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'path_type': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'target_perms': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'+'", 'to': "orm['Reporting.FilePerms']"})
        },
        'Reporting.performance': {
            'Meta': {'object_name': 'Performance'},
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'interaction': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'performance_items'", 'to': "orm['Reporting.Interaction']"}),
            'metric': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'value': ('django.db.models.fields.DecimalField', [], {'max_digits': '32', 'decimal_places': '16'})
        },
        'Reporting.posixgroupentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'POSIXGroupEntry'},
            'current_gid': ('django.db.models.fields.IntegerField', [], {'null': 'True'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'gid': ('django.db.models.fields.IntegerField', [], {'null': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'state': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.posixuserentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'POSIXUserEntry'},
            'current_gecos': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'null': 'True'}),
            'current_group': ('django.db.models.fields.CharField', [], {'max_length': '64', 'null': 'True'}),
            'current_home': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'null': 'True'}),
            'current_shell': ('django.db.models.fields.CharField', [], {'max_length': '1024', 'null': 'True'}),
            'current_uid': ('django.db.models.fields.IntegerField', [], {'null': 'True'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'gecos': ('django.db.models.fields.CharField', [], {'max_length': '1024'}),
            'group': ('django.db.models.fields.CharField', [], {'max_length': '64'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'home': ('django.db.models.fields.CharField', [], {'max_length': '1024'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'shell': ('django.db.models.fields.CharField', [], {'default': "'/bin/bash'", 'max_length': '1024'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'uid': ('django.db.models.fields.IntegerField', [], {'null': 'True'})
        },
        'Reporting.sebooleanentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'SEBooleanEntry'},
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'value': ('django.db.models.fields.BooleanField', [], {'default': 'True'})
        },
        'Reporting.sefcontextentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'SEFcontextEntry'},
            'current_selinuxtype': ('django.db.models.fields.CharField', [], {'max_length': '128', 'null': 'True'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'filetype': ('django.db.models.fields.CharField', [], {'max_length': '16'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'selinuxtype': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'state': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.seinterfaceentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'SEInterfaceEntry'},
            'current_selinuxtype': ('django.db.models.fields.CharField', [], {'max_length': '128', 'null': 'True'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'selinuxtype': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'state': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.seloginentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'SELoginEntry'},
            'current_selinuxuser': ('django.db.models.fields.CharField', [], {'max_length': '128', 'null': 'True'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'selinuxuser': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'state': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.semoduleentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'SEModuleEntry'},
            'current_disabled': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),
            'disabled': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'state': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.senodeentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'SENodeEntry'},
            'current_selinuxtype': ('django.db.models.fields.CharField', [], {'max_length': '128', 'null': 'True'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'proto': ('django.db.models.fields.CharField', [], {'max_length': '4'}),
            'selinuxtype': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'state': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.sepermissiveentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'SEPermissiveEntry'},
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'state': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.seportentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'SEPortEntry'},
            'current_selinuxtype': ('django.db.models.fields.CharField', [], {'max_length': '128', 'null': 'True'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'selinuxtype': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'state': ('django.db.models.fields.IntegerField', [], {})
        },
        'Reporting.serviceentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'ServiceEntry'},
            'current_status': ('django.db.models.fields.CharField', [], {'default': "''", 'max_length': '128'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'state': ('django.db.models.fields.IntegerField', [], {}),
            'target_status': ('django.db.models.fields.CharField', [], {'default': "''", 'max_length': '128'})
        },
        'Reporting.seuserentry': {
            'Meta': {'ordering': "('state', 'name')", 'object_name': 'SEUserEntry'},
            'current_prefix': ('django.db.models.fields.CharField', [], {'max_length': '128', 'null': 'True'}),
            'current_roles': ('django.db.models.fields.CharField', [], {'max_length': '128', 'null': 'True'}),
            'exists': ('django.db.models.fields.BooleanField', [], {'default': 'True'}),
            'hash_key': ('django.db.models.fields.BigIntegerField', [], {'db_index': 'True'}),
            'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'name': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}),
            'prefix': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'roles': ('django.db.models.fields.CharField', [], {'max_length': '128'}),
            'state': ('django.db.models.fields.IntegerField', [], {})
        }
    }

    complete_apps = ['Reporting']
########NEW FILE########
__FILENAME__ = models
"""Django models for Bcfg2 reports."""
import sys

from django.core.exceptions import ImproperlyConfigured
try:
    from django.db import models, backend, connection
except ImproperlyConfigured:
    e = sys.exc_info()[1]
    print("Reports: unable to import django models: %s" % e)
    sys.exit(1)

from django.core.cache import cache
from datetime import datetime, timedelta
from Bcfg2.Compat import cPickle


TYPE_GOOD = 0
TYPE_BAD = 1
TYPE_MODIFIED = 2
TYPE_EXTRA = 3

TYPE_CHOICES = (
    (TYPE_GOOD, 'Good'),
    (TYPE_BAD, 'Bad'),
    (TYPE_MODIFIED, 'Modified'),
    (TYPE_EXTRA, 'Extra'),
)

_our_backend = None


def convert_entry_type_to_id(type_name):
    """Convert a entry type to its entry id"""
    for e_id, e_name in TYPE_CHOICES:
        if e_name.lower() == type_name.lower():
            return e_id
    return -1


def hash_entry(entry_dict):
    """
    Build a key for this based on its data

    entry_dict = a dict of all the data identifying this
    """
    dataset = []
    for key in sorted(entry_dict.keys()):
        if key in ('id', 'hash_key') or key.startswith('_'):
            continue
        dataset.append((key, entry_dict[key]))
    return hash(cPickle.dumps(dataset))


def _quote(value):
    """
    Quote a string to use as a table name or column

    Newer versions and various drivers require an argument
    https://code.djangoproject.com/ticket/13630
    """
    global _our_backend
    if not _our_backend:
        try:
            _our_backend = backend.DatabaseOperations(connection)
        except TypeError:
            _our_backend = backend.DatabaseOperations()
    return _our_backend.quote_name(value)


class Client(models.Model):
    """Object representing every client we have seen stats for."""
    creation = models.DateTimeField(auto_now_add=True)
    name = models.CharField(max_length=128,)
    current_interaction = models.ForeignKey('Interaction',
                                            null=True, blank=True,
                                            related_name="parent_client")
    expiration = models.DateTimeField(blank=True, null=True)

    def __str__(self):
        return self.name


class InteractionManager(models.Manager):
    """Manages interactions objects."""

    def recent_ids(self, maxdate=None):
        """
        Returns the ids of most recent interactions for clients as of a date.

        Arguments:
        maxdate -- datetime object.  Most recent date to pull. (default None)

        """
        from django.db import connection
        cursor = connection.cursor()
        cfilter = "expiration is null"

        sql = 'select ri.id, x.client_id from ' + \
              '(select client_id, MAX(timestamp) as timer from ' + \
              _quote('Reporting_interaction')
        if maxdate:
            if not isinstance(maxdate, datetime):
                raise ValueError('Expected a datetime object')
            sql = sql + " where timestamp <= '%s' " % maxdate
            cfilter = "(expiration is null or expiration > '%s') and creation <= '%s'" % (maxdate, maxdate)
        sql = sql + ' GROUP BY client_id) x, ' + \
                    _quote('Reporting_interaction') + \
                    ' ri where ri.client_id = x.client_id AND' + \
                    ' ri.timestamp = x.timer and x.client_id in' + \
                    ' (select id from %s where %s)' % \
                    (_quote('Reporting_client'), cfilter)
        try:
            cursor.execute(sql)
            return [item[0] for item in cursor.fetchall()]
        except:
            '''FIXME - really need some error handling'''
            pass
        return []

    def recent(self, maxdate=None):
        """
        Returns the most recent interactions for clients as of a date
        Arguments:
        maxdate -- datetime object.  Most recent date to pull. (dafault None)

        """
        if maxdate and not isinstance(maxdate, datetime):
            raise ValueError('Expected a datetime object')
        return self.filter(id__in=self.recent_ids(maxdate))


class Interaction(models.Model):
    """ Models each reconfiguration operation interaction between
    client and server. """
    client = models.ForeignKey(Client, related_name="interactions")
    timestamp = models.DateTimeField(db_index=True)  # Timestamp for this record
    state = models.CharField(max_length=32)  # good/bad/modified/etc
    repo_rev_code = models.CharField(max_length=64)  # repo revision at time of interaction
    server = models.CharField(max_length=256)  # server used for interaction
    good_count = models.IntegerField()  # of good config-items
    total_count = models.IntegerField()  # of total config-items
    bad_count = models.IntegerField(default=0)
    modified_count = models.IntegerField(default=0)
    extra_count = models.IntegerField(default=0)

    actions = models.ManyToManyField("ActionEntry")
    packages = models.ManyToManyField("PackageEntry")
    paths = models.ManyToManyField("PathEntry")
    services = models.ManyToManyField("ServiceEntry")
    sebooleans = models.ManyToManyField("SEBooleanEntry")
    seports = models.ManyToManyField("SEPortEntry")
    sefcontexts = models.ManyToManyField("SEFcontextEntry")
    senodes = models.ManyToManyField("SENodeEntry")
    selogins = models.ManyToManyField("SELoginEntry")
    seusers = models.ManyToManyField("SEUserEntry")
    seinterfaces = models.ManyToManyField("SEInterfaceEntry")
    sepermissives = models.ManyToManyField("SEPermissiveEntry")
    semodules = models.ManyToManyField("SEModuleEntry")
    posixusers = models.ManyToManyField("POSIXUserEntry")
    posixgroups = models.ManyToManyField("POSIXGroupEntry")
    failures = models.ManyToManyField("FailureEntry")

    entry_types = ('actions', 'failures', 'packages',
                   'paths', 'services', 'sebooleans',
                   'seports', 'sefcontexts', 'senodes',
                   'selogins', 'seusers', 'seinterfaces',
                   'sepermissives', 'semodules', 'posixusers',
                   'posixgroups')

    # Formerly InteractionMetadata
    profile = models.ForeignKey("Group", related_name="+", null=True)
    groups = models.ManyToManyField("Group")
    bundles = models.ManyToManyField("Bundle")

    objects = InteractionManager()

    def __str__(self):
        return "With " + self.client.name + " @ " + self.timestamp.isoformat()

    def percentgood(self):
        if not self.total_count == 0:
            return (self.good_count / float(self.total_count)) * 100
        else:
            return 0

    def percentbad(self):
        if not self.total_count == 0:
            return ((self.total_count - self.good_count) /
                    (float(self.total_count))) * 100
        else:
            return 0

    def isclean(self):
        if (self.bad_count == 0 and self.good_count == self.total_count):
            return True
        else:
            return False

    def isstale(self):
        if (self == self.client.current_interaction):  # Is Mostrecent
            if(datetime.now() - self.timestamp > timedelta(hours=25)):
                return True
            else:
                return False
        else:
            #Search for subsequent Interaction for this client
            #Check if it happened more than 25 hrs ago.
            if (self.client.interactions.filter(timestamp__gt=self.timestamp)
                    .order_by('timestamp')[0].timestamp -
                    self.timestamp > timedelta(hours=25)):
                return True
            else:
                return False

    def save(self):
        super(Interaction, self).save()  # call the real save...
        self.client.current_interaction = self.client.interactions.latest()
        self.client.save()  # save again post update

    def delete(self):
        '''Override the default delete.  Allows us to remove
        Performance items '''
        pitems = list(self.performance_items.all())
        super(Interaction, self).delete()
        for perf in pitems:
            if perf.interaction.count() == 0:
                perf.delete()

    def badcount(self):
        return self.total_count - self.good_count

    def bad(self):
        rv = []
        for entry in self.entry_types:
            if entry == 'failures':
                continue
            rv.extend(getattr(self, entry).filter(state=TYPE_BAD))
        return rv

    def modified(self):
        rv = []
        for entry in self.entry_types:
            if entry == 'failures':
                continue
            rv.extend(getattr(self, entry).filter(state=TYPE_MODIFIED))
        return rv

    def extra(self):
        rv = []
        for entry in self.entry_types:
            if entry == 'failures':
                continue
            rv.extend(getattr(self, entry).filter(state=TYPE_EXTRA))
        return rv

    class Meta:
        get_latest_by = 'timestamp'
        ordering = ['-timestamp']
        unique_together = ("client", "timestamp")


class Performance(models.Model):
    """Object representing performance data for any interaction."""
    interaction = models.ForeignKey(Interaction,
                                    related_name="performance_items")
    metric = models.CharField(max_length=128)
    value = models.DecimalField(max_digits=32, decimal_places=16)

    def __str__(self):
        return self.metric


class Group(models.Model):
    """
    Groups extracted from interactions

    name - The group name

    TODO - Most of this is for future use
    TODO - set a default group
    """

    name = models.CharField(max_length=255, unique=True)
    profile = models.BooleanField(default=False)
    public = models.BooleanField(default=False)
    category = models.CharField(max_length=1024, blank=True)
    comment = models.TextField(blank=True)

    groups = models.ManyToManyField("self", symmetrical=False)
    bundles = models.ManyToManyField("Bundle")

    def __unicode__(self):
        return self.name

    class Meta:
        ordering = ('name',)

    @staticmethod
    def prune_orphans():
        '''Prune unused groups'''
        Group.objects.filter(interaction__isnull=True,
                             group__isnull=True).delete()


class Bundle(models.Model):
    """
    Bundles extracted from interactions

    name - The bundle name
    """

    name = models.CharField(max_length=255, unique=True)

    def __unicode__(self):
        return self.name

    class Meta:
        ordering = ('name',)

    @staticmethod
    def prune_orphans():
        '''Prune unused bundles'''
        Bundle.objects.filter(interaction__isnull=True,
                              group__isnull=True).delete()


# new interaction models
class FilePerms(models.Model):
    owner = models.CharField(max_length=128)
    group = models.CharField(max_length=128)
    mode = models.CharField(max_length=128)

    class Meta:
        unique_together = ('owner', 'group', 'mode')

    def empty(self):
        """Return true if we have no real data"""
        if self.owner or self.group or self.mode:
            return False
        else:
            return True


class FileAcl(models.Model):
    """Placeholder"""
    name = models.CharField(max_length=128, db_index=True)


class BaseEntry(models.Model):
    """ Abstract base for all entry types """
    name = models.CharField(max_length=128, db_index=True)
    hash_key = models.BigIntegerField(editable=False, db_index=True)

    class Meta:
        abstract = True

    def save(self, *args, **kwargs):
        if 'hash_key' in kwargs:
            self.hash_key = kwargs['hash_key']
            del kwargs['hash_key']
        else:
            self.hash_key = hash_entry(self.__dict__)
        super(BaseEntry, self).save(*args, **kwargs)

    def class_name(self):
        return self.__class__.__name__

    def short_list(self):
        """todo"""
        return []

    @classmethod
    def entry_from_name(cls, name):
        try:
            newcls = globals()[name]
            if not isinstance(newcls(), cls):
                raise ValueError("%s is not an instance of %s" % (name, cls))
            return newcls
        except KeyError:
            raise ValueError("Invalid type %s" % name)

    @classmethod
    def entry_from_type(cls, etype):
        for entry_cls in ENTRY_CLASSES:
            if etype == entry_cls.ENTRY_TYPE:
                return entry_cls
        else:
            raise ValueError("Invalid type %s" % etype)

    @classmethod
    def entry_get_or_create(cls, act_dict):
        """Helper to quickly lookup an object"""
        cls_name = cls().__class__.__name__
        act_hash = hash_entry(act_dict)

        # TODO - get form cache and validate
        act_key = "%s_%s" % (cls_name, act_hash)
        newact = cache.get(act_key)
        if newact:
            return newact

        acts = cls.objects.filter(hash_key=act_hash)
        if len(acts) > 0:
            for act in acts:
                for key in act_dict:
                    if act_dict[key] != getattr(act, key):
                        continue
                    #match found
                    newact = act
                    break

        # worst case, its new
        if not newact:
            newact = cls(**act_dict)
            newact.save(hash_key=act_hash)

        cache.set(act_key, newact, 60 * 60)
        return newact

    def is_failure(self):
        return isinstance(self, FailureEntry)

    @classmethod
    def prune_orphans(cls):
        '''Remove unused entries'''
        # yeat another sqlite hack
        cls_orphans = [x['id']
            for x in cls.objects.filter(interaction__isnull=True).values("id")]
        i = 0
        while i < len(cls_orphans):
            cls.objects.filter(id__in=cls_orphans[i:i + 100]).delete()
            i += 100


class SuccessEntry(BaseEntry):
    """Base for successful entries"""
    state = models.IntegerField(choices=TYPE_CHOICES)
    exists = models.BooleanField(default=True)

    ENTRY_TYPE = r"Success"

    @property
    def entry_type(self):
        return self.ENTRY_TYPE

    def is_extra(self):
        return self.state == TYPE_EXTRA

    class Meta:
        abstract = True
        ordering = ('state', 'name')

    def short_list(self):
        """Return a list of problems"""
        rv = []
        if self.is_extra():
            rv.append("Extra")
        elif not self.exists:
            rv.append("Missing")
        return rv


class FailureEntry(BaseEntry):
    """Represents objects that failed to bind"""
    entry_type = models.CharField(max_length=128)
    message = models.TextField()

    def is_failure(self):
        return True


class ActionEntry(SuccessEntry):
    """ Action entry """
    status = models.CharField(max_length=128, default="check")
    output = models.IntegerField(default=0)

    ENTRY_TYPE = r"Action"


class SEBooleanEntry(SuccessEntry):
    """ SELinux boolean """
    value = models.BooleanField(default=True)

    ENTRY_TYPE = r"SEBoolean"


class SEPortEntry(SuccessEntry):
    """ SELinux port """
    selinuxtype = models.CharField(max_length=128)
    current_selinuxtype = models.CharField(max_length=128, null=True)

    ENTRY_TYPE = r"SEPort"

    def selinuxtype_problem(self):
        """Check for an selinux type problem."""
        if not self.current_selinuxtype:
            return True
        return self.selinuxtype != self.current_selinuxtype

    def short_list(self):
        """Return a list of problems"""
        rv = super(SEPortEntry, self).short_list()
        if self.selinuxtype_problem():
            rv.append("Wrong SELinux type")
        return rv


class SEFcontextEntry(SuccessEntry):
    """ SELinux file context """
    selinuxtype = models.CharField(max_length=128)
    current_selinuxtype = models.CharField(max_length=128, null=True)
    filetype = models.CharField(max_length=16)

    ENTRY_TYPE = r"SEFcontext"

    def selinuxtype_problem(self):
        """Check for an selinux type problem."""
        if not self.current_selinuxtype:
            return True
        return self.selinuxtype != self.current_selinuxtype

    def short_list(self):
        """Return a list of problems"""
        rv = super(SEFcontextEntry, self).short_list()
        if self.selinuxtype_problem():
            rv.append("Wrong SELinux type")
        return rv


class SENodeEntry(SuccessEntry):
    """ SELinux node """
    selinuxtype = models.CharField(max_length=128)
    current_selinuxtype = models.CharField(max_length=128, null=True)
    proto = models.CharField(max_length=4)

    ENTRY_TYPE = r"SENode"

    def selinuxtype_problem(self):
        """Check for an selinux type problem."""
        if not self.current_selinuxtype:
            return True
        return self.selinuxtype != self.current_selinuxtype

    def short_list(self):
        """Return a list of problems"""
        rv = super(SENodeEntry, self).short_list()
        if self.selinuxtype_problem():
            rv.append("Wrong SELinux type")
        return rv


class SELoginEntry(SuccessEntry):
    """ SELinux login """
    selinuxuser = models.CharField(max_length=128)
    current_selinuxuser = models.CharField(max_length=128, null=True)

    ENTRY_TYPE = r"SELogin"


class SEUserEntry(SuccessEntry):
    """ SELinux user """
    roles = models.CharField(max_length=128)
    current_roles = models.CharField(max_length=128, null=True)
    prefix = models.CharField(max_length=128)
    current_prefix = models.CharField(max_length=128, null=True)

    ENTRY_TYPE = r"SEUser"


class SEInterfaceEntry(SuccessEntry):
    """ SELinux interface """
    selinuxtype = models.CharField(max_length=128)
    current_selinuxtype = models.CharField(max_length=128, null=True)

    ENTRY_TYPE = r"SEInterface"

    def selinuxtype_problem(self):
        """Check for an selinux type problem."""
        if not self.current_selinuxtype:
            return True
        return self.selinuxtype != self.current_selinuxtype

    def short_list(self):
        """Return a list of problems"""
        rv = super(SEInterfaceEntry, self).short_list()
        if self.selinuxtype_problem():
            rv.append("Wrong SELinux type")
        return rv


class SEPermissiveEntry(SuccessEntry):
    """ SELinux permissive domain """
    ENTRY_TYPE = r"SEPermissive"


class SEModuleEntry(SuccessEntry):
    """ SELinux module """
    disabled = models.BooleanField(default=False)
    current_disabled = models.BooleanField(default=False)

    ENTRY_TYPE = r"SEModule"


class POSIXUserEntry(SuccessEntry):
    """ POSIX user """
    uid = models.IntegerField(null=True)
    current_uid = models.IntegerField(null=True)
    group = models.CharField(max_length=64)
    current_group = models.CharField(max_length=64, null=True)
    gecos = models.CharField(max_length=1024)
    current_gecos = models.CharField(max_length=1024, null=True)
    home = models.CharField(max_length=1024)
    current_home = models.CharField(max_length=1024, null=True)
    shell = models.CharField(max_length=1024, default='/bin/bash')
    current_shell = models.CharField(max_length=1024, null=True)

    ENTRY_TYPE = r"POSIXUser"


class POSIXGroupEntry(SuccessEntry):
    """ POSIX group """
    gid = models.IntegerField(null=True)
    current_gid = models.IntegerField(null=True)

    ENTRY_TYPE = r"POSIXGroup"


class PackageEntry(SuccessEntry):
    """ The new model for package information """

    # if this is an extra entry trget_version will be empty
    target_version = models.CharField(max_length=1024, default='')
    current_version = models.CharField(max_length=1024)
    verification_details = models.TextField(default="")

    ENTRY_TYPE = r"Package"
    # TODO - prune

    def version_problem(self):
        """Check for a version problem."""
        if not self.current_version:
            return True
        if self.target_version != self.current_version:
            return True
        elif self.target_version == 'auto':
            return True
        else:
            return False

    def short_list(self):
        """Return a list of problems"""
        rv = super(PackageEntry, self).short_list()
        if self.is_extra():
            return rv
        if not self.version_problem() or not self.exists:
            return rv
        if not self.current_version:
            rv.append("Missing")
        else:
            rv.append("Wrong version")
        return rv


class PathEntry(SuccessEntry):
    """reason why modified or bad entry did not verify, or changed."""

    PATH_TYPES = (
        ("device", "Device"),
        ("directory", "Directory"),
        ("hardlink", "Hard Link"),
        ("nonexistent", "Non Existent"),
        ("permissions", "Permissions"),
        ("symlink", "Symlink"),
    )

    DETAIL_UNUSED = 0
    DETAIL_DIFF = 1
    DETAIL_BINARY = 2
    DETAIL_SENSITIVE = 3
    DETAIL_SIZE_LIMIT = 4
    DETAIL_VCS = 5
    DETAIL_PRUNED = 6

    DETAIL_CHOICES = (
        (DETAIL_UNUSED, 'Unused'),
        (DETAIL_DIFF, 'Diff'),
        (DETAIL_BINARY, 'Binary'),
        (DETAIL_SENSITIVE, 'Sensitive'),
        (DETAIL_SIZE_LIMIT, 'Size limit exceeded'),
        (DETAIL_VCS, 'VCS output'),
        (DETAIL_PRUNED, 'Pruned paths'),
    )

    path_type = models.CharField(max_length=128, choices=PATH_TYPES)

    target_perms = models.ForeignKey(FilePerms, related_name="+")
    current_perms = models.ForeignKey(FilePerms, related_name="+")

    acls = models.ManyToManyField(FileAcl)

    detail_type = models.IntegerField(default=0,
                                      choices=DETAIL_CHOICES)
    details = models.TextField(default='')

    ENTRY_TYPE = r"Path"

    def mode_problem(self):
        if self.current_perms.empty():
            return False
        elif self.target_perms.mode != self.current_perms.mode:
            return True
        else:
            return False

    def has_detail(self):
        return self.detail_type != PathEntry.DETAIL_UNUSED

    def is_sensitive(self):
        return self.detail_type == PathEntry.DETAIL_SENSITIVE

    def is_diff(self):
        return self.detail_type == PathEntry.DETAIL_DIFF

    def is_sensitive(self):
        return self.detail_type == PathEntry.DETAIL_SENSITIVE

    def is_binary(self):
        return self.detail_type == PathEntry.DETAIL_BINARY

    def is_too_large(self):
        return self.detail_type == PathEntry.DETAIL_SIZE_LIMIT

    def short_list(self):
        """Return a list of problems"""
        rv = super(PathEntry, self).short_list()
        if self.is_extra():
            return rv
        if self.mode_problem():
            rv.append("File mode")
        if self.detail_type == PathEntry.DETAIL_PRUNED:
            rv.append("Directory has extra files")
        elif self.detail_type != PathEntry.DETAIL_UNUSED:
            rv.append("Incorrect data")
        if hasattr(self, 'linkentry') and self.linkentry and \
                self.linkentry.target_path != self.linkentry.current_path:
            rv.append("Incorrect target")
        return rv


class LinkEntry(PathEntry):
    """Sym/Hard Link types"""
    target_path = models.CharField(max_length=1024, blank=True)
    current_path = models.CharField(max_length=1024, blank=True)

    def link_problem(self):
        return self.target_path != self.current_path


class DeviceEntry(PathEntry):
    """Device types.  Best I can tell the client driver needs work here"""
    DEVICE_TYPES = (
        ("block", "Block"),
        ("char", "Char"),
        ("fifo", "Fifo"),
    )

    device_type = models.CharField(max_length=16, choices=DEVICE_TYPES)

    target_major = models.IntegerField()
    target_minor = models.IntegerField()
    current_major = models.IntegerField()
    current_minor = models.IntegerField()


class ServiceEntry(SuccessEntry):
    """ The new model for package information """
    target_status = models.CharField(max_length=128, default='')
    current_status = models.CharField(max_length=128, default='')

    ENTRY_TYPE = r"Service"
    #TODO - prune

    def status_problem(self):
        return self.target_status != self.current_status

    def short_list(self):
        """Return a list of problems"""
        rv = super(ServiceEntry, self).short_list()
        if self.status_problem():
            rv.append("Incorrect status")
        return rv


ENTRY_TYPES = (ActionEntry, PackageEntry, PathEntry, ServiceEntry,
               SEBooleanEntry, SEPortEntry, SEFcontextEntry, SENodeEntry,
               SELoginEntry, SEUserEntry, SEInterfaceEntry, SEPermissiveEntry,
               SEModuleEntry)

########NEW FILE########
__FILENAME__ = base
"""
The base for all Storage backends
"""

import logging 

class StorageError(Exception):
    """Generic StorageError"""
    pass

class StorageImportError(StorageError):
    """Raised when a storage module fails to import"""
    pass

class StorageBase(object):
    """The base for all storages"""

    __rmi__ = ['Ping', 'GetExtra', 'GetCurrentEntry']

    def __init__(self, setup):
        """Do something here"""
        clsname = self.__class__.__name__
        self.logger = logging.getLogger(clsname)
        self.logger.debug("Loading %s storage" % clsname)
        self.setup = setup
        self.encoding = setup['encoding']

    def import_interaction(self, interaction):
        """Import the data into the backend"""
        raise NotImplementedError

    def validate(self):
        """Validate backend storage.  Should be called once when loaded"""
        raise NotImplementedError

    def shutdown(self):
        """Called at program exit"""
        pass

    def Ping(self):
        """Test for communication with reporting collector"""
        return "Pong"

    def GetExtra(self, client):
        """Return a list of extra entries for a client.  Minestruct"""
        raise NotImplementedError

    def GetCurrentEntry(self, client, e_type, e_name):
        """Get the current status of an entry on the client"""
        raise NotImplementedError


########NEW FILE########
__FILENAME__ = DjangoORM
"""
The base for the original DjangoORM (DBStats)
"""

import os
import traceback
from lxml import etree
from datetime import datetime
from time import strptime

os.environ['DJANGO_SETTINGS_MODULE'] = 'Bcfg2.settings'
from Bcfg2 import settings

from Bcfg2.Compat import md5
from Bcfg2.Reporting.Storage.base import StorageBase, StorageError
from Bcfg2.Server.Plugin.exceptions import PluginExecutionError
from django.core import management
from django.core.exceptions import ObjectDoesNotExist, MultipleObjectsReturned
from django.db.models import FieldDoesNotExist
from django.core.cache import cache
from django import db

#Used by GetCurrentEntry
import difflib
from Bcfg2.Compat import b64decode
from Bcfg2.Reporting.models import *
from Bcfg2.Reporting.Compat import transaction


class DjangoORM(StorageBase):
    def __init__(self, setup):
        super(DjangoORM, self).__init__(setup)
        self.size_limit = setup.get('reporting_file_limit')

    def _import_default(self, entry, state, entrytype=None, defaults=None,
                        mapping=None, boolean=None, xforms=None):
        """ Default entry importer.  Maps the entry (in state
        ``state``) to an appropriate *Entry object; by default, this
        is determined by the entry tag, e.g., from an Action entry an
        ActionEntry object is created.  This can be overridden with
        ``entrytype``, which should be the class to instantiate for
        this entry.

        ``defaults`` is an optional mapping of <attribute
        name>:<value> that will be used to set the default values for
        various attributes.

        ``mapping`` is a mapping of <field name>:<attribute name> that
        can be used to map fields that are named differently on the
        XML entry and in the database model.

        ``boolean`` is a list of attribute names that should be
        treated as booleans.

        ``xforms`` is a dict of <attribute name>:<function>, where the
        given function will be applied to the value of the named
        attribute before trying to store it in the database.
        """
        if entrytype is None:
            entrytype = globals()["%sEntry" % entry.tag]
        if defaults is None:
            defaults = dict()
        if mapping is None:
            mapping = dict()
        if boolean is None:
            boolean = []
        if xforms is None:
            xforms = dict()
        mapping['exists'] = 'current_exists'
        defaults['current_exists'] = 'true'
        boolean.append("current_exists")

        def boolean_xform(val):
            try:
                return val.lower() == "true"
            except AttributeError:
                return False

        for attr in boolean + ["current_exists"]:
            xforms[attr] = boolean_xform
        act_dict = dict(state=state)
        for fieldname in entrytype._meta.get_all_field_names():
            if fieldname in ['id', 'hash_key', 'state']:
                continue
            try:
                field = entrytype._meta.get_field(fieldname)
            except FieldDoesNotExist:
                continue
            attrname = mapping.get(fieldname, fieldname)
            val = entry.get(fieldname, defaults.get(attrname))
            act_dict[fieldname] = xforms.get(attrname, lambda v: v)(val)
        self.logger.debug("Adding %s:%s" % (entry.tag, entry.get("name")))
        return entrytype.entry_get_or_create(act_dict)

    def _import_Action(self, entry, state):
        return self._import_default(entry, state,
                                    defaults=dict(status='check', rc=-1),
                                    mapping=dict(output="rc"))

    def _import_Package(self, entry, state):
        name = entry.get('name')
        exists = entry.get('current_exists', default="true").lower() == "true"
        act_dict = dict(name=name, state=state, exists=exists,
                        target_version=entry.get('version', default=''),
                        current_version=entry.get('current_version',
                                                  default=''))

        # extra entries are a bit different.  They can have Instance
        # objects
        if not act_dict['target_version']:
            for instance in entry.findall("Instance"):
                # FIXME - this probably only works for rpms
                release = instance.get('release', '')
                arch = instance.get('arch', '')
                act_dict['current_version'] = instance.get('version')
                if release:
                    act_dict['current_version'] += "-" + release
                if arch:
                    act_dict['current_version'] += "." + arch
                self.logger.debug("Adding package %s %s" %
                                  (name, act_dict['current_version']))
                return PackageEntry.entry_get_or_create(act_dict)
        else:
            self.logger.debug("Adding package %s %s" %
                              (name, act_dict['target_version']))

            # not implemented yet
            act_dict['verification_details'] = \
                entry.get('verification_details', '')
            return PackageEntry.entry_get_or_create(act_dict)

    def _import_Path(self, entry, state):
        name = entry.get('name')
        exists = entry.get('current_exists', default="true").lower() == "true"
        path_type = entry.get("type").lower()
        act_dict = dict(name=name, state=state, exists=exists,
                        path_type=path_type)

        target_dict = dict(
            owner=entry.get('owner', default="root"),
            group=entry.get('group', default="root"),
            mode=entry.get('mode', default=entry.get('perms',
                                                     default=""))
        )
        fperm, created = FilePerms.objects.get_or_create(**target_dict)
        act_dict['target_perms'] = fperm

        current_dict = dict(
            owner=entry.get('current_owner', default=""),
            group=entry.get('current_group', default=""),
            mode=entry.get('current_mode',
                default=entry.get('current_perms', default=""))
        )
        fperm, created = FilePerms.objects.get_or_create(**current_dict)
        act_dict['current_perms'] = fperm

        if path_type in ('symlink', 'hardlink'):
            act_dict['target_path'] = entry.get('to', default="")
            act_dict['current_path'] = entry.get('current_to', default="")
            self.logger.debug("Adding link %s" % name)
            return LinkEntry.entry_get_or_create(act_dict)
        elif path_type == 'device':
            # TODO devices
            self.logger.warn("device path types are not supported yet")
            return

        # TODO - vcs output
        act_dict['detail_type'] = PathEntry.DETAIL_UNUSED
        if path_type == 'directory' and entry.get('prune', 'false') == 'true':
            unpruned_elist = [e.get('path') for e in entry.findall('Prune')]
            if unpruned_elist:
                act_dict['detail_type'] = PathEntry.DETAIL_PRUNED
                act_dict['details'] = "\n".join(unpruned_elist)
        elif entry.get('sensitive', 'false').lower() == 'true':
            act_dict['detail_type'] = PathEntry.DETAIL_SENSITIVE
        else:
            cdata = None
            if entry.get('current_bfile', None):
                act_dict['detail_type'] = PathEntry.DETAIL_BINARY
                cdata = entry.get('current_bfile')
            elif entry.get('current_bdiff', None):
                act_dict['detail_type'] = PathEntry.DETAIL_DIFF
                cdata = b64decode(entry.get('current_bdiff'))
            elif entry.get('current_diff', None):
                act_dict['detail_type'] = PathEntry.DETAIL_DIFF
                cdata = entry.get('current_bdiff')
            if cdata:
                if len(cdata) > self.size_limit:
                    act_dict['detail_type'] = PathEntry.DETAIL_SIZE_LIMIT
                    act_dict['details'] = md5(cdata).hexdigest()
                else:
                    act_dict['details'] = cdata
        self.logger.debug("Adding path %s" % name)
        return PathEntry.entry_get_or_create(act_dict)
        # TODO - secontext
        # TODO - acls

    def _import_Service(self, entry, state):
        return self._import_default(entry, state,
                                    defaults=dict(status='',
                                                  current_status='',
                                                  target_status=''),
                                    mapping=dict(status='target_status'))

    def _import_SEBoolean(self, entry, state):
        return self._import_default(
            entry, state,
            xforms=dict(value=lambda v: v.lower() == "on"))

    def _import_SEFcontext(self, entry, state):
        return self._import_default(entry, state,
                                    defaults=dict(filetype='all'))

    def _import_SEInterface(self, entry, state):
        return self._import_default(entry, state)

    def _import_SEPort(self, entry, state):
        return self._import_default(entry, state)

    def _import_SENode(self, entry, state):
        return self._import_default(entry, state)

    def _import_SELogin(self, entry, state):
        return self._import_default(entry, state)

    def _import_SEUser(self, entry, state):
        return self._import_default(entry, state)

    def _import_SEPermissive(self, entry, state):
        return self._import_default(entry, state)

    def _import_SEModule(self, entry, state):
        return self._import_default(entry, state,
                                    defaults=dict(disabled='false'),
                                    boolean=['disabled', 'current_disabled'])

    def _import_POSIXUser(self, entry, state):
        defaults = dict(group=entry.get("name"),
                        gecos=entry.get("name"),
                        shell='/bin/bash',
                        uid=entry.get("current_uid"))
        if entry.get('name') == 'root':
            defaults['home'] = '/root'
        else:
            defaults['home'] = '/home/%s' % entry.get('name')

        # TODO: supplementary group membership
        return self._import_default(entry, state, defaults=defaults)

    def _import_POSIXGroup(self, entry, state):
        return self._import_default(
            entry, state,
            defaults=dict(gid=entry.get("current_gid")))

    def _import_unknown(self, entry, _):
        self.logger.error("Unknown type %s not handled by reporting yet" %
                          entry.tag)
        return None

    @transaction.atomic
    def _import_interaction(self, interaction):
        """Real import function"""
        hostname = interaction['hostname']
        stats = etree.fromstring(interaction['stats'])
        metadata = interaction['metadata']
        server = metadata['server']

        client = cache.get(hostname)
        if not client:
            client, created = Client.objects.get_or_create(name=hostname)
            if created:
                self.logger.debug("Client %s added to the db" % hostname)
            cache.set(hostname, client)

        timestamp = datetime(*strptime(stats.get('time'))[0:6])
        if len(Interaction.objects.filter(client=client,
                                          timestamp=timestamp)) > 0:
            self.logger.warn("Interaction for %s at %s already exists" %
                    (hostname, timestamp))
            return

        if 'profile' in metadata:
            profile, created = \
                Group.objects.get_or_create(name=metadata['profile'])
        else:
            profile = None
        inter = Interaction(client=client,
                             timestamp=timestamp,
                             state=stats.get('state', default="unknown"),
                             repo_rev_code=stats.get('revision',
                                                          default="unknown"),
                             good_count=stats.get('good', default="0"),
                             total_count=stats.get('total', default="0"),
                             server=server,
                             profile=profile)
        inter.save()
        self.logger.debug("Interaction for %s at %s with INSERTED in to db" %
                (client.id, timestamp))

        # FIXME - this should be more efficient
        for group_name in metadata['groups']:
            group = cache.get("GROUP_" + group_name)
            if not group:
                group, created = Group.objects.get_or_create(name=group_name)
                if created:
                    self.logger.debug("Added group %s" % group)
                cache.set("GROUP_" + group_name, group)

            inter.groups.add(group)
        for bundle_name in metadata.get('bundles', []):
            bundle = cache.get("BUNDLE_" + bundle_name)
            if not bundle:
                bundle, created = \
                    Bundle.objects.get_or_create(name=bundle_name)
                if created:
                    self.logger.debug("Added bundle %s" % bundle)
                cache.set("BUNDLE_" + bundle_name, bundle)
            inter.bundles.add(bundle)
        inter.save()

        counter_fields = {TYPE_BAD: 0,
                          TYPE_MODIFIED: 0,
                          TYPE_EXTRA: 0}
        pattern = [('Bad/*', TYPE_BAD),
                   ('Extra/*', TYPE_EXTRA),
                   ('Modified/*', TYPE_MODIFIED)]
        updates = dict([(etype, []) for etype in Interaction.entry_types])
        for (xpath, state) in pattern:
            for entry in stats.findall(xpath):
                counter_fields[state] = counter_fields[state] + 1

                # handle server failures differently
                failure = entry.get('failure', '')
                if failure:
                    act_dict = dict(name=entry.get("name"),
                                    entry_type=entry.tag,
                                    message=failure)
                    newact = FailureEntry.entry_get_or_create(act_dict)
                    updates['failures'].append(newact)
                    continue

                updatetype = entry.tag.lower() + "s"
                update = getattr(self, "_import_%s" % entry.tag,
                                 self._import_unknown)(entry, state)
                if update is not None:
                    updates[updatetype].append(update)

        inter.bad_count = counter_fields[TYPE_BAD]
        inter.modified_count = counter_fields[TYPE_MODIFIED]
        inter.extra_count = counter_fields[TYPE_EXTRA]
        inter.save()
        for entry_type in updates.keys():
            # batch this for sqlite
            i = 0
            while(i < len(updates[entry_type])):
                getattr(inter, entry_type).add(*updates[entry_type][i:i + 100])
                i += 100

        # performance metrics
        for times in stats.findall('OpStamps'):
            for metric, value in list(times.items()):
                Performance(interaction=inter,
                            metric=metric,
                            value=value).save()

    def import_interaction(self, interaction):
        """Import the data into the backend"""

        try:
            self._import_interaction(interaction)
        except:
            self.logger.error("Failed to import interaction: %s" %
                    traceback.format_exc().splitlines()[-1])
        finally:
            self.logger.debug("%s: Closing database connection" %
                              self.__class__.__name__)
            db.close_connection()


    def validate(self):
        """Validate backend storage.  Should be called once when loaded"""

        settings.read_config(repo=self.setup['repo'])

        # verify our database schema
        try:
            if self.setup['debug']:
                vrb = 2
            elif self.setup['verbose']:
                vrb = 1
            else:
                vrb = 0
            management.call_command("syncdb", verbosity=vrb, interactive=False)
            management.call_command("migrate", verbosity=vrb, interactive=False)
        except:
            self.logger.error("Failed to update database schema: %s" % \
                traceback.format_exc().splitlines()[-1])
            raise StorageError

    def GetExtra(self, client):
        """Fetch extra entries for a client"""
        try:
            c_inst = Client.objects.get(name=client)
            if not c_inst.current_interaction:
                # the rare case where a client has no interations
                return None
            return [(ent.entry_type, ent.name) for ent in
                    c_inst.current_interaction.extra()]
        except ObjectDoesNotExist:
            return []
        except MultipleObjectsReturned:
            self.logger.error("%s Inconsistency: Multiple entries for %s." %
                (self.__class__.__name__, client))
            return []

    def GetCurrentEntry(self, client, e_type, e_name):
        """"GetCurrentEntry: Used by PullSource"""
        try:
            c_inst = Client.objects.get(name=client)
        except ObjectDoesNotExist:
            self.logger.error("Unknown client: %s" % client)
            raise PluginExecutionError
        except MultipleObjectsReturned:
            self.logger.error("%s Inconsistency: Multiple entries for %s." %
                (self.__class__.__name__, client))
            raise PluginExecutionError
        try:
            cls = BaseEntry.entry_from_name(e_type + "Entry")
            result = cls.objects.filter(name=e_name, state=TYPE_BAD,
                interaction=c_inst.current_interaction)
        except ValueError:
            self.logger.error("Unhandled type %s" % e_type)
            raise PluginExecutionError
        if not result:
            raise PluginExecutionError
        entry = result[0]
        ret = []
        for p_entry in ('owner', 'group', 'mode'):
            this_entry = getattr(entry.current_perms, p_entry)
            if this_entry == '':
                ret.append(getattr(entry.target_perms, p_entry))
            else:
                ret.append(this_entry)
        if entry.entry_type == 'Path':
            if entry.is_sensitive():
                raise PluginExecutionError
            elif entry.detail_type == PathEntry.DETAIL_PRUNED:
                ret.append('\n'.join(entry.details))
            elif entry.is_binary():
                ret.append(b64decode(entry.details))
            elif entry.is_diff():
                ret.append('\n'.join(difflib.restore(\
                    entry.details.split('\n'), 1)))
            elif entry.is_too_large():
                # If len is zero the object was too large to store
                raise PluginExecutionError
            else:
                ret.append(None)
        return ret

########NEW FILE########
__FILENAME__ = bcfg2_tags
import sys
from copy import copy

from django import template
from django.conf import settings
from django.core.urlresolvers import resolve, reverse, \
                                     Resolver404, NoReverseMatch
from django.template.loader import get_template_from_string
from django.utils.encoding import smart_str
from django.utils.safestring import mark_safe
from datetime import datetime, timedelta
from Bcfg2.Reporting.utils import filter_list
from Bcfg2.Reporting.models import Group

register = template.Library()

__PAGE_NAV_LIMITS__ = (10, 25, 50, 100)


@register.inclusion_tag('widgets/page_bar.html', takes_context=True)
def page_navigator(context):
    """
    Creates paginated links.

    Expects the context to be a RequestContext and
    views.prepare_paginated_list() to have populated page information.
    """
    fragment = dict()
    try:
        path = context['request'].META['PATH_INFO']
        total_pages = int(context['total_pages'])
        records_per_page = int(context['records_per_page'])
    except KeyError:
        return fragment
    except ValueError:
        return fragment

    if total_pages < 2:
        return {}

    try:
        view, args, kwargs = resolve(path)
        current_page = int(kwargs.get('page_number', 1))
        fragment['current_page'] = current_page
        fragment['page_number'] = current_page
        fragment['total_pages'] = total_pages
        fragment['records_per_page'] = records_per_page
        if current_page > 1:
            kwargs['page_number'] = current_page - 1
            fragment['prev_page'] = reverse(view, args=args, kwargs=kwargs)
        if current_page < total_pages:
            kwargs['page_number'] = current_page + 1
            fragment['next_page'] = reverse(view, args=args, kwargs=kwargs)

        view_range = 5
        if total_pages > view_range:
            pager_start = current_page - 2
            pager_end = current_page + 2
            if pager_start < 1:
                pager_end += (1 - pager_start)
                pager_start = 1
            if pager_end > total_pages:
                pager_start -= (pager_end - total_pages)
                pager_end = total_pages
        else:
            pager_start = 1
            pager_end = total_pages

        if pager_start > 1:
            kwargs['page_number'] = 1
            fragment['first_page'] = reverse(view, args=args, kwargs=kwargs)
        if pager_end < total_pages:
            kwargs['page_number'] = total_pages
            fragment['last_page'] = reverse(view, args=args, kwargs=kwargs)

        pager = []
        for page in range(pager_start, int(pager_end) + 1):
            kwargs['page_number'] = page
            pager.append((page, reverse(view, args=args, kwargs=kwargs)))

        kwargs['page_number'] = 1
        page_limits = []
        for limit in __PAGE_NAV_LIMITS__:
            kwargs['page_limit'] = limit
            page_limits.append((limit,
                                reverse(view, args=args, kwargs=kwargs)))
        # resolver doesn't like this
        del kwargs['page_number']
        del kwargs['page_limit']
        page_limits.append(('all',
                           reverse(view, args=args, kwargs=kwargs) + "|all"))

        fragment['pager'] = pager
        fragment['page_limits'] = page_limits

    except Resolver404:
        path = "404"
    except NoReverseMatch:
        nr = sys.exc_info()[1]
        path = "NoReverseMatch: %s" % nr
    except ValueError:
        path = "ValueError"
    #FIXME - Handle these

    fragment['path'] = path
    return fragment


@register.inclusion_tag('widgets/filter_bar.html', takes_context=True)
def filter_navigator(context):
    try:
        path = context['request'].META['PATH_INFO']
        view, args, kwargs = resolve(path)

        # Strip any page limits and numbers
        if 'page_number' in kwargs:
            del kwargs['page_number']
        if 'page_limit' in kwargs:
            del kwargs['page_limit']

        # get a query string
        qs = context['request'].GET.urlencode()
        if qs:
            qs = '?' + qs

        filters = []
        for filter in filter_list:
            if filter == 'group':
                continue
            if filter in kwargs:
                myargs = kwargs.copy()
                del myargs[filter]
                filters.append((filter,
                                reverse(view, args=args, kwargs=myargs) + qs))
        filters.sort(key=lambda x: x[0])

        myargs = kwargs.copy()
        selected = True
        if 'group' in myargs:
            del myargs['group']
            selected = False
        groups = [('---',
                   reverse(view, args=args, kwargs=myargs) + qs,
                   selected)]
        for group in Group.objects.values('name'):
            myargs['group'] = group['name']
            groups.append((group['name'],
                           reverse(view, args=args, kwargs=myargs) + qs,
                           group['name'] == kwargs.get('group', '')))

        return {'filters': filters, 'groups': groups}
    except (Resolver404, NoReverseMatch, ValueError, KeyError):
        pass
    return dict()


def _subtract_or_na(mdict, x, y):
    """
    Shortcut for build_metric_list
    """
    try:
        return round(mdict[x] - mdict[y], 4)
    except:
        return "n/a"


@register.filter
def build_metric_list(mdict):
    """
    Create a list of metric table entries

    Moving this here to simplify the view.
    Should really handle the case where these are missing...
    """
    td_list = []
    # parse
    td_list.append(_subtract_or_na(mdict, 'config_parse', 'config_download'))
    #probe
    td_list.append(_subtract_or_na(mdict, 'probe_upload', 'start'))
    #inventory
    td_list.append(_subtract_or_na(mdict, 'inventory', 'initialization'))
    #install
    td_list.append(_subtract_or_na(mdict, 'install', 'inventory'))
    #cfg download & parse
    td_list.append(_subtract_or_na(mdict, 'config_parse', 'probe_upload'))
    #total
    td_list.append(_subtract_or_na(mdict, 'finished', 'start'))
    return td_list


@register.filter
def isstale(timestamp, entry_max=None):
    """
    Check for a stale timestamp

    Compares two timestamps and returns True if the
    difference is greater then 24 hours.
    """
    if not entry_max:
        entry_max = datetime.now()
    return entry_max - timestamp > timedelta(hours=24)


@register.filter
def sort_interactions_by_name(value):
    """
    Sort an interaction list by client name
    """
    inters = list(value)
    inters.sort(key=lambda a: a.client.name)
    return inters


class AddUrlFilter(template.Node):
    def __init__(self, filter_name, filter_value):
        self.filter_name = filter_name
        self.filter_value = filter_value
        self.fallback_view = 'Bcfg2.Reporting.views.render_history_view'

    def render(self, context):
        link = '#'
        try:
            path = context['request'].META['PATH_INFO']
            view, args, kwargs = resolve(path)
            filter_value = self.filter_value.resolve(context, True)
            if filter_value:
                filter_name = smart_str(self.filter_name)
                filter_value = smart_str(filter_value)
                kwargs[filter_name] = filter_value
                # These two don't make sense
                if filter_name == 'server' and 'hostname' in kwargs:
                    del kwargs['hostname']
                elif filter_name == 'hostname' and 'server' in kwargs:
                    del kwargs['server']
                try:
                    link = reverse(view, args=args, kwargs=kwargs)
                except NoReverseMatch:
                    link = reverse(self.fallback_view, args=None,
                        kwargs={filter_name: filter_value})
                qs = context['request'].GET.urlencode()
                if qs:
                    link += "?" + qs
        except NoReverseMatch:
            rm = sys.exc_info()[1]
            raise rm
        except (Resolver404, ValueError):
            pass
        return link


@register.tag
def add_url_filter(parser, token):
    """
    Return a url with the filter added to the current view.

    Takes a new filter and resolves the current view with the new filter
    applied.  Resolves to Bcfg2.Reporting.views.client_history
    by default.

    {% add_url_filter server=interaction.server %}
    """
    try:
        tag_name, filter_pair = token.split_contents()
        filter_name, filter_value = filter_pair.split('=', 1)
        filter_name = filter_name.strip()
        filter_value = parser.compile_filter(filter_value)
    except ValueError:
        raise template.TemplateSyntaxError("%r tag requires exactly one argument" % token.contents.split()[0])
    if not filter_name or not filter_value:
        raise template.TemplateSyntaxError("argument should be a filter=value pair")

    return AddUrlFilter(filter_name, filter_value)


class MediaTag(template.Node):
    def __init__(self, filter_value):
        self.filter_value = filter_value

    def render(self, context):
        base = context['MEDIA_URL']
        try:
            request = context['request']
            try:
                base = request.environ['bcfg2.media_url']
            except:
                if request.path != request.META['PATH_INFO']:
                    offset = request.path.find(request.META['PATH_INFO'])
                    if offset > 0:
                        base = "%s/%s" % (request.path[:offset], \
                                context['MEDIA_URL'].strip('/'))
        except:
            pass
        return "%s/%s" % (base, self.filter_value)


@register.tag
def to_media_url(parser, token):
    """
    Return a url relative to the media_url.

    {% to_media_url /bcfg2.css %}
    """
    try:
        filter_value = token.split_contents()[1]
        filter_value = parser.compile_filter(filter_value)
    except ValueError:
        raise template.TemplateSyntaxError("%r tag requires exactly one argument" % token.contents.split()[0])

    return MediaTag(filter_value)


@register.filter
def determine_client_state(entry):
    """
    Determine client state.

    This is used to determine whether a client is reporting clean or
    dirty. If the client is reporting dirty, this will figure out just
    _how_ dirty and adjust the color accordingly.
    """
    if entry.state == 'clean':
        return "clean-lineitem"

    bad_percentage = 100 * (float(entry.bad_count) / entry.total_count)
    if bad_percentage < 33:
        thisdirty = "slightly-dirty-lineitem"
    elif bad_percentage < 66:
        thisdirty = "dirty-lineitem"
    else:
        thisdirty = "very-dirty-lineitem"
    return thisdirty


@register.tag(name='qs')
def do_qs(parser, token):
    """
    qs tag

    accepts a name value pair and inserts or replaces it in the query string
    """
    try:
        tag, name, value = token.split_contents()
    except ValueError:
        raise template.TemplateSyntaxError("%r tag requires exactly two arguments"
                                           % token.contents.split()[0])
    return QsNode(name, value)


class QsNode(template.Node):
    def __init__(self, name, value):
        self.name = template.Variable(name)
        self.value = template.Variable(value)

    def render(self, context):
        try:
            name = self.name.resolve(context)
            value = self.value.resolve(context)
            request = context['request']
            qs = copy(request.GET)
            qs[name] = value
            return "?%s" % qs.urlencode()
        except template.VariableDoesNotExist:
            return ''
        except KeyError:
            if settings.TEMPLATE_DEBUG:
                raise Exception("'qs' tag requires context['request']")
            return ''
        except:
            return ''


@register.tag
def sort_link(parser, token):
    '''
    Create a sort anchor tag.  Reverse it if active.

    {% sort_link sort_key text %}
    '''
    try:
        tag, sort_key, text = token.split_contents()
    except ValueError:
        raise template.TemplateSyntaxError("%r tag requires at least four arguments" \
            % token.split_contents()[0])

    return SortLinkNode(sort_key, text)


class SortLinkNode(template.Node):
    __TMPL__ = "{% load bcfg2_tags %}<a href='{% qs 'sort' key %}'>{{ text }}</a>"

    def __init__(self, sort_key, text):
        self.sort_key = template.Variable(sort_key)
        self.text = template.Variable(text)

    def render(self, context):
        try:
            try:
                sort = context['request'].GET['sort']
            except KeyError:
                #fall back on this
                sort = context.get('sort', '')
            sort_key = self.sort_key.resolve(context)
            text = self.text.resolve(context)

            # add arrows
            try:
                sort_base = sort_key.lstrip('-')
                if sort[0] == '-' and sort[1:] == sort_base:
                    text = text + '&#x25BC;'
                    sort_key = sort_base
                elif sort_base == sort:
                    text = text + '&#x25B2;'
                    sort_key = '-' + sort_base
            except IndexError:
                pass

            context.push()
            context['key'] = sort_key
            context['text'] = mark_safe(text)
            output = get_template_from_string(self.__TMPL__).render(context)
            context.pop()
            return output
        except:
            if settings.DEBUG:
                raise
            raise
            return ''

########NEW FILE########
__FILENAME__ = split
from django import template
register = template.Library()


@register.filter
def split(s):
    """split by newlines"""
    return s.split('\n')

########NEW FILE########
__FILENAME__ = syntax_coloring
from django import template
from django.utils.encoding import smart_str
from django.utils.html import conditional_escape
from django.utils.safestring import mark_safe

register = template.Library()

# pylint: disable=E0611
try:
    from pygments import highlight
    from pygments.lexers import get_lexer_by_name
    from pygments.formatters import HtmlFormatter
    colorize = True
except:
    colorize = False
# pylint: enable=E0611


@register.filter
def syntaxhilight(value, arg="diff", autoescape=None):
    """
    Returns a syntax-hilighted version of Code;
    requires code/language arguments
    """

    if autoescape:
        # Seems to cause a double escape
        #value = conditional_escape(value)
        arg = conditional_escape(arg)

    if colorize:
        try:
            output = smart_str('<style  type="text/css">') \
                + smart_str(HtmlFormatter().get_style_defs('.highlight')) \
                + smart_str('</style>')

            lexer = get_lexer_by_name(arg)
            output += highlight(value, lexer, HtmlFormatter())
            return mark_safe(output)
        except:
            return value
    else:
        return mark_safe(smart_str(
                            '<div class="note-box">Tip: Install pygments '
                            'for highlighting</div><pre>%s</pre>') % value)
syntaxhilight.needs_autoescape = True

########NEW FILE########
__FILENAME__ = base
"""
The base for all server -> collector Transports
"""

import os
import sys
from Bcfg2.Server.Plugin import Debuggable


class TransportError(Exception):
    """Generic TransportError"""
    pass


class TransportImportError(TransportError):
    """Raised when a transport fails to import"""
    pass


class TransportBase(Debuggable):
    """The base for all transports"""

    def __init__(self, setup):
        """Do something here"""
        clsname = self.__class__.__name__
        Debuggable.__init__(self, name=clsname)
        self.debug_log("Loading %s transport" % clsname)
        self.data = os.path.join(setup['repo'], 'Reporting', clsname)
        if not os.path.exists(self.data):
            self.logger.info("%s does not exist, creating" % self.data)
            try:
                os.makedirs(self.data)
            except OSError:
                self.logger.warning("Could not create %s: %s" %
                                    (self.data, sys.exc_info()[1]))
                self.logger.warning("The transport may not function properly")
        self.setup = setup
        self.timeout = 2

    def start_monitor(self, collector):
        """Called to start monitoring"""
        raise NotImplementedError

    def store(self, hostname, metadata, stats):
        raise NotImplementedError

    def fetch(self):
        raise NotImplementedError

    def shutdown(self):
        """Called at program exit"""
        pass

    def rpc(self, method, *args, **kwargs):
        """Send a request for data to the collector"""
        raise NotImplementedError

########NEW FILE########
__FILENAME__ = DirectStore
""" Reporting Transport that stores statistics data directly in the
storage backend """

import os
import sys
import time
import threading
from Bcfg2.Reporting.Transport.base import TransportBase, TransportError
from Bcfg2.Reporting.Storage import load_storage_from_config
from Bcfg2.Compat import Queue, Full, Empty, cPickle


class DirectStore(TransportBase, threading.Thread):
    def __init__(self, setup):
        TransportBase.__init__(self, setup)
        threading.Thread.__init__(self)
        self.save_file = os.path.join(self.data, ".saved")

        self.storage = load_storage_from_config(setup)
        self.storage.validate()

        self.queue = Queue(100000)
        self.terminate = threading.Event()
        self.debug_log("Reporting: Starting %s thread" %
                       self.__class__.__name__)
        self.start()

    def shutdown(self):
        self.terminate.set()

    def store(self, hostname, metadata, stats):
        try:
            self.queue.put_nowait(dict(
                    hostname=hostname,
                    metadata=metadata,
                    stats=stats))
        except Full:
            self.logger.warning("Reporting: Queue is full, "
                                "dropping statistics")

    def run(self):
        if not self._load():
            self.logger.warning("Reporting: Failed to load saved data, "
                                "DirectStore thread exiting")
            return
        while not self.terminate.isSet() and self.queue is not None:
            try:
                interaction = self.queue.get(block=True,
                                             timeout=self.timeout)
                start = time.time()
                self.storage.import_interaction(interaction)
                self.logger.info("Imported data for %s in %s seconds" %
                                 (interaction.get('hostname', '<unknown>'),
                                  time.time() - start))
            except Empty:
                self.debug_log("Reporting: Queue is empty")
                continue
            except:
                err = sys.exc_info()[1]
                self.logger.error("Reporting: Could not import interaction: %s"
                                  % err)
                continue
        self.debug_log("Reporting: Stopping %s thread" %
                       self.__class__.__name__)
        if self.queue is not None and not self.queue.empty():
            self._save()

    def fetch(self):
        """ no collector is necessary with this backend """
        pass

    def start_monitor(self, collector):
        """ no collector is necessary with this backend """
        pass

    def rpc(self, method, *args, **kwargs):
        try:
            return getattr(self.storage, method)(*args, **kwargs)
        except:  # pylint: disable=W0702
            msg = "Reporting: RPC method %s failed: %s" % (method,
                                                           sys.exc_info()[1])
            self.logger.error(msg)
            raise TransportError(msg)

    def _save(self):
        """ Save any saved data to a file """
        self.debug_log("Reporting: Saving pending data to %s" %
                       self.save_file)
        saved_data = []
        try:
            while not self.queue.empty():
                saved_data.append(self.queue.get_nowait())
        except Empty:
            pass

        try:
            savefile = open(self.save_file, 'w')
            cPickle.dump(saved_data, savefile)
            savefile.close()
            self.logger.info("Saved pending Reporting data")
        except (IOError, TypeError):
            err = sys.exc_info()[1]
            self.logger.warning("Failed to save pending data: %s" % err)

    def _load(self):
        """ Load any saved data from a file """
        if not os.path.exists(self.save_file):
            self.debug_log("Reporting: No saved data to load")
            return True
        saved_data = []
        try:
            savefile = open(self.save_file, 'r')
            saved_data = cPickle.load(savefile)
            savefile.close()
        except (IOError, cPickle.UnpicklingError):
            err = sys.exc_info()[1]
            self.logger.warning("Failed to load saved data: %s" % err)
            return False
        for interaction in saved_data:
            # check that shutdown wasnt called early
            if self.terminate.isSet():
                self.logger.warning("Reporting: Shutdown called while loading "
                                    " saved data")
                return False

            try:
                self.queue.put_nowait(interaction)
            except Full:
                self.logger.warning("Reporting: Queue is full, failed to "
                                    "load saved interaction data")
                break
        try:
            os.unlink(self.save_file)
        except OSError:
            self.logger.error("Reporting: Failed to unlink save file: %s" %
                              self.save_file)
        self.logger.info("Reporting: Loaded saved interaction data")
        return True

########NEW FILE########
__FILENAME__ = LocalFilesystem
"""
The local transport.  Stats are pickled and written to
<repo>/store/<hostname>-timestamp

Leans on FileMonitor to detect changes
"""

import os
import select
import time
import traceback
import Bcfg2.Server.FileMonitor
from Bcfg2.Reporting.Collector import ReportingCollector, ReportingError
from Bcfg2.Reporting.Transport.base import TransportBase, TransportError
from Bcfg2.Compat import cPickle


class LocalFilesystem(TransportBase):
    def __init__(self, setup):
        super(LocalFilesystem, self).__init__(setup)

        self.work_path = "%s/work" % self.data
        self.debug_log("LocalFilesystem: work path %s" % self.work_path)
        self.fmon = None
        self._phony_collector = None

        #setup our local paths or die
        if not os.path.exists(self.work_path):
            try:
                os.makedirs(self.work_path)
            except:
                self.logger.error("%s: Unable to create storage: %s" %
                    (self.__class__.__name__,
                        traceback.format_exc().splitlines()[-1]))
                raise TransportError

    def set_debug(self, debug):
        rv = TransportBase.set_debug(self, debug)
        if self.fmon is not None:
            self.fmon.set_debug(debug)
        return rv

    def start_monitor(self, collector):
        """Start the file monitor.  Most of this comes from BaseCore"""
        setup = self.setup
        try:
            fmon = Bcfg2.Server.FileMonitor.available[setup['filemonitor']]
        except KeyError:
            self.logger.error("File monitor driver %s not available; "
                              "forcing to default" % setup['filemonitor'])
            fmon = Bcfg2.Server.FileMonitor.available['default']
        if self.debug_flag:
            self.fmon.set_debug(self.debug_flag)
        try:
            self.fmon = fmon(debug=self.debug_flag)
            self.logger.info("Using the %s file monitor" %
                             self.fmon.__class__.__name__)
        except IOError:
            msg = "Failed to instantiate file monitor %s" % \
                setup['filemonitor']
            self.logger.error(msg, exc_info=1)
            raise TransportError(msg)
        self.fmon.start()
        self.fmon.AddMonitor(self.work_path, self)

    def store(self, hostname, metadata, stats):
        """Store the file to disk"""

        try:
            payload = cPickle.dumps(dict(hostname=hostname,
                                         metadata=metadata,
                                         stats=stats))
        except:  # pylint: disable=W0702
            msg = "%s: Failed to build interaction object: %s" % \
                (self.__class__.__name__,
                 traceback.format_exc().splitlines()[-1])
            self.logger.error(msg)
            raise TransportError(msg)

        fname = "%s-%s" % (hostname, time.time())
        save_file = os.path.join(self.work_path, fname)
        tmp_file = os.path.join(self.work_path, "." + fname)
        if os.path.exists(save_file):
            self.logger.error("%s: Oops.. duplicate statistic in directory." %
                self.__class__.__name__)
            raise TransportError

        # using a tmpfile to hopefully avoid the file monitor from grabbing too
        # soon
        saved = open(tmp_file, 'wb')
        try:
            saved.write(payload)
        except IOError:
            self.logger.error("Failed to store interaction for %s: %s" %
                (hostname, traceback.format_exc().splitlines()[-1]))
            os.unlink(tmp_file)
        saved.close()
        os.rename(tmp_file, save_file)

    def fetch(self):
        """Fetch the next object"""
        event = None
        fmonfd = self.fmon.fileno()
        if self.fmon.pending():
            event = self.fmon.get_event()
        elif fmonfd:
            select.select([fmonfd], [], [], self.timeout)
            if self.fmon.pending():
                event = self.fmon.get_event()
        else:
            # pseudo.. if nothings pending sleep and loop
            time.sleep(self.timeout)

        if not event or event.filename == self.work_path:
            return None

        #deviate from the normal routines here we only want one event
        etype = event.code2str()
        self.debug_log("Recieved event %s for %s" % (etype, event.filename))
        if os.path.basename(event.filename)[0] == '.':
            return None
        if etype in ('created', 'exists'):
            self.debug_log("Handling event %s" % event.filename)
            payload = os.path.join(self.work_path, event.filename)
            try:
                payloadfd = open(payload, "rb")
                interaction = cPickle.load(payloadfd)
                payloadfd.close()
                os.unlink(payload)
                return interaction
            except IOError:
                self.logger.error("Failed to read payload: %s" %
                    traceback.format_exc().splitlines()[-1])
            except cPickle.UnpicklingError:
                self.logger.error("Failed to unpickle payload: %s" %
                    traceback.format_exc().splitlines()[-1])
                payloadfd.close()
                raise TransportError
        return None

    def shutdown(self):
        """Called at program exit"""
        if self.fmon:
            self.fmon.shutdown()
        if self._phony_collector:
            self._phony_collector.shutdown()

    def rpc(self, method, *args, **kwargs):
        """
        Here this is more of a dummy.  Rather then start a layer
        which doesn't exist or muck with files, start the collector

        This will all change when other layers are added
        """
        try:
            if not self._phony_collector:
                self._phony_collector = ReportingCollector(self.setup)
        except ReportingError:
            raise TransportError
        except:
            self.logger.error("Failed to load collector: %s" %
                traceback.format_exc().splitlines()[-1])
            raise TransportError

        if not method in self._phony_collector.storage.__class__.__rmi__ or \
                not hasattr(self._phony_collector.storage, method):
            self.logger.error("Unknown method %s called on storage engine %s" %
                (method, self._phony_collector.storage.__class__.__name__))
            raise TransportError


        try:
            cls_method = getattr(self._phony_collector.storage, method)
            return cls_method(*args, **kwargs)
        except:
            self.logger.error("RPC method %s failed: %s" %
                (method, traceback.format_exc().splitlines()[-1]))
            raise TransportError


########NEW FILE########
__FILENAME__ = RedisTransport
"""
The Redis transport.  Stats are pickled and written to
a redis queue

"""

import time
import signal
import platform
import traceback
import threading
from Bcfg2.Reporting.Transport.base import TransportBase, TransportError
from Bcfg2.Compat import cPickle
from Bcfg2.Options import Option

try:
    import redis
    HAS_REDIS = True
except ImportError:
    HAS_REDIS = False


class RedisMessage(object):
    """An rpc message"""
    def __init__(self, channel, method, args=[], kwargs=dict()):
        self.channel = channel
        self.method = method
        self.args = args
        self.kwargs = kwargs


class RedisTransport(TransportBase):
    """ Redis Transport Class """
    STATS_KEY = 'bcfg2_statistics'
    COMMAND_KEY = 'bcfg2_command'

    def __init__(self, setup):
        super(RedisTransport, self).__init__(setup)
        self._redis = None
        self._commands = None

        self.logger.error("Warning: RedisTransport is experimental")

        if not HAS_REDIS:
            self.logger.error("redis python module is not available")
            raise TransportError

        setup.update(dict(
            reporting_redis_host=Option(
                'Redis Host',
                default='127.0.0.1',
                cf=('reporting', 'redis_host')),
            reporting_redis_port=Option(
                'Redis Port',
                default=6379,
                cf=('reporting', 'redis_port')),
            reporting_redis_db=Option(
                'Redis DB',
                default=0,
                cf=('reporting', 'redis_db')),
        ))
        setup.reparse()

        self._redis_host = setup.get('reporting_redis_host', '127.0.0.1')
        try:
            self._redis_port = int(setup.get('reporting_redis_port', 6379))
        except ValueError:
            self.logger.error("Redis port must be an integer")
            raise TransportError
        self._redis_db = setup.get('reporting_redis_db', 0)
        self._redis = redis.Redis(host=self._redis_host,
            port=self._redis_port, db=self._redis_db)


    def start_monitor(self, collector):
        """Start the monitor. Eventaully start the command thread"""
        self._commands = threading.Thread(target=self.monitor_thread, 
            args=(self._redis, collector))
        self._commands.start()


    def store(self, hostname, metadata, stats):
        """Store the file to disk"""

        try:
            payload = cPickle.dumps(dict(hostname=hostname,
                                         metadata=metadata,
                                         stats=stats))
        except:  # pylint: disable=W0702
            msg = "%s: Failed to build interaction object: %s" % \
                (self.__class__.__name__,
                 traceback.format_exc().splitlines()[-1])
            self.logger.error(msg)
            raise TransportError(msg)

        try:
            self._redis.rpush(RedisTransport.STATS_KEY, payload)
        except redis.RedisError:
            self.logger.error("Failed to store interaction for %s: %s" %
                (hostname, traceback.format_exc().splitlines()[-1]))


    def fetch(self):
        """Fetch the next object"""
        try:
            payload = self._redis.blpop(RedisTransport.STATS_KEY, timeout=5)
            if payload:
                return cPickle.loads(payload[1])
        except redis.RedisError:
            self.logger.error("Failed to fetch an interaction: %s" %
                (traceback.format_exc().splitlines()[-1]))
        except cPickle.UnpicklingError:
            self.logger.error("Failed to unpickle payload: %s" %
                    traceback.format_exc().splitlines()[-1])
            raise TransportError

        return None

    def shutdown(self):
        """Called at program exit"""
        self._redis = None


    def rpc(self, method, *args, **kwargs):
        """
        Send a command to the queue.  Timeout after 10 seconds
        """
        pubsub = self._redis.pubsub()

        channel = "%s%s" % (platform.node(), int(time.time()))
        pubsub.subscribe(channel)
        self._redis.rpush(RedisTransport.COMMAND_KEY, 
            cPickle.dumps(RedisMessage(channel, method, args, kwargs)))

        resp = pubsub.listen()
        signal.signal(signal.SIGALRM, self.shutdown)
        signal.alarm(10)
        resp.next() # clear subscribe message
        response = resp.next()
        pubsub.unsubscribe()

        try:
            return cPickle.loads(response['data'])
        except: # pylint: disable=W0702
            msg = "%s: Failed to receive response: %s" % \
                (self.__class__.__name__,
                 traceback.format_exc().splitlines()[-1])
            self.logger.error(msg)
        return None


    def monitor_thread(self, rclient, collector):
        """Watch the COMMAND_KEY queue for rpc commands"""

        self.logger.info("Command thread started")
        while not collector.terminate.isSet():
            try:
                payload = rclient.blpop(RedisTransport.COMMAND_KEY, timeout=5)
                if not payload:
                    continue
                message = cPickle.loads(payload[1])
                if not isinstance(message, RedisMessage):
                    self.logger.error("Message \"%s\" is not a RedisMessage" % 
                        message)

                if not message.method in collector.storage.__class__.__rmi__ or\
                    not hasattr(collector.storage, message.method):
                    self.logger.error(
                        "Unknown method %s called on storage engine %s" %
                        (message.method, collector.storage.__class__.__name__))
                    raise TransportError

                try:
                    cls_method = getattr(collector.storage, message.method)
                    response = cls_method(*message.args, **message.kwargs)
                    response = cPickle.dumps(response)
                except:
                    self.logger.error("RPC method %s failed: %s" %
                        (message.method, traceback.format_exc().splitlines()[-1]))
                    raise TransportError
                rclient.publish(message.channel, response)

            except redis.RedisError:
                self.logger.error("Failed to fetch an interaction: %s" %
                    (traceback.format_exc().splitlines()[-1]))
            except cPickle.UnpicklingError:
                self.logger.error("Failed to unpickle payload: %s" %
                    traceback.format_exc().splitlines()[-1])
            except TransportError:
                pass
            except: # pylint: disable=W0702
                self.logger.error("Unhandled exception in command thread: %s" %
                    traceback.format_exc().splitlines()[-1])
        self.logger.info("Command thread shutdown")



########NEW FILE########
__FILENAME__ = urls
from Bcfg2.Reporting.Compat import url, patterns  # django compat imports
from django.core.urlresolvers import reverse, NoReverseMatch
from django.http import HttpResponsePermanentRedirect
from Bcfg2.Reporting.utils import filteredUrls, paginatedUrls, timeviewUrls

handler500 = 'Bcfg2.Reporting.views.server_error'

def newRoot(request):
    try:
        grid_view = reverse('reports_grid_view')
    except NoReverseMatch:
        grid_view = '/grid'
    return HttpResponsePermanentRedirect(grid_view)

urlpatterns = patterns('Bcfg2.Reporting',
    (r'^$', newRoot),

    url(r'^manage/?$', 'views.client_manage', name='reports_client_manage'),
    url(r'^client/(?P<hostname>[^/]+)/(?P<pk>\d+)/?$', 'views.client_detail', name='reports_client_detail_pk'),
    url(r'^client/(?P<hostname>[^/]+)/?$', 'views.client_detail', name='reports_client_detail'),
    url(r'^element/(?P<entry_type>\w+)/(?P<pk>\d+)/(?P<interaction>\d+)?/?$', 'views.config_item', name='reports_item'),
    url(r'^element/(?P<entry_type>\w+)/(?P<pk>\d+)/?$', 'views.config_item', name='reports_item'),
    url(r'^entry/(?P<entry_type>\w+)/(?P<pk>\w+)/?$', 'views.entry_status', name='reports_entry'),
)

urlpatterns += patterns('Bcfg2.Reporting',
    *timeviewUrls(
        (r'^summary/?$', 'views.display_summary', None, 'reports_summary'),
        (r'^timing/?$', 'views.display_timing', None, 'reports_timing'),
        (r'^common/group/(?P<group>[^/]+)/(?P<threshold>\d+)/?$', 'views.common_problems', None, 'reports_common_problems'),
        (r'^common/group/(?P<group>[^/]+)+/?$', 'views.common_problems', None, 'reports_common_problems'),
        (r'^common/(?P<threshold>\d+)/?$', 'views.common_problems', None, 'reports_common_problems'),
        (r'^common/?$', 'views.common_problems', None, 'reports_common_problems'),
))

urlpatterns += patterns('Bcfg2.Reporting',
    *filteredUrls(*timeviewUrls(
        (r'^grid/?$', 'views.client_index', None, 'reports_grid_view'),
        (r'^detailed/?$',
            'views.client_detailed_list', None, 'reports_detailed_list'),
        (r'^elements/(?P<item_state>\w+)/?$', 'views.config_item_list', None, 'reports_item_list'),
)))

urlpatterns += patterns('Bcfg2.Reporting',
    *paginatedUrls( *filteredUrls(
        (r'^history/?$',
            'views.render_history_view', None, 'reports_history'),
        (r'^history/(?P<hostname>[^/|]+)/?$',
            'views.render_history_view', None, 'reports_client_history'),
)))

    # Uncomment this for admin:
    #(r'^admin/', include('django.contrib.admin.urls')),


## Uncomment this section if using authentication
#urlpatterns += patterns('',
#                        (r'^login/$', 'django.contrib.auth.views.login',
#                         {'template_name': 'auth/login.html'}),
#                        (r'^logout/$', 'django.contrib.auth.views.logout',
#                         {'template_name': 'auth/logout.html'})
#                        )


########NEW FILE########
__FILENAME__ = utils
"""Helper functions for reports"""
import re

"""List of filters provided by filteredUrls"""
filter_list = ('server', 'state', 'group')


class BatchFetch(object):
    """Fetch Django objects in smaller batches to save memory"""

    def __init__(self, obj, step=10000):
        self.count = 0
        self.block_count = 0
        self.obj = obj
        self.data = None
        self.step = step
        self.max = obj.count()

    def __iter__(self):
        return self

    def next(self):
        """Provide compatibility with python < 3.0"""
        return self.__next__()

    def __next__(self):
        """Return the next object from our array and fetch from the
           database when needed"""
        if self.block_count + self.count - self.step == self.max:
            raise StopIteration
        if self.block_count == 0 or self.count == self.step:
            # Without list() this turns into LIMIT 1 OFFSET x queries
            self.data = list(self.obj.all()[self.block_count: \
                                   (self.block_count + self.step)])
            self.block_count += self.step
            self.count = 0
        self.count += 1
        return self.data[self.count - 1]


def generateUrls(fn):
    """
    Parse url tuples and send to functions.

    Decorator for url generators.  Handles url tuple parsing
    before the actual function is called.
    """
    def url_gen(*urls):
        results = []
        for url_tuple in urls:
            if isinstance(url_tuple, (list, tuple)):
                results += fn(*url_tuple)
            else:
                raise ValueError("Unable to handle compiled urls")
        return results
    return url_gen


@generateUrls
def paginatedUrls(pattern, view, kwargs=None, name=None):
    """
    Takes a group of url tuples and adds paginated urls.

    Extends a url tuple to include paginated urls.
    Currently doesn't handle url() compiled patterns.

    """
    results = [(pattern, view, kwargs, name)]
    tail = ''
    mtail = re.search('(/+\+?\\*?\??\$?)$', pattern)
    if mtail:
        tail = mtail.group(1)
    pattern = pattern[:len(pattern) - len(tail)]
    results += [(pattern + "/(?P<page_number>\d+)" + tail, view, kwargs)]
    results += [(pattern + "/(?P<page_number>\d+)\|(?P<page_limit>\d+)" +
                 tail, view, kwargs)]
    if not kwargs:
        kwargs = dict()
    kwargs['page_limit'] = 0
    results += [(pattern + "/?\|(?P<page_limit>all)" + tail, view, kwargs)]
    return results


@generateUrls
def filteredUrls(pattern, view, kwargs=None, name=None):
    """
    Takes a url and adds filtered urls.

    Extends a url tuple to include filtered view urls.  Currently doesn't
    handle url() compiled patterns.
    """
    results = [(pattern, view, kwargs, name)]
    tail = ''
    mtail = re.search('(/+\+?\\*?\??\$?)$', pattern)
    if mtail:
        tail = mtail.group(1)
    pattern = pattern[:len(pattern) - len(tail)]
    for filter in ('/state/(?P<state>\w+)',
                   '/group/(?P<group>[\w\-\.]+)',
                   '/group/(?P<group>[\w\-\.]+)/(?P<state>[A-Za-z]+)',
                   '/server/(?P<server>[\w\-\.]+)',
                   '/server/(?P<server>[\w\-\.]+)/(?P<state>[A-Za-z]+)',
                   '/server/(?P<server>[\w\-\.]+)/group/(?P<group>[\w\-\.]+)',
                   '/server/(?P<server>[\w\-\.]+)/group/(?P<group>[\w\-\.]+)/(?P<state>[A-Za-z]+)'):
        results += [(pattern + filter + tail, view, kwargs)]
    return results


@generateUrls
def timeviewUrls(pattern, view, kwargs=None, name=None):
    """
    Takes a url and adds timeview urls

    Extends a url tuple to include filtered view urls.  Currently doesn't
    handle url() compiled patterns.
    """
    results = [(pattern, view, kwargs, name)]
    tail = ''
    mtail = re.search('(/+\+?\\*?\??\$?)$', pattern)
    if mtail:
        tail = mtail.group(1)
    pattern = pattern[:len(pattern) - len(tail)]
    for filter in ('/(?P<year>\d{4})-(?P<month>\d{2})-(?P<day>\d{2})/' + \
                       '(?P<hour>\d\d)-(?P<minute>\d\d)',
                   '/(?P<year>\d{4})-(?P<month>\d{2})-(?P<day>\d{2})'):
        results += [(pattern + filter + tail, view, kwargs)]
    return results

########NEW FILE########
__FILENAME__ = views
"""
Report views

Functions to handle all of the reporting views.
"""
from datetime import datetime, timedelta
import sys
from time import strptime

from django.template import Context, RequestContext
from django.http import \
        HttpResponse, HttpResponseRedirect, HttpResponseServerError, Http404
from django.shortcuts import render_to_response, get_object_or_404
from django.core.urlresolvers import \
        resolve, reverse, Resolver404, NoReverseMatch
from django.db import connection, DatabaseError
from django.db.models import Q, Count

from Bcfg2.Reporting.models import *


__SORT_FIELDS__ = ( 'client', 'state', 'good', 'bad', 'modified', 'extra', \
            'timestamp', 'server' )

class PaginationError(Exception):
    """This error is raised when pagination cannot be completed."""
    pass


def _in_bulk(model, ids):
    """
    Short cut to fetch in bulk and trap database errors.  sqlite will raise
    a "too many SQL variables" exception if this list is too long.  Try using
    django and fetch manually if an error occurs

    returns a dict of this form { id: <model instance> }
    """

    try:
        return model.objects.in_bulk(ids)
    except DatabaseError:
        pass

    # if objects.in_bulk fails so will obejcts.filter(pk__in=ids)
    bulk_dict = {}
    [bulk_dict.__setitem__(i.id, i) \
        for i in model.objects.all() if i.id in ids]
    return bulk_dict


def server_error(request):
    """
    500 error handler.

    For now always return the debug response.  Mailing isn't appropriate here.

    """
    from django.views import debug
    return debug.technical_500_response(request, *sys.exc_info())


def timeview(fn):
    """
    Setup a timeview view

    Handles backend posts from the calendar and converts date pieces
    into a 'timestamp' parameter

    """
    def _handle_timeview(request, **kwargs):
        """Send any posts back."""
        if request.method == 'POST' and request.POST.get('op', '') == 'timeview':
            cal_date = request.POST['cal_date']
            try:
                fmt = "%Y/%m/%d"
                if cal_date.find(' ') > -1:
                    fmt += " %H:%M"
                timestamp = datetime(*strptime(cal_date, fmt)[0:6])
                view, args, kw = resolve(request.META['PATH_INFO'])
                kw['year'] = "%0.4d" % timestamp.year
                kw['month'] = "%02.d" % timestamp.month
                kw['day'] = "%02.d" % timestamp.day
                if cal_date.find(' ') > -1:
                    kw['hour'] = timestamp.hour
                    kw['minute'] = timestamp.minute
                return HttpResponseRedirect(reverse(view,
                                                    args=args,
                                                    kwargs=kw))
            except KeyError:
                pass
            except:
                pass
                # FIXME - Handle this

        """Extract timestamp from args."""
        timestamp = None
        try:
            timestamp = datetime(int(kwargs.pop('year')),
                                 int(kwargs.pop('month')),
                int(kwargs.pop('day')), int(kwargs.pop('hour', 0)),
                int(kwargs.pop('minute', 0)), 0)
            kwargs['timestamp'] = timestamp
        except KeyError:
            pass
        except:
            raise
        return fn(request, **kwargs)

    return _handle_timeview


def _handle_filters(query, **kwargs):
    """
    Applies standard filters to a query object

    Returns an updated query object

    query - query object to filter

    server -- Filter interactions by server
    state -- Filter interactions by state
    group -- Filter interactions by group

    """
    if 'state' in kwargs and kwargs['state']:
        query = query.filter(state__exact=kwargs['state'])
    if 'server' in kwargs and kwargs['server']:
        query = query.filter(server__exact=kwargs['server'])

    if 'group' in kwargs and kwargs['group']:
        group = get_object_or_404(Group, name=kwargs['group'])
        query = query.filter(groups__id=group.pk)
    return query


def config_item(request, pk, entry_type, interaction=None):
    """
    Display a single entry.

    Displays information about a single entry.

    """
    try:
        cls = BaseEntry.entry_from_name(entry_type)
    except ValueError:
        # TODO - handle this
        raise
    item = get_object_or_404(cls, pk=pk)

    # TODO - timestamp
    if interaction:
        try:
            inter = Interaction.objects.get(pk=interaction)
        except Interaction.DoesNotExist:
            raise Http404("Not a valid interaction")
        timestamp = inter.timestamp
    else:
        timestamp = datetime.now()

    ts_start = timestamp.replace(hour=1, minute=0, second=0, microsecond=0)
    ts_end = ts_start + timedelta(days=1)
    associated_list = item.interaction_set.select_related('client').filter(\
        timestamp__gte=ts_start, timestamp__lt=ts_end)

    if item.is_failure():
        template = 'config_items/item-failure.html'
    else:
        template = 'config_items/item.html'
    return render_to_response(template,
                              {'item': item,
                               'associated_list': associated_list,
                               'timestamp': timestamp},
                              context_instance=RequestContext(request))


@timeview
def config_item_list(request, item_state, timestamp=None, **kwargs):
    """Render a listing of affected elements"""
    state = convert_entry_type_to_id(item_state.lower())
    if state < 0:
        raise Http404

    current_clients = Interaction.objects.recent(timestamp)
    current_clients = [q['id'] for q in _handle_filters(current_clients, **kwargs).values('id')]

    lists = []
    for etype in ENTRY_TYPES:
        ldata = etype.objects.filter(state=state, interaction__in=current_clients)\
            .annotate(num_entries=Count('id')).select_related('linkentry', 'target_perms', 'current_perms')
        if len(ldata) > 0:
            # Property doesn't render properly..
            lists.append((etype.ENTRY_TYPE, ldata))

    return render_to_response('config_items/listing.html',
                              {'item_list': lists,
                               'item_state': item_state,
                               'timestamp': timestamp},
        context_instance=RequestContext(request))


@timeview
def entry_status(request, entry_type, pk, timestamp=None, **kwargs):
    """Render a listing of affected elements by type and name"""
    try:
        cls = BaseEntry.entry_from_name(entry_type)
    except ValueError:
        # TODO - handle this
        raise
    item = get_object_or_404(cls, pk=pk)

    current_clients = Interaction.objects.recent(timestamp)
    current_clients = [i['pk'] for i in _handle_filters(current_clients, **kwargs).values('pk')]

    # There is no good way to do this...
    items = []
    seen = []
    for it in cls.objects.filter(interaction__in=current_clients, name=item.name).select_related():
        if it.pk not in seen:
            items.append((it, it.interaction_set.filter(pk__in=current_clients).order_by('client__name').select_related('client')))
            seen.append(it.pk)

    return render_to_response('config_items/entry_status.html',
                              {'entry': item,
                               'items': items,
                               'timestamp': timestamp},
        context_instance=RequestContext(request))


@timeview
def common_problems(request, timestamp=None, threshold=None, group=None):
    """Mine config entries"""

    if request.method == 'POST':
        try:
            threshold = int(request.POST['threshold'])
            view, args, kw = resolve(request.META['PATH_INFO'])
            kw['threshold'] = threshold
            return HttpResponseRedirect(reverse(view,
                                                args=args,
                                                kwargs=kw))
        except:
            pass

    try:
        threshold = int(threshold)
    except:
        threshold = 10

    if group:
        group_obj = get_object_or_404(Group, name=group)
        current_clients = [inter[0] for inter in \
            Interaction.objects.recent(timestamp)\
                .filter(groups=group_obj).values_list('id')]
    else:
        current_clients = Interaction.objects.recent_ids(timestamp)
    lists = []
    for etype in ENTRY_TYPES:
        ldata = etype.objects.exclude(state=TYPE_GOOD).filter(
            interaction__in=current_clients).annotate(num_entries=Count('id')).filter(num_entries__gte=threshold)\
                .order_by('-num_entries', 'name')
        if len(ldata) > 0:
            # Property doesn't render properly..
            lists.append((etype.ENTRY_TYPE, ldata))

    return render_to_response('config_items/common.html',
                              {'lists': lists,
                               'timestamp': timestamp,
                               'threshold': threshold},
        context_instance=RequestContext(request))


@timeview
def client_index(request, timestamp=None, **kwargs):
    """
    Render a grid view of active clients.

    Keyword parameters:
      timestamp -- datetime object to render from

    """
    list = _handle_filters(Interaction.objects.recent(timestamp), **kwargs).\
           select_related('client').order_by("client__name").all()

    return render_to_response('clients/index.html',
                              {'inter_list': list,
                               'timestamp': timestamp},
                              context_instance=RequestContext(request))


@timeview
def client_detailed_list(request, timestamp=None, **kwargs):
    """
    Provides a more detailed list view of the clients.  Allows for extra
    filters to be passed in.

    """

    try:
        sort = request.GET['sort']
        if sort[0] == '-':
            sort_key = sort[1:]
        else:
            sort_key = sort
        if not sort_key in __SORT_FIELDS__:
            raise ValueError

        if sort_key == "client":
            kwargs['orderby'] = "%s__name" % sort
        elif sort_key in ["good", "bad", "modified", "extra"]:
            kwargs['orderby'] = "%s_count" % sort
        else:
            kwargs['orderby'] = sort
        kwargs['sort'] = sort
    except (ValueError, KeyError):
        kwargs['orderby'] = "client__name"
        kwargs['sort'] = "client"

    kwargs['interaction_base'] = \
        Interaction.objects.recent(timestamp).select_related()
    kwargs['page_limit'] = 0
    return render_history_view(request, 'clients/detailed-list.html', **kwargs)


def client_detail(request, hostname=None, pk=None):
    context = dict()
    client = get_object_or_404(Client, name=hostname)
    if(pk == None):
        inter = client.current_interaction
        maxdate = None
    else:
        inter = client.interactions.get(pk=pk)
        maxdate = inter.timestamp

    etypes = {TYPE_BAD: 'bad',
              TYPE_MODIFIED: 'modified',
              TYPE_EXTRA: 'extra'}
    edict = dict()
    for label in etypes.values():
        edict[label] = []
    for ekind in inter.entry_types:
        if ekind == 'failures':
            continue
        for ent in getattr(inter, ekind).all():
            edict[etypes[ent.state]].append(ent)
    context['entry_types'] = edict

    context['interaction'] = inter
    return render_history_view(request, 'clients/detail.html', page_limit=5,
        client=client, maxdate=maxdate, context=context)


def client_manage(request):
    """Manage client expiration"""
    message = ''
    if request.method == 'POST':
        try:
            client_name = request.POST.get('client_name', None)
            client_action = request.POST.get('client_action', None)
            client = Client.objects.get(name=client_name)
            if client_action == 'expire':
                client.expiration = datetime.now()
                client.save()
                message = "Expiration for %s set to %s." % \
                    (client_name,
                     client.expiration.strftime("%Y-%m-%d %H:%M:%S"))
            elif client_action == 'unexpire':
                client.expiration = None
                client.save()
                message = "%s is now active." % client_name
            else:
                message = "Missing action"
        except Client.DoesNotExist:
            if not client_name:
                client_name = "<none>"
            message = "Couldn't find client \"%s\"" % client_name

    return render_to_response('clients/manage.html',
        {'clients': Client.objects.order_by('name').all(), 'message': message},
        context_instance=RequestContext(request))


@timeview
def display_summary(request, timestamp=None):
    """
    Display a summary of the bcfg2 world
    """
    recent_data = Interaction.objects.recent(timestamp) \
        .select_related()
    node_count = len(recent_data)
    if not timestamp:
        timestamp = datetime.now()

    collected_data = dict(clean=[],
                          bad=[],
                          modified=[],
                          extra=[],
                          stale=[])
    for node in recent_data:
        if timestamp - node.timestamp > timedelta(hours=24):
            collected_data['stale'].append(node)
            # If stale check for uptime
        if node.bad_count > 0:
            collected_data['bad'].append(node)
        else:
            collected_data['clean'].append(node)
        if node.modified_count > 0:
            collected_data['modified'].append(node)
        if node.extra_count > 0:
            collected_data['extra'].append(node)

    # label, header_text, node_list
    summary_data = []
    get_dict = lambda name, label: {'name': name,
                                    'nodes': collected_data[name],
                                    'label': label}
    if len(collected_data['clean']) > 0:
        summary_data.append(get_dict('clean',
                                     'nodes are clean.'))
    if len(collected_data['bad']) > 0:
        summary_data.append(get_dict('bad',
                                     'nodes are bad.'))
    if len(collected_data['modified']) > 0:
        summary_data.append(get_dict('modified',
                                     'nodes were modified.'))
    if len(collected_data['extra']) > 0:
        summary_data.append(get_dict('extra',
                                     'nodes have extra configurations.'))
    if len(collected_data['stale']) > 0:
        summary_data.append(get_dict('stale',
                                     'nodes did not run within the last 24 hours.'))

    return render_to_response('displays/summary.html',
        {'summary_data': summary_data, 'node_count': node_count,
         'timestamp': timestamp},
        context_instance=RequestContext(request))


@timeview
def display_timing(request, timestamp=None):
    perfs = Performance.objects.filter(interaction__in=Interaction.objects.recent_ids(timestamp))\
        .select_related('interaction__client')

    mdict = dict()
    for perf in perfs:
        client = perf.interaction.client.name
        if client not in mdict:
            mdict[client] = { 'name': client }
        mdict[client][perf.metric] = perf.value

    return render_to_response('displays/timing.html',
                              {'metrics': list(mdict.values()),
                               'timestamp': timestamp},
                              context_instance=RequestContext(request))


def render_history_view(request, template='clients/history.html', **kwargs):
    """
    Provides a detailed history of a clients interactions.

    Renders a detailed history of a clients interactions. Allows for various
    filters and settings.  Automatically sets pagination data into the context.

    Keyword arguments:
    interaction_base -- Interaction QuerySet to build on
                        (default Interaction.objects)
    context -- Additional context data to render with
    page_number -- Page to display (default 1)
    page_limit -- Number of results per page, if 0 show all (default 25)
    client -- Client object to render
    hostname -- Client hostname to lookup and render.  Returns a 404 if
                not found
    server -- Filter interactions by server
    state -- Filter interactions by state
    group -- Filter interactions by group
    entry_max -- Most recent interaction to display
    orderby -- Sort results using this field

    """

    context = kwargs.get('context', dict())
    max_results = int(kwargs.get('page_limit', 25))
    page = int(kwargs.get('page_number', 1))

    client = kwargs.get('client', None)
    if not client and 'hostname' in kwargs:
        client = get_object_or_404(Client, name=kwargs['hostname'])
    if client:
        context['client'] = client

    entry_max = kwargs.get('maxdate', None)
    context['entry_max'] = entry_max

    # Either filter by client or limit by clients
    iquery = kwargs.get('interaction_base', Interaction.objects)
    if client:
        iquery = iquery.filter(client__exact=client)
    iquery = iquery.select_related('client')

    if 'orderby' in kwargs and kwargs['orderby']:
        iquery = iquery.order_by(kwargs['orderby'])
    if 'sort' in kwargs:
        context['sort'] = kwargs['sort']

    iquery = _handle_filters(iquery, **kwargs)

    if entry_max:
        iquery = iquery.filter(timestamp__lte=entry_max)

    if max_results < 0:
        max_results = 1
    entry_list = []
    if max_results > 0:
        try:
            rec_start, rec_end = prepare_paginated_list(request,
                                                        context,
                                                        iquery,
                                                        page,
                                                        max_results)
        except PaginationError:
            page_error = sys.exc_info()[1]
            if isinstance(page_error[0], HttpResponse):
                return page_error[0]
            return HttpResponseServerError(page_error)
        context['entry_list'] = iquery.all()[rec_start:rec_end]
    else:
        context['entry_list'] = iquery.all()

    return render_to_response(template, context,
                context_instance=RequestContext(request))


def prepare_paginated_list(request, context, paged_list, page=1, max_results=25):
    """
    Prepare context and slice an object for pagination.
    """
    if max_results < 1:
        raise PaginationError("Max results less then 1")
    if paged_list == None:
        raise PaginationError("Invalid object")

    try:
        nitems = paged_list.count()
    except TypeError:
        nitems = len(paged_list)

    rec_start = (page - 1) * int(max_results)
    try:
        total_pages = (nitems / int(max_results)) + 1
    except:
        total_pages = 1
    if page > total_pages:
        # If we passed beyond the end send back
        try:
            view, args, kwargs = resolve(request.META['PATH_INFO'])
            kwargs['page_number'] = total_pages
            raise PaginationError(HttpResponseRedirect(reverse(view,
                                                               kwargs=kwargs)))
        except (Resolver404, NoReverseMatch, ValueError):
            raise "Accessing beyond last page.  Unable to resolve redirect."

    context['total_pages'] = total_pages
    context['records_per_page'] = max_results
    return (rec_start, rec_start + int(max_results))

########NEW FILE########
__FILENAME__ = Backup
""" Make a backup of the Bcfg2 repository """

import os
import time
import tarfile
import Bcfg2.Server.Admin
import Bcfg2.Options


class Backup(Bcfg2.Server.Admin.MetadataCore):
    """ Make a backup of the Bcfg2 repository """

    def __call__(self, args):
        datastore = self.setup['repo']
        timestamp = time.strftime('%Y%m%d%H%M%S')
        fmt = 'gz'
        mode = 'w:' + fmt
        filename = timestamp + '.tar' + '.' + fmt
        out = tarfile.open(os.path.join(datastore, filename), mode=mode)
        out.add(datastore, os.path.basename(datastore))
        out.close()
        print("Archive %s was stored under %s" % (filename, datastore))

########NEW FILE########
__FILENAME__ = Client
""" Create, delete, or list client entries """

import sys
import Bcfg2.Server.Admin
from Bcfg2.Server.Plugin import MetadataConsistencyError


def get_attribs(args):
    """ Get a list of attributes to set on a client when adding/updating it """
    attr_d = {}
    for i in args[2:]:
        attr, val = i.split('=', 1)
        if attr not in ['profile', 'uuid', 'password', 'floating', 'secure',
                        'address', 'auth']:
            print("Attribute %s unknown" % attr)
            raise SystemExit(1)
        attr_d[attr] = val
    return attr_d


class Client(Bcfg2.Server.Admin.MetadataCore):
    """ Create, delete, or list client entries """
    __usage__ = "[options] [add|del|update|list] [attr=val]"
    __plugin_whitelist__ = ["Metadata"]

    def __call__(self, args):
        if len(args) == 0:
            self.errExit("No argument specified.\n"
                         "Usage: %s" % self.__usage__)
        if args[0] == 'add':
            try:
                self.metadata.add_client(args[1], get_attribs(args))
            except MetadataConsistencyError:
                self.errExit("Error adding client: %s" % sys.exc_info()[1])
        elif args[0] in ['update', 'up']:
            try:
                self.metadata.update_client(args[1], get_attribs(args))
            except MetadataConsistencyError:
                self.errExit("Error updating client: %s" % sys.exc_info()[1])
        elif args[0] in ['delete', 'remove', 'del', 'rm']:
            try:
                self.metadata.remove_client(args[1])
            except MetadataConsistencyError:
                self.errExit("Error deleting client: %s" %
                             sys.exc_info()[1])
        elif args[0] in ['list', 'ls']:
            for client in self.metadata.list_clients():
                print(client)
        else:
            self.errExit("No command specified")

########NEW FILE########
__FILENAME__ = Compare
import lxml.etree
import os
import Bcfg2.Server.Admin


class Compare(Bcfg2.Server.Admin.Mode):
    """ Determine differences between files or directories of client
    specification instances """
    __usage__ = ("<old> <new>\n\n"
                 "     -r\trecursive")

    def __init__(self, setup):
        Bcfg2.Server.Admin.Mode.__init__(self, setup)
        self.important = {'Path': ['name', 'type', 'owner', 'group', 'mode',
                                   'important', 'paranoid', 'sensitive',
                                   'dev_type', 'major', 'minor', 'prune',
                                   'encoding', 'empty', 'to', 'recursive',
                                   'vcstype', 'sourceurl', 'revision',
                                   'secontext'],
                          'Package': ['name', 'type', 'version', 'simplefile',
                                      'verify'],
                          'Service': ['name', 'type', 'status', 'mode',
                                      'target', 'sequence', 'parameters'],
                          'Action': ['name', 'timing', 'when', 'status',
                                     'command'],
                          'PostInstall': ['name']
                          }

    def compareStructures(self, new, old):
        if new.tag == 'Independent':
            bundle = 'Base'
        else:
            bundle = new.get('name')

        identical = True

        for child in new.getchildren():
            if child.tag not in self.important:
                print("  %s in (new) bundle %s:\n   tag type not handled!" %
                      (child.tag, bundle))
                continue
            equiv = old.xpath('%s[@name="%s"]' %
                              (child.tag, child.get('name')))
            if len(equiv) == 0:
                print("  %s %s in bundle %s:\n   only in new configuration" %
                      (child.tag, child.get('name'), bundle))
                identical = False
                continue
            diff = []
            if child.tag == 'Path' and child.get('type') == 'file' and \
               child.text != equiv[0].text:
                diff.append('contents')
            attrdiff = [field for field in self.important[child.tag] if \
                        child.get(field) != equiv[0].get(field)]
            if attrdiff:
                diff.append('attributes (%s)' % ', '.join(attrdiff))
            if diff:
                print("  %s %s in bundle %s:\n   %s differ" % (child.tag, \
                      child.get('name'), bundle, ' and '.join(diff)))
                identical = False

        for child in old.getchildren():
            if child.tag not in self.important:
                print("  %s in (old) bundle %s:\n   tag type not handled!" %
                      (child.tag, bundle))
            elif len(new.xpath('%s[@name="%s"]' %
                     (child.tag, child.get('name')))) == 0:
                print("  %s %s in bundle %s:\n   only in old configuration" %
                      (child.tag, child.get('name'), bundle))
                identical = False

        return identical

    def compareSpecifications(self, path1, path2):
        try:
            new = lxml.etree.parse(path1).getroot()
        except IOError:
            print("Failed to read %s" % (path1))
            raise SystemExit(1)

        try:
            old = lxml.etree.parse(path2).getroot()
        except IOError:
            print("Failed to read %s" % (path2))
            raise SystemExit(1)

        for src in [new, old]:
            for bundle in src.findall('./Bundle'):
                if bundle.get('name')[-4:] == '.xml':
                    bundle.set('name', bundle.get('name')[:-4])

        identical = True

        for bundle in old.findall('./Bundle'):
            if len(new.xpath('Bundle[@name="%s"]' % (bundle.get('name')))) == 0:
                print(" Bundle %s only in old configuration" %
                      bundle.get('name'))
                identical = False
        for bundle in new.findall('./Bundle'):
            equiv = old.xpath('Bundle[@name="%s"]' % (bundle.get('name')))
            if len(equiv) == 0:
                print(" Bundle %s only in new configuration" %
                      bundle.get('name'))
                identical = False
            elif not self.compareStructures(bundle, equiv[0]):
                identical = False

        i1 = lxml.etree.Element('Independent')
        i2 = lxml.etree.Element('Independent')
        i1.extend(new.findall('./Independent/*'))
        i2.extend(old.findall('./Independent/*'))
        if not self.compareStructures(i1, i2):
            identical = False

        return identical

    def __call__(self, args):
        if len(args) == 0:
            self.errExit("No argument specified.\n"
                         "Please see bcfg2-admin compare help for usage.")
        if '-r' in args:
            args = list(args)
            args.remove('-r')
            (oldd, newd) = args
            (old, new) = [os.listdir(spot) for spot in args]
            old_extra = []
            for item in old:
                if item not in new:
                    old_extra.append(item)
                    continue
                print("File: %s" % item)
                state = self.__call__([oldd + '/' + item, newd + '/' + item])
                new.remove(item)
                if state:
                    print("File %s is good" % item)
                else:
                    print("File %s is bad" % item)
            if new:
                print("%s has extra files: %s" % (newd, ', '.join(new)))
            if old_extra:
                print("%s has extra files: %s" % (oldd, ', '.join(old_extra)))
            return
        try:
            (old, new) = args
            return self.compareSpecifications(new, old)
        except IndexError:
            self.errExit(self.__call__.__doc__)

########NEW FILE########
__FILENAME__ = Init
""" Interactively initialize a new repository. """

import os
import sys
import stat
import select
import random
import socket
import string
import getpass
import subprocess

import Bcfg2.Server.Admin
import Bcfg2.Server.Plugin
import Bcfg2.Options
import Bcfg2.Server.Plugins.Metadata
from Bcfg2.Compat import input  # pylint: disable=W0622

# default config file
CONFIG = '''[server]
repository = %s
plugins = %s
# Uncomment the following to listen on all interfaces
#listen_all = true

[statistics]
sendmailpath = %s
#web_debug = False
#time_zone =

[database]
#engine = sqlite3
# 'postgresql', 'mysql', 'mysql_old', 'sqlite3' or 'ado_mssql'.
#name =
# Or path to database file if using sqlite3.
#<repository>/etc/bcfg2.sqlite is default path if left empty
#user =
# Not used with sqlite3.
#password =
# Not used with sqlite3.
#host =
# Not used with sqlite3.
#port =

[reporting]
transport = LocalFilesystem

[communication]
protocol = %s
password = %s
certificate = %s
key = %s
ca = %s

[components]
bcfg2 = %s
'''

# Default groups
GROUPS = '''<Groups version='3.0'>
   <Group profile='true' public='true' default='true' name='basic'>
      <Group name='%s'/>
   </Group>
   <Group name='ubuntu'/>
   <Group name='debian'/>
   <Group name='freebsd'/>
   <Group name='gentoo'/>
   <Group name='redhat'/>
   <Group name='suse'/>
   <Group name='mandrake'/>
   <Group name='solaris'/>
   <Group name='arch'/>
</Groups>
'''

# Default contents of clients.xml
CLIENTS = '''<Clients version="3.0">
   <Client profile="basic" name="%s"/>
</Clients>
'''

# Mapping of operating system names to groups
OS_LIST = [('Red Hat/Fedora/RHEL/RHAS/CentOS', 'redhat'),
           ('SUSE/SLES', 'suse'),
           ('Mandrake', 'mandrake'),
           ('Debian', 'debian'),
           ('Ubuntu', 'ubuntu'),
           ('Gentoo', 'gentoo'),
           ('FreeBSD', 'freebsd'),
           ('Arch', 'arch')]


def safe_input(prompt):
    """ input() that flushes the input buffer before accepting input """
    # flush input buffer
    while len(select.select([sys.stdin.fileno()], [], [], 0.0)[0]) > 0:
        os.read(sys.stdin.fileno(), 4096)
    return input(prompt)


def gen_password(length):
    """Generates a random alphanumeric password with length characters."""
    chars = string.letters + string.digits
    return "".join(random.choice(chars) for i in range(length))


def create_key(hostname, keypath, certpath, country, state, location):
    """Creates a bcfg2.key at the directory specifed by keypath."""
    kcstr = ("openssl req -batch -x509 -nodes -subj '/C=%s/ST=%s/L=%s/CN=%s' "
             "-days 1000 -newkey rsa:2048 -keyout %s -noout" % (country,
                                                                state,
                                                                location,
                                                                hostname,
                                                                keypath))
    subprocess.call((kcstr), shell=True)
    ccstr = ("openssl req -batch -new -subj '/C=%s/ST=%s/L=%s/CN=%s' -key %s "
             "| openssl x509 -req -days 1000 -signkey %s -out %s" % (country,
                                                                     state,
                                                                     location,
                                                                     hostname,
                                                                     keypath,
                                                                     keypath,
                                                                     certpath))
    subprocess.call((ccstr), shell=True)
    os.chmod(keypath, stat.S_IRUSR | stat.S_IWUSR)  # 0600


def create_conf(confpath, confdata):
    """ create the config file """
    # Don't overwrite existing bcfg2.conf file
    if os.path.exists(confpath):
        result = safe_input("\nWarning: %s already exists. "
                            "Overwrite? [y/N]: " % confpath)
        if result not in ['Y', 'y']:
            print("Leaving %s unchanged" % confpath)
            return
    try:
        open(confpath, "w").write(confdata)
        os.chmod(confpath, stat.S_IRUSR | stat.S_IWUSR)  # 0600
    except Exception:
        err = sys.exc_info()[1]
        print("Error trying to write configuration file '%s': %s" %
              (confpath, err))
        raise SystemExit(1)


class Init(Bcfg2.Server.Admin.Mode):
    """Interactively initialize a new repository."""
    options = {'configfile': Bcfg2.Options.CFILE,
               'plugins': Bcfg2.Options.SERVER_PLUGINS,
               'proto': Bcfg2.Options.SERVER_PROTOCOL,
               'repo': Bcfg2.Options.SERVER_REPOSITORY,
               'sendmail': Bcfg2.Options.SENDMAIL_PATH}

    def __init__(self, setup):
        Bcfg2.Server.Admin.Mode.__init__(self, setup)
        self.data = dict()
        self.plugins = Bcfg2.Options.SERVER_PLUGINS.default

    def _set_defaults(self, opts):
        """Set default parameters."""
        self.data['configfile'] = opts['configfile']
        self.data['repopath'] = opts['repo']
        self.data['password'] = gen_password(8)
        self.data['server_uri'] = "https://%s:6789" % socket.getfqdn()
        self.data['sendmail'] = opts['sendmail']
        self.data['proto'] = opts['proto']
        if os.path.exists("/etc/pki/tls"):
            self.data['keypath'] = "/etc/pki/tls/private/bcfg2.key"
            self.data['certpath'] = "/etc/pki/tls/certs/bcfg2.crt"
        elif os.path.exists("/etc/ssl"):
            self.data['keypath'] = "/etc/ssl/bcfg2.key"
            self.data['certpath'] = "/etc/ssl/bcfg2.crt"
        else:
            basepath = os.path.dirname(self.configfile)
            self.data['keypath'] = os.path.join(basepath, "bcfg2.key")
            self.data['certpath'] = os.path.join(basepath, 'bcfg2.crt')

    def __call__(self, args):
        # Parse options
        opts = Bcfg2.Options.OptionParser(self.options)
        opts.parse(args)
        self._set_defaults(opts)

        # Prompt the user for input
        self._prompt_config()
        self._prompt_repopath()
        self._prompt_password()
        self._prompt_hostname()
        self._prompt_server()
        self._prompt_groups()
        self._prompt_keypath()
        self._prompt_certificate()

        # Initialize the repository
        self.init_repo()

    def _prompt_hostname(self):
        """Ask for the server hostname."""
        data = safe_input("What is the server's hostname [%s]: " %
                          socket.getfqdn())
        if data != '':
            self.data['shostname'] = data
        else:
            self.data['shostname'] = socket.getfqdn()

    def _prompt_config(self):
        """Ask for the configuration file path."""
        newconfig = safe_input("Store Bcfg2 configuration in [%s]: " %
                               self.configfile)
        if newconfig != '':
            self.data['configfile'] = os.path.abspath(newconfig)

    def _prompt_repopath(self):
        """Ask for the repository path."""
        while True:
            newrepo = safe_input("Location of Bcfg2 repository [%s]: " %
                                 self.data['repopath'])
            if newrepo != '':
                self.data['repopath'] = os.path.abspath(newrepo)
            if os.path.isdir(self.data['repopath']):
                response = safe_input("Directory %s exists. Overwrite? [y/N]:"
                                      % self.data['repopath'])
                if response.lower().strip() == 'y':
                    break
            else:
                break

    def _prompt_password(self):
        """Ask for a password or generate one if none is provided."""
        newpassword = getpass.getpass(
            "Input password used for communication verification "
            "(without echoing; leave blank for a random): ").strip()
        if len(newpassword) != 0:
            self.data['password'] = newpassword

    def _prompt_server(self):
        """Ask for the server name."""
        newserver = safe_input(
            "Input the server location (the server listens on a single "
            "interface by default) [%s]: " % self.data['server_uri'])
        if newserver != '':
            self.data['server_uri'] = newserver

    def _prompt_groups(self):
        """Create the groups.xml file."""
        prompt = '''Input base Operating System for clients:\n'''
        for entry in OS_LIST:
            prompt += "%d: %s\n" % (OS_LIST.index(entry) + 1, entry[0])
        prompt += ': '
        while True:
            try:
                osidx = int(safe_input(prompt))
                self.data['os_sel'] = OS_LIST[osidx - 1][1]
                break
            except ValueError:
                continue

    def _prompt_certificate(self):
        """Ask for the key details (country, state, and location)."""
        print("The following questions affect SSL certificate generation.")
        print("If no data is provided, the default values are used.")
        newcountry = safe_input("Country name (2 letter code) for "
                                "certificate: ")
        if newcountry != '':
            if len(newcountry) == 2:
                self.data['country'] = newcountry
            else:
                while len(newcountry) != 2:
                    newcountry = safe_input("2 letter country code (eg. US): ")
                    if len(newcountry) == 2:
                        self.data['country'] = newcountry
                        break
        else:
            self.data['country'] = 'US'

        newstate = safe_input("State or Province Name (full name) for "
                              "certificate: ")
        if newstate != '':
            self.data['state'] = newstate
        else:
            self.data['state'] = 'Illinois'

        newlocation = safe_input("Locality Name (eg, city) for certificate: ")
        if newlocation != '':
            self.data['location'] = newlocation
        else:
            self.data['location'] = 'Argonne'

    def _prompt_keypath(self):
        """ Ask for the key pair location.  Try to use sensible
        defaults depending on the OS """
        keypath = safe_input("Path where Bcfg2 server private key will be "
                             "created [%s]: " % self.data['keypath'])
        if keypath:
            self.data['keypath'] = keypath
        certpath = safe_input("Path where Bcfg2 server cert will be created "
                              "[%s]: " % self.data['certpath'])
        if certpath:
            self.data['certpath'] = certpath

    def _init_plugins(self):
        """Initialize each plugin-specific portion of the repository."""
        for plugin in self.plugins:
            if plugin == 'Metadata':
                Bcfg2.Server.Plugins.Metadata.Metadata.init_repo(
                    self.data['repopath'],
                    groups_xml=GROUPS % self.data['os_sel'],
                    clients_xml=CLIENTS % socket.getfqdn())
            else:
                try:
                    module = __import__("Bcfg2.Server.Plugins.%s" % plugin, '',
                                        '', ["Bcfg2.Server.Plugins"])
                    cls = getattr(module, plugin)
                    cls.init_repo(self.data['repopath'])
                except:  # pylint: disable=W0702
                    err = sys.exc_info()[1]
                    print("Plugin setup for %s failed: %s\n"
                          "Check that dependencies are installed" % (plugin,
                                                                     err))

    def init_repo(self):
        """Setup a new repo and create the content of the
        configuration file."""
        # Create the repository
        path = os.path.join(self.data['repopath'], 'etc')
        try:
            os.makedirs(path)
            self._init_plugins()
            print("Repository created successfuly in %s" %
                  self.data['repopath'])
        except OSError:
            print("Failed to create %s." % path)

        confdata = CONFIG % (self.data['repopath'],
                             ','.join(self.plugins),
                             self.data['sendmail'],
                             self.data['proto'],
                             self.data['password'],
                             self.data['certpath'],
                             self.data['keypath'],
                             self.data['certpath'],
                             self.data['server_uri'])

        # Create the configuration file and SSL key
        create_conf(self.data['configfile'], confdata)
        create_key(self.data['shostname'], self.data['keypath'],
                   self.data['certpath'], self.data['country'],
                   self.data['state'], self.data['location'])

########NEW FILE########
__FILENAME__ = Minestruct
""" Extract extra entry lists from statistics """
import getopt
import lxml.etree
import sys
import Bcfg2.Server.Admin
from Bcfg2.Server.Plugin import PullSource


class Minestruct(Bcfg2.Server.Admin.StructureMode):
    """ Extract extra entry lists from statistics """
    __usage__ = ("[options] <client>\n\n"
                 "     %-25s%s\n"
                 "     %-25s%s\n" %
                 ("-f <filename>", "build a particular file",
                  "-g <groups>", "only build config for groups"))

    def __call__(self, args):
        if len(args) == 0:
            self.errExit("No argument specified.\n"
                         "Please see bcfg2-admin minestruct help for usage.")
        try:
            (opts, args) = getopt.getopt(args, 'f:g:h')
        except getopt.GetoptError:
            self.errExit(self.__doc__)

        client = args[0]
        output = sys.stdout
        groups = []

        for (opt, optarg) in opts:
            if opt == '-f':
                try:
                    output = open(optarg, 'w')
                except IOError:
                    self.errExit("Failed to open file: %s" % (optarg))
            elif opt == '-g':
                groups = optarg.split(':')

        try:
            extra = set()
            for source in self.bcore.plugins_by_type(PullSource):
                for item in source.GetExtra(client):
                    extra.add(item)
        except:  # pylint: disable=W0702
            self.errExit("Failed to find extra entry info for client %s" %
                         client)
        root = lxml.etree.Element("Base")
        self.log.info("Found %d extra entries" % (len(extra)))
        add_point = root
        for grp in groups:
            add_point = lxml.etree.SubElement(add_point, "Group", name=grp)
        for tag, name in extra:
            self.log.info("%s: %s" % (tag, name))
            lxml.etree.SubElement(add_point, tag, name=name)

        lxml.etree.ElementTree(root).write(output, pretty_print=True)

########NEW FILE########
__FILENAME__ = Perf
""" Get performance data from server """

import sys
import Bcfg2.Options
import Bcfg2.Proxy
import Bcfg2.Server.Admin


class Perf(Bcfg2.Server.Admin.Mode):
    """ Get performance data from server """

    def __call__(self, args):
        output = [('Name', 'Min', 'Max', 'Mean', 'Count')]
        optinfo = {
            'ca': Bcfg2.Options.CLIENT_CA,
            'certificate': Bcfg2.Options.CLIENT_CERT,
            'key': Bcfg2.Options.SERVER_KEY,
            'password': Bcfg2.Options.SERVER_PASSWORD,
            'server': Bcfg2.Options.SERVER_LOCATION,
            'user': Bcfg2.Options.CLIENT_USER,
            'timeout': Bcfg2.Options.CLIENT_TIMEOUT}
        setup = Bcfg2.Options.OptionParser(optinfo)
        setup.parse(sys.argv[1:])
        proxy = Bcfg2.Proxy.ComponentProxy(setup['server'],
                                           setup['user'],
                                           setup['password'],
                                           key=setup['key'],
                                           cert=setup['certificate'],
                                           ca=setup['ca'],
                                           timeout=setup['timeout'])
        data = proxy.get_statistics()
        for key in sorted(data.keys()):
            output.append(
                (key, ) +
                tuple(["%.06f" % item
                       for item in data[key][:-1]] + [data[key][-1]]))
        self.print_table(output)

########NEW FILE########
__FILENAME__ = Pull
""" Retrieves entries from clients and integrates the information into
the repository """

import os
import sys
import getopt
import select
import Bcfg2.Server.Admin
from Bcfg2.Server.Plugin import PullSource, Generator
from Bcfg2.Compat import input  # pylint: disable=W0622


class Pull(Bcfg2.Server.Admin.MetadataCore):
    """ Retrieves entries from clients and integrates the information
    into the repository """
    __usage__ = ("[options] <client> <entry type> <entry name>\n\n"
                 "     %-25s%s\n"
                 "     %-25s%s\n"
                 "     %-25s%s\n"
                 "     %-25s%s\n" %
                 ("-v", "be verbose",
                  "-f", "force",
                  "-I", "interactive",
                  "-s", "stdin"))

    def __init__(self, setup):
        Bcfg2.Server.Admin.MetadataCore.__init__(self, setup)
        self.log = False
        self.mode = 'interactive'

    def __call__(self, args):
        use_stdin = False
        try:
            opts, gargs = getopt.getopt(args, 'vfIs')
        except getopt.GetoptError:
            self.errExit(self.__doc__)
        for opt in opts:
            if opt[0] == '-v':
                self.log = True
            elif opt[0] == '-f':
                self.mode = 'force'
            elif opt[0] == '-I':
                self.mode = 'interactive'
            elif opt[0] == '-s':
                use_stdin = True

        if use_stdin:
            for line in sys.stdin:
                try:
                    self.PullEntry(*line.split(None, 3))
                except SystemExit:
                    print("  for %s" % line)
                except:
                    print("Bad entry: %s" % line.strip())
        elif len(gargs) < 3:
            self.usage()
        else:
            self.PullEntry(gargs[0], gargs[1], gargs[2])

    def BuildNewEntry(self, client, etype, ename):
        """Construct a new full entry for
        given client/entry from statistics.
        """
        new_entry = {'type': etype, 'name': ename}
        pull_sources = self.bcore.plugins_by_type(PullSource)
        for plugin in pull_sources:
            try:
                (owner, group, mode, contents) = \
                    plugin.GetCurrentEntry(client, etype, ename)
                break
            except Bcfg2.Server.Plugin.PluginExecutionError:
                if plugin == pull_sources[-1]:
                    print("Pull Source failure; could not fetch current state")
                    raise SystemExit(1)

        try:
            data = {'owner': owner,
                    'group': group,
                    'mode': mode,
                    'text': contents}
        except UnboundLocalError:
            print("Unable to build entry. "
                  "Do you have a statistics plugin enabled?")
            raise SystemExit(1)
        for key, val in list(data.items()):
            if val:
                new_entry[key] = val
        return new_entry

    def Choose(self, choices):
        """Determine where to put pull data."""
        if self.mode == 'interactive':
            for choice in choices:
                print("Plugin returned choice:")
                if id(choice) == id(choices[0]):
                    print("(current entry) ")
                if choice.all:
                    print(" => global entry")
                elif choice.group:
                    print(" => group entry: %s (prio %d)" %
                          (choice.group, choice.prio))
                else:
                    print(" => host entry: %s" % (choice.hostname))

                # flush input buffer
                while len(select.select([sys.stdin.fileno()], [], [],
                                        0.0)[0]) > 0:
                    os.read(sys.stdin.fileno(), 4096)
                ans = input("Use this entry? [yN]: ") in ['y', 'Y']
                if ans:
                    return choice
            return False
        else:
            # mode == 'force'
            if not choices:
                return False
            return choices[0]

    def PullEntry(self, client, etype, ename):
        """Make currently recorded client state correct for entry."""
        new_entry = self.BuildNewEntry(client, etype, ename)

        meta = self.bcore.build_metadata(client)
        # Find appropriate plugin in bcore
        glist = [gen for gen in self.bcore.plugins_by_type(Generator)
                 if ename in gen.Entries.get(etype, {})]
        if len(glist) != 1:
            self.errExit("Got wrong numbers of matching generators for entry:"
                         "%s" % ([g.name for g in glist]))
        plugin = glist[0]
        if not isinstance(plugin, Bcfg2.Server.Plugin.PullTarget):
            self.errExit("Configuration upload not supported by plugin %s" %
                         plugin.name)
        try:
            choices = plugin.AcceptChoices(new_entry, meta)
            specific = self.Choose(choices)
            if specific:
                plugin.AcceptPullData(specific, new_entry, self.log)
        except Bcfg2.Server.Plugin.PluginExecutionError:
            self.errExit("Configuration upload not supported by plugin %s" %
                         plugin.name)
        # Commit if running under a VCS
        for vcsplugin in list(self.bcore.plugins.values()):
            if isinstance(vcsplugin, Bcfg2.Server.Plugin.Version):
                files = "%s/%s" % (plugin.data, ename)
                comment = 'file "%s" pulled from host %s' % (files, client)
                vcsplugin.commit_data([files], comment)

########NEW FILE########
__FILENAME__ = Reports
'''Admin interface for dynamic reports'''
import Bcfg2.Logger
import Bcfg2.Server.Admin
import datetime
import os
import sys
import traceback
from Bcfg2 import settings

# Load django and reports stuff _after_ we know we can load settings
from django.core import management
from Bcfg2.Reporting.utils import *

project_directory = os.path.dirname(settings.__file__)
project_name = os.path.basename(project_directory)
sys.path.append(os.path.join(project_directory, '..'))
project_module = __import__(project_name, '', '', [''])
sys.path.pop()

# Set DJANGO_SETTINGS_MODULE appropriately.
os.environ['DJANGO_SETTINGS_MODULE'] = '%s.settings' % project_name

from Bcfg2.Reporting.models import Client, Interaction, \
    Performance, Bundle, Group, FailureEntry, PathEntry, \
    PackageEntry, ServiceEntry, ActionEntry
from Bcfg2.Reporting.Compat import transaction


def printStats(fn):
    """
    Print db stats.

    Decorator for purging.  Prints database statistics after a run.
    """
    def print_stats(self, *data):
        classes = (Client, Interaction, Performance, \
            FailureEntry, ActionEntry, PathEntry, PackageEntry, \
            ServiceEntry, Group, Bundle)

        starts = {}
        for cls in classes:
            starts[cls] = cls.objects.count()

        fn(self, *data)

        for cls in classes:
            print("%s removed: %s" % (cls().__class__.__name__,
                starts[cls] - cls.objects.count()))

    return print_stats


class Reports(Bcfg2.Server.Admin.Mode):
    """ Manage dynamic reports """
    django_commands = ['dbshell', 'shell', 'sqlall', 'validate']
    __usage__ = ("[command] [options]\n"
                 "  Commands:\n"
                 "    init                 Initialize the database\n"
                 "    purge                Purge records\n"
                 "      --client [n]       Client to operate on\n"
                 "      --days   [n]       Records older then n days\n"
                 "      --expired          Expired clients only\n"
                 "    scrub                Scrub the database for duplicate "
                 "reasons and orphaned entries\n"
                 "    stats                print database statistics\n"
                 "    update               Apply any updates to the reporting "
                 "database\n"
                 "\n"
                 "  Django commands:\n    " \
                 + "\n    ".join(django_commands))

    def __init__(self, setup):
        Bcfg2.Server.Admin.Mode.__init__(self, setup)
        try:
            import south
        except ImportError:
            print("Django south is required for Reporting")
            raise SystemExit(-3)

    def __call__(self, args):
        if len(args) == 0 or args[0] == '-h':
            self.errExit(self.__usage__)

        # FIXME - dry run

        if args[0] in self.django_commands:
            self.django_command_proxy(args[0])
        elif args[0] == 'scrub':
            self.scrub()
        elif args[0] == 'stats':
            self.stats()
        elif args[0] in ['init', 'update', 'syncdb']:
            if self.setup['debug']:
                vrb = 2
            elif self.setup['verbose']:
                vrb = 1
            else:
                vrb = 0
            try:
                management.call_command("syncdb", verbosity=vrb)
                management.call_command("migrate", verbosity=vrb)
            except:
                self.errExit("Update failed: %s" % sys.exc_info()[1])
        elif args[0] == 'purge':
            expired = False
            client = None
            maxdate = None
            state = None
            i = 1
            while i < len(args):
                if args[i] == '-c' or args[i] == '--client':
                    if client:
                        self.errExit("Only one client per run")
                    client = args[i + 1]
                    print(client)
                    i = i + 1
                elif args[i] == '--days':
                    if maxdate:
                        self.errExit("Max date specified multiple times")
                    try:
                        maxdate = datetime.datetime.now() - \
                            datetime.timedelta(days=int(args[i + 1]))
                    except:
                        self.errExit("Invalid number of days: %s" %
                                     args[i + 1])
                    i = i + 1
                elif args[i] == '--expired':
                    expired = True
                i = i + 1
            if expired:
                if state:
                    self.errExit("--state is not valid with --expired")
                self.purge_expired(maxdate)
            else:
                self.purge(client, maxdate, state)
        else:
            self.errExit("Unknown command: %s" % args[0])

    @transaction.atomic
    def scrub(self):
        ''' Perform a thorough scrub and cleanup of the database '''

        # Cleanup unused entries
        for cls in (Group, Bundle, FailureEntry, ActionEntry, PathEntry,
                    PackageEntry, PathEntry):
            try:
                start_count = cls.objects.count()
                cls.prune_orphans()
                self.log.info("Pruned %d %s records" % \
                    (start_count - cls.objects.count(), cls.__class__.__name__))
            except:
                print("Failed to prune %s: %s" %
                      (cls.__class__.__name__, sys.exc_info()[1]))

    def django_command_proxy(self, command):
        '''Call a django command'''
        if command == 'sqlall':
            management.call_command(command, 'Reporting')
        else:
            management.call_command(command)

    @printStats
    def purge(self, client=None, maxdate=None, state=None):
        '''Purge historical data from the database'''

        filtered = False  # indicates whether or not a client should be deleted

        if not client and not maxdate and not state:
            self.errExit("Reports.prune: Refusing to prune all data")

        ipurge = Interaction.objects
        if client:
            try:
                cobj = Client.objects.get(name=client)
                ipurge = ipurge.filter(client=cobj)
            except Client.DoesNotExist:
                self.errExit("Client %s not in database" % client)
            self.log.debug("Filtering by client: %s" % client)

        if maxdate:
            filtered = True
            if not isinstance(maxdate, datetime.datetime):
                raise TypeError("maxdate is not a DateTime object")
            self.log.debug("Filtering by maxdate: %s" % maxdate)
            ipurge = ipurge.filter(timestamp__lt=maxdate)

        if settings.DATABASES['default']['ENGINE'] == \
                'django.db.backends.sqlite3':
            grp_limit = 100
        else:
            grp_limit = 1000
        if state:
            filtered = True
            if state not in ('dirty', 'clean', 'modified'):
                raise TypeError("state is not one of the following values: "
                                "dirty, clean, modified")
            self.log.debug("Filtering by state: %s" % state)
            ipurge = ipurge.filter(state=state)

        count = ipurge.count()
        rnum = 0
        try:
            while rnum < count:
                grp = list(ipurge[:grp_limit].values("id"))
                # just in case...
                if not grp:
                    break
                Interaction.objects.filter(id__in=[x['id']
                                                   for x in grp]).delete()
                rnum += len(grp)
                self.log.debug("Deleted %s of %s" % (rnum, count))
        except:
            self.log.error("Failed to remove interactions")
            (a, b, c) = sys.exc_info()
            msg = traceback.format_exception(a, b, c, limit=2)[-1][:-1]
            del a, b, c
            self.log.error(msg)

        # Prune any orphaned ManyToMany relations
        for m2m in (ActionEntry, PackageEntry, PathEntry, ServiceEntry, \
                FailureEntry, Group, Bundle):
            self.log.debug("Pruning any orphaned %s objects" % \
                m2m().__class__.__name__)
            m2m.prune_orphans()

        if client and not filtered:
            # Delete the client, ping data is automatic
            try:
                self.log.debug("Purging client %s" % client)
                cobj.delete()
            except:
                self.log.error("Failed to delete client %s" % client)
                (a, b, c) = sys.exc_info()
                msg = traceback.format_exception(a, b, c, limit=2)[-1][:-1]
                del a, b, c
                self.log.error(msg)

    @printStats
    def purge_expired(self, maxdate=None):
        '''Purge expired clients from the database'''

        if maxdate:
            if not isinstance(maxdate, datetime.datetime):
                raise TypeError("maxdate is not a DateTime object")
            self.log.debug("Filtering by maxdate: %s" % maxdate)
            clients = Client.objects.filter(expiration__lt=maxdate)
        else:
            clients = Client.objects.filter(expiration__isnull=False)

        for client in clients:
            self.log.debug("Purging client %s" % client)
            Interaction.objects.filter(client=client).delete()
            client.delete()

    def stats(self):
        classes = (Client, Interaction, Performance, \
            FailureEntry, ActionEntry, PathEntry, PackageEntry, \
            ServiceEntry, Group, Bundle)

        for cls in classes:
            print("%s has %s records" % (cls().__class__.__name__,
                cls.objects.count()))

########NEW FILE########
__FILENAME__ = Snapshots
from datetime import date
import sys

# Prereq issues can be signaled with ImportError, so no try needed
import sqlalchemy, sqlalchemy.orm
import Bcfg2.Server.Admin
import Bcfg2.Server.Snapshots
import Bcfg2.Server.Snapshots.model
from Bcfg2.Server.Snapshots.model import Snapshot, Client, Metadata, Base, \
     File, Group, Package, Service
# Compatibility import
from Bcfg2.Compat import u_str

class Snapshots(Bcfg2.Server.Admin.Mode):
    """ Interact with the Snapshots system """
    __usage__ = "[init|query qtype]"

    q_dispatch = {'client':   Client,
                  'group':    Group,
                  'metadata': Metadata,
                  'package':  Package,
                  'snapshot': Snapshot}

    def __init__(self, setup):
        Bcfg2.Server.Admin.Mode.__init__(self, setup)
        self.session = Bcfg2.Server.Snapshots.setup_session(self.configfile)
        self.cfile = self.configfile

    def __call__(self, args):
        if len(args) == 0 or args[0] == '-h':
            print(self.__usage__)
            raise SystemExit(0)

        if args[0] == 'query':
            if args[1] in self.q_dispatch:
                q_obj = self.q_dispatch[args[1]]
                if q_obj == Client:
                    rows = []
                    labels = ('Client', 'Active')
                    for host in \
                       self.session.query(q_obj).filter(q_obj.active == False):
                        rows.append([host.name, 'No'])
                    for host in \
                       self.session.query(q_obj).filter(q_obj.active == True):
                        rows.append([host.name, 'Yes'])
                    self.print_table([labels]+rows,
                                     justify='left',
                                     hdr=True,
                                     vdelim=" ",
                                     padding=1)
                elif q_obj == Group:
                    print("Groups:")
                    for group in self.session.query(q_obj).all():
                        print(" %s" % group.name)
                else:
                    results = self.session.query(q_obj).all()
            else:
                print('error')
                raise SystemExit(1)
        elif args[0] == 'init':
            # Initialize the Snapshots database
            dbpath = Bcfg2.Server.Snapshots.db_from_config(self.cfile)
            engine = sqlalchemy.create_engine(dbpath, echo=True)
            metadata = Base.metadata
            metadata.create_all(engine)
            Session = sqlalchemy.orm.sessionmaker()
            Session.configure(bind=engine)
            session = Session()
            session.commit()
        elif args[0] == 'dump':
            client = args[1]
            snap = Snapshot.get_current(self.session, u_str(client))
            if not snap:
                print("Current snapshot for %s not found" % client)
                sys.exit(1)
            print("Client %s last run at %s" % (client, snap.timestamp))
            for pkg in snap.packages:
                print("C:", pkg.correct, 'M:', pkg.modified)
                print("start", pkg.start.name, pkg.start.version)
                print("end", pkg.end.name, pkg.end.version)
        elif args[0] == 'reports':
            # bcfg2-admin reporting interface for Snapshots
            if '-a' in args[1:]:
                # Query all hosts for Name, Status, Revision, Timestamp
                q = self.session.query(Client.name,
                                       Snapshot.correct,
                                       Snapshot.revision,
                                       Snapshot.timestamp)\
                                       .filter(Client.id==Snapshot.client_id)\
                                       .group_by(Client.id)
                rows = []
                labels = ('Client', 'Correct', 'Revision', 'Time')
                for item in q.all():
                    cli, cor, time, rev = item
                    rows.append([cli, cor, time, rev])
                self.print_table([labels]+rows,
                                 justify='left',
                                 hdr=True, vdelim=" ",
                                 padding=1)
            elif '-b' in args[1:]:
                # Query a single host for bad entries
                if len(args) < 3:
                    print("Usage: bcfg2-admin snapshots -b <client>")
                    return
                client = args[2]
                snap = Snapshot.get_current(self.session, u_str(client))
                if not snap:
                    print("Current snapshot for %s not found" % client)
                    sys.exit(1)
                print("Bad entries:")
                bad_pkgs = [self.session.query(Package)
                                .filter(Package.id==p.start_id).one().name \
                            for p in snap.packages if p.correct == False]
                for p in bad_pkgs:
                    print(" Package:%s" % p)
                bad_files = [self.session.query(File)
                                .filter(File.id==f.start_id).one().name \
                             for f in snap.files if f.correct == False]
                for filename in bad_files:
                    print(" File:%s" % filename)
                bad_svcs = [self.session.query(Service)
                                .filter(Service.id==s.start_id).one().name \
                            for s in snap.services if s.correct == False]
                for svc in bad_svcs:
                    print(" Service:%s" % svc)
            elif '-e' in args[1:]:
                # Query a single host for extra entries
                client = args[2]
                snap = Snapshot.get_current(self.session, u_str(client))
                if not snap:
                    print("Current snapshot for %s not found" % client)
                    sys.exit(1)
                print("Extra entries:")
                for pkg in snap.extra_packages:
                    print(" Package:%s" % pkg.name)
                # FIXME: Do we know about extra files yet?
                for f in snap.extra_files:
                    print(" File:%s" % f.name)
                for svc in snap.extra_services:
                    print(" Service:%s" % svc.name)
            elif '--date' in args[1:]:
                year, month, day = args[2:]
                timestamp = date(int(year), int(month), int(day))
                snaps = []
                for client in self.session.query(Client).filter(Client.active == True):
                    snaps.append(Snapshot.get_by_date(self.session,
                                                      client.name,
                                                      timestamp))
                rows = []
                labels = ('Client', 'Correct', 'Revision', 'Time')
                for snap in snaps:
                    rows.append([snap.client.name,
                                 snap.correct,
                                 snap.revision,
                                 snap.timestamp])
                self.print_table([labels]+rows,
                                 justify='left',
                                 hdr=True,
                                 vdelim=" ",
                                 padding=1)
            else:
                print("Unknown options: ", args[1:])

########NEW FILE########
__FILENAME__ = Syncdb
import sys
import Bcfg2.settings
import Bcfg2.Options
import Bcfg2.Server.Admin
import Bcfg2.Server.models
from django.core.exceptions import ImproperlyConfigured
from django.core.management import setup_environ, call_command


class Syncdb(Bcfg2.Server.Admin.Mode):
    """ Sync the Django ORM with the configured database """
    options = {'configfile': Bcfg2.Options.WEB_CFILE}

    def __call__(self, args):
        # Parse options
        opts = Bcfg2.Options.OptionParser(self.options)
        opts.parse(args)

        setup_environ(Bcfg2.settings)
        Bcfg2.Server.models.load_models(cfile=opts['configfile'])

        try:
            call_command("syncdb", interactive=False, verbosity=0)
            self._database_available = True
        except ImproperlyConfigured:
            self.errExit("Django configuration problem: %s" %
                         sys.exc_info()[1])
        except:
            self.errExit("Database update failed: %s" % sys.exc_info()[1])

########NEW FILE########
__FILENAME__ = Viz
""" Produce graphviz diagrams of metadata structures """

import getopt
from subprocess import Popen, PIPE
import pipes
import Bcfg2.Server.Admin


class Viz(Bcfg2.Server.Admin.MetadataCore):
    """ Produce graphviz diagrams of metadata structures """
    __usage__ = ("[options]\n\n"
                 "     %-32s%s\n"
                 "     %-32s%s\n"
                 "     %-32s%s\n"
                 "     %-32s%s\n"
                 "     %-32s%s\n" %
                 ("-H, --includehosts",
                  "include hosts in the viz output",
                  "-b, --includebundles",
                  "include bundles in the viz output",
                  "-k, --includekey",
                  "show a key for different digraph shapes",
                  "-c, --only-client <clientname>",
                  "show only the groups, bundles for the named client",
                  "-o, --outfile <file>",
                  "write viz output to an output file"))

    colors = ['steelblue1', 'chartreuse', 'gold', 'magenta',
              'indianred1', 'limegreen', 'orange1', 'lightblue2',
              'green1', 'blue1', 'yellow1', 'darkturquoise', 'gray66']

    __plugin_blacklist__ = ['DBStats', 'Snapshots', 'Cfg', 'Pkgmgr',
                            'Packages', 'Rules', 'Account', 'Decisions',
                            'Deps', 'Git', 'Svn', 'Fossil', 'Bzr', 'Bundler',
                            'TGenshi', 'Base']

    def __call__(self, args):
        # First get options to the 'viz' subcommand
        try:
            opts, args = getopt.getopt(args, 'Hbkc:o:',
                                       ['includehosts', 'includebundles',
                                        'includekey', 'only-client=',
                                        'outfile='])
        except getopt.GetoptError:
            self.usage()

        hset = False
        bset = False
        kset = False
        only_client = None
        outputfile = False
        for opt, arg in opts:
            if opt in ("-H", "--includehosts"):
                hset = True
            elif opt in ("-b", "--includebundles"):
                bset = True
            elif opt in ("-k", "--includekey"):
                kset = True
            elif opt in ("-c", "--only-client"):
                only_client = arg
            elif opt in ("-o", "--outfile"):
                outputfile = arg

        data = self.Visualize(hset, bset, kset, only_client, outputfile)
        if data:
            print(data)

    def Visualize(self, hosts=False, bundles=False, key=False,
                  only_client=None, output=None):
        """Build visualization of groups file."""
        if output:
            fmt = output.split('.')[-1]
        else:
            fmt = 'png'

        cmd = ["dot", "-T", fmt]
        if output:
            cmd.extend(["-o", output])
        try:
            dotpipe = Popen(cmd, stdin=PIPE, stdout=PIPE, close_fds=True)
        except OSError:
            # on some systems (RHEL 6), you cannot run dot with
            # shell=True.  on others (Gentoo with Python 2.7), you
            # must.  In yet others (RHEL 5), either way works.  I have
            # no idea what the difference is, but it's kind of a PITA.
            cmd = ["dot", "-T", pipes.quote(fmt)]
            if output:
                cmd.extend(["-o", pipes.quote(output)])
            dotpipe = Popen(cmd, shell=True,
                            stdin=PIPE, stdout=PIPE, close_fds=True)
        try:
            dotpipe.stdin.write("digraph groups {\n")
        except:
            print("write to dot process failed. Is graphviz installed?")
            raise SystemExit(1)
        dotpipe.stdin.write('\trankdir="LR";\n')
        dotpipe.stdin.write(self.metadata.viz(hosts, bundles,
                                              key, only_client, self.colors))
        if key:
            dotpipe.stdin.write("\tsubgraph cluster_key {\n")
            dotpipe.stdin.write('\tstyle="filled";\n')
            dotpipe.stdin.write('\tcolor="lightblue";\n')
            dotpipe.stdin.write('\tBundle [ shape="septagon" ];\n')
            dotpipe.stdin.write('\tGroup [shape="ellipse"];\n')
            dotpipe.stdin.write('\tGroup Category [shape="trapezium"];\n')
            dotpipe.stdin.write('\tProfile [style="bold", shape="ellipse"];\n')
            dotpipe.stdin.write('\tHblock [label="Host1|Host2|Host3", '
                                'shape="record"];\n')
            dotpipe.stdin.write('\tlabel="Key";\n')
            dotpipe.stdin.write("\t}\n")
        dotpipe.stdin.write("}\n")
        dotpipe.stdin.close()
        return dotpipe.stdout.read()

########NEW FILE########
__FILENAME__ = Xcmd
""" XML-RPC Command Interface for bcfg2-admin"""

import sys
import Bcfg2.Options
import Bcfg2.Proxy
import Bcfg2.Server.Admin


class Xcmd(Bcfg2.Server.Admin.Mode):
    """ XML-RPC Command Interface """
    __usage__ = "<command>"

    def __call__(self, args):
        optinfo = {
            'server': Bcfg2.Options.SERVER_LOCATION,
            'user': Bcfg2.Options.CLIENT_USER,
            'password': Bcfg2.Options.SERVER_PASSWORD,
            'key': Bcfg2.Options.SERVER_KEY,
            'certificate': Bcfg2.Options.CLIENT_CERT,
            'ca': Bcfg2.Options.CLIENT_CA,
            'timeout': Bcfg2.Options.CLIENT_TIMEOUT}
        setup = Bcfg2.Options.OptionParser(optinfo)
        setup.parse(args)
        Bcfg2.Proxy.RetryMethod.max_retries = 1
        proxy = Bcfg2.Proxy.ComponentProxy(setup['server'],
                                           setup['user'],
                                           setup['password'],
                                           key=setup['key'],
                                           cert=setup['certificate'],
                                           ca=setup['ca'],
                                           timeout=setup['timeout'])
        if len(setup['args']) == 0:
            self.errExit("Usage: xcmd <xmlrpc method> <optional arguments>")
        cmd = setup['args'][0]
        args = ()
        if len(setup['args']) > 1:
            args = tuple(setup['args'][1:])
        try:
            data = getattr(proxy, cmd)(*args)
        except Bcfg2.Proxy.ProxyError:
            self.errExit("Proxy Error: %s" % sys.exc_info()[1])

        if data is not None:
            print(data)

########NEW FILE########
__FILENAME__ = BuiltinCore
""" The core of the builtin Bcfg2 server. """

import sys
import time
import socket
import daemon
import Bcfg2.Statistics
from Bcfg2.Server.Core import BaseCore, NoExposedMethod
from Bcfg2.Compat import xmlrpclib, urlparse
from Bcfg2.SSLServer import XMLRPCServer

from lockfile import LockFailed, LockTimeout
# pylint: disable=E0611
try:
    from daemon.pidfile import TimeoutPIDLockFile
except ImportError:
    from daemon.pidlockfile import TimeoutPIDLockFile
# pylint: enable=E0611


class Core(BaseCore):
    """ The built-in server core """
    name = 'bcfg2-server'

    def __init__(self, setup):
        BaseCore.__init__(self, setup)

        #: The :class:`Bcfg2.SSLServer.XMLRPCServer` instance powering
        #: this server core
        self.server = None

        daemon_args = dict(uid=self.setup['daemon_uid'],
                           gid=self.setup['daemon_gid'],
                           umask=int(self.setup['umask'], 8),
                           detach_process=True)
        if self.setup['daemon']:
            daemon_args['pidfile'] = TimeoutPIDLockFile(self.setup['daemon'],
                                                        acquire_timeout=5)
        #: The :class:`daemon.DaemonContext` used to drop
        #: privileges, write the PID file (with :class:`PidFile`),
        #: and daemonize this core.
        self.context = daemon.DaemonContext(**daemon_args)
    __init__.__doc__ = BaseCore.__init__.__doc__.split('.. -----')[0]

    def _dispatch(self, method, args, dispatch_dict):
        """ Dispatch XML-RPC method calls

        :param method: XML-RPC method name
        :type method: string
        :param args: Paramaters to pass to the method
        :type args: tuple
        :param dispatch_dict: A dict of method name -> function that
                              can be used to provide custom mappings
        :type dispatch_dict: dict
        :returns: The return value of the method call
        :raises: :exc:`xmlrpclib.Fault`
        """
        if method in dispatch_dict:
            method_func = dispatch_dict[method]
        else:
            try:
                method_func = self._resolve_exposed_method(method)
            except NoExposedMethod:
                self.logger.error("Unknown method %s" % (method))
                raise xmlrpclib.Fault(xmlrpclib.METHOD_NOT_FOUND,
                                      "Unknown method %s" % method)

        try:
            method_start = time.time()
            try:
                return method_func(*args)
            finally:
                Bcfg2.Statistics.stats.add_value(method,
                                                 time.time() - method_start)
        except xmlrpclib.Fault:
            raise
        except Exception:
            err = sys.exc_info()[1]
            if getattr(err, "log", True):
                self.logger.error(err, exc_info=True)
            raise xmlrpclib.Fault(getattr(err, "fault_code", 1), str(err))

    def _daemonize(self):
        """ Open :attr:`context` to drop privileges, write the PID
        file, and daemonize the server core. """
        try:
            self.context.open()
            self.logger.info("%s daemonized" % self.name)
            return True
        except LockFailed:
            err = sys.exc_info()[1]
            self.logger.error("Failed to daemonize %s: %s" % (self.name, err))
            return False
        except LockTimeout:
            err = sys.exc_info()[1]
            self.logger.error("Failed to daemonize %s: Failed to acquire lock "
                              "on %s" % (self.name, self.setup['daemon']))
            return False

    def _run(self):
        """ Create :attr:`server` to start the server listening. """
        hostname, port = urlparse(self.setup['location'])[1].split(':')
        server_address = socket.getaddrinfo(hostname,
                                            port,
                                            socket.AF_UNSPEC,
                                            socket.SOCK_STREAM)[0][4]
        try:
            self.server = XMLRPCServer(self.setup['listen_all'],
                                       server_address,
                                       keyfile=self.setup['key'],
                                       certfile=self.setup['cert'],
                                       register=False,
                                       ca=self.setup['ca'],
                                       protocol=self.setup['protocol'])
        except:  # pylint: disable=W0702
            err = sys.exc_info()[1]
            self.logger.error("Server startup failed: %s" % err)
            self.context.close()
            return False
        return True

    def _block(self):
        """ Enter the blocking infinite loop. """
        self.server.register_instance(self)
        try:
            self.server.serve_forever()
        finally:
            self.server.server_close()
            self.context.close()
        self.shutdown()

########NEW FILE########
__FILENAME__ = CherryPyCore
""" The core of the `CherryPy <http://www.cherrypy.org/>`_-powered
server. """

import sys
import time
import Bcfg2.Statistics
from Bcfg2.Compat import urlparse, xmlrpclib, b64decode
from Bcfg2.Server.Core import BaseCore
import cherrypy
from cherrypy.lib import xmlrpcutil
from cherrypy._cptools import ErrorTool
from cherrypy.process.plugins import Daemonizer, DropPrivileges, PIDFile


def on_error(*args, **kwargs):  # pylint: disable=W0613
    """ CherryPy error handler that handles :class:`xmlrpclib.Fault`
    objects and so allows for the possibility of returning proper
    error codes. This obviates the need to use
    :func:`cherrypy.lib.xmlrpc.on_error`, the builtin CherryPy xmlrpc
    tool, which does not handle xmlrpclib.Fault objects and returns
    the same error code for every error."""
    err = sys.exc_info()[1]
    if not isinstance(err, xmlrpclib.Fault):
        err = xmlrpclib.Fault(xmlrpclib.INTERNAL_ERROR, str(err))
    xmlrpcutil._set_response(xmlrpclib.dumps(err))  # pylint: disable=W0212

cherrypy.tools.xmlrpc_error = ErrorTool(on_error)


class Core(BaseCore):
    """ The CherryPy-based server core. """

    #: Base CherryPy config for this class.  We enable the
    #: ``xmlrpc_error`` tool created from :func:`on_error` and the
    #: ``bcfg2_authn`` tool created from :func:`do_authn`.
    _cp_config = {'tools.xmlrpc_error.on': True,
                  'tools.bcfg2_authn.on': True}

    def __init__(self, setup):
        BaseCore.__init__(self, setup)

        cherrypy.tools.bcfg2_authn = cherrypy.Tool('on_start_resource',
                                                   self.do_authn)

        #: List of exposed plugin RMI
        self.rmi = self._get_rmi()
        cherrypy.engine.subscribe('stop', self.shutdown)
    __init__.__doc__ = BaseCore.__init__.__doc__.split('.. -----')[0]

    def do_authn(self):
        """ Perform authentication by calling
        :func:`Bcfg2.Server.Core.BaseCore.authenticate`. This is
        implemented as a CherryPy tool."""
        try:
            header = cherrypy.request.headers['Authorization']
        except KeyError:
            self.critical_error("No authentication data presented")
        auth_content = header.split()[1]
        auth_content = b64decode(auth_content)
        try:
            username, password = auth_content.split(":")
        except ValueError:
            username = auth_content
            password = ""

        # FIXME: Get client cert
        cert = None
        address = (cherrypy.request.remote.ip, cherrypy.request.remote.name)
        return self.authenticate(cert, username, password, address)

    @cherrypy.expose
    def default(self, *args, **params):  # pylint: disable=W0613
        """ Handle all XML-RPC calls.  It was necessary to make enough
        changes to the stock CherryPy
        :class:`cherrypy._cptools.XMLRPCController` to support plugin
        RMI and prepending the client address that we just rewrote it.
        It clearly wasn't written with inheritance in mind."""
        rpcparams, rpcmethod = xmlrpcutil.process_body()
        if rpcmethod == 'ERRORMETHOD':
            raise Exception("Unknown error processing XML-RPC request body")
        elif "." not in rpcmethod:
            address = (cherrypy.request.remote.ip,
                       cherrypy.request.remote.name)
            rpcparams = (address, ) + rpcparams

            handler = getattr(self, rpcmethod, None)
            if not handler or not getattr(handler, "exposed", False):
                raise Exception('Method "%s" is not supported' % rpcmethod)
        else:
            try:
                handler = self.rmi[rpcmethod]
            except KeyError:
                raise Exception('Method "%s" is not supported' % rpcmethod)

        method_start = time.time()
        try:
            body = handler(*rpcparams, **params)
        finally:
            Bcfg2.Statistics.stats.add_value(rpcmethod,
                                             time.time() - method_start)

        xmlrpcutil.respond(body, 'utf-8', True)
        return cherrypy.serving.response.body

    def _daemonize(self):
        """ Drop privileges with
        :class:`cherrypy.process.plugins.DropPrivileges`, daemonize
        with :class:`cherrypy.process.plugins.Daemonizer`, and write a
        PID file with :class:`cherrypy.process.plugins.PIDFile`. """
        DropPrivileges(cherrypy.engine,
                       uid=self.setup['daemon_uid'],
                       gid=self.setup['daemon_gid'],
                       umask=int(self.setup['umask'], 8)).subscribe()
        Daemonizer(cherrypy.engine).subscribe()
        PIDFile(cherrypy.engine, self.setup['daemon']).subscribe()
        return True

    def _run(self):
        """ Start the server listening. """
        hostname, port = urlparse(self.setup['location'])[1].split(':')
        if self.setup['listen_all']:
            hostname = '0.0.0.0'

        config = {'engine.autoreload.on': False,
                  'server.socket_port': int(port),
                  'server.socket_host': hostname}
        if self.setup['cert'] and self.setup['key']:
            config.update({'server.ssl_module': 'pyopenssl',
                           'server.ssl_certificate': self.setup['cert'],
                           'server.ssl_private_key': self.setup['key']})
        if self.setup['debug']:
            config['log.screen'] = True
        cherrypy.config.update(config)
        cherrypy.tree.mount(self, '/', {'/': self.setup})
        cherrypy.engine.start()
        return True

    def _block(self):
        """ Enter the blocking infinite server
        loop. :func:`Bcfg2.Server.Core.BaseCore.shutdown` is called on
        exit by a :meth:`subscription
        <cherrypy.process.wspbus.Bus.subscribe>` on the top-level
        CherryPy engine."""
        cherrypy.engine.block()

########NEW FILE########
__FILENAME__ = Core
""" Bcfg2.Server.Core provides the base core object that server core
implementations inherit from. """

import os
import pwd
import atexit
import logging
import select
import sys
import threading
import time
import inspect
import lxml.etree
import Bcfg2.settings
import Bcfg2.Server
import Bcfg2.Logger
import Bcfg2.Server.FileMonitor
from Bcfg2.Cache import Cache
import Bcfg2.Statistics
from itertools import chain
from Bcfg2.Compat import xmlrpclib, wraps  # pylint: disable=W0622
from Bcfg2.Server.Plugin.exceptions import *  # pylint: disable=W0401,W0614
from Bcfg2.Server.Plugin.interfaces import *  # pylint: disable=W0401,W0614
from Bcfg2.Server.Plugin import track_statistics

try:
    import psyco
    psyco.full()
except ImportError:
    pass

os.environ['DJANGO_SETTINGS_MODULE'] = 'Bcfg2.settings'


def exposed(func):
    """ Decorator that sets the ``exposed`` attribute of a function to
    ``True`` expose it via XML-RPC.  This currently works for both the
    builtin and CherryPy cores, although if other cores are added this
    may need to be made a core-specific function.

    :param func: The function to decorate
    :type func: callable
    :returns: callable - the decorated function"""
    func.exposed = True
    return func


def sort_xml(node, key=None):
    """ Recursively sort an XML document in a deterministic fashion.
    This shouldn't be used to perform a *useful* sort, merely to put
    XML in a deterministic, replicable order.  The document is sorted
    in-place.

    :param node: The root node of the XML tree to sort
    :type node: lxml.etree._Element or lxml.etree.ElementTree
    :param key: The key to sort by
    :type key: callable
    :returns: None
    """
    for child in node:
        sort_xml(child, key)

    try:
        sorted_children = sorted(node, key=key)
    except TypeError:
        sorted_children = node
    node[:] = sorted_children


def close_db_connection(func):
    """ Decorator that closes the Django database connection at the end of
    the function.  This should decorate any exposed function that
    might open a database connection. """
    @wraps(func)
    def inner(self, *args, **kwargs):
        """ The decorated function """
        rv = func(self, *args, **kwargs)
        if self._database_available:  # pylint: disable=W0212
            from django import db
            self.logger.debug("%s: Closing database connection" %
                              threading.current_thread().name)
            db.close_connection()
        return rv

    return inner


class CoreInitError(Exception):
    """ Raised when the server core cannot be initialized. """
    pass


class NoExposedMethod (Exception):
    """ Raised when an XML-RPC method is called, but there is no
    method exposed with the given name. """


# pylint: disable=W0702
# in core we frequently want to catch all exceptions, regardless of
# type, so disable the pylint rule that catches that.


class BaseCore(object):
    """ The server core is the container for all Bcfg2 server logic
    and modules. All core implementations must inherit from
    ``BaseCore``. """
    name = "core"

    def __init__(self, setup):  # pylint: disable=R0912,R0915
        """
        :param setup: A Bcfg2 options dict
        :type setup: Bcfg2.Options.OptionParser

        .. automethod:: _daemonize
        .. automethod:: _run
        .. automethod:: _block
        .. -----
        .. automethod:: _file_monitor_thread
        .. automethod:: _perflog_thread
        """
        #: The Bcfg2 repository directory
        self.datastore = setup['repo']

        if setup['verbose']:
            level = logging.INFO
        else:
            level = logging.WARNING
        # we set a higher log level for the console by default.  we
        # assume that if someone is running bcfg2-server in such a way
        # that it _can_ log to console, they want more output.  if
        # level is set to DEBUG, that will get handled by
        # setup_logging and the console will get DEBUG output.
        Bcfg2.Logger.setup_logging('bcfg2-server',
                                   to_console=logging.INFO,
                                   to_syslog=setup['syslog'],
                                   to_file=setup['logging'],
                                   level=level)

        #: A :class:`logging.Logger` object for use by the core
        self.logger = logging.getLogger('bcfg2-server')

        #: Log levels for the various logging handlers with debug True
        #: and False.  Each loglevel dict is a dict of ``logger name
        #: => log level``; the logger names are set in
        #: :mod:`Bcfg2.Logger`.  The logger name ``default`` is
        #: special, and will be used for any log handlers whose name
        #: does not appear elsewhere in the dict.  At a minimum,
        #: ``default`` must be provided.
        self._loglevels = {True: dict(default=logging.DEBUG),
                           False: dict(console=logging.INFO,
                                       default=level)}

        #: Used to keep track of the current debug state of the core.
        self.debug_flag = False

        # enable debugging on the core now.  debugging is enabled on
        # everything else later
        if setup['debug']:
            self.set_core_debug(None, setup['debug'])

        try:
            filemonitor = \
                Bcfg2.Server.FileMonitor.available[setup['filemonitor']]
        except KeyError:
            self.logger.error("File monitor driver %s not available; "
                              "forcing to default" % setup['filemonitor'])
            filemonitor = Bcfg2.Server.FileMonitor.available['default']
        famargs = dict(ignore=[], debug=False)
        if 'ignore' in setup:
            famargs['ignore'] = setup['ignore']
        if 'debug' in setup:
            famargs['debug'] = setup['debug']

        try:
            #: The :class:`Bcfg2.Server.FileMonitor.FileMonitor`
            #: object used by the core to monitor for Bcfg2 data
            #: changes.
            self.fam = filemonitor(**famargs)
        except IOError:
            msg = "Failed to instantiate fam driver %s" % setup['filemonitor']
            self.logger.error(msg, exc_info=1)
            raise CoreInitError(msg)

        #: Path to bcfg2.conf
        self.cfile = setup['configfile']

        #: Dict of plugins that are enabled.  Keys are the plugin
        #: names (just the plugin name, in the correct case; e.g.,
        #: "Cfg", not "Bcfg2.Server.Plugins.Cfg"), and values are
        #: plugin objects.
        self.plugins = {}

        #: Blacklist of plugins that conflict with enabled plugins.
        #: If two plugins are loaded that conflict with each other,
        #: the first one loaded wins.
        self.plugin_blacklist = {}

        #: The Metadata plugin
        self.metadata = None

        #: Revision of the Bcfg2 specification.  This will be sent to
        #: the client in the configuration, and can be set by a
        #: :class:`Bcfg2.Server.Plugin.interfaces.Version` plugin.
        self.revision = '-1'

        #: The Bcfg2 options dict
        self.setup = setup

        atexit.register(self.shutdown)
        #: if :func:`Bcfg2.Server.Core.shutdown` is called explicitly,
        #: then :mod:`atexit` calls it *again*, so it gets called
        #: twice.  This is potentially bad, so we use
        #: :attr:`Bcfg2.Server.Core._running` as a flag to determine
        #: if the core needs to be shutdown, and only do it once.
        self._running = True

        #: Threading event to signal worker threads (e.g.,
        #: :attr:`fam_thread`) to shutdown
        self.terminate = threading.Event()

        #: RLock to be held on writes to the backend db
        self.db_write_lock = threading.RLock()

        # generate Django ORM settings.  this must be done _before_ we
        # load plugins
        Bcfg2.settings.read_config(repo=self.datastore)

        # mapping of group name => plugin name to record where groups
        # that are created by Connector plugins came from
        self._dynamic_groups = dict()

        #: Whether or not it's possible to use the Django database
        #: backend for plugins that have that capability
        self._database_available = False
        if Bcfg2.settings.HAS_DJANGO:
            db_settings = Bcfg2.settings.DATABASES['default']
            if ('daemon' in self.setup and 'daemon_uid' in self.setup and
                self.setup['daemon'] and self.setup['daemon_uid'] and
                db_settings['ENGINE'].endswith(".sqlite3") and
                not os.path.exists(db_settings['NAME'])):
                # syncdb will create the sqlite database, and we're
                # going to daemonize, dropping privs to a non-root
                # user, so we need to chown the database after
                # creating it
                do_chown = True
            else:
                do_chown = False

            from django.core.exceptions import ImproperlyConfigured
            from django.core import management
            try:
                management.call_command("syncdb", interactive=False,
                                        verbosity=0)
                self._database_available = True
            except ImproperlyConfigured:
                self.logger.error("Django configuration problem: %s" %
                                  sys.exc_info()[1])
            except:
                self.logger.error("Database update failed: %s" %
                                  sys.exc_info()[1])

            if do_chown and self._database_available:
                try:
                    os.chown(db_settings['NAME'],
                             self.setup['daemon_uid'],
                             self.setup['daemon_gid'])
                except OSError:
                    err = sys.exc_info()[1]
                    self.logger.error("Failed to set ownership of database "
                                      "at %s: %s" % (db_settings['NAME'], err))

        #: The CA that signed the server cert
        self.ca = setup['ca']

        #: The FAM :class:`threading.Thread`,
        #: :func:`_file_monitor_thread`
        self.fam_thread = \
            threading.Thread(name="%sFAMThread" % setup['filemonitor'],
                             target=self._file_monitor_thread)

        self.perflog_thread = None
        if self.setup['perflog']:
            self.perflog_thread = \
                threading.Thread(name="PerformanceLoggingThread",
                                 target=self._perflog_thread)

        #: A :func:`threading.Lock` for use by
        #: :func:`Bcfg2.Server.FileMonitor.FileMonitor.handle_event_set`
        self.lock = threading.Lock()

        #: A :class:`Bcfg2.Cache.Cache` object for caching client
        #: metadata
        self.metadata_cache = Cache()

    def expire_caches_by_type(self, base_cls, key=None):
        """ Expire caches for all
        :class:`Bcfg2.Server.Plugin.interfaces.Caching` plugins that
        are instances of ``base_cls``.

        :param base_cls: The base plugin interface class to match (see
                         :mod:`Bcfg2.Server.Plugin.interfaces`)
        :type base_cls: type
        :param key: The cache key to expire
        """
        for plugin in self.plugins_by_type(base_cls):
            if isinstance(plugin, Bcfg2.Server.Plugin.Caching):
                plugin.expire_cache(key)

    def plugins_by_type(self, base_cls):
        """ Return a list of loaded plugins that match the passed type.

        The returned list is sorted in ascending order by the plugins'
        ``sort_order`` value. The
        :attr:`Bcfg2.Server.Plugin.base.Plugin.sort_order` defaults to
        500, but can be overridden by individual plugins. Plugins with
        the same numerical sort_order value are sorted in alphabetical
        order by their name.

        :param base_cls: The base plugin interface class to match (see
                         :mod:`Bcfg2.Server.Plugin.interfaces`)
        :type base_cls: type
        :returns: list of :attr:`Bcfg2.Server.Plugin.base.Plugin`
                  objects
        """
        return sorted([plugin for plugin in self.plugins.values()
                       if isinstance(plugin, base_cls)],
                      key=lambda p: (p.sort_order, p.name))

    def _perflog_thread(self):
        """ The thread that periodically logs performance statistics
        to syslog. """
        self.logger.debug("Performance logging thread starting")
        while not self.terminate.isSet():
            self.terminate.wait(self.setup['perflog_interval'])
            if not self.terminate.isSet():
                for name, stats in self.get_statistics(None).items():
                    self.logger.info("Performance statistics: "
                                     "%s min=%.06f, max=%.06f, average=%.06f, "
                                     "count=%d" % ((name, ) + stats))
        self.logger.info("Performance logging thread terminated")

    def _file_monitor_thread(self):
        """ The thread that runs the
        :class:`Bcfg2.Server.FileMonitor.FileMonitor`. This also
        queries :class:`Bcfg2.Server.Plugin.interfaces.Version`
        plugins for the current revision of the Bcfg2 repo. """
        self.logger.debug("File monitor thread starting")
        famfd = self.fam.fileno()
        terminate = self.terminate
        while not terminate.isSet():
            try:
                if famfd:
                    select.select([famfd], [], [], 2)
                else:
                    if not self.fam.pending():
                        terminate.wait(15)
                if self.fam.pending():
                    self._update_vcs_revision()
                self.fam.handle_event_set(self.lock)
            except:
                continue
        self.logger.info("File monitor thread terminated")

    @track_statistics()
    def _update_vcs_revision(self):
        """ Update the revision of the current configuration on-disk
        from the VCS plugin """
        for plugin in self.plugins_by_type(Version):
            try:
                newrev = plugin.get_revision()
                if newrev != self.revision:
                    self.logger.debug("Updated to revision %s" % newrev)
                self.revision = newrev
                break
            except:
                self.logger.warning("Error getting revision from %s: %s" %
                                    (plugin.name, sys.exc_info()[1]))
                self.revision = '-1'

    def load_plugins(self):
        """ Load all plugins, setting
        :attr:`Bcfg2.Server.Core.BaseCore.plugins` and
        :attr:`Bcfg2.Server.Core.BaseCore.metadata` as side effects.
        This does not start plugin threads; that is done later, in
        :func:`Bcfg2.Server.Core.BaseCore.run` """
        while '' in self.setup['plugins']:
            self.setup['plugins'].remove('')

        for plugin in self.setup['plugins']:
            if plugin not in self.plugins:
                self.init_plugin(plugin)

        # Remove blacklisted plugins
        for plugin, blacklist in list(self.plugin_blacklist.items()):
            if len(blacklist) > 0:
                self.logger.error("The following plugins conflict with %s;"
                                  "Unloading %s" % (plugin, blacklist))
            for plug in blacklist:
                del self.plugins[plug]

        # Log experimental plugins
        expl = [plug for plug in list(self.plugins.values())
                if plug.experimental]
        if expl:
            self.logger.info("Loading experimental plugin(s): %s" %
                             (" ".join([x.name for x in expl])))
            self.logger.info("NOTE: Interfaces subject to change")

        # Log deprecated plugins
        depr = [plug for plug in list(self.plugins.values())
                if plug.deprecated]
        if depr:
            self.logger.info("Loading deprecated plugin(s): %s" %
                             (" ".join([x.name for x in depr])))

        # Find the metadata plugin and set self.metadata
        mlist = self.plugins_by_type(Metadata)
        if len(mlist) >= 1:
            self.metadata = mlist[0]
            if len(mlist) > 1:
                self.logger.error("Multiple Metadata plugins loaded; using %s"
                                  % self.metadata)
        else:
            self.logger.error("No Metadata plugin loaded; "
                              "failed to instantiate Core")
            raise CoreInitError("No Metadata Plugin")

        if self.debug_flag:
            # enable debugging on plugins
            self.plugins[plugin].set_debug(self.debug_flag)

    def init_plugin(self, plugin):
        """ Import and instantiate a single plugin.  The plugin is
        stored to :attr:`plugins`.

        :param plugin: The name of the plugin.  This is just the name
                       of the plugin, in the appropriate case.  I.e.,
                       ``Cfg``, not ``Bcfg2.Server.Plugins.Cfg``.
        :type plugin: string
        :returns: None
        """
        self.logger.debug("%s: Loading plugin %s" % (self.name, plugin))
        try:
            mod = getattr(__import__("Bcfg2.Server.Plugins.%s" %
                                     (plugin)).Server.Plugins, plugin)
        except ImportError:
            try:
                mod = __import__(plugin, globals(), locals(),
                                 [plugin.split('.')[-1]])
            except:
                self.logger.error("Failed to load plugin %s" % plugin)
                return
        try:
            plug = getattr(mod, plugin.split('.')[-1])
        except AttributeError:
            self.logger.error("Failed to load plugin %s: %s" %
                              (plugin, sys.exc_info()[1]))
            return
        # Blacklist conflicting plugins
        cplugs = [conflict for conflict in plug.conflicts
                  if conflict in self.plugins]
        self.plugin_blacklist[plug.name] = cplugs
        try:
            self.plugins[plugin] = plug(self, self.datastore)
        except PluginInitError:
            self.logger.error("Failed to instantiate plugin %s" % plugin,
                              exc_info=1)
        except OSError:
            err = sys.exc_info()[1]
            self.logger.error("Failed to add a file monitor while "
                              "instantiating plugin %s: %s" % (plugin, err))
        except:
            self.logger.error("Unexpected instantiation failure for plugin %s"
                              % plugin, exc_info=1)

    def shutdown(self):
        """ Perform plugin and FAM shutdown tasks. """
        if not self._running:
            self.logger.debug("%s: Core already shut down" % self.name)
            return
        self.logger.info("%s: Shutting down core..." % self.name)
        if not self.terminate.isSet():
            self.terminate.set()
        self._running = False
        self.fam.shutdown()
        self.logger.info("%s: FAM shut down" % self.name)
        for plugin in list(self.plugins.values()):
            plugin.shutdown()
        self.logger.info("%s: All plugins shut down" % self.name)
        if self._database_available:
            from django import db
            self.logger.info("%s: Closing database connection" % self.name)
            db.close_connection()

    @property
    def metadata_cache_mode(self):
        """ Get the client :attr:`metadata_cache` mode.  Options are
        off, initial, cautious, aggressive, on (synonym for
        cautious). See :ref:`server-caching` for more details. """
        mode = self.setup.cfp.get("caching", "client_metadata",
                                  default="off").lower()
        if mode == "on":
            return "cautious"
        else:
            return mode

    def client_run_hook(self, hook, metadata):
        """ Invoke hooks from
        :class:`Bcfg2.Server.Plugin.interfaces.ClientRunHooks` plugins
        for a given stage.

        :param hook: The name of the stage to run hooks for.  A stage
                     can be any abstract function defined in the
                     :class:`Bcfg2.Server.Plugin.interfaces.ClientRunHooks`
                     interface.
        :type hook: string
        :param metadata: Client metadata to run the hook for.  This
                         will be passed as the sole argument to each
                         hook.
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        """
        self.logger.debug("Running %s hooks for %s" % (hook,
                                                       metadata.hostname))
        start = time.time()
        try:
            for plugin in self.plugins_by_type(ClientRunHooks):
                try:
                    getattr(plugin, hook)(metadata)
                except AttributeError:
                    err = sys.exc_info()[1]
                    self.logger.error("Unknown attribute: %s" % err)
                    raise
                except:
                    err = sys.exc_info()[1]
                    self.logger.error("%s: Error invoking hook %s: %s" %
                                      (plugin, hook, err))
        finally:
            Bcfg2.Statistics.stats.add_value("%s:client_run_hook:%s" %
                                             (self.__class__.__name__, hook),
                                             time.time() - start)

    @track_statistics()
    def validate_structures(self, metadata, data):
        """ Checks the data structures by calling the
        :func:`Bcfg2.Server.Plugin.interfaces.StructureValidator.validate_structures`
        method of
        :class:`Bcfg2.Server.Plugin.interfaces.StructureValidator`
        plugins.

        :param metadata: Client metadata to validate structures for
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :param data: The list of structures (i.e., bundles) for this
                     client
        :type data: list of lxml.etree._Element objects
        """
        self.logger.debug("Validating structures for %s" % metadata.hostname)
        for plugin in self.plugins_by_type(StructureValidator):
            try:
                plugin.validate_structures(metadata, data)
            except ValidationError:
                err = sys.exc_info()[1]
                self.logger.error("Plugin %s structure validation failed: %s" %
                                  (plugin.name, err))
                raise
            except:
                self.logger.error("Plugin %s: unexpected structure validation "
                                  "failure" % plugin.name, exc_info=1)

    @track_statistics()
    def validate_goals(self, metadata, data):
        """ Checks that the config matches the goals enforced by
        :class:`Bcfg2.Server.Plugin.interfaces.GoalValidator` plugins
        by calling
        :func:`Bcfg2.Server.Plugin.interfaces.GoalValidator.validate_goals`.

        :param metadata: Client metadata to validate goals for
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :param data: The list of structures (i.e., bundles) for this
                     client
        :type data: list of lxml.etree._Element objects
        """
        self.logger.debug("Validating goals for %s" % metadata.hostname)
        for plugin in self.plugins_by_type(GoalValidator):
            try:
                plugin.validate_goals(metadata, data)
            except ValidationError:
                err = sys.exc_info()[1]
                self.logger.error("Plugin %s goal validation failed: %s" %
                                  (plugin.name, err.message))
                raise
            except:
                self.logger.error("Plugin %s: unexpected goal validation "
                                  "failure" % plugin.name, exc_info=1)

    @track_statistics()
    def GetStructures(self, metadata):
        """ Get all structures (i.e., bundles) for the given client

        :param metadata: Client metadata to get structures for
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :returns: list of :class:`lxml.etree._Element` objects
        """
        self.logger.debug("Getting structures for %s" % metadata.hostname)
        structures = list(
            chain(*[struct.BuildStructures(metadata)
                    for struct in self.plugins_by_type(Structure)]))
        sbundles = [b.get('name') for b in structures if b.tag == 'Bundle']
        missing = [b for b in metadata.bundles if b not in sbundles]
        if missing:
            self.logger.error("Client %s configuration missing bundles: %s" %
                              (metadata.hostname, ':'.join(missing)))
        return structures

    @track_statistics()
    def BindStructures(self, structures, metadata, config):
        """ Given a list of structures (i.e. bundles), bind all the
        entries in them and add the structures to the config.

        :param structures: The list of structures for this client
        :type structures: list of lxml.etree._Element objects
        :param metadata: Client metadata to bind structures for
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :param config: The configuration document to add fully-bound
                       structures to. Modified in-place.
        :type config: lxml.etree._Element
        """
        self.logger.debug("Binding structures for %s" % metadata.hostname)
        for astruct in structures:
            try:
                self.BindStructure(astruct, metadata)
                config.append(astruct)
            except:
                self.logger.error("error in BindStructure", exc_info=1)

    @track_statistics()
    def BindStructure(self, structure, metadata):
        """ Bind all elements in a single structure (i.e., bundle).

        :param structure: The structure to bind.  Modified in-place.
        :type structures: lxml.etree._Element
        :param metadata: Client metadata to bind structure for
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        """
        self.logger.debug("Binding structure %s for %s" %
                          (structure.get("name", "unknown"),
                           metadata.hostname))
        for entry in structure.getchildren():
            if entry.tag.startswith("Bound"):
                entry.tag = entry.tag[5:]
                continue
            try:
                self.Bind(entry, metadata)
            except:
                exc = sys.exc_info()[1]
                if 'failure' not in entry.attrib:
                    entry.set('failure', 'bind error: %s' % exc)
                if isinstance(exc, PluginExecutionError):
                    msg = "Failed to bind entry"
                else:
                    msg = "Unexpected failure binding entry"
                self.logger.error("%s %s:%s: %s" %
                                  (msg, entry.tag, entry.get('name'), exc))

    def Bind(self, entry, metadata):
        """ Bind a single entry using the appropriate generator.

        :param entry: The entry to bind.  Modified in-place.
        :type entry: lxml.etree._Element
        :param metadata: Client metadata to bind structure for
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        """
        start = time.time()
        if 'altsrc' in entry.attrib:
            oldname = entry.get('name')
            entry.set('name', entry.get('altsrc'))
            entry.set('realname', oldname)
            del entry.attrib['altsrc']
            try:
                ret = self.Bind(entry, metadata)
                entry.set('name', oldname)
                del entry.attrib['realname']
                return ret
            except:
                self.logger.error(
                    "Failed binding entry %s:%s with altsrc %s: %s" %
                    (entry.tag, entry.get('realname'), entry.get('name'),
                     sys.exc_info()[1]))
                entry.set('name', oldname)
                self.logger.error("Falling back to %s:%s" %
                                  (entry.tag, entry.get('name')))

        generators = self.plugins_by_type(Generator)
        glist = [gen for gen in generators
                 if entry.get('name') in gen.Entries.get(entry.tag, {})]
        if len(glist) == 1:
            return glist[0].Entries[entry.tag][entry.get('name')](entry,
                                                                  metadata)
        elif len(glist) > 1:
            generators = ", ".join([gen.name for gen in glist])
            self.logger.error("%s %s served by multiple generators: %s" %
                              (entry.tag, entry.get('name'), generators))
        g2list = [gen for gen in generators
                  if gen.HandlesEntry(entry, metadata)]
        try:
            if len(g2list) == 1:
                return g2list[0].HandleEntry(entry, metadata)
            entry.set('failure', 'no matching generator')
            raise PluginExecutionError("No matching generator: %s:%s" %
                                       (entry.tag, entry.get('name')))
        finally:
            Bcfg2.Statistics.stats.add_value("%s:Bind:%s" %
                                             (self.__class__.__name__,
                                              entry.tag),
                                             time.time() - start)

    def BuildConfiguration(self, client):
        """ Build the complete configuration for a client.

        :param client: The hostname of the client to build the
                       configuration for
        :type client: string
        :returns: :class:`lxml.etree._Element` - A complete Bcfg2
                  configuration document """
        self.logger.debug("Building configuration for %s" % client)
        start = time.time()
        config = lxml.etree.Element("Configuration", version='2.0',
                                    revision=self.revision)
        try:
            meta = self.build_metadata(client)
        except MetadataConsistencyError:
            self.logger.error("Metadata consistency error for client %s" %
                              client)
            return lxml.etree.Element("error", type='metadata error')

        self.client_run_hook("start_client_run", meta)

        try:
            structures = self.GetStructures(meta)
        except:
            self.logger.error("Error in GetStructures", exc_info=1)
            return lxml.etree.Element("error", type='structure error')

        self.validate_structures(meta, structures)

        # Perform altsrc consistency checking
        esrcs = {}
        for struct in structures:
            for entry in struct:
                key = (entry.tag, entry.get('name'))
                if key in esrcs:
                    if esrcs[key] != entry.get('altsrc'):
                        self.logger.error("Found inconsistent altsrc mapping "
                                          "for entry %s:%s" % key)
                else:
                    esrcs[key] = entry.get('altsrc', None)
        del esrcs

        self.BindStructures(structures, meta, config)

        self.validate_goals(meta, config)

        self.client_run_hook("end_client_run", meta)

        sort_xml(config, key=lambda e: e.get('name'))

        self.logger.info("Generated config for %s in %.03f seconds" %
                         (client, time.time() - start))
        return config

    def HandleEvent(self, event):
        """ Handle a change in the Bcfg2 config file.

        :param event: The event to handle
        :type event: Bcfg2.Server.FileMonitor.Event
        """
        if event.filename != self.cfile:
            self.logger.error("Got event for unknown file: %s" %
                              event.filename)
            return
        if event.code2str() == 'deleted':
            return
        self.setup.reparse()
        self.expire_caches_by_type(Bcfg2.Server.Plugin.Metadata)

    def block_for_fam_events(self, handle_events=False):
        """ Block until all fam events have been handleed, optionally
        handling events as well.  (Setting ``handle_events=True`` is
        useful for local server cores that don't spawn an event
        handling thread.)"""
        slept = 0
        log_interval = 3
        if handle_events:
            self.fam.handle_events_in_interval(1)
            slept += 1
        if self.setup['fam_blocking']:
            time.sleep(1)
            slept += 1
            while self.fam.pending() != 0:
                time.sleep(1)
                slept += 1
                if slept % log_interval == 0:
                    self.logger.debug("Sleeping to handle FAM events...")
        self.logger.debug("Slept %s seconds while handling FAM events" % slept)

    def run(self):
        """ Run the server core. This calls :func:`_daemonize`,
        :func:`_run`, starts the :attr:`fam_thread`, and calls
        :func:`_block`, but note that it is the responsibility of the
        server core implementation to call :func:`shutdown` under
        normal operation. This also handles creation of the directory
        containing the pidfile, if necessary. """
        if self.setup['daemon']:
            # if we're dropping privs, then the pidfile is likely
            # /var/run/bcfg2-server/bcfg2-server.pid or similar.
            # since some OSes clean directories out of /var/run on
            # reboot, we need to ensure that the directory containing
            # the pidfile exists and has the appropriate permissions
            piddir = os.path.dirname(self.setup['daemon'])
            if not os.path.exists(piddir):
                os.makedirs(piddir)
                os.chown(piddir,
                         self.setup['daemon_uid'],
                         self.setup['daemon_gid'])
                os.chmod(piddir, 493)  # 0775
            if not self._daemonize():
                return False

            # rewrite $HOME. pulp stores its auth creds in ~/.pulp, so
            # this is necessary to make that work when privileges are
            # dropped
            os.environ['HOME'] = pwd.getpwuid(self.setup['daemon_uid'])[5]
        else:
            os.umask(int(self.setup['umask'], 8))

        if not self._run():
            self.shutdown()
            return False

        try:
            self.load_plugins()

            self.fam.start()
            self.fam_thread.start()
            self.fam.AddMonitor(self.cfile, self)
            if self.perflog_thread is not None:
                self.perflog_thread.start()

            for plug in self.plugins_by_type(Threaded):
                plug.start_threads()
        except:
            self.shutdown()
            raise

        if self.debug_flag:
            self.set_debug(None, self.debug_flag)
        self.block_for_fam_events()
        self._block()

    def _daemonize(self):
        """ Daemonize the server and write the pidfile.  This must be
        overridden by a core implementation. """
        raise NotImplementedError

    def _run(self):
        """ Start up the server; this method should return
        immediately.  This must be overridden by a core
        implementation. """
        raise NotImplementedError

    def _block(self):
        """ Enter the infinite loop.  This method should not return
        until the server is killed.  This must be overridden by a core
        implementation. """
        raise NotImplementedError

    def GetDecisions(self, metadata, mode):
        """ Get the decision list for a client.

        :param metadata: Client metadata to get the decision list for
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :param mode: The decision mode ("whitelist" or "blacklist")
        :type mode: string
        :returns: list of Decision tuples ``(<entry tag>, <entry name>)``
        """
        self.logger.debug("Getting decision list for %s" % metadata.hostname)
        result = []
        for plugin in self.plugins_by_type(Decision):
            try:
                result.extend(plugin.GetDecisions(metadata, mode))
            except:
                self.logger.error("Plugin: %s failed to generate decision list"
                                  % plugin.name, exc_info=1)
        return result

    @track_statistics()
    def build_metadata(self, client_name):
        """ Build initial client metadata for a client

        :param client_name: The name of the client to build metadata
                            for
        :type client_name: string
        :returns: :class:`Bcfg2.Server.Plugins.Metadata.ClientMetadata`
        """
        if not hasattr(self, 'metadata'):
            # some threads start before metadata is even loaded
            raise MetadataRuntimeError("Metadata not loaded yet")
        if self.metadata_cache_mode == 'initial':
            # the Metadata plugin handles loading the cached data if
            # we're only caching the initial metadata object
            imd = None
        else:
            imd = self.metadata_cache.get(client_name, None)
        if not imd:
            self.logger.debug("Building metadata for %s" % client_name)
            try:
                imd = self.metadata.get_initial_metadata(client_name)
            except MetadataConsistencyError:
                self.critical_error(
                    "Client metadata resolution error for %s: %s" %
                    (client_name, sys.exc_info()[1]))
            connectors = self.plugins_by_type(Connector)
            for conn in connectors:
                groups = conn.get_additional_groups(imd)
                groupnames = []
                for group in groups:
                    if hasattr(group, "name"):
                        groupname = group.name
                        if groupname in self._dynamic_groups:
                            if self._dynamic_groups[groupname] == conn.name:
                                self.metadata.groups[groupname] = group
                            else:
                                self.logger.warning(
                                    "Refusing to clobber dynamic group %s "
                                    "defined by %s" %
                                    (self._dynamic_groups[groupname],
                                     groupname))
                        elif groupname in self.metadata.groups:
                            # not recorded as a dynamic group, but
                            # present in metadata.groups -- i.e., a
                            # static group
                            self.logger.warning(
                                "Refusing to clobber predefined group %s" %
                                groupname)
                        else:
                            self.metadata.groups[groupname] = group
                            self._dynamic_groups[groupname] = conn.name
                        groupnames.append(groupname)
                    else:
                        groupnames.append(group)

                self.metadata.merge_additional_groups(imd, groupnames)
            for conn in connectors:
                data = conn.get_additional_data(imd)
                self.metadata.merge_additional_data(imd, conn.name, data)
            imd.query.by_name = self.build_metadata
            if self.metadata_cache_mode in ['cautious', 'aggressive']:
                self.metadata_cache[client_name] = imd
        else:
            self.logger.debug("Using cached metadata object for %s" %
                              client_name)
        return imd

    def process_statistics(self, client_name, statistics):
        """ Process uploaded statistics for client.

        :param client_name: The name of the client to process
                            statistics for
        :type client_name: string
        :param statistics: The statistics document to process
        :type statistics: lxml.etree._Element
        """
        self.logger.debug("Processing statistics for %s" % client_name)
        meta = self.build_metadata(client_name)
        state = statistics.find(".//Statistics")
        if state.get('version') >= '2.0':
            for plugin in self.plugins_by_type(Statistics):
                try:
                    plugin.process_statistics(meta, statistics)
                except:
                    self.logger.error("Plugin %s failed to process stats from "
                                      "%s" % (plugin.name, meta.hostname),
                                      exc_info=1)

        self.logger.info("Client %s reported state %s" % (client_name,
                                                          state.get('state')))
        self.client_run_hook("end_statistics", meta)

    @track_statistics()
    def resolve_client(self, address, cleanup_cache=False, metadata=True):
        """ Given a client address, get the client hostname and
        optionally metadata.

        :param address: The address pair of the client to get the
                        canonical hostname for.
        :type address: tuple of (<ip address>, <hostname>)
        :param cleanup_cache: Tell the
                              :class:`Bcfg2.Server.Plugin.interfaces.Metadata`
                              plugin in :attr:`metadata` to clean up
                              any client or session cache it might
                              keep
        :type cleanup_cache: bool
        :param metadata: Build a
                         :class:`Bcfg2.Server.Plugins.Metadata.ClientMetadata`
                         object for this client as well.  This is
                         offered for convenience.
        :type metadata: bool
        :returns: tuple - If ``metadata`` is False, returns
                  ``(<canonical hostname>, None)``; if ``metadata`` is
                  True, returns ``(<canonical hostname>, <client
                  metadata object>)``
        """
        try:
            client = self.metadata.resolve_client(address,
                                                  cleanup_cache=cleanup_cache)
            if metadata:
                meta = self.build_metadata(client)
            else:
                meta = None
        except MetadataConsistencyError:
            err = sys.exc_info()[1]
            self.critical_error("Client metadata resolution error for %s: %s" %
                                (address[0], err))
        except MetadataRuntimeError:
            err = sys.exc_info()[1]
            self.critical_error('Metadata system runtime failure for %s: %s' %
                                (address[0], err))
        return (client, meta)

    def critical_error(self, message):
        """ Log an error with its traceback and return an XML-RPC fault
        to the client.

        :param message: The message to log and return to the client
        :type message: string
        :raises: :exc:`xmlrpclib.Fault`
        """
        self.logger.error(message, exc_info=1)
        raise xmlrpclib.Fault(xmlrpclib.APPLICATION_ERROR,
                              "Critical failure: %s" % message)

    def _get_rmi_objects(self):
        """ Get a dict (name: object) of all objects that may have RMI
        calls.  Currently, that includes all plugins and the FAM. """
        rv = {self.fam.__class__.__name__: self.fam}
        rv.update(self.plugins)
        return rv

    def _get_rmi(self):
        """ Get a list of RMI calls exposed by plugins """
        rmi = dict()
        for pname, pinst in self._get_rmi_objects().items():
            for mname in pinst.__rmi__:
                rmi["%s.%s" % (pname, mname)] = getattr(pinst, mname)
        return rmi

    def _resolve_exposed_method(self, method_name):
        """ Resolve a method name to the callable that implements that
        method.

        :param method_name: Name of the method to resolve
        :type method_name: string
        :returns: callable
        """
        try:
            func = getattr(self, method_name)
        except AttributeError:
            raise NoExposedMethod(method_name)
        if not getattr(func, "exposed", False):
            raise NoExposedMethod(method_name)
        return func

    # XMLRPC handlers start here

    @exposed
    def listMethods(self, address):  # pylint: disable=W0613
        """ List all exposed methods, including plugin RMI.

        :param address: Client (address, hostname) pair
        :type address: tuple
        :returns: list of exposed method names
        """
        methods = [name
                   for name, func in inspect.getmembers(self, callable)
                   if getattr(func, "exposed", False)]
        methods.extend(self._get_rmi().keys())
        return methods

    @exposed
    def methodHelp(self, address, method_name):  # pylint: disable=W0613
        """ Get help from the docstring of an exposed method

        :param address: Client (address, hostname) pair
        :type address: tuple
        :param method_name: The name of the method to get help on
        :type method_name: string
        :returns: string - The help message from the method's docstring
        """
        try:
            func = self._resolve_exposed_method(method_name)
        except NoExposedMethod:
            return ""
        return func.__doc__

    @exposed
    @track_statistics()
    @close_db_connection
    def DeclareVersion(self, address, version):
        """ Declare the client version.

        :param address: Client (address, hostname) pair
        :type address: tuple
        :param version: The client's declared version
        :type version: string
        :returns: bool - True on success
        :raises: :exc:`xmlrpclib.Fault`
        """
        client = self.resolve_client(address, metadata=False)[0]
        self.logger.debug("%s is running Bcfg2 client version %s" % (client,
                                                                     version))
        try:
            self.metadata.set_version(client, version)
        except (MetadataConsistencyError, MetadataRuntimeError):
            err = sys.exc_info()[1]
            self.critical_error("Unable to set version for %s: %s" %
                                (client, err))
        return True

    @exposed
    @close_db_connection
    def GetProbes(self, address):
        """ Fetch probes for the client.

        :param address: Client (address, hostname) pair
        :type address: tuple
        :returns: lxml.etree._Element - XML tree describing probes for
                  this client
        :raises: :exc:`xmlrpclib.Fault`
        """
        resp = lxml.etree.Element('probes')
        client, metadata = self.resolve_client(address, cleanup_cache=True)
        self.logger.debug("Getting probes for %s" % client)
        try:
            for plugin in self.plugins_by_type(Probing):
                for probe in plugin.GetProbes(metadata):
                    resp.append(probe)
            self.logger.debug("Sending probe list to %s" % client)
            return lxml.etree.tostring(resp,
                                       xml_declaration=False).decode('UTF-8')
        except:
            err = sys.exc_info()[1]
            self.critical_error("Error determining probes for %s: %s" %
                                (client, err))

    @exposed
    @close_db_connection
    def RecvProbeData(self, address, probedata):
        """ Receive probe data from clients.

        :param address: Client (address, hostname) pair
        :type address: tuple
        :returns: bool - True on success
        :raises: :exc:`xmlrpclib.Fault`
        """
        client, metadata = self.resolve_client(address)
        self.logger.debug("Receiving probe data from %s" % client)
        if self.metadata_cache_mode == 'cautious':
            # clear the metadata cache right after building the
            # metadata object; that way the cache is cleared for any
            # new probe data that's received, but the metadata object
            # that's created for RecvProbeData doesn't get cached.
            # I.e., the next metadata object that's built, after probe
            # data is processed, is cached.
            self.expire_caches_by_type(Bcfg2.Server.Plugin.Metadata)
        try:
            xpdata = lxml.etree.XML(probedata.encode('utf-8'),
                                    parser=Bcfg2.Server.XMLParser)
        except lxml.etree.XMLSyntaxError:
            err = sys.exc_info()[1]
            self.critical_error("Failed to parse probe data from client %s: %s"
                                % (client, err))

        sources = []
        for data in xpdata:
            source = data.get('source')
            if source not in sources:
                if source not in self.plugins:
                    self.logger.warning("Failed to locate plugin %s" % source)
                    continue
                sources.append(source)

        for source in sources:
            datalist = [data for data in xpdata
                        if data.get('source') == source]
            try:
                self.plugins[source].ReceiveData(metadata, datalist)
            except:
                err = sys.exc_info()[1]
                self.critical_error("Failed to process probe data from client "
                                    "%s: %s" % (client, err))
        return True

    @exposed
    @close_db_connection
    def AssertProfile(self, address, profile):
        """ Set profile for a client.

        :param address: Client (address, hostname) pair
        :type address: tuple
        :returns: bool - True on success
        :raises: :exc:`xmlrpclib.Fault`
        """
        client = self.resolve_client(address, metadata=False)[0]
        self.logger.debug("%s sets its profile to %s" % (client, profile))
        try:
            self.metadata.set_profile(client, profile, address)
        except (MetadataConsistencyError, MetadataRuntimeError):
            err = sys.exc_info()[1]
            self.critical_error("Unable to assert profile for %s: %s" %
                                (client, err))
        return True

    @exposed
    @close_db_connection
    def GetConfig(self, address):
        """ Build config for a client by calling
        :func:`BuildConfiguration`.

        :param address: Client (address, hostname) pair
        :type address: tuple
        :returns: lxml.etree._Element - The full configuration
                  document for the client
        :raises: :exc:`xmlrpclib.Fault`
        """
        client = self.resolve_client(address)[0]
        try:
            config = self.BuildConfiguration(client)
            return lxml.etree.tostring(config,
                                       xml_declaration=False).decode('UTF-8')
        except MetadataConsistencyError:
            self.critical_error("Metadata consistency failure for %s" % client)

    @exposed
    @close_db_connection
    def RecvStats(self, address, stats):
        """ Act on statistics upload with :func:`process_statistics`.

        :param address: Client (address, hostname) pair
        :type address: tuple
        :returns: bool - True on success
        :raises: :exc:`xmlrpclib.Fault`
        """
        client = self.resolve_client(address)[0]
        sdata = lxml.etree.XML(stats.encode('utf-8'),
                               parser=Bcfg2.Server.XMLParser)
        self.process_statistics(client, sdata)
        return True

    def authenticate(self, cert, user, password, address):
        """ Authenticate a client connection with
        :func:`Bcfg2.Server.Plugin.interfaces.Metadata.AuthenticateConnection`.

        :param cert: an x509 certificate
        :type cert: dict
        :param user: The username of the user trying to authenticate
        :type user: string
        :param password: The password supplied by the client
        :type password: string
        :param address: An address pair of ``(<ip address>, <hostname>)``
        :type address: tuple
        :return: bool - True if the authenticate succeeds, False otherwise
        """
        if self.ca:
            acert = cert
        else:
            # No ca, so no cert validation can be done
            acert = None
        return self.metadata.AuthenticateConnection(acert, user, password,
                                                    address)

    @exposed
    @close_db_connection
    def GetDecisionList(self, address, mode):
        """ Get the decision list for the client with :func:`GetDecisions`.

        :param address: Client (address, hostname) pair
        :type address: tuple
        :returns: list of decision tuples
        :raises: :exc:`xmlrpclib.Fault`
        """
        metadata = self.resolve_client(address)[1]
        return self.GetDecisions(metadata, mode)

    @property
    def database_available(self):
        """ True if the database is configured and available, False
        otherwise. """
        return self._database_available

    @exposed
    def get_statistics(self, _):
        """ Get current statistics about component execution from
        :attr:`Bcfg2.Statistics.stats`.

        :returns: dict - The statistics data as returned by
                  :func:`Bcfg2.Statistics.Statistics.display` """
        return Bcfg2.Statistics.stats.display()

    @exposed
    def toggle_debug(self, address):
        """ Toggle debug status of the FAM and all plugins

        :param address: Client (address, hostname) pair
        :type address: tuple
        :returns: bool - The new debug state of the FAM
        """
        return self.set_debug(address, not self.debug_flag)

    @exposed
    def toggle_core_debug(self, address):
        """ Toggle debug status of the server core

        :param address: Client (address, hostname) pair
        :type address: tuple
        :returns: bool - The new debug state of the FAM
        """
        return self.set_core_debug(address, not self.debug_flag)

    @exposed
    def toggle_fam_debug(self, address):
        """ Toggle debug status of the FAM

        :returns: bool - The new debug state of the FAM
        """
        self.logger.warning("Deprecated method set_fam_debug called by %s" %
                            address[0])
        return "This method is deprecated and will be removed in a future " + \
            "release\n%s" % self.fam.toggle_debug()

    @exposed
    def set_debug(self, address, debug):
        """ Explicitly set debug status of the FAM and all plugins

        :param address: Client (address, hostname) pair
        :type address: tuple
        :param debug: The new debug status.  This can either be a
                      boolean, or a string describing the state (e.g.,
                      "true" or "false"; case-insensitive)
        :type debug: bool or string
        :returns: bool - The new debug state
        """
        if debug not in [True, False]:
            debug = debug.lower() == "true"
        for plugin in self.plugins.values():
            plugin.set_debug(debug)
        rv = self.set_core_debug(address, debug)
        return self.fam.set_debug(debug) and rv

    @exposed
    def set_core_debug(self, _, debug):
        """ Explicity set debug status of the server core

        :param debug: The new debug status.  This can either be a
                      boolean, or a string describing the state (e.g.,
                      "true" or "false"; case-insensitive)
        :type debug: bool or string
        :returns: bool - The new debug state of the FAM
        """
        if debug not in [True, False]:
            debug = debug.lower() == "true"
        self.debug_flag = debug
        self.logger.info("Core: debug = %s" % debug)
        levels = self._loglevels[self.debug_flag]
        for handler in logging.root.handlers:
            try:
                level = levels.get(handler.name, levels['default'])
                self.logger.debug("Setting %s log handler to %s" %
                                  (handler.name, logging.getLevelName(level)))
            except AttributeError:
                level = levels['default']
                self.logger.debug("Setting unknown log handler %s to %s" %
                                  (handler, logging.getLevelName(level)))
            handler.setLevel(level)
        return self.debug_flag

    @exposed
    def set_fam_debug(self, address, debug):
        """ Explicitly set debug status of the FAM

        :param debug: The new debug status of the FAM.  This can
                      either be a boolean, or a string describing the
                      state (e.g., "true" or "false";
                      case-insensitive)
        :type debug: bool or string
        :returns: bool - The new debug state of the FAM
        """
        if debug not in [True, False]:
            debug = debug.lower() == "true"
        self.logger.warning("Deprecated method set_fam_debug called by %s" %
                            address[0])
        return "This method is deprecated and will be removed in a future " + \
            "release\n%s" % self.fam.set_debug(debug)

########NEW FILE########
__FILENAME__ = Fam
""" File monitor backend with support for the `File Alteration Monitor
<http://oss.sgi.com/projects/fam/>`_.  The FAM backend is deprecated. """

import os
import _fam
import stat
import logging
from time import time
from Bcfg2.Server.FileMonitor import FileMonitor

LOGGER = logging.getLogger(__name__)


class Fam(FileMonitor):
    """ **Deprecated** file monitor backend with support for the `File
    Alteration Monitor <http://oss.sgi.com/projects/fam/>`_ (also
    abbreviated "FAM")."""

    #: FAM is the worst actual monitor backend, so give it a low
    #: priority.
    __priority__ = 10

    def __init__(self, ignore=None, debug=False):
        FileMonitor.__init__(self, ignore=ignore, debug=debug)
        self.filemonitor = _fam.open()
        self.users = {}
        LOGGER.warning("The Fam file monitor backend is deprecated. Please "
                       "switch to a supported file monitor.")
    __init__.__doc__ = FileMonitor.__init__.__doc__

    def fileno(self):
        return self.filemonitor.fileno()
    fileno.__doc__ = FileMonitor.fileno.__doc__

    def handle_event_set(self, _=None):
        self.Service()
    handle_event_set.__doc__ = FileMonitor.handle_event_set.__doc__

    def handle_events_in_interval(self, interval):
        now = time()
        while (time() - now) < interval:
            if self.Service():
                now = time()
    handle_events_in_interval.__doc__ = \
        FileMonitor.handle_events_in_interval.__doc__

    def AddMonitor(self, path, obj, _=None):
        mode = os.stat(path)[stat.ST_MODE]
        if stat.S_ISDIR(mode):
            handle = self.filemonitor.monitorDirectory(path, None)
        else:
            handle = self.filemonitor.monitorFile(path, None)
        self.handles[handle.requestID()] = handle
        if obj is not None:
            self.users[handle.requestID()] = obj
        return handle.requestID()
    AddMonitor.__doc__ = FileMonitor.AddMonitor.__doc__

    def Service(self, interval=0.50):
        """ Handle events for the specified period of time (in
        seconds).  This call will block for ``interval`` seconds.

        :param interval: The interval, in seconds, during which events
                         should be handled.  Any events that are
                         already pending when :func:`Service` is
                         called will also be handled.
        :type interval: int
        :returns: None
        """
        count = 0
        collapsed = 0
        rawevents = []
        start = time()
        now = time()
        while (time() - now) < interval:
            if self.filemonitor.pending():
                while self.filemonitor.pending():
                    count += 1
                    rawevents.append(self.filemonitor.nextEvent())
                now = time()
        unique = []
        bookkeeping = []
        for event in rawevents:
            if self.should_ignore(event):
                continue
            if event.code2str() != 'changed':
                # process all non-change events
                unique.append(event)
            else:
                if (event.filename, event.requestID) not in bookkeeping:
                    bookkeeping.append((event.filename, event.requestID))
                    unique.append(event)
                else:
                    collapsed += 1
        for event in unique:
            if event.requestID in self.users:
                try:
                    self.users[event.requestID].HandleEvent(event)
                except:  # pylint: disable=W0702
                    LOGGER.error("Handling event for file %s" % event.filename,
                                 exc_info=1)
        end = time()
        LOGGER.info("Processed %s fam events in %03.03f seconds. "
                    "%s coalesced" % (count, (end - start), collapsed))
        return count

########NEW FILE########
__FILENAME__ = Gamin
""" File monitor backend with `Gamin
<http://people.gnome.org/~veillard/gamin/>`_ support. """

import os
import stat
from gamin import WatchMonitor, GAMCreated, GAMExists, GAMEndExist, \
    GAMChanged, GAMDeleted
from Bcfg2.Server.FileMonitor import Event, FileMonitor


class GaminEvent(Event):
    """ This class maps Gamin event constants to FAM :ref:`event codes
    <development-fam-event-codes>`. """

    #: The map of gamin event constants (which mirror FAM event names
    #: closely) to :ref:`event codes <development-fam-event-codes>`
    action_map = {GAMCreated: 'created', GAMExists: 'exists',
                  GAMChanged: 'changed', GAMDeleted: 'deleted',
                  GAMEndExist: 'endExist'}

    def __init__(self, request_id, filename, code):
        Event.__init__(self, request_id, filename, code)
        if code in self.action_map:
            self.action = self.action_map[code]
    __init__.__doc__ = Event.__init__.__doc__


class Gamin(FileMonitor):
    """ File monitor backend with `Gamin
    <http://people.gnome.org/~veillard/gamin/>`_ support. """

    #: The Gamin backend is fairly decent, particularly newer
    #: releases, so it has a fairly high priority.
    __priority__ = 90

    def __init__(self, ignore=None, debug=False):
        FileMonitor.__init__(self, ignore=ignore, debug=debug)

        #: The :class:`Gamin.WatchMonitor` object for this monitor.
        self.mon = None

        #: The counter used to produce monotonically increasing
        #: monitor handle IDs
        self.counter = 0

        #: The queue used to record monitors that are added before
        #: :func:`start` has been called and :attr:`mon` is created.
        self.add_q = []
    __init__.__doc__ = FileMonitor.__init__.__doc__

    def start(self):
        """ The Gamin watch monitor in :attr:`mon` must be created by
        the daemonized process, so is created in ``start()``. Before
        the :class:`Gamin.WatchMonitor` object is created, monitors
        are added to :attr:`add_q`, and are created once the watch
        monitor is created."""
        FileMonitor.start(self)
        self.mon = WatchMonitor()
        for monitor in self.add_q:
            self.AddMonitor(*monitor)
        self.add_q = []

    def fileno(self):
        if self.started:
            return self.mon.get_fd()
        else:
            return None
    fileno.__doc__ = FileMonitor.fileno.__doc__

    def queue(self, path, action, request_id):
        """ Create a new :class:`GaminEvent` and add it to the
        :attr:`events` queue for later handling. """
        self.events.append(GaminEvent(request_id, path, action))

    def AddMonitor(self, path, obj, handle=None):
        if handle is None:
            handle = self.counter
            self.counter += 1

        if not self.started:
            self.add_q.append((path, obj, handle))
            return handle

        mode = os.stat(path)[stat.ST_MODE]

        # Flush queued gamin events
        while self.mon.event_pending():
            self.mon.handle_one_event()

        if stat.S_ISDIR(mode):
            self.mon.watch_directory(path, self.queue, handle)
        else:
            self.mon.watch_file(path, self.queue, handle)
        self.handles[handle] = obj
        return handle
    AddMonitor.__doc__ = FileMonitor.AddMonitor.__doc__

    def pending(self):
        return FileMonitor.pending(self) or self.mon.event_pending()
    pending.__doc__ = FileMonitor.pending.__doc__

    def get_event(self):
        if self.mon.event_pending():
            self.mon.handle_one_event()
        return FileMonitor.get_event(self)
    get_event.__doc__ = FileMonitor.get_event.__doc__

########NEW FILE########
__FILENAME__ = Inotify
"""File monitor backend with `inotify <http://inotify.aiken.cz/>`_
support. """

import os
import errno
import pyinotify
from Bcfg2.Compat import reduce  # pylint: disable=W0622
from Bcfg2.Server.FileMonitor import Event
from Bcfg2.Server.FileMonitor.Pseudo import Pseudo


class Inotify(Pseudo, pyinotify.ProcessEvent):
    """ File monitor backend with `inotify
    <http://inotify.aiken.cz/>`_ support. """

    __rmi__ = Pseudo.__rmi__ + ["list_watches", "list_paths"]

    #: Inotify is the best FAM backend, so it gets a very high
    #: priority
    __priority__ = 99

    # pylint: disable=E1101
    #: Map pyinotify event constants to FAM :ref:`event codes
    #: <development-fam-event-codes>`.  The mapping is not
    #: terrifically exact.
    action_map = {pyinotify.IN_CREATE: 'created',
                  pyinotify.IN_DELETE: 'deleted',
                  pyinotify.IN_MODIFY: 'changed',
                  pyinotify.IN_MOVED_FROM: 'deleted',
                  pyinotify.IN_MOVED_TO: 'created'}
    # pylint: enable=E1101

    #: The pyinotify event mask.  We only ask for events that are
    #: listed in :attr:`action_map`
    mask = reduce(lambda x, y: x | y, action_map.keys())

    def __init__(self, ignore=None, debug=False):
        Pseudo.__init__(self, ignore=ignore, debug=debug)
        pyinotify.ProcessEvent.__init__(self)

        #: inotify can't set useful monitors directly on files, only
        #: on directories, so when a monitor is added on a file we add
        #: its parent directory to ``event_filter`` and then only
        #: produce events on a file in that directory if the file is
        #: listed in ``event_filter``.  Keys are directories -- the
        #: parent directories of individual files that are monitored
        #: -- and values are lists of full paths to files in each
        #: directory that events *should* be produced for.  An event
        #: on a file whose parent directory is in ``event_filter`` but
        #: which is not itself listed will be silently suppressed.
        self.event_filter = dict()

        #: inotify doesn't like monitoring a path twice, so we keep a
        #: dict of :class:`pyinotify.Watch` objects, keyed by monitor
        #: path, to avoid trying to create duplicate monitors.
        #: (Duplicates can happen if an object accidentally requests
        #: duplicate monitors, or if two files in a single directory
        #: are both individually monitored, since inotify can't set
        #: monitors on the files but only on the parent directories.)
        self.watches_by_path = dict()

        #: The :class:`pyinotify.ThreadedNotifier` object.  This is
        #: created in :func:`start` after the server is done
        #: daemonizing.
        self.notifier = None

        #: The :class:`pyinotify.WatchManager` object. This is created
        #: in :func:`start` after the server is done daemonizing.
        self.watchmgr = None

        #: The queue used to record monitors that are added before
        #: :func:`start` has been called and :attr:`notifier` and
        #: :attr:`watchmgr` are created.
        self.add_q = []

    def start(self):
        """ The inotify notifier and manager objects in
        :attr:`notifier` and :attr:`watchmgr` must be created by the
        daemonized process, so they are created in ``start()``. Before
        those objects are created, monitors are added to
        :attr:`add_q`, and are created once the
        :class:`pyinotify.ThreadedNotifier` and
        :class:`pyinotify.WatchManager` objects are created."""
        Pseudo.start(self)
        self.watchmgr = pyinotify.WatchManager()
        self.notifier = pyinotify.ThreadedNotifier(self.watchmgr, self)
        self.notifier.start()
        for monitor in self.add_q:
            self.AddMonitor(*monitor)
        self.add_q = []

    def fileno(self):
        if self.started:
            return self.watchmgr.get_fd()
        else:
            return None
    fileno.__doc__ = Pseudo.fileno.__doc__

    def process_default(self, ievent):
        """ Process all inotify events received.  This process a
        :class:`pyinotify._Event` object, creates a
        :class:`Bcfg2.Server.FileMonitor.Event` object from it, and
        adds that event to :attr:`events`.

        :param ievent: Event to be processed
        :type ievent: pyinotify._Event
        """
        action = ievent.maskname
        for amask, aname in self.action_map.items():
            if ievent.mask & amask:
                action = aname
                break
        else:
            # event action is not in the mask, and thus is not
            # something we care about
            self.debug_log("Ignoring event %s for %s" % (action,
                                                         ievent.pathname))
            return

        try:
            watch = self.watchmgr.watches[ievent.wd]
        except KeyError:
            self.logger.error("Error handling event %s for %s: "
                              "Watch %s not found" %
                              (action, ievent.pathname, ievent.wd))
            return
        # FAM-style file monitors return the full path to the parent
        # directory that is being watched, relative paths to anything
        # contained within the directory. since we can't use inotify
        # to watch files directly, we have to sort of guess at whether
        # this watch was actually added on a file (and thus is in
        # self.event_filter because we're filtering out other events
        # on the directory) or was added directly on a directory.
        if (watch.path == ievent.pathname or ievent.wd in self.event_filter):
            path = ievent.pathname
        else:
            # relative path
            path = os.path.basename(ievent.pathname)
        # figure out the handleID.  start with the path of the event;
        # that should catch events on files that are watched directly.
        # (we have to watch the directory that a file is in, so this
        # lets us handle events on different files in the same
        # directory -- and thus under the same watch -- with different
        # objects.)  If the path to the event doesn't have a handler,
        # use the path of the watch itself.
        handleID = ievent.pathname
        if handleID not in self.handles:
            handleID = watch.path
        evt = Event(handleID, path, action)

        if (ievent.wd not in self.event_filter or
            ievent.pathname in self.event_filter[ievent.wd]):
            self.events.append(evt)

    def AddMonitor(self, path, obj, handleID=None):
        # strip trailing slashes
        path = path.rstrip("/")

        if not self.started:
            self.add_q.append((path, obj))
            return path

        if not os.path.isdir(path):
            # inotify is a little wonky about watching files.  for
            # instance, if you watch /tmp/foo, and then do 'mv
            # /tmp/bar /tmp/foo', it processes that as a deletion of
            # /tmp/foo (which it technically _is_, but that's rather
            # useless -- we care that /tmp/foo changed, not that it
            # was first deleted and then created).  In order to
            # effectively watch a file, we have to watch the directory
            # it's in, and filter out events for other files in the
            # same directory that are not similarly watched.
            # watch_transient_file requires a Processor _class_, not
            # an object, so we can't have this object handle events,
            # which is Wrong, so we can't use that function.
            watch_path = os.path.dirname(path)
            is_dir = False
        else:
            watch_path = path
            is_dir = True

        # see if this path is already being watched
        try:
            watchdir = self.watches_by_path[watch_path]
        except KeyError:
            if not os.path.exists(watch_path):
                raise OSError(errno.ENOENT,
                              "No such file or directory: '%s'" % path)
            watchdir = self.watchmgr.add_watch(watch_path, self.mask,
                                               quiet=False)[watch_path]
            self.watches_by_path[watch_path] = watchdir

        produce_exists = True
        if not is_dir:
            if watchdir not in self.event_filter:
                self.event_filter[watchdir] = [path]
            elif path not in self.event_filter[watchdir]:
                self.event_filter[watchdir].append(path)
            else:
                # we've been asked to watch a file that we're already
                # watching, so we don't need to produce 'exists'
                # events
                produce_exists = False

        # inotify doesn't produce initial 'exists' events, so we
        # inherit from Pseudo to produce those
        if produce_exists:
            return Pseudo.AddMonitor(self, path, obj, handleID=path)
        else:
            self.handles[path] = obj
            return path
    AddMonitor.__doc__ = Pseudo.AddMonitor.__doc__

    def shutdown(self):
        if self.started and self.notifier:
            self.notifier.stop()
    shutdown.__doc__ = Pseudo.shutdown.__doc__

    def list_watches(self):
        """ XML-RPC that returns a list of current inotify watches for
        debugging purposes. """
        return list(self.watches_by_path.keys())

    def list_paths(self):
        """ XML-RPC that returns a list of paths that are handled for
        debugging purposes. Because inotify doesn't like watching
        files, but prefers to watch directories, this will be
        different from
        :func:`Bcfg2.Server.FileMonitor.Inotify.Inotify.ListWatches`. For
        instance, if a plugin adds a monitor to
        ``/var/lib/bcfg2/Plugin/foo.xml``, :func:`ListPaths` will
        return ``/var/lib/bcfg2/Plugin/foo.xml``, while
        :func:`ListWatches` will return ``/var/lib/bcfg2/Plugin``. """
        return list(self.handles.keys())

########NEW FILE########
__FILENAME__ = Pseudo
""" Pseudo provides static monitor support for file alteration events.
That is, it only produces "exists" and "endExist" events and does not
monitor for ongoing changes. """

import os
from Bcfg2.Server.FileMonitor import FileMonitor, Event


class Pseudo(FileMonitor):
    """ File monitor that only produces events on server startup and
    doesn't actually monitor for ongoing changes at all. """

    #: The ``Pseudo`` monitor should only be used if no other FAM
    #: backends are available.
    __priority__ = 1

    def AddMonitor(self, path, obj, handleID=None):
        if handleID is None:
            handleID = len(list(self.handles.keys()))
        self.events.append(Event(handleID, path, 'exists'))
        if os.path.isdir(path):
            dirlist = os.listdir(path)
            for fname in dirlist:
                self.events.append(Event(handleID, fname, 'exists'))
            self.events.append(Event(handleID, path, 'endExist'))

        if obj is not None:
            self.handles[handleID] = obj
        return handleID

########NEW FILE########
__FILENAME__ = backends
from django.contrib.auth.models import User
#from ldapauth import *
from nisauth import *

## class LDAPBackend(object):

##     def authenticate(self,username=None,password=None):
##         try:

##             l = ldapauth(username,password)
##             temp_pass = User.objects.make_random_password(100)
##             ldap_user = dict(username=l.sAMAccountName,
##                              )
##             user_session_obj = dict(
##                 email=l.email,
##                 first_name=l.name_f,
##                 last_name=l.name_l,
##                 uid=l.badge_no
##                 )
##             #fixme: need to add this user session obj to session
##             user,created = User.objects.get_or_create(username=username)
##             return user

##         except LDAPAUTHError,e:
##             return None

##     def get_user(self,user_id):
##         try:
##             return User.objects.get(pk=user_id)
##         except User.DoesNotExist, e:
##             return None


class NISBackend(object):

    def authenticate(self, username=None, password=None):
        try:
            n = nisauth(username, password)
            temp_pass = User.objects.make_random_password(100)
            nis_user = dict(username=username,
                            )

            user_session_obj = dict(
                email = username + "@mcs.anl.gov",
                first_name = None,
                last_name = None,
                uid = n.uid
                )
            user, created = User.objects.get_or_create(username=username)

            return user

        except NISAUTHError:
            e = sys.exc_info()[1]
            return None


    def get_user(self, user_id):
        try:
            return User.objects.get(pk=user_id)
        except User.DoesNotExist:
            e = sys.exc_info()[1]
            return None

########NEW FILE########
__FILENAME__ = admin
from django.contrib import admin

from models import Host, Interface, IP, MX, Name, CName, Nameserver, ZoneAddress, Zone, Log, ZoneLog

admin.site.register(Host)
admin.site.register(Interface)
admin.site.register(IP)
admin.site.register(MX)
admin.site.register(Name)
admin.site.register(CName)
admin.site.register(Nameserver)
admin.site.register(ZoneAddress)
admin.site.register(Zone)
admin.site.register(Log)
admin.site.register(ZoneLog)

########NEW FILE########
__FILENAME__ = models
from django.db import models

# Create your models here.
class Host(models.Model):
    NETGROUP_CHOICES = (
        ('none', 'none'),('cave', 'cave'),('ccst', 'ccst'),('mcs', 'mcs'),
        ('mmlab', 'mmlab'),('sp', 'sp'),('red', 'red'),('virtual', 'virtual'),
        ('win', 'win'),('xterm', 'xterm'),('lcrc', 'lcrc'),('anlext', 'anlext'),
        ('teragrid', 'teragrid')
        )
    STATUS_CHOICES = (
        ('active','active'),('dormant','dormant')
        )
    SUPPORT_CHOICES = (
        ('green','green'),('yellow','yellow'),('red','red')
        )
    CLASS_CHOICES = (
        ('scientific','scientific'),
        ('operations','operations'),('guest','guest'),
        ('confidential','confidential'),('public','public')
        )
    WHATAMI_CHOICES = (
        ('aix-3', 'aix-3'), ('aix-4', 'aix-4'),
        ('aix-5', 'aix-5'), ('baytech', 'baytech'),
        ('decserver', 'decserver'), ('dialup', 'dialup'),
        ('dos', 'dos'), ('freebsd', 'freebsd'),
        ('hpux', 'hpux'), ('irix-5', 'irix-5'),
        ('irix-6', 'irix-6'), ('linux', 'linux'),
        ('linux-2', 'linux-2'), ('linux-rh73', 'linux-rh73'),
        ('linux-rh8', 'linux-rh8'), ('linux-sles8', 'linux-sles8'),
        ('linux-sles8-64', 'linux-sles8-64'), ('linux-sles8-ia32', 'linux-sles8-ia32'),
        ('linux-sles8-ia64', 'linux-sles8-ia64'), ('mac', 'mac'),
        ('network', 'network'), ('next', 'next'),
        ('none', 'none'), ('osf', 'osf'), ('printer', 'printer'),
        ('robot', 'robot'), ('solaris-2', 'solaris-2'),
        ('sun4', 'sun4'), ('unknown', 'unknown'), ('virtual', 'virtual'),
        ('win31', 'win31'), ('win95', 'win95'),
        ('winNTs', 'winNTs'), ('winNTw', 'winNTw'),
        ('win2k', 'win2k'), ('winXP', 'winXP'), ('xterm', 'xterm')
        )
    hostname = models.CharField(max_length=64)
    whatami = models.CharField(max_length=16)
    netgroup = models.CharField(max_length=32, choices=NETGROUP_CHOICES)
    security_class = models.CharField('class', max_length=16)
    support = models.CharField(max_length=8, choices=SUPPORT_CHOICES)
    csi = models.CharField(max_length=32, blank=True)
    printq = models.CharField(max_length=32, blank=True)
    outbound_smtp = models.BooleanField()
    primary_user = models.EmailField()
    administrator = models.EmailField(blank=True)
    location = models.CharField(max_length=16)
    comments = models.TextField(blank=True)
    expiration_date = models.DateField(null=True, blank=True)
    last = models.DateField(auto_now=True, auto_now_add=True)
    status = models.CharField(max_length=7, choices=STATUS_CHOICES)
    dirty = models.BooleanField()

    class Admin:
        list_display = ('hostname', 'last')
        search_fields = ['hostname']

    def __str__(self):
        return self.hostname

    def get_logs(self):
        """
            Get host's log.
        """
        return Log.objects.filter(hostname=self.hostname)

class Interface(models.Model):
    TYPE_CHOICES = (
        ('eth', 'ethernet'), ('wl', 'wireless'), ('virtual', 'virtual'), ('myr', 'myr'),
        ('mgmt', 'mgmt'), ('tape', 'tape'), ('fe', 'fe'), ('ge', 'ge'),
        )
    # FIXME: The new admin interface has change a lot.
    #host = models.ForeignKey(Host, edit_inline=models.TABULAR, num_in_admin=2)
    host = models.ForeignKey(Host)
    # FIXME: The new admin interface has change a lot.
    #mac_addr = models.CharField(max_length=32, core=True)
    mac_addr = models.CharField(max_length=32)
    hdwr_type = models.CharField('type', max_length=16, choices=TYPE_CHOICES, blank=True)
    # FIXME: The new admin interface has change a lot.
    #                             radio_admin=True, blank=True)
    dhcp = models.BooleanField()

    def __str__(self):
        return self.mac_addr

    class Admin:
        list_display = ('mac_addr', 'host')
        search_fields = ['mac_addr']

class IP(models.Model):
    interface = models.ForeignKey(Interface)
    # FIXME: The new admin interface has change a lot.
    #                              edit_inline=models.TABULAR, num_in_admin=1)
    #ip_addr = models.IPAddressField(core=True)
    ip_addr = models.IPAddressField()

    def __str__(self):
        return self.ip_addr

    class Admin:
        pass

    class Meta:
        ordering = ('ip_addr', )

class MX(models.Model):
    priority = models.IntegerField(blank=True)
    # FIXME: The new admin interface has change a lot.
    #mx = models.CharField(max_length=64, blank=True, core=True)
    mx = models.CharField(max_length=64, blank=True)

    def __str__(self):
        return (" ".join([str(self.priority), self.mx]))

    class Admin:
        pass

class Name(models.Model):
    DNS_CHOICES = (
        ('global','global'),('internal','ANL internal'),
        ('private','private')
        )
    # FIXME: The new admin interface has change a lot.
    #ip = models.ForeignKey(IP, edit_inline=models.TABULAR, num_in_admin=1)
    ip = models.ForeignKey(IP)
    # FIXME: The new admin interface has change a lot.
    #name = models.CharField(max_length=64, core=True)
    name = models.CharField(max_length=64)
    dns_view = models.CharField(max_length=16, choices=DNS_CHOICES)
    only = models.BooleanField(blank=True)
    mxs = models.ManyToManyField(MX)

    def __str__(self):
        return self.name

    class Admin:
        pass

class CName(models.Model):
    # FIXME: The new admin interface has change a lot.
    #name = models.ForeignKey(Name, edit_inline=models.TABULAR, num_in_admin=1)
    name = models.ForeignKey(Name)
    # FIXME: The new admin interface has change a lot.
    #cname = models.CharField(max_length=64, core=True)
    cname = models.CharField(max_length=64)

    def __str__(self):
        return self.cname

    class Admin:
        pass

class Nameserver(models.Model):
    name = models.CharField(max_length=64, blank=True)

    def __str__(self):
        return self.name

    class Admin:
        pass

class ZoneAddress(models.Model):
    ip_addr = models.IPAddressField(blank=True)

    def __str__(self):
        return self.ip_addr

    class Admin:
        pass

class Zone(models.Model):
    zone = models.CharField(max_length=64)
    serial = models.IntegerField()
    admin = models.CharField(max_length=64)
    primary_master = models.CharField(max_length=64)
    expire = models.IntegerField()
    retry = models.IntegerField()
    refresh = models.IntegerField()
    ttl = models.IntegerField()
    nameservers = models.ManyToManyField(Nameserver, blank=True)
    mxs = models.ManyToManyField(MX, blank=True)
    addresses = models.ManyToManyField(ZoneAddress, blank=True)
    aux = models.TextField(blank=True)

    def __str__(self):
        return self.zone

    class Admin:
        pass

class Log(models.Model):
    # FIXME: Proposal hostname = models.ForeignKey(Host)
    hostname = models.CharField(max_length=64)
    date = models.DateTimeField(auto_now=True, auto_now_add=True)
    log = models.TextField()

    def __str__(self):
        return self.hostname

class ZoneLog(models.Model):
    zone = models.CharField(max_length=64)
    date = models.DateTimeField(auto_now=True, auto_now_add=True)
    log = models.TextField()

    def __str__(self):
        return self.zone

########NEW FILE########
__FILENAME__ = urls
# -*- coding: utf-8 -*-
from Bcfg2.Reporting.Compat.django_urls import *
from django.contrib.auth.decorators import login_required
from django.core.urlresolvers import reverse
from django.views.generic.create_update import create_object, update_object, delete_object
from django.views.generic.list_detail import object_detail, object_list

from models import Host, Zone, Log

host_detail_dict = {
    'queryset':Host.objects.all(),
    'template_name':'host.html',
    'template_object_name':'host',
}

host_delete_dict = {
    'model':Host,
    'post_delete_redirect':'/',
}

host_log_detail_dict = host_detail_dict.copy()
host_log_detail_dict['template_name'] = 'logviewer.html'

host_dns_detail_dict = host_detail_dict.copy()
host_dns_detail_dict['template_name'] = 'dns.html'

zone_new_dict = {
    'model':Zone,
    'template_name':'zonenew.html',
    'post_save_redirect':'../%(id)s',
}

zones_list_dict = {
    'queryset':Zone.objects.all(),
    'template_name':'zones.html',
    'template_object_name':'zone',
}

zone_detail_dict = {
    'queryset':Zone.objects.all(),
    'template_name':'zoneview.html',
    'template_object_name':'zone',
}

urlpatterns = patterns('',
    (r'^(?P<object_id>\d+)/$', object_detail, host_detail_dict, 'host_detail'),
    (r'^zones/new/$', login_required(create_object), zone_new_dict, 'zone_new'),
    (r'^zones/(?P<object_id>\d+)/edit', login_required(update_object), zone_new_dict, 'zone_edit'),
    (r'^zones/$', object_list, zones_list_dict, 'zone_list'),
    (r'^zones/(?P<object_id>\d+)/$', object_detail, zone_detail_dict, 'zone_detail'),
    (r'^zones/(?P<object_id>\d+)/$', object_detail, zone_detail_dict, 'zone_detail'),
    (r'^\d+/logs/(?P<object_id>\d+)/', object_detail, { 'queryset':Log.objects.all() }, 'log_detail'),
    (r'^(?P<object_id>\d+)/logs/', object_detail, host_log_detail_dict, 'host_log_list'),
    (r'^(?P<object_id>\d+)/dns', object_detail, host_dns_detail_dict, 'host_dns_list'),
    (r'^(?P<object_id>\d+)/remove', login_required(delete_object), host_delete_dict, 'host_delete'),
)

urlpatterns += patterns('Bcfg2.Server.Hostbase.hostbase.views',
    (r'^$', 'search'),
    (r'^(?P<host_id>\d+)/edit', 'edit'),
    (r'^(?P<host_id>\d+)/(?P<item>\D+)/(?P<item_id>\d+)/confirm', 'confirm'),
    (r'^(?P<host_id>\d+)/(?P<item>\D+)/(?P<item_id>\d+)/(?P<name_id>\d+)/confirm', 'confirm'),
    (r'^(?P<host_id>\d+)/dns/edit', 'dnsedit'),
    (r'^new', 'new'),
    (r'^(?P<host_id>\d+)/copy', 'copy'),
#   (r'^hostinfo', 'hostinfo'),
    (r'^zones/(?P<zone_id>\d+)/(?P<item>\D+)/(?P<item_id>\d+)/confirm', 'confirm'),
)

########NEW FILE########
__FILENAME__ = views
"""Views.py
Contains all the views associated with the hostbase app
Also has does form validation
"""
from django.http import HttpResponse, HttpResponseRedirect

from django.contrib.auth.decorators import login_required
from django.contrib.auth import logout
from django.template import RequestContext
from Bcfg2.Server.Hostbase.hostbase.models import *
from datetime import date
from django.db import connection
from django.shortcuts import render_to_response
from django import forms
from Bcfg2.Server.Hostbase import settings, regex
import re, copy

attribs = ['hostname', 'whatami', 'netgroup', 'security_class', 'support',
           'csi', 'printq', 'primary_user', 'administrator', 'location',
           'status', 'comments']

zoneattribs = ['zone', 'admin', 'primary_master', 'expire', 'retry',
               'refresh', 'ttl', 'aux']

dispatch = {'mac_addr':'i.mac_addr LIKE \'%%%%%s%%%%\'',
            'ip_addr':'p.ip_addr LIKE \'%%%%%s%%%%\'',
            'name':'n.name LIKE \'%%%%%s%%%%\'',
##             'hostname':'n.name LIKE \'%%%%%s%%%%\'',
##             'cname':'n.name LIKE \'%%%%%s%%%%\'',
            'mx':'m.mx LIKE \'%%%%%s%%%%\'',
            'dns_view':'n.dns_view = \'%s\'',
            'hdwr_type':'i.hdwr_type = \'%s\'',
            'dhcp':'i.dhcp = \'%s\''}

def search(request):
    """Search for hosts in the database
    If more than one field is entered, logical AND is used
    """
    if 'sub' in request.GET:
        querystring = """SELECT DISTINCT h.hostname, h.id, h.status
        FROM (((((hostbase_host h
        INNER JOIN hostbase_interface i ON h.id = i.host_id)
        INNER JOIN hostbase_ip p ON i.id = p.interface_id)
        INNER JOIN hostbase_name n ON p.id = n.ip_id)
        INNER JOIN hostbase_name_mxs x ON n.id = x.name_id)
        INNER JOIN hostbase_mx m ON m.id = x.mx_id)
        LEFT JOIN hostbase_cname c ON n.id = c.name_id
        WHERE """

        _and = False
        for field in request.POST:
            if request.POST[field] and field == 'hostname':
                if _and:
                    querystring += ' AND '
                querystring +=  'n.name LIKE \'%%%%%s%%%%\' or c.cname LIKE \'%%%%%s%%%%\'' % (request.POST[field], request.POST[field])
                _and = True
            elif request.POST[field] and field in dispatch:
                if _and:
                    querystring += ' AND '
                querystring += dispatch[field]  % request.POST[field]
                _and = True
            elif request.POST[field]:
                if _and:
                    querystring += ' AND '
                querystring += "h.%s LIKE \'%%%%%s%%%%\'" % (field, request.POST[field])
                _and = True

        if not _and:
            cursor = connection.cursor()
            cursor.execute("""SELECT hostname, id, status
            FROM hostbase_host ORDER BY hostname""")
            results = cursor.fetchall()
        else:
            querystring += " ORDER BY h.hostname"
            cursor = connection.cursor()
            cursor.execute(querystring)
            results = cursor.fetchall()

        return render_to_response('results.html',
                                  {'hosts': results,
                                   'logged_in': request.session.get('_auth_user_id', False)},
                                   context_instance = RequestContext(request))
    else:
        return render_to_response('search.html',
                                  {'TYPE_CHOICES': Interface.TYPE_CHOICES,
                                   'DNS_CHOICES': Name.DNS_CHOICES,
                                   'yesno': [(1, 'yes'), (0, 'no')],
                                   'logged_in': request.session.get('_auth_user_id', False)},
                                   context_instance = RequestContext(request))


def gethostdata(host_id, dnsdata=False):
    """Grabs the necessary data about a host
    Replaces a lot of repeated code"""
    hostdata = {}
    hostdata['ips'] = {}
    hostdata['names'] = {}
    hostdata['cnames'] = {}
    hostdata['mxs'] = {}
    hostdata['host'] = Host.objects.get(id=host_id)
    hostdata['interfaces'] = hostdata['host'].interface_set.all()
    for interface in hostdata['interfaces']:
        hostdata['ips'][interface.id] = interface.ip_set.all()
        if dnsdata:
            for ip in hostdata['ips'][interface.id]:
                hostdata['names'][ip.id] = ip.name_set.all()
                for name in hostdata['names'][ip.id]:
                    hostdata['cnames'][name.id] = name.cname_set.all()
                    hostdata['mxs'][name.id] = name.mxs.all()
    return hostdata

def fill(template, hostdata, dnsdata=False):
    """Fills a generic template
    Replaces a lot of repeated code"""
    if dnsdata:
        template.names = hostdata['names']
        template.cnames = hostdata['cnames']
        template.mxs = hostdata['mxs']
    template.host = hostdata['host']
    template.interfaces = hostdata['interfaces']
    template.ips = hostdata['ips']
    return template

def edit(request, host_id):
    """edit general host information"""
    manipulator = Host.ChangeManipulator(host_id)
    changename = False
    if request.method == 'POST':
        host = Host.objects.get(id=host_id)
        before = host.__dict__.copy()
        if request.POST['hostname'] != host.hostname:
            oldhostname = host.hostname.split(".")[0]
            changename = True
        interfaces = host.interface_set.all()
        old_interfaces = [interface.__dict__.copy() for interface in interfaces]

        new_data = request.POST.copy()

        errors = manipulator.get_validation_errors(new_data)
        if not errors:

            # somehow keep track of multiple interface change manipulators
            # as well as multiple ip chnage manipulators??? (add manipulators???)
            # change to many-to-many??????

            # dynamically look up mx records?
            text = ''

            for attrib in attribs:
                if host.__dict__[attrib] != request.POST[attrib]:
                    text = do_log(text, attrib, host.__dict__[attrib], request.POST[attrib])
                    host.__dict__[attrib] = request.POST[attrib]

            if 'expiration_date' in request.POST:
                ymd = request.POST['expiration_date'].split("-")
                if date(int(ymd[0]), int(ymd[1]), int(ymd[2])) != host.__dict__['expiration_date']:
                    text = do_log(text, 'expiration_date', host.__dict__['expiration_date'],
                                  request.POST['expiration_date'])
                    host.__dict__['expiration_date'] = date(int(ymd[0]), int(ymd[1]), int(ymd[2]))

            for inter in interfaces:
                changetype = False
                ips = IP.objects.filter(interface=inter.id)
                if inter.mac_addr != request.POST['mac_addr%d' % inter.id]:
                    text = do_log(text, 'mac_addr', inter.mac_addr, request.POST['mac_addr%d' % inter.id])
                    inter.mac_addr = request.POST['mac_addr%d' % inter.id].lower().replace('-',':')
                if inter.hdwr_type != request.POST['hdwr_type%d' % inter.id]:
                    oldtype = inter.hdwr_type
                    text = do_log(text, 'hdwr_type', oldtype, request.POST['hdwr_type%d' % inter.id])
                    inter.hdwr_type = request.POST['hdwr_type%d' % inter.id]
                    changetype = True
                if (('dhcp%d' % inter.id) in request.POST and not inter.dhcp or
                    not ('dhcp%d' % inter.id) in request.POST and inter.dhcp):
                    text = do_log(text, 'dhcp', inter.dhcp, int(not inter.dhcp))
                    inter.dhcp = not inter.dhcp
                for ip in ips:
                    names = ip.name_set.all()
                    if not ip.ip_addr == request.POST['ip_addr%d' % ip.id]:
                        oldip = ip.ip_addr
                        oldsubnet = oldip.split(".")[2]
                        ip.ip_addr = request.POST['ip_addr%d' % ip.id]
                        ip.save()
                        text = do_log(text, 'ip_addr', oldip, ip.ip_addr)
                        for name in names:
                            if name.name.split(".")[0].endswith('-%s' % oldsubnet):
                                name.name = name.name.replace('-%s' % oldsubnet, '-%s' % ip.ip_addr.split(".")[2])
                                name.save()
                    if changetype:
                        for name in names:
                            if name.name.split(".")[0].endswith('-%s' % oldtype):
                                name.name = name.name.replace('-%s' % oldtype, '-%s' % inter.hdwr_type)
                                name.save()
                    if changename:
                        for name in names:
                            if name.name.startswith(oldhostname):
                                name.name = name.name.replace(oldhostname, host.hostname.split(".")[0])
                                name.save()
                if request.POST['%dip_addr' % inter.id]:
                    mx, created = MX.objects.get_or_create(priority=settings.PRIORITY, mx=settings.DEFAULT_MX)
                    if created:
                        mx.save()
                    new_ip = IP(interface=inter, ip_addr=request.POST['%dip_addr' % inter.id])
                    new_ip.save()
                    text = do_log(text, '*new*', 'ip_addr', new_ip.ip_addr)
                    new_name = "-".join([host.hostname.split(".")[0],
                                         new_ip.ip_addr.split(".")[2]])
                    new_name += "." + host.hostname.split(".", 1)[1]
                    name = Name(ip=new_ip, name=new_name,
                                dns_view='global', only=False)
                    name.save()
                    name.mxs.add(mx)
                    new_name = "-".join([host.hostname.split(".")[0],
                                         inter.hdwr_type])
                    new_name += "." + host.hostname.split(".", 1)[1]
                    name = Name(ip=new_ip, name=new_name,
                                dns_view='global', only=False)
                    name.save()
                    name.mxs.add(mx)
                    name = Name(ip=new_ip, name=host.hostname,
                                dns_view='global', only=False)
                    name.save()
                    name.mxs.add(mx)
                inter.save()
            if request.POST['mac_addr_new']:
                new_inter = Interface(host=host,
                                      mac_addr=request.POST['mac_addr_new'].lower().replace('-',':'),
                                      hdwr_type=request.POST['hdwr_type_new'],
                                      dhcp=request.POST['dhcp_new'])
                text = do_log(text, '*new*', 'mac_addr', new_inter.mac_addr)
                new_inter.save()
            if request.POST['mac_addr_new'] and request.POST['ip_addr_new']:
                mx, created = MX.objects.get_or_create(priority=settings.PRIORITY, mx=settings.DEFAULT_MX)
                if created:
                    mx.save()
                new_ip = IP(interface=new_inter, ip_addr=request.POST['ip_addr_new'])
                new_ip.save()
                text = do_log(text, '*new*', 'ip_addr', new_ip.ip_addr)
                new_name = "-".join([host.hostname.split(".")[0],
                                     new_ip.ip_addr.split(".")[2]])
                new_name += "." + host.hostname.split(".", 1)[1]
                name = Name(ip=new_ip, name=new_name,
                            dns_view='global', only=False)
                name.save()
                name.mxs.add(mx)
                new_name = "-".join([host.hostname.split(".")[0],
                                     new_inter.hdwr_type])
                new_name += "." + host.hostname.split(".", 1)[1]
                name = Name(ip=new_ip, name=new_name,
                            dns_view='global', only=False)
                name.save()
                name.mxs.add(mx)
                name = Name(ip=new_ip, name=host.hostname,
                            dns_view='global', only=False)
                name.save()
                name.mxs.add(mx)
            if request.POST['ip_addr_new'] and not request.POST['mac_addr_new']:
                mx, created = MX.objects.get_or_create(priority=settings.PRIORITY, mx=settings.DEFAULT_MX)
                if created:
                    mx.save()
                new_inter = Interface(host=host, mac_addr="",
                                      hdwr_type=request.POST['hdwr_type_new'],
                                      dhcp=False)
                new_inter.save()
                new_ip = IP(interface=new_inter, ip_addr=request.POST['ip_addr_new'])
                new_ip.save()
                text = do_log(text, '*new*', 'ip_addr', new_ip.ip_addr)
                new_name = "-".join([host.hostname.split(".")[0],
                                     new_ip.ip_addr.split(".")[2]])
                new_name += "." + host.hostname.split(".", 1)[1]
                name = Name(ip=new_ip, name=new_name,
                            dns_view='global', only=False)
                name.save()
                name.mxs.add(mx)
                new_name = "-".join([host.hostname.split(".")[0],
                                     new_inter.hdwr_type])
                new_name += "." + host.hostname.split(".", 1)[1]
                name = Name(ip=new_ip, name=new_name,
                            dns_view='global', only=False)
                name.save()
                name.mxs.add(mx)
                name = Name(ip=new_ip, name=host.hostname,
                            dns_view='global', only=False)
                name.save()
                name.mxs.add(mx)
            if text:
                log = Log(hostname=host.hostname, log=text)
                log.save()
            host.save()
            return HttpResponseRedirect('/hostbase/%s/' % host.id)
        else:
            return render_to_response('errors.html',
                                      {'failures': errors,
                                       'logged_in': request.session.get('_auth_user_id', False)},
                                       context_instance = RequestContext(request))
    else:
        host = Host.objects.get(id=host_id)
        interfaces = []
        for interface in host.interface_set.all():
            interfaces.append([interface, interface.ip_set.all()])
        return render_to_response('edit.html',
                                  {'host': host,
                                   'interfaces': interfaces,
                                   'TYPE_CHOICES': Interface.TYPE_CHOICES,
                                   'logged_in': request.session.get('_auth_user_id', False)},
                                   context_instance = RequestContext(request))

def confirm(request, item, item_id, host_id=None, name_id=None, zone_id=None):
    """Asks if the user is sure he/she wants to remove an item"""
    if 'sub' in request.GET:
        if item == 'interface':
            for ip in Interface.objects.get(id=item_id).ip_set.all():
                for name in ip.name_set.all():
                    name.cname_set.all().delete()
                ip.name_set.all().delete()
            Interface.objects.get(id=item_id).ip_set.all().delete()
            Interface.objects.get(id=item_id).delete()
        elif item=='ip':
            for name in IP.objects.get(id=item_id).name_set.all():
                name.cname_set.all().delete()
            IP.objects.get(id=item_id).name_set.all().delete()
            IP.objects.get(id=item_id).delete()
        elif item=='cname':
            CName.objects.get(id=item_id).delete()
        elif item=='mx':
            mx = MX.objects.get(id=item_id)
            Name.objects.get(id=name_id).mxs.remove(mx)
        elif item=='name':
            Name.objects.get(id=item_id).cname_set.all().delete()
            Name.objects.get(id=item_id).delete()
        elif item=='nameserver':
            nameserver = Nameserver.objects.get(id=item_id)
            Zone.objects.get(id=zone_id).nameservers.remove(nameserver)
        elif item=='zonemx':
            mx = MX.objects.get(id=item_id)
            Zone.objects.get(id=zone_id).mxs.remove(mx)
        elif item=='address':
            address = ZoneAddress.objects.get(id=item_id)
            Zone.objects.get(id=zone_id).addresses.remove(address)
        if item == 'cname' or item == 'mx' or item == 'name':
            return HttpResponseRedirect('/hostbase/%s/dns/edit' % host_id)
        elif item == 'nameserver' or item == 'zonemx' or item == 'address':
            return HttpResponseRedirect('/hostbase/zones/%s/edit' % zone_id)
        else:
            return HttpResponseRedirect('/hostbase/%s/edit' % host_id)
    else:
        interface = None
        ips = []
        names = []
        cnames = []
        mxs = []
        zonemx = None
        nameserver = None
        address = None
        if item == 'interface':
            interface = Interface.objects.get(id=item_id)
            ips = interface.ip_set.all()
            for ip in ips:
                for name in ip.name_set.all():
                    names.append((ip.id, name))
                    for cname in name.cname_set.all():
                        cnames.append((name.id, cname))
                    for mx in name.mxs.all():
                        mxs.append((name.id, mx))
        elif item=='ip':
            ips = [IP.objects.get(id=item_id)]
            for name in ips[0].name_set.all():
                names.append((ips[0].id, name))
                for cname in name.cname_set.all():
                    cnames.append((name.id, cname))
                for mx in name.mxs.all():
                    mxs.append((name.id, mx))
        elif item=='name':
            names = [Name.objects.get(id=item_id)]
            for cname in names[0].cname_set.all():
                cnames.append((names[0].id, cname))
            for mx in names[0].mxs.all():
                mxs.append((names[0].id, mx))
        elif item=='cname':
            cnames = [CName.objects.get(id=item_id)]
        elif item=='mx':
            mxs = [MX.objects.get(id=item_id)]
        elif item=='zonemx':
            zonemx = MX.objects.get(id=item_id)
        elif item=='nameserver':
            nameserver = Nameserver.objects.get(id=item_id)
        elif item=='address':
            address = ZoneAddress.objects.get(id=item_id)
        return render_to_response('confirm.html',
                                  {'interface': interface,
                                   'ips': ips,
                                   'names': names,
                                   'cnames': cnames,
                                   'id': item_id,
                                   'type': item,
                                   'host_id': host_id,
                                   'mxs': mxs,
                                   'zonemx': zonemx,
                                   'nameserver': nameserver,
                                   'address': address,
                                   'zone_id': zone_id,
                                   'logged_in': request.session.get('_auth_user_id', False)},
                                   context_instance = RequestContext(request))

def dnsedit(request, host_id):
    """Edits specific DNS information
    Data is validated before committed to the database"""
    text = ''
    if 'sub' in request.GET:
        hostdata = gethostdata(host_id, True)
        for ip in hostdata['names']:
            ipaddr = IP.objects.get(id=ip)
            ipaddrstr = ipaddr.__str__()
            for name in hostdata['cnames']:
                for cname in hostdata['cnames'][name]:
                    if regex.host.match(request.POST['cname%d' % cname.id]):
                        text = do_log(text, 'cname', cname.cname, request.POST['cname%d' % cname.id])
                        cname.cname = request.POST['cname%d' % cname.id]
                        cname.save()
            for name in hostdata['mxs']:
                for mx in hostdata['mxs'][name]:
                    if (mx.priority != request.POST['priority%d' % mx.id] and mx.mx != request.POST['mx%d' % mx.id]):
                        text = do_log(text, 'mx', ' '.join([str(mx.priority), str(mx.mx)]),
                                      ' '.join([request.POST['priority%d' % mx.id], request.POST['mx%d' % mx.id]]))
                        nameobject = Name.objects.get(id=name)
                        nameobject.mxs.remove(mx)
                        newmx, created = MX.objects.get_or_create(priority=request.POST['priority%d' % mx.id], mx=request.POST['mx%d' % mx.id])
                        if created:
                            newmx.save()
                        nameobject.mxs.add(newmx)
                        nameobject.save()
            for name in hostdata['names'][ip]:
                name.name = request.POST['name%d' % name.id]
                name.dns_view = request.POST['dns_view%d' % name.id]
                if (request.POST['%dcname' % name.id] and
                regex.host.match(request.POST['%dcname' % name.id])):
                    cname = CName(name=name,
                                  cname=request.POST['%dcname' % name.id])
                    text = do_log(text, '*new*', 'cname', cname.cname)
                    cname.save()
                if (request.POST['%dpriority' % name.id] and
                    request.POST['%dmx' % name.id]):
                    mx, created = MX.objects.get_or_create(priority=request.POST['%dpriority' % name.id],
                            mx=request.POST['%dmx' % name.id])
                    if created:
                        mx.save()
                        text = do_log(text, '*new*', 'mx',
                                      ' '.join([request.POST['%dpriority' % name.id],
                                                request.POST['%dmx' % name.id]]))
                    name.mxs.add(mx)
                name.save()
            if request.POST['%sname' % ipaddrstr]:
                name = Name(ip=ipaddr,
                            dns_view=request.POST['%sdns_view' % ipaddrstr],
                            name=request.POST['%sname' % ipaddrstr], only=False)
                text = do_log(text, '*new*', 'name', name.name)
                name.save()
                if (request.POST['%scname' % ipaddrstr] and
                regex.host.match(request.POST['%scname' % ipaddrstr])):
                    cname = CName(name=name,
                                  cname=request.POST['%scname' % ipaddrstr])
                    text = do_log(text, '*new*', 'cname', cname.cname)
                    cname.save()
                if (request.POST['%smx' % ipaddrstr] and
                    request.POST['%spriority' % ipaddrstr]):
                    mx, created = MX.objects.get_or_create(priority=request.POST['%spriority' % ipaddrstr],
                            mx=request.POST['%smx' % ipaddrstr])
                    if created:
                        mx.save()
                    text = do_log(text, '*new*', 'mx',
                                  ' '.join([request.POST['%spriority' % ipaddrstr], request.POST['%smx' % ipaddrstr]]))
                    name.mxs.add(mx)
        if text:
            log = Log(hostname=hostdata['host'].hostname, log=text)
            log.save()
        return HttpResponseRedirect('/hostbase/%s/dns' % host_id)
    else:
        host = Host.objects.get(id=host_id)
        ips = []
        info = []
        cnames = []
        mxs = []
        interfaces = host.interface_set.all()
        for interface in host.interface_set.all():
            ips.extend(interface.ip_set.all())
        for ip in ips:
            info.append([ip, ip.name_set.all()])
            for name in ip.name_set.all():
                cnames.extend(name.cname_set.all())
                mxs.append((name.id, name.mxs.all()))
        return render_to_response('dnsedit.html',
                                  {'host': host,
                                   'info': info,
                                   'cnames': cnames,
                                   'mxs': mxs,
                                   'request': request,
                                   'interfaces': interfaces,
                                   'DNS_CHOICES': Name.DNS_CHOICES,
                                   'logged_in': request.session.get('_auth_user_id', False)},
                                   context_instance = RequestContext(request))

def new(request):
    """Function for creating a new host in hostbase
    Data is validated before committed to the database"""
    if 'sub' in request.GET:
        try:
            Host.objects.get(hostname=request.POST['hostname'].lower())
            return render_to_response('errors.html',
                                      {'failures': ['%s already exists in hostbase' % request.POST['hostname']],
                                       'logged_in': request.session.get('_auth_user_id', False)},
                                       context_instance = RequestContext(request))
        except:
            pass
        if not validate(request, True):
            if not request.POST['ip_addr_new'] and not request.POST['ip_addr_new2']:
                return render_to_response('errors.html',
                                          {'failures': ['ip_addr: You must enter an ip address'],
                                          'logged_in': request.session.get('_auth_user_id', False)},
                                          context_instance = RequestContext(request))
            host = Host()
            # this is the stuff that validate() should take care of
            # examine the check boxes for any changes
            host.outbound_smtp = 'outbound_smtp' in request.POST
            for attrib in attribs:
                if attrib in request.POST:
                    host.__dict__[attrib] = request.POST[attrib].lower()
            if 'comments' in request.POST:
                host.comments = request.POST['comments']
            if 'expiration_date' in request.POST:
#                ymd = request.POST['expiration_date'].split("-")
#                host.__dict__['expiration_date'] = date(int(ymd[0]), int(ymd[1]), int(ymd[2]))
                host.__dict__['expiration_date'] = date(2000, 1, 1)
            host.status = 'active'
            host.save()
        else:
            return render_to_response('errors.html',
                                      {'failures': validate(request, True),
                                       'logged_in': request.session.get('_auth_user_id', False)},
                                       context_instance = RequestContext(request))

        if request.POST['mac_addr_new']:
            new_inter = Interface(host=host,
                                  mac_addr = request.POST['mac_addr_new'].lower().replace('-',':'),
                                  hdwr_type = request.POST['hdwr_type_new'],
                                  dhcp = 'dhcp_new' in request.POST)
            new_inter.save()
        if request.POST['mac_addr_new'] and request.POST['ip_addr_new']:
            new_ip = IP(interface=new_inter, ip_addr=request.POST['ip_addr_new'])
# Change all this things. Use a "post_save" signal handler for model Host to create all sociate models
# and use a generi view.
            new_ip.save()
            mx, created = MX.objects.get_or_create(priority=settings.PRIORITY, mx=settings.DEFAULT_MX)
            if created:
                mx.save()
            new_name = "-".join([host.hostname.split(".")[0],
                                 new_ip.ip_addr.split(".")[2]])
            new_name += "." + host.hostname.split(".", 1)[1]
            name = Name(ip=new_ip, name=new_name, dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
            new_name = "-".join([host.hostname.split(".")[0],
                                 new_inter.hdwr_type])
            new_name += "." + host.hostname.split(".", 1)[1]
            name = Name(ip=new_ip, name=new_name,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
            name = Name(ip=new_ip, name=host.hostname,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
        if request.POST['ip_addr_new'] and not request.POST['mac_addr_new']:
            new_inter = Interface(host=host,
                                  mac_addr="",
                                  hdwr_type=request.POST['hdwr_type_new'],
                                  dhcp=False)
            new_inter.save()
            new_ip = IP(interface=new_inter, ip_addr=request.POST['ip_addr_new'])
            new_ip.save()
            mx, created = MX.objects.get_or_create(priority=settings.PRIORITY, mx=settings.DEFAULT_MX)
            if created:
                mx.save()
            new_name = "-".join([host.hostname.split(".")[0],
                                 new_ip.ip_addr.split(".")[2]])
            new_name += "." + host.hostname.split(".", 1)[1]
            name = Name(ip=new_ip, name=new_name,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
            new_name = "-".join([host.hostname.split(".")[0],
                                 new_inter.hdwr_type])
            new_name += "." + host.hostname.split(".", 1)[1]
            name = Name(ip=new_ip, name=new_name,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
            name = Name(ip=new_ip, name=host.hostname,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
        if request.POST['mac_addr_new2']:
            new_inter = Interface(host=host,
                                  mac_addr = request.POST['mac_addr_new2'].lower().replace('-',':'),
                                  hdwr_type = request.POST['hdwr_type_new2'],
                                  dhcp = 'dhcp_new2' in request.POST)
            new_inter.save()
        if request.POST['mac_addr_new2'] and request.POST['ip_addr_new2']:
            new_ip = IP(interface=new_inter, ip_addr=request.POST['ip_addr_new2'])
            new_ip.save()
            mx, created = MX.objects.get_or_create(priority=settings.PRIORITY, mx=settings.DEFAULT_MX)
            if created:
                mx.save()
            new_name = "-".join([host.hostname.split(".")[0],
                                 new_ip.ip_addr.split(".")[2]])
            new_name += "." + host.hostname.split(".", 1)[1]
            name = Name(ip=new_ip, name=new_name,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
            new_name = "-".join([host.hostname.split(".")[0],
                                 new_inter.hdwr_type])
            new_name += "." + host.hostname.split(".", 1)[1]
            name = Name(ip=new_ip, name=new_name,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
            name = Name(ip=new_ip, name=host.hostname,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
        if request.POST['ip_addr_new2'] and not request.POST['mac_addr_new2']:
            new_inter = Interface(host=host,
                                  mac_addr="",
                                  hdwr_type=request.POST['hdwr_type_new2'],
                                  dhcp=False)
            new_inter.save()
            new_ip = IP(interface=new_inter, ip_addr=request.POST['ip_addr_new2'])
            new_ip.save()
            mx, created = MX.objects.get_or_create(priority=settings.PRIORITY, mx=settings.DEFAULT_MX)
            if created:
                mx.save()
            new_name = "-".join([host.hostname.split(".")[0],
                                 new_ip.ip_addr.split(".")[2]])
            new_name += "." + host.hostname.split(".", 1)[1]
            name = Name(ip=new_ip, name=new_name,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
            new_name = "-".join([host.hostname.split(".")[0],
                                 new_inter.hdwr_type])
            new_name += "." + host.hostname.split(".", 1)[1]
            name = Name(ip=new_ip, name=new_name,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
            name = Name(ip=new_ip, name=host.hostname,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
        host.save()
        return HttpResponseRedirect('/hostbase/%s/' % host.id)
    else:
        return render_to_response('new.html',
                                  {'TYPE_CHOICES': Interface.TYPE_CHOICES,
                                   'NETGROUP_CHOICES': Host.NETGROUP_CHOICES,
                                   'CLASS_CHOICES': Host.CLASS_CHOICES,
                                   'SUPPORT_CHOICES': Host.SUPPORT_CHOICES,
                                   'WHATAMI_CHOICES': Host.WHATAMI_CHOICES,
                                   'logged_in': request.session.get('_auth_user_id', False)},
                                   context_instance = RequestContext(request))

def copy(request, host_id):
    """Function for creating a new host in hostbase
    Data is validated before committed to the database"""
    if 'sub' in request.GET:
        try:
            Host.objects.get(hostname=request.POST['hostname'].lower())
            return render_to_response('errors.html',
                                      {'failures': ['%s already exists in hostbase' % request.POST['hostname']],
                                       'logged_in': request.session.get('_auth_user_id', False)},
                                       context_instance = RequestContext(request))
        except:
            pass
        if not validate(request, True):
            if not request.POST['ip_addr_new'] and not request.POST['ip_addr_new2']:
                return render_to_response('errors.html',
                                          {'failures': ['ip_addr: You must enter an ip address'],
                                          'logged_in': request.session.get('_auth_user_id', False)},
                                          context_instance = RequestContext(request))
            host = Host()
            # this is the stuff that validate() should take care of
            # examine the check boxes for any changes
            host.outbound_smtp = 'outbound_smtp' in request.POST
            for attrib in attribs:
                if attrib in request.POST:
                    host.__dict__[attrib] = request.POST[attrib].lower()
            if 'comments' in request.POST:
                host.comments = request.POST['comments']
            if 'expiration_date' in request.POST:
#                ymd = request.POST['expiration_date'].split("-")
#                host.__dict__['expiration_date'] = date(int(ymd[0]), int(ymd[1]), int(ymd[2]))
                host.__dict__['expiration_date'] = date(2000, 1, 1)
            host.status = 'active'
            host.save()
        else:
            return render_to_response('errors.html',
                                      {'failures': validate(request, True),
                                       'logged_in': request.session.get('_auth_user_id', False)},
                                       context_instance = RequestContext(request))

        if request.POST['mac_addr_new']:
            new_inter = Interface(host=host,
                                  mac_addr = request.POST['mac_addr_new'].lower().replace('-',':'),
                                  hdwr_type = request.POST['hdwr_type_new'],
                                  dhcp = 'dhcp_new' in request.POST)
            new_inter.save()
        if request.POST['mac_addr_new'] and request.POST['ip_addr_new']:
            new_ip = IP(interface=new_inter, ip_addr=request.POST['ip_addr_new'])
            new_ip.save()
            mx, created = MX.objects.get_or_create(priority=settings.PRIORITY, mx=settings.DEFAULT_MX)
            if created:
                mx.save()
            new_name = "-".join([host.hostname.split(".")[0],
                                 new_ip.ip_addr.split(".")[2]])
            new_name += "." + host.hostname.split(".", 1)[1]
            name = Name(ip=new_ip, name=new_name, dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
            new_name = "-".join([host.hostname.split(".")[0],
                                 new_inter.hdwr_type])
            new_name += "." + host.hostname.split(".", 1)[1]
            name = Name(ip=new_ip, name=new_name,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
            name = Name(ip=new_ip, name=host.hostname,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
        if request.POST['ip_addr_new'] and not request.POST['mac_addr_new']:
            new_inter = Interface(host=host,
                                  mac_addr="",
                                  hdwr_type=request.POST['hdwr_type_new'],
                                  dhcp=False)
            new_inter.save()
            new_ip = IP(interface=new_inter, ip_addr=request.POST['ip_addr_new'])
            new_ip.save()
            mx, created = MX.objects.get_or_create(priority=settings.PRIORITY, mx=settings.DEFAULT_MX)
            if created:
                mx.save()
            new_name = "-".join([host.hostname.split(".")[0],
                                 new_ip.ip_addr.split(".")[2]])
            new_name += "." + host.hostname.split(".", 1)[1]
            name = Name(ip=new_ip, name=new_name,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
            new_name = "-".join([host.hostname.split(".")[0],
                                 new_inter.hdwr_type])
            new_name += "." + host.hostname.split(".", 1)[1]
            name = Name(ip=new_ip, name=new_name,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
            name = Name(ip=new_ip, name=host.hostname,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
        if request.POST['mac_addr_new2']:
            new_inter = Interface(host=host,
                                  mac_addr = request.POST['mac_addr_new2'].lower().replace('-',':'),
                                  hdwr_type = request.POST['hdwr_type_new2'],
                                  dhcp = 'dhcp_new2' in request.POST)
            new_inter.save()
        if request.POST['mac_addr_new2'] and request.POST['ip_addr_new2']:
            new_ip = IP(interface=new_inter, ip_addr=request.POST['ip_addr_new2'])
            new_ip.save()
            mx, created = MX.objects.get_or_create(priority=settings.PRIORITY, mx=settings.DEFAULT_MX)
            if created:
                mx.save()
            new_name = "-".join([host.hostname.split(".")[0],
                                 new_ip.ip_addr.split(".")[2]])
            new_name += "." + host.hostname.split(".", 1)[1]
            name = Name(ip=new_ip, name=new_name,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
            new_name = "-".join([host.hostname.split(".")[0],
                                 new_inter.hdwr_type])
            new_name += "." + host.hostname.split(".", 1)[1]
            name = Name(ip=new_ip, name=new_name,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
            name = Name(ip=new_ip, name=host.hostname,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
        if request.POST['ip_addr_new2'] and not request.POST['mac_addr_new2']:
            new_inter = Interface(host=host,
                                  mac_addr="",
                                  hdwr_type=request.POST['hdwr_type_new2'],
                                  dhcp=False)
            new_inter.save()
            new_ip = IP(interface=new_inter, ip_addr=request.POST['ip_addr_new2'])
            new_ip.save()
            mx, created = MX.objects.get_or_create(priority=settings.PRIORITY, mx=settings.DEFAULT_MX)
            if created:
                mx.save()
            new_name = "-".join([host.hostname.split(".")[0],
                                 new_ip.ip_addr.split(".")[2]])
            new_name += "." + host.hostname.split(".", 1)[1]
            name = Name(ip=new_ip, name=new_name,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
            new_name = "-".join([host.hostname.split(".")[0],
                                 new_inter.hdwr_type])
            new_name += "." + host.hostname.split(".", 1)[1]
            name = Name(ip=new_ip, name=new_name,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
            name = Name(ip=new_ip, name=host.hostname,
                        dns_view='global', only=False)
            name.save()
            name.mxs.add(mx)
        host.save()
        return HttpResponseRedirect('/hostbase/%s/' % host.id)
    else:
        host = Host.objects.get(id=host_id)
        return render_to_response('copy.html',
                                  {'host': host,
                                   'TYPE_CHOICES': Interface.TYPE_CHOICES,
                                   'NETGROUP_CHOICES': Host.NETGROUP_CHOICES,
                                   'CLASS_CHOICES': Host.CLASS_CHOICES,
                                   'SUPPORT_CHOICES': Host.SUPPORT_CHOICES,
                                   'WHATAMI_CHOICES': Host.WHATAMI_CHOICES,
                                   'logged_in': request.session.get('_auth_user_id', False)},
                                   context_instance = RequestContext(request))

# FIXME: delete all this things in a signal handler "pre_delete"
#def remove(request, host_id):
#    host = Host.objects.get(id=host_id)
#    if 'sub' in request:
#        for interface in host.interface_set.all():
#            for ip in interface.ip_set.all():
#                for name in ip.name_set.all():
#                    name.cname_set.all().delete()
#                ip.name_set.all().delete()
#            interface.ip_set.all().delete()
#            interface.delete()
#        host.delete()

def validate(request, new=False, host_id=None):
    """Function for checking form data"""
    failures = []
    if (request.POST['expiration_date']
        and regex.date.match(request.POST['expiration_date'])):
        try:
            (year, month, day) = request.POST['expiration_date'].split("-")
            date(int(year), int(month), int(day))
        except (ValueError):
            failures.append('expiration_date')
    elif request.POST['expiration_date']:
        failures.append('expiration_date')

    if not (request.POST['hostname']
            and regex.host.match(request.POST['hostname'])):
        failures.append('hostname')

##     if not regex.printq.match(request.POST['printq']) and request.POST['printq']:
##         failures.append('printq')

##     if not regex.user.match(request.POST['primary_user']):
##         failures.append('primary_user')

##     if (not regex.user.match(request.POST['administrator'])
##         and request.POST['administrator']):
##         failures.append('administrator')

##     if not (request.POST['location']
##             and regex.location.match(request.POST['location'])):
##         failures.append('location')

    if new:
        if (not regex.macaddr.match(request.POST['mac_addr_new'])
            and request.POST['mac_addr_new']):
            failures.append('mac_addr (#1)')
        if ((request.POST['mac_addr_new'] or request.POST['ip_addr_new']) and
            not 'hdwr_type_new' in request.REQUEST):
            failures.append('hdwr_type (#1)')
        if ((request.POST['mac_addr_new2'] or request.POST['ip_addr_new2']) and
            not 'hdwr_type_new2' in request.REQUEST):
            failures.append('hdwr_type (#2)')

        if (not regex.macaddr.match(request.POST['mac_addr_new2'])
            and request.POST['mac_addr_new2']):
            failures.append('mac_addr (#2)')

        if (not regex.ipaddr.match(request.POST['ip_addr_new'])
            and request.POST['ip_addr_new']):
            failures.append('ip_addr (#1)')
        if (not regex. ipaddr.match(request.POST['ip_addr_new2'])
            and request.POST['ip_addr_new2']):
            failures.append('ip_addr (#2)')

        [failures.append('ip_addr (#1)') for number in
         request.POST['ip_addr_new'].split(".")
         if number.isdigit() and int(number) > 255
         and 'ip_addr (#1)' not in failures]
        [failures.append('ip_addr (#2)') for number in
         request.POST['ip_addr_new2'].split(".")
         if number.isdigit() and int(number) > 255
         and 'ip_addr (#2)' not in failures]

    elif host_id:
        interfaces = Interface.objects.filter(host=host_id)
        for interface in interfaces:
            if (not regex.macaddr.match(request.POST['mac_addr%d' % interface.id])
                and request.POST['mac_addr%d' % interface.id]):
                failures.append('mac_addr (%s)' % request.POST['mac_addr%d' % interface.id])
            for ip in interface.ip_set.all():
                if not regex.ipaddr.match(request.POST['ip_addr%d' % ip.id]):
                    failures.append('ip_addr (%s)' % request.POST['ip_addr%d' % ip.id])
                [failures.append('ip_addr (%s)' % request.POST['ip_addr%d' % ip.id])
                 for number in request.POST['ip_addr%d' % ip.id].split(".")
                 if (number.isdigit() and int(number) > 255 and
                     'ip_addr (%s)' % request.POST['ip_addr%d' % ip.id] not in failures)]
            if (request.POST['%dip_addr' % interface.id]
                and not regex.ipaddr.match(request.POST['%dip_addr' % interface.id])):
                failures.append('ip_addr (%s)' % request.POST['%dip_addr' % interface.id])
        if (request.POST['mac_addr_new']
            and not regex.macaddr.match(request.POST['mac_addr_new'])):
            failures.append('mac_addr (%s)' % request.POST['mac_addr_new'])
        if (request.POST['ip_addr_new']
            and not regex.ipaddr.match(request.POST['ip_addr_new'])):
            failures.append('ip_addr (%s)' % request.POST['ip_addr_new'])

    if not failures:
        return 0
    return failures

def do_log(text, attribute, previous, new):
    if previous != new:
        text += "%-20s%-20s -> %s\n" % (attribute, previous, new)
    return text

## login required stuff
## uncomment the views below that you would like to restrict access to

## uncomment the lines below this point to restrict access to pages that modify the database
## anonymous users can still view data in Hostbase

edit = login_required(edit)
confirm = login_required(confirm)
dnsedit = login_required(dnsedit)
new = login_required(new)
copy = login_required(copy)
#remove = login_required(remove)
#zoneedit = login_required(zoneedit)
#zonenew = login_required(zonenew)

## uncomment the lines below this point to restrict access to all of hostbase

## search = login_required(search)
## look = login_required(look)
## dns = login_required(dns)
## zones = login_required(zones)
## zoneview = login_required(zoneview)


########NEW FILE########
__FILENAME__ = ldapauth
"""
Checks with LDAP (ActiveDirectory) to see if the current user is an LDAP(AD)
user, and returns a subset of the user's profile that is needed by Argonne/CIS
to set user level privleges in Django
"""

import os
import ldap


class LDAPAUTHError(Exception):
    """LDAPAUTHError is raised when somehting goes boom."""
    pass


class ldapauth(object):
    group_test = False
    check_member_of = os.environ['LDAP_CHECK_MBR_OF_GRP']
    securitylevel = 0
    distinguishedName = None
    sAMAccountName = None
    telephoneNumber = None
    title = None
    memberOf = None
    department = None  # this will be a list
    mail = None
    extensionAttribute1 = None  # badgenumber
    badge_no = None

    def __init__(self, login, passwd):
        """get username (if using ldap as auth the
        apache env var REMOTE_USER should be used)
        from username get user profile from AD/LDAP
        """
        #p = self.user_profile(login,passwd)
        d = self.user_dn(login)  # success, distname
        print(d[1])
        if d[0] == 'success':
            pass
            p = self.user_bind(d[1], passwd)
            if p[0] == 'success':
                #parse results
                parsed = self.parse_results(p[2])
                print(self.department)
                self.group_test = self.member_of()
                securitylevel = self.security_level()
                print("ACCESS LEVEL: " + str(securitylevel))
            else:
                raise LDAPAUTHError(p[2])
        else:
            raise LDAPAUTHError(p[2])

    def user_profile(self, login, passwd=None):
        """NOT USED RIGHT NOW"""
        ldap_login = "CN=%s" % login
        svc_acct = os.environ['LDAP_SVC_ACCT_NAME']
        svc_pass = os.environ['LDAP_SVC_ACCT_PASS']
        #svc_acct = 'CN=%s,DC=anl,DC=gov' % login
        #svc_pass = passwd

        search_pth = os.environ['LDAP_SEARCH_PTH']

        try:
            conn = ldap.initialize(os.environ['LDAP_URI'])
            conn.bind(svc_acct, svc_pass, ldap.AUTH_SIMPLE)
            result_id = conn.search(search_pth,
                                    ldap.SCOPE_SUBTREE,
                                    ldap_login,
                                    None)
            result_type, result_data = conn.result(result_id, 0)
            return ('success', 'User profile found', result_data,)
        except ldap.LDAPError:
            e = sys.exc_info()[1]
            #connection failed
            return ('error', 'LDAP connect failed', e,)

    def user_bind(self, distinguishedName, passwd):
        """Binds to LDAP Server"""
        search_pth = os.environ['LDAP_SEARCH_PTH']
        try:
            conn = ldap.initialize(os.environ['LDAP_URI'])
            conn.bind(distinguishedName, passwd, ldap.AUTH_SIMPLE)
            cn = distinguishedName.split(",")
            result_id = conn.search(search_pth,
                                    ldap.SCOPE_SUBTREE,
                                    cn[0],
                                    None)
            result_type, result_data = conn.result(result_id, 0)
            return ('success', 'User profile found', result_data,)
        except ldap.LDAPError:
            e = sys.exc_info()[1]
            #connection failed
            return ('error', 'LDAP connect failed', e,)

    def user_dn(self, cn):
        """Uses Service Account to get distinguishedName"""
        ldap_login = "CN=%s" % cn
        svc_acct = os.environ['LDAP_SVC_ACCT_NAME']
        svc_pass = os.environ['LDAP_SVC_ACCT_PASS']
        search_pth = os.environ['LDAP_SEARCH_PTH']

        try:
            conn = ldap.initialize(os.environ['LDAP_URI'])
            conn.bind(svc_acct, svc_pass, ldap.AUTH_SIMPLE)
            result_id = conn.search(search_pth,
                                    ldap.SCOPE_SUBTREE,
                                    ldap_login,
                                    None)
            result_type, result_data = conn.result(result_id, 0)
            raw_obj = result_data[0][1]
            distinguishedName = raw_obj['distinguishedName']
            return ('success', distinguishedName[0],)
        except ldap.LDAPError:
            e = sys.exc_info()[1]
            #connection failed
            return ('error', 'LDAP connect failed', e,)

    def parse_results(self, user_obj):
        """Clean up the huge ugly object handed to us in the LDAP query"""
        #user_obj is a list formatted like this:
        #[('LDAP_DN',{user_dict},),]
        try:
            raw_obj = user_obj[0][1]
            self.memberOf = raw_obj['memberOf']
            self.sAMAccountName = raw_obj['sAMAccountName'][0]
            self.distinguishedName = raw_obj['distinguishedName'][0]
            self.telephoneNumber = raw_obj['telephoneNumber'][0]
            self.title = raw_obj['title'][0]
            self.department = raw_obj['department'][0]
            self.mail = raw_obj['mail'][0]
            self.badge_no = raw_obj['extensionAttribute1'][0]
            self.email = raw_obj['extensionAttribute2'][0]
            display_name = raw_obj['displayName'][0].split(",")
            self.name_f = raw_obj['givenName'][0]
            self.name_l = display_name[0]
            self.is_staff = False
            self.is_superuser = False

            return
        except KeyError:
            e = sys.exc_info()[1]
            raise LDAPAUTHError("Portions of the LDAP User profile not present")

    def member_of(self):
        """See if this user is in our group that is allowed to login"""
        m = [g for g in self.memberOf if g == self.check_member_of]
        if len(m) == 1:
            return True
        else:
            return False

    def security_level(self):
        level = self.securitylevel

        user = os.environ['LDAP_GROUP_USER']
        m = [g for g in self.memberOf if g == user]
        if len(m) == 1:
            if level < 1:
                level = 1

        cspr = os.environ['LDAP_GROUP_SECURITY_LOW']
        m = [g for g in self.memberOf if g == cspr]
        if len(m) == 1:
            if level < 2:
                level = 2

        cspo = os.environ['LDAP_GROUP_SECURITY_HIGH']
        m = [g for g in self.memberOf if g == cspo]
        if len(m) == 1:
            if level < 3:
                level = 3

        admin = os.environ['LDAP_GROUP_ADMIN']
        m = [g for g in self.memberOf if g == admin]
        if len(m) == 1:
            if level < 4:
                level = 4

        return level

########NEW FILE########
__FILENAME__ = manage
#!/usr/bin/env python
from django.core.management import execute_manager
try:
    import settings # Assumed to be in the same directory.
except ImportError:
    import sys
    sys.stderr.write("Error: Can't find the file 'settings.py' in the directory containing %r. It appears you've customized things.\nYou'll have to run django-admin.py, passing it your settings module.\n(If the file settings.py does indeed exist, it's causing an ImportError somehow.)\n" % __file__)
    sys.exit(1)

if __name__ == "__main__":
    execute_manager(settings)

########NEW FILE########
__FILENAME__ = nisauth
"""Checks with NIS to see if the current user is in the support group"""
import os
import crypt, nis
from Bcfg2.Server.Hostbase.settings import AUTHORIZED_GROUP


class NISAUTHError(Exception):
    """NISAUTHError is raised when somehting goes boom."""
    pass

class nisauth(object):
    group_test = False
#    check_member_of = os.environ['LDAP_CHECK_MBR_OF_GRP']
    samAcctName = None
    distinguishedName = None
    sAMAccountName = None
    telephoneNumber = None
    title = None
    memberOf = None
    department = None #this will be a list
    mail = None
    extensionAttribute1 = None #badgenumber
    badge_no = None
    uid = None

    def __init__(self,login,passwd=None):
        """get user profile from NIS"""
        try:
            p = nis.match(login, 'passwd.byname').split(":")
        except:
            raise NISAUTHError('username')
        # check user password using crypt and 2 character salt from passwd file
        if p[1] == crypt.crypt(passwd, p[1][:2]):
            # check to see if user is in valid support groups
            # will have to include these groups in a settings file eventually
            if not login in nis.match(AUTHORIZED_GROUP, 'group.byname').split(':')[-1].split(',') and p[3] != nis.match(AUTHORIZED_GROUP, 'group.byname').split(':')[2]:
                raise NISAUTHError('group')
            self.uid = p[2]
        else:
            raise NISAUTHError('password')

########NEW FILE########
__FILENAME__ = regex
import re

date = re.compile('^[0-9]{4}-[0-9]{2}-[0-9]{2}$')
host = re.compile('^[a-z0-9-_]+(\.[a-z0-9-_]+)+$')
macaddr = re.compile('^[0-9abcdefABCDEF]{2}(:[0-9abcdefABCDEF]{2}){5}$|virtual')
ipaddr = re.compile('^[0-9]{1,3}(\.[0-9]{1,3}){3}$')

########NEW FILE########
__FILENAME__ = settings
import os.path
# Compatibility import
from Bcfg2.Compat import ConfigParser

PROJECT_ROOT = os.path.abspath(os.path.dirname(__file__))

c = ConfigParser.ConfigParser()
#This needs to be configurable one day somehow
c.read(['./bcfg2.conf'])

defaults = {'database_engine':'sqlite3',
            'database_name':'./dev.db',
            'database_user':'',
            'database_password':'',
            'database_host':'',
            'database_port':3306,
            'default_mx':'localhost',
            'priority':10,
            'authorized_group':'admins',
            }

if c.has_section('hostbase'):
    options = dict(c.items('hostbase'))
else:
    options = defaults

# Django settings for Hostbase project.
DEBUG = True
TEMPLATE_DEBUG = DEBUG
ADMINS = (
     ('Root', 'root'),
)
MANAGERS = ADMINS

# 'postgresql', 'mysql', 'sqlite3' or 'ado_mssql'.
DATABASE_ENGINE = options['database_engine']
# Or path to database file if using sqlite3.
DATABASE_NAME = options['database_name']
# Not used with sqlite3.
DATABASE_USER = options['database_user']
# Not used with sqlite3.
DATABASE_PASSWORD = options['database_password']
# Set to empty string for localhost. Not used with sqlite3.
DATABASE_HOST = options['database_host']
# Set to empty string for default. Not used with sqlite3.
DATABASE_PORT = int(options['database_port'])
# Local time zone for this installation. All choices can be found here:
# http://docs.djangoproject.com/en/dev/ref/settings/#time-zone
try:
    TIME_ZONE = c.get('statistics', 'time_zone')
except:
    TIME_ZONE = None

# enter the defauly MX record machines will get in Hostbase
# this setting may move elsewhere eventually
DEFAULT_MX = options['default_mx']
PRIORITY = int(options['priority'])

SESSION_EXPIRE_AT_BROWSER_CLOSE = True

# Uncomment a backend below if you would like to use it for authentication
AUTHENTICATION_BACKENDS = ('django.contrib.auth.backends.ModelBackend',
                           'Bcfg2.Server.Hostbase.backends.NISBackend',
                           #'Bcfg2.Server.Hostbase.backends.LDAPBacken',
                           )
# enter an NIS group name you'd like to give access to edit hostbase records
AUTHORIZED_GROUP = options['authorized_group']

#create login url area:
import django.contrib.auth
django.contrib.auth.LOGIN_URL = '/login'
# Absolute path to the directory that holds media.
# Example: "/home/media/media.lawrence.com/"
MEDIA_ROOT = os.path.join(PROJECT_ROOT, 'media')
# Just for development
SERVE_MEDIA = DEBUG

# Language code for this installation. All choices can be found here:
# http://www.w3.org/TR/REC-html40/struct/dirlang.html#langcodes
# http://blogs.law.harvard.edu/tech/stories/storyReader$15
LANGUAGE_CODE = 'en-us'
SITE_ID = 1
# URL that handles the media served from MEDIA_ROOT.
# Example: "http://media.lawrence.com"
MEDIA_URL = '/site_media/'
# URL prefix for admin media -- CSS, JavaScript and images. Make sure to use a
# trailing slash.
# Examples: "http://foo.com/media/", "/media/".
ADMIN_MEDIA_PREFIX = '/media/'
# Make this unique, and don't share it with anybody.
SECRET_KEY = '*%=fv=yh9zur&gvt4&*d#84o(cy^-*$ox-v1e9%32pzf2*qu#s'
# List of callables that know how to import templates from various sources.
TEMPLATE_LOADERS = (
    'django.template.loaders.filesystem.load_template_source',
    'django.template.loaders.app_directories.load_template_source',
#     'django.template.loaders.eggs.load_template_source',
)

TEMPLATE_CONTEXT_PROCESSORS = (
    "django.core.context_processors.auth",
    "django.core.context_processors.debug",
    "django.core.context_processors.i18n",
    "django.core.context_processors.request",
    "django.core.context_processors.media",
# Django development version.
#    "django.core.context_processors.csrf",
)


MIDDLEWARE_CLASSES = (
    'django.middleware.common.CommonMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.locale.LocaleMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.middleware.doc.XViewMiddleware',
)

ROOT_URLCONF = 'Bcfg2.Server.Hostbase.urls'

TEMPLATE_DIRS = (
    # Put strings here, like "/home/html/django_templates".
    # Always use forward slashes, even on Windows.
    '/usr/lib/python2.3/site-packages/Bcfg2/Server/Hostbase/hostbase/webtemplates',
    '/usr/lib/python2.4/site-packages/Bcfg2/Server/Hostbase/hostbase/webtemplates',
    '/usr/lib/python2.3/site-packages/Bcfg2/Server/Hostbase/templates',
    '/usr/lib/python2.4/site-packages/Bcfg2/Server/Hostbase/templates',
    '/usr/share/bcfg2/Hostbase/templates',
    os.path.join(PROJECT_ROOT, 'templates'),
    os.path.join(PROJECT_ROOT, 'hostbase/webtemplates'),
)

INSTALLED_APPS = (
    'django.contrib.admin',
    'django.contrib.admindocs',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.sites',
    'django.contrib.humanize',
    'Bcfg2.Server.Hostbase.hostbase',
)

LOGIN_URL = '/login/'

########NEW FILE########
__FILENAME__ = urls
from Bcfg2.Reporting.Compat.django_urls import *
from django.conf import settings
from django.views.generic.simple import direct_to_template
from django.contrib import admin


admin.autodiscover()


urlpatterns = patterns('',
    # Uncomment the admin/doc line below and add 'django.contrib.admindocs'
    # to INSTALLED_APPS to enable admin documentation:
    (r'^admin/doc/', include('django.contrib.admindocs.urls')),

    # Uncomment the next line to enable the admin:
    (r'^admin/', include(admin.site.urls)),

    (r'^$',direct_to_template, {'template':'index.html'}, 'index'),
    (r'^hostbase/', include('hostbase.urls')),
    (r'^login/$', 'django.contrib.auth.views.login', {'template_name': 'login.html'}),
    (r'^logout/$', 'django.contrib.auth.views.logout', {'template_name': 'logout.html'})
)

if settings.SERVE_MEDIA:
    urlpatterns += patterns('',
        (r'^site_media/(?P<path>.*)$', 'django.views.static.serve',
         dict(document_root=settings.MEDIA_ROOT)),)

########NEW FILE########
__FILENAME__ = Comments
""" Check files for various required comments. """

import os
import lxml.etree
import Bcfg2.Server.Lint
from Bcfg2.Server import XI_NAMESPACE
from Bcfg2.Server.Plugins.Cfg.CfgPlaintextGenerator \
    import CfgPlaintextGenerator
from Bcfg2.Server.Plugins.Cfg.CfgGenshiGenerator import CfgGenshiGenerator
from Bcfg2.Server.Plugins.Cfg.CfgCheetahGenerator import CfgCheetahGenerator
from Bcfg2.Server.Plugins.Cfg.CfgInfoXML import CfgInfoXML


class Comments(Bcfg2.Server.Lint.ServerPlugin):
    """ The Comments lint plugin checks files for header comments that
    give information about the files.  For instance, you can require
    SVN keywords in a comment, or require the name of the maintainer
    of a Genshi template, and so on. """
    def __init__(self, *args, **kwargs):
        Bcfg2.Server.Lint.ServerPlugin.__init__(self, *args, **kwargs)
        self.config_cache = {}

    def Run(self):
        self.check_bundles()
        self.check_properties()
        self.check_metadata()
        self.check_cfg()
        self.check_probes()

    @classmethod
    def Errors(cls):
        return {"unexpanded-keywords": "warning",
                "keywords-not-found": "warning",
                "comments-not-found": "warning",
                "broken-xinclude-chain": "warning"}

    def required_keywords(self, rtype):
        """ Given a file type, fetch the list of required VCS keywords
        from the bcfg2-lint config.  Valid file types are documented
        in :manpage:`bcfg2-lint.conf(5)`.

        :param rtype: The file type
        :type rtype: string
        :returns: list - the required items
        """
        return self.required_items(rtype, "keyword")

    def required_comments(self, rtype):
        """ Given a file type, fetch the list of required comments
        from the bcfg2-lint config.  Valid file types are documented
        in :manpage:`bcfg2-lint.conf(5)`.

        :param rtype: The file type
        :type rtype: string
        :returns: list - the required items
        """
        return self.required_items(rtype, "comment")

    def required_items(self, rtype, itype):
        """ Given a file type and item type (``comment`` or
        ``keyword``), fetch the list of required items from the
        bcfg2-lint config.  Valid file types are documented in
        :manpage:`bcfg2-lint.conf(5)`.

        :param rtype: The file type
        :type rtype: string
        :param itype: The item type (``comment`` or ``keyword``)
        :type itype: string
        :returns: list - the required items
        """
        if itype not in self.config_cache:
            self.config_cache[itype] = {}

        if rtype not in self.config_cache[itype]:
            rv = []
            global_item = "global_%ss" % itype
            if global_item in self.config:
                rv.extend(self.config[global_item].split(","))

            item = "%s_%ss" % (rtype.lower(), itype)
            if item in self.config:
                if self.config[item]:
                    rv.extend(self.config[item].split(","))
                else:
                    # config explicitly specifies nothing
                    rv = []
            self.config_cache[itype][rtype] = rv
        return self.config_cache[itype][rtype]

    def check_bundles(self):
        """ Check bundle files for required comments. """
        if 'Bundler' in self.core.plugins:
            for bundle in self.core.plugins['Bundler'].entries.values():
                xdata = None
                rtype = ""
                try:
                    xdata = lxml.etree.XML(bundle.data)
                    rtype = "bundler"
                except (lxml.etree.XMLSyntaxError, AttributeError):
                    xdata = \
                        lxml.etree.parse(bundle.template.filepath).getroot()
                    rtype = "genshibundler"

                self.check_xml(bundle.name, xdata, rtype)

    def check_properties(self):
        """ Check Properties files for required comments. """
        if 'Properties' in self.core.plugins:
            props = self.core.plugins['Properties']
            for propfile, pdata in props.entries.items():
                if os.path.splitext(propfile)[1] == ".xml":
                    self.check_xml(pdata.name, pdata.xdata, 'properties')

    def has_all_xincludes(self, mfile):
        """ Return True if :attr:`Bcfg2.Server.Lint.Plugin.files`
        includes all XIncludes listed in the specified metadata type,
        false otherwise. In other words, this returns True if
        bcfg2-lint is dealing with complete metadata.

        :param mfile: The metadata file ("clients.xml" or
                      "groups.xml") to check for XIncludes
        :type mfile: string
        :returns: bool
        """
        if self.files is None:
            return True
        else:
            path = os.path.join(self.metadata.data, mfile)
            if path in self.files:
                xdata = lxml.etree.parse(path)
                for el in xdata.findall('./%sinclude' % XI_NAMESPACE):
                    if not self.has_all_xincludes(el.get('href')):
                        self.LintError("broken-xinclude-chain",
                                       "Broken XInclude chain: could not "
                                       "include %s" % path)
                        return False

                return True

    def check_metadata(self):
        """ Check Metadata files for required comments. """
        if self.has_all_xincludes("groups.xml"):
            self.check_xml(os.path.join(self.metadata.data, "groups.xml"),
                           self.metadata.groups_xml.data,
                           "metadata")
        if hasattr(self.metadata, "clients_xml"):
            if self.has_all_xincludes("clients.xml"):
                self.check_xml(os.path.join(self.metadata.data, "clients.xml"),
                               self.metadata.clients_xml.data,
                               "metadata")

    def check_cfg(self):
        """ Check Cfg files and ``info.xml`` files for required
        comments. """
        if 'Cfg' in self.core.plugins:
            for entryset in self.core.plugins['Cfg'].entries.values():
                for entry in entryset.entries.values():
                    rtype = None
                    if isinstance(entry, CfgGenshiGenerator):
                        rtype = "genshi"
                    elif isinstance(entry, CfgPlaintextGenerator):
                        rtype = "cfg"
                    elif isinstance(entry, CfgCheetahGenerator):
                        rtype = "cheetah"
                    elif isinstance(entry, CfgInfoXML):
                        self.check_xml(entry.infoxml.name,
                                       entry.infoxml.pnode.data,
                                       "infoxml")
                        continue
                    if rtype:
                        self.check_plaintext(entry.name, entry.data, rtype)

    def check_probes(self):
        """ Check Probes for required comments """
        if 'Probes' in self.core.plugins:
            for probe in self.core.plugins['Probes'].probes.entries.values():
                self.check_plaintext(probe.name, probe.data, "probes")

    def check_xml(self, filename, xdata, rtype):
        """ Generic check to check an XML file for required comments.

        :param filename: The filename
        :type filename: string
        :param xdata: The file data
        :type xdata: lxml.etree._Element
        :param rtype: The type of file.  Available types are
                      documented in :manpage:`bcfg2-lint.conf(5)`.
        :type rtype: string
        """
        self.check_lines(filename,
                         [str(el)
                          for el in xdata.getiterator(lxml.etree.Comment)],
                         rtype)

    def check_plaintext(self, filename, data, rtype):
        """ Generic check to check a plain text file for required
        comments.

        :param filename: The filename
        :type filename: string
        :param data: The file data
        :type data: string
        :param rtype: The type of file.  Available types are
                      documented in :manpage:`bcfg2-lint.conf(5)`.
        :type rtype: string
        """
        self.check_lines(filename, data.splitlines(), rtype)

    def check_lines(self, filename, lines, rtype):
        """ Generic header check for a set of lines.

        :param filename: The filename
        :type filename: string
        :param lines: The data to check
        :type lines: list of strings
        :param rtype: The type of file.  Available types are
                      documented in :manpage:`bcfg2-lint.conf(5)`.
        :type rtype: string
        """
        if self.HandlesFile(filename):
            # found is trivalent:
            # False == keyword not found
            # None == keyword found but not expanded
            # True == keyword found and expanded
            found = dict((k, False) for k in self.required_keywords(rtype))

            for line in lines:
                # we check for both '$<keyword>:' and '$<keyword>$' to see
                # if the keyword just hasn't been expanded
                for (keyword, status) in found.items():
                    if not status:
                        if '$%s:' % keyword in line:
                            found[keyword] = True
                        elif '$%s$' % keyword in line:
                            found[keyword] = None

            unexpanded = [keyword for (keyword, status) in found.items()
                          if status is None]
            if unexpanded:
                self.LintError("unexpanded-keywords",
                               "%s: Required keywords(s) found but not "
                               "expanded: %s" %
                               (filename, ", ".join(unexpanded)))
            missing = [keyword for (keyword, status) in found.items()
                       if status is False]
            if missing:
                self.LintError("keywords-not-found",
                               "%s: Required keywords(s) not found: $%s$" %
                               (filename, "$, $".join(missing)))

            # next, check for required comments.  found is just
            # boolean
            found = dict((k, False) for k in self.required_comments(rtype))

            for line in lines:
                for (comment, status) in found.items():
                    if not status:
                        found[comment] = comment in line

            missing = [comment for (comment, status) in found.items()
                       if status is False]
            if missing:
                self.LintError("comments-not-found",
                               "%s: Required comments(s) not found: %s" %
                               (filename, ", ".join(missing)))

########NEW FILE########
__FILENAME__ = Genshi
""" Check Genshi templates for syntax errors. """

import sys
import Bcfg2.Server.Lint
from genshi.template import TemplateLoader, NewTextTemplate, MarkupTemplate, \
    TemplateSyntaxError
from Bcfg2.Server.Plugins.Bundler import BundleTemplateFile
from Bcfg2.Server.Plugins.Cfg.CfgGenshiGenerator import CfgGenshiGenerator


class Genshi(Bcfg2.Server.Lint.ServerPlugin):
    """ Check Genshi templates for syntax errors. """

    def Run(self):
        if 'Cfg' in self.core.plugins:
            self.check_cfg()
        if 'TGenshi' in self.core.plugins:
            self.check_tgenshi()
        if 'Bundler' in self.core.plugins:
            self.check_bundler()

    @classmethod
    def Errors(cls):
        return {"genshi-syntax-error": "error"}

    def check_cfg(self):
        """ Check genshi templates in Cfg for syntax errors. """
        for entryset in self.core.plugins['Cfg'].entries.values():
            for entry in entryset.entries.values():
                if (self.HandlesFile(entry.name) and
                    isinstance(entry, CfgGenshiGenerator) and
                    not entry.template):
                    try:
                        entry.loader.load(entry.name,
                                          cls=NewTextTemplate)
                    except TemplateSyntaxError:
                        err = sys.exc_info()[1]
                        self.LintError("genshi-syntax-error",
                                       "Genshi syntax error: %s" % err)
                    except:
                        etype, err = sys.exc_info()[:2]
                        self.LintError(
                            "genshi-syntax-error",
                            "Unexpected Genshi error on %s: %s: %s" %
                            (entry.name, etype.__name__, err))

    def check_tgenshi(self):
        """ Check templates in TGenshi for syntax errors. """
        loader = TemplateLoader()

        for eset in self.core.plugins['TGenshi'].entries.values():
            for fname, sdata in list(eset.entries.items()):
                if self.HandlesFile(fname):
                    try:
                        loader.load(sdata.name, cls=NewTextTemplate)
                    except TemplateSyntaxError:
                        err = sys.exc_info()[1]
                        self.LintError("genshi-syntax-error",
                                       "Genshi syntax error: %s" % err)

    def check_bundler(self):
        """ Check templates in Bundler for syntax errors. """
        loader = TemplateLoader()

        for entry in self.core.plugins['Bundler'].entries.values():
            if (self.HandlesFile(entry.name) and
                isinstance(entry, BundleTemplateFile)):
                try:
                    loader.load(entry.name, cls=MarkupTemplate)
                except TemplateSyntaxError:
                    err = sys.exc_info()[1]
                    self.LintError("genshi-syntax-error",
                                   "Genshi syntax error: %s" % err)

########NEW FILE########
__FILENAME__ = GroupNames
""" Ensure that all named groups are valid group names. """

import os
import re
import Bcfg2.Server.Lint
try:
    from Bcfg2.Server.Plugins.Bundler import BundleTemplateFile
    HAS_GENSHI = True
except ImportError:
    HAS_GENSHI = False


class GroupNames(Bcfg2.Server.Lint.ServerPlugin):
    """ Ensure that all named groups are valid group names. """

    #: A string regex that matches only valid group names.  Currently,
    #: a group name is considered valid if it contains only
    #: non-whitespace characters.
    pattern = r'\S+$'

    #: A compiled regex for
    #: :attr:`Bcfg2.Server.Lint.GroupNames.GroupNames.pattern`
    valid = re.compile(r'^' + pattern)

    def Run(self):
        self.check_metadata()
        if 'Rules' in self.core.plugins:
            self.check_rules()
        if 'Bundler' in self.core.plugins:
            self.check_bundles()
        if 'GroupPatterns' in self.core.plugins:
            self.check_grouppatterns()
        if 'Cfg' in self.core.plugins:
            self.check_cfg()

    @classmethod
    def Errors(cls):
        return {"invalid-group-name": "error"}

    def check_rules(self):
        """ Check groups used in the Rules plugin for validity. """
        for rules in self.core.plugins['Rules'].entries.values():
            if not self.HandlesFile(rules.name):
                continue
            xdata = rules.pnode.data
            self.check_entries(xdata.xpath("//Group"),
                               os.path.join(self.config['repo'], rules.name))

    def check_bundles(self):
        """ Check groups used in the Bundler plugin for validity. """
        for bundle in self.core.plugins['Bundler'].entries.values():
            if (self.HandlesFile(bundle.name) and
                (not HAS_GENSHI or
                 not isinstance(bundle, BundleTemplateFile))):
                self.check_entries(bundle.xdata.xpath("//Group"),
                                   bundle.name)

    def check_metadata(self):
        """ Check groups used or declared in the Metadata plugin for
        validity. """
        self.check_entries(self.metadata.groups_xml.xdata.xpath("//Group"),
                           os.path.join(self.config['repo'],
                                        self.metadata.groups_xml.name))

    def check_grouppatterns(self):
        """ Check groups used in the GroupPatterns plugin for validity """
        cfg = self.core.plugins['GroupPatterns'].config
        if not self.HandlesFile(cfg.name):
            return
        for grp in cfg.xdata.xpath('//GroupPattern/Group'):
            if not self.valid.search(grp.text):
                self.LintError("invalid-group-name",
                               "Invalid group name in %s: %s" %
                               (cfg.name, self.RenderXML(grp, keep_text=True)))

    def check_cfg(self):
        """ Check groups used in group-specific files in the Cfg
        plugin for validity. """
        for root, _, files in os.walk(self.core.plugins['Cfg'].data):
            for fname in files:
                basename = os.path.basename(root)
                if (re.search(r'^%s\.G\d\d_' % basename, fname) and
                    not re.search(r'^%s\.G\d\d_' % basename + self.pattern,
                                  fname)):
                    self.LintError("invalid-group-name",
                                   "Invalid group name referenced in %s" %
                                   os.path.join(root, fname))

    def check_entries(self, entries, fname):
        """ Check a generic list of XML entries for <Group> tags with
        invalid name attributes.

        :param entries: A list of XML <Group> tags whose ``name``
                        attributes will be validated.
        :type entries: list of lxml.etree._Element
        :param fname: The filename the entry list came from
        :type fname: string
        """
        for grp in entries:
            if not self.valid.search(grp.get("name")):
                self.LintError("invalid-group-name",
                               "Invalid group name in %s: %s" %
                               (fname, self.RenderXML(grp)))

########NEW FILE########
__FILENAME__ = InfoXML
""" Ensure that all config files have a valid info.xml file. """

import os
import Bcfg2.Options
import Bcfg2.Server.Lint
from Bcfg2.Server.Plugins.Cfg.CfgInfoXML import CfgInfoXML
from Bcfg2.Server.Plugins.Cfg.CfgLegacyInfo import CfgLegacyInfo


class InfoXML(Bcfg2.Server.Lint.ServerPlugin):
    """ Ensure that all config files have a valid info.xml file. This
    plugin can check for:

    * Missing ``info.xml`` files;
    * Use of deprecated ``info``/``:info`` files;
    * Paranoid mode disabled in an ``info.xml`` file;
    * Required attributes missing from ``info.xml``
    """
    def Run(self):
        if 'Cfg' not in self.core.plugins:
            return

        for filename, entryset in self.core.plugins['Cfg'].entries.items():
            infoxml_fname = os.path.join(entryset.path, "info.xml")
            if self.HandlesFile(infoxml_fname):
                found = False
                for entry in entryset.entries.values():
                    if isinstance(entry, CfgInfoXML):
                        self.check_infoxml(infoxml_fname,
                                           entry.infoxml.pnode.data)
                        found = True
                if not found:
                    self.LintError("no-infoxml",
                                   "No info.xml found for %s" % filename)

            for entry in entryset.entries.values():
                if isinstance(entry, CfgLegacyInfo):
                    if not self.HandlesFile(entry.path):
                        continue
                    self.LintError("deprecated-info-file",
                                   "Deprecated %s file found at %s" %
                                   (os.path.basename(entry.name),
                                    entry.path))

    @classmethod
    def Errors(cls):
        return {"no-infoxml": "warning",
                "deprecated-info-file": "warning",
                "paranoid-false": "warning",
                "required-infoxml-attrs-missing": "error"}

    def check_infoxml(self, fname, xdata):
        """ Verify that info.xml contains everything it should. """
        for info in xdata.getroottree().findall("//Info"):
            required = []
            if "required_attrs" in self.config:
                required = self.config["required_attrs"].split(",")

            missing = [attr for attr in required if info.get(attr) is None]
            if missing:
                self.LintError("required-infoxml-attrs-missing",
                               "Required attribute(s) %s not found in %s:%s" %
                               (",".join(missing), fname,
                                self.RenderXML(info)))

            if ((Bcfg2.Options.MDATA_PARANOID.value and
                 info.get("paranoid") is not None and
                 info.get("paranoid").lower() == "false") or
                (not Bcfg2.Options.MDATA_PARANOID.value and
                 (info.get("paranoid") is None or
                  info.get("paranoid").lower() != "true"))):
                self.LintError("paranoid-false",
                               "Paranoid must be true in %s:%s" %
                               (fname, self.RenderXML(info)))

########NEW FILE########
__FILENAME__ = MergeFiles
""" find Probes or Cfg files with multiple similar files that might be
merged into one """

import os
import copy
from difflib import SequenceMatcher
import Bcfg2.Server.Lint
from Bcfg2.Server.Plugins.Cfg import CfgGenerator


class MergeFiles(Bcfg2.Server.Lint.ServerPlugin):
    """ find Probes or Cfg files with multiple similar files that
    might be merged into one """
    def Run(self):
        if 'Cfg' in self.core.plugins:
            self.check_cfg()
        if 'Probes' in self.core.plugins:
            self.check_probes()

    @classmethod
    def Errors(cls):
        return {"merge-cfg": "warning",
                "merge-probes": "warning"}

    def check_cfg(self):
        """ check Cfg for similar files """
        for filename, entryset in self.core.plugins['Cfg'].entries.items():
            candidates = dict([(f, e) for f, e in entryset.entries.items()
                               if isinstance(e, CfgGenerator)])
            for mset in self.get_similar(candidates):
                self.LintError("merge-cfg",
                               "The following files are similar: %s. "
                               "Consider merging them into a single Genshi "
                               "template." %
                               ", ".join([os.path.join(filename, p)
                                          for p in mset]))

    def check_probes(self):
        """ check Probes for similar files """
        probes = self.core.plugins['Probes'].probes.entries
        for mset in self.get_similar(probes):
            self.LintError("merge-probes",
                           "The following probes are similar: %s. "
                           "Consider merging them into a single probe." %
                           ", ".join([p for p in mset]))

    def get_similar(self, entries):
        """ Get a list of similar files from the entry dict.  Return
        value is a list of lists, each of which gives the filenames of
        similar files """
        if "threshold" in self.config:
            # accept threshold either as a percent (e.g., "threshold=75") or
            # as a ratio (e.g., "threshold=.75")
            threshold = float(self.config['threshold'])
            if threshold > 1:
                threshold /= 100
        else:
            threshold = 0.75
        rv = []
        elist = list(entries.items())
        while elist:
            result = self._find_similar(elist.pop(0), copy.copy(elist),
                                        threshold)
            if len(result) > 1:
                elist = [(fname, fdata)
                         for fname, fdata in elist
                         if fname not in result]
                rv.append(result)
        return rv

    def _find_similar(self, ftuple, others, threshold):
        """ Find files similar to the one described by ftupe in the
        list of other files.  ftuple is a tuple of (filename, data);
        others is a list of such tuples.  threshold is a float between
        0 and 1 that describes how similar two files much be to rate
        as 'similar' """
        fname, fdata = ftuple
        rv = [fname]
        while others:
            cname, cdata = others.pop(0)
            seqmatch = SequenceMatcher(None, fdata.data, cdata.data)
            # perform progressively more expensive comparisons
            if (seqmatch.real_quick_ratio() > threshold and
                seqmatch.quick_ratio() > threshold and
                seqmatch.ratio() > threshold):
                rv.extend(self._find_similar((cname, cdata), copy.copy(others),
                                             threshold))
        return rv

########NEW FILE########
__FILENAME__ = RequiredAttrs
""" Verify attributes for configuration entries that cannot be
verified with an XML schema alone. """

import os
import re
import lxml.etree
import Bcfg2.Server.Lint
import Bcfg2.Client.Tools.VCS
from Bcfg2.Server.Plugins.Packages import Apt, Yum
from Bcfg2.Client.Tools.POSIX.base import device_map
try:
    from Bcfg2.Server.Plugins.Bundler import BundleTemplateFile
    HAS_GENSHI = True
except ImportError:
    HAS_GENSHI = False


# format verifying functions.  TODO: These should be moved into XML
# schemas where possible.
def is_filename(val):
    """ Return True if val is a string describing a valid full path
    """
    return val.startswith("/") and len(val) > 1


def is_selinux_type(val):
    """ Return True if val is a string describing a valid (although
    not necessarily existent) SELinux type """
    return re.match(r'^[a-z_]+_t', val)


def is_selinux_user(val):
    """ Return True if val is a string describing a valid (although
    not necessarily existent) SELinux user """
    return re.match(r'^[a-z_]+_u', val)


def is_octal_mode(val):
    """ Return True if val is a string describing a valid octal
    permissions mode """
    return re.match(r'[0-7]{3,4}', val)


def is_username(val):
    """ Return True if val is a string giving either a positive
    integer uid, or a valid Unix username """
    return re.match(r'^([A-z][-_A-z0-9]{0,30}|\d+)$', val)


def is_device_mode(val):
    """ Return True if val is a string describing a positive integer
    """
    return re.match(r'^\d+$', val)


class RequiredAttrs(Bcfg2.Server.Lint.ServerPlugin):
    """ Verify attributes for configuration entries that cannot be
    verified with an XML schema alone. """
    def __init__(self, *args, **kwargs):
        Bcfg2.Server.Lint.ServerPlugin.__init__(self, *args, **kwargs)
        self.required_attrs = dict(
            Path=dict(
                device=dict(name=is_filename,
                            owner=is_username,
                            group=is_username,
                            dev_type=lambda v: v in device_map),
                directory=dict(name=is_filename, owner=is_username,
                               group=is_username, mode=is_octal_mode),
                file=dict(name=is_filename, owner=is_username,
                          group=is_username, mode=is_octal_mode,
                          __text__=None),
                hardlink=dict(name=is_filename, to=is_filename),
                symlink=dict(name=is_filename),
                ignore=dict(name=is_filename),
                nonexistent=dict(name=is_filename),
                permissions=dict(name=is_filename, owner=is_username,
                                 group=is_username, mode=is_octal_mode),
                vcs=dict(vcstype=lambda v: (v != 'Path' and
                                            hasattr(Bcfg2.Client.Tools.VCS.VCS,
                                                    "Install%s" % v)),
                         revision=None, sourceurl=None)),
            Service={"__any__": dict(name=None),
                     "smf": dict(name=None, FMRI=None)},
            Action={None: dict(name=None,
                               timing=lambda v: v in ['pre', 'post', 'both'],
                               when=lambda v: v in ['modified', 'always'],
                               status=lambda v: v in ['ignore', 'check'],
                               command=None)},
            ACL=dict(
                default=dict(scope=lambda v: v in ['user', 'group'],
                             perms=lambda v: re.match(r'^([0-7]|[rwx\-]{0,3}',
                                                      v)),
                access=dict(scope=lambda v: v in ['user', 'group'],
                            perms=lambda v: re.match(r'^([0-7]|[rwx\-]{0,3}',
                                                     v)),
                mask=dict(perms=lambda v: re.match(r'^([0-7]|[rwx\-]{0,3}',
                                                   v))),
            Package={"__any__": dict(name=None)},
            SEBoolean={None: dict(name=None,
                                  value=lambda v: v in ['on', 'off'])},
            SEModule={None: dict(name=None, __text__=None)},
            SEPort={
                None: dict(name=lambda v: re.match(r'^\d+(-\d+)?/(tcp|udp)',
                                                   v),
                           selinuxtype=is_selinux_type)},
            SEFcontext={None: dict(name=None, selinuxtype=is_selinux_type)},
            SENode={None: dict(name=lambda v: "/" in v,
                               selinuxtype=is_selinux_type,
                               proto=lambda v: v in ['ipv6', 'ipv4'])},
            SELogin={None: dict(name=is_username,
                                selinuxuser=is_selinux_user)},
            SEUser={None: dict(name=is_selinux_user,
                               roles=lambda v: all(is_selinux_user(u)
                                                   for u in " ".split(v)),
                               prefix=None)},
            SEInterface={None: dict(name=None, selinuxtype=is_selinux_type)},
            SEPermissive={None: dict(name=is_selinux_type)},
            POSIXGroup={None: dict(name=is_username)},
            POSIXUser={None: dict(name=is_username)})

    def Run(self):
        self.check_packages()
        if "Defaults" in self.core.plugins:
            self.logger.info("Defaults plugin enabled; skipping required "
                             "attribute checks")
        else:
            self.check_rules()
            self.check_bundles()

    @classmethod
    def Errors(cls):
        return {"missing-elements": "error",
                "unknown-entry-type": "error",
                "unknown-entry-tag": "error",
                "required-attrs-missing": "error",
                "required-attr-format": "error",
                "extra-attrs": "warning"}

    def check_default_acl(self, path):
        """ Check that a default ACL contains either no entries or minimum
        required entries """
        defaults = 0
        if path.xpath("ACL[@type='default' and @scope='user' and @user='']"):
            defaults += 1
        if path.xpath("ACL[@type='default' and @scope='group' and @group='']"):
            defaults += 1
        if path.xpath("ACL[@type='default' and @scope='other']"):
            defaults += 1
        if defaults > 0 and defaults < 3:
            self.LintError(
                "missing-elements",
                "A Path must have either no default ACLs or at"
                " least default:user::, default:group:: and"
                " default:other::")

    def check_packages(self):
        """ Check Packages sources for Source entries with missing
        attributes. """
        if 'Packages' not in self.core.plugins:
            return

        for source in self.core.plugins['Packages'].sources:
            if isinstance(source, Yum.YumSource):
                if (not source.pulp_id and not source.url and
                    not source.rawurl):
                    self.LintError(
                        "required-attrs-missing",
                        "A %s source must have either a url, rawurl, or "
                        "pulp_id attribute: %s" %
                        (source.ptype, self.RenderXML(source.xsource)))
            elif not source.url and not source.rawurl:
                self.LintError(
                    "required-attrs-missing",
                    "A %s source must have either a url or rawurl attribute: "
                    "%s" %
                    (source.ptype, self.RenderXML(source.xsource)))

            if (not isinstance(source, Apt.AptSource) and
                source.recommended):
                self.LintError(
                    "extra-attrs",
                    "The recommended attribute is not supported on %s sources:"
                    " %s" %
                    (source.ptype, self.RenderXML(source.xsource)))

    def check_rules(self):
        """ check Rules for Path entries with missing attrs """
        if 'Rules' not in self.core.plugins:
            return

        for rules in self.core.plugins['Rules'].entries.values():
            xdata = rules.pnode.data
            for path in xdata.xpath("//Path"):
                self.check_entry(path, os.path.join(self.config['repo'],
                                                    rules.name))

    def check_bundles(self):
        """ Check bundles for BoundPath entries with missing
        attrs. """
        if 'Bundler' not in self.core.plugins:
            return

        for bundle in self.core.plugins['Bundler'].entries.values():
            if (self.HandlesFile(bundle.name) and
                (not HAS_GENSHI or
                 not isinstance(bundle, BundleTemplateFile))):
                try:
                    xdata = lxml.etree.XML(bundle.data)
                except (lxml.etree.XMLSyntaxError, AttributeError):
                    xdata = \
                        lxml.etree.parse(bundle.template.filepath).getroot()

                for path in \
                        xdata.xpath("//*[substring(name(), 1, 5) = 'Bound']"):
                    self.check_entry(path, bundle.name)

                # ensure that abstract Package tags have either name
                # or group specified
                for package in xdata.xpath("//Package"):
                    if ('name' not in package.attrib and
                        'group' not in package.attrib):
                        self.LintError(
                            "required-attrs-missing",
                            "Package tags require either a 'name' or 'group' "
                            "attribute: \n%s" % self.RenderXML(package))

    def check_entry(self, entry, filename):
        """ Generic entry check.

        :param entry: The XML entry to check for missing attributes.
        :type entry: lxml.etree._Element
        :param filename: The filename the entry came from
        :type filename: string
        """
        if self.HandlesFile(filename):
            name = entry.get('name')
            tag = entry.tag
            if tag.startswith("Bound"):
                tag = tag[5:]
            if tag not in self.required_attrs:
                self.LintError("unknown-entry-tag",
                               "Unknown entry tag '%s': %s" %
                               (tag, self.RenderXML(entry)))
                return

            etype = entry.get('type')
            if etype in self.required_attrs[tag]:
                required_attrs = self.required_attrs[tag][etype]
            elif '__any__' in self.required_attrs[tag]:
                required_attrs = self.required_attrs[tag]['__any__']
            else:
                self.LintError("unknown-entry-type",
                               "Unknown %s type %s: %s" %
                               (tag, etype, self.RenderXML(entry)))
                return
            attrs = set(entry.attrib.keys())

            if 'dev_type' in required_attrs:
                dev_type = entry.get('dev_type')
                if dev_type in ['block', 'char']:
                    # check if major/minor are specified
                    required_attrs['major'] = is_device_mode
                    required_attrs['minor'] = is_device_mode

            if tag == 'Path':
                self.check_default_acl(entry)

            if tag == 'ACL' and 'scope' in required_attrs:
                required_attrs[entry.get('scope')] = is_username

            if '__text__' in required_attrs:
                fmt = required_attrs['__text__']
                del required_attrs['__text__']
                if (not entry.text and
                    not entry.get('empty', 'false').lower() == 'true'):
                    self.LintError("required-attrs-missing",
                                   "Text missing for %s %s in %s: %s" %
                                   (tag, name, filename,
                                    self.RenderXML(entry)))
                if fmt is not None and not fmt(entry.text):
                    self.LintError(
                        "required-attr-format",
                        "Text content of %s %s in %s is malformed\n%s" %
                        (tag, name, filename, self.RenderXML(entry)))

            if not attrs.issuperset(required_attrs.keys()):
                self.LintError(
                    "required-attrs-missing",
                    "The following required attribute(s) are missing for %s "
                    "%s in %s: %s\n%s" %
                    (tag, name, filename,
                     ", ".join([attr
                                for attr in
                                set(required_attrs.keys()).difference(attrs)]),
                     self.RenderXML(entry)))

            for attr, fmt in required_attrs.items():
                if fmt and attr in attrs and not fmt(entry.attrib[attr]):
                    self.LintError(
                        "required-attr-format",
                        "The %s attribute of %s %s in %s is malformed\n%s" %
                        (attr, tag, name, filename, self.RenderXML(entry)))

########NEW FILE########
__FILENAME__ = TemplateAbuse
""" Check for templated scripts or executables. """

import os
import stat
import Bcfg2.Server.Lint
from Bcfg2.Compat import any  # pylint: disable=W0622
from Bcfg2.Server.Plugin import DEFAULT_FILE_METADATA
from Bcfg2.Server.Plugins.Cfg.CfgInfoXML import CfgInfoXML
from Bcfg2.Server.Plugins.Cfg.CfgGenshiGenerator import CfgGenshiGenerator
from Bcfg2.Server.Plugins.Cfg.CfgCheetahGenerator import CfgCheetahGenerator
from Bcfg2.Server.Plugins.Cfg.CfgEncryptedGenshiGenerator import \
    CfgEncryptedGenshiGenerator
from Bcfg2.Server.Plugins.Cfg.CfgEncryptedCheetahGenerator import \
    CfgEncryptedCheetahGenerator


class TemplateAbuse(Bcfg2.Server.Lint.ServerPlugin):
    """ Check for templated scripts or executables. """
    templates = [CfgGenshiGenerator, CfgCheetahGenerator,
                 CfgEncryptedGenshiGenerator, CfgEncryptedCheetahGenerator]
    extensions = [".pl", ".py", ".sh", ".rb"]

    def Run(self):
        if 'Cfg' in self.core.plugins:
            for entryset in self.core.plugins['Cfg'].entries.values():
                for entry in entryset.entries.values():
                    if (self.HandlesFile(entry.name) and
                        any(isinstance(entry, t) for t in self.templates)):
                        self.check_template(entryset, entry)

    @classmethod
    def Errors(cls):
        return {"templated-script": "warning",
                "templated-executable": "warning"}

    def check_template(self, entryset, entry):
        """ Check a template to see if it's a script or an executable. """
        # first, check for a known script extension
        ext = os.path.splitext(entryset.path)[1]
        if ext in self.extensions:
            self.LintError("templated-script",
                           "Templated script found: %s\n"
                           "File has a known script extension: %s\n"
                           "Template a config file for the script instead" %
                           (entry.name, ext))
            return

        # next, check for a shebang line
        firstline = open(entry.name).readline()
        if firstline.startswith("#!"):
            self.LintError("templated-script",
                           "Templated script found: %s\n"
                           "File starts with a shebang: %s\n"
                           "Template a config file for the script instead" %
                           (entry.name, firstline))
            return

        # finally, check for executable permissions in info.xml
        for entry in entryset.entries.values():
            if isinstance(entry, CfgInfoXML):
                for pinfo in entry.infoxml.pnode.data.xpath("//FileInfo"):
                    try:
                        mode = int(pinfo.get("mode",
                                             DEFAULT_FILE_METADATA['mode']), 8)
                    except ValueError:
                        # LintError will be produced by RequiredAttrs plugin
                        self.logger.warning("Non-octal mode: %s" % mode)
                        continue
                    if mode & stat.S_IXUSR != 0:
                        self.LintError(
                            "templated-executable",
                            "Templated executable found: %s\n"
                            "Template a config file for the executable instead"
                            % entry.name)
                        return

########NEW FILE########
__FILENAME__ = Validate
""" Ensure that all XML files in the Bcfg2 repository validate
according to their respective schemas. """

import os
import sys
import glob
import fnmatch
import lxml.etree
from subprocess import Popen, PIPE, STDOUT
import Bcfg2.Server.Lint


class Validate(Bcfg2.Server.Lint.ServerlessPlugin):
    """ Ensure that all XML files in the Bcfg2 repository validate
    according to their respective schemas. """

    def __init__(self, *args, **kwargs):
        Bcfg2.Server.Lint.ServerlessPlugin.__init__(self, *args, **kwargs)

        #: A dict of <file glob>: <schema file> that maps files in the
        #: Bcfg2 specification to their schemas.  The globs are
        #: extended :mod:`fnmatch` globs that also support ``**``,
        #: which matches any number of any characters, including
        #: forward slashes.  The schema files are relative to the
        #: schema directory, which can be controlled by the
        #: ``bcfg2-lint --schema`` option.
        self.filesets = \
            {"Metadata/groups.xml": "metadata.xsd",
             "Metadata/clients.xml": "clients.xsd",
             "Cfg/**/info.xml": "info.xsd",
             "Cfg/**/privkey.xml": "privkey.xsd",
             "Cfg/**/pubkey.xml": "pubkey.xsd",
             "Cfg/**/authorizedkeys.xml": "authorizedkeys.xsd",
             "Cfg/**/authorized_keys.xml": "authorizedkeys.xsd",
             "SSHbase/**/info.xml": "info.xsd",
             "SSLCA/**/info.xml": "info.xsd",
             "TGenshi/**/info.xml": "info.xsd",
             "TCheetah/**/info.xml": "info.xsd",
             "Bundler/*.xml": "bundle.xsd",
             "Bundler/*.genshi": "bundle.xsd",
             "Pkgmgr/*.xml": "pkglist.xsd",
             "Base/*.xml": "base.xsd",
             "Rules/*.xml": "rules.xsd",
             "Defaults/*.xml": "defaults.xsd",
             "etc/report-configuration.xml": "report-configuration.xsd",
             "Deps/*.xml": "deps.xsd",
             "Decisions/*.xml": "decisions.xsd",
             "Packages/sources.xml": "packages.xsd",
             "GroupPatterns/config.xml": "grouppatterns.xsd",
             "AWSTags/config.xml": "awstags.xsd",
             "NagiosGen/config.xml": "nagiosgen.xsd",
             "FileProbes/config.xml": "fileprobes.xsd",
             "SSLCA/**/cert.xml": "sslca-cert.xsd",
             "SSLCA/**/key.xml": "sslca-key.xsd",
             "GroupLogic/groups.xml": "grouplogic.xsd"
             }

        self.filelists = {}
        self.get_filelists()

    def Run(self):
        schemadir = self.config['schema']

        for path, schemaname in self.filesets.items():
            try:
                filelist = self.filelists[path]
            except KeyError:
                filelist = []

            if filelist:
                # avoid loading schemas for empty file lists
                schemafile = os.path.join(schemadir, schemaname)
                schema = self._load_schema(schemafile)
                if schema:
                    for filename in filelist:
                        self.validate(filename, schemafile, schema=schema)

        self.check_properties()

    @classmethod
    def Errors(cls):
        return {"schema-failed-to-parse": "warning",
                "properties-schema-not-found": "warning",
                "xml-failed-to-parse": "error",
                "xml-failed-to-read": "error",
                "xml-failed-to-verify": "error",
                "xinclude-does-not-exist": "error",
                "input-output-error": "error"}

    def check_properties(self):
        """ Check Properties files against their schemas. """
        for filename in self.filelists['props']:
            schemafile = "%s.xsd" % os.path.splitext(filename)[0]
            if os.path.exists(schemafile):
                self.validate(filename, schemafile)
            else:
                self.LintError("properties-schema-not-found",
                               "No schema found for %s" % filename)
                # ensure that it at least parses
                self.parse(filename)

    def parse(self, filename):
        """ Parse an XML file, raising the appropriate LintErrors if
        it can't be parsed or read.  Return the
        lxml.etree._ElementTree parsed from the file.

        :param filename: The full path to the file to parse
        :type filename: string
        :returns: lxml.etree._ElementTree - the parsed data"""
        try:
            xdata = lxml.etree.parse(filename)
            if self.files is None:
                self._expand_wildcard_xincludes(xdata)
                xdata.xinclude()
            return xdata
        except (lxml.etree.XIncludeError, SyntaxError):
            cmd = ["xmllint", "--noout"]
            if self.files is None:
                cmd.append("--xinclude")
            cmd.append(filename)
            lint = Popen(cmd, stdout=PIPE, stderr=STDOUT)
            self.LintError("xml-failed-to-parse",
                           "%s fails to parse:\n%s" % (filename,
                                                       lint.communicate()[0]))
            lint.wait()
            return False
        except IOError:
            self.LintError("xml-failed-to-read",
                           "Failed to open file %s" % filename)
            return False

    def _expand_wildcard_xincludes(self, xdata):
        """ a lightweight version of
        :func:`Bcfg2.Server.Plugin.helpers.XMLFileBacked._follow_xincludes` """
        xinclude = '%sinclude' % Bcfg2.Server.XI_NAMESPACE
        for el in xdata.findall('//' + xinclude):
            name = el.get("href")
            if name.startswith("/"):
                fpath = name
            else:
                fpath = os.path.join(os.path.dirname(xdata.docinfo.URL), name)

            # expand globs in xinclude, a bcfg2-specific extension
            extras = glob.glob(fpath)
            if not extras:
                msg = "%s: %s does not exist, skipping: %s" % \
                      (xdata.docinfo.URL, name, self.RenderXML(el))
                if el.findall('./%sfallback' % Bcfg2.Server.XI_NAMESPACE):
                    self.logger.debug(msg)
                else:
                    self.LintError("xinclude-does-not-exist", msg)

            parent = el.getparent()
            parent.remove(el)
            for extra in extras:
                if extra != xdata.docinfo.URL:
                    lxml.etree.SubElement(parent, xinclude, href=extra)

    def validate(self, filename, schemafile, schema=None):
        """ Validate a file against the given schema.

        :param filename: The full path to the file to validate
        :type filename: string
        :param schemafile: The full path to the schema file to
                           validate against
        :type schemafile: string
        :param schema: The loaded schema to validate against.  This
                       can be used to avoid parsing a single schema
                       file for every file that needs to be validate
                       against it.
        :type schema: lxml.etree.Schema
        :returns: bool - True if the file validates, false otherwise
        """
        if schema is None:
            # if no schema object was provided, instantiate one
            schema = self._load_schema(schemafile)
            if not schema:
                return False
        datafile = self.parse(filename)
        if not datafile:
            return False
        if not schema.validate(datafile):
            cmd = ["xmllint"]
            if self.files is None:
                cmd.append("--xinclude")
            cmd.extend(["--noout", "--schema", schemafile, filename])
            lint = Popen(cmd, stdout=PIPE, stderr=STDOUT)
            output = lint.communicate()[0]
            # py3k fix
            if not isinstance(output, str):
                output = output.decode('utf-8')
            if lint.wait():
                self.LintError("xml-failed-to-verify",
                               "%s fails to verify:\n%s" % (filename, output))
                return False
        return True

    def get_filelists(self):
        """ Get lists of different kinds of files to validate.  This
        doesn't return anything, but it sets
        :attr:`Bcfg2.Server.Lint.Validate.Validate.filelists` to a
        dict whose keys are path globs given in
        :attr:`Bcfg2.Server.Lint.Validate.Validate.filesets` and whose
        values are lists of the full paths to all files in the Bcfg2
        repository (or given with ``bcfg2-lint --stdin``) that match
        the glob."""
        if self.files is not None:
            listfiles = lambda p: fnmatch.filter(self.files,
                                                 os.path.join('*', p))
        else:
            listfiles = lambda p: glob.glob(os.path.join(self.config['repo'],
                                                         p))

        for path in self.filesets.keys():
            if '/**/' in path:
                if self.files is not None:
                    self.filelists[path] = listfiles(path)
                else:  # self.files is None
                    fpath, fname = path.split('/**/')
                    self.filelists[path] = []
                    for root, _, files in \
                            os.walk(os.path.join(self.config['repo'],
                                                 fpath)):
                        self.filelists[path].extend([os.path.join(root, f)
                                                     for f in files
                                                     if f == fname])
            else:
                self.filelists[path] = listfiles(path)

        self.filelists['props'] = listfiles("Properties/*.xml")

    def _load_schema(self, filename):
        """ Load an XML schema document, returning the Schema object
        and raising appropriate lint errors on failure.

        :param filename: The full path to the schema file to load.
        :type filename: string
        :returns: lxml.etree.Schema - The loaded schema data
        """
        try:
            return lxml.etree.XMLSchema(lxml.etree.parse(filename))
        except IOError:
            err = sys.exc_info()[1]
            self.LintError("input-output-error", str(err))
        except lxml.etree.XMLSchemaParseError:
            err = sys.exc_info()[1]
            self.LintError("schema-failed-to-parse",
                           "Failed to process schema %s: %s" %
                           (filename, err))
        return None

########NEW FILE########
__FILENAME__ = ValidateJSON
"""Ensure that all JSON files in the Bcfg2 repository are
valid. Currently, the only plugins that uses JSON are Ohai and
Properties."""

import os
import sys
import glob
import fnmatch
import Bcfg2.Server.Lint

try:
    import json
    # py2.4 json library is structured differently
    json.loads  # pylint: disable=W0104
except (ImportError, AttributeError):
    import simplejson as json


class ValidateJSON(Bcfg2.Server.Lint.ServerlessPlugin):
    """Ensure that all JSON files in the Bcfg2 repository are
    valid. Currently, the only plugins that uses JSON are Ohai and
    Properties. """

    def __init__(self, *args, **kwargs):
        Bcfg2.Server.Lint.ServerlessPlugin.__init__(self, *args, **kwargs)

        #: A list of file globs that give the path to JSON files.  The
        #: globs are extended :mod:`fnmatch` globs that also support
        #: ``**``, which matches any number of any characters,
        #: including forward slashes.
        self.globs = ["Properties/*.json", "Ohai/*.json"]
        self.files = self.get_files()

    def Run(self):
        for path in self.files:
            self.logger.debug("Validating JSON in %s" % path)
            try:
                json.load(open(path))
            except ValueError:
                self.LintError("json-failed-to-parse",
                               "%s does not contain valid JSON: %s" %
                               (path, sys.exc_info()[1]))

    @classmethod
    def Errors(cls):
        return {"json-failed-to-parse": "error"}

    def get_files(self):
        """Return a list of all JSON files to validate, based on
        :attr:`Bcfg2.Server.Lint.ValidateJSON.ValidateJSON.globs`. """
        if self.files is not None:
            listfiles = lambda p: fnmatch.filter(self.files,
                                                 os.path.join('*', p))
        else:
            listfiles = lambda p: glob.glob(os.path.join(self.config['repo'],
                                                         p))

        rv = []
        for path in self.globs:
            if '/**/' in path:
                if self.files is not None:
                    rv.extend(listfiles(path))
                else:  # self.files is None
                    fpath, fname = path.split('/**/')
                    for root, _, files in \
                            os.walk(os.path.join(self.config['repo'],
                                                 fpath)):
                        rv.extend([os.path.join(root, f)
                                   for f in files if f == fname])
            else:
                rv.extend(listfiles(path))
        return rv

########NEW FILE########
__FILENAME__ = models
""" Django database models for all plugins """

import sys
import copy
import logging
import Bcfg2.Options
import Bcfg2.Server.Plugins
from django.db import models

LOGGER = logging.getLogger('Bcfg2.Server.models')

MODELS = []


def load_models(plugins=None, cfile='/etc/bcfg2.conf', quiet=True):
    """ load models from plugins specified in the config """
    global MODELS

    if plugins is None:
        # we want to provide a different default plugin list --
        # namely, _all_ plugins, so that the database is guaranteed to
        # work, even if /etc/bcfg2.conf isn't set up properly
        plugin_opt = copy.deepcopy(Bcfg2.Options.SERVER_PLUGINS)
        plugin_opt.default = Bcfg2.Server.Plugins.__all__

        setup = \
            Bcfg2.Options.OptionParser(dict(plugins=plugin_opt,
                                            configfile=Bcfg2.Options.CFILE),
                                       quiet=quiet)
        setup.parse([Bcfg2.Options.CFILE.cmd, cfile])
        plugins = setup['plugins']

    if MODELS:
        # load_models() has been called once, so first unload all of
        # the models; otherwise we might call load_models() with no
        # arguments, end up with _all_ models loaded, and then in a
        # subsequent call only load a subset of models
        for model in MODELS:
            delattr(sys.modules[__name__], model)
        MODELS = []

    for plugin in plugins:
        try:
            mod = getattr(__import__("Bcfg2.Server.Plugins.%s" %
                                     plugin).Server.Plugins, plugin)
        except ImportError:
            try:
                err = sys.exc_info()[1]
                mod = __import__(plugin)
            except:  # pylint: disable=W0702
                if plugins != Bcfg2.Server.Plugins.__all__:
                    # only produce errors if the default plugin list
                    # was not used -- i.e., if the config file was set
                    # up.  don't produce errors when trying to load
                    # all plugins, IOW.  the error from the first
                    # attempt to import is probably more accurate than
                    # the second attempt.
                    LOGGER.error("Failed to load plugin %s: %s" % (plugin,
                                                                   err))
                continue
        for sym in dir(mod):
            obj = getattr(mod, sym)
            if hasattr(obj, "__bases__") and models.Model in obj.__bases__:
                setattr(sys.modules[__name__], sym, obj)
                MODELS.append(sym)

# basic invocation to ensure that a default set of models is loaded,
# and thus that this module will always work.
load_models(quiet=True)


class InternalDatabaseVersion(models.Model):
    """ Object that tell us to which version the database is """
    version = models.IntegerField()
    updated = models.DateTimeField(auto_now_add=True)

    def __str__(self):
        return "version %d updated the %s" % (self.version,
                                              self.updated.isoformat())

    class Meta:  # pylint: disable=C0111,W0232
        app_label = "reports"
        get_latest_by = "version"

########NEW FILE########
__FILENAME__ = MultiprocessingCore
""" The multiprocessing server core is a reimplementation of the
:mod:`Bcfg2.Server.BuiltinCore` that uses the Python
:mod:`multiprocessing` library to offload work to multiple child
processes.  As such, it requires Python 2.6+.

The parent communicates with the children over
:class:`multiprocessing.Queue` objects via a
:class:`Bcfg2.Server.MultiprocessingCore.RPCQueue` object.

A method being called via the RPCQueue must be exposed by the child by
decorating it with :func:`Bcfg2.Server.Core.exposed`.
"""

import time
import threading
import lxml.etree
import multiprocessing
import Bcfg2.Server.Plugin
from itertools import cycle
from Bcfg2.Cache import Cache
from Bcfg2.Compat import Empty, wraps
from Bcfg2.Server.Core import BaseCore, exposed
from Bcfg2.Server.BuiltinCore import Core as BuiltinCore
from multiprocessing.connection import Listener, Client


class DispatchingCache(Cache, Bcfg2.Server.Plugin.Debuggable):
    """ Implementation of :class:`Bcfg2.Cache.Cache` that propagates
    cache expiration events to child nodes. """

    #: The method to send over the pipe to expire the cache
    method = "expire_metadata_cache"

    def __init__(self, *args, **kwargs):
        self.rpc_q = kwargs.pop("queue")
        Bcfg2.Server.Plugin.Debuggable.__init__(self)
        Cache.__init__(self, *args, **kwargs)

    def expire(self, key=None):
        self.rpc_q.publish(self.method, args=[key])
        Cache.expire(self, key=key)


class RPCQueue(Bcfg2.Server.Plugin.Debuggable):
    """ An implementation of a :class:`multiprocessing.Queue` designed
    for several additional use patterns:

    * Random-access reads, based on a key that identifies the data;
    * Publish-subscribe, where a datum is sent to all hosts.

    The subscribers can deal with this as a normal Queue with no
    special handling.
    """
    poll_wait = 3.0

    def __init__(self):
        Bcfg2.Server.Plugin.Debuggable.__init__(self)
        self._terminate = threading.Event()
        self._queues = dict()
        self._listeners = []

    def add_subscriber(self, name):
        """ Add a subscriber to the queue.  This returns the
        :class:`multiprocessing.Queue` object that the subscriber
        should read from.  """
        self._queues[name] = multiprocessing.Queue()
        return self._queues[name]

    def publish(self, method, args=None, kwargs=None):
        """ Publish an RPC call to the queue for consumption by all
        subscribers. """
        for queue in self._queues.values():
            queue.put((None, (method, args or [], kwargs or dict())))

    def rpc(self, dest, method, args=None, kwargs=None):
        """ Make an RPC call to the named subscriber, expecting a
        response.  This opens a
        :class:`multiprocessing.connection.Listener` and passes the
        Listener address to the child as part of the RPC call, so that
        the child can connect to the Listener to submit its results.
        """
        listener = Listener()
        self.logger.debug("Created new RPC listener at %s" % listener.address)
        self._listeners.append(listener)
        try:
            self._queues[dest].put((listener.address,
                                    (method, args or [], kwargs or dict())))
            conn = listener.accept()
            try:
                while not self._terminate.is_set():
                    if conn.poll(self.poll_wait):
                        return conn.recv()
            finally:
                conn.close()
        finally:
            listener.close()
            self._listeners.remove(listener)

    def close(self):
        """ Close queues and connections. """
        self._terminate.set()
        self.logger.debug("Closing RPC queues")
        for name, queue in self._queues.items():
            self.logger.debug("Closing RPC queue to %s" % name)
            queue.close()

        # close any listeners that are waiting for connections
        self.logger.debug("Closing RPC connections")
        for listener in self._listeners:
            self.logger.debug("Closing RPC connection at %s" %
                              listener.address)
            listener.close()


class DualEvent(object):
    """ DualEvent is a clone of :class:`threading.Event` that
    internally implements both :class:`threading.Event` and
    :class:`multiprocessing.Event`. """

    def __init__(self, threading_event=None, multiprocessing_event=None):
        self._threading_event = threading_event or threading.Event()
        self._multiproc_event = multiprocessing_event or \
            multiprocessing.Event()
        if threading_event or multiprocessing_event:
            # initialize internal flag to false, regardless of the
            # state of either object passed in
            self.clear()

    def is_set(self):
        """ Return true if and only if the internal flag is true. """
        return self._threading_event.is_set()

    isSet = is_set

    def set(self):
        """ Set the internal flag to true. """
        self._threading_event.set()
        self._multiproc_event.set()

    def clear(self):
        """ Reset the internal flag to false. """
        self._threading_event.clear()
        self._multiproc_event.clear()

    def wait(self, timeout=None):
        """ Block until the internal flag is true, or until the
        optional timeout occurs. """
        return self._threading_event.wait(timeout=timeout)


class ChildCore(BaseCore):
    """ A child process for :class:`Bcfg2.MultiprocessingCore.Core`.
    This core builds configurations from a given
    :class:`multiprocessing.Pipe`.  Note that this is a full-fledged
    server core; the only input it gets from the parent process is the
    hostnames of clients to render.  All other state comes from the
    FAM. However, this core only is used to render configs; it doesn't
    handle anything else (authentication, probes, etc.) because those
    are all much faster.  There's no reason that it couldn't handle
    those, though, if the pipe communication "protocol" were made more
    robust. """

    #: How long to wait while polling for new RPC commands.  This
    #: doesn't affect the speed with which a command is processed, but
    #: setting it too high will result in longer shutdown times, since
    #: we only check for the termination event from the main process
    #: every ``poll_wait`` seconds.
    poll_wait = 3.0

    def __init__(self, name, setup, rpc_q, terminate):
        """
        :param name: The name of this child
        :type name: string
        :param setup: A Bcfg2 options dict
        :type setup: Bcfg2.Options.OptionParser
        :param read_q: The queue the child will read from for RPC
                       communications from the parent process.
        :type read_q: multiprocessing.Queue
        :param write_q: The queue the child will write the results of
                        RPC calls to.
        :type write_q: multiprocessing.Queue
        :param terminate: An event that flags ChildCore objects to shut
                          themselves down.
        :type terminate: multiprocessing.Event
        """
        BaseCore.__init__(self, setup)

        #: The name of this child
        self.name = name

        #: The :class:`multiprocessing.Event` that will be monitored
        #: to determine when this child should shut down.
        self.terminate = terminate

        #: The queue used for RPC communication
        self.rpc_q = rpc_q

        # override this setting so that the child doesn't try to write
        # the pidfile
        self.setup['daemon'] = False

        # ensure that the child doesn't start a perflog thread
        self.perflog_thread = None

        self._rmi = dict()

    def _run(self):
        return True

    def _daemonize(self):
        return True

    def _dispatch(self, address, data):
        """ Method dispatcher used for commands received from
        the RPC queue. """
        if address is not None:
            # if the key is None, then no response is expected.  we
            # make the return connection before dispatching the actual
            # RPC call so that the parent is blocking for a connection
            # as briefly as possible
            self.logger.debug("Connecting to parent via %s" % address)
            client = Client(address)
        method, args, kwargs = data
        func = None
        rv = None
        if "." in method:
            if method in self._rmi:
                func = self._rmi[method]
            else:
                self.logger.error("%s: Method %s does not exist" % (self.name,
                                                                    method))
        elif not hasattr(self, method):
            self.logger.error("%s: Method %s does not exist" % (self.name,
                                                                method))
        else:  # method is not a plugin RMI, and exists
            func = getattr(self, method)
            if not func.exposed:
                self.logger.error("%s: Method %s is not exposed" % (self.name,
                                                                    method))
                func = None
        if func is not None:
            self.logger.debug("%s: Calling RPC method %s" % (self.name,
                                                             method))
            rv = func(*args, **kwargs)
        if address is not None:
            # if the key is None, then no response is expected
            self.logger.debug("Returning data to parent via %s" % address)
            client.send(rv)

    def _block(self):
        self._rmi = self._get_rmi()
        while not self.terminate.is_set():
            try:
                address, data = self.rpc_q.get(timeout=self.poll_wait)
                threadname = "-".join(str(i) for i in data)
                rpc_thread = threading.Thread(name=threadname,
                                              target=self._dispatch,
                                              args=[address, data])
                rpc_thread.start()
            except Empty:
                pass
            except KeyboardInterrupt:
                break
        self.shutdown()

    def shutdown(self):
        BaseCore.shutdown(self)
        self.logger.info("%s: Closing RPC command queue" % self.name)
        self.rpc_q.close()

        while len(threading.enumerate()) > 1:
            threads = [t for t in threading.enumerate()
                       if t != threading.current_thread()]
            self.logger.info("%s: Waiting for %d thread(s): %s" %
                             (self.name, len(threads),
                              [t.name for t in threads]))
            time.sleep(1)
        self.logger.info("%s: All threads stopped" % self.name)

    def _get_rmi(self):
        rmi = dict()
        for pname, pinst in self._get_rmi_objects().items():
            for crmi in pinst.__child_rmi__:
                if isinstance(crmi, tuple):
                    mname = crmi[1]
                else:
                    mname = crmi
                rmi["%s.%s" % (pname, mname)] = getattr(pinst, mname)
        return rmi

    @exposed
    def expire_metadata_cache(self, client=None):
        """ Expire the metadata cache for a client """
        self.metadata_cache.expire(client)

    @exposed
    def RecvProbeData(self, address, _):
        """ Expire the probe cache for a client """
        self.expire_caches_by_type(Bcfg2.Server.Plugin.Probing,
                                   key=self.resolve_client(address,
                                                           metadata=False)[0])

    @exposed
    def GetConfig(self, client):
        """ Render the configuration for a client """
        self.metadata.update_client_list()
        self.logger.debug("%s: Building configuration for %s" %
                          (self.name, client))
        return lxml.etree.tostring(self.BuildConfiguration(client))


class Core(BuiltinCore):
    """ A multiprocessing core that delegates building the actual
    client configurations to
    :class:`Bcfg2.Server.MultiprocessingCore.ChildCore` objects.  The
    parent process doesn't build any children itself; all calls to
    :func:`GetConfig` are delegated to children. All other calls are
    handled by the parent process. """

    #: How long to wait for a child process to shut down cleanly
    #: before it is terminated.
    shutdown_timeout = 10.0

    def __init__(self, setup):
        BuiltinCore.__init__(self, setup)
        if setup['children'] is None:
            setup['children'] = multiprocessing.cpu_count()

        #: The flag that indicates when to stop child threads and
        #: processes
        self.terminate = DualEvent(threading_event=self.terminate)

        #: A :class:`Bcfg2.Server.MultiprocessingCore.RPCQueue` object
        #: used to send or publish commands to children.
        self.rpc_q = RPCQueue()

        self.metadata_cache = DispatchingCache(queue=self.rpc_q)

        #: A list of children that will be cycled through
        self._all_children = []

        #: An iterator that each child will be taken from in sequence,
        #: to provide a round-robin distribution of render requests
        self.children = None

    def _run(self):
        for cnum in range(self.setup['children']):
            name = "Child-%s" % cnum

            self.logger.debug("Starting child %s" % name)
            child_q = self.rpc_q.add_subscriber(name)
            childcore = ChildCore(name, self.setup, child_q, self.terminate)
            child = multiprocessing.Process(target=childcore.run, name=name)
            child.start()
            self.logger.debug("Child %s started with PID %s" % (name,
                                                                child.pid))
            self._all_children.append(name)
        self.logger.debug("Started %s children: %s" % (len(self._all_children),
                                                       self._all_children))
        self.children = cycle(self._all_children)
        return BuiltinCore._run(self)

    def shutdown(self):
        BuiltinCore.shutdown(self)
        self.logger.info("Closing RPC command queues")
        self.rpc_q.close()

        def term_children():
            """ Terminate all remaining multiprocessing children. """
            for child in multiprocessing.active_children():
                self.logger.error("Waited %s seconds to shut down %s, "
                                  "terminating" % (self.shutdown_timeout,
                                                   child.name))
                child.terminate()

        timer = threading.Timer(self.shutdown_timeout, term_children)
        timer.start()
        while len(multiprocessing.active_children()):
            self.logger.info("Waiting for %s child(ren): %s" %
                             (len(multiprocessing.active_children()),
                              [c.name
                               for c in multiprocessing.active_children()]))
            time.sleep(1)
        timer.cancel()
        self.logger.info("All children shut down")

        while len(threading.enumerate()) > 1:
            threads = [t for t in threading.enumerate()
                       if t != threading.current_thread()]
            self.logger.info("Waiting for %s thread(s): %s" %
                             (len(threads), [t.name for t in threads]))
            time.sleep(1)
        self.logger.info("Shutdown complete")

    def _get_rmi(self):
        child_rmi = dict()
        for pname, pinst in self._get_rmi_objects().items():
            for crmi in pinst.__child_rmi__:
                if isinstance(crmi, tuple):
                    parentname, childname = crmi
                else:
                    parentname = childname = crmi
                child_rmi["%s.%s" % (pname, parentname)] = \
                    "%s.%s" % (pname, childname)

        rmi = BuiltinCore._get_rmi(self)
        for method in rmi.keys():
            if method in child_rmi:
                rmi[method] = self._child_rmi_wrapper(method,
                                                      rmi[method],
                                                      child_rmi[method])
        return rmi

    def _child_rmi_wrapper(self, method, parent_rmi, child_rmi):
        """ Returns a callable that dispatches a call to the given
        child RMI to child processes, and calls the parent RMI locally
        (i.e., in the parent process). """
        @wraps(parent_rmi)
        def inner(*args, **kwargs):
            """ Function that dispatches an RMI call to child
            processes and to the (original) parent function. """
            self.logger.debug("Dispatching RMI call to %s to children: %s" %
                              (method, child_rmi))
            self.rpc_q.publish(child_rmi, args=args, kwargs=kwargs)
            return parent_rmi(*args, **kwargs)

        return inner

    @exposed
    def set_debug(self, address, debug):
        self.rpc_q.set_debug(debug)
        self.rpc_q.publish("set_debug", args=[address, debug])
        self.metadata_cache.set_debug(debug)
        return BuiltinCore.set_debug(self, address, debug)

    @exposed
    def RecvProbeData(self, address, probedata):
        rv = BuiltinCore.RecvProbeData(self, address, probedata)
        # we don't want the children to actually process probe data,
        # so we don't send the data, just the fact that we got some.
        self.rpc_q.publish("RecvProbeData", args=[address, None])
        return rv

    @exposed
    def GetConfig(self, address):
        client = self.resolve_client(address)[0]
        childname = self.children.next()
        self.logger.debug("Building configuration for %s on %s" % (client,
                                                                   childname))
        return self.rpc_q.rpc(childname, "GetConfig", args=[client])

    @exposed
    def get_statistics(self, address):
        stats = dict()

        def _aggregate_statistics(newstats, prefix=None):
            """ Aggregate a set of statistics from a child or parent
            server core.  This adds the statistics to the overall
            statistics dict (optionally prepending a prefix, such as
            "Child-1", to uniquely identify this set of statistics),
            and aggregates it with the set of running totals that are
            kept from all cores. """
            for statname, vals in newstats.items():
                if statname.startswith("ChildCore:"):
                    statname = statname[5:]
                if prefix:
                    prettyname = "%s:%s" % (prefix, statname)
                else:
                    prettyname = statname
                stats[prettyname] = vals
                totalname = "Total:%s" % statname
                if totalname not in stats:
                    stats[totalname] = vals
                else:
                    newmin = min(stats[totalname][0], vals[0])
                    newmax = max(stats[totalname][1], vals[1])
                    newcount = stats[totalname][3] + vals[3]
                    newmean = ((stats[totalname][2] * stats[totalname][3]) +
                               (vals[2] * vals[3])) / newcount
                    stats[totalname] = (newmin, newmax, newmean, newcount)

        stats = dict()
        for childname in self._all_children:
            _aggregate_statistics(
                self.rpc_q.rpc(childname, "get_statistics", args=[address]),
                prefix=childname)
        _aggregate_statistics(BuiltinCore.get_statistics(self, address))
        return stats

########NEW FILE########
__FILENAME__ = base
"""This module provides the base class for Bcfg2 server plugins."""

import os
import logging
from Bcfg2.Utils import ClassName


class Debuggable(object):
    """ Mixin to add a debugging interface to an object and expose it
    via XML-RPC on :class:`Bcfg2.Server.Plugin.base.Plugin` objects """

    #: List of names of methods to be exposed as XML-RPC functions
    __rmi__ = ['toggle_debug', 'set_debug']

    #: How exposed XML-RPC functions should be dispatched to child
    #: processes.
    __child_rmi__ = __rmi__[:]

    def __init__(self, name=None):
        """
        :param name: The name of the logger object to get.  If none is
                     supplied, the full name of the class (including
                     module) will be used.
        :type name: string

        .. autoattribute:: __rmi__
        """
        if name is None:
            name = "%s.%s" % (self.__class__.__module__,
                              self.__class__.__name__)
        self.debug_flag = False
        self.logger = logging.getLogger(name)

    def set_debug(self, debug):
        """ Explicitly enable or disable debugging.  This method is exposed
        via XML-RPC.

        :returns: bool - The new value of the debug flag
        """
        self.debug_flag = debug
        return debug

    def toggle_debug(self):
        """ Turn debugging output on or off.  This method is exposed
        via XML-RPC.

        :returns: bool - The new value of the debug flag
        """
        return self.set_debug(not self.debug_flag)

    def debug_log(self, message, flag=None):
        """ Log a message at the debug level.

        :param message: The message to log
        :type message: string
        :param flag: Override the current debug flag with this value
        :type flag: bool
        :returns: None
        """
        if (flag is None and self.debug_flag) or flag:
            self.logger.error(message)


class Plugin(Debuggable):
    """ The base class for all Bcfg2 Server plugins. """

    #: The name of the plugin.
    name = ClassName()

    #: The email address of the plugin author.
    __author__ = 'bcfg-dev@mcs.anl.gov'

    #: Plugin is experimental.  Use of this plugin will produce a log
    #: message alerting the administrator that an experimental plugin
    #: is in use.
    experimental = False

    #: Plugin is deprecated and will be removed in a future release.
    #: Use of this plugin will produce a log message alerting the
    #: administrator that an experimental plugin is in use.
    deprecated = False

    #: Plugin conflicts with the list of other plugin names
    conflicts = []

    #: Plugins of the same type are processed in order of ascending
    #: sort_order value. Plugins with the same sort_order are sorted
    #: alphabetically by their name.
    sort_order = 500

    #: Whether or not to automatically create a data directory for
    #: this plugin
    create = True

    #: List of names of methods to be exposed as XML-RPC functions
    __rmi__ = Debuggable.__rmi__

    #: How exposed XML-RPC functions should be dispatched to child
    #: processes, if :mod:`Bcfg2.Server.MultiprocessingCore` is in
    #: use.  Items ``__child_rmi__`` can either be strings (in which
    #: case the same function is called on child processes as on the
    #: parent) or 2-tuples, in which case the first element is the
    #: name of the RPC function called on the parent process, and the
    #: second element is the name of the function to call on child
    #: processes.  Functions that are not listed in the list will not
    #: be dispatched to child processes, i.e., they will only be
    #: called on the parent.  A function must be listed in ``__rmi__``
    #: in order to be exposed; functions listed in ``_child_rmi__``
    #: but not ``__rmi__`` will be ignored.
    __child_rmi__ = Debuggable.__child_rmi__

    def __init__(self, core, datastore):
        """
        :param core: The Bcfg2.Server.Core initializing the plugin
        :type core: Bcfg2.Server.Core
        :param datastore: The path to the Bcfg2 repository on the
                          filesystem
        :type datastore: string
        :raises: :exc:`OSError` if adding a file monitor failed;
                 :class:`Bcfg2.Server.Plugin.exceptions.PluginInitError`
                 on other errors

        .. autoattribute:: Bcfg2.Server.Plugin.base.Debuggable.__rmi__
        """
        Debuggable.__init__(self, name=self.name)
        self.Entries = {}
        self.core = core
        self.data = os.path.join(datastore, self.name)
        if self.create and not os.path.exists(self.data):
            self.logger.warning("%s: %s does not exist, creating" %
                                (self.name, self.data))
            os.makedirs(self.data)
        self.running = True

    @classmethod
    def init_repo(cls, repo):
        """ Perform any tasks necessary to create an initial Bcfg2
        repository.

        :param repo: The path to the Bcfg2 repository on the filesystem
        :type repo: string
        :returns: None
        """
        os.makedirs(os.path.join(repo, cls.name))

    def shutdown(self):
        """ Perform shutdown tasks for the plugin

        :returns: None """
        self.debug_log("Shutting down %s plugin" % self.name)
        self.running = False

    def set_debug(self, debug):
        self.debug_log("%s: debug = %s" % (self.name, self.debug_flag),
                       flag=True)
        for entry in self.Entries.values():
            if isinstance(entry, Debuggable):
                entry.set_debug(debug)
        return Debuggable.set_debug(self, debug)

    def __str__(self):
        return "%s Plugin" % self.__class__.__name__

########NEW FILE########
__FILENAME__ = exceptions
""" Exceptions for Bcfg2 Server Plugins."""


class PluginInitError(Exception):
    """Error raised in cases of
    :class:`Bcfg2.Server.Plugin.base.Plugin` initialization errors."""
    pass


class PluginExecutionError(Exception):
    """Error raised in case of
    :class:`Bcfg2.Server.Plugin.base.Plugin` execution errors."""
    pass


class MetadataConsistencyError(Exception):
    """This error gets raised when metadata is internally
    inconsistent."""
    pass


class MetadataRuntimeError(Exception):
    """This error is raised when the metadata engine is called prior
    to reading enough data, or for other
    :class:`Bcfg2.Server.Plugin.interfaces.Metadata` errors."""
    pass


class ValidationError(Exception):
    """ Exception raised by
    :class:`Bcfg2.Server.Plugin.interfaces.StructureValidator` and
    :class:`Bcfg2.Server.Plugin.interfaces.GoalValidator` objects """


class SpecificityError(Exception):
    """ Thrown by :class:`Bcfg2.Server.Plugin.helpers.Specificity` in
    case of filename parse failure."""
    pass

########NEW FILE########
__FILENAME__ = helpers
""" Helper classes for Bcfg2 server plugins """

import os
import re
import sys
import copy
import time
import glob
import logging
import operator
import lxml.etree
import Bcfg2.Server
import Bcfg2.Options
import Bcfg2.Statistics
from Bcfg2.Compat import CmpMixin, wraps
from Bcfg2.Server.Plugin.base import Debuggable, Plugin
from Bcfg2.Server.Plugin.interfaces import Generator
from Bcfg2.Server.Plugin.exceptions import SpecificityError, \
    PluginExecutionError, PluginInitError

try:
    import django  # pylint: disable=W0611
    HAS_DJANGO = True
except ImportError:
    HAS_DJANGO = False

#: A dict containing default metadata for Path entries from bcfg2.conf
DEFAULT_FILE_METADATA = Bcfg2.Options.OptionParser(
    dict(configfile=Bcfg2.Options.CFILE,
         owner=Bcfg2.Options.MDATA_OWNER,
         group=Bcfg2.Options.MDATA_GROUP,
         mode=Bcfg2.Options.MDATA_MODE,
         secontext=Bcfg2.Options.MDATA_SECONTEXT,
         important=Bcfg2.Options.MDATA_IMPORTANT,
         paranoid=Bcfg2.Options.MDATA_PARANOID,
         sensitive=Bcfg2.Options.MDATA_SENSITIVE))
DEFAULT_FILE_METADATA.parse([Bcfg2.Options.CFILE.cmd, Bcfg2.Options.CFILE])
del DEFAULT_FILE_METADATA['args']
del DEFAULT_FILE_METADATA['configfile']

LOGGER = logging.getLogger(__name__)

#: a compiled regular expression for parsing info and :info files
INFO_REGEX = re.compile(r'owner:\s*(?P<owner>\S+)|' +
                        r'group:\s*(?P<group>\S+)|' +
                        r'mode:\s*(?P<mode>\w+)|' +
                        r'secontext:\s*(?P<secontext>\S+)|' +
                        r'paranoid:\s*(?P<paranoid>\S+)|' +
                        r'sensitive:\s*(?P<sensitive>\S+)|' +
                        r'encoding:\s*(?P<encoding>\S+)|' +
                        r'important:\s*(?P<important>\S+)|' +
                        r'mtime:\s*(?P<mtime>\w+)')


def bind_info(entry, metadata, infoxml=None, default=DEFAULT_FILE_METADATA):
    """ Bind the file metadata in the given
    :class:`Bcfg2.Server.Plugin.helpers.InfoXML` object to the given
    entry.

    :param entry: The abstract entry to bind the info to
    :type entry: lxml.etree._Element
    :param metadata: The client metadata to get info for
    :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
    :param infoxml: The info.xml file to pull file metadata from
    :type infoxml: Bcfg2.Server.Plugin.helpers.InfoXML
    :param default: Default metadata to supply when the info.xml file
                    does not include a particular attribute
    :type default: dict
    :returns: None
    :raises: :class:`Bcfg2.Server.Plugin.exceptions.PluginExecutionError`
    """
    for attr, val in list(default.items()):
        entry.set(attr, val)
    if infoxml:
        mdata = dict()
        infoxml.pnode.Match(metadata, mdata, entry=entry)
        if 'Info' not in mdata:
            msg = "Failed to set metadata for file %s" % entry.get('name')
            LOGGER.error(msg)
            raise PluginExecutionError(msg)
        for attr, val in list(mdata['Info'][None].items()):
            entry.set(attr, val)


class track_statistics(object):  # pylint: disable=C0103
    """ Decorator that tracks execution time for the given
    :class:`Plugin` method with :mod:`Bcfg2.Statistics` for reporting
    via ``bcfg2-admin perf`` """

    def __init__(self, name=None):
        """
        :param name: The name under which statistics for this function
                     will be tracked.  By default, the name will be
                     the name of the function concatenated with the
                     name of the class the function is a member of.
        :type name: string
        """
        # if this is None, it will be set later during __call_
        self.name = name

    def __call__(self, func):
        if self.name is None:
            self.name = func.__name__

        @wraps(func)
        def inner(obj, *args, **kwargs):
            """ The decorated function """
            name = "%s:%s" % (obj.__class__.__name__, self.name)

            start = time.time()
            try:
                return func(obj, *args, **kwargs)
            finally:
                Bcfg2.Statistics.stats.add_value(name, time.time() - start)

        return inner


class DatabaseBacked(Plugin):
    """ Provides capabilities for a plugin to read and write to a
    database.

    .. private-include: _use_db
    .. private-include: _must_lock
    """

    #: The option to look up in :attr:`section` to determine whether or
    #: not to use the database capabilities of this plugin.  The option
    #: is retrieved with
    #: :py:func:`ConfigParser.SafeConfigParser.getboolean`, and so must
    #: conform to the possible values that function can handle.
    option = "use_database"

    def __init__(self, core, datastore):
        Plugin.__init__(self, core, datastore)
        use_db = self.core.setup.cfp.getboolean(self.section,
                                                self.option,
                                                default=False)
        if use_db and not HAS_DJANGO:
            raise PluginInitError("%s.%s is True but Django not found" %
                                  (self.section, self.option))
        elif use_db and not self.core.database_available:
            raise PluginInitError("%s.%s is True but the database is "
                                  "unavailable due to prior errors" %
                                  (self.section, self.option))

    def _section(self):
        """ The section to look in for :attr:`DatabaseBacked.option`
        """
        return self.name.lower()
    section = property(_section)

    @property
    def _use_db(self):
        """ Whether or not this plugin is configured to use the
        database. """
        use_db = self.core.setup.cfp.getboolean(self.section,
                                                self.option,
                                                default=False)
        if use_db and HAS_DJANGO and self.core.database_available:
            return True
        else:
            return False

    @property
    def _must_lock(self):
        """ Whether or not the backend database must acquire a thread
        lock before writing, because it does not allow multiple
        threads to write."""
        engine = \
            self.core.setup.cfp.get(Bcfg2.Options.DB_ENGINE.cf[0],
                                    Bcfg2.Options.DB_ENGINE.cf[1],
                                    default=Bcfg2.Options.DB_ENGINE.default)
        return engine == 'sqlite3'

    @staticmethod
    def get_db_lock(func):
        """ Decorator to be used by a method of a
        :class:`DatabaseBacked` plugin that will update database data. """

        @wraps(func)
        def _acquire_and_run(self, *args, **kwargs):
            """ The decorated function """
            if self._must_lock:  # pylint: disable=W0212
                try:
                    self.core.db_write_lock.acquire()
                    rv = func(self, *args, **kwargs)
                finally:
                    self.core.db_write_lock.release()
            else:
                rv = func(self, *args, **kwargs)
            return rv
        return _acquire_and_run


class PluginDatabaseModel(object):
    """ A database model mixin that all database models used by
    :class:`Bcfg2.Server.Plugin.helpers.DatabaseBacked` plugins must
    inherit from.  This is just a mixin; models must also inherit from
    django.db.models.Model to be valid Django models."""

    class Meta(object):  # pylint: disable=W0232
        """ Model metadata options """
        app_label = "Server"


class FileBacked(Debuggable):
    """ This object caches file data in memory. FileBacked objects are
    principally meant to be used as a part of
    :class:`Bcfg2.Server.Plugin.helpers.DirectoryBacked`. """

    def __init__(self, name, fam=None):
        """
        :param name: The full path to the file to cache and monitor
        :type name: string
        :param fam: The FAM object used to receive notifications of
                    changes
        :type fam: Bcfg2.Server.FileMonitor.FileMonitor
        """
        Debuggable.__init__(self)

        #: A string containing the raw data in this file
        self.data = ''

        #: The full path to the file
        self.name = name

        #: The FAM object used to receive notifications of changes
        self.fam = fam

    def HandleEvent(self, event=None):
        """ HandleEvent is called whenever the FAM registers an event.

        :param event: The event object
        :type event: Bcfg2.Server.FileMonitor.Event
        :returns: None
        """
        if event and event.code2str() not in ['exists', 'changed', 'created']:
            return
        try:
            self.data = open(self.name).read()
            self.Index()
        except IOError:
            err = sys.exc_info()[1]
            self.logger.error("Failed to read file %s: %s" % (self.name, err))
        except:
            err = sys.exc_info()[1]
            self.logger.error("Failed to parse file %s: %s" % (self.name, err))

    def Index(self):
        """ Index() is called by :func:`HandleEvent` every time the
        data changes, and parses the data into usable data as
        required."""
        pass

    def __repr__(self):
        return "%s: %s" % (self.__class__.__name__, self.name)


class DirectoryBacked(Debuggable):
    """ DirectoryBacked objects represent a directory that contains
    files, represented by objects of the type listed in
    :attr:`__child__`, and other directories recursively.  It monitors
    for new files and directories to be added, and creates new objects
    as required to track those."""

    #: The type of child objects to create for files contained within
    #: the directory that is tracked.  Default is
    #: :class:`Bcfg2.Server.Plugin.helpers.FileBacked`
    __child__ = FileBacked

    #: Only track and include files whose names (not paths) match this
    #: compiled regex.
    patterns = re.compile('.*')

    #: Preemptively ignore files whose names (not paths) match this
    #: compiled regex.  ``ignore`` can be set to ``None`` to ignore no
    #: files.  If a file is encountered that does not match
    #: :attr:`patterns` or ``ignore``, then a warning will be produced.
    ignore = None

    def __init__(self, data, fam):
        """
        :param data: The path to the data directory that will be
                     monitored
        :type data: string
        :param fam: The FAM object used to receive notifications of
                    changes
        :type fam: Bcfg2.Server.FileMonitor.FileMonitor

        .. -----
        .. autoattribute:: __child__
        """
        Debuggable.__init__(self)

        self.data = os.path.normpath(data)
        self.fam = fam

        #: self.entries contains information about the files monitored
        #: by this object. The keys of the dict are the relative
        #: paths to the files. The values are the objects (of type
        #: :attr:`__child__`) that handle their contents.
        self.entries = {}

        #: self.handles contains information about the directories
        #: monitored by this object. The keys of the dict are the
        #: values returned by the initial fam.AddMonitor() call (which
        #: appear to be integers). The values are the relative paths of
        #: the directories.
        self.handles = {}

        # Monitor everything in the plugin's directory
        if not os.path.exists(self.data):
            self.logger.warning("%s does not exist, creating" % self.data)
            os.makedirs(self.data)
        self.add_directory_monitor('')

    def set_debug(self, debug):
        for entry in self.entries.values():
            if isinstance(entry, Debuggable):
                entry.set_debug(debug)
        return Debuggable.set_debug(self, debug)

    def __getitem__(self, key):
        return self.entries[key]

    def __len__(self):
        return len(self.entries)

    def __delitem__(self, key):
        del self.entries[key]

    def __setitem__(self, key, val):
        self.entries[key] = val

    def __iter__(self):
        return iter(list(self.entries.items()))

    def add_directory_monitor(self, relative):
        """ Add a new directory to the FAM for monitoring.

        :param relative: Path name to monitor. This must be relative
                         to the plugin's directory. An empty string
                         value ("") will cause the plugin directory
                         itself to be monitored.
        :type relative: string
        :returns: None
        """
        dirpathname = os.path.join(self.data, relative)
        if relative not in self.handles.values():
            if not os.path.isdir(dirpathname):
                self.logger.error("%s is not a directory" % dirpathname)
                return
            reqid = self.fam.AddMonitor(dirpathname, self)
            self.handles[reqid] = relative

    def add_entry(self, relative, event):
        """ Add a new file to our tracked entries, and to our FAM for
        monitoring.

        :param relative: Path name to monitor. This must be relative
                         to the plugin's directory.
        :type relative: string:
        :param event: FAM event that caused this entry to be added.
        :type event: Bcfg2.Server.FileMonitor.Event
        :returns: None
        """
        self.entries[relative] = self.__child__(os.path.join(self.data,
                                                             relative),
                                                self.fam)
        self.entries[relative].HandleEvent(event)

    def HandleEvent(self, event):  # pylint: disable=R0912
        """ Handle FAM events.

        This method is invoked by the FAM when it detects a change to
        a filesystem object we have requsted to be monitored.

        This method manages the lifecycle of events related to the
        monitored objects, adding them to our list of entries and
        creating objects of type :attr:`__child__` that actually do
        the domain-specific processing. When appropriate, it
        propogates events those objects by invoking their HandleEvent
        method in turn.

        :param event: FAM event that caused this entry to be added.
        :type event: Bcfg2.Server.FileMonitor.Event
        :returns: None
        """
        action = event.code2str()

        # Exclude events for actions we don't care about
        if action == 'endExist':
            return

        if event.requestID not in self.handles:
            self.logger.warn("Got %s event with unknown handle (%s) for %s" %
                             (action, event.requestID, event.filename))
            return

        # Clean up path names
        event.filename = os.path.normpath(event.filename)
        if event.filename.startswith(self.data):
            # the first event we get is on the data directory itself
            event.filename = event.filename[len(self.data) + 1:]

        if self.ignore and self.ignore.search(event.filename):
            self.logger.debug("Ignoring event %s" % event.filename)
            return

        # Calculate the absolute and relative paths this event refers to
        abspath = os.path.join(self.data, self.handles[event.requestID],
                               event.filename)
        relpath = os.path.join(self.handles[event.requestID],
                               event.filename).lstrip('/')

        if action == 'deleted':
            for key in list(self.entries.keys()):
                if key.startswith(relpath):
                    del self.entries[key]
            # We remove values from self.entries, but not
            # self.handles, because the FileMonitor doesn't stop
            # watching a directory just because it gets deleted. If it
            # is recreated, we will start getting notifications for it
            # again without having to add a new monitor.
        elif os.path.isdir(abspath):
            # Deal with events for directories
            if action in ['exists', 'created']:
                self.add_directory_monitor(relpath)
            elif action == 'changed':
                if relpath in self.entries:
                    # Ownerships, permissions or timestamps changed on
                    # the directory. None of these should affect the
                    # contents of the files, though it could change
                    # our ability to access them.
                    #
                    # It seems like the right thing to do is to cancel
                    # monitoring the directory and then begin
                    # monitoring it again. But the current FileMonitor
                    # class doesn't support canceling, so at least let
                    # the user know that a restart might be a good
                    # idea.
                    self.logger.warn("Directory properties for %s changed, "
                                     "please consider restarting the server" %
                                     abspath)
                else:
                    # Got a "changed" event for a directory that we
                    # didn't know about. Go ahead and treat it like a
                    # "created" event, but log a warning, because this
                    # is unexpected.
                    self.logger.warn("Got %s event for unexpected dir %s" %
                                     (action, abspath))
                    self.add_directory_monitor(relpath)
            else:
                self.logger.warn("Got unknown dir event %s %s %s" %
                                 (event.requestID, event.code2str(), abspath))
        elif self.patterns.search(event.filename):
            if action in ['exists', 'created']:
                self.add_entry(relpath, event)
            elif action == 'changed':
                if relpath in self.entries:
                    self.entries[relpath].HandleEvent(event)
                else:
                    # Got a "changed" event for a file that we didn't
                    # know about. Go ahead and treat it like a
                    # "created" event, but log a warning, because this
                    # is unexpected.
                    self.logger.warn("Got %s event for unexpected file %s" %
                                     (action, abspath))
                    self.add_entry(relpath, event)
            else:
                self.logger.warn("Got unknown file event %s %s %s" %
                                 (event.requestID, event.code2str(), abspath))
        else:
            self.logger.warn("Could not process filename %s; ignoring" %
                             event.filename)


class XMLFileBacked(FileBacked):
    """ This object parses and caches XML file data in memory.  It can
    be used as a standalone object or as a part of
    :class:`Bcfg2.Server.Plugin.helpers.XMLDirectoryBacked`
    """

    #: If ``__identifier__`` is set, then a top-level tag with the
    #: specified name will be required on the file being cached.  Its
    #: value will be available as :attr:`label`.  To disable this
    #: behavior, set ``__identifier__`` to ``None``.
    __identifier__ = 'name'

    #: If ``create`` is set, then it overrides the ``create`` argument
    #: to the constructor.
    create = None

    def __init__(self, filename, fam=None, should_monitor=False, create=None):
        """
        :param filename: The full path to the file to cache and monitor
        :type filename: string
        :param fam: The FAM object used to receive notifications of
                    changes
        :type fam: Bcfg2.Server.FileMonitor.FileMonitor
        :param should_monitor: Whether or not to monitor this file for
                               changes. It may be useful to disable
                               monitoring when, for instance, the file
                               is monitored by another object (e.g.,
                               an
                               :class:`Bcfg2.Server.Plugin.helpers.XMLDirectoryBacked`
                               object).
        :type should_monitor: bool
        :param create: Create the file if it doesn't exist.
                       ``create`` can be either an
                       :class:`lxml.etree._Element` object, which will
                       be used as initial content, or a string, which
                       will be used as the name of the (empty) tag
                       that will be the initial content of the file.
        :type create: lxml.etree._Element or string

        .. -----
        .. autoattribute:: __identifier__
        """
        FileBacked.__init__(self, filename, fam=fam)

        #: The raw XML data contained in the file as an
        #: :class:`lxml.etree.ElementTree` object, with XIncludes
        #: processed.
        self.xdata = None

        #: The label of this file.  This is determined from the
        #: top-level tag in the file, which must have an attribute
        #: specified by :attr:`__identifier__`.
        self.label = ""

        #: All entries in this file.  By default, all immediate
        #: children of the top-level XML tag.
        self.entries = []

        #: "Extra" files included in this file by XInclude.
        self.extras = []

        #: Extra FAM monitors set by this object for files included by
        #: XInclude.
        self.extra_monitors = []

        if ((create is not None or self.create not in [None, False]) and
            not os.path.exists(self.name)):
            toptag = create or self.create
            self.logger.warning("%s does not exist, creating" % self.name)
            if hasattr(toptag, "getroottree"):
                el = toptag
            else:
                el = lxml.etree.Element(toptag)
            el.getroottree().write(self.name, xml_declaration=False,
                                   pretty_print=True)

        #: Whether or not to monitor this file for changes.
        self.should_monitor = should_monitor
        if fam and should_monitor:
            self.fam.AddMonitor(filename, self)

    def _follow_xincludes(self, fname=None, xdata=None):
        """ follow xincludes, adding included files to self.extras """
        xinclude = '%sinclude' % Bcfg2.Server.XI_NAMESPACE

        if xdata is None:
            if fname is None:
                xdata = self.xdata.getroottree()
            else:
                xdata = lxml.etree.parse(fname)
        for el in xdata.findall('//' + xinclude):
            name = el.get("href")
            if name.startswith("/"):
                fpath = name
            else:
                rel = fname or self.name
                fpath = os.path.join(os.path.dirname(rel), name)

            # expand globs in xinclude, a bcfg2-specific extension
            extras = glob.glob(fpath)
            if not extras:
                msg = "%s: %s does not exist, skipping" % (self.name, name)
                if el.findall('./%sfallback' % Bcfg2.Server.XI_NAMESPACE):
                    self.logger.debug(msg)
                else:
                    self.logger.error(msg)
                # add a FAM monitor for this path.  this isn't perfect
                # -- if there's an xinclude of "*.xml", we'll watch
                # the literal filename "*.xml".  but for non-globbing
                # filenames, it works fine.
                if fpath not in self.extra_monitors:
                    self.add_monitor(fpath)

            parent = el.getparent()
            parent.remove(el)
            for extra in extras:
                if extra != self.name:
                    lxml.etree.SubElement(parent, xinclude, href=extra)
                    if extra not in self.extras:
                        self.extras.append(extra)
                        self._follow_xincludes(fname=extra)
                        if extra not in self.extra_monitors:
                            self.add_monitor(extra)

    def Index(self):
        self.xdata = lxml.etree.XML(self.data, base_url=self.name,
                                    parser=Bcfg2.Server.XMLParser)
        self.extras = []
        self._follow_xincludes()
        if self.extras:
            try:
                self.xdata.getroottree().xinclude()
            except lxml.etree.XIncludeError:
                err = sys.exc_info()[1]
                self.logger.error("XInclude failed on %s: %s" % (self.name,
                                                                 err))

        self.entries = self.xdata.getchildren()
        if self.__identifier__ is not None:
            self.label = self.xdata.attrib[self.__identifier__]
    Index.__doc__ = FileBacked.Index.__doc__

    def add_monitor(self, fpath):
        """ Add a FAM monitor to a file that has been XIncluded.  This
        is only done if the constructor got a ``fam`` object,
        regardless of whether ``should_monitor`` is set to True (i.e.,
        whether or not the base file is monitored).

        :param fpath: The full path to the file to monitor
        :type fpath: string
        :returns: None
        """
        self.extra_monitors.append(fpath)
        if self.fam:
            self.fam.AddMonitor(fpath, self)

    def __iter__(self):
        return iter(self.entries)

    def __str__(self):
        return "%s at %s" % (self.__class__.__name__, self.name)


class StructFile(XMLFileBacked):
    """ StructFiles are XML files that contain a set of structure file
    formatting logic for handling ``<Group>`` and ``<Client>``
    tags. """

    #: If ``__identifier__`` is not None, then it must be the name of
    #: an XML attribute that will be required on the top-level tag of
    #: the file being cached
    __identifier__ = None

    def _include_element(self, item, metadata):
        """ determine if an XML element matches the metadata """
        if isinstance(item, lxml.etree._Comment):  # pylint: disable=W0212
            return False
        negate = item.get('negate', 'false').lower() == 'true'
        if item.tag == 'Group':
            return negate == (item.get('name') not in metadata.groups)
        elif item.tag == 'Client':
            return negate == (item.get('name') != metadata.hostname)
        else:
            return True

    def _match(self, item, metadata):
        """ recursive helper for Match() """
        if self._include_element(item, metadata):
            if item.tag == 'Group' or item.tag == 'Client':
                rv = []
                if self._include_element(item, metadata):
                    for child in item.iterchildren():
                        rv.extend(self._match(child, metadata))
                return rv
            else:
                rv = copy.deepcopy(item)
                for child in rv.iterchildren():
                    rv.remove(child)
                for child in item.iterchildren():
                    rv.extend(self._match(child, metadata))
                return [rv]
        else:
            return []

    def Match(self, metadata):
        """ Return matching fragments of the data in this file.  A tag
        is considered to match if all ``<Group>`` and ``<Client>``
        tags that are its ancestors match the metadata given.  Since
        tags are included unmodified, it's possible for a tag to
        itself match while containing non-matching children.
        Consequently, only the tags contained in the list returned by
        Match() (and *not* their descendents) should be considered to
        match the metadata.

        :param metadata: Client metadata to match against.
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :returns: list of lxml.etree._Element objects """
        rv = []
        for child in self.entries:
            rv.extend(self._match(child, metadata))
        return rv

    def _xml_match(self, item, metadata):
        """ recursive helper for XMLMatch """
        if self._include_element(item, metadata):
            if item.tag == 'Group' or item.tag == 'Client':
                for child in item.iterchildren():
                    item.remove(child)
                    item.getparent().append(child)
                    self._xml_match(child, metadata)
                if item.text:
                    if item.getparent().text is None:
                        item.getparent().text = item.text
                    else:
                        item.getparent().text += item.text
                item.getparent().remove(item)
            else:
                for child in item.iterchildren():
                    self._xml_match(child, metadata)
        else:
            item.getparent().remove(item)

    def XMLMatch(self, metadata):
        """ Return a rebuilt XML document that only contains the
        matching portions of the original file.  A tag is considered
        to match if all ``<Group>`` and ``<Client>`` tags that are its
        ancestors match the metadata given.  Unlike :func:`Match`, the
        document returned by XMLMatch will only contain matching data.
        All ``<Group>`` and ``<Client>`` tags will have been stripped
        out.

        :param metadata: Client metadata to match against.
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :returns: lxml.etree._Element """
        rv = copy.deepcopy(self.xdata)
        for child in rv.iterchildren():
            self._xml_match(child, metadata)
        return rv


class INode(object):
    """ INodes provide lists of things available at a particular group
    intersection.  INodes are deprecated; new plugins should use
    :class:`Bcfg2.Server.Plugin.helpers.StructFile` instead. """

    raw = dict(
        Client="lambda m, e:'%(name)s' == m.hostname and predicate(m, e)",
        Group="lambda m, e:'%(name)s' in m.groups and predicate(m, e)")
    nraw = dict(
        Client="lambda m, e:'%(name)s' != m.hostname and predicate(m, e)",
        Group="lambda m, e:'%(name)s' not in m.groups and predicate(m, e)")
    containers = ['Group', 'Client']
    ignore = []

    def __init__(self, data, idict, parent=None):
        self.data = data
        self.contents = {}
        if parent is None:
            self.predicate = lambda m, e: True
        else:
            predicate = parent.predicate
            if data.get('negate', 'false').lower() == 'true':
                psrc = self.nraw
            else:
                psrc = self.raw
            if data.tag in list(psrc.keys()):
                self.predicate = eval(psrc[data.tag] %
                                      {'name': data.get('name')},
                                      {'predicate': predicate})
            else:
                raise PluginExecutionError("Unknown tag: %s" % data.tag)
        self.children = []
        self._load_children(data, idict)

    def _load_children(self, data, idict):
        """ load children """
        for item in data.getchildren():
            if item.tag in self.ignore:
                continue
            elif item.tag in self.containers:
                self.children.append(self.__class__(item, idict, self))
            else:
                try:
                    self.contents[item.tag][item.get('name')] = \
                        dict(item.attrib)
                except KeyError:
                    self.contents[item.tag] = \
                        {item.get('name'): dict(item.attrib)}
                if item.text:
                    self.contents[item.tag][item.get('name')]['__text__'] = \
                        item.text
                if item.getchildren():
                    self.contents[item.tag][item.get('name')]['__children__'] \
                        = item.getchildren()
                try:
                    idict[item.tag].append(item.get('name'))
                except KeyError:
                    idict[item.tag] = [item.get('name')]

    def Match(self, metadata, data, entry=lxml.etree.Element("None")):
        """Return a dictionary of package mappings."""
        if self.predicate(metadata, entry):
            for key in self.contents:
                try:
                    data[key].update(self.contents[key])
                except:  # pylint: disable=W0702
                    data[key] = {}
                    data[key].update(self.contents[key])
            for child in self.children:
                child.Match(metadata, data, entry=entry)


class InfoNode (INode):
    """ :class:`Bcfg2.Server.Plugin.helpers.INode` implementation that
    includes ``<Path>`` tags, suitable for use with :file:`info.xml`
    files."""

    raw = dict(
        Client="lambda m, e: '%(name)s' == m.hostname and predicate(m, e)",
        Group="lambda m, e: '%(name)s' in m.groups and predicate(m, e)",
        Path="lambda m, e: ('%(name)s' == e.get('name') or " +
        "'%(name)s' == e.get('realname')) and " +
        "predicate(m, e)")
    nraw = dict(
        Client="lambda m, e: '%(name)s' != m.hostname and predicate(m, e)",
        Group="lambda m, e: '%(name)s' not in m.groups and predicate(m, e)",
        Path="lambda m, e: '%(name)s' != e.get('name') and " +
        "'%(name)s' != e.get('realname') and " +
        "predicate(m, e)")
    containers = ['Group', 'Client', 'Path']


class XMLSrc(XMLFileBacked):
    """ XMLSrc files contain a
    :class:`Bcfg2.Server.Plugin.helpers.INode` hierarchy that returns
    matching entries. XMLSrc objects are deprecated and
    :class:`Bcfg2.Server.Plugin.helpers.StructFile` should be
    preferred where possible."""
    __node__ = INode
    __cacheobj__ = dict
    __priority_required__ = True

    def __init__(self, filename, fam=None, should_monitor=False, create=None):
        XMLFileBacked.__init__(self, filename, fam, should_monitor, create)
        self.items = {}
        self.cache = None
        self.pnode = None
        self.priority = -1

    def HandleEvent(self, _=None):
        """Read file upon update."""
        self.items = {}
        try:
            xdata = lxml.etree.parse(self.name,
                                     parser=Bcfg2.Server.XMLParser).getroot()
        except lxml.etree.XMLSyntaxError:
            msg = "Failed to parse file %s: %s" % (self.name,
                                                   sys.exc_info()[1])
            self.logger.error(msg)
            raise PluginExecutionError(msg)
        self.pnode = self.__node__(xdata, self.items)
        self.cache = None
        try:
            self.priority = int(xdata.get('priority'))
        except (ValueError, TypeError):
            if self.__priority_required__:
                msg = "Got bogus priority %s for file %s" % \
                    (xdata.get('priority'), self.name)
                self.logger.error(msg)
                raise PluginExecutionError(msg)

    def Cache(self, metadata):
        """Build a package dict for a given host."""
        if self.cache is None or self.cache[0] != metadata:
            cache = (metadata, self.__cacheobj__())
            if self.pnode is None:
                self.logger.error("Cache method called early for %s; "
                                  "forcing data load" % self.name)
                self.HandleEvent()
                return
            self.pnode.Match(metadata, cache[1])
            self.cache = cache

    def __str__(self):
        return str(self.items)


class InfoXML(XMLSrc):
    """ InfoXML files contain a
    :class:`Bcfg2.Server.Plugin.helpers.InfoNode` hierarchy that
    returns matching entries, suitable for use with :file:`info.xml`
    files."""
    __node__ = InfoNode
    __priority_required__ = False


class XMLDirectoryBacked(DirectoryBacked):
    """ :class:`Bcfg2.Server.Plugin.helpers.DirectoryBacked` for XML files. """

    #: Only track and include files whose names (not paths) match this
    #: compiled regex.
    patterns = re.compile(r'^.*\.xml$')

    #: The type of child objects to create for files contained within
    #: the directory that is tracked.  Default is
    #: :class:`Bcfg2.Server.Plugin.helpers.XMLFileBacked`
    __child__ = XMLFileBacked


class PrioDir(Plugin, Generator, XMLDirectoryBacked):
    """ PrioDir handles a directory of XML files where each file has a
    set priority.

    .. -----
    .. autoattribute:: __child__
    """

    #: The type of child objects to create for files contained within
    #: the directory that is tracked.  Default is
    #: :class:`Bcfg2.Server.Plugin.helpers.XMLSrc`
    __child__ = XMLSrc

    def __init__(self, core, datastore):
        Plugin.__init__(self, core, datastore)
        Generator.__init__(self)
        XMLDirectoryBacked.__init__(self, self.data, self.core.fam)
    __init__.__doc__ = Plugin.__init__.__doc__

    def HandleEvent(self, event):
        XMLDirectoryBacked.HandleEvent(self, event)
        self.Entries = {}
        for src in list(self.entries.values()):
            for itype, children in list(src.items.items()):
                for child in children:
                    try:
                        self.Entries[itype][child] = self.BindEntry
                    except KeyError:
                        self.Entries[itype] = {child: self.BindEntry}
    HandleEvent.__doc__ = XMLDirectoryBacked.HandleEvent.__doc__

    def _matches(self, entry, metadata, rules):  # pylint: disable=W0613
        """ Whether or not a given entry has a matching entry in this
        PrioDir.  By default this does strict matching (i.e., the
        entry name is in ``rules.keys()``), but this can be overridden
        to provide regex matching, etc.

        :param entry: The entry to find a match for
        :type entry: lxml.etree._Element
        :param metadata: The metadata to get attributes for
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :rules: A dict of rules to look in for a matching rule
        :type rules: dict
        :returns: bool
        """
        return entry.get('name') in rules

    def BindEntry(self, entry, metadata):
        """ Bind the attributes that apply to an entry to it.  The
        entry is modified in-place.

        :param entry: The entry to add attributes to.
        :type entry: lxml.etree._Element
        :param metadata: The metadata to get attributes for
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :returns: None
        """
        attrs = self.get_attrs(entry, metadata)
        for key, val in list(attrs.items()):
            entry.attrib[key] = val

    def get_attrs(self, entry, metadata):
        """ Get a list of attributes to add to the entry during the
        bind.  This is a complex method, in that it both modifies the
        entry, and returns attributes that need to be added to the
        entry.  That seems sub-optimal, and should probably be changed
        at some point.  Namely:

        * The return value includes all XML attributes that need to be
          added to the entry, but it does not add them.
        * If text contents or child tags need to be added to the
          entry, they are added to the entry in place.

        :param entry: The entry to add attributes to.
        :type entry: lxml.etree._Element
        :param metadata: The metadata to get attributes for
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :returns: dict of <attr name>:<attr value>
        :raises: :class:`Bcfg2.Server.Plugin.exceptions.PluginExecutionError`
        """
        for src in self.entries.values():
            src.Cache(metadata)

        matching = [src for src in list(self.entries.values())
                    if (src.cache and
                        entry.tag in src.cache[1] and
                        self._matches(entry, metadata,
                                      src.cache[1][entry.tag]))]
        if len(matching) == 0:
            raise PluginExecutionError("No matching source for entry when "
                                       "retrieving attributes for %s(%s)" %
                                       (entry.tag, entry.attrib.get('name')))
        elif len(matching) == 1:
            index = 0
        else:
            prio = [int(src.priority) for src in matching]
            if prio.count(max(prio)) > 1:
                msg = "Found conflicting sources with same priority for " + \
                    "%s:%s for %s" % (entry.tag, entry.get("name"),
                                      metadata.hostname)
                self.logger.error(msg)
                self.logger.error([item.name for item in matching])
                self.logger.error("Priority was %s" % max(prio))
                raise PluginExecutionError(msg)
            index = prio.index(max(prio))

        for rname in list(matching[index].cache[1][entry.tag].keys()):
            if self._matches(entry, metadata, [rname]):
                data = matching[index].cache[1][entry.tag][rname]
                break
        else:
            # Fall back on __getitem__. Required if override used
            data = matching[index].cache[1][entry.tag][entry.get('name')]
        if '__text__' in data:
            entry.text = data['__text__']
        if '__children__' in data:
            for item in data['__children__']:
                entry.append(copy.copy(item))

        return dict([(key, data[key])
                     for key in list(data.keys())
                     if not key.startswith('__')])


class Specificity(CmpMixin):
    """ Represent the specificity of an object; i.e., what client(s)
    it applies to.  It can be group- or client-specific, or apply to
    all clients.

    Specificity objects are sortable; objects that are less specific
    are considered less than objects that are more specific.  Objects
    that apply to all clients are the least specific; objects that
    apply to a single client are the most specific.  Objects that
    apply to groups are sorted by priority. """

    def __init__(self, all=False, group=False,  # pylint: disable=W0622
                 hostname=False, prio=0, delta=False):
        """
        :param all: The object applies to all clients.
        :type all: bool
        :param group: The object applies only to the given group.
        :type group: string or False
        :param hostname: The object applies only to the named client.
        :type hostname: string or False
        :param prio: The object has the given priority relative to
                     other objects that also apply to the same group.
                     ``<group>`` must be specified with ``<prio>``.
        :type prio: int
        :param delta: The object is a delta (i.e., a .cat or .diff
                      file, not a full file).  Deltas are deprecated.
        :type delta: bool

        Exactly one of {all|group|hostname} should be given.
        """
        CmpMixin.__init__(self)
        self.hostname = hostname
        self.all = all
        self.group = group
        self.prio = prio
        self.delta = delta

    def matches(self, metadata):
        """ Return True if the object described by this Specificity
        object applies to the given client.  That is, if this
        Specificity applies to all clients, or to a group the client
        is a member of, or to the client individually.

        :param metadata: The client metadata
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :returns: bool
        """
        return (self.all or
                self.hostname == metadata.hostname or
                self.group in metadata.groups)

    def __cmp__(self, other):  # pylint: disable=R0911
        """Sort most to least specific."""
        if self.all:
            if other.all:
                return 0
            else:
                return 1
        elif other.all:
            return -1
        elif self.group:
            if other.hostname:
                return 1
            if other.group and other.prio > self.prio:
                return 1
            if other.group and other.prio == self.prio:
                return 0
        elif other.group:
            return -1
        elif self.hostname and other.hostname:
            return 0
        return -1

    def __str__(self):
        rv = [self.__class__.__name__, ': ']
        if self.all:
            rv.append("all")
        elif self.group:
            rv.append("Group %s, priority %s" % (self.group, self.prio))
        elif self.hostname:
            rv.append("Host %s" % self.hostname)
        if self.delta:
            rv.append(", delta=%s" % self.delta)
        return "".join(rv)


class SpecificData(object):
    """ A file that is specific to certain clients, groups, or all
    clients. """

    def __init__(self, name, specific, encoding):  # pylint: disable=W0613
        """
        :param name: The full path to the file
        :type name: string
        :param specific: A
                         :class:`Bcfg2.Server.Plugin.helpers.Specificity`
                         object describing what clients this file
                         applies to.
        :type specific: Bcfg2.Server.Plugin.helpers.Specificity
        :param encoding: The encoding to use for data in this file
        :type encoding: string
        """
        self.name = name
        self.specific = specific
        self.data = None

    def handle_event(self, event):
        """ Handle a FAM event.  Note that the SpecificData object
        itself has no FAM, so this must be produced by a parent object
        (e.g., :class:`Bcfg2.Server.Plugin.helpers.EntrySet`).

        :param event: The event that applies to this file
        :type event: Bcfg2.Server.FileMonitor.Event
        :returns: None
        :raises: :exc:`Bcfg2.Server.Plugin.exceptions.PluginExecutionError`
        """
        if event.code2str() == 'deleted':
            return
        try:
            self.data = open(self.name).read()
        except UnicodeDecodeError:
            self.data = open(self.name, mode='rb').read()
        except:  # pylint: disable=W0201
            LOGGER.error("Failed to read file %s: %s" % (self.name,
                                                         sys.exc_info()[1]))


class EntrySet(Debuggable):
    """ EntrySets deal with a collection of host- and group-specific
    files (e.g., :class:`Bcfg2.Server.Plugin.helpers.SpecificData`
    objects) in a single directory. EntrySets are usually used as part
    of :class:`Bcfg2.Server.Plugin.helpers.GroupSpool` objects."""

    #: Preemptively ignore files whose names (not paths) match this
    #: compiled regex.  ``ignore`` cannot be set to ``None``.  If a
    #: file is encountered that does not match the ``basename``
    #: argument passed to the constructor or ``ignore``, then a
    #: warning will be produced.
    ignore = re.compile(r'^(\.#.*|.*~|\..*\.(sw[px])|.*\.genshi_include)$')

    # The ``basename`` argument passed to the constructor will be
    #: processed as a string that contains a regular expression (i.e.,
    #: *not* a compiled regex object) if ``basename_is_regex`` is True,
    #: and all files that match the regex will be cincluded in the
    #: EntrySet.  If ``basename_is_regex`` is False, then it will be
    #: considered a plain string and filenames must match exactly.
    basename_is_regex = False

    def __init__(self, basename, path, entry_type, encoding):
        """
        :param basename: The filename or regular expression that files
                         in this EntrySet must match.  See
                         :attr:`basename_is_regex` for more details.
        :type basename: string
        :param path: The full path to the directory containing files
                     for this EntrySet
        :type path: string
        :param entry_type: A callable that returns an object that
                           represents files in this EntrySet.  This
                           will usually be a class object, but it can
                           be an object factory or similar callable.
                           See below for the expected signature.
        :type entry_type: callable
        :param encoding: The encoding of all files in this entry set.
        :type encoding: string

        The ``entry_type`` callable must have the following signature::

            entry_type(filepath, specificity, encoding)

        Where the parameters are:

        :param filepath: Full path to file
        :type filepath: string
        :param specific: A
                         :class:`Bcfg2.Server.Plugin.helpers.Specificity`
                         object describing what clients this file
                         applies to.
        :type specific: Bcfg2.Server.Plugin.helpers.Specificity
        :param encoding: The encoding to use for data in this file
        :type encoding: string

        Additionally, the object returned by ``entry_type`` must have
        a ``specific`` attribute that is sortable (e.g., a
        :class:`Bcfg2.Server.Plugin.helpers.Specificity` object).

        See :class:`Bcfg2.Server.Plugin.helpers.SpecificData` for an
        example of a class that can be used as an ``entry_type``.
        """
        Debuggable.__init__(self, name=basename)
        self.path = path
        self.entry_type = entry_type
        self.entries = {}
        self.metadata = DEFAULT_FILE_METADATA.copy()
        self.infoxml = None
        self.encoding = encoding

        if self.basename_is_regex:
            base_pat = basename
        else:
            base_pat = re.escape(basename)
        pattern = r'(.*/)?' + base_pat + \
            r'(\.((H_(?P<hostname>\S+))|(G(?P<prio>\d+)_(?P<group>\S+))))?$'

        #: ``specific`` is a regular expression that is used to
        #: determine the specificity of a file in this entry set.  It
        #: must have three named groups: ``hostname``, ``prio`` (the
        #: priority of a group-specific file), and ``group``.  The base
        #: regex is constructed from the ``basename`` argument. It can
        #: be overridden on a per-entry basis in :func:`entry_init`.
        self.specific = re.compile(pattern)

    def get_matching(self, metadata):
        """ Get a list of all entries that apply to the given client.
        This gets all matching entries; for example, there could be an
        entry that applies to all clients, multiple group-specific
        entries, and a client-specific entry, all of which would be
        returned by get_matching().  You can use :func:`best_matching`
        to get the single best matching entry.

        :param metadata: The client metadata to get matching entries for
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :returns: list -- all matching ``entry_type`` objects (see the
                  constructor docs for more details)
        """
        return [item for item in list(self.entries.values())
                if item.specific.matches(metadata)]

    def best_matching(self, metadata, matching=None):
        """ Return the single most specific matching entry from the
        set of matching entries.  You can use :func:`get_matching` to
        get all matching entries.

        :param metadata: The client metadata to get matching entries for
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :param matching: The set of matching entries to pick from.  If
                         this is not provided, :func:`get_matching`
                         will be called.
        :type matching: list of ``entry_type`` objects (see the constructor
                        docs for more details)
        :returns: a single object from the list of matching
                  ``entry_type`` objects
        :raises: :class:`Bcfg2.Server.Plugin.exceptions.PluginExecutionError`
                 if no matching entries are found
        """
        if matching is None:
            matching = self.get_matching(metadata)

        if matching:
            matching.sort(key=operator.attrgetter("specific"))
            return matching[0]
        else:
            raise PluginExecutionError("No matching entries available for %s "
                                       "for %s" % (self.path,
                                                   metadata.hostname))

    def handle_event(self, event):
        """ Dispatch a FAM event to the appropriate function or child
        ``entry_type`` object.  This will probably be handled by a
        call to :func:`update_metadata`, :func:`reset_metadata`,
        :func:`entry_init`, or to the ``entry_type``
        ``handle_event()`` function.

        :param event: An event that applies to a file handled by this
                      EntrySet
        :type event: Bcfg2.Server.FileMonitor.Event
        :returns: None
        """
        action = event.code2str()

        if event.filename in ['info', 'info.xml', ':info']:
            if action in ['exists', 'created', 'changed']:
                self.update_metadata(event)
            elif action == 'deleted':
                self.reset_metadata(event)
            return

        if action in ['exists', 'created']:
            self.entry_init(event)
        else:
            if event.filename not in self.entries:
                self.logger.warning("Got %s event for unknown file %s" %
                                    (action, event.filename))
                if action == 'changed':
                    # received a bogus changed event; warn, but treat
                    # it like a created event
                    self.entry_init(event)
                return
            if action == 'changed':
                self.entries[event.filename].handle_event(event)
            elif action == 'deleted':
                del self.entries[event.filename]

    def entry_init(self, event, entry_type=None, specific=None):
        """ Handle the creation of a file on the filesystem and the
        creation of an object in this EntrySet to track it.

        :param event: An event that applies to a file handled by this
                      EntrySet
        :type event: Bcfg2.Server.FileMonitor.Event
        :param entry_type: Override the default ``entry_type`` for
                           this EntrySet object and create a different
                           object for this entry.  See the constructor
                           docs for more information on
                           ``entry_type``.
        :type entry_type: callable
        :param specific: Override the default :attr:`specific` regular
                         expression used by this object with a custom
                         regular expression that will be used to
                         determine the specificity of this entry.
        :type specific: compiled regular expression object
        :returns: None
        :raises: :class:`Bcfg2.Server.Plugin.exceptions.SpecificityError`
        """
        if entry_type is None:
            entry_type = self.entry_type

        if event.filename in self.entries:
            self.logger.warn("Got duplicate add for %s" % event.filename)
        else:
            fpath = os.path.join(self.path, event.filename)
            try:
                spec = self.specificity_from_filename(event.filename,
                                                      specific=specific)
            except SpecificityError:
                if not self.ignore.match(event.filename):
                    self.logger.error("Could not process filename %s; ignoring"
                                      % fpath)
                return
            self.entries[event.filename] = entry_type(fpath, spec,
                                                      self.encoding)
        self.entries[event.filename].handle_event(event)

    def specificity_from_filename(self, fname, specific=None):
        """ Construct a
        :class:`Bcfg2.Server.Plugin.helpers.Specificity` object from a
        filename and regex. See :attr:`specific` for details on the
        regex.

        :param fname: The filename (not full path) of a file that is
                      in this EntrySet's directory.  It is not
                      necessary to determine first if the filename
                      matches this EntrySet's basename; that can be
                      done by catching
                      :class:`Bcfg2.Server.Plugin.exceptions.SpecificityError`
                      from this function.
        :type fname: string
        :param specific: Override the default :attr:`specific` regular
                         expression used by this object with a custom
                         regular expression that will be used to
                         determine the specificity of this entry.
        :type specific: compiled regular expression object
        :returns: Object representing the specificity of the file
        :rtype: :class:`Bcfg2.Server.Plugin.helpers.Specificity`
        :raises: :class:`Bcfg2.Server.Plugin.exceptions.SpecificityError`
                 if the regex does not match the filename
        """
        if specific is None:
            specific = self.specific
        data = specific.match(fname)
        if not data:
            raise SpecificityError(fname)
        kwargs = {}
        if data.group('hostname'):
            kwargs['hostname'] = data.group('hostname')
        elif data.group('group'):
            kwargs['group'] = data.group('group')
            kwargs['prio'] = int(data.group('prio'))
        else:
            kwargs['all'] = True
        if 'delta' in data.groupdict():
            kwargs['delta'] = data.group('delta')
        return Specificity(**kwargs)

    def update_metadata(self, event):
        """ Process changes to or creation of info, :info, and
        info.xml files for the EntrySet.

        :param event: An event that applies to an info handled by this
                      EntrySet
        :type event: Bcfg2.Server.FileMonitor.Event
        :returns: None
        """
        fpath = os.path.join(self.path, event.filename)
        if event.filename == 'info.xml':
            if not self.infoxml:
                self.infoxml = InfoXML(fpath)
            self.infoxml.HandleEvent(event)
        elif event.filename in [':info', 'info']:
            for line in open(fpath).readlines():
                match = INFO_REGEX.match(line)
                if not match:
                    self.logger.warning("Failed to match line in %s: %s" %
                                        (fpath, line))
                    continue
                else:
                    mgd = match.groupdict()
                    for key, value in list(mgd.items()):
                        if value:
                            self.metadata[key] = value
                    if len(self.metadata['mode']) == 3:
                        self.metadata['mode'] = "0%s" % self.metadata['mode']

    def reset_metadata(self, event):
        """ Reset metadata to defaults if info. :info, or info.xml are
        removed.

        :param event: An event that applies to an info handled by this
                      EntrySet
        :type event: Bcfg2.Server.FileMonitor.Event
        :returns: None
        """
        if event.filename == 'info.xml':
            self.infoxml = None
        elif event.filename in [':info', 'info']:
            self.metadata = DEFAULT_FILE_METADATA.copy()

    def bind_info_to_entry(self, entry, metadata):
        """ Shortcut to call :func:`bind_info` with the base
        info/info.xml for this EntrySet.

        :param entry: The abstract entry to bind the info to. This
                      will be modified in place
        :type entry: lxml.etree._Element
        :param metadata: The client metadata to get info for
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :returns: None
        """
        bind_info(entry, metadata, infoxml=self.infoxml, default=self.metadata)

    def bind_entry(self, entry, metadata):
        """ Return the single best fully-bound entry from the set of
        available entries for the specified client.

        :param entry: The abstract entry to bind the info to
        :type entry: lxml.etree._Element
        :param metadata: The client metadata to get info for
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :returns: lxml.etree._Element - the fully-bound entry
        """
        self.bind_info_to_entry(entry, metadata)
        return self.best_matching(metadata).bind_entry(entry, metadata)


class GroupSpool(Plugin, Generator):
    """ A GroupSpool is a collection of
    :class:`Bcfg2.Server.Plugin.helpers.EntrySet` objects -- i.e., a
    directory tree, each directory in which may contain files that are
    specific to groups/clients/etc. """

    #: ``filename_pattern`` is used as the ``basename`` argument to the
    #: :attr:`es_cls` callable.  It may or may not be a regex,
    #: depending on the :attr:`EntrySet.basename_is_regex` setting.
    filename_pattern = ""

    #: ``es_child_cls`` is a callable that will be used as the
    #: ``entry_type`` argument to the :attr:`es_cls` callable.  It must
    #: return objects that will represent individual files in the
    #: GroupSpool.  For instance,
    #: :class:`Bcfg2.Server.Plugin.helpers.SpecificData`.
    es_child_cls = object

    #: ``es_cls`` is a callable that must return objects that will be
    #: used to represent directories (i.e., sets of entries) within the
    #: GroupSpool.  E.g.,
    #: :class:`Bcfg2.Server.Plugin.helpers.EntrySet`.  The returned
    #: object must implement a callable called ``bind_entry`` that has
    #: the same signature as :attr:`EntrySet.bind_entry`.
    es_cls = EntrySet

    #: The entry type (i.e., the XML tag) handled by this GroupSpool
    #: object.
    entry_type = 'Path'

    def __init__(self, core, datastore):
        Plugin.__init__(self, core, datastore)
        Generator.__init__(self)

        #: See :class:`Bcfg2.Server.Plugins.interfaces.Generator` for
        #: details on the Entries attribute.
        self.Entries[self.entry_type] = {}

        #: ``entries`` is a dict whose keys are :func:`event_id` return
        #: values and whose values are :attr:`es_cls` objects. It ties
        #: the directories handled by this GroupSpools to the
        #: :attr:`es_cls` objects that handle each directory.
        self.entries = {}
        self.handles = {}
        self.AddDirectoryMonitor('')
        self.encoding = core.setup['encoding']
    __init__.__doc__ = Plugin.__init__.__doc__

    def add_entry(self, event):
        """ This method handles two functions:

        * Adding a new entry of type :attr:`es_cls` to track a new
          directory.
        * Passing off an event on a file to the correct entry object
          to handle it.

        :param event: An event that applies to a file or directory
                      handled by this GroupSpool
        :type event: Bcfg2.Server.FileMonitor.Event
        :returns: None
        """
        epath = self.event_path(event)
        ident = self.event_id(event)
        if os.path.isdir(epath):
            self.AddDirectoryMonitor(epath[len(self.data):])
        if ident not in self.entries and os.path.isfile(epath):
            dirpath = self.data + ident
            self.entries[ident] = self.es_cls(self.filename_pattern,
                                              dirpath,
                                              self.es_child_cls,
                                              self.encoding)
            self.Entries[self.entry_type][ident] = \
                self.entries[ident].bind_entry
        if not os.path.isdir(epath):
            # do not pass through directory events
            self.entries[ident].handle_event(event)

    def event_path(self, event):
        """ Return the full path to the filename affected by an event.
        :class:`Bcfg2.Server.FileMonitor.Event` objects just contain
        the filename, not the full path, so this function reconstructs
        the fill path based on the path to the :attr:`es_cls` object
        that handles the event.

        :param event: An event that applies to a file or directory
                      handled by this GroupSpool
        :type event: Bcfg2.Server.FileMonitor.Event
        :returns: string
        """
        return os.path.join(self.data,
                            self.handles[event.requestID].lstrip("/"),
                            event.filename)

    def event_id(self, event):
        """ Return a string that can be used to relate the event
        unambiguously to a single :attr:`es_cls` object in the
        :attr:`entries` dict.  In practice, this means:

        * If the event is on a directory, ``event_id`` returns the
          full path to the directory.
        * If the event is on a file, ``event_id`` returns the full
          path to the directory the file is in.

        :param event: An event that applies to a file or directory
                      handled by this GroupSpool
        :type event: Bcfg2.Server.FileMonitor.Event
        :returns: string
        """
        epath = self.event_path(event)
        if os.path.isdir(epath):
            return os.path.join(self.handles[event.requestID].lstrip("/"),
                                event.filename)
        else:
            return self.handles[event.requestID].rstrip("/")

    def set_debug(self, debug):
        for entry in self.entries.values():
            if hasattr(entry, "set_debug"):
                entry.set_debug(debug)
        return Plugin.set_debug(self, debug)
    set_debug.__doc__ = Plugin.set_debug.__doc__

    def HandleEvent(self, event):
        """ HandleEvent is the event dispatcher for GroupSpool
        objects.  It receives all events and dispatches them the
        appropriate handling object (e.g., one of the :attr:`es_cls`
        objects in :attr:`entries`), function (e.g.,
        :func:`add_entry`), or behavior (e.g., deleting an entire
        entry set).

        :param event: An event that applies to a file or directory
                      handled by this GroupSpool
        :type event: Bcfg2.Server.FileMonitor.Event
        :returns: None
        """
        action = event.code2str()
        if event.filename[0] == '/':
            return
        ident = self.event_id(event)

        if action in ['exists', 'created']:
            self.add_entry(event)
        elif action == 'changed':
            if ident in self.entries:
                self.entries[ident].handle_event(event)
            else:
                # got a changed event for a file we didn't know
                # about. go ahead and process this as a 'created', but
                # warn
                self.logger.warning("Got changed event for unknown file %s" %
                                    ident)
                self.add_entry(event)
        elif action == 'deleted':
            fbase = self.handles[event.requestID] + event.filename
            if fbase in self.entries:
                # a directory was deleted
                del self.entries[fbase]
                del self.Entries[self.entry_type][fbase]
            elif ident in self.entries:
                self.entries[ident].handle_event(event)
            elif ident not in self.entries:
                self.logger.warning("Got deleted event for unknown file %s" %
                                    ident)

    def AddDirectoryMonitor(self, relative):
        """ Add a FAM monitor to a new directory and set the
        appropriate event handler.

        :param relative: The path to the directory relative to the
                         base data directory of the GroupSpool object.
        :type relative: string
        :returns: None
        """
        if not relative.endswith('/'):
            relative += '/'
        name = self.data + relative
        if relative not in list(self.handles.values()):
            if not os.path.isdir(name):
                self.logger.error("Failed to open directory %s" % name)
                return
            reqid = self.core.fam.AddMonitor(name, self)
            self.handles[reqid] = relative

########NEW FILE########
__FILENAME__ = interfaces
""" Interface definitions for Bcfg2 server plugins """

import os
import sys
import copy
import threading
import lxml.etree
import Bcfg2.Server
from Bcfg2.Compat import Queue, Empty, Full, cPickle
from Bcfg2.Server.Plugin.base import Plugin
from Bcfg2.Server.Plugin.exceptions import PluginInitError, \
    MetadataRuntimeError, MetadataConsistencyError


class Generator(object):
    """ Generator plugins contribute to literal client configurations.
    That is, they generate entry contents.

    An entry is generated in one of two ways:

    #. The Bcfg2 core looks in the ``Entries`` dict attribute of the
       plugin object.  ``Entries`` is expected to be a dict whose keys
       are entry tags (e.g., ``"Path"``, ``"Service"``, etc.) and
       whose values are dicts; those dicts should map the ``name``
       attribute of an entry to a callable that will be called to
       generate the content.  The callable will receive two arguments:
       the abstract entry (as an lxml.etree._Element object), and the
       client metadata object the entry is being generated for.

    #. If the entry is not listed in ``Entries``, the Bcfg2 core calls
       :func:`HandlesEntry`; if that returns True, then it calls
       :func:`HandleEntry`.
    """

    def HandlesEntry(self, entry, metadata):  # pylint: disable=W0613
        """ HandlesEntry is the slow path method for routing
        configuration binding requests.  It is called if the
        ``Entries`` dict does not contain a method for binding the
        entry.

        :param entry: The entry to bind
        :type entry: lxml.etree._Element
        :param metadata: The client metadata
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :return: bool - Whether or not this plugin can handle the entry
        :raises: :class:`Bcfg2.Server.Plugin.exceptions.PluginExecutionError`
        """
        return False

    def HandleEntry(self, entry, metadata):  # pylint: disable=W0613
        """ HandleEntry is the slow path method for binding
        configuration binding requests.  It is called if the
        ``Entries`` dict does not contain a method for binding the
        entry, and :func:`HandlesEntry`
        returns True.

        :param entry: The entry to bind
        :type entry: lxml.etree._Element
        :param metadata: The client metadata
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :return: lxml.etree._Element - The fully bound entry
        :raises: :class:`Bcfg2.Server.Plugin.exceptions.PluginExecutionError`
        """
        return entry


class Structure(object):
    """ Structure Plugins contribute to abstract client
    configurations.  That is, they produce lists of entries that will
    be generated for a client. """

    def BuildStructures(self, metadata):
        """ Build a list of lxml.etree._Element objects that will be
        added to the top-level ``<Configuration>`` tag of the client
        configuration.  Consequently, each object in the list returned
        by ``BuildStructures()`` must consist of a container tag
        (e.g., ``<Bundle>`` or ``<Independent>``) which contains the
        entry tags.  It must not return a list of entry tags.

        :param metadata: The client metadata
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :return: list of lxml.etree._Element objects
        """
        raise NotImplementedError


class Metadata(object):
    """ Metadata plugins handle initial metadata construction,
    accumulating data from :class:`Connector` plugins, and producing
    :class:`Bcfg2.Server.Plugins.Metadata.ClientMetadata` objects. """

    def viz(self, hosts, bundles, key, only_client, colors):
        """ Return a string containing a graphviz document that maps
        out the Metadata for :ref:`bcfg2-admin viz <server-admin-viz>`

        :param hosts: Include hosts in the graph
        :type hosts: bool
        :param bundles: Include bundles in the graph
        :type bundles: bool
        :param key: Include a key in the graph
        :type key: bool
        :param only_client: Only include data for the specified client
        :type only_client: string
        :param colors: Use the specified graphviz colors
        :type colors: list of strings
        :return: string
        """
        raise NotImplementedError

    def set_version(self, client, version):
        """ Set the version for the named client to the specified
        version string.

        :param client: Hostname of the client
        :type client: string
        :param profile: Client Bcfg2 version
        :type profile: string
        :return: None
        :raises: :class:`Bcfg2.Server.Plugin.exceptions.MetadataRuntimeError`,
                 :class:`Bcfg2.Server.Plugin.exceptions.MetadataConsistencyError`
        """
        pass

    def set_profile(self, client, profile, address):
        """ Set the profile for the named client to the named profile
        group.

        :param client: Hostname of the client
        :type client: string
        :param profile: Name of the profile group
        :type profile: string
        :param address: Address pair of ``(<ip address>, <hostname>)``
        :type address: tuple
        :return: None
        :raises: :class:`Bcfg2.Server.Plugin.exceptions.MetadataRuntimeError`,
                 :class:`Bcfg2.Server.Plugin.exceptions.MetadataConsistencyError`
        """
        pass

    # pylint: disable=W0613
    def resolve_client(self, address, cleanup_cache=False):
        """ Resolve the canonical name of this client.  If this method
        is not implemented, the hostname claimed by the client is
        used.  (This may be a security risk; it's highly recommended
        that you implement ``resolve_client`` if you are writing a
        Metadata plugin.)

        :param address: Address pair of ``(<ip address>, <hostname>)``
        :type address: tuple
        :param cleanup_cache: Whether or not to remove expire the
                              entire client hostname resolution class
        :type cleanup_cache: bool
        :return: string - canonical client hostname
        :raises: :class:`Bcfg2.Server.Plugin.exceptions.MetadataRuntimeError`,
                 :class:`Bcfg2.Server.Plugin.exceptions.MetadataConsistencyError`
        """
        return address[1]
    # pylint: enable=W0613

    def AuthenticateConnection(self, cert, user, password, address):
        """ Authenticate the given client.

        :param cert: an x509 certificate
        :type cert: dict
        :param user: The username of the user trying to authenticate
        :type user: string
        :param password: The password supplied by the client
        :type password: string
        :param addresspair: An address pair of ``(<ip address>,
                            <hostname>)``
        :type addresspair: tuple
        :return: bool - True if the authenticate succeeds, False otherwise
        """
        raise NotImplementedError

    def get_initial_metadata(self, client_name):
        """ Return a
        :class:`Bcfg2.Server.Plugins.Metadata.ClientMetadata` object
        that fully describes everything the Metadata plugin knows
        about the named client.

        :param client_name: The hostname of the client
        :type client_name: string
        :return: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        """
        raise NotImplementedError

    def merge_additional_data(self, imd, source, data):
        """ Add arbitrary data from a
        :class:`Connector` plugin to the given
        metadata object.

        :param imd: An initial metadata object
        :type imd: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :param source: The name of the plugin providing this data
        :type source: string
        :param data: The data to add
        :type data: any
        :return: None
        """
        raise NotImplementedError

    def merge_additional_groups(self, imd, groups):
        """ Add groups from a
        :class:`Connector` plugin to the given
        metadata object.

        :param imd: An initial metadata object
        :type imd: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :param groups: The groups to add
        :type groups: list of strings
        :return: None
        """
        raise NotImplementedError

    def update_client_list(self):
        """ Re-read the cached list of clients """
        raise NotImplementedError


class Connector(object):
    """ Connector plugins augment client metadata instances with
    additional data, additional groups, or both. """

    def get_additional_groups(self, metadata):  # pylint: disable=W0613
        """ Return a list of additional groups for the given client.
        Each group can be either the name of a group (a string), or a
        :class:`Bcfg2.Server.Plugins.Metadata.MetadataGroup` object
        that defines other data besides just the name.  Note that you
        cannot return a
        :class:`Bcfg2.Server.Plugins.Metadata.MetadataGroup` object
        that clobbers a group defined by another plugin; the original
        group will be used instead.  For instance, assume the
        following in ``Metadata/groups.xml``:

        .. code-block:: xml

            <Groups>
              ...
              <Group name="foo" public="false"/>
            </Groups>

        You could not subsequently return a
        :class:`Bcfg2.Server.Plugins.Metadata.MetadataGroup` object
        with ``public=True``; a warning would be issued, and the
        original (non-public) ``foo`` group would be used.

        :param metadata: The client metadata
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :return: list of strings or
                 :class:`Bcfg2.Server.Plugins.Metadata.MetadataGroup`
                 objects.
        """
        return list()

    def get_additional_data(self, metadata):  # pylint: disable=W0613
        """ Return arbitrary additional data for the given
        ClientMetadata object.  By convention this is usually a dict
        object, but doesn't need to be.

        :param metadata: The client metadata
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :return: dict
        """
        return dict()


class Probing(object):
    """ Probing plugins can collect data from clients and process it.
    """

    def GetProbes(self, metadata):
        """ Return a list of probes for the given client.  Each probe
        should be an lxml.etree._Element object that adheres to
        the following specification.  Each probe must the following
        attributes:

        * ``name``: The unique name of the probe.
        * ``source``: The origin of the probe; probably the name of
          the plugin that supplies the probe.
        * ``interpreter``: The command that will be run on the client
          to interpret the probe script.  Compiled (i.e.,
          non-interpreted) probes are not supported.

        The text of the XML tag should be the contents of the probe,
        i.e., the code that will be run on the client.

        :param metadata: The client metadata
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :return: list of lxml.etree._Element objects
        """
        raise NotImplementedError

    def ReceiveData(self, metadata, datalist):
        """ Process data returned from the probes for the given
        client.  ``datalist`` is a list of lxml.etree._Element
        objects, each of which is a single tag; the ``name`` attribute
        holds the unique name of the probe that was run, and the text
        contents of the tag hold the results of the probe.

        :param metadata: The client metadata
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :param datalist: The probe data
        :type datalist: list of lxml.etree._Element objects
        :return: None
        """
        raise NotImplementedError


class Statistics(Plugin):
    """ Statistics plugins handle statistics for clients.  In general,
    you should avoid using Statistics and use
    :class:`ThreadedStatistics` instead."""

    create = False

    def process_statistics(self, client, xdata):
        """ Process the given XML statistics data for the specified
        client.

        :param metadata: The client metadata
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :param data: The statistics data
        :type data: lxml.etree._Element
        :return: None
        """
        raise NotImplementedError


class Threaded(object):
    """ Threaded plugins use threads in any way.  The thread must be
    started after daemonization, so this class implements a single
    method, :func:`start_threads`, that can be used to start threads
    after daemonization of the server core. """

    def start_threads(self):
        """ Start this plugin's threads after daemonization.

        :return: None
        :raises: :class:`Bcfg2.Server.Plugin.exceptions.PluginInitError`
        """
        raise NotImplementedError


class ThreadedStatistics(Statistics, Threaded, threading.Thread):
    """ ThreadedStatistics plugins process client statistics in a
    separate thread. """

    def __init__(self, core, datastore):
        Statistics.__init__(self, core, datastore)
        Threaded.__init__(self)
        threading.Thread.__init__(self)
        # Event from the core signaling an exit
        self.terminate = core.terminate
        self.work_queue = Queue(100000)
        self.pending_file = os.path.join(datastore, "etc",
                                         "%s.pending" % self.name)
        self.daemon = False

    def start_threads(self):
        self.start()

    def _save(self):
        """Save any pending data to a file."""
        pending_data = []
        try:
            while not self.work_queue.empty():
                (metadata, xdata) = self.work_queue.get_nowait()
                data = \
                    lxml.etree.tostring(xdata,
                                        xml_declaration=False).decode("UTF-8")
                pending_data.append((metadata.hostname, data))
        except Empty:
            pass

        try:
            savefile = open(self.pending_file, 'w')
            cPickle.dump(pending_data, savefile)
            savefile.close()
            self.logger.info("Saved pending %s data" % self.name)
        except (IOError, TypeError):
            err = sys.exc_info()[1]
            self.logger.warning("Failed to save pending data: %s" % err)

    def _load(self):
        """Load any pending data from a file."""
        if not os.path.exists(self.pending_file):
            return True
        pending_data = []
        try:
            savefile = open(self.pending_file, 'r')
            pending_data = cPickle.load(savefile)
            savefile.close()
        except (IOError, cPickle.UnpicklingError):
            err = sys.exc_info()[1]
            self.logger.warning("Failed to load pending data: %s" % err)
            return False
        for (pmetadata, pdata) in pending_data:
            # check that shutdown wasnt called early
            if self.terminate.isSet():
                return False

            try:
                while True:
                    try:
                        metadata = self.core.build_metadata(pmetadata)
                        break
                    except MetadataRuntimeError:
                        pass

                    self.terminate.wait(5)
                    if self.terminate.isSet():
                        return False

                self.work_queue.put_nowait(
                    (metadata,
                     lxml.etree.XML(pdata, parser=Bcfg2.Server.XMLParser)))
            except Full:
                self.logger.warning("Queue.Full: Failed to load queue data")
                break
            except lxml.etree.LxmlError:
                lxml_error = sys.exc_info()[1]
                self.logger.error("Unable to load saved interaction: %s" %
                                  lxml_error)
            except MetadataConsistencyError:
                self.logger.error("Unable to load metadata for save "
                                  "interaction: %s" % pmetadata)
        try:
            os.unlink(self.pending_file)
        except OSError:
            self.logger.error("Failed to unlink save file: %s" %
                              self.pending_file)
        self.logger.info("Loaded pending %s data" % self.name)
        return True

    def run(self):
        if not self._load():
            return
        while not self.terminate.isSet() and self.work_queue is not None:
            try:
                (client, xdata) = self.work_queue.get(block=True, timeout=2)
            except Empty:
                continue
            except:
                err = sys.exc_info()[1]
                self.logger.error("ThreadedStatistics: %s" % err)
                continue
            self.handle_statistic(client, xdata)
        if self.work_queue is not None and not self.work_queue.empty():
            self._save()

    def process_statistics(self, metadata, data):
        try:
            self.work_queue.put_nowait((metadata, copy.copy(data)))
        except Full:
            self.logger.warning("%s: Queue is full.  Dropping interactions." %
                                self.name)

    def handle_statistic(self, metadata, data):
        """ Process the given XML statistics data for the specified
        client object.  This differs from the
        :func:`Statistics.process_statistics` method only in that
        ThreadedStatistics first adds the data to a queue, and then
        processes them in a separate thread.

        :param metadata: The client metadata
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :param data: The statistics data
        :type data: lxml.etree._Element
        :return: None
        """
        raise NotImplementedError


# pylint: disable=C0111
# Someone who understands these interfaces better needs to write docs
# for PullSource and PullTarget
class PullSource(object):
    def GetExtra(self, client):  # pylint: disable=W0613
        return []

    def GetCurrentEntry(self, client, e_type, e_name):
        raise NotImplementedError


class PullTarget(object):
    def AcceptChoices(self, entry, metadata):
        raise NotImplementedError

    def AcceptPullData(self, specific, new_entry, verbose):
        raise NotImplementedError
# pylint: enable=C0111


class Decision(object):
    """ Decision plugins produce decision lists for affecting which
    entries are actually installed on clients. """

    def GetDecisions(self, metadata, mode):
        """ Return a list of tuples of ``(<entry type>, <entry
        name>)`` to be used as the decision list for the given
        client in the specified mode.

        :param metadata: The client metadata
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :param mode: The decision mode ("whitelist" or "blacklist")
        :type mode: string
        :return: list of tuples
        """
        raise NotImplementedError


class StructureValidator(object):
    """ StructureValidator plugins can modify the list of structures
    after it has been created but before the entries have been
    concretely bound. """

    def validate_structures(self, metadata, structures):
        """ Given a list of structures (i.e., of tags that contain
        entry tags), modify that list or the structures in it
        in-place.

        :param metadata: The client metadata
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :param config: A list of lxml.etree._Element objects
                       describing the structures (i.e., bundles) for
                       this client.  This can be modified in place.
        :type config: list of lxml.etree._Element
        :returns: None
        :raises: :class:`Bcfg2.Server.Plugin.exceptions.ValidationError`
        """
        raise NotImplementedError


class GoalValidator(object):
    """ GoalValidator plugins can modify the concretely-bound configuration of
    a client as a last stage before the configuration is sent to the
    client. """

    def validate_goals(self, metadata, config):
        """ Given a monolithic XML document of the full configuration,
        modify the document in-place.

        :param metadata: The client metadata
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :param config: The full configuration for the client
        :type config: lxml.etree._Element
        :returns: None
        :raises: :class:`Bcfg2.Server.Plugin.exceptions:ValidationError`
        """
        raise NotImplementedError


class Version(Plugin):
    """ Version plugins interact with various version control systems. """

    create = False

    #: The path to the VCS metadata file or directory, relative to the
    #: base of the Bcfg2 repository.  E.g., for Subversion this would
    #: be ".svn"
    __vcs_metadata_path__ = None

    def __init__(self, core, datastore):
        Plugin.__init__(self, core, datastore)

        if core.setup['vcs_root']:
            self.vcs_root = core.setup['vcs_root']
        else:
            self.vcs_root = datastore
        if self.__vcs_metadata_path__:
            self.vcs_path = os.path.join(self.vcs_root,
                                         self.__vcs_metadata_path__)

            if not os.path.exists(self.vcs_path):
                raise PluginInitError("%s is not present" % self.vcs_path)
        else:
            self.vcs_path = None
    __init__.__doc__ = Plugin.__init__.__doc__ + """
.. autoattribute:: __vcs_metadata_path__ """

    def get_revision(self):
        """ Return the current revision of the Bcfg2 specification.
        This will be included in the ``revision`` attribute of the
        top-level tag of the XML configuration sent to the client.

        :returns: string - the current version
        """
        raise NotImplementedError


class ClientRunHooks(object):
    """ ClientRunHooks can hook into various parts of a client run to
    perform actions at various times without needing to pretend to be
    a different plugin type. """

    def start_client_run(self, metadata):
        """ Invoked at the start of a client run, after all probe data
        has been received and decision lists have been queried (if
        applicable), but before the configuration is generated.

        :param metadata: The client metadata object
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :returns: None
        """
        pass

    def end_client_run(self, metadata):
        """ Invoked at the end of a client run, immediately after
        :class:`GoalValidator` plugins have been run and just before
        the configuration is returned to the client.

        :param metadata: The client metadata object
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :returns: None
        """
        pass

    def end_statistics(self, metadata):
        """ Invoked after statistics are processed for a client.

        :param metadata: The client metadata object
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :returns: None
        """
        pass


class Caching(object):
    """ A plugin that caches more than just the data received from the
    FAM.  This presents a unified interface to clear the cache. """

    def expire_cache(self, key=None):
        """ Expire the cache associated with the given key.

        :param key: The key to expire the cache for.  Because cache
                    implementations vary tremendously between plugins,
                    this could be any number of things, but generally
                    a hostname.  It also may or may not be possible to
                    expire the cache for a single host; this interface
                    does not require any guarantee about that.
        :type key: varies
        :returns: None
        """
        raise NotImplementedError

########NEW FILE########
__FILENAME__ = Account
"""This handles authentication setup."""

import Bcfg2.Server.Plugin


class Account(Bcfg2.Server.Plugin.Plugin,
              Bcfg2.Server.Plugin.Generator):
    """This module generates account config files,
    based on an internal data repo:
    static.(passwd|group|limits.conf) -> static entries
    dyn.(passwd|group) -> dynamic entries (usually acquired from yp or somesuch)
    useraccess -> users to be granted login access on some hosts
    superusers -> users to be granted root privs on all hosts
    rootlike -> users to be granted root privs on some hosts

    """
    name = 'Account'
    __author__ = 'bcfg-dev@mcs.anl.gov'
    deprecated = True

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.Generator.__init__(self)
        self.Entries = {'ConfigFile': {'/etc/passwd': self.from_yp_cb,
                                       '/etc/group': self.from_yp_cb,
                                       '/etc/security/limits.conf': self.gen_limits_cb,
                                       '/root/.ssh/authorized_keys': self.gen_root_keys_cb,
                                       '/etc/sudoers': self.gen_sudoers}}
        try:
            self.repository = Bcfg2.Server.Plugin.DirectoryBacked(self.data,
                                                                  self.core.fam)
        except:
            self.logger.error("Failed to load repos: %s, %s" % \
                              (self.data, "%s/ssh" % (self.data)))
            raise Bcfg2.Server.Plugin.PluginInitError

    def from_yp_cb(self, entry, metadata):
        """Build password file from cached yp data."""
        fname = entry.attrib['name'].split('/')[-1]
        entry.text = self.repository.entries["static.%s" % (fname)].data
        entry.text += self.repository.entries["dyn.%s" % (fname)].data
        perms = {'owner': 'root',
                 'group': 'root',
                 'mode': '0644'}
        [entry.attrib.__setitem__(key, value) for (key, value) in \
         list(perms.items())]

    def gen_limits_cb(self, entry, metadata):
        """Build limits entries based on current ACLs."""
        entry.text = self.repository.entries["static.limits.conf"].data
        superusers = self.repository.entries["superusers"].data.split()
        useraccess = [line.split(':') for line in \
                      self.repository.entries["useraccess"].data.split()]
        users = [user for (user, host) in \
                 useraccess if host == metadata.hostname.split('.')[0]]
        perms = {'owner': 'root',
                 'group': 'root',
                 'mode': '0600'}
        [entry.attrib.__setitem__(key, value) for (key, value) in \
         list(perms.items())]
        entry.text += "".join(["%s hard maxlogins 1024\n" % uname for uname in superusers + users])
        if "*" not in users:
            entry.text += "* hard maxlogins 0\n"

    def gen_root_keys_cb(self, entry, metadata):
        """Build root authorized keys file based on current ACLs."""
        superusers = self.repository.entries['superusers'].data.split()
        try:
            rootlike = [line.split(':', 1) for line in \
                        self.repository.entries['rootlike'].data.split()]
            superusers += [user for (user, host) in rootlike \
                           if host == metadata.hostname.split('.')[0]]
        except:
            pass
        rdata = self.repository.entries
        entry.text = "".join([rdata["%s.key" % user].data for user \
                              in superusers if \
                              ("%s.key" % user) in rdata])
        perms = {'owner': 'root',
                 'group': 'root',
                 'mode': '0600'}
        [entry.attrib.__setitem__(key, value) for (key, value) \
         in list(perms.items())]

    def gen_sudoers(self, entry, metadata):
        """Build root authorized keys file based on current ACLs."""
        superusers = self.repository.entries['superusers'].data.split()
        try:
            rootlike = [line.split(':', 1) for line in \
                        self.repository.entries['rootlike'].data.split()]
            superusers += [user for (user, host) in rootlike \
                           if host == metadata.hostname.split('.')[0]]
        except:
            pass
        entry.text = self.repository.entries['static.sudoers'].data
        entry.text += "".join(["%s ALL=(ALL) ALL\n" % uname \
                               for uname in superusers])
        perms = {'owner': 'root',
                 'group': 'root',
                 'mode': '0440'}
        [entry.attrib.__setitem__(key, value) for (key, value) \
         in list(perms.items())]

########NEW FILE########
__FILENAME__ = AWSTags
""" Query tags from AWS via boto, optionally setting group membership """

import os
import re
import sys
import Bcfg2.Server.Lint
import Bcfg2.Server.Plugin
from boto import connect_ec2
from Bcfg2.Cache import Cache
from Bcfg2.Compat import ConfigParser


class NoInstanceFound(Exception):
    """ Raised when there's no AWS instance for a given hostname """


class AWSTagPattern(object):
    """ Handler for a single Tag entry """

    def __init__(self, name, value, groups):
        self.name = re.compile(name)
        if value is not None:
            self.value = re.compile(value)
        else:
            self.value = value
        self.groups = groups

    def get_groups(self, tags):
        """ Get groups that apply to the given tag set """
        for key, value in tags.items():
            name_match = self.name.search(key)
            if name_match:
                if self.value is not None:
                    value_match = self.value.search(value)
                    if value_match:
                        return self._munge_groups(value_match)
                else:
                    return self._munge_groups(name_match)
                break
        return []

    def _munge_groups(self, match):
        """ Replace backreferences (``$1``, ``$2``) in Group tags with
        their values in the regex. """
        rv = []
        sub = match.groups()
        for group in self.groups:
            newg = group
            for idx in range(len(sub)):
                newg = newg.replace('$%s' % (idx + 1), sub[idx])
            rv.append(newg)
        return rv

    def __str__(self):
        if self.value:
            return "%s: %s=%s: %s" % (self.__class__.__name__, self.name,
                                      self.value, self.groups)
        else:
            return "%s: %s: %s" % (self.__class__.__name__, self.name,
                                   self.groups)


class PatternFile(Bcfg2.Server.Plugin.XMLFileBacked):
    """ representation of AWSTags config.xml """
    __identifier__ = None
    create = 'AWSTags'

    def __init__(self, filename, core=None):
        try:
            fam = core.fam
        except AttributeError:
            fam = None
        Bcfg2.Server.Plugin.XMLFileBacked.__init__(self, filename, fam=fam,
                                                   should_monitor=True)
        self.core = core
        self.tags = []

    def Index(self):
        Bcfg2.Server.Plugin.XMLFileBacked.Index(self)
        if (self.core and
            self.core.metadata_cache_mode in ['cautious', 'aggressive']):
            self.core.metadata_cache.expire()
        self.tags = []
        for entry in self.xdata.xpath('//Tag'):
            try:
                groups = [g.text for g in entry.findall('Group')]
                self.tags.append(AWSTagPattern(entry.get("name"),
                                               entry.get("value"),
                                               groups))
            except:  # pylint: disable=W0702
                self.logger.error("AWSTags: Failed to initialize pattern %s: "
                                  "%s" % (entry.get("name"),
                                          sys.exc_info()[1]))

    def get_groups(self, hostname, tags):
        """ return a list of groups that should be added to the given
        client based on patterns that match the hostname """
        ret = []
        for pattern in self.tags:
            try:
                ret.extend(pattern.get_groups(tags))
            except:  # pylint: disable=W0702
                self.logger.error("AWSTags: Failed to process pattern %s for "
                                  "%s" % (pattern, hostname),
                                  exc_info=1)
        return ret


class AWSTags(Bcfg2.Server.Plugin.Plugin,
              Bcfg2.Server.Plugin.Caching,
              Bcfg2.Server.Plugin.ClientRunHooks,
              Bcfg2.Server.Plugin.Connector):
    """ Query tags from AWS via boto, optionally setting group membership """
    __rmi__ = Bcfg2.Server.Plugin.Plugin.__rmi__ + ['expire_cache']

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.Caching.__init__(self)
        Bcfg2.Server.Plugin.ClientRunHooks.__init__(self)
        Bcfg2.Server.Plugin.Connector.__init__(self)
        try:
            key_id = self.core.setup.cfp.get("awstags", "access_key_id")
            secret_key = self.core.setup.cfp.get("awstags",
                                                 "secret_access_key")
        except (ConfigParser.NoSectionError, ConfigParser.NoOptionError):
            err = sys.exc_info()[1]
            raise Bcfg2.Server.Plugin.PluginInitError(
                "AWSTags is not configured in bcfg2.conf: %s" % err)
        self.debug_log("%s: Connecting to EC2" % self.name)
        self._ec2 = connect_ec2(aws_access_key_id=key_id,
                                aws_secret_access_key=secret_key)
        self._tagcache = Cache()
        try:
            self._keep_cache = self.core.setup.cfp.getboolean("awstags",
                                                              "cache")
        except (ConfigParser.NoSectionError, ConfigParser.NoOptionError):
            self._keep_cache = True

        self.config = PatternFile(os.path.join(self.data, 'config.xml'),
                                  core=core)

    def _load_instance(self, hostname):
        """ Load an instance from EC2 whose private DNS name matches
        the given hostname """
        self.debug_log("AWSTags: Loading instance with private-dns-name=%s" %
                       hostname)
        filters = {'private-dns-name': hostname}
        reservations = self._ec2.get_all_instances(filters=filters)
        if reservations:
            res = reservations[0]
            if res.instances:
                return res.instances[0]
        raise NoInstanceFound(
            "AWSTags: No instance found with private-dns-name=%s" %
            hostname)

    def _get_tags_from_ec2(self, hostname):
        """ Get tags for the given host from EC2. This does not use
        the local caching layer. """
        self.debug_log("AWSTags: Getting tags for %s from AWS" %
                       hostname)
        try:
            return self._load_instance(hostname).tags
        except NoInstanceFound:
            self.debug_log(sys.exc_info()[1])
            return dict()

    def get_tags(self, metadata):
        """ Get tags for the given host.  This caches the tags locally
        if 'cache' in the ``[awstags]`` section of ``bcfg2.conf`` is
        true. """
        if not self._keep_cache:
            return self._get_tags_from_ec2(metadata)

        if metadata.hostname not in self._tagcache:
            self._tagcache[metadata.hostname] = \
                self._get_tags_from_ec2(metadata.hostname)
        return self._tagcache[metadata.hostname]

    def expire_cache(self, key=None):
        self._tagcache.expire(key=key)

    def start_client_run(self, metadata):
        self.expire_cache(key=metadata.hostname)

    def get_additional_data(self, metadata):
        return self.get_tags(metadata)

    def get_additional_groups(self, metadata):
        return self.config.get_groups(metadata.hostname,
                                      self.get_tags(metadata))


class AWSTagsLint(Bcfg2.Server.Lint.ServerPlugin):
    """ ``bcfg2-lint`` plugin to check all given :ref:`AWSTags
    <server-plugins-connectors-awstags>` patterns for validity. """

    def Run(self):
        cfg = self.core.plugins['AWSTags'].config
        for entry in cfg.xdata.xpath('//Tag'):
            self.check(entry, "name")
            if entry.get("value"):
                self.check(entry, "value")

    @classmethod
    def Errors(cls):
        return {"pattern-fails-to-initialize": "error"}

    def check(self, entry, attr):
        """ Check a single attribute (``name`` or ``value``) of a
        single entry for validity. """
        try:
            re.compile(entry.get(attr))
        except re.error:
            self.LintError("pattern-fails-to-initialize",
                           "'%s' regex could not be compiled: %s\n    %s" %
                           (attr, sys.exc_info()[1], entry.get("name")))

########NEW FILE########
__FILENAME__ = Base
"""This module sets up a base list of configuration entries."""

import copy
import lxml.etree
import Bcfg2.Server.Plugin
from itertools import chain


class Base(Bcfg2.Server.Plugin.Plugin,
           Bcfg2.Server.Plugin.Structure,
           Bcfg2.Server.Plugin.XMLDirectoryBacked):
    """This Structure is good for the pile of independent configs
    needed for most actual systems.
    """
    name = 'Base'
    __author__ = 'bcfg-dev@mcs.anl.gov'
    __child__ = Bcfg2.Server.Plugin.StructFile
    deprecated = True

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.Structure.__init__(self)
        Bcfg2.Server.Plugin.XMLDirectoryBacked.__init__(self, self.data,
                                                        self.core.fam)

    def BuildStructures(self, metadata):
        """Build structures for client described by metadata."""
        ret = lxml.etree.Element("Independent", version='2.0')
        fragments = list(chain(*[base.Match(metadata)
                                 for base in list(self.entries.values())]))
        for frag in fragments:
            ret.append(copy.copy(frag))
        return [ret]

########NEW FILE########
__FILENAME__ = Bundler
"""This provides bundle clauses with translation functionality."""

import copy
import logging
import lxml.etree
import os
import os.path
import re
import sys
import Bcfg2.Server
import Bcfg2.Server.Plugin
import Bcfg2.Server.Lint

try:
    import genshi.template.base
    from Bcfg2.Server.Plugins.TGenshi import removecomment, TemplateFile
    HAS_GENSHI = True
except ImportError:
    HAS_GENSHI = False


SETUP = None


class BundleFile(Bcfg2.Server.Plugin.StructFile):
    """ Representation of a bundle XML file """
    def get_xml_value(self, metadata):
        """ get the XML data that applies to the given client """
        bundlename = os.path.splitext(os.path.basename(self.name))[0]
        bundle = lxml.etree.Element('Bundle', name=bundlename)
        for item in self.Match(metadata):
            bundle.append(copy.copy(item))
        return bundle


if HAS_GENSHI:
    class BundleTemplateFile(TemplateFile,
                             Bcfg2.Server.Plugin.StructFile):
        """ Representation of a Genshi-templated bundle XML file """

        def __init__(self, name, specific, encoding, fam=None):
            TemplateFile.__init__(self, name, specific, encoding)
            Bcfg2.Server.Plugin.StructFile.__init__(self, name, fam=fam)
            self.logger = logging.getLogger(name)

        def get_xml_value(self, metadata):
            """ get the rendered XML data that applies to the given
            client """
            if not hasattr(self, 'template'):
                msg = "No parsed template information for %s" % self.name
                self.logger.error(msg)
                raise Bcfg2.Server.Plugin.PluginExecutionError(msg)
            stream = self.template.generate(
                metadata=metadata,
                repo=SETUP['repo']).filter(removecomment)
            data = lxml.etree.XML(
                stream.render('xml', strip_whitespace=False).encode(),
                parser=Bcfg2.Server.XMLParser)
            bundlename = os.path.splitext(os.path.basename(self.name))[0]
            bundle = lxml.etree.Element('Bundle', name=bundlename)
            for item in self.Match(metadata, data):
                bundle.append(copy.deepcopy(item))
            return bundle

        def Match(self, metadata, xdata):  # pylint: disable=W0221
            """Return matching fragments of parsed template."""
            rv = []
            for child in xdata.getchildren():
                rv.extend(self._match(child, metadata))
            self.logger.debug("File %s got %d match(es)" % (self.name,
                                                            len(rv)))
            return rv

    class SGenshiTemplateFile(BundleTemplateFile):
        """ provided for backwards compat with the deprecated SGenshi
        plugin """
        pass


class Bundler(Bcfg2.Server.Plugin.Plugin,
              Bcfg2.Server.Plugin.Structure,
              Bcfg2.Server.Plugin.XMLDirectoryBacked):
    """ The bundler creates dependent clauses based on the
    bundle/translation scheme from Bcfg1. """
    __author__ = 'bcfg-dev@mcs.anl.gov'
    patterns = re.compile(r'^(?P<name>.*)\.(xml|genshi)$')

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.Structure.__init__(self)
        self.encoding = core.setup['encoding']
        self.__child__ = self.template_dispatch
        Bcfg2.Server.Plugin.XMLDirectoryBacked.__init__(self, self.data,
                                                        self.core.fam)
        global SETUP
        SETUP = core.setup

    def template_dispatch(self, name, _):
        """ Add the correct child entry type to Bundler depending on
        whether the XML file in question is a plain XML file or a
        templated bundle """
        bundle = lxml.etree.parse(name, parser=Bcfg2.Server.XMLParser)
        nsmap = bundle.getroot().nsmap
        if (name.endswith('.genshi') or
            ('py' in nsmap and
             nsmap['py'] == 'http://genshi.edgewall.org/')):
            if HAS_GENSHI:
                spec = Bcfg2.Server.Plugin.Specificity()
                return BundleTemplateFile(name, spec, self.encoding,
                                          fam=self.core.fam)
            else:
                raise Bcfg2.Server.Plugin.PluginExecutionError("Genshi not "
                                                               "available: %s"
                                                               % name)
        else:
            return BundleFile(name, fam=self.fam)

    def BuildStructures(self, metadata):
        """Build all structures for client (metadata)."""
        bundleset = []

        bundle_entries = {}
        for key, item in self.entries.items():
            bundle_entries.setdefault(
                self.patterns.match(os.path.basename(key)).group('name'),
                []).append(item)

        for bundlename in metadata.bundles:
            try:
                entries = bundle_entries[bundlename]
            except KeyError:
                self.logger.error("Bundler: Bundle %s does not exist" %
                                  bundlename)
                continue
            try:
                bundleset.append(entries[0].get_xml_value(metadata))
            except genshi.template.base.TemplateError:
                err = sys.exc_info()[1]
                self.logger.error("Bundler: Failed to render templated bundle "
                                  "%s: %s" % (bundlename, err))
            except:
                self.logger.error("Bundler: Unexpected bundler error for %s" %
                                  bundlename, exc_info=1)
        return bundleset


class BundlerLint(Bcfg2.Server.Lint.ServerPlugin):
    """ Perform various :ref:`Bundler
    <server-plugins-structures-bundler-index>` checks. """

    def Run(self):
        self.missing_bundles()
        for bundle in self.core.plugins['Bundler'].entries.values():
            if (self.HandlesFile(bundle.name) and
                (not HAS_GENSHI or
                 not isinstance(bundle, BundleTemplateFile))):
                self.bundle_names(bundle)

    @classmethod
    def Errors(cls):
        return {"bundle-not-found": "error",
                "inconsistent-bundle-name": "warning"}

    def missing_bundles(self):
        """ Find bundles listed in Metadata but not implemented in
        Bundler. """
        if self.files is None:
            # when given a list of files on stdin, this check is
            # useless, so skip it
            groupdata = self.metadata.groups_xml.xdata
            ref_bundles = set([b.get("name")
                               for b in groupdata.findall("//Bundle")])

            allbundles = self.core.plugins['Bundler'].entries.keys()
            for bundle in ref_bundles:
                xmlbundle = "%s.xml" % bundle
                genshibundle = "%s.genshi" % bundle
                if (xmlbundle not in allbundles and
                    genshibundle not in allbundles):
                    self.LintError("bundle-not-found",
                                   "Bundle %s referenced, but does not exist" %
                                   bundle)

    def bundle_names(self, bundle):
        """ Verify bundle name attribute matches filename.

        :param bundle: The bundle to verify
        :type bundle: Bcfg2.Server.Plugins.Bundler.BundleFile
        """
        try:
            xdata = lxml.etree.XML(bundle.data)
        except AttributeError:
            # genshi template
            xdata = lxml.etree.parse(bundle.template.filepath).getroot()

        fname = os.path.splitext(os.path.basename(bundle.name))[0]
        bname = xdata.get('name')
        if fname != bname:
            self.LintError("inconsistent-bundle-name",
                           "Inconsistent bundle name: filename is %s, "
                           "bundle name is %s" % (fname, bname))

########NEW FILE########
__FILENAME__ = Bzr
""" The Bzr plugin provides a revision interface for Bcfg2 repos using
bazaar. """

import Bcfg2.Server.Plugin
from bzrlib.workingtree import WorkingTree
from bzrlib import errors


class Bzr(Bcfg2.Server.Plugin.Version):
    """ The Bzr plugin provides a revision interface for Bcfg2 repos
    using bazaar. """
    __author__ = 'bcfg-dev@mcs.anl.gov'

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Version.__init__(self, core, datastore)
        self.logger.debug("Initialized Bazaar plugin with directory %s at "
                          "revision = %s" % (self.vcs_root,
                                             self.get_revision()))

    def get_revision(self):
        """Read Bazaar revision information for the Bcfg2 repository."""
        try:
            working_tree = WorkingTree.open(self.vcs_root)
            revision = str(working_tree.branch.revno())
            if (working_tree.has_changes(working_tree.basis_tree()) or
                working_tree.unknowns()):
                revision += "+"
        except errors.NotBranchError:
            msg = "Failed to read Bazaar branch"
            self.logger.error(msg)
            raise Bcfg2.Server.Plugin.PluginExecutionError(msg)
        return revision

########NEW FILE########
__FILENAME__ = CfgAuthorizedKeysGenerator
""" The CfgAuthorizedKeysGenerator generates ``authorized_keys`` files
based on an XML specification of which SSH keypairs should granted
access. """

import lxml.etree
from Bcfg2.Server.Plugin import StructFile, PluginExecutionError
from Bcfg2.Server.Plugins.Cfg import CfgGenerator, SETUP, CFG
from Bcfg2.Server.Plugins.Metadata import ClientMetadata


class CfgAuthorizedKeysGenerator(CfgGenerator, StructFile):
    """ The CfgAuthorizedKeysGenerator generates authorized_keys files
    based on an XML specification of which SSH keypairs should granted
    access. """

    #: Different configurations for different clients/groups can be
    #: handled with Client and Group tags within authorizedkeys.xml
    __specific__ = False

    #: Handle authorized keys XML files
    __basenames__ = ['authorizedkeys.xml', 'authorized_keys.xml']

    #: This handler is experimental, in part because it depends upon
    #: the (experimental) CfgPrivateKeyCreator handler
    experimental = True

    def __init__(self, fname):
        CfgGenerator.__init__(self, fname, None, None)
        StructFile.__init__(self, fname)
        self.cache = dict()
        self.core = CFG.core
    __init__.__doc__ = CfgGenerator.__init__.__doc__

    @property
    def category(self):
        """ The name of the metadata category that generated keys are
        specific to """
        if (SETUP.cfp.has_section("sshkeys") and
            SETUP.cfp.has_option("sshkeys", "category")):
            return SETUP.cfp.get("sshkeys", "category")
        return None

    def handle_event(self, event):
        CfgGenerator.handle_event(self, event)
        StructFile.HandleEvent(self, event)
        self.cache = dict()
    handle_event.__doc__ = CfgGenerator.handle_event.__doc__

    def get_data(self, entry, metadata):
        spec = self.XMLMatch(metadata)
        rv = []
        for allow in spec.findall("Allow"):
            options = []
            if allow.find("Params") is not None:
                self.logger.warning("Use of <Params> in authorized_keys.xml "
                                    "is deprecated; use <Option> instead")
                options.extend("=".join(p)
                               for p in allow.find("Params").attrib.items())

            for opt in allow.findall("Option"):
                if opt.get("value"):
                    options.append("%s=%s" % (opt.get("name"),
                                              opt.get("value")))
                else:
                    options.append(opt.get("name"))

            pubkey_name = allow.get("from")
            if pubkey_name:
                host = allow.get("host")
                group = allow.get("group")
                category = allow.get("category", self.category)
                if host:
                    key_md = self.core.build_metadata(host)
                elif group:
                    key_md = ClientMetadata("dummy", group, [group], [],
                                            set(), set(), dict(), None,
                                            None, None, None)
                elif category and not metadata.group_in_category(category):
                    self.logger.warning("Cfg: %s ignoring Allow from %s: "
                                        "No group in category %s" %
                                        (metadata.hostname, pubkey_name,
                                         category))
                    continue
                else:
                    key_md = metadata

                key_entry = lxml.etree.Element("Path", name=pubkey_name)
                try:
                    self.core.Bind(key_entry, key_md)
                except PluginExecutionError:
                    self.logger.info("Cfg: %s skipping Allow from %s: "
                                     "No key found" % (metadata.hostname,
                                                       pubkey_name))
                    continue
                if not key_entry.text:
                    self.logger.warning("Cfg: %s skipping Allow from %s: "
                                        "Empty public key" %
                                        (metadata.hostname, pubkey_name))
                    continue
                pubkey = key_entry.text
            elif allow.text:
                pubkey = allow.text.strip()
            else:
                self.logger.warning("Cfg: %s ignoring empty Allow tag: %s" %
                                    (metadata.hostname,
                                     lxml.etree.tostring(allow)))
                continue
            rv.append(" ".join([",".join(options), pubkey]).strip())
        return "\n".join(rv)
    get_data.__doc__ = CfgGenerator.get_data.__doc__

########NEW FILE########
__FILENAME__ = CfgCatFilter
""" Handle .cat files, which append lines to and remove lines from
plaintext files """

from Bcfg2.Server.Plugins.Cfg import CfgFilter


class CfgCatFilter(CfgFilter):
    """ CfgCatFilter appends lines to and remove lines from plaintext
    :ref:`server-plugins-generators-Cfg` files"""

    #: Handle .cat files
    __extensions__ = ['cat']

    #: .cat files are deprecated
    deprecated = True

    def modify_data(self, entry, metadata, data):
        datalines = data.strip().split('\n')
        for line in self.data.split('\n'):
            if not line:
                continue
            if line.startswith('+'):
                datalines.append(line[1:])
            elif line.startswith('-'):
                if line[1:] in datalines:
                    datalines.remove(line[1:])
        return "\n".join(datalines) + "\n"
    modify_data.__doc__ = CfgFilter.modify_data.__doc__

########NEW FILE########
__FILENAME__ = CfgCheetahGenerator
""" The CfgCheetahGenerator allows you to use the `Cheetah
<http://www.cheetahtemplate.org/>`_ templating system to generate
:ref:`server-plugins-generators-cfg` files. """

from Bcfg2.Server.Plugin import PluginExecutionError
from Bcfg2.Server.Plugins.Cfg import CfgGenerator, SETUP

try:
    from Cheetah.Template import Template
    HAS_CHEETAH = True
except ImportError:
    HAS_CHEETAH = False


class CfgCheetahGenerator(CfgGenerator):
    """ The CfgCheetahGenerator allows you to use the `Cheetah
    <http://www.cheetahtemplate.org/>`_ templating system to generate
    :ref:`server-plugins-generators-cfg` files. """

    #: Handle .cheetah files
    __extensions__ = ['cheetah']

    #: Low priority to avoid matching host- or group-specific
    #: .crypt.cheetah files
    __priority__ = 50

    #: :class:`Cheetah.Template.Template` compiler settings
    settings = dict(useStackFrames=False)

    def __init__(self, fname, spec, encoding):
        CfgGenerator.__init__(self, fname, spec, encoding)
        if not HAS_CHEETAH:
            raise PluginExecutionError("Cheetah is not available")
    __init__.__doc__ = CfgGenerator.__init__.__doc__

    def get_data(self, entry, metadata):
        template = Template(self.data.decode(self.encoding),
                            compilerSettings=self.settings)
        template.metadata = metadata
        template.name = entry.get('realname', entry.get('name'))
        template.path = entry.get('realname', entry.get('name'))
        template.source_path = self.name
        template.repo = SETUP['repo']
        return template.respond()
    get_data.__doc__ = CfgGenerator.get_data.__doc__

########NEW FILE########
__FILENAME__ = CfgDiffFilter
""" Handle .diff files, which apply diffs to plaintext files """

import os
import tempfile
from Bcfg2.Server.Plugin import PluginExecutionError
from subprocess import Popen, PIPE
from Bcfg2.Server.Plugins.Cfg import CfgFilter


class CfgDiffFilter(CfgFilter):
    """ CfgDiffFilter applies diffs to plaintext
    :ref:`server-plugins-generators-Cfg` files """

    #: Handle .diff files
    __extensions__ = ['diff']

    #: .diff files are deprecated
    deprecated = True

    def modify_data(self, entry, metadata, data):
        basehandle, basename = tempfile.mkstemp()
        open(basename, 'w').write(data)
        os.close(basehandle)

        cmd = ["patch", "-u", "-f", basename]
        patch = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE)
        stderr = patch.communicate(input=self.data)[1]
        ret = patch.wait()
        output = open(basename, 'r').read()
        os.unlink(basename)
        if ret != 0:
            raise PluginExecutionError("Error applying diff %s: %s" %
                                       (self.name, stderr))
        return output
    modify_data.__doc__ = CfgFilter.modify_data.__doc__

########NEW FILE########
__FILENAME__ = CfgEncryptedCheetahGenerator
""" Handle encrypted Cheetah templates (.crypt.cheetah or
.cheetah.crypt files)"""

from Bcfg2.Server.Plugins.Cfg.CfgCheetahGenerator import CfgCheetahGenerator
from Bcfg2.Server.Plugins.Cfg.CfgEncryptedGenerator \
    import CfgEncryptedGenerator


class CfgEncryptedCheetahGenerator(CfgCheetahGenerator, CfgEncryptedGenerator):
    """ CfgEncryptedCheetahGenerator lets you encrypt your Cheetah
    :ref:`server-plugins-generators-cfg` files on the server """

    #: handle .crypt.cheetah or .cheetah.crypt files
    __extensions__ = ['cheetah.crypt', 'crypt.cheetah']

    #: Override low priority from parent class
    __priority__ = 0

    def handle_event(self, event):
        CfgEncryptedGenerator.handle_event(self, event)
    handle_event.__doc__ = CfgEncryptedGenerator.handle_event.__doc__

    def get_data(self, entry, metadata):
        return CfgCheetahGenerator.get_data(self, entry, metadata)
    get_data.__doc__ = CfgCheetahGenerator.get_data.__doc__

########NEW FILE########
__FILENAME__ = CfgEncryptedGenerator
""" CfgEncryptedGenerator lets you encrypt your plaintext
:ref:`server-plugins-generators-cfg` files on the server. """

from Bcfg2.Server.Plugin import PluginExecutionError
from Bcfg2.Server.Plugins.Cfg import CfgGenerator, SETUP
try:
    from Bcfg2.Encryption import bruteforce_decrypt, EVPError, \
        get_algorithm, CFG_SECTION
    HAS_CRYPTO = True
except ImportError:
    HAS_CRYPTO = False


class CfgEncryptedGenerator(CfgGenerator):
    """ CfgEncryptedGenerator lets you encrypt your plaintext
    :ref:`server-plugins-generators-cfg` files on the server. """

    #: Handle .crypt files
    __extensions__ = ["crypt"]

    #: Low priority to avoid matching host- or group-specific
    #: .genshi.crypt and .cheetah.crypt files
    __priority__ = 50

    def __init__(self, fname, spec, encoding):
        CfgGenerator.__init__(self, fname, spec, encoding)
        if not HAS_CRYPTO:
            raise PluginExecutionError("M2Crypto is not available")

    def handle_event(self, event):
        CfgGenerator.handle_event(self, event)
        if self.data is None:
            return
        # todo: let the user specify a passphrase by name
        try:
            self.data = bruteforce_decrypt(
                self.data, setup=SETUP,
                algorithm=get_algorithm(SETUP))
        except EVPError:
            strict = SETUP.cfp.get(CFG_SECTION, "decrypt",
                                   default="strict")
            msg = "Cfg: Failed to decrypt %s" % self.name
            if strict:
                raise PluginExecutionError(msg)
            else:
                self.logger.debug(msg)

    def get_data(self, entry, metadata):
        if self.data is None:
            raise PluginExecutionError("Failed to decrypt %s" % self.name)
        return CfgGenerator.get_data(self, entry, metadata)

########NEW FILE########
__FILENAME__ = CfgEncryptedGenshiGenerator
""" Handle encrypted Genshi templates (.crypt.genshi or .genshi.crypt
files) """

from Bcfg2.Compat import StringIO
from Bcfg2.Server.Plugin import PluginExecutionError
from Bcfg2.Server.Plugins.Cfg import SETUP
from Bcfg2.Server.Plugins.Cfg.CfgGenshiGenerator import CfgGenshiGenerator

try:
    from Bcfg2.Encryption import bruteforce_decrypt, get_algorithm
    HAS_CRYPTO = True
except ImportError:
    HAS_CRYPTO = False

try:
    from genshi.template import TemplateLoader
except ImportError:
    # CfgGenshiGenerator will raise errors if genshi doesn't exist
    TemplateLoader = object  # pylint: disable=C0103


class EncryptedTemplateLoader(TemplateLoader):
    """ Subclass :class:`genshi.template.TemplateLoader` to decrypt
    the data on the fly as it's read in using
    :func:`Bcfg2.Encryption.bruteforce_decrypt` """
    def _instantiate(self, cls, fileobj, filepath, filename, encoding=None):
        plaintext = \
            StringIO(bruteforce_decrypt(fileobj.read(),
                                        algorithm=get_algorithm(SETUP)))
        return TemplateLoader._instantiate(self, cls, plaintext, filepath,
                                           filename, encoding=encoding)


class CfgEncryptedGenshiGenerator(CfgGenshiGenerator):
    """ CfgEncryptedGenshiGenerator lets you encrypt your Genshi
    :ref:`server-plugins-generators-cfg` files on the server """

    #: handle .crypt.genshi or .genshi.crypt files
    __extensions__ = ['genshi.crypt', 'crypt.genshi']

    #: Override low priority from parent class
    __priority__ = 0

    #: Use a TemplateLoader class that decrypts the data on the fly
    #: when it's read in
    __loader_cls__ = EncryptedTemplateLoader

    def __init__(self, fname, spec, encoding):
        CfgGenshiGenerator.__init__(self, fname, spec, encoding)
        if not HAS_CRYPTO:
            raise PluginExecutionError("M2Crypto is not available")

########NEW FILE########
__FILENAME__ = CfgExternalCommandVerifier
""" Invoke an external command to verify file contents """

import os
import sys
import shlex
from Bcfg2.Server.Plugin import PluginExecutionError
from subprocess import Popen, PIPE
from Bcfg2.Server.Plugins.Cfg import CfgVerifier, CfgVerificationError


class CfgExternalCommandVerifier(CfgVerifier):
    """ Invoke an external script to verify
    :ref:`server-plugins-generators-cfg` file contents """

    #: Handle :file:`:test` files
    __basenames__ = [':test']

    def __init__(self, name, specific, encoding):
        CfgVerifier.__init__(self, name, specific, encoding)
        self.cmd = []
    __init__.__doc__ = CfgVerifier.__init__.__doc__

    def verify_entry(self, entry, metadata, data):
        try:
            proc = Popen(self.cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE)
            out, err = proc.communicate(input=data)
            rv = proc.wait()
            if rv != 0:
                # pylint: disable=E1103
                raise CfgVerificationError(err.strip() or out.strip() or
                                           "Non-zero return value %s" % rv)
                # pylint: enable=E1103
        except CfgVerificationError:
            raise
        except:
            err = sys.exc_info()[1]
            raise CfgVerificationError("Error running external command "
                                       "verifier: %s" % err)
    verify_entry.__doc__ = CfgVerifier.verify_entry.__doc__

    def handle_event(self, event):
        CfgVerifier.handle_event(self, event)
        if not self.data:
            return
        self.cmd = []
        if not os.access(self.name, os.X_OK):
            bangpath = self.data.splitlines()[0].strip()
            if bangpath.startswith("#!"):
                self.cmd.extend(shlex.split(bangpath[2:].strip()))
            else:
                raise PluginExecutionError("Cannot execute %s" % self.name)
        self.cmd.append(self.name)
    handle_event.__doc__ = CfgVerifier.handle_event.__doc__

########NEW FILE########
__FILENAME__ = CfgGenshiGenerator
""" The CfgGenshiGenerator allows you to use the `Genshi
<http://genshi.edgewall.org>`_ templating system to generate
:ref:`server-plugins-generators-cfg` files. """

import re
import sys
import traceback
from Bcfg2.Server.Plugin import PluginExecutionError
from Bcfg2.Server.Plugins.Cfg import CfgGenerator, SETUP

try:
    import genshi.core
    from genshi.template import TemplateLoader, NewTextTemplate
    from genshi.template.eval import UndefinedError, Suite
    #: True if Genshi libraries are available
    HAS_GENSHI = True

    def _genshi_removes_blank_lines():
        """ Genshi 0.5 uses the Python :mod:`compiler` package to
        compile genshi snippets to AST.  Genshi 0.6 uses some bespoke
        magic, because compiler has been deprecated.
        :func:`compiler.parse` produces an AST that removes all excess
        whitespace (e.g., blank lines), while
        :func:`genshi.template.astutil.parse` does not.  In order to
        determine which actual line of code an error occurs on, we
        need to know which is in use and how it treats blank lines.
        I've beat my head against this for hours and the best/only way
        I can find is to compile some genshi code with an error and
        see which line it's on."""
        code = """d = dict()

d['a']"""
        try:
            Suite(code).execute(dict())
        except KeyError:
            line = traceback.extract_tb(sys.exc_info()[2])[-1][1]
            if line == 2:
                return True
            else:
                return False

    #: True if Genshi removes all blank lines from a code block before
    #: executing it; False indicates that Genshi only removes leading
    #: and trailing blank lines. See
    #: :func:`_genshi_removes_blank_lines` for an explanation of this.
    GENSHI_REMOVES_BLANK_LINES = _genshi_removes_blank_lines()
except ImportError:
    TemplateLoader = None  # pylint: disable=C0103
    HAS_GENSHI = False


def removecomment(stream):
    """ A Genshi filter that removes comments from the stream.  This
    function is a generator.

    :param stream: The Genshi stream to remove comments from
    :type stream: genshi.core.Stream
    :returns: tuple of ``(kind, data, pos)``, as when iterating
              through a Genshi stream
    """
    for kind, data, pos in stream:
        if kind is genshi.core.COMMENT:
            continue
        yield kind, data, pos


class CfgGenshiGenerator(CfgGenerator):
    """ The CfgGenshiGenerator allows you to use the `Genshi
    <http://genshi.edgewall.org>`_ templating system to generate
    :ref:`server-plugins-generators-cfg` files. """

    #: Handle .genshi files
    __extensions__ = ['genshi']

    #: ``__loader_cls__`` is the class that will be instantiated to
    #: load the template files.  It must implement one public function,
    #: ``load()``, as :class:`genshi.template.TemplateLoader`.
    __loader_cls__ = TemplateLoader

    #: Ignore ``.genshi_include`` files so they can be used with the
    #: Genshi ``{% include ... %}`` directive without raising warnings.
    __ignore__ = ["genshi_include"]

    #: Low priority to avoid matching host- or group-specific
    #: .crypt.genshi files
    __priority__ = 50

    #: Error-handling in Genshi is pretty obtuse.  This regex is used
    #: to extract the first line of the code block that raised an
    #: exception in a Genshi template so we can provide a decent error
    #: message that actually tells the end user where an error
    #: occurred.
    pyerror_re = re.compile(r'<\w+ u?[\'"](.*?)\s*\.\.\.[\'"]>')

    def __init__(self, fname, spec, encoding):
        CfgGenerator.__init__(self, fname, spec, encoding)
        if not HAS_GENSHI:
            raise PluginExecutionError("Genshi is not available")
        self.template = None
        self.loader = self.__loader_cls__(max_cache_size=0)
    __init__.__doc__ = CfgGenerator.__init__.__doc__

    def get_data(self, entry, metadata):
        if self.template is None:
            raise PluginExecutionError("Failed to load template %s" %
                                       self.name)

        fname = entry.get('realname', entry.get('name'))
        stream = \
            self.template.generate(name=fname,
                                   metadata=metadata,
                                   path=self.name,
                                   source_path=self.name,
                                   repo=SETUP['repo']).filter(removecomment)
        try:
            try:
                return stream.render('text', encoding=self.encoding,
                                     strip_whitespace=False)
            except TypeError:
                return stream.render('text', encoding=self.encoding)
        except UndefinedError:
            # a failure in a genshi expression _other_ than %{ python ... %}
            err = sys.exc_info()[1]
            stack = traceback.extract_tb(sys.exc_info()[2])
            for quad in stack:
                if quad[0] == self.name:
                    raise PluginExecutionError("%s: %s at '%s'" %
                                               (err.__class__.__name__, err,
                                                quad[2]))
            raise
        except:
            self._handle_genshi_exception(sys.exc_info())
    get_data.__doc__ = CfgGenerator.get_data.__doc__

    def _handle_genshi_exception(self, exc):
        """ this is horrible, and I deeply apologize to whoever gets
        to maintain this after I go to the Great Beer Garden in the
        Sky.  genshi is incredibly opaque about what's being executed,
        so the only way I can find to determine which {% python %}
        block is being executed -- if there are multiples -- is to
        iterate through them and match the snippet of the first line
        that's in the traceback with the first non-empty line of the
        block. """

        # a failure in a %{ python ... %} block -- the snippet in
        # the traceback is just the beginning of the block.
        err = exc[1]
        stack = traceback.extract_tb(exc[2])

        # find the right frame of the stack
        for frame in reversed(stack):
            if frame[0] == self.name:
                lineno, func = frame[1:3]
                break
        else:
            # couldn't even find the stack frame, wtf.
            raise PluginExecutionError("%s: %s" %
                                       (err.__class__.__name__, err))

        execs = [contents
                 for etype, contents, _ in self.template.stream
                 if etype == self.template.EXEC]
        contents = None
        if len(execs) == 1:
            contents = execs[0]
        elif len(execs) > 1:
            match = self.pyerror_re.match(func)
            if match:
                firstline = match.group(0)
                for pyblock in execs:
                    if pyblock.startswith(firstline):
                        contents = pyblock
                        break
        # else, no EXEC blocks -- WTF?
        if contents:
            # we now have the bogus block, but we need to get the
            # offending line.  To get there, we do (line number given
            # in the exception) - (firstlineno from the internal
            # genshi code object of the snippet) = (line number of the
            # line with an error within the block, with blank lines
            # removed as appropriate for
            # :attr:`GENSHI_REMOVES_BLANK_LINES`)
            code = contents.source.strip().splitlines()
            if GENSHI_REMOVES_BLANK_LINES:
                code = [l for l in code if l.strip()]
            try:
                line = code[lineno - contents.code.co_firstlineno]
                raise PluginExecutionError("%s: %s at '%s'" %
                                           (err.__class__.__name__, err,
                                            line))
            except IndexError:
                raise PluginExecutionError("%s: %s" %
                                           (err.__class__.__name__, err))
        raise

    def handle_event(self, event):
        CfgGenerator.handle_event(self, event)
        try:
            self.template = self.loader.load(self.name, cls=NewTextTemplate,
                                             encoding=self.encoding)
        except:
            raise PluginExecutionError("Failed to load template: %s" %
                                       sys.exc_info()[1])
    handle_event.__doc__ = CfgGenerator.handle_event.__doc__

########NEW FILE########
__FILENAME__ = CfgInfoXML
""" Handle info.xml files """

from Bcfg2.Server.Plugin import PluginExecutionError, InfoXML
from Bcfg2.Server.Plugins.Cfg import CfgInfo


class CfgInfoXML(CfgInfo):
    """ CfgInfoXML handles :file:`info.xml` files for
    :ref:`server-plugins-generators-cfg` """

    #: Handle :file:`info.xml` files
    __basenames__ = ['info.xml']

    def __init__(self, path):
        CfgInfo.__init__(self, path)
        self.infoxml = InfoXML(path)
    __init__.__doc__ = CfgInfo.__init__.__doc__

    def bind_info_to_entry(self, entry, metadata):
        mdata = dict()
        self.infoxml.pnode.Match(metadata, mdata, entry=entry)
        if 'Info' not in mdata:
            raise PluginExecutionError("Failed to set metadata for file %s" %
                                       entry.get('name'))
        self._set_info(entry, mdata['Info'][None])
    bind_info_to_entry.__doc__ = CfgInfo.bind_info_to_entry.__doc__

    def handle_event(self, event):
        self.infoxml.HandleEvent()
    handle_event.__doc__ = CfgInfo.handle_event.__doc__

    def _set_info(self, entry, info):
        CfgInfo._set_info(self, entry, info)
        if '__children__' in info:
            for child in info['__children__']:
                entry.append(child)
    _set_info.__doc__ = CfgInfo._set_info.__doc__

########NEW FILE########
__FILENAME__ = CfgLegacyInfo
""" Handle info and :info files """

import Bcfg2.Server.Plugin
from Bcfg2.Server.Plugins.Cfg import CfgInfo


class CfgLegacyInfo(CfgInfo):
    """ CfgLegacyInfo handles :file:`info` and :file:`:info` files for
    :ref:`server-plugins-generators-cfg` """

    #: Handle :file:`info` and :file:`:info`
    __basenames__ = ['info', ':info']

    #: CfgLegacyInfo is deprecated.  Use
    #: :class:`Bcfg2.Server.Plugins.Cfg.CfgInfoXML.CfgInfoXML` instead.
    deprecated = True

    def __init__(self, path):
        CfgInfo.__init__(self, path)
        self.path = path

        #: The set of info metadata stored in the file
        self.metadata = None
    __init__.__doc__ = CfgInfo.__init__.__doc__

    def bind_info_to_entry(self, entry, metadata):
        self._set_info(entry, self.metadata)
    bind_info_to_entry.__doc__ = CfgInfo.bind_info_to_entry.__doc__

    def handle_event(self, event):
        if event.code2str() == 'deleted':
            return
        self.metadata = dict()
        for line in open(self.path).readlines():
            match = Bcfg2.Server.Plugin.INFO_REGEX.match(line)
            if not match:
                self.logger.warning("Failed to parse line in %s: %s" %
                                    (event.filename, line))
                continue
            else:
                for key, value in list(match.groupdict().items()):
                    if value:
                        self.metadata[key] = value
        if ('mode' in self.metadata and len(self.metadata['mode']) == 3):
            self.metadata['mode'] = "0%s" % self.metadata['mode']
    handle_event.__doc__ = CfgInfo.handle_event.__doc__

########NEW FILE########
__FILENAME__ = CfgPlaintextGenerator
""" CfgPlaintextGenerator is a
:class:`Bcfg2.Server.Plugins.Cfg.CfgGenerator` that handles plain text
(i.e., non-templated) :ref:`server-plugins-generators-cfg` files."""

from Bcfg2.Server.Plugins.Cfg import CfgGenerator


class CfgPlaintextGenerator(CfgGenerator):
    """ CfgPlaintextGenerator is a
    :class:`Bcfg2.Server.Plugins.Cfg.CfgGenerator` that handles plain
    text (i.e., non-templated) :ref:`server-plugins-generators-cfg`
    files. The base Generator class already implements this
    functionality, so CfgPlaintextGenerator doesn't need to do
    anything itself."""

    #: Very low priority to avoid matching host- or group-specific
    #: files with other extensions -- e.g., .genshi, .crypt, etc.
    __priority__ = 100

########NEW FILE########
__FILENAME__ = CfgPrivateKeyCreator
""" The CfgPrivateKeyCreator creates SSH keys on the fly. """

import os
import shutil
import tempfile
import subprocess
from Bcfg2.Server.Plugin import PluginExecutionError, StructFile
from Bcfg2.Server.Plugins.Cfg import CfgCreator, CfgCreationError, SETUP
from Bcfg2.Server.Plugins.Cfg.CfgPublicKeyCreator import CfgPublicKeyCreator
try:
    import Bcfg2.Encryption
    HAS_CRYPTO = True
except ImportError:
    HAS_CRYPTO = False


class CfgPrivateKeyCreator(CfgCreator, StructFile):
    """The CfgPrivateKeyCreator creates SSH keys on the fly. """

    #: Different configurations for different clients/groups can be
    #: handled with Client and Group tags within privkey.xml
    __specific__ = False

    #: Handle XML specifications of private keys
    __basenames__ = ['privkey.xml']

    def __init__(self, fname):
        CfgCreator.__init__(self, fname)
        StructFile.__init__(self, fname)

        pubkey_path = os.path.dirname(self.name) + ".pub"
        pubkey_name = os.path.join(pubkey_path, os.path.basename(pubkey_path))
        self.pubkey_creator = CfgPublicKeyCreator(pubkey_name)

    @property
    def category(self):
        """ The name of the metadata category that generated keys are
        specific to """
        if (SETUP.cfp.has_section("sshkeys") and
            SETUP.cfp.has_option("sshkeys", "category")):
            return SETUP.cfp.get("sshkeys", "category")
        return None

    @property
    def passphrase(self):
        """ The passphrase used to encrypt private keys """
        if (HAS_CRYPTO and
            SETUP.cfp.has_section("sshkeys") and
            SETUP.cfp.has_option("sshkeys", "passphrase")):
            return Bcfg2.Encryption.get_passphrases(SETUP)[
                SETUP.cfp.get("sshkeys", "passphrase")]
        return None

    def handle_event(self, event):
        CfgCreator.handle_event(self, event)
        StructFile.HandleEvent(self, event)

    def _gen_keypair(self, metadata, spec=None):
        """ Generate a keypair according to the given client medata
        and key specification.

        :param metadata: The client metadata to generate keys for
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :param spec: The key specification to follow when creating the
                     keys. This should be an XML document that only
                     contains key specification data that applies to
                     the given client metadata, and may be obtained by
                     doing ``self.XMLMatch(metadata)``
        :type spec: lxml.etree._Element
        :returns: string - The filename of the private key
        """
        if spec is None:
            spec = self.XMLMatch(metadata)

        # set key parameters
        ktype = "rsa"
        bits = None
        params = spec.find("Params")
        if params is not None:
            bits = params.get("bits")
            ktype = params.get("type", ktype)
        try:
            passphrase = spec.find("Passphrase").text
        except AttributeError:
            passphrase = ''
        tempdir = tempfile.mkdtemp()
        try:
            filename = os.path.join(tempdir, "privkey")

            # generate key pair
            cmd = ["ssh-keygen", "-f", filename, "-t", ktype]
            if bits:
                cmd.extend(["-b", bits])
            cmd.append("-N")
            log_cmd = cmd[:]
            cmd.append(passphrase)
            if passphrase:
                log_cmd.append("******")
            else:
                log_cmd.append("''")
            self.debug_log("Cfg: Generating new SSH key pair: %s" %
                           " ".join(log_cmd))
            proc = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                    stderr=subprocess.PIPE)
            err = proc.communicate()[1]
            if proc.wait():
                raise CfgCreationError("Cfg: Failed to generate SSH key pair "
                                       "at %s for %s: %s" %
                                       (filename, metadata.hostname, err))
            elif err:
                self.logger.warning("Cfg: Generated SSH key pair at %s for %s "
                                    "with errors: %s" % (filename,
                                                         metadata.hostname,
                                                         err))
            return filename
        except:
            shutil.rmtree(tempdir)
            raise

    def get_specificity(self, metadata, spec=None):
        """ Get config settings for key generation specificity
        (per-host or per-group).

        :param metadata: The client metadata to create data for
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :param spec: The key specification to follow when creating the
                     keys. This should be an XML document that only
                     contains key specification data that applies to
                     the given client metadata, and may be obtained by
                     doing ``self.XMLMatch(metadata)``
        :type spec: lxml.etree._Element
        :returns: dict - A dict of specificity arguments suitable for
                  passing to
                  :func:`Bcfg2.Server.Plugins.Cfg.CfgCreator.write_data`
                  or
                  :func:`Bcfg2.Server.Plugins.Cfg.CfgCreator.get_filename`
        """
        if spec is None:
            spec = self.XMLMatch(metadata)
        category = spec.get("category", self.category)
        if category is None:
            per_host_default = "true"
        else:
            per_host_default = "false"
        per_host = spec.get("perhost", per_host_default).lower() == "true"

        specificity = dict(host=metadata.hostname)
        if category and not per_host:
            group = metadata.group_in_category(category)
            if group:
                specificity = dict(group=group,
                                   prio=int(spec.get("priority", 50)))
            else:
                self.logger.info("Cfg: %s has no group in category %s, "
                                 "creating host-specific key" %
                                 (metadata.hostname, category))
        return specificity

    # pylint: disable=W0221
    def create_data(self, entry, metadata):
        """ Create data for the given entry on the given client

        :param entry: The abstract entry to create data for.  This
                      will not be modified
        :type entry: lxml.etree._Element
        :param metadata: The client metadata to create data for
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :returns: string - The private key data
        """
        spec = self.XMLMatch(metadata)
        specificity = self.get_specificity(metadata, spec)
        filename = self._gen_keypair(metadata, spec)

        try:
            # write the public key, stripping the comment and
            # replacing it with a comment that specifies the filename.
            kdata = open(filename + ".pub").read().split()[:2]
            kdata.append(self.pubkey_creator.get_filename(**specificity))
            pubkey = " ".join(kdata) + "\n"
            self.pubkey_creator.write_data(pubkey, **specificity)

            # encrypt the private key, write to the proper place, and
            # return it
            privkey = open(filename).read()
            if HAS_CRYPTO and self.passphrase:
                self.debug_log("Cfg: Encrypting key data at %s" % filename)
                privkey = Bcfg2.Encryption.ssl_encrypt(
                    privkey,
                    self.passphrase,
                    algorithm=Bcfg2.Encryption.get_algorithm(SETUP))
                specificity['ext'] = '.crypt'

            self.write_data(privkey, **specificity)
            return privkey
        finally:
            shutil.rmtree(os.path.dirname(filename))
    # pylint: enable=W0221

    def Index(self):
        StructFile.Index(self)
        if HAS_CRYPTO:
            for el in self.xdata.xpath("//*[@encrypted]"):
                try:
                    el.text = self._decrypt(el).encode('ascii',
                                                       'xmlcharrefreplace')
                except UnicodeDecodeError:
                    self.logger.info("Cfg: Decrypted %s to gibberish, skipping"
                                     % el.tag)
                except Bcfg2.Encryption.EVPError:
                    default_strict = SETUP.cfp.get(
                        Bcfg2.Encryption.CFG_SECTION, "decrypt",
                        default="strict")
                    strict = self.xdata.get("decrypt",
                                            default_strict) == "strict"
                    msg = "Cfg: Failed to decrypt %s element in %s" % \
                        (el.tag, self.name)
                    if strict:
                        raise PluginExecutionError(msg)
                    else:
                        self.logger.debug(msg)

    def _decrypt(self, element):
        """ Decrypt a single encrypted element """
        if not element.text or not element.text.strip():
            return
        passes = Bcfg2.Encryption.get_passphrases(SETUP)
        try:
            passphrase = passes[element.get("encrypted")]
            try:
                return Bcfg2.Encryption.ssl_decrypt(
                    element.text,
                    passphrase,
                    algorithm=Bcfg2.Encryption.get_algorithm(SETUP))
            except Bcfg2.Encryption.EVPError:
                # error is raised below
                pass
        except KeyError:
            # bruteforce_decrypt raises an EVPError with a sensible
            # error message, so we just let it propagate up the stack
            return Bcfg2.Encryption.bruteforce_decrypt(
                element.text,
                passphrases=passes.values(),
                algorithm=Bcfg2.Encryption.get_algorithm(SETUP))
        raise Bcfg2.Encryption.EVPError("Failed to decrypt")

########NEW FILE########
__FILENAME__ = CfgPublicKeyCreator
""" The CfgPublicKeyCreator invokes
:class:`Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.CfgPrivateKeyCreator`
to create SSH keys on the fly. """

import os
import sys
import tempfile
import lxml.etree
from Bcfg2.Utils import Executor
from Bcfg2.Server.Plugin import StructFile, PluginExecutionError
from Bcfg2.Server.Plugins.Cfg import CfgCreator, CfgCreationError, CFG


class CfgPublicKeyCreator(CfgCreator, StructFile):
    """ .. currentmodule:: Bcfg2.Server.Plugins.Cfg

    The CfgPublicKeyCreator creates SSH public keys on the fly. It is
    invoked by :class:`CfgPrivateKeyCreator.CfgPrivateKeyCreator` to
    handle the creation of the public key, and can also call
    :class:`CfgPrivateKeyCreator.CfgPrivateKeyCreator` to trigger the
    creation of a keypair when a public key is created. """

    #: Different configurations for different clients/groups can be
    #: handled with Client and Group tags within privkey.xml
    __specific__ = False

    #: Handle XML specifications of private keys
    __basenames__ = ['pubkey.xml']

    def __init__(self, fname):
        CfgCreator.__init__(self, fname)
        StructFile.__init__(self, fname)
        self.cfg = CFG
        self.core = CFG.core
        self.cmd = Executor()

    def create_data(self, entry, metadata):
        if entry.get("name").endswith(".pub"):
            privkey = entry.get("name")[:-4]
        else:
            raise CfgCreationError("Cfg: Could not determine private key for "
                                   "%s: Filename does not end in .pub" %
                                   entry.get("name"))

        privkey_entry = lxml.etree.Element("Path", name=privkey)
        try:
            self.core.Bind(privkey_entry, metadata)
        except PluginExecutionError:
            raise CfgCreationError("Cfg: Could not bind %s (private key for "
                                   "%s): %s" % (privkey, self.name,
                                                sys.exc_info()[1]))

        try:
            eset = self.cfg.entries[privkey]
            creator = eset.best_matching(metadata,
                                         eset.get_handlers(metadata,
                                                           CfgCreator))
        except KeyError:
            raise CfgCreationError("Cfg: No private key defined for %s (%s)" %
                                   (self.name, privkey))
        except PluginExecutionError:
            raise CfgCreationError("Cfg: No privkey.xml defined for %s "
                                   "(private key for %s)" % (privkey,
                                                             self.name))

        specificity = creator.get_specificity(metadata)
        fname = self.get_filename(**specificity)

        # if the private key didn't exist, then creating it may have
        # created the private key, too.  check for it first.
        if os.path.exists(fname):
            return open(fname).read()
        else:
            # generate public key from private key
            fd, privfile = tempfile.mkstemp()
            try:
                os.fdopen(fd, 'w').write(privkey_entry.text)
                cmd = ["ssh-keygen", "-y", "-f", privfile]
                self.debug_log("Cfg: Extracting SSH public key from %s: %s" %
                               (privkey, " ".join(cmd)))
                result = self.cmd.run(cmd)
                if not result.success:
                    raise CfgCreationError("Cfg: Failed to extract public key "
                                           "from %s: %s" % (privkey,
                                                            result.error))
                self.write_data(result.stdout, **specificity)
                return result.stdout
            finally:
                os.unlink(privfile)

    def handle_event(self, event):
        CfgCreator.handle_event(self, event)
        StructFile.HandleEvent(self, event)
    handle_event.__doc__ = CfgCreator.handle_event.__doc__

########NEW FILE########
__FILENAME__ = Cvs
""" The Cvs plugin provides a revision interface for Bcfg2 repos using
cvs. """

from subprocess import Popen, PIPE
import Bcfg2.Server.Plugin


class Cvs(Bcfg2.Server.Plugin.Version):
    """ The Cvs plugin provides a revision interface for Bcfg2 repos
    using cvs."""
    __author__ = 'bcfg-dev@mcs.anl.gov'
    __vcs_metadata_path__ = "CVSROOT"

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Version.__init__(self, core, datastore)
        self.logger.debug("Initialized cvs plugin with CVS directory %s" %
                          self.vcs_path)

    def get_revision(self):
        """Read cvs revision information for the Bcfg2 repository."""
        try:
            data = Popen("env LC_ALL=C cvs log",
                         shell=True,
                         cwd=self.vcs_root,
                         stdout=PIPE).stdout.readlines()
            return data[3].strip('\n')
        except IndexError:
            msg = "Failed to read CVS log"
            self.logger.error(msg)
            self.logger.error('Ran command "cvs log" from directory %s' %
                              self.vcs_root)
            raise Bcfg2.Server.Plugin.PluginExecutionError(msg)

########NEW FILE########
__FILENAME__ = Darcs
""" Darcs is a version plugin for dealing with Bcfg2 repos stored in the
Darcs VCS. """

from subprocess import Popen, PIPE
import Bcfg2.Server.Plugin


class Darcs(Bcfg2.Server.Plugin.Version):
    """ Darcs is a version plugin for dealing with Bcfg2 repos stored
    in the Darcs VCS. """
    __author__ = 'bcfg-dev@mcs.anl.gov'
    __vcs_metadata_path__ = "_darcs"

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Version.__init__(self, core, datastore)
        self.logger.debug("Initialized Darcs plugin with darcs directory %s" %
                          self.vcs_path)

    def get_revision(self):
        """Read Darcs changeset information for the Bcfg2 repository."""
        try:
            data = Popen("env LC_ALL=C darcs changes",
                         shell=True,
                         cwd=self.vcs_root,
                         stdout=PIPE).stdout.readlines()
            revision = data[0].strip('\n')
        except:
            msg = "Failed to read darcs repository"
            self.logger.error(msg)
            self.logger.error('Ran command "darcs changes" from directory %s' %
                              self.vcs_root)
            raise Bcfg2.Server.Plugin.PluginExecutionError(msg)
        return revision

########NEW FILE########
__FILENAME__ = DBStats
""" DBstats provides a database-backed statistics handler """

import Bcfg2.Server.Plugin


class DBStats(Bcfg2.Server.Plugin.Plugin):
    """ DBstats provides a database-backed statistics handler """

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)
        self.logger.error("DBStats has been replaced with Reporting")
        self.logger.error("DBStats: Be sure to migrate your data "
                          "before running the report collector")
        raise Bcfg2.Server.Plugin.PluginInitError

########NEW FILE########
__FILENAME__ = Decisions
""" The Decisions plugin provides a flexible method to whitelist or
blacklist certain entries. """

import os
import lxml.etree
import Bcfg2.Server.Plugin


class DecisionFile(Bcfg2.Server.Plugin.SpecificData):
    """ Representation of a Decisions XML file """

    def __init__(self, name, specific, encoding):
        Bcfg2.Server.Plugin.SpecificData.__init__(self, name, specific,
                                                  encoding)
        self.contents = None

    def handle_event(self, event):
        Bcfg2.Server.Plugin.SpecificData.handle_event(self, event)
        self.contents = lxml.etree.XML(self.data)

    def get_decisions(self):
        """ Get a list of whitelist or blacklist tuples """
        return [(x.get('type'), x.get('name'))
                for x in self.contents.xpath('.//Decision')]


class Decisions(Bcfg2.Server.Plugin.EntrySet,
                Bcfg2.Server.Plugin.Plugin,
                Bcfg2.Server.Plugin.Decision):
    """ Decisions plugin

    Arguments:
    - `core`: Bcfg2.Core instance
    - `datastore`: File repository location
    """
    basename_is_regex = True
    __author__ = 'bcfg-dev@mcs.anl.gov'

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.Decision.__init__(self)
        Bcfg2.Server.Plugin.EntrySet.__init__(self, '(white|black)list',
                                              self.data, DecisionFile,
                                              core.setup['encoding'])
        core.fam.AddMonitor(self.data, self)

    def HandleEvent(self, event):
        """ Handle events on Decision files by passing them off to
        EntrySet.handle_event """
        if event.filename != self.path:
            return self.handle_event(event)

    def GetDecisions(self, metadata, mode):
        ret = []
        for cdt in self.get_matching(metadata):
            if os.path.basename(cdt.name).startswith(mode):
                ret.extend(cdt.get_decisions())
        return ret

########NEW FILE########
__FILENAME__ = Defaults
"""This generator provides rule-based entry mappings."""

import Bcfg2.Server.Plugin
import Bcfg2.Server.Plugins.Rules


class Defaults(Bcfg2.Server.Plugins.Rules.Rules,
               Bcfg2.Server.Plugin.GoalValidator):
    """Set default attributes on bound entries"""
    __author__ = 'bcfg-dev@mcs.anl.gov'

    # Rules is a Generator that happens to implement all of the
    # functionality we want, so we overload it, but Defaults should
    # _not_ handle any entries; it does its stuff in the structure
    # validation phase.  so we overload Handle(s)Entry and HandleEvent
    # to ensure that Defaults handles no entries, even though it's a
    # Generator.

    def HandlesEntry(self, entry, metadata):
        return False

    def HandleEvent(self, event):
        Bcfg2.Server.Plugin.XMLDirectoryBacked.HandleEvent(self, event)

    def validate_goals(self, metadata, config):
        """ Apply defaults """
        for struct in config.getchildren():
            for entry in struct.getchildren():
                try:
                    self.BindEntry(entry, metadata)
                except Bcfg2.Server.Plugin.PluginExecutionError:
                    # either no matching defaults (which is okay),
                    # or multiple matching defaults (which is not
                    # okay, but is logged).  either way, we don't
                    # care about the error.
                    pass

    @property
    def _regex_enabled(self):
        """ Defaults depends on regex matching, so force it enabled """
        return True

########NEW FILE########
__FILENAME__ = Deps
"""This plugin provides automatic dependency handling."""

import lxml.etree

import Bcfg2.Server.Plugin


class DNode(Bcfg2.Server.Plugin.INode):
    """DNode provides supports for single predicate types for dependencies."""
    def _load_children(self, data, idict):
        for item in data.getchildren():
            if item.tag in self.containers:
                self.children.append(self.__class__(item, idict, self))
            else:
                data = [(child.tag, child.get('name'))
                        for child in item.getchildren()]
                try:
                    self.contents[item.tag][item.get('name')] = data
                except KeyError:
                    self.contents[item.tag] = {item.get('name'): data}


class DepXMLSrc(Bcfg2.Server.Plugin.XMLSrc):
    __node__ = DNode


class Deps(Bcfg2.Server.Plugin.PrioDir,
           Bcfg2.Server.Plugin.StructureValidator):
    name = 'Deps'
    __author__ = 'bcfg-dev@mcs.anl.gov'
    __child__ = DepXMLSrc

    # Override the default sort_order (of 500) so that this plugin
    # gets handled after others running at the default. In particular,
    # we want to run after Packages, so we can see the final set of
    # packages that will be installed on the client.
    sort_order = 750

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.PrioDir.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.StructureValidator.__init__(self)
        self.cache = {}

    def HandleEvent(self, event):
        self.cache = {}
        Bcfg2.Server.Plugin.PrioDir.HandleEvent(self, event)

    def validate_structures(self, metadata, structures):
        """Examine the passed structures and append any additional
        prerequisite entries as defined by the files in Deps.
        """
        entries = []
        for structure in structures:
            for entry in structure.getchildren():
                tag = entry.tag
                if tag.startswith('Bound'):
                    tag = tag[5:]
                if (tag, entry.get('name')) not in entries \
                        and not isinstance(entry, lxml.etree._Comment):
                    entries.append((tag, entry.get('name')))
        entries.sort()
        entries = tuple(entries)
        gdata = list(metadata.groups)
        gdata.sort()
        gdata = tuple(gdata)

        # Check to see if we have cached the prereqs already
        if (entries, gdata) in self.cache:
            prereqs = self.cache[(entries, gdata)]
        else:
            prereqs = self.calculate_prereqs(metadata, entries)
            self.cache[(entries, gdata)] = prereqs

        newstruct = lxml.etree.Element("Independent")
        for tag, name in prereqs:
            try:
                lxml.etree.SubElement(newstruct, tag, name=name)
            except:
                self.logger.error("Failed to add dep entry for %s:%s" % (tag, name))
        structures.append(newstruct)


    def calculate_prereqs(self, metadata, entries):
        """Calculate the prerequisites defined in Deps for the passed
        set of entries.
        """
        prereqs = []
        [src.Cache(metadata) for src in self.entries.values()]

        toexamine = list(entries[:])
        while toexamine:
            entry = toexamine.pop()
            matching = [src for src in list(self.entries.values())
                        if src.cache and entry[0] in src.cache[1]
                        and entry[1] in src.cache[1][entry[0]]]
            if len(matching) > 1:
                prio = [int(src.priority) for src in matching]
                if prio.count(max(prio)) > 1:
                    self.logger.error("Found conflicting %s sources with same priority for %s, pkg %s" %
                                      (entry[0].lower(), metadata.hostname, entry[1]))
                    raise Bcfg2.Server.Plugin.PluginExecutionError
                index = prio.index(max(prio))
                matching = [matching[index]]
            elif len(matching) == 1:
                for prq in matching[0].cache[1][entry[0]][entry[1]]:
                    # XML comments seem to show up in the cache as a
                    # tuple with item 0 being callable. The logic
                    # below filters them out. Would be better to
                    # exclude them when we load the cache in the first
                    # place.
                    if prq not in prereqs and prq not in entries and not callable(prq[0]):
                        toexamine.append(prq)
                        prereqs.append(prq)
            else:
                continue

        return prereqs

########NEW FILE########
__FILENAME__ = Editor
import Bcfg2.Server.Plugin
import re
import lxml.etree


def linesub(pattern, repl, filestring):
    """Substitutes instances of pattern with repl in filestring."""
    if filestring == None:
        filestring = ''
    output = list()
    fileread = filestring.split('\n')
    for line in fileread:
        output.append(re.sub(pattern, repl, filestring))
    return '\n'.join(output)


class EditDirectives(Bcfg2.Server.Plugin.SpecificData):
    """This object handles the editing directives."""
    def ProcessDirectives(self, input):
        """Processes a list of edit directives on input."""
        temp = input
        for directive in self.data.split('\n'):
            directive = directive.split(',')
            temp = linesub(directive[0], directive[1], temp)
        return temp


class EditEntrySet(Bcfg2.Server.Plugin.EntrySet):
    def __init__(self, basename, path, entry_type, encoding):
        self.ignore = re.compile("^(\.#.*|.*~|\\..*\\.(tmp|sw[px])|%s\.H_.*)$" % path.split('/')[-1])
        Bcfg2.Server.Plugin.EntrySet.__init__(self,
                                              basename,
                                              path,
                                              entry_type,
                                              encoding)
        self.inputs = dict()

    def bind_entry(self, entry, metadata):
        client = metadata.hostname
        filename = entry.get('name')
        permdata = {'owner': 'root',
                    'group': 'root',
                    'mode': '0644'}
        [entry.attrib.__setitem__(key, permdata[key]) for key in permdata]
        entry.text = self.entries['edits'].ProcessDirectives(self.get_client_data(client))
        if not entry.text:
            entry.set('empty', 'true')
        try:
            f = open('%s/%s.H_%s' % (self.path, filename.split('/')[-1], client), 'w')
            f.write(entry.text)
            f.close()
        except:
            pass

    def get_client_data(self, client):
        return self.inputs[client]


class Editor(Bcfg2.Server.Plugin.GroupSpool,
             Bcfg2.Server.Plugin.Probing):
    name = 'Editor'
    __author__ = 'bcfg2-dev@mcs.anl.gov'
    filename_pattern = 'edits'
    es_child_cls = EditDirectives
    es_cls = EditEntrySet

    def GetProbes(self, _):
        '''Return a set of probes for execution on client'''
        probelist = list()
        for name in list(self.entries.keys()):
            probe = lxml.etree.Element('probe')
            probe.set('name', name)
            probe.set('source', "Editor")
            probe.text = "cat %s" % name
            probelist.append(probe)
        return probelist

    def ReceiveData(self, client, datalist):
        for data in datalist:
            self.entries[data.get('name')].inputs[client.hostname] = data.text

########NEW FILE########
__FILENAME__ = FileProbes
""" This module allows you to probe a client for a file, which is then
added to the specification.  On subsequent runs, the file will be
replaced on the client if it is missing; if it has changed on the
client, it can either be updated in the specification or replaced on
the client """

import os
import sys
import errno
import lxml.etree
import Bcfg2.Options
import Bcfg2.Server
import Bcfg2.Server.Plugin
from Bcfg2.Compat import b64decode

#: The probe we send to clients to get the file data.  Returns an XML
#: document describing the file and its metadata.  We avoid returning
#: a non-0 error code on most errors, since that could halt client
#: execution.
PROBECODE = """#!/usr/bin/env python

import os
import sys
import pwd
import grp
import Bcfg2.Client.XML
try:
    from Bcfg2.Compat import b64encode, oct_mode
except ImportError:
    from base64 import b64encode
    oct_mode = oct

path = "%s"

if not os.path.exists(path):
    sys.stderr.write("%%s does not exist" %% path)
    raise SystemExit(0)

try:
    stat = os.stat(path)
except:
    sys.stderr.write("Could not stat %%s: %%s" %% (path, sys.exc_info()[1]))
    raise SystemExit(0)
data = Bcfg2.Client.XML.Element("ProbedFileData",
                                name=path,
                                owner=pwd.getpwuid(stat[4])[0],
                                group=grp.getgrgid(stat[5])[0],
                                mode=oct_mode(stat[0] & 4095))
try:
    data.text = b64encode(open(path).read())
except:
    sys.stderr.write("Could not read %%s: %%s" %% (path, sys.exc_info()[1]))
    raise SystemExit(0)
print(Bcfg2.Client.XML.tostring(data, xml_declaration=False).decode('UTF-8'))
"""


class FileProbes(Bcfg2.Server.Plugin.Plugin,
                 Bcfg2.Server.Plugin.Probing):
    """ This module allows you to probe a client for a file, which is then
    added to the specification.  On subsequent runs, the file will be
    replaced on the client if it is missing; if it has changed on the
    client, it can either be updated in the specification or replaced on
    the client """
    __author__ = 'chris.a.st.pierre@gmail.com'

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.Probing.__init__(self)
        self.config = \
            Bcfg2.Server.Plugin.StructFile(os.path.join(self.data,
                                                        'config.xml'),
                                           fam=core.fam,
                                           should_monitor=True,
                                           create=self.name)
        self.entries = dict()
        self.probes = dict()

    def GetProbes(self, metadata):
        """Return a set of probes for execution on client."""
        if metadata.hostname not in self.probes:
            cfg = self.core.plugins['Cfg']
            self.entries[metadata.hostname] = dict()
            self.probes[metadata.hostname] = []
            for entry in self.config.Match(metadata):
                path = entry.get("name")
                # do not probe for files that are already in Cfg and
                # for which update is false; we can't possibly do
                # anything with the data we get from such a probe
                if (entry.get('update', 'false').lower() == "false" and
                    not cfg.has_generator(entry, metadata)):
                    continue
                self.entries[metadata.hostname][path] = entry
                probe = lxml.etree.Element('probe', name=path,
                                           source=self.name,
                                           interpreter="/usr/bin/env python")
                probe.text = PROBECODE % path
                self.probes[metadata.hostname].append(probe)
                self.debug_log("Adding file probe for %s to %s" %
                               (path, metadata.hostname))
        return self.probes[metadata.hostname]

    def ReceiveData(self, metadata, datalist):
        """Receive data from probe."""
        self.debug_log("Receiving file probe data from %s" % metadata.hostname)

        for data in datalist:
            if data.text is None:
                self.logger.error("Got null response to %s file probe from %s"
                                  % (data.get('name'), metadata.hostname))
            else:
                try:
                    self.write_data(
                        lxml.etree.XML(data.text,
                                       parser=Bcfg2.Server.XMLParser),
                        metadata)
                except lxml.etree.XMLSyntaxError:
                    # if we didn't get XML back from the probe, assume
                    # it's an error message
                    self.logger.error(data.text)

    def write_data(self, data, metadata):
        """Write the probed file data to the bcfg2 specification."""
        filename = data.get("name")
        contents = b64decode(data.text)
        entry = self.entries[metadata.hostname][filename]
        cfg = self.core.plugins['Cfg']
        specific = "%s.H_%s" % (os.path.basename(filename), metadata.hostname)
        # we can't use os.path.join() for this because specific
        # already has a leading /, which confuses os.path.join()
        fileloc = os.path.join(cfg.data,
                               os.path.join(filename, specific).lstrip("/"))

        create = False
        try:
            cfg.entries[filename].bind_entry(entry, metadata)
        except (KeyError, Bcfg2.Server.Plugin.PluginExecutionError):
            create = True

        # get current entry data
        if entry.text and entry.get("encoding") == "base64":
            entrydata = b64decode(entry.text)
        else:
            entrydata = entry.text

        if create:
            self.logger.info("Writing new probed file %s" % fileloc)
            self.write_file(fileloc, contents)
            self.verify_file(filename, contents, metadata)
            infoxml = os.path.join(cfg.data, filename.lstrip("/"), "info.xml")
            self.write_infoxml(infoxml, entry, data)
        elif entrydata == contents:
            self.debug_log("Existing %s contents match probed contents" %
                           filename)
            return
        elif (entry.get('update', 'false').lower() == "true"):
            self.logger.info("Writing updated probed file %s" % fileloc)
            self.write_file(fileloc, contents)
            self.verify_file(filename, contents, metadata)
        else:
            self.logger.info("Skipping updated probed file %s" % fileloc)
            return

    def write_file(self, fileloc, contents):
        """ Write the probed file to disk """
        try:
            os.makedirs(os.path.dirname(fileloc))
        except OSError:
            err = sys.exc_info()[1]
            if err.errno == errno.EEXIST:
                pass
            else:
                self.logger.error("Could not create parent directories for "
                                  "%s: %s" % (fileloc, err))
                return

        try:
            open(fileloc, 'wb').write(contents)
        except IOError:
            err = sys.exc_info()[1]
            self.logger.error("Could not write %s: %s" % (fileloc, err))
            return

    def verify_file(self, filename, contents, metadata):
        """ Service the FAM events queued up by the key generation so
        the data structure entries will be available for binding.

        NOTE: We wait for up to ten seconds. There is some potential
        for race condition, because if the file monitor doesn't get
        notified about the new key files in time, those entries won't
        be available for binding. In practice, this seems "good
        enough"."""
        entry = self.entries[metadata.hostname][filename]
        cfg = self.core.plugins['Cfg']
        tries = 0
        updated = False
        while not updated:
            if tries >= 10:
                self.logger.error("%s still not registered" % filename)
                return
            self.core.fam.handle_events_in_interval(1)
            try:
                cfg.entries[filename].bind_entry(entry, metadata)
            except Bcfg2.Server.Plugin.PluginExecutionError:
                tries += 1
                continue

            # get current entry data
            if entry.get("encoding") == "base64":
                entrydata = b64decode(entry.text)
            else:
                entrydata = entry.text
            if entrydata == contents:
                updated = True
            tries += 1

    def write_infoxml(self, infoxml, entry, data):
        """ write an info.xml for the file """
        if os.path.exists(infoxml):
            return

        self.logger.info("Writing %s for %s" % (infoxml, data.get("name")))
        info = lxml.etree.Element(
            "Info",
            owner=data.get("owner", Bcfg2.Options.MDATA_OWNER.value),
            group=data.get("group", Bcfg2.Options.MDATA_GROUP.value),
            mode=data.get("mode", Bcfg2.Options.MDATA_MODE.value),
            encoding=entry.get("encoding", Bcfg2.Options.ENCODING.value))

        root = lxml.etree.Element("FileInfo")
        root.append(info)
        try:
            root.getroottree().write(infoxml, xml_declaration=False,
                                     pretty_print=True)
        except IOError:
            err = sys.exc_info()[1]
            self.logger.error("Could not write %s: %s" % (infoxml, err))
            return

########NEW FILE########
__FILENAME__ = Fossil
""" The Fossil plugin provides a revision interface for Bcfg2 repos
using fossil."""

from subprocess import Popen, PIPE
import Bcfg2.Server.Plugin


class Fossil(Bcfg2.Server.Plugin.Version):
    """ The Fossil plugin provides a revision interface for Bcfg2
    repos using fossil. """
    __author__ = 'bcfg-dev@mcs.anl.gov'
    __vcs_metadata_path__ = "_FOSSIL_"

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Version.__init__(self, core, datastore)
        self.logger.debug("Initialized Fossil plugin with fossil directory %s"
                          % self.vcs_path)

    def get_revision(self):
        """Read fossil revision information for the Bcfg2 repository."""
        try:
            data = Popen("env LC_ALL=C fossil info",
                         shell=True,
                         cwd=self.vcs_root,
                         stdout=PIPE).stdout.readlines()
            revline = [line.split(': ')[1].strip() for line in data
                       if line.split(': ')[0].strip() == 'checkout'][-1]
            return revline.split(' ')[0]
        except IndexError:
            msg = "Failed to read fossil info"
            self.logger.error(msg)
            self.logger.error('Ran command "fossil info" from directory "%s"' %
                              self.vcs_root)
            raise Bcfg2.Server.Plugin.PluginExecutionError(msg)

########NEW FILE########
__FILENAME__ = Git
""" The Git plugin provides a revision interface for Bcfg2 repos using
git. """

import sys
from Bcfg2.Server.Plugin import Version, PluginExecutionError
from subprocess import Popen, PIPE

try:
    import git
    HAS_GITPYTHON = True
except ImportError:
    HAS_GITPYTHON = False


class Git(Version):
    """ The Git plugin provides a revision interface for Bcfg2 repos
    using git. """
    __author__ = 'bcfg-dev@mcs.anl.gov'
    __vcs_metadata_path__ = ".git"
    if HAS_GITPYTHON:
        __rmi__ = Version.__rmi__ + ['Update']

    def __init__(self, core, datastore):
        Version.__init__(self, core, datastore)
        if HAS_GITPYTHON:
            self.repo = git.Repo(self.vcs_root)
        else:
            self.logger.debug("Git: GitPython not found, using CLI interface "
                              "to Git")
            self.repo = None
        self.logger.debug("Initialized git plugin with git directory %s" %
                          self.vcs_path)

    def _log_git_cmd(self, output):
        """ Send output from a GitPython command to the debug log """
        for line in output.strip().splitlines():
            self.debug_log("Git: %s" % line)

    def get_revision(self):
        """Read git revision information for the Bcfg2 repository."""
        try:
            if HAS_GITPYTHON:
                return self.repo.head.commit.hexsha
            else:
                cmd = ["git", "--git-dir", self.vcs_path,
                       "--work-tree", self.vcs_root, "rev-parse", "HEAD"]
                self.debug_log("Git: Running %s" % cmd)
                proc = Popen(cmd, stdout=PIPE, stderr=PIPE)
                rv, err = proc.communicate()
                if proc.wait():
                    raise Exception(err)
                return rv
        except:
            raise PluginExecutionError("Git: Error getting revision from %s: "
                                       "%s" % (self.vcs_root,
                                               sys.exc_info()[1]))

    def Update(self, ref=None):
        """ Git.Update() => True|False
        Update the working copy against the upstream repository
        """
        self.logger.info("Git: Git.Update(ref='%s')" % ref)
        self.debug_log("Git: Performing garbage collection on repo at %s" %
                       self.vcs_root)
        try:
            self._log_git_cmd(self.repo.git.gc('--auto'))
        except git.GitCommandError:
            self.logger.warning("Git: Failed to perform garbage collection: %s"
                                % sys.exc_info()[1])

        self.debug_log("Git: Fetching all refs for repo at %s" % self.vcs_root)
        try:
            self._log_git_cmd(self.repo.git.fetch('--all'))
        except git.GitCommandError:
            self.logger.warning("Git: Failed to fetch refs: %s" %
                                sys.exc_info()[1])

        if ref:
            self.debug_log("Git: Checking out %s" % ref)
            try:
                self._log_git_cmd(self.repo.git.checkout('-f', ref))
            except git.GitCommandError:
                raise PluginExecutionError("Git: Failed to checkout %s: %s" %
                                           (ref, sys.exc_info()[1]))

        # determine if we should try to pull to get the latest commit
        # on this head
        tracking = None
        if not self.repo.head.is_detached:
            self.debug_log("Git: Determining if %s is a tracking branch" %
                           self.repo.head.ref.name)
            tracking = self.repo.head.ref.tracking_branch()

        if tracking is not None:
            self.debug_log("Git: %s is a tracking branch, pulling from %s" %
                           (self.repo.head.ref.name, tracking))
            try:
                self._log_git_cmd(self.repo.git.pull("--rebase"))
            except git.GitCommandError:
                raise PluginExecutionError("Git: Failed to pull from "
                                           "upstream: %s" % sys.exc_info()[1])

        self.logger.info("Git: Repo at %s updated to %s" %
                         (self.vcs_root, self.get_revision()))
        return True

########NEW FILE########
__FILENAME__ = GroupLogic
""" GroupLogic is a connector plugin that lets you use an XML Genshi
template to dynamically set additional groups for clients. """

import os
import lxml.etree
from threading import local
import Bcfg2.Server.Plugin
from Bcfg2.Server.Plugins.Metadata import MetadataGroup
try:
    from Bcfg2.Server.Plugins.Bundler import BundleTemplateFile
except ImportError:
    # BundleTemplateFile missing means that genshi is missing.  we
    # import genshi to get the _real_ error
    import genshi  # pylint: disable=W0611


class GroupLogicConfig(BundleTemplateFile):
    """ Representation of the GroupLogic groups.xml file """
    create = lxml.etree.Element("GroupLogic",
                                nsmap=dict(py="http://genshi.edgewall.org/"))

    def __init__(self, name, fam):
        BundleTemplateFile.__init__(self, name,
                                    Bcfg2.Server.Plugin.Specificity(), None)
        self.fam = fam
        self.should_monitor = True
        self.fam.AddMonitor(self.name, self)

    def _match(self, item, metadata):
        if item.tag == 'Group' and not len(item.getchildren()):
            return [item]
        return BundleTemplateFile._match(self, item, metadata)


class GroupLogic(Bcfg2.Server.Plugin.Plugin,
                 Bcfg2.Server.Plugin.Connector):
    """ GroupLogic is a connector plugin that lets you use an XML
    Genshi template to dynamically set additional groups for
    clients. """
    # perform grouplogic later than other Connector plugins, so it can
    # use groups set by them
    sort_order = 1000

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.Connector.__init__(self)
        self.config = GroupLogicConfig(os.path.join(self.data, "groups.xml"),
                                       core.fam)
        self._local = local()

    def get_additional_groups(self, metadata):
        if not hasattr(self._local, "building"):
            # building is a thread-local set that tracks which
            # machines GroupLogic is getting additional groups for.
            # If a get_additional_groups() is called twice for a
            # machine before the first call has completed, the second
            # call returns an empty list.  This is for infinite
            # recursion protection; without this check, it'd be
            # impossible to use things like metadata.query.in_group()
            # in GroupLogic, since that requires building all
            # metadata, which requires running
            # GroupLogic.get_additional_groups() for all hosts, which
            # requires building all metadata...
            self._local.building = set()
        if metadata.hostname in self._local.building:
            return []
        self._local.building.add(metadata.hostname)
        rv = []
        for el in self.config.get_xml_value(metadata).xpath("//Group"):
            if el.get("category"):
                rv.append(MetadataGroup(el.get("name"),
                                        category=el.get("category")))
            else:
                rv.append(el.get("name"))
        self._local.building.discard(metadata.hostname)
        return rv

########NEW FILE########
__FILENAME__ = GroupPatterns
""" set group membership based on client hostnames """

import os
import re
import sys
import Bcfg2.Server.Lint
import Bcfg2.Server.Plugin
from Bcfg2.Utils import PackedDigitRange


class PatternMap(object):
    """ Handler for a single pattern or range """

    def __init__(self, pattern, rangestr, groups):
        self.pattern = pattern
        self.rangestr = rangestr
        self.groups = groups
        if pattern is not None:
            self.re = re.compile(pattern)
            self.process = self.process_re
        elif rangestr is not None:
            if '\\' in rangestr:
                raise Exception("Backslashes are not allowed in NameRanges")
            range_finder = r'\[\[[\d\-,]+\]\]'
            self.process = self.process_range
            self.re = re.compile(r'^' + re.sub(range_finder, r'(\d+)',
                                               rangestr))
            dmatcher = re.compile(re.sub(range_finder,
                                         r'\[\[([\d\-,]+)\]\]',
                                         rangestr))
            self.dranges = [PackedDigitRange(x)
                            for x in dmatcher.match(rangestr).groups()]
        else:
            raise Exception("No pattern or range given")

    def process_range(self, name):
        """ match the given hostname against a range-based NameRange """
        match = self.re.match(name)
        if not match:
            return None
        digits = match.groups()
        for grp in range(len(digits)):
            if not self.dranges[grp].includes(digits[grp]):
                return None
        return self.groups

    def process_re(self, name):
        """ match the given hostname against a regex-based NamePattern """
        match = self.re.search(name)
        if not match:
            return None
        ret = list()
        sub = match.groups()
        for group in self.groups:
            newg = group
            for idx in range(len(sub)):
                newg = newg.replace('$%s' % (idx + 1), sub[idx])
            ret.append(newg)
        return ret

    def __str__(self):
        return "%s: %s %s" % (self.__class__.__name__, self.pattern,
                              self.groups)


class PatternFile(Bcfg2.Server.Plugin.XMLFileBacked):
    """ representation of GroupPatterns config.xml """
    __identifier__ = None
    create = 'GroupPatterns'

    def __init__(self, filename, core=None):
        try:
            fam = core.fam
        except AttributeError:
            fam = None
        Bcfg2.Server.Plugin.XMLFileBacked.__init__(self, filename, fam=fam,
                                                   should_monitor=True)
        self.core = core
        self.patterns = []

    def Index(self):
        Bcfg2.Server.Plugin.XMLFileBacked.Index(self)
        if (self.core and
            self.core.metadata_cache_mode in ['cautious', 'aggressive']):
            self.core.metadata_cache.expire()
        self.patterns = []
        for entry in self.xdata.xpath('//GroupPattern'):
            try:
                groups = [g.text for g in entry.findall('Group')]
                for pat_ent in entry.findall('NamePattern'):
                    pat = pat_ent.text
                    self.patterns.append(PatternMap(pat, None, groups))
                for range_ent in entry.findall('NameRange'):
                    rng = range_ent.text
                    self.patterns.append(PatternMap(None, rng, groups))
            except:  # pylint: disable=W0702
                self.logger.error("GroupPatterns: Failed to initialize "
                                  "pattern %s: %s" % (entry.text,
                                                      sys.exc_info()[1]))

    def process_patterns(self, hostname):
        """ return a list of groups that should be added to the given
        client based on patterns that match the hostname """
        ret = []
        for pattern in self.patterns:
            try:
                grps = pattern.process(hostname)
                if grps is not None:
                    ret.extend(grps)
            except:  # pylint: disable=W0702
                self.logger.error("GroupPatterns: Failed to process pattern "
                                  "%s for %s" % (pattern.pattern, hostname),
                                  exc_info=1)
        return ret


class GroupPatterns(Bcfg2.Server.Plugin.Plugin,
                    Bcfg2.Server.Plugin.Connector):
    """ set group membership based on client hostnames """

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.Connector.__init__(self)
        self.config = PatternFile(os.path.join(self.data, 'config.xml'),
                                  core=core)

    def get_additional_groups(self, metadata):
        return self.config.process_patterns(metadata.hostname)


class GroupPatternsLint(Bcfg2.Server.Lint.ServerPlugin):
    """ ``bcfg2-lint`` plugin to check all given :ref:`GroupPatterns
    <server-plugins-grouping-grouppatterns>` patterns for validity.
    This is simply done by trying to create a
    :class:`Bcfg2.Server.Plugins.GroupPatterns.PatternMap` object for
    each pattern, and catching exceptions and presenting them as
    ``bcfg2-lint`` errors."""

    def Run(self):
        cfg = self.core.plugins['GroupPatterns'].config
        for entry in cfg.xdata.xpath('//GroupPattern'):
            groups = [g.text for g in entry.findall('Group')]
            self.check(entry, groups, ptype='NamePattern')
            self.check(entry, groups, ptype='NameRange')

    @classmethod
    def Errors(cls):
        return {"pattern-fails-to-initialize": "error"}

    def check(self, entry, groups, ptype="NamePattern"):
        """ Check a single pattern for validity """
        if ptype == "NamePattern":
            pmap = lambda p: PatternMap(p, None, groups)
        else:
            pmap = lambda p: PatternMap(None, p, groups)

        for el in entry.findall(ptype):
            pat = el.text
            try:
                pmap(pat)
            except:  # pylint: disable=W0702
                err = sys.exc_info()[1]
                self.LintError("pattern-fails-to-initialize",
                               "Failed to initialize %s %s for %s: %s" %
                               (ptype, pat, entry.get('pattern'), err))

########NEW FILE########
__FILENAME__ = Guppy
"""
This plugin is used to trace memory leaks within the bcfg2-server
process using Guppy.  By default the remote debugger is started
when this plugin is enabled.  The debugger can be shutoff in a running
process using "bcfg2-admin xcmd Guppy.Disable" and reenabled using
"bcfg2-admin xcmd Guppy.Enable".

To attach the console run:

python -c "from guppy import hpy;hpy().monitor()"

For example:

# python -c "from guppy import hpy;hpy().monitor()"
<Monitor>
*** Connection 1 opened ***
<Monitor> lc
CID PID   ARGV
  1 25063 ['/usr/sbin/bcfg2-server', '-D', '/var/run/bcfg2-server.pid']
<Monitor> sc 1
Remote connection 1. To return to Monitor, type <Ctrl-C> or .<RETURN>
<Annex> int
Remote interactive console. To return to Annex, type '-'.
>>> hp.heap()
...


"""
import Bcfg2.Server.Plugin
from guppy.heapy import Remote


class Guppy(Bcfg2.Server.Plugin.Plugin):
    """Guppy is a debugging plugin to help trace memory leaks"""
    name = 'Guppy'
    __author__ = 'bcfg-dev@mcs.anl.gov'

    experimental = True
    __rmi__ = Bcfg2.Server.Plugin.Plugin.__rmi__ + ['Enable', 'Disable']
    __child_rmi__ = __rmi__[:]

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)

        self.Enable()

    def Enable(self):
        """Enable remote debugging"""
        try:
            Remote.on()
        except:
            self.logger.error("Failed to create Heapy context")
            raise Bcfg2.Server.Plugin.PluginInitError

    def Disable(self):
        """Disable remote debugging"""
        try:
            Remote.off()
        except:
            self.logger.error("Failed to disable Heapy")
            raise Bcfg2.Server.Plugin.PluginInitError

########NEW FILE########
__FILENAME__ = Hg
""" The Hg plugin provides a revision interface for Bcfg2 repos using
mercurial. """

import sys
from mercurial import ui, hg
import Bcfg2.Server.Plugin


class Hg(Bcfg2.Server.Plugin.Version):
    """ The Hg plugin provides a revision interface for Bcfg2 repos
    using mercurial. """
    __author__ = 'bcfg-dev@mcs.anl.gov'
    __vcs_metadata_path__ = ".hg"

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Version.__init__(self, core, datastore)
        self.logger.debug("Initialized hg plugin with hg directory %s" %
                          self.vcs_path)

    def get_revision(self):
        """Read hg revision information for the Bcfg2 repository."""
        try:
            repo_path = self.vcs_root + "/"
            repo = hg.repository(ui.ui(), repo_path)
            tip = repo.changelog.tip()
            return repo.changelog.rev(tip)
        except:
            err = sys.exc_info()[1]
            msg = "Failed to read hg repository: %s" % err
            self.logger.error(msg)
            raise Bcfg2.Server.Plugin.PluginExecutionError(msg)

########NEW FILE########
__FILENAME__ = Hostbase
"""
This file provides the Hostbase plugin.
It manages dns/dhcp/nis host information
"""

from lxml.etree import Element, SubElement
import os
import re
from time import strftime
os.environ['DJANGO_SETTINGS_MODULE'] = 'Bcfg2.Server.Hostbase.settings'
import Bcfg2.Server.Plugin
from Bcfg2.Server.Plugin import PluginExecutionError, PluginInitError
from django.template import Context, loader
from django.db import connection
# Compatibility imports
from Bcfg2.Compat import StringIO

try:
    set
except NameError:
    # deprecated since python 2.6
    from sets import Set as set


class Hostbase(Bcfg2.Server.Plugin.Plugin,
               Bcfg2.Server.Plugin.Structure,
               Bcfg2.Server.Plugin.Generator):
    """The Hostbase plugin handles host/network info."""
    name = 'Hostbase'
    __author__ = 'bcfg-dev@mcs.anl.gov'
    filepath = '/my/adm/hostbase/files/bind'
    deprecated = True

    def __init__(self, core, datastore):

        self.ready = False
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.Structure.__init__(self)
        Bcfg2.Server.Plugin.Generator.__init__(self)
        files = ['zone.tmpl',
                 'reversesoa.tmpl',
                 'named.tmpl',
                 'reverseappend.tmpl',
                 'dhcpd.tmpl',
                 'hosts.tmpl',
                 'hostsappend.tmpl']
        self.filedata = {}
        self.dnsservers = []
        self.dhcpservers = []
        self.templates = {'zone': loader.get_template('zone.tmpl'),
                          'reversesoa': loader.get_template('reversesoa.tmpl'),
                          'named': loader.get_template('named.tmpl'),
                          'namedviews': loader.get_template('namedviews.tmpl'),
                          'reverseapp': loader.get_template('reverseappend.tmpl'),
                          'dhcp': loader.get_template('dhcpd.tmpl'),
                          'hosts': loader.get_template('hosts.tmpl'),
                          'hostsapp': loader.get_template('hostsappend.tmpl'),
                          }
        self.Entries['ConfigFile'] = {}
        self.__rmi__ = ['rebuildState']
        try:
            self.rebuildState(None)
        except:
            raise PluginInitError

    def FetchFile(self, entry, metadata):
        """Return prebuilt file data."""
        fname = entry.get('name').split('/')[-1]
        if not fname in self.filedata:
            raise PluginExecutionError
        perms = {'owner': 'root',
                 'group': 'root',
                 'mode': '644'}
        [entry.attrib.__setitem__(key, value)
         for (key, value) in list(perms.items())]
        entry.text = self.filedata[fname]

    def BuildStructures(self, metadata):
        """Build hostbase bundle."""
        if metadata.hostname not in self.dnsservers or metadata.hostname not in self.dhcpservers:
            return []
        output = Element("Bundle", name='hostbase')
        if metadata.hostname in self.dnsservers:
            for configfile in self.Entries['ConfigFile']:
                if re.search('/etc/bind/', configfile):
                    SubElement(output, "ConfigFile", name=configfile)
        if metadata.hostname in self.dhcpservers:
            SubElement(output, "ConfigFile", name="/etc/dhcp3/dhcpd.conf")
        return [output]

    def rebuildState(self, _):
        """Pre-cache all state information for hostbase config files
        callable as an XMLRPC function.

        """
        self.buildZones()
        self.buildDHCP()
        self.buildHosts()
        self.buildHostsLPD()
        self.buildPrinters()
        self.buildNetgroups()
        return True

    def buildZones(self):
        """Pre-build and stash zone files."""
        cursor = connection.cursor()

        cursor.execute("SELECT id, serial FROM hostbase_zone")
        zones = cursor.fetchall()

        for zone in zones:
        # update the serial number for all zone files
            todaydate = (strftime('%Y%m%d'))
            try:
                if todaydate == str(zone[1])[:8]:
                    serial = zone[1] + 1
                else:
                    serial = int(todaydate) * 100
            except (KeyError):
                serial = int(todaydate) * 100
            cursor.execute("""UPDATE hostbase_zone SET serial = \'%s\' WHERE id = \'%s\'""" % (str(serial), zone[0]))

        cursor.execute("SELECT * FROM hostbase_zone WHERE zone NOT LIKE \'%%.rev\'")
        zones = cursor.fetchall()

        iplist = []
        hosts = {}

        for zone in zones:
            zonefile = StringIO()
            externalzonefile = StringIO()
            cursor.execute("""SELECT n.name FROM hostbase_zone_nameservers z
            INNER JOIN hostbase_nameserver n ON z.nameserver_id = n.id
            WHERE z.zone_id = \'%s\'""" % zone[0])
            nameservers = cursor.fetchall()
            cursor.execute("""SELECT i.ip_addr FROM hostbase_zone_addresses z
            INNER JOIN hostbase_zoneaddress i ON z.zoneaddress_id = i.id
            WHERE z.zone_id = \'%s\'""" % zone[0])
            addresses = cursor.fetchall()
            cursor.execute("""SELECT m.priority, m.mx FROM hostbase_zone_mxs z
            INNER JOIN hostbase_mx m ON z.mx_id = m.id
            WHERE z.zone_id = \'%s\'""" % zone[0])
            mxs = cursor.fetchall()
            context = Context({
                'zone': zone,
                'nameservers': nameservers,
                'addresses': addresses,
                'mxs': mxs
                })
            zonefile.write(self.templates['zone'].render(context))
            externalzonefile.write(self.templates['zone'].render(context))

            querystring = """SELECT h.hostname, p.ip_addr,
            n.name, c.cname, m.priority, m.mx, n.dns_view
            FROM (((((hostbase_host h INNER JOIN hostbase_interface i ON h.id = i.host_id)
            INNER JOIN hostbase_ip p ON i.id = p.interface_id)
            INNER JOIN hostbase_name n ON p.id = n.ip_id)
            INNER JOIN hostbase_name_mxs x ON n.id = x.name_id)
            INNER JOIN hostbase_mx m ON m.id = x.mx_id)
            LEFT JOIN hostbase_cname c ON n.id = c.name_id
            WHERE n.name LIKE '%%%%%s'
            AND h.status = 'active'
            ORDER BY h.hostname, n.name, p.ip_addr
            """ % zone[1]
            cursor.execute(querystring)
            zonehosts = cursor.fetchall()
            prevhost = (None, None, None, None)
            cnames = StringIO()
            cnamesexternal = StringIO()
            for host in zonehosts:
                if not host[2].split(".", 1)[1] == zone[1]:
                    zonefile.write(cnames.getvalue())
                    externalzonefile.write(cnamesexternal.getvalue())
                    cnames = StringIO()
                    cnamesexternal = StringIO()
                    continue
                if not prevhost[1] == host[1] or not prevhost[2] == host[2]:
                    zonefile.write(cnames.getvalue())
                    externalzonefile.write(cnamesexternal.getvalue())
                    cnames = StringIO()
                    cnamesexternal = StringIO()
                    zonefile.write("%-32s%-10s%-32s\n" %
                                   (host[2].split(".", 1)[0], 'A', host[1]))
                    zonefile.write("%-32s%-10s%-3s%s.\n" %
                                   ('', 'MX', host[4], host[5]))
                    if host[6] == 'global':
                        externalzonefile.write("%-32s%-10s%-32s\n" %
                                               (host[2].split(".", 1)[0], 'A', host[1]))
                        externalzonefile.write("%-32s%-10s%-3s%s.\n" %
                                               ('', 'MX', host[4], host[5]))
                elif not prevhost[5] == host[5]:
                    zonefile.write("%-32s%-10s%-3s%s.\n" %
                                   ('', 'MX', host[4], host[5]))
                    if host[6] == 'global':
                        externalzonefile.write("%-32s%-10s%-3s%s.\n" %
                                         ('', 'MX', host[4], host[5]))

                if host[3]:
                    try:
                        if host[3].split(".", 1)[1] == zone[1]:
                            cnames.write("%-32s%-10s%-32s\n" %
                                         (host[3].split(".", 1)[0],
                                          'CNAME', host[2].split(".", 1)[0]))
                            if host[6] == 'global':
                                cnamesexternal.write("%-32s%-10s%-32s\n" %
                                                     (host[3].split(".", 1)[0],
                                                      'CNAME', host[2].split(".", 1)[0]))
                        else:
                            cnames.write("%-32s%-10s%-32s\n" %
                                         (host[3] + ".",
                                          'CNAME',
                                          host[2].split(".", 1)[0]))
                            if host[6] == 'global':
                                cnamesexternal.write("%-32s%-10s%-32s\n" %
                                                     (host[3] + ".",
                                                      'CNAME',
                                                      host[2].split(".", 1)[0]))

                    except:
                        pass
                prevhost = host
            zonefile.write(cnames.getvalue())
            externalzonefile.write(cnamesexternal.getvalue())
            zonefile.write("\n\n%s" % zone[9])
            externalzonefile.write("\n\n%s" % zone[9])
            self.filedata[zone[1]] = zonefile.getvalue()
            self.filedata[zone[1] + ".external"] = externalzonefile.getvalue()
            zonefile.close()
            externalzonefile.close()
            self.Entries['ConfigFile']["%s/%s" % (self.filepath, zone[1])] = self.FetchFile
            self.Entries['ConfigFile']["%s/%s.external" % (self.filepath, zone[1])] = self.FetchFile

        cursor.execute("SELECT * FROM hostbase_zone WHERE zone LIKE \'%%.rev\' AND zone <> \'.rev\'")
        reversezones = cursor.fetchall()

        reversenames = []
        for reversezone in reversezones:
            cursor.execute("""SELECT n.name FROM hostbase_zone_nameservers z
            INNER JOIN hostbase_nameserver n ON z.nameserver_id = n.id
            WHERE z.zone_id = \'%s\'""" % reversezone[0])
            reverse_nameservers = cursor.fetchall()

            context = Context({
                'inaddr': reversezone[1].rstrip('.rev'),
                'zone': reversezone,
                'nameservers': reverse_nameservers,
                })

            self.filedata[reversezone[1]] = self.templates['reversesoa'].render(context)
            self.filedata[reversezone[1] + '.external'] = self.templates['reversesoa'].render(context)
            self.filedata[reversezone[1]] += reversezone[9]
            self.filedata[reversezone[1] + '.external'] += reversezone[9]

            subnet = reversezone[1].split(".")
            subnet.reverse()
            reversenames.append((reversezone[1].rstrip('.rev'), ".".join(subnet[1:])))

        for filename in reversenames:
            cursor.execute("""
            SELECT DISTINCT h.hostname, p.ip_addr, n.dns_view FROM ((hostbase_host h
            INNER JOIN hostbase_interface i ON h.id = i.host_id)
            INNER JOIN hostbase_ip p ON i.id = p.interface_id)
            INNER JOIN hostbase_name n ON n.ip_id = p.id
            WHERE p.ip_addr LIKE '%s%%%%' AND h.status = 'active' ORDER BY p.ip_addr
            """ % filename[1])
            reversehosts = cursor.fetchall()
            zonefile = StringIO()
            externalzonefile = StringIO()
            if len(filename[0].split(".")) == 2:
                originlist = []
                [originlist.append((".".join([ip[1].split(".")[2], filename[0]]),
                                    ".".join([filename[1], ip[1].split(".")[2]])))
                 for ip in reversehosts
                 if (".".join([ip[1].split(".")[2], filename[0]]),
                     ".".join([filename[1], ip[1].split(".")[2]])) not in originlist]
                for origin in originlist:
                    hosts = [(host[1].split("."), host[0])
                             for host in reversehosts
                             if host[1].rstrip('0123456789').rstrip('.') == origin[1]]
                    hosts_external = [(host[1].split("."), host[0])
                                     for host in reversehosts
                                     if (host[1].rstrip('0123456789').rstrip('.') == origin[1]
                                         and host[2] == 'global')]
                    context = Context({
                        'hosts': hosts,
                        'inaddr': origin[0],
                        'fileorigin': filename[0],
                        })
                    zonefile.write(self.templates['reverseapp'].render(context))
                    context = Context({
                        'hosts': hosts_external,
                        'inaddr': origin[0],
                        'fileorigin': filename[0],
                        })
                    externalzonefile.write(self.templates['reverseapp'].render(context))
            else:
                originlist = [filename[0]]
                hosts = [(host[1].split("."), host[0])
                         for host in reversehosts
                         if (host[1].split("."), host[0]) not in hosts]
                hosts_external = [(host[1].split("."), host[0])
                                  for host in reversehosts
                                  if ((host[1].split("."), host[0]) not in hosts_external
                                  and host[2] == 'global')]
                context = Context({
                    'hosts': hosts,
                    'inaddr': filename[0],
                    'fileorigin': None,
                    })
                zonefile.write(self.templates['reverseapp'].render(context))
                context = Context({
                    'hosts': hosts_external,
                    'inaddr': filename[0],
                    'fileorigin': None,
                    })
                externalzonefile.write(self.templates['reverseapp'].render(context))
            self.filedata['%s.rev' % filename[0]] += zonefile.getvalue()
            self.filedata['%s.rev.external' % filename[0]] += externalzonefile.getvalue()
            zonefile.close()
            externalzonefile.close()
            self.Entries['ConfigFile']['%s/%s.rev' % (self.filepath, filename[0])] = self.FetchFile
            self.Entries['ConfigFile']['%s/%s.rev.external' % (self.filepath, filename[0])] = self.FetchFile

        ## here's where the named.conf file gets written
        context = Context({
            'zones': zones,
            'reverses': reversenames,
            })
        self.filedata['named.conf'] = self.templates['named'].render(context)
        self.Entries['ConfigFile']['/my/adm/hostbase/files/named.conf'] = self.FetchFile
        self.filedata['named.conf.views'] = self.templates['namedviews'].render(context)
        self.Entries['ConfigFile']['/my/adm/hostbase/files/named.conf.views'] = self.FetchFile

    def buildDHCP(self):
        """Pre-build dhcpd.conf and stash in the filedata table."""

        # fetches all the hosts with DHCP == True
        cursor = connection.cursor()
        cursor.execute("""
        SELECT hostname, mac_addr, ip_addr
        FROM (hostbase_host h INNER JOIN hostbase_interface i ON h.id = i.host_id)
        INNER JOIN hostbase_ip ip ON i.id = ip.interface_id
        WHERE i.dhcp=1 AND h.status='active' AND i.mac_addr <> ''
        AND i.mac_addr <> 'float' AND i.mac_addr <> 'unknown'
        ORDER BY h.hostname, i.mac_addr
        """)

        dhcphosts = cursor.fetchall()
        count = 0
        hosts = []
        hostdata = [dhcphosts[0][0], dhcphosts[0][1], dhcphosts[0][2]]
        if len(dhcphosts) > 1:
            for x in range(1, len(dhcphosts)):
                # if an interface has 2 or more ip addresses
                # adds the ip to the current interface
                if hostdata[0].split(".")[0] == dhcphosts[x][0].split(".")[0] and hostdata[1] == dhcphosts[x][1]:
                    hostdata[2] = ", ".join([hostdata[2], dhcphosts[x][2]])
                # if a host has 2 or more interfaces
                # writes the current one and grabs the next
                elif hostdata[0].split(".")[0] == dhcphosts[x][0].split(".")[0]:
                    hosts.append(hostdata)
                    count += 1
                    hostdata = ["-".join([dhcphosts[x][0], str(count)]), dhcphosts[x][1], dhcphosts[x][2]]
                # new host found, writes current data to the template
                else:
                    hosts.append(hostdata)
                    count = 0
                    hostdata = [dhcphosts[x][0], dhcphosts[x][1], dhcphosts[x][2]]
        #makes sure the last of the data gets written out
        if hostdata not in hosts:
            hosts.append(hostdata)

        context = Context({
            'hosts': hosts,
            'numips': len(hosts),
            })

        self.filedata['dhcpd.conf'] = self.templates['dhcp'].render(context)
        self.Entries['ConfigFile']['/my/adm/hostbase/files/dhcpd.conf'] = self.FetchFile

    def buildHosts(self):
        """Pre-build and stash /etc/hosts file."""

        append_data = []

        cursor = connection.cursor()
        cursor.execute("""
        SELECT hostname FROM hostbase_host ORDER BY hostname
        """)
        hostbase = cursor.fetchall()
        domains = [host[0].split(".", 1)[1] for host in hostbase]
        domains_set = set(domains)
        domain_data = [(domain, domains.count(domain)) for domain in domains_set]
        domain_data.sort()

        cursor.execute("""
        SELECT ip_addr FROM hostbase_ip ORDER BY ip_addr
        """)
        ips = cursor.fetchall()
        three_octets = [ip[0].rstrip('0123456789').rstrip('.') \
                        for ip in ips]
        three_octets_set = set(three_octets)
        three_octets_data = [(octet, three_octets.count(octet)) \
                             for octet in three_octets_set]
        three_octets_data.sort()

        for three_octet in three_octets_data:
            querystring = """SELECT h.hostname, h.primary_user,
            p.ip_addr, n.name, c.cname
            FROM (((hostbase_host h INNER JOIN hostbase_interface i ON h.id = i.host_id)
            INNER JOIN hostbase_ip p ON i.id = p.interface_id)
            INNER JOIN hostbase_name n ON p.id = n.ip_id)
            LEFT JOIN hostbase_cname c ON n.id = c.name_id
            WHERE p.ip_addr LIKE \'%s.%%%%\' AND h.status = 'active'""" % three_octet[0]
            cursor.execute(querystring)
            tosort = list(cursor.fetchall())
            tosort.sort(lambda x, y: cmp(int(x[2].split(".")[-1]), int(y[2].split(".")[-1])))
            append_data.append((three_octet, tuple(tosort)))

        two_octets = [ip.rstrip('0123456789').rstrip('.') for ip in three_octets]
        two_octets_set = set(two_octets)
        two_octets_data = [(octet, two_octets.count(octet))
                           for octet in two_octets_set]
        two_octets_data.sort()

        context = Context({
            'domain_data': domain_data,
            'three_octets_data': three_octets_data,
            'two_octets_data': two_octets_data,
            'three_octets': three_octets,
            'num_ips': len(three_octets),
            })

        self.filedata['hosts'] = self.templates['hosts'].render(context)

        for subnet in append_data:
            ips = []
            simple = True
            namelist = [name.split('.', 1)[0] for name in [subnet[1][0][3]]]
            cnamelist = []
            if subnet[1][0][4]:
                cnamelist.append(subnet[1][0][4].split('.', 1)[0])
                simple = False
            appenddata = subnet[1][0]
            for ip in subnet[1][1:]:
                if appenddata[2] == ip[2]:
                    namelist.append(ip[3].split('.', 1)[0])
                    if ip[4]:
                        cnamelist.append(ip[4].split('.', 1)[0])
                        simple = False
                    appenddata = ip
                else:
                    if appenddata[0] == ip[0]:
                        simple = False
                    ips.append((appenddata[2], appenddata[0], set(namelist),
                                cnamelist, simple, appenddata[1]))
                    appenddata = ip
                    simple = True
                    namelist = [ip[3].split('.', 1)[0]]
                    cnamelist = []
                    if ip[4]:
                        cnamelist.append(ip[4].split('.', 1)[0])
                        simple = False
            ips.append((appenddata[2], appenddata[0], set(namelist),
                        cnamelist, simple, appenddata[1]))
            context = Context({
                'subnet': subnet[0],
                'ips': ips,
                })
            self.filedata['hosts'] += self.templates['hostsapp'].render(context)
        self.Entries['ConfigFile']['/mcs/etc/hosts'] = self.FetchFile

    def buildPrinters(self):
        """The /mcs/etc/printers.data file"""
        header = """#  This file is automatically generated. DO NOT EDIT IT!
#
Name            Room        User                            Type                      Notes
==============  ==========  ==============================  ========================  ====================
"""

        cursor = connection.cursor()
        # fetches all the printers from the database
        cursor.execute("""
        SELECT printq, location, primary_user, comments
        FROM hostbase_host
        WHERE whatami='printer' AND printq <> '' AND status = 'active'
        ORDER BY printq
        """)
        printers = cursor.fetchall()

        printersfile = header
        for printer in printers:
            # splits up the printq line and gets the
            # correct description out of the comments section
            temp = printer[3].split('\n')
            for printq in re.split(',[ ]*', printer[0]):
                if len(temp) > 1:
                    printersfile += ("%-16s%-12s%-32s%-26s%s\n" %
                                     (printq, printer[1], printer[2], temp[1], temp[0]))
                else:
                    printersfile += ("%-16s%-12s%-32s%-26s%s\n" %
                                     (printq, printer[1], printer[2], '', printer[3]))
        self.filedata['printers.data'] = printersfile
        self.Entries['ConfigFile']['/mcs/etc/printers.data'] = self.FetchFile

    def buildHostsLPD(self):
        """Creates the /mcs/etc/hosts.lpd file"""

        # this header needs to be changed to be more generic
        header = """+@machines
+@all-machines
achilles.ctd.anl.gov
raven.ops.anl.gov
seagull.hr.anl.gov
parrot.ops.anl.gov
condor.ops.anl.gov
delphi.esh.anl.gov
anlcv1.ctd.anl.gov
anlvms.ctd.anl.gov
olivia.ctd.anl.gov\n\n"""

        cursor = connection.cursor()
        cursor.execute("""
        SELECT hostname FROM hostbase_host WHERE netgroup=\"red\" AND status = 'active'
        ORDER BY hostname""")
        redmachines = list(cursor.fetchall())
        cursor.execute("""
        SELECT n.name FROM ((hostbase_host h INNER JOIN hostbase_interface i ON h.id = i.host_id)
        INNER JOIN hostbase_ip p ON i.id = p.interface_id) INNER JOIN hostbase_name n ON p.id = n.ip_id
        WHERE netgroup=\"red\" AND n.only=1 AND h.status = 'active'
        """)
        redmachines.extend(list(cursor.fetchall()))
        cursor.execute("""
        SELECT hostname FROM hostbase_host WHERE netgroup=\"win\" AND status = 'active'
        ORDER BY hostname""")
        winmachines = list(cursor.fetchall())
        cursor.execute("""
        SELECT n.name FROM ((hostbase_host h INNER JOIN hostbase_interface i ON h.id = i.host_id)
        INNER JOIN hostbase_ip p ON i.id = p.interface_id) INNER JOIN hostbase_name n ON p.id = n.ip_id
        WHERE netgroup=\"win\" AND n.only=1 AND h.status = 'active'
        """)
        winmachines.__add__(list(cursor.fetchall()))
        hostslpdfile = header
        for machine in redmachines:
            hostslpdfile += machine[0] + "\n"
        hostslpdfile += "\n"
        for machine in winmachines:
            hostslpdfile += machine[0] + "\n"
        self.filedata['hosts.lpd'] = hostslpdfile
        self.Entries['ConfigFile']['/mcs/etc/hosts.lpd'] = self.FetchFile

    def buildNetgroups(self):
        """Makes the *-machine files"""
        header = """###################################################################
#  This file lists hosts in the '%s' machine netgroup, it is
#  automatically generated. DO NOT EDIT THIS FILE!
#
#  Number of hosts in '%s' machine netgroup: %i
#\n\n"""

        cursor = connection.cursor()
        # fetches all the hosts that with valid netgroup entries
        cursor.execute("""
        SELECT h.hostname, n.name, h.netgroup, n.only FROM ((hostbase_host h
        INNER JOIN hostbase_interface i ON h.id = i.host_id)
        INNER JOIN hostbase_ip p ON i.id = p.interface_id)
        INNER JOIN hostbase_name n ON p.id = n.ip_id
        WHERE h.netgroup <> '' AND h.netgroup <> 'none' AND h.status = 'active'
        ORDER BY h.netgroup, h.hostname
        """)
        nameslist = cursor.fetchall()
        # gets the first host and initializes the hash
        hostdata = nameslist[0]
        netgroups = {hostdata[2]: [hostdata[0]]}
        for row in nameslist:
            # if new netgroup, create it
            if row[2] not in netgroups:
                netgroups.update({row[2]: []})
            # if it belongs in the netgroup and has multiple interfaces, put them in
            if hostdata[0] == row[0] and row[3]:
                netgroups[row[2]].append(row[1])
                hostdata = row
            # if its a new host, write the old one to the hash
            elif hostdata[0] != row[0]:
                netgroups[row[2]].append(row[0])
                hostdata = row

        for netgroup in netgroups:
            fileoutput = StringIO()
            fileoutput.write(header % (netgroup, netgroup, len(netgroups[netgroup])))
            for each in netgroups[netgroup]:
                fileoutput.write(each + "\n")
            self.filedata['%s-machines' % netgroup] = fileoutput.getvalue()
            fileoutput.close()
            self.Entries['ConfigFile']['/my/adm/hostbase/makenets/machines/%s-machines' % netgroup] = self.FetchFile

        cursor.execute("""
        UPDATE hostbase_host SET dirty=0
        """)

########NEW FILE########
__FILENAME__ = Ldap
import imp
import logging
import sys
import time
import traceback
import Bcfg2.Options
import Bcfg2.Server.Plugin

logger = logging.getLogger('Bcfg2.Plugins.Ldap')

try:
    import ldap
except ImportError:
    logger.error("Unable to load ldap module. Is python-ldap installed?")
    raise ImportError

# time in seconds between retries after failed LDAP connection
RETRY_DELAY = 5
# how many times to try reaching the LDAP server if a connection is broken
# at the very minimum, one retry is needed to handle a restarted LDAP daemon
RETRY_COUNT = 3

SCOPE_MAP = {
    "base": ldap.SCOPE_BASE,
    "one": ldap.SCOPE_ONELEVEL,
    "sub": ldap.SCOPE_SUBTREE,
}

LDAP_QUERIES = []


def register_query(query):
    LDAP_QUERIES.append(query)


class ConfigFile(Bcfg2.Server.Plugin.FileBacked):
    """
    Config file for the Ldap plugin

    The config file cannot be 'parsed' in the traditional sense as we would
    need some serious type checking ugliness to just get the LdapQuery
    subclasses. The alternative would be to have the user create a list with
    a predefined name that contains all queries.
    The approach implemented here is having the user call a registering
    decorator that updates a global variable in this module.
    """
    def __init__(self, filename, fam):
        self.filename = filename
        Bcfg2.Server.Plugin.FileBacked.__init__(self, self.filename)
        fam.AddMonitor(self.filename, self)

    def Index(self):
        """
        Reregisters the queries in the config file

        The config will take care of actually registering the queries,
        so we just load it once and don't keep it.
        """
        global LDAP_QUERIES
        LDAP_QUERIES = []
        imp.load_source("ldap_cfg", self.filename)


class Ldap(Bcfg2.Server.Plugin.Plugin, Bcfg2.Server.Plugin.Connector):
    """
    The Ldap plugin allows adding data from an LDAP server to your metadata.
    """
    name = "Ldap"
    experimental = True
    debug_flag = False

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.Connector.__init__(self)
        self.config = ConfigFile(self.data + "/config.py", core.fam)

    def debug_log(self, message, flag = None):
        if (flag is None) and self.debug_flag or flag:
            self.logger.error(message)

    def get_additional_data(self, metadata):
        query = None
        try:
            data = {}
            self.debug_log("LdapPlugin debug: found queries " +
                                              str(LDAP_QUERIES))
            for QueryClass in LDAP_QUERIES:
                query = QueryClass()
                if query.is_applicable(metadata):
                    self.debug_log("LdapPlugin debug: processing query '" +
                                                           query.name + "'")
                    data[query.name] = query.get_result(metadata)
                else:
                    self.debug_log("LdapPlugin debug: query '" + query.name +
                        "' not applicable to host '" + metadata.hostname + "'")
            return data
        except Exception:
            if hasattr(query, "name"):
                logger.error("LdapPlugin error: " +
                       "Exception during processing of query named '" +
                                                      str(query.name) +
                                     "', query results will be empty" +
                                       " and may cause bind failures")
            for line in traceback.format_exception(sys.exc_info()[0],
                                                   sys.exc_info()[1],
                                                   sys.exc_info()[2]):
                logger.error("LdapPlugin error: " +
                                                 line.replace("\n", ""))
            return {}

class LdapConnection(object):
    """
    Connection to an LDAP server.
    """
    def __init__(self, host = "localhost", port = 389,
                       binddn = None, bindpw = None):
        self.host = host
        self.port = port
        self.binddn = binddn
        self.bindpw = bindpw
        self.conn = None

    def __del__(self):
        if self.conn:
            self.conn.unbind()

    def init_conn(self):
        self.conn = ldap.initialize(self.url)
        if self.binddn is not None and self.bindpw is not None:
            self.conn.simple_bind_s(self.binddn, self.bindpw)

    def run_query(self, query):
        result = None
        for attempt in range(RETRY_COUNT + 1):
            if attempt >= 1:
                logger.error("LdapPlugin error: " +
                    "LDAP server down (retry " + str(attempt) + "/" +
                    str(RETRY_COUNT) + ")")
            try:
                if not self.conn:
                    self.init_conn()
                result = self.conn.search_s(
                    query.base,
                    SCOPE_MAP[query.scope],
                    query.filter.replace("\\", "\\\\"),
                    query.attrs,
                )
                break
            except ldap.SERVER_DOWN:
                self.conn = None
                time.sleep(RETRY_DELAY)
        return result

    @property
    def url(self):
        return "ldap://" + self.host + ":" + str(self.port)

class LdapQuery(object):
    """
    Query referencing an LdapConnection and providing several
    methods for query manipulation.
    """

    name = "unknown"
    base = ""
    scope = "sub"
    filter = "(objectClass=*)"
    attrs = None
    connection = None
    result = None

    def __unicode__(self):
        return "LdapQuery:" + self.name

    def is_applicable(self, metadata):
        """
        Overrideable method to determine if the query is to be executed for
        the given metadata object.
        Defaults to true.
        """
        return True

    def prepare_query(self, metadata):
        """
        Overrideable method to alter the query based on metadata.
        Defaults to doing nothing.

        In most cases, you will do something like

            self.filter = "(cn=" + metadata.hostname + ")"

        here.
        """
        pass

    def process_result(self, metadata):
        """
        Overrideable method to post-process the query result.
        Defaults to returning the unaltered result.
        """
        return self.result

    def get_result(self, metadata):
        """
        Method to handle preparing, executing and processing the query.
        """
        if isinstance(self.connection, LdapConnection):
            self.prepare_query(metadata)
            self.result = self.connection.run_query(self)
            self.result = self.process_result(metadata)
            return self.result
        else:
            logger.error("LdapPlugin error: " +
              "No valid connection defined for query " + str(self))
            return None

class LdapSubQuery(LdapQuery):
    """
    SubQueries are meant for internal use only and are not added
    to the metadata object. They are useful for situations where
    you need to run more than one query to obtain some data.
    """
    def prepare_query(self, metadata, **kwargs):
        """
        Overrideable method to alter the query based on metadata.
        Defaults to doing nothing.
        """
        pass

    def process_result(self, metadata, **kwargs):
        """
        Overrideable method to post-process the query result.
        Defaults to returning the unaltered result.
        """
        return self.result

    def get_result(self, metadata, **kwargs):
        """
        Method to handle preparing, executing and processing the query.
        """
        if isinstance(self.connection, LdapConnection):
            self.prepare_query(metadata, **kwargs)
            self.result = self.connection.run_query(self)
            return self.process_result(metadata, **kwargs)
        else:
            logger.error("LdapPlugin error: " +
              "No valid connection defined for query " + str(self))
            return None

########NEW FILE########
__FILENAME__ = Metadata
""" This file stores persistent metadata for the Bcfg2 Configuration
Repository. """

import re
import os
import sys
import time
import copy
import errno
import fcntl
import socket
import logging
import lxml.etree
import Bcfg2.Server
import Bcfg2.Server.Lint
import Bcfg2.Server.Plugin
import Bcfg2.Server.FileMonitor
from Bcfg2.Utils import locked
# pylint: disable=W0622
from Bcfg2.Compat import MutableMapping, all, any, wraps
# pylint: enable=W0622
from Bcfg2.version import Bcfg2VersionInfo

try:
    from django.db import models
    HAS_DJANGO = True
except ImportError:
    HAS_DJANGO = False

LOGGER = logging.getLogger(__name__)


if HAS_DJANGO:
    class MetadataClientModel(models.Model,
                              Bcfg2.Server.Plugin.PluginDatabaseModel):
        """ django model for storing clients in the database """
        hostname = models.CharField(max_length=255, primary_key=True)
        version = models.CharField(max_length=31, null=True)

    class ClientVersions(MutableMapping,
                         Bcfg2.Server.Plugin.DatabaseBacked):
        """ dict-like object to make it easier to access client bcfg2
        versions from the database """

        create = False

        def __getitem__(self, key):
            try:
                return MetadataClientModel.objects.get(hostname=key).version
            except MetadataClientModel.DoesNotExist:
                raise KeyError(key)

        @Bcfg2.Server.Plugin.DatabaseBacked.get_db_lock
        def __setitem__(self, key, value):
            client, created = \
                MetadataClientModel.objects.get_or_create(hostname=key)
            if created or client.version != value:
                client.version = value
                client.save()

        @Bcfg2.Server.Plugin.DatabaseBacked.get_db_lock
        def __delitem__(self, key):
            # UserDict didn't require __delitem__, but MutableMapping
            # does.  we don't want deleting a client version record to
            # delete the client, so we just set the version to None,
            # which is kinda like deleting it, but not really.
            try:
                client = MetadataClientModel.objects.get(hostname=key)
            except MetadataClientModel.DoesNotExist:
                raise KeyError(key)
            client.version = None
            client.save()

        def __len__(self):
            return MetadataClientModel.objects.count()

        def __iter__(self):
            for client in MetadataClientModel.objects.all():
                yield client.hostname

        def keys(self):
            """ Get keys for the mapping """
            return [c.hostname for c in MetadataClientModel.objects.all()]

        def __contains__(self, key):
            try:
                MetadataClientModel.objects.get(hostname=key)
                return True
            except MetadataClientModel.DoesNotExist:
                return False


class XMLMetadataConfig(Bcfg2.Server.Plugin.XMLFileBacked):
    """Handles xml config files and all XInclude statements"""

    def __init__(self, metadata, watch_clients, basefile):
        # we tell XMLFileBacked _not_ to add a monitor for this file,
        # because the main Metadata plugin has already added one.
        # then we immediately set should_monitor to the proper value,
        # so that XInclude'd files get properly watched
        fpath = os.path.join(metadata.data, basefile)
        toptag = os.path.splitext(basefile)[0].title()
        Bcfg2.Server.Plugin.XMLFileBacked.__init__(self, fpath,
                                                   fam=metadata.core.fam,
                                                   should_monitor=False,
                                                   create=toptag)
        self.should_monitor = watch_clients
        self.metadata = metadata
        self.basefile = basefile
        self.data = None
        self.basedata = None
        self.basedir = metadata.data
        self.logger = metadata.logger
        self.pseudo_monitor = isinstance(metadata.core.fam,
                                         Bcfg2.Server.FileMonitor.Pseudo)

    def _get_xdata(self):
        """ getter for xdata property """
        if not self.data:
            raise Bcfg2.Server.Plugin.MetadataRuntimeError("%s has no data" %
                                                           self.basefile)
        return self.data

    def _set_xdata(self, val):
        """ setter for xdata property. in practice this should only be
        used by the test suite """
        self.data = val

    xdata = property(_get_xdata, _set_xdata)

    @property
    def base_xdata(self):
        """ property to get the data of the base file (without any
        xincludes processed) """
        if not self.basedata:
            raise Bcfg2.Server.Plugin.MetadataRuntimeError("%s has no data" %
                                                           self.basefile)
        return self.basedata

    def load_xml(self):
        """Load changes from XML"""
        try:
            xdata = lxml.etree.parse(os.path.join(self.basedir, self.basefile),
                                     parser=Bcfg2.Server.XMLParser)
        except lxml.etree.XMLSyntaxError:
            self.logger.error('Failed to parse %s' % self.basefile)
            return
        self.extras = []
        self.basedata = copy.deepcopy(xdata)
        self._follow_xincludes(xdata=xdata)
        if self.extras:
            try:
                xdata.xinclude()
            except lxml.etree.XIncludeError:
                self.logger.error("Failed to process XInclude for file %s" %
                                  self.basefile)
        self.data = xdata

    def write(self):
        """Write changes to xml back to disk."""
        self.write_xml(os.path.join(self.basedir, self.basefile),
                       self.basedata)

    def write_xml(self, fname, xmltree):
        """Write changes to xml back to disk."""
        tmpfile = "%s.new" % fname
        datafile = None
        fd = None
        i = 0  # counter to avoid flooding logs with lock messages
        while datafile is None:
            try:
                fd = os.open(tmpfile, os.O_CREAT | os.O_EXCL | os.O_WRONLY)
                datafile = os.fdopen(fd, 'w')
            except OSError:
                err = sys.exc_info()[1]
                if err.errno == errno.EEXIST:
                    # note: not a real lock.  this is here to avoid
                    # the scenario where two threads write to the file
                    # at the same-ish time, and one writes to
                    # foo.xml.new, then the other one writes to it
                    # (losing the first thread's changes), then the
                    # first renames it, then the second tries to
                    # rename it and borks.
                    if (i % 10) == 0:
                        self.logger.info("%s is locked, waiting" % fname)
                    i += 1
                    time.sleep(0.1)
                else:
                    msg = "Failed to write %s: %s" % (tmpfile, err)
                    self.logger.error(msg)
                    raise Bcfg2.Server.Plugin.MetadataRuntimeError(msg)
        # prep data
        dataroot = xmltree.getroot()
        newcontents = lxml.etree.tostring(dataroot, xml_declaration=False,
                                          pretty_print=True).decode('UTF-8')

        while locked(fd):
            pass
        try:
            datafile.write(newcontents)
        except:
            fcntl.lockf(fd, fcntl.LOCK_UN)
            msg = "Metadata: Failed to write new xml data to %s: %s" % \
                (tmpfile, sys.exc_info()[1])
            self.logger.error(msg, exc_info=1)
            os.unlink(tmpfile)
            raise Bcfg2.Server.Plugin.MetadataRuntimeError(msg)
        datafile.close()
        # check if clients.xml is a symlink
        if os.path.islink(fname):
            fname = os.readlink(fname)

        try:
            os.rename(tmpfile, fname)
        except:  # pylint: disable=W0702
            try:
                os.unlink(tmpfile)
            except:  # pylint: disable=W0702
                pass
            msg = "Metadata: Failed to rename %s: %s" % (tmpfile,
                                                         sys.exc_info()[1])
            self.logger.error(msg)
            raise Bcfg2.Server.Plugin.MetadataRuntimeError(msg)
        self.load_xml()

    def find_xml_for_xpath(self, xpath):
        """Find and load xml file containing the xpath query"""
        if self.pseudo_monitor:
            # Reload xml if we don't have a real monitor
            self.load_xml()
        cli = self.basedata.xpath(xpath)
        if len(cli) > 0:
            return {'filename': os.path.join(self.basedir, self.basefile),
                    'xmltree': self.basedata,
                    'xquery': cli}
        else:
            # Try to find the data in included files
            for included in self.extras:
                try:
                    xdata = lxml.etree.parse(included,
                                             parser=Bcfg2.Server.XMLParser)
                    cli = xdata.xpath(xpath)
                    if len(cli) > 0:
                        return {'filename': included,
                                'xmltree': xdata,
                                'xquery': cli}
                except lxml.etree.XMLSyntaxError:
                    self.logger.error('Failed to parse %s' % included)
        return {}

    def add_monitor(self, fpath):
        self.extras.append(fpath)
        if self.fam and self.should_monitor:
            self.fam.AddMonitor(fpath, self.metadata)

    def HandleEvent(self, event=None):
        """Handle fam events"""
        filename = os.path.basename(event.filename)
        if event.filename in self.extras:
            if event.code2str() == 'exists':
                return False
        elif filename != self.basefile:
            return False
        if event.code2str() == 'endExist':
            return False
        self.load_xml()
        return True


class ClientMetadata(object):
    """This object contains client metadata."""
    # pylint: disable=R0913
    def __init__(self, client, profile, groups, bundles, aliases, addresses,
                 categories, uuid, password, version, query):
        #: The client hostname (as a string)
        self.hostname = client

        #: The client profile (as a string)
        self.profile = profile

        #: The set of all bundles this client gets
        self.bundles = bundles

        #: A list of all client aliases
        self.aliases = aliases

        #: A list of all addresses this client is known by
        self.addresses = addresses

        #: A list of groups this client is a member of
        self.groups = groups

        #: A dict of categories of this client's groups.  Keys are
        #: category names, values are corresponding group names.
        self.categories = categories

        #: The UUID identifier for this client
        self.uuid = uuid

        #: The Bcfg2 password for this client
        self.password = password

        #: Connector plugins known to this client
        self.connectors = []

        #: The version of the Bcfg2 client this client is running, as
        #: a string
        self.version = version
        try:
            #: The version of the Bcfg2 client this client is running,
            #: as a :class:`Bcfg2.version.Bcfg2VersionInfo` object.
            self.version_info = Bcfg2VersionInfo(version)
        except (ValueError, AttributeError):
            self.version_info = None

        #: A :class:`Bcfg2.Server.Plugins.Metadata.MetadataQuery`
        #: object for this client.
        self.query = query
    # pylint: enable=R0913

    def inGroup(self, group):
        """Test to see if client is a member of group.

        :returns: bool """
        return group in self.groups

    def group_in_category(self, category):
        """ Return the group in the given category that the client is
        a member of, or an empty string.

        :returns: string """
        for grp in self.query.all_groups_in_category(category):
            if grp in self.groups:
                return grp
        return ''

    def __repr__(self):
        return "%s(%s, profile=%s, groups=%s)" % (self.__class__.__name__,
                                                  self.hostname,
                                                  self.profile, self.groups)


class MetadataQuery(object):
    """ This class provides query methods for the metadata of all
    clients known to the Bcfg2 server, without being able to modify
    that data.

    Note that ``*by_groups()`` and ``*by_profiles()`` behave
    differently; for a client to be included in the return value of a
    ``*by_groups()`` method, it must be a member of *all* groups
    listed in the argument; for a client to be included in the return
    value of a ``*by_profiles()`` method, it must have *any* group
    listed as its profile group. """

    def __init__(self, by_name, get_clients, by_groups, by_profiles,
                 all_groups, all_groups_in_category):
        #: Get :class:`Bcfg2.Server.Plugins.Metadata.ClientMetadata`
        #: object for the given hostname.
        #:
        #: :returns: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        self.by_name = by_name

        #: Get a list of hostnames of clients that are in all given
        #: groups.
        #:
        #: :param groups: The groups to check clients for membership in
        #: :type groups: list
        #:
        #: :returns: list of strings
        self.names_by_groups = self._warn_string(by_groups)

        #: Get a list of hostnames of clients whose profile matches
        #: any given profile group.
        #:
        #: :param profiles: The profiles to check clients for
        #:                  membership in.
        #: :type profiles: list
        #: :returns: list of strings
        self.names_by_profiles = self._warn_string(by_profiles)

        #: Get all known client hostnames.
        #:
        #: :returns: list of strings
        self.all_clients = get_clients

        #: Get all known group names.
        #:
        #: :returns: list of strings
        self.all_groups = all_groups

        #: Get the names of all groups in the given category.
        #:
        #: :param category: The category to query for groups that
        #:                  belong to it.
        #: :type category: string
        #: :returns: list of strings
        self.all_groups_in_category = all_groups_in_category

    def _warn_string(self, func):
        """ decorator to warn that a MetadataQuery function that
        expects a list has been called with a single string argument
        instead.  this is a common mistake in templates, and it
        doesn't cause errors because strings are iterables """

        # pylint: disable=C0111
        @wraps(func)
        def inner(arg):
            if isinstance(arg, str):
                LOGGER.warning("%s: %s takes a list as argument, not a string"
                               % (self.__class__.__name__, func.__name__))
            return func(arg)
        # pylint: enable=C0111

        return inner

    def by_groups(self, groups):
        """ Get a list of
        :class:`Bcfg2.Server.Plugins.Metadata.ClientMetadata` objects
        that are in all given groups.

        :param groups: The groups to check clients for membership in.
        :type groups: list
        :returns: list of Bcfg2.Server.Plugins.Metadata.ClientMetadata
                  objects
        """
        # don't need to decorate this with _warn_string because
        # names_by_groups is decorated
        return [self.by_name(name) for name in self.names_by_groups(groups)]

    def by_profiles(self, profiles):
        """ Get a list of
        :class:`Bcfg2.Server.Plugins.Metadata.ClientMetadata` objects
        that have any of the given groups as their profile.

        :param profiles: The profiles to check clients for membership
                         in.
        :type profiles: list
        :returns: list of Bcfg2.Server.Plugins.Metadata.ClientMetadata
                  objects
        """
        # don't need to decorate this with _warn_string because
        # names_by_profiles is decorated
        return [self.by_name(name)
                for name in self.names_by_profiles(profiles)]

    def all(self):
        """ Get a list of all
        :class:`Bcfg2.Server.Plugins.Metadata.ClientMetadata` objects.

        :returns: list of Bcfg2.Server.Plugins.Metadata.ClientMetadata
        """
        return [self.by_name(name) for name in self.all_clients()]


class MetadataGroup(tuple):  # pylint: disable=E0012,R0924
    """ representation of a metadata group.  basically just a named tuple """

    # pylint: disable=R0913,W0613
    def __new__(cls, name, bundles=None, category=None, is_profile=False,
                is_public=False):
        if bundles is None:
            bundles = set()
        return tuple.__new__(cls, (bundles, category))
    # pylint: enable=W0613

    def __init__(self, name, bundles=None, category=None, is_profile=False,
                 is_public=False):
        if bundles is None:
            bundles = set()
        tuple.__init__(self)
        self.name = name
        self.bundles = bundles
        self.category = category
        self.is_profile = is_profile
        self.is_public = is_public
        # record which clients we've warned about category suppression
        self.warned = []
    # pylint: enable=R0913

    def __str__(self):
        return repr(self)

    def __repr__(self):
        return "%s %s (bundles=%s, category=%s)" % \
            (self.__class__.__name__, self.name, self.bundles,
             self.category)

    def __hash__(self):
        return hash(self.name)


class Metadata(Bcfg2.Server.Plugin.Metadata,
               Bcfg2.Server.Plugin.Caching,
               Bcfg2.Server.Plugin.ClientRunHooks,
               Bcfg2.Server.Plugin.DatabaseBacked):
    """This class contains data for bcfg2 server metadata."""
    __author__ = 'bcfg-dev@mcs.anl.gov'
    sort_order = 500

    def __init__(self, core, datastore, watch_clients=True):
        Bcfg2.Server.Plugin.Metadata.__init__(self)
        Bcfg2.Server.Plugin.Caching.__init__(self)
        Bcfg2.Server.Plugin.ClientRunHooks.__init__(self)
        Bcfg2.Server.Plugin.DatabaseBacked.__init__(self, core, datastore)
        self.watch_clients = watch_clients
        self.states = dict()
        self.extra = dict()
        self.handlers = dict()
        self.groups_xml = self._handle_file("groups.xml")
        if (self._use_db and
            os.path.exists(os.path.join(self.data, "clients.xml"))):
            self.logger.warning("Metadata: database enabled but clients.xml "
                                "found, parsing in compatibility mode")
            self.clients_xml = self._handle_file("clients.xml")
        elif not self._use_db:
            self.clients_xml = self._handle_file("clients.xml")

        # mapping of clientname -> authtype
        self.auth = dict()
        # list of clients required to have non-global password
        self.secure = []
        # list of floating clients
        self.floating = []
        # mapping of clientname -> password
        self.passwords = {}
        self.addresses = {}
        self.raddresses = {}
        # mapping of clientname -> [groups]
        self.clientgroups = {}
        # list of clients
        self.clients = []
        self.aliases = {}
        self.raliases = {}
        # mapping of groupname -> MetadataGroup object
        self.groups = {}
        # mappings of groupname -> [predicates]
        self.group_membership = dict()
        self.negated_groups = dict()
        # list of group names in document order
        self.ordered_groups = []
        # mapping of hostname -> version string
        if self._use_db:
            self.versions = ClientVersions(core, datastore)
        else:
            self.versions = dict()

        self.uuid = {}
        self.session_cache = {}
        self.default = None
        self.pdirty = False
        self.password = core.setup['password']
        self.query = MetadataQuery(core.build_metadata,
                                   self.list_clients,
                                   self.get_client_names_by_groups,
                                   self.get_client_names_by_profiles,
                                   self.get_all_group_names,
                                   self.get_all_groups_in_category)

    @classmethod
    def init_repo(cls, repo, **kwargs):
        # must use super here; inheritance works funny with class methods
        super(Metadata, cls).init_repo(repo)

        for fname in ["clients.xml", "groups.xml"]:
            aname = re.sub(r'[^A-z0-9_]', '_', fname)
            if aname in kwargs:
                open(os.path.join(repo, cls.name, fname),
                     "w").write(kwargs[aname])

    @property
    def use_database(self):
        """ Expose self._use_db publicly for use in
        :class:`Bcfg2.Server.MultiprocessingCore.ChildCore` """
        return self._use_db

    def _handle_file(self, fname):
        """ set up the necessary magic for handling a metadata file
        (clients.xml or groups.xml, e.g.) """
        if self.watch_clients:
            try:
                self.core.fam.AddMonitor(os.path.join(self.data, fname), self)
            except:
                err = sys.exc_info()[1]
                msg = "Unable to add file monitor for %s: %s" % (fname, err)
                self.logger.error(msg)
                raise Bcfg2.Server.Plugin.PluginInitError(msg)
            self.states[fname] = False
        xmlcfg = XMLMetadataConfig(self, self.watch_clients, fname)
        aname = re.sub(r'[^A-z0-9_]', '_', os.path.basename(fname))
        self.handlers[xmlcfg.HandleEvent] = getattr(self,
                                                    "_handle_%s_event" % aname)
        self.extra[fname] = []
        return xmlcfg

    def _search_xdata(self, tag, name, tree, alias=False):
        """ Generic method to find XML data (group, client, etc.) """
        for node in tree.findall("//%s" % tag):
            if node.get("name") == name:
                return node
            elif alias:
                for child in node:
                    if (child.tag == "Alias" and
                        child.attrib["name"] == name):
                        return node
        return None

    def search_group(self, group_name, tree):
        """Find a group."""
        return self._search_xdata("Group", group_name, tree)

    def search_bundle(self, bundle_name, tree):
        """Find a bundle."""
        return self._search_xdata("Bundle", bundle_name, tree)

    def search_client(self, client_name, tree):
        """ find a client in the given XML tree """
        return self._search_xdata("Client", client_name, tree, alias=True)

    def _add_xdata(self, config, tag, name, attribs=None, alias=False):
        """ Generic method to add XML data (group, client, etc.) """
        node = self._search_xdata(tag, name, config.xdata, alias=alias)
        if node is not None:
            raise Bcfg2.Server.Plugin.MetadataConsistencyError("%s \"%s\" "
                                                               "already exists"
                                                               % (tag, name))
        element = lxml.etree.SubElement(config.base_xdata.getroot(),
                                        tag, name=name)
        if attribs:
            for key, val in list(attribs.items()):
                element.set(key, val)
        config.write()
        return element

    def add_group(self, group_name, attribs):
        """Add group to groups.xml."""
        if self._use_db:
            msg = "Metadata does not support adding groups with " + \
                "use_database enabled"
            self.logger.error(msg)
            raise Bcfg2.Server.Plugin.PluginExecutionError(msg)
        else:
            return self._add_xdata(self.groups_xml, "Group", group_name,
                                   attribs=attribs)

    def add_bundle(self, bundle_name):
        """Add bundle to groups.xml."""
        if self._use_db:
            msg = "Metadata does not support adding bundles with " + \
                "use_database enabled"
            self.logger.error(msg)
            raise Bcfg2.Server.Plugin.PluginExecutionError(msg)
        else:
            return self._add_xdata(self.groups_xml, "Bundle", bundle_name)

    @Bcfg2.Server.Plugin.DatabaseBacked.get_db_lock
    def add_client(self, client_name, attribs=None):
        """Add client to clients.xml."""
        if attribs is None:
            attribs = dict()
        if self._use_db:
            if attribs:
                msg = "Metadata does not support setting client attributes " +\
                      "with use_database enabled"
                self.logger.error(msg)
                raise Bcfg2.Server.Plugin.PluginExecutionError(msg)
            try:
                client = MetadataClientModel.objects.get(hostname=client_name)
            except MetadataClientModel.DoesNotExist:
                client = MetadataClientModel(hostname=client_name)
                client.save()
            self.update_client_list()
            return client
        else:
            try:
                return self._add_xdata(self.clients_xml, "Client", client_name,
                                       attribs=attribs, alias=True)
            except Bcfg2.Server.Plugin.MetadataConsistencyError:
                # already exists
                err = sys.exc_info()[1]
                self.logger.info(err)
                return self._search_xdata("Client", client_name,
                                          self.clients_xml.xdata, alias=True)

    def _update_xdata(self, config, tag, name, attribs, alias=False):
        """ Generic method to modify XML data (group, client, etc.) """
        node = self._search_xdata(tag, name, config.xdata, alias=alias)
        if node is None:
            msg = "%s \"%s\" does not exist" % (tag, name)
            self.logger.error(msg)
            raise Bcfg2.Server.Plugin.MetadataConsistencyError(msg)
        xdict = config.find_xml_for_xpath('.//%s[@name="%s"]' %
                                          (tag, node.get('name')))
        if not xdict:
            msg = 'Unexpected error finding %s "%s"' % (tag, name)
            self.logger.error(msg)
            raise Bcfg2.Server.Plugin.MetadataConsistencyError(msg)
        for key, val in list(attribs.items()):
            xdict['xquery'][0].set(key, val)
        config.write_xml(xdict['filename'], xdict['xmltree'])

    def update_group(self, group_name, attribs):
        """Update a groups attributes."""
        if self._use_db:
            msg = "Metadata does not support updating groups with " + \
                "use_database enabled"
            self.logger.error(msg)
            raise Bcfg2.Server.Plugin.PluginExecutionError(msg)
        else:
            return self._update_xdata(self.groups_xml, "Group", group_name,
                                      attribs)

    def update_client(self, client_name, attribs):
        """Update a clients attributes."""
        if self._use_db:
            msg = "Metadata does not support updating clients with " + \
                "use_database enabled"
            self.logger.error(msg)
            raise Bcfg2.Server.Plugin.PluginExecutionError(msg)
        else:
            return self._update_xdata(self.clients_xml, "Client", client_name,
                                      attribs, alias=True)

    def list_clients(self):
        """ List all clients in client database.

        Making ``self.clients`` a property and reading the client list
        dynamically from the database on every call to
        ``self.clients`` can result in very high rates of database
        reads, so we cache the ``list_clients()`` results to reduce
        the database load.  When the database is in use, the client
        list is reread periodically with
        :func:`Bcfg2.Server.Plugins.Metadata.update_client_list`. """
        if self._use_db:
            return set([c.hostname for c in MetadataClientModel.objects.all()])
        else:
            return self.clients

    def _remove_xdata(self, config, tag, name):
        """ Generic method to remove XML data (group, client, etc.) """
        node = self._search_xdata(tag, name, config.xdata)
        if node is None:
            self.logger.error("%s \"%s\" does not exist" % (tag, name))
            raise Bcfg2.Server.Plugin.MetadataConsistencyError
        xdict = config.find_xml_for_xpath('.//%s[@name="%s"]' %
                                          (tag, node.get('name')))
        if not xdict:
            self.logger.error("Unexpected error finding %s \"%s\"" %
                              (tag, name))
            raise Bcfg2.Server.Plugin.MetadataConsistencyError
        xdict['xquery'][0].getparent().remove(xdict['xquery'][0])
        config.write_xml(xdict['filename'], xdict['xmltree'])

    def remove_group(self, group_name):
        """Remove a group."""
        if self._use_db:
            msg = "Metadata does not support removing groups with " + \
                "use_database enabled"
            self.logger.error(msg)
            raise Bcfg2.Server.Plugin.PluginExecutionError(msg)
        else:
            return self._remove_xdata(self.groups_xml, "Group", group_name)

    def remove_bundle(self, bundle_name):
        """Remove a bundle."""
        if self._use_db:
            msg = "Metadata does not support removing bundles with " + \
                "use_database enabled"
            self.logger.error(msg)
            raise Bcfg2.Server.Plugin.PluginExecutionError(msg)
        else:
            return self._remove_xdata(self.groups_xml, "Bundle", bundle_name)

    def remove_client(self, client_name):
        """Remove a client."""
        if self._use_db:
            try:
                client = MetadataClientModel.objects.get(hostname=client_name)
            except MetadataClientModel.DoesNotExist:
                msg = "Client %s does not exist" % client_name
                self.logger.warning(msg)
                raise Bcfg2.Server.Plugin.MetadataConsistencyError(msg)
            client.delete()
            self.update_client_list()
        else:
            return self._remove_xdata(self.clients_xml, "Client", client_name)

    def _handle_clients_xml_event(self, _):  # pylint: disable=R0912
        """ handle all events for clients.xml and files xincluded from
        clients.xml """
        # disable metadata builds during parsing.  this prevents
        # clients from getting bogus metadata during the brief time it
        # takes to rebuild the clients.xml data
        self.states['clients.xml'] = False

        xdata = self.clients_xml.xdata
        self.clients = []
        self.clientgroups = {}
        self.aliases = {}
        self.raliases = {}
        self.secure = []
        self.floating = []
        self.addresses = {}
        self.raddresses = {}
        for client in xdata.findall('.//Client'):
            clname = client.get('name').lower()
            if 'address' in client.attrib:
                caddr = client.get('address')
                if caddr in self.addresses:
                    self.addresses[caddr].append(clname)
                else:
                    self.addresses[caddr] = [clname]
                if clname not in self.raddresses:
                    self.raddresses[clname] = set()
                self.raddresses[clname].add(caddr)
            if 'auth' in client.attrib:
                self.auth[client.get('name')] = client.get('auth')
            if 'uuid' in client.attrib:
                self.uuid[client.get('uuid')] = clname
            if client.get('secure', 'false').lower() == 'true':
                self.secure.append(clname)
            if (client.get('location', 'fixed') == 'floating' or
                client.get('floating', 'false').lower() == 'true'):
                self.floating.append(clname)
            if 'password' in client.attrib:
                self.passwords[clname] = client.get('password')
            if 'version' in client.attrib:
                self.versions[clname] = client.get('version')

            self.raliases[clname] = set()
            for alias in client.findall('Alias'):
                self.aliases.update({alias.get('name'): clname})
                self.raliases[clname].add(alias.get('name'))
                if 'address' not in alias.attrib:
                    continue
                if alias.get('address') in self.addresses:
                    self.addresses[alias.get('address')].append(clname)
                else:
                    self.addresses[alias.get('address')] = [clname]
                if clname not in self.raddresses:
                    self.raddresses[clname] = set()
                self.raddresses[clname].add(alias.get('address'))
            self.clients.append(clname)
            profile = client.get("profile")
            if self.groups:  # check if we've parsed groups.xml yet
                if profile not in self.groups:
                    self.logger.warning("Metadata: %s has nonexistent "
                                        "profile group %s" % (clname, profile))
                elif not self.groups[profile].is_profile:
                    self.logger.warning("Metadata: %s set as profile for "
                                        "%s, but is not a profile group" %
                                        (profile, clname))
            try:
                self.clientgroups[clname].append(profile)
            except KeyError:
                self.clientgroups[clname] = [profile]
        self.update_client_list()
        self.expire_cache()
        self.states['clients.xml'] = True

    def _get_condition(self, element):
        """ Return a predicate that returns True if a client meets
        the condition specified in the given Group or Client
        element """
        negate = element.get('negate', 'false').lower() == 'true'
        pname = element.get("name")
        if element.tag == 'Group':
            return lambda c, g, _: negate != (pname in g)
        elif element.tag == 'Client':
            return lambda c, g, _: negate != (pname == c)

    def _get_category_condition(self, grpname):
        """ get a predicate that returns False if a client is already
        a member of a group in the given group's category, True
        otherwise"""
        return lambda client, _, categories: \
            bool(self._check_category(client, grpname, categories))

    def _aggregate_conditions(self, conditions):
        """ aggregate all conditions on a given group declaration
        into a single predicate """
        return lambda client, groups, cats: \
            all(cond(client, groups, cats) for cond in conditions)

    def _handle_groups_xml_event(self, _):  # pylint: disable=R0912
        """ re-read groups.xml on any event on it """
        # disable metadata builds during parsing.  this prevents
        # clients from getting bogus metadata during the brief time it
        # takes to rebuild the groups.xml data
        self.states['groups.xml'] = False

        self.groups = {}
        self.group_membership = dict()
        self.negated_groups = dict()
        self.ordered_groups = []

        # first, we get a list of all of the groups declared in the
        # file.  we do this in two stages because the old way of
        # parsing groups.xml didn't support nested groups; in the old
        # way, only Group tags under a Groups tag counted as
        # declarative.  so we parse those first, and then parse the
        # other Group tags if they haven't already been declared.
        # this lets you set options on a group (e.g., public="false")
        # at the top level and then just use the name elsewhere, which
        # is the original behavior
        for grp in self.groups_xml.xdata.xpath("//Groups/Group") + \
                self.groups_xml.xdata.xpath("//Groups/Group//Group"):
            if grp.get("name") in self.groups:
                continue
            self.groups[grp.get("name")] = \
                MetadataGroup(grp.get("name"),
                              bundles=[b.get("name")
                                       for b in grp.findall("Bundle")],
                              category=grp.get("category"),
                              is_profile=grp.get("profile", "false") == "true",
                              is_public=grp.get("public", "false") == "true")
            if grp.get('default', 'false') == 'true':
                self.default = grp.get('name')

        # confusing loop condition; the XPath query asks for all
        # elements under a Group tag under a Groups tag; that is
        # infinitely recursive, so "all" elements really means _all_
        # elements.  We then manually filter out non-Group elements
        # since there doesn't seem to be a way to get Group elements
        # of arbitrary depth with particular ultimate ancestors in
        # XPath.  We do the same thing for Client tags.
        for el in self.groups_xml.xdata.xpath("//Groups/Group//*") + \
                self.groups_xml.xdata.xpath("//Groups/Client//*"):
            if (el.tag != 'Group' and el.tag != 'Client') or el.getchildren():
                continue

            conditions = []
            for parent in el.iterancestors():
                cond = self._get_condition(parent)
                if cond:
                    conditions.append(cond)

            gname = el.get("name")
            if el.get("negate", "false").lower() == "true":
                self.negated_groups.setdefault(gname, [])
                self.negated_groups[gname].append(
                    self._aggregate_conditions(conditions))
            else:
                if self.groups[gname].category:
                    conditions.append(self._get_category_condition(gname))

                if gname not in self.ordered_groups:
                    self.ordered_groups.append(gname)
                self.group_membership.setdefault(gname, [])
                self.group_membership[gname].append(
                    self._aggregate_conditions(conditions))
        self.expire_cache()
        self.states['groups.xml'] = True

    def expire_cache(self, key=None):
        self.core.metadata_cache.expire(key)

    def HandleEvent(self, event):
        """Handle update events for data files."""
        for handles, event_handler in self.handlers.items():
            if handles(event):
                # clear the entire cache when we get an event for any
                # metadata file
                self.expire_cache()

                # clear out the list of category suppressions that
                # have been warned about, since this may change when
                # clients.xml or groups.xml changes.
                for group in self.groups.values():
                    group.warned = []
                event_handler(event)

        if False not in list(self.states.values()) and self.debug_flag:
            # check that all groups are real and complete. this is
            # just logged at a debug level because many groups might
            # be probed, and we don't want to warn about them.
            for client, groups in list(self.clientgroups.items()):
                for group in groups:
                    if group not in self.groups:
                        self.debug_log("Client %s set as nonexistent group %s"
                                       % (client, group))

    def set_profile(self, client, profile,  # pylint: disable=W0221
                    addresspair, require_public=True):
        """Set group parameter for provided client."""
        self.logger.info("Asserting client %s profile to %s" % (client,
                                                                profile))
        if False in list(self.states.values()):
            raise Bcfg2.Server.Plugin.MetadataRuntimeError("Metadata has not "
                                                           "been read yet")
        if profile not in self.groups:
            msg = "Profile group %s does not exist" % profile
            self.logger.error(msg)
            raise Bcfg2.Server.Plugin.MetadataConsistencyError(msg)
        group = self.groups[profile]
        if require_public and not group.is_public:
            msg = "Cannot set client %s to private group %s" % (client,
                                                                profile)
            self.logger.error(msg)
            raise Bcfg2.Server.Plugin.MetadataConsistencyError(msg)

        if client in self.clients:
            if self._use_db:
                msg = "DBMetadata does not support asserting client profiles"
                self.logger.error(msg)
                raise Bcfg2.Server.Plugin.PluginExecutionError(msg)

            metadata = self.core.build_metadata(client)
            if metadata.profile != profile:
                self.logger.info("Changing %s profile from %s to %s" %
                                 (client, metadata.profile, profile))
                self.update_client(client, dict(profile=profile))
                if client in self.clientgroups:
                    if metadata.profile in self.clientgroups[client]:
                        self.clientgroups[client].remove(metadata.profile)
                    self.clientgroups[client].append(profile)
                else:
                    self.clientgroups[client] = [profile]
            else:
                self.logger.debug(
                    "Ignoring %s request to change profile from %s to %s"
                    % (client, metadata.profile, profile))
        else:
            self.logger.info("Creating new client: %s, profile %s" %
                             (client, profile))
            if self._use_db:
                self.add_client(client)
            else:
                if addresspair in self.session_cache:
                    # we are working with a uuid'd client
                    self.add_client(self.session_cache[addresspair][1],
                                    dict(uuid=client, profile=profile,
                                         address=addresspair[0]))
                else:
                    self.add_client(client, dict(profile=profile))
                self.clients.append(client)
                self.clientgroups[client] = [profile]
            if not self._use_db:
                self.clients_xml.write()

    def set_version(self, client, version):
        """Set version for provided client."""
        if client not in self.clients:
            # this creates the client as a side effect
            self.get_initial_metadata(client)

        if client not in self.versions or version != self.versions[client]:
            self.logger.info("Setting client %s version to %s" % (client,
                                                                  version))
            if not self._use_db:
                self.update_client(client, dict(version=version))
                self.clients_xml.write()
            self.versions[client] = version

    def resolve_client(self, addresspair, cleanup_cache=False):
        """Lookup address locally or in DNS to get a hostname."""
        if addresspair in self.session_cache:
            # client _was_ cached, so there can be some expired
            # entries. we need to clean them up to avoid potentially
            # infinite memory swell
            cache_ttl = 90
            if cleanup_cache:
                # remove entries for this client's IP address with
                # _any_ port numbers - perhaps a priority queue could
                # be faster?
                curtime = time.time()
                for addrpair in list(self.session_cache.keys()):
                    if addresspair[0] == addrpair[0]:
                        (stamp, _) = self.session_cache[addrpair]
                        if curtime - stamp > cache_ttl:
                            del self.session_cache[addrpair]
            # return the cached data
            try:
                stamp = self.session_cache[addresspair][0]
                if time.time() - stamp < cache_ttl:
                    return self.session_cache[addresspair][1]
            except KeyError:
                # we cleaned all cached data for this client in cleanup_cache
                pass
        address = addresspair[0]
        if address in self.addresses:
            if len(self.addresses[address]) != 1:
                err = ("Address %s has multiple reverse assignments; a "
                       "uuid must be used" % address)
                self.logger.error(err)
                raise Bcfg2.Server.Plugin.MetadataConsistencyError(err)
            return self.addresses[address][0]
        try:
            cname = socket.getnameinfo(addresspair,
                                       socket.NI_NAMEREQD)[0].lower()
            if cname in self.aliases:
                return self.aliases[cname]
            return cname
        except (socket.gaierror, socket.herror):
            err = "Address resolution error for %s: %s" % (address,
                                                           sys.exc_info()[1])
            self.logger.error(err)
            raise Bcfg2.Server.Plugin.MetadataConsistencyError(err)

    def _merge_groups(self, client, groups, categories=None):
        """ set group membership based on the contents of groups.xml
        and initial group membership of this client. Returns a tuple
        of (allgroups, categories)"""
        numgroups = -1  # force one initial pass
        if categories is None:
            categories = dict()
        while numgroups != len(groups):
            numgroups = len(groups)
            newgroups = set()
            removegroups = set()
            for grpname in self.ordered_groups:
                if grpname in groups:
                    continue
                if any(p(client, groups, categories)
                       for p in self.group_membership[grpname]):
                    newgroups.add(grpname)
                    if (grpname in self.groups and
                        self.groups[grpname].category):
                        categories[self.groups[grpname].category] = grpname
            groups.update(newgroups)
            for grpname, predicates in self.negated_groups.items():
                if grpname not in groups:
                    continue
                if any(p(client, groups, categories) for p in predicates):
                    removegroups.add(grpname)
                    if (grpname in self.groups and
                        self.groups[grpname].category):
                        del categories[self.groups[grpname].category]
            groups.difference_update(removegroups)
        return (groups, categories)

    def _check_category(self, client, grpname, categories):
        """ Determine if the given client is already a member of a
        group in the same category as the named group.

        The return value is one of three possibilities:

        * If the client is already a member of a group in the same
          category, then False is returned (i.e., the category check
          failed);
        * If the group is not in any categories, then True is returned;
        * If the group is not a member of a group in the category,
          then the name of the category is returned.  This makes it
          easy to add the category to the ClientMetadata object (or
          other category list).

        If a pure boolean value is required, you can do
        ``bool(self._check_category(...))``.
        """
        if grpname not in self.groups:
            return True
        category = self.groups[grpname].category
        if not category:
            return True
        if category in categories:
            if client not in self.groups[grpname].warned:
                self.logger.warning("%s: Group %s suppressed by category %s; "
                                    "%s already a member of %s" %
                                    (self.name, grpname, category,
                                     client, categories[category]))
                self.groups[grpname].warned.append(client)
            return False
        return category

    def _check_and_add_category(self, client, grpname, categories):
        """ If the client is not a member of a group in the same
        category as the named group, then the category is added to
        ``categories``.
        :func:`Bcfg2.Server.Plugins.Metadata._check_category` is used
        to determine if the category can be added.

        If the category check failed, returns False; otherwise,
        returns True. """
        rv = self._check_category(client, grpname, categories)
        if rv and rv is not True:
            categories[rv] = grpname
            return True
        return rv

    def get_initial_metadata(self, client):  # pylint: disable=R0914,R0912
        """Return the metadata for a given client."""
        if False in list(self.states.values()):
            raise Bcfg2.Server.Plugin.MetadataRuntimeError("Metadata has not "
                                                           "been read yet")
        client = client.lower()
        if client in self.core.metadata_cache:
            return self.core.metadata_cache[client]

        if client in self.aliases:
            client = self.aliases[client]

        groups = set()
        categories = dict()
        profile = None

        def _add_group(grpname):
            """ Add a group to the set of groups for this client.
            Handles setting categories and category suppression.
            Returns the new profile for the client (which might be
            unchanged). """
            if grpname in self.groups:
                if not self._check_and_add_category(client, grpname,
                                                    categories):
                    return profile
                groups.add(grpname)
                if not profile and self.groups[grpname].is_profile:
                    return grpname
                else:
                    return profile
            else:
                groups.add(grpname)
                return profile

        if client not in self.clients:
            pgroup = None
            if client in self.clientgroups:
                pgroup = self.clientgroups[client][0]
                self.debug_log("%s: Adding new client with profile %s" %
                               (self.name, pgroup))
            elif self.default:
                pgroup = self.default
                self.debug_log("%s: Adding new client with default profile %s"
                               % (self.name, pgroup))

            if pgroup:
                self.set_profile(client, pgroup, (None, None),
                                 require_public=False)
                profile = _add_group(pgroup)
            else:
                raise Bcfg2.Server.Plugin.MetadataConsistencyError(
                    "Cannot add new client %s; no default group set" % client)

        for cgroup in self.clientgroups.get(client, []):
            if cgroup in groups:
                continue
            if cgroup not in self.groups:
                self.groups[cgroup] = MetadataGroup(cgroup)
            profile = _add_group(cgroup)

        # we do this before setting the default because there may be
        # groups set in <Client> tags in groups.xml that we want to
        # set
        groups, categories = self._merge_groups(client, groups,
                                                categories=categories)

        if len(groups) == 0 and self.default:
            # no initial groups; add the default profile
            profile = _add_group(self.default)
            groups, categories = self._merge_groups(client, groups,
                                                    categories=categories)

        bundles = set()
        for group in groups:
            try:
                bundles.update(self.groups[group].bundles)
            except KeyError:
                self.logger.warning("%s: %s is a member of undefined group %s"
                                    % (self.name, client, group))

        aliases = self.raliases.get(client, set())
        addresses = self.raddresses.get(client, set())
        version = self.versions.get(client, None)
        if client in self.passwords:
            password = self.passwords[client]
        else:
            password = None
        uuids = [item for item, value in list(self.uuid.items())
                 if value == client]
        if uuids:
            uuid = uuids[0]
        else:
            uuid = None
        if not profile:
            # one last ditch attempt at setting the profile
            profiles = [g for g in groups
                        if g in self.groups and self.groups[g].is_profile]
            if len(profiles) >= 1:
                profile = profiles[0]

        rv = ClientMetadata(client, profile, groups, bundles, aliases,
                            addresses, categories, uuid, password, version,
                            self.query)
        if self.core.metadata_cache_mode == 'initial':
            self.core.metadata_cache[client] = rv
        return rv

    def get_all_group_names(self):
        """ return a list of all group names """
        all_groups = set()
        all_groups.update(self.groups.keys())
        all_groups.update(self.group_membership.keys())
        all_groups.update(self.negated_groups.keys())
        for grp in self.clientgroups.values():
            all_groups.update(grp)
        return all_groups

    def get_all_groups_in_category(self, category):
        """ return a list of names of groups in the given category """
        return set([g.name for g in self.groups.values()
                    if g.category == category])

    def get_client_names_by_profiles(self, profiles):
        """ return a list of names of clients in the given profile groups """
        rv = []
        for client in self.list_clients():
            mdata = self.core.build_metadata(client)
            if mdata.profile in profiles:
                rv.append(client)
        return rv

    def get_client_names_by_groups(self, groups):
        """ return a list of names of clients in the given groups """
        rv = []
        for client in self.list_clients():
            mdata = self.core.build_metadata(client)
            if mdata.groups.issuperset(groups):
                rv.append(client)
        return rv

    def get_client_names_by_bundles(self, bundles):
        """ given a list of bundles, return a list of names of clients
        that use those bundles """
        rv = []
        for client in self.list_clients():
            mdata = self.core.build_metadata(client)
            if mdata.bundles.issuperset(bundles):
                rv.append(client)
        return rv

    def merge_additional_groups(self, imd, groups):
        for group in groups:
            if group in imd.groups:
                continue
            if not self._check_and_add_category(imd.hostname, group,
                                                imd.categories):
                continue
            imd.groups.add(group)

        self._merge_groups(imd.hostname, imd.groups, categories=imd.categories)
        for group in imd.groups:
            if group in self.groups:
                imd.bundles.update(self.groups[group].bundles)

        if not imd.profile:
            # if the client still doesn't have a profile group after
            # initial metadata, try to find one in the additional
            # groups
            profiles = [g for g in groups
                        if g in self.groups and self.groups[g].is_profile]
            if len(profiles) >= 1:
                imd.profile = profiles[0]
            elif self.default:
                imd.profile = self.default

    def merge_additional_data(self, imd, source, data):
        if not hasattr(imd, source):
            setattr(imd, source, data)
            imd.connectors.append(source)

    def validate_client_address(self, client, addresspair):
        """Check address against client."""
        address = addresspair[0]
        if client in self.floating:
            self.debug_log("Client %s is floating" % client)
            return True
        if address in self.addresses:
            if client in self.addresses[address]:
                self.debug_log("Client %s matches address %s" %
                               (client, address))
                return True
            else:
                self.logger.error("Got request for non-float client %s from %s"
                                  % (client, address))
                return False
        resolved = self.resolve_client(addresspair)
        if resolved.lower() == client.lower():
            return True
        else:
            self.logger.error("Got request for %s from incorrect address %s" %
                              (client, address))
            self.logger.error("Resolved to %s" % resolved)
            return False

    # pylint: disable=R0911,R0912
    def AuthenticateConnection(self, cert, user, password, address):
        """This function checks auth creds."""
        if not isinstance(user, str):
            user = user.decode('utf-8')
        if cert:
            id_method = 'cert'
            certinfo = dict([x[0] for x in cert['subject']])
            # look at cert.cN
            client = certinfo['commonName']
            self.debug_log("Got cN %s; using as client name" % client)
            auth_type = self.auth.get(client,
                                      self.core.setup['authentication'])
        elif user == 'root':
            id_method = 'address'
            try:
                client = self.resolve_client(address)
            except Bcfg2.Server.Plugin.MetadataConsistencyError:
                err = sys.exc_info()[1]
                self.logger.error("Client %s failed to resolve: %s" %
                                  (address[0], err))
                return False
        else:
            id_method = 'uuid'
            # user maps to client
            if user not in self.uuid:
                client = user
                self.uuid[user] = user
            else:
                client = self.uuid[user]

        # we have the client name
        self.debug_log("Authenticating client %s" % client)

        # next we validate the address
        if (id_method != 'uuid' and
            not self.validate_client_address(client, address)):
            return False

        if id_method == 'cert' and auth_type != 'cert+password':
            # remember the cert-derived client name for this connection
            if client in self.floating:
                self.session_cache[address] = (time.time(), client)
            # we are done if cert+password not required
            return True

        if client not in self.passwords and client in self.secure:
            self.logger.error("Client %s in secure mode but has no password" %
                              address[0])
            return False

        if client not in self.secure:
            if client in self.passwords:
                plist = [self.password, self.passwords[client]]
            else:
                plist = [self.password]
            if password not in plist:
                self.logger.error("Client %s failed to use an allowed password"
                                  % address[0])
                return False
        else:
            # client in secure mode and has a client password
            if password != self.passwords[client]:
                self.logger.error("Client %s failed to use client password in "
                                  "secure mode" % address[0])
                return False
        # populate the session cache
        if user != 'root':
            self.session_cache[address] = (time.time(), client)
        return True
    # pylint: enable=R0911,R0912

    def update_client_list(self):
        """ Re-read the client list from the database (if the database is in
        use) """
        if self._use_db:
            self.logger.debug("Metadata: Re-reading client list from database")
            old = set(self.clients)
            self.clients = self.list_clients()

            # we could do this with set.symmetric_difference(), but we
            # want detailed numbers of added/removed clients for
            # logging
            new = set(self.clients)
            added = new - old
            removed = old - new
            self.logger.debug("Metadata: Added %s clients: %s" %
                              (len(added), added))
            self.logger.debug("Metadata: Removed %s clients: %s" %
                              (len(removed), removed))

            for client in added.union(removed):
                self.expire_cache(client)

    def start_client_run(self, metadata):
        """ Hook to reread client list if the database is in use """
        self.update_client_list()

    def end_statistics(self, metadata):
        """ Hook to toggle clients in bootstrap mode """
        if self.auth.get(metadata.hostname,
                         self.core.setup['authentication']) == 'bootstrap':
            self.update_client(metadata.hostname, dict(auth='cert'))

    def viz(self, hosts, bundles, key, only_client, colors):
        """Admin mode viz support."""
        clientmeta = None
        if only_client:
            clientmeta = self.core.build_metadata(only_client)

        groups = self.groups_xml.xdata.getroot()
        categories = {'default': 'grey83'}
        viz_str = []
        egroups = groups.findall("Group") + groups.findall('.//Groups/Group')
        color = 0
        for group in egroups:
            if not group.get('category') in categories:
                categories[group.get('category')] = colors[color]
                color = (color + 1) % len(colors)
            group.set('color', categories[group.get('category')])
        if None in categories:
            del categories[None]
        if hosts:
            viz_str.extend(self._viz_hosts(only_client))
        if bundles:
            viz_str.extend(self._viz_bundles(bundles, clientmeta))
        viz_str.extend(self._viz_groups(egroups, bundles, clientmeta))
        if key:
            for category in categories:
                viz_str.append('"%s" [label="%s", shape="trapezium", '
                               'style="filled", fillcolor="%s"];' %
                               (category, category, categories[category]))
        return "\n".join("\t" + s for s in viz_str)

    def _viz_hosts(self, only_client):
        """ add hosts to the viz graph """
        def include_client(client):
            """ return True if the given client should be included in
            the graph"""
            return not only_client or client != only_client

        instances = {}
        rv = []
        for client in list(self.list_clients()):
            if not include_client(client):
                continue
            if client in self.clientgroups:
                grps = self.clientgroups[client]
            elif self.default:
                grps = [self.default]
            else:
                continue
            for group in grps:
                try:
                    instances[group].append(client)
                except KeyError:
                    instances[group] = [client]
        for group, clist in list(instances.items()):
            clist.sort()
            rv.append('"%s-instances" [ label="%s", shape="record" ];' %
                      (group, '|'.join(clist)))
            rv.append('"%s-instances" -> "group-%s";' % (group, group))
        return rv

    def _viz_bundles(self, bundles, clientmeta):
        """ add bundles to the viz graph """

        def include_bundle(bundle):
            """ return True if the given bundle should be included in
            the graph"""
            return not clientmeta or bundle in clientmeta.bundles

        bundles = \
            list(set(bund.get('name')
                     for bund in self.groups_xml.xdata.findall('.//Bundle')
                     if include_bundle(bund.get('name'))))
        bundles.sort()
        return ['"bundle-%s" [ label="%s", shape="septagon"];' % (bundle,
                                                                  bundle)
                for bundle in bundles]

    def _viz_groups(self, egroups, bundles, clientmeta):
        """ add groups to the viz graph """

        def include_group(group):
            """ return True if the given group should be included in
            the graph """
            return not clientmeta or group in clientmeta.groups

        rv = []
        gseen = []
        for group in egroups:
            if group.get('profile', 'false') == 'true':
                style = "filled, bold"
            else:
                style = "filled"
            gseen.append(group.get('name'))
            if include_group(group.get('name')):
                rv.append('"group-%s" [label="%s", style="%s", fillcolor=%s];'
                          % (group.get('name'), group.get('name'), style,
                             group.get('color')))
                if bundles:
                    for bundle in group.findall('Bundle'):
                        rv.append('"group-%s" -> "bundle-%s";' %
                                  (group.get('name'), bundle.get('name')))
        gfmt = '"group-%s" [label="%s", style="filled", fillcolor="grey83"];'
        for group in egroups:
            for parent in group.findall('Group'):
                if (parent.get('name') not in gseen and
                    include_group(parent.get('name'))):
                    rv.append(gfmt % (parent.get('name'),
                                      parent.get('name')))
                    gseen.append(parent.get("name"))
                if include_group(group.get('name')):
                    rv.append('"group-%s" -> "group-%s";' %
                              (group.get('name'), parent.get('name')))
        return rv


class MetadataLint(Bcfg2.Server.Lint.ServerPlugin):
    """ ``bcfg2-lint`` plugin for :ref:`Metadata
    <server-plugins-grouping-metadata>`.  This checks for several things:

    * ``<Client>`` tags nested inside other ``<Client>`` tags;
    * Deprecated options (like ``location="floating"``);
    * Profiles that don't exist, or that aren't profile groups;
    * Groups or clients that are defined multiple times;
    * Multiple default groups or a default group that isn't a profile
      group.
    """

    def Run(self):
        self.nested_clients()
        self.deprecated_options()
        self.bogus_profiles()
        self.duplicate_groups()
        self.duplicate_default_groups()
        self.duplicate_clients()
        self.default_is_profile()

    @classmethod
    def Errors(cls):
        return {"nested-client-tags": "warning",
                "deprecated-clients-options": "warning",
                "nonexistent-profile-group": "error",
                "non-profile-set-as-profile": "error",
                "duplicate-group": "error",
                "duplicate-client": "error",
                "multiple-default-groups": "error",
                "default-is-not-profile": "error"}

    def deprecated_options(self):
        """ Check for the ``location='floating'`` option, which has
        been deprecated in favor of ``floating='true'``. """
        if not hasattr(self.metadata, "clients_xml"):
            # using metadata database
            return
        clientdata = self.metadata.clients_xml.xdata
        for el in clientdata.xpath("//Client"):
            loc = el.get("location")
            if loc:
                if loc == "floating":
                    floating = True
                else:
                    floating = False
                self.LintError("deprecated-clients-options",
                               "The location='%s' option is deprecated.  "
                               "Please use floating='%s' instead:\n%s" %
                               (loc, floating, self.RenderXML(el)))

    def nested_clients(self):
        """ Check for a ``<Client/>`` tag inside a ``<Client/>`` tag,
        which is either redundant or will never match. """
        groupdata = self.metadata.groups_xml.xdata
        for el in groupdata.xpath("//Client//Client"):
            self.LintError("nested-client-tags",
                           "Client %s nested within Client tag: %s" %
                           (el.get("name"), self.RenderXML(el)))

    def bogus_profiles(self):
        """ Check for clients that have profiles that are either not
        flagged as profile groups in ``groups.xml``, or don't exist. """
        if not hasattr(self.metadata, "clients_xml"):
            # using metadata database
            return
        for client in self.metadata.clients_xml.xdata.findall('.//Client'):
            profile = client.get("profile")
            if profile not in self.metadata.groups:
                self.LintError("nonexistent-profile-group",
                               "%s has nonexistent profile group %s:\n%s" %
                               (client.get("name"), profile,
                                self.RenderXML(client)))
            elif not self.metadata.groups[profile].is_profile:
                self.LintError("non-profile-set-as-profile",
                               "%s is set as profile for %s, but %s is not a "
                               "profile group:\n%s" %
                               (profile, client.get("name"), profile,
                                self.RenderXML(client)))

    def duplicate_default_groups(self):
        """ Check for multiple default groups. """
        defaults = []
        for grp in self.metadata.groups_xml.xdata.xpath("//Groups/Group") + \
                self.metadata.groups_xml.xdata.xpath("//Groups/Group//Group"):
            if grp.get("default", "false").lower() == "true":
                defaults.append(self.RenderXML(grp))
        if len(defaults) > 1:
            self.LintError("multiple-default-groups",
                           "Multiple default groups defined:\n%s" %
                           "\n".join(defaults))

    def duplicate_clients(self):
        """ Check for clients that are defined more than once. """
        if not hasattr(self.metadata, "clients_xml"):
            # using metadata database
            return
        self.duplicate_entries(
            self.metadata.clients_xml.xdata.xpath("//Client"),
            "client")

    def duplicate_groups(self):
        """ Check for groups that are defined more than once. There are two
        ways this can happen:

        1. The group is listed twice with contradictory options.
        2. The group is listed with no options *first*, and then with
           options later.

        In this context, 'first' refers to the order in which groups
        are parsed; see the loop condition below and
        _handle_groups_xml_event above for details. """
        groups = dict()
        duplicates = dict()
        for grp in self.metadata.groups_xml.xdata.xpath("//Groups/Group") + \
                self.metadata.groups_xml.xdata.xpath("//Groups/Group//Group"):
            grpname = grp.get("name")
            if grpname in duplicates:
                duplicates[grpname].append(grp)
            elif set(grp.attrib.keys()).difference(['negate', 'name']):
                # group has options
                if grpname in groups:
                    duplicates[grpname] = [grp, groups[grpname]]
                else:
                    groups[grpname] = grp
            else:  # group has no options
                groups[grpname] = grp
        for grpname, grps in duplicates.items():
            self.LintError("duplicate-group",
                           "Group %s is defined multiple times:\n%s" %
                           (grpname,
                            "\n".join(self.RenderXML(g) for g in grps)))

    def duplicate_entries(self, allentries, etype):
        """ Generic duplicate entry finder.

        :param allentries: A list of all entries to check for
                           duplicates.
        :type allentries: list of lxml.etree._Element
        :param etype: The entry type. This will be used to determine
                      the error name (``duplicate-<etype>``) and for
                      display to the end user.
        :type etype: string
        """
        entries = dict()
        for el in allentries:
            if el.get("name") in entries:
                entries[el.get("name")].append(self.RenderXML(el))
            else:
                entries[el.get("name")] = [self.RenderXML(el)]
        for ename, els in entries.items():
            if len(els) > 1:
                self.LintError("duplicate-%s" % etype,
                               "%s %s is defined multiple times:\n%s" %
                               (etype.title(), ename, "\n".join(els)))

    def default_is_profile(self):
        """ Ensure that the default group is a profile group. """
        if (self.metadata.default and
            not self.metadata.groups[self.metadata.default].is_profile):
            xdata = \
                self.metadata.groups_xml.xdata.xpath("//Group[@name='%s']" %
                                                     self.metadata.default)[0]
            self.LintError("default-is-not-profile",
                           "Default group is not a profile group:\n%s" %
                           self.RenderXML(xdata))

########NEW FILE########
__FILENAME__ = NagiosGen
'''This module implements a Nagios configuration generator'''

import os
import re
import sys
import glob
import socket
import Bcfg2.Server
import Bcfg2.Server.Plugin


class NagiosGen(Bcfg2.Server.Plugin.Plugin,
                Bcfg2.Server.Plugin.Generator):
    """ NagiosGen is a Bcfg2 plugin that dynamically generates Nagios
    configuration file based on Bcfg2 data. """
    __author__ = 'bcfg-dev@mcs.anl.gov'
    line_fmt = '\t%-32s %s'

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.Generator.__init__(self)
        self.config = \
            Bcfg2.Server.Plugin.StructFile(os.path.join(self.data,
                                                        'config.xml'),
                                           core.fam, should_monitor=True,
                                           create=self.name)
        self.Entries = {
            'Path': {'/etc/nagiosgen.status': self.createhostconfig,
                     '/etc/nagios/nagiosgen.cfg': self.createserverconfig}}

        self.client_attrib = {'encoding': 'ascii',
                              'owner': 'root',
                              'group': 'root',
                              'type': 'file',
                              'mode': '0400'}
        self.server_attrib = {'encoding': 'ascii',
                              'owner': 'nagios',
                              'group': 'nagios',
                              'type': 'file',
                              'mode': '0440'}

    def createhostconfig(self, entry, metadata):
        """Build host specific configuration file."""
        try:
            host_address = socket.gethostbyname(metadata.hostname)
        except socket.gaierror:
            self.logger.error("Failed to find IP address for %s" %
                              metadata.hostname)
            raise Bcfg2.Server.Plugin.PluginExecutionError
        host_groups = [grp for grp in metadata.groups
                       if os.path.isfile('%s/%s-group.cfg' % (self.data, grp))]
        host_config = ['define host {',
                       self.line_fmt % ('host_name', metadata.hostname),
                       self.line_fmt % ('alias', metadata.hostname),
                       self.line_fmt % ('address', host_address)]

        if host_groups:
            host_config.append(self.line_fmt % ("hostgroups",
                                                ",".join(host_groups)))

        # read the config
        xtra = dict()
        for el in self.config.Match(metadata):
            if el.tag == 'Option':
                xtra[el.get("name")] = el.text

        if xtra:
            host_config.extend([self.line_fmt % (opt, val)
                                for opt, val in list(xtra.items())])
        if 'use' not in xtra:
            host_config.append(self.line_fmt % ('use', 'default'))

        host_config.append('}')
        entry.text = "%s\n" % "\n".join(host_config)
        for (key, value) in list(self.client_attrib.items()):
            entry.attrib.__setitem__(key, value)
        fname = os.path.join(self.data, metadata.hostname + "-host.cfg")
        try:
            open(fname, 'w').write(entry.text)
        except OSError:
            err = sys.exc_info()[1]
            self.logger.error("Failed to write %s: %s" % (fname, err))

    def createserverconfig(self, entry, _):
        """Build monolithic server configuration file."""
        host_configs = glob.glob(os.path.join(self.data, '*-host.cfg'))
        group_configs = glob.glob(os.path.join(self.data, '*-group.cfg'))
        host_data = []
        group_data = []
        for host in host_configs:
            host_data.append(open(host, 'r').read())

        group_list = []
        for line in "\n".join(host_data).splitlines():
            # only include those groups which are actually used
            if "hostgroup" in line:
                group_list += line.split()[1].split(',')

        group_list = list(set(group_list))

        for group in group_configs:
            group_name = re.sub("(-group.cfg|.*/(?=[^/]+))", "", group)
            if group_name in group_list:
                groupfile = open(group, 'r')
                group_data.append(groupfile.read())
                groupfile.close()

        entry.text = "%s\n\n%s" % ("\n".join(group_data), "\n".join(host_data))
        for (key, value) in list(self.server_attrib.items()):
            entry.attrib.__setitem__(key, value)
        fname = os.path.join(self.data, "nagiosgen.cfg")
        try:
            open(fname, 'w').write(entry.text)
        except OSError:
            err = sys.exc_info()[1]
            self.logger.error("Failed to write %s: %s" % (fname, err))

########NEW FILE########
__FILENAME__ = Ohai
"""The Ohai plugin is used to detect information about the client
operating system using ohai
(http://wiki.opscode.com/display/chef/Ohai) """

import os
import sys
import glob
import lxml.etree
import Bcfg2.Server.Plugin

try:
    import json
    # py2.4 json library is structured differently
    json.loads  # pylint: disable=W0104
except (ImportError, AttributeError):
    import simplejson as json

PROBECODE = """#!/bin/sh

export PATH=$PATH:/sbin:/usr/sbin

if type ohai >& /dev/null; then
    ohai
else
    # an empty dict, so "'foo' in metadata.Ohai" tests succeed
    echo '{}'
fi
"""


class OhaiCache(object):
    """ Storage for Ohai output on the local filesystem so that the
    output can be used by bcfg2-info, etc. """
    def __init__(self, dirname):
        self.dirname = dirname
        self.cache = dict()

    def hostpath(self, host):
        """ Get the path to the file that contains Ohai data for the
        given host """
        return os.path.join(self.dirname, "%s.json" % host)

    def __setitem__(self, item, value):
        if value is None:
            # simply return if the client returned nothing
            return
        self.cache[item] = json.loads(value)
        open(self.hostpath(item), 'w').write(value)

    def __getitem__(self, item):
        if item not in self.cache:
            try:
                data = open(self.hostpath(item)).read()
            except:
                raise KeyError(item)
            self.cache[item] = json.loads(data)
        return self.cache[item]

    def __delitem__(self, item):
        if item in self.cache:
            del self.cache[item]
        try:
            os.unlink(self.hostpath(item))
        except:
            raise IndexError("Could not unlink %s: %s" % (self.hostpath(item),
                                                          sys.exc_info()[1]))

    def __len__(self):
        return len(glob.glob(self.hostpath('*')))

    def __iter__(self):
        data = list(self.cache.keys())
        data.extend([x[:-5] for x in os.listdir(self.dirname)])
        return data.__iter__()


class Ohai(Bcfg2.Server.Plugin.Plugin,
           Bcfg2.Server.Plugin.Probing,
           Bcfg2.Server.Plugin.Connector):
    """The Ohai plugin is used to detect information
    about the client operating system.
    """
    name = 'Ohai'
    experimental = True

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.Probing.__init__(self)
        Bcfg2.Server.Plugin.Connector.__init__(self)
        self.probe = lxml.etree.Element('probe', name='Ohai', source='Ohai',
                                        interpreter='/bin/sh')
        self.probe.text = PROBECODE
        self.cache = OhaiCache(self.data)

    def GetProbes(self, _):
        return [self.probe]

    def ReceiveData(self, meta, datalist):
        self.cache[meta.hostname] = datalist[0].text

    def get_additional_data(self, meta):
        if meta.hostname in self.cache:
            return self.cache[meta.hostname]
        return dict()

########NEW FILE########
__FILENAME__ = Apt
""" APT backend for :mod:`Bcfg2.Server.Plugins.Packages` """

import re
import gzip
from Bcfg2.Server.Plugins.Packages.Collection import Collection
from Bcfg2.Server.Plugins.Packages.Source import Source


class AptCollection(Collection):
    """ Handle collections of APT sources.  This is a no-op object
    that simply inherits from
    :class:`Bcfg2.Server.Plugins.Packages.Collection.Collection`,
    overrides nothing, and defers all operations to :class:`PacSource`
    """

    def __init__(self, metadata, sources, cachepath, basepath, fam,
                 debug=False):
        # we define an __init__ that just calls the parent __init__,
        # so that we can set the docstring on __init__ to something
        # different from the parent __init__ -- namely, the parent
        # __init__ docstring, minus everything after ``.. -----``,
        # which we use to delineate the actual docs from the
        # .. autoattribute hacks we have to do to get private
        # attributes included in sphinx 1.0 """
        Collection.__init__(self, metadata, sources, cachepath, basepath, fam,
                            debug=debug)
    __init__.__doc__ = Collection.__init__.__doc__.split(".. -----")[0]

    def get_config(self):
        """ Get an APT configuration file (i.e., ``sources.list``).

        :returns: string """
        lines = ["# This config was generated automatically by the Bcfg2 "
                 "Packages plugin", '']

        for source in self:
            if source.rawurl:
                self.logger.info("Packages: Skipping rawurl %s" %
                                 source.rawurl)
            else:
                lines.append("deb %s %s %s" % (source.url, source.version,
                                               " ".join(source.components)))
                if source.debsrc:
                    lines.append("deb-src %s %s %s" %
                                 (source.url,
                                  source.version,
                                  " ".join(source.components)))
                lines.append("")

        return "\n".join(lines)


class AptSource(Source):
    """ Handle APT sources """

    #: :ref:`server-plugins-generators-packages-magic-groups` for
    #: ``AptSource`` are "apt", "debian", "ubuntu", and "nexenta"
    basegroups = ['apt', 'debian', 'ubuntu', 'nexenta']

    #: AptSource sets the ``type`` on Package entries to "deb"
    ptype = 'deb'

    @property
    def urls(self):
        """ A list of URLs to the base metadata file for each
        repository described by this source. """
        if not self.rawurl:
            rv = []
            for part in self.components:
                for arch in self.arches:
                    rv.append("%sdists/%s/%s/binary-%s/Packages.gz" %
                              (self.url, self.version, part, arch))
            return rv
        else:
            return ["%sPackages.gz" % self.rawurl]

    def read_files(self):
        bdeps = dict()
        bprov = dict()
        self.essentialpkgs = set()
        depfnames = ['Depends', 'Pre-Depends']
        if self.recommended:
            depfnames.append('Recommends')
        for fname in self.files:
            if not self.rawurl:
                barch = [x
                         for x in fname.split('@')
                         if x.startswith('binary-')][0][7:]
            else:
                # RawURL entries assume that they only have one <Arch></Arch>
                # element and that it is the architecture of the source.
                barch = self.arches[0]
            if barch not in bdeps:
                bdeps[barch] = dict()
                bprov[barch] = dict()
            try:
                reader = gzip.GzipFile(fname)
            except:
                self.logger.error("Packages: Failed to read file %s" % fname)
                raise
            for line in reader.readlines():
                if not isinstance(line, str):
                    line = line.decode('utf-8')
                words = str(line.strip()).split(':', 1)
                if words[0] == 'Package':
                    pkgname = words[1].strip().rstrip()
                    self.pkgnames.add(pkgname)
                    bdeps[barch][pkgname] = []
                elif words[0] == 'Essential' and self.essential:
                    self.essentialpkgs.add(pkgname)
                elif words[0] in depfnames:
                    vindex = 0
                    for dep in words[1].split(','):
                        if '|' in dep:
                            cdeps = [re.sub(r'\s+', '',
                                            re.sub(r'\(.*\)', '', cdep))
                                     for cdep in dep.split('|')]
                            dyn_dname = "choice-%s-%s-%s" % (pkgname,
                                                             barch,
                                                             vindex)
                            vindex += 1
                            bdeps[barch][pkgname].append(dyn_dname)
                            bprov[barch][dyn_dname] = set(cdeps)
                        else:
                            raw_dep = re.sub(r'\(.*\)', '', dep)
                            raw_dep = raw_dep.rstrip().strip()
                            bdeps[barch][pkgname].append(raw_dep)
                elif words[0] == 'Provides':
                    for pkg in words[1].split(','):
                        dname = pkg.rstrip().strip()
                        if dname not in bprov[barch]:
                            bprov[barch][dname] = set()
                        bprov[barch][dname].add(pkgname)
        self.process_files(bdeps, bprov)
    read_files.__doc__ = Source.read_files.__doc__

########NEW FILE########
__FILENAME__ = Collection
""" ``Collection`` objects represent the set of
:class:`Bcfg2.Server.Plugins.Packages.Source.Source` objects that
apply to a given client, and can be used to query all software
repositories for a client in aggregate.  In some cases this can give
faster or more accurate results.

In most cases, ``Collection`` methods have been designed to defer the
call to the Sources in the ``Collection`` and aggregate the results as
appropriate.  The simplest ``Collection`` implemention is thus often a
simple subclass that adds no additional functionality.

Overriding Methods
------------------

As noted above, the ``Collection`` object is written expressly so that
you can subclass it and override no methods or attributes, and it will
work by deferring all calls to the Source objects it contains.  There
are thus three approaches to writing a ``Collection`` subclass:

#. Keep the superclass almost entirely intact and defer to the
   ``Source`` objects inside it. For an example of this kind of
   ``Collection`` object, see
   :mod:`Bcfg2.Server.Plugins.Packages.Apt`.

#. Keep :func:`Collection.complete` intact, and override the methods
   it calls: :func:`Collection.is_package`,
   :func:`Collection.is_virtual_package`, :func:`Collection.get_deps`,
   :func:`Collection.get_provides`, :func:`Collection.get_vpkgs`, and
   :func:`Collection.setup_data`.  There are no examples of this kind
   of ``Collection`` subclass yet.

#. Provide your own implementation of :func:`Collection.complete`, in
   which case you do not have to override the above methods.  You may
   want to override :func:`Collection.packages_from_entry`,
   :func:`Collection.packages_to_entry`, and
   :func:`Collection.get_new_packages`.  For an example of this kind
   of ``Collection`` object, see
   :mod:`Bcfg2.Server.Plugins.Packages.yum`.

In either case, you may want to override
:func:`Collection.get_groups`, :func:`Collection.get_group`,
:func:`Collection.get_essential`, :func:`Collection.get_config`,
:func:`Collection.filter_unknown`, and
:func:`Collection.build_extra_structures`.

.. _pkg-objects:

Conversion Between Package Objects and XML Entries
--------------------------------------------------

Collection objects have to translate Bcfg2 entries,
:class:`lxml.etree._Element` objects, into objects suitable for use by
the backend for resolving dependencies.  This is handled by two
functions:

* :func:`Collection.packages_from_entry` is called to translate an XML
  entry into a list of packages;

* :func:`Collection.packages_to_entry` is called to translate a list
  of packages back into an XML entry.

Because of this translation layer, the return type of any functions
below that return packages (e.g., :func:`Collection.get_group`) is
actually indeterminate; they must return an object suitable for
passing to :func:`Collection.packages_to_entry`.  Similarly, functions
that take a package as an argument (e.g.,
:func:`Collection.is_package`) take the appropriate package object.
In the documentation below, the actual parameter return type (usually
.``string``) used in this base implementation is noted, as well as
this fact.

The Collection Module
---------------------
"""

import sys
import copy
import logging
import lxml.etree
import Bcfg2.Server.Plugin
from Bcfg2.Compat import any, md5  # pylint: disable=W0622

LOGGER = logging.getLogger(__name__)


class Collection(list, Bcfg2.Server.Plugin.Debuggable):
    """ ``Collection`` objects represent the set of
    :class:`Bcfg2.Server.Plugins.Packages.Source` objects that apply
    to a given client, and can be used to query all software
    repositories for a client in aggregate.  In some cases this can
    give faster or more accurate results. """

    #: Whether or not this Packages backend supports package groups
    __package_groups__ = False

    def __init__(self, metadata, sources, cachepath, basepath, fam,
                 debug=False):
        """
        :param metadata: The client metadata for this collection
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :param sources: A list of all sources known to the server that
                        will be used to generate the list of sources
                        that apply to this client
        :type sources: list of
                       :class:`Bcfg2.Server.Plugins.Packages.Source.Source`
                       objects
        :param cachepath: The filesystem path where cache and other temporary
                          data will be stored
        :type cachepath: string
        :param basepath: The filesystem path to the Packages plugin
                         directory, where more permanent data can be
                         stored
        :type basepath: string
        :param fam: A file monitor object to use if this Collection
                    needs to monitor for file activity
        :type fam: Bcfg2.Server.FileMonitor.FileMonitor
        :param debug: Enable debugging output
        :type debug: bool

        .. -----
        .. autoattribute:: __package_groups__
        """
        Bcfg2.Server.Plugin.Debuggable.__init__(self)
        list.__init__(self, sources)
        self.debug_flag = debug
        self.metadata = metadata
        self.basepath = basepath
        self.cachepath = cachepath
        self.virt_pkgs = dict()
        self.fam = fam

        try:
            self.setup = sources[0].setup
            self.ptype = sources[0].ptype
        except IndexError:
            self.setup = None
            self.ptype = "unknown"

    @property
    def cachekey(self):
        """ A unique identifier for the set of sources contained in
        this ``Collection`` object.  This is unique to a set of
        sources, **not** necessarily to the client, which lets clients
        with identical sources share cache data."""
        return md5(self.sourcelist().encode('UTF-8')).hexdigest()

    def get_config(self):
        """ Get the configuration for the package tool used by this
        source type.  This should be a config appropriate for use on
        either the server (to resolve dependencies) or the client.

        Subclasses must override this method in order to be able to
        generate configs.  By default it logs an error and returns the
        empty string.

        :returns: string """
        self.logger.error("Packages: Cannot generate config for host %s with "
                          "no sources or multiple source types" %
                          self.metadata.hostname)
        return ""

    def sourcelist(self):
        """ Get a human-readable list of sources in this collection,
        including some information about each source.

        :returns: string """
        srcs = []
        for source in self:
            for url_map in source.url_map:
                if url_map['arch'] not in self.metadata.groups:
                    continue
                reponame = source.get_repo_name(url_map)
                srcs.append("Name: %s" % reponame)
                srcs.append("  Type: %s" % source.ptype)
                if url_map['url']:
                    srcs.append("  URL: %s" % url_map['url'])
                elif url_map['rawurl']:
                    srcs.append("  RAWURL: %s" % url_map['rawurl'])
                if source.gpgkeys:
                    srcs.append("  GPG Key(s): %s" % ", ".join(source.gpgkeys))
                else:
                    srcs.append("  GPG Key(s): None")
                if len(source.blacklist):
                    srcs.append("  Blacklist: %s" %
                                ", ".join(source.blacklist))
                if len(source.whitelist):
                    srcs.append("  Whitelist: %s" %
                                ", ".join(source.whitelist))
                srcs.append("")
        return "\n".join(srcs)

    def get_relevant_groups(self):
        """ Get all groups that might be relevant to determining which
        sources apply to this collection's client.

        The base implementation simply aggregates the results of
        :func:`Bcfg2.Server.Plugins.Packages.Source.Source.get_relevant_groups`

        :return: list of strings - group names
        """
        groups = []
        for source in self:
            groups.extend(source.get_relevant_groups(self.metadata))
        return sorted(list(set(groups)))

    @property
    def basegroups(self):
        """ Get a list of group names used by this Collection type in
        resolution of
        :ref:`server-plugins-generators-packages-magic-groups`.

        The base implementation simply aggregates the results of
        :attr:`Bcfg2.Server.Plugins.Packages.Source.Source.basegroups`."""
        groups = set()
        for source in self:
            groups.update(source.basegroups)
        return list(groups)

    @property
    def cachefiles(self):
        """ A list of the full path to all cachefiles used by this
        collection.

        The base implementation simply aggregates
        :attr:`Bcfg2.Server.Plugins.Packages.Source.Source.cachefile`
        attributes."""
        cachefiles = set()
        for source in self:
            cachefiles.add(source.cachefile)
        return list(cachefiles)

    @Bcfg2.Server.Plugin.track_statistics()
    def get_groups(self, grouplist):
        """ Given a list of package group names, return a dict of
        ``<group name>: <list of packages>``.  This method is provided
        since some backends may be able to query multiple groups at
        once faster than serially.

        The base implementation simply aggregates the results of
        :func:`Bcfg2.Server.Plugins.Packages.Source.Source.get_group`.

        :param grouplist: The list of groups to query
        :type grouplist: list of strings - group names
        :returns: dict of ``<group name>: <list of packages>``

        In this implementation the packages will be strings, but see
        :ref:`pkg-objects`."""
        rv = dict()
        for group, ptype in grouplist:
            rv[group] = self.get_group(group, ptype)
        return rv

    @Bcfg2.Server.Plugin.track_statistics()
    def get_group(self, group, ptype=None):
        """ Get the list of packages of the given type in a package
        group.

        The base implementation simply aggregates the results of
        :func:`Bcfg2.Server.Plugins.Packages.Source.Source.get_group`.

        :param group: The name of the group to query
        :type group: string
        :param ptype: The type of packages to get, for backends that
                      support multiple package types in package groups
                      (e.g., "recommended," "optional," etc.)
        :type ptype: string
        :returns: list of strings - package names, but see
                  :ref:`pkg-objects`
        """
        if not self.__package_groups__:
            self.logger.error("Packages: Package groups are not supported by "
                              "%s" % self.__class__.__name__)
            return []

        for source in self:
            pkgs = source.get_group(self.metadata, group, ptype=ptype)
            if pkgs:
                return pkgs
        self.logger.warning("Packages: '%s' is not a valid group" % group)
        return []

    def is_package(self, package):
        """ Return True if a package is a package, False otherwise.

        The base implementation returns True if any Source object's
        :func:`Bcfg2.Server.Plugins.Packages.Source.Source.is_package`
        returns True.

        :param package: The name of the package, but see :ref:`pkg-objects`
        :type package: string
        :returns: bool
        """
        return any(source.is_package(self.metadata, package)
                   for source in self)

    def is_virtual_package(self, package):
        """ Return True if a name is a virtual package (i.e., is a
        symbol provided by a real package), False otherwise.

        The base implementation returns True if any Source object's
        :func:`Bcfg2.Server.Plugins.Packages.Source.Source.is_virtual_package`
        returns True.

        :param package: The name of the symbol, but see :ref:`pkg-objects`
        :type package: string
        :returns: bool
        """
        return any(source.is_virtual_package(self.metadata, package)
                   for source in self)

    def get_deps(self, package):
        """ Get a list of the dependencies of the given package.

        The base implementation simply aggregates the results of
        :func:`Bcfg2.Server.Plugins.Packages.Source.Source.get_deps`.

        :param package: The name of the symbol, but see :ref:`pkg-objects`
        :type package: string
        :returns: list of strings, but see :ref:`pkg-objects`
        """
        for source in self:
            if source.is_package(self.metadata, package):
                return source.get_deps(self.metadata, package)
        return []

    def get_essential(self):
        """ Get a list of packages that are essential to the repository.

        The base implementation simply aggregates
        :attr:`Bcfg2.Server.Plugins.Packages.Source.Source.essentialpkgs`
        attributes

        :returns: list of strings, but see :ref:`pkg-objects`
        """
        essential = set()
        for source in self:
            essential |= source.essentialpkgs
        return essential

    def get_provides(self, package):
        """ Get a list of all symbols provided by the given package.

        The base implementation simply aggregates the results of
        :func:`Bcfg2.Server.Plugins.Packages.Source.Source.get_provides`.

        :param package: The name of the package, but see :ref:`pkg-objects`
        :type package: string
        :returns: list of strings, but see :ref:`pkg-objects`
        """
        for source in self:
            providers = source.get_provides(self.metadata, package)
            if providers:
                return providers
        return []

    def get_vpkgs(self):
        """ Get a list of all virtual packages provided by all sources.

        The base implementation simply aggregates the results of
        :func:`Bcfg2.Server.Plugins.Packages.Source.Source.get_vpkgs`.

        :returns: list of strings, but see :ref:`pkg-objects`
        """
        vpkgs = dict()
        for source in self:
            s_vpkgs = source.get_vpkgs(self.metadata)
            for name, prov_set in list(s_vpkgs.items()):
                if name not in vpkgs:
                    vpkgs[name] = set(prov_set)
                else:
                    vpkgs[name].update(prov_set)
        return vpkgs

    def filter_unknown(self, unknown):
        """ After :func:`complete`, filter out packages that appear in
        the list of unknown packages but should not be presented to
        the user.  E.g., packages that you expect to be unknown.

        The base implementation filters out packages that are expected
        to be unknown by any source in this collection.

        :param unknown: A set of unknown packages.  The set should be
                        modified in place.
        :type unknown: set of strings, but see :ref:`pkg-objects`
        """
        for source in self:
            source.filter_unknown(unknown)

    def magic_groups_match(self):
        """ Returns True if the client's
        :ref:`server-plugins-generators-packages-magic-groups` match
        the magic groups for any of the sources contained in this
        Collection.

        The base implementation returns True if any source
        :func:`Bcfg2.Server.Plugins.Packages.Source.Source.magic_groups_match`
        returns True.

        :returns: bool
        """
        return any(s.magic_groups_match(self.metadata) for s in self)

    def build_extra_structures(self, independent):
        """ Add additional entries to the ``<Independent/>`` section
        of the final configuration.  This can be used to handle, e.g.,
        GPG keys and other entries besides packages that need to be
        handled for a complete client configuration.

        :param independent: The XML tag to add extra entries to.  This
                            is modified in place.
        :type independent: lxml.etree._Element
        """
        pass

    def get_additional_data(self):
        """ Get additional
        :class:`Bcfg2.Server.Plugin.interfaces.Connector` data to be
        supplied to
        :func:`Bcfg2.Server.Plugins.Packages.Packages.get_additional_data`
        (and thence to client metadata objects).

        The base implementation simply aggregates
        :attr:`Bcfg2.Server.Plugins.Packages.Source.Source.url_map`
        attributes.

        :returns: list of additional Connector data
        """
        sdata = []
        for source in self:
            sdata.extend(copy.deepcopy(source.url_map))
        return sdata

    def setup_data(self, force_update=False):
        """ Do any collection-level data setup tasks. This is called
        when sources are loaded or reloaded by
        :class:`Bcfg2.Server.Plugins.Packages.Packages`.

        The base implementation is a no-op; the child
        :class:`Bcfg2.Server.Plugins.Packages.Source.Source` objects
        will handle all data setup.

        :param force_update: Ignore all local cache and setup data
                             from its original upstream sources (i.e.,
                             the package repositories)
        :type force_update: bool
        """
        pass

    def packages_from_entry(self, entry):
        """ Given a Package or BoundPackage entry, get a list of the
        package(s) described by it in a format appropriate for passing
        to :func:`complete`.  By default, that's just the name; only
        the :mod:`Bcfg2.Server.Plugins.Packages.Yum` backend supports
        versions or other extended data. See :ref:`pkg-objects` for
        more details.

        :param entry: The XML entry describing the package or packages.
        :type entry: lxml.etree._Element
        :returns: list of strings, but see :ref:`pkg-objects`
        """
        return [entry.get("name")]

    def packages_to_entry(self, pkglist, entry):
        """ Given a list of package objects as returned by
        :func:`packages_from_entry` or :func:`complete`, return an XML
        tree describing the BoundPackage entries that should be
        included in the client configuration. See :ref:`pkg-objects`
        for more details.

        :param pkglist: A list of packages as returned by
                        :func:`complete`
        :type pkglist: list of strings, but see :ref:`pkg-objects`
        :param entry: The base XML entry to add all of the Package
                      entries to.  This should be modified in place.
        :type entry: lxml.etree._Element
        """
        for pkg in pkglist:
            lxml.etree.SubElement(entry, 'BoundPackage', name=pkg,
                                  version=self.setup.cfp.get("packages",
                                                             "version",
                                                             default="auto"),
                                  type=self.ptype, origin='Packages')

    def get_new_packages(self, initial, complete):
        """ Compute the difference between the complete package list
        (as returned by :func:`complete`) and the initial package list
        computed from the specification.  This is necessary because
        the format may be different between the two lists due to
        :func:`packages_to_entry` and :func:`packages_from_entry`. See
        :ref:`pkg-objects` for more details.

        :param initial: The initial package list
        :type initial: set of strings, but see :ref:`pkg-objects`
        :param complete: The final package list
        :type complete: set of strings, but see :ref:`pkg-objects`
        :return: set of strings, but see :ref:`pkg-objects` - the set
                 of packages that are in ``complete`` but not in
                 ``initial``
        """
        return list(complete.difference(initial))

    @Bcfg2.Server.Plugin.track_statistics()
    def complete(self, packagelist):  # pylint: disable=R0912,R0914
        """ Build a complete list of all packages and their dependencies.

        :param packagelist: Set of initial packages computed from the
                            specification.
        :type packagelist: set of strings, but see :ref:`pkg-objects`
        :returns: tuple of sets - The first element contains a set of
                  strings (but see :ref:`pkg-objects`) describing the
                  complete package list, and the second element is a
                  set of symbols whose dependencies could not be
                  resolved.
        """
        # setup vpkg cache
        pgrps = tuple(self.get_relevant_groups())
        if pgrps not in self.virt_pkgs:
            self.virt_pkgs[pgrps] = self.get_vpkgs()
        vpkg_cache = self.virt_pkgs[pgrps]

        # unclassified is set of unsatisfied requirements (may be pkg
        # for vpkg)
        unclassified = set(packagelist)
        vpkgs = set()
        both = set()
        pkgs = set(packagelist)

        packages = set()
        examined = set()
        unknown = set()

        final_pass = False
        really_done = False
        # do while unclassified or vpkgs or both or pkgs
        while unclassified or pkgs or both or final_pass:
            if really_done:
                break
            if len(unclassified) + len(pkgs) + len(both) == 0:
                # one more pass then exit
                really_done = True

            while unclassified:
                current = unclassified.pop()
                examined.add(current)
                is_pkg = False
                if self.is_package(current):
                    is_pkg = True

                is_vpkg = current in vpkg_cache

                if is_pkg and is_vpkg:
                    both.add(current)
                elif is_pkg and not is_vpkg:
                    pkgs.add(current)
                elif is_vpkg and not is_pkg:
                    vpkgs.add(current)
                elif not is_vpkg and not is_pkg:
                    unknown.add(current)

            while pkgs:
                # direct packages; current can be added, and all deps
                # should be resolved
                current = pkgs.pop()
                self.debug_log("Packages: handling package requirement %s" %
                               (current,))
                packages.add(current)
                deps = self.get_deps(current)
                newdeps = set(deps).difference(examined)
                if newdeps:
                    self.debug_log("Packages: Package %s added requirements %s"
                                   % (current, newdeps))
                unclassified.update(newdeps)

            satisfied_vpkgs = set()
            for current in vpkgs:
                # virtual dependencies, satisfied if one of N in the
                # config, or can be forced if only one provider
                if len(vpkg_cache[current]) == 1:
                    self.debug_log("Packages: requirement %s satisfied by %s" %
                                   (current, vpkg_cache[current]))
                    unclassified.update(
                        vpkg_cache[current].difference(examined))
                    satisfied_vpkgs.add(current)
                else:
                    satisfiers = [item for item in vpkg_cache[current]
                                  if item in packages]
                    self.debug_log("Packages: requirement %s satisfied by %s" %
                                   (current, satisfiers))
                    satisfied_vpkgs.add(current)
            vpkgs.difference_update(satisfied_vpkgs)

            satisfied_both = set()
            for current in both:
                # packages that are both have virtual providers as
                # well as a package with that name. allow use of virt
                # through explicit specification, then fall back to
                # forcing current on last pass
                satisfiers = [item for item in vpkg_cache[current]
                              if item in packages]
                if satisfiers:
                    self.debug_log("Packages: requirement %s satisfied by %s" %
                                   (current, satisfiers))
                    satisfied_both.add(current)
                elif current in packagelist or final_pass:
                    pkgs.add(current)
                    satisfied_both.add(current)
            both.difference_update(satisfied_both)

            if len(unclassified) + len(pkgs) == 0:
                final_pass = True
            else:
                final_pass = False

            self.filter_unknown(unknown)
        return packages, unknown

    def __repr__(self):
        return "%s(%s)" % (self.__class__.__name__,
                           list.__repr__(self))


def get_collection_class(source_type):
    """ Given a source type, determine the class of Collection object
    that should be used to contain these sources.  Note that
    ``source_type`` is *not* a
    :class:`Bcfg2.Server.Plugins.Packages.Source.Source` subclass;
    it's the name of a source type as given in ``sources.xml``.

    :param source_type: The type of source, e.g., "yum" or "apt"
    :type source_type: string
    :returns: type - the Collection subclass that should be used to
              instantiate an object to contain sources of the given type. """
    modname = "Bcfg2.Server.Plugins.Packages.%s" % source_type.title()
    try:
        module = sys.modules[modname]
    except KeyError:
        try:
            module = __import__(modname).Server.Plugins.Packages
        except ImportError:
            msg = "Packages: Unknown source type %s" % source_type
            LOGGER.error(msg)
            raise Bcfg2.Server.Plugin.PluginExecutionError(msg)

    try:
        cclass = getattr(module, source_type.title() + "Collection")
    except AttributeError:
        msg = "Packages: No collection class found for %s sources" % \
            source_type
        LOGGER.error(msg)
        raise Bcfg2.Server.Plugin.PluginExecutionError(msg)
    return cclass

########NEW FILE########
__FILENAME__ = Pac
""" Pacman backend for :mod:`Bcfg2.Server.Plugins.Packages` """

import tarfile
from Bcfg2.Server.Plugins.Packages.Collection import Collection
from Bcfg2.Server.Plugins.Packages.Source import Source


class PacCollection(Collection):
    """ Handle collections of Pacman sources.  This is a no-op object
    that simply inherits from
    :class:`Bcfg2.Server.Plugins.Packages.Collection.Collection`,
    overrides nothing, and defers all operations to :class:`PacSource`
    """

    def __init__(self, metadata, sources, cachepath, basepath, fam,
                 debug=False):
        # we define an __init__ that just calls the parent __init__,
        # so that we can set the docstring on __init__ to something
        # different from the parent __init__ -- namely, the parent
        # __init__ docstring, minus everything after ``.. -----``,
        # which we use to delineate the actual docs from the
        # .. autoattribute hacks we have to do to get private
        # attributes included in sphinx 1.0 """
        Collection.__init__(self, metadata, sources, cachepath, basepath, fam,
                            debug=debug)
    __init__.__doc__ = Collection.__init__.__doc__.split(".. -----")[0]


class PacSource(Source):
    """ Handle Pacman sources """

    #: :ref:`server-plugins-generators-packages-magic-groups` for
    #: ``PacSource`` are "arch" and "parabola"
    basegroups = ['arch', 'parabola']

    #: PacSource sets the ``type`` on Package entries to "pacman"
    ptype = 'pacman'

    @property
    def urls(self):
        """ A list of URLs to the base metadata file for each
        repository described by this source. """
        if not self.rawurl:
            rv = []
            for part in self.components:
                for arch in self.arches:
                    rv.append("%s%s/os/%s/%s.db.tar.gz" %
                              (self.url, part, arch, part))
            return rv
        else:
            raise Exception("PacSource : RAWUrl not supported (yet)")

    def read_files(self):
        bdeps = dict()
        bprov = dict()

        depfnames = ['Depends', 'Pre-Depends']
        if self.recommended:
            depfnames.append('Recommends')

        for fname in self.files:
            if not self.rawurl:
                barch = [x for x in fname.split('@') if x in self.arches][0]
            else:
                # RawURL entries assume that they only have one <Arch></Arch>
                # element and that it is the architecture of the source.
                barch = self.arches[0]

            if barch not in bdeps:
                bdeps[barch] = dict()
                bprov[barch] = dict()
            try:
                self.debug_log("Packages: try to read %s" % fname)
                tar = tarfile.open(fname, "r")
            except:
                self.logger.error("Packages: Failed to read file %s" % fname)
                raise

            for tarinfo in tar:
                if tarinfo.isdir():
                    self.pkgnames.add(tarinfo.name.rsplit("-", 2)[0])
                    self.debug_log("Packages: added %s" %
                                   tarinfo.name.rsplit("-", 2)[0])
            tar.close()
        self.process_files(bdeps, bprov)
    read_files.__doc__ = Source.read_files.__doc__

########NEW FILE########
__FILENAME__ = PackagesSources
""" PackagesSources handles the
:ref:`server-plugins-generators-packages` ``sources.xml`` file"""

import os
import sys
import Bcfg2.Server.Plugin
from Bcfg2.Server.Plugins.Packages.Source import SourceInitError


# pylint: disable=E0012,R0924
class PackagesSources(Bcfg2.Server.Plugin.StructFile,
                      Bcfg2.Server.Plugin.Debuggable):
    """ PackagesSources handles parsing of the
    :mod:`Bcfg2.Server.Plugins.Packages` ``sources.xml`` file, and the
    creation of the appropriate
    :class:`Bcfg2.Server.Plugins.Packages.Source.Source` object for
    each ``Source`` tag. """

    __identifier__ = None
    create = "Sources"

    def __init__(self, filename, cachepath, fam, packages, setup):
        """
        :param filename: The full path to ``sources.xml``
        :type filename: string
        :param cachepath: The full path to the directory where
                          :class:`Bcfg2.Server.Plugins.Packages.Source.Source`
                          data will be cached
        :type cachepath: string
        :param fam: The file access monitor to use to create watches
                    on ``sources.xml`` and any XIncluded files.
        :type fam: Bcfg2.Server.FileMonitor.FileMonitor
        :param packages: The Packages plugin object ``sources.xml`` is
                         being parsed on behalf of (i.e., the calling
                         object)
        :type packages: Bcfg2.Server.Plugins.Packages.Packages
        :param setup: A Bcfg2 options dict
        :type setup: dict

        :raises: :class:`Bcfg2.Server.Plugin.exceptions.PluginInitError` -
                 If ``sources.xml`` cannot be read
        """
        Bcfg2.Server.Plugin.Debuggable.__init__(self)
        Bcfg2.Server.Plugin.StructFile.__init__(self, filename, fam=fam,
                                                should_monitor=True)

        #: The full path to the directory where
        #: :class:`Bcfg2.Server.Plugins.Packages.Source.Source` data
        #: will be cached
        self.cachepath = cachepath

        if not os.path.exists(self.cachepath):
            # create cache directory if needed
            try:
                os.makedirs(self.cachepath)
            except OSError:
                err = sys.exc_info()[1]
                self.logger.error("Could not create Packages cache at %s: %s" %
                                  (self.cachepath, err))
        #: The Bcfg2 options dict
        self.setup = setup

        #: The :class:`Bcfg2.Server.Plugins.Packages.Packages` that
        #: instantiated this ``PackagesSources`` object
        self.pkg_obj = packages

        #: The set of all XML files that have been successfully
        #: parsed.  This is used by :attr:`loaded` to determine if the
        #: sources have been fully parsed and the
        #: :class:`Bcfg2.Server.Plugins.Packages.Packages` plugin
        #: should be told to reload its data.
        self.parsed = set()

    def set_debug(self, debug):
        Bcfg2.Server.Plugin.Debuggable.set_debug(self, debug)
        for source in self.entries:
            source.set_debug(debug)
    set_debug.__doc__ = Bcfg2.Server.Plugin.Plugin.set_debug.__doc__

    def HandleEvent(self, event=None):
        """ HandleEvent is called whenever the FAM registers an event.

        When :attr:`loaded` becomes True,
        :func:`Bcfg2.Server.Plugins.Packages.Packages.Reload` is
        called to reload all plugin data from the configured sources.

        :param event: The event object
        :type event: Bcfg2.Server.FileMonitor.Event
        :returns: None
        """
        if event and event.filename != self.name:
            for fpath in self.extras:
                if fpath == os.path.abspath(event.filename):
                    self.parsed.add(fpath)
                    break
        Bcfg2.Server.Plugin.StructFile.HandleEvent(self, event=event)
        if self.loaded:
            self.logger.info("Reloading Packages plugin")
            self.pkg_obj.Reload()

    @property
    def loaded(self):
        """ Whether or not all XML files (``sources.xml`` and
        everything XIncluded in it) have been parsed. This flag is
        used to determine if the Packages plugin should be told to
        load its data. """
        return sorted(list(self.parsed)) == sorted(self.extras)

    @Bcfg2.Server.Plugin.track_statistics()
    def Index(self):
        Bcfg2.Server.Plugin.StructFile.Index(self)
        self.entries = []
        if self.loaded:
            for xsource in self.xdata.findall('.//Source'):
                source = self.source_from_xml(xsource)
                if source is not None:
                    self.entries.append(source)
    Index.__doc__ = Bcfg2.Server.Plugin.StructFile.Index.__doc__ + """

        ``Index`` is responsible for calling :func:`source_from_xml`
        for each ``Source`` tag in each file. """

    @Bcfg2.Server.Plugin.track_statistics()
    def source_from_xml(self, xsource):
        """ Create a
        :class:`Bcfg2.Server.Plugins.Packages.Source.Source` subclass
        object from XML representation of a source in ``sources.xml``.
        ``source_from_xml`` determines the appropriate subclass of
        ``Source`` to instantiate according to the ``type`` attribute
        of the ``Source`` tag.

        :param xsource: The XML tag representing the source
        :type xsource: lxml.etree._Element
        :returns: :class:`Bcfg2.Server.Plugins.Packages.Source.Source`
                  subclass, or None on error
        """
        stype = xsource.get("type")
        if stype is None:
            self.logger.error("Packages: No type specified for source at %s, "
                              "skipping" % (xsource.get("rawurl",
                                                        xsource.get("url"))))
            return None

        try:
            module = getattr(__import__("Bcfg2.Server.Plugins.Packages.%s" %
                                        stype.title()).Server.Plugins.Packages,
                             stype.title())
            cls = getattr(module, "%sSource" % stype.title())
        except (ImportError, AttributeError):
            err = sys.exc_info()[1]
            self.logger.error("Packages: Unknown source type %s (%s)" % (stype,
                                                                         err))
            return None

        try:
            source = cls(self.cachepath, xsource, self.setup)
        except SourceInitError:
            err = sys.exc_info()[1]
            self.logger.error("Packages: %s" % err)
            source = None

        return source

    def __getitem__(self, key):
        return self.entries[key]

    def __repr__(self):
        return "PackagesSources: %s" % repr(self.entries)

    def __str__(self):
        return "PackagesSources: %s sources" % len(self.entries)

    def __len__(self):
        return len(self.entries)

########NEW FILE########
__FILENAME__ = Source
""" ``Source`` objects represent a single <Source> tag in
``sources.xml``.  Note that a single Source tag can itself describe
multiple repositories (if it uses the "url" attribute instead of
"rawurl"), and so can the ``Source`` object.  This can be the source
(har har) of some confusion.  See
:func:`Bcfg2.Server.Plugins.Packages.Collection.Collection.sourcelist`
for the proper way to get all repos from a ``Source`` object.

Source objects are aggregated into
:class:`Bcfg2.Server.Plugins.Packages.Collection.Collection`
objects, which are actually called by
:class:`Bcfg2.Server.Plugins.Packages.Packages`.  This way a more
advanced subclass can query repositories in aggregate rather than
individually, which may give faster or more accurate results.

The base ``Source`` object must be subclassed to handle each
repository type.  How you subclass ``Source`` will depend on how you
subclassed
:class:`Bcfg2.Server.Plugins.Packages.Collection.Collection`; see
:mod:`Bcfg2.Server.Plugins.Packages.Collection` for more details on
different methods for doing that.

If you are using the stock (or a near-stock)
:class:`Bcfg2.Server.Plugins.Packages.Collection.Collection` object,
then you will need to implement the following methods and attributes
in your ``Source`` subclass:

* :func:`Source.urls`
* :func:`Source.read_files`
* :attr:`Source.basegroups`

Additionally, you may want to consider overriding the following
methods and attributes:

* :func:`Source.is_virtual_package`
* :func:`Source.get_group`
* :attr:`Source.unknown_filter`
* :attr:`Source.load_state`
* :attr:`Source.save_state`

For an example of this kind of ``Source`` object, see
:mod:`Bcfg2.Server.Plugins.Packages.Apt`.

If you are overriding the ``Collection`` object in more depth, then
you have more leeway in what you might want to override or implement
in your ``Source`` subclass.  For an example of this kind of
``Source`` object, see :mod:`Bcfg2.Server.Plugins.Packages.Yum`.
"""

import os
import re
import sys
import Bcfg2.Server.Plugin
from Bcfg2.Compat import HTTPError, HTTPBasicAuthHandler, \
    HTTPPasswordMgrWithDefaultRealm, install_opener, build_opener, urlopen, \
    cPickle, md5


def fetch_url(url):
    """ Return the content of the given URL.

    :param url: The URL to fetch content from.
    :type url: string
    :raises: ValueError - Malformed URL
    :raises: URLError - Failure fetching URL
    :returns: string - the content of the page at the given URL """
    if '@' in url:
        mobj = re.match(r'(\w+://)([^:]+):([^@]+)@(.*)$', url)
        if not mobj:
            raise ValueError("Invalid URL")
        user = mobj.group(2)
        passwd = mobj.group(3)
        url = mobj.group(1) + mobj.group(4)
        auth = HTTPBasicAuthHandler(HTTPPasswordMgrWithDefaultRealm())
        auth.add_password(None, url, user, passwd)
        install_opener(build_opener(auth))
    return urlopen(url).read()


class SourceInitError(Exception):
    """ Raised when a :class:`Source` object fails instantiation. """
    pass


#: A regular expression used to determine the base name of a repo from
#: its URL.  This is used when generating repo configs and by
#: :func:`Source.get_repo_name`.  It handles `Pulp
#: <http://www.pulpproject.org/>`_ and `mrepo
#: <http://dag.wieers.com/home-made/mrepo/>`_ repositories specially,
#: and otherwise grabs the last component of the URL (as delimited by
#: slashes).
REPO_RE = re.compile(r'(?:pulp/repos/|/RPMS\.|/)([^/]+)/?$')


class Source(Bcfg2.Server.Plugin.Debuggable):  # pylint: disable=R0902
    """ ``Source`` objects represent a single <Source> tag in
    ``sources.xml``.  Note that a single Source tag can itself
    describe multiple repositories (if it uses the "url" attribute
    instead of "rawurl"), and so can the ``Source`` object.

    Note that a number of the attributes of this object may be more or
    less specific to one backend (e.g., :attr:`essentialpkgs`,
    :attr:`recommended`, :attr:`gpgkeys`, but they are included in the
    superclass to make the parsing of sources from XML more
    consistent, and to make it trivial for other backends to support
    those features.
    """

    #: The list of
    #: :ref:`server-plugins-generators-packages-magic-groups` that
    #: make sources of this type available to clients.
    basegroups = []

    #: The Package type handled by this Source class.  The ``type``
    #: attribute of Package entries will be set to the value ``ptype``
    #: when they are handled by :mod:`Bcfg2.Server.Plugins.Packages`.
    ptype = None

    def __init__(self, basepath, xsource, setup):  # pylint: disable=R0912
        """
        :param basepath: The base filesystem path under which cache
                         data for this source should be stored
        :type basepath: string
        :param xsource: The XML tag that describes this source
        :type source: lxml.etree._Element
        :param setup: A Bcfg2 options dict
        :type setup: dict
        :raises: :class:`Bcfg2.Server.Plugins.Packages.Source.SourceInitError`
        """
        Bcfg2.Server.Plugin.Debuggable.__init__(self)

        #: The base filesystem path under which cache data for this
        #: source should be stored
        self.basepath = basepath

        #: The XML tag that describes this source
        self.xsource = xsource

        #: A Bcfg2 options dict
        self.setup = setup

        #: A set of package names that are deemed "essential" by this
        #: source
        self.essentialpkgs = set()

        #: A list of the text of all 'Component' attributes of this
        #: source from XML
        self.components = [item.text for item in xsource.findall('Component')]

        #: A list of the arches supported by this source
        self.arches = [item.text for item in xsource.findall('Arch')]

        #: A list of the the names of packages that are blacklisted
        #: from this source
        self.blacklist = [item.text for item in xsource.findall('Blacklist')]

        #: A list of the the names of packages that are whitelisted in
        #: this source
        self.whitelist = [item.text for item in xsource.findall('Whitelist')]

        #: Whether or not to include deb-src lines in the generated APT
        #: configuration
        self.debsrc = xsource.get('debsrc', 'false') == 'true'

        #: A dict of repository options that will be included in the
        #: configuration generated on the server side (if such is
        #: applicable; most backends do not generate any sort of
        #: repository configuration on the Bcfg2 server)
        self.server_options = dict()

        #: A dict of repository options that will be included in the
        #: configuration generated for the client (if that is
        #: supported by the backend)
        self.client_options = dict()
        opts = xsource.findall("Options")
        for el in opts:
            repoopts = dict([(k, v)
                             for k, v in el.attrib.items()
                             if k != "clientonly" and k != "serveronly"])
            if el.get("clientonly", "false").lower() == "false":
                self.server_options.update(repoopts)
            if el.get("serveronly", "false").lower() == "false":
                self.client_options.update(repoopts)

        #: A list of URLs to GPG keys that apply to this source
        self.gpgkeys = [el.text for el in xsource.findall("GPGKey")]

        #: Whether or not to include essential packages from this source
        self.essential = xsource.get('essential', 'true').lower() == 'true'

        #: Whether or not to include recommended packages from this source
        self.recommended = xsource.get('recommended',
                                       'false').lower() == 'true'

        #: The "rawurl" attribute from :attr:`xsource`, if applicable.
        #: A trailing slash is automatically appended to this if there
        #: wasn't one already present.
        self.rawurl = xsource.get('rawurl', '')
        if self.rawurl and not self.rawurl.endswith("/"):
            self.rawurl += "/"

        #: The "url" attribute from :attr:`xsource`, if applicable.  A
        #: trailing slash is automatically appended to this if there
        #: wasn't one already present.
        self.url = xsource.get('url', '')
        if self.url and not self.url.endswith("/"):
            self.url += "/"

        #: The "version" attribute from :attr:`xsource`
        self.version = xsource.get('version', '')

        #: A list of predicates that are used to determine if this
        #: source applies to a given
        #: :class:`Bcfg2.Server.Plugins.Metadata.ClientMetadata`
        #: object.
        self.conditions = []
        #: Formerly, :ref:`server-plugins-generators-packages` only
        #: supported applying package sources to groups; that is, they
        #: could not be assigned by more complicated logic like
        #: per-client repositories and group or client negation.  This
        #: attribute attempts to provide for some limited backwards
        #: compat with older code that relies on this.
        self.groups = []
        for el in xsource.iterancestors():
            if el.tag == "Group":
                if el.get("negate", "false").lower() == "true":
                    self.conditions.append(lambda m, el=el:
                                           el.get("name") not in m.groups)
                else:
                    self.groups.append(el.get("name"))
                    self.conditions.append(lambda m, el=el:
                                           el.get("name") in m.groups)
            elif el.tag == "Client":
                if el.get("negate", "false").lower() == "true":
                    self.conditions.append(lambda m, el=el:
                                           el.get("name") != m.hostname)
                else:
                    self.conditions.append(lambda m, el=el:
                                           el.get("name") == m.hostname)

        #: A set of all package names in this source.  This will not
        #: necessarily be populated, particularly by backends that
        #: reimplement large portions of
        #: :class:`Bcfg2.Server.Plugins.Packages.Collection.Collection`
        self.pkgnames = set()

        #: A dict of ``<package name>`` -> ``<list of dependencies>``.
        #: This will not necessarily be populated, particularly by
        #: backends that reimplement large portions of
        #: :class:`Bcfg2.Server.Plugins.Packages.Collection.Collection`
        self.deps = dict()

        #: A dict of ``<package name>`` -> ``<list of provided
        #: symbols>``.  This will not necessarily be populated,
        #: particularly by backends that reimplement large portions of
        #: :class:`Bcfg2.Server.Plugins.Packages.Collection.Collection`
        self.provides = dict()

        #: The file (or directory) used for this source's cache data
        self.cachefile = os.path.join(self.basepath,
                                      "cache-%s" % self.cachekey)
        if not self.rawurl:
            baseurl = self.url + "%(version)s/%(component)s/%(arch)s/"
        else:
            baseurl = self.rawurl

        #: A list of dicts, each of which describes the URL to one
        #: repository contained in this source.  Each dict contains
        #: the following keys:
        #:
        #: * ``version``: The version of the repo (``None`` for
        #:   ``rawurl`` repos)
        #: * ``component``: The component use to form this URL
        #:   (``None`` for ``rawurl`` repos)
        #: * ``arch``: The architecture of this repo
        #: * ``baseurl``: Either the ``rawurl`` attribute, or the
        #:   format string built from the ``url`` attribute
        #: * ``url``: The actual URL to the repository
        self.url_map = []
        for arch in self.arches:
            if self.url:
                usettings = [dict(version=self.version, component=comp,
                                  arch=arch)
                             for comp in self.components]
            else:  # rawurl given
                usettings = [dict(version=self.version, component=None,
                                  arch=arch)]

            for setting in usettings:
                if not self.rawurl:
                    setting['baseurl'] = self.url
                else:
                    setting['baseurl'] = self.rawurl
                setting['url'] = baseurl % setting
            self.url_map.extend(usettings)

    @property
    def cachekey(self):
        """ A unique key for this source that will be used to generate
        :attr:`cachefile` and other cache paths """
        return md5(cPickle.dumps([self.version, self.components, self.url,
                                  self.rawurl, self.arches])).hexdigest()

    def get_relevant_groups(self, metadata):
        """ Get all groups that might be relevant to determining which
        sources apply to this collection's client.

        :return: list of strings - group names
        """
        return sorted(list(set([g for g in metadata.groups
                                if (g in self.basegroups or
                                    g in self.groups or
                                    g in self.arches)])))

    def load_state(self):
        """ Load saved state from :attr:`cachefile`.  If caching and
        state is handled by the package library, then this function
        does not need to be implemented.

        :raises: OSError - If the saved data cannot be read
        :raises: cPickle.UnpicklingError - If the saved data is corrupt """
        data = open(self.cachefile, 'rb')
        (self.pkgnames, self.deps, self.provides,
         self.essentialpkgs) = cPickle.load(data)

    def save_state(self):
        """ Save state to :attr:`cachefile`.  If caching and
        state is handled by the package library, then this function
        does not need to be implemented. """
        cache = open(self.cachefile, 'wb')
        cPickle.dump((self.pkgnames, self.deps, self.provides,
                      self.essentialpkgs), cache, 2)
        cache.close()

    @Bcfg2.Server.Plugin.track_statistics()
    def setup_data(self, force_update=False):
        """ Perform all data fetching and setup tasks.  For most
        backends, this involves downloading all metadata from the
        repository, parsing it, and caching the parsed data locally.
        The order of operations is:

        #. Call :func:`load_state` to try to load data from the local
           cache.
        #. If that fails, call :func:`read_files` to read and parse
           the locally downloaded metadata files.
        #. If that fails, call :func:`update` to fetch the metadata,
           then :func:`read_files` to parse it.

        Obviously with a backend that leverages repo access libraries
        to avoid downloading all metadata, many of the functions
        called by ``setup_data`` can be no-ops (or nearly so).

        :param force_update: Ignore all locally cached and downloaded
                             data and fetch the metadata anew from the
                             upstream repository.
        :type force_update: bool
        """
        # pylint: disable=W0702
        if not force_update:
            if os.path.exists(self.cachefile):
                try:
                    self.load_state()
                except:
                    err = sys.exc_info()[1]
                    self.logger.error("Packages: Cachefile %s load failed: %s"
                                      % (self.cachefile, err))
                    self.logger.error("Falling back to file read")

                    try:
                        self.read_files()
                    except:
                        err = sys.exc_info()[1]
                        self.logger.error("Packages: File read failed: %s" %
                                          err)
                        self.logger.error("Falling back to file download")
                        force_update = True
            else:
                force_update = True

        if force_update:
            try:
                self.update()
                self.read_files()
            except:
                err = sys.exc_info()[1]
                self.logger.error("Packages: Failed to load data for %s: %s" %
                                  (self, err))
                self.logger.error("Some Packages will be missing")
        # pylint: enable=W0702

    def get_repo_name(self, url_map):
        """ Try to find a sensible name for a repository. Since
        ``sources.xml`` doesn't provide for repository names, we have
        to try to guess at the names when generating config files or
        doing other operations that require repository names.  This
        function tries several approaches:

        #. First, if the map contains a ``component`` key, use that as
           the name.
        #. If not, then try to match the repository URL against
           :attr:`Bcfg2.Server.Plugins.Packages.Source.REPO_RE`.  If
           that succeeds, use the first matched group; additionally,
           if the Source tag that describes this repo is contained in
           a ``<Group>`` tag, prepend that to the name.
        #. If :attr:`Bcfg2.Server.Plugins.Packages.Source.REPO_RE`
           does not match the repository, and the Source tag that
           describes this repo is contained in a ``<Group>`` tag, use
           the name of the group.
        #. Failing that, use the full URL to this repository, with the
           protocol and trailing slash stripped off if possible.

        Once that is done, all characters disallowed in yum source
        names are replaced by dashes.  See below for the exact regex.
        The yum rules are used here because they are so restrictive.

        ``get_repo_name`` is **not** guaranteed to return a unique
        name.  If you require a unique name, then you will need to
        generate all repo names and make them unique through the
        approach of your choice, e.g., appending numbers to non-unique
        repository names.  See
        :func:`Bcfg2.Server.Plugins.Packages.Yum.Source.get_repo_name`
        for an example.

        :param url_map: A single :attr:`url_map` dict, i.e., any
                        single element of :attr:`url_map`.
        :type url_map: dict
        :returns: string - the name of the repository.
        """
        if url_map['component']:
            rname = url_map['component']
        else:
            match = REPO_RE.search(url_map['url'])
            if match:
                rname = match.group(1)
                if self.groups:
                    rname = "%s-%s" % (self.groups[0], rname)
            elif self.groups:
                rname = self.groups[0]
            else:
                # a global source with no reasonable name.  Try to
                # strip off the protocol and trailing slash.
                match = re.search(r'^[A-z]://(.*?)/?', url_map['url'])
                if match:
                    rname = match.group(1)
                else:
                    # what kind of crazy url is this?  I give up!
                    # just use the full url and let the regex below
                    # make it even uglier.
                    rname = url_map['url']
        # see yum/__init__.py in the yum source, lines 441-449, for
        # the source of this regex.  yum doesn't like anything but
        # string.ascii_letters, string.digits, and [-_.:].  There
        # doesn't seem to be a reason for this, because yum.
        return re.sub(r'[^A-Za-z0-9-_.:]', '-', rname)

    def __repr__(self):
        if self.rawurl:
            return "%s at %s" % (self.__class__.__name__, self.rawurl)
        elif self.url:
            return "%s at %s" % (self.__class__.__name__, self.url)
        else:
            return self.__class__.__name__

    @property
    def urls(self):
        """ A list of URLs to the base metadata file for each
        repository described by this source. """
        return []

    @property
    def files(self):
        """ A list of files stored in the local cache by this backend.
        """
        return [self.escape_url(url) for url in self.urls]

    def get_vpkgs(self, metadata):
        """ Get a list of all virtual packages provided by all sources.

        :returns: list of strings
        """
        agroups = ['global'] + [a for a in self.arches
                                if a in metadata.groups]
        vdict = dict()
        for agrp in agroups:
            if agrp not in self.provides:
                self.logger.warning("%s provides no packages for %s" %
                                    (self, agrp))
                continue
            for key, value in list(self.provides[agrp].items()):
                if key not in vdict:
                    vdict[key] = set(value)
                else:
                    vdict[key].update(value)
        return vdict

    def is_virtual_package(self, metadata, package):  # pylint: disable=W0613
        """ Return True if a name is a virtual package (i.e., is a
        symbol provided by a real package), False otherwise.

        :param package: The name of the symbol, but see :ref:`pkg-objects`
        :type package: string
        :returns: bool
        """
        return False

    def escape_url(self, url):
        """ Given a URL to a repository metadata file, return the full
        path to a file suitable for storing that file locally.  This
        is acheived by replacing all forward slashes in the URL with
        ``@``.

        :param url: The URL to escape
        :type url: string
        :returns: string
        """
        return os.path.join(self.basepath, url.replace('/', '@'))

    def read_files(self):
        """ Read and parse locally downloaded metadata files and
        populates
        :attr:`Bcfg2.Server.Plugins.Packages.Source.Source.pkgnames`. Should
        call
        :func:`Bcfg2.Server.Plugins.Packages.Source.Source.process_files`
        as its final step."""
        pass

    def process_files(self, dependencies, provides):
        """ Given dicts of depends and provides generated by
        :func:`read_files`, this generates :attr:`deps` and
        :attr:`provides` and calls :func:`save_state` to save the
        cached data to disk.

        Both arguments are dicts of dicts of lists.  Keys are the
        arches of packages contained in this source; values are dicts
        whose keys are package names and values are lists of either
        dependencies for each package the symbols provided by each
        package.

        :param dependencies: A dict of dependencies found in the
                             metadata for this source.
        :type dependencies: dict; see above.
        :param provides: A dict of symbols provided by packages in
                        this repository.
        :type provides: dict; see above.
        """
        self.deps['global'] = dict()
        self.provides['global'] = dict()
        for barch in dependencies:
            self.deps[barch] = dict()
            self.provides[barch] = dict()
        for pkgname in self.pkgnames:
            pset = set()
            for barch in dependencies:
                if pkgname not in dependencies[barch]:
                    dependencies[barch][pkgname] = []
                pset.add(tuple(dependencies[barch][pkgname]))
            if len(pset) == 1:
                self.deps['global'][pkgname] = pset.pop()
            else:
                for barch in dependencies:
                    self.deps[barch][pkgname] = dependencies[barch][pkgname]
        provided = set()
        for bprovided in list(provides.values()):
            provided.update(set(bprovided))
        for prov in provided:
            prset = set()
            for barch in provides:
                if prov not in provides[barch]:
                    continue
                prset.add(tuple(provides[barch].get(prov, ())))
            if len(prset) == 1:
                self.provides['global'][prov] = prset.pop()
            else:
                for barch in provides:
                    self.provides[barch][prov] = provides[barch].get(prov, ())
        self.save_state()

    def unknown_filter(self, package):
        """ A predicate that is used by :func:`filter_unknown` to
        filter packages from the results of
        :func:`Bcfg2.Server.Plugins.Packages.Collection.Collection.complete`
        that should not be shown to the end user (i.e., that are not
        truly unknown, but are rather packaging system artifacts).  By
        default, excludes any package whose name starts with "choice"

        :param package: The name of a package that was unknown to the
                        backend
        :type package: string
        :returns: bool
        """
        return package.startswith("choice")

    def filter_unknown(self, unknown):
        """ After
        :func:`Bcfg2.Server.Plugins.Packages.Collection.Collection.complete`,
        filter out packages that appear in the list of unknown
        packages but should not be presented to the user.
        :attr:`unknown_filter` is called to assess whether or not a
        package is expected to be unknown.

        :param unknown: A set of unknown packages.  The set should be
                        modified in place.
        :type unknown: set of strings
        """
        unknown.difference_update(set([u for u in unknown
                                       if self.unknown_filter(u)]))

    def update(self):
        """ Download metadata from the upstream repository and cache
        it locally.

        :raises: ValueError - If any URL in :attr:`urls` is malformed
        :raises: OSError - If there is an error writing the local
                 cache
        :raises: HTTPError - If there is an error fetching the remote
                 data
        """
        for url in self.urls:
            self.logger.info("Packages: Updating %s" % url)
            fname = self.escape_url(url)
            try:
                open(fname, 'wb').write(fetch_url(url))
            except ValueError:
                self.logger.error("Packages: Bad url string %s" % url)
                raise
            except OSError:
                err = sys.exc_info()[1]
                self.logger.error("Packages: Could not write data from %s to "
                                  "local cache at %s: %s" % (url, fname, err))
                raise
            except HTTPError:
                err = sys.exc_info()[1]
                self.logger.error("Packages: Failed to fetch url %s. HTTP "
                                  "response code=%s" % (url, err.code))
                raise

    def applies(self, metadata):
        """ Return true if this source applies to the given client,
        i.e., the client is in all necessary groups and
        :ref:`server-plugins-generators-packages-magic-groups`.

        :param metadata: The client metadata to check to see if this
                         source applies
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :returns: bool
        """
        # check base groups
        if not self.magic_groups_match(metadata):
            return False

        # check Group/Client tags from sources.xml
        for condition in self.conditions:
            if not condition(metadata):
                return False

        return True

    def get_arches(self, metadata):
        """ Get a list of architectures that the given client has and
        for which this source provides packages for.  The return value
        will always include ``global``.

        :param metadata: The client metadata to get matching
                         architectures for
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :returns: list of strings
        """
        return ['global'] + [a for a in self.arches if a in metadata.groups]

    def get_deps(self, metadata, package):
        """ Get a list of the dependencies of the given package.

        :param package: The name of the symbol
        :type package: string
        :returns: list of strings
        """
        for arch in self.get_arches(metadata):
            if package in self.deps[arch]:
                return self.deps[arch][package]
        return []

    def get_provides(self, metadata, package):
        """ Get a list of all symbols provided by the given package.

        :param package: The name of the package
        :type package: string
        :returns: list of strings
        """
        for arch in self.get_arches(metadata):
            if package in self.provides[arch]:
                return self.provides[arch][package]
        return []

    def is_package(self, metadata, package):  # pylint: disable=W0613
        """ Return True if a package is a package, False otherwise.

        :param package: The name of the package
        :type package: string
        :returns: bool
        """
        return (package in self.pkgnames and
                package not in self.blacklist and
                (len(self.whitelist) == 0 or package in self.whitelist))

    def get_group(self, metadata, group, ptype=None):  # pylint: disable=W0613
        """ Get the list of packages of the given type in a package
        group.

        :param group: The name of the group to query
        :type group: string
        :param ptype: The type of packages to get, for backends that
                      support multiple package types in package groups
                      (e.g., "recommended," "optional," etc.)
        :type ptype: string
        :returns: list of strings - package names
        """
        return []

    def magic_groups_match(self, metadata):
        """ Returns True if the client's
        :ref:`server-plugins-generators-packages-magic-groups` match
        the magic groups this source.  Also returns True if magic
        groups are off in the configuration and the client's
        architecture matches (i.e., architecture groups are *always*
        checked).

        :returns: bool
        """
        found_arch = False
        for arch in self.arches:
            if arch in metadata.groups:
                found_arch = True
                break
        if not found_arch:
            return False

        if not self.setup.cfp.getboolean("packages", "magic_groups",
                                         default=False):
            return True
        else:
            for group in self.basegroups:
                if group in metadata.groups:
                    return True
            return False

########NEW FILE########
__FILENAME__ = Yum
""" Yum backend for :mod:`Bcfg2.Server.Plugins.Packages`.  This module
is the most complex backend because it has to handle Yum sources
without yum Python libraries, with yum Python libraries, and Pulp
sources.  (See :ref:`native-yum-libraries` for details on using the
yum Python libraries and :ref:`pulp-source-support` for details on
Pulp sources.)

.. _bcfg2-yum-helper:

bcfg2-yum-helper
~~~~~~~~~~~~~~~~

If using the yum Python libraries, :class:`YumCollection` makes shell
calls to an external command, ``bcfg2-yum-helper``, which performs the
actual yum API calls.  This is done because the yum libs have horrific
memory leaks, and apparently the right way to get around that in
long-running processes it to have a short-lived helper.  This is how
it's done by yum itself in ``yum-updatesd``, which is a long-running
daemon that checks for and applies updates.

.. _yum-pkg-objects:

Package Objects
~~~~~~~~~~~~~~~

:class:`Bcfg2.Server.Plugins.Packages.Collection.Collection` objects
have the option to translate from some backend-specific representation
of packages to XML entries; see :ref:`pkg-objects` for more
information on this.  If you are using the Python yum libraries,
:class:`Bcfg2.Server.Plugins.Packages.Yum.YumCollection` opts to do
this, using the yum tuple representation of packages, which is::

    (<name>, <arch>, <epoch>, <version>, <release>)

For shorthand this is occasionally abbrevated "naevr".  Any datum that
is not defined is ``None``.  So a normal package entry that can be any
version would be passed to :ref:`bcfg2-yum-helper` as::

    ("somepackage", None, None, None, None)

A package returned from the helper might look more like this::

    ("somepackage", "x86_64", None, "1.2.3", "1.el6")

We translate between this representation and the XML representation of
packages with :func:`YumCollection.packages_from_entry` and
:func:`YumCollection.packages_to_entry`.

The Yum Backend
~~~~~~~~~~~~~~~
"""

import os
import re
import sys
import time
import copy
import errno
import socket
import logging
import lxml.etree
import Bcfg2.Server.Plugin
from lockfile import FileLock
from Bcfg2.Utils import Executor
# pylint: disable=W0622
from Bcfg2.Compat import StringIO, cPickle, HTTPError, URLError, \
    ConfigParser, any
# pylint: enable=W0622
from Bcfg2.Server.Plugins.Packages.Collection import Collection
from Bcfg2.Server.Plugins.Packages.Source import SourceInitError, Source, \
    fetch_url

LOGGER = logging.getLogger(__name__)

# pylint: disable=E0611
try:
    from pulp.client.consumer.config import ConsumerConfig
    from pulp.client.api.repository import RepositoryAPI
    from pulp.client.api.consumer import ConsumerAPI
    from pulp.client.api import server
    HAS_PULP = True
except ImportError:
    HAS_PULP = False
# pylint: enable=E0611

try:
    import yum
    try:
        import json
        # py2.4 json library is structured differently
        json.loads  # pylint: disable=W0104
    except (ImportError, AttributeError):
        import simplejson as json
    HAS_YUM = True
except ImportError:
    HAS_YUM = False
    LOGGER.info("Packages: No yum libraries found; forcing use of internal "
                "dependency resolver")


XP = '{http://linux.duke.edu/metadata/common}'
RP = '{http://linux.duke.edu/metadata/rpm}'
RPO = '{http://linux.duke.edu/metadata/repo}'
FL = '{http://linux.duke.edu/metadata/filelists}'

PULPSERVER = None
PULPCONFIG = None


def _setup_pulp(setup):
    """ Connect to a Pulp server and pass authentication credentials.
    This only needs to be called once, but multiple calls won't hurt
    anything.

    :param setup: A Bcfg2 options dict
    :type setup: dict
    :returns: :class:`pulp.client.api.server.PulpServer`
    """
    global PULPSERVER, PULPCONFIG
    if not HAS_PULP:
        msg = "Packages: Cannot create Pulp collection: Pulp libraries " + \
            "not found"
        LOGGER.error(msg)
        raise Bcfg2.Server.Plugin.PluginInitError(msg)

    if PULPSERVER is None:
        try:
            username = setup.cfp.get("packages:pulp", "username")
            password = setup.cfp.get("packages:pulp", "password")
        except ConfigParser.NoSectionError:
            msg = "Packages: No [pulp] section found in bcfg2.conf"
            LOGGER.error(msg)
            raise Bcfg2.Server.Plugin.PluginInitError(msg)
        except ConfigParser.NoOptionError:
            msg = "Packages: Required option not found in bcfg2.conf: %s" % \
                sys.exc_info()[1]
            LOGGER.error(msg)
            raise Bcfg2.Server.Plugin.PluginInitError(msg)

        PULPCONFIG = ConsumerConfig()
        serveropts = PULPCONFIG.server

        PULPSERVER = server.PulpServer(serveropts['host'],
                                       int(serveropts['port']),
                                       serveropts['scheme'],
                                       serveropts['path'])
        PULPSERVER.set_basic_auth_credentials(username, password)
        server.set_active_server(PULPSERVER)
    return PULPSERVER


class PulpCertificateData(Bcfg2.Server.Plugin.SpecificData):
    """ Handle pulp consumer certificate data for
    :class:`PulpCertificateSet` """

    def bind_entry(self, entry, _):
        """ Given an abstract entry, add data to it and return it.
        :class:`PulpCertificateSet` handles binding entry metadata.

        :param entry: The abstract entry to bind data to
        :type entry: lxml.etree._Element
        :returns: lxml.etree._Element - the bound entry
        """
        entry.set("type", "file")
        if self.data:
            entry.text = self.data
        else:
            entry.set("empty", "true")
        return entry


class PulpCertificateSet(Bcfg2.Server.Plugin.EntrySet):
    """ Handle Pulp consumer certificates. """

    #: The path to certificates on consumer machines
    certpath = "/etc/pki/consumer/cert.pem"

    def __init__(self, path, fam):
        """
        :param path: The path to the directory where Pulp consumer
                     certificates will be stored
        :type path: string
        """
        Bcfg2.Server.Plugin.EntrySet.__init__(self,
                                              os.path.basename(self.certpath),
                                              path,
                                              PulpCertificateData,
                                              "UTF-8")
        self.metadata = dict(owner='root',
                             group='root',
                             mode='0644',
                             secontext='__default__',
                             important='true',
                             sensitive='true',
                             paranoid=self.metadata['paranoid'])
        self.fam = fam
        self.fam.AddMonitor(path, self)

    def HandleEvent(self, event):
        """ Handle FAM events on certificate files.

        :param event: The event to handle
        :type event: Bcfg2.Server.FileMonitor.Event """
        if event.filename != self.path:
            return self.handle_event(event)

    def write_data(self, data, metadata):
        """ Write a new certificate to the filesystem.

        :param data: The new certificate data
        :type data: string
        :param metadata: Metadata for the client to write the
                         certificate for
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        """
        specific = "%s.H_%s" % (os.path.basename(self.certpath),
                                metadata.hostname)
        fileloc = os.path.join(self.path, specific)

        self.logger.info("Packages: Writing certificate data for %s to %s" %
                         (metadata.hostname, fileloc))
        try:
            open(fileloc, 'wb').write(data)
        except IOError:
            err = sys.exc_info()[1]
            self.logger.error("Could not write %s: %s" % (fileloc, err))
            return
        self.verify_file(specific)

    def verify_file(self, filename):
        """ Service the FAM events queued up by the key generation so
        the data structure entries will be available for binding.

        NOTE: We wait for up to ten seconds. There is some potential
        for race condition, because if the file monitor doesn't get
        notified about the new key files in time, those entries won't
        be available for binding. In practice, this seems "good
        enough."

        :param filename: The filename to check for events on
        :type filename: string
        """
        tries = 0
        updated = False
        while not updated:
            if tries >= 10:
                self.logger.error("%s still not registered" % filename)
                return
            self.fam.handle_events_in_interval(1)
            if filename in self.entries:
                break
            else:
                tries += 1
                continue


class YumCollection(Collection):
    """ Handle collections of Yum sources.  If we're using the yum
    Python libraries, then this becomes a very full-featured
    :class:`Bcfg2.Server.Plugins.Packages.Collection.Collection`
    object; if not, then it defers to the :class:`YumSource`
    object.

    .. private-include: _add_gpg_instances, _get_pulp_consumer
    """

    _helper = None

    #: Options that are included in the [packages:yum] section of the
    #: config but that should not be included in the temporary
    #: yum.conf we write out
    option_blacklist = ["use_yum_libraries", "helper"]

    #: :class:`PulpCertificateSet` object used to handle Pulp certs
    pulp_cert_set = None

    def __init__(self, metadata, sources, cachepath, basepath, fam,
                 debug=False):
        Collection.__init__(self, metadata, sources, cachepath, basepath, fam,
                            debug=debug)
        self.keypath = os.path.join(self.cachepath, "keys")

        #: A :class:`Bcfg2.Utils.Executor` object to use to run
        #: external commands
        self.cmd = Executor()

        if self.use_yum:
            #: Define a unique cache file for this collection to use
            #: for cached yum metadata
            self.cachefile = os.path.join(self.cachepath,
                                          "cache-%s" % self.cachekey)

            #: The path to the server-side config file used when
            #: resolving packages with the Python yum libraries
            self.cfgfile = os.path.join(self.cachefile, "yum.conf")

            if not os.path.exists(self.cachefile):
                self.debug_log("Creating common cache %s" % self.cachefile)
                os.mkdir(self.cachefile)
                if not self.disableMetaData:
                    self.setup_data()
        else:
            self.cachefile = None

        if HAS_PULP and self.has_pulp_sources:
            _setup_pulp(self.setup)
            if self.pulp_cert_set is None:
                certdir = os.path.join(
                    self.basepath,
                    "pulp",
                    os.path.basename(PulpCertificateSet.certpath))
                try:
                    os.makedirs(certdir)
                except OSError:
                    err = sys.exc_info()[1]
                    if err.errno == errno.EEXIST:
                        pass
                    else:
                        self.logger.error("Could not create Pulp consumer "
                                          "cert directory at %s: %s" %
                                          (certdir, err))
                self.__class__.pulp_cert_set = PulpCertificateSet(certdir,
                                                                  self.fam)

    @property
    def disableMetaData(self):  # pylint: disable=C0103
        """ Report whether or not metadata processing is enabled.
        This duplicates code in Packages/__init__.py, and can probably
        be removed in Bcfg2 1.4 when we have a module-level setup
        object. """
        if self.setup is None:
            return True
        try:
            return not self.setup.cfp.getboolean("packages", "resolver")
        except (ConfigParser.NoSectionError, ConfigParser.NoOptionError):
            return False
        except ValueError:
            # for historical reasons we also accept "enabled" and
            # "disabled"
            return self.setup.cfp.get(
                "packages",
                "metadata",
                default="enabled").lower() == "disabled"

    @property
    def __package_groups__(self):
        return True

    @property
    def helper(self):
        """ The full path to :file:`bcfg2-yum-helper`.  First, we
        check in the config file to see if it has been explicitly
        specified; next we see if it's in $PATH (which we do by making
        a call to it; I wish there was a way to do this without
        forking, but apparently not); finally we check in /usr/sbin,
        the default location. """
        # pylint: disable=W0212
        if not self.__class__._helper:
            try:
                self.__class__._helper = self.setup.cfp.get("packages:yum",
                                                            "helper")
            except (ConfigParser.NoOptionError, ConfigParser.NoSectionError):
                # first see if bcfg2-yum-helper is in PATH
                try:
                    self.debug_log("Checking for bcfg2-yum-helper in $PATH")
                    self.cmd.run(['bcfg2-yum-helper'])
                    self.__class__._helper = 'bcfg2-yum-helper'
                except OSError:
                    self.__class__._helper = "/usr/sbin/bcfg2-yum-helper"
        return self.__class__._helper
        # pylint: enable=W0212

    @property
    def use_yum(self):
        """ True if we should use the yum Python libraries, False
        otherwise """
        return HAS_YUM and self.setup.cfp.getboolean("packages:yum",
                                                     "use_yum_libraries",
                                                     default=False)

    @property
    def has_pulp_sources(self):
        """ True if there are any Pulp sources to handle, False
        otherwise """
        return any(s.pulp_id for s in self)

    @property
    def cachefiles(self):
        """ A list of the full path to all cachefiles used by this
        collection."""
        cachefiles = set(Collection.cachefiles.fget(self))
        if self.cachefile:
            cachefiles.add(self.cachefile)
        return list(cachefiles)

    @Bcfg2.Server.Plugin.track_statistics()
    def write_config(self):
        """ Write the server-side config file to :attr:`cfgfile` based
        on the data from :func:`get_config`"""
        if not os.path.exists(self.cfgfile):
            yumconf = self.get_config(raw=True)
            yumconf.add_section("main")

            # we set installroot to the cache directory so
            # bcfg2-yum-helper works with an empty rpmdb.  otherwise
            # the rpmdb is so hopelessly intertwined with yum that we
            # have to totally reinvent the dependency resolver.
            mainopts = dict(cachedir='/',
                            persistdir='/',
                            installroot=self.cachefile,
                            keepcache="0",
                            debuglevel="0",
                            sslverify="0",
                            reposdir="/dev/null")
            if self.setup['debug']:
                mainopts['debuglevel'] = "5"
            elif self.setup['verbose']:
                mainopts['debuglevel'] = "2"

            try:
                for opt in self.setup.cfp.options("packages:yum"):
                    if opt not in self.option_blacklist:
                        mainopts[opt] = self.setup.cfp.get("packages:yum", opt)
            except ConfigParser.NoSectionError:
                pass

            for opt, val in list(mainopts.items()):
                yumconf.set("main", opt, val)

            yumconf.write(open(self.cfgfile, 'w'))

    def get_arch(self):
        """ If 'arch' for each source is the same, return that arch, otherwise
        None.

        This helps bcfg2-yum-helper when the client arch is
        incompatible with the bcfg2 server's arch.

        In case multiple arches are found, punt back to the default behavior.
        """
        arches = set()
        for source in self:
            for url_map in source.url_map:
                if url_map['arch'] in self.metadata.groups:
                    arches.add(url_map['arch'])
        if len(arches) == 1:
            return arches.pop()
        else:
            return None

    def get_config(self, raw=False):  # pylint: disable=W0221
        """ Get the yum configuration for this collection.

        :param raw: Return a :class:`ConfigParser.SafeConfigParser`
                    object representing the configuration instead of a
                    string.  This is useful if you need to modify the
                    config before writing it (as :func:`write_config`
                    does in order to produce a server-specific
                    configuration).
        :type raw: bool
        :returns: string or ConfigParser.SafeConfigParser """

        config = ConfigParser.SafeConfigParser()
        for source in self:
            for url_map in source.url_map:
                if url_map['arch'] not in self.metadata.groups:
                    continue
                basereponame = source.get_repo_name(url_map)
                reponame = basereponame

                added = False
                while not added:
                    try:
                        config.add_section(reponame)
                        added = True
                    except ConfigParser.DuplicateSectionError:
                        match = re.search(r'-(\d+)', reponame)
                        if match:
                            rid = int(match.group(1)) + 1
                        else:
                            rid = 1
                        reponame = "%s-%d" % (basereponame, rid)

                config.set(reponame, "name", reponame)
                config.set(reponame, "baseurl", url_map['url'])
                config.set(reponame, "enabled", "1")
                if len(source.gpgkeys):
                    config.set(reponame, "gpgcheck", "1")
                    config.set(reponame, "gpgkey",
                               " ".join(source.gpgkeys))
                else:
                    config.set(reponame, "gpgcheck", "0")

                if len(source.blacklist):
                    config.set(reponame, "exclude",
                               " ".join(source.blacklist))
                if len(source.whitelist):
                    config.set(reponame, "includepkgs",
                               " ".join(source.whitelist))

                if raw:
                    opts = source.server_options
                else:
                    opts = source.client_options
                for opt, val in opts.items():
                    config.set(reponame, opt, val)

        if raw:
            return config
        else:
            # configparser only writes to file, so we have to use a
            # StringIO object to get the data out as a string
            buf = StringIO()
            config.write(buf)
            return "# This config was generated automatically by the Bcfg2 " \
                   "Packages plugin\n\n" + buf.getvalue()

    @Bcfg2.Server.Plugin.track_statistics()
    def build_extra_structures(self, independent):
        """ Add additional entries to the ``<Independent/>`` section
        of the final configuration.  This adds several kinds of
        entries:

        * For GPG keys, adds a ``Package`` entry that describes the
          version and release of all expected ``gpg-pubkey`` packages;
          and ``Path`` entries to copy all of the GPG keys to the
          appropriate place on the client filesystem.  Calls
          :func:`_add_gpg_instances`.

        * For Pulp Sources, adds a ``Path`` entry for the consumer
          certificate; and ``Action`` entries to update the
          consumer-side Pulp config if the consumer is newly
          registered.  Creates a new Pulp consumer from the Bcfg2
          server as necessary.

        :param independent: The XML tag to add extra entries to.  This
                            is modified in place.
        :type independent: lxml.etree._Element
        """
        needkeys = set()
        for source in self:
            for key in source.gpgkeys:
                needkeys.add(key)

        if len(needkeys):
            if HAS_YUM:
                # this must be be HAS_YUM, not use_yum, because
                # regardless of whether the user wants to use the yum
                # resolver we want to include gpg key data
                keypkg = lxml.etree.Element('BoundPackage', name="gpg-pubkey",
                                            type=self.ptype, origin='Packages')
            else:
                self.logger.warning("GPGKeys were specified for yum sources "
                                    "in sources.xml, but no yum libraries "
                                    "were found")
                self.logger.warning("GPG key version/release data cannot be "
                                    "determined automatically")
                self.logger.warning("Install yum libraries, or manage GPG "
                                    "keys manually")
                keypkg = None

            for key in needkeys:
                # figure out the path of the key on the client
                keydir = self.setup.cfp.get("global", "gpg_keypath",
                                            default="/etc/pki/rpm-gpg")
                remotekey = os.path.join(keydir, os.path.basename(key))
                localkey = os.path.join(self.keypath, os.path.basename(key))
                kdata = open(localkey).read()

                # copy the key to the client
                keypath = lxml.etree.Element("BoundPath", name=remotekey,
                                             encoding='ascii',
                                             owner='root', group='root',
                                             type='file', mode='0644',
                                             important='true')
                keypath.text = kdata

                # hook to add version/release info if possible
                self._add_gpg_instances(keypkg, localkey, remotekey,
                                        keydata=kdata)
                independent.append(keypath)
            if keypkg is not None:
                independent.append(keypkg)

        if self.has_pulp_sources:
            consumerapi = ConsumerAPI()
            consumer = self._get_pulp_consumer(consumerapi=consumerapi)
            if consumer is None:
                try:
                    consumer = \
                        consumerapi.create(self.metadata.hostname,
                                           self.metadata.hostname,
                                           capabilities=dict(bind=False))
                    lxml.etree.SubElement(
                        independent, "BoundAction", name="pulp-update",
                        timing="pre", when="always", status="check",
                        command="pulp-consumer consumer update")
                    self.pulp_cert_set.write_data(consumer['certificate'],
                                                  self.metadata)
                except server.ServerRequestError:
                    err = sys.exc_info()[1]
                    self.logger.error("Packages: Could not create Pulp "
                                      "consumer %s: %s" %
                                      (self.metadata.hostname, err))

            for source in self:
                # each pulp source can only have one arch, so we don't
                # have to check the arch in url_map
                if (source.pulp_id and
                    source.pulp_id not in consumer['repoids']):
                    try:
                        consumerapi.bind(self.metadata.hostname,
                                         source.pulp_id)
                    except server.ServerRequestError:
                        err = sys.exc_info()[1]
                        self.logger.error("Packages: Could not bind %s to "
                                          "Pulp repo %s: %s" %
                                          (self.metadata.hostname,
                                           source.pulp_id, err))

            crt = lxml.etree.SubElement(independent, "BoundPath",
                                        name=self.pulp_cert_set.certpath)
            self.pulp_cert_set.bind_entry(crt, self.metadata)

    @Bcfg2.Server.Plugin.track_statistics()
    def _get_pulp_consumer(self, consumerapi=None):
        """ Get a Pulp consumer object for the client.

        :param consumerapi: A Pulp ConsumerAPI object.  If none is
                            passed, one will be instantiated.
        :type consumerapi: pulp.client.api.consumer.ConsumerAPI
        :returns: dict - the consumer.  Returns None on failure
                  (including if there is no existing Pulp consumer for
                  this client.
        """
        if consumerapi is None:
            consumerapi = ConsumerAPI()
        consumer = None
        try:
            consumer = consumerapi.consumer(self.metadata.hostname)
        except server.ServerRequestError:
            # consumer does not exist
            pass
        except socket.error:
            err = sys.exc_info()[1]
            self.logger.error("Packages: Could not contact Pulp server: %s" %
                              err)
        except:
            err = sys.exc_info()[1]
            self.logger.error("Packages: Unknown error querying Pulp server: "
                              "%s" % err)
        return consumer

    @Bcfg2.Server.Plugin.track_statistics()
    def _add_gpg_instances(self, keyentry, localkey, remotekey, keydata=None):
        """ Add GPG keys instances to a ``Package`` entry.  This is
        called from :func:`build_extra_structures` to add GPG keys to
        the specification.

        :param keyentry: The ``Package`` entry to add key instances
                         to.  This will be modified in place.
        :type keyentry: lxml.etree._Element
        :param localkey: The full path to the key file on the Bcfg2 server
        :type localkey: string
        :param remotekey: The full path to the key file on the client.
                          (If they key is not yet on the client, this
                          will be the full path to where the key file
                          will go eventually.)
        :type remotekey: string
        :param keydata: The contents of the key file.  If this is not
                        provided, read the data from ``localkey``.
        :type keydata: string
        """
        # this must be be HAS_YUM, not use_yum, because regardless of
        # whether the user wants to use the yum resolver we want to
        # include gpg key data
        if not HAS_YUM:
            return

        if keydata is None:
            keydata = open(localkey).read()

        try:
            kinfo = yum.misc.getgpgkeyinfo(keydata)
            version = yum.misc.keyIdToRPMVer(kinfo['keyid'])
            release = yum.misc.keyIdToRPMVer(kinfo['timestamp'])

            lxml.etree.SubElement(keyentry, 'Instance',
                                  version=version,
                                  release=release,
                                  simplefile=remotekey)
        except ValueError:
            err = sys.exc_info()[1]
            self.logger.error("Packages: Could not read GPG key %s: %s" %
                              (localkey, err))

    @Bcfg2.Server.Plugin.track_statistics()
    def get_groups(self, grouplist):
        """ If using the yum libraries, given a list of package group
        names, return a dict of ``<group name>: <list of packages>``.
        This is much faster than implementing
        :func:`Bcfg2.Server.Plugins.Packages.Collection.Collection.get_group`,
        since we have to make a call to the bcfg2 Yum helper, and each
        time we do that we make another call to yum, which means we
        set up yum metadata from the cache (hopefully) each time.  So
        resolving ten groups once is much faster than resolving one
        group ten times.

        If you are using the builtin yum parser, this raises a warning
        and returns an empty dict.

        :param grouplist: The list of groups to query
        :type grouplist: list of strings - group names
        :returns: dict of ``<group name>: <list of packages>``

        In this implementation the packages may be strings or tuples.
        See :ref:`yum-pkg-objects` for more information. """
        if not grouplist:
            return dict()

        gdicts = []
        for group, ptype in grouplist:
            if group.startswith("@"):
                group = group[1:]
            if not ptype:
                ptype = "default"
            gdicts.append(dict(group=group, type=ptype))

        if self.use_yum:
            try:
                return self.call_helper("get_groups", inputdata=gdicts)
            except ValueError:
                return dict()
        else:
            pkgs = dict()
            for gdict in gdicts:
                pkgs[gdict['group']] = Collection.get_group(self,
                                                            gdict['group'],
                                                            gdict['type'])
            return pkgs

    def _element_to_pkg(self, el, name):
        """ Convert a Package or Instance element to a package tuple """
        rv = (name, el.get("arch"), el.get("epoch"),
              el.get("version"), el.get("release"))
        if rv[3] in ['any', 'auto']:
            rv = (rv[0], rv[1], rv[2], None, None)
        # if a package requires no specific version, we just use
        # the name, not the tuple.  this limits the amount of JSON
        # encoding/decoding that has to be done to pass the
        # package list to bcfg2-yum-helper.
        if rv[1:] == (None, None, None, None):
            return name
        else:
            return rv

    def packages_from_entry(self, entry):
        """ When using the Python yum libraries, convert a Package
        entry to a list of package tuples.  See :ref:`yum-pkg-objects`
        and :ref:`pkg-objects` for more information on this process.

        :param entry: The Package entry to convert
        :type entry: lxml.etree._Element
        :returns: list of tuples
        """
        if not self.use_yum:
            return Collection.packages_from_entry(self, entry)

        rv = set()
        name = entry.get("name")

        for inst in entry.getchildren():
            if inst.tag != "Instance":
                continue
            rv.add(self._element_to_pkg(inst, name))
        if not rv:
            rv.add(self._element_to_pkg(entry, name))
        return list(rv)

    def _get_entry_attrs(self, pkgtup):
        """ Given a package tuple, return a dict of attributes
        suitable for applying to either a Package or an Instance
        tag """
        attrs = dict(version=self.setup.cfp.get("packages", "version",
                                                default="auto"))
        if attrs['version'] == 'any' or not isinstance(pkgtup, tuple):
            return attrs

        try:
            if pkgtup[1]:
                attrs['arch'] = pkgtup[1]
            if pkgtup[2]:
                attrs['epoch'] = pkgtup[2]
            if pkgtup[3]:
                attrs['version'] = pkgtup[3]
            if pkgtup[4]:
                attrs['release'] = pkgtup[4]
        except IndexError:
            self.logger.warning("Malformed package tuple: %s" % pkgtup)
        return attrs

    def packages_to_entry(self, pkglist, entry):
        """ When using the Python yum libraries, convert a list of
        package tuples to a Package entry.  See :ref:`yum-pkg-objects`
        and :ref:`pkg-objects` for more information on this process.

        If pkglist contains only one package, then its data is
        converted to a single ``BoundPackage`` entry that is added as
        a subelement of ``entry``.  If pkglist contains more than one
        package, then a parent ``BoundPackage`` entry is created and
        child ``Instance`` entries are added to it.

        :param pkglist: A list of package tuples to convert to an XML
                         Package entry
        :type pkglist: list of tuples
        :param entry: The base XML entry to add Package entries to.
                      This is modified in place.
        :type entry: lxml.etree._Element
        :returns: None
        """
        if not self.use_yum:
            return Collection.packages_to_entry(self, pkglist, entry)

        packages = dict()
        for pkg in pkglist:
            try:
                packages[pkg[0]].append(pkg)
            except KeyError:
                packages[pkg[0]] = [pkg]
        for name, instances in packages.items():
            pkgattrs = dict(type=self.ptype,
                            origin='Packages',
                            name=name)
            if len(instances) > 1:
                pkg_el = lxml.etree.SubElement(entry, 'BoundPackage',
                                               **pkgattrs)
                for inst in instances:
                    lxml.etree.SubElement(pkg_el, "Instance",
                                          self._get_entry_attrs(inst))
            else:
                attrs = self._get_entry_attrs(instances[0])
                attrs.update(pkgattrs)
                lxml.etree.SubElement(entry, 'BoundPackage', **attrs)

    def get_new_packages(self, initial, complete):
        """ Compute the difference between the complete package list
        (as returned by :func:`complete`) and the initial package list
        computed from the specification, allowing for package tuples.
        See :ref:`yum-pkg-objects` and :ref:`pkg-objects` for more
        information on this process.

        :param initial: The initial package list
        :type initial: set of strings, but see :ref:`pkg-objects`
        :param complete: The final package list
        :type complete: set of strings, but see :ref:`pkg-objects`
        :return: set of tuples
        """
        initial_names = []
        for pkg in initial:
            if isinstance(pkg, tuple):
                initial_names.append(pkg[0])
            else:
                initial_names.append(pkg)
        new = []
        for pkg in complete:
            if isinstance(pkg, tuple):
                name = pkg[0]
            else:
                name = pkg
            if name not in initial_names:
                new.append(pkg)
        return new

    @Bcfg2.Server.Plugin.track_statistics()
    def complete(self, packagelist):
        """ Build a complete list of all packages and their dependencies.

        When using the Python yum libraries, this defers to the
        :ref:`bcfg2-yum-helper`; when using the builtin yum parser,
        this defers to
        :func:`Bcfg2.Server.Plugins.Packages.Collection.Collection.complete`.

        :param packagelist: Set of initial packages computed from the
                            specification.
        :type packagelist: set of strings, but see :ref:`pkg-objects`
        :returns: tuple of sets - The first element contains a set of
                  strings (but see :ref:`pkg-objects`) describing the
                  complete package list, and the second element is a
                  set of symbols whose dependencies could not be
                  resolved.
        """
        if not self.use_yum:
            return Collection.complete(self, packagelist)

        lock = FileLock(os.path.join(self.cachefile, "lock"))
        slept = 0
        while lock.is_locked():
            if slept > 30:
                self.logger.warning("Packages: Timeout waiting for yum cache "
                                    "to release its lock")
                return set(), set()
            self.logger.debug("Packages: Yum cache is locked, waiting...")
            time.sleep(3)
            slept += 3

        if packagelist:
            try:
                helper_dict = dict(packages=list(packagelist),
                                   groups=list(self.get_relevant_groups()))
                arch = self.get_arch()
                if arch is not None:
                    helper_dict['arch'] = arch
                result = self.call_helper("complete", helper_dict)
            except ValueError:
                # error reported by call_helper()
                return set(), packagelist
            # json doesn't understand sets or tuples, so we get back a
            # lists of lists (packages) and a list of unicode strings
            # (unknown).  turn those into a set of tuples and a set of
            # strings, respectively.
            unknown = set([str(u) for u in result['unknown']])
            packages = set([tuple(p) for p in result['packages']])
            self.filter_unknown(unknown)
            return packages, unknown
        else:
            return set(), set()

    @Bcfg2.Server.Plugin.track_statistics()
    def call_helper(self, command, inputdata=None):
        """ Make a call to :ref:`bcfg2-yum-helper`.  The yum libs have
        horrific memory leaks, so apparently the right way to get
        around that in long-running processes it to have a short-lived
        helper.  No, seriously -- check out the yum-updatesd code.
        It's pure madness.

        :param command: The :ref:`bcfg2-yum-helper` command to call.
        :type command: string
        :param inputdata: The input to pass to ``bcfg2-yum-helper`` on
                          stdin.  If this is None, no input will be
                          given at all.
        :type inputdata: Any JSON-encodable data structure.
        :returns: Varies depending on the return value of the
                  ``bcfg2-yum-helper`` command.
        """
        cmd = [self.helper, "-c", self.cfgfile]
        if self.setup['verbose']:
            cmd.append("-v")
        if self.debug_flag:
            if not self.setup['verbose']:
                # ensure that running in debug gets -vv, even if
                # verbose is not enabled
                cmd.append("-v")
            cmd.append("-v")
        cmd.append(command)
        self.debug_log("Packages: running %s" % " ".join(cmd))

        if inputdata:
            result = self.cmd.run(cmd, timeout=self.setup['client_timeout'],
                                  inputdata=json.dumps(inputdata))
        else:
            result = self.cmd.run(cmd, timeout=self.setup['client_timeout'])
        if not result.success:
            self.logger.error("Packages: error running bcfg2-yum-helper: %s" %
                              result.error)
        elif result.stderr:
            self.debug_log("Packages: debug info from bcfg2-yum-helper: %s" %
                           result.stderr)

        try:
            return json.loads(result.stdout)
        except ValueError:
            if result.stdout:
                err = sys.exc_info()[1]
                self.logger.error("Packages: Error reading bcfg2-yum-helper "
                                  "output: %s" % err)
                self.logger.error("Packages: bcfg2-yum-helper output: %s" %
                                  result.stdout)
            else:
                self.logger.error("Packages: No bcfg2-yum-helper output")
            raise

    def setup_data(self, force_update=False):
        """ Do any collection-level data setup tasks. This is called
        when sources are loaded or reloaded by
        :class:`Bcfg2.Server.Plugins.Packages.Packages`.

        If the builtin yum parsers are in use, this defers to
        :func:`Bcfg2.Server.Plugins.Packages.Collection.Collection.setup_data`.
        If using the yum Python libraries, this cleans up cached yum
        metadata, regenerates the server-side yum config (in order to
        catch any new sources that have been added to this server),
        then regenerates the yum cache.

        :param force_update: Ignore all local cache and setup data
                             from its original upstream sources (i.e.,
                             the package repositories)
        :type force_update: bool
        """
        if not self.use_yum:
            return Collection.setup_data(self, force_update)

        if force_update:
            # clean up data from the old config
            try:
                self.call_helper("clean")
            except ValueError:
                # error reported by call_helper
                pass

        if os.path.exists(self.cfgfile):
            os.unlink(self.cfgfile)
        self.write_config()

        try:
            self.call_helper("makecache")
        except ValueError:
            # error reported by call_helper
            pass


class YumSource(Source):
    """ Handle yum sources """

    #: :ref:`server-plugins-generators-packages-magic-groups` for
    #: ``YumSource`` are "yum", "redhat", "centos", and "fedora"
    basegroups = ['yum', 'redhat', 'centos', 'fedora']

    #: YumSource sets the ``type`` on Package entries to "yum"
    ptype = 'yum'

    def __init__(self, basepath, xsource, setup):
        Source.__init__(self, basepath, xsource, setup)
        self.pulp_id = None
        if HAS_PULP and xsource.get("pulp_id"):
            self.pulp_id = xsource.get("pulp_id")

            _setup_pulp(self.setup)
            repoapi = RepositoryAPI()
            try:
                self.repo = repoapi.repository(self.pulp_id)
                self.gpgkeys = [os.path.join(PULPCONFIG.cds['keyurl'], key)
                                for key in repoapi.listkeys(self.pulp_id)]
            except server.ServerRequestError:
                err = sys.exc_info()[1]
                if err[0] == 401:
                    msg = "Packages: Error authenticating to Pulp: %s" % err[1]
                elif err[0] == 404:
                    msg = "Packages: Pulp repo id %s not found: %s" % \
                          (self.pulp_id, err[1])
                else:
                    msg = "Packages: Error %d fetching pulp repo %s: %s" % \
                          (err[0], self.pulp_id, err[1])
                raise SourceInitError(msg)
            except socket.error:
                err = sys.exc_info()[1]
                raise SourceInitError("Could not contact Pulp server: %s" %
                                      err)
            except:
                err = sys.exc_info()[1]
                raise SourceInitError("Unknown error querying Pulp server: %s"
                                      % err)
            self.rawurl = "%s/%s" % (PULPCONFIG.cds['baseurl'],
                                     self.repo['relative_path'])
            self.arches = [self.repo['arch']]

        self.packages = dict()
        self.deps = dict([('global', dict())])
        self.provides = dict([('global', dict())])
        self.filemap = dict([(x, dict())
                             for x in ['global'] + self.arches])
        self.needed_paths = set()
        self.file_to_arch = dict()
        self.yumgroups = dict()
    __init__.__doc__ = Source.__init__.__doc__

    @property
    def use_yum(self):
        """ True if we should use the yum Python libraries, False
        otherwise """
        return HAS_YUM and self.setup.cfp.getboolean("packages:yum",
                                                     "use_yum_libraries",
                                                     default=False)

    def save_state(self):
        """ If using the builtin yum parser, save state to
        :attr:`cachefile`.  If using the Python yum libraries, yum
        handles caching and state and this method is a no-op."""
        if not self.use_yum:
            cache = open(self.cachefile, 'wb')
            cPickle.dump((self.packages, self.deps, self.provides,
                          self.filemap, self.url_map,
                          self.yumgroups), cache, 2)
            cache.close()

    def load_state(self):
        """ If using the builtin yum parser, load saved state from
        :attr:`cachefile`.  If using the Python yum libraries, yum
        handles caching and state and this method is a no-op."""
        if not self.use_yum:
            data = open(self.cachefile)
            (self.packages, self.deps, self.provides,
             self.filemap, self.url_map, self.yumgroups) = cPickle.load(data)

    @property
    def urls(self):
        """ A list of URLs to the base metadata file for each
        repository described by this source. """
        rv = []
        for umap in self.url_map:
            rv.extend(self._get_urls_from_repodata(umap['url'], umap['arch']))
        return rv

    def _get_urls_from_repodata(self, url, arch):
        """ When using the builtin yum parser, given the base URL of a
        repository, return the URLs of the various repo metadata files
        needed to get package data from the repo.

        If using the yum Python libraries, this just returns ``url``
        as it was passed in, but should realistically not be called.

        :param url: The base URL to the repository (i.e., the
                    directory that contains the ``repodata/`` directory)
        :type url: string
        :param arch: The architecture of the directory.
        :type arch: string
        :return: list of strings - URLs to metadata files
        """
        if self.use_yum:
            return [url]

        rmdurl = '%srepodata/repomd.xml' % url
        try:
            repomd = fetch_url(rmdurl)
        except ValueError:
            self.logger.error("Packages: Bad url string %s" % rmdurl)
            return []
        except HTTPError:
            err = sys.exc_info()[1]
            self.logger.error("Packages: Failed to fetch url %s. code=%s" %
                              (rmdurl, err.code))
            return []
        except URLError:
            err = sys.exc_info()[1]
            self.logger.error("Packages: Failed to fetch url %s. %s" %
                              (rmdurl, err))
            return []
        try:
            xdata = lxml.etree.XML(repomd)
        except lxml.etree.XMLSyntaxError:
            err = sys.exc_info()[1]
            self.logger.error("Packages: Failed to process metadata at %s: %s"
                              % (rmdurl, err))
            return []

        urls = []
        for elt in xdata.findall(RPO + 'data'):
            if elt.get('type') in ['filelists', 'primary', 'group']:
                floc = elt.find(RPO + 'location')
                fullurl = url + floc.get('href')
                urls.append(fullurl)
                self.file_to_arch[self.escape_url(fullurl)] = arch
        return urls

    @Bcfg2.Server.Plugin.track_statistics()
    def read_files(self):
        """ When using the builtin yum parser, read and parse locally
        downloaded metadata files.  This diverges from the stock
        :func:`Bcfg2.Server.Plugins.Packages.Source.Source.read_files`
        quite a bit. """

        # we have to read primary.xml first, and filelists.xml afterwards;
        primaries = list()
        filelists = list()
        groups = list()
        for fname in self.files:
            if fname.endswith('primary.xml.gz'):
                primaries.append(fname)
            elif fname.endswith('filelists.xml.gz'):
                filelists.append(fname)
            elif fname.find('comps'):
                groups.append(fname)

        for fname in primaries:
            farch = self.file_to_arch[fname]
            fdata = lxml.etree.parse(fname).getroot()
            self.parse_primary(fdata, farch)
        for fname in filelists:
            farch = self.file_to_arch[fname]
            fdata = lxml.etree.parse(fname).getroot()
            self.parse_filelist(fdata, farch)
        for fname in groups:
            fdata = lxml.etree.parse(fname).getroot()
            self.parse_group(fdata)

        # merge data
        sdata = list(self.packages.values())
        try:
            self.packages['global'] = copy.deepcopy(sdata.pop())
        except IndexError:
            self.logger.error("Packages: No packages in repo")
            self.packages['global'] = set()
        while sdata:
            self.packages['global'].update(sdata.pop())

        for key in self.packages:
            if key == 'global':
                continue
            self.packages[key] = \
                self.packages[key].difference(self.packages['global'])
        self.save_state()

    @Bcfg2.Server.Plugin.track_statistics()
    def parse_filelist(self, data, arch):
        """ parse filelists.xml.gz data """
        if arch not in self.filemap:
            self.filemap[arch] = dict()
        for pkg in data.findall(FL + 'package'):
            for fentry in pkg.findall(FL + 'file'):
                if fentry.text in self.needed_paths:
                    if fentry.text in self.filemap[arch]:
                        self.filemap[arch][fentry.text].add(pkg.get('name'))
                    else:
                        self.filemap[arch][fentry.text] = \
                            set([pkg.get('name')])

    @Bcfg2.Server.Plugin.track_statistics()
    def parse_primary(self, data, arch):
        """ parse primary.xml.gz data """
        if arch not in self.packages:
            self.packages[arch] = set()
        if arch not in self.deps:
            self.deps[arch] = dict()
        if arch not in self.provides:
            self.provides[arch] = dict()
        for pkg in data.getchildren():
            if not pkg.tag.endswith('package'):
                continue
            pkgname = pkg.find(XP + 'name').text
            self.packages[arch].add(pkgname)

            pdata = pkg.find(XP + 'format')
            self.deps[arch][pkgname] = set()
            pre = pdata.find(RP + 'requires')
            if pre is not None:
                for entry in pre.getchildren():
                    self.deps[arch][pkgname].add(entry.get('name'))
                    if entry.get('name').startswith('/'):
                        self.needed_paths.add(entry.get('name'))
            pro = pdata.find(RP + 'provides')
            if pro is not None:
                for entry in pro.getchildren():
                    prov = entry.get('name')
                    if prov not in self.provides[arch]:
                        self.provides[arch][prov] = list()
                    self.provides[arch][prov].append(pkgname)

    @Bcfg2.Server.Plugin.track_statistics()
    def parse_group(self, data):
        """ parse comps.xml.gz data """
        for group in data.getchildren():
            if not group.tag.endswith('group'):
                continue
            try:
                groupid = group.xpath('id')[0].text
                self.yumgroups[groupid] = {'mandatory': list(),
                                           'default': list(),
                                           'optional': list(),
                                           'conditional': list()}
            except IndexError:
                continue
            try:
                packagelist = group.xpath('packagelist')[0]
            except IndexError:
                continue
            for pkgreq in packagelist.getchildren():
                pkgtype = pkgreq.get('type', None)
                if pkgtype == 'mandatory':
                    self.yumgroups[groupid]['mandatory'].append(pkgreq.text)
                elif pkgtype == 'default':
                    self.yumgroups[groupid]['default'].append(pkgreq.text)
                elif pkgtype == 'optional':
                    self.yumgroups[groupid]['optional'].append(pkgreq.text)
                elif pkgtype == 'conditional':
                    self.yumgroups[groupid]['conditional'].append(pkgreq.text)

    def is_package(self, metadata, package):
        arch = [a for a in self.arches if a in metadata.groups]
        if not arch:
            return False
        return ((package in self.packages['global'] or
                 package in self.packages[arch[0]]) and
                package not in self.blacklist and
                (len(self.whitelist) == 0 or package in self.whitelist))
    is_package.__doc__ = Source.is_package.__doc__

    def get_vpkgs(self, metadata):
        if self.use_yum:
            return dict()

        rv = Source.get_vpkgs(self, metadata)
        for arch, fmdata in list(self.filemap.items()):
            if arch not in metadata.groups and arch != 'global':
                continue
            for filename, pkgs in list(fmdata.items()):
                rv[filename] = pkgs
        return rv
    get_vpkgs.__doc__ = Source.get_vpkgs.__doc__

    def unknown_filter(self, package):
        """ By default,
        :class:`Bcfg2.Server.Plugins.Packages.Source.Source` filters
        out unknown packages that start with "choice", but that
        doesn't mean anything to Yum or RPM.  Instead, we filter out
        unknown packages that start with "rpmlib", although this is
        likely legacy behavior; that would seem to indicate that a
        package required some RPM feature that isn't provided, which
        is a bad thing.  This should probably go away at some point.

        :param package: The name of a package that was unknown to the
                        backend
        :type package: string
        :returns: bool
        """
        return package.startswith("rpmlib")

    def filter_unknown(self, unknown):
        if self.use_yum:
            filtered = set()
            for unk in unknown:
                try:
                    if self.unknown_filter(unk):
                        filtered.update(unk)
                except AttributeError:
                    try:
                        if self.unknown_filter(unk[0]):
                            filtered.update(unk)
                    except (IndexError, AttributeError):
                        pass
            unknown.difference_update(filtered)
        else:
            Source.filter_unknown(self, unknown)
    filter_unknown.__doc__ = Source.filter_unknown.__doc__

    def setup_data(self, force_update=False):
        if not self.use_yum:
            Source.setup_data(self, force_update=force_update)
    setup_data.__doc__ = \
        "``setup_data`` is only used by the builtin yum parser.  " + \
        Source.setup_data.__doc__

    def get_repo_name(self, url_map):
        """ Try to find a sensible name for a repository.  First use a
        repository's Pulp ID, if it has one; if not, then defer to
        :class:`Bcfg2.Server.Plugins.Packages.Source.Source.get_repo_name`

        :param url_map: A single :attr:`url_map` dict, i.e., any
                        single element of :attr:`url_map`.
        :type url_map: dict
        :returns: string - the name of the repository.
        """
        if self.pulp_id:
            return self.pulp_id
        else:
            return Source.get_repo_name(self, url_map)

    def get_group(self, metadata, group, ptype=None):  # pylint: disable=W0613
        """ Get the list of packages of the given type in a package
        group.

        :param group: The name of the group to query
        :type group: string
        :param ptype: The type of packages to get, for backends that
                      support multiple package types in package groups
                      (e.g., "recommended," "optional," etc.)
        :type ptype: string
        :returns: list of strings - package names
        """
        try:
            yumgroup = self.yumgroups[group]
        except KeyError:
            return []
        packages = yumgroup['conditional'] + yumgroup['mandatory']
        if ptype in ['default', 'optional', 'all']:
            packages += yumgroup['default']
        if ptype in ['optional', 'all']:
            packages += yumgroup['optional']
        return packages

########NEW FILE########
__FILENAME__ = Pkgmgr
'''This module implements a package management scheme for all images'''

import os
import re
import glob
import logging
import lxml.etree
import Bcfg2.Server.Plugin
import Bcfg2.Server.Lint

try:
    set
except NameError:
    # deprecated since python 2.6
    from sets import Set as set

logger = logging.getLogger('Bcfg2.Plugins.Pkgmgr')


class FuzzyDict(dict):
    fuzzy = re.compile('(?P<name>.*):(?P<alist>\S+(,\S+)*)')

    def __getitem__(self, key):
        if isinstance(key, str):
            mdata = self.fuzzy.match(key)
            if mdata:
                return dict.__getitem__(self, mdata.groupdict()['name'])
        else:
            print("got non-string key %s" % str(key))
        return dict.__getitem__(self, key)

    def __contains__(self, key):
        if isinstance(key, str):
            mdata = self.fuzzy.match(key)
            if mdata:
                return dict.__contains__(self, mdata.groupdict()['name'])
        else:
            print("got non-string key %s" % str(key))
        return dict.__contains__(self, key)

    def get(self, key, default=None):
        try:
            return self.__getitem__(key)
        except:
            if default:
                return default
            raise


class PNode(Bcfg2.Server.Plugin.INode):
    """PNode has a list of packages available at a
    particular group intersection.
    """
    splitters = {'rpm': re.compile('^(.*/)?(?P<name>[\w\+\d\.]+(-[\w\+\d\.]+)*)-' + \
                                  '(?P<version>[\w\d\.]+-([\w\d\.]+))\.(?P<arch>\S+)\.rpm$'),
                 'encap': re.compile('^(?P<name>[\w-]+)-(?P<version>[\w\d\.+-]+).encap.*$')}
    ignore = ['Package']

    def Match(self, metadata, data, entry=lxml.etree.Element("None")):
        """Return a dictionary of package mappings."""
        if self.predicate(metadata, entry):
            for key in self.contents:
                try:
                    data[key].update(self.contents[key])
                except:
                    data[key] = FuzzyDict()
                    data[key].update(self.contents[key])
            for child in self.children:
                child.Match(metadata, data)

    def __init__(self, data, pdict, parent=None):
        # copy local attributes to all child nodes if no local attribute exists
        if 'Package' not in pdict:
            pdict['Package'] = set()
        for child in data.getchildren():
            attrs = set(data.attrib.keys()).difference(child.attrib.keys() + ['name'])
            for attr in attrs:
                try:
                    child.set(attr, data.get(attr))
                except:
                    # don't fail on things like comments and other immutable elements
                    pass
        Bcfg2.Server.Plugin.INode.__init__(self, data, pdict, parent)
        if 'Package' not in self.contents:
            self.contents['Package'] = FuzzyDict()
        for pkg in data.findall('./Package'):
            if 'name' in pkg.attrib and pkg.get('name') not in pdict['Package']:
                pdict['Package'].add(pkg.get('name'))
            if pkg.get('name') != None:
                self.contents['Package'][pkg.get('name')] = {}
                if pkg.getchildren():
                    self.contents['Package'][pkg.get('name')]['__children__'] \
                                                                          = pkg.getchildren()
            if 'simplefile' in pkg.attrib:
                pkg.set('url', "%s/%s" % (pkg.get('uri'), pkg.get('simplefile')))
                self.contents['Package'][pkg.get('name')].update(pkg.attrib)
            else:
                if 'file' in pkg.attrib:
                    if 'multiarch' in pkg.attrib:
                        archs = pkg.get('multiarch').split()
                        srcs = pkg.get('srcs', pkg.get('multiarch')).split()
                        url = ' '.join(["%s/%s" % (pkg.get('uri'),
                                                   pkg.get('file') % {'src':srcs[idx],
                                                                      'arch':archs[idx]})
                                        for idx in range(len(archs))])
                        pkg.set('url', url)
                    else:
                        pkg.set('url', '%s/%s' % (pkg.get('uri'),
                                                  pkg.get('file')))
                if pkg.get('type') in self.splitters and pkg.get('file') != None:
                    mdata = self.splitters[pkg.get('type')].match(pkg.get('file'))
                    if not mdata:
                        logger.error("Failed to match pkg %s" % pkg.get('file'))
                        continue
                    pkgname = mdata.group('name')
                    self.contents['Package'][pkgname] = mdata.groupdict()
                    self.contents['Package'][pkgname].update(pkg.attrib)
                    if pkg.attrib.get('file'):
                        self.contents['Package'][pkgname]['url'] = pkg.get('url')
                        self.contents['Package'][pkgname]['type'] = pkg.get('type')
                        if pkg.get('verify'):
                            self.contents['Package'][pkgname]['verify'] = pkg.get('verify')
                        if pkg.get('multiarch'):
                            self.contents['Package'][pkgname]['multiarch'] = pkg.get('multiarch')
                    if pkgname not in pdict['Package']:
                        pdict['Package'].add(pkgname)
                    if pkg.getchildren():
                        self.contents['Package'][pkgname]['__children__'] = pkg.getchildren()
                else:
                    self.contents['Package'][pkg.get('name')].update(pkg.attrib)


class PkgSrc(Bcfg2.Server.Plugin.XMLSrc):
    """PkgSrc files contain a PNode hierarchy that
    returns matching package entries.
    """
    __node__ = PNode
    __cacheobj__ = FuzzyDict


class Pkgmgr(Bcfg2.Server.Plugin.PrioDir):
    """This is a generator that handles package assignments."""
    name = 'Pkgmgr'
    __author__ = 'bcfg-dev@mcs.anl.gov'
    __child__ = PkgSrc
    __element__ = 'Package'

    def HandleEvent(self, event):
        '''Handle events and update dispatch table'''
        Bcfg2.Server.Plugin.XMLDirectoryBacked.HandleEvent(self, event)
        for src in list(self.entries.values()):
            for itype, children in list(src.items.items()):
                for child in children:
                    try:
                        self.Entries[itype][child] = self.BindEntry
                    except KeyError:
                        self.Entries[itype] = FuzzyDict([(child,
                                                          self.BindEntry)])

    def BindEntry(self, entry, metadata):
        """Bind data for entry, and remove instances that are not requested."""
        pname = entry.get('name')
        Bcfg2.Server.Plugin.PrioDir.BindEntry(self, entry, metadata)
        if entry.findall('Instance'):
            mdata = FuzzyDict.fuzzy.match(pname)
            if mdata:
                arches = mdata.group('alist').split(',')
                [entry.remove(inst) for inst in \
                 entry.findall('Instance') \
                 if inst.get('arch') not in arches]

    def HandlesEntry(self, entry, metadata):
        return entry.tag == 'Package' and entry.get('name').split(':')[0] in list(self.Entries['Package'].keys())

    def HandleEntry(self, entry, metadata):
        self.BindEntry(entry, metadata)


class PkgmgrLint(Bcfg2.Server.Lint.ServerlessPlugin):
    """ Find duplicate :ref:`Pkgmgr
    <server-plugins-generators-pkgmgr>` entries with the same
    priority. """

    def Run(self):
        pset = set()
        for pfile in glob.glob(os.path.join(self.config['repo'], 'Pkgmgr',
                                            '*.xml')):
            if self.HandlesFile(pfile):
                xdata = lxml.etree.parse(pfile).getroot()
                # get priority, type, group
                priority = xdata.get('priority')
                ptype = xdata.get('type')
                for pkg in xdata.xpath("//Package"):
                    if pkg.getparent().tag == 'Group':
                        grp = pkg.getparent().get('name')
                        if (type(grp) is not str and
                            grp.getparent().tag == 'Group'):
                            pgrp = grp.getparent().get('name')
                        else:
                            pgrp = 'none'
                    else:
                        grp = 'none'
                        pgrp = 'none'
                    ptuple = (pkg.get('name'), priority, ptype, grp, pgrp)
                    # check if package is already listed with same
                    # priority, type, grp
                    if ptuple in pset:
                        self.LintError(
                            "duplicate-package",
                            "Duplicate Package %s, priority:%s, type:%s" %
                            (pkg.get('name'), priority, ptype))
                    else:
                        pset.add(ptuple)

    @classmethod
    def Errors(cls):
        return {"duplicate-packages": "error"}

########NEW FILE########
__FILENAME__ = POSIXCompat
"""This plugin provides a compatibility layer which turns new-style
POSIX entries into old-style entries.
"""

import Bcfg2.Server.Plugin


class POSIXCompat(Bcfg2.Server.Plugin.Plugin,
                  Bcfg2.Server.Plugin.GoalValidator):
    """POSIXCompat is a goal validator plugin for POSIX entries."""

    create = False

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.GoalValidator.__init__(self)

    def validate_goals(self, metadata, goals):
        """Verify that we are generating correct old POSIX entries."""
        if metadata.version_info and metadata.version_info >= (1, 3, 0, '', 0):
            # do not care about a client that is _any_ 1.3.0 release
            # (including prereleases and RCs)
            return

        for goal in goals:
            for entry in goal.getchildren():
                if entry.tag == 'Path' and 'mode' in entry.keys():
                    entry.set('perms', entry.get('mode'))

########NEW FILE########
__FILENAME__ = Probes
""" A plugin to gather information from a client machine """

import re
import os
import sys
import time
import copy
import operator
import lxml.etree
import Bcfg2.Server
import Bcfg2.Server.Plugin
from Bcfg2.Compat import unicode  # pylint: disable=W0622

try:
    from django.db import models
    from django.core.exceptions import MultipleObjectsReturned
    HAS_DJANGO = True

    class ProbesDataModel(models.Model,
                          Bcfg2.Server.Plugin.PluginDatabaseModel):
        """ The database model for storing probe data """
        hostname = models.CharField(max_length=255)
        probe = models.CharField(max_length=255)
        timestamp = models.DateTimeField(auto_now=True)
        data = models.TextField(null=True)

    class ProbesGroupsModel(models.Model,
                            Bcfg2.Server.Plugin.PluginDatabaseModel):
        """ The database model for storing probe groups """
        hostname = models.CharField(max_length=255)
        group = models.CharField(max_length=255)
except ImportError:
    HAS_DJANGO = False

try:
    import json
    # py2.4 json library is structured differently
    json.loads  # pylint: disable=W0104
    HAS_JSON = True
except (ImportError, AttributeError):
    try:
        import simplejson as json
        HAS_JSON = True
    except ImportError:
        HAS_JSON = False

try:
    import yaml
    HAS_YAML = True
except ImportError:
    HAS_YAML = False


class ClientProbeDataSet(dict):
    """ dict of probe => [probe data] that records a timestamp for
    each host """
    def __init__(self, *args, **kwargs):
        if "timestamp" in kwargs and kwargs['timestamp'] is not None:
            self.timestamp = kwargs.pop("timestamp")
        else:
            self.timestamp = time.time()
        dict.__init__(self, *args, **kwargs)


class ProbeData(str):  # pylint: disable=E0012,R0924
    """ a ProbeData object emulates a str object, but also has .xdata,
    .json, and .yaml properties to provide convenient ways to use
    ProbeData objects as XML, JSON, or YAML data """
    def __new__(cls, data):
        if isinstance(data, unicode):
            return str.__new__(cls, data.encode('utf-8'))
        else:
            return str.__new__(cls, data)

    def __init__(self, data):  # pylint: disable=W0613
        str.__init__(self)
        self._xdata = None
        self._json = None
        self._yaml = None

    @property
    def data(self):
        """ provide backwards compatibility with broken ProbeData
        object in bcfg2 1.2.0 thru 1.2.2 """
        return str(self)

    @property
    def xdata(self):
        """ The probe data as a lxml.etree._Element document """
        if self._xdata is None:
            try:
                self._xdata = lxml.etree.XML(self.data,
                                             parser=Bcfg2.Server.XMLParser)
            except lxml.etree.XMLSyntaxError:
                pass
        return self._xdata

    @property
    def json(self):
        """ The probe data as a decoded JSON data structure """
        if self._json is None and HAS_JSON:
            try:
                self._json = json.loads(self.data)
            except ValueError:
                pass
        return self._json

    @property
    def yaml(self):
        """ The probe data as a decoded YAML data structure """
        if self._yaml is None and HAS_YAML:
            try:
                self._yaml = yaml.load(self.data)
            except yaml.YAMLError:
                pass
        return self._yaml


class ProbeSet(Bcfg2.Server.Plugin.EntrySet):
    """ Handle universal and group- and host-specific probe files """
    ignore = re.compile(r'^(\.#.*|.*~|\..*\.(tmp|sw[px])|probed\.xml)$')
    probename = \
        re.compile(r'(.*/)?(?P<basename>\S+?)(\.(?P<mode>(?:G\d\d)|H)_\S+)?$')
    bangline = re.compile(r'^#!\s*(?P<interpreter>.*)$')
    basename_is_regex = True

    def __init__(self, path, fam, encoding, plugin_name):
        self.plugin_name = plugin_name
        Bcfg2.Server.Plugin.EntrySet.__init__(self, r'[0-9A-Za-z_\-]+', path,
                                              Bcfg2.Server.Plugin.SpecificData,
                                              encoding)
        fam.AddMonitor(path, self)

    def HandleEvent(self, event):
        """ handle events on everything but probed.xml """
        if (event.filename != self.path and
            not event.filename.endswith("probed.xml")):
            return self.handle_event(event)

    def get_probe_data(self, metadata):
        """ Get an XML description of all probes for a client suitable
        for sending to that client.

        :param metadata: The client metadata to get probes for.
        :type metadata: Bcfg2.Server.Plugins.Metadata.ClientMetadata
        :returns: list of lxml.etree._Element objects, each of which
                  represents one probe.
        """
        ret = []
        build = dict()
        candidates = self.get_matching(metadata)
        candidates.sort(key=operator.attrgetter('specific'))
        for entry in candidates:
            rem = self.probename.match(entry.name)
            pname = rem.group('basename')
            if pname not in build:
                build[pname] = entry

        for (name, entry) in list(build.items()):
            probe = lxml.etree.Element('probe')
            probe.set('name', os.path.basename(name))
            probe.set('source', self.plugin_name)
            if (metadata.version_info and
                metadata.version_info > (1, 3, 1, '', 0)):
                try:
                    probe.text = entry.data.decode('utf-8')
                except AttributeError:
                    probe.text = entry.data
            else:
                try:
                    probe.text = entry.data
                except:  # pylint: disable=W0702
                    self.logger.error("Client unable to handle unicode "
                                      "probes. Skipping %s" %
                                      probe.get('name'))
                    continue
            match = self.bangline.match(entry.data.split('\n')[0])
            if match:
                probe.set('interpreter', match.group('interpreter'))
            else:
                probe.set('interpreter', '/bin/sh')
            ret.append(probe)
        return ret

    def __str__(self):
        return "ProbeSet for %s" % self.plugin_name


class Probes(Bcfg2.Server.Plugin.Probing,
             Bcfg2.Server.Plugin.Caching,
             Bcfg2.Server.Plugin.Connector,
             Bcfg2.Server.Plugin.DatabaseBacked):
    """ A plugin to gather information from a client machine """
    __author__ = 'bcfg-dev@mcs.anl.gov'

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Probing.__init__(self)
        Bcfg2.Server.Plugin.Caching.__init__(self)
        Bcfg2.Server.Plugin.Connector.__init__(self)
        Bcfg2.Server.Plugin.DatabaseBacked.__init__(self, core, datastore)

        try:
            self.probes = ProbeSet(self.data, core.fam, core.setup['encoding'],
                                   self.name)
        except:
            err = sys.exc_info()[1]
            raise Bcfg2.Server.Plugin.PluginInitError(err)

        self.allowed_cgroups = core.setup['probe_allowed_groups']
        self.probedata = dict()
        self.cgroups = dict()
        self.load_data()
    __init__.__doc__ = Bcfg2.Server.Plugin.DatabaseBacked.__init__.__doc__

    @Bcfg2.Server.Plugin.track_statistics()
    def write_data(self, client):
        """ Write probe data out for use with bcfg2-info """
        if self._use_db:
            return self._write_data_db(client)
        else:
            return self._write_data_xml(client)

    def _write_data_xml(self, _):
        """ Write received probe data to probed.xml """
        top = lxml.etree.Element("Probed")
        for client, probed in sorted(self.probedata.items()):
            # make a copy of probe data for this client in case it
            # submits probe data while we're trying to write
            # probed.xml
            probedata = copy.copy(probed)
            ctag = \
                lxml.etree.SubElement(top, 'Client', name=client,
                                      timestamp=str(int(probedata.timestamp)))
            for probe in sorted(probedata):
                try:
                    lxml.etree.SubElement(
                        ctag, 'Probe', name=probe,
                        value=str(
                            self.probedata[client][probe]).decode('utf-8'))
                except AttributeError:
                    lxml.etree.SubElement(
                        ctag, 'Probe', name=probe,
                        value=str(self.probedata[client][probe]))
            for group in sorted(self.cgroups[client]):
                lxml.etree.SubElement(ctag, "Group", name=group)
        try:
            top.getroottree().write(os.path.join(self.data, 'probed.xml'),
                                    xml_declaration=False,
                                    pretty_print='true')
        except IOError:
            err = sys.exc_info()[1]
            self.logger.error("Failed to write probed.xml: %s" % err)

    @Bcfg2.Server.Plugin.DatabaseBacked.get_db_lock
    def _write_data_db(self, client):
        """ Write received probe data to the database """
        for probe, data in self.probedata[client.hostname].items():
            try:
                pdata = ProbesDataModel.objects.get_or_create(
                    hostname=client.hostname,
                    probe=probe)[0]
            except MultipleObjectsReturned:
                ProbesDataModel.objects.filter(hostname=client.hostname,
                                               probe=probe).delete()
                ProbesDataModel.objects.get_or_create(
                    hostname=client.hostname,
                    probe=probe)
            if pdata.data != data:
                pdata.data = data
                pdata.save()

        ProbesDataModel.objects.filter(
            hostname=client.hostname).exclude(
            probe__in=self.probedata[client.hostname]).delete()

        for group in self.cgroups[client.hostname]:
            try:
                ProbesGroupsModel.objects.get_or_create(
                    hostname=client.hostname,
                    group=group)
            except MultipleObjectsReturned:
                ProbesGroupsModel.objects.filter(hostname=client.hostname,
                                                 group=group).delete()
                ProbesGroupsModel.objects.get_or_create(
                    hostname=client.hostname,
                    group=group)
        ProbesGroupsModel.objects.filter(
            hostname=client.hostname).exclude(
            group__in=self.cgroups[client.hostname]).delete()

    def expire_cache(self, key=None):
        self.load_data(client=key)

    def load_data(self, client=None):
        """ Load probe data from the appropriate backend (probed.xml
        or the database) """
        if self._use_db:
            return self._load_data_db(client=client)
        else:
            # the XML backend doesn't support loading data for single
            # clients, so it reloads all data
            return self._load_data_xml()

    def _load_data_xml(self):
        """ Load probe data from probed.xml """
        try:
            data = lxml.etree.parse(os.path.join(self.data, 'probed.xml'),
                                    parser=Bcfg2.Server.XMLParser).getroot()
        except (IOError, lxml.etree.XMLSyntaxError):
            err = sys.exc_info()[1]
            self.logger.error("Failed to read file probed.xml: %s" % err)
            return
        self.probedata = {}
        self.cgroups = {}
        for client in data.getchildren():
            self.probedata[client.get('name')] = \
                ClientProbeDataSet(timestamp=client.get("timestamp"))
            self.cgroups[client.get('name')] = []
            for pdata in client:
                if pdata.tag == 'Probe':
                    self.probedata[client.get('name')][pdata.get('name')] = \
                        ProbeData(pdata.get("value"))
                elif pdata.tag == 'Group':
                    self.cgroups[client.get('name')].append(pdata.get('name'))

        if self.core.metadata_cache_mode in ['cautious', 'aggressive']:
            self.core.expire_caches_by_type(Bcfg2.Server.Plugin.Metadata)

    def _load_data_db(self, client=None):
        """ Load probe data from the database """
        if client is None:
            self.probedata = {}
            self.cgroups = {}
            probedata = ProbesDataModel.objects.all()
            groupdata = ProbesGroupsModel.objects.all()
        else:
            self.probedata.pop(client, None)
            self.cgroups.pop(client, None)
            probedata = ProbesDataModel.objects.filter(hostname=client)
            groupdata = ProbesGroupsModel.objects.filter(hostname=client)

        for pdata in probedata:
            if pdata.hostname not in self.probedata:
                self.probedata[pdata.hostname] = ClientProbeDataSet(
                    timestamp=time.mktime(pdata.timestamp.timetuple()))
            self.probedata[pdata.hostname][pdata.probe] = ProbeData(pdata.data)
        for pgroup in groupdata:
            if pgroup.hostname not in self.cgroups:
                self.cgroups[pgroup.hostname] = []
            self.cgroups[pgroup.hostname].append(pgroup.group)

        if self.core.metadata_cache_mode in ['cautious', 'aggressive']:
            self.core.expire_caches_by_type(Bcfg2.Server.Plugin.Metadata,
                                            key=client)

    @Bcfg2.Server.Plugin.track_statistics()
    def GetProbes(self, meta):
        return self.probes.get_probe_data(meta)
    GetProbes.__doc__ = Bcfg2.Server.Plugin.Probing.GetProbes.__doc__

    @Bcfg2.Server.Plugin.track_statistics()
    def ReceiveData(self, client, datalist):
        if self.core.metadata_cache_mode in ['cautious', 'aggressive']:
            if client.hostname in self.cgroups:
                olddata = copy.copy(self.cgroups[client.hostname])
            else:
                olddata = []

        cgroups = []
        cprobedata = ClientProbeDataSet()
        for data in datalist:
            self.ReceiveDataItem(client, data, cgroups, cprobedata)
        self.cgroups[client.hostname] = cgroups
        self.probedata[client.hostname] = cprobedata

        if (self.core.metadata_cache_mode in ['cautious', 'aggressive'] and
            olddata != self.cgroups[client.hostname]):
            self.core.metadata_cache.expire(client.hostname)
        self.write_data(client)
    ReceiveData.__doc__ = Bcfg2.Server.Plugin.Probing.ReceiveData.__doc__

    def ReceiveDataItem(self, client, data, cgroups, cprobedata):
        """Receive probe results pertaining to client."""
        if data.text is None:
            self.logger.info("Got null response to probe %s from %s" %
                             (data.get('name'), client.hostname))
            cprobedata[data.get('name')] = ProbeData('')
            return
        dlines = data.text.split('\n')
        self.logger.debug("Processing probe from %s: %s:%s" %
                          (client.hostname, data.get('name'),
                           [line.strip() for line in dlines]))
        for line in dlines[:]:
            if line.split(':')[0] == 'group':
                newgroup = line.split(':')[1].strip()
                if newgroup not in cgroups:
                    if self._group_allowed(newgroup):
                        cgroups.append(newgroup)
                    else:
                        self.logger.info(
                            "Disallowed group assignment %s from %s" %
                            (newgroup, client.hostname))
                dlines.remove(line)
        dobj = ProbeData("\n".join(dlines))
        cprobedata[data.get('name')] = dobj

    def _group_allowed(self, group):
        """ Determine if the named group can be set as a probe group
        by checking the regexes listed in the [probes] groups_allowed
        setting """
        return any(r.match(group) for r in self.allowed_cgroups)

    def get_additional_groups(self, meta):
        return self.cgroups.get(meta.hostname, list())
    get_additional_groups.__doc__ = \
        Bcfg2.Server.Plugin.Connector.get_additional_groups.__doc__

    def get_additional_data(self, meta):
        return self.probedata.get(meta.hostname, ClientProbeDataSet())
    get_additional_data.__doc__ = \
        Bcfg2.Server.Plugin.Connector.get_additional_data.__doc__

########NEW FILE########
__FILENAME__ = Properties
""" The properties plugin maps property files into client metadata
instances. """

import os
import re
import sys
import copy
import logging
import lxml.etree
import Bcfg2.Server.Plugin
from Bcfg2.Server.Plugin import PluginExecutionError
try:
    import Bcfg2.Encryption
    HAS_CRYPTO = True
except ImportError:
    HAS_CRYPTO = False

try:
    import json
    # py2.4 json library is structured differently
    json.loads  # pylint: disable=W0104
    HAS_JSON = True
except (ImportError, AttributeError):
    try:
        import simplejson as json
        HAS_JSON = True
    except ImportError:
        HAS_JSON = False

try:
    import yaml
    HAS_YAML = True
except ImportError:
    HAS_YAML = False

LOGGER = logging.getLogger(__name__)

SETUP = None


class PropertyFile(object):
    """ Base Properties file handler """

    def __init__(self, name):
        """
        :param name: The filename of this properties file.

        .. automethod:: _write
        """
        self.name = name

    def write(self):
        """ Write the data in this data structure back to the property
        file. This public method performs checking to ensure that
        writing is possible and then calls :func:`_write`. """
        if not SETUP.cfp.getboolean("properties", "writes_enabled",
                                    default=True):
            msg = "Properties files write-back is disabled in the " + \
                "configuration"
            LOGGER.error(msg)
            raise PluginExecutionError(msg)
        try:
            self.validate_data()
        except PluginExecutionError:
            msg = "Cannot write %s: %s" % (self.name, sys.exc_info()[1])
            LOGGER.error(msg)
            raise PluginExecutionError(msg)
        try:
            return self._write()
        except IOError:
            err = sys.exc_info()[1]
            msg = "Failed to write %s: %s" % (self.name, err)
            LOGGER.error(msg)
            raise PluginExecutionError(msg)

    def _write(self):
        """ Write the data in this data structure back to the property
        file. """
        raise NotImplementedError

    def validate_data(self):
        """ Verify that the data in this file is valid. """
        raise NotImplementedError

    def get_additional_data(self, metadata):  # pylint: disable=W0613
        """ Get file data for inclusion in client metadata. """
        return copy.copy(self)


class JSONPropertyFile(Bcfg2.Server.Plugin.FileBacked, PropertyFile):
    """ Handle JSON Properties files. """

    def __init__(self, name, fam=None):
        Bcfg2.Server.Plugin.FileBacked.__init__(self, name, fam=fam)
        PropertyFile.__init__(self, name)
        self.json = None
    __init__.__doc__ = Bcfg2.Server.Plugin.FileBacked.__init__.__doc__

    def Index(self):
        try:
            self.json = json.loads(self.data)
        except ValueError:
            err = sys.exc_info()[1]
            raise PluginExecutionError("Could not load JSON data from %s: %s" %
                                       (self.name, err))
    Index.__doc__ = Bcfg2.Server.Plugin.FileBacked.Index.__doc__

    def _write(self):
        json.dump(self.json, open(self.name, 'wb'))
        return True
    _write.__doc__ = PropertyFile._write.__doc__

    def validate_data(self):
        try:
            json.dumps(self.json)
        except:
            err = sys.exc_info()[1]
            raise PluginExecutionError("Data for %s cannot be dumped to JSON: "
                                       "%s" % (self.name, err))
    validate_data.__doc__ = PropertyFile.validate_data.__doc__

    def __str__(self):
        return str(self.json)

    def __repr__(self):
        return repr(self.json)


class YAMLPropertyFile(Bcfg2.Server.Plugin.FileBacked, PropertyFile):
    """ Handle YAML Properties files. """

    def __init__(self, name, fam=None):
        Bcfg2.Server.Plugin.FileBacked.__init__(self, name, fam=fam)
        PropertyFile.__init__(self, name)
        self.yaml = None
    __init__.__doc__ = Bcfg2.Server.Plugin.FileBacked.__init__.__doc__

    def Index(self):
        try:
            self.yaml = yaml.load(self.data)
        except yaml.YAMLError:
            err = sys.exc_info()[1]
            raise PluginExecutionError("Could not load YAML data from %s: %s" %
                                       (self.name, err))
    Index.__doc__ = Bcfg2.Server.Plugin.FileBacked.Index.__doc__

    def _write(self):
        yaml.dump(self.yaml, open(self.name, 'wb'))
        return True
    _write.__doc__ = PropertyFile._write.__doc__

    def validate_data(self):
        try:
            yaml.dump(self.yaml)
        except yaml.YAMLError:
            err = sys.exc_info()[1]
            raise PluginExecutionError("Data for %s cannot be dumped to YAML: "
                                       "%s" % (self.name, err))
    validate_data.__doc__ = PropertyFile.validate_data.__doc__

    def __str__(self):
        return str(self.yaml)

    def __repr__(self):
        return repr(self.yaml)


class XMLPropertyFile(Bcfg2.Server.Plugin.StructFile, PropertyFile):
    """ Handle XML Properties files. """

    def __init__(self, name, fam=None, should_monitor=False):
        Bcfg2.Server.Plugin.StructFile.__init__(self, name, fam=fam,
                                                should_monitor=should_monitor)
        PropertyFile.__init__(self, name)

    def _write(self):
        open(self.name, "wb").write(
            lxml.etree.tostring(self.xdata,
                                xml_declaration=False,
                                pretty_print=True).decode('UTF-8'))
        return True

    def validate_data(self):
        """ ensure that the data in this object validates against the
        XML schema for this property file (if a schema exists) """
        schemafile = self.name.replace(".xml", ".xsd")
        if os.path.exists(schemafile):
            try:
                schema = lxml.etree.XMLSchema(file=schemafile)
            except lxml.etree.XMLSchemaParseError:
                err = sys.exc_info()[1]
                raise PluginExecutionError("Failed to process schema for %s: "
                                           "%s" % (self.name, err))
        else:
            # no schema exists
            return True

        if not schema.validate(self.xdata):
            raise PluginExecutionError("Data for %s fails to validate; run "
                                       "bcfg2-lint for more details" %
                                       self.name)
        else:
            return True

    def Index(self):
        Bcfg2.Server.Plugin.StructFile.Index(self)
        if HAS_CRYPTO:
            for el in self.xdata.xpath("//*[@encrypted]"):
                try:
                    el.text = self._decrypt(el).encode('ascii',
                                                       'xmlcharrefreplace')
                except UnicodeDecodeError:
                    self.logger.info("Properties: Decrypted %s to gibberish, "
                                     "skipping" % el.tag)
                except (TypeError, Bcfg2.Encryption.EVPError):
                    strict = self.xdata.get(
                        "decrypt",
                        SETUP.cfp.get(Bcfg2.Encryption.CFG_SECTION, "decrypt",
                                      default="strict")) == "strict"
                    msg = "Properties: Failed to decrypt %s element in %s" % \
                          (el.tag, self.name)
                    if strict:
                        raise PluginExecutionError(msg)
                    else:
                        self.logger.debug(msg)

    def _decrypt(self, element):
        """ Decrypt a single encrypted properties file element """
        if not element.text or not element.text.strip():
            return
        passes = Bcfg2.Encryption.get_passphrases(SETUP)
        try:
            passphrase = passes[element.get("encrypted")]
            return Bcfg2.Encryption.ssl_decrypt(
                element.text, passphrase,
                algorithm=Bcfg2.Encryption.get_algorithm(SETUP))
        except KeyError:
            raise Bcfg2.Encryption.EVPError("No passphrase named '%s'" %
                                            element.get("encrypted"))
        raise Bcfg2.Encryption.EVPError("Failed to decrypt")

    def get_additional_data(self, metadata):
        if SETUP.cfp.getboolean("properties", "automatch", default=False):
            default_automatch = "true"
        else:
            default_automatch = "false"

        if self.xdata.get("automatch", default_automatch).lower() == "true":
            return self.XMLMatch(metadata)
        else:
            return copy.copy(self)

    def __str__(self):
        return str(self.xdata)

    def __repr__(self):
        return repr(self.xdata)


class Properties(Bcfg2.Server.Plugin.Plugin,
                 Bcfg2.Server.Plugin.Connector,
                 Bcfg2.Server.Plugin.DirectoryBacked):
    """ The properties plugin maps property files into client metadata
    instances. """

    #: Extensions that are understood by Properties.
    extensions = ["xml"]
    if HAS_JSON:
        extensions.append("json")
    if HAS_YAML:
        extensions.extend(["yaml", "yml"])

    #: Only track and include files whose names and paths match this
    #: regex.  Created on-the-fly based on which libraries are
    #: installed (and thus which data formats are supported).
    #: Candidates are ``.xml`` (always supported), ``.json``,
    #: ``.yaml``, and ``.yml``.
    patterns = re.compile(r'.*\.%s$' % '|'.join(extensions))

    #: Ignore XML schema (``.xsd``) files
    ignore = re.compile(r'.*\.xsd$')

    def __init__(self, core, datastore):
        global SETUP  # pylint: disable=W0603
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.Connector.__init__(self)
        Bcfg2.Server.Plugin.DirectoryBacked.__init__(self, self.data, core.fam)
        SETUP = core.setup

        #: Instead of creating children of this object with a static
        #: object, we use :func:`property_dispatcher` to create a
        #: child of the appropriate subclass of :class:`PropertyFile`
        self.__child__ = self.property_dispatcher
    __init__.__doc__ = Bcfg2.Server.Plugin.Plugin.__init__.__doc__

    def property_dispatcher(self, fname, fam):
        """ Dispatch an event on a Properties file to the
        appropriate object.

        :param fname: The name of the file that received the event
        :type fname: string
        :param fam: The file monitor the event was received by
        :type fam: Bcfg2.Server.FileMonitor.FileMonitor
        :returns: An object of the appropriate subclass of
                  :class:`PropertyFile`
        """
        if fname.endswith(".xml"):
            return XMLPropertyFile(fname, fam)
        elif HAS_JSON and fname.endswith(".json"):
            return JSONPropertyFile(fname, fam)
        elif HAS_YAML and (fname.endswith(".yaml") or fname.endswith(".yml")):
            return YAMLPropertyFile(fname, fam)
        else:
            raise Bcfg2.Server.Plugin.PluginExecutionError(
                "Properties: Unknown extension %s" % fname)

    def get_additional_data(self, metadata):
        rv = dict()
        for fname, pfile in self.entries.items():
            rv[fname] = pfile.get_additional_data(metadata)
        return rv
    get_additional_data.__doc__ = \
        Bcfg2.Server.Plugin.Connector.get_additional_data.__doc__

########NEW FILE########
__FILENAME__ = PuppetENC
""" A plugin to run Puppet external node classifiers """

import os
import sys
import Bcfg2.Server
import Bcfg2.Server.Plugin
from subprocess import Popen, PIPE

try:
    from syck import load as yaml_load, error as yaml_error
except ImportError:
    try:
        from yaml import load as yaml_load, YAMLError as yaml_error
    except ImportError:
        raise ImportError("No yaml library could be found")


class PuppetENCFile(Bcfg2.Server.Plugin.FileBacked):
    """ A representation of a Puppet external node classifier script """

    def HandleEvent(self, event=None):
        return


class PuppetENC(Bcfg2.Server.Plugin.Plugin,
                Bcfg2.Server.Plugin.Connector,
                Bcfg2.Server.Plugin.ClientRunHooks,
                Bcfg2.Server.Plugin.DirectoryBacked):
    """ A plugin to run Puppet external node classifiers
    (http://docs.puppetlabs.com/guides/external_nodes.html) """
    experimental = True
    __child__ = PuppetENCFile

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.Connector.__init__(self)
        Bcfg2.Server.Plugin.ClientRunHooks.__init__(self)
        Bcfg2.Server.Plugin.DirectoryBacked.__init__(self, self.data,
                                                     self.core.fam)
        self.cache = dict()

    def _run_encs(self, metadata):
        """ Run all Puppet ENCs """
        cache = dict(groups=[], params=dict())
        for enc in self.entries.keys():
            epath = os.path.join(self.data, enc)
            self.debug_log("PuppetENC: Running ENC %s for %s" %
                           (enc, metadata.hostname))
            proc = Popen([epath, metadata.hostname], stdin=PIPE, stdout=PIPE,
                         stderr=PIPE)
            (out, err) = proc.communicate()
            rv = proc.wait()
            if rv != 0:
                msg = "PuppetENC: Error running ENC %s for %s (%s): %s" % \
                    (enc, metadata.hostname, rv, err)
                self.logger.error(msg)
                raise Bcfg2.Server.Plugin.PluginExecutionError(msg)
            if err:
                self.debug_log("ENC Error: %s" % err)

            try:
                yaml = yaml_load(out)
                self.debug_log("Loaded data from %s for %s: %s" %
                               (enc, metadata.hostname, yaml))
            except yaml_error:
                err = sys.exc_info()[1]
                msg = "Error decoding YAML from %s for %s: %s" % \
                    (enc, metadata.hostname, err)
                self.logger.error(msg)
                raise Bcfg2.Server.Plugin.PluginExecutionError(msg)

            groups = dict()
            if "classes" in yaml:
                # stock Puppet ENC output format
                groups = yaml['classes']
            elif "groups" in yaml:
                # more Bcfg2-ish output format
                groups = yaml['groups']
            if groups:
                if isinstance(groups, list):
                    self.debug_log("ENC %s adding groups to %s: %s" %
                                   (enc, metadata.hostname, groups))
                    cache['groups'].extend(groups)
                else:
                    self.debug_log("ENC %s adding groups to %s: %s" %
                                   (enc, metadata.hostname, groups.keys()))
                    for group, params in groups.items():
                        cache['groups'].append(group)
                        if params:
                            cache['params'].update(params)
            if "parameters" in yaml and yaml['parameters']:
                cache['params'].update(yaml['parameters'])
            if "environment" in yaml:
                self.logger.info("Ignoring unsupported environment section of "
                                 "ENC %s for %s" % (enc, metadata.hostname))

        self.cache[metadata.hostname] = cache

    def get_additional_groups(self, metadata):
        if metadata.hostname not in self.cache:
            self._run_encs(metadata)
        return self.cache[metadata.hostname]['groups']

    def get_additional_data(self, metadata):
        if metadata.hostname not in self.cache:
            self._run_encs(metadata)
        return self.cache[metadata.hostname]['params']

    def end_client_run(self, metadata):
        """ clear the entire cache at the end of each client run. this
        guarantees that each client will run all ENCs at or near the
        start of each run; we have to clear the entire cache instead
        of just the cache for this client because a client that builds
        templates that use metadata for other clients will populate
        the cache for those clients, which we don't want. This makes
        the caching less than stellar, but it does prevent multiple
        runs of ENCs for a single host a) for groups and data
        separately; and b) when a single client's metadata is
        generated multiple times by separate templates """
        self.cache = dict()
        if self.core.metadata_cache_mode == 'aggressive':
            # clear the metadata client cache if we're in aggressive
            # mode, and produce a warning.  PuppetENC really isn't
            # compatible with aggressive mode, since we don't know
            # when the output from a given ENC has changed, and thus
            # can't invalidate the cache sanely.
            self.logger.warning("PuppetENC is incompatible with aggressive "
                                "client metadata caching, try 'cautious' or "
                                "'initial' instead")
            self.core.expire_caches_by_type(Bcfg2.Server.Plugin.Metadata)

    def end_statistics(self, metadata):
        self.end_client_run(self, metadata)

########NEW FILE########
__FILENAME__ = Reporting
""" Unified statistics and reporting plugin """

import sys
import time
import platform
import traceback
import lxml.etree
from Bcfg2.Reporting.Transport import load_transport_from_config, \
    TransportError
from Bcfg2.Options import REPORTING_COMMON_OPTIONS
from Bcfg2.Server.Plugin import Statistics, PullSource, Threaded, \
    Debuggable, PluginInitError, PluginExecutionError

# required for reporting
try:
    import south  # pylint: disable=W0611
    HAS_SOUTH = True
except ImportError:
    HAS_SOUTH = False


def _rpc_call(method):
    """ Given the name of a Reporting Transport method, get a function
    that defers an XML-RPC call to that method """
    def _real_rpc_call(self, *args, **kwargs):
        """Wrapper for calls to the reporting collector"""
        try:
            return self.transport.rpc(method, *args, **kwargs)
        except TransportError:
            # this is needed for Admin.Pull
            raise PluginExecutionError(sys.exc_info()[1])
    return _real_rpc_call


# pylint: disable=W0223
class Reporting(Statistics, Threaded, PullSource, Debuggable):
    """ Unified statistics and reporting plugin """
    __rmi__ = Debuggable.__rmi__ + ['Ping', 'GetExtra', 'GetCurrentEntry']

    CLIENT_METADATA_FIELDS = ('profile', 'bundles', 'aliases', 'addresses',
                              'groups', 'categories', 'uuid', 'version')

    def __init__(self, core, datastore):
        Statistics.__init__(self, core, datastore)
        PullSource.__init__(self)
        Threaded.__init__(self)
        Debuggable.__init__(self)

        self.whoami = platform.node()
        self.transport = None

        core.setup.update(REPORTING_COMMON_OPTIONS)
        core.setup.reparse()

        if not HAS_SOUTH:
            msg = "Django south is required for Reporting"
            self.logger.error(msg)
            raise PluginInitError(msg)

    def start_threads(self):
        try:
            self.transport = load_transport_from_config(self.core.setup)
        except TransportError:
            msg = "%s: Failed to load transport: %s" % \
                (self.name, traceback.format_exc().splitlines()[-1])
            self.logger.error(msg)
            raise PluginInitError(msg)
        if self.debug_flag:
            self.transport.set_debug(self.debug_flag)

    def set_debug(self, debug):
        rv = Debuggable.set_debug(self, debug)
        if self.transport is not None:
            self.transport.set_debug(debug)
        return rv

    def process_statistics(self, client, xdata):
        stats = xdata.find("Statistics")
        stats.set('time', time.asctime(time.localtime()))

        cdata = {'server': self.whoami}
        for field in self.CLIENT_METADATA_FIELDS:
            try:
                value = getattr(client, field)
            except AttributeError:
                continue
            if value:
                if isinstance(value, set):
                    value = [v for v in value]
                cdata[field] = value

        # try 3 times to store the data
        for i in [1, 2, 3]:
            try:
                self.transport.store(
                    client.hostname, cdata,
                    lxml.etree.tostring(
                        stats,
                        xml_declaration=False))
                self.debug_log("%s: Queued statistics data for %s" %
                               (self.__class__.__name__, client.hostname))
                return
            except TransportError:
                continue
            except:
                self.logger.error("%s: Attempt %s: Failed to add statistic %s"
                                  % (self.__class__.__name__, i,
                                     traceback.format_exc().splitlines()[-1]))
        self.logger.error("%s: Retry limit reached for %s" %
                          (self.__class__.__name__, client.hostname))

    def shutdown(self):
        super(Reporting, self).shutdown()
        if self.transport:
            self.transport.shutdown()

    Ping = _rpc_call('Ping')
    GetExtra = _rpc_call('GetExtra')
    GetCurrentEntry = _rpc_call('GetCurrentEntry')

########NEW FILE########
__FILENAME__ = Rules
"""This generator provides rule-based entry mappings."""

import re
import Bcfg2.Server.Plugin


class Rules(Bcfg2.Server.Plugin.PrioDir):
    """This is a generator that handles service assignments."""
    __author__ = 'bcfg-dev@mcs.anl.gov'

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.PrioDir.__init__(self, core, datastore)
        self._regex_cache = dict()

    def HandlesEntry(self, entry, metadata):
        if entry.tag in self.Entries:
            return self._matches(entry, metadata,
                                 self.Entries[entry.tag].keys())
        return False

    def BindEntry(self, entry, metadata):
        attrs = self.get_attrs(entry, metadata)
        for key, val in list(attrs.items()):
            if key not in entry.attrib:
                entry.attrib[key] = val

    HandleEntry = BindEntry

    def _matches(self, entry, metadata, rules):
        if Bcfg2.Server.Plugin.PrioDir._matches(self, entry, metadata, rules):
            return True
        elif (entry.tag == "Path" and
              ((entry.get('name').endswith("/") and
                entry.get('name').rstrip("/") in rules) or
               (not entry.get('name').endswith("/") and
                entry.get('name') + '/' in rules))):
            # special case for Path tags:
            # http://trac.mcs.anl.gov/projects/bcfg2/ticket/967
            return True
        elif self._regex_enabled:
            # attempt regular expression matching
            for rule in rules:
                if rule not in self._regex_cache:
                    self._regex_cache[rule] = re.compile("%s$" % rule)
                if self._regex_cache[rule].match(entry.get('name')):
                    return True
        return False

    @property
    def _regex_enabled(self):
        """ Return True if rules regexes are enabled, False otherwise """
        return self.core.setup.cfp.getboolean("rules", "regex", default=False)

########NEW FILE########
__FILENAME__ = SEModules
"""
The SEModules plugin handles SELinux module entries.  It supports
group- and host-specific module versions, and enabling/disabling
modules.

You can use ``tools/selinux_baseline.py`` to create a baseline of all
of your installed modules.

See :ref:`server-selinux` for more information.
"""

import os
import Bcfg2.Server.Plugin
from Bcfg2.Compat import b64encode


class SEModuleData(Bcfg2.Server.Plugin.SpecificData):
    """ Representation of a single SELinux module file.  Encodes the
    data using base64 automatically """

    def bind_entry(self, entry, _):
        """ Return a fully-bound entry.  The module data is
        automatically encoded with base64.

        :param entry: The abstract entry to bind the module for
        :type entry: lxml.etree._Element
        :returns: lxml.etree._Element - the fully bound entry
        """
        entry.set('encoding', 'base64')
        entry.text = b64encode(self.data)
        return entry


class SEModules(Bcfg2.Server.Plugin.GroupSpool):
    """ Handle SELinux 'module' entries """
    __author__ = 'chris.a.st.pierre@gmail.com'

    #: SEModules is a :class:`Bcfg2.Server.Plugin.helpers.GroupSpool`
    #: that uses :class:`Bcfg2.Server.Plugins.SEModules.SEModuleData`
    #: objects as its EntrySet children.
    es_child_cls = SEModuleData

    #: SEModules manages ``SEModule`` entries
    entry_type = 'SEModule'

    #: The SEModules plugin is experimental
    experimental = True

    def _get_module_filename(self, entry):
        """ GroupSpool stores entries as /foo.pp, but we want people
        to be able to specify module entries as name='foo' or
        name='foo.pp', so we put this abstraction in between """
        if entry.get("name").endswith(".pp"):
            name = entry.get("name")
        else:
            name = entry.get("name") + ".pp"
        return "/" + name

    def _get_module_name(self, entry):
        """ On the client we do most of our logic on just the module
        name, but we want people to be able to specify module entries
        as name='foo' or name='foo.pp', so we put this abstraction in
        between"""
        if entry.get("name").endswith(".pp"):
            name = entry.get("name")[:-3]
        else:
            name = entry.get("name")
        return name.lstrip("/")

    def HandlesEntry(self, entry, metadata):
        if entry.tag in self.Entries:
            return self._get_module_filename(entry) in self.Entries[entry.tag]
        return Bcfg2.Server.Plugin.GroupSpool.HandlesEntry(self, entry,
                                                           metadata)
    HandlesEntry.__doc__ = Bcfg2.Server.Plugin.GroupSpool.HandlesEntry.__doc__

    def HandleEntry(self, entry, metadata):
        entry.set("name", self._get_module_name(entry))
        bind = self.Entries[entry.tag][self._get_module_filename(entry)]
        return bind(entry, metadata)
    HandleEntry.__doc__ = Bcfg2.Server.Plugin.GroupSpool.HandleEntry.__doc__

    def add_entry(self, event):
        self.filename_pattern = \
            os.path.basename(os.path.dirname(self.event_path(event)))
        Bcfg2.Server.Plugin.GroupSpool.add_entry(self, event)
    add_entry.__doc__ = Bcfg2.Server.Plugin.GroupSpool.add_entry.__doc__

########NEW FILE########
__FILENAME__ = ServiceCompat
""" Use old-style service modes for older clients """

import Bcfg2.Server.Plugin


class ServiceCompat(Bcfg2.Server.Plugin.Plugin,
                    Bcfg2.Server.Plugin.GoalValidator):
    """ Use old-style service modes for older clients """

    create = False

    __author__ = 'bcfg-dev@mcs.anl.gov'
    mode_map = {('true', 'true'): 'default',
                ('interactive', 'true'): 'interactive_only',
                ('false', 'false'): 'manual'}

    def validate_goals(self, metadata, config):
        """ Apply defaults """
        if metadata.version_info and metadata.version_info >= (1, 3, 0, '', 0):
            # do not care about a client that is _any_ 1.3.0 release
            # (including prereleases and RCs)
            return

        for entry in config.xpath("//BoundService|//Service"):
            mode_key = (entry.get("restart", "true").lower(),
                        entry.get("install", "true").lower())
            try:
                mode = self.mode_map[mode_key]
            except KeyError:
                self.logger.info("Could not map restart and install settings "
                                 "of %s:%s to an old-style Service mode for "
                                 "%s; using 'manual'" %
                                 (entry.tag, entry.get("name"),
                                  metadata.hostname))
                mode = "manual"
            entry.set("mode", mode)

########NEW FILE########
__FILENAME__ = Snapshots
import logging
import difflib
import Bcfg2.Server.Plugin
import Bcfg2.Server.Snapshots
import Bcfg2.Logger
from Bcfg2.Server.Snapshots.model import Snapshot
import sys
import time
import threading

# Compatibility import
from Bcfg2.Compat import Queue, u_str, b64decode

logger = logging.getLogger('Snapshots')

ftypes = ['ConfigFile', 'SymLink', 'Directory']
datafields = {
              'Package': ['version'],
              'Path': ['type'],
              'Service': ['status'],
              'ConfigFile': ['owner', 'group', 'mode'],
              'Directory': ['owner', 'group', 'mode'],
              'SymLink': ['to'],
             }


def build_snap_ent(entry):
    basefields = []
    if entry.tag in ['Package', 'Service']:
        basefields += ['type']
    desired = dict([(key, u_str(entry.get(key))) for key in basefields])
    state = dict([(key, u_str(entry.get(key))) for key in basefields])
    desired.update([(key, u_str(entry.get(key))) for key in \
                 datafields[entry.tag]])
    if entry.tag == 'ConfigFile' or \
       ((entry.tag == 'Path') and (entry.get('type') == 'file')):
        if entry.text == None:
            desired['contents'] = None
        else:
            if entry.get('encoding', 'ascii') == 'ascii':
                desired['contents'] = u_str(entry.text)
            else:
                desired['contents'] = u_str(b64decode(entry.text))

        if 'current_bfile' in entry.attrib:
            state['contents'] = u_str(b64decode(entry.get('current_bfile')))
        elif 'current_bdiff' in entry.attrib:
            diff = b64decode(entry.get('current_bdiff'))
            state['contents'] = u_str( \
                '\n'.join(difflib.restore(diff.split('\n'), 1)))

    state.update([(key, u_str(entry.get('current_' + key, entry.get(key)))) \
                  for key in datafields[entry.tag]])
    if entry.tag in ['ConfigFile', 'Path'] and entry.get('exists', 'true') == 'false':
        state = None
    return [desired, state]


class Snapshots(Bcfg2.Server.Plugin.Statistics):
    name = 'Snapshots'
    deprecated = True

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Statistics.__init__(self, core, datastore)
        self.session = Bcfg2.Server.Snapshots.setup_session(core.cfile)
        self.work_queue = Queue()
        self.loader = threading.Thread(target=self.load_snapshot)

    def start_threads(self):
        self.loader.start()

    def load_snapshot(self):
        while self.running:
            try:
                (metadata, data) = self.work_queue.get(block=True, timeout=5)
            except:
                continue
            self.statistics_from_old_stats(metadata, data)

    def process_statistics(self, metadata, data):
        return self.work_queue.put((metadata, data))

    def statistics_from_old_stats(self, metadata, xdata):
        # entries are name -> (modified, correct, start, desired, end)
        # not sure we can get all of this from old format stats
        t1 = time.time()
        entries = dict([('Package', dict()),
                        ('Service', dict()), ('Path', dict())])
        extra = dict([('Package', dict()), ('Service', dict()),
                      ('Path', dict())])
        bad = []
        state = xdata.find('.//Statistics')
        correct = state.get('state') == 'clean'
        revision = u_str(state.get('revision', '-1'))
        for entry in state.find('.//Bad'):
            data = [False, False, u_str(entry.get('name'))] \
                   + build_snap_ent(entry)
            if entry.tag in ftypes:
                etag = 'Path'
            else:
                etag = entry.tag
            entries[etag][entry.get('name')] = data
        for entry in state.find('.//Modified'):
            if entry.tag in ftypes:
                etag = 'Path'
            else:
                etag = entry.tag
            if entry.get('name') in entries[etag]:
                data = [True, False, u_str(entry.get('name'))] + \
                       build_snap_ent(entry)
            else:
                data = [True, False, u_str(entry.get('name'))] + \
                       build_snap_ent(entry)
        for entry in state.find('.//Extra'):
            if entry.tag in datafields:
                data = build_snap_ent(entry)[1]
                ename = u_str(entry.get('name'))
                data['name'] = ename
                extra[entry.tag][ename] = data
            else:
                print("extra", entry.tag, entry.get('name'))
        t2 = time.time()
        snap = Snapshot.from_data(self.session, correct, revision,
                                  metadata, entries, extra)
        self.session.add(snap)
        self.session.commit()
        t3 = time.time()
        logger.info("Snapshot storage took %fs" % (t3 - t2))
        return True

########NEW FILE########
__FILENAME__ = SSHbase
"""This module manages ssh key files for bcfg2"""

import re
import os
import sys
import socket
import shutil
import logging
import tempfile
from itertools import chain
from subprocess import Popen, PIPE
import Bcfg2.Server.Plugin
from Bcfg2.Server.Plugin import PluginExecutionError
from Bcfg2.Compat import any, u_str, b64encode  # pylint: disable=W0622

LOGGER = logging.getLogger(__name__)


class KeyData(Bcfg2.Server.Plugin.SpecificData):
    """ class to handle key data for HostKeyEntrySet """

    def __init__(self, name, specific, encoding):
        Bcfg2.Server.Plugin.SpecificData.__init__(self,
                                                  name,
                                                  specific,
                                                  encoding)
        self.encoding = encoding

    def __lt__(self, other):
        return self.name < other.name

    def bind_entry(self, entry, _):
        """ Bind the entry with the data of this key

        :param entry: The abstract entry to bind.  This will be
                      modified in place.
        :type entry: lxml.etree._Element
        :returns: None
        """
        entry.set('type', 'file')
        if entry.get('encoding') == 'base64':
            entry.text = b64encode(self.data)
        else:
            try:
                entry.text = u_str(self.data, self.encoding)
            except UnicodeDecodeError:
                msg = "Failed to decode %s: %s" % (entry.get('name'),
                                                   sys.exc_info()[1])
                LOGGER.error(msg)
                LOGGER.error("Please verify you are using the proper encoding")
                raise Bcfg2.Server.Plugin.PluginExecutionError(msg)
            except ValueError:
                msg = "Error in specification for %s: %s" % (entry.get('name'),
                                                             sys.exc_info()[1])
                LOGGER.error(msg)
                LOGGER.error("You need to specify base64 encoding for %s" %
                             entry.get('name'))
                raise Bcfg2.Server.Plugin.PluginExecutionError(msg)
        if entry.text in ['', None]:
            entry.set('empty', 'true')


class HostKeyEntrySet(Bcfg2.Server.Plugin.EntrySet):
    """ EntrySet to handle all kinds of host keys """
    def __init__(self, basename, path):
        if basename.startswith("ssh_host_key"):
            encoding = "base64"
        else:
            encoding = None
        Bcfg2.Server.Plugin.EntrySet.__init__(self, basename, path, KeyData,
                                              encoding)
        self.metadata = {'owner': 'root',
                         'group': 'root',
                         'type': 'file'}
        if encoding is not None:
            self.metadata['encoding'] = encoding
        if basename.endswith('.pub'):
            self.metadata['mode'] = '0644'
        else:
            self.metadata['mode'] = '0600'


class KnownHostsEntrySet(Bcfg2.Server.Plugin.EntrySet):
    """ EntrySet to handle the ssh_known_hosts file """
    def __init__(self, path):
        Bcfg2.Server.Plugin.EntrySet.__init__(self, "ssh_known_hosts", path,
                                              KeyData, None)
        self.metadata = {'owner': 'root',
                         'group': 'root',
                         'type': 'file',
                         'mode': '0644'}


class SSHbase(Bcfg2.Server.Plugin.Plugin,
              Bcfg2.Server.Plugin.Caching,
              Bcfg2.Server.Plugin.Generator,
              Bcfg2.Server.Plugin.PullTarget):
    """
       The sshbase generator manages ssh host keys (both v1 and v2)
       for hosts.  It also manages the ssh_known_hosts file. It can
       integrate host keys from other management domains and similarly
       export its keys. The repository contains files in the following
       formats:

       ssh_host_key.H_(hostname) -> the v1 host private key for
         (hostname)
       ssh_host_key.pub.H_(hostname) -> the v1 host public key
         for (hostname)
       ssh_host_(ec)(dr)sa_key.H_(hostname) -> the v2 ssh host
         private key for (hostname)
       ssh_host_(ec)(dr)sa_key.pub.H_(hostname) -> the v2 ssh host
         public key for (hostname)
       ssh_known_hosts -> the current known hosts file. this
         is regenerated each time a new key is generated.

    """
    __author__ = 'bcfg-dev@mcs.anl.gov'
    keypatterns = ["ssh_host_dsa_key",
                   "ssh_host_ecdsa_key",
                   "ssh_host_rsa_key",
                   "ssh_host_key",
                   "ssh_host_dsa_key.pub",
                   "ssh_host_ecdsa_key.pub",
                   "ssh_host_rsa_key.pub",
                   "ssh_host_key.pub"]

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.Caching.__init__(self)
        Bcfg2.Server.Plugin.Generator.__init__(self)
        Bcfg2.Server.Plugin.PullTarget.__init__(self)
        self.ipcache = {}
        self.namecache = {}
        self.__skn = False

        # keep track of which bogus keys we've warned about, and only
        # do so once
        self.badnames = dict()

        core.fam.AddMonitor(self.data, self)

        self.static = dict()
        self.entries = dict()
        self.Entries['Path'] = dict()

        self.entries['/etc/ssh/ssh_known_hosts'] = \
            KnownHostsEntrySet(self.data)
        self.Entries['Path']['/etc/ssh/ssh_known_hosts'] = self.build_skn
        for keypattern in self.keypatterns:
            self.entries["/etc/ssh/" + keypattern] = \
                HostKeyEntrySet(keypattern, self.data)
            self.Entries['Path']["/etc/ssh/" + keypattern] = self.build_hk

    def expire_cache(self, key=None):
        self.__skn = False

    def get_skn(self):
        """Build memory cache of the ssh known hosts file."""
        if not self.__skn:
            # if no metadata is registered yet, defer
            if len(self.core.metadata.query.all()) == 0:
                self.__skn = False
                return self.__skn

            skn = [s.data.rstrip()
                   for s in list(self.static.values())]

            mquery = self.core.metadata.query

            # build hostname cache
            names = dict()
            for cmeta in mquery.all():
                names[cmeta.hostname] = set([cmeta.hostname])
                names[cmeta.hostname].update(cmeta.aliases)
                newnames = set()
                newips = set()
                for name in names[cmeta.hostname]:
                    newnames.add(name.split('.')[0])
                    try:
                        newips.update(self.get_ipcache_entry(name)[0])
                    except:  # pylint: disable=W0702
                        continue
                names[cmeta.hostname].update(newnames)
                names[cmeta.hostname].update(cmeta.addresses)
                names[cmeta.hostname].update(newips)
                # TODO: Only perform reverse lookups on IPs if an
                # option is set.
                if True:
                    for ip in newips:
                        try:
                            names[cmeta.hostname].update(
                                self.get_namecache_entry(ip))
                        except:  # pylint: disable=W0702
                            continue
                names[cmeta.hostname] = sorted(names[cmeta.hostname])

            pubkeys = [pubk for pubk in list(self.entries.keys())
                       if pubk.endswith('.pub')]
            pubkeys.sort()
            for pubkey in pubkeys:
                for entry in sorted(self.entries[pubkey].entries.values(),
                                    key=lambda e: (e.specific.hostname or
                                                   e.specific.group)):
                    specific = entry.specific
                    hostnames = []
                    if specific.hostname and specific.hostname in names:
                        hostnames = names[specific.hostname]
                    elif specific.group:
                        hostnames = list(
                            chain(
                                *[names[cmeta.hostname]
                                  for cmeta in
                                  mquery.by_groups([specific.group])]))
                    elif specific.all:
                        # a generic key for all hosts?  really?
                        hostnames = list(chain(*list(names.values())))
                    if not hostnames:
                        if specific.hostname:
                            key = specific.hostname
                            ktype = "host"
                        elif specific.group:
                            key = specific.group
                            ktype = "group"
                        else:
                            # user has added a global SSH key, but
                            # have no clients yet.  don't warn about
                            # this.
                            continue

                        if key not in self.badnames:
                            self.badnames[key] = True
                            self.logger.info("Ignoring key for unknown %s %s" %
                                             (ktype, key))
                        continue

                    skn.append("%s %s" % (','.join(hostnames),
                                          entry.data.rstrip()))

            self.__skn = "\n".join(skn) + "\n"
        return self.__skn

    def set_skn(self, value):
        """Set backing data for skn."""
        self.__skn = value
    skn = property(get_skn, set_skn)

    def HandleEvent(self, event=None):
        """Local event handler that does skn regen on pubkey change."""
        # skip events we don't care about
        action = event.code2str()
        if action == "endExist" or event.filename == self.data:
            return

        for entry in list(self.entries.values()):
            if entry.specific.match(event.filename):
                entry.handle_event(event)
                if any(event.filename.startswith(kp)
                       for kp in self.keypatterns
                       if kp.endswith(".pub")):
                    self.debug_log("New public key %s; invalidating "
                                   "ssh_known_hosts cache" % event.filename)
                    self.skn = False
                return

        if event.filename in ['info', 'info.xml', ':info']:
            for entry in list(self.entries.values()):
                entry.handle_event(event)
            return

        if event.filename.endswith('.static'):
            self.logger.info("Static key %s %s; invalidating ssh_known_hosts "
                             "cache" % (event.filename, action))
            if action == "deleted" and event.filename in self.static:
                del self.static[event.filename]
                self.skn = False
            else:
                self.static[event.filename] = Bcfg2.Server.Plugin.FileBacked(
                    os.path.join(self.data, event.filename))
                self.static[event.filename].HandleEvent(event)
                self.skn = False
            return

        self.logger.warn("SSHbase: Got unknown event %s %s" %
                         (event.filename, action))

    def get_ipcache_entry(self, client):
        """Build a cache of dns results."""
        if client in self.ipcache:
            if self.ipcache[client]:
                return self.ipcache[client]
            else:
                raise socket.gaierror
        else:
            # need to add entry
            try:
                ipaddr = set([info[4][0]
                              for info in socket.getaddrinfo(client, None)])
                self.ipcache[client] = (ipaddr, client)
                return (ipaddr, client)
            except socket.gaierror:
                ipaddr = Popen(["getent", "hosts", client],
                               stdout=PIPE).stdout.read().strip().split()
                if ipaddr:
                    self.ipcache[client] = (ipaddr, client)
                    return (ipaddr, client)
                self.ipcache[client] = False
                self.logger.error("Failed to find IP address for %s" % client)
                raise socket.gaierror

    def get_namecache_entry(self, cip):
        """Build a cache of name lookups from client IP addresses."""
        if cip in self.namecache:
            # lookup cached name from IP
            if self.namecache[cip]:
                return self.namecache[cip]
            else:
                raise socket.gaierror
        else:
            # add an entry that has not been cached
            try:
                rvlookup = socket.gethostbyaddr(cip)
                if rvlookup[0]:
                    self.namecache[cip] = [rvlookup[0]]
                else:
                    self.namecache[cip] = []
                self.namecache[cip].extend(rvlookup[1])
                return self.namecache[cip]
            except socket.gaierror:
                self.namecache[cip] = False
                self.logger.error("Failed to find any names associated with "
                                  "IP address %s" % cip)
                raise

    def build_skn(self, entry, metadata):
        """This function builds builds a host specific known_hosts file."""
        try:
            self.entries[entry.get('name')].bind_entry(entry, metadata)
        except Bcfg2.Server.Plugin.PluginExecutionError:
            entry.text = self.skn
            hostkeys = []
            for key in self.keypatterns:
                if key.endswith(".pub"):
                    try:
                        hostkeys.append(
                            self.entries["/etc/ssh/" +
                                         key].best_matching(metadata))
                    except Bcfg2.Server.Plugin.PluginExecutionError:
                        pass
            hostkeys.sort()
            for hostkey in hostkeys:
                entry.text += "localhost,localhost.localdomain,127.0.0.1 %s" \
                    % hostkey.data
            self.entries[entry.get('name')].bind_info_to_entry(entry, metadata)

    def build_hk(self, entry, metadata):
        """This binds host key data into entries."""
        try:
            self.entries[entry.get('name')].bind_entry(entry, metadata)
        except Bcfg2.Server.Plugin.PluginExecutionError:
            filename = entry.get('name').split('/')[-1]
            self.GenerateHostKeyPair(metadata.hostname, filename)
            # Service the FAM events queued up by the key generation
            # so the data structure entries will be available for
            # binding.
            #
            # NOTE: We wait for up to ten seconds. There is some
            # potential for race condition, because if the file
            # monitor doesn't get notified about the new key files in
            # time, those entries won't be available for binding. In
            # practice, this seems "good enough".
            tries = 0
            is_bound = False
            while not is_bound:
                if tries >= 10:
                    msg = "%s still not registered" % filename
                    self.logger.error(msg)
                    raise Bcfg2.Server.Plugin.PluginExecutionError(msg)
                self.core.fam.handle_events_in_interval(1)
                tries += 1
                try:
                    self.entries[entry.get('name')].bind_entry(entry, metadata)
                    is_bound = True
                except Bcfg2.Server.Plugin.PluginExecutionError:
                    pass

    def GenerateHostKeyPair(self, client, filename):
        """Generate new host key pair for client."""
        match = re.search(r'(ssh_host_(?:((?:ecd|d|r)sa)_)?key)', filename)
        if match:
            hostkey = "%s.H_%s" % (match.group(1), client)
            if match.group(2):
                keytype = match.group(2)
            else:
                keytype = 'rsa1'
        else:
            raise PluginExecutionError("Unknown key filename: %s" % filename)

        fileloc = os.path.join(self.data, hostkey)
        publoc = os.path.join(self.data,
                              ".".join([hostkey.split('.')[0], 'pub',
                                        "H_%s" % client]))
        tempdir = tempfile.mkdtemp()
        temploc = os.path.join(tempdir, hostkey)
        cmd = ["ssh-keygen", "-q", "-f", temploc, "-N", "",
               "-t", keytype, "-C", "root@%s" % client]
        self.debug_log("SSHbase: Running: %s" % " ".join(cmd))
        proc = Popen(cmd, stdout=PIPE, stdin=PIPE)
        err = proc.communicate()[1]
        if proc.wait():
            raise PluginExecutionError("SSHbase: Error running ssh-keygen: %s"
                                       % err)

        try:
            shutil.copy(temploc, fileloc)
            shutil.copy("%s.pub" % temploc, publoc)
        except IOError:
            err = sys.exc_info()[1]
            raise PluginExecutionError("Temporary SSH keys not found: %s" %
                                       err)

        try:
            os.unlink(temploc)
            os.unlink("%s.pub" % temploc)
            os.rmdir(tempdir)
        except OSError:
            err = sys.exc_info()[1]
            raise PluginExecutionError("Failed to unlink temporary ssh keys: "
                                       "%s" % err)

    def AcceptChoices(self, _, metadata):
        return [Bcfg2.Server.Plugin.Specificity(hostname=metadata.hostname)]

    def AcceptPullData(self, specific, entry, log):
        """Per-plugin bcfg2-admin pull support."""
        # specific will always be host specific
        filename = os.path.join(self.data,
                                "%s.H_%s" % (entry['name'].split('/')[-1],
                                             specific.hostname))
        try:
            open(filename, 'w').write(entry['text'])
            if log:
                print("Wrote file %s" % filename)
        except KeyError:
            self.logger.error("Failed to pull %s. This file does not "
                              "currently exist on the client" %
                              entry.get('name'))

########NEW FILE########
__FILENAME__ = SSLCA
""" The SSLCA generator handles the creation and management of ssl
certificates and their keys. """

import os
import sys
import logging
import tempfile
import lxml.etree
from subprocess import Popen, PIPE, STDOUT
import Bcfg2.Options
import Bcfg2.Server.Plugin
from Bcfg2.Compat import ConfigParser
from Bcfg2.Server.Plugin import PluginExecutionError

LOGGER = logging.getLogger(__name__)


class SSLCAXMLSpec(Bcfg2.Server.Plugin.StructFile):
    """ Base class to handle key.xml and cert.xml """
    attrs = dict()
    tag = None

    def get_spec(self, metadata):
        """ Get a specification for the type of object described by
        this SSLCA XML file for the given client metadata object """
        entries = [e for e in self.Match(metadata) if e.tag == self.tag]
        if len(entries) == 0:
            raise PluginExecutionError("No matching %s entry found for %s "
                                       "in %s" % (self.tag,
                                                  metadata.hostname,
                                                  self.name))
        elif len(entries) > 1:
            LOGGER.warning("More than one matching %s entry found for %s in "
                           "%s; using first match" % (self.tag,
                                                      metadata.hostname,
                                                      self.name))
        rv = dict()
        for attr, default in self.attrs.items():
            val = entries[0].get(attr.lower(), default)
            if default in ['true', 'false']:
                rv[attr] = val == 'true'
            else:
                rv[attr] = val
        return rv


class SSLCAKeySpec(SSLCAXMLSpec):
    """ Handle key.xml files """
    attrs = dict(bits='2048', type='rsa')
    tag = 'Key'


class SSLCACertSpec(SSLCAXMLSpec):
    """ Handle cert.xml files """
    attrs = dict(ca='default',
                 format='pem',
                 key=None,
                 days='365',
                 C=None,
                 L=None,
                 ST=None,
                 OU=None,
                 O=None,
                 emailAddress=None,
                 append_chain='false')
    tag = 'Cert'

    def get_spec(self, metadata):
        rv = SSLCAXMLSpec.get_spec(self, metadata)
        rv['subjectaltname'] = [e.text for e in self.Match(metadata)
                                if e.tag == "subjectAltName"]
        return rv


class SSLCADataFile(Bcfg2.Server.Plugin.SpecificData):
    """ Handle key and cert files """
    def bind_entry(self, entry, _):
        """ Bind the data in the file to the given abstract entry """
        entry.text = self.data
        entry.set("type", "file")
        return entry


class SSLCAEntrySet(Bcfg2.Server.Plugin.EntrySet):
    """ Entry set to handle SSLCA entries and XML files """
    def __init__(self, _, path, entry_type, encoding, parent=None):
        Bcfg2.Server.Plugin.EntrySet.__init__(self, os.path.basename(path),
                                              path, entry_type, encoding)
        self.parent = parent
        self.key = None
        self.cert = None

    def handle_event(self, event):
        action = event.code2str()
        fpath = os.path.join(self.path, event.filename)

        if event.filename == 'key.xml':
            if action in ['exists', 'created', 'changed']:
                self.key = SSLCAKeySpec(fpath)
            self.key.HandleEvent(event)
        elif event.filename == 'cert.xml':
            if action in ['exists', 'created', 'changed']:
                self.cert = SSLCACertSpec(fpath)
            self.cert.HandleEvent(event)
        else:
            Bcfg2.Server.Plugin.EntrySet.handle_event(self, event)

    def build_key(self, entry, metadata):
        """
        either grabs a prexisting key hostfile, or triggers the generation
        of a new key if one doesn't exist.
        """
        # TODO: verify key fits the specs
        filename = "%s.H_%s" % (os.path.basename(entry.get('name')),
                                metadata.hostname)
        self.logger.info("SSLCA: Generating new key %s" % filename)
        key_spec = self.key.get_spec(metadata)
        ktype = key_spec['type']
        bits = key_spec['bits']
        if ktype == 'rsa':
            cmd = ["openssl", "genrsa", bits]
        elif ktype == 'dsa':
            cmd = ["openssl", "dsaparam", "-noout", "-genkey", bits]
        self.debug_log("SSLCA: Generating new key: %s" % " ".join(cmd))
        proc = Popen(cmd, stdout=PIPE, stderr=PIPE)
        key, err = proc.communicate()
        if proc.wait():
            raise PluginExecutionError("SSLCA: Failed to generate key %s for "
                                       "%s: %s" % (entry.get("name"),
                                                   metadata.hostname, err))
        open(os.path.join(self.path, filename), 'w').write(key)
        return key

    def build_cert(self, entry, metadata, keyfile):
        """ generate a new cert """
        filename = "%s.H_%s" % (os.path.basename(entry.get('name')),
                                metadata.hostname)
        self.logger.info("SSLCA: Generating new cert %s" % filename)
        cert_spec = self.cert.get_spec(metadata)
        ca = self.parent.get_ca(cert_spec['ca'])
        req_config = None
        req = None
        try:
            req_config = self.build_req_config(metadata)
            req = self.build_request(keyfile, req_config, metadata)
            days = cert_spec['days']
            cmd = ["openssl", "ca", "-config", ca['config'], "-in", req,
                   "-days", days, "-batch"]
            passphrase = ca.get('passphrase')
            if passphrase:
                cmd.extend(["-passin", "pass:%s" % passphrase])

                def _scrub_pass(arg):
                    """ helper to scrub the passphrase from the
                    argument list """
                    if arg.startswith("pass:"):
                        return "pass:******"
                    else:
                        return arg
            else:
                _scrub_pass = lambda a: a

            self.debug_log("SSLCA: Generating new certificate: %s" %
                           " ".join(_scrub_pass(a) for a in cmd))
            proc = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE)
            (cert, err) = proc.communicate()
            if proc.wait():
                # pylint: disable=E1103
                raise PluginExecutionError("SSLCA: Failed to generate cert: %s"
                                           % err.splitlines()[-1])
                # pylint: enable=E1103
        finally:
            try:
                if req_config and os.path.exists(req_config):
                    os.unlink(req_config)
                if req and os.path.exists(req):
                    os.unlink(req)
            except OSError:
                self.logger.error("SSLCA: Failed to unlink temporary files: %s"
                                  % sys.exc_info()[1])
        if cert_spec['append_chain'] and 'chaincert' in ca:
            cert += open(ca['chaincert']).read()

        open(os.path.join(self.path, filename), 'w').write(cert)
        return cert

    def build_req_config(self, metadata):
        """
        generates a temporary openssl configuration file that is
        used to generate the required certificate request
        """
        # create temp request config file
        fd, fname = tempfile.mkstemp()
        cfp = ConfigParser.ConfigParser({})
        cfp.optionxform = str
        defaults = {
            'req': {
                'default_md': 'sha1',
                'distinguished_name': 'req_distinguished_name',
                'req_extensions': 'v3_req',
                'x509_extensions': 'v3_req',
                'prompt': 'no'
            },
            'req_distinguished_name': {},
            'v3_req': {
                'subjectAltName': '@alt_names'
            },
            'alt_names': {}
        }
        for section in list(defaults.keys()):
            cfp.add_section(section)
            for key in defaults[section]:
                cfp.set(section, key, defaults[section][key])
        cert_spec = self.cert.get_spec(metadata)
        altnamenum = 1
        altnames = cert_spec['subjectaltname']
        altnames.extend(list(metadata.aliases))
        altnames.append(metadata.hostname)
        for altname in altnames:
            cfp.set('alt_names', 'DNS.' + str(altnamenum), altname)
            altnamenum += 1
        for item in ['C', 'L', 'ST', 'O', 'OU', 'emailAddress']:
            if cert_spec[item]:
                cfp.set('req_distinguished_name', item, cert_spec[item])
        cfp.set('req_distinguished_name', 'CN', metadata.hostname)
        self.debug_log("SSLCA: Writing temporary request config to %s" % fname)
        try:
            cfp.write(os.fdopen(fd, 'w'))
        except IOError:
            raise PluginExecutionError("SSLCA: Failed to write temporary CSR "
                                       "config file: %s" % sys.exc_info()[1])
        return fname

    def build_request(self, keyfile, req_config, metadata):
        """
        creates the certificate request
        """
        fd, req = tempfile.mkstemp()
        os.close(fd)
        days = self.cert.get_spec(metadata)['days']
        cmd = ["openssl", "req", "-new", "-config", req_config,
               "-days", days, "-key", keyfile, "-text", "-out", req]
        self.debug_log("SSLCA: Generating new CSR: %s" % " ".join(cmd))
        proc = Popen(cmd, stdout=PIPE, stderr=PIPE)
        err = proc.communicate()[1]
        if proc.wait():
            raise PluginExecutionError("SSLCA: Failed to generate CSR: %s" %
                                       err)
        return req

    def verify_cert(self, filename, keyfile, entry, metadata):
        """ Perform certification verification against the CA and
        against the key """
        ca = self.parent.get_ca(self.cert.get_spec(metadata)['ca'])
        do_verify = ca.get('chaincert')
        if do_verify:
            return (self.verify_cert_against_ca(filename, entry, metadata) and
                    self.verify_cert_against_key(filename, keyfile))
        return True

    def verify_cert_against_ca(self, filename, entry, metadata):
        """
        check that a certificate validates against the ca cert,
        and that it has not expired.
        """
        ca = self.parent.get_ca(self.cert.get_spec(metadata)['ca'])
        chaincert = ca.get('chaincert')
        cert = os.path.join(self.path, filename)
        cmd = ["openssl", "verify"]
        is_root = ca.get('root_ca', "false").lower() == 'true'
        if is_root:
            cmd.append("-CAfile")
        else:
            # verifying based on an intermediate cert
            cmd.extend(["-purpose", "sslserver", "-untrusted"])
        cmd.extend([chaincert, cert])
        self.debug_log("SSLCA: Verifying %s against CA: %s" %
                       (entry.get("name"), " ".join(cmd)))
        res = Popen(cmd, stdout=PIPE, stderr=STDOUT).stdout.read()
        if res == cert + ": OK\n":
            self.debug_log("SSLCA: %s verified successfully against CA" %
                           entry.get("name"))
            return True
        self.logger.warning("SSLCA: %s failed verification against CA: %s" %
                            (entry.get("name"), res))
        return False

    def verify_cert_against_key(self, filename, keyfile):
        """
        check that a certificate validates against its private key.
        """
        def _modulus(fname, ftype="x509"):
            """ get the modulus from the given file """
            cmd = ["openssl", ftype, "-noout", "-modulus", "-in", fname]
            self.debug_log("SSLCA: Getting modulus of %s for verification: %s"
                           % (fname, " ".join(cmd)))
            proc = Popen(cmd, stdout=PIPE, stderr=PIPE)
            rv, err = proc.communicate()
            if proc.wait():
                self.logger.warning("SSLCA: Failed to get modulus of %s: %s" %
                                    (fname, err))
            return rv.strip()  # pylint: disable=E1103

        certfile = os.path.join(self.path, filename)
        cert = _modulus(certfile)
        key = _modulus(keyfile, ftype="rsa")
        if cert == key:
            self.debug_log("SSLCA: %s verified successfully against key %s" %
                           (filename, keyfile))
            return True
        self.logger.warning("SSLCA: %s failed verification against key %s" %
                            (filename, keyfile))
        return False

    def bind_entry(self, entry, metadata):
        if self.key:
            self.bind_info_to_entry(entry, metadata)
            try:
                return self.best_matching(metadata).bind_entry(entry, metadata)
            except PluginExecutionError:
                entry.text = self.build_key(entry, metadata)
                entry.set("type", "file")
                return entry
        elif self.cert:
            key = self.cert.get_spec(metadata)['key']
            cleanup_keyfile = False
            try:
                keyfile = self.parent.entries[key].best_matching(metadata).name
            except PluginExecutionError:
                cleanup_keyfile = True
                # create a temp file with the key in it
                fd, keyfile = tempfile.mkstemp()
                os.chmod(keyfile, 384)  # 0600
                el = lxml.etree.Element('Path', name=key)
                self.parent.core.Bind(el, metadata)
                os.fdopen(fd, 'w').write(el.text)

            try:
                self.bind_info_to_entry(entry, metadata)
                try:
                    best = self.best_matching(metadata)
                    if self.verify_cert(best.name, keyfile, entry, metadata):
                        return best.bind_entry(entry, metadata)
                except PluginExecutionError:
                    pass
                # if we get here, it's because either a) there was no best
                # matching entry; or b) the existing cert did not verify
                entry.text = self.build_cert(entry, metadata, keyfile)
                entry.set("type", "file")
                return entry
            finally:
                if cleanup_keyfile:
                    try:
                        os.unlink(keyfile)
                    except OSError:
                        err = sys.exc_info()[1]
                        self.logger.error("SSLCA: Failed to unlink temporary "
                                          "key %s: %s" % (keyfile, err))


class SSLCA(Bcfg2.Server.Plugin.GroupSpool):
    """ The SSLCA generator handles the creation and management of ssl
    certificates and their keys. """
    __author__ = 'g.hagger@gmail.com'
    # python 2.5 doesn't support mixing *magic and keyword arguments
    es_cls = lambda self, *args: SSLCAEntrySet(*args, **dict(parent=self))
    es_child_cls = SSLCADataFile

    def get_ca(self, name):
        """ get a dict describing a CA from the config file """
        return dict(self.core.setup.cfp.items("sslca_%s" % name))

########NEW FILE########
__FILENAME__ = Statistics
'''This file manages the statistics collected by the BCFG2 Server'''

import copy
import difflib
import logging
import lxml.etree
import os
import sys
from time import asctime, localtime, time, strptime, mktime
import threading
from Bcfg2.Compat import b64decode
import Bcfg2.Server.Plugin


class StatisticsStore(object):
    """Manages the memory and file copy of statistics collected about client runs."""
    __min_write_delay__ = 0

    def __init__(self, filename):
        self.filename = filename
        self.element = lxml.etree.Element('Dummy')
        self.dirty = 0
        self.lastwrite = 0
        self.logger = logging.getLogger('Bcfg2.Server.Statistics')
        self.ReadFromFile()

    def WriteBack(self, force=0):
        """Write statistics changes back to persistent store."""
        if (self.dirty and (self.lastwrite + self.__min_write_delay__ <= time())) \
                or force:
            try:
                fout = open(self.filename + '.new', 'w')
            except IOError:
                ioerr = sys.exc_info()[1]
                self.logger.error("Failed to open %s for writing: %s" % (self.filename + '.new', ioerr))
            else:
                fout.write(lxml.etree.tostring(self.element,
                                               xml_declaration=False).decode('UTF-8'))
                fout.close()
                os.rename(self.filename + '.new', self.filename)
                self.dirty = 0
                self.lastwrite = time()

    def ReadFromFile(self):
        """Reads current state regarding statistics."""
        try:
            fin = open(self.filename, 'r')
            data = fin.read()
            fin.close()
            self.element = lxml.etree.XML(data)
            self.dirty = 0
        except (IOError, lxml.etree.XMLSyntaxError):
            self.logger.error("Creating new statistics file %s"%(self.filename))
            self.element = lxml.etree.Element('ConfigStatistics')
            self.WriteBack()
            self.dirty = 0

    def updateStats(self, xml, client):
        """Updates the statistics of a current node with new data."""

        # Current policy:
        # - Keep anything less than 24 hours old
        #   - Keep latest clean run for clean nodes
        #   - Keep latest clean and dirty run for dirty nodes
        newstat = xml.find('Statistics')

        if newstat.get('state') == 'clean':
            node_dirty = 0
        else:
            node_dirty = 1

        # Find correct node entry in stats data
        # The following list comprehension should be guarenteed to return at
        # most one result
        nodes = [elem for elem in self.element.findall('Node') \
                 if elem.get('name') == client]
        nummatch = len(nodes)
        if nummatch == 0:
            # Create an entry for this node
            node = lxml.etree.SubElement(self.element, 'Node', name=client)
        elif nummatch == 1 and not node_dirty:
            # Delete old instance
            node = nodes[0]
            [node.remove(elem) for elem in node.findall('Statistics') \
             if self.isOlderThan24h(elem.get('time'))]
        elif nummatch == 1 and node_dirty:
            # Delete old dirty statistics entry
            node = nodes[0]
            [node.remove(elem) for elem in node.findall('Statistics') \
             if (elem.get('state') == 'dirty' \
                 and self.isOlderThan24h(elem.get('time')))]
        else:
            # Shouldn't be reached
            self.logger.error("Duplicate node entry for %s"%(client))

        # Set current time for stats
        newstat.set('time', asctime(localtime()))

        # Add statistic
        node.append(copy.copy(newstat))

        # Set dirty
        self.dirty = 1
        self.WriteBack(force=1)

    def isOlderThan24h(self, testTime):
        """Helper function to determine if <time> string is older than 24 hours."""
        now = time()
        utime = mktime(strptime(testTime))
        secondsPerDay = 60*60*24

        return (now-utime) > secondsPerDay


class Statistics(Bcfg2.Server.Plugin.ThreadedStatistics,
                 Bcfg2.Server.Plugin.PullSource):
    name = 'Statistics'
    deprecated = True

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.ThreadedStatistics.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.PullSource.__init__(self)
        fpath = "%s/etc/statistics.xml" % datastore
        self.data_file = StatisticsStore(fpath)

    def handle_statistic(self, metadata, data):
        self.data_file.updateStats(data, metadata.hostname)

    def FindCurrent(self, client):
        rt = self.data_file.element.xpath('//Node[@name="%s"]' % client)[0]
        maxtime = max([strptime(stat.get('time')) for stat \
                       in rt.findall('Statistics')])
        return [stat for stat in rt.findall('Statistics') \
                if strptime(stat.get('time')) == maxtime][0]

    def GetExtra(self, client):
        return [(entry.tag, entry.get('name')) for entry \
                in self.FindCurrent(client).xpath('.//Extra/*')]

    def GetCurrentEntry(self, client, e_type, e_name):
        curr = self.FindCurrent(client)
        entry = curr.xpath('.//Bad/%s[@name="%s"]' % (e_type, e_name))
        if not entry:
            raise Bcfg2.Server.Plugin.PluginExecutionError
        cfentry = entry[-1]

        owner = cfentry.get('current_owner', cfentry.get('owner'))
        group = cfentry.get('current_group', cfentry.get('group'))
        mode = cfentry.get('current_mode', cfentry.get('mode'))
        if cfentry.get('sensitive') in ['true', 'True']:
            raise Bcfg2.Server.Plugin.PluginExecutionError
        elif 'current_bfile' in cfentry.attrib:
            contents = b64decode(cfentry.get('current_bfile'))
        elif 'current_bdiff' in cfentry.attrib:
            diff = b64decode(cfentry.get('current_bdiff'))
            contents = '\n'.join(difflib.restore(diff.split('\n'), 1))
        else:
            contents = None

        return (owner, group, mode, contents)

########NEW FILE########
__FILENAME__ = Svn
""" The Svn plugin provides a revision interface for Bcfg2 repos using
Subversion. If PySvn libraries are installed, then it exposes two
additional XML-RPC methods for committing data to the repository and
updating the repository. """

import sys
import Bcfg2.Server.Plugin
from Bcfg2.Compat import ConfigParser
try:
    import pysvn
    HAS_SVN = True
except ImportError:
    import pipes
    from subprocess import Popen, PIPE
    HAS_SVN = False


class Svn(Bcfg2.Server.Plugin.Version):
    """Svn is a version plugin for dealing with Bcfg2 repos."""
    __author__ = 'bcfg-dev@mcs.anl.gov'
    __vcs_metadata_path__ = ".svn"
    if HAS_SVN:
        __rmi__ = Bcfg2.Server.Plugin.Version.__rmi__ + ['Update', 'Commit']
    else:
        __vcs_metadata_path__ = ".svn"

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Version.__init__(self, core, datastore)

        self.revision = None
        self.svn_root = None
        if not HAS_SVN:
            self.logger.debug("Svn: PySvn not found, using CLI interface to "
                              "SVN")
            self.client = None
        else:
            self.client = pysvn.Client()
            # pylint: disable=E1101
            choice = pysvn.wc_conflict_choice.postpone
            try:
                resolution = self.core.setup.cfp.get(
                    "svn",
                    "conflict_resolution").replace('-', '_')
                if resolution in ["edit", "launch", "working"]:
                    self.logger.warning("Svn: Conflict resolver %s requires "
                                        "manual intervention, using %s" %
                                        choice)
                else:
                    choice = getattr(pysvn.wc_conflict_choice, resolution)
            except AttributeError:
                self.logger.warning("Svn: Conflict resolver %s does not "
                                    "exist, using %s" % choice)
            except (ConfigParser.NoSectionError, ConfigParser.NoOptionError):
                self.logger.info("Svn: No conflict resolution method "
                                 "selected, using %s" % choice)
            # pylint: enable=E1101
            self.debug_log("Svn: Conflicts will be resolved with %s" %
                           choice)
            self.client.callback_conflict_resolver = \
                self.get_conflict_resolver(choice)

            try:
                if self.core.setup.cfp.get(
                        "svn",
                        "always_trust").lower() == "true":
                    self.client.callback_ssl_server_trust_prompt = \
                        self.ssl_server_trust_prompt
            except (ConfigParser.NoSectionError, ConfigParser.NoOptionError):
                self.logger.debug("Svn: Using subversion cache for SSL "
                                  "certificate trust")

            try:
                if (self.core.setup.cfp.get("svn", "user") and
                    self.core.setup.cfp.get("svn", "password")):
                    self.client.callback_get_login = \
                        self.get_login
            except (ConfigParser.NoSectionError, ConfigParser.NoOptionError):
                self.logger.info("Svn: Using subversion cache for "
                                 "password-based authetication")

        self.logger.debug("Svn: Initialized svn plugin with SVN directory %s" %
                          self.vcs_path)

    # pylint: disable=W0613
    def get_login(self, realm, username, may_save):
        """ PySvn callback to get credentials for HTTP basic authentication """
        self.logger.debug("Svn: Logging in with username: %s" %
                          self.core.setup.cfp.get("svn", "user"))
        return True, \
            self.core.setup.cfp.get("svn", "user"), \
            self.core.setup.cfp.get("svn", "password"), \
            False
    # pylint: enable=W0613

    def ssl_server_trust_prompt(self, trust_dict):
        """ PySvn callback to always trust SSL certificates from SVN server """
        self.logger.debug("Svn: Trusting SSL certificate from %s, "
                          "issued by %s for realm %s" %
                          (trust_dict['hostname'],
                           trust_dict['issuer_dname'],
                           trust_dict['realm']))
        return True, trust_dict['failures'], False

    def get_conflict_resolver(self, choice):
        """ Get a PySvn conflict resolution callback """
        def callback(conflict_description):
            """ PySvn callback function to resolve conflicts """
            self.logger.info("Svn: Resolving conflict for %s with %s" %
                             (conflict_description['path'], choice))
            return choice, None, False

        return callback

    def get_revision(self):
        """Read svn revision information for the Bcfg2 repository."""
        msg = None
        if HAS_SVN:
            try:
                info = self.client.info(self.vcs_root)
                self.revision = info.revision
                self.svn_root = info.url
                return str(self.revision.number)
            except pysvn.ClientError:  # pylint: disable=E1101
                msg = "Svn: Failed to get revision: %s" % sys.exc_info()[1]
        else:
            try:
                data = Popen("env LC_ALL=C svn info %s" %
                             pipes.quote(self.vcs_root), shell=True,
                             stdout=PIPE).communicate()[0].split('\n')
                return [line.split(': ')[1] for line in data
                        if line[:9] == 'Revision:'][-1]
            except IndexError:
                msg = "Failed to read svn info"
                self.logger.error('Ran command "svn info %s"' % self.vcs_root)
        self.revision = None
        raise Bcfg2.Server.Plugin.PluginExecutionError(msg)

    def Update(self):
        '''Svn.Update() => True|False\nUpdate svn working copy\n'''
        try:
            old_revision = self.revision.number
            self.revision = self.client.update(self.vcs_root, recurse=True)[0]
        except pysvn.ClientError:  # pylint: disable=E1101
            err = sys.exc_info()[1]
            # try to be smart about the error we got back
            details = None
            if "callback_ssl_server_trust_prompt" in str(err):
                details = "SVN server certificate is not trusted"
            elif "callback_get_login" in str(err):
                details = "SVN credentials not cached"

            if details is None:
                self.logger.error("Svn: Failed to update server repository",
                                  exc_info=1)
            else:
                self.logger.error("Svn: Failed to update server repository: "
                                  "%s" % details)
            return False

        if old_revision == self.revision.number:
            self.logger.debug("repository is current")
        else:
            self.logger.info("Updated %s from revision %s to %s" %
                             (self.vcs_root, old_revision,
                              self.revision.number))
        return True

    def Commit(self):
        """Svn.Commit() => True|False\nCommit svn repository\n"""
        # First try to update
        if not self.Update():
            self.logger.error("Failed to update svn repository, refusing to "
                              "commit changes")
            return False

        try:
            self.revision = self.client.checkin([self.vcs_root],
                                                'Svn: autocommit',
                                                recurse=True)
            self.revision = self.client.update(self.vcs_root, recurse=True)[0]
            self.logger.info("Svn: Commited changes. At %s" %
                             self.revision.number)
            return True
        except pysvn.ClientError:  # pylint: disable=E1101
            err = sys.exc_info()[1]
            # try to be smart about the error we got back
            details = None
            if "callback_ssl_server_trust_prompt" in str(err):
                details = "SVN server certificate is not trusted"
            elif "callback_get_login" in str(err):
                details = "SVN credentials not cached"

            if details is None:
                self.logger.error("Svn: Failed to commit changes",
                                  exc_info=1)
            else:
                self.logger.error("Svn: Failed to commit changes: %s" %
                                  details)
            return False

########NEW FILE########
__FILENAME__ = TCheetah
'''This module implements a templating generator based on Cheetah'''

import logging
import sys
import traceback
import Bcfg2.Server.Plugin

from Bcfg2.Compat import unicode, b64encode

logger = logging.getLogger('Bcfg2.Plugins.TCheetah')

try:
    import Cheetah.Template
    import Cheetah.Parser
except:
    logger.error("TCheetah: Failed to import Cheetah. Is it installed?")
    raise


class TemplateFile:
    """Template file creates Cheetah template structures for the loaded file."""

    def __init__(self, name, specific, encoding):
        self.name = name
        self.specific = specific
        self.encoding = encoding
        self.template = None
        self.searchlist = dict()

    def handle_event(self, event):
        """Handle all fs events for this template."""
        if event.code2str() == 'deleted':
            return
        try:
            s = {'useStackFrames': False}
            self.template = Cheetah.Template.Template(open(self.name).read(),
                                                      compilerSettings=s,
                                                      searchList=self.searchlist)
        except Cheetah.Parser.ParseError:
            perror = sys.exc_info()[1]
            logger.error("Cheetah parse error for file %s" % (self.name))
            logger.error(perror.report())

    def bind_entry(self, entry, metadata):
        """Build literal file information."""
        self.template.metadata = metadata
        self.searchlist['metadata'] = metadata
        self.template.path = entry.get('realname', entry.get('name'))
        self.searchlist['path'] = entry.get('realname', entry.get('name'))
        self.template.source_path = self.name
        self.searchlist['source_path'] = self.name

        if entry.tag == 'Path':
            entry.set('type', 'file')
        try:
            if type(self.template) == unicode:
                entry.text = self.template
            else:
                if entry.get('encoding') == 'base64':
                    # take care of case where file needs base64 encoding
                    entry.text = b64encode(self.template)
                else:
                    entry.text = unicode(str(self.template), self.encoding)
        except:
            (a, b, c) = sys.exc_info()
            msg = traceback.format_exception(a, b, c, limit=2)[-1][:-1]
            logger.error(msg)
            logger.error("TCheetah template error for %s" % self.searchlist['path'])
            del a, b, c
            raise Bcfg2.Server.Plugin.PluginExecutionError


class TCheetah(Bcfg2.Server.Plugin.GroupSpool):
    """The TCheetah generator implements a templating mechanism for configuration files."""
    name = 'TCheetah'
    __author__ = 'bcfg-dev@mcs.anl.gov'
    filename_pattern = 'template'
    es_child_cls = TemplateFile
    deprecated = True

########NEW FILE########
__FILENAME__ = TemplateHelper
""" A plugin to provide helper classes and functions to templates """

import re
import imp
import sys
import logging
import Bcfg2.Server.Lint
import Bcfg2.Server.Plugin

LOGGER = logging.getLogger(__name__)

MODULE_RE = re.compile(r'(?P<filename>(?P<module>[^\/]+)\.py)$')


def safe_module_name(module):
    """ Munge the name of a TemplateHelper module to avoid collisions
    with other Python modules.  E.g., if someone has a helper named
    'ldap.py', it should not be added to ``sys.modules`` as ``ldap``,
    but rather as something more obscure. """
    return '__TemplateHelper_%s' % module


class HelperModule(object):
    """ Representation of a TemplateHelper module """

    def __init__(self, name, fam=None):
        self.name = name
        self.fam = fam

        #: The name of the module as used by get_additional_data().
        #: the name of the file with .py stripped off.
        self._module_name = MODULE_RE.search(self.name).group('module')

        #: The attributes exported by this module
        self._attrs = []

    def HandleEvent(self, event=None):
        """ HandleEvent is called whenever the FAM registers an event.

        :param event: The event object
        :type event: Bcfg2.Server.FileMonitor.Event
        :returns: None
        """
        if event and event.code2str() not in ['exists', 'changed', 'created']:
            return

        try:
            module = imp.load_source(safe_module_name(self._module_name),
                                     self.name)
        except:  # pylint: disable=W0702
            err = sys.exc_info()[1]
            LOGGER.error("TemplateHelper: Failed to import %s: %s" %
                         (self.name, err))
            return

        if not hasattr(module, "__export__"):
            LOGGER.error("TemplateHelper: %s has no __export__ list" %
                         self.name)
            return

        newattrs = []
        for sym in module.__export__:
            if sym not in self._attrs and hasattr(self, sym):
                LOGGER.warning("TemplateHelper: %s: %s is a reserved keyword, "
                               "skipping export" % (self.name, sym))
                continue
            try:
                setattr(self, sym, getattr(module, sym))
                newattrs.append(sym)
            except AttributeError:
                LOGGER.warning("TemplateHelper: %s exports %s, but has no "
                               "such attribute" % (self.name, sym))
        # remove old exports
        for sym in set(self._attrs) - set(newattrs):
            delattr(self, sym)

        self._attrs = newattrs


class TemplateHelper(Bcfg2.Server.Plugin.Plugin,
                     Bcfg2.Server.Plugin.Connector,
                     Bcfg2.Server.Plugin.DirectoryBacked):
    """ A plugin to provide helper classes and functions to templates """
    __author__ = 'chris.a.st.pierre@gmail.com'
    ignore = re.compile(r'^(\.#.*|.*~|\..*\.(sw[px])|.*\.py[co])$')
    patterns = MODULE_RE
    __child__ = HelperModule

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.Connector.__init__(self)
        Bcfg2.Server.Plugin.DirectoryBacked.__init__(self, self.data, core.fam)

    def get_additional_data(self, _):
        return dict([(h._module_name, h)  # pylint: disable=W0212
                     for h in self.entries.values()])


class TemplateHelperLint(Bcfg2.Server.Lint.ServerPlugin):
    """ ``bcfg2-lint`` plugin to ensure that all :ref:`TemplateHelper
    <server-plugins-connectors-templatehelper>` modules are valid.
    This can check for:

    * A TemplateHelper module that cannot be imported due to syntax or
      other compile-time errors;
    * A TemplateHelper module that does not have an ``__export__``
      attribute, or whose ``__export__`` is not a list;
    * Bogus symbols listed in ``__export__``, including symbols that
      don't exist, that are reserved, or that start with underscores.
    """

    def __init__(self, *args, **kwargs):
        Bcfg2.Server.Lint.ServerPlugin.__init__(self, *args, **kwargs)
        self.reserved_keywords = dir(HelperModule("foo.py"))

    def Run(self):
        for helper in self.core.plugins['TemplateHelper'].entries.values():
            if self.HandlesFile(helper.name):
                self.check_helper(helper.name)

    def check_helper(self, helper):
        """ Check a single helper module.

        :param helper: The filename of the helper module
        :type helper: string
        """
        module_name = MODULE_RE.search(helper).group(1)

        try:
            module = imp.load_source(safe_module_name(module_name), helper)
        except:  # pylint: disable=W0702
            err = sys.exc_info()[1]
            self.LintError("templatehelper-import-error",
                           "Failed to import %s: %s" %
                           (helper, err))
            return

        if not hasattr(module, "__export__"):
            self.LintError("templatehelper-no-export",
                           "%s has no __export__ list" % helper)
            return
        elif not isinstance(module.__export__, list):
            self.LintError("templatehelper-nonlist-export",
                           "__export__ is not a list in %s" % helper)
            return

        for sym in module.__export__:
            if not hasattr(module, sym):
                self.LintError("templatehelper-nonexistent-export",
                               "%s: exported symbol %s does not exist" %
                               (helper, sym))
            elif sym in self.reserved_keywords:
                self.LintError("templatehelper-reserved-export",
                               "%s: exported symbol %s is reserved" %
                               (helper, sym))
            elif sym.startswith("_"):
                self.LintError("templatehelper-underscore-export",
                               "%s: exported symbol %s starts with underscore"
                               % (helper, sym))

    @classmethod
    def Errors(cls):
        return {"templatehelper-import-error": "error",
                "templatehelper-no-export": "error",
                "templatehelper-nonlist-export": "error",
                "templatehelper-nonexistent-export": "error",
                "templatehelper-reserved-export": "error",
                "templatehelper-underscore-export": "warning"}

########NEW FILE########
__FILENAME__ = TGenshi
"""This module implements a templating generator based on Genshi."""

import logging
import sys
import Bcfg2.Server.Plugin

from Bcfg2.Compat import unicode, b64encode

logger = logging.getLogger('Bcfg2.Plugins.TGenshi')

# try to import genshi stuff
try:
    import genshi.core
    import genshi.input
    from genshi.template import TemplateLoader, \
                                TextTemplate, MarkupTemplate, TemplateError
except ImportError:
    logger.error("TGenshi: Failed to import Genshi. Is it installed?")
    raise
try:
    from genshi.template import NewTextTemplate
    have_ntt = True
except:
    have_ntt = False

def removecomment(stream):
    """A genshi filter that removes comments from the stream."""
    for kind, data, pos in stream:
        if kind is genshi.core.COMMENT:
            continue
        yield kind, data, pos


class TemplateFile(object):
    """Template file creates Genshi template structures for the loaded file."""

    def __init__(self, name, specific, encoding):
        self.name = name
        self.specific = specific
        self.encoding = encoding
        if self.specific.all:
            matchname = self.name
        elif self.specific.group:
            matchname = self.name[:self.name.find('.G')]
        else:
            matchname = self.name[:self.name.find('.H')]
        if matchname.endswith('.txt'):
            self.template_cls = TextTemplate
        elif matchname.endswith('.newtxt'):
            if not have_ntt:
                logger.error("Genshi NewTextTemplates not supported by this version of Genshi")
            else:
                self.template_cls = NewTextTemplate
        else:
            self.template_cls = MarkupTemplate
        self.HandleEvent = self.handle_event

    def handle_event(self, event=None):
        """Handle all fs events for this template."""
        if event and event.code2str() == 'deleted':
            return
        try:
            loader = TemplateLoader()
            try:
                self.template = loader.load(self.name, cls=self.template_cls,
                                            encoding=self.encoding)
            except LookupError:
                lerror = sys.exc_info()[1]
                logger.error('Genshi lookup error: %s' % lerror)
        except TemplateError:
            terror = sys.exc_info()[1]
            logger.error('Genshi template error: %s' % terror)
        except genshi.input.ParseError:
            perror = sys.exc_info()[1]
            logger.error('Genshi parse error: %s' % perror)

    def bind_entry(self, entry, metadata):
        """Build literal file information."""
        fname = entry.get('realname', entry.get('name'))
        if entry.tag == 'Path':
            entry.set('type', 'file')
        try:
            stream = self.template.generate( \
                name=fname, metadata=metadata,
                path=self.name).filter(removecomment)
            if have_ntt:
                ttypes = [TextTemplate, NewTextTemplate]
            else:
                ttypes = [TextTemplate]
            if True in [isinstance(self.template, t) for t in ttypes]:
                try:
                    textdata = stream.render('text', strip_whitespace=False)
                except TypeError:
                    textdata = stream.render('text')
                if type(textdata) == unicode:
                    entry.text = textdata
                else:
                    if entry.get('encoding') == 'base64':
                        # take care of case where file needs base64 encoding
                        entry.text = b64encode(textdata)
                    else:
                        entry.text = unicode(textdata, self.encoding)
            else:
                try:
                    xmldata = stream.render('xml', strip_whitespace=False)
                except TypeError:
                    xmldata = stream.render('xml')
                if type(xmldata) == unicode:
                    entry.text = xmldata
                else:
                    entry.text = unicode(xmldata, self.encoding)
            if entry.text == '':
                entry.set('empty', 'true')
        except TemplateError:
            err = sys.exc_info()[1]
            logger.exception('Genshi template error')
            raise Bcfg2.Server.Plugin.PluginExecutionError('Genshi template error: %s' % err)
        except AttributeError:
            err = sys.exc_info()[1]
            logger.exception('Genshi template loading error')
            raise Bcfg2.Server.Plugin.PluginExecutionError('Genshi template loading error: %s' % err)


class TemplateEntrySet(Bcfg2.Server.Plugin.EntrySet):
    basename_is_regex = True


class TGenshi(Bcfg2.Server.Plugin.GroupSpool):
    """
    The TGenshi generator implements a templating
    mechanism for configuration files.

    """
    name = 'TGenshi'
    __author__ = 'jeff@ocjtech.us'
    filename_pattern = 'template\.(txt|newtxt|xml)'
    es_cls = TemplateEntrySet
    es_child_cls = TemplateFile
    deprecated = True

########NEW FILE########
__FILENAME__ = Trigger
""" Trigger is a plugin that calls external scripts (on the server) """

import os
import pipes
import Bcfg2.Server.Plugin
from subprocess import Popen, PIPE


class TriggerFile(Bcfg2.Server.Plugin.FileBacked):
    """ Representation of a trigger script file """

    def HandleEvent(self, event=None):
        return

    def __str__(self):
        return "%s: %s" % (self.__class__.__name__, self.name)


class Trigger(Bcfg2.Server.Plugin.Plugin,
              Bcfg2.Server.Plugin.ClientRunHooks,
              Bcfg2.Server.Plugin.DirectoryBacked):
    """Trigger is a plugin that calls external scripts (on the server)."""
    __author__ = 'bcfg-dev@mcs.anl.gov'

    def __init__(self, core, datastore):
        Bcfg2.Server.Plugin.Plugin.__init__(self, core, datastore)
        Bcfg2.Server.Plugin.ClientRunHooks.__init__(self)
        Bcfg2.Server.Plugin.DirectoryBacked.__init__(self, self.data,
                                                     self.core.fam)

    def async_run(self, args):
        """ Run the trigger script asynchronously in a forked process
        """
        pid = os.fork()
        if pid:
            os.waitpid(pid, 0)
        else:
            dpid = os.fork()
            if not dpid:
                self.debug_log("Running %s" % " ".join(pipes.quote(a)
                                                       for a in args))
                proc = Popen(args, stdin=PIPE, stdout=PIPE, stderr=PIPE)
                err = proc.communicate()[1]
                rv = proc.wait()
                if rv != 0:
                    self.logger.error("Trigger: Error running %s (%s): %s" %
                                      (args[0], rv, err))
                elif err:
                    self.debug_log("Trigger: Error: %s" % err)
            os._exit(0)  # pylint: disable=W0212

    def end_client_run(self, metadata):
        args = [metadata.hostname, '-p', metadata.profile, '-g',
                ':'.join([g for g in metadata.groups])]
        for notifier in self.entries.keys():
            npath = os.path.join(self.data, notifier)
            self.async_run([npath] + args)

########NEW FILE########
__FILENAME__ = models
"""Django models for Bcfg2 reports."""
import sys

from django.core.exceptions import ImproperlyConfigured
try:
    from django.db import models
except ImproperlyConfigured:
    e = sys.exc_info()[1]
    print("Reports: unable to import django models: %s" % e)
    sys.exit(1)

from django.db import connection
from django.db.models import Q
from datetime import datetime, timedelta
from time import strptime

from Bcfg2.Reporting.Compat import transaction

KIND_CHOICES = (
    #These are the kinds of config elements
    ('Package', 'Package'),
    ('Path', 'directory'),
    ('Path', 'file'),
    ('Path', 'permissions'),
    ('Path', 'symlink'),
    ('Service', 'Service'),
)
TYPE_GOOD = 0
TYPE_BAD = 1
TYPE_MODIFIED = 2
TYPE_EXTRA = 3

TYPE_CHOICES = (
    (TYPE_GOOD, 'Good'),
    (TYPE_BAD, 'Bad'),
    (TYPE_MODIFIED, 'Modified'),
    (TYPE_EXTRA, 'Extra'),
)


def convert_entry_type_to_id(type_name):
    """Convert a entry type to its entry id"""
    for e_id, e_name in TYPE_CHOICES:
        if e_name.lower() == type_name.lower():
            return e_id
    return -1


class ClientManager(models.Manager):
    """Extended client manager functions."""
    def active(self, timestamp=None):
        """returns a set of clients that have been created and have not
        yet been expired as of optional timestmamp argument. Timestamp
        should be a datetime object."""

        if timestamp == None:
            timestamp = datetime.now()
        elif not isinstance(timestamp, datetime):
            raise ValueError('Expected a datetime object')
        else:
            try:
                timestamp = datetime(*strptime(timestamp,
                                               "%Y-%m-%d %H:%M:%S")[0:6])
            except ValueError:
                return self.none()

        return self.filter(Q(expiration__gt=timestamp) | Q(expiration__isnull=True),
                           creation__lt=timestamp)


class Client(models.Model):
    """Object representing every client we have seen stats for."""
    creation = models.DateTimeField(auto_now_add=True)
    name = models.CharField(max_length=128,)
    current_interaction = models.ForeignKey('Interaction',
                                            null=True, blank=True,
                                            related_name="parent_client")
    expiration = models.DateTimeField(blank=True, null=True)

    def __str__(self):
        return self.name

    objects = ClientManager()

    class Admin:
        pass


class InteractiveManager(models.Manager):
    """Manages interactions objects."""

    def interaction_per_client(self, maxdate=None, active_only=True):
        """
        Returns the most recent interactions for clients as of a date

        Arguments:
        maxdate -- datetime object.  Most recent date to pull. (dafault None)
        active_only -- Include only active clients (default True)

        """

        if maxdate and not isinstance(maxdate, datetime):
            raise ValueError('Expected a datetime object')
        return self.filter(id__in=self.get_interaction_per_client_ids(maxdate, active_only))

    def get_interaction_per_client_ids(self, maxdate=None, active_only=True):
        """
        Returns the ids of most recent interactions for clients as of a date.

        Arguments:
        maxdate -- datetime object.  Most recent date to pull. (dafault None)
        active_only -- Include only active clients (default True)

        """
        from django.db import connection
        cursor = connection.cursor()
        cfilter = "expiration is null"

        sql = 'select reports_interaction.id, x.client_id from (select client_id, MAX(timestamp) ' + \
                    'as timer from reports_interaction'
        if maxdate:
            if not isinstance(maxdate, datetime):
                raise ValueError('Expected a datetime object')
            sql = sql + " where timestamp <= '%s' " % maxdate
            cfilter = "(expiration is null or expiration > '%s') and creation <= '%s'" % (maxdate, maxdate)
        sql = sql + ' GROUP BY client_id) x, reports_interaction where ' + \
                    'reports_interaction.client_id = x.client_id AND reports_interaction.timestamp = x.timer'
        if active_only:
            sql = sql + " and x.client_id in (select id from reports_client where %s)" % \
                cfilter
        try:
            cursor.execute(sql)
            return [item[0] for item in cursor.fetchall()]
        except:
            '''FIXME - really need some error handling'''
            pass
        return []


class Interaction(models.Model):
    """Models each reconfiguration operation interaction between client and server."""
    client = models.ForeignKey(Client, related_name="interactions")
    timestamp = models.DateTimeField(db_index=True)  # Timestamp for this record
    state = models.CharField(max_length=32)  # good/bad/modified/etc
    repo_rev_code = models.CharField(max_length=64)  # repo revision at time of interaction
    goodcount = models.IntegerField()  # of good config-items
    totalcount = models.IntegerField()  # of total config-items
    server = models.CharField(max_length=256)  # Name of the server used for the interaction
    bad_entries = models.IntegerField(default=-1)
    modified_entries = models.IntegerField(default=-1)
    extra_entries = models.IntegerField(default=-1)

    def __str__(self):
        return "With " + self.client.name + " @ " + self.timestamp.isoformat()

    def percentgood(self):
        if not self.totalcount == 0:
            return (self.goodcount / float(self.totalcount)) * 100
        else:
            return 0

    def percentbad(self):
        if not self.totalcount == 0:
            return ((self.totalcount - self.goodcount) / (float(self.totalcount))) * 100
        else:
            return 0

    def isclean(self):
        if (self.bad_entry_count() == 0 and self.goodcount == self.totalcount):
            return True
        else:
            return False

    def isstale(self):
        if (self == self.client.current_interaction):  # Is Mostrecent
            if(datetime.now() - self.timestamp > timedelta(hours=25)):
                return True
            else:
                return False
        else:
            #Search for subsequent Interaction for this client
            #Check if it happened more than 25 hrs ago.
            if (self.client.interactions.filter(timestamp__gt=self.timestamp)
                    .order_by('timestamp')[0].timestamp -
                    self.timestamp > timedelta(hours=25)):
                return True
            else:
                return False

    def save(self):
        super(Interaction, self).save()  # call the real save...
        self.client.current_interaction = self.client.interactions.latest()
        self.client.save()  # save again post update

    def delete(self):
        '''Override the default delete.  Allows us to remove Performance items'''
        pitems = list(self.performance_items.all())
        super(Interaction, self).delete()
        for perf in pitems:
            if perf.interaction.count() == 0:
                perf.delete()

    def badcount(self):
        return self.totalcount - self.goodcount

    def bad(self):
        return Entries_interactions.objects.select_related().filter(interaction=self, type=TYPE_BAD)

    def bad_entry_count(self):
        """Number of bad entries.  Store the count in the interation field to save db queries."""
        if self.bad_entries < 0:
            self.bad_entries = Entries_interactions.objects.filter(interaction=self, type=TYPE_BAD).count()
            self.save()
        return self.bad_entries

    def modified(self):
        return Entries_interactions.objects.select_related().filter(interaction=self, type=TYPE_MODIFIED)

    def modified_entry_count(self):
        """Number of modified entries.  Store the count in the interation field to save db queries."""
        if self.modified_entries < 0:
            self.modified_entries = Entries_interactions.objects.filter(interaction=self, type=TYPE_MODIFIED).count()
            self.save()
        return self.modified_entries

    def extra(self):
        return Entries_interactions.objects.select_related().filter(interaction=self, type=TYPE_EXTRA)

    def extra_entry_count(self):
        """Number of extra entries.  Store the count in the interation field to save db queries."""
        if self.extra_entries < 0:
            self.extra_entries = Entries_interactions.objects.filter(interaction=self, type=TYPE_EXTRA).count()
            self.save()
        return self.extra_entries

    objects = InteractiveManager()

    class Admin:
        list_display = ('client', 'timestamp', 'state')
        list_filter = ['client', 'timestamp']
        pass

    class Meta:
        get_latest_by = 'timestamp'
        ordering = ['-timestamp']
        unique_together = ("client", "timestamp")


class Reason(models.Model):
    """reason why modified or bad entry did not verify, or changed."""
    owner = models.CharField(max_length=255, blank=True)
    current_owner = models.CharField(max_length=255, blank=True)
    group = models.CharField(max_length=255, blank=True)
    current_group = models.CharField(max_length=255, blank=True)
    perms = models.CharField(max_length=4, blank=True)
    current_perms = models.CharField(max_length=4, blank=True)
    status = models.CharField(max_length=128, blank=True)
    current_status = models.CharField(max_length=128, blank=True)
    to = models.CharField(max_length=1024, blank=True)
    current_to = models.CharField(max_length=1024, blank=True)
    version = models.CharField(max_length=1024, blank=True)
    current_version = models.CharField(max_length=1024, blank=True)
    current_exists = models.BooleanField()  # False means its missing. Default True
    current_diff = models.TextField(max_length=1024*1024, blank=True)
    is_binary = models.BooleanField(default=False)
    is_sensitive = models.BooleanField(default=False)
    unpruned = models.TextField(max_length=4096, blank=True, default='')

    def _str_(self):
        return "Reason"

    def short_list(self):
        rv = []
        if self.current_owner or self.current_group or self.current_perms:
            rv.append("File permissions")
        if self.current_status:
            rv.append("Incorrect status")
        if self.current_to:
            rv.append("Incorrect target")
        if self.current_version or self.version == 'auto':
            rv.append("Wrong version")
        if not self.current_exists:
            rv.append("Missing")
        if self.current_diff or self.is_sensitive:
            rv.append("Incorrect data")
        if self.unpruned:
            rv.append("Directory has extra files")
        if len(rv) == 0:
            rv.append("Exists")
        return rv

    @staticmethod
    @transaction.atomic
    def prune_orphans():
        '''Prune oprhaned rows... no good way to use the ORM'''
        cursor = connection.cursor()
        cursor.execute('delete from reports_reason where not exists (select rei.id from reports_entries_interactions rei where rei.reason_id = reports_reason.id)')
        transaction.set_dirty()


class Entries(models.Model):
    """Contains all the entries feed by the client."""
    name = models.CharField(max_length=128, db_index=True)
    kind = models.CharField(max_length=16, choices=KIND_CHOICES, db_index=True)

    def __str__(self):
        return self.name

    @staticmethod
    @transaction.atomic
    def prune_orphans():
        '''Prune oprhaned rows... no good way to use the ORM'''
        cursor = connection.cursor()
        cursor.execute('delete from reports_entries where not exists (select rei.id from reports_entries_interactions rei where rei.entry_id = reports_entries.id)')
        transaction.set_dirty()

    class Meta:
        unique_together = ("name", "kind")


class Entries_interactions(models.Model):
    """Define the relation between the reason, the interaction and the entry."""
    entry = models.ForeignKey(Entries)
    reason = models.ForeignKey(Reason)
    interaction = models.ForeignKey(Interaction)
    type = models.IntegerField(choices=TYPE_CHOICES)


class Performance(models.Model):
    """Object representing performance data for any interaction."""
    interaction = models.ManyToManyField(Interaction, related_name="performance_items")
    metric = models.CharField(max_length=128)
    value = models.DecimalField(max_digits=32, decimal_places=16)

    def __str__(self):
        return self.metric

    @staticmethod
    @transaction.atomic
    def prune_orphans():
        '''Prune oprhaned rows... no good way to use the ORM'''
        cursor = connection.cursor()
        cursor.execute('delete from reports_performance where not exists (select ri.id from reports_performance_interaction ri where ri.performance_id = reports_performance.id)')
        transaction.set_dirty()


class Group(models.Model):
    """
    Groups extracted from interactions

    name - The group name

    TODO - Most of this is for future use
    TODO - set a default group
    """

    name = models.CharField(max_length=255, unique=True)
    profile = models.BooleanField(default=False)
    public = models.BooleanField(default=False)
    category = models.CharField(max_length=1024, blank=True)
    comment = models.TextField(blank=True)

    groups = models.ManyToManyField("self", symmetrical=False)
    bundles = models.ManyToManyField("Bundle")

    def __unicode__(self):
        return self.name


class Bundle(models.Model):
    """
    Bundles extracted from interactions

    name - The bundle name
    """

    name = models.CharField(max_length=255, unique=True)

    def __unicode__(self):
        return self.name


class InteractionMetadata(models.Model):
    """
    InteractionMetadata

    Hold extra data associated with the client and interaction
    """

    interaction = models.OneToOneField(Interaction, primary_key=True, related_name='metadata')
    profile = models.ForeignKey(Group, related_name="+")
    groups = models.ManyToManyField(Group)
    bundles = models.ManyToManyField(Bundle)



########NEW FILE########
__FILENAME__ = updatefix
import Bcfg2.settings

from django.db import connection
import django.core.management
import sys
import logging
import traceback
from Bcfg2.Server.models import InternalDatabaseVersion
logger = logging.getLogger('Bcfg2.Server.Reports.UpdateFix')


# all update function should go here
def _merge_database_table_entries():
    cursor = connection.cursor()
    insert_cursor = connection.cursor()
    find_cursor = connection.cursor()
    cursor.execute("""
    Select name, kind from reports_bad
    union 
    select name, kind from reports_modified
    union 
    select name, kind from reports_extra
    """)
    # this fetch could be better done
    entries_map = {}
    for row in cursor.fetchall():
        insert_cursor.execute("insert into reports_entries (name, kind) \
            values (%s, %s)", (row[0], row[1]))
        entries_map[(row[0], row[1])] = insert_cursor.lastrowid

    cursor.execute("""
        Select name, kind, reason_id, interaction_id, 1 from reports_bad
        inner join reports_bad_interactions on reports_bad.id=reports_bad_interactions.bad_id
        union
        Select name, kind, reason_id, interaction_id, 2 from reports_modified
        inner join reports_modified_interactions on reports_modified.id=reports_modified_interactions.modified_id
        union
        Select name, kind, reason_id, interaction_id, 3 from reports_extra
        inner join reports_extra_interactions on reports_extra.id=reports_extra_interactions.extra_id
    """)
    for row in cursor.fetchall():
        key = (row[0], row[1])
        if entries_map.get(key, None):
            entry_id = entries_map[key]
        else:
            find_cursor.execute("Select id from reports_entries where name=%s and kind=%s", key)
            rowe = find_cursor.fetchone()
            entry_id = rowe[0]
        insert_cursor.execute("insert into reports_entries_interactions \
            (entry_id, interaction_id, reason_id, type) values (%s, %s, %s, %s)", (entry_id, row[3], row[2], row[4]))


def _interactions_constraint_or_idx():
    '''sqlite doesn't support alter tables.. or constraints'''
    cursor = connection.cursor()
    try:
        cursor.execute('alter table reports_interaction add constraint reports_interaction_20100601 unique (client_id,timestamp)')
    except:
        cursor.execute('create unique index reports_interaction_20100601 on reports_interaction (client_id,timestamp)')


def _populate_interaction_entry_counts():
    '''Populate up the type totals for the interaction table'''
    cursor = connection.cursor()
    count_field = {1: 'bad_entries',
                   2: 'modified_entries',
                   3: 'extra_entries'}

    for type in list(count_field.keys()):
        cursor.execute("select count(type), interaction_id " +
                "from reports_entries_interactions where type = %s group by interaction_id" % type)
        updates = []
        for row in cursor.fetchall():
            updates.append(row)
        try:
            cursor.executemany("update reports_interaction set " + count_field[type] + "=%s where id = %s", updates)
        except Exception:
            e = sys.exc_info()[1]
            print(e)
    cursor.close()


def update_noop():
    return True

# be sure to test your upgrade query before reflecting the change in the models
# the list of function and sql command to do should go here
_fixes = [_merge_database_table_entries,
          # this will remove unused tables
          "drop table reports_bad;",
          "drop table reports_bad_interactions;",
          "drop table reports_extra;",
          "drop table reports_extra_interactions;",
          "drop table reports_modified;",
          "drop table reports_modified_interactions;",
          "drop table reports_repository;",
          "drop table reports_metadata;",
          "alter table reports_interaction add server varchar(256) not null default 'N/A';",
          # fix revision data type to support $VCS hashes
          "alter table reports_interaction add repo_rev_code varchar(64) default '';",
          # Performance enhancements for large sites
          'alter table reports_interaction add column bad_entries integer not null default -1;',
          'alter table reports_interaction add column modified_entries integer not null default -1;',
          'alter table reports_interaction add column extra_entries integer not null default -1;',
          _populate_interaction_entry_counts,
          _interactions_constraint_or_idx,
          'alter table reports_reason add is_binary bool NOT NULL default False;',
          'alter table reports_reason add is_sensitive bool NOT NULL default False;',
          update_noop, #_remove_table_column('reports_interaction', 'client_version'),
          "alter table reports_reason add unpruned varchar(1280) not null default 'N/A';",
]

# this will calculate the last possible version of the database
lastversion = len(_fixes)


def rollupdate(current_version):
    """ function responsible to coordinates all the updates
    need current_version as integer
    """
    ret = None
    if current_version < lastversion:
        for i in range(current_version, lastversion):
            try:
                if type(_fixes[i]) == str:
                    connection.cursor().execute(_fixes[i])
                else:
                    _fixes[i]()
            except:
                logger.error("Failed to perform db update %s" % (_fixes[i]), exc_info=1)
            # since array start at 0 but version start at 1 we add 1 to the normal count
            ret = InternalDatabaseVersion.objects.create(version=i + 1)
        return ret
    else:
        return None


def update_database():
    ''' methode to search where we are in the revision of the database models and update them '''
    try:
        logger.debug("Running upgrade of models to the new one")
        django.core.management.call_command("syncdb", interactive=False, verbosity=0)
        know_version = InternalDatabaseVersion.objects.order_by('-version')
        if not know_version:
            logger.debug("No version, creating initial version")
            know_version = InternalDatabaseVersion.objects.create(version=lastversion)
        else:
            know_version = know_version[0]
        logger.debug("Presently at %s" % know_version)
        if know_version.version > 13000:
            # SchemaUpdater stuff
            return
        elif know_version.version < lastversion:
            new_version = rollupdate(know_version.version)
            if new_version:
                logger.debug("upgraded to %s" % new_version)
    except:
        logger.error("Error while updating the database")
        for x in traceback.format_exc().splitlines():
            logger.error(x)

########NEW FILE########
__FILENAME__ = model
import sys
from sqlalchemy import Table, Column, Integer, Unicode, ForeignKey, Boolean, \
                       DateTime, UnicodeText, desc
import datetime
import sqlalchemy.exceptions
from sqlalchemy.orm import relation, backref
from sqlalchemy.ext.declarative import declarative_base

from Bcfg2.Compat import u_str


class Uniquer(object):
    force_rt = True

    @classmethod
    def by_value(cls, session, **kwargs):
        if cls.force_rt:
            try:
                return session.query(cls).filter_by(**kwargs).one()
            except sqlalchemy.exceptions.InvalidRequestError:
                return cls(**kwargs)
        else:
            return cls(**kwargs)

    @classmethod
    def from_record(cls, session, data):
        return cls.by_value(session, **data)

Base = declarative_base()


class Administrator(Uniquer, Base):
    __tablename__ = 'administrator'
    id = Column(Integer, primary_key=True)
    name = Column(Unicode(20), unique=True)
    email = Column(Unicode(64))

admin_client = Table('admin_client', Base.metadata,
                     Column('admin_id',
                            Integer,
                            ForeignKey('administrator.id')),
                     Column('client_id',
                            Integer,
                            ForeignKey('client.id')))

admin_group = Table('admin_group', Base.metadata,
                    Column('admin_id',
                           Integer,
                           ForeignKey('administrator.id')),
                    Column('group_id',
                           Integer,
                           ForeignKey('group.id')))


class Client(Uniquer, Base):
    __tablename__ = 'client'
    id = Column(Integer, primary_key=True)
    name = Column(Unicode(64), unique=True)
    admins = relation("Administrator", secondary=admin_client,
                      backref='clients')
    active = Column(Boolean, default=True)
    online = Column(Boolean, default=True)
    online_ts = Column(DateTime)


class Group(Uniquer, Base):
    __tablename__ = 'group'
    id = Column(Integer, primary_key=True)
    name = Column(Unicode(32), unique=True)
    admins = relation("Administrator", secondary=admin_group,
                      backref='groups')


class ConnectorKeyVal(Uniquer, Base):
    __tablename__ = 'connkeyval'
    id = Column(Integer, primary_key=True)
    connector = Column(Unicode(16))
    key = Column(Unicode(32))
    value = Column(UnicodeText)

meta_group = Table('meta_group', Base.metadata,
                   Column('metadata_id',
                          Integer,
                          ForeignKey('metadata.id')),
                   Column('group_id',
                          Integer,
                          ForeignKey('group.id')))

meta_conn = Table('meta_conn', Base.metadata,
                  Column('metadata_id',
                         Integer,
                         ForeignKey('metadata.id')),
                  Column('connkeyval_id',
                         Integer,
                         ForeignKey('connkeyval.id')))


class Metadata(Base):
    __tablename__ = 'metadata'
    id = Column(Integer, primary_key=True)
    client_id = Column(Integer, ForeignKey('client.id'))
    client = relation(Client)
    groups = relation("Group", secondary=meta_group)
    keyvals = relation(ConnectorKeyVal, secondary=meta_conn)
    timestamp = Column(DateTime)

    @classmethod
    def from_metadata(cls, mysession, mymetadata):
        client = Client.by_value(mysession, name=u_str(mymetadata.hostname))
        m = cls(client=client)
        for group in mymetadata.groups:
            m.groups.append(Group.by_value(mysession, name=u_str(group)))
        for connector in mymetadata.connectors:
            data = getattr(mymetadata, connector)
            if not isinstance(data, dict):
                continue
            for key, value in list(data.items()):
                if not isinstance(value, str):
                    continue
                m.keyvals.append(ConnectorKeyVal.by_value(mysession,
                                                          connector=u_str(connector),
                                                          key=u_str(key),
                                                          value=u_str(value)))
        return m


class Package(Base, Uniquer):
    __tablename__ = 'package'
    id = Column(Integer, primary_key=True)
    name = Column(Unicode(24))
    type = Column(Unicode(16))
    version = Column(Unicode(16))
    verification_status = Column(Boolean)


class CorrespondenceType(object):
    mtype = Package

    @classmethod
    def from_record(cls, mysession, record):
        (mod, corr, name, s_dict, e_dict) = record
        if not s_dict:
            start = None
        else:
            start = cls.mtype.by_value(mysession, name=name, **s_dict)
        if s_dict != e_dict:
            end = cls.mtype.by_value(mysession, name=name, **e_dict)
        else:
            end = start
        return cls(start=start, end=end, modified=mod, correct=corr)


class PackageCorrespondence(Base, CorrespondenceType):
    mtype = Package
    __tablename__ = 'package_pair'
    id = Column(Integer, primary_key=True)
    start_id = Column(Integer, ForeignKey('package.id'))
    start = relation(Package, primaryjoin=start_id == Package.id)
    end_id = Column(Integer, ForeignKey('package.id'), nullable=True)
    end = relation(Package, primaryjoin=end_id == Package.id)
    modified = Column(Boolean)
    correct = Column(Boolean)

package_snap = Table('package_snap', Base.metadata,
                     Column('ppair_id',
                            Integer,
                            ForeignKey('package_pair.id')),
                     Column('snapshot_id',
                            Integer,
                            ForeignKey('snapshot.id')))


class Service(Base, Uniquer):
    __tablename__ = 'service'
    id = Column(Integer, primary_key=True)
    name = Column(Unicode(16))
    type = Column(Unicode(12))
    status = Column(Boolean)


class ServiceCorrespondence(Base, CorrespondenceType):
    mtype = Service
    __tablename__ = 'service_pair'
    id = Column(Integer, primary_key=True)
    start_id = Column(Integer, ForeignKey('service.id'))
    start = relation(Service, primaryjoin=start_id == Service.id)
    end_id = Column(Integer, ForeignKey('service.id'), nullable=True)
    end = relation(Service, primaryjoin=end_id == Service.id)
    modified = Column(Boolean)
    correct = Column(Boolean)

service_snap = Table('service_snap', Base.metadata,
                     Column('spair_id',
                            Integer,
                            ForeignKey('service_pair.id')),
                     Column('snapshot_id',
                            Integer,
                            ForeignKey('snapshot.id')))


class File(Base, Uniquer):
    __tablename__ = 'file'
    id = Column(Integer, primary_key=True)
    name = Column(UnicodeText)
    type = Column(Unicode(12))
    owner = Column(Unicode(12))
    group = Column(Unicode(16))
    perms = Column(Integer)
    contents = Column(UnicodeText)


class FileCorrespondence(Base, CorrespondenceType):
    mtype = File
    __tablename__ = 'file_pair'
    id = Column(Integer, primary_key=True)
    start_id = Column(Integer, ForeignKey('file.id'))
    start = relation(File, primaryjoin=start_id == File.id)
    end_id = Column(Integer, ForeignKey('file.id'), nullable=True)
    end = relation(File, primaryjoin=end_id == File.id)
    modified = Column(Boolean)
    correct = Column(Boolean)

file_snap = Table('file_snap', Base.metadata,
                  Column('fpair_id',
                         Integer,
                         ForeignKey('file_pair.id')),
                  Column('snapshot_id',
                         Integer,
                         ForeignKey('snapshot.id')))

extra_pkg_snap = Table('extra_pkg_snap', Base.metadata,
                       Column('package_id',
                              Integer,
                              ForeignKey('package.id')),
                       Column('snapshot_id',
                              Integer,
                              ForeignKey('snapshot.id')))

extra_file_snap = Table('extra_file_snap', Base.metadata,
                       Column('file_id',
                              Integer,
                              ForeignKey('file.id')),
                       Column('snapshot_id',
                              Integer,
                              ForeignKey('snapshot.id')))

extra_service_snap = Table('extra_service_snap', Base.metadata,
                       Column('service_id',
                              Integer,
                              ForeignKey('service.id')),
                       Column('snapshot_id',
                              Integer,
                              ForeignKey('snapshot.id')))


class Action(Base):
    __tablename__ = 'action'
    id = Column(Integer, primary_key=True)
    command = Column(UnicodeText)
    return_code = Column(Integer)
    output = Column(UnicodeText)

action_snap = Table('action_snap', Base.metadata,
                    Column('action_id', Integer, ForeignKey('action.id')),
                    Column('snapshot_id', Integer, ForeignKey('snapshot.id')))


class Snapshot(Base):
    __tablename__ = 'snapshot'
    id = Column(Integer, primary_key=True)
    correct = Column(Boolean)
    revision = Column(Unicode(36))
    metadata_id = Column(Integer, ForeignKey('metadata.id'))
    client_metadata = relation(Metadata, primaryjoin=metadata_id == Metadata.id)
    timestamp = Column(DateTime, default=datetime.datetime.now)
    client_id = Column(Integer, ForeignKey('client.id'))
    client = relation(Client, backref=backref('snapshots'))
    packages = relation(PackageCorrespondence, secondary=package_snap)
    services = relation(ServiceCorrespondence, secondary=service_snap)
    files = relation(FileCorrespondence, secondary=file_snap)
    actions = relation(Action, secondary=action_snap)
    extra_packages = relation(Package, secondary=extra_pkg_snap)
    extra_services = relation(Service, secondary=extra_service_snap)
    extra_files = relation(File, secondary=extra_file_snap)

    c_dispatch = dict([('Package', ('packages', PackageCorrespondence)),
                       ('Service', ('services', ServiceCorrespondence)),
                       ('Path', ('files', FileCorrespondence))])
    e_dispatch = dict([('Package', ('extra_packages', Package)),
                       ('Service', ('extra_services', Service)),
                       ('Path', ('extra_files', File))])

    @classmethod
    def from_data(cls, session, correct, revision, metadata, entries, extra):
        dbm = Metadata.from_metadata(session, metadata)
        snap = cls(correct=correct, client_metadata=dbm, revision=revision,
                   timestamp=datetime.datetime.now(), client=dbm.client)
        for (dispatch, data) in [(cls.c_dispatch, entries),
                                 (cls.e_dispatch, extra)]:
            for key in dispatch:
                dest, ecls = dispatch[key]
                for edata in list(data[key].values()):
                    getattr(snap, dest).append(ecls.from_record(session, edata))
        return snap

    @classmethod
    def by_client(cls, session, clientname):
        return session.query(cls).join(cls.client_metadata,
                                       Metadata.client).filter(Client.name == clientname)

    @classmethod
    def get_current(cls, session, clientname):
        return session.query(Snapshot).join(Snapshot.client_metadata,
                                            Metadata.client).filter(Client.name == clientname).order_by(desc(Snapshot.timestamp)).first()

    @classmethod
    def get_by_date(cls, session, clientname, timestamp):
        return session.query(Snapshot)\
                      .join(Snapshot.client_metadata, Metadata.client)\
                      .filter(Snapshot.timestamp < timestamp)\
                      .filter(Client.name == clientname)\
                      .order_by(desc(Snapshot.timestamp))\
                      .first()

########NEW FILE########
__FILENAME__ = settings
""" Django settings for the Bcfg2 server """

import os
import sys
import Bcfg2.Options

try:
    import django
    HAS_DJANGO = True
except ImportError:
    HAS_DJANGO = False

# required for reporting
try:
    import south  # pylint: disable=W0611
    HAS_SOUTH = True
except ImportError:
    HAS_SOUTH = False

DATABASES = dict()

# Django < 1.2 compat
DATABASE_ENGINE = None
DATABASE_NAME = None
DATABASE_USER = None
DATABASE_PASSWORD = None
DATABASE_HOST = None
DATABASE_PORT = None
DATABASE_OPTIONS = None
DATABASE_SCHEMA = None

TIME_ZONE = None

DEBUG = False
TEMPLATE_DEBUG = DEBUG

ALLOWED_HOSTS = ['*']

MEDIA_URL = '/site_media/'


def _default_config():
    """ get the default config file.  returns /etc/bcfg2-web.conf,
    UNLESS /etc/bcfg2.conf exists AND /etc/bcfg2-web.conf does not
    exist. """
    optinfo = dict(cfile=Bcfg2.Options.CFILE,
                   web_cfile=Bcfg2.Options.WEB_CFILE)
    setup = Bcfg2.Options.OptionParser(optinfo, quiet=True)
    setup.parse(sys.argv[1:], do_getopt=False)
    if (not os.path.exists(setup['web_cfile']) and
        os.path.exists(setup['cfile'])):
        return setup['cfile']
    else:
        return setup['web_cfile']

DEFAULT_CONFIG = _default_config()


def read_config(cfile=DEFAULT_CONFIG, repo=None, quiet=False):
    """ read the config file and set django settings based on it """
    # pylint: disable=W0602,W0603
    global DATABASE_ENGINE, DATABASE_NAME, DATABASE_USER, DATABASE_PASSWORD, \
        DATABASE_HOST, DATABASE_PORT, DATABASE_OPTIONS, DATABASE_SCHEMA, \
        DEBUG, TEMPLATE_DEBUG, TIME_ZONE, MEDIA_URL
    # pylint: enable=W0602,W0603

    if not os.path.exists(cfile) and os.path.exists(DEFAULT_CONFIG):
        print("%s does not exist, using %s for database configuration" %
              (cfile, DEFAULT_CONFIG))
        cfile = DEFAULT_CONFIG

    optinfo = Bcfg2.Options.DATABASE_COMMON_OPTIONS
    optinfo['repo'] = Bcfg2.Options.SERVER_REPOSITORY
    # when setting a different config file, it has to be set in either
    # sys.argv or in the OptionSet() constructor AS WELL AS the argv
    # that's passed to setup.parse()
    argv = [Bcfg2.Options.CFILE.cmd, cfile,
            Bcfg2.Options.WEB_CFILE.cmd, cfile]
    setup = Bcfg2.Options.OptionParser(optinfo, argv=argv, quiet=quiet)
    setup.parse(argv)

    if repo is None:
        repo = setup['repo']

    if setup['db_engine'] == 'ibm_db_django':
        db_engine = setup['db_engine']
    else:
        db_engine = "django.db.backends.%s" % setup['db_engine']

    DATABASES['default'] = \
        dict(ENGINE=db_engine,
             NAME=setup['db_name'],
             USER=setup['db_user'],
             PASSWORD=setup['db_password'],
             HOST=setup['db_host'],
             PORT=setup['db_port'],
             OPTIONS=setup['db_options'],
             SCHEMA=setup['db_schema'])

    if HAS_DJANGO and django.VERSION[0] == 1 and django.VERSION[1] < 2:
        DATABASE_ENGINE = setup['db_engine']
        DATABASE_NAME = DATABASES['default']['NAME']
        DATABASE_USER = DATABASES['default']['USER']
        DATABASE_PASSWORD = DATABASES['default']['PASSWORD']
        DATABASE_HOST = DATABASES['default']['HOST']
        DATABASE_PORT = DATABASES['default']['PORT']
        DATABASE_OPTIONS = DATABASES['default']['OPTIONS']
        DATABASE_SCHEMA = DATABASES['default']['SCHEMA']

    # dropping the version check.  This was added in 1.1.2
    TIME_ZONE = setup['time_zone']

    DEBUG = setup['django_debug']
    TEMPLATE_DEBUG = DEBUG
    if DEBUG:
        print("Warning: Setting web_debug to True causes extraordinary memory "
              "leaks.  Only use this setting if you know what you're doing.")

    if setup['web_prefix']:
        MEDIA_URL = setup['web_prefix'].rstrip('/') + MEDIA_URL
    else:
        MEDIA_URL = '/site_media/'

# initialize settings from /etc/bcfg2-web.conf or /etc/bcfg2.conf, or
# set up basic defaults.  this lets manage.py work in all cases
read_config(quiet=True)

ADMINS = (('Root', 'root'))
MANAGERS = ADMINS

# Language code for this installation. All choices can be found here:
# http://www.w3.org/TR/REC-html40/struct/dirlang.html#langcodes
# http://blogs.law.harvard.edu/tech/stories/storyReader$15
LANGUAGE_CODE = 'en-us'

SITE_ID = 1

# TODO - sanitize this
INSTALLED_APPS = (
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.sites',
    'django.contrib.admin',
    'Bcfg2.Server',
)
if HAS_SOUTH:
    INSTALLED_APPS = INSTALLED_APPS + (
        'south',
        'Bcfg2.Reporting',
    )
if 'BCFG2_LEGACY_MODELS' in os.environ:
    INSTALLED_APPS += ('Bcfg2.Server.Reports.reports',)

# Imported from Bcfg2.Server.Reports
MEDIA_ROOT = ''

# URL prefix for admin media -- CSS, JavaScript and images. Make sure to use a
# trailing slash.
STATIC_URL = '/media/'

# TODO - make this unique
# Make this unique, and don't share it with anybody.
SECRET_KEY = 'eb5+y%oy-qx*2+62vv=gtnnxg1yig_odu0se5$h0hh#pc*lmo7'

if HAS_DJANGO and django.VERSION[0] == 1 and django.VERSION[1] < 3:
    CACHE_BACKEND = 'locmem:///'
else:
    CACHES = {
        'default': {
            'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',
        }
    }

if HAS_DJANGO and django.VERSION[0] == 1 and django.VERSION[1] < 2:
    TEMPLATE_LOADERS = (
        'django.template.loaders.filesystem.load_template_source',
        'django.template.loaders.app_directories.load_template_source',
    )
else:
    TEMPLATE_LOADERS = (
        'django.template.loaders.filesystem.Loader',
        'django.template.loaders.app_directories.Loader',
    )

# TODO - review these.  auth and sessions aren't really used
MIDDLEWARE_CLASSES = (
    'django.middleware.common.CommonMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.middleware.doc.XViewMiddleware',
)

# TODO - move this to a higher root and dynamically import
ROOT_URLCONF = 'Bcfg2.Reporting.urls'

# TODO - this isn't usable
# Authentication Settings
AUTHENTICATION_BACKENDS = ('django.contrib.auth.backends.ModelBackend')

LOGIN_URL = '/login'

SESSION_EXPIRE_AT_BROWSER_CLOSE = True

TEMPLATE_DIRS = (
    # App loaders should take care of this.. not sure why this is here
    '/usr/share/python-support/python-django/django/contrib/admin/templates/',
)

# TODO - sanitize this
if HAS_DJANGO and django.VERSION[0] == 1 and django.VERSION[1] < 2:
    TEMPLATE_CONTEXT_PROCESSORS = (
        'django.core.context_processors.auth',
        'django.core.context_processors.debug',
        'django.core.context_processors.i18n',
        'django.core.context_processors.media',
        'django.core.context_processors.request'
    )
else:
    TEMPLATE_CONTEXT_PROCESSORS = (
        'django.contrib.auth.context_processors.auth',
        'django.core.context_processors.debug',
        'django.core.context_processors.i18n',
        'django.core.context_processors.media',
        'django.core.context_processors.request'
    )

########NEW FILE########
__FILENAME__ = SSLServer
""" Bcfg2 SSL server used by the builtin server core
(:mod:`Bcfg2.Server.BuiltinCore`).  This needs to be documented
better. """

import os
import sys
import socket
import signal
import logging
import ssl
import threading
import time
from Bcfg2.Compat import xmlrpclib, SimpleXMLRPCServer, SocketServer, \
    b64decode


class XMLRPCDispatcher(SimpleXMLRPCServer.SimpleXMLRPCDispatcher):
    """ An XML-RPC dispatcher. """

    logger = logging.getLogger("Bcfg2.SSLServer.XMLRPCDispatcher")

    def __init__(self, allow_none, encoding):
        try:
            SimpleXMLRPCServer.SimpleXMLRPCDispatcher.__init__(self,
                                                               allow_none,
                                                               encoding)
        except:
            # Python 2.4?
            SimpleXMLRPCServer.SimpleXMLRPCDispatcher.__init__(self)

        self.allow_none = allow_none
        self.encoding = encoding

    def _marshaled_dispatch(self, address, data):
        params, method = xmlrpclib.loads(data)
        try:
            if '.' not in method:
                params = (address, ) + params
            response = self.instance._dispatch(method, params, self.funcs)
            # py3k compatibility
            if type(response) not in [bool, str, list, dict]:
                response = (response.decode('utf-8'), )
            else:
                response = (response, )
            raw_response = xmlrpclib.dumps(response, methodresponse=1,
                                           allow_none=self.allow_none,
                                           encoding=self.encoding)
        except xmlrpclib.Fault:
            fault = sys.exc_info()[1]
            raw_response = xmlrpclib.dumps(fault,
                                           allow_none=self.allow_none,
                                           encoding=self.encoding)
        except:
            err = sys.exc_info()
            self.logger.error("Unexpected handler error", exc_info=1)
            # report exception back to server
            raw_response = xmlrpclib.dumps(
                xmlrpclib.Fault(1, "%s:%s" % (err[0].__name__, err[1])),
                allow_none=self.allow_none, encoding=self.encoding)
        return raw_response


class SSLServer(SocketServer.TCPServer, object):
    """ TCP server supporting SSL encryption. """

    allow_reuse_address = True
    logger = logging.getLogger("Bcfg2.SSLServer.SSLServer")

    def __init__(self, listen_all, server_address, RequestHandlerClass,
                 keyfile=None, certfile=None, reqCert=False, ca=None,
                 timeout=None, protocol='xmlrpc/ssl'):
        """
        :param listen_all: Listen on all interfaces
        :type listen_all: bool
        :param server_address: Address to bind to the server
        :param RequestHandlerClass: Request handler used by TCP server
        :param keyfile: Full path to SSL encryption key file
        :type keyfile: string
        :param certfile: Full path to SSL certificate file
        :type certfile: string
        :param reqCert: Require client to present certificate
        :type reqCert: bool
        :param ca: Full path to SSL CA that signed the key and cert
        :type ca: string
        :param timeout: Timeout for non-blocking request handling
        :param protocol: The protocol to serve.  Supported values are
                         ``xmlrpc/ssl`` and ``xmlrpc/tlsv1``.
        :type protocol: string
        """
        # check whether or not we should listen on all interfaces
        if listen_all:
            listen_address = ('', server_address[1])
        else:
            listen_address = (server_address[0], server_address[1])

        # check for IPv6 address
        if ':' in server_address[0]:
            self.address_family = socket.AF_INET6

        try:
            SocketServer.TCPServer.__init__(self, listen_address,
                                            RequestHandlerClass)
        except socket.gaierror:
            e = sys.exc_info()[1]
            self.logger.error("Failed to bind to socket: %s" % e)
            raise
        except socket.error:
            self.logger.error("Failed to bind to socket")
            raise

        self.timeout = timeout
        self.socket.settimeout(timeout)
        self.keyfile = keyfile
        if (keyfile is not None and
            (keyfile == False or
             not os.path.exists(keyfile) or
             not os.access(keyfile, os.R_OK))):
            msg = "Keyfile %s does not exist or is not readable" % keyfile
            self.logger.error(msg)
            raise Exception(msg)
        self.certfile = certfile
        if (certfile is not None and
            (certfile == False or
             not os.path.exists(certfile) or
             not os.access(certfile, os.R_OK))):
            msg = "Certfile %s does not exist or is not readable" % certfile
            self.logger.error(msg)
            raise Exception(msg)
        self.ca = ca
        if (ca is not None and
            (ca == False or
             not os.path.exists(ca) or
             not os.access(ca, os.R_OK))):
            msg = "CA %s does not exist or is not readable" % ca
            self.logger.error(msg)
            raise Exception(msg)
        self.reqCert = reqCert
        if ca and certfile:
            self.mode = ssl.CERT_OPTIONAL
        else:
            self.mode = ssl.CERT_NONE
        if protocol == 'xmlrpc/ssl':
            self.ssl_protocol = ssl.PROTOCOL_SSLv23
        elif protocol == 'xmlrpc/tlsv1':
            self.ssl_protocol = ssl.PROTOCOL_TLSv1
        else:
            self.logger.error("Unknown protocol %s" % (protocol))
            raise Exception("unknown protocol %s" % protocol)

    def get_request(self):
        (sock, sockinfo) = self.socket.accept()
        sock.settimeout(self.timeout)  # pylint: disable=E1101
        sslsock = ssl.wrap_socket(sock,
                                  server_side=True,
                                  certfile=self.certfile,
                                  keyfile=self.keyfile,
                                  cert_reqs=self.mode,
                                  ca_certs=self.ca,
                                  ssl_version=self.ssl_protocol)
        return sslsock, sockinfo

    def close_request(self, request):
        try:
            request.unwrap()
        except:
            pass
        try:
            request.close()
        except:
            pass

    def _get_url(self):
        port = self.socket.getsockname()[1]
        hostname = socket.gethostname()
        protocol = "https"
        return "%s://%s:%i" % (protocol, hostname, port)
    url = property(_get_url)


class XMLRPCRequestHandler(SimpleXMLRPCServer.SimpleXMLRPCRequestHandler):
    """ XML-RPC request handler.

    Adds support for HTTP authentication.
    """
    logger = logging.getLogger("Bcfg2.SSLServer.XMLRPCRequestHandler")

    def authenticate(self):
        try:
            header = self.headers['Authorization']
        except KeyError:
            self.logger.error("No authentication data presented")
            return False
        auth_content = b64decode(header.split()[1])
        try:
            # py3k compatibility
            try:
                username, password = auth_content.split(":")
            except TypeError:
                # pylint: disable=E0602
                username, pw = auth_content.split(bytes(":", encoding='utf-8'))
                password = pw.decode('utf-8')
                # pylint: enable=E0602
        except ValueError:
            username = auth_content
            password = ""
        cert = self.request.getpeercert()
        client_address = self.request.getpeername()
        return self.server.instance.authenticate(cert, username,
                                                 password, client_address)

    def parse_request(self):
        """Extends parse_request.

        Optionally check HTTP authentication when parsing.
        """
        if not SimpleXMLRPCServer.SimpleXMLRPCRequestHandler.parse_request(self):
            return False
        try:
            if not self.authenticate():
                self.logger.error("Authentication Failure")
                self.send_error(401, self.responses[401][0])
                return False
        except:  # pylint: disable=W0702
            self.logger.error("Unexpected Authentication Failure", exc_info=1)
            self.send_error(401, self.responses[401][0])
            return False
        return True

    def do_POST(self):
        try:
            max_chunk_size = 10 * 1024 * 1024
            size_remaining = int(self.headers["content-length"])
            L = []
            while size_remaining:
                chunk_size = min(size_remaining, max_chunk_size)
                chunk = self.rfile.read(chunk_size).decode('utf-8')
                if not chunk:
                    break
                L.append(chunk)
                size_remaining -= len(L[-1])
            data = ''.join(L)
            if data is None:
                return  # response has been sent

            response = self.server._marshaled_dispatch(self.client_address,
                                                       data)
            if sys.hexversion >= 0x03000000:
                response = response.encode('utf-8')
        except:  # pylint: disable=W0702
            try:
                self.send_response(500)
                self.send_header("Content-length", "0")
                self.end_headers()
            except:
                (etype, msg) = sys.exc_info()[:2]
                self.logger.error("Error sending 500 response (%s): %s" %
                                  (etype.__name__, msg))
                raise
        else:
            # got a valid XML RPC response
            try:
                self.send_response(200)
                self.send_header("Content-type", "text/xml")
                self.send_header("Content-length", str(len(response)))
                self.end_headers()
                failcount = 0
                while True:
                    try:
                        # If we hit SSL3_WRITE_PENDING here try to resend.
                        self.wfile.write(response)
                        break
                    except ssl.SSLError:
                        e = sys.exc_info()[1]
                        if str(e).find("SSL3_WRITE_PENDING") < 0:
                            raise
                        self.logger.error("SSL3_WRITE_PENDING")
                        failcount += 1
                        if failcount < 5:
                            continue
                        raise
            except socket.error:
                err = sys.exc_info()[1]
                if isinstance(err, socket.timeout):
                    self.logger.warning("Connection timed out for %s" %
                                        self.client_address[0])
                elif err[0] == 32:
                    self.logger.warning("Connection dropped from %s" %
                                        self.client_address[0])
                elif err[0] == 104:
                    self.logger.warning("Connection reset by peer: %s" %
                                        self.client_address[0])
                else:
                    self.logger.warning("Socket error sending response to %s: "
                                        "%s" % (self.client_address[0], err))
            except ssl.SSLError:
                err = sys.exc_info()[1]
                self.logger.warning("SSLError handling client %s: %s" %
                                    (self.client_address[0], err))
            except:
                etype, err = sys.exc_info()[:2]
                self.logger.error("Unknown error sending response to %s: "
                                  "%s (%s)" %
                                  (self.client_address[0], err,
                                   etype.__name__))

    def finish(self):
        # shut down the connection
        try:
            SimpleXMLRPCServer.SimpleXMLRPCRequestHandler.finish(self)
        except socket.error:
            err = sys.exc_info()[1]
            self.logger.warning("Error closing connection: %s" % err)


class XMLRPCServer(SocketServer.ThreadingMixIn, SSLServer,
                   XMLRPCDispatcher, object):
    """ Component XMLRPCServer. """

    def __init__(self, listen_all, server_address, RequestHandlerClass=None,
                 keyfile=None, certfile=None, ca=None, protocol='xmlrpc/ssl',
                 timeout=10, logRequests=False,
                 register=True, allow_none=True, encoding=None):
        """
        :param listen_all: Listen on all interfaces
        :type listen_all: bool
        :param server_address: Address to bind to the server
        :param RequestHandlerClass: request handler used by TCP server
        :param keyfile: Full path to SSL encryption key file
        :type keyfile: string
        :param certfile: Full path to SSL certificate file
        :type certfile: string
        :param ca: Full path to SSL CA that signed the key and cert
        :type ca: string
        :param logRequests: Log all requests
        :type logRequests: bool
        :param register: Presence should be reported to service-location
        :type register: bool
        :param allow_none: Allow None values in XML-RPC
        :type allow_none: bool
        :param encoding: Encoding to use for XML-RPC
        """

        XMLRPCDispatcher.__init__(self, allow_none, encoding)

        if not RequestHandlerClass:
            # pylint: disable=E0102
            class RequestHandlerClass(XMLRPCRequestHandler):
                """A subclassed request handler to prevent
                class-attribute conflicts."""
            # pylint: enable=E0102

        SSLServer.__init__(self,
                           listen_all,
                           server_address,
                           RequestHandlerClass,
                           ca=ca,
                           timeout=timeout,
                           keyfile=keyfile,
                           certfile=certfile,
                           protocol=protocol)
        self.logRequests = logRequests
        self.serve = False
        self.register = register
        self.register_introspection_functions()
        self.register_function(self.ping)
        self.logger.info("service available at %s" % self.url)
        self.timeout = timeout

    def _tasks_thread(self):
        try:
            while self.serve:
                try:
                    if self.instance and hasattr(self.instance, 'do_tasks'):
                        self.instance.do_tasks()
                except:
                    self.logger.error("Unexpected task failure", exc_info=1)
                time.sleep(self.timeout)
        except:
            self.logger.error("tasks_thread failed", exc_info=1)

    def server_close(self):
        SSLServer.server_close(self)
        self.logger.info("server_close()")

    def _get_require_auth(self):
        return getattr(self.RequestHandlerClass, "require_auth", False)

    def _set_require_auth(self, value):
        self.RequestHandlerClass.require_auth = value
    require_auth = property(_get_require_auth, _set_require_auth)

    def _get_credentials(self):
        try:
            return self.RequestHandlerClass.credentials
        except AttributeError:
            return dict()

    def _set_credentials(self, value):
        self.RequestHandlerClass.credentials = value
    credentials = property(_get_credentials, _set_credentials)

    def register_instance(self, instance, *args, **kwargs):
        XMLRPCDispatcher.register_instance(self, instance, *args, **kwargs)
        try:
            name = instance.name
        except AttributeError:
            name = "unknown"
        if hasattr(instance, '_get_rmi'):
            for fname, func in instance._get_rmi().items():
                self.register_function(func, name=fname)
        self.logger.info("serving %s at %s" % (name, self.url))

    def serve_forever(self):
        """Serve single requests until (self.serve == False)."""
        self.serve = True
        self.task_thread = \
            threading.Thread(name="%sThread" % self.__class__.__name__,
                             target=self._tasks_thread)
        self.task_thread.start()
        self.logger.info("serve_forever() [start]")
        signal.signal(signal.SIGINT, self._handle_shutdown_signal)
        signal.signal(signal.SIGTERM, self._handle_shutdown_signal)

        try:
            while self.serve:
                try:
                    self.handle_request()
                except socket.timeout:
                    pass
                except:
                    self.logger.error("Got unexpected error in handle_request",
                                      exc_info=1)
        finally:
            self.logger.info("serve_forever() [stop]")

    def shutdown(self):
        """Signal that automatic service should stop."""
        self.serve = False

    def _handle_shutdown_signal(self, *_):
        self.shutdown()

    def ping(self, *args):
        """Echo response."""
        self.logger.info("ping(%s)" % (", ".join([repr(arg) for arg in args])))
        return args

########NEW FILE########
__FILENAME__ = Statistics
""" Module for tracking execution time statistics from the Bcfg2
server core.  This data is exposed by
:func:`Bcfg2.Server.Core.BaseCore.get_statistics`."""


class Statistic(object):
    """ A single named statistic, tracking minimum, maximum, and
    average execution time, and number of invocations. """

    def __init__(self, name, initial_value):
        """
        :param name: The name of this statistic
        :type name: string
        :param initial_value: The initial value to be added to this
                              statistic
        :type initial_value: int or float
        """
        self.name = name
        self.min = float(initial_value)
        self.max = float(initial_value)
        self.ave = float(initial_value)
        self.count = 1

    def add_value(self, value):
        """ Add a value to the statistic, recalculating the various
        metrics.

        :param value: The value to add to this statistic
        :type value: int or float
        """
        self.min = min(self.min, float(value))
        self.max = max(self.max, float(value))
        self.count += 1
        self.ave = (((self.ave * (self.count - 1)) + value) / self.count)

    def get_value(self):
        """ Get a tuple of all the stats tracked on this named item.
        The tuple is in the format::

            (<name>, (min, max, average, number of values))

        This makes it very easy to cast to a dict in
        :func:`Statistics.display`.

        :returns: tuple
        """
        return (self.name, (self.min, self.max, self.ave, self.count))

    def __repr__(self):
        return "%s(%s, (min=%s, avg=%s, max=%s, count=%s))" % (
            self.__class__.__name__,
            self.name, self.min, self.ave, self.max, self.count)


class Statistics(object):
    """ A collection of named :class:`Statistic` objects. """

    def __init__(self):
        self.data = dict()

    def add_value(self, name, value):
        """ Add a value to the named :class:`Statistic`.  This just
        proxies to :func:`Statistic.add_value` or the
        :class:`Statistic` constructor as appropriate.

        :param name: The name of the :class:`Statistic` to add the
                     value to
        :type name: string
        :param value: The value to add to the Statistic
        :type value: int or float
        """
        if name not in self.data:
            self.data[name] = Statistic(name, value)
        else:
            self.data[name].add_value(value)

    def display(self):
        """ Return a dict of all :class:`Statistic` object values.
        Keys are the statistic names, and values are tuples of the
        statistic metrics as returned by
        :func:`Statistic.get_value`. """
        return dict([value.get_value() for value in list(self.data.values())])


#: A module-level :class:`Statistics` objects used to track all
#: execution time metrics for the server.
stats = Statistics()  # pylint: disable=C0103

########NEW FILE########
__FILENAME__ = Utils
""" Miscellaneous useful utility functions, classes, etc., that are
used by both client and server.  Stuff that doesn't fit anywhere
else. """

import shlex
import fcntl
import logging
import threading
import subprocess
from Bcfg2.Compat import any  # pylint: disable=W0622


class ClassName(object):
    """ This very simple descriptor class exists only to get the name
    of the owner class.  This is used because, for historical reasons,
    we expect every server plugin and every client tool to have a
    ``name`` attribute that is in almost all cases the same as the
    ``__class__.__name__`` attribute of the plugin object.  This makes
    that more dynamic so that each plugin and tool isn't repeating its own
    name."""

    def __get__(self, inst, owner):
        return owner.__name__


class PackedDigitRange(object):  # pylint: disable=E0012,R0924
    """ Representation of a set of integer ranges. A range is
    described by a comma-delimited string of integers and ranges,
    e.g.::

        1,10-12,15-20

    Ranges are inclusive on both bounds, and may include 0.  Negative
    numbers are not supported."""

    def __init__(self, *ranges):
        """ May be instantiated in one of two ways::

            PackedDigitRange(<comma-delimited list of ranges>)

        Or::

            PackedDigitRange(<int_or_range>[, <int_or_range>[, ...]])

        E.g., both of the following are valid::

            PackedDigitRange("1-5,7, 10-12")
            PackedDigitRange("1-5", 7, "10-12")
        """
        self.ranges = []
        self.ints = []
        self.str = ",".join(str(r) for r in ranges)
        if len(ranges) == 1 and "," in ranges[0]:
            ranges = ranges[0].split(",")
        for item in ranges:
            item = str(item).strip()
            if item.endswith("-"):
                self.ranges.append((int(item[:-1]), None))
            elif '-' in str(item):
                self.ranges.append(tuple(int(x) for x in item.split('-')))
            else:
                self.ints.append(int(item))

    def includes(self, other):
        """ Return True if ``other`` is included in this range.
        Functionally equivalent to ``other in range``, which should be
        used instead. """
        return other in self

    def __contains__(self, other):
        other = int(other)
        if other in self.ints:
            return True
        return any((end is None and other >= start) or
                   (end is not None and other >= start and other <= end)
                   for start, end in self.ranges)

    def __repr__(self):
        return "%s:%s" % (self.__class__.__name__, str(self))

    def __str__(self):
        return "[%s]" % self.str


def locked(fd):
    """ Acquire a lock on a file.

    :param fd: The file descriptor to lock
    :type fd: int
    :returns: bool - True if the file is already locked, False
              otherwise """
    try:
        fcntl.lockf(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
    except IOError:
        return True
    return False


class ExecutorResult(object):
    """ Returned as the result of a call to
    :func:`Bcfg2.Utils.Executor.run`. The result can be accessed via
    the instance variables, documented below, as a boolean (which is
    equivalent to :attr:`Bcfg2.Utils.ExecutorResult.success`), or as a
    tuple, which, for backwards compatibility, is equivalent to
    ``(result.retval, result.stdout.splitlines())``."""

    def __init__(self, stdout, stderr, retval):
        #: The output of the command
        if isinstance(stdout, str):
            self.stdout = stdout
        else:
            self.stdout = stdout.decode('utf-8')

        #: The error produced by the command
        if isinstance(stdout, str):
            self.stderr = stderr
        else:
            self.stderr = stderr.decode('utf-8')

        #: The return value of the command.
        self.retval = retval

        #: Whether or not the command was successful.  If the
        #: ExecutorResult is used as a boolean, ``success`` is
        #: returned.
        self.success = retval == 0

        #: A friendly error message
        self.error = None
        if self.retval:
            if self.stderr:
                self.error = "%s (rv: %s)" % (self.stderr, self.retval)
            elif self.stdout:
                self.error = "%s (rv: %s)" % (self.stdout, self.retval)
            else:
                self.error = "No output or error; return value %s" % \
                    self.retval

    def __repr__(self):
        if self.error:
            return "Errored command result: %s" % self.error
        elif self.stdout:
            return "Successful command result: %s" % self.stdout
        else:
            return "Successful command result: No output"

    def __getitem__(self, idx):
        """ This provides compatibility with the old Executor, which
        returned a tuple of (return value, stdout split by lines). """
        return (self.retval, self.stdout.splitlines())[idx]

    def __len__(self):
        """ This provides compatibility with the old Executor, which
        returned a tuple of (return value, stdout split by lines). """
        return 2

    def __delitem__(self, _):
        raise TypeError("'%s' object doesn't support item deletion" %
                        self.__class__.__name__)

    def __setitem__(self, idx, val):
        raise TypeError("'%s' object does not support item assignment" %
                        self.__class__.__name__)

    def __nonzero__(self):
        return self.__bool__()

    def __bool__(self):
        return self.success


class Executor(object):
    """ A convenient way to run external commands with
    :class:`subprocess.Popen` """

    def __init__(self, timeout=None):
        """
        :param timeout: Set a default timeout for all commands run by
                        this Executor object
        :type timeout: float
        """
        self.logger = logging.getLogger(self.__class__.__name__)
        self.timeout = timeout

    def _timeout(self, proc):
        """ A function suitable for passing to
        :class:`threading.Timer` that kills the given process.

        :param proc: The process to kill upon timeout.
        :type proc: subprocess.Popen
        :returns: None """
        if proc.poll() is None:
            try:
                proc.kill()
                self.logger.warning("Process exceeeded timeout, killing")
            except OSError:
                pass

    def run(self, command, inputdata=None, shell=False, timeout=None):
        """ Run a command, given as a list, optionally giving it the
        specified input data.

        :param command: The command to run, as a list (preferred) or
                        as a string.  See :class:`subprocess.Popen` for
                        details.
        :type command: list or string
        :param inputdata: Data to pass to the command on stdin
        :type inputdata: string
        :param shell: Run the given command in a shell (not recommended)
        :type shell: bool
        :param timeout: Kill the command if it runs longer than this
                        many seconds.  Set to 0 or -1 to explicitly
                        override a default timeout.
        :type timeout: float
        :returns: :class:`Bcfg2.Utils.ExecutorResult`
        """
        if isinstance(command, str):
            cmdstr = command

            if not shell:
                command = shlex.split(cmdstr)
        else:
            cmdstr = " ".join(command)
        self.logger.debug("Running: %s" % cmdstr)
        try:
            proc = subprocess.Popen(command, shell=shell, bufsize=16384,
                                    close_fds=True,
                                    stdin=subprocess.PIPE,
                                    stdout=subprocess.PIPE,
                                    stderr=subprocess.PIPE)
        except OSError:
            return ExecutorResult('', 'No such command: %s' % cmdstr,
                                  127)
        if timeout is None:
            timeout = self.timeout
        if timeout is not None:
            timer = threading.Timer(float(timeout), self._timeout, [proc])
            timer.start()
        try:
            if inputdata:
                for line in inputdata.splitlines():
                    self.logger.debug('> %s' % line)
            (stdout, stderr) = proc.communicate(input=inputdata)

            # py3k fixes
            if not isinstance(stdout, str):
                stdout = stdout.decode('utf-8')  # pylint: disable=E1103
            if not isinstance(stderr, str):
                stderr = stderr.decode('utf-8')  # pylint: disable=E1103

            for line in stdout.splitlines():  # pylint: disable=E1103
                self.logger.debug('< %s' % line)
            for line in stderr.splitlines():  # pylint: disable=E1103
                self.logger.info(line)
            return ExecutorResult(stdout, stderr, proc.wait())
        finally:
            if timeout is not None:
                timer.cancel()

########NEW FILE########
__FILENAME__ = version
""" bcfg2 version declaration and handling """

import re

__version__ = "1.3.4"


class Bcfg2VersionInfo(tuple):  # pylint: disable=E0012,R0924
    """ object to make granular version operations (particularly
    comparisons) easier """

    v_re = re.compile(r'(\d+)(\w+)(\d+)')

    def __new__(cls, vstr):
        (major, minor, rest) = vstr.split(".")
        match = cls.v_re.match(rest)
        if match:
            micro, releaselevel, serial = match.groups()
        else:
            micro = rest
            releaselevel = 'final'
            serial = 0
        return tuple.__new__(cls, [int(major), int(minor), int(micro),
                                   releaselevel, int(serial)])

    def __init__(self, vstr):  # pylint: disable=W0613
        tuple.__init__(self)
        self.major, self.minor, self.micro, self.releaselevel, self.serial = \
            tuple(self)

    def __repr__(self):
        return "%s(major=%s, minor=%s, micro=%s, releaselevel=%s, serial=%s)" \
            % ((self.__class__.__name__,) + tuple(self))

    def _release_cmp(self, rel1, rel2):  # pylint: disable=R0911
        """ compare two release numbers """
        if rel1 == rel2:
            return 0
        elif rel1 == "final":
            return -1
        elif rel2 == "final":
            return 1
        elif rel1 == "rc":
            return -1
        elif rel2 == "rc":
            return 1
            # should never get to anything past this point
        elif rel1 == "pre":
            return -1
        elif rel2 == "pre":
            return 1
        else:
            # wtf?
            return 0

    def __gt__(self, version):
        if version is None:
            # older bcfg2 clients didn't report their version, so we
            # handle this case specially and assume that any reported
            # version is newer than any indeterminate version
            return True
        try:
            for i in range(3):
                if self[i] != version[i]:
                    return self[i] > version[i]
            rel = self._release_cmp(self[3], version[3])
            if rel != 0:
                return rel < 0
            return self[4] > version[4]
        except TypeError:
            return self > Bcfg2VersionInfo(version)

    def __lt__(self, version):
        if version is None:
            # older bcfg2 clients didn't report their version, so we
            # handle this case specially and assume that any reported
            # version is newer than any indeterminate version
            return False
        try:
            for i in range(3):
                if self[i] != version[i]:
                    return self[i] < version[i]
            rel = self._release_cmp(self[3], version[3])
            if rel != 0:
                return rel > 0
            return self[4] < version[4]
        except TypeError:
            return self < Bcfg2VersionInfo(version)

    def __eq__(self, version):
        if version is None:
            # older bcfg2 clients didn't report their version, so we
            # handle this case specially and assume that any reported
            # version is newer than any indeterminate version
            return False
        try:
            rv = True
            for i in range(len(self)):
                rv &= self[i] == version[i]
            return rv
        except TypeError:
            return self == Bcfg2VersionInfo(version)

    def __ge__(self, version):
        return not self < version

    def __le__(self, version):
        return not self > version

########NEW FILE########
__FILENAME__ = common
""" In order to make testing easier and more consistent, we provide a
number of convenience functions, variables, and classes, for a wide
variety of reasons. To import this module, first set up
:ref:`development-unit-testing-relative-imports` and then simply do:

.. code-block:: python

    from common import *
"""

import os
import re
import sys
import codecs
import unittest
import lxml.etree
from mock import patch, MagicMock, _patch, DEFAULT
from Bcfg2.Compat import wraps

#: The path to the Bcfg2 specification root for the tests.  Using the
#: root directory exposes a lot of potential problems with building
#: paths.
datastore = "/"

#: The XInclude namespace name
XI_NAMESPACE = "http://www.w3.org/2001/XInclude"

#: The XInclude namespace in a format suitable for use in XPath
#: expressions
XI = "{%s}" % XI_NAMESPACE

#: Whether or not the tests are being run on Python 3.
inPy3k = False
if sys.hexversion >= 0x03000000:
    inPy3k = True

try:
    from django.core.management import setup_environ
    has_django = True

    os.environ['DJANGO_SETTINGS_MODULE'] = "Bcfg2.settings"

    import Bcfg2.settings
    Bcfg2.settings.DATABASE_NAME = \
        os.path.join(os.path.dirname(os.path.abspath(__file__)), "test.sqlite")
    Bcfg2.settings.DATABASES['default']['NAME'] = Bcfg2.settings.DATABASE_NAME
except ImportError:
    has_django = False


try:
    from mock import call
except ImportError:
    def call(*args, **kwargs):
        """ Analog to the Mock call object, which is a fairly recent
        addition, but it's very very useful, so we create our own
        function to create Mock calls"""
        return (args, kwargs)

#: The name of the builtin module, for mocking Python builtins.  In
#: Python 2, this is ``__builtin__``, in Python 3 ``builtins``.  To
#: patch a builtin, you must do something like:
#:
#: .. code-block:: python
#:
#:     @patch("%s.open" % open)
#:     def test_something(self, mock_open):
#:         ...
builtins = "__builtin__"


if inPy3k:
    builtins = "builtins"

    def u(s):
        """ Get a unicode string, whatever that means.  In Python 2,
        returns a unicode object; in Python 3, returns a str object.

        :param s: The string to unicode-ify.
        :type s: str
        :returns: str or unicode """
        return s
else:
    def u(s):
        """ Get a unicode string, whatever that means.  In Python 2,
        returns a unicode object; in Python 3, returns a str object.

        :param s: The string to unicode-ify.
        :type s: str
        :returns: str or unicode """
        return codecs.unicode_escape_decode(s)[0]


#: Whether or not skipping tests is natively supported by
#: :mod:`unittest`.  If it isn't, then we have to make tests that
#: would be skipped succeed instead.
can_skip = False

if hasattr(unittest, "skip"):
    can_skip = True

    #: skip decorator from :func:`unittest.skip`
    skip = unittest.skip

    #: skipIf decorator from :func:`unittest.skipIf`
    skipIf = unittest.skipIf

    #: skipUnless decorator from :func:`unittest.skipUnless`
    skipUnless = unittest.skipUnless
else:
    # we can't actually skip tests, we just make them pass
    can_skip = False

    def skip(msg):
        """ skip decorator used when :mod:`unittest` doesn't support
        skipping tests.  Replaces the decorated function with a
        no-op. """
        def decorator(func):
            return lambda *args, **kwargs: None
        return decorator

    def skipIf(condition, msg):
        """ skipIf decorator used when :mod:`unittest` doesn't support
        skipping tests """
        if not condition:
            return lambda f: f
        else:
            return skip(msg)

    def skipUnless(condition, msg):
        """ skipUnless decorator used when :mod:`unittest` doesn't
        support skipping tests """
        if condition:
            return lambda f: f
        else:
            return skip(msg)


def _count_diff_all_purpose(actual, expected):
    '''Returns list of (cnt_act, cnt_exp, elem) triples where the
    counts differ'''
    # elements need not be hashable
    s, t = list(actual), list(expected)
    m, n = len(s), len(t)
    NULL = object()
    result = []
    for i, elem in enumerate(s):
        if elem is NULL:
            continue
        cnt_s = cnt_t = 0
        for j in range(i, m):
            if s[j] == elem:
                cnt_s += 1
                s[j] = NULL
        for j, other_elem in enumerate(t):
            if other_elem == elem:
                cnt_t += 1
                t[j] = NULL
        if cnt_s != cnt_t:
            diff = (cnt_s, cnt_t, elem)
            result.append(diff)

    for i, elem in enumerate(t):
        if elem is NULL:
            continue
        cnt_t = 0
        for j in range(i, n):
            if t[j] == elem:
                cnt_t += 1
                t[j] = NULL
        diff = (0, cnt_t, elem)
        result.append(diff)
    return result


def _assertion(predicate, default_msg=None):
    @wraps(predicate)
    def inner(self, *args, **kwargs):
        if 'msg' in kwargs:
            msg = kwargs['msg']
            del kwargs['msg']
        else:
            try:
                msg = default_msg % args
            except TypeError:
                # message passed as final (non-keyword) argument?
                msg = args[-1]
                args = args[:-1]
        assert predicate(*args, **kwargs), msg
    return inner


def _regex_matches(val, regex):
    if hasattr(regex, 'search'):
        return regex.search(val)
    else:
        return re.search(regex, val)


class Bcfg2TestCase(unittest.TestCase):
    """ Base TestCase class that inherits from
    :class:`unittest.TestCase`.  This class does a few things:

    * Adds :func:`assertXMLEqual`, a useful assertion method given all
      the XML used by Bcfg2;

    * Defines convenience methods that were (mostly) added in Python
      2.7.
    """
    if not hasattr(unittest.TestCase, "assertItemsEqual"):
        # TestCase in Py3k lacks assertItemsEqual, but has the other
        # convenience methods.  this code is (mostly) cribbed from the
        # py2.7 unittest library
        def assertItemsEqual(self, expected_seq, actual_seq, msg=None):
            """ Implementation of
            :func:`unittest.TestCase.assertItemsEqual` for python
            versions that lack it """
            first_seq, second_seq = list(actual_seq), list(expected_seq)
            differences = _count_diff_all_purpose(first_seq, second_seq)

            if differences:
                standardMsg = 'Element counts were not equal:\n'
                lines = ['First has %d, Second has %d:  %r' % diff
                         for diff in differences]
                diffMsg = '\n'.join(lines)
                standardMsg += diffMsg
                if msg is None:
                    msg = standardMsg
                else:
                    msg = "%s : %s" % (standardMsg, msg)
                self.fail(msg)

    if not hasattr(unittest.TestCase, "assertRegexpMatches"):
        # Some versions of TestCase in Py3k seem to lack
        # assertRegexpMatches, but have the other convenience methods.
        assertRegexpMatches = _assertion(lambda s, r: _regex_matches(s, r),
                                         "%s does not contain /%s/")

    if not hasattr(unittest.TestCase, "assertNotRegexpMatches"):
        # Some versions of TestCase in Py3k seem to lack
        # assertNotRegexpMatches even though they have
        # assertRegexpMatches
        assertNotRegexpMatches = \
            _assertion(lambda s, r: not _regex_matches(s, r),
                       "%s contains /%s/")

    if not hasattr(unittest.TestCase, "assertIn"):
        # versions of TestCase before python 2.7 and python 3.1 lacked
        # a lot of the really handy convenience methods, so we provide
        # them -- at least the easy ones and the ones we use.
        assertIs = _assertion(lambda a, b: a is b, "%s is not %s")
        assertIsNot = _assertion(lambda a, b: a is not b, "%s is %s")
        assertIsNone = _assertion(lambda x: x is None, "%s is not None")
        assertIsNotNone = _assertion(lambda x: x is not None, "%s is None")
        assertIn = _assertion(lambda a, b: a in b, "%s is not in %s")
        assertNotIn = _assertion(lambda a, b: a not in b, "%s is in %s")
        assertIsInstance = _assertion(isinstance, "%s is not instance of %s")
        assertNotIsInstance = _assertion(lambda a, b: not isinstance(a, b),
                                         "%s is instance of %s")
        assertGreater = _assertion(lambda a, b: a > b,
                                   "%s is not greater than %s")
        assertGreaterEqual = _assertion(lambda a, b: a >= b,
                                        "%s is not greater than or equal to %s")
        assertLess = _assertion(lambda a, b: a < b, "%s is not less than %s")
        assertLessEqual = _assertion(lambda a, b: a <= b,
                                     "%s is not less than or equal to %s")

    def assertXMLEqual(self, el1, el2, msg=None):
        """ Test that the two XML trees given are equal. """
        if msg is None:
            msg = "XML trees are not equal: %s"
        else:
            msg += ": %s"
        fullmsg = msg + "\nFirst:  %s" % lxml.etree.tostring(el1) + \
            "\nSecond: %s" % lxml.etree.tostring(el2)

        self.assertEqual(el1.tag, el2.tag, msg=fullmsg % "Tags differ")
        if el1.text is not None and el2.text is not None:
            self.assertEqual(el1.text.strip(), el2.text.strip(),
                             msg=fullmsg % "Text content differs")
        else:
            self.assertEqual(el1.text, el2.text,
                             msg=fullmsg % "Text content differs")
        self.assertItemsEqual(el1.attrib.items(), el2.attrib.items(),
                              msg=fullmsg % "Attributes differ")
        self.assertEqual(len(el1.getchildren()),
                         len(el2.getchildren()),
                         msg=fullmsg % "Different numbers of children")
        matched = []
        for child1 in el1.getchildren():
            for child2 in el2.xpath(child1.tag):
                if child2 in matched:
                    continue
                try:
                    self.assertXMLEqual(child1, child2)
                    matched.append(child2)
                    break
                except AssertionError:
                    continue
            else:
                assert False, \
                    fullmsg % ("Element %s is missing from second" %
                               lxml.etree.tostring(child1))
        self.assertItemsEqual(el2.getchildren(), matched,
                              msg=fullmsg % "Second has extra element(s)")


class DBModelTestCase(Bcfg2TestCase):
    """ Test case class for Django database models """
    models = []

    @skipUnless(has_django, "Django not found, skipping")
    def test_syncdb(self):
        """ Create the test database and sync the schema """
        setup_environ(Bcfg2.settings)
        import django.core.management
        django.core.management.call_command("syncdb", interactive=False,
                                            verbosity=0)
        self.assertTrue(os.path.exists(Bcfg2.settings.DATABASE_NAME))

    @skipUnless(has_django, "Django not found, skipping")
    def test_cleandb(self):
        """ Ensure that we a) can connect to the database; b) start
        with a clean database """
        for model in self.models:
            model.objects.all().delete()
            self.assertItemsEqual(list(model.objects.all()), [])


def syncdb(modeltest):
    """ Given an instance of a :class:`DBModelTestCase` object, sync
    and clean the database """
    inst = modeltest(methodName='test_syncdb')
    inst.test_syncdb()
    inst.test_cleandb()


# in order for patchIf() to decorate a function in the same way as
# patch(), we override the default behavior of __enter__ and __exit__
# on the _patch() object to basically be noops.
class _noop_patch(_patch):
    def __enter__(self):
        return MagicMock(name=self.attribute)

    def __exit__(self, *args):
        pass


class patchIf(object):
    """ Decorator class to perform conditional patching.  This is
    necessary because some libraries might not be installed (e.g.,
    selinux, pylibacl), and patching will barf on that.  Other
    workarounds are not available to us; e.g., context managers aren't
    in python 2.4, and using inner functions doesn't work because
    python 2.6 parses all decorators at compile-time, not at run-time,
    so decorating inner functions does not prevent the decorators from
    being run. """

    def __init__(self, condition, target, new=DEFAULT, spec=None, create=False,
                 spec_set=None):
        """
        :param condition: The condition to evaluate to decide if the
                          patch will be applied.
        :type condition: bool
        :param target: The name of the target object to patch
        :type target: str
        :param new: The new object to replace the target with.  If
                    this is omitted, a new :class:`mock.MagicMock` is
                    created and passed as an extra argument to the
                    decorated function.
        :type new: any
        :param spec: Spec passed to the MagicMock object if
                     ``patchIf`` is creating one for you.
        :type spec: List of strings or existing object
        :param create: Tell patch to create attributes on the fly.
                       See the documentation for :func:`mock.patch`
                       for more details on this.
        :type create: bool
        :param spec_set: Spec set passed to the MagicMock object if
                         ``patchIf`` is creating one for you.
        :type spec_set: List of strings or existing object
        """
        self.condition = condition
        self.target = target

        self.patch_args = dict(new=new, spec=spec, create=create,
                               spec_set=spec_set)

    def __call__(self, func):
        if self.condition:
            return patch(self.target, **self.patch_args)(func)
        else:
            args = [lambda: True,
                    self.target.rsplit('.', 1)[-1],
                    self.patch_args['new'], self.patch_args['spec'],
                    self.patch_args['create'], None,
                    self.patch_args['spec_set']]
            try:
                # in older versions of mock, _patch() takes 8 args
                return _noop_patch(*args)(func)
            except TypeError:
                # in some intermediate versions of mock, _patch
                # takes 11 args
                args.extend([None, None, None])
                try:
                    return _noop_patch(*args)(func)
                except TypeError:
                    # in the latest versions of mock, _patch() takes
                    # 10 args -- mocksignature has been removed
                    args.pop(5)
                    return _noop_patch(*args)(func)


#: The type of compiled regular expression objects
re_type = None
try:
    re_type = re._pattern_type
except AttributeError:
    re_type = type(re.compile(""))

########NEW FILE########
__FILENAME__ = exception_messages
try:
    from logilab import astng as ast
    from pylint.interfaces import IASTNGChecker as IChecker
    PYLINT = 0  # pylint 0.something
except ImportError:
    import astroid as ast
    from pylint.interfaces import IAstroidChecker as IChecker
    PYLINT = 1  # pylint 1.something
from pylint.checkers import BaseChecker
from pylint.checkers.utils import safe_infer

if PYLINT == 0:
    # this is not quite correct; later versions of pylint 0.* wanted a
    # three-tuple for messages as well
    msg = ('Exception raised without arguments',
           'Used when an exception is raised without any arguments')
else:
    msg = ('Exception raised without arguments',
           'exception-without-args',
           'Used when an exception is raised without any arguments')
msgs = {'R9901': msg}


class ExceptionMessageChecker(BaseChecker):
    __implements__ = IChecker

    name = 'Exception Messages'
    options = (
        ('exceptions-without-args',
         dict(default=('NotImplementedError',),
              type='csv',
              metavar='<exception names>',
              help='List of exception names that may be raised without arguments')),)
    # this is important so that your checker is executed before others
    priority = -1

    def visit_raise(self, node):
        if node.exc is None:
            return
        if isinstance(node.exc, ast.Name):
            raised = safe_infer(node.exc)
            if (isinstance(raised, ast.Class) and
                raised.name not in self.config.exceptions_without_args):
                self.add_message('R9901', node=node.exc)


def register(linter):
    """required method to auto register this checker"""
    linter.register_checker(ExceptionMessageChecker(linter))

########NEW FILE########
__FILENAME__ = test_schema
import os
import sys
import glob
import lxml.etree
from subprocess import Popen, PIPE, STDOUT

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
_path = os.path.dirname(__file__)
while _path != '/':
    if os.path.basename(_path).lower().startswith("test"):
        sys.path.append(_path)
    if os.path.basename(_path) == "testsuite":
        break
    _path = os.path.dirname(_path)
from common import *

# path to Bcfg2 schema directory
srcpath = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..",
                                       "schemas"))

# test for xmllint existence
try:
    Popen(['xmllint'], stdout=PIPE, stderr=STDOUT).wait()
    HAS_XMLLINT = True
except OSError:
    HAS_XMLLINT = False


XS = "http://www.w3.org/2001/XMLSchema"
XS_NS = "{%s}" % XS
NSMAP = dict(xs=XS)


class TestSchemas(Bcfg2TestCase):
    schema_url = "http://www.w3.org/2001/XMLSchema.xsd"

    @skipUnless(HAS_XMLLINT, "xmllint not installed")
    def test_valid(self):
        schemas = [s for s in glob.glob(os.path.join(srcpath, '*.xsd'))]
        xmllint = Popen(['xmllint', '--xinclude', '--noout', '--schema',
                         self.schema_url] + schemas,
                        stdout=PIPE, stderr=STDOUT)
        print(xmllint.communicate()[0].decode())
        self.assertEqual(xmllint.wait(), 0)

    def test_duplicates(self):
        entities = dict()
        for root, _, files in os.walk(srcpath):
            for fname in files:
                if not fname.endswith(".xsd"):
                    continue
                path = os.path.join(root, fname)
                relpath = path[len(srcpath):].strip("/")
                schema = lxml.etree.parse(path).getroot()
                ns = schema.get("targetNamespace")
                if ns not in entities:
                    entities[ns] = dict(group=dict(),
                                        attributeGroup=dict(),
                                        simpleType=dict(),
                                        complexType=dict())
                for entity in schema.xpath("//xs:*[@name]", namespaces=NSMAP):
                    tag = entity.tag[len(XS_NS):]
                    if tag not in entities[ns]:
                        continue
                    name = entity.get("name")
                    if name in entities[ns][tag]:
                        self.assertNotIn(name, entities[ns][tag],
                                         "Duplicate %s %s (in %s and %s)" %
                                         (tag, name, fname,
                                          entities[ns][tag][name]))
                    entities[ns][tag][name] = fname

########NEW FILE########
__FILENAME__ = TestAugeas
# -*- coding: utf-8 -*-
import os
import sys
import copy
import lxml.etree
import tempfile
from mock import Mock, MagicMock, patch
try:
    from Bcfg2.Client.Tools.POSIX.Augeas import *
    HAS_AUGEAS = True
except ImportError:
    POSIXAugeas = None
    HAS_AUGEAS = False

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from TestPOSIX.Testbase import TestPOSIXTool
from common import *


test_data = """<Test>
  <Empty/>
  <Text>content with spaces</Text>
  <Attrs foo="foo" bar="bar"/>
  <Children identical="false">
    <Foo/>
    <Bar attr="attr"/>
  </Children>
  <Children identical="true">
    <Thing>one</Thing>
    <Thing>two</Thing>
  </Children>
  <Children multi="true">
    <Thing>same</Thing>
    <Thing>same</Thing>
    <Thing>same</Thing>
    <Thing>same</Thing>
  </Children>
</Test>
"""

test_xdata = lxml.etree.XML(test_data)

if can_skip or HAS_AUGEAS:
    class TestPOSIXAugeas(TestPOSIXTool):
        test_obj = POSIXAugeas

        applied_commands = dict(
            insert=lxml.etree.Element(
                "Insert", label="Thing",
                path='Test/Children[#attribute/identical = "true"]/Thing'),
            set=lxml.etree.Element("Set", path="Test/Text/#text",
                                   value="content with spaces"),
            move=lxml.etree.Element(
                "Move", source="Test/Foo",
                destination='Test/Children[#attribute/identical = "false"]/Foo'),
            remove=lxml.etree.Element("Remove", path="Test/Bar"),
            clear=lxml.etree.Element("Clear", path="Test/Empty/#text"),
            setm=lxml.etree.Element(
                "SetMulti", sub="#text", value="same",
                base='Test/Children[#attribute/multi = "true"]/Thing'))

        @skipUnless(HAS_AUGEAS, "Python Augeas libraries not found")
        def setUp(self):
            fd, self.tmpfile = tempfile.mkstemp()
            os.fdopen(fd, 'w').write(test_data)

        def tearDown(self):
            tmpfile = getattr(self, "tmpfile", None)
            if tmpfile and os.path.exists(tmpfile):
                os.unlink(tmpfile)

        def test_fully_specified(self):
            ptool = self.get_obj()

            entry = lxml.etree.Element("Path", name="/test", type="augeas")
            self.assertFalse(ptool.fully_specified(entry))

            lxml.etree.SubElement(entry, "Set", path="/test", value="test")
            self.assertTrue(ptool.fully_specified(entry))

        def test_install(self):
            # this is tested adequately by the other tests
            pass

        def test_verify(self):
            # this is tested adequately by the other tests
            pass

        @patch("Bcfg2.Client.Tools.POSIX.Augeas.POSIXTool.verify")
        def _verify(self, commands, mock_verify):
            ptool = self.get_obj()
            mock_verify.return_value = True

            entry = lxml.etree.Element("Path", name=self.tmpfile,
                                       type="augeas", lens="Xml")
            entry.extend(commands)

            modlist = []
            self.assertTrue(ptool.verify(entry, modlist))
            mock_verify.assert_called_with(ptool, entry, modlist)
            self.assertXMLEqual(lxml.etree.parse(self.tmpfile).getroot(),
                                test_xdata)

        def test_verify_insert(self):
            """ Test successfully verifying an Insert command """
            self._verify([self.applied_commands['insert']])

        def test_verify_set(self):
            """ Test successfully verifying a Set command """
            self._verify([self.applied_commands['set']])

        def test_verify_move(self):
            """ Test successfully verifying a Move command """
            self._verify([self.applied_commands['move']])

        def test_verify_remove(self):
            """ Test successfully verifying a Remove command """
            self._verify([self.applied_commands['remove']])

        def test_verify_clear(self):
            """ Test successfully verifying a Clear command """
            self._verify([self.applied_commands['clear']])

        def test_verify_set_multi(self):
            """ Test successfully verifying a SetMulti command """
            self._verify([self.applied_commands['setm']])

        def test_verify_all(self):
            """ Test successfully verifying multiple commands """
            self._verify(self.applied_commands.values())

        @patch("Bcfg2.Client.Tools.POSIX.Augeas.POSIXTool.install")
        def _install(self, commands, expected, mock_install, **attrs):
            ptool = self.get_obj()
            mock_install.return_value = True

            entry = lxml.etree.Element("Path", name=self.tmpfile,
                                       type="augeas", lens="Xml")
            for key, val in attrs.items():
                entry.set(key, val)
            entry.extend(commands)

            self.assertTrue(ptool.install(entry))
            mock_install.assert_called_with(ptool, entry)
            self.assertXMLEqual(lxml.etree.parse(self.tmpfile).getroot(),
                                expected)

        def test_install_set_existing(self):
            """ Test setting the value of an existing node """
            expected = copy.deepcopy(test_xdata)
            expected.find("Text").text = "Changed content"
            self._install([lxml.etree.Element("Set", path="Test/Text/#text",
                                              value="Changed content")],
                          expected)

        def test_install_set_new(self):
            """ Test setting the value of an new node """
            expected = copy.deepcopy(test_xdata)
            newtext = lxml.etree.SubElement(expected, "NewText")
            newtext.text = "new content"
            self._install([lxml.etree.Element("Set", path="Test/NewText/#text",
                                              value="new content")],
                          expected)

        def test_install_remove(self):
            """ Test removing a node """
            expected = copy.deepcopy(test_xdata)
            expected.remove(expected.find("Attrs"))
            self._install(
                [lxml.etree.Element("Remove",
                                    path='Test/*[#attribute/foo = "foo"]')],
                expected)

        def test_install_move(self):
            """ Test moving a node """
            expected = copy.deepcopy(test_xdata)
            foo = expected.xpath("//Foo")[0]
            expected.append(foo)
            self._install(
                [lxml.etree.Element("Move", source='Test/Children/Foo',
                                    destination='Test/Foo')],
                expected)

        def test_install_clear(self):
            """ Test clearing a node """
            # TODO: clearing a node doesn't seem to work with the XML lens
            #
            # % augtool -b
            # augtool> set /augeas/load/Xml/incl[3] "/tmp/test.xml"
            # augtool> load
            # augtool> clear '/files/tmp/test.xml/Test/Text/#text'
            # augtool> save
            # error: Failed to execute command
            # saving failed (run 'print /augeas//error' for details)
            # augtool> print /augeas//error
            #
            # The error isn't useful.
            pass

        def test_install_set_multi(self):
            """ Test setting multiple nodes at once """
            expected = copy.deepcopy(test_xdata)
            for thing in expected.xpath("Children[@identical='true']/Thing"):
                thing.text = "same"
            self._install(
                [lxml.etree.Element(
                    "SetMulti", value="same",
                    base='Test/Children[#attribute/identical = "true"]',
                    sub="Thing/#text")],
                expected)

        def test_install_insert(self):
            """ Test inserting a node """
            expected = copy.deepcopy(test_xdata)
            children = expected.xpath("Children[@identical='true']")[0]
            thing = lxml.etree.Element("Thing")
            thing.text = "three"
            children.append(thing)
            self._install(
                [lxml.etree.Element(
                    "Insert",
                    path='Test/Children[#attribute/identical = "true"]/Thing[2]',
                    label="Thing", where="after"),
                 lxml.etree.Element(
                     "Set",
                     path='Test/Children[#attribute/identical = "true"]/Thing[3]/#text',
                     value="three")],
                expected)

        def test_install_initial(self):
            """ Test creating initial content and then modifying it """
            os.unlink(self.tmpfile)
            expected = copy.deepcopy(test_xdata)
            expected.find("Text").text = "Changed content"
            initial = lxml.etree.Element("Initial")
            initial.text = test_data
            modify = lxml.etree.Element("Set", path="Test/Text/#text",
                                        value="Changed content")
            self._install([initial, modify], expected, current_exists="false")

########NEW FILE########
__FILENAME__ = Testbase
import os
import sys
import copy
import stat
import lxml.etree
from mock import Mock, MagicMock, patch
import Bcfg2.Client.Tools
from Bcfg2.Client.Tools.POSIX.base import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from TestTools.Test_init import TestTool
from common import *

try:
    import selinux
    HAS_SELINUX = True
except ImportError:
    HAS_SELINUX = False

try:
    import posix1e
    HAS_ACLS = True
except ImportError:
    HAS_ACLS = False


class TestPOSIXTool(TestTool):
    test_obj = POSIXTool

    def test_fully_specified(self):
        # fully_specified should do no checking on the abstract
        # POSIXTool object
        ptool = self.get_obj()
        self.assertTrue(ptool.fully_specified(Mock()))

    @patch('os.stat')
    @patch('os.walk')
    def test_verify(self, mock_walk, mock_stat):
        ptool = self.get_obj()
        ptool._verify_metadata = Mock()
        entry = lxml.etree.Element("Path", name="/test", type="file")

        mock_stat.return_value = MagicMock()
        ptool._verify_metadata.return_value = False
        self.assertFalse(ptool.verify(entry, []))
        ptool._verify_metadata.assert_called_with(entry)

        ptool._verify_metadata.reset_mock()
        ptool._verify_metadata.return_value = True
        self.assertTrue(ptool.verify(entry, []))
        ptool._verify_metadata.assert_called_with(entry)

        ptool._verify_metadata.reset_mock()
        entry.set("recursive", "true")
        walk_rv = [("/", ["dir1", "dir2"], ["file1", "file2"]),
                   ("/dir1", ["dir3"], []),
                   ("/dir2", [], ["file3", "file4"])]
        mock_walk.return_value = walk_rv
        self.assertTrue(ptool.verify(entry, []))
        mock_walk.assert_called_with(entry.get("name"))
        all_verifies = [call(entry)]
        for root, dirs, files in walk_rv:
            all_verifies.extend([call(entry, path=os.path.join(root, p))
                                 for p in dirs + files])
        self.assertItemsEqual(ptool._verify_metadata.call_args_list, all_verifies)

    @patch('os.walk')
    def test_install(self, mock_walk):
        ptool = self.get_obj()
        ptool._set_perms = Mock()
        entry = lxml.etree.Element("Path", name="/test", type="file")

        ptool._set_perms.return_value = True
        self.assertTrue(ptool.install(entry))
        ptool._set_perms.assert_called_with(entry)

        ptool._set_perms.reset_mock()
        entry.set("recursive", "true")
        walk_rv = [("/", ["dir1", "dir2"], ["file1", "file2"]),
                   ("/dir1", ["dir3"], []),
                   ("/dir2", [], ["file3", "file4"])]
        mock_walk.return_value = walk_rv

        ptool._set_perms.return_value = True
        self.assertTrue(ptool.install(entry))
        mock_walk.assert_called_with(entry.get("name"))
        all_set_perms = [call(entry)]
        for root, dirs, files in walk_rv:
            all_set_perms.extend([call(entry, path=os.path.join(root, p))
                                 for p in dirs + files])
        self.assertItemsEqual(ptool._set_perms.call_args_list,
                              all_set_perms)

        mock_walk.reset_mock()
        ptool._set_perms.reset_mock()

        def set_perms_rv(entry, path=None):
            if path == '/dir2/file3':
                return False
            else:
                return True
        ptool._set_perms.side_effect = set_perms_rv

        self.assertFalse(ptool.install(entry))
        mock_walk.assert_called_with(entry.get("name"))
        self.assertItemsEqual(ptool._set_perms.call_args_list, all_set_perms)

    @patch('os.rmdir')
    @patch('os.unlink')
    @patch('shutil.rmtree')
    @patch('os.path.isdir')
    @patch('os.path.islink')
    def test_remove(self, mock_islink, mock_isdir, mock_rmtree, mock_unlink,
                    mock_rmdir):
        ptool = self.get_obj()
        entry = lxml.etree.Element("Path", name="/etc/foo")

        def reset():
            mock_islink.reset_mock()
            mock_isdir.reset_mock()
            mock_rmtree.reset_mock()
            mock_unlink.reset_mock()
            mock_rmdir.reset_mock()

        mock_islink.return_value = True
        mock_isdir.return_value = False
        ptool._remove(entry)
        mock_unlink.assert_called_with(entry.get('name'))
        self.assertFalse(mock_rmtree.called)
        self.assertFalse(mock_rmdir.called)

        reset()
        mock_islink.return_value = False
        mock_isdir.return_value = True
        ptool._remove(entry)
        mock_rmtree.assert_called_with(entry.get('name'))
        self.assertFalse(mock_unlink.called)
        self.assertFalse(mock_rmdir.called)

        reset()
        ptool._remove(entry, recursive=False)
        mock_rmdir.assert_called_with(entry.get('name'))
        self.assertFalse(mock_unlink.called)
        self.assertFalse(mock_rmtree.called)

        reset()
        mock_islink.return_value = False
        mock_isdir.return_value = False
        ptool._remove(entry, recursive=False)
        mock_unlink.assert_called_with(entry.get('name'))
        self.assertFalse(mock_rmtree.called)
        self.assertFalse(mock_rmdir.called)

    @patch('os.lstat')
    def test_exists(self, mock_lstat):
        entry = lxml.etree.Element("Path", name="/etc/foo", type="file")

        ptool = self.get_obj()
        ptool._remove = Mock()

        def reset():
            mock_lstat.reset_mock()
            ptool._remove.reset_mock()

        mock_lstat.side_effect = OSError
        self.assertFalse(ptool._exists(entry))
        mock_lstat.assert_called_with(entry.get('name'))
        self.assertFalse(ptool._remove.called)

        reset()
        rv = MagicMock()
        mock_lstat.return_value = rv
        mock_lstat.side_effect = None
        self.assertEqual(ptool._exists(entry), rv)
        mock_lstat.assert_called_with(entry.get('name'))
        self.assertFalse(ptool._remove.called)

        reset()
        self.assertEqual(ptool._exists(entry, remove=True), None)
        mock_lstat.assert_called_with(entry.get('name'))
        ptool._remove.assert_called_with(entry)

        reset()
        ptool._remove.side_effect = OSError
        self.assertEqual(ptool._exists(entry, remove=True), rv)
        mock_lstat.assert_called_with(entry.get('name'))
        ptool._remove.assert_called_with(entry)

    @patch("os.chown")
    @patch("os.chmod")
    @patch("os.utime")
    def test_set_perms(self, mock_utime, mock_chmod, mock_chown):
        ptool = self.get_obj()
        ptool._norm_entry_uid = Mock()
        ptool._norm_entry_gid = Mock()
        ptool._set_acls = Mock()
        ptool._set_secontext = Mock()
        def reset():
            ptool._set_secontext.reset_mock()
            ptool._set_acls.reset_mock()
            ptool._norm_entry_gid.reset_mock()
            ptool._norm_entry_uid.reset_mock()
            mock_chmod.reset_mock()
            mock_chown.reset_mock()
            mock_utime.reset_mock()

        entry = lxml.etree.Element("Path", name="/etc/foo", to="/etc/bar",
                                   type="symlink")
        ptool._set_acls.return_value = True
        ptool._set_secontext.return_value = True
        self.assertTrue(ptool._set_perms(entry))
        ptool._set_secontext.assert_called_with(entry, path=entry.get("name"))
        ptool._set_acls.assert_called_with(entry, path=entry.get("name"))

        entry = lxml.etree.Element("Path", name="/etc/foo", owner="owner",
                                   group="group", mode="644", type="file")
        ptool._norm_entry_uid.return_value = 10
        ptool._norm_entry_gid.return_value = 100

        reset()
        self.assertTrue(ptool._set_perms(entry))
        ptool._norm_entry_uid.assert_called_with(entry)
        ptool._norm_entry_gid.assert_called_with(entry)
        mock_chown.assert_called_with(entry.get("name"), 10, 100)
        mock_chmod.assert_called_with(entry.get("name"),
                                      int(entry.get("mode"), 8))
        self.assertFalse(mock_utime.called)
        ptool._set_secontext.assert_called_with(entry, path=entry.get("name"))
        ptool._set_acls.assert_called_with(entry, path=entry.get("name"))

        reset()
        mtime = 1344459042
        entry.set("mtime", str(mtime))
        self.assertTrue(ptool._set_perms(entry))
        ptool._norm_entry_uid.assert_called_with(entry)
        ptool._norm_entry_gid.assert_called_with(entry)
        mock_chown.assert_called_with(entry.get("name"), 10, 100)
        mock_chmod.assert_called_with(entry.get("name"),
                                      int(entry.get("mode"), 8))
        mock_utime.assert_called_with(entry.get("name"), (mtime, mtime))
        ptool._set_secontext.assert_called_with(entry, path=entry.get("name"))
        ptool._set_acls.assert_called_with(entry, path=entry.get("name"))

        reset()
        self.assertTrue(ptool._set_perms(entry, path='/etc/bar'))
        ptool._norm_entry_uid.assert_called_with(entry)
        ptool._norm_entry_gid.assert_called_with(entry)
        mock_chown.assert_called_with('/etc/bar', 10, 100)
        mock_chmod.assert_called_with('/etc/bar', int(entry.get("mode"), 8))
        mock_utime.assert_called_with(entry.get("name"), (mtime, mtime))
        ptool._set_secontext.assert_called_with(entry, path='/etc/bar')
        ptool._set_acls.assert_called_with(entry, path='/etc/bar')

        # test dev_type modification of perms, failure of chown
        reset()
        def chown_rv(path, owner, group):
            if owner == 0 and group == 0:
                return True
            else:
                raise KeyError
        os.chown.side_effect = chown_rv
        entry.set("type", "device")
        entry.set("dev_type", list(device_map.keys())[0])
        self.assertFalse(ptool._set_perms(entry))
        ptool._norm_entry_uid.assert_called_with(entry)
        ptool._norm_entry_gid.assert_called_with(entry)
        mock_chown.assert_called_with(entry.get("name"), 0, 0)
        mock_chmod.assert_called_with(entry.get("name"),
                                      int(entry.get("mode"), 8) | list(device_map.values())[0])
        mock_utime.assert_called_with(entry.get("name"), (mtime, mtime))
        ptool._set_secontext.assert_called_with(entry, path=entry.get("name"))
        ptool._set_acls.assert_called_with(entry, path=entry.get("name"))

        # test failure of chmod
        reset()
        os.chown.side_effect = None
        os.chmod.side_effect = OSError
        entry.set("type", "file")
        del entry.attrib["dev_type"]
        self.assertFalse(ptool._set_perms(entry))
        ptool._norm_entry_uid.assert_called_with(entry)
        ptool._norm_entry_gid.assert_called_with(entry)
        mock_chown.assert_called_with(entry.get("name"), 10, 100)
        mock_chmod.assert_called_with(entry.get("name"),
                                      int(entry.get("mode"), 8))
        mock_utime.assert_called_with(entry.get("name"), (mtime, mtime))
        ptool._set_secontext.assert_called_with(entry, path=entry.get("name"))
        ptool._set_acls.assert_called_with(entry, path=entry.get("name"))

        # test that even when everything fails, we try to do it all.
        # e.g., when chmod fails, we still try to apply acls, set
        # selinux context, etc.
        reset()
        os.chown.side_effect = OSError
        os.utime.side_effect = OSError
        ptool._set_acls.return_value = False
        ptool._set_secontext.return_value = False
        self.assertFalse(ptool._set_perms(entry))
        ptool._norm_entry_uid.assert_called_with(entry)
        ptool._norm_entry_gid.assert_called_with(entry)
        mock_chown.assert_called_with(entry.get("name"), 10, 100)
        mock_chmod.assert_called_with(entry.get("name"),
                                      int(entry.get("mode"), 8))
        mock_utime.assert_called_with(entry.get("name"), (mtime, mtime))
        ptool._set_secontext.assert_called_with(entry, path=entry.get("name"))
        ptool._set_acls.assert_called_with(entry, path=entry.get("name"))

    @skipUnless(HAS_ACLS, "ACLS not found, skipping")
    @patchIf(HAS_ACLS, "posix1e.ACL")
    @patchIf(HAS_ACLS, "posix1e.Entry")
    @patch("os.path.isdir")
    def test_set_acls(self, mock_isdir, mock_Entry, mock_ACL):
        ptool = self.get_obj()
        ptool._list_entry_acls = Mock()
        ptool._norm_uid = Mock()
        ptool._norm_gid = Mock()
        entry = lxml.etree.Element("Path", name="/etc/foo", type="file")

        # disable acls for the initial test
        Bcfg2.Client.Tools.POSIX.base.HAS_ACLS = False
        self.assertTrue(ptool._set_acls(entry))
        Bcfg2.Client.Tools.POSIX.base.HAS_ACLS = True

        # build a set of file ACLs to return from posix1e.ACL(file=...)
        file_acls = []
        acl = Mock()
        acl.tag_type = posix1e.ACL_USER
        acl.name = "remove"
        file_acls.append(acl)
        acl = Mock()
        acl.tag_type = posix1e.ACL_GROUP
        acl.name = "remove"
        file_acls.append(acl)
        acl = Mock()
        acl.tag_type = posix1e.ACL_MASK
        acl.name = "keep"
        file_acls.append(acl)
        remove_acls = [a for a in file_acls if a.name == "remove"]

        # build a set of ACLs listed on the entry as returned by
        # _list_entry_acls()
        entry_acls = {("default", posix1e.ACL_USER, "user"): 7,
                      ("access", posix1e.ACL_GROUP, "group"): 5}
        ptool._list_entry_acls.return_value = entry_acls
        ptool._norm_uid.return_value = 10
        ptool._norm_gid.return_value = 100

        # set up the unreasonably complex return value for
        # posix1e.ACL(), which has three separate uses
        fileacl_rv = MagicMock()
        fileacl_rv.valid.return_value = True
        fileacl_rv.__iter__.return_value = iter(file_acls)
        filedef_rv = MagicMock()
        filedef_rv.valid.return_value = True
        filedef_rv.__iter__.return_value = iter(file_acls)
        acl_rv = MagicMock()
        def mock_acl_rv(file=None, filedef=None, acl=None):
            if file:
                return fileacl_rv
            elif filedef:
                return filedef_rv
            elif acl:
                return acl_rv

        # set up the equally unreasonably complex return value for
        # posix1e.Entry, which returns a new entry and adds it to
        # an ACL, so we have to track the Mock objects it returns.
        # why can't they just have an acl.add_entry() method?!?
        acl_entries = []
        def mock_entry_rv(acl):
            rv = MagicMock()
            rv.acl = acl
            rv.permset = set()
            acl_entries.append(rv)
            return rv
        mock_Entry.side_effect = mock_entry_rv

        def reset():
            mock_isdir.reset_mock()
            mock_ACL.reset_mock()
            mock_Entry.reset_mock()
            fileacl_rv.reset_mock()

        # test fs mounted noacl
        mock_ACL.side_effect = IOError(95, "Operation not permitted")
        self.assertFalse(ptool._set_acls(entry))

        # test other error
        reset()
        mock_ACL.side_effect = IOError
        self.assertFalse(ptool._set_acls(entry))

        reset()
        mock_ACL.side_effect = mock_acl_rv
        mock_isdir.return_value = True
        self.assertTrue(ptool._set_acls(entry))
        self.assertItemsEqual(mock_ACL.call_args_list,
                              [call(file=entry.get("name")),
                               call(filedef=entry.get("name"))])
        self.assertItemsEqual(fileacl_rv.delete_entry.call_args_list,
                              [call(a) for a in remove_acls])
        self.assertItemsEqual(filedef_rv.delete_entry.call_args_list,
                              [call(a) for a in remove_acls])
        ptool._list_entry_acls.assert_called_with(entry)
        ptool._norm_uid.assert_called_with("user")
        ptool._norm_gid.assert_called_with("group")
        fileacl_rv.calc_mask.assert_any_call()
        fileacl_rv.applyto.assert_called_with(entry.get("name"),
                                              posix1e.ACL_TYPE_ACCESS)
        filedef_rv.calc_mask.assert_any_call()
        filedef_rv.applyto.assert_called_with(entry.get("name"),
                                              posix1e.ACL_TYPE_DEFAULT)

        # build tuples of the Entry objects that were added to acl
        # and defacl so they're easier to compare for equality
        added_acls = []
        for acl in acl_entries:
            added_acls.append((acl.acl, acl.tag_type, acl.qualifier,
                               sum(acl.permset)))
        self.assertItemsEqual(added_acls,
                              [(filedef_rv, posix1e.ACL_USER, 10, 7),
                               (fileacl_rv, posix1e.ACL_GROUP, 100, 5)])

        reset()
        # have to reassign these because they're iterators, and
        # they've already been iterated over once
        fileacl_rv.__iter__.return_value = iter(file_acls)
        filedef_rv.__iter__.return_value = iter(file_acls)
        ptool._list_entry_acls.reset_mock()
        ptool._norm_uid.reset_mock()
        ptool._norm_gid.reset_mock()
        mock_isdir.return_value = False
        acl_entries = []
        self.assertTrue(ptool._set_acls(entry, path="/bin/bar"))
        mock_ACL.assert_called_with(file="/bin/bar")
        self.assertItemsEqual(fileacl_rv.delete_entry.call_args_list,
                              [call(a) for a in remove_acls])
        ptool._list_entry_acls.assert_called_with(entry)
        ptool._norm_gid.assert_called_with("group")
        fileacl_rv.calc_mask.assert_any_call()
        fileacl_rv.applyto.assert_called_with("/bin/bar",
                                              posix1e.ACL_TYPE_ACCESS)

        added_acls = []
        for acl in acl_entries:
            added_acls.append((acl.acl, acl.tag_type, acl.qualifier,
                               sum(acl.permset)))
        self.assertItemsEqual(added_acls,
                              [(fileacl_rv, posix1e.ACL_GROUP, 100, 5)])

    @skipUnless(HAS_SELINUX, "SELinux not found, skipping")
    @patchIf(HAS_SELINUX, "selinux.restorecon")
    @patchIf(HAS_SELINUX, "selinux.lsetfilecon")
    def test_set_secontext(self, mock_lsetfilecon, mock_restorecon):
        ptool = self.get_obj()
        entry = lxml.etree.Element("Path", name="/etc/foo", type="file")

        # disable selinux for the initial test
        Bcfg2.Client.Tools.POSIX.base.HAS_SELINUX = False
        self.assertTrue(ptool._set_secontext(entry))
        Bcfg2.Client.Tools.POSIX.base.HAS_SELINUX = True

        # no context given
        self.assertTrue(ptool._set_secontext(entry))
        self.assertFalse(mock_restorecon.called)
        self.assertFalse(mock_lsetfilecon.called)

        mock_restorecon.reset_mock()
        mock_lsetfilecon.reset_mock()
        entry.set("secontext", "__default__")
        self.assertTrue(ptool._set_secontext(entry))
        mock_restorecon.assert_called_with(entry.get("name"))
        self.assertFalse(mock_lsetfilecon.called)

        mock_restorecon.reset_mock()
        mock_lsetfilecon.reset_mock()
        mock_lsetfilecon.return_value = 0
        entry.set("secontext", "foo_t")
        self.assertTrue(ptool._set_secontext(entry))
        self.assertFalse(mock_restorecon.called)
        mock_lsetfilecon.assert_called_with(entry.get("name"), "foo_t")

        mock_restorecon.reset_mock()
        mock_lsetfilecon.reset_mock()
        mock_lsetfilecon.return_value = 1
        self.assertFalse(ptool._set_secontext(entry))
        self.assertFalse(mock_restorecon.called)
        mock_lsetfilecon.assert_called_with(entry.get("name"), "foo_t")

    @patch("grp.getgrnam")
    def test_norm_gid(self, mock_getgrnam):
        ptool = self.get_obj()
        self.assertEqual(5, ptool._norm_gid("5"))
        self.assertFalse(mock_getgrnam.called)

        mock_getgrnam.reset_mock()
        mock_getgrnam.return_value = ("group", "x", 5, [])
        self.assertEqual(5, ptool._norm_gid("group"))
        mock_getgrnam.assert_called_with("group")

    def test_norm_entry_gid(self):
        ptool = self.get_obj()
        ptool._norm_gid = Mock()
        entry = lxml.etree.Element("Path", name="/test", type="file",
                                   group="group", owner="user")
        self.assertEqual(ptool._norm_entry_gid(entry),
                         ptool._norm_gid.return_value)
        ptool._norm_gid.assert_called_with(entry.get("group"))

        ptool._norm_gid.reset_mock()
        ptool._norm_gid.side_effect = KeyError
        self.assertEqual(ptool._norm_entry_gid(entry), 0)
        ptool._norm_gid.assert_called_with(entry.get("group"))

    @patch("pwd.getpwnam")
    def test_norm_uid(self, mock_getpwnam):
        ptool = self.get_obj()
        self.assertEqual(5, ptool._norm_uid("5"))
        self.assertFalse(mock_getpwnam.called)

        mock_getpwnam.reset_mock()
        mock_getpwnam.return_value = ("user", "x", 5, 5, "User", "/home/user",
                                      "/bin/zsh")
        self.assertEqual(5, ptool._norm_uid("user"))
        mock_getpwnam.assert_called_with("user")

    def test_norm_entry_uid(self):
        ptool = self.get_obj()
        ptool._norm_uid = Mock()
        entry = lxml.etree.Element("Path", name="/test", type="file",
                                   group="group", owner="user")
        self.assertEqual(ptool._norm_entry_uid(entry),
                         ptool._norm_uid.return_value)
        ptool._norm_uid.assert_called_with(entry.get("owner"))

        ptool._norm_uid.reset_mock()
        ptool._norm_uid.side_effect = KeyError
        self.assertEqual(ptool._norm_entry_uid(entry), 0)
        ptool._norm_uid.assert_called_with(entry.get("owner"))

    def test_norm_acl_perms(self):
        # there's basically no reasonable way to test the Permset
        # object parsing feature without writing our own Mock object
        # that re-implements Permset.test(). silly pylibacl won't let
        # us create standalone Entry or Permset objects.
        ptool = self.get_obj()
        self.assertEqual(5, ptool._norm_acl_perms("5"))
        self.assertEqual(0, ptool._norm_acl_perms("55"))
        self.assertEqual(5, ptool._norm_acl_perms("rx"))
        self.assertEqual(5, ptool._norm_acl_perms("r-x"))
        self.assertEqual(6, ptool._norm_acl_perms("wr-"))
        self.assertEqual(0, ptool._norm_acl_perms("rwrw"))
        self.assertEqual(0, ptool._norm_acl_perms("-"))
        self.assertEqual(0, ptool._norm_acl_perms("a"))
        self.assertEqual(6, ptool._norm_acl_perms("rwa"))
        self.assertEqual(4, ptool._norm_acl_perms("rr"))

    @patch('os.lstat')
    def test__gather_data(self, mock_lstat):
        ptool = self.get_obj()
        path = '/test'
        mock_lstat.side_effect = OSError
        self.assertFalse(ptool._gather_data(path)[0])
        mock_lstat.assert_called_with(path)

        mock_lstat.reset_mock()
        mock_lstat.side_effect = None
        # create a return value
        stat_rv = MagicMock()
        def stat_getitem(key):
            if int(key) == stat.ST_UID:
                return 0
            elif int(key) == stat.ST_GID:
                return 10
            elif int(key) == stat.ST_MODE:
                # return extra bits in the mode to emulate a device
                # and ensure that they're stripped
                return int('060660', 8)
        stat_rv.__getitem__ = Mock(side_effect=stat_getitem)
        mock_lstat.return_value = stat_rv

        # disable selinux and acls for this call -- we test them
        # separately so that we can skip those tests as appropriate
        states = (Bcfg2.Client.Tools.POSIX.base.HAS_SELINUX,
                  Bcfg2.Client.Tools.POSIX.base.HAS_ACLS)
        Bcfg2.Client.Tools.POSIX.base.HAS_SELINUX = False
        Bcfg2.Client.Tools.POSIX.base.HAS_ACLS = False
        self.assertEqual(ptool._gather_data(path),
                         (stat_rv, '0', '10', '0660', None, None))
        Bcfg2.Client.Tools.POSIX.base.HAS_SELINUX, \
            Bcfg2.Client.Tools.POSIX.base.HAS_ACLS = states
        mock_lstat.assert_called_with(path)

    @skipUnless(HAS_SELINUX, "SELinux not found, skipping")
    def test__gather_data_selinux(self):
        ptool = self.get_obj()
        context = 'system_u:object_r:root_t:s0'
        path = '/test'

        @patch('os.lstat')
        @patchIf(HAS_SELINUX, "selinux.lgetfilecon")
        def inner(mock_lgetfilecon, mock_lstat):
            mock_lgetfilecon.return_value = [len(context) + 1, context]
            mock_lstat.return_value = MagicMock()
            mock_lstat.return_value.__getitem__.return_value = MagicMock()
            # disable acls for this call and test them separately
            state = (Bcfg2.Client.Tools.POSIX.base.HAS_ACLS,
                     Bcfg2.Client.Tools.POSIX.base.HAS_SELINUX)
            Bcfg2.Client.Tools.POSIX.base.HAS_ACLS = False
            Bcfg2.Client.Tools.POSIX.base.HAS_SELINUX = True
            self.assertEqual(ptool._gather_data(path)[4], 'root_t')
            Bcfg2.Client.Tools.POSIX.base.HAS_ACLS, \
                Bcfg2.Client.Tools.POSIX.base.HAS_SELINUX = state
            mock_lgetfilecon.assert_called_with(path)

        inner()

    @skipUnless(HAS_ACLS, "ACLS not found, skipping")
    @patch('os.lstat')
    @patch('stat.S_ISLNK')
    def test__gather_data_acls(self, mock_S_ISLNK, mock_lstat):
        ptool = self.get_obj()
        ptool._list_file_acls = Mock()
        acls = {("default", posix1e.ACL_USER, "testuser"): "rwx",
                ("access", posix1e.ACL_GROUP, "testgroup"): "rx"}
        ptool._list_file_acls.return_value = acls
        path = '/test'
        mock_lstat.return_value = MagicMock()
        mock_lstat.return_value.__getitem__.return_value = MagicMock()
        mock_S_ISLNK.return_value = False
        # disable selinux for this call and test it separately
        state = (Bcfg2.Client.Tools.POSIX.base.HAS_ACLS,
                 Bcfg2.Client.Tools.POSIX.base.HAS_SELINUX)
        Bcfg2.Client.Tools.POSIX.base.HAS_ACLS = True
        Bcfg2.Client.Tools.POSIX.base.HAS_SELINUX = False
        self.assertItemsEqual(ptool._gather_data(path)[5], acls)
        ptool._list_file_acls.assert_called_with(path)

        # symlinks can't have their own ACLs, so ensure that the
        # _list_file_acls call is skipped and no ACLs are returned
        mock_S_ISLNK.return_value = True
        ptool._list_file_acls.reset_mock()
        self.assertEqual(ptool._gather_data(path)[5], None)
        self.assertFalse(ptool._list_file_acls.called)

        Bcfg2.Client.Tools.POSIX.base.HAS_ACLS, \
            Bcfg2.Client.Tools.POSIX.base.HAS_SELINUX = state

    @patchIf(HAS_SELINUX, "selinux.matchpathcon")
    def test_verify_metadata(self, mock_matchpathcon):
        ptool = self.get_obj()
        ptool._norm_entry_uid = Mock()
        ptool._norm_entry_gid = Mock()
        ptool._verify_acls = Mock()
        ptool._gather_data = Mock()
        entry = lxml.etree.Element("Path", name="/test", type="file",
                                   group="group", owner="user", mode="664",
                                   secontext='etc_t')
        # _verify_metadata() mutates the entry, so we keep a backup so we
        # can start fresh every time
        orig_entry = copy.deepcopy(entry)

        def reset():
            ptool._gather_data.reset_mock()
            ptool._verify_acls.reset_mock()
            ptool._norm_entry_uid.reset_mock()
            ptool._norm_entry_gid.reset_mock()
            return copy.deepcopy(orig_entry)

        # test nonexistent file
        ptool._gather_data.return_value = (False, None, None, None, None, None)
        self.assertFalse(ptool._verify_metadata(entry))
        self.assertEqual(entry.get("current_exists", "").lower(), "false")
        ptool._gather_data.assert_called_with(entry.get("name"))

        # expected data.  tuple of attr, return value index, value
        expected = [('current_owner', 1, '0'),
                    ('current_group', 2, '10'),
                    ('current_mode', 3, '0664'),
                    ('current_secontext', 4, 'etc_t')]
        ptool._norm_entry_uid.return_value = 0
        ptool._norm_entry_gid.return_value = 10
        gather_data_rv = [MagicMock(), None, None, None, None, []]
        for attr, idx, val in expected:
            gather_data_rv[idx] = val

        entry = reset()
        ptool._gather_data.return_value = tuple(gather_data_rv)
        self.assertTrue(ptool._verify_metadata(entry))
        ptool._gather_data.assert_called_with(entry.get("name"))
        ptool._verify_acls.assert_called_with(entry, path=entry.get("name"))
        self.assertEqual(entry.get("current_exists", 'true'), 'true')
        for attr, idx, val in expected:
            self.assertEqual(entry.get(attr), val)

        # test when secontext is None
        entry = reset()
        gather_data_rv[4] = None
        sestate = Bcfg2.Client.Tools.POSIX.base.HAS_SELINUX
        Bcfg2.Client.Tools.POSIX.base.HAS_SELINUX = False
        ptool._gather_data.return_value = tuple(gather_data_rv)
        self.assertTrue(ptool._verify_metadata(entry))
        ptool._gather_data.assert_called_with(entry.get("name"))
        ptool._verify_acls.assert_called_with(entry, path=entry.get("name"))
        self.assertEqual(entry.get("current_exists", 'true'), 'true')
        for attr, idx, val in expected:
            if attr != 'current_secontext':
                self.assertEqual(entry.get(attr), val)
        Bcfg2.Client.Tools.POSIX.base.HAS_SELINUX = sestate

        gather_data_rv = [MagicMock(), None, None, None, None, []]
        for attr, idx, val in expected:
            gather_data_rv[idx] = val
        ptool._gather_data.return_value = tuple(gather_data_rv)

        stat_mode = 17407
        mtime = 1344430414
        stat_rv = (stat_mode, Mock(), Mock(), Mock(), Mock(), Mock(), Mock(),
                   Mock(), mtime, Mock())
        gather_data_rv[0] = stat_rv
        entry = reset()
        entry.set("mtime", str(mtime))
        ptool._gather_data.return_value = tuple(gather_data_rv)
        self.assertTrue(ptool._verify_metadata(entry))
        ptool._gather_data.assert_called_with(entry.get("name"))
        ptool._verify_acls.assert_called_with(entry, path=entry.get("name"))
        self.assertEqual(entry.get("current_exists", 'true'), 'true')
        for attr, idx, val in expected:
            self.assertEqual(entry.get(attr), val)
        self.assertEqual(entry.get("current_mtime"), str(mtime))

        # failure modes for each checked datum. tuple of changed attr,
        # return value index, new (failing) value
        failures = [('current_owner', 1, '10'),
                    ('current_group', 2, '100'),
                    ('current_mode', 3, '0660')]
        if HAS_SELINUX:
            failures.append(('current_secontext', 4, 'root_t'))

        for fail_attr, fail_idx, fail_val in failures:
            entry = reset()
            entry.set("mtime", str(mtime))
            gather_data_rv = [stat_rv, None, None, None, None, []]
            for attr, idx, val in expected:
                gather_data_rv[idx] = val
            gather_data_rv[fail_idx] = fail_val
            ptool._gather_data.return_value = tuple(gather_data_rv)
            self.assertFalse(ptool._verify_metadata(entry))
            ptool._gather_data.assert_called_with(entry.get("name"))
            ptool._verify_acls.assert_called_with(entry, path=entry.get("name"))
            self.assertEqual(entry.get("current_exists", 'true'), 'true')
            self.assertEqual(entry.get(fail_attr), fail_val)
            for attr, idx, val in expected:
                if attr != fail_attr:
                    self.assertEqual(entry.get(attr), val)
            self.assertEqual(entry.get("current_mtime"), str(mtime))

        # failure mode for mtime
        fail_mtime = 1344431162
        entry = reset()
        entry.set("mtime", str(mtime))
        fail_stat_rv = MagicMock()
        fail_stat_rv.__getitem__.return_value = fail_mtime
        gather_data_rv = [fail_stat_rv, None, None, None, None, []]
        for attr, idx, val in expected:
            gather_data_rv[idx] = val
        ptool._gather_data.return_value = tuple(gather_data_rv)
        self.assertFalse(ptool._verify_metadata(entry))
        ptool._gather_data.assert_called_with(entry.get("name"))
        ptool._verify_acls.assert_called_with(entry, path=entry.get("name"))
        self.assertEqual(entry.get("current_exists", 'true'), 'true')
        for attr, idx, val in expected:
            self.assertEqual(entry.get(attr), val)
        self.assertEqual(entry.get("current_mtime"), str(fail_mtime))

        if HAS_SELINUX:
            # test success and failure for __default__ secontext
            entry = reset()
            entry.set("mtime", str(mtime))
            entry.set("secontext", "__default__")

            context1 = "system_u:object_r:etc_t:s0"
            context2 = "system_u:object_r:root_t:s0"
            mock_matchpathcon.return_value = [1 + len(context1),
                                              context1]
            gather_data_rv = [stat_rv, None, None, None, None, []]
            for attr, idx, val in expected:
                gather_data_rv[idx] = val
            ptool._gather_data.return_value = tuple(gather_data_rv)
            self.assertTrue(ptool._verify_metadata(entry))
            ptool._gather_data.assert_called_with(entry.get("name"))
            ptool._verify_acls.assert_called_with(entry,
                                                path=entry.get("name"))
            mock_matchpathcon.assert_called_with(entry.get("name"), stat_mode)
            self.assertEqual(entry.get("current_exists", 'true'), 'true')
            for attr, idx, val in expected:
                self.assertEqual(entry.get(attr), val)
            self.assertEqual(entry.get("current_mtime"), str(mtime))

            entry = reset()
            entry.set("mtime", str(mtime))
            entry.set("secontext", "__default__")
            mock_matchpathcon.return_value = [1 + len(context2),
                                              context2]
            self.assertFalse(ptool._verify_metadata(entry))
            ptool._gather_data.assert_called_with(entry.get("name"))
            ptool._verify_acls.assert_called_with(entry,
                                                path=entry.get("name"))
            mock_matchpathcon.assert_called_with(entry.get("name"), stat_mode)
            self.assertEqual(entry.get("current_exists", 'true'), 'true')
            for attr, idx, val in expected:
                self.assertEqual(entry.get(attr), val)
            self.assertEqual(entry.get("current_mtime"), str(mtime))

    @skipUnless(HAS_ACLS, "ACLS not found, skipping")
    def test_list_entry_acls(self):
        ptool = self.get_obj()
        entry = lxml.etree.Element("Path", name="/test", type="file")
        lxml.etree.SubElement(entry, "ACL", scope="user", type="default",
                              user="user", perms="rwx")
        lxml.etree.SubElement(entry, "ACL", scope="group", type="access",
                              group="group", perms="5")
        self.assertItemsEqual(ptool._list_entry_acls(entry),
                              {("default", posix1e.ACL_USER, "user"): 7,
                               ("access", posix1e.ACL_GROUP, "group"): 5})

    @skipUnless(HAS_ACLS, "ACLS not found, skipping")
    @patchIf(HAS_ACLS, "posix1e.ACL")
    @patch("pwd.getpwuid")
    @patch("grp.getgrgid")
    @patch("os.path.isdir")
    def test_list_file_acls(self, mock_isdir, mock_getgrgid, mock_getpwuid,
                            mock_ACL):
        ptool = self.get_obj()
        path = '/test'

        # build a set of file ACLs to return from posix1e.ACL(file=...)
        file_acls = []
        acl = Mock()
        acl.tag_type = posix1e.ACL_USER
        acl.qualifier = 10
        # yes, this is a bogus permset.  thanks to _norm_acl_perms
        # it works and is easier than many of the alternatives.
        acl.permset = 'rwx'
        file_acls.append(acl)
        acl = Mock()
        acl.tag_type = posix1e.ACL_GROUP
        acl.qualifier = 100
        acl.permset = 'rx'
        file_acls.append(acl)
        acl = Mock()
        acl.tag_type = posix1e.ACL_MASK
        file_acls.append(acl)
        acls = {("access", posix1e.ACL_USER, "user"): 7,
                ("access", posix1e.ACL_GROUP, "group"): 5}

        # set up the unreasonably complex return value for
        # posix1e.ACL(), which has two separate uses
        fileacl_rv = MagicMock()
        fileacl_rv.valid.return_value = True
        fileacl_rv.__iter__.return_value = iter(file_acls)
        filedef_rv = MagicMock()
        filedef_rv.valid.return_value = True
        filedef_rv.__iter__.return_value = iter(file_acls)
        def mock_acl_rv(file=None, filedef=None):
            if file:
                return fileacl_rv
            elif filedef:
                return filedef_rv
        # other return values
        mock_isdir.return_value = False
        mock_getgrgid.return_value = ("group", "x", 5, [])
        mock_getpwuid.return_value = ("user", "x", 5, 5, "User",
                                      "/home/user", "/bin/zsh")

        def reset():
            mock_isdir.reset_mock()
            mock_getgrgid.reset_mock()
            mock_getpwuid.reset_mock()
            mock_ACL.reset_mock()

        mock_ACL.side_effect = IOError(95, "Operation not supported")
        self.assertItemsEqual(ptool._list_file_acls(path), dict())

        reset()
        mock_ACL.side_effect = IOError
        self.assertItemsEqual(ptool._list_file_acls(path), dict())

        reset()
        mock_ACL.side_effect = mock_acl_rv
        self.assertItemsEqual(ptool._list_file_acls(path), acls)
        mock_isdir.assert_called_with(path)
        mock_getgrgid.assert_called_with(100)
        mock_getpwuid.assert_called_with(10)
        mock_ACL.assert_called_with(file=path)

        reset()
        mock_isdir.return_value = True
        fileacl_rv.__iter__.return_value = iter(file_acls)
        filedef_rv.__iter__.return_value = iter(file_acls)

        defacls = acls
        for akey, perms in list(acls.items()):
            defacls[('default', akey[1], akey[2])] = perms
        self.assertItemsEqual(ptool._list_file_acls(path), defacls)
        mock_isdir.assert_called_with(path)
        self.assertItemsEqual(mock_getgrgid.call_args_list,
                              [call(100), call(100)])
        self.assertItemsEqual(mock_getpwuid.call_args_list,
                              [call(10), call(10)])
        self.assertItemsEqual(mock_ACL.call_args_list,
                              [call(file=path), call(filedef=path)])

    @skipUnless(HAS_ACLS, "ACLS not found, skipping")
    def test_verify_acls(self):
        ptool = self.get_obj()
        ptool._list_file_acls = Mock()
        ptool._list_entry_acls = Mock()
        entry = lxml.etree.Element("Path", name="/test", type="file")
        # we can't test to make sure that errors get properly sorted
        # into (missing, extra, wrong) without refactoring the
        # _verify_acls code, and I don't feel like doing that, so eff
        # it.  let's just test to make sure that failures are
        # identified at all for now.

        acls = {("access", posix1e.ACL_USER, "user"): 7,
                ("default", posix1e.ACL_GROUP, "group"): 5}
        extra_acls = copy.deepcopy(acls)
        extra_acls[("access", posix1e.ACL_USER, "user2")] = 4

        ptool._list_entry_acls.return_value = acls
        ptool._list_file_acls.return_value = acls
        self.assertTrue(ptool._verify_acls(entry))
        ptool._list_entry_acls.assert_called_with(entry)
        ptool._list_file_acls.assert_called_with(entry.get("name"))

        # test missing
        ptool._list_entry_acls.reset_mock()
        ptool._list_file_acls.reset_mock()
        ptool._list_file_acls.return_value = extra_acls
        self.assertFalse(ptool._verify_acls(entry))
        ptool._list_entry_acls.assert_called_with(entry)
        ptool._list_file_acls.assert_called_with(entry.get("name"))

        # test extra
        ptool._list_entry_acls.reset_mock()
        ptool._list_file_acls.reset_mock()
        ptool._list_entry_acls.return_value = extra_acls
        ptool._list_file_acls.return_value = acls
        self.assertFalse(ptool._verify_acls(entry))
        ptool._list_entry_acls.assert_called_with(entry)
        ptool._list_file_acls.assert_called_with(entry.get("name"))

        # test wrong
        wrong_acls = copy.deepcopy(extra_acls)
        wrong_acls[("access", posix1e.ACL_USER, "user2")] = 5
        ptool._list_entry_acls.reset_mock()
        ptool._list_file_acls.reset_mock()
        ptool._list_entry_acls.return_value = extra_acls
        ptool._list_file_acls.return_value = wrong_acls
        self.assertFalse(ptool._verify_acls(entry))
        ptool._list_entry_acls.assert_called_with(entry)
        ptool._list_file_acls.assert_called_with(entry.get("name"))

    @patch("os.makedirs")
    @patch("os.path.exists")
    def test_makedirs(self, mock_exists, mock_makedirs):
        ptool = self.get_obj()
        ptool._set_perms = Mock()
        entry = lxml.etree.Element("Path", name="/test/foo/bar",
                                   type="directory", mode="0644")
        parent_entry = lxml.etree.Element("Path", name="/test/foo/bar",
                                          type="directory", mode="0755")

        def reset():
            mock_exists.reset_mock()
            ptool._set_perms.reset_mock()
            mock_makedirs.reset_mock()

        ptool._set_perms.return_value = True
        def path_exists_rv(path):
            if path == "/test":
                return True
            else:
                return False
        mock_exists.side_effect = path_exists_rv
        self.assertTrue(ptool._makedirs(entry))
        self.assertItemsEqual(mock_exists.call_args_list,
                              [call("/test"), call("/test/foo"),
                               call("/test/foo/bar")])
        for args in ptool._set_perms.call_args_list:
            self.assertXMLEqual(args[0][0], parent_entry)
        self.assertItemsEqual([a[1] for a in ptool._set_perms.call_args_list],
                              [dict(path="/test/foo"),
                               dict(path="/test/foo/bar")])
        mock_makedirs.assert_called_with(entry.get("name"))

        reset()
        mock_makedirs.side_effect = OSError
        self.assertFalse(ptool._makedirs(entry))
        for args in ptool._set_perms.call_args_list:
            self.assertXMLEqual(args[0][0], parent_entry)
        self.assertItemsEqual([a[1] for a in ptool._set_perms.call_args_list],
                              [dict(path="/test/foo"),
                               dict(path="/test/foo/bar")])

        reset()
        mock_makedirs.side_effect = None
        def set_perms_rv(entry, path=None):
            if path == '/test/foo':
                return False
            else:
                return True
        ptool._set_perms.side_effect = set_perms_rv
        self.assertTrue(ptool._makedirs(entry))
        self.assertItemsEqual(mock_exists.call_args_list,
                              [call("/test"), call("/test/foo"),
                               call("/test/foo/bar")])
        for args in ptool._set_perms.call_args_list:
            self.assertXMLEqual(args[0][0], parent_entry)
        self.assertItemsEqual([a[1] for a in ptool._set_perms.call_args_list],
                              [dict(path="/test/foo"),
                               dict(path="/test/foo/bar")])
        mock_makedirs.assert_called_with(entry.get("name"))


class TestPOSIXLinkTool(TestPOSIXTool):
    test_obj = POSIXLinkTool

    @patch("Bcfg2.Client.Tools.POSIX.base.POSIXTool.verify")
    def test_verify(self, mock_verify):
        entry = lxml.etree.Element("Path", name="/test", type="testlink",
                                   to="/dest")
        ptool = self.get_obj()
        linktype = ptool.__linktype__
        ptool.__linktype__ = "test"
        ptool._verify = Mock()

        ptool._verify.return_value = True
        mock_verify.return_value = False
        self.assertFalse(ptool.verify(entry, []))
        ptool._verify.assert_called_with(entry)
        mock_verify.assert_called_with(ptool, entry, [])

        ptool._verify.reset_mock()
        mock_verify.reset_mock()
        mock_verify.return_value = True
        self.assertTrue(ptool.verify(entry, []))
        ptool._verify.assert_called_with(entry)
        mock_verify.assert_called_with(ptool, entry, [])

        ptool._verify.reset_mock()
        mock_verify.reset_mock()
        ptool._verify.return_value = False
        self.assertFalse(ptool.verify(entry, []))
        ptool._verify.assert_called_with(entry)
        mock_verify.assert_called_with(ptool, entry, [])

        ptool._verify.reset_mock()
        mock_verify.reset_mock()
        ptool._verify.side_effect = OSError
        self.assertFalse(ptool.verify(entry, []))
        ptool._verify.assert_called_with(entry)
        ptool.__linktype__ = linktype

    def test__verify(self):
        ptool = self.get_obj()
        self.assertRaises(NotImplementedError, ptool._verify, Mock())

    @patch("Bcfg2.Client.Tools.POSIX.base.POSIXTool.install")
    def test_install(self, mock_install):
        entry = lxml.etree.Element("Path", name="/test", type="symlink",
                                   to="/dest")
        ptool = self.get_obj()
        linktype = ptool.__linktype__
        ptool.__linktype__ = "test"
        ptool._exists = Mock()
        ptool._link = Mock()

        ptool._exists.return_value = False
        mock_install.return_value = True
        self.assertTrue(ptool.install(entry))
        ptool._exists.assert_called_with(entry, remove=True)
        ptool._link.assert_called_with(entry)
        mock_install.assert_called_with(ptool, entry)

        ptool._link.reset_mock()
        ptool._exists.reset_mock()
        mock_install.reset_mock()
        ptool._link.side_effect = OSError
        self.assertFalse(ptool.install(entry))
        ptool._exists.assert_called_with(entry, remove=True)
        ptool._link.assert_called_with(entry)
        mock_install.assert_called_with(ptool, entry)
        ptool.__linktype__ = linktype

    def test__link(self):
        ptool = self.get_obj()
        self.assertRaises(NotImplementedError, ptool._link, Mock())

########NEW FILE########
__FILENAME__ = TestDevice
import os
import sys
import copy
import lxml.etree
from mock import Mock, MagicMock, patch
from Bcfg2.Client.Tools.POSIX.Device import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from Testbase import TestPOSIXTool
from common import *


class TestPOSIXDevice(TestPOSIXTool):
    test_obj = POSIXDevice

    def test_fully_specified(self):
        ptool = self.get_obj()
        orig_entry = lxml.etree.Element("Path", name="/test", type="device",
                                        dev_type="fifo")
        self.assertTrue(ptool.fully_specified(orig_entry))
        for dtype in ["block", "char"]:
            for attr in ["major", "minor"]:
                entry = copy.deepcopy(orig_entry)
                entry.set("dev_type", dtype)
                entry.set(attr, "0")
                self.assertFalse(ptool.fully_specified(entry))
            entry = copy.deepcopy(orig_entry)
            entry.set("dev_type", dtype)
            entry.set("major", "0")
            entry.set("minor", "0")
            self.assertTrue(ptool.fully_specified(entry))

    @patch("os.major")
    @patch("os.minor")
    @patch("Bcfg2.Client.Tools.POSIX.base.POSIXTool.verify")
    def test_verify(self, mock_verify, mock_minor, mock_major):
        entry = lxml.etree.Element("Path", name="/test", type="device",
                                   mode='0644', owner='root', group='root',
                                   dev_type="block", major="0", minor="10")
        ptool = self.get_obj()
        ptool._exists = Mock()

        def reset():
            ptool._exists.reset_mock()
            mock_verify.reset_mock()
            mock_minor.reset_mock()
            mock_major.reset_mock()

        ptool._exists.return_value = False
        self.assertFalse(ptool.verify(entry, []))
        ptool._exists.assert_called_with(entry)

        reset()
        ptool._exists.return_value = MagicMock()
        mock_major.return_value = 0
        mock_minor.return_value = 10
        mock_verify.return_value = True
        self.assertTrue(ptool.verify(entry, []))
        mock_verify.assert_called_with(ptool, entry, [])
        ptool._exists.assert_called_with(entry)
        mock_major.assert_called_with(ptool._exists.return_value.st_rdev)
        mock_minor.assert_called_with(ptool._exists.return_value.st_rdev)

        reset()
        ptool._exists.return_value = MagicMock()
        mock_major.return_value = 0
        mock_minor.return_value = 10
        mock_verify.return_value = False
        self.assertFalse(ptool.verify(entry, []))
        mock_verify.assert_called_with(ptool, entry, [])
        ptool._exists.assert_called_with(entry)
        mock_major.assert_called_with(ptool._exists.return_value.st_rdev)
        mock_minor.assert_called_with(ptool._exists.return_value.st_rdev)

        reset()
        mock_verify.return_value = True
        entry = lxml.etree.Element("Path", name="/test", type="device",
                                   mode='0644', owner='root', group='root',
                                   dev_type="fifo")
        self.assertTrue(ptool.verify(entry, []))
        ptool._exists.assert_called_with(entry)
        mock_verify.assert_called_with(ptool, entry, [])
        self.assertFalse(mock_major.called)
        self.assertFalse(mock_minor.called)

    @patch("os.makedev")
    @patch("os.mknod")
    @patch("Bcfg2.Client.Tools.POSIX.base.POSIXTool.install")
    def test_install(self, mock_install, mock_mknod, mock_makedev):
        entry = lxml.etree.Element("Path", name="/test", type="device",
                                   mode='0644', owner='root', group='root',
                                   dev_type="block", major="0", minor="10")
        ptool = self.get_obj()
        ptool._exists = Mock()

        ptool._exists.return_value = False
        mock_makedev.return_value = Mock()
        mock_install.return_value = True
        self.assertTrue(ptool.install(entry))
        ptool._exists.assert_called_with(entry, remove=True)
        mock_makedev.assert_called_with(0, 10)
        mock_mknod.assert_called_with(entry.get("name"),               # 0o644
                                      device_map[entry.get("dev_type")] | 420,
                                      mock_makedev.return_value)
        mock_install.assert_called_with(ptool, entry)

        mock_makedev.reset_mock()
        mock_mknod.reset_mock()
        ptool._exists.reset_mock()
        mock_install.reset_mock()
        mock_makedev.side_effect = OSError
        self.assertFalse(ptool.install(entry))

        mock_makedev.reset_mock()
        mock_mknod.reset_mock()
        ptool._exists.reset_mock()
        mock_install.reset_mock()
        mock_mknod.side_effect = OSError
        self.assertFalse(ptool.install(entry))

        mock_makedev.reset_mock()
        mock_mknod.reset_mock()
        ptool._exists.reset_mock()
        mock_install.reset_mock()
        mock_mknod.side_effect = None
        entry = lxml.etree.Element("Path", name="/test", type="device",
                                   mode='0644', owner='root', group='root',
                                   dev_type="fifo")

        self.assertTrue(ptool.install(entry))
        ptool._exists.assert_called_with(entry, remove=True)
        mock_mknod.assert_called_with(entry.get("name"),               # 0o644
                                      device_map[entry.get("dev_type")] | 420)
        mock_install.assert_called_with(ptool, entry)

########NEW FILE########
__FILENAME__ = TestDirectory
import os
import sys
import stat
import copy
import lxml.etree
from mock import Mock, MagicMock, patch
from Bcfg2.Client.Tools.POSIX.Directory import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from Testbase import TestPOSIXTool
from common import *


class TestPOSIXDirectory(TestPOSIXTool):
    test_obj = POSIXDirectory

    @patch("os.listdir")
    @patch("Bcfg2.Client.Tools.POSIX.base.POSIXTool.verify")
    def test_verify(self, mock_verify, mock_listdir):
        ptool = self.get_obj()
        ptool._exists = Mock()
        entry = lxml.etree.Element("Path", name="/test", type="directory",
                                   mode='0644', owner='root', group='root')

        ptool._exists.return_value = False
        self.assertFalse(ptool.verify(entry, []))
        ptool._exists.assert_called_with(entry)

        ptool._exists.reset_mock()
        exists_rv = MagicMock()
        exists_rv.__getitem__.return_value = stat.S_IFREG | 420 # 0o644
        ptool._exists.return_value = exists_rv
        self.assertFalse(ptool.verify(entry, []))
        ptool._exists.assert_called_with(entry)

        ptool._exists.reset_mock()
        mock_verify.return_value = False
        exists_rv.__getitem__.return_value = stat.S_IFDIR | 420 # 0o644
        self.assertFalse(ptool.verify(entry, []))
        ptool._exists.assert_called_with(entry)
        mock_verify.assert_called_with(ptool, entry, [])

        ptool._exists.reset_mock()
        mock_verify.reset_mock()
        mock_verify.return_value = True
        self.assertTrue(ptool.verify(entry, []))
        ptool._exists.assert_called_with(entry)
        mock_verify.assert_called_with(ptool, entry, [])

        ptool._exists.reset_mock()
        mock_verify.reset_mock()
        entry.set("prune", "true")
        orig_entry = copy.deepcopy(entry)

        entries = ["foo", "bar", "bar/baz"]
        mock_listdir.return_value = entries
        modlist = [os.path.join(entry.get("name"), entries[0])]
        self.assertFalse(ptool.verify(entry, modlist))
        ptool._exists.assert_called_with(entry)
        mock_verify.assert_called_with(ptool, entry, modlist)
        mock_listdir.assert_called_with(entry.get("name"))
        expected = [os.path.join(entry.get("name"), e)
                    for e in entries
                    if os.path.join(entry.get("name"), e) not in modlist]
        actual = [e.get("name") for e in entry.findall("Prune")]
        self.assertItemsEqual(expected, actual)

        mock_verify.reset_mock()
        ptool._exists.reset_mock()
        mock_listdir.reset_mock()
        entry = copy.deepcopy(orig_entry)
        modlist = [os.path.join(entry.get("name"), e)
                   for e in entries]
        self.assertTrue(ptool.verify(entry, modlist))
        ptool._exists.assert_called_with(entry)
        mock_verify.assert_called_with(ptool, entry, modlist)
        mock_listdir.assert_called_with(entry.get("name"))
        self.assertEqual(len(entry.findall("Prune")), 0)

    @patch("os.unlink")
    @patch("Bcfg2.Client.Tools.POSIX.base.POSIXTool.install")
    def test_install(self, mock_install, mock_unlink):
        entry = lxml.etree.Element("Path", name="/test/foo/bar",
                                   type="directory", mode='0644',
                                   owner='root', group='root')

        ptool = self.get_obj()
        ptool._exists = Mock()
        ptool._makedirs = Mock()
        ptool._remove = Mock()

        def reset():
            ptool._exists.reset_mock()
            mock_install.reset_mock()
            mock_unlink.reset_mock()
            ptool._makedirs.reset_mock()
            ptool._remove.reset_mock()

        ptool._makedirs.return_value = True
        ptool._exists.return_value = False
        mock_install.return_value = True
        self.assertTrue(ptool.install(entry))
        ptool._exists.assert_called_with(entry)
        mock_install.assert_called_with(ptool, entry)
        ptool._makedirs.assert_called_with(entry)

        reset()
        exists_rv = MagicMock()
        exists_rv.__getitem__.return_value = stat.S_IFREG | 420 # 0o644
        ptool._exists.return_value = exists_rv
        self.assertTrue(ptool.install(entry))
        mock_unlink.assert_called_with(entry.get("name"))
        ptool._exists.assert_called_with(entry)
        ptool._makedirs.assert_called_with(entry)
        mock_install.assert_called_with(ptool, entry)

        reset()
        exists_rv.__getitem__.return_value = stat.S_IFDIR | 420 # 0o644
        mock_install.return_value = True
        self.assertTrue(ptool.install(entry))
        ptool._exists.assert_called_with(entry)
        mock_install.assert_called_with(ptool, entry)

        reset()
        mock_install.return_value = False
        self.assertFalse(ptool.install(entry))
        mock_install.assert_called_with(ptool, entry)

        entry.set("prune", "true")
        prune = ["/test/foo/bar/prune1", "/test/foo/bar/prune2"]
        for path in prune:
            lxml.etree.SubElement(entry, "Prune", name=path)

        reset()
        mock_install.return_value = True

        self.assertTrue(ptool.install(entry))
        ptool._exists.assert_called_with(entry)
        mock_install.assert_called_with(ptool, entry)
        self.assertItemsEqual([c[0][0].get("name")
                               for c in ptool._remove.call_args_list],
                              prune)

########NEW FILE########
__FILENAME__ = TestFile
# -*- coding: utf-8 -*-
import os
import sys
import copy
import difflib
import lxml.etree
from Bcfg2.Compat import b64encode, u_str
from mock import Mock, MagicMock, patch
from Bcfg2.Client.Tools.POSIX.File import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from TestPOSIX.Testbase import TestPOSIXTool
from common import *


class TestPOSIXFile(TestPOSIXTool):
    test_obj = POSIXFile

    def test_fully_specified(self):
        ptool = self.get_obj()

        entry = lxml.etree.Element("Path", name="/test", type="file")
        self.assertFalse(ptool.fully_specified(entry))

        entry.set("empty", "true")
        self.assertTrue(ptool.fully_specified(entry))

        entry.set("empty", "false")
        entry.text = "text"
        self.assertTrue(ptool.fully_specified(entry))

    def test_is_string(self):
        ptool = self.get_obj()

        for char in list(range(8)) + list(range(14, 32)):
            self.assertFalse(ptool._is_string("foo" + chr(char) + "bar",
                                              'UTF-8'))
        for char in list(range(9, 14)) + list(range(33, 128)):
            self.assertTrue(ptool._is_string("foo" + chr(char) + "bar",
                                             'UTF-8'))
        ustr = ''
        self.assertTrue(ptool._is_string(ustr, 'UTF-8'))
        if not inPy3k:
            self.assertFalse(ptool._is_string("foo" + chr(128) + "bar",
                                              'ascii'))
            self.assertFalse(ptool._is_string(ustr, 'ascii'))

    def test_get_data(self):
        orig_entry = lxml.etree.Element("Path", name="/test", type="file")
        ptool = self.get_obj(setup=dict(encoding="ascii", ppath='/',
                                        max_copies=5))

        entry = copy.deepcopy(orig_entry)
        entry.text = b64encode("test")
        entry.set("encoding", "base64")
        self.assertEqual(ptool._get_data(entry), ("test", True))

        entry = copy.deepcopy(orig_entry)
        entry.set("encoding", "base64")
        entry.set("empty", "true")
        self.assertEqual(ptool._get_data(entry), ("", True))

        entry = copy.deepcopy(orig_entry)
        entry.set("empty", "true")
        self.assertEqual(ptool._get_data(entry), ("", False))

        entry = copy.deepcopy(orig_entry)
        self.assertEqual(ptool._get_data(entry), ("", False))

        entry = copy.deepcopy(orig_entry)
        entry.text = "test"
        self.assertEqual(ptool._get_data(entry), ("test", False))

        if inPy3k:
            ustr = ''
        else:
            ustr = u_str('', 'UTF-8')
        entry = copy.deepcopy(orig_entry)
        entry.text = ustr
        self.assertEqual(ptool._get_data(entry), (ustr, False))

    @patch("%s.open" % builtins)
    @patch("Bcfg2.Client.Tools.POSIX.base.POSIXTool.verify")
    def test_verify(self, mock_verify, mock_open):
        entry = lxml.etree.Element("Path", name="/test", type="file")
        ptool = self.get_obj(setup=dict(interactive=False, ppath='/',
                                        max_copies=5))
        ptool._exists = Mock()
        ptool._get_data = Mock()
        ptool._get_diffs = Mock()

        def reset():
            ptool._get_diffs.reset_mock()
            ptool._get_data.reset_mock()
            ptool._exists.reset_mock()
            mock_verify.reset_mock()
            mock_open.reset_mock()

        ptool._get_data.return_value = ("test", False)
        ptool._exists.return_value = False
        mock_verify.return_value = True
        self.assertFalse(ptool.verify(entry, []))
        ptool._exists.assert_called_with(entry)
        mock_verify.assert_called_with(ptool, entry, [])
        ptool._get_diffs.assert_called_with(entry, interactive=False,
                                          sensitive=False, is_binary=False,
                                          content="")

        reset()
        exists_rv = MagicMock()
        exists_rv.__getitem__.return_value = 5
        ptool._exists.return_value = exists_rv
        ptool._get_data.return_value = ("test", True)
        self.assertFalse(ptool.verify(entry, []))
        ptool._exists.assert_called_with(entry)
        mock_verify.assert_called_with(ptool, entry, [])
        ptool._get_diffs.assert_called_with(entry, interactive=False,
                                          sensitive=False, is_binary=True,
                                          content=None)

        reset()
        ptool._get_data.return_value = ("test", False)
        exists_rv.__getitem__.return_value = 4
        entry.set("sensitive", "true")
        mock_open.return_value.read.return_value = "tart"
        self.assertFalse(ptool.verify(entry, []))
        ptool._exists.assert_called_with(entry)
        mock_verify.assert_called_with(ptool, entry, [])
        mock_open.assert_called_with(entry.get("name"))
        mock_open.return_value.read.assert_called_with()
        ptool._get_diffs.assert_called_with(entry, interactive=False,
                                          sensitive=True, is_binary=False,
                                          content="tart")

        reset()
        mock_open.return_value.read.return_value = "test"
        self.assertTrue(ptool.verify(entry, []))
        ptool._exists.assert_called_with(entry)
        mock_verify.assert_called_with(ptool, entry, [])
        mock_open.assert_called_with(entry.get("name"))
        mock_open.return_value.read.assert_called_with()
        self.assertFalse(ptool._get_diffs.called)

        reset()
        mock_open.side_effect = IOError
        self.assertFalse(ptool.verify(entry, []))
        ptool._exists.assert_called_with(entry)
        mock_open.assert_called_with(entry.get("name"))

    @patch("os.fdopen")
    @patch("tempfile.mkstemp")
    def test_write_tmpfile(self, mock_mkstemp, mock_fdopen):
        ptool = self.get_obj()
        ptool._get_data = Mock()
        entry = lxml.etree.Element("Path", name="/test", type="file",
                                   mode='0644', owner='root', group='root')
        newfile = "/foo/bar"

        def reset():
            ptool._get_data.reset_mock()
            mock_mkstemp.reset_mock()
            mock_fdopen.reset_mock()

        ptool._get_data.return_value = ("test", False)
        mock_mkstemp.return_value = (5, newfile)
        self.assertEqual(ptool._write_tmpfile(entry), newfile)
        ptool._get_data.assert_called_with(entry)
        mock_mkstemp.assert_called_with(prefix='test', dir='/')
        mock_fdopen.assert_called_with(5, 'w')
        mock_fdopen.return_value.write.assert_called_with("test")

        reset()
        mock_mkstemp.side_effect = OSError
        self.assertFalse(ptool._write_tmpfile(entry))
        mock_mkstemp.assert_called_with(prefix='test', dir='/')

        reset()
        mock_mkstemp.side_effect = None
        mock_fdopen.side_effect = OSError
        self.assertFalse(ptool._write_tmpfile(entry))
        mock_mkstemp.assert_called_with(prefix='test', dir='/')
        ptool._get_data.assert_called_with(entry)
        mock_fdopen.assert_called_with(5, 'w')

    @patch("os.rename")
    @patch("os.unlink")
    def test_rename_tmpfile(self, mock_unlink, mock_rename):
        ptool = self.get_obj()
        entry = lxml.etree.Element("Path", name="/test", type="file",
                                   mode='0644', owner='root', group='root')
        newfile = "/foo/bar"

        self.assertTrue(ptool._rename_tmpfile(newfile, entry))
        mock_rename.assert_called_with(newfile, entry.get("name"))

        mock_rename.reset_mock()
        mock_unlink.reset_mock()
        mock_rename.side_effect = OSError
        self.assertFalse(ptool._rename_tmpfile(newfile, entry))
        mock_rename.assert_called_with(newfile, entry.get("name"))
        mock_unlink.assert_called_with(newfile)

        # even if the unlink fails, return false gracefully
        mock_rename.reset_mock()
        mock_unlink.reset_mock()
        mock_unlink.side_effect = OSError
        self.assertFalse(ptool._rename_tmpfile(newfile, entry))
        mock_rename.assert_called_with(newfile, entry.get("name"))
        mock_unlink.assert_called_with(newfile)

    @patch("%s.open" % builtins)
    def test__get_diffs(self, mock_open):
        orig_entry = lxml.etree.Element("Path", name="/test", type="file",
                                        mode='0644', owner='root',
                                        group='root')
        orig_entry.text = "test"
        ondisk = "test2"
        ptool = self.get_obj(setup=dict(encoding="utf-8", ppath='/',
                                        max_copies=5))
        ptool._get_data = Mock()
        ptool._diff = Mock()
        ptool._is_string = Mock()

        def reset():
            ptool._is_string.reset_mock()
            ptool._get_data.reset_mock()
            ptool._diff.reset_mock()
            mock_open.reset_mock()
            return copy.deepcopy(orig_entry)

        ptool._is_string.return_value = True
        ptool._get_data.return_value = (orig_entry.text, False)
        mock_open.return_value.read.return_value = ondisk
        ptool._diff.return_value = ["-test2", "+test"]

        # binary data in the entry
        entry = reset()
        ptool._get_diffs(entry, is_binary=True)
        mock_open.assert_called_with(entry.get("name"))
        mock_open.return_value.read.assert_any_call()
        self.assertFalse(ptool._diff.called)
        self.assertEqual(entry.get("current_bfile"), b64encode(ondisk))

        # binary data on disk
        entry = reset()
        ptool._is_string.return_value = False
        ptool._get_diffs(entry, content=ondisk)
        self.assertFalse(mock_open.called)
        self.assertFalse(ptool._diff.called)
        self.assertEqual(entry.get("current_bfile"), b64encode(ondisk))

        # sensitive, non-interactive -- do nothing
        entry = reset()
        ptool._is_string.return_value = True
        ptool._get_diffs(entry, sensitive=True, interactive=False)
        self.assertFalse(mock_open.called)
        self.assertFalse(ptool._diff.called)
        self.assertXMLEqual(entry, orig_entry)

        # sensitive, interactive
        entry = reset()
        ptool._get_diffs(entry, sensitive=True, interactive=True)
        mock_open.assert_called_with(entry.get("name"))
        mock_open.return_value.read.assert_any_call()
        ptool._diff.assert_called_with(ondisk, entry.text,
                                       difflib.unified_diff,
                                       filename=entry.get("name"))
        self.assertIsNotNone(entry.get("qtext"))
        del entry.attrib['qtext']
        self.assertItemsEqual(orig_entry.attrib, entry.attrib)

        # non-sensitive, non-interactive
        entry = reset()
        ptool._get_diffs(entry, content=ondisk)
        self.assertFalse(mock_open.called)
        ptool._diff.assert_called_with(ondisk, entry.text, difflib.ndiff,
                                     filename=entry.get("name"))
        self.assertIsNone(entry.get("qtext"))
        self.assertEqual(entry.get("current_bdiff"),
                         b64encode("\n".join(ptool._diff.return_value)))
        del entry.attrib["current_bdiff"]
        self.assertItemsEqual(orig_entry.attrib, entry.attrib)

        # non-sensitive, interactive -- do everything. also test
        # appending to qtext
        entry = reset()
        entry.set("qtext", "test")
        ptool._get_diffs(entry, interactive=True)
        mock_open.assert_called_with(entry.get("name"))
        mock_open.return_value.read.assert_any_call()
        self.assertItemsEqual(ptool._diff.call_args_list,
                              [call(ondisk, entry.text, difflib.unified_diff,
                                    filename=entry.get("name")),
                               call(ondisk, entry.text, difflib.ndiff,
                                    filename=entry.get("name"))])
        self.assertIsNotNone(entry.get("qtext"))
        self.assertTrue(entry.get("qtext").startswith("test\n"))
        self.assertEqual(entry.get("current_bdiff"),
                         b64encode("\n".join(ptool._diff.return_value)))
        del entry.attrib['qtext']
        del entry.attrib["current_bdiff"]
        self.assertItemsEqual(orig_entry.attrib, entry.attrib)

        # non-sensitive, interactive with unicode data
        entry = reset()
        entry.text = u("tst")
        encoded = entry.text.encode(ptool.setup['encoding'])
        ptool._diff.return_value = ["-test2", "+tst"]
        ptool._get_data.return_value = (encoded, False)
        ptool._get_diffs(entry, interactive=True)
        mock_open.assert_called_with(entry.get("name"))
        mock_open.return_value.read.assert_any_call()
        self.assertItemsEqual(ptool._diff.call_args_list,
                              [call(ondisk, encoded, difflib.unified_diff,
                                    filename=entry.get("name")),
                               call(ondisk, encoded, difflib.ndiff,
                                    filename=entry.get("name"))])
        self.assertIsNotNone(entry.get("qtext"))
        self.assertEqual(entry.get("current_bdiff"),
                         b64encode("\n".join(ptool._diff.return_value)))
        del entry.attrib['qtext']
        del entry.attrib["current_bdiff"]
        self.assertItemsEqual(orig_entry.attrib, entry.attrib)

    @patch("os.path.exists")
    @patch("Bcfg2.Client.Tools.POSIX.base.POSIXTool.install")
    def test_install(self, mock_install, mock_exists):
        ptool = self.get_obj()
        ptool._makedirs = Mock()
        ptool._set_perms = Mock()
        ptool._write_tmpfile = Mock()
        ptool._rename_tmpfile = Mock()
        entry = lxml.etree.Element("Path", name="/test", type="file",
                                   mode='0644', owner='root', group='root')

        def reset():
            ptool._rename_tmpfile.reset_mock()
            ptool._write_tmpfile.reset_mock()
            ptool._set_perms.reset_mock()
            ptool._makedirs.reset_mock()
            mock_install.reset_mock()
            mock_exists.reset_mock()

        mock_exists.return_value = False
        ptool._makedirs.return_value = False
        self.assertFalse(ptool.install(entry))
        mock_exists.assert_called_with("/")
        ptool._makedirs.assert_called_with(entry, path="/")

        reset()
        ptool._makedirs.return_value = True
        ptool._write_tmpfile.return_value = False
        self.assertFalse(ptool.install(entry))
        mock_exists.assert_called_with("/")
        ptool._makedirs.assert_called_with(entry, path="/")
        ptool._write_tmpfile.assert_called_with(entry)

        reset()
        newfile = '/test.X987yS'
        ptool._write_tmpfile.return_value = newfile
        ptool._set_perms.return_value = False
        ptool._rename_tmpfile.return_value = False
        self.assertFalse(ptool.install(entry))
        mock_exists.assert_called_with("/")
        ptool._makedirs.assert_called_with(entry, path="/")
        ptool._write_tmpfile.assert_called_with(entry)
        ptool._set_perms.assert_called_with(entry, path=newfile)
        ptool._rename_tmpfile.assert_called_with(newfile, entry)

        reset()
        ptool._rename_tmpfile.return_value = True
        mock_install.return_value = False
        self.assertFalse(ptool.install(entry))
        mock_exists.assert_called_with("/")
        ptool._makedirs.assert_called_with(entry, path="/")
        ptool._write_tmpfile.assert_called_with(entry)
        ptool._set_perms.assert_called_with(entry, path=newfile)
        ptool._rename_tmpfile.assert_called_with(newfile, entry)
        mock_install.assert_called_with(ptool, entry)

        reset()
        mock_install.return_value = True
        self.assertFalse(ptool.install(entry))
        mock_exists.assert_called_with("/")
        ptool._makedirs.assert_called_with(entry, path="/")
        ptool._write_tmpfile.assert_called_with(entry)
        ptool._set_perms.assert_called_with(entry, path=newfile)
        ptool._rename_tmpfile.assert_called_with(newfile, entry)
        mock_install.assert_called_with(ptool, entry)

        reset()
        ptool._set_perms.return_value = True
        self.assertTrue(ptool.install(entry))
        mock_exists.assert_called_with("/")
        ptool._makedirs.assert_called_with(entry, path="/")
        ptool._write_tmpfile.assert_called_with(entry)
        ptool._set_perms.assert_called_with(entry, path=newfile)
        ptool._rename_tmpfile.assert_called_with(newfile, entry)
        mock_install.assert_called_with(ptool, entry)

        reset()
        mock_exists.return_value = True
        self.assertTrue(ptool.install(entry))
        mock_exists.assert_called_with("/")
        self.assertFalse(ptool._makedirs.called)
        ptool._write_tmpfile.assert_called_with(entry)
        ptool._set_perms.assert_called_with(entry, path=newfile)
        ptool._rename_tmpfile.assert_called_with(newfile, entry)
        mock_install.assert_called_with(ptool, entry)

    @patch("time.time")
    def test_diff(self, mock_time):
        ptool = self.get_obj()
        content1 = "line1\nline2"
        content2 = "line3"

        self.now = 1345640723

        def time_rv():
            self.now += 1
            return self.now
        mock_time.side_effect = time_rv

        rv = ["line1", "line2", "line3"]
        func = Mock()
        func.return_value = rv
        self.assertItemsEqual(ptool._diff(content1, content2, func), rv)
        func.assert_called_with(["line1", "line2"], ["line3"])

        func.reset_mock()
        mock_time.reset_mock()
        def time_rv():
            self.now += 5
            return self.now
        mock_time.side_effect = time_rv

        def slow_diff(content1, content2):
            for i in range(1, 10):
                yield "line%s" % i
        func.side_effect = slow_diff
        self.assertFalse(ptool._diff(content1, content2, func), rv)
        func.assert_called_with(["line1", "line2"], ["line3"])

########NEW FILE########
__FILENAME__ = TestHardlink
import os
import sys
import copy
import lxml.etree
from mock import Mock, MagicMock, patch
from Bcfg2.Client.Tools.POSIX.Hardlink import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from Testbase import TestPOSIXLinkTool
from common import *


class TestPOSIXHardlink(TestPOSIXLinkTool):
    test_obj = POSIXHardlink

    @patch("os.path.samefile")
    def test__verify(self, mock_samefile):
        entry = lxml.etree.Element("Path", name="/test", type="hardlink",
                                   to="/dest")
        ptool = self.get_obj()
        self.assertEqual(ptool._verify(entry), mock_samefile.return_value)
        self.assertItemsEqual(mock_samefile.call_args[0],
                              [entry.get("name"), entry.get("to")])

    @patch("os.link")
    def test__link(self, mock_link):
        entry = lxml.etree.Element("Path", name="/test", type="hardlink",
                                   to="/dest")
        ptool = self.get_obj()
        self.assertEqual(ptool._link(entry), mock_link.return_value)
        mock_link.assert_called_with(entry.get("to"), entry.get("name"))

########NEW FILE########
__FILENAME__ = TestNonexistent
import os
import sys
import copy
import lxml.etree
from mock import Mock, MagicMock, patch
from Bcfg2.Client.Tools.POSIX.Nonexistent import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from Test__init import get_config
from Testbase import TestPOSIXTool
from common import *


class TestPOSIXNonexistent(TestPOSIXTool):
    test_obj = POSIXNonexistent

    @patch("os.path.lexists")
    def test_verify(self, mock_lexists):
        ptool = self.get_obj()
        entry = lxml.etree.Element("Path", name="/test", type="nonexistent")

        for val in [True, False]:
            mock_lexists.reset_mock()
            mock_lexists.return_value = val
            self.assertEqual(ptool.verify(entry, []), not val)
            mock_lexists.assert_called_with(entry.get("name"))

    def test_install(self):
        entry = lxml.etree.Element("Path", name="/test", type="nonexistent")

        ptool = self.get_obj()
        ptool._remove = Mock()

        def reset():
            ptool._remove.reset_mock()

        self.assertTrue(ptool.install(entry))
        ptool._remove.assert_called_with(entry, recursive=False)

        reset()
        entry.set("recursive", "true")
        self.assertTrue(ptool.install(entry))
        ptool._remove.assert_called_with(entry, recursive=True)

        reset()
        child_entry = lxml.etree.Element("Path", name="/test/foo",
                                         type="nonexistent")
        ptool = self.get_obj(config=get_config([child_entry]))
        ptool._remove = Mock()
        self.assertTrue(ptool.install(entry))
        ptool._remove.assert_called_with(entry, recursive=True)

        reset()
        child_entry = lxml.etree.Element("Path", name="/test/foo",
                                         type="file")
        ptool = self.get_obj(config=get_config([child_entry]))
        ptool._remove = Mock()
        self.assertFalse(ptool.install(entry))
        self.assertFalse(ptool._remove.called)

        reset()
        entry.set("recursive", "false")
        ptool._remove.side_effect = OSError
        self.assertFalse(ptool.install(entry))
        ptool._remove.assert_called_with(entry, recursive=False)

########NEW FILE########
__FILENAME__ = TestPermissions
from Bcfg2.Client.Tools.POSIX.Permissions import *
from Testbase import TestPOSIXTool


class TestPOSIXPermissions(TestPOSIXTool):
    test_obj = POSIXPermissions

########NEW FILE########
__FILENAME__ = TestSymlink
import os
import sys
import lxml.etree
from mock import Mock, MagicMock, patch
from Bcfg2.Client.Tools.POSIX.Symlink import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from Testbase import TestPOSIXLinkTool
from common import *


class TestPOSIXSymlink(TestPOSIXLinkTool):
    test_obj = POSIXSymlink

    @patch("os.readlink")
    def test__verify(self, mock_readlink):
        entry = lxml.etree.Element("Path", name="/test", type="symlink",
                                   to="/dest")
        ptool = self.get_obj()

        mock_readlink.return_value = entry.get("to")
        self.assertTrue(ptool._verify(entry))
        mock_readlink.assert_called_with(entry.get("name"))

        mock_readlink.reset_mock()
        mock_readlink.return_value = "/bogus"
        self.assertFalse(ptool._verify(entry))
        mock_readlink.assert_called_with(entry.get("name"))

    @patch("os.symlink")
    def test__link(self, mock_symlink):
        entry = lxml.etree.Element("Path", name="/test", type="symlink",
                                   to="/dest")
        ptool = self.get_obj()
        self.assertEqual(ptool._link(entry),
                         mock_symlink.return_value)
        mock_symlink.assert_called_with(entry.get("to"), entry.get("name"))

########NEW FILE########
__FILENAME__ = Test__init
import os
import sys
import lxml.etree
from mock import Mock, MagicMock, patch
import Bcfg2.Client.Tools
from Bcfg2.Client.Tools.POSIX import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestTools.Test_init import TestTool


def get_config(entries):
    config = lxml.etree.Element("Configuration")
    bundle = lxml.etree.SubElement(config, "Bundle", name="test")
    bundle.extend(entries)
    return config


class TestPOSIX(TestTool):
    test_obj = POSIX

    def test__init(self):
        entries = [lxml.etree.Element("Path", name="test", type="file")]
        posix = self.get_obj(config=get_config(entries))
        self.assertIsInstance(posix, Bcfg2.Client.Tools.Tool)
        self.assertIsInstance(posix, POSIX)
        self.assertIn('Path', posix.__req__)
        self.assertGreater(len(posix.__req__['Path']), 0)
        self.assertGreater(len(posix.__handles__), 0)
        self.assertItemsEqual(posix.handled, entries)

    @patch("Bcfg2.Client.Tools.Tool.canVerify")
    def test_canVerify(self, mock_canVerify):
        posix = self.get_obj()
        entry = lxml.etree.Element("Path", name="test", type="file")

        # first, test superclass canVerify failure
        mock_canVerify.return_value = False
        self.assertFalse(posix.canVerify(entry))
        mock_canVerify.assert_called_with(posix, entry)

        # next, test fully_specified failure
        posix.logger.error.reset_mock()
        mock_canVerify.reset_mock()
        mock_canVerify.return_value = True
        mock_fully_spec = Mock()
        mock_fully_spec.return_value = False
        posix._handlers[entry.get("type")].fully_specified = \
            mock_fully_spec
        self.assertFalse(posix.canVerify(entry))
        mock_canVerify.assert_called_with(posix, entry)
        mock_fully_spec.assert_called_with(entry)
        self.assertTrue(posix.logger.error.called)

        # finally, test success
        posix.logger.error.reset_mock()
        mock_canVerify.reset_mock()
        mock_fully_spec.reset_mock()
        mock_fully_spec.return_value = True
        self.assertTrue(posix.canVerify(entry))
        mock_canVerify.assert_called_with(posix, entry)
        mock_fully_spec.assert_called_with(entry)
        self.assertFalse(posix.logger.error.called)

    @patch("Bcfg2.Client.Tools.Tool.canInstall")
    def test_canInstall(self, mock_canInstall):
        posix = self.get_obj()
        entry = lxml.etree.Element("Path", name="test", type="file")

        # first, test superclass canInstall failure
        mock_canInstall.return_value = False
        self.assertFalse(posix.canInstall(entry))
        mock_canInstall.assert_called_with(posix, entry)

        # next, test fully_specified failure
        posix.logger.error.reset_mock()
        mock_canInstall.reset_mock()
        mock_canInstall.return_value = True
        mock_fully_spec = Mock()
        mock_fully_spec.return_value = False
        posix._handlers[entry.get("type")].fully_specified = \
            mock_fully_spec
        self.assertFalse(posix.canInstall(entry))
        mock_canInstall.assert_called_with(posix, entry)
        mock_fully_spec.assert_called_with(entry)
        self.assertTrue(posix.logger.error.called)

        # finally, test success
        posix.logger.error.reset_mock()
        mock_canInstall.reset_mock()
        mock_fully_spec.reset_mock()
        mock_fully_spec.return_value = True
        self.assertTrue(posix.canInstall(entry))
        mock_canInstall.assert_called_with(posix, entry)
        mock_fully_spec.assert_called_with(entry)
        self.assertFalse(posix.logger.error.called)

    def test_InstallPath(self):
        posix = self.get_obj()
        entry = lxml.etree.Element("Path", name="test", type="file")

        mock_install = Mock()
        mock_install.return_value = True
        posix._handlers[entry.get("type")].install = mock_install
        self.assertTrue(posix.InstallPath(entry))
        mock_install.assert_called_with(entry)

    def test_VerifyPath(self):
        posix = self.get_obj()
        entry = lxml.etree.Element("Path", name="test", type="file")
        modlist = []

        mock_verify = Mock()
        mock_verify.return_value = True
        posix._handlers[entry.get("type")].verify = mock_verify
        self.assertTrue(posix.VerifyPath(entry, modlist))
        mock_verify.assert_called_with(entry, modlist)

        mock_verify.reset_mock()
        mock_verify.return_value = False
        posix.setup.__getitem__.return_value = True
        self.assertFalse(posix.VerifyPath(entry, modlist))
        self.assertIsNotNone(entry.get('qtext'))

    @patch('os.remove')
    def test_prune_old_backups(self, mock_remove):
        entry = lxml.etree.Element("Path", name="/etc/foo", type="file")
        setup = dict(ppath='/', max_copies=5, paranoid=True)
        posix = self.get_obj(setup=setup)

        remove = ["_etc_foo_2012-07-20T04:13:22.364989",
                  "_etc_foo_2012-07-31T04:13:23.894958",
                  "_etc_foo_2012-07-17T04:13:22.493316",]
        keep = ["_etc_foo_bar_2011-08-07T04:13:22.519978",
                "_etc_foo_2012-08-04T04:13:22.519978",
                "_etc_Foo_2011-08-07T04:13:22.519978",
                "_etc_foo_2012-08-06T04:13:22.519978",
                "_etc_foo_2012-08-03T04:13:22.191895",
                "_etc_test_2011-08-07T04:13:22.519978",
                "_etc_foo_2012-08-07T04:13:22.519978",]

        @patch('os.listdir')
        def inner(mock_listdir):
            mock_listdir.side_effect = OSError
            posix._prune_old_backups(entry)
            self.assertTrue(posix.logger.error.called)
            self.assertFalse(mock_remove.called)
            mock_listdir.assert_called_with(setup['ppath'])

            mock_listdir.reset_mock()
            mock_remove.reset_mock()
            mock_listdir.side_effect = None
            mock_listdir.return_value = keep + remove

            posix._prune_old_backups(entry)
            mock_listdir.assert_called_with(setup['ppath'])
            self.assertItemsEqual(mock_remove.call_args_list,
                                  [call(os.path.join(setup['ppath'], p))
                                   for p in remove])

            mock_listdir.reset_mock()
            mock_remove.reset_mock()
            mock_remove.side_effect = OSError
            posix.logger.error.reset_mock()
            # test to ensure that we call os.remove() for all files that
            # need to be removed even if we get an error
            posix._prune_old_backups(entry)
            mock_listdir.assert_called_with(setup['ppath'])
            self.assertItemsEqual(mock_remove.call_args_list,
                                  [call(os.path.join(setup['ppath'], p))
                                   for p in remove])
            self.assertTrue(posix.logger.error.called)

        inner()

    @patch("shutil.copy")
    @patch("os.path.isdir")
    def test_paranoid_backup(self, mock_isdir, mock_copy):
        entry = lxml.etree.Element("Path", name="/etc/foo", type="file")
        setup = dict(ppath='/', max_copies=5, paranoid=False)
        posix = self.get_obj(setup=setup)
        posix._prune_old_backups = Mock()

        # paranoid false globally
        posix._paranoid_backup(entry)
        self.assertFalse(posix._prune_old_backups.called)
        self.assertFalse(mock_copy.called)

        # paranoid false on the entry
        setup['paranoid'] = True
        posix = self.get_obj(setup=setup)
        posix._prune_old_backups = Mock()

        def reset():
            mock_isdir.reset_mock()
            mock_copy.reset_mock()
            posix._prune_old_backups.reset_mock()

        reset()
        posix._paranoid_backup(entry)
        self.assertFalse(posix._prune_old_backups.called)
        self.assertFalse(mock_copy.called)

        # entry does not exist on filesystem
        reset()
        entry.set("paranoid", "true")
        entry.set("current_exists", "false")
        posix._paranoid_backup(entry)
        self.assertFalse(posix._prune_old_backups.called)
        self.assertFalse(mock_copy.called)

        # entry is a directory on the filesystem
        reset()
        entry.set("current_exists", "true")
        mock_isdir.return_value = True
        posix._paranoid_backup(entry)
        self.assertFalse(posix._prune_old_backups.called)
        self.assertFalse(mock_copy.called)
        mock_isdir.assert_called_with(entry.get("name"))

        # test the actual backup now
        reset()
        mock_isdir.return_value = False
        posix._paranoid_backup(entry)
        mock_isdir.assert_called_with(entry.get("name"))
        posix._prune_old_backups.assert_called_with(entry)
        # it's basically impossible to test the shutil.copy() call
        # exactly because the destination includes microseconds, so we
        # just test it good enough
        self.assertEqual(mock_copy.call_args[0][0],
                         entry.get("name"))
        bkupnam = os.path.join(setup['ppath'],
                               entry.get('name').replace('/', '_')) + '_'
        self.assertEqual(bkupnam, mock_copy.call_args[0][1][:len(bkupnam)])

########NEW FILE########
__FILENAME__ = TestPOSIXUsers
import os
import sys
import copy
import lxml.etree
import subprocess
from mock import Mock, MagicMock, patch
import Bcfg2.Client.Tools
from Bcfg2.Client.Tools.POSIXUsers import *
from Bcfg2.Utils import PackedDigitRange

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestTools.Test_init import TestTool


class TestPOSIXUsers(TestTool):
    test_obj = POSIXUsers

    def get_obj(self, logger=None, setup=None, config=None):
        if setup is None:
            setup = MagicMock()
            def getitem(key):
                if key == 'encoding':
                    return 'UTF-8'
                else:
                    return []

            setup.__getitem__.side_effect = getitem

        return TestTool.get_obj(self, logger, setup, config)

    @patch("pwd.getpwall")
    @patch("grp.getgrall")
    def test_existing(self, mock_getgrall, mock_getpwall):
        users = self.get_obj()
        mock_getgrall.return_value = MagicMock()
        mock_getpwall.return_value = MagicMock()

        def reset():
            mock_getgrall.reset_mock()
            mock_getpwall.reset_mock()

        # make sure we start clean
        self.assertIsNone(users._existing)
        self.assertIsInstance(users.existing, dict)
        self.assertIn("POSIXUser", users.existing)
        self.assertIn("POSIXGroup", users.existing)
        mock_getgrall.assert_called_with()
        mock_getpwall.assert_called_with()

        reset()
        self.assertIsInstance(users._existing, dict)
        self.assertIsInstance(users.existing, dict)
        self.assertEqual(users.existing, users._existing)
        self.assertIn("POSIXUser", users.existing)
        self.assertIn("POSIXGroup", users.existing)
        self.assertFalse(mock_getgrall.called)
        self.assertFalse(mock_getpwall.called)

        reset()
        users._existing = None
        self.assertIsInstance(users.existing, dict)
        self.assertIn("POSIXUser", users.existing)
        self.assertIn("POSIXGroup", users.existing)
        mock_getgrall.assert_called_with()
        mock_getpwall.assert_called_with()

    def test__in_managed_range(self):
        users = self.get_obj()
        users._whitelist = dict(POSIXGroup=PackedDigitRange("1-10"))
        users._blacklist = dict(POSIXGroup=PackedDigitRange("8-100"))
        self.assertTrue(users._in_managed_range("POSIXGroup", "9"))

        users._whitelist = dict(POSIXGroup=None)
        users._blacklist = dict(POSIXGroup=PackedDigitRange("8-100"))
        self.assertFalse(users._in_managed_range("POSIXGroup", "9"))

        users._whitelist = dict(POSIXGroup=None)
        users._blacklist = dict(POSIXGroup=PackedDigitRange("100-"))
        self.assertTrue(users._in_managed_range("POSIXGroup", "9"))

        users._whitelist = dict(POSIXGroup=PackedDigitRange("1-10"))
        users._blacklist = dict(POSIXGroup=None)
        self.assertFalse(users._in_managed_range("POSIXGroup", "25"))

    @patch("Bcfg2.Client.Tools.Tool.canInstall")
    def test_canInstall(self, mock_canInstall):
        users = self.get_obj()
        users._in_managed_range = Mock()
        users._in_managed_range.return_value = False
        mock_canInstall.return_value = False

        def reset():
            users._in_managed_range.reset()
            mock_canInstall.reset()

        # test failure of inherited method
        entry = lxml.etree.Element("POSIXUser", name="test")
        self.assertFalse(users.canInstall(entry))
        mock_canInstall.assertCalledWith(users, entry)

        # test with no uid specified
        reset()
        mock_canInstall.return_value = True
        self.assertTrue(users.canInstall(entry))
        mock_canInstall.assertCalledWith(users, entry)

        # test with uid specified, not in managed range
        reset()
        entry.set("uid", "1000")
        self.assertFalse(users.canInstall(entry))
        mock_canInstall.assertCalledWith(users, entry)
        users._in_managed_range.assert_called_with(entry.tag, "1000")

        # test with uid specified, in managed range
        reset()
        users._in_managed_range.return_value = True
        self.assertTrue(users.canInstall(entry))
        mock_canInstall.assertCalledWith(users, entry)
        users._in_managed_range.assert_called_with(entry.tag, "1000")

    @patch("Bcfg2.Client.Tools.Tool.Inventory")
    def test_Inventory(self, mock_Inventory):
        config = lxml.etree.Element("Configuration")
        bundle = lxml.etree.SubElement(config, "Bundle", name="test")
        lxml.etree.SubElement(bundle, "POSIXUser", name="test", group="test")
        lxml.etree.SubElement(bundle, "POSIXUser", name="test2", group="test2")
        lxml.etree.SubElement(bundle, "POSIXGroup", name="test2")

        orig_bundle = copy.deepcopy(bundle)

        users = self.get_obj(config=config)
        users.set_defaults['POSIXUser'] = Mock()
        users.set_defaults['POSIXUser'].side_effect = lambda e: e

        states = dict()
        self.assertEqual(users.Inventory(states),
                         mock_Inventory.return_value)
        mock_Inventory.assert_called_with(users, states, config.getchildren())
        lxml.etree.SubElement(orig_bundle, "POSIXGroup", name="test")
        self.assertXMLEqual(orig_bundle, bundle)

    def test_FindExtra(self):
        users = self.get_obj()
        users._in_managed_range = Mock()
        users._in_managed_range.side_effect = lambda t, i: i < 100

        def getSupportedEntries():
            return [lxml.etree.Element("POSIXUser", name="test1"),
                    lxml.etree.Element("POSIXGroup", name="test1")]

        users.getSupportedEntries = Mock()
        users.getSupportedEntries.side_effect = getSupportedEntries

        users._existing = dict(POSIXUser=dict(test1=("test1", "x", 15),
                                              test2=("test2", "x", 25),
                                              test3=("test3", "x", 115)),
                               POSIXGroup=dict(test2=("test2", "x", 25)))
        extra = users.FindExtra()
        self.assertEqual(len(extra), 2)
        self.assertItemsEqual([e.tag for e in extra],
                              ["POSIXUser", "POSIXGroup"])
        self.assertItemsEqual([e.get("name") for e in extra],
                              ["test2", "test2"])
        self.assertItemsEqual(users._in_managed_range.call_args_list,
                              [call("POSIXUser", 25),
                               call("POSIXUser", 115),
                               call("POSIXGroup", 25)])

    def test_populate_user_entry(self):
        users = self.get_obj()
        users._existing = dict(POSIXUser=dict(),
                               POSIXGroup=dict(root=('root', 'x', 0, [])))

        cases = [(lxml.etree.Element("POSIXUser", name="test"),
                  lxml.etree.Element("POSIXUser", name="test", group="test",
                                     gecos="test", shell="/bin/bash",
                                     home="/home/test")),
                 (lxml.etree.Element("POSIXUser", name="root", gecos="Root",
                                     shell="/bin/zsh"),
                  lxml.etree.Element("POSIXUser", name="root", group='root',
                                     gid='0', gecos="Root", shell="/bin/zsh",
                                     home='/root')),
                 (lxml.etree.Element("POSIXUser", name="test2", gecos="",
                                     shell="/bin/zsh"),
                  lxml.etree.Element("POSIXUser", name="test2", group='test2',
                                     gecos="", shell="/bin/zsh",
                                     home='/home/test2'))]

        for initial, expected in cases:
            actual = users.populate_user_entry(initial)
            self.assertXMLEqual(actual, expected)

    def test_user_supplementary_groups(self):
        users = self.get_obj()
        users._existing = \
            dict(POSIXUser=dict(),
                 POSIXGroup=dict(root=('root', 'x', 0, []),
                                 wheel=('wheel', 'x', 10, ['test']),
                                 users=('users', 'x', 100, ['test'])))
        entry = lxml.etree.Element("POSIXUser", name="test")
        self.assertItemsEqual(users.user_supplementary_groups(entry),
                              [users.existing['POSIXGroup']['wheel'],
                               users.existing['POSIXGroup']['users']])
        entry.set('name', 'test2')
        self.assertItemsEqual(users.user_supplementary_groups(entry), [])

    def test_VerifyPOSIXUser(self):
        users = self.get_obj()
        users._verify = Mock()
        users._verify.return_value = True
        users.populate_user_entry = Mock()
        users.user_supplementary_groups = Mock()
        users.user_supplementary_groups.return_value = \
            [('wheel', 'x', 10, ['test']), ('users', 'x', 100, ['test'])]

        def reset():
            users._verify.reset_mock()
            users.populate_user_entry.reset_mock()
            users.user_supplementary_groups.reset_mock()

        entry = lxml.etree.Element("POSIXUser", name="test")
        self.assertFalse(users.VerifyPOSIXUser(entry, []))
        users.populate_user_entry.assert_called_with(entry)
        users._verify.assert_called_with(users.populate_user_entry.return_value)
        users.user_supplementary_groups.assert_called_with(entry)

        reset()
        m1 = lxml.etree.SubElement(entry, "MemberOf", group="wheel")
        m2 = lxml.etree.SubElement(entry, "MemberOf")
        m2.text = "users"
        self.assertTrue(users.VerifyPOSIXUser(entry, []))
        users.populate_user_entry.assert_called_with(entry)
        users._verify.assert_called_with(users.populate_user_entry.return_value)
        users.user_supplementary_groups.assert_called_with(entry)

        reset()
        m3 = lxml.etree.SubElement(entry, "MemberOf", group="extra")
        self.assertFalse(users.VerifyPOSIXUser(entry, []))
        users.populate_user_entry.assert_called_with(entry)
        users._verify.assert_called_with(users.populate_user_entry.return_value)
        users.user_supplementary_groups.assert_called_with(entry)

        reset()
        def _verify(entry):
            entry.set("current_exists", "false")
            return False

        users._verify.side_effect = _verify
        self.assertFalse(users.VerifyPOSIXUser(entry, []))
        users.populate_user_entry.assert_called_with(entry)
        users._verify.assert_called_with(users.populate_user_entry.return_value)

    def test_VerifyPOSIXGroup(self):
        users = self.get_obj()
        users._verify = Mock()
        entry = lxml.etree.Element("POSIXGroup", name="test")
        self.assertEqual(users._verify.return_value,
                         users.VerifyPOSIXGroup(entry, []))

    def test__verify(self):
        users = self.get_obj()
        users._existing = \
            dict(POSIXUser=dict(test=('test', 'x', 1000, 1000, 'Test McTest',
                                      '/home/test', '/bin/zsh')),
                 POSIXGroup=dict(test=('test', 'x', 1000, [])))

        entry = lxml.etree.Element("POSIXUser", name="nonexistent")
        self.assertFalse(users._verify(entry))
        self.assertEqual(entry.get("current_exists"), "false")

        entry = lxml.etree.Element("POSIXUser", name="test", group="test",
                                   gecos="Bogus", shell="/bin/bash",
                                   home="/home/test")
        self.assertFalse(users._verify(entry))

        entry = lxml.etree.Element("POSIXUser", name="test", group="test",
                                   gecos="Test McTest", shell="/bin/zsh",
                                   home="/home/test")
        self.assertTrue(users._verify(entry))

        entry = lxml.etree.Element("POSIXUser", name="test", group="test",
                                   gecos="Test McTest", shell="/bin/zsh",
                                   home="/home/test", uid="1000", gid="1000")
        self.assertTrue(users._verify(entry))

        entry = lxml.etree.Element("POSIXUser", name="test", group="test",
                                   gecos="Test McTest", shell="/bin/zsh",
                                   home="/home/test", uid="1001")
        self.assertFalse(users._verify(entry))

    def test_Install(self):
        users = self.get_obj()
        users._install = Mock()
        users._existing = MagicMock()


        entries = [lxml.etree.Element("POSIXUser", name="test"),
                   lxml.etree.Element("POSIXGroup", name="test"),
                   lxml.etree.Element("POSIXUser", name="test2")]
        states = dict()

        users.Install(entries, states)
        self.assertItemsEqual(entries, states.keys())
        for state in states.values():
            self.assertEqual(state, users._install.return_value)
        # need to verify two things about _install calls:
        # 1) _install was called for each entry;
        # 2) _install was called for all groups before any users
        self.assertItemsEqual(users._install.call_args_list,
                              [call(e) for e in entries])
        users_started = False
        for args in users._install.call_args_list:
            if args[0][0].tag == "POSIXUser":
                users_started = True
            elif users_started:
                assert False, "_install() called on POSIXGroup after installing one or more POSIXUsers"

    def test__install(self):
        users = self.get_obj()
        users._get_cmd = Mock()
        users.cmd = Mock()
        users.set_defaults = dict(POSIXUser=Mock(), POSIXGroup=Mock())
        users._existing = \
            dict(POSIXUser=dict(test=('test', 'x', 1000, 1000, 'Test McTest',
                                      '/home/test', '/bin/zsh')),
                 POSIXGroup=dict(test=('test', 'x', 1000, [])))

        def reset():
            users._get_cmd.reset_mock()
            users.cmd.reset_mock()
            for setter in users.set_defaults.values():
                setter.reset_mock()
            users.modified = []

        cmd_rv = Mock()
        cmd_rv.success = True
        users.cmd.run.return_value = cmd_rv

        reset()
        entry = lxml.etree.Element("POSIXUser", name="test2")
        self.assertTrue(users._install(entry))
        users.set_defaults[entry.tag].assert_called_with(entry)
        users._get_cmd.assert_called_with("add",
                                          users.set_defaults[entry.tag].return_value)
        users.cmd.run.assert_called_with(users._get_cmd.return_value)
        self.assertIn(entry, users.modified)

        reset()
        entry = lxml.etree.Element("POSIXUser", name="test")
        self.assertTrue(users._install(entry))
        users.set_defaults[entry.tag].assert_called_with(entry)
        users._get_cmd.assert_called_with("mod",
                                          users.set_defaults[entry.tag].return_value)
        users.cmd.run.assert_called_with(users._get_cmd.return_value)
        self.assertIn(entry, users.modified)

        reset()
        cmd_rv.success = False
        self.assertFalse(users._install(entry))
        users.set_defaults[entry.tag].assert_called_with(entry)
        users._get_cmd.assert_called_with("mod",
                                          users.set_defaults[entry.tag].return_value)
        users.cmd.run.assert_called_with(users._get_cmd.return_value)
        self.assertNotIn(entry, users.modified)

    def test__get_cmd(self):
        users = self.get_obj()

        entry = lxml.etree.Element("POSIXUser", name="test", group="test",
                                   home="/home/test", shell="/bin/zsh",
                                   gecos="Test McTest")
        m1 = lxml.etree.SubElement(entry, "MemberOf", group="wheel")
        m2 = lxml.etree.SubElement(entry, "MemberOf")
        m2.text = "users"

        cases = [(lxml.etree.Element("POSIXGroup", name="test"), []),
                 (lxml.etree.Element("POSIXGroup", name="test", gid="1001"),
                  ["-g", "1001"]),
                 (lxml.etree.Element("POSIXUser", name="test", group="test",
                                     home="/home/test", shell="/bin/zsh",
                                     gecos="Test McTest"),
                  ["-g", "test", "-d", "/home/test", "-s", "/bin/zsh",
                   "-c", "Test McTest"]),
                 (lxml.etree.Element("POSIXUser", name="test", group="test",
                                     home="/home/test", shell="/bin/zsh",
                                     gecos="Test McTest", uid="1001"),
                  ["-u", "1001", "-g", "test", "-d", "/home/test",
                   "-s", "/bin/zsh", "-c", "Test McTest"]),
                 (entry,
                  ["-g", "test", "-G", "wheel,users", "-d", "/home/test",
                   "-s", "/bin/zsh", "-c", "Test McTest"])]
        for entry, expected in cases:
            for action in ["add", "mod", "del"]:
                actual = users._get_cmd(action, entry)
                if entry.tag == "POSIXGroup":
                    etype = "group"
                else:
                    etype = "user"
                self.assertEqual(actual[0], "/usr/sbin/%s%s" % (etype, action))
                self.assertEqual(actual[-1], entry.get("name"))
                if action != "del":
                    self.assertItemsEqual(actual[1:-1], expected)

    @patch("grp.getgrnam")
    def test_Remove(self, mock_getgrnam):
        users = self.get_obj()
        users._remove = Mock()
        users.FindExtra = Mock()
        users._existing = MagicMock()
        users.extra = MagicMock()

        def reset():
            users._remove.reset_mock()
            users.FindExtra.reset_mock()
            users._existing = MagicMock()
            users.extra = MagicMock()
            mock_getgrnam.reset_mock()

        entries = [lxml.etree.Element("POSIXUser", name="test"),
                   lxml.etree.Element("POSIXGroup", name="test"),
                   lxml.etree.Element("POSIXUser", name="test2")]

        users.Remove(entries)
        self.assertIsNone(users._existing)
        users.FindExtra.assert_called_with()
        self.assertEqual(users.extra, users.FindExtra.return_value)
        mock_getgrnam.assert_called_with("test")
        # need to verify two things about _remove calls:
        # 1) _remove was called for each entry;
        # 2) _remove was called for all users before any groups
        self.assertItemsEqual(users._remove.call_args_list,
                              [call(e) for e in entries])
        groups_started = False
        for args in users._remove.call_args_list:
            if args[0][0].tag == "POSIXGroup":
                groups_started = True
            elif groups_started:
                assert False, "_remove() called on POSIXUser after removing one or more POSIXGroups"

        reset()
        mock_getgrnam.side_effect = KeyError
        users.Remove(entries)
        self.assertIsNone(users._existing)
        users.FindExtra.assert_called_with()
        self.assertEqual(users.extra, users.FindExtra.return_value)
        mock_getgrnam.assert_called_with("test")
        self.assertItemsEqual(users._remove.call_args_list,
                              [call(e) for e in entries
                               if e.tag == "POSIXUser"])

    def test__remove(self):
        users = self.get_obj()
        users._get_cmd = Mock()
        users.cmd = Mock()
        cmd_rv = Mock()
        cmd_rv.success = True
        users.cmd.run.return_value = cmd_rv

        def reset():
            users._get_cmd.reset_mock()
            users.cmd.reset_mock()

        entry = lxml.etree.Element("POSIXUser", name="test2")
        self.assertTrue(users._remove(entry))
        users._get_cmd.assert_called_with("del", entry)
        users.cmd.run.assert_called_with(users._get_cmd.return_value)

        reset()
        cmd_rv.success = False
        self.assertFalse(users._remove(entry))
        users._get_cmd.assert_called_with("del", entry)
        users.cmd.run.assert_called_with(users._get_cmd.return_value)

########NEW FILE########
__FILENAME__ = Test_init
import os
import sys
import lxml.etree
from mock import Mock, MagicMock, patch
from Bcfg2.Compat import long
from Bcfg2.Client.Tools import Tool, SvcTool, PkgTool, \
    ToolInstantiationError

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *


class TestTool(Bcfg2TestCase):
    test_obj = Tool

    def get_obj(self, logger=None, setup=None, config=None):
        if config is None:
            config = lxml.etree.Element("Configuration")
        if not logger:
            def print_msg(msg):
                print(msg)
            logger = Mock()
            logger.error = Mock(side_effect=print_msg)
            logger.warning = Mock(side_effect=print_msg)
            logger.info = Mock(side_effect=print_msg)
            logger.debug = Mock(side_effect=print_msg)
        if not setup:
            setup = MagicMock()
        if 'command_timeout' not in setup:
            setup['command_timeout'] = None
        execs = self.test_obj.__execs__
        self.test_obj.__execs__ = []
        rv = self.test_obj(logger, setup, config)
        self.test_obj.__execs__ = execs
        return rv

    def test__init(self):
        @patch("%s.%s._check_execs" % (self.test_obj.__module__,
                                       self.test_obj.__name__))
        @patch("%s.%s._analyze_config" % (self.test_obj.__module__,
                                          self.test_obj.__name__))
        def inner(mock_analyze_config, mock_check_execs):
            t = self.get_obj()
            mock_analyze_config.assert_called_with()
            mock_check_execs.assert_called_with()

    def test__analyze_config(self):
        t = self.get_obj()
        t.getSupportedEntries = Mock()

        t.__important__ = ["/test"]
        important = []
        t.config = lxml.etree.Element("Config")
        bundle1 = lxml.etree.SubElement(t.config, "Bundle")
        important.append(lxml.etree.SubElement(bundle1, "Path",
                                               name="/foo", important="true"))
        lxml.etree.SubElement(bundle1, "Package", name="bar", important="true")
        lxml.etree.SubElement(bundle1, "Path", name="/bar")
        bundle2 = lxml.etree.SubElement(t.config, "Bundle")
        important.append(lxml.etree.SubElement(bundle2, "Path", name="/quux",
                                               important="true"))
        lxml.etree.SubElement(bundle2, "Path", name="/baz", important="false")

        t._analyze_config()
        self.assertItemsEqual(t.__important__,
                              ["/test"] + [e.get("name") for e in important])
        t.getSupportedEntries.assert_called_with()

    def test__check_execs(self):
        t = self.get_obj()
        if t.__execs__ == []:
            t.__execs__.append("/bin/true")

        @patch("os.stat")
        def inner(mock_stat):
            mock_stat.return_value = (33261, 2245040, long(64770), 1, 0, 0,
                                      25552, 1360831382, 1352194410,
                                      1354626626)
            t._check_execs()
            self.assertItemsEqual(mock_stat.call_args_list,
                                  [call(e) for e in t.__execs__])

            # not executable
            mock_stat.reset_mock()
            mock_stat.return_value = (33188, 2245040, long(64770), 1, 0, 0,
                                      25552, 1360831382, 1352194410,
                                      1354626626)
            self.assertRaises(ToolInstantiationError, t._check_execs)

            # non-existant
            mock_stat.reset_mock()
            mock_stat.side_effect = OSError
            self.assertRaises(ToolInstantiationError, t._check_execs)

        inner()

    def test_BundleUpdated(self):
        pass

    def test_BundleNotUpdated(self):
        pass

    def test_Inventory(self):
        t = self.get_obj()
        t.canVerify = Mock()
        t.canVerify.side_effect = lambda e: e.get("verify") != "false"
        t.buildModlist = Mock()
        t.FindExtra = Mock()
        t.VerifyPath = Mock()
        t.VerifyPackage = Mock()
        t.VerifyService = Mock()

        def reset():
            t.canVerify.reset_mock()
            t.buildModlist.reset_mock()
            t.FindExtra.reset_mock()
            t.VerifyPath.reset_mock()
            t.VerifyPackage.reset_mock()
            t.VerifyService.reset_mock()

        paths = []
        packages = []
        services = []
        config = lxml.etree.Element("Configuration")
        bundle1 = lxml.etree.SubElement(config, "Bundle")
        paths.append(lxml.etree.SubElement(bundle1, "Path", name="/foo"))
        lxml.etree.SubElement(bundle1, "Package", name="foo", verify="false")
        packages.append(lxml.etree.SubElement(bundle1, "Package", name="bar"))
        lxml.etree.SubElement(bundle1, "Bogus")

        bundle2 = lxml.etree.SubElement(config, "Bundle")
        paths.append(lxml.etree.SubElement(bundle2, "Path", name="/bar"))
        services.append(lxml.etree.SubElement(bundle2, "Service", name="bar"))
        lxml.etree.SubElement(bundle2, "Path", name="/baz", verify="false")

        expected_states = dict([(e, t.VerifyPath.return_value)
                                for e in paths])
        expected_states.update(dict([(e, t.VerifyPackage.return_value)
                                     for e in packages]))
        expected_states.update(dict([(e, t.VerifyService.return_value)
                                     for e in services]))

        def perform_assertions(states):
            t.buildModlist.assert_called_with()
            t.FindExtra.assert_called_with()
            self.assertItemsEqual(t.canVerify.call_args_list,
                                  [call(e) for e in bundle1.getchildren()] + \
                                      [call(e) for e in bundle2.getchildren()])
            self.assertItemsEqual(t.VerifyPath.call_args_list,
                                  [call(e, t.buildModlist.return_value)
                                   for e in paths])
            self.assertItemsEqual(t.VerifyPackage.call_args_list,
                                  [call(e, t.buildModlist.return_value)
                                   for e in packages])
            self.assertItemsEqual(t.VerifyService.call_args_list,
                                  [call(e, t.buildModlist.return_value)
                                   for e in services])
            self.assertItemsEqual(states, expected_states)
            self.assertEqual(t.extra, t.FindExtra.return_value)

        actual_states = dict()
        t.Inventory(actual_states, structures=[bundle1, bundle2])
        perform_assertions(actual_states)

        reset()
        actual_states = dict()
        t.config = config
        t.Inventory(actual_states)
        perform_assertions(actual_states)

    def test_Install(self):
        t = self.get_obj()
        t.InstallPath = Mock()
        t.InstallPackage = Mock()
        t.InstallService = Mock()

        t.InstallPath.side_effect = lambda e: e.get("modified") == "true"
        t.InstallPackage.side_effect = lambda e: e.get("modified") == "true"
        t.InstallService.side_effect = lambda e: e.get("modified") == "true"

        entries = [lxml.etree.Element("Path", name="/foo", modified="true"),
                   lxml.etree.Element("Package", name="bar", modified="true"),
                   lxml.etree.Element("Bogus"),
                   lxml.etree.Element("Path", name="/bar", modified="true"),
                   lxml.etree.Element("Service", name="bar")]

        expected_states = dict([(e, t.InstallPath.return_value)
                                for e in entries if e.tag == "Path"])
        expected_states.update(dict([(e, t.InstallPackage.return_value)
                                     for e in entries if e.tag == "Package"]))
        expected_states.update(dict([(e, t.InstallService.return_value)
                                     for e in entries if e.tag == "Service"]))

        actual_states = dict()
        t.modified = []
        t.Install(entries, actual_states)
        self.assertItemsEqual(t.InstallPath.call_args_list,
                              [call(e) for e in entries if e.tag == "Path"])
        self.assertItemsEqual(t.InstallPackage.call_args_list,
                              [call(e) for e in entries if e.tag == "Package"])
        self.assertItemsEqual(t.InstallService.call_args_list,
                              [call(e) for e in entries if e.tag == "Service"])
        self.assertItemsEqual(actual_states, expected_states)
        self.assertItemsEqual(t.modified,
                              [e for e in entries
                               if e.get("modified") == "true"])

    def rest_Remove(self):
        pass

    def test_getSupportedEntries(self):
        t = self.get_obj()

        def handlesEntry(entry):
            return entry.get("handled") == "true"
        t.handlesEntry = Mock()
        t.handlesEntry.side_effect = handlesEntry

        handled = []
        t.config = lxml.etree.Element("Config")
        bundle1 = lxml.etree.SubElement(t.config, "Bundle")
        lxml.etree.SubElement(bundle1, "Path", name="/foo")
        handled.append(lxml.etree.SubElement(bundle1, "Path", name="/bar",
                                             handled="true"))
        bundle2 = lxml.etree.SubElement(t.config, "Bundle")
        handled.append(lxml.etree.SubElement(bundle2, "Path", name="/quux",
                                             handled="true"))
        lxml.etree.SubElement(bundle2, "Path", name="/baz")

        self.assertItemsEqual(handled,
                              t.getSupportedEntries())

    def test_handlesEntry(self):
        t = self.get_obj()
        handles = t.__handles__
        t.__handles__ = [("Path", "file"),
                         ("Package", "yum")]
        self.assertTrue(t.handlesEntry(lxml.etree.Element("Path", type="file",
                                                          name="/foo")))
        self.assertFalse(t.handlesEntry(lxml.etree.Element("Path",
                                                           type="permissions",
                                                           name="/bar")))
        self.assertFalse(t.handlesEntry(lxml.etree.Element("Bogus",
                                                           type="file",
                                                           name="/baz")))
        self.assertTrue(t.handlesEntry(lxml.etree.Element("Package",
                                                          type="yum",
                                                          name="quux")))
        t.__handles__ = handles

    def test_buildModlist(self):
        t = self.get_obj()
        paths = []

        t.config = lxml.etree.Element("Config")
        bundle1 = lxml.etree.SubElement(t.config, "Bundle")
        paths.append(lxml.etree.SubElement(bundle1, "Path", name="/foo"))
        lxml.etree.SubElement(bundle1, "Package", name="bar")
        paths.append(lxml.etree.SubElement(bundle1, "Path", name="/bar"))
        bundle2 = lxml.etree.SubElement(t.config, "Bundle")
        paths.append(lxml.etree.SubElement(bundle2, "Path", name="/quux"))
        lxml.etree.SubElement(bundle2, "Service", name="baz")

        self.assertItemsEqual([p.get("name") for p in paths],
                              t.buildModlist())

    def test_missing_attrs(self):
        t = self.get_obj()
        req = t.__req__
        t.__req__ = dict(Path=dict(file=["name"],
                                   permissions=["name", "owner", "group"]),
                         Package=["name"])
        # tuples of <entry>, <return value>
        cases = [
            (lxml.etree.Element("Path", name="/foo"), ["type"]),
            (lxml.etree.Element("Path", type="file"), ["name"]),
            (lxml.etree.Element("Path", type="file", name="/foo"), []),
            (lxml.etree.Element("Path", type="permissions", name="/foo"),
             ["owner", "group"]),
            (lxml.etree.Element("Path", type="permissions", name="/foo",
                                owner="root", group="root", mode="0644"), []),
            (lxml.etree.Element("Package", type="yum"), ["name"]),
            (lxml.etree.Element("Package", type="yum", name="/bar"), []),
            (lxml.etree.Element("Package", type="apt", name="/bar"), [])]
        for entry, expected in cases:
            self.assertItemsEqual(t.missing_attrs(entry), expected)

        t.__req__ = req

    def test_canVerify(self):
        t = self.get_obj()
        entry = Mock()
        t._entry_is_complete = Mock()
        self.assertEqual(t.canVerify(entry),
                         t._entry_is_complete.return_value)
        t._entry_is_complete.assert_called_with(entry, action="verify")

    def test_FindExtra(self):
        t = self.get_obj()
        self.assertItemsEqual(t.FindExtra(), [])

    def test_canInstall(self):
        t = self.get_obj()
        entry = Mock()
        t._entry_is_complete = Mock()
        self.assertEqual(t.canInstall(entry),
                         t._entry_is_complete.return_value)
        t._entry_is_complete.assert_called_with(entry, action="install")

    def test__entry_is_complete(self):
        t = self.get_obj()
        t.handlesEntry = Mock()
        t.missing_attrs = Mock()

        def reset():
            t.handlesEntry.reset_mock()
            t.missing_attrs.reset_mock()

        entry = lxml.etree.Element("Path", name="/test")

        t.handlesEntry.return_value = False
        t.missing_attrs.return_value = []
        self.assertFalse(t._entry_is_complete(entry))

        reset()
        t.handlesEntry.return_value = True
        t.missing_attrs.return_value = ["type"]
        self.assertFalse(t._entry_is_complete(entry))

        reset()
        t.missing_attrs.return_value = []
        self.assertTrue(t._entry_is_complete(entry))

        reset()
        entry.set("failure", "failure")
        self.assertFalse(t._entry_is_complete(entry))


class TestPkgTool(TestTool):
    test_obj = PkgTool

    def get_obj(self, **kwargs):
        @patch("%s.%s.RefreshPackages" % (self.test_obj.__module__,
                                          self.test_obj.__name__), Mock())
        def inner():
            return TestTool.get_obj(self, **kwargs)

        return inner()

    def test_VerifyPackage(self):
        pt = self.get_obj()
        self.assertRaises(NotImplementedError,
                          pt.VerifyPackage, Mock(), Mock())

    def test_Install(self):
        pt = self.get_obj()
        pt.cmd = Mock()
        pt.RefreshPackages = Mock()
        pt.VerifyPackage = Mock()
        pt._get_package_command = Mock()
        pt._get_package_command.side_effect = lambda pkgs: \
            [p.get("name") for p in pkgs]
        packages = [lxml.etree.Element("Package", type="echo", name="foo",
                                       version="1.2.3"),
                    lxml.etree.Element("Package", type="echo", name="bar",
                                       version="any"),
                    lxml.etree.Element("Package", type="echo", name="baz",
                                       version="2.3.4")]

        def reset():
            pt.cmd.reset_mock()
            pt.RefreshPackages.reset_mock()
            pt.VerifyPackage.reset_mock()
            pt._get_package_command.reset_mock()
            pt.modified = []

        # test single-pass install success
        reset()
        pt.cmd.run.return_value = True
        states = dict([(p, False) for p in packages])
        pt.Install(packages, states)
        pt._get_package_command.assert_called_with(packages)
        pt.cmd.run.assert_called_with([p.get("name") for p in packages])
        self.assertItemsEqual(states,
                              dict([(p, True) for p in packages]))
        self.assertItemsEqual(pt.modified, packages)

        # test failed single-pass install
        reset()

        def run(cmd):
            if "foo" in cmd:
                # fail when installing all packages, and when installing foo
                return False
            # succeed otherwise
            return True

        pt.VerifyPackage.side_effect = lambda p, m: p.get("name") == "bar"

        pt.cmd.run.side_effect = run
        states = dict([(p, False) for p in packages])
        pt.Install(packages, states)
        pt._get_package_command.assert_any_call(packages)
        for pkg in packages:
            pt.VerifyPackage.assert_any_call(pkg, [])
            if pkg.get("name") != "bar":
                pt._get_package_command.assert_any_call([pkg])
        # pt.cmd.run is called once for all packages, and then once
        # for each package that does not verify.  "bar" verifies, so
        # it's run for foo and baz
        self.assertItemsEqual(pt.cmd.run.call_args_list,
                              [call([p.get("name") for p in packages]),
                               call(["foo"]),
                               call(["baz"])])
        pt.RefreshPackages.assert_called_with()
        self.assertItemsEqual(states,
                              dict([(p, p.get("name") != "bar")
                                    for p in packages]))
        # bar is modified, because it verifies successfully; baz is
        # modified, because it is installed successfully.  foo is not
        # installed successfully, so is not modified.
        self.assertItemsEqual(pt.modified,
                              [p for p in packages if p.get("name") != "foo"])

    def test__get_package_command(self):
        packages = [lxml.etree.Element("Package", type="test", name="foo",
                                       version="1.2.3"),
                    lxml.etree.Element("Package", type="test", name="bar",
                                       version="any"),
                    lxml.etree.Element("Package", type="test", name="baz",
                                       version="2.3.4")]
        pt = self.get_obj()
        pkgtool = pt.pkgtool
        pt.pkgtool = ("install %s", ("%s-%s", ["name", "version"]))
        self.assertEqual(pt._get_package_command([
                    lxml.etree.Element("Package", type="test", name="foo",
                                       version="1.2.3")]),
                         "install foo-1.2.3")
        self.assertItemsEqual(pt._get_package_command(packages).split(),
                              ["install", "foo-1.2.3", "bar-any", "baz-2.3.4"])

    def test_RefreshPackages(self):
        pt = self.get_obj()
        self.assertRaises(NotImplementedError, pt.RefreshPackages)

    def test_FindExtra(self):
        pt = self.get_obj()
        pt.getSupportedEntries = Mock()
        pt.getSupportedEntries.return_value = [
            lxml.etree.Element("Package", name="foo"),
            lxml.etree.Element("Package", name="bar"),
            lxml.etree.Element("Package", name="baz")]
        pt.installed = dict(foo="1.2.3",
                            bar="2.3.4",
                            quux="3.4.5",
                            xyzzy="4.5.6")
        extra = pt.FindExtra()
        self.assertEqual(len(extra), 2)
        self.assertItemsEqual([e.get("name") for e in extra],
                              ["quux", "xyzzy"])
        for el in extra:
            self.assertEqual(el.tag, "Package")
            self.assertEqual(el.get("type"), pt.pkgtype)


class TestSvcTool(TestTool):
    test_obj = SvcTool

    def test_start_service(self):
        st = self.get_obj()
        st.get_svc_command = Mock()
        st.cmd = MagicMock()
        service = lxml.etree.Element("Service", name="foo", type="test")
        self.assertEqual(st.start_service(service),
                         st.cmd.run.return_value)
        st.get_svc_command.assert_called_with(service, "start")
        st.cmd.run.assert_called_with(st.get_svc_command.return_value)

    def test_stop_service(self):
        st = self.get_obj()
        st.get_svc_command = Mock()
        st.cmd = MagicMock()
        service = lxml.etree.Element("Service", name="foo", type="test")
        self.assertEqual(st.stop_service(service),
                         st.cmd.run.return_value)
        st.get_svc_command.assert_called_with(service, "stop")
        st.cmd.run.assert_called_with(st.get_svc_command.return_value)

    def test_restart_service(self):
        st = self.get_obj()
        st.get_svc_command = Mock()
        st.cmd = MagicMock()

        def reset():
            st.get_svc_command.reset_mock()
            st.cmd.reset_mock()

        service = lxml.etree.Element("Service", name="foo", type="test")
        self.assertEqual(st.restart_service(service),
                         st.cmd.run.return_value)
        st.get_svc_command.assert_called_with(service, "restart")
        st.cmd.run.assert_called_with(st.get_svc_command.return_value)

        reset()
        service.set('target', 'reload')
        self.assertEqual(st.restart_service(service),
                         st.cmd.run.return_value)
        st.get_svc_command.assert_called_with(service, "reload")
        st.cmd.run.assert_called_with(st.get_svc_command.return_value)

    def test_check_service(self):
        st = self.get_obj()
        st.get_svc_command = Mock()
        st.cmd = MagicMock()
        service = lxml.etree.Element("Service", name="foo", type="test")

        def reset():
            st.get_svc_command.reset_mock()
            st.cmd.reset_mock()

        st.cmd.run.return_value = True
        self.assertEqual(st.check_service(service), True)
        st.get_svc_command.assert_called_with(service, "status")
        st.cmd.run.assert_called_with(st.get_svc_command.return_value)

        reset()
        st.cmd.run.return_value = False
        self.assertEqual(st.check_service(service), False)
        st.get_svc_command.assert_called_with(service, "status")
        st.cmd.run.assert_called_with(st.get_svc_command.return_value)

    def test_Remove(self):
        st = self.get_obj()
        st.InstallService = Mock()
        services = [lxml.etree.Element("Service", type="test", name="foo"),
                    lxml.etree.Element("Service", type="test", name="bar",
                                       status="on")]
        st.Remove(services)
        self.assertItemsEqual(st.InstallService.call_args_list,
                              [call(e) for e in services])
        for entry in services:
            self.assertEqual(entry.get("status"), "off")

    @patch("Bcfg2.Client.prompt")
    def test_BundleUpdated(self, mock_prompt):
        st = self.get_obj(setup=dict(interactive=False,
                                     servicemode='default'))
        st.handlesEntry = Mock()
        st.handlesEntry.side_effect = lambda e: e.tag == "Service"
        st.stop_service = Mock()
        st.stop_service.return_value = 0
        st.restart_service = Mock()
        st.restart_service.side_effect = lambda e: \
            int(e.get("name") != "failed")

        def reset():
            st.handlesEntry.reset_mock()
            st.stop_service.reset_mock()
            st.restart_service.reset_mock()
            mock_prompt.reset_mock()
            st.restarted = []

        norestart = lxml.etree.Element("Service", type="test",
                                       name="norestart", restart="false")
        interactive = lxml.etree.Element("Service", type="test",
                                         name="interactive", status="on",
                                         restart="interactive")
        interactive2 = lxml.etree.Element("Service", type="test",
                                          name="interactive2", status="on",
                                          restart="interactive")
        stop = lxml.etree.Element("Service", type="test", name="stop",
                                  status="off")
        restart = lxml.etree.Element("Service", type="test", name="restart",
                                     status="on")
        duplicate = lxml.etree.Element("Service", type="test", name="restart",
                                       status="on")
        failed = lxml.etree.Element("Service", type="test", name="failed",
                                    status="on")
        unhandled = lxml.etree.Element("Path", type="file", name="/unhandled")
        services = [norestart, interactive, interactive2, stop, restart,
                    duplicate, failed]
        entries = services + [unhandled]
        bundle = lxml.etree.Element("Bundle")
        bundle.extend(entries)

        # test in non-interactive mode
        reset()
        states = dict()
        st.BundleUpdated(bundle, states)
        self.assertItemsEqual(st.handlesEntry.call_args_list,
                              [call(e) for e in entries])
        st.stop_service.assert_called_with(stop)
        self.assertItemsEqual(st.restart_service.call_args_list,
                              [call(restart), call(failed)])
        self.assertItemsEqual(st.restarted, [restart.get("name")])
        self.assertFalse(mock_prompt.called)

        # test in interactive mode
        reset()
        mock_prompt.side_effect = lambda p: "interactive2" not in p
        st.setup['interactive'] = True
        states = dict()
        st.BundleUpdated(bundle, states)
        self.assertItemsEqual(st.handlesEntry.call_args_list,
                              [call(e) for e in entries])
        st.stop_service.assert_called_with(stop)
        self.assertItemsEqual(st.restart_service.call_args_list,
                              [call(restart), call(failed), call(interactive)])
        self.assertItemsEqual(st.restarted, [restart.get("name"),
                                             interactive.get("name")])
        self.assertEqual(len(mock_prompt.call_args_list), 4)

        # test in build mode
        reset()
        st.setup['interactive'] = False
        st.setup['servicemode'] = 'build'
        states = dict()
        st.BundleUpdated(bundle, states)
        self.assertItemsEqual(st.handlesEntry.call_args_list,
                              [call(e) for e in entries])
        self.assertItemsEqual(st.stop_service.call_args_list,
                              [call(restart), call(duplicate), call(failed),
                               call(stop)])
        self.assertFalse(mock_prompt.called)
        self.assertFalse(st.restart_service.called)
        self.assertItemsEqual(st.restarted, [])

    @patch("Bcfg2.Client.Tools.Tool.Install")
    def test_Install(self, mock_Install):
        install = [lxml.etree.Element("Service", type="test", name="foo")]
        services = install + [lxml.etree.Element("Service", type="test",
                                                 name="bar", install="false")]
        st = self.get_obj()
        states = Mock()
        self.assertEqual(st.Install(services, states),
                         mock_Install.return_value)
        mock_Install.assert_called_with(st, install, states)

    def test_InstallService(self):
        st = self.get_obj()
        self.assertRaises(NotImplementedError, st.InstallService, Mock())

########NEW FILE########
__FILENAME__ = TestEncryption
# -*- coding: utf-8 -*-
import os
import sys
from Bcfg2.Compat import b64decode
from mock import Mock, MagicMock, patch

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *

try:
    from Bcfg2.Encryption import *
    HAS_CRYPTO = True
except ImportError:
    HAS_CRYPTO = False


if can_skip or HAS_CRYPTO:
    class TestEncryption(Bcfg2TestCase):
        plaintext = """foo bar
baz

\t\tquux
""" + "a" * 16384  # 16K is completely arbitrary
        iv = "0123456789ABCDEF"
        salt = "01234567"
        algo = "des_cbc"

        @skipUnless(HAS_CRYPTO, "Encryption libraries not found")
        def setUp(self):
            pass

        def test_str_crypt(self):
            """ test str_encrypt/str_decrypt """
            key = "a simple key"

            # simple symmetrical test with no options
            crypted = str_encrypt(self.plaintext, key)
            self.assertEqual(self.plaintext, str_decrypt(crypted, key))

            # symmetrical test with lots of options
            crypted = str_encrypt(self.plaintext, key,
                                  iv=self.iv, salt=self.salt,
                                  algorithm=self.algo)
            self.assertEqual(self.plaintext,
                             str_decrypt(crypted, key, iv=self.iv,
                                         algorithm=self.algo))

            # test that different algorithms are actually used
            self.assertNotEqual(str_encrypt(self.plaintext, key),
                                str_encrypt(self.plaintext, key,
                                            algorithm=self.algo))

            # test that different keys are actually used
            self.assertNotEqual(str_encrypt(self.plaintext, key),
                                str_encrypt(self.plaintext, "different key"))

            # test that different IVs are actually used
            self.assertNotEqual(str_encrypt(self.plaintext, key, iv=self.iv),
                                str_encrypt(self.plaintext, key))

            # test that errors are raised on bad decrypts
            crypted = str_encrypt(self.plaintext, key, algorithm=self.algo)
            self.assertRaises(EVPError, str_decrypt,
                              crypted, "bogus key", algorithm=self.algo)
            self.assertRaises(EVPError, str_decrypt,
                              crypted, key)  # bogus algorithm

        def test_ssl_crypt(self):
            """ test ssl_encrypt/ssl_decrypt """
            passwd = "a simple passphrase"

            # simple symmetrical test
            crypted = ssl_encrypt(self.plaintext, passwd)
            self.assertEqual(self.plaintext, ssl_decrypt(crypted, passwd))

            # more complex symmetrical test
            crypted = ssl_encrypt(self.plaintext, passwd, algorithm=self.algo,
                                  salt=self.salt)
            self.assertEqual(self.plaintext,
                             ssl_decrypt(crypted, passwd, algorithm=self.algo))

            # test that different algorithms are actually used
            self.assertNotEqual(ssl_encrypt(self.plaintext, passwd),
                                ssl_encrypt(self.plaintext, passwd,
                                            algorithm=self.algo))

            # test that different passwords are actually used
            self.assertNotEqual(ssl_encrypt(self.plaintext, passwd),
                                ssl_encrypt(self.plaintext, "different pass"))

            # there's no reasonable test we can do to see if the
            # output is base64-encoded, unfortunately, but if it's
            # obviously not we fail
            crypted = ssl_encrypt(self.plaintext, passwd)
            self.assertRegexpMatches(crypted, r'^[A-Za-z0-9+/]+[=]{0,2}$')

            # test that errors are raised on bad decrypts
            crypted = ssl_encrypt(self.plaintext, passwd,
                                  algorithm=self.algo)
            self.assertRaises(EVPError, ssl_decrypt,
                              crypted, "bogus passwd", algorithm=self.algo)
            self.assertRaises(EVPError, ssl_decrypt,
                              crypted, passwd)  # bogus algorithm

        def test_get_algorithm(self):
            setup = Mock()
            # we don't care what the default is, as long as there is
            # one
            setup.cfp.get.return_value = ALGORITHM
            self.assertRegexpMatches(get_algorithm(setup),
                                     r'^[a-z0-9]+_[a-z0-9_]+$')
            setup.cfp.get.assert_called_with(CFG_SECTION, CFG_ALGORITHM,
                                             default=ALGORITHM)

            setup.cfp.get.return_value = self.algo
            self.assertEqual(get_algorithm(setup), self.algo)
            setup.cfp.get.assert_called_with(CFG_SECTION, CFG_ALGORITHM,
                                             default=ALGORITHM)

            # test that get_algorithm converts algorithms given in
            # OpenSSL style to M2Crypto style
            setup.cfp.get.return_value = "DES-EDE3-CFB8"
            self.assertEqual(get_algorithm(setup), "des_ede3_cfb8")
            setup.cfp.get.assert_called_with(CFG_SECTION, CFG_ALGORITHM,
                                             default=ALGORITHM)

        def test_get_passphrases(self):
            setup = Mock()
            setup.cfp.has_section.return_value = False
            self.assertEqual(get_passphrases(setup), dict())

            setup.cfp.has_section.return_value = True
            setup.cfp.options.return_value = ["foo", "bar", CFG_ALGORITHM]
            setup.cfp.get.return_value = "passphrase"
            self.assertItemsEqual(get_passphrases(setup),
                                  dict(foo="passphrase",
                                       bar="passphrase"))

        @patch("Bcfg2.Encryption.get_passphrases")
        def test_bruteforce_decrypt(self, mock_passphrases):
            passwd = "a simple passphrase"
            crypted = ssl_encrypt(self.plaintext, passwd)
            setup = Mock()

            # test with no passphrases given nor in config
            mock_passphrases.return_value = dict()
            self.assertRaises(EVPError,
                              bruteforce_decrypt,
                              crypted, setup=setup)
            mock_passphrases.assert_called_with(setup)

            # test with good passphrase given in function call
            mock_passphrases.reset_mock()
            self.assertEqual(self.plaintext,
                             bruteforce_decrypt(crypted,
                                                passphrases=["bogus pass",
                                                             passwd,
                                                             "also bogus"]))
            self.assertFalse(mock_passphrases.called)

            # test with no good passphrase given nor in config
            mock_passphrases.reset_mock()
            self.assertRaises(EVPError,
                              bruteforce_decrypt,
                              crypted, passphrases=["bogus", "also bogus"])
            self.assertFalse(mock_passphrases.called)

            # test with good passphrase in config file
            mock_passphrases.reset_mock()
            mock_passphrases.return_value = dict(bogus="bogus",
                                                 real=passwd,
                                                 bogus2="also bogus")
            self.assertEqual(self.plaintext,
                             bruteforce_decrypt(crypted, setup=setup))
            mock_passphrases.assert_called_with(setup)

            # test that passphrases given in function call take
            # precedence over config
            mock_passphrases.reset_mock()
            self.assertRaises(EVPError,
                              bruteforce_decrypt,
                              crypted, setup=setup,
                              passphrases=["bogus", "also bogus"])
            self.assertFalse(mock_passphrases.called)

            # test that different algorithms are used
            mock_passphrases.reset_mock()
            crypted = ssl_encrypt(self.plaintext, passwd, algorithm=self.algo)
            self.assertEqual(self.plaintext,
                             bruteforce_decrypt(crypted, setup=setup,
                                                algorithm=self.algo))

########NEW FILE########
__FILENAME__ = TestOptions
import os
import sys
from mock import Mock, MagicMock, patch
from Bcfg2.Options import *
from Bcfg2.Compat import ConfigParser

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != '/':
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *

class TestDefaultConfigParser(Bcfg2TestCase):
    @patch("%s.ConfigParser.get" % ConfigParser.__name__)
    def test_get(self, mock_get):
        dcp = DefaultConfigParser()
        mock_get.return_value = "foo"
        self.assertEqual(dcp.get("section", "option"), "foo")
        mock_get.assert_called_with(dcp, "section", "option")
        
        mock_get.reset_mock()
        self.assertEqual(dcp.get("section", "option",
                                 default="bar", other="test"), "foo")
        mock_get.assert_called_with(dcp, "section", "option", other="test")

        for etype, err in [(ConfigParser.NoOptionError,
                            ConfigParser.NoOptionError(None, None)),
                           (ConfigParser.NoSectionError,
                            ConfigParser.NoSectionError(None))]:
            mock_get.side_effect = err
            mock_get.reset_mock()
            self.assertEqual(dcp.get("section", "option", default="bar"), "bar")
            mock_get.assert_called_with(dcp, "section", "option")

            mock_get.reset_mock()
            self.assertRaises(etype, dcp.get, "section", "option")
            mock_get.assert_called_with(dcp, "section", "option")

    @patch("%s.ConfigParser.getboolean" % ConfigParser.__name__)
    def test_getboolean(self, mock_getboolean):
        dcp = DefaultConfigParser()
        mock_getboolean.return_value = True
        self.assertEqual(dcp.getboolean("section", "option"), True)
        mock_getboolean.assert_called_with(dcp, "section", "option")
        
        mock_getboolean.reset_mock()
        self.assertEqual(dcp.getboolean("section", "option",
                                 default=False, other="test"), True)
        mock_getboolean.assert_called_with(dcp, "section", "option",
                                           other="test")

        for etype, err in [(ConfigParser.NoOptionError,
                            ConfigParser.NoOptionError(None, None)),
                           (ConfigParser.NoSectionError,
                            ConfigParser.NoSectionError(None))]:
            mock_getboolean.side_effect = err
            mock_getboolean.reset_mock()
            self.assertEqual(dcp.getboolean("section", "option", default=False),
                             False)
            mock_getboolean.assert_called_with(dcp, "section", "option")

            mock_getboolean.reset_mock()
            self.assertRaises(etype, dcp.getboolean, "section", "option")
            mock_getboolean.assert_called_with(dcp, "section", "option")
            

class TestOption(Bcfg2TestCase):
    def test__init(self):
        self.assertRaises(OptionFailure,
                          Option,
                          'foo', False, cmd='f')
        self.assertRaises(OptionFailure,
                          Option,
                          'foo', False, cmd='--f')
        self.assertRaises(OptionFailure,
                          Option,
                          'foo', False, cmd='-foo')
        self.assertRaises(OptionFailure,
                          Option,
                          'foo', False, cmd='-foo', long_arg=True)
        opt = Option('foo', False)
        self.assertTrue(opt.boolean)
        opt = Option('foo', False, odesc='<val>')
        self.assertFalse(opt.boolean)
        opt = Option('foo', False, cook=get_bool)
        self.assertFalse(opt.boolean)
        opt = Option('foo', "foo")
        self.assertFalse(opt.boolean)
        
    def test_get_cooked_value(self):
        opt = Option('foo', False)
        opt.boolean = True
        self.assertTrue(opt.get_cooked_value("anything"))

        opt = Option('foo', 'foo')
        opt.boolean = False
        opt.cook = False
        self.assertEqual("foo", opt.get_cooked_value("foo"))
        
        opt = Option('foo', 'foo')
        opt.boolean = False
        opt.cook = Mock()
        self.assertEqual(opt.cook.return_value, opt.get_cooked_value("foo"))
        opt.cook.assert_called_with("foo")

    def test_buildHelpMessage(self):
        opt = Option('foo', False)
        self.assertEqual(opt.buildHelpMessage(), '')

        opt = Option('foo', False, '-f')
        self.assertEqual(opt.buildHelpMessage().split(),
                         ["-f", "foo"])

        opt = Option('foo', False, cmd="--foo", long_arg=True)
        self.assertEqual(opt.buildHelpMessage().split(),
                         ["--foo", "foo"])

        opt = Option('foo', False, cmd="-f", odesc='<val>')
        self.assertEqual(opt.buildHelpMessage().split(),
                         ["-f", "<val>", "foo"])

        opt = Option('foo', False, cmd="--foo", long_arg=True, odesc='<val>')
        self.assertEqual(opt.buildHelpMessage().split(),
                         ["--foo=<val>", "foo"])

    def test_buildGetopt(self):
        opt = Option('foo', False)
        self.assertEqual(opt.buildGetopt(), '')

        opt = Option('foo', False, '-f')
        self.assertEqual(opt.buildGetopt(), "f")

        opt = Option('foo', False, cmd="--foo", long_arg=True)
        self.assertEqual(opt.buildGetopt(), '')

        opt = Option('foo', False, cmd="-f", odesc='<val>')
        self.assertEqual(opt.buildGetopt(), 'f:')

        opt = Option('foo', False, cmd="--foo", long_arg=True, odesc='<val>')
        self.assertEqual(opt.buildGetopt(), '')

    def test_buildLongGetopt(self):
        opt = Option('foo', False, cmd="--foo", long_arg=True)
        self.assertEqual(opt.buildLongGetopt(), 'foo')

        opt = Option('foo', False, cmd="--foo", long_arg=True, odesc='<val>')
        self.assertEqual(opt.buildLongGetopt(), 'foo=')

    def test_parse(self):
        cf = ('communication', 'password')
        o = Option('foo', default='test4', cmd='-F', env='TEST2',
                                 odesc='bar', cf=cf)
        o.parse([], ['-F', 'test'])
        self.assertEqual(o.value, 'test')
        o.parse([('-F', 'test2')], [])
        self.assertEqual(o.value, 'test2')

        os.environ['TEST2'] = 'test3'
        o.parse([], [])
        self.assertEqual(o.value, 'test3')
        del os.environ['TEST2']

        cfp = DefaultConfigParser()
        cfp.get = Mock()
        cfp.get.return_value = 'test5'
        o.parse([], [], configparser=cfp)
        cfp.get.assert_any_call(*cf)
        self.assertEqual(o.value, 'test5')

        o.cf = False
        o.parse([], [])
        assert o.value == 'test4'


class TestOptionSet(Bcfg2TestCase):
    def test_buildGetopt(self):
        opts = [('foo', Option('foo', 'test1', cmd='-G')),
                ('bar', Option('foo', 'test2')),
                ('baz', Option('foo', 'test1', cmd='-H',
                                             odesc='1'))]
        oset = OptionSet(opts)
        res = oset.buildGetopt()
        self.assertIn('H:', res)
        self.assertIn('G', res)
        self.assertEqual(len(res), 3)

    def test_buildLongGetopt(self):
        opts = [('foo', Option('foo', 'test1', cmd='-G')),
                ('bar', Option('foo', 'test2')),
                ('baz', Option('foo', 'test1', cmd='--H',
                                             odesc='1', long_arg=True))]
        oset = OptionSet(opts)
        res = oset.buildLongGetopt()
        self.assertIn('H=', res)
        self.assertEqual(len(res), 1)

    def test_parse(self):
        opts = [('foo', Option('foo', 'test1', cmd='-G')),
                ('bar', Option('foo', 'test2')),
                ('baz', Option('foo', 'test1', cmd='-H',
                                             odesc='1'))]
        oset = OptionSet(opts)
        self.assertRaises(SystemExit,
                          oset.parse,
                          ['-G', '-H'])
        oset2 = OptionSet(opts)
        self.assertRaises(SystemExit,
                          oset2.parse,
                          ['-h'])
        oset3 = OptionSet(opts)
        oset3.parse(['-G'])
        self.assertTrue(oset3['foo'])


class TestOptionParser(Bcfg2TestCase):
    def test__init(self):
        opts = [('foo', Option('foo', 'test1', cmd='-h')),
                ('bar', Option('foo', 'test2')),
                ('baz', Option('foo', 'test1', cmd='-H',
                                             odesc='1'))]
        oset1 = OptionParser(opts)
        self.assertEqual(oset1.cfile,
                         DEFAULT_CONFIG_LOCATION)
        sys.argv = ['foo', '-C', '/usr/local/etc/bcfg2.conf']
        oset2 = OptionParser(opts)
        self.assertEqual(oset2.cfile,
                         '/usr/local/etc/bcfg2.conf')
        sys.argv = []
        oset3 = OptionParser(opts)
        self.assertEqual(oset3.cfile,
                         DEFAULT_CONFIG_LOCATION)

########NEW FILE########
__FILENAME__ = Testbase
import os
import sys
import logging
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugin.base import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != '/':
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *


class TestDebuggable(Bcfg2TestCase):
    test_obj = Debuggable

    def get_obj(self):
        return self.test_obj()

    def test__init(self):
        d = self.get_obj()
        self.assertIsInstance(d.logger, logging.Logger)
        self.assertFalse(d.debug_flag)

    def test_set_debug(self):
        d = self.get_obj()
        self.assertEqual(True, d.set_debug(True))
        self.assertEqual(d.debug_flag, True)

        self.assertEqual(False, d.set_debug(False))
        self.assertEqual(d.debug_flag, False)

    def test_toggle_debug(self):
        d = self.get_obj()
        d.set_debug = Mock()
        orig = d.debug_flag
        self.assertEqual(d.toggle_debug(),
                         d.set_debug.return_value)
        d.set_debug.assert_called_with(not orig)

    def test_debug_log(self):
        d = self.get_obj()
        d.logger = Mock()
        d.debug_flag = False
        d.debug_log("test")
        self.assertFalse(d.logger.error.called)

        d.logger.reset_mock()
        d.debug_log("test", flag=True)
        self.assertTrue(d.logger.error.called)

        d.logger.reset_mock()
        d.debug_flag = True
        d.debug_log("test")
        self.assertTrue(d.logger.error.called)


class TestPlugin(TestDebuggable):
    test_obj = Plugin

    def get_obj(self, core=None):
        if core is None:
            core = Mock()
            core.setup = MagicMock()
        @patchIf(not isinstance(os.makedirs, Mock), "os.makedirs", Mock())
        def inner():
            return self.test_obj(core, datastore)
        return inner()

    @patch("os.makedirs")
    @patch("os.path.exists")
    def test__init(self, mock_exists, mock_makedirs):
        if self.test_obj.create:
            core = Mock()
            core.setup = MagicMock()

            mock_exists.return_value = True
            p = self.get_obj(core=core)
            self.assertEqual(p.data, os.path.join(datastore, p.name))
            self.assertEqual(p.core, core)
            mock_exists.assert_any_call(p.data)
            self.assertFalse(mock_makedirs.called)

            mock_exists.reset_mock()
            mock_makedirs.reset_mock()
            mock_exists.return_value = False
            p = self.get_obj(core=core)
            self.assertEqual(p.data, os.path.join(datastore, p.name))
            self.assertEqual(p.core, core)
            mock_exists.assert_any_call(p.data)
            mock_makedirs.assert_any_call(p.data)

    @patch("os.makedirs")
    def test_init_repo(self, mock_makedirs):
        self.test_obj.init_repo(datastore)
        mock_makedirs.assert_called_with(os.path.join(datastore,
                                                      self.test_obj.name))

########NEW FILE########
__FILENAME__ = Testexceptions
import os
import sys
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugin.exceptions import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != '/':
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *


class TestPluginInitError(Bcfg2TestCase):
    """ placeholder for future tests """
    pass


class TestPluginExecutionError(Bcfg2TestCase):
    """ placeholder for future tests """
    pass


class TestMetadataConsistencyError(Bcfg2TestCase):
    """ placeholder for future tests """
    pass


class TestMetadataRuntimeError(Bcfg2TestCase):
    """ placeholder for future tests """
    pass


class TestValidationError(Bcfg2TestCase):
    """ placeholder for future tests """
    pass


class TestSpecificityError(Bcfg2TestCase):
    """ placeholder for future tests """
    pass


########NEW FILE########
__FILENAME__ = Testhelpers
import os
import sys
import copy
import lxml.etree
import Bcfg2.Server
from Bcfg2.Compat import reduce
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugin.helpers import *
from Bcfg2.Server.Plugin.exceptions import PluginInitError

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != '/':
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestServer.TestPlugin.Testbase import TestPlugin, TestDebuggable
from TestServer.TestPlugin.Testinterfaces import TestGenerator


def tostring(el):
    return lxml.etree.tostring(el, xml_declaration=False).decode('UTF-8')


class FakeElementTree(lxml.etree._ElementTree):
    xinclude = Mock()
    parse = Mock


class TestFunctions(Bcfg2TestCase):
    def test_bind_info(self):
        entry = lxml.etree.Element("Path", name="/test")
        metadata = Mock()
        default = dict(test1="test1", test2="test2")
        # test without infoxml
        bind_info(entry, metadata, default=default)
        self.assertItemsEqual(entry.attrib,
                              dict(test1="test1",
                                   test2="test2",
                                   name="/test"))

        # test with bogus infoxml
        entry = lxml.etree.Element("Path", name="/test")
        infoxml = Mock()
        self.assertRaises(PluginExecutionError,
                          bind_info,
                          entry, metadata, infoxml=infoxml)
        infoxml.pnode.Match.assert_called_with(metadata, dict(), entry=entry)

        # test with valid infoxml
        entry = lxml.etree.Element("Path", name="/test")
        infoxml.reset_mock()
        infodata = {None: {"test3": "test3", "test4": "test4"}}
        def infoxml_rv(metadata, rv, entry=None):
            rv['Info'] = infodata
        infoxml.pnode.Match.side_effect = infoxml_rv
        bind_info(entry, metadata, infoxml=infoxml, default=default)
        # mock objects don't properly track the called-with value of
        # arguments whose value is changed by the function, so it
        # thinks Match() was called with the final value of the mdata
        # arg, not the initial value.  makes this test a little less
        # worthwhile, TBH.
        infoxml.pnode.Match.assert_called_with(metadata, dict(Info=infodata),
                                               entry=entry)
        self.assertItemsEqual(entry.attrib,
                              dict(test1="test1",
                                   test2="test2",
                                   test3="test3",
                                   test4="test4",
                                   name="/test"))


class TestDatabaseBacked(TestPlugin):
    test_obj = DatabaseBacked

    def get_obj(self, core=None):
        if not HAS_DJANGO:
            if core is None:
                core = MagicMock()
            # disable the database
            core.setup.cfp.getboolean.return_value = False
        return TestPlugin.get_obj(self, core=core)

    @skipUnless(HAS_DJANGO, "Django not found")
    def test__use_db(self):
        core = Mock()
        core.setup.cfp.getboolean.return_value = True
        db = self.get_obj(core)
        self.assertTrue(db._use_db)

        core = Mock()
        core.setup.cfp.getboolean.return_value = False
        db = self.get_obj(core)
        self.assertFalse(db._use_db)

        Bcfg2.Server.Plugin.helpers.HAS_DJANGO = False
        core = Mock()
        core.setup.cfp.getboolean.return_value = False
        db = self.get_obj(core)
        self.assertFalse(db._use_db)

        core = Mock()
        core.setup.cfp.getboolean.return_value = True
        self.assertRaises(PluginInitError, self.get_obj, core)
        Bcfg2.Server.Plugin.helpers.HAS_DJANGO = True


class TestPluginDatabaseModel(Bcfg2TestCase):
    """ placeholder for future tests """
    pass


class TestFileBacked(Bcfg2TestCase):
    test_obj = FileBacked
    path = os.path.join(datastore, "test")

    def get_obj(self, path=None, fam=None):
        if path is None:
            path = self.path
        return self.test_obj(path, fam=fam)

    @patch("%s.open" % builtins)
    def test_HandleEvent(self, mock_open):
        fb = self.get_obj()
        fb.Index = Mock()

        def reset():
            fb.Index.reset_mock()
            mock_open.reset_mock()

        for evt in ["exists", "changed", "created"]:
            reset()
            event = Mock()
            event.code2str.return_value = evt
            fb.HandleEvent(event)
            mock_open.assert_called_with(self.path)
            mock_open.return_value.read.assert_any_call()
            fb.Index.assert_any_call()

        reset()
        event = Mock()
        event.code2str.return_value = "endExist"
        fb.HandleEvent(event)
        self.assertFalse(mock_open.called)
        self.assertFalse(fb.Index.called)


class TestDirectoryBacked(Bcfg2TestCase):
    test_obj = DirectoryBacked
    testpaths = {1: '',
                 2: '/foo',
                 3: '/foo/bar',
                 4: '/foo/bar/baz',
                 5: 'quux',
                 6: 'xyzzy/',
                 7: 'xyzzy/plugh/'}
    testfiles = ['foo', 'bar/baz.txt', 'plugh.py']
    ignore = []  # ignore no events
    badevents = []  # DirectoryBacked handles all files, so there's no
                    # such thing as a bad event

    def test_child_interface(self):
        """ ensure that the child object has the correct interface """
        self.assertTrue(hasattr(self.test_obj.__child__, "HandleEvent"))

    @patch("os.makedirs", Mock())
    def get_obj(self, fam=None):
        if fam is None:
            fam = Mock()

        @patch("%s.%s.add_directory_monitor" % (self.test_obj.__module__,
                                                self.test_obj.__name__),
               Mock())
        def inner():
            return self.test_obj(os.path.join(datastore,
                                              self.test_obj.__name__),
                                 fam)
        return inner()

    @patch("os.makedirs")
    @patch("os.path.exists")
    def test__init(self, mock_exists, mock_makedirs):
        @patch("%s.%s.add_directory_monitor" % (self.test_obj.__module__,
                                                self.test_obj.__name__))
        def inner(mock_add_monitor):
            mock_exists.return_value = True
            db = self.test_obj(datastore, Mock())
            mock_add_monitor.assert_called_with('')
            mock_exists.assert_called_with(db.data)
            self.assertFalse(mock_makedirs.called)

            mock_add_monitor.reset_mock()
            mock_exists.reset_mock()
            mock_makedirs.reset_mock()
            mock_exists.return_value = False
            db = self.test_obj(datastore, Mock())
            mock_add_monitor.assert_called_with('')
            mock_exists.assert_called_with(db.data)
            mock_makedirs.assert_called_with(db.data)

        inner()

    def test__getitem(self):
        db = self.get_obj()
        db.entries.update(dict(a=1, b=2, c=3))
        self.assertEqual(db['a'], 1)
        self.assertEqual(db['b'], 2)
        expected = KeyError
        try:
            db['d']
        except expected:
            pass
        except:
            err = sys.exc_info()[1]
            self.assertFalse(True, "%s raised instead of %s" %
                             (err.__class__.__name__,
                              expected.__class__.__name__))
        else:
            self.assertFalse(True,
                             "%s not raised" % expected.__class__.__name__)

    def test__iter(self):
        db = self.get_obj()
        db.entries.update(dict(a=1, b=2, c=3))
        self.assertEqual([i for i in db],
                         [i for i in db.entries.items()])

    @patch("os.path.isdir")
    def test_add_directory_monitor(self, mock_isdir):
        db = self.get_obj()
        db.fam = Mock()
        db.fam.rv = 0

        def reset():
            db.fam.rv += 1
            db.fam.AddMonitor.return_value = db.fam.rv
            db.fam.reset_mock()
            mock_isdir.reset_mock()

        mock_isdir.return_value = True
        for path in self.testpaths.values():
            reset()
            db.add_directory_monitor(path)
            db.fam.AddMonitor.assert_called_with(os.path.join(db.data, path),
                                                 db)
            self.assertIn(db.fam.rv, db.handles)
            self.assertEqual(db.handles[db.fam.rv], path)

        reset()
        # test duplicate adds
        for path in self.testpaths.values():
            reset()
            db.add_directory_monitor(path)
            self.assertFalse(db.fam.AddMonitor.called)

        reset()
        mock_isdir.return_value = False
        db.add_directory_monitor('bogus')
        self.assertFalse(db.fam.AddMonitor.called)
        self.assertNotIn(db.fam.rv, db.handles)

    def test_add_entry(self):
        db = self.get_obj()
        db.fam = Mock()

        class MockChild(Mock):
            def __init__(self, path, fam, **kwargs):
                Mock.__init__(self, **kwargs)
                self.path = path
                self.fam = fam
                self.HandleEvent = Mock()
        db.__child__ = MockChild

        for path in self.testpaths.values():
            event = Mock()
            db.add_entry(path, event)
            self.assertIn(path, db.entries)
            self.assertEqual(db.entries[path].path,
                             os.path.join(db.data, path))
            self.assertEqual(db.entries[path].fam, db.fam)
            db.entries[path].HandleEvent.assert_called_with(event)

    @patch("os.path.isdir")
    def test_HandleEvent(self, mock_isdir):
        db = self.get_obj()
        db.add_entry = Mock()
        db.add_directory_monitor = Mock()
        # a path with a leading / should never get into
        # DirectoryBacked.handles, so strip that test case
        for rid, path in self.testpaths.items():
            path = path.lstrip('/')
            db.handles[rid] = path

        def reset():
            mock_isdir.reset_mock()
            db.add_entry.reset_mock()
            db.add_directory_monitor.reset_mock()

        def get_event(filename, action, requestID):
            event = Mock()
            event.code2str.return_value = action
            event.filename = filename
            event.requestID = requestID
            return event

        # test events on the data directory itself
        reset()
        mock_isdir.return_value = True
        event = get_event(db.data, "exists", 1)
        db.HandleEvent(event)
        db.add_directory_monitor.assert_called_with("")

        # test events on paths that aren't handled
        reset()
        mock_isdir.return_value = False
        event = get_event('/' + self.testfiles[0], 'created',
                          max(self.testpaths.keys()) + 1)
        db.HandleEvent(event)
        self.assertFalse(db.add_directory_monitor.called)
        self.assertFalse(db.add_entry.called)

        for req_id, path in self.testpaths.items():
            # a path with a leading / should never get into
            # DirectoryBacked.handles, so strip that test case
            path = path.lstrip('/')
            basepath = os.path.join(datastore, path)
            for fname in self.testfiles:
                relpath = os.path.join(path, fname)
                abspath = os.path.join(basepath, fname)

                # test endExist does nothing
                reset()
                event = get_event(fname, 'endExist', req_id)
                db.HandleEvent(event)
                self.assertFalse(db.add_directory_monitor.called)
                self.assertFalse(db.add_entry.called)

                mock_isdir.return_value = True
                for evt in ["created", "exists", "changed"]:
                    # test that creating or changing a directory works
                    reset()
                    event = get_event(fname, evt, req_id)
                    db.HandleEvent(event)
                    db.add_directory_monitor.assert_called_with(relpath)
                    self.assertFalse(db.add_entry.called)

                mock_isdir.return_value = False
                for evt in ["created", "exists"]:
                    # test that creating a file works
                    reset()
                    event = get_event(fname, evt, req_id)
                    db.HandleEvent(event)
                    db.add_entry.assert_called_with(relpath, event)
                    self.assertFalse(db.add_directory_monitor.called)
                    db.entries[relpath] = MagicMock()

                # test that changing a file that already exists works
                reset()
                event = get_event(fname, "changed", req_id)
                db.HandleEvent(event)
                db.entries[relpath].HandleEvent.assert_called_with(event)
                self.assertFalse(db.add_directory_monitor.called)
                self.assertFalse(db.add_entry.called)

                # test that deleting an entry works
                reset()
                event = get_event(fname, "deleted", req_id)
                db.HandleEvent(event)
                self.assertNotIn(relpath, db.entries)

                # test that changing a file that doesn't exist works
                reset()
                event = get_event(fname, "changed", req_id)
                db.HandleEvent(event)
                db.add_entry.assert_called_with(relpath, event)
                self.assertFalse(db.add_directory_monitor.called)
                db.entries[relpath] = MagicMock()

        # test that deleting a directory works. this is a little
        # strange because the _parent_ directory has to handle the
        # deletion
        reset()
        event = get_event('quux', "deleted", 1)
        db.HandleEvent(event)
        for key in db.entries.keys():
            self.assertFalse(key.startswith('quux'))

        # test bad events
        for fname in self.badevents:
            reset()
            event = get_event(fname, "created", 1)
            db.HandleEvent(event)
            self.assertFalse(db.add_entry.called)
            self.assertFalse(db.add_directory_monitor.called)

        # test ignored events
        for fname in self.ignore:
            reset()
            event = get_event(fname, "created", 1)
            db.HandleEvent(event)
            self.assertFalse(mock_isdir.called,
                             msg="Failed to ignore %s" % fname)
            self.assertFalse(db.add_entry.called,
                             msg="Failed to ignore %s" % fname)
            self.assertFalse(db.add_directory_monitor.called,
                             msg="Failed to ignore %s" % fname)


class TestXMLFileBacked(TestFileBacked):
    test_obj = XMLFileBacked

    # can be set to True (on child test cases where should_monitor is
    # always True) or False (on child test cases where should_monitor
    # is always False)
    should_monitor = None
    path = os.path.join(datastore, "test", "test1.xml")

    def get_obj(self, path=None, fam=None, should_monitor=False):
        if path is None:
            path = self.path

        @patchIf(not isinstance(os.path.exists, Mock),
                 "os.path.exists", Mock())
        def inner():
            return self.test_obj(path, fam=fam, should_monitor=should_monitor)
        return inner()

    def test__init(self):
        fam = Mock()
        xfb = self.get_obj()
        if self.should_monitor:
            self.assertIsNotNone(xfb.fam)
            fam.reset_mock()
            xfb = self.get_obj(fam=fam, should_monitor=True)
            fam.AddMonitor.assert_called_with(self.path, xfb)
        else:
            self.assertIsNone(xfb.fam)
            xfb = self.get_obj(fam=fam)
            self.assertFalse(fam.AddMonitor.called)

    @patch("glob.glob")
    @patch("lxml.etree.parse")
    def test_follow_xincludes(self, mock_parse, mock_glob):
        xfb = self.get_obj()
        xfb.add_monitor = Mock()
        xfb.add_monitor.side_effect = lambda p: xfb.extras.append(p)

        def reset():
            xfb.add_monitor.reset_mock()
            mock_glob.reset_mock()
            mock_parse.reset_mock()
            xfb.extras = []

        xdata = dict()
        mock_parse.side_effect = lambda p: xdata[p]
        mock_glob.side_effect = lambda g: [g]

        base = os.path.dirname(self.path)

        # basic functionality
        test2 = os.path.join(base, 'test2.xml')
        xdata[test2] = lxml.etree.Element("Test").getroottree()
        xfb._follow_xincludes(xdata=xdata[test2])
        self.assertFalse(xfb.add_monitor.called)

        if (not hasattr(self.test_obj, "xdata") or
            not isinstance(self.test_obj.xdata, property)):
            # if xdata is settable, test that method of getting data
            # to _follow_xincludes
            reset()
            xfb.xdata = xdata[test2].getroot()
            xfb._follow_xincludes()
            self.assertFalse(xfb.add_monitor.called)
            xfb.xdata = None

        reset()
        xfb._follow_xincludes(fname=test2)
        self.assertFalse(xfb.add_monitor.called)

        # test one level of xinclude
        xdata[self.path] = lxml.etree.Element("Test").getroottree()
        lxml.etree.SubElement(xdata[self.path].getroot(),
                              Bcfg2.Server.XI_NAMESPACE + "include",
                              href=test2)
        reset()
        xfb._follow_xincludes(fname=self.path)
        xfb.add_monitor.assert_called_with(test2)
        self.assertItemsEqual(mock_parse.call_args_list,
                              [call(f) for f in xdata.keys()])
        mock_glob.assert_called_with(test2)

        reset()
        xfb._follow_xincludes(fname=self.path, xdata=xdata[self.path])
        xfb.add_monitor.assert_called_with(test2)
        self.assertItemsEqual(mock_parse.call_args_list,
                              [call(f) for f in xdata.keys()
                               if f != self.path])
        mock_glob.assert_called_with(test2)

        # test two-deep level of xinclude, with some files in another
        # directory
        test3 = os.path.join(base, "test3.xml")
        test4 = os.path.join(base, "test_dir", "test4.xml")
        test5 = os.path.join(base, "test_dir", "test5.xml")
        test6 = os.path.join(base, "test_dir", "test6.xml")
        xdata[test3] = lxml.etree.Element("Test").getroottree()
        lxml.etree.SubElement(xdata[test3].getroot(),
                              Bcfg2.Server.XI_NAMESPACE + "include",
                              href=test4)
        xdata[test4] = lxml.etree.Element("Test").getroottree()
        lxml.etree.SubElement(xdata[test4].getroot(),
                              Bcfg2.Server.XI_NAMESPACE + "include",
                              href=test5)
        xdata[test5] = lxml.etree.Element("Test").getroottree()
        xdata[test6] = lxml.etree.Element("Test").getroottree()
        # relative includes
        lxml.etree.SubElement(xdata[self.path].getroot(),
                              Bcfg2.Server.XI_NAMESPACE + "include",
                              href="test3.xml")
        lxml.etree.SubElement(xdata[test3].getroot(),
                              Bcfg2.Server.XI_NAMESPACE + "include",
                              href="test_dir/test6.xml")

        reset()
        xfb._follow_xincludes(fname=self.path)
        expected = [call(f) for f in xdata.keys() if f != self.path]
        self.assertItemsEqual(xfb.add_monitor.call_args_list, expected)
        self.assertItemsEqual(mock_parse.call_args_list,
                              [call(f) for f in xdata.keys()])
        self.assertItemsEqual(mock_glob.call_args_list, expected)

        reset()
        xfb._follow_xincludes(fname=self.path, xdata=xdata[self.path])
        expected = [call(f) for f in xdata.keys() if f != self.path]
        self.assertItemsEqual(xfb.add_monitor.call_args_list, expected)
        self.assertItemsEqual(mock_parse.call_args_list, expected)
        self.assertItemsEqual(mock_glob.call_args_list, expected)

        # test wildcard xinclude
        reset()
        xdata[self.path] = lxml.etree.Element("Test").getroottree()
        lxml.etree.SubElement(xdata[self.path].getroot(),
                              Bcfg2.Server.XI_NAMESPACE + "include",
                              href="*.xml")

        def glob_rv(path):
            if path == os.path.join(base, '*.xml'):
                return [self.path, test2, test3]
            else:
                return [path]
        mock_glob.side_effect = glob_rv

        xfb._follow_xincludes(xdata=xdata[self.path])
        expected = [call(f) for f in xdata.keys() if f != self.path]
        self.assertItemsEqual(xfb.add_monitor.call_args_list, expected)
        self.assertItemsEqual(mock_parse.call_args_list, expected)
        self.assertItemsEqual(mock_glob.call_args_list,
                              [call(os.path.join(base, '*.xml')), call(test4),
                               call(test5), call(test6)])


    @patch("lxml.etree._ElementTree", FakeElementTree)
    @patch("Bcfg2.Server.Plugin.helpers.%s._follow_xincludes" %
           test_obj.__name__)
    def test_Index(self, mock_follow):
        xfb = self.get_obj()

        def reset():
            mock_follow.reset_mock()
            FakeElementTree.xinclude.reset_mock()
            xfb.extras = []
            xfb.xdata = None

        # no xinclude
        reset()
        xdata = lxml.etree.Element("Test", name="test")
        children = [lxml.etree.SubElement(xdata, "Foo"),
                    lxml.etree.SubElement(xdata, "Bar", name="bar")]
        xfb.data = tostring(xdata)
        xfb.Index()
        mock_follow.assert_any_call()
        try:
            self.assertEqual(xfb.xdata.base, self.path)
        except AttributeError:
            # python 2.4 and/or lxml 2.0 don't store the base_url in
            # .base -- no idea where it's stored.
            pass
        self.assertItemsEqual([tostring(e) for e in xfb.entries],
                              [tostring(e) for e in children])

        # with xincludes
        reset()
        mock_follow.side_effect = \
            lambda: xfb.extras.extend(["/test/test2.xml",
                                       "/test/test_dir/test3.xml"])
        children.extend([
                lxml.etree.SubElement(xdata,
                                      Bcfg2.Server.XI_NAMESPACE + "include",
                                      href="/test/test2.xml"),
                lxml.etree.SubElement(xdata,
                                      Bcfg2.Server.XI_NAMESPACE + "include",
                                      href="/test/test_dir/test3.xml")])
        test2 = lxml.etree.Element("Test", name="test2")
        lxml.etree.SubElement(test2, "Baz")
        test3 = lxml.etree.Element("Test", name="test3")
        replacements = {"/test/test2.xml": test2,
                        "/test/test_dir/test3.xml": test3}
        def xinclude():
            for el in xfb.xdata.findall('//%sinclude' %
                                        Bcfg2.Server.XI_NAMESPACE):
                xfb.xdata.replace(el, replacements[el.get("href")])
        FakeElementTree.xinclude.side_effect = xinclude

        xfb.data = tostring(xdata)
        xfb.Index()
        mock_follow.assert_any_call()
        FakeElementTree.xinclude.assert_any_call
        try:
            self.assertEqual(xfb.xdata.base, self.path)
        except AttributeError:
            pass
        self.assertItemsEqual([tostring(e) for e in xfb.entries],
                              [tostring(e) for e in children])

    def test_add_monitor(self):
        xfb = self.get_obj()
        xfb.add_monitor("/test/test2.xml")
        self.assertIn("/test/test2.xml", xfb.extra_monitors)

        fam = Mock()
        fam.reset_mock()
        xfb = self.get_obj(fam=fam)
        if xfb.fam:
            xfb.add_monitor("/test/test4.xml")
            fam.AddMonitor.assert_called_with("/test/test4.xml", xfb)
            self.assertIn("/test/test4.xml", xfb.extra_monitors)


class TestStructFile(TestXMLFileBacked):
    test_obj = StructFile

    def _get_test_data(self):
        """ build a very complex set of test data """
        # top-level group and client elements
        groups = dict()
        # group and client elements that are descendents of other group or
        # client elements
        subgroups = dict()
        # children of elements in `groups' that should be included in
        # match results
        children = dict()
        # children of elements in `subgroups' that should be included in
        # match results
        subchildren = dict()
        # top-level tags that are not group elements
        standalone = []
        xdata = lxml.etree.Element("Test", name="test")
        groups[0] = lxml.etree.SubElement(xdata, "Group", name="group1",
                                          include="true")
        children[0] = [lxml.etree.SubElement(groups[0], "Child", name="c1"),
                       lxml.etree.SubElement(groups[0], "Child", name="c2")]
        subgroups[0] = [lxml.etree.SubElement(groups[0], "Group",
                                              name="subgroup1", include="true"),
                        lxml.etree.SubElement(groups[0],
                                              "Client", name="client1",
                                              include="false")]
        subchildren[0] = \
            [lxml.etree.SubElement(subgroups[0][0], "Child", name="sc1"),
             lxml.etree.SubElement(subgroups[0][0], "Child", name="sc2",
                                   attr="some attr"),
             lxml.etree.SubElement(subgroups[0][0], "Child", name="sc3")]
        lxml.etree.SubElement(subchildren[0][-1], "SubChild", name="subchild")
        lxml.etree.SubElement(subgroups[0][1], "Child", name="sc4")

        groups[1] = lxml.etree.SubElement(xdata, "Group", name="group2",
                                          include="false")
        children[1] = []
        subgroups[1] = []
        subchildren[1] = []
        lxml.etree.SubElement(groups[1], "Child", name="c3")
        lxml.etree.SubElement(groups[1], "Child", name="c4")

        standalone.append(lxml.etree.SubElement(xdata, "Standalone", name="s1"))

        groups[2] = lxml.etree.SubElement(xdata, "Client", name="client2",
                                          include="false")
        children[2] = []
        subgroups[2] = []
        subchildren[2] = []
        lxml.etree.SubElement(groups[2], "Child", name="c5")
        lxml.etree.SubElement(groups[2], "Child", name="c6")

        standalone.append(lxml.etree.SubElement(xdata, "Standalone", name="s2",
                                                attr="some attr"))

        groups[3] = lxml.etree.SubElement(xdata, "Client", name="client3",
                                          include="true")
        children[3] = [lxml.etree.SubElement(groups[3], "Child", name="c7",
                                             attr="some_attr"),
                       lxml.etree.SubElement(groups[3], "Child", name="c8")]
        subgroups[3] = []
        subchildren[3] = []
        lxml.etree.SubElement(children[3][-1], "SubChild", name="subchild")

        standalone.append(lxml.etree.SubElement(xdata, "Standalone", name="s3"))
        lxml.etree.SubElement(standalone[-1], "SubStandalone", name="sub1")

        children[4] = standalone
        return (xdata, groups, subgroups, children, subchildren, standalone)

    def test_include_element(self):
        sf = self.get_obj()
        metadata = Mock()
        metadata.groups = ["group1", "group2"]
        metadata.hostname = "foo.example.com"

        inc = lambda tag, **attrs: \
            sf._include_element(lxml.etree.Element(tag, **attrs), metadata)

        self.assertFalse(sf._include_element(lxml.etree.Comment("test"),
                                             metadata))

        self.assertFalse(inc("Group", name="group3"))
        self.assertFalse(inc("Group", name="group2", negate="true"))
        self.assertFalse(inc("Group", name="group2", negate="tRuE"))
        self.assertTrue(inc("Group", name="group2"))
        self.assertTrue(inc("Group", name="group2", negate="false"))
        self.assertTrue(inc("Group", name="group2", negate="faLSe"))
        self.assertTrue(inc("Group", name="group3", negate="true"))
        self.assertTrue(inc("Group", name="group3", negate="tRUe"))

        self.assertFalse(inc("Client", name="bogus.example.com"))
        self.assertFalse(inc("Client", name="foo.example.com", negate="true"))
        self.assertFalse(inc("Client", name="foo.example.com", negate="tRuE"))
        self.assertTrue(inc("Client", name="foo.example.com"))
        self.assertTrue(inc("Client", name="foo.example.com", negate="false"))
        self.assertTrue(inc("Client", name="foo.example.com", negate="faLSe"))
        self.assertTrue(inc("Client", name="bogus.example.com", negate="true"))
        self.assertTrue(inc("Client", name="bogus.example.com", negate="tRUe"))

        self.assertTrue(inc("Other"))

    @patch("Bcfg2.Server.Plugin.helpers.%s._include_element" %
           test_obj.__name__)
    def test__match(self, mock_include):
        sf = self.get_obj()
        metadata = Mock()

        (xdata, groups, subgroups, children, subchildren, standalone) = \
            self._get_test_data()

        mock_include.side_effect = \
            lambda x, _: (x.tag not in ['Client', 'Group'] or
                          x.get("include") == "true")

        for i, group in groups.items():
            actual = sf._match(group, metadata)
            expected = children[i] + subchildren[i]
            self.assertEqual(len(actual), len(expected))
            # easiest way to compare the values is actually to make
            # them into an XML document and let assertXMLEqual compare
            # them
            xactual = lxml.etree.Element("Container")
            xactual.extend(actual)
            xexpected = lxml.etree.Element("Container")
            xexpected.extend(expected)
            self.assertXMLEqual(xactual, xexpected)

        for el in standalone:
            self.assertXMLEqual(el, sf._match(el, metadata)[0])

    @patch("Bcfg2.Server.Plugin.helpers.%s._match" % test_obj.__name__)
    def test_Match(self, mock_match):
        sf = self.get_obj()
        metadata = Mock()

        (xdata, groups, subgroups, children, subchildren, standalone) = \
            self._get_test_data()
        sf.entries.extend(copy.deepcopy(xdata).getchildren())

        def match_rv(el, _):
            if el.tag not in ['Client', 'Group']:
                return [el]
            elif el.get("include") == "true":
                return el.getchildren()
            else:
                return []
        mock_match.side_effect = match_rv
        actual = sf.Match(metadata)
        expected = reduce(lambda x, y: x + y,
                          list(children.values()) + list(subgroups.values()))
        self.assertEqual(len(actual), len(expected))
        # easiest way to compare the values is actually to make
        # them into an XML document and let assertXMLEqual compare
        # them
        xactual = lxml.etree.Element("Container")
        xactual.extend(actual)
        xexpected = lxml.etree.Element("Container")
        xexpected.extend(expected)
        self.assertXMLEqual(xactual, xexpected)

    @patch("Bcfg2.Server.Plugin.helpers.%s._include_element" %
           test_obj.__name__)
    def test__xml_match(self, mock_include):
        sf = self.get_obj()
        metadata = Mock()

        (xdata, groups, subgroups, children, subchildren, standalone) = \
            self._get_test_data()

        mock_include.side_effect = \
            lambda x, _: (x.tag not in ['Client', 'Group'] or
                          x.get("include") == "true")

        actual = copy.deepcopy(xdata)
        for el in actual.getchildren():
            sf._xml_match(el, metadata)
        expected = lxml.etree.Element(xdata.tag, **dict(xdata.attrib))
        expected.text = xdata.text
        expected.extend(reduce(lambda x, y: x + y,
                               list(children.values()) + list(subchildren.values())))
        expected.extend(standalone)
        self.assertXMLEqual(actual, expected)

    @patch("Bcfg2.Server.Plugin.helpers.%s._xml_match" % test_obj.__name__)
    def test_XMLMatch(self, mock_xml_match):
        sf = self.get_obj()
        metadata = Mock()

        (sf.xdata, groups, subgroups, children, subchildren, standalone) = \
            self._get_test_data()

        sf.XMLMatch(metadata)
        actual = []
        for call in mock_xml_match.call_args_list:
            actual.append(call[0][0])
            self.assertEqual(call[0][1], metadata)
        expected = list(groups.values()) + standalone
        # easiest way to compare the values is actually to make
        # them into an XML document and let assertXMLEqual compare
        # them
        xactual = lxml.etree.Element("Container")
        xactual.extend(actual)
        xexpected = lxml.etree.Element("Container")
        xexpected.extend(expected)
        self.assertXMLEqual(xactual, xexpected)


class TestINode(Bcfg2TestCase):
    test_obj = INode

    # INode.__init__ and INode._load_children() call each other
    # recursively, which makes this class kind of a nightmare to test.
    # we have to first patch INode._load_children so that we can
    # create an INode object with no children loaded, then we unpatch
    # INode._load_children and patch INode.__init__ so that child
    # objects aren't actually created.  but in order to test things
    # atomically, we do this umpteen times in order to test with
    # different data.  this convenience method makes this a little
    # easier.  fun fun fun.
    @patch("Bcfg2.Server.Plugin.helpers.%s._load_children" %
           test_obj.__name__, Mock())
    def _get_inode(self, data, idict):
        return self.test_obj(data, idict)

    def test_raw_predicates(self):
        metadata = Mock()
        metadata.groups = ["group1", "group2"]
        metadata.hostname = "foo.example.com"
        entry = None

        parent_predicate = lambda m, e: True
        pred = eval(self.test_obj.raw['Client'] % dict(name="foo.example.com"),
                    dict(predicate=parent_predicate))
        self.assertTrue(pred(metadata, entry))
        pred = eval(self.test_obj.raw['Client'] % dict(name="bar.example.com"),
                    dict(predicate=parent_predicate))
        self.assertFalse(pred(metadata, entry))

        pred = eval(self.test_obj.raw['Group'] % dict(name="group1"),
                    dict(predicate=parent_predicate))
        self.assertTrue(pred(metadata, entry))
        pred = eval(self.test_obj.raw['Group'] % dict(name="group3"),
                    dict(predicate=parent_predicate))
        self.assertFalse(pred(metadata, entry))

        pred = eval(self.test_obj.nraw['Client'] % dict(name="foo.example.com"),
                    dict(predicate=parent_predicate))
        self.assertFalse(pred(metadata, entry))
        pred = eval(self.test_obj.nraw['Client'] % dict(name="bar.example.com"),
                    dict(predicate=parent_predicate))
        self.assertTrue(pred(metadata, entry))

        pred = eval(self.test_obj.nraw['Group'] % dict(name="group1"),
                    dict(predicate=parent_predicate))
        self.assertFalse(pred(metadata, entry))
        pred = eval(self.test_obj.nraw['Group'] % dict(name="group3"),
                    dict(predicate=parent_predicate))
        self.assertTrue(pred(metadata, entry))

        parent_predicate = lambda m, e: False
        pred = eval(self.test_obj.raw['Client'] % dict(name="foo.example.com"),
                    dict(predicate=parent_predicate))
        self.assertFalse(pred(metadata, entry))
        pred = eval(self.test_obj.raw['Group'] % dict(name="group1"),
                    dict(predicate=parent_predicate))
        self.assertFalse(pred(metadata, entry))
        pred = eval(self.test_obj.nraw['Client'] % dict(name="bar.example.com"),
                    dict(predicate=parent_predicate))
        self.assertFalse(pred(metadata, entry))
        pred = eval(self.test_obj.nraw['Group'] % dict(name="group3"),
                    dict(predicate=parent_predicate))
        self.assertFalse(pred(metadata, entry))

        self.assertItemsEqual(self.test_obj.containers,
                              self.test_obj.raw.keys())
        self.assertItemsEqual(self.test_obj.containers,
                              self.test_obj.nraw.keys())

    @patch("Bcfg2.Server.Plugin.helpers.INode._load_children")
    def test__init(self, mock_load_children):
        data = lxml.etree.Element("Bogus")
        # called with no parent, should not raise an exception; it's a
        # top-level tag in an XML file and so is not expected to be a
        # proper predicate
        INode(data, dict())
        self.assertRaises(PluginExecutionError,
                          INode, data, dict(), Mock())

        data = lxml.etree.Element("Client", name="foo.example.com")
        idict = dict()
        inode = INode(data, idict)
        mock_load_children.assert_called_with(data, idict)
        self.assertTrue(inode.predicate(Mock(), Mock()))

        parent = Mock()
        parent.predicate = lambda m, e: True
        metadata = Mock()
        metadata.groups = ["group1", "group2"]
        metadata.hostname = "foo.example.com"
        entry = None

        # test setting predicate with parent object
        mock_load_children.reset_mock()
        inode = INode(data, idict, parent=parent)
        mock_load_children.assert_called_with(data, idict)
        self.assertTrue(inode.predicate(metadata, entry))

        # test negation
        data = lxml.etree.Element("Client", name="foo.example.com",
                                  negate="true")
        mock_load_children.reset_mock()
        inode = INode(data, idict, parent=parent)
        mock_load_children.assert_called_with(data, idict)
        self.assertFalse(inode.predicate(metadata, entry))

        # test failure of a matching predicate (client names do not match)
        data = lxml.etree.Element("Client", name="foo.example.com")
        metadata.hostname = "bar.example.com"
        mock_load_children.reset_mock()
        inode = INode(data, idict, parent=parent)
        mock_load_children.assert_called_with(data, idict)
        self.assertFalse(inode.predicate(metadata, entry))

        # test that parent predicate is AND'ed in correctly
        parent.predicate = lambda m, e: False
        metadata.hostname = "foo.example.com"
        mock_load_children.reset_mock()
        inode = INode(data, idict, parent=parent)
        mock_load_children.assert_called_with(data, idict)
        self.assertFalse(inode.predicate(metadata, entry))

    def test_load_children(self):
        data = lxml.etree.Element("Parent")
        child1 = lxml.etree.SubElement(data, "Client", name="foo.example.com")
        child2 = lxml.etree.SubElement(data, "Group", name="bar", negate="true")
        idict = dict()

        inode = self._get_inode(data, idict)

        @patch("Bcfg2.Server.Plugin.helpers.%s.__init__" %
               inode.__class__.__name__)
        def inner(mock_init):
            mock_init.return_value = None
            inode._load_children(data, idict)
            self.assertItemsEqual(mock_init.call_args_list,
                                  [call(child1, idict, inode),
                                   call(child2, idict, inode)])
            self.assertEqual(idict, dict())
            self.assertItemsEqual(inode.contents, dict())

        inner()

        data = lxml.etree.Element("Parent")
        child1 = lxml.etree.SubElement(data, "Data", name="child1",
                                       attr="some attr")
        child1.text = "text"
        subchild1 = lxml.etree.SubElement(child1, "SubChild", name="subchild")
        child2 = lxml.etree.SubElement(data, "Group", name="bar", negate="true")
        idict = dict()

        inode = self._get_inode(data, idict)
        inode.ignore = []

        @patch("Bcfg2.Server.Plugin.helpers.%s.__init__" %
               inode.__class__.__name__)
        def inner2(mock_init):
            mock_init.return_value = None
            inode._load_children(data, idict)
            mock_init.assert_called_with(child2, idict, inode)
            tag = child1.tag
            name = child1.get("name")
            self.assertEqual(idict, dict(Data=[name]))
            self.assertIn(tag, inode.contents)
            self.assertIn(name, inode.contents[tag])
            self.assertItemsEqual(inode.contents[tag][name],
                                  dict(name=name,
                                       attr=child1.get('attr'),
                                       __text__=child1.text,
                                       __children__=[subchild1]))

        inner2()

        # test ignore.  no ignore is set on INode by default, so we
        # have to set one
        old_ignore = copy.copy(self.test_obj.ignore)
        self.test_obj.ignore.append("Data")
        idict = dict()

        inode = self._get_inode(data, idict)

        @patch("Bcfg2.Server.Plugin.helpers.%s.__init__" %
               inode.__class__.__name__)
        def inner3(mock_init):
            mock_init.return_value = None
            inode._load_children(data, idict)
            mock_init.assert_called_with(child2, idict, inode)
            self.assertEqual(idict, dict())
            self.assertItemsEqual(inode.contents, dict())

        inner3()
        self.test_obj.ignore = old_ignore

    def test_Match(self):
        idata = lxml.etree.Element("Parent")
        contents = lxml.etree.SubElement(idata, "Data", name="contents",
                                         attr="some attr")
        child = lxml.etree.SubElement(idata, "Group", name="bar", negate="true")

        inode = INode(idata, dict())
        inode.predicate = Mock()
        inode.predicate.return_value = False

        metadata = Mock()
        metadata.groups = ['foo']
        data = dict()
        entry = child

        inode.Match(metadata, data, entry=child)
        self.assertEqual(data, dict())
        inode.predicate.assert_called_with(metadata, child)

        inode.predicate.reset_mock()
        inode.Match(metadata, data)
        self.assertEqual(data, dict())
        # can't easily compare XML args without the original
        # object, and we're testing that Match() works without an
        # XML object passed in, so...
        self.assertEqual(inode.predicate.call_args[0][0],
                         metadata)
        self.assertXMLEqual(inode.predicate.call_args[0][1],
                            lxml.etree.Element("None"))

        inode.predicate.reset_mock()
        inode.predicate.return_value = True
        inode.Match(metadata, data, entry=child)
        self.assertEqual(data, inode.contents)
        inode.predicate.assert_called_with(metadata, child)


class TestInfoNode(TestINode):
    __test__ = True
    test_obj = InfoNode

    def test_raw_predicates(self):
        TestINode.test_raw_predicates(self)
        metadata = Mock()
        entry = lxml.etree.Element("Path", name="/tmp/foo",
                                   realname="/tmp/bar")

        parent_predicate = lambda m, d: True
        pred = eval(self.test_obj.raw['Path'] % dict(name="/tmp/foo"),
                    dict(predicate=parent_predicate))
        self.assertTrue(pred(metadata, entry))
        pred = eval(InfoNode.raw['Path'] % dict(name="/tmp/bar"),
                    dict(predicate=parent_predicate))
        self.assertTrue(pred(metadata, entry))
        pred = eval(InfoNode.raw['Path'] % dict(name="/tmp/bogus"),
                    dict(predicate=parent_predicate))
        self.assertFalse(pred(metadata, entry))

        pred = eval(self.test_obj.nraw['Path'] % dict(name="/tmp/foo"),
                    dict(predicate=parent_predicate))
        self.assertFalse(pred(metadata, entry))
        pred = eval(InfoNode.nraw['Path'] % dict(name="/tmp/bar"),
                    dict(predicate=parent_predicate))
        self.assertFalse(pred(metadata, entry))
        pred = eval(InfoNode.nraw['Path'] % dict(name="/tmp/bogus"),
                    dict(predicate=parent_predicate))
        self.assertTrue(pred(metadata, entry))

        parent_predicate = lambda m, d: False
        pred = eval(self.test_obj.raw['Path'] % dict(name="/tmp/foo"),
                    dict(predicate=parent_predicate))
        self.assertFalse(pred(metadata, entry))
        pred = eval(InfoNode.raw['Path'] % dict(name="/tmp/bar"),
                    dict(predicate=parent_predicate))
        self.assertFalse(pred(metadata, entry))
        pred = eval(InfoNode.nraw['Path'] % dict(name="/tmp/bogus"),
                    dict(predicate=parent_predicate))
        self.assertFalse(pred(metadata, entry))


class TestXMLSrc(TestXMLFileBacked):
    test_obj = XMLSrc

    def test_node_interface(self):
        # ensure that the node object has the necessary interface
        self.assertTrue(hasattr(self.test_obj.__node__, "Match"))

    @patch("lxml.etree.parse")
    def test_HandleEvent(self, mock_parse):
        xdata = lxml.etree.Element("Test")
        lxml.etree.SubElement(xdata, "Path", name="path", attr="whatever")

        xsrc = self.get_obj("/test/foo.xml")
        xsrc.__node__ = Mock()
        mock_parse.return_value = xdata.getroottree()

        if xsrc.__priority_required__:
            # test with no priority at all
            self.assertRaises(PluginExecutionError,
                              xsrc.HandleEvent, Mock())

            # test with bogus priority
            xdata.set("priority", "cow")
            mock_parse.return_value = xdata.getroottree()
            self.assertRaises(PluginExecutionError,
                              xsrc.HandleEvent, Mock())

            # assign a priority to use in future tests
            xdata.set("priority", "10")
            mock_parse.return_value = xdata.getroottree()

        mock_parse.reset_mock()
        xsrc = self.get_obj("/test/foo.xml")
        xsrc.__node__ = Mock()
        xsrc.HandleEvent(Mock())
        mock_parse.assert_called_with("/test/foo.xml",
                                      parser=Bcfg2.Server.XMLParser)
        self.assertXMLEqual(xsrc.__node__.call_args[0][0], xdata)
        self.assertEqual(xsrc.__node__.call_args[0][1], dict())
        self.assertEqual(xsrc.pnode, xsrc.__node__.return_value)
        self.assertEqual(xsrc.cache, None)

    @patch("Bcfg2.Server.Plugin.helpers.XMLSrc.HandleEvent")
    def test_Cache(self, mock_HandleEvent):
        xsrc = self.get_obj("/test/foo.xml")
        metadata = Mock()
        xsrc.Cache(metadata)
        mock_HandleEvent.assert_any_call()

        xsrc.pnode = Mock()
        xsrc.Cache(metadata)
        xsrc.pnode.Match.assert_called_with(metadata, xsrc.__cacheobj__())
        self.assertEqual(xsrc.cache[0], metadata)

        xsrc.pnode.reset_mock()
        xsrc.Cache(metadata)
        self.assertFalse(xsrc.pnode.Mock.called)
        self.assertEqual(xsrc.cache[0], metadata)

        xsrc.cache = ("bogus")
        xsrc.Cache(metadata)
        xsrc.pnode.Match.assert_called_with(metadata, xsrc.__cacheobj__())
        self.assertEqual(xsrc.cache[0], metadata)


class TestInfoXML(TestXMLSrc):
    test_obj = InfoXML


class TestXMLDirectoryBacked(TestDirectoryBacked):
    test_obj = XMLDirectoryBacked
    testfiles = ['foo.xml', 'bar/baz.xml', 'plugh.plugh.xml']
    badpaths = ["foo", "foo.txt", "foo.xsd", "xml"]


class TestPrioDir(TestPlugin, TestGenerator, TestXMLDirectoryBacked):
    test_obj = PrioDir

    def get_obj(self, core=None):
        if core is None:
            core = Mock()

        @patch("%s.%s.add_directory_monitor" %
               (self.test_obj.__module__, self.test_obj.__name__),
               Mock())
        @patchIf(not isinstance(os.makedirs, Mock), "os.makedirs", Mock())
        def inner():
            return self.test_obj(core, datastore)

        return inner()

    def test_HandleEvent(self):
        TestXMLDirectoryBacked.test_HandleEvent(self)

        @patch("Bcfg2.Server.Plugin.helpers.XMLDirectoryBacked.HandleEvent",
               Mock())
        def inner():
            pd = self.get_obj()
            test1 = Mock()
            test1.items = dict(Path=["/etc/foo.conf", "/etc/bar.conf"])
            test2 = Mock()
            test2.items = dict(Path=["/etc/baz.conf"],
                               Package=["quux", "xyzzy"])
            pd.entries = {"/test1.xml": test1,
                          "/test2.xml": test2}
            pd.HandleEvent(Mock())
            self.assertItemsEqual(pd.Entries,
                                  dict(Path={"/etc/foo.conf": pd.BindEntry,
                                             "/etc/bar.conf": pd.BindEntry,
                                             "/etc/baz.conf": pd.BindEntry},
                                       Package={"quux": pd.BindEntry,
                                                "xyzzy": pd.BindEntry}))

        inner()

    def test__matches(self):
        pd = self.get_obj()
        self.assertTrue(pd._matches(lxml.etree.Element("Test",
                                                       name="/etc/foo.conf"),
                                    Mock(),
                                    {"/etc/foo.conf": pd.BindEntry,
                                     "/etc/bar.conf": pd.BindEntry}))
        self.assertFalse(pd._matches(lxml.etree.Element("Test",
                                                        name="/etc/baz.conf"),
                                     Mock(),
                                     {"/etc/foo.conf": pd.BindEntry,
                                      "/etc/bar.conf": pd.BindEntry}))

    def test_BindEntry(self):
        pd = self.get_obj()
        pd.get_attrs = Mock(return_value=dict(test1="test1", test2="test2"))
        entry = lxml.etree.Element("Path", name="/etc/foo.conf", test1="bogus")
        metadata = Mock()
        pd.BindEntry(entry, metadata)
        pd.get_attrs.assert_called_with(entry, metadata)
        self.assertItemsEqual(entry.attrib,
                              dict(name="/etc/foo.conf",
                                   test1="test1", test2="test2"))

    def test_get_attrs(self):
        pd = self.get_obj()
        entry = lxml.etree.Element("Path", name="/etc/foo.conf")
        children = [lxml.etree.Element("Child")]
        metadata = Mock()
        pd.entries = dict()

        def reset():
            metadata.reset_mock()
            for src in pd.entries.values():
                src.reset_mock()
                src.cache = None

        # test with no matches
        self.assertRaises(PluginExecutionError,
                          pd.get_attrs, entry, metadata)

        def add_entry(name, data, prio=10):
            path = os.path.join(pd.data, name)
            pd.entries[path] = Mock()
            pd.entries[path].priority = prio
            def do_Cache(metadata):
                pd.entries[path].cache = (metadata, data)
            pd.entries[path].Cache.side_effect = do_Cache

        add_entry('test1.xml',
                  dict(Path={'/etc/foo.conf': dict(attr="attr1",
                                                   __children__=children),
                             '/etc/bar.conf': dict()}))
        add_entry('test2.xml',
                  dict(Path={'/etc/bar.conf': dict(__text__="text",
                                                   attr="attr1")},
                       Package={'quux': dict(),
                                'xyzzy': dict()}),
                  prio=20)
        add_entry('test3.xml',
                  dict(Path={'/etc/baz.conf': dict()},
                       Package={'xyzzy': dict()}),
                  prio=20)

        # test with exactly one match, __children__
        reset()
        self.assertItemsEqual(pd.get_attrs(entry, metadata),
                              dict(attr="attr1"))
        for src in pd.entries.values():
            src.Cache.assert_called_with(metadata)
        self.assertEqual(len(entry.getchildren()), 1)
        self.assertXMLEqual(entry.getchildren()[0], children[0])

        # test with multiple matches with different priorities, __text__
        reset()
        entry = lxml.etree.Element("Path", name="/etc/bar.conf")
        self.assertItemsEqual(pd.get_attrs(entry, metadata),
                              dict(attr="attr1"))
        for src in pd.entries.values():
            src.Cache.assert_called_with(metadata)
        self.assertEqual(entry.text, "text")

        # test with multiple matches with identical priorities
        reset()
        entry = lxml.etree.Element("Package", name="xyzzy")
        self.assertRaises(PluginExecutionError,
                          pd.get_attrs, entry, metadata)


class TestSpecificity(Bcfg2TestCase):
    test_obj = Specificity

    def get_obj(self, **kwargs):
        return self.test_obj(**kwargs)

    def test_matches(self):
        metadata = Mock()
        metadata.hostname = "foo.example.com"
        metadata.groups = ["group1", "group2"]
        self.assertTrue(self.get_obj(all=True).matches(metadata))
        self.assertTrue(self.get_obj(group="group1").matches(metadata))
        self.assertTrue(self.get_obj(hostname="foo.example.com").matches(metadata))
        self.assertFalse(self.get_obj().matches(metadata))
        self.assertFalse(self.get_obj(group="group3").matches(metadata))
        self.assertFalse(self.get_obj(hostname="bar.example.com").matches(metadata))

    def test__cmp(self):
        specs = [self.get_obj(all=True),
                 self.get_obj(group="group1", prio=10),
                 self.get_obj(group="group1", prio=20),
                 self.get_obj(hostname="foo.example.com")]

        for i in range(len(specs)):
            for j in range(len(specs)):
                if i == j:
                    self.assertEqual(0, specs[i].__cmp__(specs[j]))
                    self.assertEqual(0, specs[j].__cmp__(specs[i]))
                elif i > j:
                    self.assertEqual(-1, specs[i].__cmp__(specs[j]))
                    self.assertEqual(1, specs[j].__cmp__(specs[i]))
                elif i < j:
                    self.assertEqual(1, specs[i].__cmp__(specs[j]))
                    self.assertEqual(-1, specs[j].__cmp__(specs[i]))

    def test_cmp(self):
        """ test __lt__/__gt__/__eq__ """
        specs = [self.get_obj(all=True),
                 self.get_obj(group="group1", prio=10),
                 self.get_obj(group="group1", prio=20),
                 self.get_obj(hostname="foo.example.com")]

        for i in range(len(specs)):
            for j in range(len(specs)):
                if i < j:
                    self.assertGreater(specs[i], specs[j])
                    self.assertLess(specs[j], specs[i])
                    self.assertGreaterEqual(specs[i], specs[j])
                    self.assertLessEqual(specs[j], specs[i])
                elif i == j:
                    self.assertEqual(specs[i], specs[j])
                    self.assertEqual(specs[j], specs[i])
                    self.assertLessEqual(specs[i], specs[j])
                    self.assertGreaterEqual(specs[j], specs[i])
                elif i > j:
                    self.assertLess(specs[i], specs[j])
                    self.assertGreater(specs[j], specs[i])
                    self.assertLessEqual(specs[i], specs[j])
                    self.assertGreaterEqual(specs[j], specs[i])


class TestSpecificData(Bcfg2TestCase):
    test_obj = SpecificData
    path = os.path.join(datastore, "test.txt")

    def get_obj(self, name=None, specific=None, encoding=None):
        if name is None:
            name = self.path
        if specific is None:
            specific = Mock()
        return self.test_obj(name, specific, encoding)

    def test__init(self):
        pass

    @patch("%s.open" % builtins)
    def test_handle_event(self, mock_open):
        event = Mock()
        event.code2str.return_value = 'deleted'
        sd = self.get_obj()
        sd.handle_event(event)
        self.assertFalse(mock_open.called)
        if hasattr(sd, 'data'):
            self.assertIsNone(sd.data)
        else:
            self.assertFalse(hasattr(sd, 'data'))

        event = Mock()
        mock_open.return_value.read.return_value = "test"
        sd.handle_event(event)
        mock_open.assert_called_with(self.path)
        mock_open.return_value.read.assert_any_call()
        self.assertEqual(sd.data, "test")


class TestEntrySet(TestDebuggable):
    test_obj = EntrySet
    # filenames that should be matched successfully by the EntrySet
    # 'specific' regex.  these are filenames alone -- a specificity
    # will be added to these
    basenames = ["test", "test.py", "test with spaces.txt",
                 "test.multiple.dots.py", "test_underscores.and.dots",
                 "really_misleading.G10_test",
                 "name$with*regex(special){chars}",
                 "misleading.H_hostname.test.com"]
    # filenames that do not match any of the basenames (or the
    # basename regex, if applicable)
    bogus_names = ["bogus"]
    # filenames that should be ignored
    ignore = ["foo~", ".#foo", ".foo.swp", ".foo.swx",
              "test.txt.genshi_include", "test.G_foo.genshi_include"]

    def get_obj(self, basename="test", path=datastore, entry_type=MagicMock(),
                encoding=None):
        return self.test_obj(basename, path, entry_type, encoding)

    def test__init(self):
        for basename in self.basenames:
            eset = self.get_obj(basename=basename)
            self.assertIsInstance(eset.specific, re_type)
            self.assertTrue(eset.specific.match(os.path.join(datastore,
                                                             basename)))
            ppath = os.path.join(datastore, "Plugin", basename)
            self.assertTrue(eset.specific.match(ppath))
            self.assertTrue(eset.specific.match(ppath + ".G20_foo"))
            self.assertTrue(eset.specific.match(ppath + ".G1_foo"))
            self.assertTrue(eset.specific.match(ppath + ".G32768_foo"))
            # a group named '_'
            self.assertTrue(eset.specific.match(ppath + ".G10__"))
            self.assertTrue(eset.specific.match(ppath + ".H_hostname"))
            self.assertTrue(eset.specific.match(ppath + ".H_fqdn.subdomain.example.com"))
            self.assertTrue(eset.specific.match(ppath + ".G20_group_with_underscores"))

            self.assertFalse(eset.specific.match(ppath + ".G20_group with spaces"))
            self.assertFalse(eset.specific.match(ppath + ".G_foo"))
            self.assertFalse(eset.specific.match(ppath + ".G_"))
            self.assertFalse(eset.specific.match(ppath + ".G20_"))
            self.assertFalse(eset.specific.match(ppath + ".H_"))

            for bogus in self.bogus_names:
                self.assertFalse(eset.specific.match(os.path.join(datastore,
                                                                  "Plugin",
                                                                  bogus)))

            for ignore in self.ignore:
                self.assertTrue(eset.ignore.match(ignore),
                                "%s should be ignored but wasn't" % ignore)

            self.assertFalse(eset.ignore.match(basename))
            self.assertFalse(eset.ignore.match(basename + ".G20_foo"))
            self.assertFalse(eset.ignore.match(basename + ".G1_foo"))
            self.assertFalse(eset.ignore.match(basename + ".G32768_foo"))
            self.assertFalse(eset.ignore.match(basename + ".G10__"))
            self.assertFalse(eset.ignore.match(basename + ".H_hostname"))
            self.assertFalse(eset.ignore.match(basename + ".H_fqdn.subdomain.example.com"))
            self.assertFalse(eset.ignore.match(basename + ".G20_group_with_underscores"))

    def test_get_matching(self):
        items = {0: Mock(), 1: Mock(), 2: Mock(), 3: Mock(), 4: Mock(),
                 5: Mock()}
        items[0].specific.matches.return_value = False
        items[1].specific.matches.return_value = True
        items[2].specific.matches.return_value = False
        items[3].specific.matches.return_value = False
        items[4].specific.matches.return_value = True
        items[5].specific.matches.return_value = True
        metadata = Mock()
        eset = self.get_obj()
        eset.entries = items
        self.assertItemsEqual(eset.get_matching(metadata),
                              [items[1], items[4], items[5]])
        for i in items.values():
            i.specific.matches.assert_called_with(metadata)

    def test_best_matching(self):
        eset = self.get_obj()
        eset.get_matching = Mock()
        metadata = Mock()
        matching = []

        def reset():
            eset.get_matching.reset_mock()
            metadata.reset_mock()
            for m in matching:
                m.reset_mock()

        def specific(all=False, group=False, prio=None, hostname=False):
            spec = Mock()
            spec.specific = Specificity(all=all, group=group, prio=prio,
                                        hostname=hostname)
            return spec

        self.assertRaises(PluginExecutionError,
                          eset.best_matching, metadata, matching=[])

        reset()
        eset.get_matching.return_value = matching
        self.assertRaises(PluginExecutionError,
                          eset.best_matching, metadata)
        eset.get_matching.assert_called_with(metadata)

        # test with a single file for all
        reset()
        expected = specific(all=True)
        matching.append(expected)
        eset.get_matching.return_value = matching
        self.assertEqual(eset.best_matching(metadata), expected)
        eset.get_matching.assert_called_with(metadata)

        # test with a single group-specific file
        reset()
        expected = specific(group=True, prio=10)
        matching.append(expected)
        eset.get_matching.return_value = matching
        self.assertEqual(eset.best_matching(metadata), expected)
        eset.get_matching.assert_called_with(metadata)

        # test with multiple group-specific files
        reset()
        expected = specific(group=True, prio=20)
        matching.append(expected)
        eset.get_matching.return_value = matching
        self.assertEqual(eset.best_matching(metadata), expected)
        eset.get_matching.assert_called_with(metadata)

        # test with host-specific file
        reset()
        expected = specific(hostname=True)
        matching.append(expected)
        eset.get_matching.return_value = matching
        self.assertEqual(eset.best_matching(metadata), expected)
        eset.get_matching.assert_called_with(metadata)

    def test_handle_event(self):
        eset = self.get_obj()
        eset.entry_init = Mock()
        eset.reset_metadata = Mock()
        eset.update_metadata = Mock()

        def reset():
            eset.update_metadata.reset_mock()
            eset.reset_metadata.reset_mock()
            eset.entry_init.reset_mock()

        for fname in ["info", "info.xml", ":info"]:
            for evt in ["exists", "created", "changed"]:
                reset()
                event = Mock()
                event.code2str.return_value = evt
                event.filename = fname
                eset.handle_event(event)
                eset.update_metadata.assert_called_with(event)
                self.assertFalse(eset.entry_init.called)
                self.assertFalse(eset.reset_metadata.called)

            reset()
            event = Mock()
            event.code2str.return_value = "deleted"
            event.filename = fname
            eset.handle_event(event)
            eset.reset_metadata.assert_called_with(event)
            self.assertFalse(eset.entry_init.called)
            self.assertFalse(eset.update_metadata.called)

        for evt in ["exists", "created", "changed"]:
            reset()
            event = Mock()
            event.code2str.return_value = evt
            event.filename = "test.txt"
            eset.handle_event(event)
            eset.entry_init.assert_called_with(event)
            self.assertFalse(eset.reset_metadata.called)
            self.assertFalse(eset.update_metadata.called)

        reset()
        entry = Mock()
        eset.entries["test.txt"] = entry
        event = Mock()
        event.code2str.return_value = "changed"
        event.filename = "test.txt"
        eset.handle_event(event)
        entry.handle_event.assert_called_with(event)
        self.assertFalse(eset.entry_init.called)
        self.assertFalse(eset.reset_metadata.called)
        self.assertFalse(eset.update_metadata.called)

        reset()
        entry = Mock()
        eset.entries["test.txt"] = entry
        event = Mock()
        event.code2str.return_value = "deleted"
        event.filename = "test.txt"
        eset.handle_event(event)
        self.assertNotIn("test.txt", eset.entries)

    def test_entry_init(self):
        eset = self.get_obj()
        eset.specificity_from_filename = Mock()

        def reset():
            eset.entry_type.reset_mock()
            eset.specificity_from_filename.reset_mock()

        event = Mock()
        event.code2str.return_value = "created"
        event.filename = "test.txt"
        eset.entry_init(event)
        eset.specificity_from_filename.assert_called_with("test.txt",
                                                          specific=None)
        eset.entry_type.assert_called_with(os.path.join(eset.path, "test.txt"),
                                           eset.specificity_from_filename.return_value, None)
        eset.entry_type.return_value.handle_event.assert_called_with(event)
        self.assertIn("test.txt", eset.entries)

        # test duplicate add
        reset()
        eset.entry_init(event)
        self.assertFalse(eset.specificity_from_filename.called)
        self.assertFalse(eset.entry_type.called)
        eset.entries["test.txt"].handle_event.assert_called_with(event)

        # test keyword args
        etype = Mock()
        specific = Mock()
        event = Mock()
        event.code2str.return_value = "created"
        event.filename = "test2.txt"
        eset.entry_init(event, entry_type=etype, specific=specific)
        eset.specificity_from_filename.assert_called_with("test2.txt",
                                                          specific=specific)
        etype.assert_called_with(os.path.join(eset.path, "test2.txt"),
                                 eset.specificity_from_filename.return_value,
                                 None)
        etype.return_value.handle_event.assert_called_with(event)
        self.assertIn("test2.txt", eset.entries)

        # test specificity error
        event = Mock()
        event.code2str.return_value = "created"
        event.filename = "test3.txt"
        eset.specificity_from_filename.side_effect = SpecificityError
        eset.entry_init(event)
        eset.specificity_from_filename.assert_called_with("test3.txt",
                                                          specific=None)
        self.assertFalse(eset.entry_type.called)

    @patch("Bcfg2.Server.Plugin.helpers.Specificity")
    def test_specificity_from_filename(self, mock_spec):
        # There's a strange scoping issue in py3k that prevents this
        # test from working as expected on sub-classes of EntrySet.
        # No idea what's going on, but until I can figure it out we
        # skip this test on subclasses
        if inPy3k and self.test_obj != EntrySet:
            return skip("Skipping this test for py3k scoping issues")

        def test(eset, fname, **kwargs):
            mock_spec.reset_mock()
            if "specific" in kwargs:
                specific = kwargs['specific']
                del kwargs['specific']
            else:
                specific = None
            self.assertEqual(eset.specificity_from_filename(fname,
                                                            specific=specific),
                             mock_spec.return_value)
            mock_spec.assert_called_with(**kwargs)

        def fails(eset, fname, specific=None):
            mock_spec.reset_mock()
            self.assertRaises(SpecificityError,
                              eset.specificity_from_filename, fname,
                              specific=specific)

        for basename in self.basenames:
            eset = self.get_obj(basename=basename)
            ppath = os.path.join(datastore, "Plugin", basename)
            test(eset, ppath, all=True)
            test(eset, ppath + ".G20_foo", group="foo", prio=20)
            test(eset, ppath + ".G1_foo", group="foo", prio=1)
            test(eset, ppath + ".G32768_foo", group="foo", prio=32768)
            test(eset, ppath + ".G10__", group="_", prio=10)
            test(eset, ppath + ".H_hostname", hostname="hostname")
            test(eset, ppath + ".H_fqdn.subdomain.example.com",
                 hostname="fqdn.subdomain.example.com")
            test(eset, ppath + ".G20_group_with_underscores",
                 group="group_with_underscores", prio=20)

            for bogus in self.bogus_names:
                fails(eset, bogus)
            fails(eset, ppath + ".G_group with spaces")
            fails(eset, ppath + ".G_foo")
            fails(eset, ppath + ".G_")
            fails(eset, ppath + ".G20_")
            fails(eset, ppath + ".H_")

    @patch("%s.open" % builtins)
    @patch("Bcfg2.Server.Plugin.helpers.InfoXML")
    def test_update_metadata(self, mock_InfoXML, mock_open):
        # There's a strange scoping issue in py3k that prevents this
        # test from working as expected on sub-classes of EntrySet.
        # No idea what's going on, but until I can figure it out we
        # skip this test on subclasses
        if inPy3k and self.test_obj != EntrySet:
            return skip("Skipping this test for py3k scoping issues")

        eset = self.get_obj()

        # add info.xml
        event = Mock()
        event.filename = "info.xml"
        eset.update_metadata(event)
        mock_InfoXML.assert_called_with(os.path.join(eset.path, "info.xml"))
        mock_InfoXML.return_value.HandleEvent.assert_called_with(event)
        self.assertEqual(eset.infoxml, mock_InfoXML.return_value)

        # modify info.xml
        mock_InfoXML.reset_mock()
        eset.update_metadata(event)
        self.assertFalse(mock_InfoXML.called)
        eset.infoxml.HandleEvent.assert_called_with(event)

        for fname in [':info', 'info']:
            event = Mock()
            event.filename = fname

            idata = ["owner:owner",
                     "group:             GROUP",
                     "mode: 775",
                     "important:     true",
                     "bogus: line"]
            mock_open.return_value.readlines.return_value = idata
            eset.update_metadata(event)
            expected = DEFAULT_FILE_METADATA.copy()
            expected['owner'] = 'owner'
            expected['group'] = 'GROUP'
            expected['mode'] = '0775'
            expected['important'] = 'true'
            self.assertItemsEqual(eset.metadata,
                                  expected)

    def test_reset_metadata(self):
        eset = self.get_obj()

        # test info.xml
        event = Mock()
        event.filename = "info.xml"
        eset.infoxml = Mock()
        eset.reset_metadata(event)
        self.assertIsNone(eset.infoxml)

        for fname in [':info', 'info']:
            event = Mock()
            event.filename = fname
            eset.metadata = Mock()
            eset.reset_metadata(event)
            self.assertItemsEqual(eset.metadata, DEFAULT_FILE_METADATA)

    @patch("Bcfg2.Server.Plugin.helpers.bind_info")
    def test_bind_info_to_entry(self, mock_bind_info):
        # There's a strange scoping issue in py3k that prevents this
        # test from working as expected on sub-classes of EntrySet.
        # No idea what's going on, but until I can figure it out we
        # skip this test on subclasses
        if inPy3k and self.test_obj != EntrySet:
            return skip("Skipping this test for py3k scoping issues")

        eset = self.get_obj()
        entry = Mock()
        metadata = Mock()
        eset.bind_info_to_entry(entry, metadata)
        mock_bind_info.assert_called_with(entry, metadata,
                                          infoxml=eset.infoxml,
                                          default=eset.metadata)

    def test_bind_entry(self):
        eset = self.get_obj()
        eset.best_matching = Mock()
        eset.bind_info_to_entry = Mock()

        entry = Mock()
        metadata = Mock()
        eset.bind_entry(entry, metadata)
        eset.bind_info_to_entry.assert_called_with(entry, metadata)
        eset.best_matching.assert_called_with(metadata)
        eset.best_matching.return_value.bind_entry.assert_called_with(entry,
                                                                      metadata)


class TestGroupSpool(TestPlugin, TestGenerator):
    test_obj = GroupSpool

    def get_obj(self, core=None):
        if core is None:
            core = MagicMock()
            core.setup = MagicMock()
        else:
            try:
                core.setup['encoding']
            except TypeError:
                core.setup.__getitem__ = MagicMock()

        @patch("%s.%s.AddDirectoryMonitor" % (self.test_obj.__module__,
                                              self.test_obj.__name__),
               Mock())
        def inner():
            return TestPlugin.get_obj(self, core=core)

        return inner()

    def test__init(self):
        @patchIf(not isinstance(os.makedirs, Mock), "os.makedirs", Mock())
        @patch("%s.%s.AddDirectoryMonitor" % (self.test_obj.__module__,
                                              self.test_obj.__name__))
        def inner(mock_Add):
            gs = self.test_obj(MagicMock(), datastore)
            mock_Add.assert_called_with('')
            self.assertItemsEqual(gs.Entries, {gs.entry_type: {}})

        inner()

    @patch("os.path.isdir")
    @patch("os.path.isfile")
    def test_add_entry(self, mock_isfile, mock_isdir):
        gs = self.get_obj()
        gs.es_cls = Mock()
        gs.es_child_cls = Mock()
        gs.event_id = Mock()
        gs.event_path = Mock()
        gs.AddDirectoryMonitor = Mock()

        def reset():
            gs.es_cls.reset_mock()
            gs.es_child_cls.reset_mock()
            gs.AddDirectoryMonitor.reset_mock()
            gs.event_path.reset_mock()
            gs.event_id.reset_mock()
            mock_isfile.reset_mock()
            mock_isdir.reset_mock()

        # directory
        event = Mock()
        event.filename = "foo"
        basedir = "test"
        epath = os.path.join(gs.data, basedir, event.filename)
        ident = os.path.join(basedir, event.filename)
        gs.event_path.return_value = epath
        gs.event_id.return_value = ident
        mock_isdir.return_value = True
        mock_isfile.return_value = False
        gs.add_entry(event)
        gs.AddDirectoryMonitor.assert_called_with(os.path.join("/" + basedir,
                                                               event.filename))
        self.assertNotIn(ident, gs.entries)
        mock_isdir.assert_called_with(epath)

        # file that is not in self.entries
        reset()
        event = Mock()
        event.filename = "foo"
        basedir = "test/foo/"
        epath = os.path.join(gs.data, basedir, event.filename)
        ident = basedir[:-1]
        gs.event_path.return_value = epath
        gs.event_id.return_value = ident
        mock_isdir.return_value = False
        mock_isfile.return_value = True
        gs.add_entry(event)
        self.assertFalse(gs.AddDirectoryMonitor.called)
        gs.es_cls.assert_called_with(gs.filename_pattern,
                                     gs.data + ident,
                                     gs.es_child_cls,
                                     gs.encoding)
        self.assertIn(ident, gs.entries)
        self.assertEqual(gs.entries[ident], gs.es_cls.return_value)
        self.assertIn(ident, gs.Entries[gs.entry_type])
        self.assertEqual(gs.Entries[gs.entry_type][ident],
                         gs.es_cls.return_value.bind_entry)
        gs.entries[ident].handle_event.assert_called_with(event)
        mock_isfile.assert_called_with(epath)

        # file that is in self.entries
        reset()
        gs.add_entry(event)
        self.assertFalse(gs.AddDirectoryMonitor.called)
        self.assertFalse(gs.es_cls.called)
        gs.entries[ident].handle_event.assert_called_with(event)

    def test_event_path(self):
        gs = self.get_obj()
        gs.handles[1] = "/var/lib/foo/"
        gs.handles[2] = "/etc/foo/"
        gs.handles[3] = "/usr/share/foo/"
        event = Mock()
        event.filename = "foo"
        for i in range(1, 4):
            event.requestID = i
            self.assertEqual(gs.event_path(event),
                             os.path.join(datastore, gs.name,
                                          gs.handles[event.requestID].lstrip('/'),
                                          event.filename))

    @patch("os.path.isdir")
    def test_event_id(self, mock_isdir):
        gs = self.get_obj()
        gs.event_path = Mock()

        def reset():
            gs.event_path.reset_mock()
            mock_isdir.reset_mock()

        gs.handles[1] = "/var/lib/foo/"
        gs.handles[2] = "/etc/foo/"
        gs.handles[3] = "/usr/share/foo/"
        event = Mock()
        event.filename = "foo"
        for i in range(1, 4):
            event.requestID = i
            reset()
            mock_isdir.return_value = True
            self.assertEqual(gs.event_id(event),
                             os.path.join(gs.handles[event.requestID].lstrip('/'),
                                          event.filename))
            mock_isdir.assert_called_with(gs.event_path.return_value)

            reset()
            mock_isdir.return_value = False
            self.assertEqual(gs.event_id(event),
                             gs.handles[event.requestID].rstrip('/'))
            mock_isdir.assert_called_with(gs.event_path.return_value)

    def test_set_debug(self):
        gs = self.get_obj()
        gs.entries = {"/foo": Mock(),
                      "/bar": Mock(),
                      "/baz/quux": Mock()}

        @patch("Bcfg2.Server.Plugin.helpers.Plugin.set_debug")
        def inner(mock_debug):
            gs.set_debug(True)
            mock_debug.assert_called_with(gs, True)
            for entry in gs.entries.values():
                entry.set_debug.assert_called_with(True)

        inner()

        TestPlugin.test_set_debug(self)

    def test_HandleEvent(self):
        gs = self.get_obj()
        gs.entries = {"/foo": Mock(),
                      "/bar": Mock(),
                      "/baz": Mock(),
                      "/baz/quux": Mock()}
        for path in gs.entries.keys():
            gs.Entries[gs.entry_type] = {path: Mock()}
        gs.handles = {1: "/foo/",
                      2: "/bar/",
                      3: "/baz/",
                      4: "/baz/quux"}

        gs.add_entry = Mock()
        gs.event_id = Mock()

        def reset():
            gs.add_entry.reset_mock()
            gs.event_id.reset_mock()
            for entry in gs.entries.values():
                entry.reset_mock()

        # test event creation, changing entry that doesn't exist
        for evt in ["exists", "created", "changed"]:
            reset()
            event = Mock()
            event.filename = "foo"
            event.requestID = 1
            event.code2str.return_value = evt
            gs.HandleEvent(event)
            gs.event_id.assert_called_with(event)
            gs.add_entry.assert_called_with(event)

        # test deleting entry, changing entry that does exist
        for evt in ["changed", "deleted"]:
            reset()
            event = Mock()
            event.filename = "quux"
            event.requestID = 4
            event.code2str.return_value = evt
            gs.event_id.return_value = "/baz/quux"
            gs.HandleEvent(event)
            gs.event_id.assert_called_with(event)
            self.assertIn(gs.event_id.return_value, gs.entries)
            gs.entries[gs.event_id.return_value].handle_event.assert_called_with(event)
            self.assertFalse(gs.add_entry.called)

        # test deleting directory
        reset()
        event = Mock()
        event.filename = "quux"
        event.requestID = 3
        event.code2str.return_value = "deleted"
        gs.event_id.return_value = "/baz/quux"
        gs.HandleEvent(event)
        gs.event_id.assert_called_with(event)
        self.assertNotIn("/baz/quux", gs.entries)
        self.assertNotIn("/baz/quux", gs.Entries[gs.entry_type])

########NEW FILE########
__FILENAME__ = Testinterfaces
import os
import sys
import lxml.etree
import Bcfg2.Server
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugin.interfaces import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != '/':
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestServer.TestPlugin.Testbase import TestPlugin


class TestGenerator(Bcfg2TestCase):
    test_obj = Generator

    def test_HandlesEntry(self):
        pass

    def test_HandleEntry(self):
        pass


class TestStructure(Bcfg2TestCase):
    test_obj = Structure

    def get_obj(self):
        return self.test_obj()

    def test_BuildStructures(self):
        s = self.get_obj()
        self.assertRaises(NotImplementedError,
                          s.BuildStructures, None)


class TestMetadata(Bcfg2TestCase):
    test_obj = Metadata

    def get_obj(self):
        return self.test_obj()

    def test_AuthenticateConnection(self):
        m = self.get_obj()
        self.assertRaises(NotImplementedError,
                          m.AuthenticateConnection,
                          None, None, None, (None, None))

    def test_get_initial_metadata(self):
        m = self.get_obj()
        self.assertRaises(NotImplementedError,
                          m.get_initial_metadata, None)

    def test_merge_additional_data(self):
        m = self.get_obj()
        self.assertRaises(NotImplementedError,
                          m.merge_additional_data, None, None, None)

    def test_merge_additional_groups(self):
        m = self.get_obj()
        self.assertRaises(NotImplementedError,
                          m.merge_additional_groups, None, None)


class TestConnector(Bcfg2TestCase):
    """ placeholder """
    def test_get_additional_groups(self):
        pass

    def test_get_additional_data(self):
        pass


class TestProbing(Bcfg2TestCase):
    test_obj = Probing

    def get_obj(self):
        return self.test_obj()

    def test_GetProbes(self):
        p = self.get_obj()
        self.assertRaises(NotImplementedError,
                          p.GetProbes, None)

    def test_ReceiveData(self):
        p = self.get_obj()
        self.assertRaises(NotImplementedError,
                          p.ReceiveData, None, None)


class TestStatistics(TestPlugin):
    test_obj = Statistics

    def test_process_statistics(self):
        s = self.get_obj()
        self.assertRaises(NotImplementedError,
                          s.process_statistics, None, None)


class TestThreaded(Bcfg2TestCase):
    test_obj = Threaded

    def get_obj(self):
        return self.test_obj()

    def test_start_threads(self):
        s = self.get_obj()
        self.assertRaises(NotImplementedError,
                          s.start_threads)


class TestThreadedStatistics(TestStatistics, TestThreaded):
    test_obj = ThreadedStatistics
    data = [("foo.example.com", "<foo/>"),
            ("bar.example.com", "<bar/>")]

    def get_obj(self, core=None):
        return TestStatistics.get_obj(self, core=core)

    @patch("threading.Thread.start")
    def test_start_threads(self, mock_start):
        ts = self.get_obj()
        ts.start_threads()
        mock_start.assert_any_call()

    @patch("%s.open" % builtins)
    @patch("%s.dump" % cPickle.__name__)
    @patch("Bcfg2.Server.Plugin.interfaces.ThreadedStatistics.run", Mock())
    def test_save(self, mock_dump, mock_open):
        core = Mock()
        ts = self.get_obj(core)
        queue = Mock()
        queue.empty = Mock(side_effect=Empty)
        ts.work_queue = queue

        mock_open.side_effect = IOError
        # test that save does _not_ raise an exception even when
        # everything goes pear-shaped
        ts._save()
        queue.empty.assert_any_call()
        mock_open.assert_called_with(ts.pending_file, 'w')

        queue.reset_mock()
        mock_open.reset_mock()

        queue.data = []
        for hostname, xml in self.data:
            md = Mock()
            md.hostname = hostname
            queue.data.append((md, lxml.etree.XML(xml)))
        queue.empty.side_effect = lambda: len(queue.data) == 0
        queue.get_nowait = Mock(side_effect=lambda: queue.data.pop())
        mock_open.side_effect = None

        ts._save()
        queue.empty.assert_any_call()
        queue.get_nowait.assert_any_call()
        mock_open.assert_called_with(ts.pending_file, 'w')
        mock_open.return_value.close.assert_any_call()
        # the order of the queue data gets changed, so we have to
        # verify this call in an ugly way
        self.assertItemsEqual(mock_dump.call_args[0][0], self.data)
        self.assertEqual(mock_dump.call_args[0][1], mock_open.return_value)

    @patch("os.unlink")
    @patch("os.path.exists")
    @patch("%s.open" % builtins)
    @patch("lxml.etree.XML")
    @patch("%s.load" % cPickle.__name__)
    @patch("Bcfg2.Server.Plugin.interfaces.ThreadedStatistics.run", Mock())
    def test_load(self, mock_load, mock_XML, mock_open, mock_exists,
                  mock_unlink):
        core = Mock()
        core.terminate.isSet.return_value = False
        ts = self.get_obj(core)

        ts.work_queue = Mock()
        ts.work_queue.data = []
        def reset():
            core.reset_mock()
            mock_open.reset_mock()
            mock_exists.reset_mock()
            mock_unlink.reset_mock()
            mock_load.reset_mock()
            mock_XML.reset_mock()
            ts.work_queue.reset_mock()
            ts.work_queue.data = []

        mock_exists.return_value = False
        self.assertTrue(ts._load())
        mock_exists.assert_called_with(ts.pending_file)

        reset()
        mock_exists.return_value = True
        mock_open.side_effect = IOError
        self.assertFalse(ts._load())
        mock_exists.assert_called_with(ts.pending_file)
        mock_open.assert_called_with(ts.pending_file, 'r')

        reset()
        mock_open.side_effect = None
        mock_load.return_value = self.data
        ts.work_queue.put_nowait.side_effect = Full
        self.assertTrue(ts._load())
        mock_exists.assert_called_with(ts.pending_file)
        mock_open.assert_called_with(ts.pending_file, 'r')
        mock_open.return_value.close.assert_any_call()
        mock_load.assert_called_with(mock_open.return_value)

        reset()
        core.build_metadata.side_effect = lambda x: x
        mock_XML.side_effect = lambda x, parser=None: x
        ts.work_queue.put_nowait.side_effect = None
        self.assertTrue(ts._load())
        mock_exists.assert_called_with(ts.pending_file)
        mock_open.assert_called_with(ts.pending_file, 'r')
        mock_open.return_value.close.assert_any_call()
        mock_load.assert_called_with(mock_open.return_value)
        self.assertItemsEqual(mock_XML.call_args_list,
                              [call(x, parser=Bcfg2.Server.XMLParser)
                               for h, x in self.data])
        self.assertItemsEqual(ts.work_queue.put_nowait.call_args_list,
                              [call((h, x)) for h, x in self.data])
        mock_unlink.assert_called_with(ts.pending_file)

    @patch("threading.Thread.start", Mock())
    @patch("Bcfg2.Server.Plugin.interfaces.ThreadedStatistics._load")
    @patch("Bcfg2.Server.Plugin.interfaces.ThreadedStatistics._save")
    @patch("Bcfg2.Server.Plugin.interfaces.ThreadedStatistics.handle_statistic")
    def test_run(self, mock_handle, mock_save, mock_load):
        core = Mock()
        ts = self.get_obj(core)
        mock_load.return_value = True
        ts.work_queue = Mock()

        def reset():
            mock_handle.reset_mock()
            mock_save.reset_mock()
            mock_load.reset_mock()
            core.reset_mock()
            ts.work_queue.reset_mock()
            ts.work_queue.data = self.data[:]
            ts.work_queue.get_calls = 0

        reset()

        def get_rv(**kwargs):
            ts.work_queue.get_calls += 1
            try:
                return ts.work_queue.data.pop()
            except:
                raise Empty
        ts.work_queue.get.side_effect = get_rv
        def terminate_isset():
            # this lets the loop go on a few iterations with an empty
            # queue to test that it doesn't error out
            return ts.work_queue.get_calls > 3
        core.terminate.isSet.side_effect = terminate_isset

        ts.work_queue.empty.return_value = False
        ts.run()
        mock_load.assert_any_call()
        self.assertGreaterEqual(ts.work_queue.get.call_count, len(self.data))
        self.assertItemsEqual(mock_handle.call_args_list,
                              [call(h, x) for h, x in self.data])
        mock_save.assert_any_call()

    @patch("copy.copy", Mock(side_effect=lambda x: x))
    @patch("Bcfg2.Server.Plugin.interfaces.ThreadedStatistics.run", Mock())
    def test_process_statistics(self):
        core = Mock()
        ts = self.get_obj(core)
        ts.work_queue = Mock()
        ts.process_statistics(*self.data[0])
        ts.work_queue.put_nowait.assert_called_with(self.data[0])

        ts.work_queue.reset_mock()
        ts.work_queue.put_nowait.side_effect = Full
        # test that no exception is thrown
        ts.process_statistics(*self.data[0])

    def test_handle_statistic(self):
        ts = self.get_obj()
        self.assertRaises(NotImplementedError,
                          ts.handle_statistic, None, None)


class TestPullSource(Bcfg2TestCase):
    def test_GetCurrentEntry(self):
        ps = PullSource()
        self.assertRaises(NotImplementedError,
                          ps.GetCurrentEntry, None, None, None)


class TestPullTarget(Bcfg2TestCase):
    def test_AcceptChoices(self):
        pt = PullTarget()
        self.assertRaises(NotImplementedError,
                          pt.AcceptChoices, None, None)

    def test_AcceptPullData(self):
        pt = PullTarget()
        self.assertRaises(NotImplementedError,
                          pt.AcceptPullData, None, None, None)


class TestDecision(Bcfg2TestCase):
    test_obj = Decision

    def get_obj(self):
        return self.test_obj()

    def test_GetDecisions(self):
        d = self.get_obj()
        self.assertRaises(NotImplementedError,
                          d.GetDecisions, None, None)


class TestStructureValidator(Bcfg2TestCase):
    test_obj = StructureValidator

    def get_obj(self):
        return self.test_obj()

    def test_validate_structures(self):
        sv = self.get_obj()
        self.assertRaises(NotImplementedError,
                          sv.validate_structures, None, None)


class TestGoalValidator(Bcfg2TestCase):
    test_obj = GoalValidator

    def get_obj(self):
        return self.test_obj()

    def test_validate_goals(self):
        gv = self.get_obj()
        self.assertRaises(NotImplementedError,
                          gv.validate_goals, None, None)


class TestVersion(TestPlugin):
    test_obj = Version

    def test_get_revision(self):
        d = self.get_obj()
        self.assertRaises(NotImplementedError, d.get_revision)


class TestClientRunHooks(Bcfg2TestCase):
    """ placeholder for future tests """
    pass

########NEW FILE########
__FILENAME__ = TestAWSTags
import os
import sys
import lxml.etree
import Bcfg2.Server.Plugin
from mock import Mock, MagicMock, patch
try:
    from Bcfg2.Server.Plugins.AWSTags import *
    HAS_BOTO = True
except ImportError:
    HAS_BOTO = False

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestPlugin import TestPlugin, TestConnector, TestClientRunHooks

config = '''
<AWSTags>
  <Tag name="name-only">
    <Group>group1</Group>
    <Group>group2</Group>
  </Tag>
  <Tag name="name-and-value" value="value">
    <Group>group3</Group>
  </Tag>
  <Tag name="regex-(.*)">
    <Group>group-$1</Group>
  </Tag>
  <Tag name="regex-value" value="(.*)">
    <Group>group-$1</Group>
  </Tag>
</AWSTags>
'''

tags = {
    "empty.example.com": {},
    "no-matches.example.com": {"nameonly": "foo",
                               "Name": "no-matches",
                               "foo": "bar"},
    "foo.example.com": {"name-only": "name-only",
                        "name-and-value": "wrong",
                        "regex-name": "foo"},
    "bar.example.com": {"name-and-value": "value",
                        "regex-value": "bar"}}

groups = {
    "empty.example.com": [],
    "no-matches.example.com": [],
    "foo.example.com": ["group1", "group2", "group-name"],
    "bar.example.com": ["group3", "group-value", "group-bar"]}


def make_instance(name):
    rv = Mock()
    rv.private_dns_name = name
    rv.tags = tags[name]
    return rv


instances = [make_instance(n) for n in tags.keys()]


def get_all_instances(filters=None):
    insts = [i for i in instances
             if i.private_dns_name == filters['private-dns-name']]
    res = Mock()
    res.instances = insts
    return [res]


if HAS_BOTO:
    class TestAWSTags(TestPlugin, TestClientRunHooks, TestConnector):
        test_obj = AWSTags

        def get_obj(self, core=None):
            @patchIf(not isinstance(Bcfg2.Server.Plugins.AWSTags.connect_ec2,
                                    Mock),
                     "Bcfg2.Server.Plugins.AWSTags.connect_ec2", Mock())
            @patch("lxml.etree.Element", Mock())
            def inner():
                obj = TestPlugin.get_obj(self, core=core)
                obj.config.data = config
                obj.config.Index()
                return obj
            return inner()

        @patch("Bcfg2.Server.Plugins.AWSTags.connect_ec2")
        def test_connect(self, mock_connect_ec2):
            """ Test connection to EC2 """
            key_id = "a09sdbipasdf"
            access_key = "oiilb234ipwe9"

            def cfp_get(section, option):
                if option == "access_key_id":
                    return key_id
                elif option == "secret_access_key":
                    return access_key
                else:
                    return Mock()

            core = Mock()
            core.setup.cfp.get = Mock(side_effect=cfp_get)
            awstags = self.get_obj(core=core)
            mock_connect_ec2.assert_called_with(
                aws_access_key_id=key_id,
                aws_secret_access_key=access_key)

        def test_get_additional_data(self):
            """ Test AWSTags.get_additional_data() """
            awstags = self.get_obj()
            awstags._ec2.get_all_instances = \
                Mock(side_effect=get_all_instances)

            for hostname, expected in tags.items():
                metadata = Mock()
                metadata.hostname = hostname
                self.assertItemsEqual(awstags.get_additional_data(metadata),
                                      expected)

        def test_get_additional_groups_caching(self):
            """ Test AWSTags.get_additional_groups() with caching enabled """
            awstags = self.get_obj()
            awstags._ec2.get_all_instances = \
                Mock(side_effect=get_all_instances)

            for hostname, expected in groups.items():
                metadata = Mock()
                metadata.hostname = hostname
                actual = awstags.get_additional_groups(metadata)
                msg = """%s has incorrect groups:
actual:   %s
expected: %s""" % (hostname, actual, expected)
                self.assertItemsEqual(actual, expected, msg)

########NEW FILE########
__FILENAME__ = TestCfgAuthorizedKeysGenerator
import os
import sys
import lxml.etree
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugins.Cfg.CfgAuthorizedKeysGenerator import *
import Bcfg2.Server.Plugins.Cfg.CfgAuthorizedKeysGenerator

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestServer.TestPlugins.TestCfg.Test_init import TestCfgGenerator
from TestServer.TestPlugin.Testhelpers import TestStructFile


class TestCfgAuthorizedKeysGenerator(TestCfgGenerator, TestStructFile):
    test_obj = CfgAuthorizedKeysGenerator
    should_monitor = False

    def get_obj(self, name=None, core=None, fam=None):
        if name is None:
            name = self.path
        Bcfg2.Server.Plugins.Cfg.CfgAuthorizedKeysGenerator.CFG = Mock()
        if core is not None:
            Bcfg2.Server.Plugins.Cfg.CfgAuthorizedKeysGenerator.CFG.core = core
        return self.test_obj(name)

    @patch("Bcfg2.Server.Plugins.Cfg.CfgGenerator.handle_event")
    @patch("Bcfg2.Server.Plugin.helpers.StructFile.HandleEvent")
    def test_handle_event(self, mock_HandleEvent, mock_handle_event):
        akg = self.get_obj()
        evt = Mock()
        akg.handle_event(evt)
        mock_HandleEvent.assert_called_with(akg, evt)
        mock_handle_event.assert_called_with(akg, evt)

    def test_category(self):
        akg = self.get_obj()
        cfp = Mock()
        cfp.has_section.return_value = False
        cfp.has_option.return_value = False
        Bcfg2.Server.Plugins.Cfg.CfgAuthorizedKeysGenerator.SETUP = Mock()
        Bcfg2.Server.Plugins.Cfg.CfgAuthorizedKeysGenerator.SETUP.cfp = cfp

        self.assertIsNone(akg.category)
        cfp.has_section.assert_called_with("sshkeys")

        cfp.reset_mock()
        cfp.has_section.return_value = True
        self.assertIsNone(akg.category)
        cfp.has_section.assert_called_with("sshkeys")
        cfp.has_option.assert_called_with("sshkeys", "category")

        cfp.reset_mock()
        cfp.has_option.return_value = True
        self.assertEqual(akg.category, cfp.get.return_value)
        cfp.has_section.assert_called_with("sshkeys")
        cfp.has_option.assert_called_with("sshkeys", "category")
        cfp.get.assert_called_with("sshkeys", "category")

    @patch("Bcfg2.Server.Plugins.Cfg.CfgAuthorizedKeysGenerator.ClientMetadata")
    @patch("Bcfg2.Server.Plugins.Cfg.CfgAuthorizedKeysGenerator.CfgAuthorizedKeysGenerator.category", "category")
    def test_get_data(self, mock_ClientMetadata):
        akg = self.get_obj()
        akg.XMLMatch = Mock()

        def ClientMetadata(host, profile, groups, *args):
            rv = Mock()
            rv.hostname = host
            rv.profile = profile
            rv.groups = groups
            return rv

        mock_ClientMetadata.side_effect = ClientMetadata

        def build_metadata(host):
            rv = Mock()
            rv.hostname = host
            rv.profile = host
            return rv

        akg.core.build_metadata = Mock()
        akg.core.build_metadata.side_effect = build_metadata

        def Bind(ent, md):
            ent.text = "%s %s" % (md.profile, ent.get("name"))
            return ent

        akg.core.Bind = Mock()
        akg.core.Bind.side_effect = Bind
        metadata = Mock()
        metadata.profile = "profile"
        metadata.group_in_category.return_value = "profile"
        entry = lxml.etree.Element("Path", name="/root/.ssh/authorized_keys")

        def reset():
            mock_ClientMetadata.reset_mock()
            akg.XMLMatch.reset_mock()
            akg.core.build_metadata.reset_mock()
            akg.core.Bind.reset_mock()
            metadata.reset_mock()

        pubkey = "/home/foo/.ssh/id_rsa.pub"
        spec = lxml.etree.Element("AuthorizedKeys")
        lxml.etree.SubElement(spec, "Allow", attrib={"from": pubkey})
        akg.XMLMatch.return_value = spec
        self.assertEqual(akg.get_data(entry, metadata), "profile %s" % pubkey)
        akg.XMLMatch.assert_called_with(metadata)
        self.assertEqual(akg.core.Bind.call_args[0][0].get("name"), pubkey)
        self.assertEqual(akg.core.Bind.call_args[0][1], metadata)

        reset()
        group = "somegroup"
        spec = lxml.etree.Element("AuthorizedKeys")
        lxml.etree.SubElement(spec, "Allow",
                              attrib={"from": pubkey, "group": group})
        akg.XMLMatch.return_value = spec
        self.assertEqual(akg.get_data(entry, metadata),
                         "%s %s" % (group, pubkey))
        akg.XMLMatch.assert_called_with(metadata)
        self.assertItemsEqual(mock_ClientMetadata.call_args[0][2], [group])
        self.assertEqual(akg.core.Bind.call_args[0][0].get("name"), pubkey)
        self.assertIn(group, akg.core.Bind.call_args[0][1].groups)

        reset()
        host = "baz.example.com"
        spec = lxml.etree.Element("AuthorizedKeys")
        lxml.etree.SubElement(
            lxml.etree.SubElement(spec,
                                  "Allow",
                                  attrib={"from": pubkey, "host": host}),
            "Params", foo="foo", bar="bar=bar")
        akg.XMLMatch.return_value = spec
        params, actual_host, actual_pubkey = akg.get_data(entry,
                                                          metadata).split()
        self.assertEqual(actual_host, host)
        self.assertEqual(actual_pubkey, pubkey)
        self.assertItemsEqual(params.split(","), ["foo=foo", "bar=bar=bar"])
        akg.XMLMatch.assert_called_with(metadata)
        akg.core.build_metadata.assert_called_with(host)
        self.assertEqual(akg.core.Bind.call_args[0][0].get("name"), pubkey)
        self.assertEqual(akg.core.Bind.call_args[0][1].hostname, host)

        reset()
        spec = lxml.etree.Element("AuthorizedKeys")
        text = lxml.etree.SubElement(spec, "Allow")
        text.text = "ssh-rsa publickey /foo/bar\n"
        lxml.etree.SubElement(text, "Params", foo="foo")
        akg.XMLMatch.return_value = spec
        self.assertEqual(akg.get_data(entry, metadata),
                         "foo=foo %s" % text.text.strip())
        akg.XMLMatch.assert_called_with(metadata)
        self.assertFalse(akg.core.build_metadata.called)
        self.assertFalse(akg.core.Bind.called)

        reset()
        lxml.etree.SubElement(spec, "Allow", attrib={"from": pubkey})
        akg.XMLMatch.return_value = spec
        self.assertItemsEqual(akg.get_data(entry, metadata).splitlines(),
                              ["foo=foo %s" % text.text.strip(),
                               "profile %s" % pubkey])
        akg.XMLMatch.assert_called_with(metadata)

        reset()
        metadata.group_in_category.return_value = ''
        spec = lxml.etree.Element("AuthorizedKeys")
        lxml.etree.SubElement(spec, "Allow", attrib={"from": pubkey})
        akg.XMLMatch.return_value = spec
        self.assertEqual(akg.get_data(entry, metadata), '')
        akg.XMLMatch.assert_called_with(metadata)
        self.assertFalse(akg.core.build_metadata.called)
        self.assertFalse(akg.core.Bind.called)
        self.assertFalse(mock_ClientMetadata.called)

########NEW FILE########
__FILENAME__ = TestCfgCheetahGenerator
import os
import sys
import lxml.etree
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugins.Cfg.CfgCheetahGenerator import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestServer.TestPlugins.TestCfg.Test_init import TestCfgGenerator


if HAS_CHEETAH or can_skip:
    class TestCfgCheetahGenerator(TestCfgGenerator):
        test_obj = CfgCheetahGenerator

        @skipUnless(HAS_CHEETAH, "Cheetah libraries not found, skipping")
        def setUp(self):
            pass

        @patch("Bcfg2.Server.Plugins.Cfg.CfgCheetahGenerator.Template")
        def test_get_data(self, mock_Template):
            ccg = self.get_obj(encoding='UTF-8')
            ccg.data = "data"
            entry = lxml.etree.Element("Path", name="/test.txt")
            metadata = Mock()
            Bcfg2.Server.Plugins.Cfg.CfgCheetahGenerator.SETUP = MagicMock()

            self.assertEqual(ccg.get_data(entry, metadata),
                             mock_Template.return_value.respond.return_value)
            Bcfg2.Server.Plugins.Cfg.CfgCheetahGenerator.SETUP.__getitem__.assert_called_with("repo")
            mock_Template.assert_called_with("data".decode(ccg.encoding),
                                             compilerSettings=ccg.settings)
            tmpl = mock_Template.return_value
            tmpl.respond.assert_called_with()
            self.assertEqual(tmpl.metadata, metadata)
            self.assertEqual(tmpl.name, entry.get("name"))
            self.assertEqual(tmpl.path, entry.get("name"))
            self.assertEqual(tmpl.source_path, ccg.name)
            self.assertEqual(tmpl.repo,
                             Bcfg2.Server.Plugins.Cfg.CfgCheetahGenerator.SETUP.__getitem__.return_value)

########NEW FILE########
__FILENAME__ = TestCfgEncryptedCheetahGenerator
import os
import sys
from Bcfg2.Server.Plugins.Cfg.CfgEncryptedCheetahGenerator import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *

try:
    from TestServer.TestPlugins.TestCfg.TestCfgCheetahGenerator import \
        TestCfgCheetahGenerator
    from Bcfg2.Server.Plugins.Cfg.CfgCheetahGenerator import HAS_CHEETAH
except ImportError:
    TestCfgCheetahGenerator = object
    HAS_CHEETAH = False

try:
    from TestServer.TestPlugins.TestCfg.TestCfgEncryptedGenerator import \
        TestCfgEncryptedGenerator
    from Bcfg2.Server.Plugins.Cfg.CfgEncryptedGenerator import HAS_CRYPTO
except ImportError:
    TestCfgEncryptedGenerator = object
    HAS_CRYPTO = False


if can_skip or (HAS_CRYPTO and HAS_CHEETAH):
    class TestCfgEncryptedCheetahGenerator(TestCfgCheetahGenerator,
                                           TestCfgEncryptedGenerator):
        test_obj = CfgEncryptedCheetahGenerator

        @skipUnless(HAS_CRYPTO, "Encryption libraries not found, skipping")
        @skipUnless(HAS_CHEETAH, "Cheetah libraries not found, skipping")
        def setUp(self):
            pass

        def test_handle_event(self):
            TestCfgEncryptedGenerator.test_handle_event(self)

        def test_get_data(self):
            TestCfgCheetahGenerator.test_get_data(self)

########NEW FILE########
__FILENAME__ = TestCfgEncryptedGenerator
import os
import sys
import lxml.etree
import Bcfg2.Server.Plugins.Cfg
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugins.Cfg.CfgEncryptedGenerator import *
from Bcfg2.Server.Plugin import PluginExecutionError

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestServer.TestPlugins.TestCfg.Test_init import TestCfgGenerator


if can_skip or HAS_CRYPTO:
    class TestCfgEncryptedGenerator(TestCfgGenerator):
        test_obj = CfgEncryptedGenerator

        @skipUnless(HAS_CRYPTO, "Encryption libraries not found, skipping")
        def setUp(self):
            pass

        @patchIf(HAS_CRYPTO,
                 "Bcfg2.Server.Plugins.Cfg.CfgEncryptedGenerator.get_algorithm")
        @patchIf(HAS_CRYPTO,
                 "Bcfg2.Server.Plugins.Cfg.CfgEncryptedGenerator.bruteforce_decrypt")
        @patch("Bcfg2.Server.Plugins.Cfg.CfgEncryptedGenerator.SETUP")
        def test_handle_event(self, mock_SETUP, mock_decrypt,
                              mock_get_algorithm):
            @patch("Bcfg2.Server.Plugins.Cfg.CfgGenerator.handle_event")
            def inner(mock_handle_event):
                def reset():
                    mock_decrypt.reset_mock()
                    mock_get_algorithm.reset_mock()
                    mock_handle_event.reset_mock()

                def get_event_data(obj, event):
                    obj.data = "encrypted"

                mock_handle_event.side_effect = get_event_data
                mock_decrypt.side_effect = lambda d, **kw: "plaintext"
                event = Mock()
                ceg = self.get_obj()
                ceg.handle_event(event)
                mock_handle_event.assert_called_with(ceg, event)
                mock_decrypt.assert_called_with(
                    "encrypted",
                    setup=mock_SETUP,
                    algorithm=mock_get_algorithm.return_value)
                self.assertEqual(ceg.data, "plaintext")

                reset()
                mock_decrypt.side_effect = EVPError
                self.assertRaises(PluginExecutionError,
                                  ceg.handle_event, event)
            inner()

            # to perform the tests from the parent test object, we
            # make bruteforce_decrypt just return whatever data was
            # given to it
            mock_decrypt.side_effect = lambda d, **kw: d
            TestCfgGenerator.test_handle_event(self)

        def test_get_data(self):
            ceg = self.get_obj()
            ceg.data = None
            entry = lxml.etree.Element("Path", name="/test.txt")
            metadata = Mock()

            self.assertRaises(PluginExecutionError,
                              ceg.get_data, entry, metadata)

            TestCfgGenerator.test_get_data(self)

########NEW FILE########
__FILENAME__ = TestCfgEncryptedGenshiGenerator
import os
import sys
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugins.Cfg.CfgEncryptedGenshiGenerator import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *

try:
    from TestServer.TestPlugins.TestCfg.TestCfgGenshiGenerator import \
        TestCfgGenshiGenerator
    HAS_GENSHI = True
except ImportError:
    TestCfgGenshiGenerator = object
    HAS_GENSHI = False


if can_skip or (HAS_CRYPTO and HAS_GENSHI):
    class TestCfgEncryptedGenshiGenerator(TestCfgGenshiGenerator):
        test_obj = CfgEncryptedGenshiGenerator

        @skipUnless(HAS_CRYPTO, "Encryption libraries not found, skipping")
        @skipUnless(HAS_GENSHI, "Genshi libraries not found, skipping")
        def setUp(self):
            pass

########NEW FILE########
__FILENAME__ = TestCfgExternalCommandVerifier
import os
import sys
import lxml.etree
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugins.Cfg.CfgExternalCommandVerifier import *
from Bcfg2.Server.Plugin import PluginExecutionError

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestServer.TestPlugins.TestCfg.Test_init import TestCfgVerifier


class TestCfgExternalCommandVerifier(TestCfgVerifier):
    test_obj = CfgExternalCommandVerifier

    @patch("Bcfg2.Server.Plugins.Cfg.CfgExternalCommandVerifier.Popen")
    def test_verify_entry(self, mock_Popen):
        proc = Mock()
        mock_Popen.return_value = proc
        proc.wait.return_value = 0
        proc.communicate.return_value = ("stdout", "stderr")
        entry = lxml.etree.Element("Path", name="/test.txt")
        metadata = Mock()

        ecv = self.get_obj()
        ecv.cmd = ["/bin/bash", "-x", "foo"]
        ecv.verify_entry(entry, metadata, "data")
        self.assertEqual(mock_Popen.call_args[0], (ecv.cmd,))
        proc.communicate.assert_called_with(input="data")
        proc.wait.assert_called_with()

        mock_Popen.reset_mock()
        proc.wait.return_value = 13
        self.assertRaises(CfgVerificationError,
                          ecv.verify_entry, entry, metadata, "data")
        self.assertEqual(mock_Popen.call_args[0], (ecv.cmd,))
        proc.communicate.assert_called_with(input="data")
        proc.wait.assert_called_with()

        mock_Popen.reset_mock()
        mock_Popen.side_effect = OSError
        self.assertRaises(CfgVerificationError,
                          ecv.verify_entry, entry, metadata, "data")
        self.assertEqual(mock_Popen.call_args[0], (ecv.cmd,))

    @patch("os.access")
    def test_handle_event(self, mock_access):
        @patch("Bcfg2.Server.Plugins.Cfg.CfgVerifier.handle_event")
        def inner(mock_handle_event):
            ecv = self.get_obj()
            event = Mock()
            mock_access.return_value = False
            ecv.data = "data"
            self.assertRaises(PluginExecutionError,
                              ecv.handle_event, event)
            mock_handle_event.assert_called_with(ecv, event)
            mock_access.assert_called_with(ecv.name, os.X_OK)
            self.assertItemsEqual(ecv.cmd, [])

            mock_access.reset_mock()
            mock_handle_event.reset_mock()
            ecv.data = "#! /bin/bash -x\ntrue"
            ecv.handle_event(event)
            mock_handle_event.assert_called_with(ecv, event)
            mock_access.assert_called_with(ecv.name, os.X_OK)
            self.assertEqual(ecv.cmd, ["/bin/bash", "-x", ecv.name])

            mock_access.reset_mock()
            mock_handle_event.reset_mock()
            mock_access.return_value = True
            ecv.data = "true"
            ecv.handle_event(event)
            mock_handle_event.assert_called_with(ecv, event)
            mock_access.assert_called_with(ecv.name, os.X_OK)
            self.assertItemsEqual(ecv.cmd, [ecv.name])

        inner()
        mock_access.return_value = True
        TestCfgVerifier.test_handle_event(self)

########NEW FILE########
__FILENAME__ = TestCfgGenshiGenerator
import os
import sys
import lxml.etree
from mock import Mock, MagicMock, patch
import Bcfg2.Server.Plugins.Cfg.CfgGenshiGenerator
from Bcfg2.Server.Plugins.Cfg.CfgGenshiGenerator import *
from Bcfg2.Server.Plugin import PluginExecutionError

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestServer.TestPlugins.TestCfg.Test_init import TestCfgGenerator


if can_skip or HAS_GENSHI:
    class TestCfgGenshiGenerator(TestCfgGenerator):
        test_obj = CfgGenshiGenerator

        @skipUnless(HAS_GENSHI, "Genshi libraries not found, skipping")
        def setUp(self):
            pass

        def test_removecomment(self):
            data = [(None, "test", 1),
                    (None, "test2", 2)]
            stream = [(genshi.core.COMMENT, "test", 0),
                      data[0],
                      (genshi.core.COMMENT, "test3", 0),
                      data[1]]
            self.assertItemsEqual(list(removecomment(stream)), data)

        def test__init(self):
            TestCfgGenerator.test__init(self)
            cgg = self.get_obj()
            self.assertIsInstance(cgg.loader, cgg.__loader_cls__)

        def test_get_data(self):
            cgg = self.get_obj()
            cgg._handle_genshi_exception = Mock()
            cgg.template = Mock()
            fltr = Mock()
            cgg.template.generate.return_value = fltr
            stream = Mock()
            fltr.filter.return_value = stream
            entry = lxml.etree.Element("Path", name="/test.txt")
            metadata = Mock()

            Bcfg2.Server.Plugins.Cfg.CfgGenshiGenerator.SETUP = MagicMock()

            def reset():
                cgg.template.reset_mock()
                cgg._handle_genshi_exception.reset_mock()
                Bcfg2.Server.Plugins.Cfg.CfgGenshiGenerator.SETUP.reset_mock()

            template_vars = dict(
                name=entry.get("name"),
                metadata=metadata,
                path=cgg.name,
                source_path=cgg.name,
                repo=Bcfg2.Server.Plugins.Cfg.CfgGenshiGenerator.SETUP.__getitem__.return_value)

            self.assertEqual(cgg.get_data(entry, metadata),
                             stream.render.return_value)
            cgg.template.generate.assert_called_with(**template_vars)
            Bcfg2.Server.Plugins.Cfg.CfgGenshiGenerator.SETUP.__getitem__.assert_called_with("repo")
            fltr.filter.assert_called_with(removecomment)
            stream.render.assert_called_with("text", encoding=cgg.encoding,
                                             strip_whitespace=False)

            reset()
            def render(fmt, **kwargs):
                stream.render.side_effect = None
                raise TypeError
            stream.render.side_effect = render
            self.assertEqual(cgg.get_data(entry, metadata),
                             stream.render.return_value)
            cgg.template.generate.assert_called_with(**template_vars)
            Bcfg2.Server.Plugins.Cfg.CfgGenshiGenerator.SETUP.__getitem__.assert_called_with("repo")
            fltr.filter.assert_called_with(removecomment)
            self.assertEqual(stream.render.call_args_list,
                             [call("text", encoding=cgg.encoding,
                                  strip_whitespace=False),
                              call("text", encoding=cgg.encoding)])

            reset()
            stream.render.side_effect = UndefinedError("test")
            self.assertRaises(UndefinedError,
                              cgg.get_data, entry, metadata)
            cgg.template.generate.assert_called_with(**template_vars)
            Bcfg2.Server.Plugins.Cfg.CfgGenshiGenerator.SETUP.__getitem__.assert_called_with("repo")
            fltr.filter.assert_called_with(removecomment)
            stream.render.assert_called_with("text", encoding=cgg.encoding,
                                             strip_whitespace=False)

            reset()
            stream.render.side_effect = ValueError
            cgg._handle_genshi_exception.side_effect = ValueError
            self.assertRaises(ValueError,
                              cgg.get_data, entry, metadata)
            cgg.template.generate.assert_called_with(**template_vars)
            Bcfg2.Server.Plugins.Cfg.CfgGenshiGenerator.SETUP.__getitem__.assert_called_with("repo")
            fltr.filter.assert_called_with(removecomment)
            stream.render.assert_called_with("text", encoding=cgg.encoding,
                                             strip_whitespace=False)
            self.assertTrue(cgg._handle_genshi_exception.called)

        def test_handle_event(self):
            cgg = self.get_obj()
            cgg.loader = Mock()
            event = Mock()
            cgg.handle_event(event)
            cgg.loader.load.assert_called_with(cgg.name,
                                               cls=NewTextTemplate,
                                               encoding=cgg.encoding)

            cgg.loader.reset_mock()
            cgg.loader.load.side_effect = OSError
            self.assertRaises(PluginExecutionError,
                              cgg.handle_event, event)
            cgg.loader.load.assert_called_with(cgg.name,
                                               cls=NewTextTemplate,
                                               encoding=cgg.encoding)

########NEW FILE########
__FILENAME__ = TestCfgInfoXML
import os
import sys
import lxml.etree
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugins.Cfg.CfgInfoXML import *
from Bcfg2.Server.Plugin import InfoXML, PluginExecutionError

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestServer.TestPlugins.TestCfg.Test_init import TestCfgInfo


class TestCfgInfoXML(TestCfgInfo):
    test_obj = CfgInfoXML

    def test__init(self):
        TestCfgInfo.test__init(self)
        ci = self.get_obj()
        self.assertIsInstance(ci.infoxml, InfoXML)

    def test_bind_info_to_entry(self):
        entry = lxml.etree.Element("Path", name="/test.txt")
        metadata = Mock()
        ci = self.get_obj()
        ci.infoxml = Mock()
        ci._set_info = Mock()

        self.assertRaises(PluginExecutionError,
                          ci.bind_info_to_entry, entry, metadata)
        ci.infoxml.pnode.Match.assert_called_with(metadata, dict(),
                                                  entry=entry)
        self.assertFalse(ci._set_info.called)

        ci.infoxml.reset_mock()
        ci._set_info.reset_mock()
        mdata_value = Mock()
        def set_mdata(metadata, mdata, entry=None):
            mdata['Info'] = {None: mdata_value}

        ci.infoxml.pnode.Match.side_effect = set_mdata
        ci.bind_info_to_entry(entry, metadata)
        ci.infoxml.pnode.Match.assert_called_with(metadata,
                                                  dict(Info={None: mdata_value}),
                                                  entry=entry)
        ci._set_info.assert_called_with(entry, mdata_value)

    def test_handle_event(self):
        ci = self.get_obj()
        ci.infoxml = Mock()
        ci.handle_event(Mock)
        ci.infoxml.HandleEvent.assert_called_with()

    def test__set_info(self):
        @patch("Bcfg2.Server.Plugins.Cfg.CfgInfo._set_info")
        def inner(mock_set_info):
            ci = self.get_obj()
            entry = Mock()
            info = {"foo": "foo",
                    "__children__": ["one", "two"]}
            ci._set_info(entry, info)
            self.assertItemsEqual(entry.append.call_args_list,
                                  [call(c) for c in info['__children__']])

        inner()
        TestCfgInfo.test__set_info(self)

########NEW FILE########
__FILENAME__ = TestCfgPlaintextGenerator
import os
import sys
from Bcfg2.Server.Plugins.Cfg.CfgPlaintextGenerator import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from TestServer.TestPlugins.TestCfg.Test_init import TestCfgGenerator


class TestCfgPlaintextGenerator(TestCfgGenerator):
    test_obj = CfgPlaintextGenerator

########NEW FILE########
__FILENAME__ = TestCfgPrivateKeyCreator
import os
import sys
import lxml.etree
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugins.Cfg import CfgCreationError
from Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator import *
from Bcfg2.Server.Plugin import PluginExecutionError
import Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator
try:
    from Bcfg2.Encryption import EVPError
    HAS_CRYPTO = True
except:
    HAS_CRYPTO = False

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestServer.TestPlugins.TestCfg.Test_init import TestCfgCreator
from TestServer.TestPlugin.Testhelpers import TestStructFile


class TestCfgPrivateKeyCreator(TestCfgCreator, TestStructFile):
    test_obj = CfgPrivateKeyCreator
    should_monitor = False

    def get_obj(self, name=None, fam=None):
        Bcfg2.Server.Plugins.Cfg.CfgPublicKeyCreator.CFG = Mock()
        return TestCfgCreator.get_obj(self, name=name)

    @patch("Bcfg2.Server.Plugins.Cfg.CfgCreator.handle_event")
    @patch("Bcfg2.Server.Plugin.helpers.StructFile.HandleEvent")
    def test_handle_event(self, mock_HandleEvent, mock_handle_event):
        pkc = self.get_obj()
        evt = Mock()
        pkc.handle_event(evt)
        mock_HandleEvent.assert_called_with(pkc, evt)
        mock_handle_event.assert_called_with(pkc, evt)

    def test_category(self):
        pkc = self.get_obj()
        cfp = Mock()
        cfp.has_section.return_value = False
        cfp.has_option.return_value = False
        Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.SETUP = Mock()
        Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.SETUP.cfp = cfp

        self.assertIsNone(pkc.category)
        cfp.has_section.assert_called_with("sshkeys")

        cfp.reset_mock()
        cfp.has_section.return_value = True
        self.assertIsNone(pkc.category)
        cfp.has_section.assert_called_with("sshkeys")
        cfp.has_option.assert_called_with("sshkeys", "category")

        cfp.reset_mock()
        cfp.has_option.return_value = True
        self.assertEqual(pkc.category, cfp.get.return_value)
        cfp.has_section.assert_called_with("sshkeys")
        cfp.has_option.assert_called_with("sshkeys", "category")
        cfp.get.assert_called_with("sshkeys", "category")

    @skipUnless(HAS_CRYPTO, "No crypto libraries found, skipping")
    @patchIf(HAS_CRYPTO, "Bcfg2.Encryption.get_passphrases")
    def test_passphrase(self, mock_get_passphrases):
        pkc = self.get_obj()
        cfp = Mock()
        cfp.has_section.return_value = False
        cfp.has_option.return_value = False
        Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.SETUP = Mock()
        Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.SETUP.cfp = cfp

        self.assertIsNone(pkc.passphrase)
        cfp.has_section.assert_called_with("sshkeys")

        cfp.reset_mock()
        cfp.has_section.return_value = True
        self.assertIsNone(pkc.passphrase)
        cfp.has_section.assert_called_with("sshkeys")
        cfp.has_option.assert_called_with("sshkeys", "passphrase")

        cfp.reset_mock()
        cfp.get.return_value = "test"
        mock_get_passphrases.return_value = dict(test="foo", test2="bar")
        cfp.has_option.return_value = True
        self.assertEqual(pkc.passphrase, "foo")
        cfp.has_section.assert_called_with("sshkeys")
        cfp.has_option.assert_called_with("sshkeys", "passphrase")
        cfp.get.assert_called_with("sshkeys", "passphrase")
        mock_get_passphrases.assert_called_with(Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.SETUP)

    @patch("shutil.rmtree")
    @patch("tempfile.mkdtemp")
    @patch("subprocess.Popen")
    def test__gen_keypair(self, mock_Popen, mock_mkdtemp, mock_rmtree):
        pkc = self.get_obj()
        pkc.XMLMatch = Mock()
        mock_mkdtemp.return_value = datastore
        metadata = Mock()

        proc = Mock()
        proc.wait.return_value = 0
        proc.communicate.return_value = MagicMock()
        mock_Popen.return_value = proc

        spec = lxml.etree.Element("PrivateKey")
        pkc.XMLMatch.return_value = spec

        def reset():
            pkc.XMLMatch.reset_mock()
            mock_Popen.reset_mock()
            mock_mkdtemp.reset_mock()
            mock_rmtree.reset_mock()

        self.assertEqual(pkc._gen_keypair(metadata),
                         os.path.join(datastore, "privkey"))
        pkc.XMLMatch.assert_called_with(metadata)
        mock_mkdtemp.assert_called_with()
        self.assertItemsEqual(mock_Popen.call_args[0][0],
                              ["ssh-keygen", "-f",
                               os.path.join(datastore, "privkey"),
                               "-t", "rsa", "-N", ""])

        reset()
        lxml.etree.SubElement(spec, "Params", bits="768", type="dsa")
        passphrase = lxml.etree.SubElement(spec, "Passphrase")
        passphrase.text = "foo"

        self.assertEqual(pkc._gen_keypair(metadata),
                         os.path.join(datastore, "privkey"))
        pkc.XMLMatch.assert_called_with(metadata)
        mock_mkdtemp.assert_called_with()
        self.assertItemsEqual(mock_Popen.call_args[0][0],
                              ["ssh-keygen", "-f",
                               os.path.join(datastore, "privkey"),
                               "-t", "dsa", "-b", "768", "-N", "foo"])

        reset()
        proc.wait.return_value = 1
        self.assertRaises(CfgCreationError, pkc._gen_keypair, metadata)
        mock_rmtree.assert_called_with(datastore)

    def test_get_specificity(self):
        pkc = self.get_obj()
        pkc.XMLMatch = Mock()

        metadata = Mock()

        def reset():
            pkc.XMLMatch.reset_mock()
            metadata.group_in_category.reset_mock()

        category = "Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.CfgPrivateKeyCreator.category"
        @patch(category, None)
        def inner():
            pkc.XMLMatch.return_value = lxml.etree.Element("PrivateKey")
            self.assertItemsEqual(pkc.get_specificity(metadata),
                                  dict(host=metadata.hostname))
        inner()

        @patch(category, "foo")
        def inner2():
            pkc.XMLMatch.return_value = lxml.etree.Element("PrivateKey")
            self.assertItemsEqual(pkc.get_specificity(metadata),
                                  dict(group=metadata.group_in_category.return_value,
                                       prio=50))
            metadata.group_in_category.assert_called_with("foo")

            reset()
            pkc.XMLMatch.return_value = lxml.etree.Element("PrivateKey",
                                                           perhost="true")
            self.assertItemsEqual(pkc.get_specificity(metadata),
                                  dict(host=metadata.hostname))

            reset()
            pkc.XMLMatch.return_value = lxml.etree.Element("PrivateKey",
                                                           category="bar")
            self.assertItemsEqual(pkc.get_specificity(metadata),
                                  dict(group=metadata.group_in_category.return_value,
                                       prio=50))
            metadata.group_in_category.assert_called_with("bar")

            reset()
            pkc.XMLMatch.return_value = lxml.etree.Element("PrivateKey",
                                                           prio="10")
            self.assertItemsEqual(pkc.get_specificity(metadata),
                                  dict(group=metadata.group_in_category.return_value,
                                       prio=10))
            metadata.group_in_category.assert_called_with("foo")

            reset()
            pkc.XMLMatch.return_value = lxml.etree.Element("PrivateKey")
            metadata.group_in_category.return_value = ''
            self.assertItemsEqual(pkc.get_specificity(metadata),
                                  dict(host=metadata.hostname))
            metadata.group_in_category.assert_called_with("foo")

        inner2()

    @patch("shutil.rmtree")
    @patch("%s.open" % builtins)
    def test_create_data(self, mock_open, mock_rmtree):
        pkc = self.get_obj()
        pkc.XMLMatch = Mock()
        pkc.get_specificity = Mock()
        # in order to make ** magic work in older versions of python,
        # get_specificity() must return an actual dict, not just a
        # Mock object that works like a dict.  in order to test that
        # the get_specificity() return value is being used
        # appropriately, we put some dummy data in it and test for
        # that data
        pkc.get_specificity.side_effect = lambda m, s: dict(group="foo")
        pkc._gen_keypair = Mock()
        privkey = os.path.join(datastore, "privkey")
        pkc._gen_keypair.return_value = privkey
        pkc.pubkey_creator = Mock()
        pkc.pubkey_creator.get_filename.return_value = "pubkey.filename"
        pkc.write_data = Mock()

        entry = lxml.etree.Element("Path", name="/home/foo/.ssh/id_rsa")
        metadata = Mock()

        def open_read_rv():
            mock_open.return_value.read.side_effect = lambda: "privatekey"
            return "ssh-rsa publickey foo@bar.com"

        def reset():
            mock_open.reset_mock()
            mock_rmtree.reset_mock()
            pkc.XMLMatch.reset_mock()
            pkc.get_specificity.reset_mock()
            pkc._gen_keypair.reset_mock()
            pkc.pubkey_creator.reset_mock()
            pkc.write_data.reset_mock()
            mock_open.return_value.read.side_effect = open_read_rv

        reset()
        passphrase = "Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.CfgPrivateKeyCreator.passphrase"

        @patch(passphrase, None)
        def inner():
            self.assertEqual(pkc.create_data(entry, metadata), "privatekey")
            pkc.XMLMatch.assert_called_with(metadata)
            pkc.get_specificity.assert_called_with(metadata,
                                                   pkc.XMLMatch.return_value)
            pkc._gen_keypair.assert_called_with(metadata,
                                                pkc.XMLMatch.return_value)
            self.assertItemsEqual(mock_open.call_args_list,
                                  [call(privkey + ".pub"), call(privkey)])
            pkc.pubkey_creator.get_filename.assert_called_with(group="foo")
            pkc.pubkey_creator.write_data.assert_called_with(
                "ssh-rsa publickey pubkey.filename\n", group="foo")
            pkc.write_data.assert_called_with("privatekey", group="foo")
            mock_rmtree.assert_called_with(datastore)

        inner()

        if HAS_CRYPTO:
            @patch(passphrase, "foo")
            @patch("Bcfg2.Encryption.ssl_encrypt")
            @patch("Bcfg2.Encryption.get_algorithm")
            def inner2(mock_get_algorithm, mock_ssl_encrypt):
                reset()
                mock_ssl_encrypt.return_value = "encryptedprivatekey"
                Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.HAS_CRYPTO = True
                self.assertEqual(pkc.create_data(entry, metadata),
                                 "encryptedprivatekey")
                pkc.XMLMatch.assert_called_with(metadata)
                pkc.get_specificity.assert_called_with(
                    metadata,
                    pkc.XMLMatch.return_value)
                pkc._gen_keypair.assert_called_with(metadata,
                                                    pkc.XMLMatch.return_value)
                self.assertItemsEqual(mock_open.call_args_list,
                                      [call(privkey + ".pub"), call(privkey)])
                pkc.pubkey_creator.get_filename.assert_called_with(group="foo")
                pkc.pubkey_creator.write_data.assert_called_with(
                    "ssh-rsa publickey pubkey.filename\n", group="foo")
                pkc.write_data.assert_called_with("encryptedprivatekey",
                                                  group="foo", ext=".crypt")
                mock_ssl_encrypt.assert_called_with(
                    "privatekey", "foo",
                    algorithm=mock_get_algorithm.return_value)
                mock_rmtree.assert_called_with(datastore)

            inner2()

    def test_Index(self):
        has_crypto = Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.HAS_CRYPTO
        Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.HAS_CRYPTO = False
        TestStructFile.test_Index(self)
        Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.HAS_CRYPTO = has_crypto

    @skipUnless(HAS_CRYPTO, "No crypto libraries found, skipping")
    def test_Index_crypto(self):
        Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.SETUP = Mock()
        Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.SETUP.cfp.get.return_value = "strict"

        pkc = self.get_obj()
        pkc._decrypt = Mock()
        pkc._decrypt.return_value = 'plaintext'
        pkc.data = '''
<PrivateKey>
  <Group name="test">
    <Passphrase encrypted="foo">crypted</Passphrase>
  </Group>
  <Group name="test" negate="true">
    <Passphrase>plain</Passphrase>
  </Group>
</PrivateKey>'''

        # test successful decryption
        pkc.Index()
        self.assertItemsEqual(
            pkc._decrypt.call_args_list,
            [call(el)
             for el in pkc.xdata.xpath("//Passphrase[@encrypted]")])
        for el in pkc.xdata.xpath("//Crypted"):
            self.assertEqual(el.text, pkc._decrypt.return_value)

        # test failed decryption, strict
        pkc._decrypt.reset_mock()
        pkc._decrypt.side_effect = EVPError
        self.assertRaises(PluginExecutionError, pkc.Index)

        # test failed decryption, lax
        Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.SETUP.cfp.get.return_value = "lax"
        pkc._decrypt.reset_mock()
        pkc.Index()
        self.assertItemsEqual(
            pkc._decrypt.call_args_list,
            [call(el)
             for el in pkc.xdata.xpath("//Passphrase[@encrypted]")])

    @skipUnless(HAS_CRYPTO, "No crypto libraries found, skipping")
    @patchIf(HAS_CRYPTO, "Bcfg2.Encryption.ssl_decrypt")
    @patchIf(HAS_CRYPTO, "Bcfg2.Encryption.get_algorithm")
    @patchIf(HAS_CRYPTO, "Bcfg2.Encryption.get_passphrases")
    @patchIf(HAS_CRYPTO, "Bcfg2.Encryption.bruteforce_decrypt")
    def test_decrypt(self, mock_bruteforce, mock_get_passphrases,
                     mock_get_algorithm, mock_ssl):
        pkc = self.get_obj()
        Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.SETUP = MagicMock()

        def reset():
            mock_bruteforce.reset_mock()
            mock_get_algorithm.reset_mock()
            mock_get_passphrases.reset_mock()
            mock_ssl.reset_mock()

        # test element without text contents
        self.assertIsNone(pkc._decrypt(lxml.etree.Element("Test")))
        self.assertFalse(mock_bruteforce.called)
        self.assertFalse(mock_get_passphrases.called)
        self.assertFalse(mock_ssl.called)

        # test element with a passphrase in the config file
        reset()
        el = lxml.etree.Element("Test", encrypted="foo")
        el.text = "crypted"
        mock_get_passphrases.return_value = dict(foo="foopass",
                                                 bar="barpass")
        mock_get_algorithm.return_value = "bf_cbc"
        mock_ssl.return_value = "decrypted with ssl"
        self.assertEqual(pkc._decrypt(el), mock_ssl.return_value)
        mock_get_passphrases.assert_called_with(
            Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.SETUP)
        mock_get_algorithm.assert_called_with(
            Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.SETUP)
        mock_ssl.assert_called_with(el.text, "foopass",
                                    algorithm="bf_cbc")
        self.assertFalse(mock_bruteforce.called)

        # test failure to decrypt element with a passphrase in the config
        reset()
        mock_ssl.side_effect = EVPError
        self.assertRaises(EVPError, pkc._decrypt, el)
        mock_get_passphrases.assert_called_with(
            Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.SETUP)
        mock_get_algorithm.assert_called_with(
            Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.SETUP)
        mock_ssl.assert_called_with(el.text, "foopass",
                                    algorithm="bf_cbc")
        self.assertFalse(mock_bruteforce.called)

        # test element without valid passphrase
        reset()
        el.set("encrypted", "true")
        mock_bruteforce.return_value = "decrypted with bruteforce"
        self.assertEqual(pkc._decrypt(el), mock_bruteforce.return_value)
        mock_get_passphrases.assert_called_with(
            Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.SETUP)
        mock_get_algorithm.assert_called_with(
            Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.SETUP)
        mock_bruteforce.assert_called_with(el.text,
                                           passphrases=["foopass",
                                                        "barpass"],
                                           algorithm="bf_cbc")
        self.assertFalse(mock_ssl.called)

        # test failure to decrypt element without valid passphrase
        reset()
        mock_bruteforce.side_effect = EVPError
        self.assertRaises(EVPError, pkc._decrypt, el)
        mock_get_passphrases.assert_called_with(
            Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.SETUP)
        mock_get_algorithm.assert_called_with(
            Bcfg2.Server.Plugins.Cfg.CfgPrivateKeyCreator.SETUP)
        mock_bruteforce.assert_called_with(el.text,
                                           passphrases=["foopass",
                                                        "barpass"],
                                           algorithm="bf_cbc")
        self.assertFalse(mock_ssl.called)

########NEW FILE########
__FILENAME__ = TestCfgPublicKeyCreator
import os
import sys
import lxml.etree
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugins.Cfg import CfgCreationError, CfgCreator
from Bcfg2.Server.Plugins.Cfg.CfgPublicKeyCreator import *
import Bcfg2.Server.Plugins.Cfg.CfgPublicKeyCreator
from Bcfg2.Server.Plugin import PluginExecutionError

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestServer.TestPlugins.TestCfg.Test_init import TestCfgCreator
from TestServer.TestPlugin.Testhelpers import TestStructFile


class TestCfgPublicKeyCreator(TestCfgCreator, TestStructFile):
    test_obj = CfgPublicKeyCreator
    should_monitor = False

    def get_obj(self, name=None, fam=None):
        Bcfg2.Server.Plugins.Cfg.CfgPublicKeyCreator.CFG = Mock()
        return TestCfgCreator.get_obj(self, name=name)

    @patch("Bcfg2.Server.Plugins.Cfg.CfgCreator.handle_event")
    @patch("Bcfg2.Server.Plugin.helpers.StructFile.HandleEvent")
    def test_handle_event(self, mock_HandleEvent, mock_handle_event):
        pkc = self.get_obj()
        evt = Mock()
        pkc.handle_event(evt)
        mock_HandleEvent.assert_called_with(pkc, evt)
        mock_handle_event.assert_called_with(pkc, evt)

    @patch("os.unlink")
    @patch("os.path.exists")
    @patch("tempfile.mkstemp")
    @patch("os.fdopen", Mock())
    @patch("%s.open" % builtins)
    def test_create_data(self, mock_open, mock_mkstemp, mock_exists,
                         mock_unlink):
        metadata = Mock()
        pkc = self.get_obj()
        pkc.cfg = Mock()
        pkc.core = Mock()
        pkc.cmd = Mock()
        pkc.write_data = Mock()

        pubkey = "public key data"
        privkey_entryset = Mock()
        privkey_creator = Mock()
        privkey_creator.get_specificity = Mock()
        privkey_creator.get_specificity.return_value = dict()
        fileloc = pkc.get_filename()
        pkc.cfg.entries = {"/home/foo/.ssh/id_rsa": privkey_entryset}

        def reset():
            privkey_creator.reset_mock()
            pkc.cmd.reset_mock()
            pkc.core.reset_mock()
            pkc.write_data.reset_mock()
            mock_exists.reset_mock()
            mock_unlink.reset_mock()
            mock_mkstemp.reset_mock()
            mock_open.reset_mock()

        # public key doesn't end in .pub
        entry = lxml.etree.Element("Path", name="/home/bar/.ssh/bogus")
        self.assertRaises(CfgCreationError,
                          pkc.create_data, entry, metadata)
        self.assertFalse(pkc.write_data.called)

        # cannot bind private key
        reset()
        pkc.core.Bind.side_effect = PluginExecutionError
        entry = lxml.etree.Element("Path", name="/home/foo/.ssh/id_rsa.pub")
        self.assertRaises(CfgCreationError,
                          pkc.create_data, entry, metadata)
        self.assertFalse(pkc.write_data.called)

        # private key not in cfg.entries
        reset()
        pkc.core.Bind.side_effect = None
        pkc.core.Bind.return_value = "private key data"
        entry = lxml.etree.Element("Path", name="/home/bar/.ssh/id_rsa.pub")
        self.assertRaises(CfgCreationError,
                          pkc.create_data, entry, metadata)
        self.assertFalse(pkc.write_data.called)

        # no privkey.xml defined
        reset()
        privkey_entryset.best_matching.side_effect = PluginExecutionError
        entry = lxml.etree.Element("Path", name="/home/foo/.ssh/id_rsa.pub")
        self.assertRaises(CfgCreationError,
                          pkc.create_data, entry, metadata)
        self.assertFalse(pkc.write_data.called)

        # successful operation, create new key
        reset()
        pkc.cmd.run.return_value = Mock()
        pkc.cmd.run.return_value.success = True
        pkc.cmd.run.return_value.stdout = pubkey
        mock_mkstemp.return_value = (Mock(), str(Mock()))
        mock_exists.return_value = False
        privkey_entryset.best_matching.side_effect = None
        privkey_entryset.best_matching.return_value = privkey_creator
        entry = lxml.etree.Element("Path", name="/home/foo/.ssh/id_rsa.pub")
        self.assertEqual(pkc.create_data(entry, metadata), pubkey)
        self.assertTrue(pkc.core.Bind.called)
        (privkey_entry, md) = pkc.core.Bind.call_args[0]
        self.assertXMLEqual(privkey_entry,
                            lxml.etree.Element("Path",
                                               name="/home/foo/.ssh/id_rsa"))
        self.assertEqual(md, metadata)

        privkey_entryset.get_handlers.assert_called_with(metadata, CfgCreator)
        privkey_entryset.best_matching.assert_called_with(
            metadata,
            privkey_entryset.get_handlers.return_value)
        mock_exists.assert_called_with(fileloc)
        pkc.cmd.run.assert_called_with(["ssh-keygen", "-y", "-f",
                                        mock_mkstemp.return_value[1]])
        self.assertEqual(pkc.write_data.call_args[0][0], pubkey)
        mock_unlink.assert_called_with(mock_mkstemp.return_value[1])
        self.assertFalse(mock_open.called)

        # successful operation, no need to create new key
        reset()
        mock_exists.return_value = True
        mock_open.return_value = Mock()
        mock_open.return_value.read.return_value = pubkey
        pkc.cmd.run.return_value.stdout = None
        self.assertEqual(pkc.create_data(entry, metadata), pubkey)
        self.assertTrue(pkc.core.Bind.called)
        (privkey_entry, md) = pkc.core.Bind.call_args[0]
        self.assertXMLEqual(privkey_entry,
                            lxml.etree.Element("Path",
                                               name="/home/foo/.ssh/id_rsa"))
        self.assertEqual(md, metadata)

        privkey_entryset.get_handlers.assert_called_with(metadata, CfgCreator)
        privkey_entryset.best_matching.assert_called_with(
            metadata,
            privkey_entryset.get_handlers.return_value)
        mock_exists.assert_called_with(fileloc)
        mock_open.assert_called_with(fileloc)
        self.assertFalse(mock_mkstemp.called)
        self.assertFalse(pkc.write_data.called)

########NEW FILE########
__FILENAME__ = Test_init
import os
import sys
import errno
import lxml.etree
import Bcfg2.Options
from Bcfg2.Compat import walk_packages
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugins.Cfg import *
from Bcfg2.Server.Plugin import PluginExecutionError, Specificity

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestPlugin import TestSpecificData, TestEntrySet, TestGroupSpool, \
    TestPullTarget


class TestCfgBaseFileMatcher(TestSpecificData):
    test_obj = CfgBaseFileMatcher
    path = os.path.join(datastore, "test+test.txt")

    def test_get_regex(self):
        if self.test_obj.__basenames__:
            basenames = self.test_obj.__basenames__
        else:
            basenames = [os.path.basename(self.path)]
        if self.test_obj.__extensions__:
            extensions = self.test_obj.__extensions__
        else:
            extensions = ['']
        for extension in extensions:
            regex = self.test_obj.get_regex(basenames)
            for basename in basenames:
                def test_match(spec):
                    mstr = basename
                    if spec:
                        mstr += "." + spec
                    if extension:
                        mstr += "." + extension
                    return regex.match(mstr)

                self.assertTrue(test_match(''))
                self.assertFalse(regex.match("bogus"))
                if self.test_obj.__specific__:
                    if extension:
                        self.assertFalse(regex.match("bogus." + extension))
                    self.assertTrue(test_match("G20_foo"))
                    self.assertTrue(test_match("G1_foo"))
                    self.assertTrue(test_match("G32768_foo"))
                    # a group named '_'
                    self.assertTrue(test_match("G10__"))
                    self.assertTrue(test_match("H_hostname"))
                    self.assertTrue(test_match("H_fqdn.subdomain.example.com"))
                    self.assertTrue(test_match("G20_group_with_underscores"))

                    self.assertFalse(test_match("G20_group with spaces"))
                    self.assertFalse(test_match("G_foo"))
                    self.assertFalse(test_match("G_"))
                    self.assertFalse(test_match("G20_"))
                    self.assertFalse(test_match("H_"))
                else:
                    self.assertFalse(test_match("G20_foo"))
                    self.assertFalse(test_match("H_hostname"))

    @patch("Bcfg2.Server.Plugins.Cfg.CfgBaseFileMatcher.get_regex")
    def test_handles(self, mock_get_regex):
        match = Mock()
        mock_get_regex.return_value = Mock()
        mock_get_regex.return_value.match = match

        evt = Mock()
        evt.filename = "event.txt"

        if self.test_obj.__basenames__:
            match.return_value = False
            self.assertFalse(self.test_obj.handles(evt))
            mock_get_regex.assert_called_with(
                [b for b in self.test_obj.__basenames__])
            print("match calls: %s" % match.call_args_list)
            print("expected: %s" % [call(evt.filename)
                                   for b in self.test_obj.__basenames__])
            match.assert_called_with(evt.filename)

            mock_get_regex.reset_mock()
            match.reset_mock()
            match.return_value = True
            self.assertTrue(self.test_obj.handles(evt))
            match.assert_called_with(evt.filename)
        else:
            match.return_value = False
            self.assertFalse(self.test_obj.handles(evt,
                                         basename=os.path.basename(self.path)))
            mock_get_regex.assert_called_with([os.path.basename(self.path)])
            match.assert_called_with(evt.filename)

            mock_get_regex.reset_mock()
            match.reset_mock()
            match.return_value = True
            self.assertTrue(self.test_obj.handles(evt,
                                        basename=os.path.basename(self.path)))
            mock_get_regex.assert_called_with([os.path.basename(self.path)])
            match.assert_called_with(evt.filename)

    def test_ignore(self):
        evt = Mock()
        evt.filename = "event.txt"

        if not self.test_obj.__ignore__:
            self.assertFalse(self.test_obj.ignore(evt))
        else:
            self.assertFalse(self.test_obj.ignore(evt))
            for extension in self.test_obj.__ignore__:
                for name in ["event.txt", "....", extension, "." + extension]:
                    for filler in ['', '.blah', '......', '.' + extension]:
                        evt.filename = name + filler + '.' + extension
                        self.assertTrue(self.test_obj.ignore(evt))


class TestCfgGenerator(TestCfgBaseFileMatcher):
    test_obj = CfgGenerator

    def test_get_data(self):
        cg = self.get_obj()
        cg.data = "foo bar baz"
        self.assertEqual(cg.data, cg.get_data(Mock(), Mock()))


class TestCfgFilter(TestCfgBaseFileMatcher):
    test_obj = CfgFilter

    def test_modify_data(self):
        cf = self.get_obj()
        self.assertRaises(NotImplementedError,
                          cf.modify_data, Mock(), Mock(), Mock())


class TestCfgInfo(TestCfgBaseFileMatcher):
    test_obj = CfgInfo

    def get_obj(self, name=None):
        if name is None:
            name = self.path
        return self.test_obj(name)

    @patch("Bcfg2.Server.Plugins.Cfg.CfgBaseFileMatcher.__init__")
    def test__init(self, mock__init):
        ci = self.get_obj("test.txt")
        mock__init.assert_called_with(ci, "test.txt", None, None)

    def test_bind_info_to_entry(self):
        ci = self.get_obj()
        self.assertRaises(NotImplementedError,
                          ci.bind_info_to_entry, Mock(), Mock())

    def test__set_info(self):
        ci = self.get_obj()
        entry = Mock()
        entry.attrib = dict()

        info = {"foo": "foo",
                "_bar": "bar",
                "bar:baz=quux": "quux",
                "baz__": "baz",
                "__quux": "quux"}
        ci._set_info(entry, info)
        self.assertItemsEqual(entry.attrib,
                              dict([(k, v) for k, v in info.items()
                                    if not k.startswith("__")]))


class TestCfgVerifier(TestCfgBaseFileMatcher):
    test_obj = CfgVerifier

    def test_verify_entry(self):
        cf = self.get_obj()
        self.assertRaises(NotImplementedError,
                          cf.verify_entry, Mock(), Mock(), Mock())


class TestCfgCreator(TestCfgBaseFileMatcher):
    test_obj = CfgCreator
    path = "/foo/bar/test.txt"

    def get_obj(self, name=None):
        if name is None:
            name = self.path
        return self.test_obj(name)

    def test_create_data(self):
        cc = self.get_obj()
        self.assertRaises(NotImplementedError,
                          cc.create_data, Mock(), Mock())

    def test_get_filename(self):
        cc = self.get_obj()

        # tuples of (args to get_filename(), expected result)
        cases = [(dict(), "/foo/bar/bar"),
                 (dict(prio=50), "/foo/bar/bar"),
                 (dict(ext=".crypt"), "/foo/bar/bar.crypt"),
                 (dict(ext="bar"), "/foo/bar/barbar"),
                 (dict(host="foo.bar.example.com"),
                  "/foo/bar/bar.H_foo.bar.example.com"),
                 (dict(host="foo.bar.example.com", prio=50, ext=".crypt"),
                  "/foo/bar/bar.H_foo.bar.example.com.crypt"),
                 (dict(group="group", prio=1), "/foo/bar/bar.G01_group"),
                 (dict(group="group", prio=50), "/foo/bar/bar.G50_group"),
                 (dict(group="group", prio=50, ext=".crypt"),
                  "/foo/bar/bar.G50_group.crypt")]

        for args, expected in cases:
            self.assertEqual(cc.get_filename(**args), expected)

    @patch("os.makedirs")
    @patch("%s.open" % builtins)
    def test_write_data(self, mock_open, mock_makedirs):
        cc = self.get_obj()
        data = "test\ntest"
        parent = os.path.dirname(self.path)

        def reset():
            mock_open.reset_mock()
            mock_makedirs.reset_mock()

        # test writing file
        reset()
        spec = dict(group="foogroup", prio=9)
        cc.write_data(data, **spec)
        mock_makedirs.assert_called_with(parent)
        mock_open.assert_called_with(cc.get_filename(**spec), "wb")
        mock_open.return_value.write.assert_called_with(data)

        # test already-exists error from makedirs
        reset()
        mock_makedirs.side_effect = OSError(errno.EEXIST, self.path)
        cc.write_data(data)
        mock_makedirs.assert_called_with(parent)
        mock_open.assert_called_with(cc.get_filename(), "wb")
        mock_open.return_value.write.assert_called_with(data)

        # test error from open
        reset()
        mock_open.side_effect = IOError
        self.assertRaises(CfgCreationError, cc.write_data, data)

        # test real error from makedirs
        reset()
        mock_makedirs.side_effect = OSError
        self.assertRaises(CfgCreationError, cc.write_data, data)


class TestCfgDefaultInfo(TestCfgInfo):
    test_obj = CfgDefaultInfo

    def get_obj(self, defaults=None):
        if defaults is None:
            defaults = dict()
        return self.test_obj(defaults)

    @patch("Bcfg2.Server.Plugins.Cfg.CfgInfo.__init__")
    def test__init(self, mock__init):
        defaults = Mock()
        cdi = self.get_obj(defaults=defaults)
        mock__init.assert_called_with(cdi, '')
        self.assertEqual(defaults, cdi.defaults)

    def test_handle_event(self):
        # this CfgInfo handler doesn't handle any events -- it's not
        # file-driven, but based on the built-in defaults
        pass

    def test_bind_info_to_entry(self):
        cdi = self.get_obj()
        cdi._set_info = Mock()
        entry = Mock()
        cdi.bind_info_to_entry(entry, Mock())
        cdi._set_info.assert_called_with(entry, cdi.defaults)


class TestCfgEntrySet(TestEntrySet):
    test_obj = CfgEntrySet

    def test__init(self):
        pass

    def test_handlers(self):
        # this is really really difficult to mock out, so we just get
        # a list of handlers and make sure that it roughly matches
        # what's on the filesystem
        expected = []
        for submodule in walk_packages(path=Bcfg2.Server.Plugins.Cfg.__path__,
                                       prefix="Bcfg2.Server.Plugins.Cfg."):
            expected.append(submodule[1].rsplit('.', 1)[-1])
        self.assertItemsEqual(expected, [h.__name__ for h in handlers()])

    @patch("Bcfg2.Server.Plugins.Cfg.handlers")
    def test_handle_event(self, mock_handlers):
        eset = self.get_obj()
        eset.entry_init = Mock()
        mock_handlers.return_value = [Mock(), Mock(), Mock()]
        for hdlr in mock_handlers.return_value:
            hdlr.__name__ = "handler"
        eset.entries = dict()

        def reset():
            eset.entry_init.reset_mock()
            for hdlr in mock_handlers.return_value:
                hdlr.reset_mock()

        # test that a bogus deleted event is discarded
        evt = Mock()
        evt.code2str.return_value = "deleted"
        evt.filename = os.path.join(datastore, "test.txt")
        eset.handle_event(evt)
        self.assertFalse(eset.entry_init.called)
        self.assertItemsEqual(eset.entries, dict())
        for hdlr in mock_handlers.return_value:
            self.assertFalse(hdlr.handles.called)
            self.assertFalse(hdlr.ignore.called)

        # test creation of a new file
        for action in ["exists", "created", "changed"]:
            evt = Mock()
            evt.code2str.return_value = action
            evt.filename = os.path.join(datastore, "test.txt")

            # test with no handler that handles
            for hdlr in mock_handlers.return_value:
                hdlr.handles.return_value = False
                hdlr.ignore.return_value = False

            reset()
            eset.handle_event(evt)
            self.assertFalse(eset.entry_init.called)
            self.assertItemsEqual(eset.entries, dict())
            for hdlr in mock_handlers.return_value:
                hdlr.handles.assert_called_with(evt, basename=eset.path)
                hdlr.ignore.assert_called_with(evt, basename=eset.path)

            # test with a handler that handles the entry
            reset()
            mock_handlers.return_value[-1].handles.return_value = True
            eset.handle_event(evt)
            eset.entry_init.assert_called_with(evt, mock_handlers.return_value[-1])
            for hdlr in mock_handlers.return_value:
                hdlr.handles.assert_called_with(evt, basename=eset.path)
                if not hdlr.return_value:
                    hdlr.ignore.assert_called_with(evt, basename=eset.path)

            # test with a handler that ignores the entry before one
            # that handles it
            reset()
            mock_handlers.return_value[0].ignore.return_value = True
            eset.handle_event(evt)
            self.assertFalse(eset.entry_init.called)
            mock_handlers.return_value[0].handles.assert_called_with(evt,
                                                        basename=eset.path)
            mock_handlers.return_value[0].ignore.assert_called_with(evt,
                                                       basename=eset.path)
            for hdlr in mock_handlers.return_value[1:]:
                self.assertFalse(hdlr.handles.called)
                self.assertFalse(hdlr.ignore.called)

        # test changed event with an entry that already exists
        reset()
        evt = Mock()
        evt.code2str.return_value = "changed"
        evt.filename = os.path.join(datastore, "test.txt")
        eset.entries[evt.filename] = Mock()
        eset.handle_event(evt)
        self.assertFalse(eset.entry_init.called)
        for hdlr in mock_handlers.return_value:
            self.assertFalse(hdlr.handles.called)
            self.assertFalse(hdlr.ignore.called)
        eset.entries[evt.filename].handle_event.assert_called_with(evt)

        # test deleted event with an entry that already exists
        reset()
        evt.code2str.return_value = "deleted"
        eset.handle_event(evt)
        self.assertFalse(eset.entry_init.called)
        for hdlr in mock_handlers.return_value:
            self.assertFalse(hdlr.handles.called)
            self.assertFalse(hdlr.ignore.called)
        self.assertItemsEqual(eset.entries, dict())

    def test_get_matching(self):
        eset = self.get_obj()
        eset.get_handlers = Mock()
        metadata = Mock()
        self.assertEqual(eset.get_matching(metadata),
                         eset.get_handlers.return_value)
        eset.get_handlers.assert_called_with(metadata, CfgGenerator)

    @patch("Bcfg2.Server.Plugin.EntrySet.entry_init")
    def test_entry_init(self, mock_entry_init):
        eset = self.get_obj()
        eset.entries = dict()
        evt = Mock()
        evt.filename = "test.txt"
        handler = Mock()
        handler.__basenames__ = []
        handler.__extensions__ = []
        handler.deprecated = False
        handler.experimental = False
        handler.__specific__ = True

        # test handling an event with the parent entry_init
        eset.entry_init(evt, handler)
        mock_entry_init.assert_called_with(eset, evt, entry_type=handler,
                                           specific=handler.get_regex.return_value)
        self.assertItemsEqual(eset.entries, dict())

        # test handling the event with a Cfg handler
        handler.__specific__ = False
        eset.entry_init(evt, handler)
        handler.assert_called_with(os.path.join(eset.path, evt.filename))
        self.assertItemsEqual(eset.entries,
                              {evt.filename: handler.return_value})
        handler.return_value.handle_event.assert_called_with(evt)

        # test handling an event for an entry that already exists with
        # a Cfg handler
        handler.reset_mock()
        eset.entry_init(evt, handler)
        self.assertFalse(handler.called)
        self.assertItemsEqual(eset.entries,
                              {evt.filename: handler.return_value})
        eset.entries[evt.filename].handle_event.assert_called_with(evt)

    @patch("Bcfg2.Server.Plugins.Cfg.u_str")
    @patch("Bcfg2.Server.Plugins.Cfg.b64encode")
    def test_bind_entry(self, mock_b64encode, mock_u_str):
        Bcfg2.Server.Plugins.Cfg.SETUP = dict(validate=False)

        mock_u_str.side_effect = lambda x: x

        eset = self.get_obj()
        eset.bind_info_to_entry = Mock()
        eset._generate_data = Mock()
        eset.get_handlers = Mock()
        eset._validate_data = Mock()

        def reset():
            mock_b64encode.reset_mock()
            mock_u_str.reset_mock()
            eset.bind_info_to_entry.reset_mock()
            eset._generate_data.reset_mock()
            eset.get_handlers.reset_mock()
            eset._validate_data.reset_mock()
            return lxml.etree.Element("Path", name="/test.txt")

        entry = reset()
        metadata = Mock()

        # test basic entry, no validation, no filters, etc.
        eset._generate_data.return_value = ("data", None)
        eset.get_handlers.return_value = []
        bound = eset.bind_entry(entry, metadata)
        eset.bind_info_to_entry.assert_called_with(entry, metadata)
        eset._generate_data.assert_called_with(entry, metadata)
        self.assertFalse(eset._validate_data.called)
        expected = lxml.etree.Element("Path", name="/test.txt")
        expected.text = "data"
        self.assertXMLEqual(bound, expected)
        self.assertEqual(bound, entry)

        # test empty entry
        entry = reset()
        eset._generate_data.return_value = ("", None)
        bound = eset.bind_entry(entry, metadata)
        eset.bind_info_to_entry.assert_called_with(entry, metadata)
        eset._generate_data.assert_called_with(entry, metadata)
        self.assertFalse(eset._validate_data.called)
        expected = lxml.etree.Element("Path", name="/test.txt", empty="true")
        self.assertXMLEqual(bound, expected)
        self.assertEqual(bound, entry)

        # test filters
        entry = reset()
        generator = Mock()
        generator.specific = Specificity(all=True)
        eset._generate_data.return_value = ("initial data", generator)
        filters = [Mock(), Mock()]
        filters[0].modify_data.return_value = "modified data"
        filters[1].modify_data.return_value = "final data"
        eset.get_handlers.return_value = filters
        bound = eset.bind_entry(entry, metadata)
        eset.bind_info_to_entry.assert_called_with(entry, metadata)
        eset._generate_data.assert_called_with(entry, metadata)
        filters[0].modify_data.assert_called_with(entry, metadata,
                                                  "initial data")
        filters[1].modify_data.assert_called_with(entry, metadata,
                                                  "modified data")
        self.assertFalse(eset._validate_data.called)
        expected = lxml.etree.Element("Path", name="/test.txt")
        expected.text = "final data"
        self.assertXMLEqual(bound, expected)

        # test base64 encoding
        entry = reset()
        entry.set("encoding", "base64")
        mock_b64encode.return_value = "base64 data"
        eset.get_handlers.return_value = []
        eset._generate_data.return_value = ("data", None)
        bound = eset.bind_entry(entry, metadata)
        eset.bind_info_to_entry.assert_called_with(entry, metadata)
        eset._generate_data.assert_called_with(entry, metadata)
        self.assertFalse(eset._validate_data.called)
        mock_b64encode.assert_called_with("data")
        self.assertFalse(mock_u_str.called)
        expected = lxml.etree.Element("Path", name="/test.txt",
                                      encoding="base64")
        expected.text = "base64 data"
        self.assertXMLEqual(bound, expected)
        self.assertEqual(bound, entry)

        # test successful validation
        entry = reset()
        Bcfg2.Server.Plugins.Cfg.SETUP['validate'] = True
        bound = eset.bind_entry(entry, metadata)
        eset.bind_info_to_entry.assert_called_with(entry, metadata)
        eset._generate_data.assert_called_with(entry, metadata)
        eset._validate_data.assert_called_with(entry, metadata, "data")
        expected = lxml.etree.Element("Path", name="/test.txt")
        expected.text = "data"
        self.assertXMLEqual(bound, expected)
        self.assertEqual(bound, entry)

        # test failed validation
        entry = reset()
        eset._validate_data.side_effect = CfgVerificationError
        self.assertRaises(PluginExecutionError,
                          eset.bind_entry, entry, metadata)
        eset.bind_info_to_entry.assert_called_with(entry, metadata)
        eset._generate_data.assert_called_with(entry, metadata)
        eset._validate_data.assert_called_with(entry, metadata, "data")

    def test_get_handlers(self):
        eset = self.get_obj()
        eset.entries['test1.txt'] = CfgInfo("test1.txt")
        eset.entries['test2.txt'] = CfgGenerator("test2.txt", Mock(), None)
        eset.entries['test2.txt'].specific.matches.return_value = True
        eset.entries['test3.txt'] = CfgInfo("test3.txt")
        eset.entries['test4.txt'] = CfgGenerator("test4.txt", Mock(), None)
        eset.entries['test4.txt'].specific.matches.return_value = False
        eset.entries['test5.txt'] = CfgGenerator("test5.txt", Mock(), None)
        eset.entries['test5.txt'].specific.matches.return_value = True
        eset.entries['test6.txt'] = CfgVerifier("test6.txt", Mock(), None)
        eset.entries['test6.txt'].specific.matches.return_value = True
        eset.entries['test7.txt'] = CfgFilter("test7.txt", Mock(), None)
        eset.entries['test7.txt'].specific.matches.return_value = False

        def reset():
            for e in eset.entries.values():
                if hasattr(e.specific, "reset_mock"):
                    e.specific.reset_mock()

        metadata = Mock()
        self.assertItemsEqual(eset.get_handlers(metadata, CfgGenerator),
                              [eset.entries['test2.txt'],
                               eset.entries['test5.txt']])
        for ename in ['test2.txt', 'test4.txt', 'test5.txt']:
            eset.entries[ename].specific.matches.assert_called_with(metadata)
        for ename in ['test6.txt', 'test7.txt']:
            self.assertFalse(eset.entries[ename].specific.matches.called)

        reset()
        self.assertItemsEqual(eset.get_handlers(metadata, CfgInfo),
                              [eset.entries['test1.txt'],
                               eset.entries['test3.txt']])
        for entry in eset.entries.values():
            if hasattr(entry.specific.matches, "called"):
                self.assertFalse(entry.specific.matches.called)

        reset()
        self.assertItemsEqual(eset.get_handlers(metadata, CfgVerifier),
                              [eset.entries['test6.txt']])
        eset.entries['test6.txt'].specific.matches.assert_called_with(metadata)
        for ename, entry in eset.entries.items():
            if (ename != 'test6.txt' and
                hasattr(entry.specific.matches, "called")):
                self.assertFalse(entry.specific.matches.called)

        reset()
        self.assertItemsEqual(eset.get_handlers(metadata, CfgFilter), [])
        eset.entries['test7.txt'].specific.matches.assert_called_with(metadata)
        for ename, entry in eset.entries.items():
            if (ename != 'test7.txt' and
                hasattr(entry.specific.matches, "called")):
                self.assertFalse(entry.specific.matches.called)

        reset()
        self.assertItemsEqual(eset.get_handlers(metadata, Mock), [])
        for ename, entry in eset.entries.items():
            if hasattr(entry.specific.matches, "called"):
                self.assertFalse(entry.specific.matches.called)

    def test_bind_info_to_entry(self):
        default_info = Bcfg2.Server.Plugins.Cfg.DEFAULT_INFO
        eset = self.get_obj()
        eset.get_handlers = Mock()
        eset.get_handlers.return_value = []
        Bcfg2.Server.Plugins.Cfg.DEFAULT_INFO = Mock()
        metadata = Mock()

        def reset():
            eset.get_handlers.reset_mock()
            Bcfg2.Server.Plugins.Cfg.DEFAULT_INFO.reset_mock()
            return lxml.etree.Element("Path", name="/test.txt")

        # test with no info handlers
        entry = reset()
        eset.bind_info_to_entry(entry, metadata)
        eset.get_handlers.assert_called_with(metadata, CfgInfo)
        Bcfg2.Server.Plugins.Cfg.DEFAULT_INFO.bind_info_to_entry.assert_called_with(entry, metadata)
        self.assertEqual(entry.get("type"), "file")

        # test with one info handler
        entry = reset()
        handler = Mock()
        eset.get_handlers.return_value = [handler]
        eset.bind_info_to_entry(entry, metadata)
        eset.get_handlers.assert_called_with(metadata, CfgInfo)
        Bcfg2.Server.Plugins.Cfg.DEFAULT_INFO.bind_info_to_entry.assert_called_with(entry, metadata)
        handler.bind_info_to_entry.assert_called_with(entry, metadata)
        self.assertEqual(entry.get("type"), "file")

        # test with more than one info handler
        entry = reset()
        handlers = [Mock(), Mock()]
        eset.get_handlers.return_value = handlers
        eset.bind_info_to_entry(entry, metadata)
        eset.get_handlers.assert_called_with(metadata, CfgInfo)
        Bcfg2.Server.Plugins.Cfg.DEFAULT_INFO.bind_info_to_entry.assert_called_with(entry, metadata)
        # we don't care which handler gets called as long as exactly
        # one of them does
        called = 0
        for handler in handlers:
            if handler.bind_info_to_entry.called:
                handler.bind_info_to_entry.assert_called_with(entry, metadata)
                called += 1
        self.assertEqual(called, 1)
        self.assertEqual(entry.get("type"), "file")

        Bcfg2.Server.Plugins.Cfg.DEFAULT_INFO = default_info

    def test_create_data(self):
        eset = self.get_obj()
        eset.best_matching = Mock()
        creator = Mock()
        creator.create_data.return_value = "data"
        eset.best_matching.return_value = creator
        eset.get_handlers = Mock()
        entry = lxml.etree.Element("Path", name="/test.txt", mode="0640")
        metadata = Mock()

        def reset():
            eset.best_matching.reset_mock()
            eset.get_handlers.reset_mock()

        # test success
        self.assertEqual(eset._create_data(entry, metadata), "data")
        eset.get_handlers.assert_called_with(metadata, CfgCreator)
        eset.best_matching.assert_called_with(metadata,
                                              eset.get_handlers.return_value)

        # test failure to create data
        reset()
        creator.create_data.side_effect = OSError
        self.assertRaises(PluginExecutionError,
                          eset._create_data, entry, metadata)

    def test_generate_data(self):
        eset = self.get_obj()
        eset.best_matching = Mock()
        eset._create_data = Mock()
        generator = Mock()
        generator.get_data.return_value = "data"
        eset.best_matching.return_value = generator
        eset.get_handlers = Mock()
        entry = lxml.etree.Element("Path", name="/test.txt", mode="0640")
        metadata = Mock()

        def reset():
            eset.best_matching.reset_mock()
            eset.get_handlers.reset_mock()
            eset._create_data.reset_mock()

        # test success
        self.assertEqual(eset._generate_data(entry, metadata)[0],
                         "data")
        eset.get_handlers.assert_called_with(metadata, CfgGenerator)
        eset.best_matching.assert_called_with(metadata,
                                              eset.get_handlers.return_value)
        self.assertFalse(eset._create_data.called)

        # test failure to generate data
        reset()
        generator.get_data.side_effect = OSError
        self.assertRaises(PluginExecutionError,
                          eset._generate_data, entry, metadata)

        # test no generator found
        reset()
        eset.best_matching.side_effect = PluginExecutionError
        self.assertEqual(eset._generate_data(entry, metadata),
                         (eset._create_data.return_value, None))
        eset.get_handlers.assert_called_with(metadata, CfgGenerator)
        eset.best_matching.assert_called_with(metadata,
                                              eset.get_handlers.return_value)
        eset._create_data.assert_called_with(entry, metadata)


    def test_validate_data(self):
        class MockChild1(Mock):
            pass

        class MockChild2(Mock):
            pass

        eset = self.get_obj()
        eset.get_handlers = Mock()
        handlers1 = [MockChild1(), MockChild1()]
        handlers2 = [MockChild2()]
        eset.get_handlers.return_value = [handlers1[0], handlers2[0],
                                          handlers1[1]]
        eset.best_matching = Mock()
        eset.best_matching.side_effect = lambda m, v: v[0]
        entry = lxml.etree.Element("Path", name="/test.txt")
        metadata = Mock()
        data = "data"

        eset._validate_data(entry, metadata, data)
        eset.get_handlers.assert_called_with(metadata, CfgVerifier)
        self.assertItemsEqual(eset.best_matching.call_args_list,
                              [call(metadata, handlers1),
                               call(metadata, handlers2)])
        handlers1[0].verify_entry.assert_called_with(entry, metadata, data)
        handlers2[0].verify_entry.assert_called_with(entry, metadata, data)

    def test_specificity_from_filename(self):
        pass


class TestCfg(TestGroupSpool, TestPullTarget):
    test_obj = Cfg

    def get_obj(self, core=None):
        if core is None:
            core = Mock()
        core.setup = MagicMock()
        return TestGroupSpool.get_obj(self, core=core)

    @patch("Bcfg2.Server.Plugin.GroupSpool.__init__")
    @patch("Bcfg2.Server.Plugin.PullTarget.__init__")
    def test__init(self, mock_pulltarget_init, mock_groupspool_init):
        core = Mock()
        core.setup = MagicMock()
        cfg = self.test_obj(core, datastore)
        mock_pulltarget_init.assert_called_with(cfg)
        mock_groupspool_init.assert_called_with(cfg, core, datastore)
        core.setup.add_option.assert_called_with("validate",
                                                 Bcfg2.Options.CFG_VALIDATION)
        core.setup.reparse.assert_called_with()

        core.reset_mock()
        core.setup.reset_mock()
        mock_pulltarget_init.reset_mock()
        mock_groupspool_init.reset_mock()
        core.setup.__contains__.return_value = True
        cfg = self.test_obj(core, datastore)
        mock_pulltarget_init.assert_called_with(cfg)
        mock_groupspool_init.assert_called_with(cfg, core, datastore)
        self.assertFalse(core.setup.add_option.called)
        self.assertFalse(core.setup.reparse.called)

    def test_has_generator(self):
        cfg = self.get_obj()
        cfg.entries = dict()
        entry = lxml.etree.Element("Path", name="/test.txt")
        metadata = Mock()

        self.assertFalse(cfg.has_generator(entry, metadata))

        eset = Mock()
        eset.get_handlers.return_value = []
        cfg.entries[entry.get("name")] = eset
        self.assertFalse(cfg.has_generator(entry, metadata))
        eset.get_handlers.assert_called_with(metadata, CfgGenerator)

        eset.get_handlers.reset_mock()
        eset.get_handlers.return_value = [Mock()]
        self.assertTrue(cfg.has_generator(entry, metadata))
        eset.get_handlers.assert_called_with(metadata, CfgGenerator)

########NEW FILE########
__FILENAME__ = TestDefaults
import os
import sys
import lxml.etree
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugins.Defaults import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestRules import TestRules
from Testinterfaces import TestGoalValidator


class TestDefaults(TestRules, TestGoalValidator):
    test_obj = Defaults

    def get_obj(self, *args, **kwargs):
        return TestRules.get_obj(self, *args, **kwargs)

    def test_HandlesEntry(self):
        d = self.get_obj()
        self.assertFalse(d.HandlesEntry(Mock(), Mock()))

    @patch("Bcfg2.Server.Plugin.helpers.XMLDirectoryBacked.HandleEvent")
    def test_HandleEvent(self, mock_HandleEvent):
        d = self.get_obj()
        evt = Mock()
        d.HandleEvent(evt)
        mock_HandleEvent.assert_called_with(d, evt)

    def test_validate_goals(self):
        d = self.get_obj()
        d.BindEntry = Mock()
        metadata = Mock()

        entries = []
        config = lxml.etree.Element("Configuration")
        b1 = lxml.etree.SubElement(config, "Bundle")
        entries.append(lxml.etree.SubElement(b1, "Path", name="/foo"))
        entries.append(lxml.etree.SubElement(b1, "Path", name="/bar"))
        b2 = lxml.etree.SubElement(config, "Bundle")
        entries.append(lxml.etree.SubElement(b2, "Package", name="quux"))

        d.validate_goals(metadata, config)
        self.assertItemsEqual(d.BindEntry.call_args_list,
                              [call(e, metadata) for e in entries])

    def test__matches_regex_disabled(self):
        """ cannot disable regex in Defaults plugin """
        pass

    def set_regex_enabled(self, rules_obj, state):
        pass

    def test__regex_enabled(self):
        r = self.get_obj()
        self.assertTrue(r._regex_enabled)

########NEW FILE########
__FILENAME__ = TestGroupPatterns
import os
import sys
import lxml.etree
import Bcfg2.Server.Plugin
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugins.GroupPatterns import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestPlugin import TestXMLFileBacked, TestPlugin, TestConnector


class TestPatternMap(Bcfg2TestCase):
    def test_ranges(self):
        """ test processing NameRange patterns """
        tests = [("foo[[1-5]]",
                  ["foo1", "foo2", "foo5"],
                  ["foo", "foo0", "foo10"]),
                 ("[[10-99]]foo",
                  ["10foo", "99foo", "25foo"],
                  ["foo", "1foo", "999foo", "110foo"]),
                 ("foo[[1,3,5-10]]bar",
                  ["foo1bar", "foo7bar", "foo10bar"],
                  ["foo2bar", "foobar", "foo3", "5bar"]),
                 ("[[9-15]]foo[[16-20]]",
                  ["9foo18", "13foo17"],
                  ["8foo21", "12foo21", "8foo18", "16foo16",
                   "15foo15", "29foo20", "9foo200", "29foo200"])]

        groups = MagicMock()
        for rng, inc, exc in tests:
            pmap = PatternMap(None, rng, groups)
            for test in inc:
                self.assertEqual(pmap.process(test), groups)
            for test in exc:
                self.assertIsNone(pmap.process(test))

    def test_simple_patterns(self):
        """ test processing NamePatterns without backreferences """
        tests = [("foo.*",
                  ["foo", "foobar", "barfoo", "barfoobar"],
                  ["bar", "fo0"]),
                 ("^[A-z]fooo?$",
                  ["Afoo", "bfooo"],
                  ["foo", "fooo", "AAfoo", "Afoooo"])]

        groups = ["a", "b", "c"]
        for rng, inc, exc in tests:
            pmap = PatternMap(rng, None, groups)
            for test in inc:
                self.assertItemsEqual(pmap.process(test), groups)
            for test in exc:
                self.assertIsNone(pmap.process(test))

    def test_backref_patterns(self):
        """ test NamePatterns with backreferences """
        tests = [("foo(.*)", ['a', 'a$1', '$1a', '$$', '$a', '$1'],
                  {"foo": ['a', 'a', 'a', '$$', '$a', ''],
                   "foooOOo": ['a', 'aoOOo', 'oOOoa', '$$', '$a', 'oOOo'],
                   "barfoo$1": ['a', 'a$1', '$1a', '$$', '$a', '$1']}),
                 ("^([a-z])foo(.+)", ['a', 'a$1', '$1a$2', '$1$$2', '$2'],
                  {"foo": None,
                   "afooa": ['a', 'aa', 'aaa', 'a$a', 'a'],
                   "bfoobar": ['a', 'ab', 'babar', 'b$bar', 'bar']})]

        for rng, groups, cases in tests:
            pmap = PatternMap(rng, None, groups)
            for name, ret in cases.items():
                if ret is None:
                    self.assertIsNone(pmap.process(name))
                else:
                    self.assertItemsEqual(pmap.process(name), ret)


class TestPatternFile(TestXMLFileBacked):
    test_obj = PatternFile
    should_monitor = True

    def get_obj(self, path=None, fam=None, core=None, should_monitor=True):
        if path is None:
            path = self.path
        if fam and not core:
            core = Mock()
            core.fam = fam
        elif not core:
            core = Mock()

        @patchIf(not isinstance(lxml.etree.Element, Mock),
                 "lxml.etree.Element", Mock())
        def inner():
            return self.test_obj(path, core=core)
        return inner()

    @patch("Bcfg2.Server.Plugins.GroupPatterns.PatternMap")
    def test_Index(self, mock_PatternMap):
        TestXMLFileBacked.test_Index(self)
        core = Mock()
        pf = self.get_obj(core=core)

        pf.data = """
<GroupPatterns>
  <GroupPattern>
    <NamePattern>foo.*</NamePattern>
    <Group>test1</Group>
    <Group>test2</Group>
  </GroupPattern>
  <GroupPattern>
    <NameRange>foo[[1-5]]</NameRange>
    <Group>test3</Group>
  </GroupPattern>
</GroupPatterns>"""

        core.metadata_cache_mode = 'aggressive'
        pf.Index()
        core.metadata_cache.expire.assert_called_with()
        self.assertItemsEqual(mock_PatternMap.call_args_list,
                              [call("foo.*", None, ["test1", "test2"]),
                               call(None, "foo[[1-5]]", ["test3"])])

    def test_process_patterns(self):
        pf = self.get_obj()
        pf.patterns = [Mock(), Mock(), Mock()]
        pf.patterns[0].process.return_value = ["a", "b"]
        pf.patterns[1].process.return_value = None
        pf.patterns[2].process.return_value = ["b", "c"]
        self.assertItemsEqual(pf.process_patterns("foo.example.com"),
                              ["a", "b", "b", "c"])
        for pat in pf.patterns:
            pat.process.assert_called_with("foo.example.com")


class TestGroupPatterns(TestPlugin, TestConnector):
    test_obj = GroupPatterns

    def get_obj(self, core=None):
        @patchIf(not isinstance(lxml.etree.Element, Mock),
                 "lxml.etree.Element", Mock())
        def inner():
            return TestPlugin.get_obj(self, core=core)
        return inner()


    def test_get_additional_groups(self):
        gp = self.get_obj()
        gp.config = Mock()
        metadata = Mock()
        self.assertEqual(gp.get_additional_groups(metadata),
                         gp.config.process_patterns.return_value)
        gp.config.process_patterns.assert_called_with(metadata.hostname)

########NEW FILE########
__FILENAME__ = TestMetadata
import os
import sys
import copy
import time
import socket
import lxml.etree
import Bcfg2.Server
import Bcfg2.Server.Plugin
from Bcfg2.Server.Plugins.Metadata import *
from mock import Mock, MagicMock, patch

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestPlugin import TestXMLFileBacked, TestMetadata as _TestMetadata, \
    TestClientRunHooks, TestDatabaseBacked


def get_clients_test_tree():
    return lxml.etree.XML('''
<Clients>
  <Client name="client1" address="1.2.3.1" auth="cert"
          location="floating" password="password2" profile="group1"/>
  <Client name="client2" address="1.2.3.2" secure="true" profile="group2"/>
  <Client name="client3" address="1.2.3.3" uuid="uuid1" profile="group1"
          password="password2">
    <Alias name="alias1"/>
  </Client>
  <Client name="client4" profile="group1">
    <Alias name="alias2" address="1.2.3.2"/>
    <Alias name="alias3"/>
  </Client>
  <Client name="client5" profile="group1"/>
  <Client name="client6" profile="group1" auth="bootstrap"/>
  <Client name="client7" profile="group1" auth="cert" address="1.2.3.4"/>
  <Client name="client8" profile="group1" auth="cert+password"
          address="1.2.3.5"/>
  <Client name="client9" profile="group2" secure="true" password="password3"/>
  <Client name="client10" profile="group1" floating="true"/>
</Clients>''').getroottree()


def get_groups_test_tree():
    return lxml.etree.XML('''
<Groups xmlns:xi="http://www.w3.org/2001/XInclude">
  <Client name="client8">
    <Group name="group8"/>
  </Client>
  <Client name="client9">
    <Group name="group8"/>
  </Client>

  <Group name="group1" default="true" profile="true" public="true"
         category="category1"/>
  <Group name="group2" profile="true" public="true" category="category1">
    <Bundle name="bundle1"/>
    <Bundle name="bundle2"/>
    <Group name="group1"/>
    <Group name="group4"/>
  </Group>
  <Group name="group3" category="category2" public="false"/>
  <Group name="group4" category="category1">
    <Group name="group1"/>
    <Group name="group6"/>
  </Group>
  <Group name="group5"/>
  <Group name="group7">
    <Bundle name="bundle3"/>
  </Group>
  <Group name="group8">
    <Group name="group9"/>
    <Client name="client9">
      <Group name="group11"/>
      <Group name="group9" negate="true"/>
    </Client>
    <Group name="group1">
      <Group name="group10"/>
    </Group>
  </Group>
  <Group name="group12" category="category3" public="false"/>
</Groups>''').getroottree()


def get_metadata_object(core=None, watch_clients=False, use_db=False):
    if core is None:
        core = Mock()
        core.setup = MagicMock()
        core.metadata_cache = MagicMock()
    core.setup.cfp.getboolean = Mock(return_value=use_db)

    @patchIf(not isinstance(os.makedirs, Mock), "os.makedirs", Mock())
    @patchIf(not isinstance(lxml.etree.Element, Mock),
             "lxml.etree.Element", Mock())
    def inner():
        return Metadata(core, datastore, watch_clients=watch_clients)
    return inner()


class TestMetadataDB(DBModelTestCase):
    if HAS_DJANGO:
        models = [MetadataClientModel]


if HAS_DJANGO or can_skip:
    class TestClientVersions(TestDatabaseBacked):
        test_clients = dict(client1="1.2.0",
                            client2="1.2.2",
                            client3="1.3.0pre1",
                            client4="1.1.0",
                            client5=None,
                            client6=None)

        @skipUnless(HAS_DJANGO, "Django not found")
        def setUp(self):
            self.test_obj = ClientVersions
            syncdb(TestMetadataDB)
            for client, version in self.test_clients.items():
                MetadataClientModel(hostname=client, version=version).save()

        def test__contains(self):
            v = self.get_obj()
            self.assertIn("client1", v)
            self.assertIn("client5", v)
            self.assertNotIn("client__contains", v)

        def test_keys(self):
            v = self.get_obj()
            self.assertItemsEqual(self.test_clients.keys(), v.keys())

        def test__setitem(self):
            v = self.get_obj()

            # test setting version of existing client
            v["client1"] = "1.2.3"
            self.assertIn("client1", v)
            self.assertEqual(v['client1'], "1.2.3")
            client = MetadataClientModel.objects.get(hostname="client1")
            self.assertEqual(client.version, "1.2.3")

            # test adding new client
            new = "client__setitem"
            v[new] = "1.3.0"
            self.assertIn(new, v)
            self.assertEqual(v[new], "1.3.0")
            client = MetadataClientModel.objects.get(hostname=new)
            self.assertEqual(client.version, "1.3.0")

            # test adding new client with no version
            new2 = "client__setitem_2"
            v[new2] = None
            self.assertIn(new2, v)
            self.assertEqual(v[new2], None)
            client = MetadataClientModel.objects.get(hostname=new2)
            self.assertEqual(client.version, None)

        def test__getitem(self):
            v = self.get_obj()

            # test getting existing client
            self.assertEqual(v['client2'], "1.2.2")
            self.assertIsNone(v['client5'])

            # test exception on nonexistent client
            expected = KeyError
            try:
                v['clients__getitem']
            except expected:
                pass
            except:
                err = sys.exc_info()[1]
                self.assertFalse(True, "%s raised instead of %s" %
                                 (err.__class__.__name__,
                                  expected.__class__.__name__))
            else:
                self.assertFalse(True,
                                 "%s not raised" % expected.__class__.__name__)

        def test__len(self):
            v = self.get_obj()
            self.assertEqual(len(v), MetadataClientModel.objects.count())

        def test__iter(self):
            v = self.get_obj()
            self.assertItemsEqual([h for h in iter(v)], v.keys())

        def test__delitem(self):
            v = self.get_obj()

            # test adding new client
            new = "client__delitem"
            v[new] = "1.3.0"

            del v[new]
            self.assertIn(new, v)
            self.assertIsNone(v[new])


class TestXMLMetadataConfig(TestXMLFileBacked):
    test_obj = XMLMetadataConfig
    path = os.path.join(datastore, 'Metadata', 'clients.xml')

    def get_obj(self, basefile="clients.xml", core=None, watch_clients=False):
        self.metadata = get_metadata_object(core=core,
                                            watch_clients=watch_clients)
        @patchIf(not isinstance(lxml.etree.Element, Mock),
                 "lxml.etree.Element", Mock())
        def inner():
            return XMLMetadataConfig(self.metadata, watch_clients, basefile)
        return inner()

    def test__init(self):
        xmc = self.get_obj()
        self.assertEqual(self.metadata.core.fam, xmc.fam)
        self.assertFalse(xmc.fam.AddMonitor.called)

    def test_xdata(self):
        config = self.get_obj()
        expected = Bcfg2.Server.Plugin.MetadataRuntimeError
        try:
            config.xdata
        except expected:
            pass
        except:
            err = sys.exc_info()[1]
            self.assertFalse(True, "%s raised instead of %s" %
                             (err.__class__.__name__,
                              expected.__class__.__name__))
        else:
            self.assertFalse(True,
                             "%s not raised" % expected.__class__.__name__)
            pass

        config.data = "<test/>"
        self.assertEqual(config.xdata, "<test/>")

    def test_base_xdata(self):
        config = self.get_obj()
        # we can't use assertRaises here because base_xdata is a property
        expected = Bcfg2.Server.Plugin.MetadataRuntimeError
        try:
            config.base_xdata
        except expected:
            pass
        except:
            err = sys.exc_info()[1]
            self.assertFalse(True, "%s raised instead of %s" %
                             (err.__class__.__name__,
                              expected.__class__.__name__))
        else:
            self.assertFalse(True,
                             "%s not raised" % expected.__class__.__name__)
            pass

        config.basedata = "<test/>"
        self.assertEqual(config.base_xdata, "<test/>")

    def test_add_monitor(self):
        core = MagicMock()
        config = self.get_obj(core=core)

        fname = "test.xml"
        fpath = os.path.join(self.metadata.data, fname)

        config.extras = []
        config.add_monitor(fpath)
        self.assertFalse(core.fam.AddMonitor.called)
        self.assertEqual(config.extras, [fpath])

        config = self.get_obj(core=core, watch_clients=True)
        config.add_monitor(fpath)
        core.fam.AddMonitor.assert_called_with(fpath, config.metadata)
        self.assertItemsEqual(config.extras, [fpath])

    def test_Index(self):
        # Index() isn't used on XMLMetadataConfig objects
        pass

    @patch("lxml.etree.parse")
    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig._follow_xincludes")
    def test_load_xml(self, mock_follow, mock_parse):
        config = self.get_obj("clients.xml")

        def reset():
            mock_parse.reset_mock()
            mock_follow.reset_mock()
            config.data = None
            config.basedata = None

        reset()
        config.load_xml()
        mock_follow.assert_called_with(xdata=mock_parse.return_value)
        mock_parse.assert_called_with(os.path.join(config.basedir,
                                                   "clients.xml"),
                                      parser=Bcfg2.Server.XMLParser)
        self.assertFalse(mock_parse.return_value.xinclude.called)
        self.assertEqual(config.data, mock_parse.return_value)
        self.assertIsNotNone(config.basedata)

        reset()
        mock_parse.side_effect = lxml.etree.XMLSyntaxError(None, None, None,
                                                           None)
        config.load_xml()
        mock_parse.assert_called_with(os.path.join(config.basedir,
                                                   "clients.xml"),
                                      parser=Bcfg2.Server.XMLParser)
        self.assertIsNone(config.data)
        self.assertIsNone(config.basedata)

        reset()
        mock_parse.side_effect = None
        def follow_xincludes(xdata=None):
            config.extras = [Mock(), Mock()]
        mock_follow.side_effect = follow_xincludes
        config.load_xml()
        mock_follow.assert_called_with(xdata=mock_parse.return_value)
        mock_parse.assert_called_with(os.path.join(config.basedir,
                                                   "clients.xml"),
                                      parser=Bcfg2.Server.XMLParser)
        mock_parse.return_value.xinclude.assert_any_call()
        self.assertEqual(config.data, mock_parse.return_value)
        self.assertIsNotNone(config.basedata)


    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.write_xml")
    def test_write(self, mock_write_xml):
        config = self.get_obj("clients.xml")
        config.basedata = "<test/>"
        config.write()
        mock_write_xml.assert_called_with(os.path.join(self.metadata.data,
                                                       "clients.xml"),
                                          "<test/>")

    @patch('Bcfg2.Utils.locked', Mock(return_value=False))
    @patch('fcntl.lockf', Mock())
    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml")
    @patch('os.open')
    @patch('os.fdopen')
    @patch('os.unlink')
    @patch('os.rename')
    @patch('os.path.islink')
    @patch('os.readlink')
    def test_write_xml(self, mock_readlink, mock_islink, mock_rename,
                       mock_unlink, mock_fdopen, mock_open, mock_load_xml):
        fname = "clients.xml"
        config = self.get_obj(fname)
        fpath = os.path.join(self.metadata.data, fname)
        tmpfile = "%s.new" % fpath
        linkdest = os.path.join(self.metadata.data, "client-link.xml")

        def reset():
            mock_readlink.reset_mock()
            mock_islink.reset_mock()
            mock_rename.reset_mock()
            mock_unlink.reset_mock()
            mock_fdopen.reset_mock()
            mock_open.reset_mock()
            mock_load_xml.reset_mock()

        mock_islink.return_value = False

        # basic test - everything works
        config.write_xml(fpath, get_clients_test_tree())
        mock_open.assert_called_with(tmpfile,
                                     os.O_CREAT | os.O_EXCL | os.O_WRONLY)
        mock_fdopen.assert_called_with(mock_open.return_value, 'w')
        self.assertTrue(mock_fdopen.return_value.write.called)
        mock_islink.assert_called_with(fpath)
        mock_rename.assert_called_with(tmpfile, fpath)
        mock_load_xml.assert_called_with()

        # test: clients.xml.new is locked the first time we write it
        def rv(fname, mode):
            mock_open.side_effect = None
            raise OSError(17, fname)

        reset()
        mock_open.side_effect = rv
        config.write_xml(fpath, get_clients_test_tree())
        self.assertItemsEqual(mock_open.call_args_list,
                              [call(tmpfile,
                                    os.O_CREAT | os.O_EXCL | os.O_WRONLY),
                               call(tmpfile,
                                    os.O_CREAT | os.O_EXCL | os.O_WRONLY)])
        mock_fdopen.assert_called_with(mock_open.return_value, 'w')
        self.assertTrue(mock_fdopen.return_value.write.called)
        mock_islink.assert_called_with(fpath)
        mock_rename.assert_called_with(tmpfile, fpath)
        mock_load_xml.assert_called_with()

        # test writing a symlinked clients.xml
        reset()
        mock_open.side_effect = None
        mock_islink.return_value = True
        mock_readlink.return_value = linkdest
        config.write_xml(fpath, get_clients_test_tree())
        mock_rename.assert_called_with(tmpfile, linkdest)
        mock_load_xml.assert_called_with()

        # test failure of os.rename()
        reset()
        mock_rename.side_effect = OSError
        self.assertRaises(Bcfg2.Server.Plugin.MetadataRuntimeError,
                          config.write_xml, fpath, get_clients_test_tree())
        mock_unlink.assert_called_with(tmpfile)

        # test failure of file.write()
        reset()
        mock_open.return_value.write.side_effect = IOError
        self.assertRaises(Bcfg2.Server.Plugin.MetadataRuntimeError,
                          config.write_xml, fpath, get_clients_test_tree())
        mock_unlink.assert_called_with(tmpfile)

        # test failure of os.open() (other than EEXIST)
        reset()
        mock_open.side_effect = OSError
        self.assertRaises(Bcfg2.Server.Plugin.MetadataRuntimeError,
                          config.write_xml, fpath, get_clients_test_tree())

        # test failure of os.fdopen()
        reset()
        mock_fdopen.side_effect = OSError
        self.assertRaises(Bcfg2.Server.Plugin.MetadataRuntimeError,
                          config.write_xml, fpath, get_clients_test_tree())

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    @patch('lxml.etree.parse')
    def test_find_xml_for_xpath(self, mock_parse):
        config = self.get_obj("groups.xml")
        config.basedata = get_groups_test_tree()
        xpath = "//Group[@name='group1']"
        self.assertItemsEqual(config.find_xml_for_xpath(xpath),
                              dict(filename=os.path.join(self.metadata.data,
                                                         "groups.xml"),
                                   xmltree=get_groups_test_tree(),
                                   xquery=get_groups_test_tree().xpath(xpath)))

        self.assertEqual(config.find_xml_for_xpath("//boguselement"), dict())

        config.extras = [os.path.join(self.metadata.data, p)
                         for p in ["foo.xml", "bar.xml", "clients.xml"]]

        def parse_side_effect(fname, parser=Bcfg2.Server.XMLParser):
            if fname == os.path.join(self.metadata.data, "clients.xml"):
                return get_clients_test_tree()
            else:
                return lxml.etree.XML("<null/>").getroottree()

        mock_parse.side_effect = parse_side_effect
        xpath = "//Client[@secure='true']"
        self.assertItemsEqual(config.find_xml_for_xpath(xpath),
                              dict(filename=os.path.join(self.metadata.data,
                                                         "clients.xml"),
                                   xmltree=get_clients_test_tree(),
                                   xquery=get_clients_test_tree().xpath(xpath)))

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml")
    def test_HandleEvent(self, mock_load_xml):
        config = self.get_obj("groups.xml")
        evt = Mock()
        evt.filename = os.path.join(self.metadata.data, "groups.xml")
        evt.code2str = Mock(return_value="changed")
        self.assertTrue(config.HandleEvent(evt))
        mock_load_xml.assert_called_with()


class TestClientMetadata(Bcfg2TestCase):
    def test_inGroup(self):
        cm = ClientMetadata("client1", "group1", ["group1", "group2"],
                            ["bundle1"], [], [], [], None, None, None, None)
        self.assertTrue(cm.inGroup("group1"))
        self.assertFalse(cm.inGroup("group3"))


class TestMetadata(_TestMetadata, TestClientRunHooks, TestDatabaseBacked):
    test_obj = Metadata
    use_db = False

    def get_obj(self, core=None, watch_clients=False):
        return get_metadata_object(core=core, watch_clients=watch_clients,
                                   use_db=self.use_db)

    @skipUnless(HAS_DJANGO, "Django not found")
    def test__use_db(self):
        # with the way we've set up our metadata tests, it's unweildy
        # to test _use_db.  however, given the way get_obj works, if
        # there was a bug in _use_db it'd be almost certain to shake
        # out in the rest of the testing.
        pass

    def get_nonexistent_client(self, metadata, prefix="newclient"):
        if metadata is None:
            metadata = self.load_clients_data()
        i = 0
        client_name = "%s%s" % (prefix, i)
        while client_name in metadata.clients:
            i += 1
            client_name = "%s%s" % (prefix, i)
        return client_name

    def test__init(self):
        # test with watch_clients=False
        core = MagicMock()
        metadata = self.get_obj(core=core)
        self.assertIsInstance(metadata, Bcfg2.Server.Plugin.Plugin)
        self.assertIsInstance(metadata, Bcfg2.Server.Plugin.Metadata)
        self.assertIsInstance(metadata, Bcfg2.Server.Plugin.ClientRunHooks)
        self.assertIsInstance(metadata.clients_xml, XMLMetadataConfig)
        self.assertIsInstance(metadata.groups_xml, XMLMetadataConfig)
        self.assertIsInstance(metadata.query, MetadataQuery)
        self.assertEqual(metadata.states, dict())

        # test with watch_clients=True
        core.fam = MagicMock()
        metadata = self.get_obj(core=core, watch_clients=True)
        self.assertEqual(len(metadata.states), 2)
        core.fam.AddMonitor.assert_any_call(os.path.join(metadata.data,
                                                         "groups.xml"),
                                            metadata)
        core.fam.AddMonitor.assert_any_call(os.path.join(metadata.data,
                                                         "clients.xml"),
                                            metadata)

        core.fam.reset_mock()
        core.fam.AddMonitor = Mock(side_effect=IOError)
        self.assertRaises(Bcfg2.Server.Plugin.PluginInitError,
                          self.get_obj, core=core, watch_clients=True)

    @patch('os.makedirs', Mock())
    @patch('%s.open' % builtins)
    def test_init_repo(self, mock_open):
        Metadata.init_repo(datastore,
                           groups_xml="groups", clients_xml="clients")
        mock_open.assert_any_call(os.path.join(datastore, "Metadata",
                                               "groups.xml"), "w")
        mock_open.assert_any_call(os.path.join(datastore, "Metadata",
                                               "clients.xml"), "w")

    def test_search_xdata(self):
        # test finding a node with the proper name
        metadata = self.get_obj()
        tree = get_groups_test_tree()
        res = metadata._search_xdata("Group", "group1", tree)
        self.assertIsInstance(res, lxml.etree._Element)
        self.assertEqual(res.get("name"), "group1")

        # test finding a node with the wrong name but correct alias
        metadata = self.get_obj()
        tree = get_clients_test_tree()
        res = metadata._search_xdata("Client", "alias3", tree, alias=True)
        self.assertIsInstance(res, lxml.etree._Element)
        self.assertNotEqual(res.get("name"), "alias3")

        # test failure finding a node
        metadata = self.get_obj()
        tree = get_clients_test_tree()
        res = metadata._search_xdata("Client",
                                     self.get_nonexistent_client(metadata),
                                     tree, alias=True)
        self.assertIsNone(res)

    def search_xdata(self, tag, name, tree, alias=False):
        metadata = self.get_obj()
        res = metadata._search_xdata(tag, name, tree, alias=alias)
        self.assertIsInstance(res, lxml.etree._Element)
        if not alias:
            self.assertEqual(res.get("name"), name)

    def test_search_group(self):
        # test finding a group with the proper name
        tree = get_groups_test_tree()
        self.search_xdata("Group", "group1", tree)

    def test_search_bundle(self):
        # test finding a bundle with the proper name
        tree = get_groups_test_tree()
        self.search_xdata("Bundle", "bundle1", tree)

    def test_search_client(self):
        # test finding a client with the proper name
        tree = get_clients_test_tree()
        self.search_xdata("Client", "client1", tree, alias=True)
        self.search_xdata("Client", "alias1", tree, alias=True)

    def test_add_group(self):
        metadata = self.get_obj()
        metadata.groups_xml.write = Mock()
        metadata.groups_xml.data = lxml.etree.XML('<Groups/>').getroottree()
        metadata.groups_xml.basedata = copy.copy(metadata.groups_xml.data)

        metadata.add_group("test1", dict())
        metadata.groups_xml.write.assert_any_call()
        grp = metadata.search_group("test1", metadata.groups_xml.base_xdata)
        self.assertIsNotNone(grp)
        self.assertEqual(grp.attrib, dict(name='test1'))

        # have to call this explicitly -- usually load_xml does this
        # on FAM events
        metadata.groups_xml.basedata = copy.copy(metadata.groups_xml.data)

        metadata.add_group("test2", dict(foo='bar'))
        metadata.groups_xml.write.assert_any_call()
        grp = metadata.search_group("test2", metadata.groups_xml.base_xdata)
        self.assertIsNotNone(grp)
        self.assertEqual(grp.attrib, dict(name='test2', foo='bar'))

        # have to call this explicitly -- usually load_xml does this
        # on FAM events
        metadata.groups_xml.basedata = copy.copy(metadata.groups_xml.data)

        metadata.groups_xml.write.reset_mock()
        self.assertRaises(Bcfg2.Server.Plugin.MetadataConsistencyError,
                          metadata.add_group,
                          "test1", dict())
        self.assertFalse(metadata.groups_xml.write.called)

    def test_update_group(self):
        metadata = self.get_obj()
        metadata.groups_xml.write_xml = Mock()
        metadata.groups_xml.data = copy.deepcopy(get_groups_test_tree())
        metadata.groups_xml.basedata = copy.copy(metadata.groups_xml.data)

        metadata.update_group("group1", dict(foo="bar"))
        grp = metadata.search_group("group1", metadata.groups_xml.base_xdata)
        self.assertIsNotNone(grp)
        self.assertIn("foo", grp.attrib)
        self.assertEqual(grp.get("foo"), "bar")
        self.assertTrue(metadata.groups_xml.write_xml.called)

        self.assertRaises(Bcfg2.Server.Plugin.MetadataConsistencyError,
                          metadata.update_group,
                          "bogus_group", dict())

    def test_remove_group(self):
        metadata = self.get_obj()
        metadata.groups_xml.write_xml = Mock()
        metadata.groups_xml.data = copy.deepcopy(get_groups_test_tree())
        metadata.groups_xml.basedata = copy.copy(metadata.groups_xml.data)

        metadata.remove_group("group5")
        grp = metadata.search_group("group5", metadata.groups_xml.base_xdata)
        self.assertIsNone(grp)
        self.assertTrue(metadata.groups_xml.write_xml.called)

        self.assertRaises(Bcfg2.Server.Plugin.MetadataConsistencyError,
                          metadata.remove_group,
                          "bogus_group")

    def test_add_bundle(self):
        metadata = self.get_obj()
        metadata.groups_xml.write = Mock()
        metadata.groups_xml.data = lxml.etree.XML('<Groups/>').getroottree()
        metadata.groups_xml.basedata = copy.copy(metadata.groups_xml.data)

        metadata.add_bundle("bundle1")
        metadata.groups_xml.write.assert_any_call()
        bundle = metadata.search_bundle("bundle1",
                                        metadata.groups_xml.base_xdata)
        self.assertIsNotNone(bundle)
        self.assertEqual(bundle.attrib, dict(name='bundle1'))

        # have to call this explicitly -- usually load_xml does this
        # on FAM events
        metadata.groups_xml.basedata = copy.copy(metadata.groups_xml.data)

        metadata.groups_xml.write.reset_mock()
        self.assertRaises(Bcfg2.Server.Plugin.MetadataConsistencyError,
                          metadata.add_bundle,
                          "bundle1")
        self.assertFalse(metadata.groups_xml.write.called)

    def test_remove_bundle(self):
        metadata = self.get_obj()
        metadata.groups_xml.write_xml = Mock()
        metadata.groups_xml.data = copy.deepcopy(get_groups_test_tree())
        metadata.groups_xml.basedata = copy.copy(metadata.groups_xml.data)

        metadata.remove_bundle("bundle1")
        grp = metadata.search_bundle("bundle1", metadata.groups_xml.base_xdata)
        self.assertIsNone(grp)
        self.assertTrue(metadata.groups_xml.write_xml.called)

        self.assertRaises(Bcfg2.Server.Plugin.MetadataConsistencyError,
                          metadata.remove_bundle,
                          "bogus_bundle")

    def test_add_client(self):
        metadata = self.get_obj()
        metadata.clients_xml.write = Mock()
        metadata.clients_xml.data = lxml.etree.XML('<Clients/>').getroottree()
        metadata.clients_xml.basedata = copy.copy(metadata.clients_xml.data)

        new1 = self.get_nonexistent_client(metadata)
        new1_client = metadata.add_client(new1, dict())
        metadata.clients_xml.write.assert_any_call()
        grp = metadata.search_client(new1, metadata.clients_xml.base_xdata)
        self.assertIsNotNone(grp)
        self.assertEqual(grp.attrib, dict(name=new1))

        # have to call this explicitly -- usually load_xml does this
        # on FAM events
        metadata.clients_xml.basedata = copy.copy(metadata.clients_xml.data)
        metadata._handle_clients_xml_event(Mock())

        new2 = self.get_nonexistent_client(metadata)
        metadata.add_client(new2, dict(foo='bar'))
        metadata.clients_xml.write.assert_any_call()
        grp = metadata.search_client(new2, metadata.clients_xml.base_xdata)
        self.assertIsNotNone(grp)
        self.assertEqual(grp.attrib, dict(name=new2, foo='bar'))

        # have to call this explicitly -- usually load_xml does this
        # on FAM events
        metadata.clients_xml.basedata = copy.copy(metadata.clients_xml.data)

        metadata.clients_xml.write.reset_mock()
        self.assertXMLEqual(metadata.add_client(new1, dict()),
                            new1_client)
        self.assertFalse(metadata.clients_xml.write.called)

    def test_update_client(self):
        metadata = self.get_obj()
        metadata.clients_xml.write_xml = Mock()
        metadata.clients_xml.data = copy.deepcopy(get_clients_test_tree())
        metadata.clients_xml.basedata = copy.copy(metadata.clients_xml.data)

        metadata.update_client("client1", dict(foo="bar"))
        grp = metadata.search_client("client1", metadata.clients_xml.base_xdata)
        self.assertIsNotNone(grp)
        self.assertIn("foo", grp.attrib)
        self.assertEqual(grp.get("foo"), "bar")
        self.assertTrue(metadata.clients_xml.write_xml.called)

        new = self.get_nonexistent_client(metadata)
        self.assertRaises(Bcfg2.Server.Plugin.MetadataConsistencyError,
                          metadata.update_client,
                          new, dict())

    def load_clients_data(self, metadata=None, xdata=None):
        if metadata is None:
            metadata = self.get_obj()
        metadata.clients_xml.data = \
            xdata or copy.deepcopy(get_clients_test_tree())
        metadata.clients_xml.basedata = copy.copy(metadata.clients_xml.data)
        evt = Mock()
        evt.filename = os.path.join(datastore, "Metadata", "clients.xml")
        evt.code2str = Mock(return_value="changed")
        metadata.HandleEvent(evt)
        return metadata

    def test_handle_clients_xml_event(self):
        metadata = self.get_obj()
        metadata.profiles = ["group1", "group2"]

        metadata.clients_xml = Mock()
        metadata.clients_xml.xdata = copy.deepcopy(get_clients_test_tree())
        metadata._handle_clients_xml_event(Mock())

        if not self.use_db:
            self.assertItemsEqual(metadata.clients,
                                  dict([(c.get("name"), c.get("profile"))
                                        for c in get_clients_test_tree().findall("//Client")]))
        aliases = dict([(a.get("name"), a.getparent().get("name"))
                        for a in get_clients_test_tree().findall("//Alias")])
        self.assertItemsEqual(metadata.aliases, aliases)

        raliases = dict([(c.get("name"), set())
                         for c in get_clients_test_tree().findall("//Client")])
        for alias in get_clients_test_tree().findall("//Alias"):
            raliases[alias.getparent().get("name")].add(alias.get("name"))
        self.assertItemsEqual(metadata.raliases, raliases)

        self.assertEqual(metadata.secure,
                         [c.get("name")
                          for c in get_clients_test_tree().findall("//Client[@secure='true']")])
        self.assertEqual(metadata.floating, ["client1", "client10"])

        addresses = dict([(c.get("address"), [])
                           for c in get_clients_test_tree().findall("//*[@address]")])
        raddresses = dict()
        for client in get_clients_test_tree().findall("//Client[@address]"):
            addresses[client.get("address")].append(client.get("name"))
            try:
                raddresses[client.get("name")].append(client.get("address"))
            except KeyError:
                raddresses[client.get("name")] = [client.get("address")]
        for alias in get_clients_test_tree().findall("//Alias[@address]"):
            addresses[alias.get("address")].append(alias.getparent().get("name"))
            try:
                raddresses[alias.getparent().get("name")].append(alias.get("address"))
            except KeyError:
                raddresses[alias.getparent().get("name")] = alias.get("address")

        self.assertItemsEqual(metadata.addresses, addresses)
        self.assertItemsEqual(metadata.raddresses, raddresses)
        self.assertTrue(metadata.states['clients.xml'])

    def load_groups_data(self, metadata=None, xdata=None):
        if metadata is None:
            metadata = self.get_obj()
        metadata.groups_xml.data = \
            xdata or copy.deepcopy(get_groups_test_tree())
        metadata.groups_xml.basedata = copy.copy(metadata.groups_xml.data)
        evt = Mock()
        evt.filename = os.path.join(datastore, "Metadata", "groups.xml")
        evt.code2str = Mock(return_value="changed")
        metadata.HandleEvent(evt)
        return metadata

    def test_handle_groups_xml_event(self):
        metadata = self.get_obj()
        metadata.groups_xml = Mock()
        metadata.groups_xml.xdata = get_groups_test_tree()
        metadata._handle_groups_xml_event(Mock())

        self.assertTrue(metadata.states['groups.xml'])
        self.assertTrue(metadata.groups['group1'].is_public)
        self.assertTrue(metadata.groups['group2'].is_public)
        self.assertFalse(metadata.groups['group3'].is_public)
        self.assertTrue(metadata.groups['group1'].is_profile)
        self.assertTrue(metadata.groups['group2'].is_profile)
        self.assertFalse(metadata.groups['group3'].is_profile)
        self.assertItemsEqual(metadata.groups.keys(),
                              set(g.get("name")
                                  for g in get_groups_test_tree().findall("//Group")))
        self.assertEqual(metadata.groups['group1'].category, 'category1')
        self.assertEqual(metadata.groups['group2'].category, 'category1')
        self.assertEqual(metadata.groups['group3'].category, 'category2')
        self.assertEqual(metadata.groups['group4'].category, 'category1')
        self.assertEqual(metadata.default, "group1")

        all_groups = set()
        negated_groups = set()
        for group in get_groups_test_tree().xpath("//Groups/Client//*") + \
                get_groups_test_tree().xpath("//Groups/Group//*"):
            if group.tag == 'Group' and not group.getchildren():
                if group.get("negate", "false").lower() == 'true':
                    negated_groups.add(group.get("name"))
                else:
                    all_groups.add(group.get("name"))
        self.assertItemsEqual(metadata.ordered_groups, all_groups)
        self.assertItemsEqual(metadata.group_membership.keys(), all_groups)
        self.assertItemsEqual(metadata.negated_groups.keys(), negated_groups)

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    def test_set_profile(self):
        metadata = self.get_obj()
        if 'clients.xml' in metadata.states:
            metadata.states['clients.xml'] = False
            self.assertRaises(Bcfg2.Server.Plugin.MetadataRuntimeError,
                              metadata.set_profile,
                              None, None, None)

        self.load_groups_data(metadata=metadata)
        self.load_clients_data(metadata=metadata)

        self.assertRaises(Bcfg2.Server.Plugin.MetadataConsistencyError,
                          metadata.set_profile,
                          "client1", "group5", None)

        self.assertRaises(Bcfg2.Server.Plugin.MetadataConsistencyError,
                          metadata.set_profile,
                          "client1", "group3", None)

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    def test_set_profile_db(self):
        metadata = self.load_clients_data(metadata=self.load_groups_data())
        if metadata._use_db:
            profile = "group1"
            client_name = self.get_nonexistent_client(metadata)
            metadata.set_profile(client_name, profile, None)
            self.assertIn(client_name, metadata.clients)
            self.assertRaises(Bcfg2.Server.Plugin.PluginExecutionError,
                              metadata.set_profile,
                              client_name, profile, None)

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    @patch("Bcfg2.Server.Plugins.Metadata.Metadata.add_client")
    @patch("Bcfg2.Server.Plugins.Metadata.Metadata.update_client")
    def test_set_profile_xml(self, mock_update_client, mock_add_client):
        metadata = self.load_clients_data(metadata=self.load_groups_data())
        if not metadata._use_db:
            metadata.clients_xml.write = Mock()
            metadata.core.build_metadata = Mock()
            metadata.core.build_metadata.side_effect = \
                lambda c: metadata.get_initial_metadata(c)

            metadata.set_profile("client1", "group2", None)
            mock_update_client.assert_called_with("client1",
                                                  dict(profile="group2"))
            self.assertEqual(metadata.clientgroups["client1"], ["group2"])

            metadata.clients_xml.write.reset_mock()
            new1 = self.get_nonexistent_client(metadata)
            metadata.set_profile(new1, "group1", None)
            mock_add_client.assert_called_with(new1, dict(profile="group1"))
            metadata.clients_xml.write.assert_any_call()
            self.assertEqual(metadata.clientgroups[new1], ["group1"])

            metadata.clients_xml.write.reset_mock()
            new2 = self.get_nonexistent_client(metadata)
            metadata.session_cache[('1.2.3.6', None)] = (None, new2)
            metadata.set_profile("uuid_new", "group1", ('1.2.3.6', None))
            mock_add_client.assert_called_with(new2,
                                               dict(uuid='uuid_new',
                                                    profile="group1",
                                                    address='1.2.3.6'))
            metadata.clients_xml.write.assert_any_call()
            self.assertEqual(metadata.clientgroups["uuid_new"], ["group1"])

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    @patch("socket.getnameinfo")
    def test_resolve_client(self, mock_getnameinfo):
        metadata = self.load_clients_data(metadata=self.load_groups_data())
        metadata.session_cache[('1.2.3.3', None)] = (time.time(), 'client3')
        self.assertEqual(metadata.resolve_client(('1.2.3.3', None)), 'client3')

        self.assertRaises(Bcfg2.Server.Plugin.MetadataConsistencyError,
                          metadata.resolve_client,
                          ('1.2.3.2', None))
        self.assertEqual(metadata.resolve_client(('1.2.3.1', None)), 'client1')

        metadata.session_cache[('1.2.3.3', None)] = (time.time() - 100,
                                                     'client3')
        self.assertEqual(metadata.resolve_client(('1.2.3.3', None)), 'client3')
        self.assertEqual(metadata.resolve_client(('1.2.3.3', None),
                                                 cleanup_cache=True), 'client3')
        self.assertEqual(metadata.session_cache, dict())

        mock_getnameinfo.return_value = ('client6', [], ['1.2.3.6'])
        self.assertEqual(metadata.resolve_client(('1.2.3.6', 6789)), 'client6')
        mock_getnameinfo.assert_called_with(('1.2.3.6', 6789), socket.NI_NAMEREQD)

        mock_getnameinfo.reset_mock()
        mock_getnameinfo.return_value = ('alias3', [], ['1.2.3.7'])
        self.assertEqual(metadata.resolve_client(('1.2.3.7', 6789)), 'client4')
        mock_getnameinfo.assert_called_with(('1.2.3.7', 6789), socket.NI_NAMEREQD)

        mock_getnameinfo.reset_mock()
        mock_getnameinfo.return_value = None
        mock_getnameinfo.side_effect = socket.herror
        self.assertRaises(Bcfg2.Server.Plugin.MetadataConsistencyError,
                          metadata.resolve_client,
                          ('1.2.3.8', 6789))
        mock_getnameinfo.assert_called_with(('1.2.3.8', 6789), socket.NI_NAMEREQD)

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.write_xml", Mock())
    @patch("Bcfg2.Server.Plugins.Metadata.ClientMetadata")
    def test_get_initial_metadata(self, mock_clientmetadata):
        metadata = self.get_obj()
        if 'clients.xml' in metadata.states:
            metadata.states['clients.xml'] = False
            self.assertRaises(Bcfg2.Server.Plugin.MetadataRuntimeError,
                              metadata.get_initial_metadata, None)

        self.load_groups_data(metadata=metadata)
        self.load_clients_data(metadata=metadata)

        # test address, password
        metadata.get_initial_metadata("client1")
        mock_clientmetadata.assert_called_with("client1", "group1",
                                               set(["group1"]), set(), set(),
                                               set(["1.2.3.1"]),
                                               dict(category1='group1'), None,
                                               'password2', None,
                                               metadata.query)

        # test address, bundles, category suppression
        metadata.get_initial_metadata("client2")
        mock_clientmetadata.assert_called_with("client2", "group2",
                                               set(["group2"]),
                                               set(["bundle1", "bundle2"]),
                                               set(), set(["1.2.3.2"]),
                                               dict(category1="group2"),
                                               None, None, None,
                                               metadata.query)

        # test aliases, address, uuid, password
        imd = metadata.get_initial_metadata("alias1")
        mock_clientmetadata.assert_called_with("client3", "group1",
                                               set(["group1"]), set(),
                                               set(['alias1']),
                                               set(["1.2.3.3"]),
                                               dict(category1="group1"),
                                               'uuid1', 'password2', None,
                                               metadata.query)

        # test new client creation
        new1 = self.get_nonexistent_client(metadata)
        imd = metadata.get_initial_metadata(new1)
        mock_clientmetadata.assert_called_with(new1, "group1", set(["group1"]),
                                               set(), set(), set(),
                                               dict(category1="group1"), None,
                                               None, None, metadata.query)

        # test nested groups, address, per-client groups
        imd = metadata.get_initial_metadata("client8")
        mock_clientmetadata.assert_called_with("client8", "group1",
                                               set(["group1", "group8",
                                                    "group9", "group10"]),
                                               set(),
                                               set(), set(["1.2.3.5"]),
                                               dict(category1="group1"),
                                               None, None, None, metadata.query)

        # test setting per-client groups, group negation, nested groups
        imd = metadata.get_initial_metadata("client9")
        mock_clientmetadata.assert_called_with("client9", "group2",
                                               set(["group2", "group8",
                                                    "group11"]),
                                               set(["bundle1", "bundle2"]),
                                               set(), set(),
                                               dict(category1="group2"), None,
                                               "password3", None,
                                               metadata.query)

        # test new client with no default profile
        metadata.default = None
        new2 = self.get_nonexistent_client(metadata)
        self.assertRaises(Bcfg2.Server.Plugin.MetadataConsistencyError,
                          metadata.get_initial_metadata, new2)

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    def test_merge_groups(self):
        metadata = self.get_obj()
        self.load_groups_data(metadata=metadata)
        self.load_clients_data(metadata=metadata)

        self.assertEqual(metadata._merge_groups("client1", set(["group1"]),
                                                categories=dict(group1="category1")),
                         (set(["group1"]), dict(group1="category1")))

        self.assertEqual(metadata._merge_groups("client8",
                                                set(["group1", "group8", "group9"]),
                                                categories=dict(group1="category1")),
                         (set(["group1", "group8", "group9", "group10"]),
                          dict(group1="category1")))

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    def test_get_all_group_names(self):
        metadata = self.load_groups_data()
        self.assertItemsEqual(metadata.get_all_group_names(),
                              set([g.get("name")
                                   for g in get_groups_test_tree().findall("//Group")]))

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    def test_get_all_groups_in_category(self):
        metadata = self.load_groups_data()
        self.assertItemsEqual(metadata.get_all_groups_in_category("category1"),
                              set([g.get("name")
                                   for g in get_groups_test_tree().findall("//Group[@category='category1']")]))

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    def test_get_client_names_by_profiles(self):
        metadata = self.load_clients_data(metadata=self.load_groups_data())
        metadata.core.build_metadata = Mock()
        metadata.core.build_metadata.side_effect = \
            lambda c: metadata.get_initial_metadata(c)
        self.assertItemsEqual(metadata.get_client_names_by_profiles(["group2"]),
                              [c.get("name")
                               for c in get_clients_test_tree().findall("//Client[@profile='group2']")])

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    def test_get_client_names_by_groups(self):
        metadata = self.load_clients_data(metadata=self.load_groups_data())
        # this is not the best test in the world, since we mock
        # core.build_metadata to just build _initial_ metadata, which
        # is not at all the same thing.  it turns out that mocking
        # this out without starting a Bcfg2 server is pretty
        # non-trivial, so this works-ish
        metadata.core.build_metadata = Mock()
        metadata.core.build_metadata.side_effect = \
            lambda c: metadata.get_initial_metadata(c)
        self.assertItemsEqual(metadata.get_client_names_by_groups(["group2"]),
                              [c.get("name")
                               for c in get_clients_test_tree().findall("//Client[@profile='group2']")])

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    def test_merge_additional_groups(self):
        metadata = self.load_clients_data(metadata=self.load_groups_data())
        imd = metadata.get_initial_metadata("client2")

        # test adding a group excluded by categories
        oldgroups = imd.groups
        metadata.merge_additional_groups(imd, ["group4"])
        self.assertEqual(imd.groups, oldgroups)

        # test adding a private group
        oldgroups = imd.groups
        metadata.merge_additional_groups(imd, ["group3"])
        self.assertEqual(imd.groups, oldgroups)

        # test adding groups with bundles
        oldgroups = imd.groups
        oldbundles = imd.bundles
        metadata.merge_additional_groups(imd, ["group7"])
        self.assertEqual(imd.groups, oldgroups.union(["group7"]))
        self.assertEqual(imd.bundles, oldbundles.union(["bundle3"]))

        # test adding groups with categories
        oldgroups = imd.groups
        metadata.merge_additional_groups(imd, ["group12"])
        self.assertEqual(imd.groups, oldgroups.union(["group12"]))
        self.assertIn("category3", imd.categories)
        self.assertEqual(imd.categories["category3"], "group12")

        # test adding multiple groups
        imd = metadata.get_initial_metadata("client2")
        oldgroups = imd.groups
        metadata.merge_additional_groups(imd, ["group6", "group8"])
        self.assertItemsEqual(imd.groups,
                              oldgroups.union(["group6", "group8", "group9"]))

        # test adding a group that is not defined in groups.xml
        imd = metadata.get_initial_metadata("client2")
        oldgroups = imd.groups
        metadata.merge_additional_groups(imd, ["group6", "newgroup"])
        self.assertItemsEqual(imd.groups,
                              oldgroups.union(["group6", "newgroup"]))

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    def test_merge_additional_data(self):
        metadata = self.load_clients_data(metadata=self.load_groups_data())
        imd = metadata.get_initial_metadata("client1")

        # we need to use a unique attribute name for this test.  this
        # is probably overkill, but it works
        pattern = "connector%d"
        for i in range(0, 100):
            connector = pattern % i
            if not hasattr(imd, connector):
                break
        self.assertFalse(hasattr(imd, connector),
                         "Could not find unique connector name to test "
                         "merge_additional_data()")

        metadata.merge_additional_data(imd, connector, "test data")
        self.assertEqual(getattr(imd, connector), "test data")
        self.assertIn(connector, imd.connectors)

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    @patch("Bcfg2.Server.Plugins.Metadata.Metadata.resolve_client")
    def test_validate_client_address(self, mock_resolve_client):
        metadata = self.load_clients_data(metadata=self.load_groups_data())
        self.assertTrue(metadata.validate_client_address("client1",
                                                         (None, None)))
        self.assertTrue(metadata.validate_client_address("client2",
                                                         ("1.2.3.2", None)))
        self.assertFalse(metadata.validate_client_address("client2",
                                                          ("1.2.3.8", None)))
        self.assertTrue(metadata.validate_client_address("client4",
                                                         ("1.2.3.2", None)))
        # this is upper case to ensure that case is folded properly in
        # validate_client_address()
        mock_resolve_client.return_value = "CLIENT4"
        self.assertTrue(metadata.validate_client_address("client4",
                                                         ("1.2.3.7", None)))
        mock_resolve_client.assert_called_with(("1.2.3.7", None))

        mock_resolve_client.reset_mock()
        self.assertFalse(metadata.validate_client_address("client5",
                                                         ("1.2.3.5", None)))

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    @patch("Bcfg2.Server.Plugins.Metadata.Metadata.validate_client_address")
    @patch("Bcfg2.Server.Plugins.Metadata.Metadata.resolve_client")
    def test_AuthenticateConnection(self, mock_resolve_client,
                                    mock_validate_client_address):
        metadata = self.load_clients_data(metadata=self.load_groups_data())
        metadata.password = "password1"

        cert = dict(subject=[[("commonName", "client1")]])
        mock_validate_client_address.return_value = False
        self.assertFalse(metadata.AuthenticateConnection(cert, "root", None,
                                                         "1.2.3.1"))
        mock_validate_client_address.return_value = True
        self.assertTrue(metadata.AuthenticateConnection(cert, "root", None,
                                                        "1.2.3.1"))
        # floating cert-auth clients add themselves to the cache
        self.assertIn("1.2.3.1", metadata.session_cache)
        self.assertEqual(metadata.session_cache["1.2.3.1"][1], "client1")

        cert = dict(subject=[[("commonName", "client7")]])
        self.assertTrue(metadata.AuthenticateConnection(cert, "root", None,
                                                        "1.2.3.4"))
        # non-floating cert-auth clients do not add themselves to the cache
        self.assertNotIn("1.2.3.4", metadata.session_cache)

        cert = dict(subject=[[("commonName", "client8")]])

        mock_resolve_client.return_value = "client5"
        self.assertTrue(metadata.AuthenticateConnection(None, "root",
                                                        "password1", "1.2.3.8"))

        mock_resolve_client.side_effect = \
            Bcfg2.Server.Plugin.MetadataConsistencyError
        self.assertFalse(metadata.AuthenticateConnection(None, "root",
                                                         "password1",
                                                         "1.2.3.8"))

        # secure mode, no password
        self.assertFalse(metadata.AuthenticateConnection(None, 'client2', None,
                                                         "1.2.3.2"))

        self.assertTrue(metadata.AuthenticateConnection(None, 'uuid1',
                                                        "password1", "1.2.3.3"))
        # non-root, non-cert clients populate session cache
        self.assertIn("1.2.3.3", metadata.session_cache)
        self.assertEqual(metadata.session_cache["1.2.3.3"][1], "client3")

        # use alternate password
        self.assertTrue(metadata.AuthenticateConnection(None, 'client3',
                                                        "password2", "1.2.3.3"))

        # test secure mode
        self.assertFalse(metadata.AuthenticateConnection(None, 'client9',
                                                         "password1",
                                                         "1.2.3.9"))
        self.assertTrue(metadata.AuthenticateConnection(None, 'client9',
                                                        "password3", "1.2.3.9"))

        self.assertFalse(metadata.AuthenticateConnection(None, "client5",
                                                         "password2",
                                                         "1.2.3.7"))

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    @patch("Bcfg2.Server.Plugins.Metadata.Metadata.update_client")
    def test_end_statistics(self, mock_update_client):
        metadata = self.load_clients_data(metadata=self.load_groups_data())
        md = Mock()
        md.hostname = "client6"
        metadata.end_statistics(md)
        mock_update_client.assert_called_with(md.hostname, dict(auth='cert'))

        mock_update_client.reset_mock()
        md.hostname = "client5"
        metadata.end_statistics(md)
        self.assertFalse(mock_update_client.called)

    def test_viz(self):
        pass


class TestMetadataBase(TestMetadata):
    """ base test object for testing Metadata with database enabled """
    __test__ = False
    use_db = True

    @skipUnless(HAS_DJANGO, "Django not found")
    def setUp(self):
        syncdb(TestMetadataDB)

    def load_clients_data(self, metadata=None, xdata=None):
        if metadata is None:
            metadata = get_obj()
        for client in get_clients_test_tree().findall("Client"):
            metadata.add_client(client.get("name"))
        return metadata

    def get_nonexistent_client(self, _, prefix="newclient"):
        clients = [o.hostname for o in MetadataClientModel.objects.all()]
        i = 0
        client_name = "%s%s" % (prefix, i)
        while client_name in clients:
            i += 1
            client_name = "%s%s" % (prefix, i)
        return client_name

    @patch('os.path.exists')
    def test__init(self, mock_exists):
        core = MagicMock()
        core.fam = Mock()
        mock_exists.return_value = False
        metadata = self.get_obj(core=core, watch_clients=True)
        self.assertIsInstance(metadata, Bcfg2.Server.Plugin.DatabaseBacked)
        core.fam.AddMonitor.assert_called_once_with(os.path.join(metadata.data,
                                                                 "groups.xml"),
                                                    metadata)

        mock_exists.return_value = True
        core.fam.reset_mock()
        metadata = self.get_obj(core=core, watch_clients=True)
        core.fam.AddMonitor.assert_any_call(os.path.join(metadata.data,
                                                         "groups.xml"),
                                            metadata)
        core.fam.AddMonitor.assert_any_call(os.path.join(metadata.data,
                                                         "clients.xml"),
                                            metadata)

    def test_add_group(self):
        pass

    def test_add_bundle(self):
        pass

    def test_add_client(self):
        metadata = self.get_obj()
        hostname = self.get_nonexistent_client(metadata)
        client = metadata.add_client(hostname)
        self.assertIsInstance(client, MetadataClientModel)
        self.assertEqual(client.hostname, hostname)
        self.assertIn(hostname, metadata.clients)
        self.assertIn(hostname, metadata.list_clients())
        self.assertItemsEqual(metadata.clients,
                              [c.hostname
                               for c in MetadataClientModel.objects.all()])

    def test_update_group(self):
        pass

    def test_update_bundle(self):
        pass

    def test_update_client(self):
        pass

    def test_list_clients(self):
        metadata = self.get_obj()
        self.assertItemsEqual(metadata.list_clients(),
                              [c.hostname
                               for c in MetadataClientModel.objects.all()])

    def test_remove_group(self):
        pass

    def test_remove_bundle(self):
        pass

    def test_remove_client(self):
        metadata = self.get_obj()
        client_name = self.get_nonexistent_client(metadata)

        self.assertRaises(Bcfg2.Server.Plugin.MetadataConsistencyError,
                          metadata.remove_client,
                          client_name)

        metadata.add_client(client_name)
        metadata.remove_client(client_name)
        self.assertNotIn(client_name, metadata.clients)
        self.assertNotIn(client_name, metadata.list_clients())
        self.assertItemsEqual(metadata.clients,
                              [c.hostname
                               for c in MetadataClientModel.objects.all()])

    def test_process_statistics(self):
        pass


class TestMetadata_NoClientsXML(TestMetadataBase):
    """ test Metadata without a clients.xml. we have to disable or
    override tests that rely on client options """
    # only run these tests if it's possible to skip tests or if we
    # have django.  otherwise they'll all get run because our fake
    # skipping decorators for python < 2.7 won't work when they
    # decorate setUp()
    if can_skip or HAS_DJANGO:
        __test__ = True

    def load_groups_data(self, metadata=None, xdata=None):
        if metadata is None:
            metadata = self.get_obj()
        if not xdata:
            xdata = copy.deepcopy(get_groups_test_tree())
            for client in get_clients_test_tree().findall("Client"):
                newclient = \
                    lxml.etree.SubElement(xdata.getroot(),
                                          "Client", name=client.get("name"))
                lxml.etree.SubElement(newclient, "Group",
                                      name=client.get("profile"))
        metadata.groups_xml.data = xdata
        metadata.groups_xml.basedata = copy.copy(metadata.groups_xml.data)
        evt = Mock()
        evt.filename = os.path.join(datastore, "Metadata", "groups.xml")
        evt.code2str = Mock(return_value="changed")
        metadata.HandleEvent(evt)
        return metadata

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.write_xml", Mock())
    @patch("Bcfg2.Server.Plugins.Metadata.ClientMetadata")
    def test_get_initial_metadata(self, mock_clientmetadata):
        metadata = self.get_obj()
        if 'clients.xml' in metadata.states:
            metadata.states['clients.xml'] = False
            self.assertRaises(Bcfg2.Server.Plugin.MetadataRuntimeError,
                              metadata.get_initial_metadata, None)

        self.load_groups_data(metadata=metadata)
        self.load_clients_data(metadata=metadata)

        # test basic client metadata
        metadata.get_initial_metadata("client1")
        mock_clientmetadata.assert_called_with("client1", "group1",
                                               set(["group1"]), set(), set(),
                                               set(), dict(category1='group1'),
                                               None, None, None, metadata.query)

        # test bundles, category suppression
        metadata.get_initial_metadata("client2")
        mock_clientmetadata.assert_called_with("client2", "group2",
                                               set(["group2"]),
                                               set(["bundle1", "bundle2"]),
                                               set(), set(),
                                               dict(category1="group2"), None,
                                               None, None, metadata.query)

        # test new client creation
        new1 = self.get_nonexistent_client(metadata)
        imd = metadata.get_initial_metadata(new1)
        mock_clientmetadata.assert_called_with(new1, "group1", set(["group1"]),
                                               set(), set(), set(),
                                               dict(category1="group1"), None,
                                               None, None, metadata.query)

        # test nested groups, per-client groups
        imd = metadata.get_initial_metadata("client8")
        mock_clientmetadata.assert_called_with("client8", "group1",
                                               set(["group1", "group8",
                                                    "group9", "group10"]),
                                               set(), set(), set(),
                                               dict(category1="group1"), None,
                                               None, None, metadata.query)

        # test per-client groups, group negation, nested groups
        imd = metadata.get_initial_metadata("client9")
        mock_clientmetadata.assert_called_with("client9", "group2",
                                               set(["group2", "group8",
                                                    "group11"]),
                                               set(["bundle1", "bundle2"]),
                                               set(), set(),
                                               dict(category1="group2"), None,
                                               None, None, metadata.query)

        # test exception on new client with no default profile
        metadata.default = None
        new2 = self.get_nonexistent_client(metadata)
        self.assertRaises(Bcfg2.Server.Plugin.MetadataConsistencyError,
                          metadata.get_initial_metadata,
                          new2)

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    @patch("Bcfg2.Server.Plugins.Metadata.Metadata.resolve_client")
    def test_validate_client_address(self, mock_resolve_client):
        metadata = self.load_clients_data(metadata=self.load_groups_data())
        # this is upper case to ensure that case is folded properly in
        # validate_client_address()
        mock_resolve_client.return_value = "CLIENT4"
        self.assertTrue(metadata.validate_client_address("client4",
                                                         ("1.2.3.7", None)))
        mock_resolve_client.assert_called_with(("1.2.3.7", None))

        mock_resolve_client.reset_mock()
        self.assertFalse(metadata.validate_client_address("client5",
                                                         ("1.2.3.5", None)))

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    @patch("Bcfg2.Server.Plugins.Metadata.Metadata.validate_client_address")
    @patch("Bcfg2.Server.Plugins.Metadata.Metadata.resolve_client")
    def test_AuthenticateConnection(self, mock_resolve_client,
                                    mock_validate_client_address):
        metadata = self.load_clients_data(metadata=self.load_groups_data())
        metadata.password = "password1"

        cert = dict(subject=[[("commonName", "client1")]])
        mock_validate_client_address.return_value = False
        self.assertFalse(metadata.AuthenticateConnection(cert, "root", None,
                                                         "1.2.3.1"))
        mock_validate_client_address.return_value = True
        self.assertTrue(metadata.AuthenticateConnection(cert, "root",
                                                        metadata.password,
                                                        "1.2.3.1"))

        cert = dict(subject=[[("commonName", "client8")]])

        mock_resolve_client.return_value = "client5"
        self.assertTrue(metadata.AuthenticateConnection(None, "root",
                                                        "password1", "1.2.3.8"))

        mock_resolve_client.side_effect = \
            Bcfg2.Server.Plugin.MetadataConsistencyError
        self.assertFalse(metadata.AuthenticateConnection(None, "root",
                                                         "password1",
                                                         "1.2.3.8"))

    @patch("Bcfg2.Server.Plugins.Metadata.XMLMetadataConfig.load_xml", Mock())
    @patch("socket.getnameinfo")
    def test_resolve_client(self, mock_getnameinfo):
        metadata = self.load_clients_data(metadata=self.load_groups_data())
        metadata.session_cache[('1.2.3.3', None)] = (time.time(), 'client3')
        self.assertEqual(metadata.resolve_client(('1.2.3.3', None)), 'client3')

        metadata.session_cache[('1.2.3.3', None)] = (time.time() - 100,
                                                     'client3')
        mock_getnameinfo.return_value = ("client3", [], ['1.2.3.3'])
        self.assertEqual(metadata.resolve_client(('1.2.3.3', None),
                                                 cleanup_cache=True), 'client3')
        self.assertEqual(metadata.session_cache, dict())

        mock_getnameinfo.return_value = ('client6', [], ['1.2.3.6'])
        self.assertEqual(metadata.resolve_client(('1.2.3.6', 6789), socket.NI_NAMEREQD), 'client6')
        mock_getnameinfo.assert_called_with(('1.2.3.6', 6789), socket.NI_NAMEREQD)

        mock_getnameinfo.reset_mock()
        mock_getnameinfo.return_value = None
        mock_getnameinfo.side_effect = socket.herror
        self.assertRaises(Bcfg2.Server.Plugin.MetadataConsistencyError,
                          metadata.resolve_client,
                          ('1.2.3.8', 6789), socket.NI_NAMEREQD)
        mock_getnameinfo.assert_called_with(('1.2.3.8', 6789), socket.NI_NAMEREQD)

    def test_handle_clients_xml_event(self):
        pass

    def test_end_statistics(self):
        # bootstrap mode, which is what is being tested here, doesn't
        # work without clients.xml
        pass

class TestMetadata_ClientsXML(TestMetadataBase):
    """ test Metadata with a clients.xml.  """
    # only run these tests if it's possible to skip tests or if we
    # have django.  otherwise they'll all get run because our fake
    # skipping decorators for python < 2.7 won't work when they
    # decorate setUp()
    if can_skip or HAS_DJANGO:
        __test__ = True

    def load_clients_data(self, metadata=None, xdata=None):
        if metadata is None:
            metadata = self.get_obj()
        metadata.core.fam = Mock()
        @patchIf(not isinstance(lxml.etree.Element, Mock),
                 "lxml.etree.Element", Mock())
        def inner():
            metadata.clients_xml = metadata._handle_file("clients.xml")
        inner()
        metadata = TestMetadata.load_clients_data(self, metadata=metadata,
                                                  xdata=xdata)
        return TestMetadataBase.load_clients_data(self, metadata=metadata,
                                                    xdata=xdata)

########NEW FILE########
__FILENAME__ = TestProbes
import os
import re
import sys
import copy
import time
import lxml.etree
import Bcfg2.version
import Bcfg2.Server
import Bcfg2.Server.Plugin
from mock import Mock, MagicMock, patch

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from Bcfg2.Server.Plugins.Probes import *
from TestPlugin import TestEntrySet, TestProbing, TestConnector, \
    TestDatabaseBacked

# test data for JSON and YAML tests
test_data = dict(a=1, b=[1, 2, 3], c="test",
                 d=dict(a=1, b=dict(a=1), c=(1, "2", 3)))


class FakeElement(lxml.etree._Element):
     getroottree = Mock()

     def __init__(self, el):
         self._element = el

     def __getattribute__(self, attr):
         el = lxml.etree._Element.__getattribute__(self,
                                                   '__dict__')['_element']
         if attr == "getroottree":
             return FakeElement.getroottree
         elif attr == "_element":
             return el
         else:
             return getattr(el, attr)


class StoringElement(object):
    OriginalElement = copy.copy(lxml.etree.Element)

    def __init__(self):
        self.element = None
        self.return_value = None

    def __call__(self, *args, **kwargs):
        self.element = self.OriginalElement(*args, **kwargs)
        self.return_value = FakeElement(self.element)
        return self.return_value


class StoringSubElement(object):
    OriginalSubElement = copy.copy(lxml.etree.SubElement)

    def __call__(self, parent, tag, **kwargs):
        try:
            return self.OriginalSubElement(parent._element, tag,
                                           **kwargs)
        except AttributeError:
            return self.OriginalSubElement(parent, tag, **kwargs)


class FakeList(list):
    pass


class TestProbesDB(DBModelTestCase):
    if HAS_DJANGO:
        models = [ProbesGroupsModel, ProbesDataModel]


class TestClientProbeDataSet(Bcfg2TestCase):
    def test__init(self):
        ds = ClientProbeDataSet()
        self.assertLessEqual(ds.timestamp, time.time())
        self.assertIsInstance(ds, dict)
        self.assertNotIn("timestamp", ds)

        ds = ClientProbeDataSet(timestamp=123)
        self.assertEqual(ds.timestamp, 123)
        self.assertNotIn("timestamp", ds)


class TestProbeData(Bcfg2TestCase):
    def test_str(self):
        # a value that is not valid XML, JSON, or YAML
        val = "'test"

        # test string behavior
        data = ProbeData(val)
        self.assertIsInstance(data, str)
        self.assertEqual(data, val)
        # test 1.2.0-1.2.2 broken behavior
        self.assertEqual(data.data, val)
        # test that formatted data accessors return None
        self.assertIsNone(data.xdata)
        self.assertIsNone(data.yaml)
        self.assertIsNone(data.json)

    def test_xdata(self):
        xdata = lxml.etree.Element("test")
        lxml.etree.SubElement(xdata, "test2")
        data = ProbeData(lxml.etree.tostring(xdata,
                                             xml_declaration=False).decode('UTF-8'))
        self.assertIsNotNone(data.xdata)
        self.assertIsNotNone(data.xdata.find("test2"))

    @skipUnless(HAS_JSON, "JSON libraries not found, skipping JSON tests")
    def test_json(self):
        jdata = json.dumps(test_data)
        data = ProbeData(jdata)
        self.assertIsNotNone(data.json)
        self.assertItemsEqual(test_data, data.json)

    @skipUnless(HAS_YAML, "YAML libraries not found, skipping YAML tests")
    def test_yaml(self):
        jdata = yaml.dump(test_data)
        data = ProbeData(jdata)
        self.assertIsNotNone(data.yaml)
        self.assertItemsEqual(test_data, data.yaml)


class TestProbeSet(TestEntrySet):
    test_obj = ProbeSet
    basenames = ["test", "_test", "test-test"]
    ignore = ["foo~", ".#foo", ".foo.swp", ".foo.swx", "probed.xml"]
    bogus_names = ["test.py"]

    def get_obj(self, path=datastore, fam=None, encoding=None,
                plugin_name="Probes", basename=None):
        # get_obj() accepts the basename argument, accepted by the
        # parent get_obj() method, and just throws it away, since
        # ProbeSet uses a regex for the "basename"
        if fam is None:
            fam = Mock()
        rv = self.test_obj(path, fam, encoding, plugin_name)
        rv.entry_type = MagicMock()
        return rv

    def test__init(self):
        fam = Mock()
        ps = self.get_obj(fam=fam)
        self.assertEqual(ps.plugin_name, "Probes")
        fam.AddMonitor.assert_called_with(datastore, ps)
        TestEntrySet.test__init(self)

    def test_HandleEvent(self):
        ps = self.get_obj()
        ps.handle_event = Mock()

        # test that events on the data store itself are skipped
        evt = Mock()
        evt.filename = datastore
        ps.HandleEvent(evt)
        self.assertFalse(ps.handle_event.called)

        # test that events on probed.xml are skipped
        evt.reset_mock()
        evt.filename = "probed.xml"
        ps.HandleEvent(evt)
        self.assertFalse(ps.handle_event.called)

        # test that other events are processed appropriately
        evt.reset_mock()
        evt.filename = "fooprobe"
        ps.HandleEvent(evt)
        ps.handle_event.assert_called_with(evt)

    @patch("%s.list" % builtins, FakeList)
    def test_get_probe_data(self):
        ps = self.get_obj()

        # build some fairly complex test data for this.  in the end,
        # we want the probe data to include only the most specific
        # version of a given probe, and by basename only, not full
        # (specific) name. We don't fully test the specificity stuff,
        # we just check to make sure sort() is called and trust that
        # sort() does the right thing on Specificity objects.  (I.e.,
        # trust that Specificity is well-tested. Hah!)  We also test
        # to make sure the interpreter is determined correctly.
        ps.get_matching = Mock()
        matching = FakeList()
        matching.sort = Mock()

        p1 = Mock()
        p1.specific = Bcfg2.Server.Plugin.Specificity(group=True, prio=10)
        p1.name = "fooprobe.G10_foogroup"
        p1.data = """#!/bin/bash
group-specific"""
        matching.append(p1)

        p2 = Mock()
        p2.specific = Bcfg2.Server.Plugin.Specificity(all=True)
        p2.name = "fooprobe"
        p2.data = "#!/bin/bash"
        matching.append(p2)

        p3 = Mock()
        p3.specific = Bcfg2.Server.Plugin.Specificity(all=True)
        p3.name = "barprobe"
        p3.data = "#! /usr/bin/env python"
        matching.append(p3)

        p4 = Mock()
        p4.specific = Bcfg2.Server.Plugin.Specificity(all=True)
        p4.name = "bazprobe"
        p4.data = ""
        matching.append(p4)

        ps.get_matching.return_value = matching

        metadata = Mock()
        metadata.version_info = \
            Bcfg2.version.Bcfg2VersionInfo(Bcfg2.version.__version__)
        pdata = ps.get_probe_data(metadata)
        ps.get_matching.assert_called_with(metadata)
        # we can't create a matching operator.attrgetter object, and I
        # don't feel the need to mock that out -- this is a good
        # enough check
        self.assertTrue(matching.sort.called)

        self.assertEqual(len(pdata), 3,
                         "Found: %s" % [p.get("name") for p in pdata])
        for probe in pdata:
            if probe.get("name") == "fooprobe":
                self.assertIn("group-specific", probe.text)
                self.assertEqual(probe.get("interpreter"), "/bin/bash")
            elif probe.get("name") == "barprobe":
                self.assertEqual(probe.get("interpreter"),
                                 "/usr/bin/env python")
            elif probe.get("name") == "bazprobe":
                self.assertIsNotNone(probe.get("interpreter"))
            else:
                assert False, "Strange probe found in get_probe_data() return"


class TestProbes(TestProbing, TestConnector, TestDatabaseBacked):
    test_obj = Probes

    def get_obj(self, core=None):
        return TestDatabaseBacked.get_obj(self, core=core)

    def get_test_probedata(self):
        test_xdata = lxml.etree.Element("test")
        lxml.etree.SubElement(test_xdata, "test", foo="foo")
        rv = dict()
        rv["foo.example.com"] = ClientProbeDataSet(timestamp=time.time())
        rv["foo.example.com"]["xml"] = \
            ProbeData(lxml.etree.tostring(test_xdata,
                                          xml_declaration=False).decode('UTF-8'))
        rv["foo.example.com"]["text"] = ProbeData("freeform text")
        rv["foo.example.com"]["multiline"] = ProbeData("""multiple
lines
of
freeform
text
""")
        rv["bar.example.com"] = ClientProbeDataSet(timestamp=time.time())
        rv["bar.example.com"]["empty"] = ProbeData("")
        if HAS_JSON:
            rv["bar.example.com"]["json"] = ProbeData(json.dumps(test_data))
        if HAS_YAML:
            rv["bar.example.com"]["yaml"] = ProbeData(yaml.dump(test_data))
        return rv

    def get_test_cgroups(self):
        return {"foo.example.com": ["group", "group with spaces",
                                    "group-with-dashes"],
                "bar.example.com": []}

    def get_probes_object(self, use_db=False, load_data=None):
        core = MagicMock()
        core.setup.cfp.getboolean = Mock()
        core.setup.cfp.getboolean.return_value = use_db
        if load_data is None:
            load_data = MagicMock()
        # we have to patch load_data() in a funny way because
        # different versions of Mock have different scopes for
        # patching.  in some versions, a patch applied to
        # get_probes_object() would only apply to that function, while
        # in others it would also apply to the calling function (e.g.,
        # test__init(), which relies on being able to check the calls
        # of load_data(), and thus on load_data() being consistently
        # mocked)
        @patch("%s.%s.load_data" % (self.test_obj.__module__,
                                    self.test_obj.__name__), new=load_data)
        def inner():
            return self.get_obj(core)

        rv = inner()
        rv.allowed_cgroups = [re.compile("^.*$")]
        return rv

    def test__init(self):
        mock_load_data = Mock()
        probes = self.get_probes_object(load_data=mock_load_data)
        probes.core.fam.AddMonitor.assert_called_with(os.path.join(datastore,
                                                                   probes.name),
                                                      probes.probes)
        mock_load_data.assert_any_call()
        self.assertEqual(probes.probedata, ClientProbeDataSet())
        self.assertEqual(probes.cgroups, dict())

    @patch("Bcfg2.Server.Plugins.Probes.Probes.load_data", Mock())
    def test__use_db(self):
        probes = self.get_probes_object()
        self.assertFalse(probes._use_db)
        probes.core.setup.cfp.getboolean.assert_called_with("probes",
                                                            "use_database",
                                                            default=False)

    @skipUnless(HAS_DJANGO, "Django not found, skipping")
    @patch("Bcfg2.Server.Plugins.Probes.Probes._write_data_db", Mock())
    @patch("Bcfg2.Server.Plugins.Probes.Probes._write_data_xml", Mock())
    def test_write_data_xml(self):
        probes = self.get_probes_object(use_db=False)
        probes.write_data("test")
        probes._write_data_xml.assert_called_with("test")
        self.assertFalse(probes._write_data_db.called)

    @skipUnless(HAS_DJANGO, "Django not found, skipping")
    @patch("Bcfg2.Server.Plugins.Probes.Probes._write_data_db", Mock())
    @patch("Bcfg2.Server.Plugins.Probes.Probes._write_data_xml", Mock())
    def test_write_data_db(self):
        probes = self.get_probes_object(use_db=True)
        probes.write_data("test")
        probes._write_data_db.assert_called_with("test")
        self.assertFalse(probes._write_data_xml.called)

    def test__write_data_xml(self):
        probes = self.get_probes_object(use_db=False)
        probes.probedata = self.get_test_probedata()
        probes.cgroups = self.get_test_cgroups()

        @patch("lxml.etree.Element")
        @patch("lxml.etree.SubElement", StoringSubElement())
        def inner(mock_Element):
            mock_Element.side_effect = StoringElement()
            probes._write_data_xml(None)

            top = mock_Element.side_effect.return_value
            write = top.getroottree.return_value.write
            self.assertEqual(write.call_args[0][0],
                             os.path.join(datastore, probes.name,
                                          "probed.xml"))

            data = top._element
            foodata = data.find("Client[@name='foo.example.com']")
            self.assertIsNotNone(foodata)
            self.assertIsNotNone(foodata.get("timestamp"))
            self.assertEqual(len(foodata.findall("Probe")),
                             len(probes.probedata['foo.example.com']))
            self.assertEqual(len(foodata.findall("Group")),
                             len(probes.cgroups['foo.example.com']))
            xml = foodata.find("Probe[@name='xml']")
            self.assertIsNotNone(xml)
            self.assertIsNotNone(xml.get("value"))
            xdata = lxml.etree.XML(xml.get("value"))
            self.assertIsNotNone(xdata)
            self.assertIsNotNone(xdata.find("test"))
            self.assertEqual(xdata.find("test").get("foo"), "foo")
            text = foodata.find("Probe[@name='text']")
            self.assertIsNotNone(text)
            self.assertIsNotNone(text.get("value"))
            multiline = foodata.find("Probe[@name='multiline']")
            self.assertIsNotNone(multiline)
            self.assertIsNotNone(multiline.get("value"))
            self.assertGreater(len(multiline.get("value").splitlines()), 1)

            bardata = data.find("Client[@name='bar.example.com']")
            self.assertIsNotNone(bardata)
            self.assertIsNotNone(bardata.get("timestamp"))
            self.assertEqual(len(bardata.findall("Probe")),
                             len(probes.probedata['bar.example.com']))
            self.assertEqual(len(bardata.findall("Group")),
                             len(probes.cgroups['bar.example.com']))
            empty = bardata.find("Probe[@name='empty']")
            self.assertIsNotNone(empty)
            self.assertIsNotNone(empty.get("value"))
            self.assertEqual(empty.get("value"), "")
            if HAS_JSON:
                jdata = bardata.find("Probe[@name='json']")
                self.assertIsNotNone(jdata)
                self.assertIsNotNone(jdata.get("value"))
                self.assertItemsEqual(test_data,
                                      json.loads(jdata.get("value")))
            if HAS_YAML:
                ydata = bardata.find("Probe[@name='yaml']")
                self.assertIsNotNone(ydata)
                self.assertIsNotNone(ydata.get("value"))
                self.assertItemsEqual(test_data,
                                      yaml.load(ydata.get("value")))

        inner()

    @skipUnless(HAS_DJANGO, "Django not found, skipping")
    def test__write_data_db(self):
        syncdb(TestProbesDB)
        probes = self.get_probes_object(use_db=True)
        probes.probedata = self.get_test_probedata()
        probes.cgroups = self.get_test_cgroups()

        for cname in ["foo.example.com", "bar.example.com"]:
            client = Mock()
            client.hostname = cname
            probes._write_data_db(client)

            pdata = ProbesDataModel.objects.filter(hostname=cname).all()
            self.assertEqual(len(pdata), len(probes.probedata[cname]))

            for probe in pdata:
                self.assertEqual(probe.hostname, client.hostname)
                self.assertIsNotNone(probe.data)
                if probe.probe == "xml":
                    xdata = lxml.etree.XML(probe.data)
                    self.assertIsNotNone(xdata)
                    self.assertIsNotNone(xdata.find("test"))
                    self.assertEqual(xdata.find("test").get("foo"), "foo")
                elif probe.probe == "text":
                    pass
                elif probe.probe == "multiline":
                    self.assertGreater(len(probe.data.splitlines()), 1)
                elif probe.probe == "empty":
                    self.assertEqual(probe.data, "")
                elif probe.probe == "yaml":
                    self.assertItemsEqual(test_data, yaml.load(probe.data))
                elif probe.probe == "json":
                    self.assertItemsEqual(test_data, json.loads(probe.data))
                else:
                    assert False, "Strange probe found in _write_data_db data"

            pgroups = ProbesGroupsModel.objects.filter(hostname=cname).all()
            self.assertEqual(len(pgroups), len(probes.cgroups[cname]))

        # test that old probe data is removed properly
        cname = 'foo.example.com'
        del probes.probedata[cname]['text']
        probes.cgroups[cname].pop()
        client = Mock()
        client.hostname = cname
        probes._write_data_db(client)

        pdata = ProbesDataModel.objects.filter(hostname=cname).all()
        self.assertEqual(len(pdata), len(probes.probedata[cname]))
        pgroups = ProbesGroupsModel.objects.filter(hostname=cname).all()
        self.assertEqual(len(pgroups), len(probes.cgroups[cname]))

    @skipUnless(HAS_DJANGO, "Django not found, skipping")
    @patch("Bcfg2.Server.Plugins.Probes.Probes._load_data_db", Mock())
    @patch("Bcfg2.Server.Plugins.Probes.Probes._load_data_xml", Mock())
    def test_load_data_xml(self):
        probes = self.get_probes_object(use_db=False)
        probes.load_data()
        probes._load_data_xml.assert_any_call()
        self.assertFalse(probes._load_data_db.called)

    @skipUnless(HAS_DJANGO, "Django not found, skipping")
    @patch("Bcfg2.Server.Plugins.Probes.Probes._load_data_db", Mock())
    @patch("Bcfg2.Server.Plugins.Probes.Probes._load_data_xml", Mock())
    def test_load_data_db(self):
        probes = self.get_probes_object(use_db=True)
        probes.load_data()
        probes._load_data_db.assert_any_call(client=None)
        self.assertFalse(probes._load_data_xml.called)

    @patch("lxml.etree.parse")
    def test__load_data_xml(self, mock_parse):
        probes = self.get_probes_object(use_db=False)
        probes.probedata = self.get_test_probedata()
        probes.cgroups = self.get_test_cgroups()

        # to get the value for lxml.etree.parse to parse, we call
        # _write_data_xml, mock the lxml.etree._ElementTree.write()
        # call, and grab the data that gets "written" to probed.xml
        @patch("lxml.etree.Element")
        @patch("lxml.etree.SubElement", StoringSubElement())
        def inner(mock_Element):
            mock_Element.side_effect = StoringElement()
            probes._write_data_xml(None)
            top = mock_Element.side_effect.return_value
            return top._element

        xdata = inner()
        mock_parse.return_value = xdata.getroottree()
        probes.probedata = dict()
        probes.cgroups = dict()

        probes._load_data_xml()
        mock_parse.assert_called_with(os.path.join(datastore, probes.name,
                                                   'probed.xml'),
                                      parser=Bcfg2.Server.XMLParser)
        self.assertItemsEqual(probes.probedata, self.get_test_probedata())
        self.assertItemsEqual(probes.cgroups, self.get_test_cgroups())

    @skipUnless(HAS_DJANGO, "Django not found, skipping")
    def test__load_data_db(self):
        syncdb(TestProbesDB)
        probes = self.get_probes_object(use_db=True)
        probes.probedata = self.get_test_probedata()
        probes.cgroups = self.get_test_cgroups()
        for cname in probes.probedata.keys():
            client = Mock()
            client.hostname = cname
            probes._write_data_db(client)

        probes.probedata = dict()
        probes.cgroups = dict()
        probes._load_data_db()
        self.assertItemsEqual(probes.probedata, self.get_test_probedata())
        # the db backend does not store groups at all if a client has
        # no groups set, so we can't just use assertItemsEqual here,
        # because loading saved data may _not_ result in the original
        # data if some clients had no groups set.
        test_cgroups = self.get_test_cgroups()
        for cname, groups in test_cgroups.items():
            if cname in probes.cgroups:
                self.assertEqual(groups, probes.cgroups[cname])
            else:
                self.assertEqual(groups, [])

    @patch("Bcfg2.Server.Plugins.Probes.ProbeSet.get_probe_data")
    def test_GetProbes(self, mock_get_probe_data):
        probes = self.get_probes_object()
        metadata = Mock()
        probes.GetProbes(metadata)
        mock_get_probe_data.assert_called_with(metadata)

    @patch("Bcfg2.Server.Plugins.Probes.Probes.write_data")
    @patch("Bcfg2.Server.Plugins.Probes.Probes.ReceiveDataItem")
    def test_ReceiveData(self, mock_ReceiveDataItem, mock_write_data):
        # we use a simple (read: bogus) datalist here to make this
        # easy to test
        datalist = ["a", "b", "c"]

        probes = self.get_probes_object()
        probes.core.metadata_cache_mode = 'off'
        client = Mock()
        client.hostname = "foo.example.com"
        probes.ReceiveData(client, datalist)

        cgroups = []
        cprobedata = ClientProbeDataSet()
        self.assertItemsEqual(mock_ReceiveDataItem.call_args_list,
                              [call(client, "a", cgroups, cprobedata),
                               call(client, "b", cgroups, cprobedata),
                               call(client, "c", cgroups, cprobedata)])
        mock_write_data.assert_called_with(client)
        self.assertFalse(probes.core.metadata_cache.expire.called)

        # change the datalist, ensure that the cache is cleared
        probes.cgroups[client.hostname] = datalist
        probes.core.metadata_cache_mode = 'aggressive'
        probes.ReceiveData(client, ['a', 'b', 'd'])

        mock_write_data.assert_called_with(client)
        probes.core.metadata_cache.expire.assert_called_with(client.hostname)

    def test_ReceiveDataItem(self):
        probes = self.get_probes_object()
        for cname, cdata in self.get_test_probedata().items():
            client = Mock()
            client.hostname = cname
            cgroups = []
            cprobedata = ClientProbeDataSet()
            for pname, pdata in cdata.items():
                dataitem = lxml.etree.Element("Probe", name=pname)
                if pname == "text":
                    # add some groups to the plaintext test to test
                    # group parsing
                    data = [pdata]
                    for group in self.get_test_cgroups()[cname]:
                        data.append("group:%s" % group)
                    dataitem.text = "\n".join(data)
                else:
                    dataitem.text = str(pdata)

                probes.ReceiveDataItem(client, dataitem, cgroups, cprobedata)

            probes.cgroups[client.hostname] = cgroups
            probes.probedata[client.hostname] = cprobedata
            self.assertIn(client.hostname, probes.probedata)
            self.assertIn(pname, probes.probedata[cname])
            self.assertEqual(pdata, probes.probedata[cname][pname])
            self.assertIn(client.hostname, probes.cgroups)
            self.assertEqual(probes.cgroups[cname],
                             self.get_test_cgroups()[cname])

        # test again, with an explicit list of allowed groups
        probes.allowed_cgroups = [re.compile(r'^.*s$')]
        for cname, cdata in self.get_test_probedata().items():
            client = Mock()
            client.hostname = cname
            cgroups = []
            cprobedata = ClientProbeDataSet()
            for pname, pdata in cdata.items():
                dataitem = lxml.etree.Element("Probe", name=pname)
                if pname == "text":
                    # add some groups to the plaintext test to test
                    # group parsing
                    data = [pdata]
                    for group in self.get_test_cgroups()[cname]:
                        data.append("group:%s" % group)
                    dataitem.text = "\n".join(data)
                else:
                    dataitem.text = str(pdata)

                probes.ReceiveDataItem(client, dataitem, cgroups, cprobedata)

            probes.cgroups[client.hostname] = cgroups
            probes.probedata[client.hostname] = cprobedata
            self.assertIn(client.hostname, probes.probedata)
            self.assertIn(pname, probes.probedata[cname])
            self.assertEqual(pdata, probes.probedata[cname][pname])
            self.assertIn(client.hostname, probes.cgroups)
            self.assertEqual(probes.cgroups[cname],
                             [g for g in self.get_test_cgroups()[cname]
                              if g.endswith("s")])

    def test_get_additional_groups(self):
        TestConnector.test_get_additional_groups(self)

        probes = self.get_probes_object()
        test_cgroups = self.get_test_cgroups()
        probes.cgroups = self.get_test_cgroups()
        for cname in test_cgroups.keys():
            metadata = Mock()
            metadata.hostname = cname
            self.assertEqual(test_cgroups[cname],
                             probes.get_additional_groups(metadata))
        # test a non-existent client
        metadata = Mock()
        metadata.hostname = "nonexistent"
        self.assertEqual(probes.get_additional_groups(metadata),
                         list())

    def test_get_additional_data(self):
        TestConnector.test_get_additional_data(self)

        probes = self.get_probes_object()
        test_probedata = self.get_test_probedata()
        probes.probedata = self.get_test_probedata()
        for cname in test_probedata.keys():
            metadata = Mock()
            metadata.hostname = cname
            self.assertEqual(test_probedata[cname],
                             probes.get_additional_data(metadata))
        # test a non-existent client
        metadata = Mock()
        metadata.hostname = "nonexistent"
        self.assertEqual(probes.get_additional_data(metadata),
                         ClientProbeDataSet())

########NEW FILE########
__FILENAME__ = TestProperties
import os
import sys
import lxml.etree
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugins.Properties import *
from Bcfg2.Server.Plugin import PluginExecutionError

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestPlugin import TestStructFile, TestFileBacked, TestConnector, \
    TestPlugin, TestDirectoryBacked

try:
    from Bcfg2.Encryption import EVPError
    HAS_CRYPTO = True
except:
    HAS_CRYPTO = False

try:
    import json
    JSON = "json"
except ImportError:
    JSON = "simplejson"


class TestPropertyFile(Bcfg2TestCase):
    test_obj = PropertyFile
    path = os.path.join(datastore, "test")

    def get_obj(self, path=None):
        if path is None:
            path = self.path
        return self.test_obj(path)

    def test_write(self):
        Bcfg2.Server.Plugins.Properties.SETUP = MagicMock()
        pf = self.get_obj()
        pf.validate_data = Mock()
        pf._write = Mock()

        xstr = u("<Properties/>\n")
        pf.xdata = lxml.etree.XML(xstr)

        def reset():
            pf.validate_data.reset_mock()
            pf._write.reset_mock()
            Bcfg2.Server.Plugins.Properties.SETUP.reset_mock()

        # test writes disabled
        Bcfg2.Server.Plugins.Properties.SETUP.cfp.getboolean.return_value = False
        self.assertRaises(PluginExecutionError, pf.write)
        self.assertFalse(pf.validate_data.called)
        self.assertFalse(pf._write.called)
        Bcfg2.Server.Plugins.Properties.SETUP.cfp.getboolean.assert_called_with("properties",
                                                "writes_enabled",
                                                default=True)

        # test successful write
        reset()
        Bcfg2.Server.Plugins.Properties.SETUP.cfp.getboolean.return_value = True
        self.assertEqual(pf.write(), pf._write.return_value)
        pf.validate_data.assert_called_with()
        pf._write.assert_called_with()

        # test error from _write
        reset()
        pf._write.side_effect = IOError
        self.assertRaises(PluginExecutionError, pf.write)
        pf.validate_data.assert_called_with()
        pf._write.assert_called_with()

        # test error from validate_data
        reset()
        pf.validate_data.side_effect = PluginExecutionError
        self.assertRaises(PluginExecutionError, pf.write)
        pf.validate_data.assert_called_with()

    def test__write(self):
        pf = self.get_obj()
        self.assertRaises(NotImplementedError, pf._write)

    def test_validate_data(self):
        pf = self.get_obj()
        self.assertRaises(NotImplementedError, pf.validate_data)

    @patch("copy.copy")
    def test_get_additional_data(self, mock_copy):
        pf = self.get_obj()
        self.assertEqual(pf.get_additional_data(Mock()),
                         mock_copy.return_value)
        mock_copy.assert_called_with(pf)


if can_skip or HAS_JSON:
    class TestJSONPropertyFile(TestFileBacked, TestPropertyFile):
        test_obj = JSONPropertyFile

        def get_obj(self, *args, **kwargs):
            return TestFileBacked.get_obj(self, *args, **kwargs)

        @skipUnless(HAS_JSON, "JSON libraries not found, skipping")
        def setUp(self):
            pass

        @patch("%s.loads" % JSON)
        def test_Index(self, mock_loads):
            pf = self.get_obj()
            pf.Index()
            mock_loads.assert_called_with(pf.data)
            self.assertEqual(pf.json, mock_loads.return_value)

            mock_loads.reset_mock()
            mock_loads.side_effect = ValueError
            self.assertRaises(PluginExecutionError, pf.Index)
            mock_loads.assert_called_with(pf.data)

        @patch("%s.dump" % JSON)
        @patch("%s.open" % builtins)
        def test__write(self, mock_open, mock_dump):
            pf = self.get_obj()
            self.assertTrue(pf._write())
            mock_open.assert_called_with(pf.name, 'wb')
            mock_dump.assert_called_with(pf.json, mock_open.return_value)

        @patch("%s.dumps" % JSON)
        def test_validate_data(self, mock_dumps):
            pf = self.get_obj()
            pf.validate_data()
            mock_dumps.assert_called_with(pf.json)

            mock_dumps.reset_mock()
            mock_dumps.side_effect = ValueError
            self.assertRaises(PluginExecutionError, pf.validate_data)
            mock_dumps.assert_called_with(pf.json)


if can_skip or HAS_YAML:
    class TestYAMLPropertyFile(TestFileBacked, TestPropertyFile):
        test_obj = YAMLPropertyFile

        def get_obj(self, *args, **kwargs):
            return TestFileBacked.get_obj(self, *args, **kwargs)

        @skipUnless(HAS_YAML, "YAML libraries not found, skipping")
        def setUp(self):
            pass

        @patch("yaml.load")
        def test_Index(self, mock_load):
            pf = self.get_obj()
            pf.Index()
            mock_load.assert_called_with(pf.data)
            self.assertEqual(pf.yaml, mock_load.return_value)

            mock_load.reset_mock()
            mock_load.side_effect = yaml.YAMLError
            self.assertRaises(PluginExecutionError, pf.Index)
            mock_load.assert_called_with(pf.data)

        @patch("yaml.dump")
        @patch("%s.open" % builtins)
        def test__write(self, mock_open, mock_dump):
            pf = self.get_obj()
            self.assertTrue(pf._write())
            mock_open.assert_called_with(pf.name, 'wb')
            mock_dump.assert_called_with(pf.yaml, mock_open.return_value)

        @patch("yaml.dump")
        def test_validate_data(self, mock_dump):
            pf = self.get_obj()
            pf.validate_data()
            mock_dump.assert_called_with(pf.yaml)

            mock_dump.reset_mock()
            mock_dump.side_effect = yaml.YAMLError
            self.assertRaises(PluginExecutionError, pf.validate_data)
            mock_dump.assert_called_with(pf.yaml)


class TestXMLPropertyFile(TestPropertyFile, TestStructFile):
    test_obj = XMLPropertyFile
    path = TestStructFile.path

    def get_obj(self, *args, **kwargs):
        return TestStructFile.get_obj(self, *args, **kwargs)

    @patch("%s.open" % builtins)
    def test__write(self, mock_open):
        pf = self.get_obj()
        pf.xdata = lxml.etree.Element("Test")
        self.assertTrue(pf._write())
        mock_open.assert_called_with(pf.name, "wb")
        self.assertXMLEqual(pf.xdata,
                            lxml.etree.XML(mock_open.return_value.write.call_args[0][0]))

    @patch("os.path.exists")
    @patch("lxml.etree.XMLSchema")
    def test_validate_data(self, mock_XMLSchema, mock_exists):
        pf = self.get_obj()
        pf.name = os.path.join(datastore, "Properties", "test.xml")
        schemafile = os.path.join(datastore, "Properties", "test.xsd")

        def reset():
            mock_XMLSchema.reset_mock()
            mock_exists.reset_mock()

        # test no schema file
        mock_exists.return_value = False
        self.assertTrue(pf.validate_data())
        mock_exists.assert_called_with(schemafile)

        # test schema file exists, valid data
        reset()
        mock_exists.return_value = True
        mock_XMLSchema.return_value = Mock()
        mock_XMLSchema.return_value.validate.return_value = True
        self.assertTrue(pf.validate_data())
        mock_exists.assert_called_with(schemafile)
        mock_XMLSchema.assert_called_with(file=schemafile)
        mock_XMLSchema.return_value.validate.assert_called_with(pf.xdata)

        # test schema file exists, invalid data
        reset()
        mock_XMLSchema.return_value = Mock()
        mock_XMLSchema.return_value.validate.return_value = False
        self.assertRaises(PluginExecutionError, pf.validate_data)
        mock_exists.assert_called_with(schemafile)
        mock_XMLSchema.assert_called_with(file=schemafile)
        mock_XMLSchema.return_value.validate.assert_called_with(pf.xdata)

        # test invalid schema file
        reset()
        mock_XMLSchema.side_effect = lxml.etree.XMLSchemaParseError(pf.xdata)
        self.assertRaises(PluginExecutionError, pf.validate_data)
        mock_exists.assert_called_with(schemafile)
        mock_XMLSchema.assert_called_with(file=schemafile)

    def test_Index(self):
        TestStructFile.test_Index(self)

        pf = self.get_obj()
        pf.xdata = lxml.etree.Element("Properties")
        lxml.etree.SubElement(pf.xdata, "Crypted", encrypted="foo")
        pf.data = lxml.etree.tostring(pf.xdata)

    @skipUnless(HAS_CRYPTO, "No crypto libraries found, skipping")
    def test_Index_crypto(self):
        pf = self.get_obj()
        pf._decrypt = Mock()
        pf._decrypt.return_value = 'plaintext'
        pf.data = '''
<Properties decrypt="strict">
  <Crypted encrypted="foo">
    crypted
    <Plain foo="bar">plain</Plain>
  </Crypted>
  <Crypted encrypted="bar">crypted</Crypted>
  <Plain bar="baz">plain</Plain>
  <Plain>
    <Crypted encrypted="foo">crypted</Crypted>
  </Plain>
</Properties>'''

        # test successful decryption
        pf.Index()
        self.assertItemsEqual(pf._decrypt.call_args_list,
                              [call(el) for el in pf.xdata.xpath("//Crypted")])
        for el in pf.xdata.xpath("//Crypted"):
            self.assertEqual(el.text, pf._decrypt.return_value)

        # test failed decryption, strict
        pf._decrypt.reset_mock()
        pf._decrypt.side_effect = EVPError
        self.assertRaises(PluginExecutionError, pf.Index)

        # test failed decryption, lax
        pf.data = pf.data.replace("strict", "lax")
        pf._decrypt.reset_mock()
        pf.Index()
        self.assertItemsEqual(pf._decrypt.call_args_list,
                              [call(el) for el in pf.xdata.xpath("//Crypted")])

    @skipUnless(HAS_CRYPTO, "No crypto libraries found, skipping")
    @patchIf(HAS_CRYPTO, "Bcfg2.Encryption.ssl_decrypt")
    @patchIf(HAS_CRYPTO, "Bcfg2.Encryption.get_algorithm")
    @patchIf(HAS_CRYPTO, "Bcfg2.Encryption.get_passphrases")
    def test_decrypt(self, mock_get_passphrases,
                     mock_get_algorithm, mock_ssl):
        pf = self.get_obj()
        Bcfg2.Server.Plugins.Properties.SETUP = MagicMock()

        def reset():
            mock_get_algorithm.reset_mock()
            mock_get_passphrases.reset_mock()
            mock_ssl.reset_mock()

        # test element without text contents
        self.assertIsNone(pf._decrypt(lxml.etree.Element("Test")))
        self.assertFalse(mock_get_passphrases.called)
        self.assertFalse(mock_ssl.called)

        # test element with a passphrase in the config file
        reset()
        el = lxml.etree.Element("Test", encrypted="foo")
        el.text = "crypted"
        mock_get_passphrases.return_value = dict(foo="foopass",
                                                 bar="barpass")
        mock_get_algorithm.return_value = "bf_cbc"
        mock_ssl.return_value = "decrypted with ssl"
        self.assertEqual(pf._decrypt(el), mock_ssl.return_value)
        mock_get_passphrases.assert_called_with(
            Bcfg2.Server.Plugins.Properties.SETUP)
        mock_get_algorithm.assert_called_with(
            Bcfg2.Server.Plugins.Properties.SETUP)
        mock_ssl.assert_called_with(el.text, "foopass",
                                    algorithm="bf_cbc")

        # test failure to decrypt element with a passphrase in the config
        reset()
        mock_ssl.side_effect = EVPError
        self.assertRaises(EVPError, pf._decrypt, el)
        mock_get_passphrases.assert_called_with(
            Bcfg2.Server.Plugins.Properties.SETUP)
        mock_get_algorithm.assert_called_with(
            Bcfg2.Server.Plugins.Properties.SETUP)
        mock_ssl.assert_called_with(el.text, "foopass",
                                    algorithm="bf_cbc")

        # test element without valid passphrase
        reset()
        el.set("encrypted", "true")
        self.assertRaises(EVPError, pf._decrypt, el)
        self.assertFalse(mock_ssl.called)

    @patch("copy.copy")
    def test_get_additional_data(self, mock_copy):
        Bcfg2.Server.Plugins.Properties.SETUP = Mock()
        pf = self.get_obj()
        pf.XMLMatch = Mock()
        metadata = Mock()

        def reset():
            mock_copy.reset_mock()
            pf.XMLMatch.reset_mock()
            Bcfg2.Server.Plugins.Properties.SETUP.reset_mock()

        pf.xdata = lxml.etree.Element("Properties", automatch="true")
        for automatch in [True, False]:
            reset()
            Bcfg2.Server.Plugins.Properties.SETUP.cfp.getboolean.return_value = automatch
            self.assertEqual(pf.get_additional_data(metadata),
                             pf.XMLMatch.return_value)
            pf.XMLMatch.assert_called_with(metadata)
            Bcfg2.Server.Plugins.Properties.SETUP.cfp.getboolean.assert_called_with("properties", "automatch", default=False)
            self.assertFalse(mock_copy.called)

        pf.xdata = lxml.etree.Element("Properties", automatch="false")
        for automatch in [True, False]:
            reset()
            Bcfg2.Server.Plugins.Properties.SETUP.cfp.getboolean.return_value = automatch
            self.assertEqual(pf.get_additional_data(metadata),
                             mock_copy.return_value)
            mock_copy.assert_called_with(pf)
            self.assertFalse(pf.XMLMatch.called)
            Bcfg2.Server.Plugins.Properties.SETUP.cfp.getboolean.assert_called_with("properties", "automatch", default=False)

        pf.xdata = lxml.etree.Element("Properties")
        reset()
        Bcfg2.Server.Plugins.Properties.SETUP.cfp.getboolean.return_value = False
        self.assertEqual(pf.get_additional_data(metadata),
                         mock_copy.return_value)
        mock_copy.assert_called_with(pf)
        self.assertFalse(pf.XMLMatch.called)
        Bcfg2.Server.Plugins.Properties.SETUP.cfp.getboolean.assert_called_with("properties", "automatch", default=False)

        reset()
        Bcfg2.Server.Plugins.Properties.SETUP.cfp.getboolean.return_value = True
        self.assertEqual(pf.get_additional_data(metadata),
                         pf.XMLMatch.return_value)
        pf.XMLMatch.assert_called_with(metadata)
        Bcfg2.Server.Plugins.Properties.SETUP.cfp.getboolean.assert_called_with("properties", "automatch", default=False)
        self.assertFalse(mock_copy.called)


class TestProperties(TestPlugin, TestConnector, TestDirectoryBacked):
    test_obj = Properties
    testfiles = ['foo.xml', 'bar.baz.xml']
    if HAS_JSON:
        testfiles.extend(["foo.json", "foo.xml.json"])
    if HAS_YAML:
        testfiles.extend(["foo.yaml", "foo.yml", "foo.xml.yml"])
    ignore = ['foo.xsd', 'bar.baz.xsd', 'quux.xml.xsd']
    badevents = ['bogus.txt']

    def get_obj(self, core=None):
        @patch("%s.%s.add_directory_monitor" % (self.test_obj.__module__,
                                                self.test_obj.__name__),
               Mock())
        def inner():
            return TestPlugin.get_obj(self, core=core)
        return inner()

    @patch("copy.copy")
    def test_get_additional_data(self, mock_copy):
        TestConnector.test_get_additional_data(self)

        p = self.get_obj()
        metadata = Mock()
        p.entries = {"foo.xml": Mock(),
                     "foo.yml": Mock()}
        rv = p.get_additional_data(metadata)
        expected = dict()
        for name, entry in p.entries.items():
            entry.get_additional_data.assert_called_with(metadata)
            expected[name] = entry.get_additional_data.return_value
        self.assertItemsEqual(rv, expected)

########NEW FILE########
__FILENAME__ = TestRules
import os
import sys
import lxml.etree
import Bcfg2.Server.Plugin
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugins.Rules import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestPlugin import TestPrioDir


class TestRules(TestPrioDir):
    test_obj = Rules

    def test_HandlesEntry(self):
        r = self.get_obj()
        r.Entries = dict(Path={"/etc/foo.conf": Mock(),
                               "/etc/bar.conf": Mock()})
        r._matches = Mock()
        metadata = Mock()

        entry = lxml.etree.Element("Path", name="/etc/foo.conf")
        self.assertEqual(r.HandlesEntry(entry, metadata),
                         r._matches.return_value)
        r._matches.assert_called_with(entry, metadata,
                                      r.Entries['Path'].keys())

        r._matches.reset_mock()
        entry = lxml.etree.Element("Path", name="/etc/baz.conf")
        self.assertEqual(r.HandlesEntry(entry, metadata),
                         r._matches.return_value)
        r._matches.assert_called_with(entry, metadata,
                                      r.Entries['Path'].keys())

        r._matches.reset_mock()
        entry = lxml.etree.Element("Package", name="foo")
        self.assertFalse(r.HandlesEntry(entry, metadata))

    def test_BindEntry(self, method="BindEntry"):
        r = self.get_obj()
        r.get_attrs = Mock()
        r.get_attrs.return_value = dict(overwrite="new", add="add",
                                        text="text")
        entry = lxml.etree.Element("Test", overwrite="old", keep="keep")
        metadata = Mock()

        getattr(r, method)(entry, metadata)
        r.get_attrs.assert_called_with(entry, metadata)
        self.assertItemsEqual(entry.attrib,
                              dict(overwrite="old", add="add", keep="keep",
                                   text="text"))

    def test_HandleEntry(self):
        self.test_BindEntry(method="HandleEntry")

    @patch("Bcfg2.Server.Plugin.PrioDir._matches")
    def test__matches(self, mock_matches):
        """ test _matches() behavior regardless of state of _regex_enabled """
        r = self.get_obj()
        metadata = Mock()

        entry = lxml.etree.Element("Path", name="/etc/foo.conf")
        rules = []
        mock_matches.return_value = True
        self.assertTrue(r._matches(entry, metadata, rules))
        mock_matches.assert_called_with(r, entry, metadata, rules)

        # test special Path cases -- adding and removing trailing slash
        mock_matches.reset_mock()
        mock_matches.return_value = False
        rules = ["/etc/foo/", "/etc/bar"]
        entry = lxml.etree.Element("Path", name="/etc/foo")
        self.assertTrue(r._matches(entry, metadata, rules))
        mock_matches.assert_called_with(r, entry, metadata, rules)

        mock_matches.reset_mock()
        entry = lxml.etree.Element("Path", name="/etc/bar/")
        self.assertTrue(r._matches(entry, metadata, rules))
        mock_matches.assert_called_with(r, entry, metadata, rules)

    @patch("Bcfg2.Server.Plugin.PrioDir._matches")
    def test__matches_regex_disabled(self, mock_matches):
        """ test failure to match with regex disabled """
        r = self.get_obj()
        self.set_regex_enabled(r, False)
        metadata = Mock()
        mock_matches.return_value = False

        entry = lxml.etree.Element("Path", name="/etc/foo.conf")
        rules = []
        self.assertFalse(r._matches(entry, metadata, rules))
        mock_matches.assert_called_with(r, entry, metadata, rules)

    @patch("Bcfg2.Server.Plugin.PrioDir._matches")
    def test__matches_regex_enabled(self, mock_matches):
        """ test match with regex enabled """
        r = self.get_obj()
        self.set_regex_enabled(r, True)
        metadata = Mock()
        mock_matches.return_value = False

        entry = lxml.etree.Element("Path", name="/etc/foo.conf")
        rules = ["/etc/.*\.conf", "/etc/bar"]
        self.assertTrue(r._matches(entry, metadata, rules))
        mock_matches.assert_called_with(r, entry, metadata, rules)
        self.assertIn("/etc/.*\.conf", r._regex_cache.keys())

    def set_regex_enabled(self, rules_obj, state):
        """ set the state of regex_enabled for this implementation of
        Rules """
        if not isinstance(rules_obj.core.setup, MagicMock):
            rules_obj.core.setup = MagicMock()
        rules_obj.core.setup.cfp.getboolean.return_value = state

    def test__regex_enabled(self):
        r = self.get_obj()
        r.core.setup = MagicMock()
        self.assertEqual(r._regex_enabled,
                         r.core.setup.cfp.getboolean.return_value)
        r.core.setup.cfp.getboolean.assert_called_with("rules", "regex",
                                                       default=False)

########NEW FILE########
__FILENAME__ = TestSEModules
import os
import sys
import lxml.etree
from Bcfg2.Compat import b64encode
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugins.SEModules import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestPlugin import TestSpecificData, TestGroupSpool


class TestSEModuleData(TestSpecificData):
    test_obj = SEModuleData
    path = os.path.join(datastore, "SEModules", "test.pp", "test.pp")

    def test_bind_entry(self):
        data = self.get_obj()
        data.data = "test"
        entry = lxml.etree.Element("test", name=self.path)
        data.bind_entry(entry, Mock())
        self.assertEqual(entry.get("name"), self.path)
        self.assertEqual(entry.get("encoding"), "base64")
        self.assertEqual(entry.text, b64encode(data.data))


class TestSEModules(TestGroupSpool):
    test_obj = SEModules

    def test__get_module_name(self):
        modules = self.get_obj()
        for mname in ["foo", "foo.pp"]:
            entry = lxml.etree.Element("SEModule", type="module", name=mname)
            self.assertEqual(modules._get_module_name(entry), "foo")

    def test__get_module_filename(self):
        modules = self.get_obj()
        for mname in ["foo", "foo.pp"]:
            entry = lxml.etree.Element("SEModule", type="module", name=mname)
            self.assertEqual(modules._get_module_filename(entry), "/foo.pp")

    def test_HandlesEntry(self):
        modules = self.get_obj()
        modules._get_module_filename = Mock()
        modules.Entries['SEModule']['/foo.pp'] = Mock()
        modules.Entries['SEModule']['/bar.pp'] = Mock()
        for el in [lxml.etree.Element("Path", name="foo.pp"),
                   lxml.etree.Element("SEModule", name="baz.pp")]:
            modules._get_module_filename.return_value = "/" + el.get("name")
            self.assertFalse(modules.HandlesEntry(el, Mock()))
            if el.tag == "SEModule":
                modules._get_module_filename.assert_called_with(el)

        for el in [lxml.etree.Element("SEModule", name="foo.pp"),
                   lxml.etree.Element("SEModule", name="bar.pp")]:
            modules._get_module_filename.return_value = "/" + el.get("name")
            self.assertTrue(modules.HandlesEntry(el, Mock()),
                            msg="SEModules fails to handle %s" % el.get("name"))
            modules._get_module_filename.assert_called_with(el)

        TestGroupSpool.test_HandlesEntry(self)

    def test_HandleEntry(self):
        modules = self.get_obj()
        modules._get_module_name = Mock()
        handler = Mock()
        modules.Entries['SEModule']['/foo.pp'] = handler
        modules._get_module_name.return_value = "foo"

        entry = lxml.etree.Element("SEModule", type="module", name="foo")
        metadata = Mock()
        self.assertEqual(modules.HandleEntry(entry, metadata),
                         handler.return_value)
        modules._get_module_name.assert_called_with(entry)
        self.assertEqual(entry.get("name"),
                         modules._get_module_name.return_value)
        handler.assert_called_with(entry, metadata)

        TestGroupSpool.test_HandlesEntry(self)

    def test_add_entry(self):
        @patch("%s.%s.add_entry" % (self.test_obj.__base__.__module__,
                                    self.test_obj.__base__.__name__))
        def inner(mock_add_entry):
            modules = self.get_obj()
            modules.event_path = Mock()

            evt = Mock()
            evt.filename = "test.pp.G10_foo"

            modules.event_path.return_value = \
                os.path.join(datastore,
                             self.test_obj.__name__,
                             "test.pp",
                             "test.pp.G10_foo")
            modules.add_entry(evt)
            self.assertEqual(modules.filename_pattern, "test.pp")
            mock_add_entry.assert_called_with(modules, evt)
            modules.event_path.assert_called_with(evt)

        inner()
        TestGroupSpool.test_add_entry(self)

########NEW FILE########
__FILENAME__ = TestTemplateHelper
import os
import sys
import Bcfg2.Server.Plugin
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugins.TemplateHelper import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestPlugin import TestDirectoryBacked, TestConnector, TestPlugin, \
    TestFileBacked


class TestHelperModule(Bcfg2TestCase):
    test_obj = HelperModule
    path = os.path.join(datastore, "test.py")

    def get_obj(self, path=None):
        if path is None:
            path = self.path
        return self.test_obj(path, fam=Mock())

    def test__init(self):
        hm = self.get_obj()
        self.assertEqual(hm._module_name, "test")
        self.assertEqual(hm._attrs, [])

    @patch("imp.load_source")
    def test_HandleEvent(self, mock_load_source):
        hm = self.get_obj()

        mock_load_source.side_effect = ImportError
        attrs = dir(hm)
        hm.HandleEvent()
        mock_load_source.assert_called_with(safe_module_name(hm._module_name),
                                            hm.name)
        self.assertEqual(attrs, dir(hm))
        self.assertEqual(hm._attrs, [])

        mock_load_source.reset()
        mock_load_source.side_effect = None
        # a regular Mock (not a MagicMock) won't automatically create
        # __export__, so this triggers a failure condition in HandleEvent
        mock_load_source.return_value = Mock()
        attrs = dir(hm)
        hm.HandleEvent()
        mock_load_source.assert_called_with(safe_module_name(hm._module_name),
                                            hm.name)
        self.assertEqual(attrs, dir(hm))
        self.assertEqual(hm._attrs, [])

        # test reserved attributes
        module = Mock()
        module.__export__ = ["_attrs", "HandleEvent", "__init__"]
        mock_load_source.reset()
        mock_load_source.return_value = module
        attrs = dir(hm)
        hm.HandleEvent()
        mock_load_source.assert_called_with(safe_module_name(hm._module_name),
                                            hm.name)
        self.assertEqual(attrs, dir(hm))
        self.assertEqual(hm._attrs, [])

        # test adding attributes
        module = Mock()
        module.__export__ = ["foo", "bar", "baz", "HandleEvent"]
        mock_load_source.reset()
        mock_load_source.return_value = module
        hm.HandleEvent()
        mock_load_source.assert_called_with(safe_module_name(hm._module_name),
                                            hm.name)
        self.assertTrue(hasattr(hm, "foo"))
        self.assertTrue(hasattr(hm, "bar"))
        self.assertTrue(hasattr(hm, "baz"))
        self.assertEqual(hm._attrs, ["foo", "bar", "baz"])

        # test removing attributes
        module = Mock()
        module.__export__ = ["foo", "bar", "quux", "HandleEvent"]
        mock_load_source.reset()
        mock_load_source.return_value = module
        hm.HandleEvent()
        mock_load_source.assert_called_with(safe_module_name(hm._module_name),
                                            hm.name)
        self.assertTrue(hasattr(hm, "foo"))
        self.assertTrue(hasattr(hm, "bar"))
        self.assertTrue(hasattr(hm, "quux"))
        self.assertFalse(hasattr(hm, "baz"))
        self.assertEqual(hm._attrs, ["foo", "bar", "quux"])


class TestTemplateHelper(TestPlugin, TestConnector, TestDirectoryBacked):
    test_obj = TemplateHelper
    testfiles = ['foo.py', 'foo_bar.py', 'foo.bar.py']
    ignore = ['fooo.py~', 'fooo.pyc', 'fooo.pyo']
    badevents = ['foo']

    def get_obj(self, core=None, fam=None):
        if core is None:
            core = Mock()
        if fam is not None:
            core.fam = fam

        @patch("%s.%s.add_directory_monitor" % (self.test_obj.__module__,
                                                self.test_obj.__name__),
               Mock())
        def inner():
            return TestPlugin.get_obj(self, core=core)
        return inner()

    def test_get_additional_data(self):
        TestConnector.test_get_additional_data(self)

        th = self.get_obj()
        modules = ['foo', 'bar']
        rv = dict()
        for mname in modules:
            module = Mock()
            module._module_name = mname
            rv[mname] = module
            th.entries['%s.py' % mname] = module
        actual = th.get_additional_data(Mock())
        self.assertItemsEqual(actual, rv)

########NEW FILE########
__FILENAME__ = TestTrigger
import os
import sys
from mock import Mock, patch
from subprocess import PIPE
from Bcfg2.Server.Plugins.Trigger import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestPlugin import TestDirectoryBacked, TestClientRunHooks, TestPlugin, \
    TestFileBacked


class TestTriggerFile(TestFileBacked):
    test_obj = TriggerFile

    def test_HandleEvent(self):
        pass


class TestTrigger(TestPlugin, TestClientRunHooks, TestDirectoryBacked):
    test_obj = Trigger

    def get_obj(self, core=None, fam=None):
        if core is None:
            core = Mock()
        if fam is not None:
            core.fam = fam

        @patch("%s.%s.add_directory_monitor" % (self.test_obj.__module__,
                                                self.test_obj.__name__),
               Mock())
        def inner():
            return TestPlugin.get_obj(self, core=core)
        return inner()

    @patch("os.fork")
    @patch("os._exit")
    @patch("os.waitpid")
    @patch("subprocess.Popen")
    @skip("Tests that call os.fork are broken, even when os.fork is mocked")
    def test_async_run(self, mock_Popen, mock_waitpid, mock_exit, mock_fork):
        trigger = self.get_obj()

        def reset():
            mock_Popen.reset_mock()
            mock_waitpid.reset_mock()
            mock_exit.reset_mock()
            mock_fork.reset_mock()

        mock_fork.return_value = 0
        trigger.async_run(["foo", "bar"])
        self.assertItemsEqual(mock_fork.call_args_list,
                              [call(), call()])
        mock_Popen.assert_called_with(["foo", "bar"], stdin=PIPE, stdout=PIPE,
                                      stderr=PIPE)
        mock_Popen.return_value.wait.assert_called_with()
        mock_exit.assert_called_with(0)

        reset()
        mock_fork.return_value = 123
        trigger.async_run(["foo", "bar"])
        mock_fork.assert_called_with()
        mock_waitpid.assert_called_with(123, 0)
        self.assertFalse(mock_Popen.called)

    def test_end_client_run(self):
        trigger = self.get_obj()
        trigger.async_run = Mock()
        trigger.entries = {'foo.sh': Mock(), 'bar': Mock()}

        metadata = Mock()
        metadata.hostname = "host"
        metadata.profile = "profile"
        metadata.groups = ['a', 'b', 'c']
        args = ['host', '-p', 'profile', '-g', 'a:b:c']

        trigger.end_client_run(metadata)
        self.assertItemsEqual([[os.path.join(trigger.data, 'foo.sh')] + args,
                               [os.path.join(trigger.data, 'bar')] + args],
                              [c[0][0]
                               for c in trigger.async_run.call_args_list])

########NEW FILE########
__FILENAME__ = TestStatistics
import os
import sys
from mock import Mock, MagicMock, patch

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *

from Bcfg2.Statistics import *


class TestStatistic(Bcfg2TestCase):
    def test_stat(self):
        stat = Statistic("test", 1)
        self.assertEqual(stat.get_value(), ("test", (1.0, 1.0, 1.0, 1)))
        stat.add_value(10)
        self.assertEqual(stat.get_value(), ("test", (1.0, 10.0, 5.5, 2)))
        stat.add_value(100)
        self.assertEqual(stat.get_value(), ("test", (1.0, 100.0, 37.0, 3)))
        stat.add_value(12.345)
        self.assertEqual(stat.get_value(), ("test", (1.0, 100.0, 30.83625, 4)))
        stat.add_value(0.655)
        self.assertEqual(stat.get_value(), ("test", (0.655, 100.0, 24.8, 5)))


class TestStatistics(Bcfg2TestCase):
    def test_stats(self):
        stats = Statistics()
        self.assertEqual(stats.display(), dict())
        stats.add_value("test1", 1)
        self.assertEqual(stats.display(), dict(test1=(1.0, 1.0, 1.0, 1)))
        stats.add_value("test2", 1.23)
        self.assertEqual(stats.display(), dict(test1=(1.0, 1.0, 1.0, 1),
                                               test2=(1.23, 1.23, 1.23, 1)))
        stats.add_value("test1", 10)
        self.assertEqual(stats.display(), dict(test1=(1.0, 10.0, 5.5, 2),
                                               test2=(1.23, 1.23, 1.23, 1)))

########NEW FILE########
__FILENAME__ = TestUtils
import os
import sys
import copy
import lxml.etree
import subprocess
from mock import Mock, MagicMock, patch
from Bcfg2.Utils import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *


class TestPackedDigitRange(Bcfg2TestCase):
    def test_ranges(self):
        # test cases.  tuples of (ranges, included numbers, excluded
        # numbers)
        # tuples of (range description, numbers that are included,
        # numebrs that are excluded)
        tests = [(["0-3"], ["0", 1, "2", 3], [4]),
                 (["1"], [1], [0, "2"]),
                 (["10-11"], [10, 11], [0, 1]),
                 (["9-9"], [9], [8, 10]),
                 (["0-100"], [0, 10, 99, 100], []),
                 (["1", "3", "5"], [1, 3, 5], [0, 2, 4, 6]),
                 (["1-5", "7"], [1, 3, 5, 7], [0, 6, 8]),
                 (["1-5", 7, "9-11"], [1, 3, 5, 7, 9, 11], [0, 6, 8, 12]),
                 (["1-5,   7,9-11  "], [1, 3, 5, 7, 9, 11], [0, 6, 8, 12]),
                 (["852-855", "321-497", 763], [852, 855, 321, 400, 497, 763],
                  [851, 320, 766, 999]),
                 (["0-"], [0, 1, 100, 100000], []),
                 ([1, "5-10", "1000-"], [1, 5, 10, 1000, 10000000],
                  [4, 11, 999])]
        for ranges, inc, exc in tests:
            rng = PackedDigitRange(*ranges)
            for test in inc:
                self.assertIn(test, rng)
                self.assertTrue(rng.includes(test))
            for test in exc:
                self.assertNotIn(test, rng)
                self.assertFalse(rng.includes(test))

########NEW FILE########
__FILENAME__ = test_code_checks
import os
import re
import sys
import glob
import copy
from subprocess import Popen, PIPE, STDOUT

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
_path = os.path.dirname(__file__)
while _path != '/':
    if os.path.basename(_path).lower().startswith("test"):
        sys.path.append(_path)
    if os.path.basename(_path) == "testsuite":
        break
    _path = os.path.dirname(_path)
from common import *

# path to base testsuite directory
testdir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

# path to Bcfg2 src directory
srcpath = os.path.abspath(os.path.join(testdir, "..", "src"))

# path to pylint rc file
rcfile = os.path.join(testdir, "pylintrc.conf")

# perform checks on the listed files only if the module listed in the
# keys can be imported
contingent_checks = {
    ("django",): {"lib/Bcfg2": ["Reporting"],
                  "lib/Bcfg2/Server": ["Reports",
                                       "SchemaUpdater",
                                       "models.py"],
                  "lib/Bcfg2/Server/Admin": ["Reports.py", "Syncdb.py"],
                  "sbin": ["bcfg2-reports"]},
    ("pyinotify",): {"lib/Bcfg2/Server/FileMonitor": ["Inotify.py"]},
    ("yum",): {"lib/Bcfg2/Client/Tools": ["YUM.py"]},
    ("genshi",): {"lib/Bcfg2/Server/Plugins/Cfg": ["CfgGenshiGenerator.py"]},
    ("Cheetah",): {"lib/Bcfg2/Server/Plugins/Cfg": ["CfgCheetahGenerator.py"]},
    ("M2Crypto",): {"lib/Bcfg2": ["Encryption.py"],
                    "lib/Bcfg2/Server/Plugins/Cfg":
                        ["CfgEncryptedGenerator.py"]},
    ("M2Crypto", "genshi"): {"lib/Bcfg2/Server/Plugins/Cfg":
                                 ["CfgEncryptedGenshiGenerator.py"]},
    ("M2Crypto", "Cheetah"): {"lib/Bcfg2/Server/Plugins/Cfg":
                                  ["CfgEncryptedCheetahGenerator.py"]},
    }

# perform only error checking on the listed files
error_checks = {
    "sbin": ["bcfg2-build-reports"],
    "lib/Bcfg2": ["Proxy.py", "SSLServer.py", "Reporting"],
    "lib/Bcfg2/Server": ["Reports", "SchemaUpdater"],
    "lib/Bcfg2/Server/Admin": ["Compare.py",
                               "Snapshots.py"],
    "lib/Bcfg2/Client/Tools": ["OpenCSW.py",
                               "Blast.py",
                               "FreeBSDInit.py",
                               "VCS.py",
                               "YUM24.py"],
    "lib/Bcfg2/Server/Plugins": ["Deps.py",
                                 "Ldap.py",
                                 "Pkgmgr.py"]
    }

# perform no checks at all on the listed files
no_checks = {
    "lib/Bcfg2/Client/Tools": ["APT.py", "RPM.py", "rpmtools.py"],
    "lib/Bcfg2/Server": ["Snapshots", "Hostbase"],
    "lib/Bcfg2": ["manage.py"],
    "lib/Bcfg2/Server/Reports": ["manage.py"],
    "lib/Bcfg2/Server/Plugins": ["Account.py",
                                 "Base.py",
                                 "Editor.py",
                                 "Hostbase.py",
                                 "Snapshots.py",
                                 "Statistics.py",
                                 "TCheetah.py",
                                 "TGenshi.py"],
    }
if sys.version_info < (2, 6):
    # multiprocessing core requires py2.6
    no_checks['lib/Bcfg2/Server'].append('MultiprocessingCore.py')

try:
    any
except NameError:
    def any(iterable):
        """ implementation of builtin any() for python 2.4 """
        for element in iterable:
            if element:
                return True
        return False


def expand_path_dict(pathdict):
    """ given a path dict as above, return a list of all the paths """
    rv = []
    for parent, modules in pathdict.items():
        for mod in modules:
            rv.extend(glob.glob(os.path.join(srcpath, parent, mod)))
    return rv


def whitelist_filter(filelist, whitelist):
    rv = []
    for fpath in filelist:
        if fpath in whitelist:
            rv.append(fpath)
            continue
        # check if the path is in any directories that are in the
        # whitelist
        if any(fpath.startswith(wpath + "/") for wpath in whitelist):
            rv.append(fpath)
            continue
    return rv


def blacklist_filter(filelist, blacklist):
    rv = []
    for fpath in filelist:
        if fpath in blacklist:
            continue
        # check that the path isn't in any directories that are in
        # the blacklist
        if any(fpath.startswith(bpath + "/") for bpath in blacklist):
            continue
        rv.append(fpath)
    return rv


class CodeTestCase(Bcfg2TestCase):
    __test__ = False

    # build the blacklists
    blacklist = expand_path_dict(no_checks)

    contingent_blacklist = []
    for filedict in contingent_checks.values():
        contingent_blacklist += expand_path_dict(filedict)

    full_blacklist = expand_path_dict(error_checks) + contingent_blacklist + \
        blacklist

    command = [None]

    has_command = None

    # extra arguments when running tests on sbin/*
    sbin_args = []

    # extra arguments when running tests on lib/*
    lib_args = []

    # extra arguments for full tests
    full_args = []

    # extra arguments for error tests
    error_args = []

    def has_exec(self):
        if self.has_command is None:
            try:
                Popen(self.command,
                      stdin=PIPE, stdout=PIPE, stderr=STDOUT).wait()
                self.has_command = True
            except OSError:
                self.has_command = False
        return self.has_command

    def get_env(self):
        if ('PYTHONPATH' not in os.environ or
            testdir not in os.environ['PYTHONPATH'].split(":")):
            env = copy.copy(os.environ)
            env['PYTHONPATH'] = ':'.join([env.get("PYTHONPATH", ""),
                                          testdir])
            return env
        else:
            return os.environ

    def _test_full(self, files, extra_args=None):
        """ test select files for all problems """
        if not len(files):
            return
        if extra_args is None:
            extra_args = []
        cmd = self.command + self.full_args + extra_args + \
            [os.path.join(srcpath, f) for f in files]
        proc = Popen(cmd, stdout=PIPE, stderr=STDOUT, env=self.get_env())
        print(proc.communicate()[0].decode())
        self.assertEqual(proc.wait(), 0)

    def _test_errors(self, files, extra_args=None):
        """ test select files for errors """
        if not len(files):
            return
        if extra_args is None:
            extra_args = []
        cmd = self.command + self.error_args + extra_args + \
            [os.path.join(srcpath, f) for f in files]
        proc = Popen(cmd, stdout=PIPE, stderr=STDOUT, env=self.get_env())
        print(proc.communicate()[0].decode())
        self.assertEqual(proc.wait(), 0)

    @skipIf(not os.path.exists(srcpath), "%s does not exist" % srcpath)
    @skipIf(not os.path.exists(rcfile), "%s does not exist" % rcfile)
    def test_lib_full(self):
        @skipUnless(self.has_exec(),
                    "%s not found, skipping" % self.command[0])
        def inner():
            full_list = []
            for root, _, files in os.walk(os.path.join(srcpath, "lib")):
                full_list.extend(blacklist_filter([os.path.join(root, f)
                                                   for f in files
                                                   if f.endswith(".py")],
                                                  self.full_blacklist))
            self._test_full(full_list, extra_args=self.lib_args)

        inner()

    @skipIf(not os.path.exists(srcpath), "%s does not exist" % srcpath)
    @skipIf(not os.path.exists(rcfile), "%s does not exist" % rcfile)
    def test_contingent_full(self):
        @skipUnless(self.has_exec(),
                    "%s not found, skipping" % self.command[0])
        def inner():
            filelist = []
            blacklist = set(expand_path_dict(error_checks) + self.blacklist)
            for (mods, filedict) in contingent_checks.items():
                try:
                    for mod in mods:
                        __import__(mod)
                except ImportError:
                    continue
                filelist.extend(expand_path_dict(filedict))
            self._test_full(blacklist_filter(filelist, blacklist),
                            extra_args=self.lib_args)

        inner()

    @skipIf(not os.path.exists(srcpath), "%s does not exist" % srcpath)
    @skipIf(not os.path.exists(rcfile), "%s does not exist" % rcfile)
    def test_sbin(self):
        @skipUnless(self.has_exec(),
                    "%s not found, skipping" % self.command[0])
        def inner():
            all_sbin = [os.path.join(srcpath, "sbin", f)
                        for f in glob.glob(os.path.join(srcpath, "sbin", "*"))]
            full_list = blacklist_filter([f for f in all_sbin
                                          if not os.path.islink(f)],
                                         self.full_blacklist)
            self._test_full(full_list, extra_args=self.sbin_args)

            errors_list = blacklist_filter([f for f in all_sbin
                                            if not os.path.islink(f)],
                                           self.contingent_blacklist)
            self._test_errors(errors_list, extra_args=self.sbin_args)

        inner()

    @skipIf(not os.path.exists(srcpath), "%s does not exist" % srcpath)
    @skipIf(not os.path.exists(rcfile), "%s does not exist" % rcfile)
    def test_contingent_errors(self):
        @skipUnless(self.has_exec(),
                    "%s not found, skipping" % self.command[0])
        def inner():
            filelist = []
            whitelist = expand_path_dict(error_checks)
            for (mods, filedict) in contingent_checks.items():
                try:
                    for mod in mods:
                        __import__(mod)
                except ImportError:
                    continue
                filelist.extend(expand_path_dict(filedict))
            flist = blacklist_filter(whitelist_filter(filelist, whitelist),
                                     self.blacklist)
            self._test_errors(flist, extra_args=self.lib_args)

        inner()

    @skipIf(not os.path.exists(srcpath), "%s does not exist" % srcpath)
    @skipIf(not os.path.exists(rcfile), "%s does not exist" % rcfile)
    def test_lib_errors(self):
        @skipUnless(self.has_exec(),
                    "%s not found, skipping" % self.command[0])
        def inner():
            filelist = blacklist_filter(expand_path_dict(error_checks),
                                        self.contingent_blacklist)
            return self._test_errors(filelist, extra_args=self.lib_args)

        inner()


class TestPylint(CodeTestCase):
    __test__ = True
    command = ["pylint", "--rcfile", rcfile, "--init-hook",
               "import sys;sys.path.append('%s')" %
               os.path.join(srcpath, "lib")]

    sbin_args = ["--module-rgx", "[a-z_-][a-z0-9_-]*$"]
    error_args = ["-f", "parseable", "-d", "R0801,E1103"]

    # regex to find errors and fatal errors
    error_re = re.compile(r':\d+:\s+\[[EF]\d{4}')

    def __init__(self, *args, **kwargs):
        CodeTestCase.__init__(self, *args, **kwargs)
        for mods, filedict in contingent_checks.items():
            if "django" in mods:
                # there's some issue with running pylint on modules
                # that use django in Travis CI (but not elsewhere), so
                # skip these for now
                self.blacklist += expand_path_dict(filedict)

    def _test_errors(self, files, extra_args=None):
        """ test all files for fatals and errors """
        if not len(files):
            return
        if extra_args is None:
            extra_args = []
        args = self.command + self.error_args + extra_args + \
            [os.path.join(srcpath, p) for p in files]
        pylint = Popen(args, stdout=PIPE, stderr=STDOUT, env=self.get_env())
        output = pylint.communicate()[0].decode()
        rv = pylint.wait()

        for line in output.splitlines():
            if self.error_re.search(str(line)):
                print(line)
        # pylint returns a bitmask, where 1 means fatal errors
        # were encountered and 2 means errors were encountered.
        self.assertEqual(rv & 3, 0)


class TestPEP8(CodeTestCase):
    __test__ = True
    command = ["pep8", "--ignore=E125,E129,E501"]

    def _test_errors(self, files, extra_args=None):
        pass

########NEW FILE########
__FILENAME__ = basebuilder
#!/usr/bin/env python

from sys import argv
from elementtree.ElementTree import Element, SubElement, tostring

if __name__ == '__main__':
    dir = argv[1]
    imagename = dir.split('/')[-1]
    e = Element("Image", name=imagename)
    for line in open("%s/base.ConfigFile" % (dir)).readlines():
        SubElement(e, "ConfigFile", name=line.strip())
    for line in open("%s/base.Package" % (dir)).readlines():
        SubElement(e, "Package", name=line.strip())
    for line in open("%s/base.Service" % (dir)).readlines():
        SubElement(e, "Service", name=line.strip().split()[0])

    print(tostring(e))

########NEW FILE########
__FILENAME__ = batchadd
#!/usr/bin/python

from datetime import date
import os
import sys

os.environ['DJANGO_SETTINGS_MODULE'] = 'Bcfg2.Server.Hostbase.settings'
from Bcfg2.Server.Hostbase.hostbase.models import *
from Bcfg2.Server.Hostbase.settings import DEFAULT_MX, PRIORITY
import Bcfg2.Server.Hostbase.regex

host_attribs = ['administrator',
                'comments',
                'csi',
                'expiration_date',
                'hostname',
                'location',
                'netgroup',
                'outbound_smtp',
                'primary_user',
                'printq',
                'security_class',
                'support',
                'whatami']


def handle_error(field):
    if '-f' in sys.argv:
        return
    print("Error: %s is already defined in hostbase" % field)
    if '-s' in sys.argv:
        sys.exit(1)


def checkformat(values, indices):
    """Ensures file contains all necessary attributes in order """
    filelist = [pair[0] for pair in values]

    #    lines = len(filelist)

    filelist = filelist[indices[0]:]

    for index in indices:
        if filelist[0:13] != host_attribs:
            # figure out what to do here
            return False
        else:
            # process rest of host attributes
            try:
                next = filelist[1:].index('hostname')
                remaining = filelist[13:next + 1]
                filelist = filelist[next + 1:]
            except:
                remaining = filelist[13:]
            needfields = ['mac_addr', 'hdwr_type', 'ip_addr']
            if [item for item in needfields if item not in remaining]:
                return False
    return True


if __name__ == '__main__':

    # argument handling for batchadd
    try:
        fd = open(sys.argv[1], 'r')
    except (IndexError, IOError):
        print("\nUsage: batchadd.py filename\n")
        sys.exit()

    lines = fd.readlines()
    # splits and strips lines into (attribute, value)
    info = [[item.strip() for item in line.split("->")] for line in lines
            if line.lstrip(' ')[0] != '#' and line != '\n']

    if info[0][0] == 'mx' and info[1][0] == 'priority':
        mx, created = MX.objects.get_or_create(mx=info[0][1],
                                               priority=info[1][1])
        info = info[2:]
    else:
        mx, created = MX.objects.get_or_create(mx=DEFAULT_MX,
                                               priority=PRIORITY)
    if created:
        mx.save()

    hostindices = [num for num in range(0, len(info))
                   if info[num][0] == 'hostname']

    if not checkformat(info, hostindices):
        print("Error: file format")
        sys.exit()

#################

    for host in hostindices:
        try:
            host = Host.objects.get(hostname=info[host][1])
            handle_error(info[host][1])
        except:
            # do something here
            pass

    macindices = [num for num in range(0, len(info))
                  if info[num][0] == 'mac_addr']
    for mac_addr in macindices:
        try:
            host = Interface.objects.get(mac_addr=info[mac_addr][1])
            handle_error(info[mac_addr][1])
        except:
            # do something here
            pass

    for host in hostindices:
        blank = Host()
        for attrib in host_attribs:
            pair = info.pop(0)
            if pair[0] == 'outbound_smtp':
                if pair[1] == 'y':
                    blank.__dict__[pair[0]] = 1
                else:
                    blank.__dict__[pair[0]] = 0
            elif pair[0] == 'expiration_date':
                (year, month, day) = pair[1].split("-")
                blank.expiration_date = date(int(year),
                                             int(month),
                                             int(day))
            else:
                blank.__dict__[pair[0]] = pair[1]
        blank.status = 'active'
        blank.save()
        newhostname = blank.hostname.split(".")[0]
        newdomain = blank.hostname.split(".", 1)[1]
        while info and info[0][0] != 'hostname':
            if info[0][0] == 'mac_addr':
                pair = info.pop(0)
                inter = Interface.objects.create(host=blank,
                                                 mac_addr=pair[1],
                                                 hdwr_type='eth')
                if not pair[1]:
                    inter.dhcp = False
                inter.save()
            elif info[0][0] == 'hdwr_type':
                pair = info.pop(0)
                inter.hdwr_type = pair[1]
                inter.save()
            elif info[0][0] == 'ip_addr':
                pair = info.pop(0)
                ip = IP.objects.create(interface=inter, ip_addr=pair[1])
                hostnamenode = Name(ip=ip,
                                    name=blank.hostname,
                                    dns_view='global',
                                    only=False)
                hostnamenode.save()
                namenode = Name(ip=ip,
                                name=".".join([newhostname + "-" + inter.hdwr_type,
                                               newdomain]),
                                dns_view="global", only=False)
                namenode.save()
                subnetnode = Name(ip=ip, name=newhostname + "-" +
                                  ip.ip_addr.split(".")[2] + "." +
                                  newdomain, dns_view="global", only=False)
                subnetnode.save()
                hostnamenode.mxs.add(mx)
                namenode.mxs.add(mx)
                subnetnode.mxs.add(mx)
            elif info[0][0] == 'cname':
                pair = info.pop(0)
                cname = CName.objects.create(name=hostnamenode, cname=pair[1])
                cname.save()

########NEW FILE########
__FILENAME__ = bcfg2-profile-templates
#!/usr/bin/python -Ott
# -*- coding: utf-8 -*-
""" Benchmark template rendering times """

import sys
import time
import math
import signal
import logging
import operator
import Bcfg2.Logger
import Bcfg2.Options
import Bcfg2.Server.Core


def stdev(nums):
    mean = float(sum(nums)) / len(nums)
    return math.sqrt(sum((n - mean)**2 for n in nums) / float(len(nums)))


def get_sigint_handler(core):
    """ Get a function that handles SIGINT/Ctrl-C by shutting down the
    core and exiting properly."""

    def hdlr(sig, frame):  # pylint: disable=W0613
        """ Handle SIGINT/Ctrl-C by shutting down the core and exiting
        properly. """
        core.shutdown()
        os._exit(1)  # pylint: disable=W0212

    return hdlr


def main():
    optinfo = dict(
        client=Bcfg2.Options.Option("Benchmark templates for one client",
                                    cmd="--client",
                                    odesc="<client>",
                                    long_arg=True,
                                    default=None),
        runs=Bcfg2.Options.Option("Number of rendering passes per template",
                                  cmd="--runs",
                                  odesc="<runs>",
                                  long_arg=True,
                                  default=5,
                                  cook=int))
    optinfo.update(Bcfg2.Options.CLI_COMMON_OPTIONS)
    optinfo.update(Bcfg2.Options.SERVER_COMMON_OPTIONS)
    setup = Bcfg2.Options.OptionParser(optinfo)
    setup.parse(sys.argv[1:])

    if setup['debug']:
        level = logging.DEBUG
    elif setup['verbose']:
        level = logging.INFO
    else:
        level = logging.WARNING
    Bcfg2.Logger.setup_logging("bcfg2-test",
                               to_console=setup['verbose'] or setup['debug'],
                               to_syslog=False,
                               to_file=setup['logging'],
                               level=level)
    logger = logging.getLogger(sys.argv[0])

    core = Bcfg2.Server.Core.BaseCore(setup)
    signal.signal(signal.SIGINT, get_sigint_handler(core))
    logger.info("Bcfg2 server core loaded")
    core.load_plugins()
    logger.debug("Plugins loaded")
    core.block_for_fam_events(handle_events=True)
    logger.debug("Repository events processed")

    if setup['args']:
        templates = setup['args']
    else:
        templates = []

    if setup['client'] is None:
        clients = [core.build_metadata(c) for c in core.metadata.clients]
    else:
        clients = [core.build_metadata(setup['client'])]

    times = dict()
    client_count = 0
    for metadata in clients:
        client_count += 1
        logger.info("Rendering templates for client %s (%s/%s)" %
                    (metadata.hostname, client_count, len(clients)))
        structs = core.GetStructures(metadata)
        struct_count = 0
        for struct in structs:
            struct_count += 1
            logger.info("Rendering templates from structure %s:%s (%s/%s)" %
                        (struct.tag, struct.get("name"), struct_count,
                         len(structs)))
            entries = struct.xpath("//Path")
            entry_count = 0
            for entry in entries:
                entry_count += 1
                if templates and entry.get("name") not in templates:
                    continue
                logger.info("Rendering Path:%s (%s/%s)..." %
                            (entry.get("name"), entry_count, len(entries)))
                ptimes = times.setdefault(entry.get("name"), [])
                for i in range(setup['runs']):
                    start = time.time()
                    try:
                        core.Bind(entry, metadata)
                        ptimes.append(time.time() - start)
                    except:
                        break
                if ptimes:
                    avg = sum(ptimes) / len(ptimes)
                    if avg:
                        logger.debug("   %s: %.02f sec" %
                                     (metadata.hostname, avg))

    # print out per-file results
    tmpltimes = []
    for tmpl, ptimes in times.items():
        try:
            mean = float(sum(ptimes)) / len(ptimes)
        except ZeroDivisionError:
            continue
        ptimes.sort()
        median = ptimes[len(ptimes) / 2]
        std = stdev(ptimes)
        if mean > 0.01 or median > 0.01 or std > 1 or templates:
            tmpltimes.append((tmpl, mean, median, std))
    print("%-50s %-9s  %-11s  %6s" %
          ("Template", "Mean Time", "Median Time", ""))
    for info in reversed(sorted(tmpltimes, key=operator.itemgetter(1))):
        print("%-50s %9.02f  %11.02f  %6.02f" % info)
    core.shutdown()


if __name__ == "__main__":
    sys.exit(main())

########NEW FILE########
__FILENAME__ = bcfg2_local
#!/usr/bin/env python
""" This tool performs a full Bcfg2 run entirely against a local
repository, i.e., without a server.  It starts up a local instance of
the server core, then uses that to get probes, run them, and so on."""

import sys
import socket
import Bcfg2.Options
from Bcfg2.Client.Client import Client
from Bcfg2.Server.Core import BaseCore


class LocalCore(BaseCore):
    """ Local server core similar to the one started by bcfg2-info """

    def __init__(self, setup):
        saved = (setup['syslog'], setup['logging'])
        setup['syslog'] = False
        setup['logging'] = None
        Bcfg2.Server.Core.BaseCore.__init__(self, setup=setup)
        setup['syslog'], setup['logging'] = saved
        self.load_plugins()
        self.block_for_fam_events(handle_events=True)

    def _daemonize(self):
        return True

    def _run(self):
        return True

    def _block(self):
        pass


class LocalProxy(object):
    """ A local proxy (as opposed to XML-RPC) that proxies from the
    Client object to the LocalCore object, adding a client address
    pair to the argument list of each proxied call """

    def __init__(self, core):
        self.core = core
        self.hostname = socket.gethostname()
        self.ipaddr = socket.gethostbyname(self.hostname)

    def __getattr__(self, attr):
        if hasattr(self.core, attr):
            func = getattr(self.core, attr)
            if func.exposed:
                def inner(*args, **kwargs):
                    # the port portion of the addresspair tuple isn't
                    # actually used, so it's safe to hardcode 6789
                    # here.
                    args = ((self.ipaddr, 6789), ) + args
                    return func(*args, **kwargs)
                return inner
        raise AttributeError(attr)


class LocalClient(Client):
    """ A version of the Client class that uses LocalProxy instead of
    an XML-RPC proxy to make its calls """

    def __init__(self, setup, proxy):
        Client.__init__(self, setup)
        self._proxy = proxy


def main():
    optinfo = Bcfg2.Options.CLIENT_COMMON_OPTIONS
    optinfo.update(Bcfg2.Options.SERVER_COMMON_OPTIONS)
    if 'bundle_quick' in optinfo:
        # CLIENT_BUNDLEQUICK option uses -Q, just like the server repo
        # option.  the server repo is more important for this
        # application.
        optinfo['bundle_quick'] = Bcfg2.Options.Option('bundlequick',
                                                       default=False)
    setup = Bcfg2.Options.OptionParser(optinfo)
    setup.parse(sys.argv[1:])

    core = LocalCore(setup)
    try:
        LocalClient(setup, LocalProxy(core)).run()
    finally:
        core.shutdown()

if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = bcfg2_svnlog
#!/usr/bin/python -O
""" Send email about Bcfg2 commits from an SVN postcommit hook

This script can be used to send email from a Subversion postcommit
hook.  It emails out a list of diffs, with a few exceptions:

* If a file was deleted, the deletion is noted but no diff is included
* If the file matches a set of blacklist patterns (configurable; by
  default: /Ohai/*.json, */Probes/probed.xml, */SSHbase/*,
  */Packages/packages.conf), then the diff is not included but the file
  is listed as 'sensitive.'  (This is a bit of a broad brush, since the
  stuff in Probes and Ohai isn't necessarily sensitive, just annoying to
  get diffs for.)
* If the file is a directory, not a file, it is omitted
* If he file is binary, that is noted instead of a diff being included
* If the diff exceeds 100 lines (configurable), then a 'large diff' is
  mentioned, but not included.
* If the file is a Property file and is flagged as sensitive in the
  opening Property tag, then it is listed as sensitive and no diff is
  included.
* If the file is flagged as sensitive in its info.xml, then it is
  listed as sensitive and no diff is included.

The script attempts to look up the committing user's email address in
LDAP; it uses the system LDAP config to do so.  Currently it looks in
/etc/ldap.conf, /etc/openldap/ldap.conf, and /etc/nss_ldap.conf to
figure out the LDAP config, so it doesn't work with SSSD or with OSes
that keep their LDAP configs in other places.

The config file, /etc/bcfg2_svnlog.conf, should contain one stanza per
repository.  (If you just have one Bcfg2 repo, then you only need one
stanza.  This script unfortunately does not support different
configurations for different branches.)  Each stanza should look like this:

[<repo name>]
email=<address to email on commit>
subject=<tag to prepend to the Subject line>
largediff=<# of lines a diff must exceed to be considered too large to
           include in the email>
blacklist=<space-delimited list of shell glob patterns to consider
           sensitive>

Only 'email' is required.

The commit message can itself contain some magic that will influence
the email sent out. The following patterns are special:

* Subject: <subject>
  Use the specified text as the subject of the message. Otherwise, the
  first line (up to the first [.!;:?] or 120 characters) will be used.
* Resolve: <ticket number>
  Add some magic to the email that will resolve the specified RT ticket.

These patterns can either be listed on a line by themselves, or
enclosed in curly braces ({...}). Whitespace after the colon is
optional. The patterns are all case-insensitive. So these two commits
are identical:
 
svn ci -m '{resolve:108934}Fixed DNS error'
svn ci -m 'Fixed DNS error
Resolve: 108934'
"""

__author__ = "Chris St Pierre"
__email__ = "chris.a.st.pierre@gmail.com"

import re
import os
import sys
import ldap
import pysvn
import shutil
import fnmatch
import smtplib
import logging
import logging.handlers
import tempfile
import lxml.etree
from email.Message import Message
from optparse import OptionParser, OptionError
from ConfigParser import SafeConfigParser

SEPARATOR = "=" * 67

LOGGER = None

def get_logger(verbose=0):
    """ set up logging according to the verbose level given on the
    command line """
    global LOGGER
    if LOGGER is None:
        LOGGER = logging.getLogger(sys.argv[0])
        stderr = logging.StreamHandler()
        level = logging.WARNING
        lformat = "%(message)s"
        if verbose == 1:
            level = logging.INFO
        elif verbose > 1:
            stderr.setFormatter(logging.Formatter("%(asctime)s: %(levelname)s: %(message)s"))
            level = logging.DEBUG
        LOGGER.setLevel(level)
        LOGGER.addHandler(stderr)
        syslog = logging.handlers.SysLogHandler("/dev/log")
        syslog.setFormatter(logging.Formatter("%(name)s: %(message)s"))
        LOGGER.addHandler(syslog)
        LOGGER.debug("Setting verbose to %s" % verbose)
    return LOGGER

def parse_log_message(message):
    """ Parse the commit log message """
    keywords = dict(subject=None, resolve=None)
    logger = get_logger()
    for keyword in keywords.iterkeys():
        pattern = re.compile((r'(?:\A|\n|\{)%s:\s*([^\}\n]+)(?:\Z|\n|\})' %
                              keyword),
                             re.IGNORECASE | re.MULTILINE)
        match = pattern.search(message)
        if match:
            keywords[keyword] = match.group(1).strip()
            logger.debug("Found log message keyword %s=%s" % (keyword,
                                                              match.group(0)))
            message = pattern.sub('', message)
    return (message, keywords)

def build_summary(changes):
    """ build a summary of changes """
    summary = dict()
    logger = get_logger()
    for change in changes:
        logger.info("Summarizing %s file %s" % (change.summarize_kind,
                                                change.path))
        if change.summarize_kind not in summary:
            summary[change.summarize_kind] = []
        summary[change.summarize_kind].append(change.path)
    return summary

def get_author_email(author):
    """looks up author email in ldap"""
    logger = get_logger()
    ldapconf = dict()
    for conffile in ["/etc/ldap.conf", "/etc/openldap/ldap.conf",
                     "/etc/nss_ldap.conf"]:
        # short-circuit if we have both a base and a host
        if 'base' in ldapconf and 'host' in ldapconf:
            break
        logger.debug("Reading LDAP configuration from %s" % conffile)
        try:
            for line in open(conffile).read().splitlines():
                match = re.search(r'^(base|host|ssl)\s+(.*)', line)
                if match:
                    ldapconf[match.group(1)] = match.group(2)
        except IOError:
            pass

    if 'base' in ldapconf and 'host' in ldapconf:
	    # host can be a space-delimited list; in that case, we just
	    # use the first host
        ldapconf['host'] = ldapconf['host'].split()[0]

        # ensure that we have an ldap uri
        if not re.search(r'^ldap[si]?://', ldapconf['host']):
            if ('ssl' in ldapconf and
                ldapconf['ssl'] in ['on', 'yes', 'start_tls']):
                ldapconf['host'] = "ldaps://%s" % ldapconf['host']
            else:
                ldapconf['host'] = "ldap://%s" % ldapconf['host']

        logger.debug("Connecting to LDAP server at %s" % ldapconf['host'])
        try:
            conn = ldap.initialize(ldapconf['host'])
        except ldap.LDAPError, err:
            logger.warn("Could not connect to LDAP server at %s: %s" %
                        (ldapconf['host'], err))
            return author

        if 'ssl' in ldapconf and ldapconf['ssl'] == 'start_tls':
            # try TLS, but don't require it.  if starting TLS fails
            # but the connection requires confidentiality, the search
            # will fail below
            logger.debug("Starting TLS")
            try:
                conn.start_tls_s()
            except ldap.LDAPError, err:
                if err[0]['info'] != 'TLS already started':
                    logger.warn("Could not start TLS: %s" % err)

        ldap_filter = "uid=%s" % author
        logger.debug("Searching for %s in %s" % (ldap_filter, ldapconf['base']))
        try:
            res = conn.search_s(ldapconf['base'], ldap.SCOPE_SUBTREE,
                                ldap_filter, ['mail'])
            if len(res) == 1:
                attrs = res.pop()[1]
                logger.debug("Got %s for email address" % attrs['mail'][0])
                return attrs['mail'][0]
            elif len(res):
                logger.warn("More than one LDAP entry found for %s" %
                             ldap_filter)
                return author
            elif not res:
                logger.warn("No LDAP entries found for %s" % ldap_filter)
                return author
        except ldap.LDAPError, err:
            logger.warn("Could not search for %s in LDAP at %s: %s" %
                        (ldap_filter, ldapconf['host'], err))
            return author
    else:
        logger.warn("Could not determine LDAP configuration")
        return author

def get_diff_set(change, baseuri, largediff=100, rev=None,
                 blacklist=None):
    """ generate diffs for the given change object.  returns a tuple
    of (<diff type>, <diff data>).  Type is one of None, 'sensitive',
    'large', 'binary', or 'diff'"""
    logger = get_logger()

    client = pysvn.Client()
    revision = pysvn.Revision(pysvn.opt_revision_kind.number, rev)
    previous = pysvn.Revision(pysvn.opt_revision_kind.number, rev - 1)

    logger.info("Diffing %s file %s" % (change.summarize_kind, change.path))
    change_uri = os.path.join(baseuri, change.path)

    if plugin_blacklist is None:
        plugin_blacklist = []

    # There are a number of reasons a diff might not be included in an
    # svnlog message:
    #
    # * The file was deleted
    # * The file matches a blacklist pattern (default */Ohai/*.json,
    #   */Probes/probed.xml, */SSHbase/*, */Packages/packages.conf)
    # * The file is a directory, not a file
    # * The file is binary
    # * The diff exceeds 100 lines
    # * The file is a Property file and is flagged as sensitive in the
    #   opening Property tag
    # * The file is flagged as sensitive in its info.xml
    #
    # These are listed here in approximate order from least expensive
    # to most expensive.  Consequently, if we can do a simple filename
    # match and avoid generating a diff, we win; and so on.

    if change.summarize_kind == pysvn.diff_summarize_kind.delete:
        logger.debug("%s was %s, skipping diff" % (change.path,
                                                   change.summarize_kind))
        return (None, None)

    if ("/SSHbase/" in change.path or
        change.path.endswith("/Packages/packages.conf")):
        logger.debug("%s is hard-coded as sensitive, skipping diff" %
                     change.path)
        return ("sensitive", change.path)

    for pattern in blacklist:
        if fnmatch.fnmatch(change.path, pattern):
            logger.debug("% is blacklisted, skipping diff")
            return (None, None)

    info = client.info2(change_uri, revision=revision, recurse=False)[0][1]
    if info.kind == pysvn.node_kind.dir:
        logger.debug("%s is a directory, skipping diff" % change.path)
        return (None, None)

    mime = client.propget('svn:mime-type', change_uri, revision=revision)
    if change_uri in mime:
        logger.debug("%s is binary (%s), skipping diff" %
                     (change.path, mime[change_uri]))
        return ('binary', change.path)

    diff = None
    if change.summarize_kind == pysvn.diff_summarize_kind.modified:
        tempdir = tempfile.mkdtemp()
        diff = client.diff(tempdir, change_uri,
                           revision1=previous,
                           revision2=revision)
        shutil.rmtree(tempdir)
    else:
        diff = ("Added: %s\n%s\n%s" %
                (change.path, SEPARATOR,
                 client.cat(change_uri, revision=revision)))
    
    if len(diff.splitlines()) > largediff:
        logger.debug("Diff for %s is large (%d lines), skipping diff" %
                     (change.path, len(diff.splitlines())))
        return ('large', change.path)
    
    if fnmatch.fnmatch(change.path, "*/Properties/*.xml"):
        logger.info("Checking out %s" % os.path.dirname(change.path))
        tempdir = tempfile.mkdtemp()
        try:
            client.checkout(os.path.join(baseuri, os.path.dirname(change.path)),
                            tempdir, revision=revision)
            xdata = \
                lxml.etree.parse(os.path.join(tempdir,
                                              os.path.basename(change.path)))
        finally:
            shutil.rmtree(tempdir)
        if xdata.getroot().get("sensitive", "false").lower() == "true":
            return ("sensitive", change.path)

    if ("/Cfg/" in change.path and
        os.path.basename(change.path) != "info.xml"):
        # try to check out an info.xml for this file
        logger.info("Checking out %s" % os.path.dirname(change.path))
        tempdir = tempfile.mkdtemp()
        # in python 2.4, try...except...finally isn't supported; you
        # have to nest a try...except block inside try...finally
        try:
            try:
                client.checkout(os.path.join(baseuri,
                                             os.path.dirname(change.path)),
                                tempdir, revision=revision)
                root = lxml.etree.parse(os.path.join(tempdir,
                                                     "info.xml")).getroot()
            except IOError:
                logger.debug("No info.xml found for %s" % change.path)
            except:
                raise
        finally:
            shutil.rmtree(tempdir)

        if root is not None:
            for el in root.xpath("//Info"):
                if el.get("sensitive", "false").lower() == "true":
                    return ("sensitive", change.path)

    return ('diff', diff)

def parse_args():
    """ parse command-line arguments """
    usage = """Usage: bcfg2_svnlog.py [options] -r <revision> <repos>"""
    parser = OptionParser(usage=usage)
    parser.add_option("-v", "--verbose", help="Be verbose", action="count")
    parser.add_option("-c", "--config", help="Config file",
                      default="/etc/bcfg2_svnlog.conf")
    parser.add_option("-r", "--rev", help="Revision")
    parser.add_option("--stdout", help="Print log message to stdout")
    try:
        (options, args) = parser.parse_args()
    except OptionError:
        parser.print_help()
        raise SystemExit(1)

    if not len(args):
        parser.print_help()
        raise SystemExit(1)

    get_logger(options.verbose)
    return (options, args.pop())

def get_config(configfile, repos_name):
    """ read config for the given repository """
    logger = get_logger()
    defaults = dict(largediff=100,
                    subject='',
                    blacklist="*/Ohai/*.json */Probes/probed.xml */SSHbase/ssh_host*_key.[GH]* */Packages/packages.conf")
    config = SafeConfigParser(defaults)
    if os.path.exists(configfile):
        config.read(configfile)
    else:
        logger.fatal("Config file %s does not exist" % configfile)
        raise SystemExit(1)

    if not config.has_section(repos_name):
        logger.fatal("No configuration section found for '%s' repo, aborting" %
                     repos_name)
        raise SystemExit(2)

    return config

def main():
    """ main subroutine """
    (options, path) = parse_args()
    uri = "file://%s" % path
    logger = get_logger()

    repos_name = os.path.basename(uri)
    config = get_config(options.config, repos_name)
    
    client = pysvn.Client()
    revision = pysvn.Revision(pysvn.opt_revision_kind.number, options.rev)
    previous = pysvn.Revision(pysvn.opt_revision_kind.number,
                              int(options.rev) - 1)
    changes = client.diff_summarize(uri,
                                    revision1=previous,
                                    revision2=revision)

    # parse log message
    log = client.log(uri, revision_end=revision)[0]
    logger.info("Examining commit %s by %s" % (options.rev, log.author))
    (message, keywords) = parse_log_message(log.message)

    summary = build_summary(changes)

    diffs = dict(diff=[], large=[], binary=[], sensitive=[])
    for change in changes:
        (dtype, ddata) = get_diff_set(change, uri,
                                      rev=int(options.rev),
                                      largediff=int(config.get(repos_name,
                                                               'largediff')))
        if dtype is not None:
            diffs[dtype].append(ddata)

    # construct the email
    body = [message.strip(),
            '',
            "Author: %s" % log.author,
            "Revision: %s" % options.rev,
            '',
            "Affected files:", '']
    for ctype in summary:
        body.extend(["%-65s %-10s" % (f, ctype) for f in summary[ctype]])
    body.append('')

    if diffs['binary']:
        body.extend([SEPARATOR, '', "The following binary files were changed:",
                     ''])
        body.extend(diffs['binary'])
        body.append('')

    if diffs['large']:
        body.extend([SEPARATOR, '',
                     "Diffs for the following files were too large to include:",
                     ''])
        body.extend(diffs['large'])
        body.append('')
        
    if diffs['sensitive']:
        body.extend([SEPARATOR, '',
                     "The following sensitive files were changed:", ''])
        body.extend(diffs['sensitive'])
        body.append('')

    if diffs['diff']:
        body.extend([SEPARATOR, '', "The following files were changed:", ''])
        body.extend(diffs['diff'])

    if keywords['resolve']:
        body.extend(['',
                     "RT-AddRefersTo: %s" % keywords['resolve'],
                     "RT-AddReferredToBy: %s" % keywords['resolve'],
                     "RT-ResolveTicket: %s" % keywords['resolve']])

    if config.has_option(repos_name, 'email') and not options.stdout:
        msg = Message()
        msg.set_payload("\n".join(body))
        subject = None
        if keywords['subject']:
            subject = keywords['subject']
        elif "\n" in message:
            subject = message[0:max(120, message.index("\n"))]
        else:
            subject = message[0:120]
        msg['Subject'] = "%s %s" % (config.get(repos_name, 'subject'), subject)

        msg['From'] = get_author_email(log.author)
        msg['To'] = config.get(repos_name, 'email')

        logger.debug("Sending message from %s to %s: %s" % (msg['From'],
                                                            msg['To'],
                                                            msg['Subject']))
        smtp = smtplib.SMTP('localhost')
        if options.verbose > 2:
            # this is _really_ verbose
            smtp.set_debuglevel(options.verbose - 1)
        smtp.sendmail(msg['From'], [msg['To']], msg.as_string())
        smtp.quit()
    else:
        print("\n".join(body))
 
if __name__ == "__main__":
    sys.exit(main())



########NEW FILE########
__FILENAME__ = create-debian-pkglist-gp
#!/usr/bin/env python

'''Build debian/ubuntu package indexes'''

# Original code from Bcfg2 sources

import gzip
import os
import sys
import subprocess

# Compatibility imports
from Bcfg2.Compat import StringIO
from Bcfg2.Compat import ConfigParser
from Bcfg2.Compat import urlopen

def debug(msg):
    '''print debug messages'''
    if '-v' in sys.argv:
        sys.stdout.write(msg)


def get_as_list(somestring):
    """ Input : a string like this : 'a, g, f,w'
        Output : a list like this : ['a', 'g', 'f', 'w'] """
    return somestring.replace(' ', '').split(',')


def list_contains_all_the_same_values(l):
    if len(l) == 0:
        return True
    # The list contains all the same values if all elements in
    # the list are equal to the first element.
    first = l[0]
    for elem in l:
        if first != elem:
            return False
    return True


class SourceURL:
    def __init__(self, deb_url):
        deb_url_tokens = deb_url.split()
        # ex: deb http://somemirror.com/ubuntu dapper main restricted universe
        self.url = deb_url_tokens[1]
        self.distribution = deb_url_tokens[2]
        self.sections = deb_url_tokens[3:]

    def __str__(self):
        return "deb %s %s %s" % (self.url, self.distribution, ' '.join(self.sections))

    def __repr__(self):
        return "<%s %s>" % (self.__class__.__name__, str(self))


class Source:
    def __init__(self, confparser, section, bcfg2_repos_prefix):
        self.filename = "%s/Pkgmgr/%s.xml" % (bcfg2_repos_prefix, section)
        self.groups = get_as_list(confparser.get(section, "group_names"))
        self.priority = confparser.getint(section, "priority")
        self.architectures = get_as_list(confparser.get(section, "architectures"))

        self.source_urls = []
        self.source_urls.append(SourceURL(confparser.get(section, "deb_url")))
        # Agregate urls in the form of deb_url0, deb_url1, ... to deb_url9
        for i in range(10):  # 0 to 9
            option_name = "deb_url%s" % i
            if confparser.has_option(section, option_name):
                self.source_urls.append(SourceURL(confparser.get(section, option_name)))

        self.file = None
        self.indent_level = 0

    def __str__(self):
        return """File: %s
Groups: %s
Priority: %s
Architectures: %s
Source URLS: %s""" % (self.filename, self.groups, self.priority, self.architectures, self.source_urls)

    def __repr__(self):
        return "<%s %s>" % (self.__class__.__name__, str(self))

    def _open_file(self):
        self.file = open(self.filename + '~', 'w')

    def _close_file(self):
        self.file.close()

    def _write_to_file(self, msg):
        self.file.write("%s%s\n" % (self.indent_level * '    ', msg))

    def _rename_file(self):
        os.rename(self.filename + '~', self.filename)

    def _pkg_version_is_older(self, version1, version2):
        """ Use dpkg to compare the two version
            Return true if version1 < version2 """
        # Avoid forking a new process if the two strings are equals
        if version1 == version2:
            return False
        (status, output) = subprocess.getstatusoutput("/usr/bin/dpkg --compare-versions %s lt %s" % (version1,
                                                                                                     version2))
        #print "%s dpkg --compare-versions %s lt %s" % (status, version1, version2)
        return status == 0

    def _update_pkgdata(self, pkgdata, source_url):
        for section in source_url.sections:
            for arch in self.architectures:
                url = "%s/dists/%s/%s/binary-%s/Packages.gz" % (source_url.url, source_url.distribution, section, arch)
                debug("Processing url %s\n" % (url))
                try:
                    data = urlopen(url)
                    buf = StringIO(''.join(data.readlines()))
                    reader = gzip.GzipFile(fileobj=buf)
                    for line in reader.readlines():
                        if line[:8] == 'Package:':
                            pkgname = line.split(' ')[1].strip()
                        elif line[:8] == 'Version:':
                            version = line.split(' ')[1].strip()
                            if pkgname in pkgdata:
                                if arch in pkgdata[pkgname]:
                                    # The package is listed twice for the same architecture
                                    # We keep the most recent version
                                    old_version = pkgdata[pkgname][arch]
                                    if self._pkg_version_is_older(old_version, version):
                                        pkgdata[pkgname][arch] = version
                                else:
                                    # The package data exists for another architecture,
                                    # but not for this one. Add it.
                                    pkgdata[pkgname][arch] = version
                            else:
                                # First entry for this package
                                pkgdata[pkgname] = {arch: version}
                        else:
                            continue
                except:
                    raise Exception("Could not process URL %s\n%s\nPlease "
                                    "verify the URL." % (url, sys.exc_info()[1]))
        return pkgdata

    def _get_sorted_pkg_keys(self, pkgdata):
        pkgs = []
        for k in list(pkgdata.keys()):
            pkgs.append(k)
        pkgs.sort()
        return pkgs

    def _write_common_entries(self, pkgdata):
        # Write entries for packages that have the same version
        # across all architectures
        #coalesced = 0
        for pkg in self._get_sorted_pkg_keys(pkgdata):
            # Dictionary of archname: pkgversion
            # (There is exactly one version per architecture)
            archdata = pkgdata[pkg]
            # List of versions for all architectures of this package
            pkgversions = list(archdata.values())
            # If the versions for all architectures are the same
            if list_contains_all_the_same_values(pkgversions):
                # Write the package data
                ver = pkgversions[0]
                self._write_to_file('<Package name="%s" version="%s"/>' % (pkg, ver))
                #coalesced += 1
                # Remove this package entry
                del pkgdata[pkg]

    def _write_perarch_entries(self, pkgdata):
        # Write entries that are left, i.e. packages that have different
        # versions per architecture
        #perarch = 0
        if pkgdata:
            for arch in self.architectures:
                self._write_to_file('<Group name="%s">' % (arch))
                self.indent_level = self.indent_level + 1
                for pkg in self._get_sorted_pkg_keys(pkgdata):
                    if arch in pkgdata[pkg]:
                        self._write_to_file('<Package name="%s" version="%s"/>' % (pkg, pkgdata[pkg][arch]))
                        #perarch += 1
                self.indent_level = self.indent_level - 1
                self._write_to_file('</Group>')
        #debug("Got %s coalesced, %s per-arch\n" % (coalesced, perarch))

    def process(self):
        '''Build package indices for source'''

        # First, build the pkgdata structure without touching the file,
        # so the file does not contain incomplete informations if the
        # network in not reachable.
        pkgdata = {}
        for source_url in self.source_urls:
            pkgdata = self._update_pkgdata(pkgdata, source_url)

        # Construct the file.
        self._open_file()
        for source_url in self.source_urls:
            self._write_to_file('<!-- %s -->' % source_url)

        self._write_to_file('<PackageList priority="%s" type="deb">' % self.priority)

        self.indent_level = self.indent_level + 1
        for group in self.groups:
            self._write_to_file('<Group name="%s">' % group)
            self.indent_level = self.indent_level + 1

        self._write_common_entries(pkgdata)
        self._write_perarch_entries(pkgdata)

        for group in self.groups:
            self.indent_level = self.indent_level - 1
            self._write_to_file('</Group>')
        self.indent_level = self.indent_level - 1
        self._write_to_file('</PackageList>')
        self._close_file()
        self._rename_file()

if __name__ == '__main__':
    main_conf_parser = ConfigParser.SafeConfigParser()
    main_conf_parser.read(['/etc/bcfg2.conf'])
    repo = main_conf_parser.get('server', 'repository')

    confparser = ConfigParser.SafeConfigParser()
    confparser.read(os.path.join(repo, "etc/debian-pkglist.conf"))

    # We read the whole configuration file before processing each entries
    # to avoid doing work if there is a problem in the file.
    sources_list = []
    for section in confparser.sections():
        sources_list.append(Source(confparser, section, repo))

    for source in sources_list:
        source.process()

########NEW FILE########
__FILENAME__ = create-debian-pkglist
#!/usr/bin/env python

'''Build debian/ubuntu package indexes'''

# Original code from Bcfg2 sources

import apt_pkg
import gzip
import os
import re
import sys

# Compatibility imports
from Bcfg2.Compat import StringIO
from Bcfg2.Compat import ConfigParser
from Bcfg2.Compat import urlopen

apt_pkg.init()


def debug(msg):
    '''print debug messages'''
    if '-v' in sys.argv:
        sys.stdout.write(msg)


def get_as_list(somestring):
    """ Input : a string like this : 'a, g, f,w'
        Output : a list like this : ['a', 'g', 'f', 'w'] """
    return somestring.replace(' ', '').split(',')


def list_contains_all_the_same_values(l):
    if len(l) == 0:
        return True
    # The list contains all the same values if all elements in
    # the list are equal to the first element.
    first = l[0]
    for elem in l:
        if first != elem:
            return False
    return True


class SourceURL:
    def __init__(self, deb_url, arch):
        deb_url_tokens = deb_url.split()
        # ex: deb http://somemirror.com/ubuntu dapper main restricted universe
        self.url = deb_url_tokens[1]
        self.distribution = deb_url_tokens[2]
        self.sections = deb_url_tokens[3:]
        self.arch = arch

    def __str__(self):
        return "deb %s %s %s" % (self.url, self.distribution, ' '.join(self.sections))

    def __repr__(self):
        return "<%s %s>" % (self.__class__.__name__, str(self))


class Source:
    def __init__(self, confparser, section, bcfg2_repos_prefix):
        self.filename = "%s/Pkgmgr/%s.xml" % (bcfg2_repos_prefix, section)
        self.groups = get_as_list(confparser.get(section, "group_names"))
        self.priority = confparser.getint(section, "priority")
        try:
            self.pattern = confparser.get(section, "pattern", raw=True)
        except:
            self.pattern = '.*'
        self.architectures = get_as_list(confparser.get(section, "architectures"))
        self.arch_specialurl = set()

        self.source_urls = []
        self.source_urls.append(SourceURL(confparser.get(section, "deb_url"),
                                          "all"))
        # Agregate urls in the form of deb_url0, deb_url1, ... to deb_url9
        for i in range(10):  # 0 to 9
            option_name = "deb_url%s" % i
            if confparser.has_option(section, option_name):
                self.source_urls.append(SourceURL(confparser.get(section,
                                                                 option_name),
                                                  "all"))

        # Aggregate architecture specific urls (if present)
        for arch in self.architectures:
            if not confparser.has_option(section, "deb_" + arch + "_url"):
                continue
            self.source_urls.append(SourceURL(confparser.get(section,
                                                             "deb_" + arch + "_url"),
                                              arch))
            # Agregate urls in the form of deb_url0, deb_url1, ... to deb_url9
            for i in range(10):  # 0 to 9
                option_name = "deb_" + arch + "_url%s" % i
                if confparser.has_option(section, option_name):
                    self.source_urls.append(SourceURL(confparser.get(section,
                                                                     option_name),
                                                      arch))
                    self.arch_specialurl.add(arch)

        self.file = None
        self.indent_level = 0

    def __str__(self):
        return """File: %s
Groups: %s
Priority: %s
Architectures: %s
Source URLS: %s""" % (self.filename, self.groups, self.priority, self.architectures, self.source_urls)

    def __repr__(self):
        return "<%s %s>" % (self.__class__.__name__, str(self))

    def _open_file(self):
        self.file = open(self.filename + '~', 'w')

    def _close_file(self):
        self.file.close()

    def _write_to_file(self, msg):
        self.file.write("%s%s\n" % (self.indent_level * '    ', msg))

    def _rename_file(self):
        os.rename(self.filename + '~', self.filename)

    def _pkg_version_is_older(self, version1, version2):
        """ Use dpkg to compare the two version
            Return true if version1 < version2 """
        # Avoid forking a new process if the two strings are equals
        if version1 == version2:
            return False
        status = apt_pkg.VersionCompare(version1, version2)
        return status < 0

    def _update_pkgdata(self, pkgdata, source_url):
        for section in source_url.sections:
            for arch in self.architectures:
                if source_url.arch != arch and source_url.arch != "all":
                    continue
                if source_url.arch == "all" and arch in self.arch_specialurl:
                    continue
                url = "%s/dists/%s/%s/binary-%s/Packages.gz" % (source_url.url,
                                                                source_url.distribution,
                                                                section,
                                                                arch)
                debug("Processing url %s\n" % (url))
                try:
                    data = urlopen(url)
                    buf = StringIO(''.join(data.readlines()))
                    reader = gzip.GzipFile(fileobj=buf)
                    for line in reader.readlines():
                        if line[:8] == 'Package:':
                            pkgname = line.split(' ')[1].strip()
                        elif line[:8] == 'Version:':
                            version = line.split(' ')[1].strip()
                            if pkgname in pkgdata:
                                if arch in pkgdata[pkgname]:
                                    # The package is listed twice for the same architecture
                                    # We keep the most recent version
                                    old_version = pkgdata[pkgname][arch]
                                    if self._pkg_version_is_older(old_version, version):
                                        pkgdata[pkgname][arch] = version
                                else:
                                    # The package data exists for another architecture,
                                    # but not for this one. Add it.
                                    pkgdata[pkgname][arch] = version
                            else:
                                # First entry for this package
                                pkgdata[pkgname] = {arch: version}
                        else:
                            continue
                except:
                    raise Exception("Could not process URL %s\n%s\nPlease "
                                    "verify the URL." % (url, sys.exc_info()[1]))
        return dict((k, v) for (k, v) in list(pkgdata.items()) \
                    if re.search(self.pattern, k))

    def _get_sorted_pkg_keys(self, pkgdata):
        pkgs = []
        for k in list(pkgdata.keys()):
            pkgs.append(k)
        pkgs.sort()
        return pkgs

    def _write_common_entries(self, pkgdata):
        # Write entries for packages that have the same version
        # across all architectures
        #coalesced = 0
        for pkg in self._get_sorted_pkg_keys(pkgdata):
            # Dictionary of archname: pkgversion
            # (There is exactly one version per architecture)
            archdata = pkgdata[pkg]
            # List of versions for all architectures of this package
            pkgversions = list(archdata.values())
            # If the versions for all architectures are the same
            if len(self.architectures) == len(pkgversions) and list_contains_all_the_same_values(pkgversions):
                # Write the package data
                ver = pkgversions[0]
                self._write_to_file('<Package name="%s" version="%s"/>' % (pkg, ver))
                #coalesced += 1
                # Remove this package entry
                del pkgdata[pkg]

    def _write_perarch_entries(self, pkgdata):
        # Write entries that are left, i.e. packages that have different
        # versions per architecture
        #perarch = 0
        if pkgdata:
            for arch in self.architectures:
                self._write_to_file('<Group name="%s">' % (arch))
                self.indent_level = self.indent_level + 1
                for pkg in self._get_sorted_pkg_keys(pkgdata):
                    if arch in pkgdata[pkg]:
                        self._write_to_file('<Package name="%s" version="%s"/>' % (pkg, pkgdata[pkg][arch]))
                        #perarch += 1
                self.indent_level = self.indent_level - 1
                self._write_to_file('</Group>')
        #debug("Got %s coalesced, %s per-arch\n" % (coalesced, perarch))

    def process(self):
        '''Build package indices for source'''

        # First, build the pkgdata structure without touching the file,
        # so the file does not contain incomplete informations if the
        # network in not reachable.
        pkgdata = {}
        for source_url in self.source_urls:
            pkgdata = self._update_pkgdata(pkgdata, source_url)

        # Construct the file.
        self._open_file()
        for source_url in self.source_urls:
            self._write_to_file('<!-- %s -->' % source_url)

        self._write_to_file('<PackageList priority="%s" type="deb">' % self.priority)

        self.indent_level = self.indent_level + 1
        for group in self.groups:
            self._write_to_file('<Group name="%s">' % group)
            self.indent_level = self.indent_level + 1

        self._write_common_entries(pkgdata)
        self._write_perarch_entries(pkgdata)

        for group in self.groups:
            self.indent_level = self.indent_level - 1
            self._write_to_file('</Group>')
        self.indent_level = self.indent_level - 1
        self._write_to_file('</PackageList>')
        self._close_file()
        self._rename_file()

if __name__ == '__main__':
    # Prefix is relative to script path
    complete_script_path = os.path.join(os.getcwd(), sys.argv[0])
    prefix = complete_script_path[:-len('etc/create-debian-pkglist.py')]

    confparser = ConfigParser.SafeConfigParser()
    confparser.read(prefix + "etc/debian-pkglist.conf")

    # We read the whole configuration file before processing each entries
    # to avoid doing work if there is a problem in the file.
    sources_list = []
    for section in confparser.sections():
        sources_list.append(Source(confparser, section, prefix))

    for source in sources_list:
        source.process()

########NEW FILE########
__FILENAME__ = create-rpm-pkglist
#!/usr/bin/env python
#
# Copyright (c) 2010  Fabian Affolter, Bernewireless.net.
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without modification,
# are permitted provided that the following conditions are met:

# * Redistributions of source code must retain the above copyright notice, this
# list of conditions and the following disclaimer.
# * Redistributions in binary form must reproduce the above copyright notice,
# this list of conditions and the following disclaimer in the documentation
# and/or other materials provided with the distribution.
# * Neither the name of the Bernewireless nor the names of its contributors
# may be used to endorse or promote products derived from this software
# without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ''AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR ANY
# DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
# (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
# ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
#
# Author: Fabian Affolter <fabian at bernewireless.net>
#

from lxml import etree
from optparse import OptionParser
import os
import yum

__author__ = 'Fabian Affolter <fabian@bernewireless.net>'
__version__ = '0.1'


def retrievePackages():
    """Getting the installed packages with yum."""
    yb = yum.YumBase()
    yb.conf.cache = os.geteuid() != 1
    pl = yb.doPackageLists('installed')
    pkglist = []
    for pkg in sorted(pl.installed):
        pkgdata = pkg.name, pkg.version
        pkglist.append(pkgdata)

    return pkglist


def parse_command_line_parameters():
    """Parses command line arguments."""
    usage = "usage: %prog [options]"
    version = 'Version: %prog ' + __version__
    parser = OptionParser(usage, version=version)
    parser.add_option("-s", "--show", action="store_true",
                      help="Prints the result to STOUT")
    parser.add_option("-v", "--pkgversion", action="store_true",
                      help="Include Package version")
    parser.add_option("-f", "--filename", dest="filename",
                      type="string",
                      metavar="FILE", default="packages.xml",
                      help="Write the output to an XML FILE")

    (options, args) = parser.parse_args()
    num_args = 1

    return options, args


def indent(elem, level=0):
    """Helps clean up the XML."""
    # Stolen from http://effbot.org/zone/element-lib.htm
    i = "\n" + level * "  "
    if len(elem):
        if not elem.text or not elem.text.strip():
            elem.text = i + "  "
        for e in elem:
            indent(e, level + 1)
            if not e.tail or not e.tail.strip():
                e.tail = i + "  "
        if not e.tail or not e.tail.strip():
            e.tail = i
    else:
        if level and (not elem.tail or not elem.tail.strip()):
            elem.tail = i


def transformXML():
    """Transform the package list to an XML file."""
    packagelist = retrievePackages()
    root = etree.Element("PackageList")
    for i, j in packagelist:
        root.append(etree.Element("Package", name=i, version=j))
    #Print the content
    #print(etree.tostring(root, pretty_print=True))
    tree = etree.ElementTree(root)
    return tree


def main():
    options, args = parse_command_line_parameters()
    filename = options.filename
    packagelist = transformXML()

    if options.show == True:
        tree = etree.parse(filename)
        for node in tree.findall("//Package"):
            print(node.attrib["name"])
        indent(packagelist.getroot())
        packagelist.write(filename, encoding="utf-8")

    if options.pkgversion == True:
        tree = etree.parse(filename)
        for node in tree.findall("//Package"):
            print("%s-%s" % (node.attrib["name"], node.attrib["version"]))

#FIXME : This should be changed to the standard way of optparser
#FIXME : Make an option available to strip the version number of the pkg
    if options.pkgversion == None and options.show == None:
        indent(packagelist.getroot())
        packagelist.write(filename, encoding="utf-8")

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = export
#!/usr/bin/env python
# encoding: utf-8

"""
Second attempt to make our export script more portable than export.sh

"""

import fileinput
from subprocess import Popen, PIPE
import sys
# This will need to be replaced with argsparse when we make a 2.7+/3.2+ version
import optparse
import datetime

# py3k compatibility
try:
    from email.Utils import formatdate
except ImportError:
    from email.utils import formatdate

# In lieu of a config file
help_message = \
    """This script creates a tag in the Bcfg2 git repo and exports
a tar file of the code at that tag.

This script must be run at the top of your git repository.
"""


pkgname = 'bcfg2'
ftphost = 'terra.mcs.anl.gov'
ftpdir = '/mcs/ftp/pub/bcfg'


def run(command):
    return Popen(command, shell=True, stdout=PIPE).communicate()


def find_and_replace(f, iftest, rline, startswith=False, dryrun=False):
    if dryrun:
        inplace = 0
        print("*** dry-run: New '%s' will look like this:" % f)
    else:
        inplace = 1
    for line in fileinput.input(f, inplace):
        if startswith:
            if line.startswith(iftest):
                line = line.replace(line, rline)
            sys.stdout.write(line)
        else:
            if iftest in line and line != "Version: %{version}\n":
                line = line.replace(line, rline)
            sys.stdout.write(line)
    if dryrun:
        print("*** End '%s'" % f)


def main():
    # This is where the options are set up
    p = optparse.OptionParser(description=help_message,
                              prog=sys.argv[0],
                              version='0.1',
                              usage='%prog [-h|--help] [-v|--version] '
                                    '[-n|--dry-run] [-d|--debug]')
    p.add_option('--verbose', '-v',
                 action='store_true',
                 help='turns on verbose mode',
                 default=False,
                 dest='verbose')
    p.add_option('--dry-run', '-n',
                 action='store_true',
                 help='run in dry-run mode; '
                      'no changes will be made to the system',
                 default=False,
                 dest='dryrun')
    p.add_option('--debug', '-d',
                 action='store_true',
                 help='run in debun mode',
                 default=False,
                 dest='debug')
    p.add_option('--paranoid', '-P',
                 action='store_true',
                 help='run in paranoid mode, '
                      'make changes but do not commit to repository',
                 default=False,
                 dest='paranoid')
    options = p.parse_args()[0]

    if options.debug:
        print(options)
        print("What should debug mode do?")

    # py3k compatibility
    try:
        version = raw_input("Please enter the Bcfg2 version "
                            "you are tagging (e.g. 1.0.0): ")
        name = raw_input("Your name: ")
        email = raw_input("Your email: ")
    except NameError:
        version = input("Please enter the Bcfg2 version "
                        "you are tagging (e.g. 1.0.0): ")
        name = input("Your name: ")
        email = input("Your email: ")

    # parse version into Major.Minor.MicroBuild and validate
    vkeys = ["major", "minor", "microbuild"]
    try:
        version_info = dict(zip(vkeys, version.split(".")))
        version_info["micro"] = version_info["microbuild"][0:1]
        version_info["build"] = version_info["microbuild"][1:]
        version_release = "%s.%s.%s" % (version_info['major'],
                                        version_info['minor'],
                                        version_info['micro'])

        if options.debug:
            print("version is %s" % version)
            print("version_info is %s" % version_info)
            print("version_release is %s" % version_release)

        if not version_info["major"].isdigit() \
           or not version_info["minor"].isdigit() \
           or not version_info["micro"]:
            raise VersionError('isdigit() test failed')
        if len(version_info["micro"]) > 1:
            raise VersionError('micro must be single digit because '
                               'IFMinorVersion restrictions in '
                               'Mac OS X Packaging')
    except:
        print("""Version must be of the form Major.Minor.MicroBuild,
where Major and Minor are integers and
Micro is a single digit optionally followed by Build (i.e. pre##)
E.G. 1.2.0pre1 is a valid version.
""")
        quit()

    tarname = '/tmp/%s-%s.tar.gz' % (pkgname, version)

    newchangelog = """bcfg2 (%s-0.0) unstable; urgency=low

  * New upstream release

 -- %s <%s>  %s

""" % (version,
       name,
       email,
       formatdate(localtime=True))

    # write out the new debian changelog
    if options.dryrun:
        print("*** Add the following to the top of debian/changelog:\n%s\n"
              % newchangelog)
    else:
        try:
            with open('debian/changelog', 'r+') as f:
                old = f.read()
                f.seek(0)
                f.write(newchangelog + old)
            f.close()
        except:
            print("Problem opening debian/changelog")
            print(help_message)
            quit()

    # update solaris version
    find_and_replace('solaris/Makefile', 'VERS=',
                     'VERS=%s-1\n' % version,
                     startswith=True,
                     dryrun=options.dryrun)
    find_and_replace('solaris/pkginfo.bcfg2', 'VERSION=',
                     'VERSION="%s"\n' % version,
                     startswith=True,
                     dryrun=options.dryrun)
    find_and_replace('solaris/pkginfo.bcfg2-server', 'VERSION=',
                     'VERSION="%s"\n' % version,
                     startswith=True,
                     dryrun=options.dryrun)
    # update solaris IPS version
    find_and_replace('solaris-ips/Makefile', 'VERS=',
                     'VERS=%s-1\n' % version,
                     startswith=True,
                     dryrun=options.dryrun)
    find_and_replace('solaris-ips/MANIFEST.bcfg2.header',
                     'set name=pkg.fmri value="pkg://bcfg2/bcfg2@',
                     'set name=pkg.fmri value="pkg://bcfg2/bcfg2@%s"\n' % version,
                     startswith=True,
                     dryrun=options.dryrun)
    find_and_replace('solaris-ips/MANIFEST.bcfg2-server.header',
                     'set name=pkg.fmri value="pkg://bcfg2/bcfg2-server@',
                     'set name=pkg.fmri value="pkg://bcfg2/bcfg2-server@%s"\n' % version,
                     startswith=True,
                     dryrun=options.dryrun)
    find_and_replace('solaris-ips/pkginfo.bcfg2', 'VERSION=',
                     'VERSION="%s"\n' % version,
                     startswith=True,
                     dryrun=options.dryrun)
    find_and_replace('solaris-ips/pkginfo.bcfg2-server', 'VERSION=',
                     'VERSION="%s"\n' % version,
                     startswith=True,
                     dryrun=options.dryrun)
    # set new version in Bcfg2/version.py
    find_and_replace('src/lib/Bcfg2/version.py',
                     '__version__ =',
                     '__version__ = "%s"\n' % version,
                     dryrun=options.dryrun)
    # replace version in misc/bcfg2.spec and misc/bcfg2-selinux.spec
    find_and_replace('misc/bcfg2.spec', 'Version:',
                     'Version:          %s\n' % version_release,
                     dryrun=options.dryrun)
    find_and_replace('misc/bcfg2-selinux.spec', 'Version:',
                     'Version:          %s\n' % version_release,
                     dryrun=options.dryrun)
    if version_info['build'].startswith('rc'):
        find_and_replace('misc/bcfg2.spec', 'global _rc ',
                         '%%global _rc %s\n' % version_info['build'],
                         dryrun=options.dryrun)
        find_and_replace('misc/bcfg2-selinux.spec', 'global _rc ',
                         '%%global _rc %s\n' % version_info['build'],
                         dryrun=options.dryrun)
    elif version_info['build'].startswith('pre'):
        find_and_replace('misc/bcfg2.spec', 'global _pre ',
                         '%%global _pre %s\n' % version_info['build'],
                         dryrun=options.dryrun)
        find_and_replace('misc/bcfg2-selinux.spec', 'global _pre ',
                         '%%global _pre %s\n' % version_info['build'],
                         dryrun=options.dryrun)
    else:
        # comment out pre/rc
        find_and_replace('misc/bcfg2.spec', 'global _pre ',
                         '#%%global _pre 2\n',
                         dryrun=options.dryrun)
        find_and_replace('misc/bcfg2-selinux.spec', 'global _pre ',
                         '#%%global _pre 2\n',
                         dryrun=options.dryrun)
        find_and_replace('misc/bcfg2.spec', 'global _rc ',
                         '#%%global _rc 1\n',
                         dryrun=options.dryrun)
        find_and_replace('misc/bcfg2-selinux.spec', 'global _rc ',
                         '#%%global _rc 1\n',
                         dryrun=options.dryrun)

        find_and_replace('misc/bcfg2.spec', 'Release: ',
                         'Release:          1%{?_pre_rc}%{?dist}\n',
                         startswith=True,
                         dryrun=options.dryrun)
        find_and_replace('misc/bcfg2-selinux.spec', 'Release: ',
                         'Release:          1%{?_pre_rc}%{?dist}\n',
                         startswith=True,
                         dryrun=options.dryrun)
    find_and_replace('misc/bcfg2.spec', '%setup',
                     '%setup -q -n %{name}-%{version}%{?_pre_rc}\n',
                     startswith=True,
                     dryrun=options.dryrun)
    find_and_replace('misc/bcfg2-selinux.spec', '%setup',
                     '%setup -q -n %{name}-%{version}%{?_pre_rc}\n',
                     startswith=True,
                     dryrun=options.dryrun)
    find_and_replace('misc/bcfg2.spec', 'BuildRoot',
                     'BuildRoot:        %%{_tmppath}/%%{name}-%%{version}%s-%%{release}-root-%%(%%{__id_u} -n)\n' %
                     version_info['build'],
                     startswith=True,
                     dryrun=options.dryrun)
    find_and_replace('misc/bcfg2-selinux.spec', 'BuildRoot',
                     'BuildRoot:        %%{_tmppath}/%%{name}-%%{version}%s-%%{release}-root-%%(%%{__id_u} -n)\n' %
                     version_info['build'],
                     startswith=True,
                     dryrun=options.dryrun)
    # fix pre problem noted in
    # http://trac.mcs.anl.gov/projects/bcfg2/ticket/1129
    find_and_replace('misc/bcfg2.spec',
                     'Source0',
                     'Source0:          ftp://ftp.mcs.anl.gov/pub/bcfg/%{name}-%{version}%{?_pre_rc}.tar.gz\n',
                     startswith=True,
                     dryrun=options.dryrun)
    find_and_replace('misc/bcfg2-selinux.spec',
                     'Source0',
                     'Source0:          ftp://ftp.mcs.anl.gov/pub/bcfg/%{name}-%{version}%{?_pre_rc}.tar.gz\n',
                     startswith=True,
                     dryrun=options.dryrun)
    # update the version in reports
    find_and_replace('src/lib/Bcfg2/Reporting/templates/base.html',
                     'Bcfg2 Version',
                     '    <span>Bcfg2 Version %s</span>\n' % version,
                     dryrun=options.dryrun)
    # update the version in the docs
    find_and_replace('doc/conf.py', 'version =',
                     'version = \'%s.%s\'\n' % (version_info['major'],
                                                version_info['minor']),
                     startswith=True,
                     dryrun=options.dryrun)
    find_and_replace('doc/conf.py', 'release =',
                     'release = \'%s\'\n' % (version_release),
                     startswith=True,
                     dryrun=options.dryrun)
    # update osx Makefile
    find_and_replace('osx/Makefile', 'BCFGVER =',
                     'BCFGVER = %s\n' % (version),
                     startswith=True,
                     dryrun=options.dryrun)
    find_and_replace('osx/Makefile', 'MAJOR =',
                     'MAJOR = %s\n' % (version_info['major']),
                     startswith=True,
                     dryrun=options.dryrun)
    find_and_replace('osx/Makefile', 'MINOR =',
                     'MINOR = %s%s\n' % (version_info['minor'],
                                         version_info['micro']),
                     startswith=True,
                     dryrun=options.dryrun)
    # update osx Portfile
    find_and_replace('osx/macports/Portfile', 'version ',
                     'version             %s\n' % version,
                     startswith=True,
                     dryrun=options.dryrun)

    # tag the release
    #FIXME: do this using python-dulwich
    commando = {}

    commando["vcs_diff"] = "git diff"

    commando["vcs_commit"] = "git commit -asm 'Version bump to %s'" % version

    # NOTE: This will use the default email address key. If you want to sign
    #       the tag using a different key, you will need to set 'signingkey'
    #       to the proper value in the [user] section of your git
    #       configuration.
    commando["vcs_tag"] = "git tag -s v%s -m 'tagged %s release'" % (version,
                                                                     version)

    commando["create_archive"] = \
            "git archive --format=tar --prefix=%s-%s/ v%s | gzip > %s" \
            % (pkgname, version, version, tarname)

    commando["gpg_encrypt"] = "gpg --armor --output %s.gpg --detach-sig  %s" \
            % (tarname, tarname)

    # upload release to ftp
    commando["scp_archive"] = "scp %s* terra.mcs.anl.gov:/mcs/ftp/pub/bcfg/" \
            % tarname

    # Execute the commands
    if options.paranoid:
        commando_orders = ["vcs_diff"]
    else:
        commando_orders = ["vcs_commit",
                           "vcs_tag",
                           "create_archive",
                           "gpg_encrypt",
                           "scp_archive"]

    if options.dryrun:
        for cmd in commando_orders:
            print("*** dry-run: %s" % commando[cmd])
    else:
        for cmd in commando_orders:
            output = run(commando[cmd])[0].strip()
            if options.verbose:
                print(output)
                print("Ran '%s' with above output." % cmd)

if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = git_commit
#!/usr/bin/env python
""" Trigger script to commit selected changes to a local repository
back to git.  To use this script, enable the Trigger plugin, put this
script in /var/lib/bcfg2/Trigger/, and create /etc/bcfg2-commit.conf.

The config file, /etc/bcfg2-commit.conf, may contain four options in
the [global] section:

* "config" is the path to the Bcfg2 server config file.  (Default:
  /etc/bcfg2.conf)
* "commit" is a comma-separated list of globs giving the paths that
  should be committed back to the repository.  Default is 'SSLCA/*,
  SSHbase/*, Cfg/*', which will commit data back for SSLCA, SSHbase,
  Cfg, FileProbes, etc., but not, for instance, Probes/probed.xml.
  You may wish to add Metadata/clients.xml to the commit list.
* "debug" and "verbose" let you set the log level for git_commit.py
  itself.
"""


import os
import sys
import git
import logging
import Bcfg2.Logger
import Bcfg2.Options
from Bcfg2.Compat import ConfigParser
from fnmatch import fnmatch

# config file path
CONFIG = "/etc/bcfg2-commit.conf"

# config defaults.  all config options are in the [global] section
DEFAULTS = dict(config='/etc/bcfg2.conf',
                commit="SSLCA/*, SSHbase/*, Cfg/*")


def list_changed_files(repo):
    return [d for d in repo.index.diff(None)
            if (d.a_blob is not None and not d.deleted_file and
                not d.renamed and not d.new_file)]


def add_to_commit(patterns, path, repo, relpath):
    progname = os.path.basename(sys.argv[0])
    logger = logging.getLogger(progname)
    for pattern in patterns:
        if fnmatch(path, os.path.join(relpath, pattern)):
            logger.debug("%s: Adding %s to commit" % (progname, path))
            repo.index.add([path])
            return True
    return False


def parse_options():
    config = ConfigParser.SafeConfigParser(DEFAULTS)
    config.read(CONFIG)

    optinfo = dict(
        profile=Bcfg2.Options.CLIENT_PROFILE,
        dryrun=Bcfg2.Options.CLIENT_DRYRUN,
        groups=Bcfg2.Options.Option("Groups",
                                    default=[],
                                    cmd="-g",
                                    odesc='<group>:<group>',
                                    cook=Bcfg2.Options.colon_split))
    optinfo.update(Bcfg2.Options.CLI_COMMON_OPTIONS)
    optinfo.update(Bcfg2.Options.SERVER_COMMON_OPTIONS)
    argv = [Bcfg2.Options.CFILE.cmd, config.get("global", "config")]
    argv.extend(sys.argv[1:])
    setup = Bcfg2.Options.OptionParser(optinfo, argv=argv)
    setup.parse(argv)

    setup['commit'] = Bcfg2.Options.list_split(config.get("global",
                                                          "commit"))
    for opt in ['debug', 'verbose']:
        try:
            setup[opt] = config.getboolean("global", opt)
        except ConfigParser.NoOptionError:
            pass

    try:
        hostname = setup['args'][0]
    except IndexError:
        print(setup.hm)
        raise SystemExit(1)
    return (setup, hostname)


def setup_logging(setup):
    progname = os.path.basename(sys.argv[0])
    log_args = dict(to_syslog=setup['syslog'], to_console=sys.stdout.isatty(),
                    to_file=setup['logging'], level=logging.WARNING)
    if setup['debug']:
        log_args['level'] = logging.DEBUG
    elif setup['verbose']:
        log_args['level'] = logging.INFO
    Bcfg2.Logger.setup_logging(progname, **log_args)
    return logging.getLogger(progname)


def main():
    progname = os.path.basename(sys.argv[0])
    setup, hostname = parse_options()
    logger = setup_logging(setup)
    if setup['dryrun']:
        logger.info("%s: In dry-run mode, changes will not be committed" %
                    progname)

    if setup['vcs_root']:
        gitroot = os.path.realpath(setup['vcs_root'])
    else:
        gitroot = os.path.realpath(setup['repo'])
    logger.info("%s: Using Git repo at %s" % (progname, gitroot))
    try:
        repo = git.Repo(gitroot)
    except:  # pylint: disable=W0702
        logger.error("%s: Error setting up Git repo at %s: %s" %
                     (progname, gitroot, sys.exc_info()[1]))
        return 1

    # canonicalize the repo path so that git will recognize it as
    # being inside the git repo
    bcfg2root = os.path.realpath(setup['repo'])

    if not bcfg2root.startswith(gitroot):
        logger.error("%s: Bcfg2 repo %s is not inside Git repo %s" %
                     (progname, bcfg2root, gitroot))
        return 1

    # relative path to Bcfg2 root from VCS root
    if gitroot == bcfg2root:
        relpath = ''
    else:
        relpath = bcfg2root[len(gitroot) + 1:]

    new = 0
    changed = 0
    logger.debug("%s: Untracked files: %s" % (progname, repo.untracked_files))
    for path in repo.untracked_files:
        if add_to_commit(setup['commit'], path, repo, relpath):
            new += 1
        else:
            logger.debug("%s: Not adding %s to commit" % (progname, path))
    logger.debug("%s: Untracked files after building commit: %s" %
                 (progname, repo.untracked_files))

    changes = list_changed_files(repo)
    logger.info("%s: Changed files: %s" % (progname,
                                           [d.a_blob.path for d in changes]))
    for diff in changes:
        if add_to_commit(setup['commit'], diff.a_blob.path, repo, relpath):
            changed += 1
        else:
            logger.debug("%s: Not adding %s to commit" % (progname,
                                                          diff.a_blob.path))
    logger.info("%s: Changed files after building commit: %s" %
                (progname, [d.a_blob.path for d in list_changed_files(repo)]))

    if new + changed > 0:
        logger.debug("%s: Committing %s new files and %s changed files" %
                     (progname, new, changed))
        if setup['dryrun']:
            logger.warning("%s: In dry-run mode, skipping commit and push" %
                           progname)
        else:
            output = repo.index.commit("Auto-commit with %s from %s run" %
                                       (progname, hostname))
            if output:
                logger.debug("%s: %s" % (progname, output))
            remote = repo.remote()
            logger.debug("%s: Pushing to remote %s at %s" % (progname, remote,
                                                             remote.url))
            output = remote.push()
            if output:
                logger.debug("%s: %s" % (progname, output))
    else:
        logger.info("%s: No changes to commit" % progname)

if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = hostbase
#!/usr/bin/python
import os
from getopt import getopt, GetoptError
from re import split
import sys

os.environ['DJANGO_SETTINGS_MODULE'] = 'Hostbase.settings'
from Hostbase.hostbase.models import Host

attribs = ['administrator',
           'comments',
           'csi',
           'dhcp',
           'expiration_date',
           'hostname',
           'last',
           'location',
           'netgroup',
           'outbound_smtp',
           'primary_user',
           'printq',
           'security_class',
           'support',
           'status',
           'whatami']

already_exists = None
#here's my attempt at making the command line idiot proof
#you must supply and arugument and hostname for hostbase.py to run
try:
    (opts, args) = getopt(sys.argv[1:], 'l:c:')
    sys.argv[1]
    if len(split("\.", opts[0][1])) == 1:
        hosttouse = opts[0][1] + ".mcs.anl.gov"
    else:
        hosttouse = opts[0][1]
except (GetoptError, IndexError):
    print("\nUsage: hostbase.py -flag (hostname)\n")
    print("Flags:")
    print("\t-l   look (hostname)\n")
#    print("\t-c   copy (hostname)\n")
    sys.exit()

try:
    host = Host.objects.get(hostname=hosttouse)
except:
    print("Error: host %s not in hostbase" % hosttouse)
    sys.exit(1)
interfaces = []
for interface in host.interface_set.all():
    interfaces.append([interface, interface.ip_set.all()])
hostinfo = "\n"
for attrib in attribs:
    if not (opts[0][0] == '-c' and attrib in ['status', 'last']):
        if attrib == 'dhcp' or attrib == 'outbound_smtp':
            if host.__dict__[attrib]:
                hostinfo += "%-32s-> %s\n" % (attrib, 'y')
            else:
                hostinfo += "%-32s-> %s\n" % (attrib, 'n')
        else:
            hostinfo += "%-32s-> %s\n" % (attrib, host.__dict__[attrib])
for interface in interfaces:
    hostinfo += "\n%-32s-> %s\n" % ('mac_addr', interface[0].mac_addr)
    hostinfo += "%-32s-> %s\n" % ('hdwr_type', interface[0].hdwr_type)
    for ip in interface[1]:
        hostinfo += "%-32s-> %s\n" % ('ip_addr', ip.ip_addr)

if opts[0][0] == '-l':
    """Displays general host information"""
    print(hostinfo)

if opts[0][0] == '-c':
    """Provides pre-filled template to copy a host record"""
    fd = open('/tmp/hostbase.%s.tmp' % host.id, 'w')
    fd.write(hostinfo)
    fd.close()
    os.system('vi + /tmp/hostbase.%s.tmp' % host.id)
    os.system('batchadd.py /tmp/hostbase.%s.tmp' % host.id)
    os.system('rm /tmp/hostbase.%s.tmp' % host.id)

########NEW FILE########
__FILENAME__ = hostbasepush
#!/usr/bin/python

import os
import Bcfg2.Client.Proxy

if not os.getuid() == 0:
    print("this command must be run as root")
    raise SystemExit

proxy = Bcfg2.Client.Proxy.bcfg2()
print("building files...")
proxy.run_method('Hostbase.rebuildState', ())
print("running bcfg...")
os.system('bcfg2 -q -d -v')

########NEW FILE########
__FILENAME__ = hostinfo
#!/usr/bin/python
"""Hostinfo queries the hostbase database according to user-defined data"""

from os import system, environ
environ['DJANGO_SETTINGS_MODULE'] = 'Hostbase.settings'
from getopt import gnu_getopt, GetoptError
from django.db import connection
import sys

logic_ops = ["and", "or"]
host_attribs = ["hostname", "whatami", "netgroup", "security_class",
                "support", "csi", "memory", "printq", "dhcp", "outbound_smtp",
                "primary_user", "administrator", "location",
                "comments", "last", "expiration_date"]
dispatch = {'mac_addr': ' i.',
            'hdwr_type': ' i.',
            'ip_addr': ' p.',
            'name': ' n.',
            'dns_view': ' n.',
            'cname': ' c.',
            'mx': ' m.',
            'priority': ' m.'}


def pinger(hosts):
    """Function that uses fping to ping multiple hosts in parallel"""
    hostnames = ""
    for each in hosts:
        hostnames += each[0] + " "
    system("fping -r 1" + hostnames)
    sys.exit()


def get_query(arguments):
    """Parses the command line options and returns the necessary
    data for an SQL query"""
    logic = None
    resultset = []
    querystring = ''
    while 1:
        notflag = False
        if arguments[0] == 'not':
            notflag = True
            querypos = 1
        elif arguments[0] in logic_ops:
            logic = arguments[0]
            if arguments[1] == 'not':
                notflag = True
                querypos = 2
            else:
                querypos = 1
        else:
            querypos = 0
        if len(arguments[querypos].split("==")) > 1:
            operator = "="
            if notflag:
                operator = "<>"
            querysplit = arguments[querypos].split("==")
            if querysplit[0] in host_attribs:
                querystring = " h.%s%s\'%s\'" % (querysplit[0],
                                                 operator,
                                                 querysplit[1])
            elif querysplit[0] in dispatch:
                querystring = dispatch[querysplit[0]]
                querystring += "%s%s\'%s\'" % (querysplit[0],
                                               operator,
                                               querysplit[1])
        elif len(arguments[querypos].split("=")) > 1:
            notstring = ''
            if notflag:
                notstring = 'NOT '
            querysplit = arguments[querypos].split("=")
            if querysplit[0] in host_attribs:
                querystring = " h.%s %sLIKE \'%%%%%s%%%%\'" % (querysplit[0],
                                                               notstring,
                                                               querysplit[1])
            elif querysplit[0] in dispatch:
                querystring = dispatch[querysplit[0]]
                querystring += "%s %sLIKE \'%%%%%s%%%%\'" % (querysplit[0],
                                                             notstring,
                                                             querysplit[1])
        else:
            print("ERROR: bad query format")
            sys.exit()
        if not querystring:
            print("ERROR: bad query format")
            sys.exit()
        resultset.append((querystring, logic))
        arguments = arguments[querypos + 1:]
        if arguments == [] or arguments[0] not in logic_ops:
            break
    return resultset

try:
    (opts, args) = gnu_getopt(sys.argv[1:],
                             'q:', ['showfields', 'fields', 'ping', 'summary'])
    cursor = connection.cursor()
    if ('--showfields', '') in opts:
        print("\nhost fields:\n")
        for field in host_attribs:
            print(field)
        for field in dispatch:
            print(field)
        print("")
        sys.exit()
    if opts[0][0] == '-q':
        results = get_query(sys.argv[2:])
        queryoptions = ""
        for result in results:
            if result[1] == 'and':
                queryoptions += " AND " + result[0]
            elif result[1] == 'or':
                queryoptions += " OR " + result[0]
            else:
                queryoptions += result[0]
    if ('--summary', '') in opts:
        fields = "h.hostname, h.whatami, h.location, h.primary_user"
        query = """SELECT DISTINCT %s FROM (((((hostbase_host h
        INNER JOIN hostbase_interface i ON h.id = i.host_id)
        INNER JOIN hostbase_ip p ON i.id = p.interface_id)
        INNER JOIN hostbase_name n ON p.id = n.ip_id)
        INNER JOIN hostbase_name_mxs x ON x.name_id = n.id)
        INNER JOIN hostbase_mx m ON m.id = x.mx_id)
        LEFT JOIN hostbase_cname c ON n.id = c.name_id
        WHERE %s ORDER BY h.hostname
        """ % (fields, queryoptions)
        cursor.execute(query)
        results = cursor.fetchall()
        if not results:
            print("No matches were found for your query")
            sys.exit()
        print("\n%-32s %-10s %-10s %-10s" % ('Hostname', 'Type', 'Location', 'User'))
        print("================================ ========== ========== ==========")
        for host in results:
            print("%-32s %-10s %-10s %-10s" % (host))
        print("")
    elif ('--fields', '') in opts:
        tolook = [arg for arg in args if arg in host_attribs or arg in dispatch]
        fields = ""
        fields = ", ".join(tolook)
        if not fields:
            print("No valid fields were entered.  exiting...")
            sys.exit()
        query = """SELECT DISTINCT %s FROM (((((hostbase_host h
        INNER JOIN hostbase_interface i ON h.id = i.host_id)
        INNER JOIN hostbase_ip p ON i.id = p.interface_id)
        INNER JOIN hostbase_name n ON p.id = n.ip_id)
        INNER JOIN hostbase_name_mxs x ON x.name_id = n.id)
        INNER JOIN hostbase_mx m ON m.id = x.mx_id)
        LEFT JOIN hostbase_cname c ON n.id = c.name_id
        WHERE %s ORDER BY h.hostname
        """ % (fields, queryoptions)

        cursor.execute(query)
        results = cursor.fetchall()

        last = results[0]
        for field in results[0]:
            print(repr(field) + "\t")
        for host in results:
            if not host == last:
                for field in host:
                    print(repr(field) + "\t")
            last = host
            print("")
    else:
        basequery = """SELECT DISTINCT h.hostname FROM (((((hostbase_host h
        INNER JOIN hostbase_interface i ON h.id = i.host_id)
        INNER JOIN hostbase_ip p ON i.id = p.interface_id)
        INNER JOIN hostbase_name n ON p.id = n.ip_id)
        INNER JOIN hostbase_name_mxs x ON x.name_id = n.id)
        INNER JOIN hostbase_mx m ON m.id = x.mx_id)
        LEFT JOIN hostbase_cname c ON n.id = c.name_id
        WHERE
        """
        cursor.execute(basequery + queryoptions + " ORDER BY h.hostname")
        results = cursor.fetchall()

        if not results:
            print("No matches were found for your query")
            sys.exit()

        if ("--ping", '') in opts:
            pinger(results)

        for host in results:
            print(host[0])


except (GetoptError, IndexError):
    print("\nUsage: hostinfo.py -q <field>=[=]<value> [and/or <field>=<value> [--long option]]")
    print("       hostinfo.py --showfields\tshows all data fields")
    print("\n    long options:")
    print("\t --fields f1 f2 ...\tspecifies the fields displayed from the queried hosts")
    print("\t --summary\t\tprints out a predetermined set of fields")
    print("\t --ping\t\t\tuses fping to ping all queried hosts\n")
    sys.exit()

########NEW FILE########
__FILENAME__ = pkgmgr_gen
#!/usr/bin/python
"""Program to generate a bcfg2 Pkgmgr configuration file from a list
   of directories that contain RPMS.

   All versions or only the latest may be included in the output.
   rpm.labelCompare is used to compare the package versions, so that
   a proper rpm version comparison is done (epoch:version-release).

   The output file may be formated for use with the RPM or Yum
   bcfg2 client drivers.  The output can also contain the PackageList
   and nested group headers.
"""
import collections
import datetime
import glob
import gzip
import optparse
import os
import rpm
import sys
from lxml.etree import parse
import xml.sax
from xml.sax.handler import ContentHandler

# Compatibility imports
from Bcfg2.Compat import urljoin


def info(object, spacing=10, collapse=1):
    """Print methods and doc strings.
       Takes module, class, list, dictionary, or string.
    """
    methodList = [method for method in dir(object)
                  if isinstance(getattr(object, method),
                                collections.Callable)]
    processFunc = collapse and (lambda s: " ".join(s.split())) or (lambda s: s)
    print("\n".join(["%s %s" %
                      (method.ljust(spacing),
                       processFunc(str(getattr(object, method).__doc__)))
                     for method in methodList]))


def readRpmHeader(ts, filename):
    """
        Read an rpm header from an RPM file.
    """
    try:
        fd = os.open(filename, os.O_RDONLY)
    except:
        print("Failed to open RPM file %s" % filename)

    h = ts.hdrFromFdno(fd)
    os.close(fd)
    return h


def sortedDictValues(adict):
    """
        Sort a dictionary by its keys and return the items in sorted key order.
    """
    keys = list(adict.keys())
    keys.sort()
    return list(map(adict.get, keys))


def cmpRpmHeader(a, b):
    """
        cmp() implemetation suitable for use with sort.

        a and b are dictionaries as created by loadRpms().  Comparison is made
        by package name and then by the full rpm version (epoch, version, release).
        rpm.labelCompare is used for the version part of the comparison.
    """
    n1 = str(a['name'])
    e1 = str(a['epoch'])
    v1 = str(a['version'])
    r1 = str(a['release'])
    n2 = str(b['name'])
    e2 = str(b['epoch'])
    v2 = str(b['version'])
    r2 = str(b['release'])

    ret = cmp(n1, n2)
    if ret == 0:
        ret = rpm.labelCompare((e1, v1, r1), (e2, v2, r2))
    return ret


def loadRpms(dirs):
    """
       dirs is a list of directories to search for rpms.

       Builds a dictionary keyed by the package name.  Dictionary item is a list,
       one entry per package instance found.

       The list entries are dictionaries.  Keys are 'filename', 'mtime' 'name',
       'arch', 'epoch', 'version' and 'release'.

       e.g.

       packages = {
       'bcfg2' : [
           {'filename':'bcfg2-0.9.2-0.0rc1.noarch.rpm', 'mtime':'' 'name':"bcfg2',
            ''arch':'noarch', 'epoch':None, 'version':'0.9.2', 'release':'0.0rc1'}
           {'filename':'bcfg2-0.9.2-0.0rc5.noarch.rpm', 'mtime':'' 'name':"bcfg2',
            ''arch':'noarch', 'epoch':None, 'version':'0.9.2', 'release':'0.0rc5'}],
       'bcfg2-server' : [
           {'filename':'bcfg2-server-0.9.2-0.0rc1.noarch.rpm', 'mtime':'' 'name':"bcfg2-server',
            ''arch':'noarch', 'epoch':None, 'version':'0.9.2', 'release':'0.0rc1'}
           {'filename':'bcfg2-server-0.9.2-0.0rc5.noarch.rpm', 'mtime':'' 'name':"bcfg2-server',
            ''arch':'noarch', 'epoch':None, 'version':'0.9.2', 'release':'0.0rc5'}],
       }

    """
    packages = {}
    ts = rpm.TransactionSet()
    vsflags = 0
    vsflags |= rpm._RPMVSF_NODIGESTS
    vsflags |= rpm._RPMVSF_NOSIGNATURES
    ovsflags = ts.setVSFlags(vsflags)
    for dir in dirs:

        if options.verbose:
            print("Scanning directory: %s" % dir)

        for file in [files for files in os.listdir(dir)
                           if files.endswith('.rpm')]:

            filename = os.path.join(dir, file)

            # Get the mtime of the RPM file.
            file_mtime = datetime.date.fromtimestamp(os.stat(filename).st_mtime)

            # Get the RPM header
            header = readRpmHeader(ts, filename)

            # Get what we are interesting in out of the header.
            name = header[rpm.RPMTAG_NAME]
            epoch = header[rpm.RPMTAG_EPOCH]
            version = header[rpm.RPMTAG_VERSION]
            release = header[rpm.RPMTAG_RELEASE]
            subarch = header[rpm.RPMTAG_ARCH]

            # Only load RPMs with subarchitectures as calculated from the --archs option.
            if subarch in subarchs or 'all' in subarchs:

                # Store what we want in our structure.
                packages.setdefault(name, []).append({'filename': file,
                                                      'mtime': file_mtime,
                                                      'name': name,
                                                      'arch': subarch,
                                                      'epoch': epoch,
                                                      'version': version,
                                                      'release': release})

            # Print '.' for each package. stdio is line buffered, so have to flush it.
            if options.verbose:
                sys.stdout.write('.')
                sys.stdout.flush()
        if options.verbose:
            sys.stdout.write('\n')

    return packages


class pkgmgr_URLopener(urllib.FancyURLopener):
    """
        Override default error handling so that we can see what the errors are.
    """
    def http_error_default(self, url, fp, errcode, errmsg, headers):
        """
            Override default error handling so that we can see what the errors are.
        """
        print("ERROR %s: Unable to retrieve %s" % (errcode, url))


class PrimaryParser(ContentHandler):
    def __init__(self, packages):
        self.inPackage = 0
        self.inName = 0
        self.inArch = 0
        self.packages = packages

    def startElement(self, name, attrs):
        if name == "package":
            self.package = {'file': None, 'name': '', 'subarch': '',
                            'epoch': None, 'version': None, 'release': None}
            self.inPackage = 1
        elif self.inPackage:
            if name == "name":
                self.inName = 1
            elif name == "arch":
                self.inArch = 1
            elif name == "version":
                self.package['epoch'] = attrs.getValue('epoch')
                self.package['version'] = attrs.getValue('ver')
                self.package['release'] = attrs.getValue('rel')
            elif name == "location":
                self.package['file'] = attrs.getValue('href')

    def endElement(self, name):
        if name == "package":
            self.inPackage = 0
            # Only load RPMs with subarchitectures as calculated from the --archs option.
            if self.package['subarch'] in subarchs or 'all' in subarchs:
                self.packages.setdefault(self.package['name'], []).append(
                    {'filename': self.package['file'],
                     'name': self.package['name'],
                     'arch': self.package['subarch'],
                     'epoch': self.package['epoch'],
                     'version': self.package['version'],
                     'release': self.package['release']})
            # Print '.' for each package. stdio is line buffered, so have to flush it.
            if options.verbose:
                sys.stdout.write('.')
                sys.stdout.flush()
        elif self.inPackage:
            if name == "name":
                self.inName = 0
            elif name == "arch":
                self.inArch = 0

    def characters(self, content):
        if self.inPackage:
            if self.inName:
                self.package['name'] += content
            if self.inArch:
                self.package['subarch'] += content


def loadRepos(repolist):
    '''
       repolist is a list of urls to yum repositories.

       Builds a dictionary keyed by the package name.  Dictionary item is a list,
       one entry per package instance found.

       The list entries are dictionaries.  Keys are 'filename', 'mtime' 'name',
       'arch', 'epoch', 'version' and 'release'.

       e.g.

       packages = {
       'bcfg2' : [
           {'filename':'bcfg2-0.9.2-0.0rc1.noarch.rpm', 'mtime':'' 'name':"bcfg2',
            ''arch':'noarch', 'epoch':None, 'version':'0.9.2', 'release':'0.0rc1'}
           {'filename':'bcfg2-0.9.2-0.0rc5.noarch.rpm', 'mtime':'' 'name':"bcfg2',
            ''arch':'noarch', 'epoch':None, 'version':'0.9.2', 'release':'0.0rc5'}],
       'bcfg2-server' : [
           {'filename':'bcfg2-server-0.9.2-0.0rc1.noarch.rpm', 'mtime':'' 'name':"bcfg2-server',
            ''arch':'noarch', 'epoch':None, 'version':'0.9.2', 'release':'0.0rc1'}
           {'filename':'bcfg2-server-0.9.2-0.0rc5.noarch.rpm', 'mtime':'' 'name':"bcfg2-server',
            ''arch':'noarch', 'epoch':None, 'version':'0.9.2', 'release':'0.0rc5'}],
       }

    '''
    packages = {}
    for repo in repolist:
        url = urljoin(repo, './repodata/repomd.xml')

        if options.verbose:
            print("Loading repo metadata : %s" % url)

        try:
            opener = pkgmgr_URLopener()
            file, message = opener.retrieve(url)
        except:
            sys.exit()

        try:
            tree = parse(file)
        except IOError:
            print("ERROR: Unable to parse retrieved repomd.xml.")
            sys.exit()

        repomd = tree.getroot()
        for element in repomd:
            if element.tag.endswith('data') and element.get('type') == 'primary':
                for property in element:
                    if property.tag.endswith('location'):
                        primaryhref = property.get('href')

        url = urljoin(repo, './' + primaryhref)

        if options.verbose:
            print("Loading : %s" % url)

        try:
            opener = pkgmgr_URLopener()
            file, message = opener.retrieve(url)
        except:
            sys.exit()

        try:
            repo_file = gzip.open(file)
        except IOError:
            print("ERROR: Unable to parse retrieved file.")
            sys.exit()

        parser = xml.sax.make_parser()
        parser.setContentHandler(PrimaryParser(packages))
        parser.parse(repo_file)

        if options.verbose:
            sys.stdout.write('\n')
        repo_file.close()
    return packages


def printInstance(instance, group_count):
    """
        Print the details for a package instance with the appropriate indentation and
        in the specified format (rpm or yum).
    """
    group_count = group_count + 1
    name = instance['name']
    epoch = instance['epoch']
    version = instance['version']
    release = instance['release']
    arch = instance['arch']

    output_line = ''
    if options.format == 'rpm':
        output_line = '%s<Instance simplefile=\'%s\' ' % (indent * group_count, instance['filename'])
    else:
        output_line = '%s<Instance ' % (indent * group_count)

    if epoch:
        output_line += 'epoch=\'%s\' ' % (epoch)

    output_line += 'version=\'%s\' release=\'%s\' arch=\'%s\'/>\n' % (version, release, arch)
    output.write(output_line)


def printPackage(entry, group_count):
    """
       Print the details of a package with the appropriate indentation.
       Only the specified (all or latest) release(s) is printed.

       entry is a single package entry as created in loadRpms().
    """
    output.write('%s<Package name=\'%s\' type=\'%s\'>\n' \
                  % (group_count * indent, entry[0]['name'], options.format))

    subarch_dict = {}
    arch_dict = {}
    # Split instances of this package into subarchitectures.
    for instance in entry:
        if instance['arch'] == 'src':
            continue

        if instance['arch'] in subarch_dict:
            subarch_dict[instance['arch']].append(instance)
        else:
            subarch_dict[instance['arch']] = [instance]

        # Keep track of the subarchitectures we have found in each architecture.
        if subarch_mapping[instance['arch']] in arch_dict:
            if instance['arch'] not in arch_dict[subarch_mapping[instance['arch']]]:
                arch_dict[subarch_mapping[instance['arch']]].append(instance['arch'])
        else:
            arch_dict[subarch_mapping[instance['arch']]] = [instance['arch']]

    # Only keep the 'highest' subarchitecture in each architecture.
    for arch in list(arch_dict.keys()):
        if len(arch_dict[arch]) > 1:
            arch_dict[arch].sort()
            for s in arch_dict[arch][:-1]:
                del subarch_dict[s]

    # Sort packages within each architecture into version order
    for arch in subarch_dict:
        subarch_dict[arch].sort(cmpRpmHeader)

        if options.release == 'all':
            # Output all instances
            for header in subarch_dict[arch]:
                printInstance(header, group_count)
        else:
            # Output the latest
            printInstance(subarch_dict[arch][-1], group_count)

    output.write('%s</Package>\n' % (group_count * indent))


def main():

    if options.verbose:
        print("Loading package headers")

    if options.rpmdirs:
        package_dict = loadRpms(search_dirs)
    elif options.yumrepos:
        package_dict = loadRepos(repos)

    if options.verbose:
        print("Processing package headers")

    if options.pkgmgrhdr:
        if options.format == "rpm":
            output.write("<PackageList uri='%s' priority='%s' type='rpm'>\n" % (options.uri, options.priority))
        else:
            output.write("<PackageList priority='%s' type='yum'>\n" % (options.priority))

    group_count = 1
    if groups_list:
        for group in groups_list:
            output.write("%s<Group name='%s'>\n" % (indent * group_count, group))
            group_count = group_count + 1

    # Process packages in name order
    for package_entry in sortedDictValues(package_dict):
        printPackage(package_entry, group_count)

    if groups_list:
        group_count = group_count - 1
        while group_count:
            output.write('%s</Group>\n' % (indent * group_count))
            group_count = group_count - 1

    if options.pkgmgrhdr:
        output.write('</PackageList>\n')

    if options.verbose:
        print("%i package instances were processed" % len(package_dict))


if __name__ == "__main__":

    p = optparse.OptionParser()

    p.add_option('--archs', '-a',  action='store', \
                                   default='all', \
                                   type='string', \
                                   help='''Comma separated list of subarchitectures to include.
                                           The highest subarichitecture required in an
                                           architecture group should specified.   Lower
                                           subarchitecture packages will be loaded if that
                                           is all that is available. e.g. The higher of i386,
                                           i486 and i586 packages will be loaded if -a i586
                                           is specified. (Default: all).
                                        ''')

    p.add_option('--rpmdirs', '-d', action='store',
                                   type='string', \
                                   help='''Comma separated list of directories to scan for RPMS.
                                           Wilcards are permitted.
                                        ''')

    p.add_option('--enddate', '-e', action='store', \
                                   type='string', \
                                   help='End date for RPM file selection.')

    p.add_option('--format', '-f', action='store', \
                                   default='yum', \
                                   type='choice', \
                                   choices=('yum', 'rpm'), \
                                   help='''Format of the Output. Choices are yum or rpm.
                                           (Default: yum)
                                        ''')

    p.add_option('--groups', '-g', action='store', \
                                   type='string', \
                                   help='''List of comma separated groups to nest Package
                                           entities in.
                                        ''')

    p.add_option('--indent', '-i', action='store', \
                                   default=4, \
                                   type='int', \
                                   help='''Number of leading spaces to indent nested entries in the
                                           output. (Default:4)
                                        ''')

    p.add_option('--outfile', '-o', action='store', \
                                   type='string', \
                                   help='Output file name.')

    p.add_option('--pkgmgrhdr', '-P', action='store_true', \
                                   help='Include PackageList header in output.')

    p.add_option('--priority', '-p', action='store', \
                                   default=0, \
                                   type='int', \
                                   help='''Value to set priority attribute in the PackageList Tag.
                                           (Default: 0)
                                        ''')

    p.add_option('--release', '-r', action='store', \
                                   default='latest', \
                                   type='choice', \
                                   choices=('all', 'latest'), \
                                   help='''Which releases to include in the output. Choices are
                                           all or latest.  (Default: latest).''')

    p.add_option('--startdate', '-s', action='store', \
                                   type='string', \
                                   help='Start date for RPM file selection.')

    p.add_option('--uri', '-u',    action='store', \
                                   type='string', \
                                   help='URI for PackageList header required for RPM format ouput.')

    p.add_option('--verbose', '-v', action='store_true', \
                                    help='Enable verbose output.')

    p.add_option('--yumrepos', '-y', action='store',
                                   type='string', \
                                   help='''Comma separated list of YUM repository URLs to load.
                                           NOTE: Each URL must end in a '/' character.''')

    options, arguments = p.parse_args()

    if options.pkgmgrhdr and options.format == 'rpm' and not options.uri:
        print("Option --uri must be specified to produce a PackageList Tag "
              "for rpm formatted files.")
        sys.exit(1)

    if not options.rpmdirs and not options.yumrepos:
        print("One of --rpmdirs and --yumrepos must be specified")
        sys.exit(1)

    # Set up list of directories to search
    if options.rpmdirs:
        search_dirs = []
        for d in options.rpmdirs.split(','):
            search_dirs += glob.glob(d)
        if options.verbose:
            print("The following directories will be scanned:")
            for d in search_dirs:
                print("    %s" % d)

    # Setup list of repos
    if options.yumrepos:
        repos = []
        for r in options.yumrepos.split(','):
            repos.append(r)
        if options.verbose:
            print("The following repositories will be scanned:")
            for d in repos:
                print("    %s" % d)

    # Set up list of architectures to include and some mappings
    # to use later.
    arch_mapping = {'x86': ['i686', 'i586', 'i486', 'i386', 'athlon'],
                    'x86_64': ['x86_64'],
                    'ia64': ['ia64'],
                    'ppc': ['ppc'],
                    'ppc64': ['ppc64'],
                    'sparc': ['sparc'],
                    'noarch': ['noarch']}
    subarch_mapping = {'i686': 'x86',
                       'i586': 'x86',
                       'i486': 'x86',
                       'i386': 'x86',
                       'athlon': 'x86',
                       'x86_64': 'x86_64',
                       'ia64': 'ia64',
                       'ppc': 'ppc',
                       'ppc64': 'ppc64',
                       'sparc': 'sparc',
                       'noarch': 'noarch'}
    commandline_subarchs = options.archs.split(',')
    arch_list = []
    subarchs = []
    if 'all' in commandline_subarchs:
        subarchs.append('all')
    else:
        for s in commandline_subarchs:
            if s not in subarch_mapping:
                print("Error: Invalid subarchitecture specified: ", s)
                sys.exit(1)
            # Only allow one subarchitecture per architecture to be specified.
            if s not in arch_list:
                arch_list.append(s)

                # Add subarchitectures lower than the one specified to the list.
                # e.g. If i486 is specified this will add i386 to the list of
                # subarchitectures to load.
                i = arch_mapping[subarch_mapping[s]].index(s)
                #if i != len(arch_mapping[subarch_mapping[s]]):
                subarchs += arch_mapping[subarch_mapping[s]][i:]
            else:
                print("Error: Multiple subarchitecutes of the same "
                      "architecture specified.")
                sys.exit(1)

    indent = ' ' * options.indent

    if options.groups:
        groups_list = options.groups.split(',')
    else:
        groups_list = None

    if options.outfile:
        output = file(options.outfile, "w")
    else:
        output = sys.stdout

    main()

########NEW FILE########
__FILENAME__ = pkgmgr_update
#!/usr/bin/python

"""
    Program to update an existing bcfg2 Pkgmgr configuration file from a list
    of directories that contain RPMS.

    Only the epoch, version, release and simplefiles attributes are updated
    in existing entries.  All other entries and attributes are preserved.

    This is a total hack until a proper more generalised system for managing
    Pkgmgr configuation files is developed.
"""

__version__ = '0.1'

import datetime
import glob
import gzip
import optparse
import os
import rpm
import sys

# Compatibility imports
from Bcfg2.Compat import urljoin

try:
    from lxml.etree import parse, tostring
except:
    from elementtree.ElementTree import parse, tostring

installOnlyPkgs = ['kernel',
                   'kernel-bigmem',
                   'kernel-enterprise',
                   'kernel-smp',
                   'kernel-modules',
                   'kernel-debug',
                   'kernel-unsupported',
                   'kernel-source',
                   'kernel-devel',
                   'kernel-default',
                   'kernel-largesmp-devel',
                   'kernel-largesmp',
                   'kernel-xen',
                   'gpg-pubkey']


def readRpmHeader(ts, filename):
    """
        Read an rpm header from an RPM file.
    """
    try:
        fd = os.open(filename, os.O_RDONLY)
    except:
        print("Failed to open RPM file %s" % filename)

    h = ts.hdrFromFdno(fd)
    os.close(fd)
    return h


def sortedDictValues(adict):
    """
        Sort a dictionary by its keys and return the items in sorted key order.
    """
    keys = list(adict.keys())
    keys.sort()
    return list(map(adict.get, keys))


def cmpRpmHeader(a, b):
    """
        cmp() implemetation suitable for use with sort.
    """
    n1 = str(a.get('name'))
    e1 = str(a.get('epoch'))
    v1 = str(a.get('version'))
    r1 = str(a.get('release'))
    n2 = str(b.get('name'))
    e2 = str(b.get('epoch'))
    v2 = str(b.get('version'))
    r2 = str(b.get('release'))

    return rpm.labelCompare((e1, v1, r1), (e2, v2, r2))


def loadRpms(dirs):
    """
       dirs is a list of directories to search for rpms.

       Builds a multilevel dictionary keyed by the package name and arch.
       Arch dictionary item is a list, one entry per package instance found.

       The list entries are dictionaries.  Keys are 'filename', 'mtime' 'name',
       'arch', 'epoch', 'version' and 'release'.

       e.g.

       packages = {
       'bcfg2' : { 'noarch' : [ {'filename':'bcfg2-0.9.2-0.0rc1.noarch.rpm', 'mtime':'',
                                 'name':'bcfg2', 'arch':'noarch', 'epoch':None, 'version':'0.9.2',
                                 'release':'0.0rc1'}
                                {'filename':'bcfg2-0.9.2-0.0rc5.noarch.rpm', 'mtime':'',
                                 'name':'bcfg2', 'arch':'noarch', 'epoch':None, 'version':'0.9.2',
                                 'release':'0.0rc5'}]},
       'bcfg2-server' { 'noarch' : [ {'filename':'bcfg2-server-0.9.2-0.0rc1.noarch.rpm', 'mtime':'',
                                      'name':'bcfg2-server', 'arch':'noarch', 'epoch':None,
                                      'version':'0.9.2', 'release':'0.0rc1'}
                                     {'filename':'bcfg2-server-0.9.2-0.0rc5.noarch.rpm', 'mtime':'',
                                      'name':"bcfg2-server', 'arch':'noarch', 'epoch':None,
                                      'version':'0.9.2', 'release':'0.0rc5'}]},
       }
    """
    packages = {}
    ts = rpm.TransactionSet()
    vsflags = 0
    vsflags |= rpm._RPMVSF_NODIGESTS
    vsflags |= rpm._RPMVSF_NOSIGNATURES
    ovsflags = ts.setVSFlags(vsflags)
    for dir in dirs:

        if options.verbose:
            print("Scanning directory: %s" % dir)

        for file in [files for files in os.listdir(dir)
                           if files.endswith('.rpm')]:

            filename = os.path.join(dir, file)

            # Get the mtime of the RPM file.
            file_mtime = datetime.date.fromtimestamp(os.stat(filename).st_mtime)

            # Get the RPM header
            header = readRpmHeader(ts, filename)

            # Get what we are interesting in out of the header.
            name = header[rpm.RPMTAG_NAME]
            epoch = header[rpm.RPMTAG_EPOCH]
            version = header[rpm.RPMTAG_VERSION]
            release = header[rpm.RPMTAG_RELEASE]
            subarch = header[rpm.RPMTAG_ARCH]

            if name not in installOnlyPkgs:
                packages.setdefault(name, {}).setdefault(subarch, []).append({'filename': file,
                                                                              'mtime': file_mtime,
                                                                              'name': name,
                                                                              'arch': subarch,
                                                                              'epoch': epoch,
                                                                              'version': version,
                                                                              'release': release})
            if options.verbose:
                sys.stdout.write('.')
                sys.stdout.flush()
        if options.verbose:
            sys.stdout.write('\n')

    return packages


class pkgmgr_URLopener(urllib.FancyURLopener):
    """
        Override default error handling so that we can see what the errors are.
    """
    def http_error_default(self, url, fp, errcode, errmsg, headers):
        """
            Override default error handling so that we can see what the errors are.
        """
        print("ERROR %s: Unable to retrieve %s" % (errcode, url))


def loadRepos(repolist):
    """
       repolist is a list of urls to yum repositories.

       Builds a multilevel dictionary keyed by the package name and arch.
       Arch dictionary item is a list, one entry per package instance found.

       The list entries are dictionaries.  Keys are 'filename', 'mtime' 'name',
       'arch', 'epoch', 'version' and 'release'.

       e.g.

       packages = {
       'bcfg2' : { 'noarch' : [ {'filename':'bcfg2-0.9.2-0.0rc1.noarch.rpm', 'mtime':'',
                                 'name':'bcfg2', 'arch':'noarch', 'epoch':None, 'version':'0.9.2',
                                 'release':'0.0rc1'}
                                {'filename':'bcfg2-0.9.2-0.0rc5.noarch.rpm', 'mtime':'',
                                 'name':'bcfg2', 'arch':'noarch', 'epoch':None, 'version':'0.9.2',
                                 'release':'0.0rc5'}]},
       'bcfg2-server' { 'noarch' : [ {'filename':'bcfg2-server-0.9.2-0.0rc1.noarch.rpm', 'mtime':'',
                                      'name':'bcfg2-server', 'arch':'noarch', 'epoch':None,
                                      'version':'0.9.2', 'release':'0.0rc1'}
                                     {'filename':'bcfg2-server-0.9.2-0.0rc5.noarch.rpm', 'mtime':'',
                                      'name':"bcfg2-server', 'arch':'noarch', 'epoch':None,
                                      'version':'0.9.2', 'release':'0.0rc5'}]},
       }

    """
    packages = {}
    for repo in repolist:
        url = urljoin(repo, './repodata/repomd.xml')

        try:
            opener = pkgmgr_URLopener()
            file, message = opener.retrieve(url)
        except:
            sys.exit()

        try:
            tree = parse(file)
        except IOError:
            print("ERROR: Unable to parse retrieved repomd.xml.")
            sys.exit()

        repomd = tree.getroot()
        for element in repomd:
            if element.tag.endswith('data') and element.attrib['type'] == 'primary':
                for property in element:
                    if property.tag.endswith('location'):
                        primaryhref = property.attrib['href']

        url = urljoin(repo, './' + primaryhref)

        if options.verbose:
            print("Loading : %s" % url)

        try:
            opener = pkgmgr_URLopener()
            file, message = opener.retrieve(url)
        except:
            sys.exit()

        try:
            repo_file = gzip.open(file)
            tree = parse(repo_file)
        except IOError:
            print("ERROR: Unable to parse retrieved file.")
            sys.exit()

        root = tree.getroot()
        for element in root:
            if element.tag.endswith('package'):
                for property in element:
                    if property.tag.endswith('name'):
                        name = property.text
                    elif property.tag.endswith('arch'):
                        subarch = property.text
                    elif property.tag.endswith('version'):
                        version = property.get('ver')
                        epoch = property.get('epoch')
                        release = property.get('rel')
                    elif property.tag.endswith('location'):
                        file = property.get('href')

                if name not in installOnlyPkgs:
                    packages.setdefault(name, {}).setdefault(subarch, []).append({'filename': file,
                                                                                  'name': name,
                                                                                  'arch': subarch,
                                                                                  'epoch': epoch,
                                                                                  'version': version,
                                                                                  'release': release})
                if options.verbose:
                    sys.stdout.write('.')
                    sys.stdout.flush()
        if options.verbose:
            sys.stdout.write('\n')

    return packages


def str_evra(instance):
    """
        Convert evra dict entries to a string.
    """
    if instance.get('epoch', '*') == '*' or instance.get('epoch', '*') == None:
        return '%s-%s.%s' % (instance.get('version', '*'), instance.get('release', '*'),
                             instance.get('arch', '*'))
    else:
        return '%s:%s-%s.%s' % (instance.get('epoch', '*'), instance.get('version', '*'),
                                instance.get('release', '*'), instance.get('arch', '*'))


def updatepkg(pkg):
    """
    """
    global package_dict
    name = pkg.get('name')
    if name not in installOnlyPkgs:
        for inst in [inst for inst in pkg if inst.tag == 'Instance']:
            arch = inst.get('arch')
            if name in package_dict:
                if arch in package_dict[name]:
                    package_dict[name][arch].sort(cmpRpmHeader)
                    latest = package_dict[name][arch][-1]
                    if cmpRpmHeader(inst, latest) == -1:
                        if options.verbose:
                            print("Found newer version of package %s" % name)
                            print("    Updating %s to %s" % (str_evra(inst),
                                                             str_evra(latest)))
                        if latest['epoch'] != None:
                            inst.attrib['epoch'] = str(latest['epoch'])
                        inst.attrib['version'] = latest['version']
                        inst.attrib['release'] = latest['release']
                        if inst.get('simplefile', False):
                            inst.attrib['simplefile'] = latest['filename']
                        if options.altconfigfile:
                            ignoretags = pkg.xpath(".//Ignore")
                            # if we find Ignore tags, then assume they're correct;
                            # otherwise, check the altconfigfile
                            if not ignoretags:
                                altpkgs = alttree.xpath(".//Package[@name='%s'][Ignore]" % name)
                                if (len(altpkgs) == 1):
                                    for ignoretag in altpkgs[0].xpath(".//Ignore"):
                                        if options.verbose:
                                            print("    Found Ignore tag in altconfigfile for package %s" % name)
                                        pkg.append(ignoretag)


def main():
    global package_dict
    global alttree
    if options.verbose:
        print("Loading Pkgmgr config file %s." % (options.configfile))

    tree = parse(options.configfile)
    config = tree.getroot()

    if options.altconfigfile:
        if options.verbose:
            print("Loading Pkgmgr alternate config file %s." %
                  (options.altconfigfile))

        alttree = parse(options.altconfigfile)

    if options.verbose:
        print("Loading package headers")

    if options.rpmdirs:
        package_dict = loadRpms(search_dirs)
    elif options.yumrepos:
        package_dict = loadRepos(repos)

    if options.verbose:
        print("Processing package headers")

    for pkg in config.getiterator('Package'):
        updatepkg(pkg)

    output.write(tostring(config))

if __name__ == "__main__":

    p = optparse.OptionParser()

    p.add_option('--configfile', '-c', action='store', \
                                   type='string', \
                                   help='Existing Pkgmgr configuration  file name.')
    p.add_option('--altconfigfile', '-a', action='store', \
                                   type='string', \
                                   help='''Alternate, existing Pkgmgr configuration file name to read
                                           Ignore tags from (used for upgrades).''')

    p.add_option('--rpmdirs', '-d', action='store',
                                   type='string', \
                                   help='''Comma separated list of directories to scan for RPMS.
                                           Wilcards are permitted.''')

    p.add_option('--outfile', '-o', action='store', \
                                   type='string', \
                                   help='Output file name or new Pkgrmgr file.')

    p.add_option('--verbose', '-v', action='store_true', \
                                    help='Enable verbose output.')

    p.add_option('--yumrepos', '-y', action='store',
                                   type='string', \
                                   help='''Comma separated list of YUM repository URLs to load.
                                           NOTE: Each URL must end in a '/' character.''')
    options, arguments = p.parse_args()

    if not options.configfile:
        print("An existing Pkgmgr configuration file must be specified with "
              "the -c option.")
        sys.exit()

    if not options.rpmdirs and not options.yumrepos:
        print("One of --rpmdirs and --yumrepos must be specified")
        sys.exit(1)

    # Set up list of directories to search
    if options.rpmdirs:
        search_dirs = []
        for d in options.rpmdirs.split(','):
            search_dirs += glob.glob(d)
        if options.verbose:
            print("The following directories will be scanned:")
            for d in search_dirs:
                print("    %s" % d)

    # Setup list of repos
    if options.yumrepos:
        repos = []
        for r in options.yumrepos.split(','):
            repos.append(r)
        if options.verbose:
            print("The following repositories will be scanned:")
            for d in repos:
                print("    %s" % d)

    if options.outfile:
        output = file(options.outfile, "w")
    else:
        output = sys.stdout

    package_dict = {}

    main()

########NEW FILE########
__FILENAME__ = posixusers_baseline
#!/usr/bin/env python

import grp
import sys
import logging
import lxml.etree
import Bcfg2.Logger
from Bcfg2.Client.Tools.POSIXUsers import POSIXUsers
from Bcfg2.Options import OptionParser, Option, get_bool, CLIENT_COMMON_OPTIONS


def get_setup():
    optinfo = CLIENT_COMMON_OPTIONS
    optinfo['nouids'] = Option("Do not include UID numbers for users",
                               default=False,
                               cmd='--no-uids',
                               long_arg=True,
                               cook=get_bool)
    optinfo['nogids'] = Option("Do not include GID numbers for groups",
                               default=False,
                               cmd='--no-gids',
                               long_arg=True,
                               cook=get_bool)
    setup = OptionParser(optinfo)
    setup.parse(sys.argv[1:])

    if setup['args']:
        print("posixuser_[baseline.py takes no arguments, only options")
        print(setup.buildHelpMessage())
        raise SystemExit(1)
    level = 30
    if setup['verbose']:
        level = 20
    if setup['debug']:
        level = 0
    Bcfg2.Logger.setup_logging('posixusers_baseline.py',
                               to_syslog=False,
                               level=level,
                               to_file=setup['logging'])
    return setup


def main():
    setup = get_setup()
    if setup['file']:
        config = lxml.etree.parse(setup['file']).getroot()
    else:
        config = lxml.etree.Element("Configuration")
    logger = logging.getLogger('posixusers_baseline.py')
    users = POSIXUsers(logger, setup, config)

    baseline = lxml.etree.Element("Bundle", name="posixusers_baseline")
    for entry in users.FindExtra():
        data = users.existing[entry.tag][entry.get("name")]
        for attr, idx in users.attr_mapping[entry.tag].items():
            if (entry.get(attr) or
                (attr == 'uid' and setup['nouids']) or
                (attr == 'gid' and setup['nogids'])):
                continue
            entry.set(attr, str(data[idx]))
        if entry.tag == 'POSIXUser':
            try:
                entry.set("group", grp.getgrgid(data[3])[0])
            except KeyError:
                logger.warning("User %s is a member of nonexistent group %s" %
                               (entry.get("name"), data[3]))
                entry.set("group", str(data[3]))
            for group in users.user_supplementary_groups(entry):
                memberof = lxml.etree.SubElement(entry, "MemberOf",
                                                 group=group[0])

        entry.tag = "Bound" + entry.tag
        baseline.append(entry)

    print(lxml.etree.tostring(baseline, pretty_print=True))

if __name__ == "__main__":
    sys.exit(main())

########NEW FILE########
__FILENAME__ = rpmlisting
#!/usr/bin/python -u

import os
import sys
import subprocess
import getopt
import re
import datetime
from socket import gethostname


def run_or_die(command):
    """run a command, returning output.  raise an exception if it fails."""
    (status, stdio) = subprocess.getstatusoutput(command)
    if status != 0:
        raise Exception("command '%s' failed with exit status %d and output '%s'" %
                        (command, status, stdio))
    return stdio


def rpmblob_cmp(a, b):
    """cmp() implementation for rpmblobs, suitable for use with sort()."""
    ret = cmp(a['name'], b['name'])
    if ret == 0:
        ret = verstr_cmp(a['version'], b['version'])
        if ret == 0:
            ret = verstr_cmp(a['release'], b['release'])
    return ret


def verstr_cmp(a, b):
    """cmp() implementation for version strings, suitable for use with sort()."""
    ret = 0
    index = 0
    a_parts = subdivide(a)
    b_parts = subdivide(b)
    prerelease_pattern = re.compile('rc|pre')
    while ret == 0 and index < min(len(a_parts), len(b_parts)):
        subindex = 0
        a_subparts = a_parts[index]
        b_subparts = b_parts[index]
        while ret == 0 and subindex < min(len(a_subparts), len(b_subparts)):
            ret = cmp(a_subparts[subindex], b_subparts[subindex])
            if ret != 0:
                return ret
            subindex = subindex + 1
        if len(a_subparts) != len(b_subparts):
            # handle prerelease special case at subpart level (ie, '4.0.2rc5').
            if len(a_subparts) > len(b_subparts) and prerelease_pattern.match(str(a_subparts[subindex])):
                return -1
            elif len(a_subparts) < len(b_subparts) and prerelease_pattern.match(str(b_subparts[subindex])):
                return 1
            else:
                return len(a_subparts) - len(b_subparts)
        index = index + 1
    if len(a_parts) != len(b_parts):
        # handle prerelease special case at part level (ie, '4.0.2.rc5).
        if len(a_parts) > len(b_parts) and prerelease_pattern.match(str(a_parts[index][0])):
            return -1
        elif len(a_parts) < len(b_parts) and prerelease_pattern.match(str(b_parts[index][0])):
            return 1
        else:
            return len(a_parts) - len(b_parts)
    return ret


def subdivide(verstr):
    """subdivide takes a version or release string and attempts to subdivide
    it into components to facilitate sorting.  The string is divided into a
    two level hierarchy of sub-parts.  The upper level is subdivided by
    periods, and the lower level is subdivided by boundaries between digit,
    alpha, and other character groupings.
    """
    parts = []
    # parts is a list of lists representing the subsections which make up a version string.
    # example:
    # 4.0.2b3 would be represented as [[4],[0],[2,'b',3]].
    major_parts = verstr.split('.')
    for major_part in major_parts:
        minor_parts = []
        index = 0
        while index < len(major_part):
            # handle digit subsection
            if major_part[index].isdigit():
                digit_str_part = ""
                while index < len(major_part) and major_part[index].isdigit():
                    digit_str_part = digit_str_part + major_part[index]
                    index = index + 1
                digit_part = int(digit_str_part)
                minor_parts.append(digit_part)
            # handle alpha subsection
            elif major_part[index].isalpha():
                alpha_part = ""
                while index < len(major_part) and major_part[index].isalpha():
                    alpha_part = alpha_part + major_part[index]
                    index = index + 1
                minor_parts.append(alpha_part)
            # handle other characters.  this should only be '_', but we will treat is as a subsection to keep it general.
            elif not major_part[index].isalnum():
                other_part = ""
                while index < len(major_part) and not major_part[index].isalnum():
                    other_part = other_part + major_part[index]
                    index = index + 1
                minor_parts.append(other_part)
            parts.append(minor_parts)
    return parts


subarch_mapping = {'athlon': 'x86',
                   'i686': 'x86',
                   'i586': 'x86',
                   'i486': 'x86',
                   'i386': 'x86',
                   'x86_64': 'x86_64',
                   'noarch': 'noarch'}
arch_mapping = {'x86': ['athlon',
                        'i686',
                        'i586',
                        'i486',
                        'i386'],
                'x86_64': ['x86_64'],
                'noarch': ['noarch']}


def parse_rpm(path, filename):
    """read the name, version, release, and subarch of an rpm.
    this version reads the rpm headers.
    """
    cmd = 'rpm --nosignature --queryformat \'%%{NAME} %%{VERSION} %%{RELEASE} %%{ARCH}\' -q -p %s/%s' % (path, filename)
    output = run_or_die(cmd)
    (name, version, release, subarch) = output.split()
    if subarch not in list(subarch_mapping.keys()):
        raise Exception("%s/%s has invalid subarch %s" % (path, filename, subarch))
    return (name, version, release, subarch)


def parse_rpm_filename(path, filename):
    """read the name, version, release, and subarch of an rpm.
    this version tries to parse the filename directly, and calls
    'parse_rpm' as a fallback.
    """
    name, version, release, subarch = None, None, None, None
    try:
        (major, minor) = sys.version_info[:2]
        if major >= 2 and minor >= 4:
            (blob, subarch, extension) = filename.rsplit('.', 2)
            (name, version, release) = blob.rsplit('-', 2)
        else:
            (rextension, rsubarch, rblob) = filename[::-1].split('.', 2)
            (blob, subarch, extension) = (rblob[::-1], rsubarch[::-1], rextension[::-1])
            (rrelease, rversion, rname) = blob[::-1].split('-', 2)
            (name, version, release) = (rname[::-1], rversion[::-1], rrelease[::-1])
        if subarch not in list(subarch_mapping.keys()):
            raise "%s/%s has invalid subarch %s." % (path, filename, subarch)
    except:
        # for incorrectly named rpms (ie, sun's java rpms) we fall back to reading the rpm headers.
        sys.stderr.write("Warning: could not parse filename %s/%s.  Attempting to parse rpm headers.\n" % (path, filename))
        (name, version, release, subarch) = parse_rpm(path, filename)
    return (name, version, release, subarch)


def get_pkgs(rpmdir):
    """scan a dir of rpms and generate a pkgs structure. first try parsing
    the filename. if that fails, try parsing the rpm headers.
    """
    pkgs = {}
    """
pkgs structure:
* pkgs is a dict of package name, rpmblob list pairs:
  pkgs = {name:[rpmblob,rpmblob...], name:[rpmblob,rpmblob...]}
* rpmblob is a dict describing an rpm file:
  rpmblob = {'file':'foo-0.1-5.i386.rpm', 'name':'foo', 'version':'0.1', 'release':'5', 'subarch':'i386'},

example:
pkgs = {
'foo' : [
  {'file':'foo-0.1-5.i386.rpm', 'name':'foo', 'version':'0.1', 'release':'5', 'subarch':'i386'},
  {'file':'foo-0.2-3.i386.rpm', 'name':'foo', 'version':'0.2', 'release':'3', 'subarch':'i386'}],
'bar' : [
  {'file':'bar-3.2a-12.mips.rpm', 'name':'bar', 'version':'3.2a', 'release':'12', 'subarch':'mips'},
  {'file':'bar-3.7j-4.mips.rpm', 'name':'bar', 'version':'3.7j', 'release':'4', 'subarch':'mips'}]
}
"""
    rpms = [item for item in os.listdir(rpmdir) if item.endswith('.rpm')]
    for filename in rpms:
        (name, version, release, subarch) = parse_rpm_filename(rpmdir, filename)
        rpmblob = {'file': filename,
                   'name': name,
                   'version': version,
                   'release': release,
                   'subarch': subarch}
        if name in pkgs:
            pkgs[name].append(rpmblob)
        else:
            pkgs[name] = [rpmblob]
    return pkgs


def prune_pkgs_latest(pkgs):
    """prune a pkgs structure to contain only the latest version
    of each package (includes multiarch results).
    """
    latest_pkgs = {}
    for rpmblobs in list(pkgs.values()):
        (major, minor) = sys.version_info[:2]
        if major >= 2 and minor >= 4:
            rpmblobs.sort(rpmblob_cmp, reverse=True)
        else:
            rpmblobs.sort(rpmblob_cmp)
            rpmblobs.reverse()
        pkg_name = rpmblobs[0]['name']
        all_archs = [blob for blob in rpmblobs if blob['version'] == rpmblobs[0]['version'] and
                                                  blob['release'] == rpmblobs[0]['release']]
        latest_pkgs[pkg_name] = all_archs
    return latest_pkgs


def prune_pkgs_archs(pkgs):
    """prune a pkgs structure to contain no more than one subarch
    per architecture for each set of packages.
    """
    pruned_pkgs = {}
    for rpmblobs in list(pkgs.values()):
        pkg_name = rpmblobs[0]['name']
        arch_sifter = {}
        for challenger in rpmblobs:
            arch = subarch_mapping[challenger['subarch']]
            incumbent = arch_sifter.get(arch)
            if incumbent == None:
                arch_sifter[arch] = challenger
            else:
                subarchs = arch_mapping[arch]
                challenger_index = subarchs.index(challenger['subarch'])
                incumbent_index = subarchs.index(incumbent['subarch'])
                if challenger_index < incumbent_index:
                    arch_sifter[arch] = challenger
        pruned_pkgs[pkg_name] = list(arch_sifter.values())
    return pruned_pkgs


def get_date_from_desc(date_desc):
    """calls the unix 'date' command to turn a date
    description into a python date object.

    example: get_date_from_desc("last sunday 1 week ago")
    """
    stdio = run_or_die('date -d "' + date_desc + '" "+%Y %m %d"')
    (year_str, month_str, day_str) = stdio.split()
    year = int(year_str)
    month = int(month_str)
    day = int(day_str)
    date_obj = datetime.date(year, month, day)
    return date_obj


def get_mtime_date(path):
    """return a naive date object based on the file's mtime."""
    return datetime.date.fromtimestamp(os.stat(path).st_mtime)


def prune_pkgs_timely(pkgs, start_date_desc=None, end_date_desc=None, rpmdir='.'):
    """prune a pkgs structure to contain only rpms with
    an mtime within a certain temporal window.
    """
    start_date = None
    if start_date_desc != None:
        start_date = get_date_from_desc(start_date_desc)
    end_date = None
    if end_date_desc != None:
        end_date = get_date_from_desc(end_date_desc)
    if start_date == None and end_date == None:
        return pkgs
    if start_date != None:
        for rpmblobs in list(pkgs.values()):
            pkg_name = rpmblobs[0]['name']
            timely_blobs = [blob for blob in rpmblobs if start_date < get_mtime_date(rpmdir + '/' + blob['file'])]
            if len(timely_blobs) == 0:
                del pkgs[pkg_name]
            else:
                pkgs[pkg_name] = timely_blobs
    if end_date != None:
        for rpmblobs in list(pkgs.values()):
            pkg_name = rpmblobs[0]['name']
            timely_blobs = [blob for blob in rpmblobs if get_mtime_date(rpmdir + '/' + blob['file']) <= end_date]
            if len(timely_blobs) == 0:
                del pkgs[pkg_name]
            else:
                pkgs[pkg_name] = timely_blobs
    return pkgs


# from http://aspn.activestate.com/ASPN/Python/Cookbook/Recipe/52306
def sorted_values(adict):
    """return a list of values from a dict, sorted by key."""
    items = list(adict.items())
    items.sort()
    return [value for key, value in items]


def scan_rpm_dir(rpmdir, uri, group, priority=0, output=sys.stdout, start_date_desc=None, end_date_desc=None):
    """the meat of this library."""
    output.write('<PackageList uri="%s" type="yum" priority="%s">\n' % (uri, priority))
    output.write(' <Group name="%s">\n' % group)
    pkgs = prune_pkgs_archs(prune_pkgs_latest(prune_pkgs_timely(get_pkgs(rpmdir), start_date_desc, end_date_desc, rpmdir)))
    for rpmblobs in sorted_values(pkgs):
        if len(rpmblobs) == 1:
            # regular pkgmgr entry
            rpmblob = rpmblobs[0]
            output.write('  <Package name="%s" simplefile="%s" version="%s-%s"/>\n' %
                         (rpmblob['name'], rpmblob['file'], rpmblob['version'], rpmblob['release']))
        else:
            # multiarch pkgmgr entry
            rpmblob = rpmblobs[0]
            subarchs = [blob['subarch'] for blob in rpmblobs]
            subarchs.sort()
            multiarch_string = ' '.join(subarchs)
            pattern_string = '\.(%s)\.rpm$' % '|'.join(subarchs)  # e.g., '\.(i386|x86_64)\.rpm$'
            pattern = re.compile(pattern_string)
            multiarch_file = pattern.sub('.%(arch)s.rpm', rpmblob['file'])  # e.g., 'foo-1.0-1.%(arch)s.rpm'
            output.write('  <Package name="%s" file="%s" version="%s-%s" multiarch="%s"/>\n' %
                         (rpmblob['name'], multiarch_file, rpmblob['version'], rpmblob['release'], multiarch_string))
    output.write(' </Group>\n')
    output.write('</PackageList>\n')


def usage(output=sys.stdout):
    output.write("Usage: %s [-g <groupname>] [-u <uri>] [-d <dir>] [-p <priority>] [-o <output>]\n" % sys.argv[0])


if __name__ == "__main__":
    try:
        opts, args = getopt.getopt(sys.argv[1:], "g:u:d:p:o:",
                                   ["group=", "uir=", "dir=", "priority=", "output="])
    except getopt.GetoptError:
        usage(sys.stderr)
        sys.exit(1)

    group = "base"
    uri = "http://" + gethostname() + "/rpms"
    rpmdir = "."
    priority = "0"
    output = None

    for opt, arg in opts:
        if opt in ['-g', '--group']:
            group = arg
        elif opt in ['-u', '--uri']:
            uri = arg
        elif opt in ['-d', '--dir']:
            rpmdir = arg
        elif opt in ['-p', '--priority']:
            priority = arg
        elif opt in ['-o', '--output']:
            output = arg

    if output == None:
        output = sys.stdout
    else:
        output = file(output, "w")

    scan_rpm_dir(rpmdir, uri, group, priority, output)

########NEW FILE########
__FILENAME__ = selinux_baseline
#!/usr/bin/env python

import sys
import logging
import lxml.etree

import Bcfg2.Logger
import Bcfg2.Options
from Bcfg2.Client.Tools.SELinux import *

LOGGER = None

def get_setup():
    global LOGGER
    optinfo = Bcfg2.Options.CLIENT_COMMON_OPTIONS
    setup = Bcfg2.Options.OptionParser(optinfo)
    setup.parse(sys.argv[1:])

    if setup['args']:
        print("selinux_baseline.py takes no arguments, only options")
        print(setup.buildHelpMessage())
        raise SystemExit(1)
    level = 30
    if setup['verbose']:
        level = 20
    if setup['debug']:
        level = 0
    Bcfg2.Logger.setup_logging('selinux_base',
                               to_syslog=False,
                               level=level,
                               to_file=setup['logging'])
    LOGGER = logging.getLogger('bcfg2')
    return setup

def main():
    setup = get_setup()
    config = lxml.etree.Element("Configuration")
    selinux = SELinux(LOGGER, setup, config)

    baseline = lxml.etree.Element("Bundle", name="selinux_baseline")
    for etype, handler in selinux.handlers.items():
        baseline.append(lxml.etree.Comment("%s entries" % etype))
        extra = handler.FindExtra()
        for entry in extra:
            if etype != "SEModule":
                entry.tag = "Bound%s" % etype
            else:
                entry.tag = "%s" % etype
        baseline.extend(extra)

    print(lxml.etree.tostring(baseline, pretty_print=True))

if __name__ == "__main__":
    sys.exit(main())

########NEW FILE########
__FILENAME__ = posixunified
#!/usr/bin/env python

from copy import deepcopy
import lxml.etree
import os
import sys

import Bcfg2.Options

"""
NOTE: This script takes a conservative approach when it comes to
      updating your Rules. It creates a new unified-rules.xml file
      without the attributes you have defined in your current rules. The
      reason for this is to keep this script simple so we don't have
      to go through and determine the priorities associated with your
      current rules definitions.
"""

if __name__ == '__main__':
    opts = {
               'repo': Bcfg2.Options.SERVER_REPOSITORY,
           }
    setup = Bcfg2.Options.OptionParser(opts)
    setup.parse(sys.argv[1:])
    repo = setup['repo']
    unifiedposixrules = "%s/Rules/unified-rules.xml" % repo
    rulesroot = lxml.etree.Element("Rules")

    for plug in ['Base', 'Bundler']:
        for root, dirs, files in os.walk('%s/%s' % (repo, plug)):
            if '.svn' in dirs:
                dirs.remove('.svn')
            for filename in files:
                if filename.startswith('new'):
                    continue
                xdata = lxml.etree.parse(os.path.join(root, filename))
                # replace ConfigFile elements
                for c in xdata.findall('//ConfigFile'):
                    parent = c.getparent()
                    oldc = c
                    c.tag = 'Path'
                    parent.replace(oldc, c)
                # replace Directory elements
                for d in xdata.findall('//Directory'):
                    parent = d.getparent()
                    oldd = d
                    d.tag = 'Path'
                    parent.replace(oldd, d)
                    # Create new-style Rules entry
                    newd = deepcopy(d)
                    newd.set('type', 'directory')
                    rulesroot.append(newd)
                # replace BoundDirectory elements
                for d in xdata.findall('//BoundDirectory'):
                    parent = d.getparent()
                    oldd = d
                    d.tag = 'BoundPath'
                    parent.replace(oldd, d)
                    # Create new-style entry
                    newd = deepcopy(d)
                    newd.set('type', 'directory')
                # replace Permissions elements
                for p in xdata.findall('//Permissions'):
                    parent = p.getparent()
                    oldp = p
                    p.tag = 'Path'
                    parent.replace(oldp, p)
                    # Create new-style Rules entry
                    newp = deepcopy(p)
                    newp.set('type', 'permissions')
                    rulesroot.append(newp)
                # replace BoundPermissions elements
                for p in xdata.findall('//BoundPermissions'):
                    parent = p.getparent()
                    oldp = p
                    p.tag = 'BoundPath'
                    parent.replace(oldp, p)
                    # Create new-style entry
                    newp = deepcopy(p)
                    newp.set('type', 'permissions')
                # replace SymLink elements
                for s in xdata.findall('//SymLink'):
                    parent = s.getparent()
                    olds = s
                    s.tag = 'Path'
                    parent.replace(olds, s)
                    # Create new-style Rules entry
                    news = deepcopy(s)
                    news.set('type', 'symlink')
                    rulesroot.append(news)
                # replace BoundSymLink elements
                for s in xdata.findall('//BoundSymLink'):
                    parent = s.getparent()
                    olds = s
                    s.tag = 'BoundPath'
                    parent.replace(olds, s)
                    # Create new-style entry
                    news = deepcopy(s)
                    news.set('type', 'symlink')
                # write out the new bundle
                try:
                    newbundle = open("%s/%s/new%s" % (repo, plug, filename), 'w')
                except IOError:
                    print("Failed to write %s" % filename)
                    continue
                newbundle.write(lxml.etree.tostring(xdata, pretty_print=True))
                newbundle.close()

    try:
        newrules = open(unifiedposixrules, 'w')
        rulesroot.set('priority', '1')
        newrules.write(lxml.etree.tostring(rulesroot, pretty_print=True))
        newrules.close()
    except IOError:
        print("Failed to write %s" % unifiedposixrules)

########NEW FILE########
__FILENAME__ = nagiosgen-convert
#!/usr/bin/env python

import os
import sys
import lxml.etree

import Bcfg2.Options

def main():
    opts = {'repo': Bcfg2.Options.SERVER_REPOSITORY}
    setup = Bcfg2.Options.OptionParser(opts)
    setup.parse(sys.argv[1:])
    repo = setup['repo']
    oldconfigfile = os.path.join(repo, 'Properties', 'NagiosGen.xml')
    newconfigpath = os.path.join(repo, 'NagiosGen')
    newconfigfile = os.path.join(newconfigpath, 'config.xml')
    parentsfile   = os.path.join(newconfigpath, 'parents.xml')

    if not os.path.exists(oldconfigfile):
        print("%s does not exist, nothing to do" % oldconfigfile)
        return 1

    if not os.path.exists(newconfigpath):
        print("%s does not exist, cannot write %s" %
              (newconfigpath, newconfigfile))
        return 2

    newconfig = lxml.etree.XML("<NagiosGen/>")

    oldconfig = lxml.etree.parse(oldconfigfile)
    for host in oldconfig.getroot().getchildren():
        if host.tag == lxml.etree.Comment:
            # skip comments
            continue
        
        if host.tag == 'default':
            print("default tag will not be converted; use a suitable Group tag instead")
            continue
        
        newhost = lxml.etree.Element("Client", name=host.tag)
        for opt in host:
            newopt = lxml.etree.Element("Option", name=opt.tag)
            newopt.text = opt.text
            newhost.append(newopt)
        newconfig.append(newhost)

    # parse the parents config, if it exists
    if os.path.exists(parentsfile):
        parentsconfig = lxml.etree.parse(parentsfile)
        for el in parentsconfig.xpath("//Depend"):
            newhost = newconfig.find("Client[@name='%s']" % el.get("name"))
            if newhost is not None:
                newparents = newhost.find("Option[@name='parents']")
                if newparents is not None:
                    newparents.text += "," + el.get("on")
                else:
                    newparents = lxml.etree.Element("Option", name="parents")
                    newparents.text = el.get("on")
                    newhost.append(newparents)
            else:
                newhost = lxml.etree.Element("Client", name=el.get("name"))
                newparents = lxml.etree.Element("Option", name="parents")
                newparents.text = el.get("on")
                newhost.append(newparents)
                newconfig.append(newhost)

    try:
        open(newconfigfile, 'w').write(lxml.etree.tostring(newconfig,
                                                           pretty_print=True))
        print("%s written" % newconfigfile)
    except IOError:
        print("Failed to write %s" % newconfigfile)

if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = packages-convert
#!/usr/bin/env python

import os
import sys
import lxml.etree
from Bcfg2.Compat import ConfigParser
import Bcfg2.Options

XI_NAMESPACE = "http://www.w3.org/2001/XInclude"
XI = "{%s}" % XI_NAMESPACE

def place_source(xdata, source, groups):
    """ given a source's group memberships, place it appropriately
    within the given XML document tree """
    if not groups:
        xdata.append(source)
    else:
        for group in groups:
            match = xdata.xpath("Group[@name='%s']" % group)
            if match:
                groups.remove(group)
                xdata.replace(match[0], place_source(match[0], source, groups))
                return xdata

        # no group found to put this source into
        group = groups.pop()
        xdata.append(place_source(lxml.etree.Element("Group", name=group),
                                  source, groups))

    return xdata

def main():
    opts = {'repo': Bcfg2.Options.SERVER_REPOSITORY}
    setup = Bcfg2.Options.OptionParser(opts)
    setup.parse(sys.argv[1:])
    repo = setup['repo']
    configpath = os.path.join(repo, 'Packages')
    oldconfigfile  = os.path.join(configpath, 'config.xml')
    newconfigfile  = os.path.join(configpath, 'packages.conf')
    newsourcesfile = os.path.join(configpath, 'sources.xml')

    if not os.path.exists(oldconfigfile):
        print("%s does not exist, nothing to do" % oldconfigfile)
        return 1

    if not os.path.exists(configpath):
        print("%s does not exist, cannot write %s" % (configpath,
                                                      newconfigfile))
        return 2

    newconfig = ConfigParser.SafeConfigParser()
    newconfig.add_section("global")

    oldconfig = lxml.etree.parse(oldconfigfile).getroot()

    config = oldconfig.xpath('//Sources/Config')
    if config:
        if config[0].get("resolver", "enabled").lower() == "disabled":
            newconfig.add_option("global", "resolver", "disabled")
        if config[0].get("metadata", "enabled").lower() == "disabled":
            newconfig.add_option("global", "metadata", "disabled")
    newconfig.write(open(newconfigfile, "w"))
    print("%s written" % newconfigfile)

    oldsources = [oldconfigfile]
    while oldsources:
        oldfile = oldsources.pop()
        oldsource = lxml.etree.parse(oldfile).getroot()

        if oldfile == oldconfigfile:
            newfile = newsourcesfile
        else:
            newfile = os.path.join(configpath,
                                   oldfile.replace("%s/" % configpath, ''))
        newsource = lxml.etree.Element("Sources", nsmap=oldsource.nsmap)

        for el in oldsource.getchildren():
            if el.tag == lxml.etree.Comment or el.tag == 'Config':
                # skip comments and Config
                continue
        
            if el.tag == XI + 'include':
                oldsources.append(os.path.join(configpath, el.get('href')))
                newsource.append(el)
                continue

            # element must be a *Source
            newel = lxml.etree.Element("Source",
                                       type=el.tag.replace("Source",
                                                           "").lower())
            try:
                newel.set('recommended', el.find('Recommended').text.lower())
            except AttributeError:
                pass

            for tag in ['RawURL', 'URL', 'Version']:
                try:
                    newel.set(tag.lower(), el.find(tag).text)
                except AttributeError:
                    pass
            
            for child in el.getchildren():
                if child.tag in ['Component', 'Blacklist', 'Whitelist', 'Arch']:
                    newel.append(child)

            groups = [e.text for e in el.findall("Group")]
            newsource = place_source(newsource, newel, groups)

        try:
            open(newfile, 'w').write(lxml.etree.tostring(newsource,
                                                         pretty_print=True))
            print("%s written" % newfile)
        except IOError:
            print("Failed to write %s" % newfile)

if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = migrate_configs
#!/usr/bin/env python

import os
import sys
from Bcfg2.Compat import ConfigParser
import Bcfg2.Options

def copy_section(src_file, tgt_cfg, section, newsection=None):
    if newsection is None:
        newsection = section

    cfg = ConfigParser.ConfigParser()
    if len(cfg.read(src_file)) == 1:
        if cfg.has_section(section):
            try:
                tgt_cfg.add_section(newsection)
            except ConfigParser.DuplicateSectionError:
                print("[%s] section already exists in %s, adding options" %
                      (newsection, setup['configfile']))
            for opt in cfg.options(section):
                val = cfg.get(section, opt)
                if tgt_cfg.has_option(newsection, opt):
                    print("%s in [%s] already populated in %s, skipping" %
                          (opt, newsection, setup['configfile']))
                    print("  %s: %s" % (setup['configfile'],
                                        tgt_cfg.get(newsection, opt)))
                    print("  %s: %s" % (src_file, val))
                else:
                    print("Set %s in [%s] to %s" % (opt, newsection, val))
                    tgt_cfg.set(newsection, opt, val)

def main():
    opts = dict(repo=Bcfg2.Options.SERVER_REPOSITORY,
                configfile=Bcfg2.Options.CFILE)
    setup = Bcfg2.Options.OptionParser(opts)
    setup.parse(sys.argv[1:])

    # files that you should remove manually
    remove = []

    # move rules config out of rules.conf and into bcfg2.conf
    rules_conf = os.path.join(setup['repo'], 'Rules', 'rules.conf')
    if os.path.exists(rules_conf):
        remove.append(rules_conf)
        copy_section(rules_conf, setup.cfp, "rules")

    # move packages config out of packages.conf and into bcfg2.conf
    pkgs_conf = os.path.join(setup['repo'], 'Packages', 'packages.conf')
    if os.path.exists(pkgs_conf):
        remove.append(pkgs_conf)
        copy_section(pkgs_conf, setup.cfp, "global", newsection="packages")
        for section in ["apt", "yum", "pulp"]:
            copy_section(pkgs_conf, setup.cfp, section,
                         newsection="packages:" + section)

    # move reports database config into [database] section
    if setup.cfp.has_section("statistics"):
        if not setup.cfp.has_section("database"):
            setup.cfp.add_section("database")
        for opt in setup.cfp.options("statistics"):
            if opt.startswith("database_"):
                newopt = opt[9:]
                if setup.cfp.has_option("database", newopt):
                    print("%s in [database] already populated, skipping" %
                          newopt)
                else:
                    setup.cfp.set("database", newopt,
                                  setup.cfp.get("statistics", opt))
                    setup.cfp.remove_option("statistics", opt)

    print("Writing %s" % setup['configfile'])
    try:
        setup.cfp.write(open(setup['configfile'], "w"))
        if len(remove):
            print("Settings were migrated, but you must remove these files "
                  "manually:")
            for path in remove:
                print("  %s" % path)
    except IOError:
        err = sys.exc_info()[1]
        print("Could not write %s: %s" % (setup['configfile'], err))

if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = migrate_dbstats
#!/usr/bin/env python

import os
os.environ['BCFG2_LEGACY_MODELS'] = '1'
os.environ['DJANGO_SETTINGS_MODULE'] = 'Bcfg2.settings'

import sys
import logging
import time
import Bcfg2.Logger
import Bcfg2.Options
from django.core.cache import cache
from django.db import connection, backend

from Bcfg2.Server.Admin.Reports import Reports
from Bcfg2.Reporting import models as new_models
from Bcfg2.Reporting.utils import BatchFetch
from Bcfg2.Reporting.Compat import transaction
from Bcfg2.Server.Reports.reports import models as legacy_models

logger = logging.getLogger(__name__)

_our_backend = None


def _quote(value):
    """
    Quote a string to use as a table name or column

    Newer versions and various drivers require an argument
    https://code.djangoproject.com/ticket/13630
    """
    global _our_backend
    if not _our_backend:
        try:
            _our_backend = backend.DatabaseOperations(connection)
        except TypeError:
            _our_backend = backend.DatabaseOperations()
    return _our_backend.quote_name(value)


@transaction.atomic
def _migrate_perms():
    """helper"""

    fperms = {}

    logger.info("Creating FilePerms objects")
    for data in (('owner', 'group', 'perms'),
            ('current_owner', 'current_group', 'current_perms')):
        for grp in legacy_models.Reason.objects.values_list(*data).distinct():
            if grp in fperms:
                continue
            fp = new_models.FilePerms(owner=grp[0], group=grp[1], mode=grp[2])
            fp.save()
            fperms[grp] = fp

    return fperms


@transaction.atomic
def _migrate_transaction(inter, entries, fperms):
    """helper"""

    logger.debug("Migrating interaction %s for %s" %
        (inter.id, inter.client.name))

    newint = new_models.Interaction(id=inter.id,
        client_id=inter.client_id,
        timestamp=inter.timestamp,
        state=inter.state,
        repo_rev_code=inter.repo_rev_code,
        server=inter.server,
        good_count=inter.goodcount,
        total_count=inter.totalcount,
        bad_count=inter.bad_entries,
        modified_count=inter.modified_entries,
        extra_count=inter.extra_entries)

    if inter.metadata:
        newint.profile_id = inter.metadata.profile.id
        groups = [grp.pk for grp in inter.metadata.groups.all()]
        bundles = [bun.pk for bun in inter.metadata.bundles.all()]
    else:
        groups = []
        bundles = []
    super(new_models.Interaction, newint).save()
    if bundles:
        newint.bundles.add(*bundles)
    if groups:
        newint.groups.add(*groups)

    updates = dict(paths=[], packages=[], actions=[], services=[])
    for ei in legacy_models.Entries_interactions.objects.select_related('reason')\
            .filter(interaction=inter):
        ent = entries[ei.entry_id]
        name = ent.name
        act_dict = dict(name=name, exists=ei.reason.current_exists,
            state=ei.type)

        if ent.kind == 'Action':
            act_dict['status'] = ei.reason.status
            if not act_dict['status']:
                act_dict['status'] = "check"
            act_dict['output'] = -1
            logger.debug("Adding action %s" % name)
            updates['actions'].append(new_models.ActionEntry.entry_get_or_create(act_dict))

        elif ent.kind == 'Package':
            act_dict['target_version'] = ei.reason.version
            act_dict['current_version'] = ei.reason.current_version
            logger.debug("Adding package %s %s" %
                (name, act_dict['target_version']))
            updates['packages'].append(new_models.PackageEntry.entry_get_or_create(act_dict))
        elif ent.kind == 'Path':
            # these might be hard.. they aren't one to one with the old model
            act_dict['path_type'] = 'file'

            act_dict['target_perms'] = fperms[(
                ei.reason.owner,
                ei.reason.group,
                ei.reason.perms
            )]

            act_dict['current_perms'] = fperms[(
                ei.reason.current_owner,
                ei.reason.current_group,
                ei.reason.current_perms
            )]

            if ei.reason.to:
                act_dict['path_type'] = 'symlink'
                act_dict['target_path'] = ei.reason.to
                act_dict['current_path'] = ei.reason.current_to
                logger.debug("Adding link %s" % name)
                updates['paths'].append(new_models.LinkEntry.entry_get_or_create(act_dict))
                continue

            act_dict['detail_type'] = new_models.PathEntry.DETAIL_UNUSED
            if ei.reason.unpruned:
                # this is the only other case we know what the type really is
                act_dict['path_type'] = 'directory'
                act_dict['detail_type'] = new_models.PathEntry.DETAIL_PRUNED
                act_dict['details'] = ei.reason.unpruned

            if ei.reason.is_sensitive:
                act_dict['detail_type'] = new_models.PathEntry.DETAIL_SENSITIVE
            elif ei.reason.is_binary:
                act_dict['detail_type'] = new_models.PathEntry.DETAIL_BINARY
                act_dict['details'] = ei.reason.current_diff
            elif ei.reason.current_diff:
                act_dict['detail_type'] = new_models.PathEntry.DETAIL_DIFF
                act_dict['details'] = ei.reason.current_diff
            logger.debug("Adding path %s" % name)
            updates['paths'].append(new_models.PathEntry.entry_get_or_create(act_dict))

        elif ent.kind == 'Service':
            act_dict['target_status'] = ei.reason.status
            act_dict['current_status'] = ei.reason.current_status
            logger.debug("Adding service %s" % name)
            updates['services'].append(new_models.ServiceEntry.entry_get_or_create(act_dict))
        else:
            logger.warn("Skipping type %s" % ent.kind)

    for entry_type in updates.keys():
        i = 0
        while(i < len(updates[entry_type])):
            getattr(newint, entry_type).add(*updates[entry_type][i:i + 100])
            i += 100

    for perf in inter.performance_items.all():
        new_models.Performance(
            interaction=newint,
            metric=perf.metric,
            value=perf.value).save()


def _shove(old_table, new_table, columns):
    cols = ",".join([_quote(f) for f in columns])
    sql = "insert into %s(%s) select %s from %s" % (
        _quote(new_table),
        cols,
        cols,
        _quote(old_table))

    cursor = connection.cursor()
    cursor.execute(sql)
    cursor.close()


@transaction.atomic
def migrate_stage1():
    logger.info("Migrating clients")
    try:
        _shove(legacy_models.Client._meta.db_table, new_models.Client._meta.db_table,
            ('id', 'name', 'creation', 'expiration'))
    except:
        logger.error("Failed to migrate clients", exc_info=1)
        return False

    logger.info("Migrating Bundles")
    try:
        _shove(legacy_models.Bundle._meta.db_table, new_models.Bundle._meta.db_table,
            ('id', 'name'))
    except:
        logger.error("Failed to migrate bundles", exc_info=1)
        return False

    logger.info("Migrating Groups")
    try:
        _shove(legacy_models.Group._meta.db_table, new_models.Group._meta.db_table,
            ('id', 'name', 'profile', 'public', 'category', 'comment'))
    except:
        logger.error("Failed to migrate groups", exc_info=1)
        return False
    return True


def _restructure():
    """major restructure of reporting data"""

    # run any migrations from the previous schema
    try:
        from Bcfg2.Server.Reports.updatefix import update_database
        update_database()
    except:
        logger.error("Failed to run legacy schema updates", exc_info=1)
        return False

    # try to avoid dangling transactions
    if not migrate_stage1():
        return

    try:
        entries = {}
        for ent in legacy_models.Entries.objects.all():
            entries[ent.id] = ent
    except:
        logger.error("Failed to populate entries dict", exc_info=1)
        return False

    try:
        fperms = _migrate_perms()
    except:
        logger.error("Failed create FilePerms objects", exc_info=1)
        return False

    failures = []
    int_count = legacy_models.Interaction.objects.count()
    if int_count == 0:
        logger.error("Found no legacy interactions")
        return False
    int_ctr = 0
    start_time = 0
    for inter in BatchFetch(legacy_models.Interaction.objects.\
            select_related('metadata', 'client').all()):
        if int_ctr % 1000 == 0:
            if int_ctr > 0:
                logger.info("Migrated %s of %s interactions in %ss" % \
                    (int_ctr, int_count, time.time() - start_time))
            else:
                logger.info("Migrating interactions")
            start_time = time.time()
        try:
            _migrate_transaction(inter, entries, fperms)
        except:
            logger.error("Failed to migrate interaction %s for %s" %
                (inter.id, inter.client.name), exc_info=1)
            failures.append(inter.id)
        int_ctr += 1
    if not failures:
        logger.info("Successfully restructured reason data")
        return True

    logger.info("Updating recent interactions")
    for newint in new_models.Interaction.objects.recent():
        try:
            newint.save()
        except:
            logger.error("Failed to set current interaction %s for %s" %
                (newint.id, newint.client.name), exc_info=1)


if __name__ == '__main__':
    Bcfg2.Logger.setup_logging('bcfg2-report-collector',
                                   to_console=logging.INFO,
                                   level=logging.INFO)

    optinfo = dict()
    optinfo.update(Bcfg2.Options.CLI_COMMON_OPTIONS)
    optinfo.update(Bcfg2.Options.SERVER_COMMON_OPTIONS)
    setup = Bcfg2.Options.OptionParser(optinfo)
    setup.parse(sys.argv[1:])

    #sync!
    Reports(setup).__call__(['update'])

    _restructure()

########NEW FILE########
__FILENAME__ = migrate_info
#!/usr/bin/env python

import os
import re
import sys
import lxml.etree
import Bcfg2.Options
from Bcfg2.Server.Plugin import INFO_REGEX


PERMS_REGEX = re.compile(r'perms:\s*(?P<perms>\w+)')


def convert(info_file):
    info_xml = os.path.join(os.path.dirname(info_file), "info.xml")
    if os.path.exists(info_xml):
        print("%s already exists, not converting %s" % (info_xml, info_file))
        return
    print("Converting %s to %s" % (info_file, info_xml))
    fileinfo = lxml.etree.Element("FileInfo")
    info = lxml.etree.SubElement(fileinfo, "Info")
    for line in open(info_file).readlines():
        match = INFO_REGEX.match(line) or PERMS_REGEX.match(line)
        if match:
            mgd = match.groupdict()
            for key, value in list(mgd.items()):
                if value:
                    info.set(key, value)

    open(info_xml, "w").write(lxml.etree.tostring(fileinfo, pretty_print=True))
    os.unlink(info_file)


def main():
    opts = dict(repo=Bcfg2.Options.SERVER_REPOSITORY,
                configfile=Bcfg2.Options.CFILE,
                plugins=Bcfg2.Options.SERVER_PLUGINS)
    setup = Bcfg2.Options.OptionParser(opts)
    setup.parse(sys.argv[1:])

    for plugin in setup['plugins']:
        if plugin not in ['SSLCA', 'Cfg', 'TGenshi', 'TCheetah', 'SSHbase']:
            continue
        for root, dirs, files in os.walk(os.path.join(setup['repo'], plugin)):
            for fname in files:
                if fname in [":info", "info"]:
                    convert(os.path.join(root, fname))


if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = migrate_perms_to_mode
#!/usr/bin/env python

import lxml.etree
import os
import sys
from fnmatch import fnmatch
from Bcfg2.Compat import any
import Bcfg2.Options


def setmodeattr(elem):
    """Set the mode attribute for a given element."""
    if elem.attrib.has_key('perms'):
        elem.set('mode', elem.get('perms'))
        del elem.attrib['perms']
        return True
    return False


def writefile(f, xdata):
    """Write xml data to a file"""
    newfile = open(f, 'w')
    newfile.write(lxml.etree.tostring(xdata, pretty_print=True))
    newfile.close()


def convertinfo(ifile):
    """Do perms -> mode conversion for info.xml files."""
    try:
        xdata = lxml.etree.parse(ifile)
    except lxml.etree.XMLSyntaxError:
        err = sys.exc_info()[1]
        print("Could not parse %s, skipping: %s" % (ifile, err))
        return
    found = False
    for i in xdata.findall('//Info'):
        found |= setmodeattr(i)
    if found:
        writefile(ifile, xdata)


def convertstructure(structfile):
    """Do perms -> mode conversion for structure files."""
    try:
        xdata = lxml.etree.parse(structfile)
    except lxml.etree.XMLSyntaxError:
        err = sys.exc_info()[1]
        print("Could not parse %s, skipping: %s" % (structfile, err))
        return
    found = False
    for path in xdata.xpath('//BoundPath|//Path'):
        found |= setmodeattr(path)
    if found:
        writefile(structfile, xdata)


def skip_path(path, setup):
    return any(fnmatch(path, p) or fnmatch(os.path.basename(path), p)
               for p in setup['ignore'])


def main():
    opts = dict(repo=Bcfg2.Options.SERVER_REPOSITORY,
                configfile=Bcfg2.Options.CFILE,
                ignore=Bcfg2.Options.SERVER_FAM_IGNORE,
                plugins=Bcfg2.Options.SERVER_PLUGINS)
    setup = Bcfg2.Options.OptionParser(opts)
    setup.parse(sys.argv[1:])
    repo = setup['repo']

    for plugin in setup['plugins']:
        if plugin in ['Base', 'Bundler', 'Rules']:
            for root, dirs, files in os.walk(os.path.join(repo, plugin)):
                if skip_path(root, setup):
                    continue
                for fname in files:
                    if skip_path(fname, setup):
                        continue
                    convertstructure(os.path.join(root, fname))
        if plugin not in ['Cfg', 'TGenshi', 'TCheetah', 'SSHbase', 'SSLCA']:
            continue
        for root, dirs, files in os.walk(os.path.join(repo, plugin)):
            if skip_path(root, setup):
                continue
            for fname in files:
                if fname == 'info.xml':
                    convertinfo(os.path.join(root, fname))

if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = migrate_probe_groups_to_db
#!/bin/env python
""" Migrate Probe host and group data from XML to DB backend for Metadata
and Probe plugins. Does not migrate individual probe return data. Assumes
migration to BOTH Metadata and Probe to database backends. """

import os
os.environ['DJANGO_SETTINGS_MODULE'] = 'Bcfg2.settings'

import lxml.etree
import sys
import Bcfg2.Options

from Bcfg2.Server.Plugins.Metadata import MetadataClientModel
from Bcfg2.Server.Plugins.Probes import ProbesGroupsModel

def migrate(xclient):
    """ Helper to do the migration given a <Client/> XML element """
    client_name = xclient.get('name')
    try:
        try:
            client = MetadataClientModel.objects.get(hostname=client_name)
        except MetadataClientModel.DoesNotExist:
            client = MetadataClientModel(hostname=client_name)
            client.save()
    except:
        print("Failed to migrate client %s" % (client))
        return False

    try:
        cgroups = []
        for xgroup in xclient.findall('Group'):
            group_name = xgroup.get('name')
            cgroups.append(group_name)
            try:
                group = ProbesGroupsModel.objects.get(hostname=client_name, group=group_name)
            except ProbesGroupsModel.DoesNotExist:
                group = ProbesGroupsModel(hostname=client_name, group=group_name)
                group.save()

        ProbesGroupsModel.objects.filter(
            hostname=client.hostname).exclude(
            group__in=cgroups).delete()

    except:
        print("Failed to migrate groups")
        return False
    return True

def main():
    """ Main """
    opts = dict(repo=Bcfg2.Options.SERVER_REPOSITORY)
    setup = Bcfg2.Options.OptionParser(opts)
    setup.parse(sys.argv[1:])

    probefile = os.path.join(setup['repo'], 'Probes', "probed.xml")

    try:
        xdata = lxml.etree.parse(probefile)
    except lxml.etree.XMLSyntaxError:
        err = sys.exc_info()[1]
        print("Could not parse %s, skipping: %s" % (probefile, err))
    
    for xclient in xdata.findall('Client'):
        print "Migrating Metadata and Probe groups for %s" % xclient.get('name')
        migrate(xclient)

if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = service_modes
#!/usr/bin/env python

import os
import sys
import glob
import lxml.etree
import Bcfg2.Options

def main():
    opts = dict(repo=Bcfg2.Options.SERVER_REPOSITORY)
    setup = Bcfg2.Options.OptionParser(opts)
    setup.parse(sys.argv[1:])

    files = []
    for plugin in ['Bundler', 'Rules', 'Default']:
        files.extend(glob.glob(os.path.join(setup['repo'], plugin, "*")))

    for bfile in files:
        bdata = lxml.etree.parse(bfile)
        changed = False
        for svc in bdata.xpath("//Service|//BoundService"):
            if "mode" not in svc.attrib:
                continue
            mode = svc.get("mode")
            del svc.attrib["mode"]
            if mode not in ["default", "supervised", "interactive_only",
                            "manual"]:
                print("Unrecognized mode on Service:%s: %s.  Assuming default" %
                      (svc.get("name"), mode))
                mode = "default"
            if mode == "default" or mode == "supervised":
                svc.set("restart", "true")
                svc.set("install", "true")
            elif mode == "interactive_only":
                svc.set("restart", "interactive")
                svc.set("install", "true")
            elif mode == "manual":
                svc.set("restart", "false")
                svc.set("install", "false")
            changed = True
        if changed:
            print("Writing %s" % bfile)
            try:
                open(bfile, "w").write(lxml.etree.tostring(bdata))
            except IOError:
                err = sys.exc_info()[1]
                print("Could not write %s: %s" % (bfile, err))

if __name__ == '__main__':
    sys.exit(main())

########NEW FILE########
__FILENAME__ = yum-listpkgs-xml
#!/usr/bin/python
import sys
sys.path.append('/usr/bin/')
sys.path.append('/usr/share/yum-cli')

import yummain


def mySimpleList(self, pkg):
    print("<Package name='%s' version='%s'/>" % (pkg.name, pkg.printVer()))


def myListPkgs(self, lst, description, outputType):
    """outputs based on whatever outputType is. Current options:
    'list' - simple pkg list
    'info' - similar to rpm -qi output"""

    if outputType in ['list', 'info']:
        thingslisted = 0
        if len(lst) > 0:
            thingslisted = 1
            from yum.misc import sortPkgObj
            lst.sort(sortPkgObj)
            for pkg in lst:
                if outputType == 'list':
                    self.simpleList(pkg)
                elif outputType == 'info':
                    self.infoOutput(pkg)
                else:
                    pass

        if thingslisted == 0:
            return 1, ['No Packages to list']

yummain.cli.output.YumOutput.listPkgs = myListPkgs
yummain.cli.output.YumOutput.simpleList = mySimpleList

try:
    sys.argv = [sys.argv[0], '-d', '0', 'list']
    yummain.main(sys.argv[1:])
except KeyboardInterrupt:
    sys.stderr.write("\n\nExiting on user cancel.")
    sys.exit(1)

########NEW FILE########

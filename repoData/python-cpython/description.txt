.. highlightlang:: c

.. _descriptor-objects:

Descriptor Objects
------------------

"Descriptors" are objects that describe some attribute of an object. They are
found in the dictionary of type objects.

.. XXX document these!

.. c:var:: PyTypeObject PyProperty_Type

   The type object for the built-in descriptor types.


.. c:function:: PyObject* PyDescr_NewGetSet(PyTypeObject *type, struct PyGetSetDef *getset)


.. c:function:: PyObject* PyDescr_NewMember(PyTypeObject *type, struct PyMemberDef *meth)


.. c:function:: PyObject* PyDescr_NewMethod(PyTypeObject *type, struct PyMethodDef *meth)


.. c:function:: PyObject* PyDescr_NewWrapper(PyTypeObject *type, struct wrapperbase *wrapper, void *wrapped)


.. c:function:: PyObject* PyDescr_NewClassMethod(PyTypeObject *type, PyMethodDef *method)


.. c:function:: int PyDescr_IsData(PyObject *descr)

   Return true if the descriptor objects *descr* describes a data attribute, or
   false if it describes a method.  *descr* must be a descriptor object; there is
   no error checking.


.. c:function:: PyObject* PyWrapper_New(PyObject *, PyObject *)

======================
Descriptor HowTo Guide
======================

:Author: Raymond Hettinger
:Contact: <python at rcn dot com>

.. Contents::

Abstract
--------

Defines descriptors, summarizes the protocol, and shows how descriptors are
called.  Examines a custom descriptor and several built-in python descriptors
including functions, properties, static methods, and class methods.  Shows how
each works by giving a pure Python equivalent and a sample application.

Learning about descriptors not only provides access to a larger toolset, it
creates a deeper understanding of how Python works and an appreciation for the
elegance of its design.


Definition and Introduction
---------------------------

In general, a descriptor is an object attribute with "binding behavior", one
whose attribute access has been overridden by methods in the descriptor
protocol.  Those methods are :meth:`__get__`, :meth:`__set__`, and
:meth:`__delete__`.  If any of those methods are defined for an object, it is
said to be a descriptor.

The default behavior for attribute access is to get, set, or delete the
attribute from an object's dictionary.  For instance, ``a.x`` has a lookup chain
starting with ``a.__dict__['x']``, then ``type(a).__dict__['x']``, and
continuing through the base classes of ``type(a)`` excluding metaclasses. If the
looked-up value is an object defining one of the descriptor methods, then Python
may override the default behavior and invoke the descriptor method instead.
Where this occurs in the precedence chain depends on which descriptor methods
were defined.

Descriptors are a powerful, general purpose protocol.  They are the mechanism
behind properties, methods, static methods, class methods, and :func:`super()`.
They are used throughout Python itself to implement the new style classes
introduced in version 2.2.  Descriptors simplify the underlying C-code and offer
a flexible set of new tools for everyday Python programs.


Descriptor Protocol
-------------------

``descr.__get__(self, obj, type=None) --> value``

``descr.__set__(self, obj, value) --> None``

``descr.__delete__(self, obj) --> None``

That is all there is to it.  Define any of these methods and an object is
considered a descriptor and can override default behavior upon being looked up
as an attribute.

If an object defines both :meth:`__get__` and :meth:`__set__`, it is considered
a data descriptor.  Descriptors that only define :meth:`__get__` are called
non-data descriptors (they are typically used for methods but other uses are
possible).

Data and non-data descriptors differ in how overrides are calculated with
respect to entries in an instance's dictionary.  If an instance's dictionary
has an entry with the same name as a data descriptor, the data descriptor
takes precedence.  If an instance's dictionary has an entry with the same
name as a non-data descriptor, the dictionary entry takes precedence.

To make a read-only data descriptor, define both :meth:`__get__` and
:meth:`__set__` with the :meth:`__set__` raising an :exc:`AttributeError` when
called.  Defining the :meth:`__set__` method with an exception raising
placeholder is enough to make it a data descriptor.


Invoking Descriptors
--------------------

A descriptor can be called directly by its method name.  For example,
``d.__get__(obj)``.

Alternatively, it is more common for a descriptor to be invoked automatically
upon attribute access.  For example, ``obj.d`` looks up ``d`` in the dictionary
of ``obj``.  If ``d`` defines the method :meth:`__get__`, then ``d.__get__(obj)``
is invoked according to the precedence rules listed below.

The details of invocation depend on whether ``obj`` is an object or a class.

For objects, the machinery is in :meth:`object.__getattribute__` which
transforms ``b.x`` into ``type(b).__dict__['x'].__get__(b, type(b))``.  The
implementation works through a precedence chain that gives data descriptors
priority over instance variables, instance variables priority over non-data
descriptors, and assigns lowest priority to :meth:`__getattr__` if provided.  The
full C implementation can be found in :c:func:`PyObject_GenericGetAttr()` in
`Objects/object.c <http://svn.python.org/view/python/trunk/Objects/object.c?view=markup>`_\.

For classes, the machinery is in :meth:`type.__getattribute__` which transforms
``B.x`` into ``B.__dict__['x'].__get__(None, B)``.  In pure Python, it looks
like::

    def __getattribute__(self, key):
        "Emulate type_getattro() in Objects/typeobject.c"
        v = object.__getattribute__(self, key)
        if hasattr(v, '__get__'):
           return v.__get__(None, self)
        return v

The important points to remember are:

* descriptors are invoked by the :meth:`__getattribute__` method
* overriding :meth:`__getattribute__` prevents automatic descriptor calls
* :meth:`object.__getattribute__` and :meth:`type.__getattribute__` make
  different calls to :meth:`__get__`.
* data descriptors always override instance dictionaries.
* non-data descriptors may be overridden by instance dictionaries.

The object returned by ``super()`` also has a custom :meth:`__getattribute__`
method for invoking descriptors.  The call ``super(B, obj).m()`` searches
``obj.__class__.__mro__`` for the base class ``A`` immediately following ``B``
and then returns ``A.__dict__['m'].__get__(obj, B)``.  If not a descriptor,
``m`` is returned unchanged.  If not in the dictionary, ``m`` reverts to a
search using :meth:`object.__getattribute__`.

The implementation details are in :c:func:`super_getattro()` in
`Objects/typeobject.c <http://svn.python.org/view/python/trunk/Objects/typeobject.c?view=markup>`_
and a pure Python equivalent can be found in `Guido's Tutorial`_.

.. _`Guido's Tutorial`: http://www.python.org/2.2.3/descrintro.html#cooperation

The details above show that the mechanism for descriptors is embedded in the
:meth:`__getattribute__()` methods for :class:`object`, :class:`type`, and
:func:`super`.  Classes inherit this machinery when they derive from
:class:`object` or if they have a meta-class providing similar functionality.
Likewise, classes can turn-off descriptor invocation by overriding
:meth:`__getattribute__()`.


Descriptor Example
------------------

The following code creates a class whose objects are data descriptors which
print a message for each get or set.  Overriding :meth:`__getattribute__` is
alternate approach that could do this for every attribute.  However, this
descriptor is useful for monitoring just a few chosen attributes::

    class RevealAccess(object):
        """A data descriptor that sets and returns values
           normally and prints a message logging their access.
        """

        def __init__(self, initval=None, name='var'):
            self.val = initval
            self.name = name

        def __get__(self, obj, objtype):
            print('Retrieving', self.name)
            return self.val

        def __set__(self, obj, val):
            print('Updating', self.name)
            self.val = val

    >>> class MyClass(object):
        x = RevealAccess(10, 'var "x"')
        y = 5

    >>> m = MyClass()
    >>> m.x
    Retrieving var "x"
    10
    >>> m.x = 20
    Updating var "x"
    >>> m.x
    Retrieving var "x"
    20
    >>> m.y
    5

The protocol is simple and offers exciting possibilities.  Several use cases are
so common that they have been packaged into individual function calls.
Properties, bound and unbound methods, static methods, and class methods are all
based on the descriptor protocol.


Properties
----------

Calling :func:`property` is a succinct way of building a data descriptor that
triggers function calls upon access to an attribute.  Its signature is::

    property(fget=None, fset=None, fdel=None, doc=None) -> property attribute

The documentation shows a typical use to define a managed attribute ``x``::

    class C(object):
        def getx(self): return self.__x
        def setx(self, value): self.__x = value
        def delx(self): del self.__x
        x = property(getx, setx, delx, "I'm the 'x' property.")

To see how :func:`property` is implemented in terms of the descriptor protocol,
here is a pure Python equivalent::

    class Property(object):
        "Emulate PyProperty_Type() in Objects/descrobject.c"

        def __init__(self, fget=None, fset=None, fdel=None, doc=None):
            self.fget = fget
            self.fset = fset
            self.fdel = fdel
            if doc is None and fget is not None:
                doc = fget.__doc__
            self.__doc__ = doc

        def __get__(self, obj, objtype=None):
            if obj is None:
                return self
            if self.fget is None:
                raise AttributeError("unreadable attribute")
            return self.fget(obj)

        def __set__(self, obj, value):
            if self.fset is None:
                raise AttributeError("can't set attribute")
            self.fset(obj, value)

        def __delete__(self, obj):
            if self.fdel is None:
                raise AttributeError("can't delete attribute")
            self.fdel(obj)

        def getter(self, fget):
            return type(self)(fget, self.fset, self.fdel, self.__doc__)

        def setter(self, fset):
            return type(self)(self.fget, fset, self.fdel, self.__doc__)

        def deleter(self, fdel):
            return type(self)(self.fget, self.fset, fdel, self.__doc__)

The :func:`property` builtin helps whenever a user interface has granted
attribute access and then subsequent changes require the intervention of a
method.

For instance, a spreadsheet class may grant access to a cell value through
``Cell('b10').value``. Subsequent improvements to the program require the cell
to be recalculated on every access; however, the programmer does not want to
affect existing client code accessing the attribute directly.  The solution is
to wrap access to the value attribute in a property data descriptor::

    class Cell(object):
        . . .
        def getvalue(self, obj):
            "Recalculate cell before returning value"
            self.recalc()
            return obj._value
        value = property(getvalue)


Functions and Methods
---------------------

Python's object oriented features are built upon a function based environment.
Using non-data descriptors, the two are merged seamlessly.

Class dictionaries store methods as functions.  In a class definition, methods
are written using :keyword:`def` and :keyword:`lambda`, the usual tools for
creating functions.  The only difference from regular functions is that the
first argument is reserved for the object instance.  By Python convention, the
instance reference is called *self* but may be called *this* or any other
variable name.

To support method calls, functions include the :meth:`__get__` method for
binding methods during attribute access.  This means that all functions are
non-data descriptors which return bound or unbound methods depending whether
they are invoked from an object or a class.  In pure python, it works like
this::

    class Function(object):
        . . .
        def __get__(self, obj, objtype=None):
            "Simulate func_descr_get() in Objects/funcobject.c"
            return types.MethodType(self, obj, objtype)

Running the interpreter shows how the function descriptor works in practice::

    >>> class D(object):
         def f(self, x):
              return x

    >>> d = D()
    >>> D.__dict__['f'] # Stored internally as a function
    <function f at 0x00C45070>
    >>> D.f             # Get from a class becomes an unbound method
    <unbound method D.f>
    >>> d.f             # Get from an instance becomes a bound method
    <bound method D.f of <__main__.D object at 0x00B18C90>>

The output suggests that bound and unbound methods are two different types.
While they could have been implemented that way, the actual C implementation of
:c:type:`PyMethod_Type` in
`Objects/classobject.c <http://svn.python.org/view/python/trunk/Objects/classobject.c?view=markup>`_
is a single object with two different representations depending on whether the
:attr:`im_self` field is set or is *NULL* (the C equivalent of *None*).

Likewise, the effects of calling a method object depend on the :attr:`im_self`
field. If set (meaning bound), the original function (stored in the
:attr:`im_func` field) is called as expected with the first argument set to the
instance.  If unbound, all of the arguments are passed unchanged to the original
function. The actual C implementation of :func:`instancemethod_call()` is only
slightly more complex in that it includes some type checking.


Static Methods and Class Methods
--------------------------------

Non-data descriptors provide a simple mechanism for variations on the usual
patterns of binding functions into methods.

To recap, functions have a :meth:`__get__` method so that they can be converted
to a method when accessed as attributes.  The non-data descriptor transforms a
``obj.f(*args)`` call into ``f(obj, *args)``.  Calling ``klass.f(*args)``
becomes ``f(*args)``.

This chart summarizes the binding and its two most useful variants:

      +-----------------+----------------------+------------------+
      | Transformation  | Called from an       | Called from a    |
      |                 | Object               | Class            |
      +=================+======================+==================+
      | function        | f(obj, \*args)       | f(\*args)        |
      +-----------------+----------------------+------------------+
      | staticmethod    | f(\*args)            | f(\*args)        |
      +-----------------+----------------------+------------------+
      | classmethod     | f(type(obj), \*args) | f(klass, \*args) |
      +-----------------+----------------------+------------------+

Static methods return the underlying function without changes.  Calling either
``c.f`` or ``C.f`` is the equivalent of a direct lookup into
``object.__getattribute__(c, "f")`` or ``object.__getattribute__(C, "f")``. As a
result, the function becomes identically accessible from either an object or a
class.

Good candidates for static methods are methods that do not reference the
``self`` variable.

For instance, a statistics package may include a container class for
experimental data.  The class provides normal methods for computing the average,
mean, median, and other descriptive statistics that depend on the data. However,
there may be useful functions which are conceptually related but do not depend
on the data.  For instance, ``erf(x)`` is handy conversion routine that comes up
in statistical work but does not directly depend on a particular dataset.
It can be called either from an object or the class:  ``s.erf(1.5) --> .9332`` or
``Sample.erf(1.5) --> .9332``.

Since staticmethods return the underlying function with no changes, the example
calls are unexciting::

    >>> class E(object):
         def f(x):
              print(x)
         f = staticmethod(f)

    >>> print(E.f(3))
    3
    >>> print(E().f(3))
    3

Using the non-data descriptor protocol, a pure Python version of
:func:`staticmethod` would look like this::

    class StaticMethod(object):
     "Emulate PyStaticMethod_Type() in Objects/funcobject.c"

     def __init__(self, f):
          self.f = f

     def __get__(self, obj, objtype=None):
          return self.f

Unlike static methods, class methods prepend the class reference to the
argument list before calling the function.  This format is the same
for whether the caller is an object or a class::

    >>> class E(object):
         def f(klass, x):
              return klass.__name__, x
         f = classmethod(f)

    >>> print(E.f(3))
    ('E', 3)
    >>> print(E().f(3))
    ('E', 3)


This behavior is useful whenever the function only needs to have a class
reference and does not care about any underlying data.  One use for classmethods
is to create alternate class constructors.  In Python 2.3, the classmethod
:func:`dict.fromkeys` creates a new dictionary from a list of keys.  The pure
Python equivalent is::

    class Dict(object):
        . . .
        def fromkeys(klass, iterable, value=None):
            "Emulate dict_fromkeys() in Objects/dictobject.c"
            d = klass()
            for key in iterable:
                d[key] = value
            return d
        fromkeys = classmethod(fromkeys)

Now a new dictionary of unique keys can be constructed like this::

    >>> Dict.fromkeys('abracadabra')
    {'a': None, 'r': None, 'b': None, 'c': None, 'd': None}

Using the non-data descriptor protocol, a pure Python version of
:func:`classmethod` would look like this::

    class ClassMethod(object):
         "Emulate PyClassMethod_Type() in Objects/funcobject.c"

         def __init__(self, f):
              self.f = f

         def __get__(self, obj, klass=None):
              if klass is None:
                   klass = type(obj)
              def newfunc(*args):
                   return self.f(klass, *args)
              return newfunc


Python Documentation README
~~~~~~~~~~~~~~~~~~~~~~~~~~~

This directory contains the reStructuredText (reST) sources to the Python
documentation.  You don't need to build them yourself, prebuilt versions are
available at <https://docs.python.org/dev/download.html>.

Documentation on authoring Python documentation, including information about
both style and markup, is available in the "Documenting Python" chapter of the
developers guide <http://docs.python.org/devguide/documenting.html>.


Building the docs
=================

You need to have Sphinx <http://sphinx-doc.org/> installed; it is the toolset
used to build the docs.  It is not included in this tree, but maintained
separately and available from PyPI <http://pypi.python.org/pypi/Sphinx>.


Using make
----------

A Makefile has been prepared so that on Unix, provided you have installed
Sphinx, you can just run ::

   make html

to build the HTML output files.

On Windows, we try to emulate the Makefile as closely as possible with a
``make.bat`` file.

To use a Python interpreter that's not called ``python``, use the standard
way to set Makefile variables, using e.g. ::

   make html PYTHON=python3

On Windows, set the PYTHON environment variable instead.

To use a specific sphinx-build (something other than ``sphinx-build``), set
the SPHINXBUILD variable.

Available make targets are:

 * "clean", which removes all build files.

 * "html", which builds standalone HTML files for offline viewing.

 * "htmlview", which re-uses the "html" builder, but then opens the main page
   in your default web browser.

 * "htmlhelp", which builds HTML files and a HTML Help project file usable to
   convert them into a single Compiled HTML (.chm) file -- these are popular
   under Microsoft Windows, but very handy on every platform.

   To create the CHM file, you need to run the Microsoft HTML Help Workshop
   over the generated project (.hhp) file.  The make.bat script does this for
   you on Windows.

 * "latex", which builds LaTeX source files as input to "pdflatex" to produce
   PDF documents.

 * "text", which builds a plain text file for each source file.

 * "epub", which builds an EPUB document, suitable to be viewed on e-book
   readers.

 * "linkcheck", which checks all external references to see whether they are
   broken, redirected or malformed, and outputs this information to stdout as
   well as a plain-text (.txt) file.

 * "changes", which builds an overview over all versionadded/versionchanged/
   deprecated items in the current version. This is meant as a help for the
   writer of the "What's New" document.

 * "coverage", which builds a coverage overview for standard library modules and
   C API.

 * "pydoc-topics", which builds a Python module containing a dictionary with
   plain text documentation for the labels defined in
   `tools/sphinxext/pyspecific.py` -- pydoc needs these to show topic and
   keyword help.

 * "suspicious", which checks the parsed markup for text that looks like
   malformed and thus unconverted reST.

 * "check", which checks for frequent markup errors.

 * "serve", which serves the build/html directory on port 8000.

 * "dist", (Unix only) which creates distributable archives of HTML, text,
   PDF, and EPUB builds.


Without make
------------

Install the Sphinx package and its dependencies from PyPI.

Then, from the ``Doc`` directory, run ::

   sphinx-build -b<builder> . build/<builder>

where ``<builder>`` is one of html, text, latex, or htmlhelp (for explanations
see the make targets above).


Contributing
============

Bugs in the content should be reported to the Python bug tracker at
http://bugs.python.org.

Bugs in the toolset should be reported in the Sphinx bug tracker at
http://www.bitbucket.org/birkenfeld/sphinx/issues/.

You can also send a mail to the Python Documentation Team at docs@python.org,
and we will process your request as soon as possible.

If you want to help the Documentation Team, you are always welcome.  Just send
a mail to docs@python.org.


Copyright notice
================

The Python source is copyrighted, but you can freely use and copy it
as long as you don't change or remove the copyright notice:

----------------------------------------------------------------------
Copyright (c) 2000-2014 Python Software Foundation.
All rights reserved.

Copyright (c) 2000 BeOpen.com.
All rights reserved.

Copyright (c) 1995-2000 Corporation for National Research Initiatives.
All rights reserved.

Copyright (c) 1991-1995 Stichting Mathematisch Centrum.
All rights reserved.

See the file "license.rst" for information on usage and redistribution
of this file, and for a DISCLAIMER OF ALL WARRANTIES.
----------------------------------------------------------------------

Files in this directory from from Bob Ippolito's py2app.

License: Any components of the py2app suite may be distributed under
the MIT or PSF open source licenses.

This is version 1.0, SVN revision 789, from 2006/01/25.
The main repository is http://svn.red-bean.com/bob/macholib/trunk/macholib/
This directory contains the Distutils package.

There's a full documentation available at:

    http://docs.python.org/distutils/

The Distutils-SIG web page is also a good starting point:

    http://www.python.org/sigs/distutils-sig/

WARNING : Distutils must remain compatible with 2.3

$Id$

README FOR IDLE TESTS IN IDLELIB.IDLE_TEST


1. Test Files

The idle directory, idlelib, has over 60 xyz.py files. The idle_test
subdirectory should contain a test_xyy.py for each. (For test modules, make
'xyz' lower case, and possibly shorten it.) Each file should start with the
something like the following template, with the blanks after after '.' and 'as',
and before and after '_' filled in.
---
import unittest
from test.support import requires
import idlelib. as

class _Test(unittest.TestCase):

    def test_(self):

if __name__ == '__main__':
    unittest.main(verbosity=2, exit=2)
---
Idle tests are run with unittest; do not use regrtest's test_main.

Once test_xyy is written, the following should go at the end of xyy.py,
with xyz (lowercased) added after 'test_'.
---
if __name__ == "__main__":
    from test import support; support.use_resources = ['gui']
    import unittest
    unittest.main('idlelib.idle_test.test_', verbosity=2, exit=False)
---


2. Gui Tests

Gui tests need 'requires' and 'use_resources' from test.support
(test.test_support in 2.7). A test is a gui test if it creates a Tk root or
master object either directly or indirectly by instantiating a tkinter or
idle class. For the benefit of buildbot machines that do not have a graphics
screen, gui tests must be 'guarded' by "requires('gui')" in a setUp
function or method. This will typically be setUpClass.

To avoid interfering with other gui tests, all gui objects must be destroyed
and deleted by the end of the test.  If a widget, such as a Tk root, is created
in a setUpX function, destroy it in the corresponding tearDownX.  For module
and class attributes, also delete the widget.
---
    @classmethod
    def setUpClass(cls):
        requires('gui')
        cls.root = tk.Tk()

    @classmethod
    def tearDownClass(cls):
        cls.root.destroy()
        del cls.root
---

Support.requires('gui') returns true if it is either called in a main module
(which never happens on buildbots) or if use_resources contains 'gui'.
Use_resources is set by test.regrtest but not by unittest. So when running
tests in another module with unittest, we set it ourselves, as in the xyz.py
template above.

Since non-gui tests always run, but gui tests only sometimes, tests of non-gui
operations should best avoid needing a gui. Methods that make incidental use of
tkinter (tk) variables and messageboxes can do this by using the mock classes in
idle_test/mock_tk.py. There is also a mock text that will handle some uses of the
tk Text widget.


3. Running Tests

Assume that xyz.py and test_xyz.py end with the "if __name__" statements given
above. In Idle, pressing F5 in an editor window with either loaded will run all
tests in the test_xyz file with the version of Python running Idle.  The test
report and any tracebacks will appear in the Shell window. The options in these
"if __name__" statements are appropriate for developers running (as opposed to
importing) either of the files during development: verbosity=2 lists all test
methods in the file; exit=False avoids a spurious sys.exit traceback that would
otherwise occur when running in Idle. The following command lines also run
all test methods, including gui tests, in test_xyz.py. (The exceptions are that
idlelib and idlelib.idle start Idle and idlelib.PyShell should (issue 18330).)

python -m idlelib.xyz  # With the capitalization of the xyz module
python -m idlelib.idle_test.test_xyz

To run all idle_test/test_*.py tests, either interactively
('>>>', with unittest imported) or from a command line, use one of the
following. (Notes: unittest does not run gui tests; in 2.7, 'test ' (with the
space) is 'test.regrtest '; where present, -v and -ugui can be omitted.)

>>> unittest.main('idlelib.idle_test', verbosity=2, exit=False)
python -m unittest -v idlelib.idle_test
python -m test -v -ugui test_idle
python -m test.test_idle

The idle tests are 'discovered' by idlelib.idle_test.__init__.load_tests,
which is also imported into test.test_idle. Normally, neither file should be
changed when working on individual test modules. The third command runs runs
unittest indirectly through regrtest. The same happens when the entire test
suite is run with 'python -m test'. So that command must work for buildbots
to stay green. Idle tests must not disturb the environment in a way that
makes other tests fail (issue 18081).

To run an individual Testcase or test method, extend the dotted name given to
unittest on the command line. (But gui tests will not this way.)

python -m unittest -v idlelib.idle_test.test_xyz.Test_case.test_meth

IDLE is Python's Tkinter-based Integrated DeveLopment Environment.

IDLE emphasizes a lightweight, clean design with a simple user interface.
Although it is suitable for beginners, even advanced users will find that
IDLE has everything they really need to develop pure Python code.

IDLE features a multi-window text editor with multiple undo, Python colorizing,
and many other capabilities, e.g. smart indent, call tips, and autocompletion.

The editor has comprehensive search functions, including searching through
multiple files.  Class browsers and path browsers provide fast access to
code objects from a top level viewpoint without dealing with code folding.

There is a Python Shell window which features colorizing and command recall.

IDLE executes Python code in a separate process, which is restarted for each
Run (F5) initiated from an editor window.  The environment can also be 
restarted from the Shell window without restarting IDLE.

This enhancement has often been requested, and is now finally available.  The
magic "reload/import *" incantations are no longer required when editing and
testing a module two or three steps down the import chain.

(Personal firewall software may warn about the connection IDLE makes to its
subprocess using this computer's internal loopback interface.  This connection
is not visible on any external interface and no data is sent to or received
from the Internet.)

It is possible to interrupt tightly looping user code, even on Windows.

Applications which cannot support subprocesses and/or sockets can still run
IDLE in a single process.

IDLE has an integrated debugger with stepping, persistent breakpoints, and call
stack visibility.

There is a GUI configuration manager which makes it easy to select fonts,
colors, keybindings, and startup options.  This facility includes a feature
which allows the user to specify additional help sources, either locally or on
the web.

IDLE is coded in 100% pure Python, using the Tkinter GUI toolkit (Tk/Tcl)
and is cross-platform, working on Unix, Mac, and Windows.

IDLE accepts command line arguments.  Try idle -h to see the options.


If you find bugs or have suggestions or patches, let us know about
them by using the Python issue tracker:

http://bugs.python.org

For further details and links, read the Help files and check the IDLE home
page at

http://www.python.org/idle/

There is a mail list for IDLE: idle-dev@python.org.  You can join at

http://mail.python.org/mailman/listinfo/idle-dev

In this directory:
- py2_test_grammar.py -- test file that exercises most/all of Python 2.x's grammar.
- py3_test_grammar.py -- test file that exercises most/all of Python 3.x's grammar.
- infinite_recursion.py -- test file that causes lib2to3's faster recursive pattern matching
  scheme to fail, but passes when lib2to3 falls back to iterative pattern matching.
- fixes/ -- for use by test_refactor.py

This directory exists so that 3rd party packages can be installed
here.  Read the source for site.py for more details.

This directory only contains tests for outstanding bugs that cause the
interpreter to segfault.  Ideally this directory should always be empty, but
sometimes it may not be easy to fix the underlying cause and the bug is deemed
too obscure to invest the effort.

Each test should fail when run from the command line:

	./python Lib/test/crashers/weakref_in_del.py

Put as much info into a docstring or comments to help determine the cause of the
failure, as well as a bugs.python.org issue number if it exists.  Particularly
note if the cause is system or environment dependent and what the variables are.

Once the crash is fixed, the test case should be moved into an appropriate test
(even if it was originally from the test suite).  This ensures the regression
doesn't happen again.  And if it does, it should be easier to track down.

Also see Lib/test_crashers.py which exercises the crashers in this directory.
In particular, make sure to add any new infinite loop crashers to the black
list so it doesn't try to run them.

This empty directory serves as destination for temporary files
created by some tests, in particular, the test_codecmaps_* tests.

This directory contains test cases that are known to leak references.
The idea is that you can import these modules while in the interpreter
and call the leak function repeatedly.  This will only be helpful if
the interpreter was built in debug mode.  If the total ref count
doesn't increase, the bug has been fixed and the file should be removed
from the repository.

Note:  be careful to check for cyclic garbage.  Sometimes it may be helpful
to define the leak function like:

def leak():
    def inner_leak():
        # this is the function that leaks, but also creates cycles
    inner_leak()
    gc.collect() ; gc.collect() ; gc.collect()

Here's an example interpreter session for test_gestalt which still leaks:

>>> from test.leakers.test_gestalt import leak
[24275 refs]
>>> leak()
[28936 refs]
>>> leak()
[28938 refs]
>>> leak()
[28940 refs]
>>> 

Once the leak is fixed, the test case should be moved into an appropriate
test (even if it was originally from the test suite).  This ensures the
regression doesn't happen again.  And if it does, it should be easier
to track down.

Sound file samples used by Lib/test/test_sndhdr.py and generated using the
following commands:

   dd if=/dev/zero of=sndhdr.raw bs=20 count=1
   sox -s -2 -c 2 -r 44100 sndhdr.raw sndhdr.<format>

Sound file samples used by Lib/test/test_sndhdr.py and generated using the
following commands:

   dd if=/dev/zero of=sndhdr.raw bs=20 count=1
   sox -s -2 -c 2 -r 44100 sndhdr.raw sndhdr.<format>


Writing new tests
=================

Precaution
----------

    New tests should always use only one Tk window at once, like all the
    current tests do. This means that you have to destroy the current window
    before creating another one, and clean up after the test. The motivation
    behind this is that some tests may depend on having its window focused
    while it is running to work properly, and it may be hard to force focus
    on your window across platforms (right now only test_traversal at
    test_ttk.test_widgets.NotebookTest depends on this).


Building a Python Mac OS X distribution
=======================================

The ``build-install.py`` script creates Python distributions, including
certain third-party libraries as necessary.  It builds a complete 
framework-based Python out-of-tree, installs it in a funny place with 
$DESTROOT, massages that installation to remove .pyc files and such, creates 
an Installer package from the installation plus other files in ``resources`` 
and ``scripts`` and placed that on a ``.dmg`` disk image.

For Python 3.4.0, PSF practice is to build two installer variants
for each release.

1.  32-bit-only, i386 and PPC universal, capable on running on all machines
    supported by Mac OS X 10.5 through (at least) 10.9::

        /path/to/bootstrap/python2.7 build-installer.py \
            --sdk-path=/Developer/SDKs/MacOSX10.5.sdk \
            --universal-archs=32-bit \
            --dep-target=10.5

    - builds the following third-party libraries

        * NCurses 5.9 (http://bugs.python.org/issue15037)
        * SQLite 3.8.3.1
        * XZ 5.0.5

    - uses system-supplied versions of third-party libraries

        * readline module links with Apple BSD editline (libedit)

    - requires ActiveState ``Tcl/Tk 8.4`` (currently 8.4.20) to be installed for building

    - recommended build environment:

        * Mac OS X 10.5.8 Intel or PPC
        * Xcode 3.1.4
        * ``MacOSX10.5`` SDK
        * ``MACOSX_DEPLOYMENT_TARGET=10.5``
        * Apple ``gcc-4.2``
        * bootstrap non-framework Python 2.7 for documentation build with
          Sphinx (as of 3.4.1)

    - alternate build environments:

        * Mac OS X 10.6.8 with Xcode 3.2.6
            - need to change ``/System/Library/Frameworks/{Tcl,Tk}.framework/Version/Current`` to ``8.4``
        * Note Xcode 4.* does not support building for PPC so cannot be used for this build

2.  64-bit / 32-bit, x86_64 and i386 universal, for OS X 10.6 (and later)::

        /path/to/bootstrap/python2.7 build-installer.py \
            --sdk-path=/Developer/SDKs/MacOSX10.6.sdk \
            --universal-archs=intel \
            --dep-target=10.6

    - builds the following third-party libraries

        * NCurses 5.9 (http://bugs.python.org/issue15037)
        * SQLite 3.8.3.1
        * XZ 5.0.5

    - uses system-supplied versions of third-party libraries

        * readline module links with Apple BSD editline (libedit)

    - requires ActiveState Tcl/Tk 8.5.15.1 (or later) to be installed for building

    - recommended build environment:

        * Mac OS X 10.6.8 (or later)
        * Xcode 3.2.6
        * ``MacOSX10.6`` SDK
        * ``MACOSX_DEPLOYMENT_TARGET=10.6``
        * Apple ``gcc-4.2``
        * bootstrap non-framework Python 2.7 for documentation build with
          Sphinx (as of 3.4.1)

    - alternate build environments:

        * none.  Xcode 4.x currently supplies two C compilers.
          ``llvm-gcc-4.2.1`` has been found to miscompile Python 3.3.x and
          produce a non-functional Python executable.  As it appears to be
          considered a migration aid by Apple and is not likely to be fixed,
          its use should be avoided.  The other compiler, ``clang``, has been
          undergoing rapid development.  While it appears to have become
          production-ready in the most recent Xcode 5 releases, the versions
          available on the deprecated Xcode 4.x for 10.6 were early releases
          and did not receive the level of exposure in production environments
          that the Xcode 3 gcc-4.2 compiler has had.


*   For Python 2.7.x and 3.2.x, the 32-bit-only installer was configured to
    support Mac OS X 10.3.9 through (at least) 10.6.  Because it is
    believed that there are few systems still running OS X 10.3 or 10.4
    and because it has become increasingly difficult to test and
    support the differences in these earlier systems, as of Python 3.3.0 the PSF
    32-bit installer no longer supports them.  For reference in building such
    an installer yourself, the details are::

        /usr/bin/python build-installer.py \
            --sdk-path=/Developer/SDKs/MacOSX10.4u.sdk \
            --universal-archs=32-bit \
            --dep-target=10.3 

    - builds the following third-party libraries

        * Bzip2
        * NCurses
        * GNU Readline (GPL)
        * SQLite 3
        * XZ
        * Zlib 1.2.3
        * Oracle Sleepycat DB 4.8 (Python 2.x only)

    - requires ActiveState ``Tcl/Tk 8.4`` (currently 8.4.20) to be installed for building

    - recommended build environment:
        
        * Mac OS X 10.5.8 PPC or Intel
        * Xcode 3.1.4 (or later)
        * ``MacOSX10.4u`` SDK (later SDKs do not support PPC G3 processors)
        * ``MACOSX_DEPLOYMENT_TARGET=10.3``
        * Apple ``gcc-4.0``
        * system Python 2.5 for documentation build with Sphinx

    - alternate build environments:

        * Mac OS X 10.6.8 with Xcode 3.2.6
            - need to change ``/System/Library/Frameworks/{Tcl,Tk}.framework/Version/Current`` to ``8.4``



General Prerequisites
---------------------

* No Fink (in ``/sw``) or MacPorts (in ``/opt/local``) or other local
  libraries or utilities (in ``/usr/local``) as they could
  interfere with the build.

* The documentation for the release is built using Sphinx
  because it is included in the installer.  For 2.7.x and 3.x.x up to and
  including 3.4.0, the ``Doc/Makefile`` uses ``svn`` to download repos of
  ``Sphinx`` and its dependencies.  Beginning with 3.4.1, the ``Doc/Makefile``
  assumes there is an externally-provided ``sphinx-build`` and requires at
  least Python 2.6 to run.  Because of this, it is no longer possible to
  build a 3.4.1 or later installer on OS X 10.5 using the Apple-supplied
  Python 2.5.

* It is safest to start each variant build with an empty source directory
  populated with a fresh copy of the untarred source.

* It is recommended that you remove any existing installed version of the
  Python being built::

      sudo rm -rf /Library/Frameworks/Python.framework/Versions/n.n


The Recipe
----------

Here are the steps you need to follow to build a Python installer:

* Run ``build-installer.py``. Optionally you can pass a number of arguments
  to specify locations of various files. Please see the top of
  ``build-installer.py`` for its usage.

  Running this script takes some time, it will not only build Python itself
  but also some 3th-party libraries that are needed for extensions.

* When done the script will tell you where the DMG image is (by default
  somewhere in ``/tmp/_py``).

Building other universal installers
...................................

It is also possible to build a 4-way universal installer that runs on 
OS X 10.5 Leopard or later::

    /usr/bin/python /build-installer.py \
        --dep-target=10.5
        --universal-archs=all
        --sdk-path=/Developer/SDKs/MacOSX10.5.sdk

This requires that the deployment target is 10.5, and hence
also that you are building on at least OS X 10.5.  4-way includes
``i386``, ``x86_64``, ``ppc``, and ``ppc64`` (G5).  ``ppc64`` executable
variants can only be run on G5 machines running 10.5.  Note that,
while OS X 10.6 is only supported on Intel-based machines, it is possible
to run ``ppc`` (32-bit) executables unmodified thanks to the Rosetta ppc
emulation in OS X 10.5 and 10.6.  The 4-way installer variant must be
built with Xcode 3.  It is not regularly built or tested.

Other ``--universal-archs`` options are ``64-bit`` (``x86_64``, ``ppc64``),
and ``3-way`` (``ppc``, ``i386``, ``x86_64``).  None of these options
are regularly exercised; use at your own risk.


Testing
-------

Ideally, the resulting binaries should be installed and the test suite run
on all supported OS X releases and architectures.  As a practical matter,
that is generally not possible.  At a minimum, variant 1 should be run on
a PPC G4 system with OS X 10.5 and at least one Intel system running OS X
10.9, 10.8, 10.7, 10.6, or 10.5.  Variant 2 should be run on 10.9, 10.8,
10.7, and 10.6 systems in both 32-bit and 64-bit modes.::

    /usr/local/bin/pythonn.n -m test -w -u all,-largefile
    /usr/local/bin/pythonn.n-32 -m test -w -u all
    
Certain tests will be skipped and some cause the interpreter to fail
which will likely generate ``Python quit unexpectedly`` alert messages
to be generated at several points during a test run.  These are normal
during testing and can be ignored.

It is also recommend to launch IDLE and verify that it is at least
functional.  Double-click on the IDLE app icon in ``/Applications/Python n.n``.
It should also be tested from the command line::

    /usr/local/bin/idlen.n


This package will install Python $FULL_VERSION for Mac OS X
$MACOSX_DEPLOYMENT_TARGET for the following architecture(s):
$ARCHITECTURES.

               **** IMPORTANT ****

Installing on OS X 10.8 (Mountain Lion) or later systems
========================================================

If you are attempting to install on an OS X 10.8+ system, you may
see a message that Python can't be installed because it is from an
unidentified developer.  This is because this Python installer
package is not yet compatible with the Gatekeeper security feature
introduced in OS X 10.8.  To allow Python to be installed, you
can override the Gatekeeper policy for this install.  In the Finder,
instead of double-clicking, control-click or right click the "Python"
installer package icon.  Then select "Open using ... Installer" from
the contextual menu that appears.

               **** IMPORTANT ****

Update your version of Tcl/Tk to use IDLE or other Tk applications
==================================================================

To use IDLE or other programs that use the Tkinter graphical user
interface toolkit, you may need to install a newer third-party version
of the Tcl/Tk frameworks.  Visit http://www.python.org/download/mac/tcltk/
for current information about supported and recommended versions of
Tcl/Tk for this version of Python and of Mac OS X.

              **NEW* As of Python 3.4.0b1:

New Installation Options and Defaults
=====================================

The Python installer now includes an option to automatically install
or upgrade pip, a tool for installing and managing Python packages.
This option is enabled by default and no Internet access is required.
If you do not want the installer to do this, select the "Customize"
option at the "Installation Type" step and uncheck the "Install or
ugprade pip" option.

To make it easier to use scripts installed by third-party Python
packages, with pip or by other means, the "Shell profile updater"
option is now enabled by default, as has been the case with Python
2.7.x installers. You can also turn this option off by selecting
"Customize" and unchecking the "Shell profile updater" option. You
can also update your shell profile later by launching the "Update
Shell Profile" command found in the /Applications/Python $VERSION
folder.  You may need to start a new terminal window for the
changes to take effect.

Python.org Python $VERSION and 2.7.x versions can both be installed and
will not conflict. Command names for Python 3 contain a 3 in them,
python3 (or python$VERSION), idle3 (or idle$VERSION), pip3 (or pip$VERSION), etc.
Python 2.7 command names contain a 2 or no digit: python2 (or
python2.7 or python), idle2 (or idle2.7 or idle), etc. If you want to
use pip with Python 2.7.x, you will need to download and install a
separate copy of it from the Python Package Index
(https://pypi.python.org/pypi).

Using this version of Python on OS X
====================================

Python consists of the Python programming language interpreter, plus
a set of programs to allow easy access to it for Mac users including
an integrated development environment, IDLE, plus a set of pre-built
extension modules that open up specific Macintosh technologies to
Python programs.

The installer puts applications, an "Update Shell Profile" command,
and a link to the optionally installed Python Documentation into the
"Python $VERSION" subfolder of the system Applications folder,
and puts the underlying machinery into the folder
$PYTHONFRAMEWORKINSTALLDIR. It can
optionally place links to the command-line tools in /usr/local/bin as
well. Double-click on the "Update Shell Profile" command to add the
"bin" directory inside the framework to your shell's search path.

You must install onto your current boot disk, even though the
installer may not enforce this, otherwise things will not work.

You can verify the integrity of the disk image file containing the
installer package and this ReadMe file by comparing its md5 checksum
and size with the values published on the release page linked at
http://www.python.org/download/

Installation requires approximately $INSTALL_SIZE MB of disk space,
ignore the message that it will take zero bytes.

More information on Python in general can be found at
http://www.python.org.

The icons for use on MacOS X were created by Jacob Rus <jrus@fas.harvard.edu>
with some feedback from the folks on pythonmac-sig@python.org.


=========================
Python on Mac OS X README
=========================

:Authors:
    Jack Jansen (2004-07),
    Ronald Oussoren (2010-04),
    Ned Deily (2012-06)

:Version: 3.4.0

This document provides a quick overview of some Mac OS X specific features in
the Python distribution.

OS X specific arguments to configure
====================================

* ``--enable-framework[=DIR]``

  If this argument is specified the build will create a Python.framework rather
  than a traditional Unix install. See the section
  _`Building and using a framework-based Python on Mac OS X` for more 
  information on frameworks.

  If the optional directory argument is specified the framework is installed
  into that directory. This can be used to install a python framework into
  your home directory::

     $ ./configure --enable-framework=/Users/ronald/Library/Frameworks
     $ make && make install

  This will install the framework itself in ``/Users/ronald/Library/Frameworks``,
  the applications in a subdirectory of ``/Users/ronald/Applications`` and the
  command-line tools in ``/Users/ronald/bin``.

* ``--with-framework-name=NAME``

  Specify the name for the python framework, defaults to ``Python``. This option
  is only valid when ``--enable-framework`` is specified.

* ``--enable-universalsdk[=PATH]``

  Create a universal binary build of Python. This can be used with both
  regular and framework builds.

  The optional argument specifies which OS X SDK should be used to perform the
  build.  If xcodebuild is available and configured, this defaults to
  the Xcode default MacOS X SDK, otherwise ``/Developer/SDKs/MacOSX.10.4u.sdk``
  if available or ``/`` if not.

  See the section _`Building and using a universal binary of Python on Mac OS X`
  for more information.

* ``--with-univeral-archs=VALUE``

  Specify the kind of universal binary that should be created. This option is 
  only valid when ``--enable-universalsdk`` is specified.  The default is
  ``32-bit`` if a building with a SDK that supports PPC, otherwise defaults
  to ``intel``.


Building and using a universal binary of Python on Mac OS X
===========================================================

1. What is a universal binary
-----------------------------

A universal binary build of Python contains object code for more than one
CPU architecture.  A universal OS X executable file or library combines the
architecture-specific code into one file and can therefore run at native
speed on all supported architectures.  Universal files were introduced in
OS X 10.4 to add support for Intel-based Macs to the existing PowerPC (PPC)
machines.  In OS X 10.5 support was extended to 64-bit Intel and 64-bit PPC
architectures.  It is possible to build Python with various combinations
of architectures depending on the build tools and OS X version in use.

2. How do I build a universal binary
------------------------------------

You can enable universal binaries by specifying the "--enable-universalsdk"
flag to configure::

  $ ./configure --enable-universalsdk
  $ make
  $ make install

This flag can be used with a framework build of python, but also with a classic
unix build. Universal builds were first supported with OS X 10.4 with Xcode 2.1
and the 10.4u SDK.  Starting with Xcode 3 and OS X 10.5, more configurations are
available.

In general, universal builds depend on specific features provided by the
Apple-supplied compilers and other build tools included in Apple's Xcode
development tools.  You should install Xcode and the command line tools
component appropriate for the OS X release you are running on.  See the
Python Developer's Guide (http://docs.python.org/devguide/setup.html)
for more information.

2.1 Flavors of universal binaries
.................................

It is possible to build a number of flavors of the universal binary build,
the default is a 32-bit only binary (i386 and ppc) in build environments that
support ppc (10.4 with Xcode 2, 10.5 and 10.6 with Xcode 3) or an
Intel-32/-64-bit binary (i386 and X86_64) in build environments that do not
support ppc (Xcode 4 on 10.6 and later systems).  The flavor can be specified
using the configure option ``--with-universal-archs=VALUE``. The following
values are available:

  * ``intel``:	  ``i386``, ``x86_64``

  * ``intel-32``: ``i386``

  * ``32-bit``:   ``ppc``, ``i386``

  * ``3-way``:	  ``i386``, ``x86_64``, ``ppc``

  * ``64-bit``:   ``ppc64``, ``x86_64``

  * ``all``:      ``ppc``, ``ppc64``, ``i386``, ``x86_64``

To build a universal binary that includes a 64-bit architecture, you must build
on a system running OS X 10.5 or later.  The ``all`` and ``64-bit`` flavors can
only be built with an 10.5 SDK because ``ppc64`` support was only included with
OS X 10.5.  Although legacy ``ppc`` support was included with Xcode 3 on OS X
10.6, it was removed in Xcode 4, versions of which were released on OS X 10.6
and which is the standard for OS X 10.7.  To summarize, the
following combinations of SDKs and universal-archs flavors are available:

  * 10.4u SDK with Xcode 2 supports ``32-bit`` only

  * 10.5 SDK with Xcode 3.1.x supports all flavors

  * 10.6 SDK with Xcode 3.2.x supports ``intel``, ``3-way``, and ``32-bit``

  * 10.6 SDK with Xcode 4 supports ``intel`` only

  * 10.7 and 10.8 SDKs with Xcode 4 support ``intel`` only

  * 10.8 and 10.9 SDKs with Xcode 5 support ``intel`` only

The makefile for a framework build will also install ``python3.4-32``
binaries when the universal architecture includes at least one 32-bit
architecture (that is, for all flavors but ``64-bit``).

Running a specific architecture
...............................

You can run code using a specific architecture using the ``arch`` command::

   $ arch -i386 python

Or to explicitly run in 32-bit mode, regardless of the machine hardware::

   $ arch -i386 -ppc python

NOTE: When you're using a framework install of Python this requires at least
Python 2.7 or 3.2, in earlier versions the python (and pythonw) commands are
wrapper tools that execute the real interpreter without ensuring that the
real interpreter runs with the same architecture.

Using ``arch`` is not a perfect solution as the selected architecture will
not automatically carry through to subprocesses launched by programs and tests
under that Python.  If you want to ensure that Python interpreters launched in
subprocesses also run in 32-bit-mode if the main interpreter does, use
a ``python3.4-32`` binary and use the value of ``sys.executable`` as the
``subprocess`` ``Popen`` executable value.

Building and using a framework-based Python on Mac OS X.
========================================================


1. Why would I want a framework Python instead of a normal static Python?
--------------------------------------------------------------------------

The main reason is because you want to create GUI programs in Python. With the
exception of X11/XDarwin-based GUI toolkits all GUI programs need to be run 
from a Mac OS X application bundle (".app").

While it is technically possible to create a .app without using frameworks you
will have to do the work yourself if you really want this.

A second reason for using frameworks is that they put Python-related items in
only two places: "/Library/Framework/Python.framework" and 
"/Applications/Python <VERSION>" where ``<VERSION>`` can be e.g. "3.4",
"2.7", etc.  This simplifies matters for users installing
Python from a binary distribution if they want to get rid of it again. Moreover,
due to the way frameworks work, a user without admin privileges can install a
binary distribution in his or her home directory without recompilation.

2. How does a framework Python differ from a normal static Python?
------------------------------------------------------------------

In everyday use there is no difference, except that things are stored in
a different place. If you look in /Library/Frameworks/Python.framework
you will see lots of relative symlinks, see the Apple documentation for
details. If you are used to a normal unix Python file layout go down to
Versions/Current and you will see the familiar bin and lib directories.

3. Do I need extra packages?
----------------------------

Yes, probably.  If you want Tkinter support you need to get the OS X AquaTk
distribution, this is installed by default on Mac OS X 10.4 or later.  Be
aware, though, that the Cocoa-based AquaTk's supplied starting with OS X
10.6 have proven to be unstable.  If possible, you should consider
installing a newer version before building on OS X 10.6 or later, such as
the ActiveTcl 8.5.  See http://www.python.org/download/mac/tcltk/.  If you
are building with an SDK, ensure that the newer Tcl and Tk frameworks are
seen in the SDK's ``Library/Frameworks`` directory; you may need to
manually create symlinks to their installed location, ``/Library/Frameworks``.
If you want wxPython you need to get that.
If you want Cocoa you need to get PyObjC.

4. How do I build a framework Python?
-------------------------------------

This directory contains a Makefile that will create a couple of python-related
applications (full-blown OS X .app applications, that is) in
"/Applications/Python <VERSION>", and a hidden helper application Python.app
inside the Python.framework, and unix tools including "python" into
/usr/local/bin.  In addition it has a target "installmacsubtree" that installs
the relevant portions of the Mac subtree into the Python.framework.

It is normally invoked indirectly through the main Makefile, as the last step
in the sequence

 1. ./configure --enable-framework

 2. make
 
 3. make install

This sequence will put the framework in ``/Library/Framework/Python.framework``,
the applications in ``/Applications/Python <VERSION>`` and the unix tools in 
``/usr/local/bin``.

Installing in another place, for instance ``$HOME/Library/Frameworks`` if you
have no admin privileges on your machine, is possible. This can be accomplished
by configuring with ``--enable-framework=$HOME/Library/Frameworks``.
The other two directories will then also be installed in your home directory,
at ``$HOME/Applications/Python-<VERSION>`` and ``$HOME/bin``.

If you want to install some part, but not all, read the main Makefile. The
frameworkinstall is composed of a couple of sub-targets that install the
framework itself, the Mac subtree, the applications and the unix tools.

There is an extra target frameworkinstallextras that is not part of the
normal frameworkinstall which installs the Tools directory into
"/Applications/Python <VERSION>", this is useful for binary
distributions.

What do all these programs do?
===============================

"IDLE.app" is an integrated development environment for Python: editor,
debugger, etc.

"Python Launcher.app" is a helper application that will handle things when you
double-click a .py, .pyc or .pyw file. For the first two it creates a Terminal
window and runs the scripts with the normal command-line Python. For the
latter it runs the script in the Python.app interpreter so the script can do
GUI-things. Keep the ``Option`` key depressed while dragging or double-clicking
a script to set runtime options. These options can be set persistently
through Python Launcher's preferences dialog.

The program ``pythonx.x`` runs python scripts from the command line.
Previously, various compatibility aliases were also installed, including
``pythonwx.x`` which in early releases of Python on OS X was required to run
GUI programs.  As of 3.4.0, the ``pythonwx.x`` aliases are no longer installed.

How do I create a binary distribution?
======================================

Download and unpack the source release from http://www.python.org/download/.
Go to the directory ``Mac/BuildScript``. There you will find a script
``build-installer.py`` that does all the work. This will download and build
a number of 3rd-party libaries, configures and builds a framework Python,
installs it, creates the installer package files and then packs this in a
DMG image.  The script also builds an HTML copy of the current Python
documentation set for this release for inclusion in the framework.  The
installer package will create links to the documentation for use by IDLE,
pydoc, shell users, and Finder user.

The script will build a universal binary so you'll therefore have to run this
script on Mac OS X 10.4 or later and with Xcode 2.1 or later installed.
However, the Python build process itself has several build dependencies not
available out of the box with OS X 10.4 so you may have to install
additional software beyond what is provided with Xcode 2.  OS X 10.5
provides a recent enough system Python (in ``/usr/bin``) to build
the Python documentation set.  It should be possible to use SDKs and/or older
versions of Xcode to build installers that are compatible with older systems
on a newer system but this may not be completely foolproof so the resulting
executables, shared libraries, and ``.so`` bundles should be carefully
examined and tested on all supported systems for proper dynamic linking
dependencies.  It is safest to build the distribution on a system running the
minimum OS X version supported.

All of this is normally done completely isolated in /tmp/_py, so it does not
use your normal build directory nor does it install into /.

Because of the way the script locates the files it needs you have to run it
from within the BuildScript directory. The script accepts a number of 
command-line arguments, run it with --help for more information.

Configure warnings
==================

The configure script sometimes emits warnings like the one below::

   configure: WARNING: libintl.h: present but cannot be compiled
   configure: WARNING: libintl.h:     check for missing prerequisite headers?
   configure: WARNING: libintl.h: see the Autoconf documentation
   configure: WARNING: libintl.h:     section "Present But Cannot Be Compiled"
   configure: WARNING: libintl.h: proceeding with the preprocessor's result
   configure: WARNING: libintl.h: in the future, the compiler will take precedence
   configure: WARNING:     ## -------------------------------------- ##
   configure: WARNING:     ## Report this to http://bugs.python.org/ ##
   configure: WARNING:     ## -------------------------------------- ##

This almost always means you are trying to build a universal binary for
Python and have libraries in ``/usr/local`` that don't contain the required
architectures. Temporarily move ``/usr/local`` aside to finish the build.


Uninstalling a framework install, including the binary installer
================================================================

Uninstalling a framework can be done by manually removing all bits that got installed.
That's true for both installations from source and installations using the binary installer.
OS X does not provide a central uninstaller.

The main bit of a framework install is the framework itself, installed in
``/Library/Frameworks/Python.framework``. This can contain multiple versions
of Python, if you want to remove just one version you have to remove the
version-specific subdirectory: ``/Library/Frameworks/Python.framework/Versions/X.Y``.
If you do that, ensure that ``/Library/Frameworks/Python.framework/Versions/Current``
is a symlink that points to an installed version of Python.

A framework install also installs some applications in ``/Applications/Python X.Y``,

And lastly a framework installation installs files in ``/usr/local/bin``, all of
them symbolic links to files in ``/Library/Frameworks/Python.framework/Versions/X.Y/bin``.


Resources
=========

  *  http://www.python.org/download/mac/

  *  http://www.python.org/community/sigs/current/pythonmac-sig/

  *  http://docs.python.org/devguide/
Python Misc subdirectory
========================

This directory contains files that wouldn't fit in elsewhere.  Some
documents are only of historic importance.

Files found here
----------------

ACKS                    Acknowledgements
gdbinit                 Handy stuff to put in your .gdbinit file, if you use gdb
HISTORY                 News from previous releases -- oldest last
indent.pro              GNU indent profile approximating my C style
NEWS                    News for this release (for some meaning of "this")
Porting                 Mini-FAQ on porting to new platforms
python-config.in        Python script template for python-config
python.man              UNIX man page for the python interpreter
python.pc.in            Package configuration info template for pkg-config
python-wing*.wpr        Wing IDE project file
README                  The file you're reading now
README.AIX              Information about using Python on AIX
README.coverity         Information about running Coverity's Prevent on Python
README.valgrind         Information for Valgrind users, see valgrind-python.supp
RPM                     (Old) tools to build RPMs
SpecialBuilds.txt       Describes extra symbols you can set for debug builds
svnmap.txt              Map of old SVN revs and branches to hg changeset ids
valgrind-python.supp    Valgrind suppression file, see README.valgrind
vgrindefs               Python configuration for vgrind (a generic pretty printer)


This documentation tries to help people who intend to use Python on
AIX.

There used to be many issues with Python on AIX, but the major ones
have been corrected for version 3.2, so that Python should now work
rather well on this platform. The remaining known issues are listed in
this document.


======================================================================
			   Compiling Python
----------------------------------------------------------------------

You can compile Python with gcc or the native AIX compiler. The native
compiler used to give better performances on this system with older
versions of Python.  With Python 3.2 it may not be the case anymore,
as this compiler does not allow compiling Python with computed gotos.
Some benchmarks need to be done.

Compiling with gcc:

cd Python-3.2
CC=gcc OPT="-O2" ./configure --enable-shared
make

There are various aliases for the native compiler.  The recommended
alias for compiling Python is 'xlc_r', which provides a better level of
compatibility and handles thread initialization properly.

It is a good idea to add the '-qmaxmem=70000' option, otherwise the
compiler considers various files too complex to optimize.

Compiling with xlc:

cd Python-3.2
CC=xlc_r OPT="-O2 -qmaxmem=70000" ./configure --without-computed-gotos --enable-shared
make

Note:
On AIX 5.3 and earlier, you will also need to specify the
"--disable-ipv6" flag to configure. This has been corrected in AIX
6.1.


======================================================================
			  Memory Limitations
----------------------------------------------------------------------

Note: this section may not apply when compiling Python as a 64 bit
application.

By default on AIX each program gets one segment register for its data
segment. As each segment register covers 256 MB, a Python program that
would use more than 256MB will raise a MemoryError.  The standard
Python test suite is one such application.

To allocate more segment registers to Python, you must use the linker
option -bmaxdata or the ldedit tool to specify the number of bytes you
need in the data segment.

For example, if you want to allow 512MB of memory for Python (this is
enough for the test suite to run without MemoryErrors), you should run
the following command at the end of compilation:

ldedit -b maxdata:0x20000000 ./python

You can allow up to 2GB of memory for Python by using the value
0x80000000 for maxdata.

It is also possible to go beyond 2GB of memory by activating Large
Page Use. You should consult the IBM documentation if you need to use
this option. You can also follow the discussion of this problem
in issue 11212 at bugs.python.org.

http://publib.boulder.ibm.com/infocenter/aix/v6r1/index.jsp?topic=/com.ibm.aix.cmds/doc/aixcmds3/ldedit.htm


======================================================================
			     Known issues
----------------------------------------------------------------------

Those issues are currently affecting Python on AIX:

* Python has not been fully tested on AIX when compiled as a 64 bit
  application.

* issue 3526: the memory used by a Python process will never be
  released to the system. If you have a Python application on AIX that
  uses a lot of memory, you should read this issue and you may
  consider using the provided patch that implements a custom malloc
  implementation

* issue 11184: support for large files is currently broken

* issue 11185: os.wait4 does not behave correctly with option WNOHANG

* issue 1745108: there may be some problems with curses.panel

* issue 11192: test_socket fails

* issue 11190: test_locale fails

* issue 11193: test_subprocess fails

* issue 9920: minor arithmetic issues in cmath

* issue 11215: test_fileio fails

* issue 11188: test_time fails


======================================================================
		Implementation details for developers
----------------------------------------------------------------------

Python and python modules can now be built as shared libraries on AIX
as usual.

AIX shared libraries require that an "export" and "import" file be
provided at compile time to list all extern symbols which may be
shared between modules.  The "export" file (named python.exp) for the
modules and the libraries that belong to the Python core is created by
the "makexp_aix" script before performing the link of the python
binary. It lists all global symbols (exported during the link) of the
modules and the libraries that make up the python executable.

When shared library modules (.so files) are made, a second shell
script is invoked.  This script is named "ld_so_aix" and is also
provided with the distribution in the Modules subdirectory.  This
script acts as an "ld" wrapper which hides the explicit management of
"export" and "import" files; it adds the appropriate arguments (in the
appropriate order) to the link command that creates the shared module.
Among other things, it specifies that the "python.exp" file is an
"import" file for the shared module.

This mechanism should be transparent.


Coverity has a static analysis tool (Prevent) which is similar to Klocwork.
They run their tool on the Python source code (SVN head) on a daily basis.
The results are available at:

     http://scan.coverity.com/

About 20 people have access to the analysis reports.  Other
people can be added by request.

Prevent was first run on the Python 2.5 source code in March 2006.
There were originally about 100 defects reported.  Some of these
were false positives.  Over 70 issues were uncovered.

Each warning has a unique id and comments that can be made on it.
When checking in changes due to a warning, the unique id
as reported by the tool was added to the SVN commit message.

False positives were annotated so that the comments can
be reviewed and reversed if the analysis was incorrect.

Contact python-dev@python.org for more information.

This document describes some caveats about the use of Valgrind with
Python.  Valgrind is used periodically by Python developers to try
to ensure there are no memory leaks or invalid memory reads/writes.

If you don't want to read about the details of using Valgrind, there
are still two things you must do to suppress the warnings.  First,
you must use a suppressions file.  One is supplied in
Misc/valgrind-python.supp.  Second, you must do one of the following:

  * Uncomment Py_USING_MEMORY_DEBUGGER in Objects/obmalloc.c,
    then rebuild Python
  * Uncomment the lines in Misc/valgrind-python.supp that
    suppress the warnings for PyObject_Free and PyObject_Realloc

If you want to use Valgrind more effectively and catch even more
memory leaks, you will need to configure python --without-pymalloc.
PyMalloc allocates a few blocks in big chunks and most object
allocations don't call malloc, they use chunks doled about by PyMalloc
from the big blocks.  This means Valgrind can't detect
many allocations (and frees), except for those that are forwarded
to the system malloc.  Note: configuring python --without-pymalloc
makes Python run much slower, especially when running under Valgrind.
You may need to run the tests in batches under Valgrind to keep
the memory usage down to allow the tests to complete.  It seems to take
about 5 times longer to run --without-pymalloc.

Apr 15, 2006:
  test_ctypes causes Valgrind 3.1.1 to fail (crash).
  test_socket_ssl should be skipped when running valgrind.
	The reason is that it purposely uses uninitialized memory.
	This causes many spurious warnings, so it's easier to just skip it.


Details:
--------
Python uses its own small-object allocation scheme on top of malloc,
called PyMalloc.

Valgrind may show some unexpected results when PyMalloc is used.
Starting with Python 2.3, PyMalloc is used by default.  You can disable
PyMalloc when configuring python by adding the --without-pymalloc option.
If you disable PyMalloc, most of the information in this document and
the supplied suppressions file will not be useful.  As discussed above,
disabling PyMalloc can catch more problems.

If you use valgrind on a default build of Python,  you will see
many errors like:

        ==6399== Use of uninitialised value of size 4
        ==6399== at 0x4A9BDE7E: PyObject_Free (obmalloc.c:711)
        ==6399== by 0x4A9B8198: dictresize (dictobject.c:477)

These are expected and not a problem.  Tim Peters explains
the situation:

        PyMalloc needs to know whether an arbitrary address is one
	that's managed by it, or is managed by the system malloc.
	The current scheme allows this to be determined in constant
	time, regardless of how many memory areas are under pymalloc's
	control.

        The memory pymalloc manages itself is in one or more "arenas",
	each a large contiguous memory area obtained from malloc.
	The base address of each arena is saved by pymalloc
	in a vector.  Each arena is carved into "pools", and a field at
	the start of each pool contains the index of that pool's arena's
	base address in that vector.

        Given an arbitrary address, pymalloc computes the pool base
	address corresponding to it, then looks at "the index" stored
	near there.  If the index read up is out of bounds for the
	vector of arena base addresses pymalloc maintains, then
	pymalloc knows for certain that this address is not under
	pymalloc's control.  Otherwise the index is in bounds, and
	pymalloc compares

            the arena base address stored at that index in the vector

        to

            the arbitrary address pymalloc is investigating

        pymalloc controls this arbitrary address if and only if it lies
        in the arena the address's pool's index claims it lies in.

        It doesn't matter whether the memory pymalloc reads up ("the
	index") is initialized.  If it's not initialized, then
	whatever trash gets read up will lead pymalloc to conclude
	(correctly) that the address isn't controlled by it, either
	because the index is out of bounds, or the index is in bounds
	but the arena it represents doesn't contain the address.

        This determination has to be made on every call to one of
	pymalloc's free/realloc entry points, so its speed is critical
	(Python allocates and frees dynamic memory at a ferocious rate
	-- everything in Python, from integers to "stack frames",
	lives in the heap).

This directory contains support file used to build RPM releases of
Python.  Its contents are maintained by Sean Reifschneider
<jafo@tummy.com>.

If you wish to build RPMs from the base Python release tar-file, note
that you will have to download the
"doc/<version>/html-<version>.tar.bz2"
file from python.org and place it into your "SOURCES" directory for
the build to complete.  This is the same directory that you place the
Python-2.3.1 release tar-file in.  You can then use the ".spec" file in
this directory to build RPMs.

You may also wish to pursue RPMs provided by distribution makers to see if
they have one suitable for your uses.  If, for example, you just want a
slightly newer version of Python than what the distro provides, you could
pick up the closest SRPM your distro provides, and then modify it to
the newer version, and build that.  It may be as simple as just changing
the "version" information in the spec file (or it may require fixing
patches).

NOTE: I am *NOT* recommending just using the binary RPM, and never do an
install with "--force" or "--nodeps".

Also worth pursuing may be newer versions provided by similar distros.  For
example, a Python 3 SRPM from Fedora may be a good baseline to try building
on CentOS.

Many newer SRPMs won't install on older distros because of format changes.
You can manually extract these SRPMS with:

   mkdir foo
   cd foo
   rpm2cpio <../python3-*.src.rpm | cpio -ivd

To generate or modify mapping headers
-------------------------------------
Mapping headers are imported from CJKCodecs as pre-generated form.
If you need to tweak or add something on it, please look at tools/
subdirectory of CJKCodecs' distribution.



Notes on implmentation characteristics of each codecs
-----------------------------------------------------

1) Big5 codec

  The big5 codec maps the following characters as cp950 does rather
  than conforming Unicode.org's that maps to 0xFFFD.

    BIG5        Unicode     Description

    0xA15A      0x2574      SPACING UNDERSCORE
    0xA1C3      0xFFE3      SPACING HEAVY OVERSCORE
    0xA1C5      0x02CD      SPACING HEAVY UNDERSCORE
    0xA1FE      0xFF0F      LT DIAG UP RIGHT TO LOW LEFT
    0xA240      0xFF3C      LT DIAG UP LEFT TO LOW RIGHT
    0xA2CC      0x5341      HANGZHOU NUMERAL TEN
    0xA2CE      0x5345      HANGZHOU NUMERAL THIRTY

  Because unicode 0x5341, 0x5345, 0xFF0F, 0xFF3C is mapped to another
  big5 codes already, a roundtrip compatibility is not guaranteed for
  them.


2) cp932 codec

  To conform to Windows's real mapping, cp932 codec maps the following
  codepoints in addition of the official cp932 mapping.

    CP932     Unicode     Description

    0x80      0x80        UNDEFINED
    0xA0      0xF8F0      UNDEFINED
    0xFD      0xF8F1      UNDEFINED
    0xFE      0xF8F2      UNDEFINED
    0xFF      0xF8F3      UNDEFINED


3) euc-jisx0213 codec

  The euc-jisx0213 codec maps JIS X 0213 Plane 1 code 0x2140 into
  unicode U+FF3C instead of U+005C as on unicode.org's mapping.
  Because euc-jisx0213 has REVERSE SOLIDUS on 0x5c already and A140
  is shown as a full width character, mapping to U+FF3C can make
  more sense.

  The euc-jisx0213 codec is enabled to decode JIS X 0212 codes on
  codeset 2. Because JIS X 0212 and JIS X 0213 Plane 2 don't have
  overlapped by each other, it doesn't bother standard conformations
  (and JIS X 0213 Plane 2 is intended to use so.) On encoding
  sessions, the codec will try to encode kanji characters in this
  order:

    JIS X 0213 Plane 1 -> JIS X 0213 Plane 2 -> JIS X 0212


4) euc-jp codec

  The euc-jp codec is a compatibility instance on these points:
   - U+FF3C FULLWIDTH REVERSE SOLIDUS is mapped to EUC-JP A1C0 (vice versa)
   - U+00A5 YEN SIGN is mapped to EUC-JP 0x5c. (one way)
   - U+203E OVERLINE is mapped to EUC-JP 0x7e. (one way)


5) shift-jis codec

  The shift-jis codec is mapping 0x20-0x7e area to U+20-U+7E directly
  instead of using JIS X 0201 for compatibility. The differences are:
   - U+005C REVERSE SOLIDUS is mapped to SHIFT-JIS 0x5c.
   - U+007E TILDE is mapped to SHIFT-JIS 0x7e.
   - U+FF3C FULL-WIDTH REVERSE SOLIDUS is mapped to SHIFT-JIS 815f.


ZLIB DATA COMPRESSION LIBRARY

zlib 1.2.8 is a general purpose data compression library.  All the code is
thread safe.  The data format used by the zlib library is described by RFCs
(Request for Comments) 1950 to 1952 in the files
http://tools.ietf.org/html/rfc1950 (zlib format), rfc1951 (deflate format) and
rfc1952 (gzip format).

All functions of the compression library are documented in the file zlib.h
(volunteer to write man pages welcome, contact zlib@gzip.org).  A usage example
of the library is given in the file test/example.c which also tests that
the library is working correctly.  Another example is given in the file
test/minigzip.c.  The compression library itself is composed of all source
files in the root directory.

To compile all files and run the test program, follow the instructions given at
the top of Makefile.in.  In short "./configure; make test", and if that goes
well, "make install" should work for most flavors of Unix.  For Windows, use
one of the special makefiles in win32/ or contrib/vstudio/ .  For VMS, use
make_vms.com.

Questions about zlib should be sent to <zlib@gzip.org>, or to Gilles Vollant
<info@winimage.com> for the Windows DLL version.  The zlib home page is
http://zlib.net/ .  Before reporting a problem, please check this site to
verify that you have the latest version of zlib; otherwise get the latest
version and check whether the problem still exists or not.

PLEASE read the zlib FAQ http://zlib.net/zlib_faq.html before asking for help.

Mark Nelson <markn@ieee.org> wrote an article about zlib for the Jan.  1997
issue of Dr.  Dobb's Journal; a copy of the article is available at
http://marknelson.us/1997/01/01/zlib-engine/ .

The changes made in version 1.2.8 are documented in the file ChangeLog.

Unsupported third party contributions are provided in directory contrib/ .

zlib is available in Java using the java.util.zip package, documented at
http://java.sun.com/developer/technicalArticles/Programming/compression/ .

A Perl interface to zlib written by Paul Marquess <pmqs@cpan.org> is available
at CPAN (Comprehensive Perl Archive Network) sites, including
http://search.cpan.org/~pmqs/IO-Compress-Zlib/ .

A Python interface to zlib written by A.M. Kuchling <amk@amk.ca> is
available in Python 1.5 and later versions, see
http://docs.python.org/library/zlib.html .

zlib is built into tcl: http://wiki.tcl.tk/4610 .

An experimental package to read and write files in .zip format, written on top
of zlib by Gilles Vollant <info@winimage.com>, is available in the
contrib/minizip directory of zlib.


Notes for some targets:

- For Windows DLL versions, please see win32/DLL_FAQ.txt

- For 64-bit Irix, deflate.c must be compiled without any optimization. With
  -O, one libpng test fails. The test works in 32 bit mode (with the -n32
  compiler flag). The compiler bug has been reported to SGI.

- zlib doesn't work with gcc 2.6.3 on a DEC 3000/300LX under OSF/1 2.1 it works
  when compiled with cc.

- On Digital Unix 4.0D (formely OSF/1) on AlphaServer, the cc option -std1 is
  necessary to get gzprintf working correctly. This is done by configure.

- zlib doesn't work on HP-UX 9.05 with some versions of /bin/cc. It works with
  other compilers. Use "make test" to check your compiler.

- gzdopen is not supported on RISCOS or BEOS.

- For PalmOs, see http://palmzlib.sourceforge.net/


Acknowledgments:

  The deflate format used by zlib was defined by Phil Katz.  The deflate and
  zlib specifications were written by L.  Peter Deutsch.  Thanks to all the
  people who reported problems and suggested various improvements in zlib; they
  are too numerous to cite here.

Copyright notice:

 (C) 1995-2013 Jean-loup Gailly and Mark Adler

  This software is provided 'as-is', without any express or implied
  warranty.  In no event will the authors be held liable for any damages
  arising from the use of this software.

  Permission is granted to anyone to use this software for any purpose,
  including commercial applications, and to alter it and redistribute it
  freely, subject to the following restrictions:

  1. The origin of this software must not be misrepresented; you must not
     claim that you wrote the original software. If you use this software
     in a product, an acknowledgment in the product documentation would be
     appreciated but is not required.
  2. Altered source versions must be plainly marked as such, and must not be
     misrepresented as being the original software.
  3. This notice may not be removed or altered from any source distribution.

  Jean-loup Gailly        Mark Adler
  jloup@gzip.org          madler@alumni.caltech.edu

If you use the zlib library in a product, we would appreciate *not* receiving
lengthy legal documents to sign.  The sources are provided for free but without
warranty of any kind.  The library has been entirely written by Jean-loup
Gailly and Mark Adler; it does not include third-party code.

If you redistribute modified sources, we would appreciate that you include in
the file ChangeLog history information documenting your changes.  Please read
the FAQ for more information on the distribution of modified source versions.

dlcompat for Darwin
=========================

This is dlcompat, a small library that emulates the dlopen()
interface on top of Darwin's dyld API.

dlcompat allows loading a ".dylib" library (as long as the RTLD_LOCAL 
flag isn't passed to dlopen()). It can be configured to yield a warning 
when trying to close it (dynamic libraries cannot currently be unloaded).

It automatically searches for modules in several directories when no 
absolute path is specified and the module is not found in the current 
directory.

The paths searched are those specified in the environment variables
LD_LIBRARY_PATH and DYLD_LIBRARY_PATH plus /lib, /usr/local/lib and 
/usr/lib or the path specified in the environment variable 
DYLD_FALLBACK_LIBRARY_PATH.

In the default install the behavior of dlsym is to automatically prepend
an underscore to passed in symbol names, this allows easier porting of
applications which were written specifically for ELF based lifeforms.

Installation
--------------
Type:
	./configure
	make
	sudo make install

This will compile the source file, generate both a static and shared
library called libdl and install it into /usr/local/lib. The header
file dlfcn.h will be installed in /usr/local/include.

If you want to place the files somewhere else, run

  make clean
  ./configure --prefix=<prefix>
  make
  sudo make install

where <prefix> is the hierarchy you want to install into, e.g. /usr
for /usr/lib and /usr/include (_NOT_ recommended!).

To enable debugging output (useful for me), run

  make clean
  ./configure --enable-debug
  make
  sudo make install
  
If you want old dlcompat style behavior of not prepending the underscore
on calls to dlsym then type:

  make clean
  ./configure --enable-fink
  make
  sudo make install

Usage
-------
Software that uses GNU autoconf will likely check for a library called
libdl, that's why I named it that way. For software that doesn't find
the library on its own, you must add a '-ldl' to the appropriate
Makefile (or environment) variable, usually LIBS.

If you installed dlcompat into a directory other than /usr/local/lib,
you must tell the compiler where to find it. Add '-L<prefix>/lib' to
LDFLAGS (or CFLAGS) and '-I<prefix>/include' to CPPFLAGS (or CFLAGS).

Notes
-----
If you are writing new software and plan to have Mac OX X compatibility you
should look at the dyld api's in /usr/include/mach-o/dyld.h, rather than
using dlcompat, using the native api's is the supported method of loading
dynamically on Mac OS X, if you want an small example, look at dlfcn_simple.c,
which should help get you started.

Also note that the functions in dlcompat are not thread safe, and while it is not
POSIX spec compliant, it is about as close to compliance as it is going to get though.

You can always get the latest version from opendarwin cvs:

  cvs -d :pserver:anonymous@anoncvs.opendarwin.org:/cvs/od login
  cvs -z3 -d :pserver:anonymous@anoncvs.opendarwin.org:/cvs/od \
               co -d dlcompat proj/dlcompat


It is hoped that this library will be useful, and as bug free as possible, if you find
any bugs please let us know about them so they can be fixed.

Please send bug reports to Peter O'Gorman <ogorman@users.sourceforge.net>

Thanks.


The files in this directory are taken from
http://www.opendarwin.org/cgi-bin/cvsweb.cgi/~checkout~/proj/dlcompat/

The LICENSE in this directory applies to these files.

Thomas Heller, Jan 2003

These files have been modified so they fall back to the system
dlfcn calls if available in libSystem.

Bob Ippolito, Feb 2006

Status
======

libffi-3.0.13 was released on March 17, 2013.  Check the libffi web
page for updates: <URL:http://sourceware.org/libffi/>.


What is libffi?
===============

Compilers for high level languages generate code that follow certain
conventions. These conventions are necessary, in part, for separate
compilation to work. One such convention is the "calling
convention". The "calling convention" is essentially a set of
assumptions made by the compiler about where function arguments will
be found on entry to a function. A "calling convention" also specifies
where the return value for a function is found.

Some programs may not know at the time of compilation what arguments
are to be passed to a function. For instance, an interpreter may be
told at run-time about the number and types of arguments used to call
a given function. Libffi can be used in such programs to provide a
bridge from the interpreter program to compiled code.

The libffi library provides a portable, high level programming
interface to various calling conventions. This allows a programmer to
call any function specified by a call interface description at run
time.  

FFI stands for Foreign Function Interface.  A foreign function
interface is the popular name for the interface that allows code
written in one language to call code written in another language. The
libffi library really only provides the lowest, machine dependent
layer of a fully featured foreign function interface. A layer must
exist above libffi that handles type conversions for values passed
between the two languages.


Supported Platforms
===================

Libffi has been ported to many different platforms.
For specific configuration details and testing status, please
refer to the wiki page here:

 http://www.moxielogic.org/wiki/index.php?title=Libffi_3.0.13

At the time of release, the following basic configurations have been
tested:

|-----------------+------------------+-------------------------|
| Architecture    | Operating System | Compiler                |
|-----------------+------------------+-------------------------|
| AArch64         | Linux            | GCC                     |
| Alpha           | Linux            | GCC                     |
| Alpha           | Tru64            | GCC                     |
| ARM             | Linux            | GCC                     |
| ARM             | iOS              | GCC                     |
| AVR32           | Linux            | GCC                     |
| Blackfin        | uClinux          | GCC                     |
| HPPA            | HPUX             | GCC                     |
| IA-64           | Linux            | GCC                     |
| M68K            | FreeMiNT         | GCC                     |
| M68K            | Linux	     | GCC                     |
| M68K            | RTEMS            | GCC                     |
| Meta            | Linux            | GCC                     |
| MicroBlaze      | Linux            | GCC                     |
| MIPS            | IRIX             | GCC                     |
| MIPS            | Linux            | GCC                     |
| MIPS            | RTEMS            | GCC                     |
| MIPS64          | Linux            | GCC                     |
| Moxie		  | Bare metal	     | GCC
| PowerPC 32-bit  | AIX              | IBM XL C                |
| PowerPC 64-bit  | AIX              | IBM XL C                |
| PowerPC         | AMIGA            | GCC                     |
| PowerPC         | Linux            | GCC                     |
| PowerPC         | Mac OSX          | GCC                     |
| PowerPC         | FreeBSD          | GCC                     |
| PowerPC 64-bit  | FreeBSD          | GCC                     |
| PowerPC 64-bit  | Linux            | GCC                     |
| S390            | Linux            | GCC                     |
| S390X           | Linux            | GCC                     |
| SPARC           | Linux            | GCC                     |
| SPARC           | Solaris          | GCC                     |
| SPARC           | Solaris          | Oracle Solaris Studio C |
| SPARC64         | Linux            | GCC                     |
| SPARC64         | FreeBSD          | GCC                     |
| SPARC64         | Solaris          | Oracle Solaris Studio C |
| TILE-Gx/TILEPro | Linux            | GCC                     |
| X86             | FreeBSD          | GCC                     |
| X86             | GNU HURD         | GCC                     |
| X86             | Interix          | GCC                     |
| X86             | kFreeBSD         | GCC                     |
| X86             | Linux            | GCC                     |
| X86             | Mac OSX          | GCC                     |
| X86             | OpenBSD          | GCC                     |
| X86             | OS/2             | GCC                     |
| X86             | Solaris          | GCC                     |
| X86             | Solaris          | Oracle Solaris Studio C |
| X86             | Windows/Cygwin   | GCC                     |
| X86             | Windows/MingW    | GCC                     |
| X86-64          | FreeBSD          | GCC                     |
| X86-64          | Linux            | GCC                     |
| X86-64          | Linux/x32        | GCC                     |
| X86-64          | OpenBSD          | GCC                     |
| X86-64          | Solaris          | Oracle Solaris Studio C |
| X86-64          | Windows/MingW    | GCC                     |
| Xtensa          | Linux            | GCC                     |
|-----------------+------------------+-------------------------|

Please send additional platform test results to
libffi-discuss@sourceware.org and feel free to update the wiki page
above.

Installing libffi
=================

First you must configure the distribution for your particular
system. Go to the directory you wish to build libffi in and run the
"configure" program found in the root directory of the libffi source
distribution.

You may want to tell configure where to install the libffi library and
header files. To do that, use the --prefix configure switch.  Libffi
will install under /usr/local by default. 

If you want to enable extra run-time debugging checks use the the
--enable-debug configure switch. This is useful when your program dies
mysteriously while using libffi. 

Another useful configure switch is --enable-purify-safety. Using this
will add some extra code which will suppress certain warnings when you
are using Purify with libffi. Only use this switch when using 
Purify, as it will slow down the library.

It's also possible to build libffi on Windows platforms with
Microsoft's Visual C++ compiler.  In this case, use the msvcc.sh
wrapper script during configuration like so:

path/to/configure CC=path/to/msvcc.sh LD=link CPP=\"cl -nologo -EP\"

For 64-bit Windows builds, use CC="path/to/msvcc.sh -m64".
You may also need to specify --build appropriately. When building with MSVC
under a MingW environment, you may need to remove the line in configure
that sets 'fix_srcfile_path' to a 'cygpath' command. ('cygpath' is not
present in MingW, and is not required when using MingW-style paths.)

For iOS builds, the 'libffi.xcodeproj' Xcode project is available.

Configure has many other options. Use "configure --help" to see them all.

Once configure has finished, type "make". Note that you must be using
GNU make.  You can ftp GNU make from ftp.gnu.org:/pub/gnu/make .

To ensure that libffi is working as advertised, type "make check".
This will require that you have DejaGNU installed.

To install the library and header files, type "make install".


History
=======

See the ChangeLog files for details.

3.0.13 Mar-17-13
	Add Meta support.
	Add missing Moxie bits.
	Fix stack alignment bug on 32-bit x86.
	Build fix for m68000 targets.
	Build fix for soft-float Power targets.
	Fix the install dir location for some platforms when building
	  with GCC (OS X, Solaris).
	Fix Cygwin regression.

3.0.12 Feb-11-13
        Add Moxie support.
	Add AArch64 support.
	Add Blackfin support.
	Add TILE-Gx/TILEPro support.
	Add MicroBlaze support.
	Add Xtensa support.
	Add support for PaX enabled kernels with MPROTECT.
	Add support for native vendor compilers on
	  Solaris and AIX.
	Work around LLVM/GCC interoperability issue on x86_64.

3.0.11 Apr-11-12
        Lots of build fixes.
	Add Amiga newer MacOS support.
	Add support for variadic functions (ffi_prep_cif_var).
	Add Linux/x32 support.
	Add thiscall, fastcall and MSVC cdecl support on Windows.
	Add Amiga and newer MacOS support.
	Add m68k FreeMiNT support.
	Integration with iOS' xcode build tools.
	Fix Octeon and MC68881 support.
	Fix code pessimizations.

3.0.10 Aug-23-11
        Add support for Apple's iOS.
	Add support for ARM VFP ABI.
        Add RTEMS support for MIPS and M68K.
	Fix instruction cache clearing problems on
	  ARM and SPARC.
	Fix the N64 build on mips-sgi-irix6.5.
	Enable builds with Microsoft's compiler.
	Enable x86 builds with Oracle's Solaris compiler.
	Fix support for calling code compiled with Oracle's Sparc
	  Solaris compiler.
	Testsuite fixes for Tru64 Unix.
	Additional platform support.

3.0.9 Dec-31-09
        Add AVR32 and win64 ports.  Add ARM softfp support.
	Many fixes for AIX, Solaris, HP-UX, *BSD.
	Several PowerPC and x86-64 bug fixes.
	Build DLL for windows.

3.0.8 Dec-19-08
        Add *BSD, BeOS, and PA-Linux support.

3.0.7 Nov-11-08
        Fix for ppc FreeBSD.
	(thanks to Andreas Tobler)

3.0.6 Jul-17-08
        Fix for closures on sh.
	Mark the sh/sh64 stack as non-executable.
	(both thanks to Kaz Kojima)

3.0.5 Apr-3-08
        Fix libffi.pc file.
	Fix #define ARM for IcedTea users.
	Fix x86 closure bug.

3.0.4 Feb-24-08
        Fix x86 OpenBSD configury.

3.0.3 Feb-22-08
        Enable x86 OpenBSD thanks to Thomas Heller, and
	x86-64 FreeBSD thanks to Bjrn Knig and Andreas Tobler.
	Clean up test instruction in README.

3.0.2 Feb-21-08
        Improved x86 FreeBSD support.
	Thanks to Bjrn Knig.

3.0.1 Feb-15-08
        Fix instruction cache flushing bug on MIPS.
	Thanks to David Daney.

3.0.0 Feb-15-08
        Many changes, mostly thanks to the GCC project.
	Cygnus Solutions is now Red Hat.

  [10 years go by...]

1.20 Oct-5-98
	Raffaele Sena produces ARM port.

1.19 Oct-5-98
	Fixed x86 long double and long long return support.
	m68k bug fixes from Andreas Schwab.
	Patch for DU assembler compatibility for the Alpha from Richard
	Henderson.

1.18 Apr-17-98
	Bug fixes and MIPS configuration changes.

1.17 Feb-24-98
	Bug fixes and m68k port from Andreas Schwab. PowerPC port from
	Geoffrey Keating. Various bug x86, Sparc and MIPS bug fixes.

1.16 Feb-11-98
	Richard Henderson produces Alpha port.

1.15 Dec-4-97
	Fixed an n32 ABI bug. New libtool, auto* support.

1.14 May-13-97
	libtool is now used to generate shared and static libraries.
	Fixed a minor portability problem reported by Russ McManus
	<mcmanr@eq.gs.com>.

1.13 Dec-2-96
	Added --enable-purify-safety to keep Purify from complaining
	about certain low level code.
	Sparc fix for calling functions with < 6 args.
	Linux x86 a.out fix.

1.12 Nov-22-96
	Added missing ffi_type_void, needed for supporting void return 
	types. Fixed test case for non MIPS machines. Cygnus Support 
	is now Cygnus Solutions. 

1.11 Oct-30-96
	Added notes about GNU make.

1.10 Oct-29-96
	Added configuration fix for non GNU compilers.

1.09 Oct-29-96
	Added --enable-debug configure switch. Clean-ups based on LCLint 
	feedback. ffi_mips.h is always installed. Many configuration 
	fixes. Fixed ffitest.c for sparc builds.

1.08 Oct-15-96
	Fixed n32 problem. Many clean-ups.

1.07 Oct-14-96
	Gordon Irlam rewrites v8.S again. Bug fixes.

1.06 Oct-14-96
	Gordon Irlam improved the sparc port. 

1.05 Oct-14-96
	Interface changes based on feedback.

1.04 Oct-11-96
	Sparc port complete (modulo struct passing bug).

1.03 Oct-10-96
	Passing struct args, and returning struct values works for
	all architectures/calling conventions. Expanded tests.

1.02 Oct-9-96
	Added SGI n32 support. Fixed bugs in both o32 and Linux support.
	Added "make test".

1.01 Oct-8-96
	Fixed float passing bug in mips version. Restructured some
	of the code. Builds cleanly with SGI tools.

1.00 Oct-7-96
	First release. No public announcement.


Authors & Credits
=================

libffi was originally written by Anthony Green <green@redhat.com>.

The developers of the GNU Compiler Collection project have made
innumerable valuable contributions.  See the ChangeLog file for
details.

Some of the ideas behind libffi were inspired by Gianni Mariani's free
gencall library for Silicon Graphics machines.

The closure mechanism was designed and implemented by Kresten Krab
Thorup.

Major processor architecture ports were contributed by the following
developers:

aarch64		Marcus Shawcroft, James Greenhalgh
alpha		Richard Henderson
arm		Raffaele Sena
blackfin        Alexandre Keunecke I. de Mendonca
cris		Simon Posnjak, Hans-Peter Nilsson
frv		Anthony Green
ia64		Hans Boehm
m32r		Kazuhiro Inaoka
m68k		Andreas Schwab
microblaze	Nathan Rossi
mips		Anthony Green, Casey Marshall
mips64		David Daney
moxie		Anthony Green
pa		Randolph Chung, Dave Anglin, Andreas Tobler
powerpc		Geoffrey Keating, Andreas Tobler, 
			 David Edelsohn, John Hornkvist
powerpc64	Jakub Jelinek
s390		Gerhard Tonn, Ulrich Weigand
sh		Kaz Kojima
sh64		Kaz Kojima
sparc		Anthony Green, Gordon Irlam
tile-gx/tilepro Walter Lee
x86		Anthony Green, Jon Beniston
x86-64		Bo Thorsen
xtensa		Chris Zankel

Jesper Skov and Andrew Haley both did more than their fair share of
stepping through the code and tracking down bugs.

Thanks also to Tom Tromey for bug fixes, documentation and
configuration help.

Thanks to Jim Blandy, who provided some useful feedback on the libffi
interface.

Andreas Tobler has done a tremendous amount of work on the testsuite.

Alex Oliva solved the executable page problem for SElinux.

The list above is almost certainly incomplete and inaccurate.  I'm
happy to make corrections or additions upon request.

If you have a problem, or have found a bug, please send a note to the
author at green@moxielogic.com, or the project mailing list at
libffi-discuss@sourceware.org.

This directory contains the libffi package, which is not part of GCC but
shipped with GCC as convenience.

Status
======

libffi-2.00 has not been released yet! This is a development snapshot!

libffi-1.20 was released on October 5, 1998. Check the libffi web
page for updates: <URL:http://sources.redhat.com/libffi/>.


What is libffi?
===============

Compilers for high level languages generate code that follow certain
conventions. These conventions are necessary, in part, for separate
compilation to work. One such convention is the "calling
convention". The "calling convention" is essentially a set of
assumptions made by the compiler about where function arguments will
be found on entry to a function. A "calling convention" also specifies
where the return value for a function is found.

Some programs may not know at the time of compilation what arguments
are to be passed to a function. For instance, an interpreter may be
told at run-time about the number and types of arguments used to call
a given function. Libffi can be used in such programs to provide a
bridge from the interpreter program to compiled code.

The libffi library provides a portable, high level programming
interface to various calling conventions. This allows a programmer to
call any function specified by a call interface description at run
time.  

Ffi stands for Foreign Function Interface. A foreign function
interface is the popular name for the interface that allows code
written in one language to call code written in another language. The
libffi library really only provides the lowest, machine dependent
layer of a fully featured foreign function interface. A layer must
exist above libffi that handles type conversions for values passed
between the two languages.


Supported Platforms and Prerequisites
=====================================

Libffi has been ported to:

	SunOS 4.1.3 & Solaris 2.x (SPARC-V8, SPARC-V9)

	Irix 5.3 & 6.2 (System V/o32 & n32)

	Intel x86 - Linux (System V ABI)

	Alpha - Linux and OSF/1

	m68k - Linux (System V ABI)

	PowerPC - Linux (System V ABI, Darwin, AIX)

	ARM - Linux (System V ABI)

Libffi has been tested with the egcs 1.0.2 gcc compiler. Chances are
that other versions will work.  Libffi has also been built and tested
with the SGI compiler tools.

On PowerPC, the tests failed (see the note below).

You must use GNU make to build libffi. SGI's make will not work.
Sun's probably won't either.
	
If you port libffi to another platform, please let me know! I assume
that some will be easy (x86 NetBSD), and others will be more difficult
(HP).


Installing libffi
=================

[Note: before actually performing any of these installation steps,
 you may wish to read the "Platform Specific Notes" below.]

First you must configure the distribution for your particular
system. Go to the directory you wish to build libffi in and run the
"configure" program found in the root directory of the libffi source
distribution.

You may want to tell configure where to install the libffi library and
header files. To do that, use the --prefix configure switch.  Libffi
will install under /usr/local by default. 

If you want to enable extra run-time debugging checks use the the
--enable-debug configure switch. This is useful when your program dies
mysteriously while using libffi. 

Another useful configure switch is --enable-purify-safety. Using this
will add some extra code which will suppress certain warnings when you
are using Purify with libffi. Only use this switch when using 
Purify, as it will slow down the library.

Configure has many other options. Use "configure --help" to see them all.

Once configure has finished, type "make". Note that you must be using
GNU make. SGI's make will not work.  Sun's probably won't either.
You can ftp GNU make from prep.ai.mit.edu:/pub/gnu.

To ensure that libffi is working as advertised, type "make test".

To install the library and header files, type "make install".


Using libffi
============

	The Basics
	----------

Libffi assumes that you have a pointer to the function you wish to
call and that you know the number and types of arguments to pass it,
as well as the return type of the function.

The first thing you must do is create an ffi_cif object that matches
the signature of the function you wish to call. The cif in ffi_cif
stands for Call InterFace. To prepare a call interface object, use the
following function:

ffi_status ffi_prep_cif(ffi_cif *cif, ffi_abi abi,
			unsigned int nargs, 
			ffi_type *rtype, ffi_type **atypes);

	CIF is a pointer to the call interface object you wish
		to initialize.

	ABI is an enum that specifies the calling convention 
		to use for the call. FFI_DEFAULT_ABI defaults
		to the system's native calling convention. Other
		ABI's may be used with care. They are system
		specific.

	NARGS is the number of arguments this function accepts.	
		libffi does not yet support vararg functions.

	RTYPE is a pointer to an ffi_type structure that represents
		the return type of the function. Ffi_type objects
		describe the types of values. libffi provides
		ffi_type objects for many of the native C types:
		signed int, unsigned int, signed char, unsigned char,
		etc. There is also a pointer ffi_type object and
		a void ffi_type. Use &ffi_type_void for functions that 
		don't return values.

	ATYPES is a vector of ffi_type pointers. ARGS must be NARGS long.
		If NARGS is 0, this is ignored.


ffi_prep_cif will return a status code that you are responsible 
for checking. It will be one of the following:

	FFI_OK - All is good.

	FFI_BAD_TYPEDEF - One of the ffi_type objects that ffi_prep_cif
		came across is bad.


Before making the call, the VALUES vector should be initialized 
with pointers to the appropriate argument values.

To call the the function using the initialized ffi_cif, use the
ffi_call function:

void ffi_call(ffi_cif *cif, void *fn, void *rvalue, void **avalues);

	CIF is a pointer to the ffi_cif initialized specifically
		for this function.

	FN is a pointer to the function you want to call.

	RVALUE is a pointer to a chunk of memory that is to hold the
		result of the function call. Currently, it must be
		at least one word in size (except for the n32 version
		under Irix 6.x, which must be a pointer to an 8 byte 
		aligned value (a long long). It must also be at least 
		word aligned (depending on the return type, and the
		system's alignment requirements). If RTYPE is 
		&ffi_type_void, this is ignored. If RVALUE is NULL, 
		the return value is discarded.

	AVALUES is a vector of void* that point to the memory locations
		holding the argument values for a call.
		If NARGS is 0, this is ignored.


If you are expecting a return value from FN it will have been stored
at RVALUE.



	An Example
	----------

Here is a trivial example that calls puts() a few times.

    #include <stdio.h>
    #include <ffi.h>
    
    int main()
    {
      ffi_cif cif;
      ffi_type *args[1];
      void *values[1];
      char *s;
      int rc;
      
      /* Initialize the argument info vectors */    
      args[0] = &ffi_type_uint;
      values[0] = &s;
      
      /* Initialize the cif */
      if (ffi_prep_cif(&cif, FFI_DEFAULT_ABI, 1, 
    		       &ffi_type_uint, args) == FFI_OK)
        {
          s = "Hello World!";
          ffi_call(&cif, puts, &rc, values);
          /* rc now holds the result of the call to puts */
          
          /* values holds a pointer to the function's arg, so to 
	     call puts() again all we need to do is change the 
             value of s */
          s = "This is cool!";
          ffi_call(&cif, puts, &rc, values);
        }
      
      return 0;
    }



	Aggregate Types
	---------------

Although libffi has no special support for unions or bit-fields, it is
perfectly happy passing structures back and forth. You must first
describe the structure to libffi by creating a new ffi_type object
for it. Here is the definition of ffi_type:

    typedef struct _ffi_type
    {
      unsigned size;
      short alignment;
      short type;
      struct _ffi_type **elements;
    } ffi_type;
    
All structures must have type set to FFI_TYPE_STRUCT.  You may set
size and alignment to 0. These will be calculated and reset to the
appropriate values by ffi_prep_cif().

elements is a NULL terminated array of pointers to ffi_type objects
that describe the type of the structure elements. These may, in turn,
be structure elements.

The following example initializes a ffi_type object representing the
tm struct from Linux's time.h:

				    struct tm {
					int tm_sec;
					int tm_min;
					int tm_hour;
					int tm_mday;
					int tm_mon;
					int tm_year;
					int tm_wday;
					int tm_yday;
					int tm_isdst;
					/* Those are for future use. */
					long int __tm_gmtoff__;
					__const char *__tm_zone__;
				    };

    {
      ffi_type tm_type;
      ffi_type *tm_type_elements[12];
      int i;

      tm_type.size = tm_type.alignment = 0;
      tm_type.elements = &tm_type_elements;
    
      for (i = 0; i < 9; i++)
          tm_type_elements[i] = &ffi_type_sint;

      tm_type_elements[9] = &ffi_type_slong;
      tm_type_elements[10] = &ffi_type_pointer;
      tm_type_elements[11] = NULL;

      /* tm_type can now be used to represent tm argument types and
	 return types for ffi_prep_cif() */
    }



Platform Specific Notes
=======================

	Intel x86
	---------

There are no known problems with the x86 port.

	Sun SPARC - SunOS 4.1.3 & Solaris 2.x
	-------------------------------------

You must use GNU Make to build libffi on Sun platforms.

	MIPS - Irix 5.3 & 6.x
	---------------------

Irix 6.2 and better supports three different calling conventions: o32,
n32 and n64. Currently, libffi only supports both o32 and n32 under
Irix 6.x, but only o32 under Irix 5.3. Libffi will automatically be
configured for whichever calling convention it was built for.

By default, the configure script will try to build libffi with the GNU
development tools. To build libffi with the SGI development tools, set
the environment variable CC to either "cc -32" or "cc -n32" before
running configure under Irix 6.x (depending on whether you want an o32
or n32 library), or just "cc" for Irix 5.3.

With the n32 calling convention, when returning structures smaller
than 16 bytes, be sure to provide an RVALUE that is 8 byte aligned.
Here's one way of forcing this:

	double struct_storage[2];
	my_small_struct *s = (my_small_struct *) struct_storage;  
	/* Use s for RVALUE */

If you don't do this you are liable to get spurious bus errors. 

"long long" values are not supported yet.

You must use GNU Make to build libffi on SGI platforms.

	ARM - System V ABI
	------------------

The ARM port was performed on a NetWinder running ARM Linux ELF
(2.0.31) and gcc 2.8.1.



	PowerPC System V ABI
	--------------------

There are two `System V ABI's which libffi implements for PowerPC.
They differ only in how small structures are returned from functions.

In the FFI_SYSV version, structures that are 8 bytes or smaller are
returned in registers.  This is what GCC does when it is configured
for solaris, and is what the System V ABI I have (dated September
1995) says.

In the FFI_GCC_SYSV version, all structures are returned the same way:
by passing a pointer as the first argument to the function.  This is
what GCC does when it is configured for linux or a generic sysv
target.

EGCS 1.0.1 (and probably other versions of EGCS/GCC) also has a
inconsistency with the SysV ABI: When a procedure is called with many
floating-point arguments, some of them get put on the stack.  They are
all supposed to be stored in double-precision format, even if they are
only single-precision, but EGCS stores single-precision arguments as
single-precision anyway.  This causes one test to fail (the `many
arguments' test).


What's With The Crazy Comments?
===============================

You might notice a number of cryptic comments in the code, delimited
by /*@ and @*/. These are annotations read by the program LCLint, a
tool for statically checking C programs. You can read all about it at
<http://larch-www.lcs.mit.edu:8001/larch/lclint/index.html>.


History
=======

1.20 Oct-5-98
	Raffaele Sena produces ARM port.

1.19 Oct-5-98
	Fixed x86 long double and long long return support.
	m68k bug fixes from Andreas Schwab.
	Patch for DU assembler compatibility for the Alpha from Richard
	Henderson.

1.18 Apr-17-98
	Bug fixes and MIPS configuration changes.

1.17 Feb-24-98
	Bug fixes and m68k port from Andreas Schwab. PowerPC port from
	Geoffrey Keating. Various bug x86, Sparc and MIPS bug fixes.

1.16 Feb-11-98
	Richard Henderson produces Alpha port.

1.15 Dec-4-97
	Fixed an n32 ABI bug. New libtool, auto* support.

1.14 May-13-97
	libtool is now used to generate shared and static libraries.
	Fixed a minor portability problem reported by Russ McManus
	<mcmanr@eq.gs.com>.

1.13 Dec-2-96
	Added --enable-purify-safety to keep Purify from complaining
	about certain low level code.
	Sparc fix for calling functions with < 6 args.
	Linux x86 a.out fix.

1.12 Nov-22-96
	Added missing ffi_type_void, needed for supporting void return 
	types. Fixed test case for non MIPS machines. Cygnus Support 
	is now Cygnus Solutions. 

1.11 Oct-30-96
	Added notes about GNU make.

1.10 Oct-29-96
	Added configuration fix for non GNU compilers.

1.09 Oct-29-96
	Added --enable-debug configure switch. Clean-ups based on LCLint 
	feedback. ffi_mips.h is always installed. Many configuration 
	fixes. Fixed ffitest.c for sparc builds.

1.08 Oct-15-96
	Fixed n32 problem. Many clean-ups.

1.07 Oct-14-96
	Gordon Irlam rewrites v8.S again. Bug fixes.

1.06 Oct-14-96
	Gordon Irlam improved the sparc port. 

1.05 Oct-14-96
	Interface changes based on feedback.

1.04 Oct-11-96
	Sparc port complete (modulo struct passing bug).

1.03 Oct-10-96
	Passing struct args, and returning struct values works for
	all architectures/calling conventions. Expanded tests.

1.02 Oct-9-96
	Added SGI n32 support. Fixed bugs in both o32 and Linux support.
	Added "make test".

1.01 Oct-8-96
	Fixed float passing bug in mips version. Restructured some
	of the code. Builds cleanly with SGI tools.

1.00 Oct-7-96
	First release. No public announcement.


Authors & Credits
=================

libffi was written by Anthony Green <green@cygnus.com>.

Portions of libffi were derived from Gianni Mariani's free gencall
library for Silicon Graphics machines.

The closure mechanism was designed and implemented by Kresten Krab
Thorup.

The Sparc port was derived from code contributed by the fine folks at
Visible Decisions Inc <http://www.vdi.com>. Further enhancements were
made by Gordon Irlam at Cygnus Solutions <http://www.cygnus.com>.

The Alpha port was written by Richard Henderson at Cygnus Solutions.

Andreas Schwab ported libffi to m68k Linux and provided a number of
bug fixes.

Geoffrey Keating ported libffi to the PowerPC.

Raffaele Sena ported libffi to the ARM.

Jesper Skov and Andrew Haley both did more than their fair share of
stepping through the code and tracking down bugs.

Thanks also to Tom Tromey for bug fixes and configuration help.

Thanks to Jim Blandy, who provided some useful feedback on the libffi
interface.

If you have a problem, or have found a bug, please send a note to
green@cygnus.com.

The purpose is to hack the libffi sources so that they can be compiled
with MSVC, and to extend them so that they have the features I need
for ctypes.

I retrieved the libffi sources from the gcc cvs repository on
2004-01-27.  Then I did 'configure' in a 'build' subdirectory on a x86
linux system, and copied the files I found useful.

This directory contains the libffi package, which is not part of GCC but
shipped with GCC as convenience.

Status
======

libffi-2.00 has not been released yet! This is a development snapshot!

libffi-1.20 was released on October 5, 1998. Check the libffi web
page for updates: <URL:http://sources.redhat.com/libffi/>.


What is libffi?
===============

Compilers for high level languages generate code that follow certain
conventions. These conventions are necessary, in part, for separate
compilation to work. One such convention is the "calling
convention". The "calling convention" is essentially a set of
assumptions made by the compiler about where function arguments will
be found on entry to a function. A "calling convention" also specifies
where the return value for a function is found.

Some programs may not know at the time of compilation what arguments
are to be passed to a function. For instance, an interpreter may be
told at run-time about the number and types of arguments used to call
a given function. Libffi can be used in such programs to provide a
bridge from the interpreter program to compiled code.

The libffi library provides a portable, high level programming
interface to various calling conventions. This allows a programmer to
call any function specified by a call interface description at run
time.  

Ffi stands for Foreign Function Interface. A foreign function
interface is the popular name for the interface that allows code
written in one language to call code written in another language. The
libffi library really only provides the lowest, machine dependent
layer of a fully featured foreign function interface. A layer must
exist above libffi that handles type conversions for values passed
between the two languages.


Supported Platforms and Prerequisites
=====================================

Libffi has been ported to:

	SunOS 4.1.3 & Solaris 2.x (SPARC-V8, SPARC-V9)

	Irix 5.3 & 6.2 (System V/o32 & n32)

	Intel x86 - Linux (System V ABI)

	Alpha - Linux and OSF/1

	m68k - Linux (System V ABI)

	PowerPC - Linux (System V ABI, Darwin, AIX)

	ARM - Linux (System V ABI)

Libffi has been tested with the egcs 1.0.2 gcc compiler. Chances are
that other versions will work.  Libffi has also been built and tested
with the SGI compiler tools.

On PowerPC, the tests failed (see the note below).

You must use GNU make to build libffi. SGI's make will not work.
Sun's probably won't either.
	
If you port libffi to another platform, please let me know! I assume
that some will be easy (x86 NetBSD), and others will be more difficult
(HP).


Installing libffi
=================

[Note: before actually performing any of these installation steps,
 you may wish to read the "Platform Specific Notes" below.]

First you must configure the distribution for your particular
system. Go to the directory you wish to build libffi in and run the
"configure" program found in the root directory of the libffi source
distribution.

You may want to tell configure where to install the libffi library and
header files. To do that, use the --prefix configure switch.  Libffi
will install under /usr/local by default. 

If you want to enable extra run-time debugging checks use the the
--enable-debug configure switch. This is useful when your program dies
mysteriously while using libffi. 

Another useful configure switch is --enable-purify-safety. Using this
will add some extra code which will suppress certain warnings when you
are using Purify with libffi. Only use this switch when using 
Purify, as it will slow down the library.

Configure has many other options. Use "configure --help" to see them all.

Once configure has finished, type "make". Note that you must be using
GNU make. SGI's make will not work.  Sun's probably won't either.
You can ftp GNU make from prep.ai.mit.edu:/pub/gnu.

To ensure that libffi is working as advertised, type "make test".

To install the library and header files, type "make install".


Using libffi
============

	The Basics
	----------

Libffi assumes that you have a pointer to the function you wish to
call and that you know the number and types of arguments to pass it,
as well as the return type of the function.

The first thing you must do is create an ffi_cif object that matches
the signature of the function you wish to call. The cif in ffi_cif
stands for Call InterFace. To prepare a call interface object, use the
following function:

ffi_status ffi_prep_cif(ffi_cif *cif, ffi_abi abi,
			unsigned int nargs, 
			ffi_type *rtype, ffi_type **atypes);

	CIF is a pointer to the call interface object you wish
		to initialize.

	ABI is an enum that specifies the calling convention 
		to use for the call. FFI_DEFAULT_ABI defaults
		to the system's native calling convention. Other
		ABI's may be used with care. They are system
		specific.

	NARGS is the number of arguments this function accepts.	
		libffi does not yet support vararg functions.

	RTYPE is a pointer to an ffi_type structure that represents
		the return type of the function. Ffi_type objects
		describe the types of values. libffi provides
		ffi_type objects for many of the native C types:
		signed int, unsigned int, signed char, unsigned char,
		etc. There is also a pointer ffi_type object and
		a void ffi_type. Use &ffi_type_void for functions that 
		don't return values.

	ATYPES is a vector of ffi_type pointers. ARGS must be NARGS long.
		If NARGS is 0, this is ignored.


ffi_prep_cif will return a status code that you are responsible 
for checking. It will be one of the following:

	FFI_OK - All is good.

	FFI_BAD_TYPEDEF - One of the ffi_type objects that ffi_prep_cif
		came across is bad.


Before making the call, the VALUES vector should be initialized 
with pointers to the appropriate argument values.

To call the the function using the initialized ffi_cif, use the
ffi_call function:

void ffi_call(ffi_cif *cif, void *fn, void *rvalue, void **avalues);

	CIF is a pointer to the ffi_cif initialized specifically
		for this function.

	FN is a pointer to the function you want to call.

	RVALUE is a pointer to a chunk of memory that is to hold the
		result of the function call. Currently, it must be
		at least one word in size (except for the n32 version
		under Irix 6.x, which must be a pointer to an 8 byte 
		aligned value (a long long). It must also be at least 
		word aligned (depending on the return type, and the
		system's alignment requirements). If RTYPE is 
		&ffi_type_void, this is ignored. If RVALUE is NULL, 
		the return value is discarded.

	AVALUES is a vector of void* that point to the memory locations
		holding the argument values for a call.
		If NARGS is 0, this is ignored.


If you are expecting a return value from FN it will have been stored
at RVALUE.



	An Example
	----------

Here is a trivial example that calls puts() a few times.

    #include <stdio.h>
    #include <ffi.h>
    
    int main()
    {
      ffi_cif cif;
      ffi_type *args[1];
      void *values[1];
      char *s;
      int rc;
      
      /* Initialize the argument info vectors */    
      args[0] = &ffi_type_uint;
      values[0] = &s;
      
      /* Initialize the cif */
      if (ffi_prep_cif(&cif, FFI_DEFAULT_ABI, 1, 
    		       &ffi_type_uint, args) == FFI_OK)
        {
          s = "Hello World!";
          ffi_call(&cif, puts, &rc, values);
          /* rc now holds the result of the call to puts */
          
          /* values holds a pointer to the function's arg, so to 
	     call puts() again all we need to do is change the 
             value of s */
          s = "This is cool!";
          ffi_call(&cif, puts, &rc, values);
        }
      
      return 0;
    }



	Aggregate Types
	---------------

Although libffi has no special support for unions or bit-fields, it is
perfectly happy passing structures back and forth. You must first
describe the structure to libffi by creating a new ffi_type object
for it. Here is the definition of ffi_type:

    typedef struct _ffi_type
    {
      unsigned size;
      short alignment;
      short type;
      struct _ffi_type **elements;
    } ffi_type;
    
All structures must have type set to FFI_TYPE_STRUCT.  You may set
size and alignment to 0. These will be calculated and reset to the
appropriate values by ffi_prep_cif().

elements is a NULL terminated array of pointers to ffi_type objects
that describe the type of the structure elements. These may, in turn,
be structure elements.

The following example initializes a ffi_type object representing the
tm struct from Linux's time.h:

				    struct tm {
					int tm_sec;
					int tm_min;
					int tm_hour;
					int tm_mday;
					int tm_mon;
					int tm_year;
					int tm_wday;
					int tm_yday;
					int tm_isdst;
					/* Those are for future use. */
					long int __tm_gmtoff__;
					__const char *__tm_zone__;
				    };

    {
      ffi_type tm_type;
      ffi_type *tm_type_elements[12];
      int i;

      tm_type.size = tm_type.alignment = 0;
      tm_type.elements = &tm_type_elements;
    
      for (i = 0; i < 9; i++)
          tm_type_elements[i] = &ffi_type_sint;

      tm_type_elements[9] = &ffi_type_slong;
      tm_type_elements[10] = &ffi_type_pointer;
      tm_type_elements[11] = NULL;

      /* tm_type can now be used to represent tm argument types and
	 return types for ffi_prep_cif() */
    }



Platform Specific Notes
=======================

	Intel x86
	---------

There are no known problems with the x86 port.

	Sun SPARC - SunOS 4.1.3 & Solaris 2.x
	-------------------------------------

You must use GNU Make to build libffi on Sun platforms.

	MIPS - Irix 5.3 & 6.x
	---------------------

Irix 6.2 and better supports three different calling conventions: o32,
n32 and n64. Currently, libffi only supports both o32 and n32 under
Irix 6.x, but only o32 under Irix 5.3. Libffi will automatically be
configured for whichever calling convention it was built for.

By default, the configure script will try to build libffi with the GNU
development tools. To build libffi with the SGI development tools, set
the environment variable CC to either "cc -32" or "cc -n32" before
running configure under Irix 6.x (depending on whether you want an o32
or n32 library), or just "cc" for Irix 5.3.

With the n32 calling convention, when returning structures smaller
than 16 bytes, be sure to provide an RVALUE that is 8 byte aligned.
Here's one way of forcing this:

	double struct_storage[2];
	my_small_struct *s = (my_small_struct *) struct_storage;  
	/* Use s for RVALUE */

If you don't do this you are liable to get spurious bus errors. 

"long long" values are not supported yet.

You must use GNU Make to build libffi on SGI platforms.

	ARM - System V ABI
	------------------

The ARM port was performed on a NetWinder running ARM Linux ELF
(2.0.31) and gcc 2.8.1.



	PowerPC System V ABI
	--------------------

There are two `System V ABI's which libffi implements for PowerPC.
They differ only in how small structures are returned from functions.

In the FFI_SYSV version, structures that are 8 bytes or smaller are
returned in registers.  This is what GCC does when it is configured
for solaris, and is what the System V ABI I have (dated September
1995) says.

In the FFI_GCC_SYSV version, all structures are returned the same way:
by passing a pointer as the first argument to the function.  This is
what GCC does when it is configured for linux or a generic sysv
target.

EGCS 1.0.1 (and probably other versions of EGCS/GCC) also has a
inconsistency with the SysV ABI: When a procedure is called with many
floating-point arguments, some of them get put on the stack.  They are
all supposed to be stored in double-precision format, even if they are
only single-precision, but EGCS stores single-precision arguments as
single-precision anyway.  This causes one test to fail (the `many
arguments' test).


What's With The Crazy Comments?
===============================

You might notice a number of cryptic comments in the code, delimited
by /*@ and @*/. These are annotations read by the program LCLint, a
tool for statically checking C programs. You can read all about it at
<http://larch-www.lcs.mit.edu:8001/larch/lclint/index.html>.


History
=======

1.20 Oct-5-98
	Raffaele Sena produces ARM port.

1.19 Oct-5-98
	Fixed x86 long double and long long return support.
	m68k bug fixes from Andreas Schwab.
	Patch for DU assembler compatibility for the Alpha from Richard
	Henderson.

1.18 Apr-17-98
	Bug fixes and MIPS configuration changes.

1.17 Feb-24-98
	Bug fixes and m68k port from Andreas Schwab. PowerPC port from
	Geoffrey Keating. Various bug x86, Sparc and MIPS bug fixes.

1.16 Feb-11-98
	Richard Henderson produces Alpha port.

1.15 Dec-4-97
	Fixed an n32 ABI bug. New libtool, auto* support.

1.14 May-13-97
	libtool is now used to generate shared and static libraries.
	Fixed a minor portability problem reported by Russ McManus
	<mcmanr@eq.gs.com>.

1.13 Dec-2-96
	Added --enable-purify-safety to keep Purify from complaining
	about certain low level code.
	Sparc fix for calling functions with < 6 args.
	Linux x86 a.out fix.

1.12 Nov-22-96
	Added missing ffi_type_void, needed for supporting void return 
	types. Fixed test case for non MIPS machines. Cygnus Support 
	is now Cygnus Solutions. 

1.11 Oct-30-96
	Added notes about GNU make.

1.10 Oct-29-96
	Added configuration fix for non GNU compilers.

1.09 Oct-29-96
	Added --enable-debug configure switch. Clean-ups based on LCLint 
	feedback. ffi_mips.h is always installed. Many configuration 
	fixes. Fixed ffitest.c for sparc builds.

1.08 Oct-15-96
	Fixed n32 problem. Many clean-ups.

1.07 Oct-14-96
	Gordon Irlam rewrites v8.S again. Bug fixes.

1.06 Oct-14-96
	Gordon Irlam improved the sparc port. 

1.05 Oct-14-96
	Interface changes based on feedback.

1.04 Oct-11-96
	Sparc port complete (modulo struct passing bug).

1.03 Oct-10-96
	Passing struct args, and returning struct values works for
	all architectures/calling conventions. Expanded tests.

1.02 Oct-9-96
	Added SGI n32 support. Fixed bugs in both o32 and Linux support.
	Added "make test".

1.01 Oct-8-96
	Fixed float passing bug in mips version. Restructured some
	of the code. Builds cleanly with SGI tools.

1.00 Oct-7-96
	First release. No public announcement.


Authors & Credits
=================

libffi was written by Anthony Green <green@cygnus.com>.

Portions of libffi were derived from Gianni Mariani's free gencall
library for Silicon Graphics machines.

The closure mechanism was designed and implemented by Kresten Krab
Thorup.

The Sparc port was derived from code contributed by the fine folks at
Visible Decisions Inc <http://www.vdi.com>. Further enhancements were
made by Gordon Irlam at Cygnus Solutions <http://www.cygnus.com>.

The Alpha port was written by Richard Henderson at Cygnus Solutions.

Andreas Schwab ported libffi to m68k Linux and provided a number of
bug fixes.

Geoffrey Keating ported libffi to the PowerPC.

Raffaele Sena ported libffi to the ARM.

Jesper Skov and Andrew Haley both did more than their fair share of
stepping through the code and tracking down bugs.

Thanks also to Tom Tromey for bug fixes and configuration help.

Thanks to Jim Blandy, who provided some useful feedback on the libffi
interface.

If you have a problem, or have found a bug, please send a note to
green@cygnus.com.

This directory contains a slightly modified version of libffi, extracted from
the GCC source-tree.

The only modifications are those that are necessary to compile libffi using 
the Apple provided compiler and outside of the GCC source tree.



libmpdec
========

libmpdec is a fast C/C++ library for correctly-rounded arbitrary precision
decimal floating point arithmetic. It is a complete implementation of
Mike Cowlishaw/IBM's General Decimal Arithmetic Specification.


Files required for the Python _decimal module
=============================================

  Core files for small and medium precision arithmetic
  ----------------------------------------------------

    basearith.{c,h}  ->  Core arithmetic in base 10**9 or 10**19.
    bits.h           ->  Portable detection of least/most significant one-bit.
    constants.{c,h}  ->  Constants that are used in multiple files.
    context.c        ->  Context functions.
    io.{c,h}         ->  Conversions between mpd_t and ASCII strings,
                         mpd_t formatting (allows UTF-8 fill character).
    memory.{c,h}     ->  Allocation handlers with overflow detection
                         and functions for switching between static
                         and dynamic mpd_t.
    mpdecimal.{c,h}  ->  All (quiet) functions of the specification.
    typearith.h      ->  Fast primitives for double word multiplication,
                         division etc.

    Visual Studio only:
    ~~~~~~~~~~~~~~~~~~~
      vccompat.h    ->  snprintf <==> sprintf_s and similar things.
      vcstdint.h    ->  stdint.h (included in VS 2010 but not in VS 2008).
      vcdiv64.asm   ->  Double word division used in typearith.h. VS 2008 does
                        not allow inline asm for x64. Also, it does not provide
                        an intrinsic for double word division.

  Files for bignum arithmetic:
  ----------------------------

    The following files implement the Fast Number Theoretic Transform
    used for multiplying coefficients with more than 1024 words (see
    mpdecimal.c: _mpd_fntmul()).

      umodarith.h        ->  Fast low level routines for unsigned modular arithmetic.
      numbertheory.{c,h} ->  Routines for setting up the Number Theoretic Transform.
      difradix2.{c,h}    ->  Decimation in frequency transform, used as the
                             "base case" by the following three files:

        fnt.{c,h}        ->  Transform arrays up to 4096 words.
        sixstep.{c,h}    ->  Transform larger arrays of length 2**n.
        fourstep.{c,h}   ->  Transform larger arrays of length 3 * 2**n.

      convolute.{c,h}    ->  Fast convolution using one of the three transform
                             functions.
      transpose.{c,h}    ->  Transpositions needed for the sixstep algorithm.
      crt.{c,h}          ->  Chinese Remainder Theorem: use information from three
                             transforms modulo three different primes to get the
                             final result.


Pointers to literature, proofs and more
=======================================

  literature/
  -----------

    REFERENCES.txt  ->  List of relevant papers.
    bignum.txt      ->  Explanation of the Fast Number Theoretic Transform (FNT).
    fnt.py          ->  Verify constants used in the FNT; Python demo for the
                        O(N**2) discrete transform.

    matrix-transform.txt -> Proof for the Matrix Fourier Transform used in
                            fourstep.c.
    six-step.txt         -> Show that the algorithm used in sixstep.c is
                            a variant of the Matrix Fourier Transform.
    mulmod-64.txt        -> Proof for the mulmod64 algorithm from
                            umodarith.h.
    mulmod-ppro.txt      -> Proof for the x87 FPU modular multiplication
                            from umodarith.h.
    umodarith.lisp       -> ACL2 proofs for many functions from umodarith.h.


Library Author
==============

  Stefan Krah <skrah@bytereef.org>






About
=====

_decimal.c is a wrapper for the libmpdec library. libmpdec is a fast C
library for correctly-rounded arbitrary precision decimal floating point
arithmetic. It is a complete implementation of Mike Cowlishaw/IBM's
General Decimal Arithmetic Specification.


Build process for the module
============================

As usual, the build process for _decimal.so is driven by setup.py in the top
level directory. setup.py autodetects the following build configurations:

   1) x64         - 64-bit Python, x86_64 processor (AMD, Intel)

   2) uint128     - 64-bit Python, compiler provides __uint128_t (gcc)

   3) ansi64      - 64-bit Python, ANSI C

   4) ppro        - 32-bit Python, x86 CPU, PentiumPro or later

   5) ansi32      - 32-bit Python, ANSI C

   6) ansi-legacy - 32-bit Python, compiler without uint64_t

   7) universal   - Mac OS only (multi-arch)


It is possible to override autodetection by exporting:

   PYTHON_DECIMAL_WITH_MACHINE=value, where value is one of the above options.


NOTE
====

decimal.so is not built from a static libmpdec.a since doing so led to
failures on AIX (user report) and Windows (mixing static and dynamic CRTs
causes locale problems and more).






This directory contains extended tests and a benchmark against decimal.py:

  bench.py  ->  Benchmark for small and large precisions.
  Usage: ../../../python bench.py

  formathelper.py   ->
  randdec.py        ->  Generate test cases for deccheck.py.
  randfloat.py      ->

  deccheck.py  ->  Run extended tests.
  Usage: ../../../python deccheck.py [--short|--medium|--long|--all]



bits shared by the stringobject and unicodeobject implementations (and
possibly other modules, in a not too distant future).

the stuff in here is included into relevant places; see the individual
source files for details.

--------------------------------------------------------------------
the following defines used by the different modules:

STRINGLIB_CHAR

    the type used to hold a character (char or Py_UNICODE)

STRINGLIB_EMPTY

    a PyObject representing the empty string, only to be used if
    STRINGLIB_MUTABLE is 0

Py_ssize_t STRINGLIB_LEN(PyObject*)

    returns the length of the given string object (which must be of the
    right type)

PyObject* STRINGLIB_NEW(STRINGLIB_CHAR*, Py_ssize_t)

    creates a new string object

STRINGLIB_CHAR* STRINGLIB_STR(PyObject*)

    returns the pointer to the character data for the given string
    object (which must be of the right type)

int STRINGLIB_CHECK_EXACT(PyObject *)

    returns true if the object is an instance of our type, not a subclass

STRINGLIB_MUTABLE

    must be 0 or 1 to tell the cpp macros in stringlib code if the object
    being operated on is mutable or not


XXX Write description
XXX Dont't forget to mention upx

XXX Add pointer to this file into PC/README.txt

Example Python extension for Windows NT
=======================================

This directory contains everything needed (except for the Python
distribution!) to build a Python extension module using Microsoft VC++.
Notice that you need to use the same compiler version that was used to build 
Python itself.

The simplest way to build this example is to use the distutils script
'setup.py'.  To do this, simply execute:

  % python setup.py install

after everything builds and installs, you can test it:

  % python -c "import example; example.foo()"
  Hello, world

See setup.py for more details.  alternatively, see below for instructions on 
how to build inside the Visual Studio environment.

Visual Studio Build Instructions
================================

These are instructions how to build an extension using Visual C++.  The
instructions and project files have not been updated to the latest VC
version.  In general, it is recommended you use the 'setup.py' instructions
above.

It has been tested with VC++ 7.1 on Python 2.4.  You can also use earlier 
versions of VC to build Python extensions, but the sample VC project file 
(example.dsw in this directory) is in VC 7.1 format.

COPY THIS DIRECTORY!
--------------------
This "example_nt" directory is a subdirectory of the PC directory, in order
to keep all the PC-specific files under the same directory.  However, the
example_nt directory can't actually be used from this location.  You first
need to copy or move it up one level, so that example_nt is a direct
sibling of the PC\ and Include\ directories.  Do all your work from within
this new location -- sorry, but you'll be sorry if you don't.

OPEN THE PROJECT
----------------
From VC 7.1, use the
    File -> Open Solution...
dialog (*not* the "File -> Open..." dialog!).  Navigate to and select the
file "example.sln", in the *copy* of the example_nt directory you made
above.
Click Open.

BUILD THE EXAMPLE DLL
---------------------
In order to check that everything is set up right, try building:

1. Select a configuration.  This step is optional.  Do
       Build -> Configuration Manager... -> Active Solution Configuration
   and select either "Release" or "Debug".
   If you skip this step, you'll use the Debug configuration by default.

2. Build the DLL.  Do
       Build -> Build Solution
   This creates all intermediate and result files in a subdirectory which
   is called either Debug or Release, depending on which configuration you
   picked in the preceding step.

TESTING THE DEBUG-MODE DLL
--------------------------
Once the Debug build has succeeded, bring up a DOS box, and cd to
example_nt\Debug.  You should now be able to repeat the following session
("C>" is the DOS prompt, ">>>" is the Python prompt) (note that various
debug output from Python may not match this screen dump exactly):

    C>..\..\PCbuild\python_d
    Adding parser accelerators ...
    Done.
    Python 2.2c1+ (#28, Dec 14 2001, 18:06:39) [MSC 32 bit (Intel)] on win32
    Type "help", "copyright", "credits" or "license" for more information.
    >>> import example
    [7052 refs]
    >>> example.foo()
    Hello, world
    [7052 refs]
    >>>

TESTING THE RELEASE-MODE DLL
----------------------------
Once the Release build has succeeded, bring up a DOS box, and cd to
example_nt\Release.  You should now be able to repeat the following session
("C>" is the DOS prompt, ">>>" is the Python prompt):

    C>..\..\PCbuild\python
    Python 2.2c1+ (#28, Dec 14 2001, 18:06:04) [MSC 32 bit (Intel)] on win32
    Type "help", "copyright", "credits" or "license" for more information.
    >>> import example
    >>> example.foo()
    Hello, world
    >>>

Congratulations!  You've successfully built your first Python extension
module.

CREATING YOUR OWN PROJECT
-------------------------
Choose a name ("spam" is always a winner :-) and create a directory for
it.  Copy your C sources into it.  Note that the module source file name
does not necessarily have to match the module name, but the "init" function
name should match the module name -- i.e. you can only import a module
"spam" if its init function is called "initspam()", and it should call
Py_InitModule with the string "spam" as its first argument (use the minimal
example.c in this directory as a guide).  By convention, it lives in a file
called "spam.c" or "spammodule.c".  The output file should be called
"spam.dll" or "spam.pyd" (the latter is supported to avoid confusion with a
system library "spam.dll" to which your module could be a Python interface)
in Release mode, or spam_d.dll or spam_d.pyd in Debug mode.

Now your options are:

1) Copy example.sln and example.vcproj, rename them to spam.*, and edit them
by hand.

or

2) Create a brand new project; instructions are below.

In either case, copy example_nt\example.def to spam\spam.def, and edit the
new spam.def so its second line contains the string "initspam".  If you
created a new project yourself, add the file spam.def to the project now.
(This is an annoying little file with only two lines.  An alternative
approach is to forget about the .def file, and add the option
"/export:initspam" somewhere to the Link settings, by manually editing the
"Project -> Properties -> Linker -> Command Line -> Additional Options" 
box).

You are now all set to build your extension, unless it requires other
external libraries, include files, etc.  See Python's Extending and
Embedding manual for instructions on how to write an extension.


CREATING A BRAND NEW PROJECT
----------------------------
Use the
    File -> New -> Project...
dialog to create a new Project Workspace.  Select "Visual C++ Projects/Win32/
Win32 Project", enter the name ("spam"), and make sure the "Location" is 
set to parent of the spam directory you have created (which should be a direct 
subdirectory of the Python build tree, a sibling of Include and PC).  
In "Application Settings", select "DLL", and "Empty Project".  Click OK.

You should now create the file spam.def as instructed in the previous
section. Add the source files (including the .def file) to the project, 
using "Project", "Add Existing Item".

Now open the
    Project -> spam properties...
dialog.  (Impressive, isn't it? :-) You only need to change a few
settings.  Make sure "All Configurations" is selected from the "Settings
for:" dropdown list.  Select the "C/C++" tab.  Choose the "General"
category in the popup menu at the top.  Type the following text in the
entry box labeled "Addditional Include Directories:"

    ..\Include,..\PC

Then, choose the "General" category in the "Linker" tab, and enter
    ..\PCbuild
in the "Additional library Directories" box.

Now you need to add some mode-specific settings (select "Accept"
when asked to confirm your changes):

Select "Release" in the "Configuration" dropdown list.  Click the
"Link" tab, choose the "Input" Category, and append "python24.lib" to the
list in the "Additional Dependencies" box.

Select "Debug" in the "Settings for:" dropdown list, and append
"python24_d.lib" to the list in the Additional Dependencies" box.  Then
click on the C/C++ tab, select "Code Generation", and select 
"Multi-threaded Debug DLL" from the "Runtime library" dropdown list.

Select "Release" again from the "Settings for:" dropdown list.
Select "Multi-threaded DLL" from the "Use run-time library:" dropdown list.

That's all <wink>.

Welcome to the "PC" subdirectory of the Python distribution
***********************************************************

This "PC" subdirectory contains complete project files to make
several older PC ports of Python, as well as all the PC-specific
Python source files.  It should be located in the root of the
Python distribution, and there should be directories "Modules",
"Objects", "Python", etc. in the parent directory of this "PC"
subdirectory.  Be sure to read the documentation in the Python
distribution.

Python requires library files such as string.py to be available in
one or more library directories.  The search path of libraries is
set up when Python starts.  To see the current Python library search
path, start Python and enter "import sys" and "print sys.path".

All PC ports use this scheme to try to set up a module search path:

  1) The script location; the current directory without script.
  2) The PYTHONPATH variable, if set.
  3) For Win32 platforms (NT/95), paths specified in the Registry.
  4) Default directories lib, lib/win, lib/test, lib/tkinter;
     these are searched relative to the environment variable
     PYTHONHOME, if set, or relative to the executable and its
     ancestors, if a landmark file (Lib/string.py) is found ,
     or the current directory (not useful).
  5) The directory containing the executable.

The best installation strategy is to put the Python executable (and
DLL, for Win32 platforms) in some convenient directory such as
C:/python, and copy all library files and subdirectories (using XCOPY)
to C:/python/lib.  Then you don't need to set PYTHONPATH.  Otherwise,
set the environment variable PYTHONPATH to your Python search path.
For example,
   set PYTHONPATH=.;d:\python\lib;d:\python\lib\win;d:\python\lib\dos-8x3

There are several add-in modules to build Python programs which use
the native Windows operating environment.  The ports here just make
"QuickWin" and DOS Python versions which support a character-mode
(console) environment.  Look in www.python.org for Tkinter, PythonWin,
WPY and wxPython.

To make a Python port, start the Integrated Development Environment
(IDE) of your compiler, and read in the native "project file"
(or makefile) provided.  This will enable you to change any source
files or build settings so you can make custom builds.

pyconfig.h    An important configuration file specific to PC's.

config.c    The list of C modules to include in the Python PC
            version.  Manually edit this file to add or
            remove Python modules.

testpy.py   A Python test program.  Run this to test your
            Python port.  It should produce copious output,
	    ending in a report on how many tests were OK, how many
	    failed, and how many were skipped.  Don't worry about
	    skipped tests (these test unavailable optional features).


Additional files and subdirectories for 32-bit Windows
======================================================

python_nt.rc   Resource compiler input for python15.dll.

dl_nt.c
               Additional sources used for 32-bit Windows features.

getpathp.c     Default sys.path calculations (for all PC platforms).

dllbase_nt.txt A (manually maintained) list of base addresses for
               various DLLs, to avoid run-time relocation.

example_nt     A subdirectory showing how to build an extension as a
               DLL.

Legacy support for older versions of Visual Studio
==================================================
The subdirectories VC6, VS7.1 and VS8.0 contain legacy support older
versions of Microsoft Visual Studio. See PCbuild/readme.txt.

Note for Windows 3.x and DOS users
==================================

Neither Windows 3.x nor DOS is supported any more.  The last Python
version that supported these was Python 1.5.2; the support files were
present in Python 2.0 but weren't updated, and it is not our intention
to support these platforms for Python 2.x.

Building Python using Microsoft Visual C++
------------------------------------------

This directory is used to build CPython for Microsoft Windows NT version
5.1 or higher (Windows XP, Windows Server 2003, or later) on 32 and 64
bit platforms.  Using this directory requires an installation of
Microsoft Visual C++ 2010 (MSVC 10.0) of any edition.  The specific
requirements are as follows:

Visual C++ 2010 Express Edition
    Required for building 32-bit Debug and Release configuration builds.
    This edition does not support "solution folders", which pcbuild.sln
    uses; this will not prevent building.
Visual Studio 2010 Professional Edition
    Required for building 64-bit Debug and Release configuration builds
Visual Studio 2010 Premium Edition
    Required for building Release configuration builds that make use of
    Profile Guided Optimization (PGO), on either platform.

The official Python releases are built with PGO using Visual Studio 2010
Ultimate Edition.

All you need to do to build is open the solution "pcbuild.sln" in Visual
Studio, select the desired combination of configuration and platform,
then build with "Build Solution" or the F7 keyboard shortcut.  You can
also build from the command line using the "build.bat" script in this
directory.  The solution is configured to build the projects in the
correct order.

The solution currently supports two platforms.  The Win32 platform is
used to build standard x86-compatible 32-bit binaries, output into this
directory.  The x64 platform is used for building 64-bit AMD64 (aka
x86_64 or EM64T) binaries, output into the amd64 sub-directory which
will be created if it doesn't already exist.  The Itanium (IA-64)
platform is no longer supported.  See the "Building for AMD64" section
below for more information about 64-bit builds.

Four configuration options are supported by the solution:
Debug
    Used to build Python with extra debugging capabilities, equivalent
    to using ./configure --with-pydebug on UNIX.  All binaries built
    using this configuration have "_d" added to their name:
    python35_d.dll, python_d.exe, parser_d.pyd, and so on.  Both the
    build and rt (run test) batch files in this directory accept a -d
    option for debug builds.  If you are building Python to help with
    development of CPython, you will most likely use this configuration.
PGInstrument, PGUpdate
    Used to build Python in Release configuration using PGO, which
    requires Professional Edition of Visual Studio.  See the "Profile
    Guided Optimization" section below for more information.  Build
    output from each of these configurations lands in its own
    sub-directory of this directory.  The official Python releases are
    built using these configurations.
Release
    Used to build Python as it is meant to be used in production
    settings, though without PGO.


Legacy support
--------------

You can find build directories for older versions of Visual Studio and
Visual C++ in the PC directory. The legacy build directories are no
longer actively maintained and may not work out of the box.

Currently, the only legacy build directory is PC\VS9.0, for Visual
Studio 2008 (9.0).


C Runtime
---------

Visual Studio 2010 uses version 10 of the C runtime (MSVCRT10).  The
executables no longer use the "Side by Side" assemblies used in previous
versions of the compiler.  This simplifies distribution of applications.

The run time libraries are available under the VC/Redist folder of your
Visual Studio distribution. For more info, see the Readme in the
VC/Redist folder.


Sub-Projects
------------

The CPython project is split up into several smaller sub-projects which
are managed by the pcbuild.sln solution file.  Each sub-project is
represented by a .vcxproj and a .vcxproj.filters file starting with the
name of the sub-project.  These sub-projects fall into a few general
categories:

The following sub-projects represent the bare minimum required to build
a functioning CPython interpreter.  If nothing else builds but these,
you'll have a very limited but usable python.exe:
pythoncore
    .dll and .lib
python
    .exe
kill_python
    kill_python.exe, a small program designed to kill any instances of
    python(_d).exe that are running and live in the build output
    directory; this is meant to avoid build issues due to locked files
make_buildinfo, make_versioninfo
    helpers to provide necessary information to the build process

These sub-projects provide extra executables that are useful for running
CPython in different ways:
pythonw
    pythonw.exe, a variant of python.exe that doesn't open a Command
    Prompt window
pylauncher
    py.exe, the Python Launcher for Windows, see
        http://docs.python.org/3/using/windows.html#launcher
pywlauncher
    pyw.exe, a variant of py.exe that doesn't open a Command Prompt
    window
_testembed
    _testembed.exe, a small program that embeds Python for testing
    purposes, used by test_capi.py

These are miscellaneous sub-projects that don't really fit the other
categories.  By default, these projects do not build in Debug
configuration:
_freeze_importlib
    _freeze_importlib.exe, used to regenerate Python\importlib.h after
    changes have been made to Lib\importlib\_bootstrap.py
bdist_wininst
    ..\Lib\distutils\command\wininst-10.0[-amd64].exe, the base
    executable used by the distutils bdist_wininst command
python3dll
    python3.dll, the PEP 384 Stable ABI dll
xxlimited
    builds an example module that makes use of the PEP 384 Stable ABI,
    see Modules\xxlimited.c

The following sub-projects are for individual modules of the standard
library which are implemented in C; each one builds a DLL (renamed to
.pyd) of the same name as the project:
_ctypes
_ctypes_test
_decimal
_elementtree
_hashlib
_msi
_multiprocessing
_overlapped
_socket
_testcapi
_testbuffer
_testimportmultiple
pyexpat
select
unicodedata
winsound

The following Python-controlled sub-projects wrap external projects.
Note that these external libraries are not necessary for a working
interpreter, but they do implement several major features.  See the
"Getting External Sources" section below for additional information
about getting the source for building these libraries.  The sub-projects
are:
_bz2
    Python wrapper for version 1.0.6 of the libbzip2 compression library
    Homepage:
        http://www.bzip.org/
_lzma
    Python wrapper for the liblzma compression library, using pre-built
    binaries of XZ Utils version 5.0.5
    Homepage:
        http://tukaani.org/xz/
_ssl
    Python wrapper for version 1.0.1g of the OpenSSL secure sockets
    library, which is built by ssl.vcxproj
    Homepage:
        http://www.openssl.org/

    Building OpenSSL requires nasm.exe (the Netwide Assembler), version
    2.10 or newer from
        http://www.nasm.us/
    to be somewhere on your PATH.  More recent versions of OpenSSL may
    need a later version of NASM. If OpenSSL's self tests don't pass,
    you should first try to update NASM and do a full rebuild of
    OpenSSL.

    The ssl sub-project expects your OpenSSL sources to have already
    been configured and be ready to build.  If you get your sources
    from svn.python.org as suggested in the "Getting External Sources"
    section below, the OpenSSL source will already be ready to go.  If
    you want to build a different version, you will need to run

       PCbuild\prepare_ssl.py path\to\openssl-source-dir

    That script will prepare your OpenSSL sources in the same way that
    those available on svn.python.org have been prepared.  Note that
    Perl must be installed and available on your PATH to configure
    OpenSSL.  ActivePerl is recommended and is available from
        http://www.activestate.com/activeperl/

    The ssl sub-project does not have the ability to clean the OpenSSL
    build; if you need to rebuild, you'll have to clean it by hand.
_sqlite3
    Wraps SQLite 3.8.3.1, which is itself built by sqlite3.vcxproj
    Homepage:
        http://www.sqlite.org/
_tkinter
    Wraps version 8.6.1 of the Tk windowing system.
    Homepage:
        http://www.tcl.tk/

    Tkinter's dependencies are built by the tcl.vcxproj and tk.vcxproj
    projects.  The tix.vcxproj project also builds the Tix extended
    widget set for use with Tkinter.

    Those three projects install their respective components in a
    directory alongside the source directories called "tcltk" on
    Win32 and "tcltk64" on x64.  They also copy the Tcl and Tk DLLs
    into the current output directory, which should ensure that Tkinter
    is able to load Tcl/Tk without having to change your PATH.

    The tcl, tk, and tix sub-projects do not have the ability to clean
    their builds; if you need to rebuild, you'll have to clean them by
    hand.


Getting External Sources
------------------------

The last category of sub-projects listed above wrap external projects
Python doesn't control, and as such a little more work is required in
order to download the relevant source files for each project before they
can be built.  The buildbots must ensure that all libraries are present
before building, so the easiest approach is to run either external.bat
or external-amd64.bat (depending on platform) in the ..\Tools\buildbot
directory from ..\, i.e.:

    C:\python\cpython\PCbuild>cd ..
    C:\python\cpython>Tools\buildbot\external.bat

This extracts all the external sub-projects from
    http://svn.python.org/projects/external
via Subversion (so you'll need an svn.exe on your PATH) and places them
in ..\.. (relative to this directory).

It is also possible to download sources from each project's homepage,
though you may have to change the names of some folders in order to make
things work.  For instance, if you were to download a version 5.0.7 of
XZ Utils, you would need to extract the archive into ..\..\xz-5.0.5
anyway, since that is where the solution is set to look for xz.  The
same is true for all other external projects.


Building for AMD64
------------------

The build process for AMD64 / x64 is very similar to standard builds,
you just have to set x64 as platform. In addition, the HOST_PYTHON
environment variable must point to a Python interpreter (at least 2.4),
to support cross-compilation from Win32.  Note that Visual Studio
requires Professional Edition or better in order to build 64-bit
binaries.


Profile Guided Optimization
---------------------------

The solution has two configurations for PGO. The PGInstrument
configuration must be built first. The PGInstrument binaries are linked
against a profiling library and contain extra debug information. The
PGUpdate configuration takes the profiling data and generates optimized
binaries.

The build_pgo.bat script automates the creation of optimized binaries.
It creates the PGI files, runs the unit test suite or PyBench with the
PGI python, and finally creates the optimized files.

See
    http://msdn.microsoft.com/en-us/library/e7k32f4k(VS.100).aspx
for more on this topic.


Static library
--------------

The solution has no configuration for static libraries. However it is
easy to build a static library instead of a DLL. You simply have to set
the "Configuration Type" to "Static Library (.lib)" and alter the
preprocessor macro "Py_ENABLE_SHARED" to "Py_NO_ENABLE_SHARED". You may
also have to change the "Runtime Library" from "Multi-threaded DLL
(/MD)" to "Multi-threaded (/MT)".


Visual Studio properties
------------------------

The PCbuild solution makes heavy use of Visual Studio property files
(*.props). The properties can be viewed and altered in the Property
Manager (View -> Other Windows -> Property Manager).

The property files used are (+-- = "also imports"):
 * debug (debug macro: _DEBUG)
 * pginstrument (PGO)
 * pgupdate (PGO)
    +-- pginstrument
 * pyd (python extension, release build)
    +-- release
    +-- pyproject
 * pyd_d (python extension, debug build)
    +-- debug
    +-- pyproject
 * pyproject (base settings for all projects, user macros like PyDllName)
 * release (release macro: NDEBUG)
 * sqlite3 (used only by sqlite3.vcxproj)
 * tcltk (used by _tkinter, tcl, tk and tix projects)
 * x64 (AMD64 / x64 platform specific settings)

The pyproject property file defines _WIN32 and x64 defines _WIN64 and
_M_X64 although the macros are set by the compiler, too. The GUI doesn't
always know about the macros and confuse the user with false
information.


Your Own Extension DLLs
-----------------------

If you want to create your own extension module DLL (.pyd), there's an
example with easy-to-follow instructions in ..\PC\example\; read the
file readme.txt there first.

This is Python version 3.5.0 alpha 1
====================================

Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011,
2012, 2013, 2014 Python Software Foundation.  All rights reserved.

Python 3.x is a new version of the language, which is incompatible with the 2.x
line of releases.  The language is mostly the same, but many details, especially
how built-in objects like dictionaries and strings work, have changed
considerably, and a lot of deprecated features have finally been removed.


Build Instructions
------------------

On Unix, Linux, BSD, OSX, and Cygwin:

New text

    ./configure
    make
    make test
    sudo make install

This will install Python as python3.

You can pass many options to the configure script; run "./configure --help" to
find out more.  On OSX and Cygwin, the executable is called python.exe;
elsewhere it's just python.

On Mac OS X, if you have configured Python with --enable-framework, you should
use "make frameworkinstall" to do the installation.  Note that this installs the
Python executable in a place that is not normally on your PATH, you may want to
set up a symlink in /usr/local/bin.

On Windows, see PCbuild/readme.txt.

If you wish, you can create a subdirectory and invoke configure from there.  For
example:

    mkdir debug
    cd debug
    ../configure --with-pydebug
    make
    make test

(This will fail if you *also* built at the top-level directory.  You should do a
"make clean" at the toplevel first.)


What's New
----------

We try to have a comprehensive overview of the changes in the "What's New in
Python 3.5" document, found at

    http://docs.python.org/3.5/whatsnew/3.5.html

For a more detailed change log, read Misc/NEWS (though this file, too, is
incomplete, and also doesn't list anything merged in from the 2.7 release under
development).

If you want to install multiple versions of Python see the section below
entitled "Installing multiple versions".


Documentation
-------------

Documentation for Python 3.5 is online, updated daily:

    http://docs.python.org/3.5/

It can also be downloaded in many formats for faster access.  The documentation
is downloadable in HTML, PDF, and reStructuredText formats; the latter version
is primarily for documentation authors, translators, and people with special
formatting requirements.

If you would like to contribute to the development of Python, relevant
documentation is available at:

    http://docs.python.org/devguide/

For information about building Python's documentation, refer to Doc/README.txt.


Converting From Python 2.x to 3.x
---------------------------------

Python starting with 2.6 contains features to help locating code that needs to
be changed, such as optional warnings when deprecated features are used, and
backported versions of certain key Python 3.x features.

A source-to-source translation tool, "2to3", can take care of the mundane task
of converting large amounts of source code.  It is not a complete solution but
is complemented by the deprecation warnings in 2.6.  See
http://docs.python.org/3.5/library/2to3.html for more information.


Testing
-------

To test the interpreter, type "make test" in the top-level directory.  The test
set produces some output.  You can generally ignore the messages about skipped
tests due to optional features which can't be imported.  If a message is printed
about a failed test or a traceback or core dump is produced, something is wrong.

By default, tests are prevented from overusing resources like disk space and
memory.  To enable these tests, run "make testall".

IMPORTANT: If the tests fail and you decide to mail a bug report, *don't*
include the output of "make test".  It is useless.  Run the failing test
manually, as follows:

        ./python -m test -v test_whatever

(substituting the top of the source tree for '.' if you built in a different
directory).  This runs the test in verbose mode.


Installing multiple versions
----------------------------

On Unix and Mac systems if you intend to install multiple versions of Python
using the same installation prefix (--prefix argument to the configure script)
you must take care that your primary python executable is not overwritten by the
installation of a different version.  All files and directories installed using
"make altinstall" contain the major and minor version and can thus live
side-by-side.  "make install" also creates ${prefix}/bin/python3 which refers to
${prefix}/bin/pythonX.Y.  If you intend to install multiple versions using the
same prefix you must decide which version (if any) is your "primary" version.
Install that version using "make install".  Install all other versions using
"make altinstall".

For example, if you want to install Python 2.6, 2.7 and 3.5 with 2.7 being the
primary version, you would execute "make install" in your 2.7 build directory
and "make altinstall" in the others.


Issue Tracker and Mailing List
------------------------------

We're soliciting bug reports about all aspects of the language.  Fixes are also
welcome, preferable in unified diff format.  Please use the issue tracker:

    http://bugs.python.org/

If you're not sure whether you're dealing with a bug or a feature, use the
mailing list:

    python-dev@python.org

To subscribe to the list, use the mailman form:

    http://mail.python.org/mailman/listinfo/python-dev/


Proposals for enhancement
-------------------------

If you have a proposal to change Python, you may want to send an email to the
comp.lang.python or python-ideas mailing lists for inital feedback.  A Python
Enhancement Proposal (PEP) may be submitted if your idea gains ground.  All
current PEPs, as well as guidelines for submitting a new PEP, are listed at
http://www.python.org/dev/peps/.


Release Schedule
----------------

See PEP 429 for release details: http://www.python.org/dev/peps/pep-0429/


Copyright and License Information
---------------------------------

Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011,
2012, 2013, 2014 Python Software Foundation.  All rights reserved.

Copyright (c) 2000 BeOpen.com.  All rights reserved.

Copyright (c) 1995-2001 Corporation for National Research Initiatives.  All
rights reserved.

Copyright (c) 1991-1995 Stichting Mathematisch Centrum.  All rights reserved.

See the file "LICENSE" for information on the history of this software, terms &
conditions for usage, and a DISCLAIMER OF ALL WARRANTIES.

This Python distribution contains *no* GNU General Public License (GPL) code, so
it may be used in proprietary projects.  There are interfaces to some GNU code
but these are entirely optional.

All trademarks referenced herein are property of their respective holders.

This directory contains a collection of demonstration scripts for
various aspects of Python programming.

beer.py        Well-known programming example: Bottles of beer.
eiffel.py      Python advanced magic: A metaclass for Eiffel post/preconditions.
hanoi.py       Well-known programming example: Towers of Hanoi.
life.py        Curses programming: Simple game-of-life.
markov.py      Algorithms: Markov chain simulation.
mcast.py       Network programming: Send and receive UDP multicast packets.
queens.py      Well-known programming example: N-Queens problem.
redemo.py      Regular Expressions: GUI script to test regexes.
rpython.py     Network programming: Small client for remote code execution.
rpythond.py    Network programming: Small server for remote code execution.
sortvisu.py    GUI programming: Visualization of different sort algorithms.
ss1.py         GUI/Application programming: A simple spreadsheet application.
vector.py      Python basics: A vector class with demonstrating special methods.
THE FREEZE SCRIPT
=================

(Directions for Windows are at the end of this file.)


What is Freeze?
---------------

Freeze make it possible to ship arbitrary Python programs to people
who don't have Python.  The shipped file (called a "frozen" version of
your Python program) is an executable, so this only works if your
platform is compatible with that on the receiving end (this is usually
a matter of having the same major operating system revision and CPU
type).

The shipped file contains a Python interpreter and large portions of
the Python run-time.  Some measures have been taken to avoid linking
unneeded modules, but the resulting binary is usually not small.

The Python source code of your program (and of the library modules
written in Python that it uses) is not included in the binary --
instead, the compiled byte-code (the instruction stream used
internally by the interpreter) is incorporated.  This gives some
protection of your Python source code, though not much -- a
disassembler for Python byte-code is available in the standard Python
library.  At least someone running "strings" on your binary won't see
the source.


How does Freeze know which modules to include?
----------------------------------------------

Previous versions of Freeze used a pretty simple-minded algorithm to
find the modules that your program uses, essentially searching for
lines starting with the word "import".  It was pretty easy to trick it
into making mistakes, either missing valid import statements, or
mistaking string literals (e.g. doc strings) for import statements.

This has been remedied: Freeze now uses the regular Python parser to
parse the program (and all its modules) and scans the generated byte
code for IMPORT instructions.  It may still be confused -- it will not
know about calls to the __import__ built-in function, or about import
statements constructed on the fly and executed using the 'exec'
statement, and it will consider import statements even when they are
unreachable (e.g. "if 0: import foobar").

This new version of Freeze also knows about Python's new package
import mechanism, and uses exactly the same rules to find imported
modules and packages.  One exception: if you write 'from package
import *', Python will look into the __all__ variable of the package
to determine which modules are to be imported, while Freeze will do a
directory listing.

One tricky issue: Freeze assumes that the Python interpreter and
environment you're using to run Freeze is the same one that would be
used to run your program, which should also be the same whose sources
and installed files you will learn about in the next section.  In
particular, your PYTHONPATH setting should be the same as for running
your program locally.  (Tip: if the program doesn't run when you type
"python hello.py" there's little chance of getting the frozen version
to run.)


How do I use Freeze?
--------------------

Normally, you should be able to use it as follows:

	python freeze.py hello.py

where hello.py is your program and freeze.py is the main file of
Freeze (in actuality, you'll probably specify an absolute pathname
such as /usr/joe/python/Tools/freeze/freeze.py).


What do I do next?
------------------

Freeze creates a number of files: frozen.c, config.c and Makefile,
plus one file for each Python module that gets included named
M_<module>.c.  To produce the frozen version of your program, you can
simply type "make".  This should produce a binary file.  If the
filename argument to Freeze was "hello.py", the binary will be called
"hello".

Note: you can use the -o option to freeze to specify an alternative
directory where these files are created. This makes it easier to
clean up after you've shipped the frozen binary.  You should invoke
"make" in the given directory.


Freezing Tkinter programs
-------------------------

Unfortunately, it is currently not possible to freeze programs that
use Tkinter without a Tcl/Tk installation. The best way to ship a
frozen Tkinter program is to decide in advance where you are going
to place the Tcl and Tk library files in the distributed setup, and
then declare these directories in your frozen Python program using
the TCL_LIBRARY, TK_LIBRARY and TIX_LIBRARY environment variables.

For example, assume you will ship your frozen program in the directory 
<root>/bin/windows-x86 and will place your Tcl library files 
in <root>/lib/tcl8.2 and your Tk library files in <root>/lib/tk8.2. Then
placing the following lines in your frozen Python script before importing
Tkinter or Tix would set the environment correctly for Tcl/Tk/Tix:

import os
import os.path
RootDir = os.path.dirname(os.path.dirname(os.getcwd()))

import sys
if sys.platform == "win32":
   sys.path = ['', '..\\..\\lib\\python-2.0']
   os.environ['TCL_LIBRARY'] = RootDir + '\\lib\\tcl8.2'
   os.environ['TK_LIBRARY'] = RootDir + '\\lib\\tk8.2'
   os.environ['TIX_LIBRARY'] = RootDir + '\\lib\\tix8.1'
elif sys.platform == "linux2":
   sys.path = ['', '../../lib/python-2.0']
   os.environ['TCL_LIBRARY'] = RootDir + '/lib/tcl8.2'
   os.environ['TK_LIBRARY'] = RootDir + '/lib/tk8.2'
   os.environ['TIX_LIBRARY'] = RootDir + '/lib/tix8.1'
elif sys.platform == "solaris":
   sys.path = ['', '../../lib/python-2.0']
   os.environ['TCL_LIBRARY'] = RootDir + '/lib/tcl8.2'
   os.environ['TK_LIBRARY'] = RootDir + '/lib/tk8.2'
   os.environ['TIX_LIBRARY'] = RootDir + '/lib/tix8.1'

This also adds <root>/lib/python-2.0 to your Python path
for any Python files such as _tkinter.pyd you may need.

Note that the dynamic libraries (such as tcl82.dll tk82.dll python20.dll
under Windows, or libtcl8.2.so and libtcl8.2.so under Unix) are required
at program load time, and are searched by the operating system loader
before Python can be started. Under Windows, the environment
variable PATH is consulted, and under Unix, it may be the
environment variable LD_LIBRARY_PATH and/or the system
shared library cache (ld.so). An additional preferred directory for
finding the dynamic libraries is built into the .dll or .so files at
compile time - see the LIB_RUNTIME_DIR variable in the Tcl makefile. 
The OS must find the dynamic libraries or your frozen program won't start. 
Usually I make sure that the .so or .dll files are in the same directory
as the executable, but this may not be foolproof.

A workaround to installing your Tcl library files with your frozen
executable would be possible, in which the Tcl/Tk library files are
incorporated in a frozen Python module as string literals and written
to a temporary location when the program runs; this is currently left
as an exercise for the reader.  An easier approach is to freeze the
Tcl/Tk/Tix code into the dynamic libraries using the Tcl ET code,
or the Tix Stand-Alone-Module code. Of course, you can also simply 
require that Tcl/Tk is required on the target installation, but be 
careful that the version corresponds.

There are some caveats using frozen Tkinter applications:
	Under Windows if you use the -s windows option, writing
to stdout or stderr is an error.
	The Tcl [info nameofexecutable] will be set to where the
program was frozen, not where it is run from.
	The global variables argc and argv do not exist.


A warning about shared library modules
--------------------------------------

When your Python installation uses shared library modules such as 
_tkinter.pyd, these will not be incorporated in the frozen program.
 Again, the frozen program will work when you test it, but it won't
 work when you ship it to a site without a Python installation.

Freeze prints a warning when this is the case at the end of the
freezing process:

	Warning: unknown modules remain: ...

When this occurs, the best thing to do is usually to rebuild Python
using static linking only. Or use the approach described in the previous
section to declare a library path using sys.path, and place the modules
such as _tkinter.pyd there.


Troubleshooting
---------------

If you have trouble using Freeze for a large program, it's probably
best to start playing with a really simple program first (like the file
hello.py).  If you can't get that to work there's something
fundamentally wrong -- perhaps you haven't installed Python.  To do a
proper install, you should do "make install" in the Python root
directory.


Usage under Windows 95 or NT
----------------------------

Under Windows 95 or NT, you *must* use the -p option and point it to
the top of the Python source tree.

WARNING: the resulting executable is not self-contained; it requires
the Python DLL, currently PYTHON20.DLL (it does not require the
standard library of .py files though).  It may also require one or
more extension modules loaded from .DLL or .PYD files; the module
names are printed in the warning message about remaining unknown
modules.

The driver script generates a Makefile that works with the Microsoft
command line C compiler (CL).  To compile, run "nmake"; this will
build a target "hello.exe" if the source was "hello.py".  Only the
files frozenmain.c and frozen.c are used; no config.c is generated or
used, since the standard DLL is used.

In order for this to work, you must have built Python using the VC++
(Developer Studio) 5.0 compiler.  The provided project builds
python20.lib in the subdirectory pcbuild\Release of thje Python source
tree, and this is where the generated Makefile expects it to be.  If
this is not the case, you can edit the Makefile or (probably better)
winmakemakefile.py (e.g., if you are using the 4.2 compiler, the
python20.lib file is generated in the subdirectory vc40 of the Python
source tree).

It is possible to create frozen programs that don't have a console
window, by specifying the option '-s windows'. See the Usage below.

Usage
-----

Here is a list of all of the options (taken from freeze.__doc__):

usage: freeze [options...] script [module]...

Options:
-p prefix:    This is the prefix used when you ran ``make install''
              in the Python build directory.
              (If you never ran this, freeze won't work.)
              The default is whatever sys.prefix evaluates to.
              It can also be the top directory of the Python source
              tree; then -P must point to the build tree.

-P exec_prefix: Like -p but this is the 'exec_prefix', used to
                install objects etc.  The default is whatever sys.exec_prefix
                evaluates to, or the -p argument if given.
                If -p points to the Python source tree, -P must point
                to the build tree, if different.

-e extension: A directory containing additional .o files that
              may be used to resolve modules.  This directory
              should also have a Setup file describing the .o files.
              On Windows, the name of a .INI file describing one
              or more extensions is passed.
              More than one -e option may be given.

-o dir:       Directory where the output files are created; default '.'.

-m:           Additional arguments are module names instead of filenames.

-a package=dir: Additional directories to be added to the package's
                __path__.  Used to simulate directories added by the
                package at runtime (eg, by OpenGL and win32com).
                More than one -a option may be given for each package.

-l file:      Pass the file to the linker (windows only)

-d:           Debugging mode for the module finder.

-q:           Make the module finder totally quiet.

-h:           Print this help message.

-x module     Exclude the specified module.

-i filename:  Include a file with additional command line options.  Used
              to prevent command lines growing beyond the capabilities of
              the shell/OS.  All arguments specified in filename
              are read and the -i option replaced with the parsed
              params (note - quoting args in this file is NOT supported)

-s subsystem: Specify the subsystem (For Windows only.); 
              'console' (default), 'windows', 'service' or 'com_dll'
              
-w:           Toggle Windows (NT or 95) behavior.
              (For debugging only -- on a win32 platform, win32 behavior
              is automatic.)

Arguments:

script:       The Python script to be executed by the resulting binary.

module ...:   Additional Python modules (referenced by pathname)
              that will be included in the resulting binary.  These
              may be .py or .pyc files.  If -m is specified, these are
              module names that are search in the path instead.



--Guido van Rossum (home page: http://www.python.org/~guido/)

Importbench is a set of micro-benchmarks for various import scenarios.

It should not be used as an overall benchmark of import performance, but rather
an easy way to measure impact of possible code changes. For a real-world
benchmark of import, use the normal_startup benchmark from
hg.python.org/benchmarks.

Packaging Python as a Microsoft Installer Package (MSI)
=======================================================

Using this library, Python can be packaged as a MS-Windows
MSI file. To generate an installer package, you need
a build tree. By default, the build tree root directory
is assumed to be in "../..". This location can be changed
by adding a file config.py; see the beginning of msi.py
for additional customization options.

The packaging process assumes that binaries have been 
generated according to the instructions in PCBuild/README.txt,
and that you have either Visual Studio or the Platform SDK
installed. In addition, you need the Python COM extensions,
either from PythonWin, or from ActivePython.

To invoke the script, open a cmd.exe window which has 
cabarc.exe in its PATH (e.g. "Visual Studio .NET 2003
Command Prompt"). Then invoke

<path-to-python.exe> msi.py

If everything succeeds, pythonX.Y.Z.msi is generated
in the current directory.


________________________________________________________________________

PYBENCH - A Python Benchmark Suite
________________________________________________________________________

     Extendable suite of low-level benchmarks for measuring
          the performance of the Python implementation 
                 (interpreter, compiler or VM).

pybench is a collection of tests that provides a standardized way to
measure the performance of Python implementations. It takes a very
close look at different aspects of Python programs and let's you
decide which factors are more important to you than others, rather
than wrapping everything up in one number, like the other performance
tests do (e.g. pystone which is included in the Python Standard
Library).

pybench has been used in the past by several Python developers to
track down performance bottlenecks or to demonstrate the impact of
optimizations and new features in Python.

The command line interface for pybench is the file pybench.py. Run
this script with option '--help' to get a listing of the possible
options. Without options, pybench will simply execute the benchmark
and then print out a report to stdout.


Micro-Manual
------------

Run 'pybench.py -h' to see the help screen.  Run 'pybench.py' to run
the benchmark suite using default settings and 'pybench.py -f <file>'
to have it store the results in a file too.

It is usually a good idea to run pybench.py multiple times to see
whether the environment, timers and benchmark run-times are suitable
for doing benchmark tests. 

You can use the comparison feature of pybench.py ('pybench.py -c
<file>') to check how well the system behaves in comparison to a
reference run. 

If the differences are well below 10% for each test, then you have a
system that is good for doing benchmark testings.  Of you get random
differences of more than 10% or significant differences between the
values for minimum and average time, then you likely have some
background processes running which cause the readings to become
inconsistent. Examples include: web-browsers, email clients, RSS
readers, music players, backup programs, etc.

If you are only interested in a few tests of the whole suite, you can
use the filtering option, e.g. 'pybench.py -t string' will only
run/show the tests that have 'string' in their name.

This is the current output of pybench.py --help:

"""
------------------------------------------------------------------------
PYBENCH - a benchmark test suite for Python interpreters/compilers.
------------------------------------------------------------------------

Synopsis:
 pybench.py [option] files...

Options and default settings:
  -n arg           number of rounds (10)
  -f arg           save benchmark to file arg ()
  -c arg           compare benchmark with the one in file arg ()
  -s arg           show benchmark in file arg, then exit ()
  -w arg           set warp factor to arg (10)
  -t arg           run only tests with names matching arg ()
  -C arg           set the number of calibration runs to arg (20)
  -d               hide noise in comparisons (0)
  -v               verbose output (not recommended) (0)
  --with-gc        enable garbage collection (0)
  --with-syscheck  use default sys check interval (0)
  --timer arg      use given timer (time.time)
  -h               show this help text
  --help           show this help text
  --debug          enable debugging
  --copyright      show copyright
  --examples       show examples of usage

Version:
 2.1

The normal operation is to run the suite and display the
results. Use -f to save them for later reuse or comparisons.

Available timers:

   time.time
   time.clock
   systimes.processtime

Examples:

python3.0 pybench.py -f p30.pybench
python3.1 pybench.py -f p31.pybench
python pybench.py -s p31.pybench -c p30.pybench
"""

License
-------

See LICENSE file.


Sample output
-------------

"""
-------------------------------------------------------------------------------
PYBENCH 2.1
-------------------------------------------------------------------------------
* using CPython 3.0
* disabled garbage collection
* system check interval set to maximum: 2147483647
* using timer: time.time

Calibrating tests. Please wait...

Running 10 round(s) of the suite at warp factor 10:

* Round 1 done in 6.388 seconds.
* Round 2 done in 6.485 seconds.
* Round 3 done in 6.786 seconds.
...
* Round 10 done in 6.546 seconds.

-------------------------------------------------------------------------------
Benchmark: 2006-06-12 12:09:25
-------------------------------------------------------------------------------

    Rounds: 10
    Warp:   10
    Timer:  time.time

    Machine Details:
       Platform ID:  Linux-2.6.8-24.19-default-x86_64-with-SuSE-9.2-x86-64
       Processor:    x86_64

    Python:
       Implementation: CPython
       Executable:   /usr/local/bin/python
       Version:      3.0
       Compiler:     GCC 3.3.4 (pre 3.3.5 20040809)
       Bits:         64bit
       Build:        Oct  1 2005 15:24:35 (#1)
       Unicode:      UCS2


Test                             minimum  average  operation  overhead
-------------------------------------------------------------------------------
          BuiltinFunctionCalls:    126ms    145ms    0.28us    0.274ms
           BuiltinMethodLookup:    124ms    130ms    0.12us    0.316ms
                 CompareFloats:    109ms    110ms    0.09us    0.361ms
         CompareFloatsIntegers:    100ms    104ms    0.12us    0.271ms
               CompareIntegers:    137ms    138ms    0.08us    0.542ms
        CompareInternedStrings:    124ms    127ms    0.08us    1.367ms
                  CompareLongs:    100ms    104ms    0.10us    0.316ms
                CompareStrings:    111ms    115ms    0.12us    0.929ms
                CompareUnicode:    108ms    128ms    0.17us    0.693ms
                 ConcatStrings:    142ms    155ms    0.31us    0.562ms
                 ConcatUnicode:    119ms    127ms    0.42us    0.384ms
               CreateInstances:    123ms    128ms    1.14us    0.367ms
            CreateNewInstances:    121ms    126ms    1.49us    0.335ms
       CreateStringsWithConcat:    130ms    135ms    0.14us    0.916ms
       CreateUnicodeWithConcat:    130ms    135ms    0.34us    0.361ms
                  DictCreation:    108ms    109ms    0.27us    0.361ms
             DictWithFloatKeys:    149ms    153ms    0.17us    0.678ms
           DictWithIntegerKeys:    124ms    126ms    0.11us    0.915ms
            DictWithStringKeys:    114ms    117ms    0.10us    0.905ms
                      ForLoops:    110ms    111ms    4.46us    0.063ms
                    IfThenElse:    118ms    119ms    0.09us    0.685ms
                   ListSlicing:    116ms    120ms    8.59us    0.103ms
                NestedForLoops:    125ms    137ms    0.09us    0.019ms
          NormalClassAttribute:    124ms    136ms    0.11us    0.457ms
       NormalInstanceAttribute:    110ms    117ms    0.10us    0.454ms
           PythonFunctionCalls:    107ms    113ms    0.34us    0.271ms
             PythonMethodCalls:    140ms    149ms    0.66us    0.141ms
                     Recursion:    156ms    166ms    3.32us    0.452ms
                  SecondImport:    112ms    118ms    1.18us    0.180ms
           SecondPackageImport:    118ms    127ms    1.27us    0.180ms
         SecondSubmoduleImport:    140ms    151ms    1.51us    0.180ms
       SimpleComplexArithmetic:    128ms    139ms    0.16us    0.361ms
        SimpleDictManipulation:    134ms    136ms    0.11us    0.452ms
         SimpleFloatArithmetic:    110ms    113ms    0.09us    0.571ms
      SimpleIntFloatArithmetic:    106ms    111ms    0.08us    0.548ms
       SimpleIntegerArithmetic:    106ms    109ms    0.08us    0.544ms
        SimpleListManipulation:    103ms    113ms    0.10us    0.587ms
          SimpleLongArithmetic:    112ms    118ms    0.18us    0.271ms
                    SmallLists:    105ms    116ms    0.17us    0.366ms
                   SmallTuples:    108ms    128ms    0.24us    0.406ms
         SpecialClassAttribute:    119ms    136ms    0.11us    0.453ms
      SpecialInstanceAttribute:    143ms    155ms    0.13us    0.454ms
                StringMappings:    115ms    121ms    0.48us    0.405ms
              StringPredicates:    120ms    129ms    0.18us    2.064ms
                 StringSlicing:    111ms    127ms    0.23us    0.781ms
                     TryExcept:    125ms    126ms    0.06us    0.681ms
                TryRaiseExcept:    133ms    137ms    2.14us    0.361ms
                  TupleSlicing:    117ms    120ms    0.46us    0.066ms
               UnicodeMappings:    156ms    160ms    4.44us    0.429ms
             UnicodePredicates:    117ms    121ms    0.22us    2.487ms
             UnicodeProperties:    115ms    153ms    0.38us    2.070ms
                UnicodeSlicing:    126ms    129ms    0.26us    0.689ms
-------------------------------------------------------------------------------
Totals:                           6283ms   6673ms
"""
________________________________________________________________________

Writing New Tests
________________________________________________________________________

pybench tests are simple modules defining one or more pybench.Test
subclasses.

Writing a test essentially boils down to providing two methods:
.test() which runs .rounds number of .operations test operations each
and .calibrate() which does the same except that it doesn't actually
execute the operations.


Here's an example:
------------------

from pybench import Test

class IntegerCounting(Test):

    # Version number of the test as float (x.yy); this is important
    # for comparisons of benchmark runs - tests with unequal version
    # number will not get compared.
    version = 1.0
    
    # The number of abstract operations done in each round of the
    # test. An operation is the basic unit of what you want to
    # measure. The benchmark will output the amount of run-time per
    # operation. Note that in order to raise the measured timings
    # significantly above noise level, it is often required to repeat
    # sets of operations more than once per test round. The measured
    # overhead per test round should be less than 1 second.
    operations = 20

    # Number of rounds to execute per test run. This should be
    # adjusted to a figure that results in a test run-time of between
    # 1-2 seconds (at warp 1).
    rounds = 100000

    def test(self):

	""" Run the test.

	    The test needs to run self.rounds executing
	    self.operations number of operations each.

        """
        # Init the test
        a = 1

        # Run test rounds
	#
        for i in range(self.rounds):

            # Repeat the operations per round to raise the run-time
            # per operation significantly above the noise level of the
            # for-loop overhead. 

	    # Execute 20 operations (a += 1):
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1

    def calibrate(self):

	""" Calibrate the test.

	    This method should execute everything that is needed to
	    setup and run the test - except for the actual operations
	    that you intend to measure. pybench uses this method to
            measure the test implementation overhead.

        """
        # Init the test
        a = 1

        # Run test rounds (without actually doing any operation)
        for i in range(self.rounds):

	    # Skip the actual execution of the operations, since we
	    # only want to measure the test's administration overhead.
            pass

Registering a new test module
-----------------------------

To register a test module with pybench, the classes need to be
imported into the pybench.Setup module. pybench will then scan all the
symbols defined in that module for subclasses of pybench.Test and
automatically add them to the benchmark suite.


Breaking Comparability
----------------------

If a change is made to any individual test that means it is no
longer strictly comparable with previous runs, the '.version' class
variable should be updated. Therefafter, comparisons with previous
versions of the test will list as "n/a" to reflect the change.


Version History
---------------

  2.1: made some minor changes for compatibility with Python 3.0:
        - replaced cmp with divmod and range with max in Calls.py
          (cmp no longer exists in 3.0, and range is a list in
          Python 2.x and an iterator in Python 3.x)

  2.0: rewrote parts of pybench which resulted in more repeatable
       timings:
        - made timer a parameter
        - changed the platform default timer to use high-resolution
          timers rather than process timers (which have a much lower
          resolution)
        - added option to select timer
        - added process time timer (using systimes.py)
        - changed to use min() as timing estimator (average
          is still taken as well to provide an idea of the difference)
        - garbage collection is turned off per default
        - sys check interval is set to the highest possible value
        - calibration is now a separate step and done using
          a different strategy that allows measuring the test
          overhead more accurately
        - modified the tests to each give a run-time of between
          100-200ms using warp 10
        - changed default warp factor to 10 (from 20)
        - compared results with timeit.py and confirmed measurements
        - bumped all test versions to 2.0
        - updated platform.py to the latest version
        - changed the output format a bit to make it look
          nicer
        - refactored the APIs somewhat
  1.3+: Steve Holden added the NewInstances test and the filtering 
       option during the NeedForSpeed sprint; this also triggered a long 
       discussion on how to improve benchmark timing and finally
       resulted in the release of 2.0
  1.3: initial checkin into the Python SVN repository


Have fun,
--
Marc-Andre Lemburg
mal@lemburg.com

Pynche - The PYthonically Natural Color and Hue Editor

Contact: Barry A. Warsaw
Email:   bwarsaw@python.org
Version: 1.3

Introduction

    Pynche is a color editor based largely on a similar program that I
    originally wrote back in 1987 for the Sunview window system.  That
    editor was called ICE, the Interactive Color Editor.  I'd always
    wanted to port this program to X but didn't feel like hacking X
    and C code to do it.  Fast forward many years, to where Python +
    Tkinter provides such a nice programming environment, with enough
    power, that I finally buckled down and re-implemented it.  I
    changed the name because these days, too many other systems have
    the acronym `ICE'.

    Pynche should work with any variant of Python after 1.5.2
    (e.g. 2.0.1 and 2.1.1), using Tk 8.0.x.  It's been tested on
    Solaris 2.6, Windows NT 4, and various Linux distros.  You'll want
    to be sure to have at least Tk 8.0.3 for Windows.  Also, Pynche is
    very colormap intensive, so it doesn't work very well on 8-bit
    graphics cards; 24bit+ graphics cards are so cheap these days,
    I'll probably never "fix" that.

    Pynche must find a text database of colors names in order to
    provide `nearest' color matching.  Pynche is distributed with an
    rgb.txt file from the X11R6.4 distribution for this reason, along
    with other "Web related" database (see below).  You can use a
    different file with the -d option.  The file xlicense.txt contains
    the license only for rgb.txt and both files are in the X/
    subdirectory.

    Pynche is pronounced: Pin'-chee


Running Standalone

    On Unix, start it by running the `pynche' script.  On Windows, run
    pynche.pyw to inhibit the console window.  When run from the
    command line, the following options are recognized:

    --database file
    -d file
        Alternate location of the color database file.  Without this
        option, the first valid file found will be used (see below).

    --initfile file
    -i file
        Alternate location of the persistent initialization file.  See 
        the section on Persistency below.

    --ignore
    -X
        Ignore the persistent initialization file when starting up.
        Pynche will still write the current option settings to the
        persistent init file when it quits.

    --help
    -h
        Print the help message.

    initialcolor
        a Tk color name or #rrggbb color spec to be used as the
        initially selected color.  This overrides any color saved in
        the persistent init file.  Since `#' needs to be escaped in
        many shells, it is optional in the spec (e.g. #45dd1f is the
        same as 45dd1f).


Running as a Modal Dialog

    Pynche can be run as a modal dialog, inside another application,
    say as a general color chooser.  In fact, Grail 0.6 uses Pynche
    and a future version of IDLE may as well.  Pynche supports the API
    implemented by the Tkinter standard tkColorChooser module, with a
    few changes as described below.  By importing pyColorChooser from
    the Pynche package, you can run

        pyColorChooser.askcolor()

    which will popup Pynche as a modal dialog, and return the selected 
    color.

    There are some UI differences when running as a modal
    vs. standalone.  When running as a modal, there is no "Quit" menu
    item under the "File" menu.  Instead there are "Okay" and "Cancel"
    buttons.

    When "Okay" is hit, askcolor() returns the tuple

        ((r, g, b), "name")

    where r, g, and b are red, green, and blue color values
    respectively (in the range 0 to 255).  "name" will be a color name
    from the color database if there is an exact match, otherwise it
    will be an X11 color spec of the form "#rrggbb".  Note that this
    is different than tkColorChooser, which doesn't know anything
    about color names.

    askcolor() supports the following optional keyword arguments:

        color
            the color to set as the initial selected color

        master[*]
            the master window to use as the parent of the modal
            dialog.  Without this argument, pyColorChooser will create 
            its own Tkinter.Tk instance as the master.  This may not
            be what you want.

        databasefile
            similar to the --database option, the value must be a
            file name

        initfile[*]
            similar to the --initfile option, the value must be a
            file name

        ignore[*]
            similar to the --ignore flag, the value is a boolean

        wantspec
            When this is true, the "name" field in the return tuple
            will always be a color spec of the form "#rrggbb".  It
            will not return a color name even if there is a match;
            this is so pyColorChooser can exactly match the API of
            tkColorChooser.

        [*] these arguments must be specified the first time
        askcolor() is used and cannot be changed on subsequent calls.


The Colorstrip Window

    The top part of the main Pynche window contains the "variation
    strips".  Each strip contains a number of "color chips".  The
    strips always indicate the currently selected color by a highlight
    rectangle around the selected color chip, with an arrow pointing
    to the chip.  Each arrow has an associated number giving you the
    color value along the variation's axis.  Each variation strip
    shows you the colors that are reachable from the selected color by
    varying just one axis of the color solid.

    For example, when the selected color is (in Red/Green/Blue
    notation) 127/127/127, the Red Variations strip shows you every
    color in the range 0/127/127 to 255/127/127.  Similarly for the
    green and blue axes.  You can select any color by clicking on its
    chip.  This will update the highlight rectangle and the arrow, as
    well as other displays in Pynche.

    Click on "Update while dragging" if you want Pynche to update the
    selected color while you drag along any variation strip (this will
    be a bit slower).  Click on "Hexadecimal" to display the arrow
    numbers in hex.

    There are also two shortcut buttons in this window, which
    auto-select Black (0/0/0) and White (255/255/255).


The Proof Window

    In the lower left corner of the main window you see two larger
    color chips.  The Selected chip shows you a larger version of the
    color selected in the variation strips, along with its X11 color
    specification.  The Nearest chip shows you the closest color in
    the X11 database to the selected color, giving its X11 color
    specification, and below that, its X11 color name.  When the
    Selected chip color exactly matches the Nearest chip color, you
    will see the color name appear below the color specification for
    the Selected chip.
    
    Clicking on the Nearest color chip selects that color.  Color
    distance is calculated in the 3D space of the RGB color solid and
    if more than one color name is the same distance from the selected
    color, the first one found will be chosen.

    Note that there may be more than one X11 color name for the same
    RGB value.  In that case, the first one found in the text database
    is designated the "primary" name, and this is shown under the
    Nearest chip.  The other names are "aliases" and they are visible
    in the Color List Window (see below).

    Both the color specifications and color names are selectable for
    copying and pasting into another window.


The Type-in Window

    At the lower right of the main window are three entry fields.
    Here you can type numeric values for any of the three color axes.
    Legal values are between 0 and 255, and these fields do not allow
    you to enter illegal values.  You must hit Enter or Tab to select
    the new color.

    Click on "Update while typing" if you want Pynche to select the
    color on every keystroke (well, every one that produces a legal
    value!)  Click on "Hexadecimal" to display and enter color values
    in hex.


Other Views

    There are three secondary windows which are not displayed by
    default.  You can bring these up via the "View" menu on the main
    Pynche window.


The Text Window

    The "Text Window" allows you to see what effects various colors
    have on the standard Tk text widget elements.  In the upper part
    of the window is a plain Tk text widget and here you can edit the
    text, select a region of text, etc.  Below this is a button "Track
    color changes".  When this is turned on, any colors selected in
    the other windows will change the text widget element specified in
    the radio buttons below.  When this is turned off, text widget
    elements are not affected by color selection.

    You can choose which element gets changed by color selection by
    clicking on one of the radio buttons in the bottom part of this
    window.  Text foreground and background affect the text in the
    upper part of the window.  Selection foreground and background
    affect the colors of the primary selection which is what you see
    when you click the middle button (depending on window system) and
    drag it through some text.

    The Insertion is the insertion cursor in the text window, where
    new text will be inserted as you type.  The insertion cursor only
    has a background.


The Color List Window

    The "Color List" window shows every named color in the color name
    database (this window may take a while to come up).  In the upper
    part of the window you see a scrolling list of all the color names
    in the database, in alphabetical order.  Click on any color to
    select it.  In the bottom part of the window is displayed any
    aliases for the selected color (those color names that have the
    same RGB value, but were found later in the text database).  For
    example, find the color "Black" and you'll see that its aliases
    are "gray0" and "grey0".

    If the color has no aliases you'll see "<no aliases>" here.  If you
    just want to see if a color has an alias, and do not want to select a
    color when you click on it, turn off "Update on Click".

    Note that the color list is always updated when a color is selected
    from the main window.  There's no way to turn this feature off.  If
    the selected color has no matching color name you'll see
    "<no matching color>" in the Aliases window.


The Details Window

    The "Details" window gives you more control over color selection
    than just clicking on a color chip in the main window.  The row of
    buttons along the top apply the specified increment and decrement
    amounts to the selected color.  These delta amounts are applied to
    the variation strips specified by the check boxes labeled "Move
    Sliders".  Thus if just Red and Green are selected, hitting -10
    will subtract 10 from the color value along the red and green
    variation only.  Note the message under the checkboxes; this
    indicates the primary color level being changed when more than one
    slider is tied together.  For example, if Red and Green are
    selected, you will be changing the Yellow level of the selected
    color.

    The "At Boundary" behavior determines what happens when any color
    variation hits either the lower or upper boundaries (0 or 255) as
    a result of clicking on the top row buttons:

    Stop
        When the increment or decrement would send any of the tied
        variations out of bounds, the entire delta is discarded.

    Wrap Around
        When the increment or decrement would send any of the tied
        variations out of bounds, the out of bounds value is wrapped
        around to the other side.  Thus if red were at 238 and +25
        were clicked, red would have the value 7.

    Preserve Distance
        When the increment or decrement would send any of the tied
        variations out of bounds, all tied variations are wrapped as
        one, so as to preserve the distance between them.  Thus if
        green and blue were tied, and green was at 238 while blue was
        at 223, and +25 were clicked, green would be at 15 and blue
        would be at 0.

    Squash
        When the increment or decrement would send any of the tied
        variations out of bounds, the out of bounds variation is set
        to the ceiling of 255 or floor of 0, as appropriate.  In this
        way, all tied variations are squashed to one edge or the
        other.

    The top row buttons have the following keyboard accelerators:

    -25 == Shift Left Arrow
    -10 == Control Left Arrow
     -1 == Left Arrow
     +1 == Right Arrow
    +10 == Control Right Arrow
    +25 == Shift Right Arrow


Keyboard Accelerators

    Alt-w in any secondary window dismisses the window.  In the main
    window it exits Pynche (except when running as a modal).

    Alt-q in any window exits Pynche (except when running as a modal).


Persistency

    Pynche remembers various settings of options and colors between
    invocations, storing these values in a `persistent initialization
    file'.  The actual location of this file is specified by the
    --initfile option (see above), and defaults to ~/.pynche.

    When Pynche exits, it saves these values in the init file, and
    re-reads them when it starts up.  There is no locking on this
    file, so if you run multiple instances of Pynche at a time, you
    may clobber the init file.

    The actual options stored include

    - the currently selected color

    - all settings of checkbox and radio button options in all windows

    - the contents of the text window, the current text selection and
      insertion point, and all current text widget element color
      settings.

    - the name of the color database file (but not its contents)

    You can inhibit Pynche from reading the init file by supplying the
    --ignore option on the command line.  However, you cannot suppress
    the storing of the settings in the init file on Pynche exit.  If
    you really want to do this, use /dev/null as the init file, using
    --initfile.


Color Name Database Files

    Pynche uses a color name database file to calculate the nearest
    color to the selected color, and to display in the Color List
    view.  Several files are distributed with Pynche, described
    below.  By default, the X11 color name database file is selected.
    Other files:

    html40colors.txt -- the HTML 4.0 guaranteed color names

    websafe.txt -- the 216 "Web-safe" colors that Netscape and MSIE
    guarantee will not be dithered.  These are specified in #rrggbb
    format for both values and names

    webcolors.txt -- The 140 color names that Tim Peters and his
    sister say NS and MSIE both understand (with some controversy over 
    AliceBlue).

    namedcolors.txt -- an alternative set of Netscape colors.

    You can switch between files by choosing "Load palette..." from
    the "File" menu.  This brings up a standard Tk file dialog.
    Choose the file you want and then click "Ok".  If Pynche
    understands the format in this file, it will load the database and 
    update the appropriate windows.  If not, it will bring up an error 
    dialog.


To Do

    Here's a brief list of things I want to do (some mythical day):

    - Better support for resizing the top level windows

    - More output views, e.g. color solids

    - Have the notion of a `last color selected'; this may require a
      new output view

    - Support setting the font in the text view

    - Support distutils setup.py for installation

    I'm open to suggestions!



Local Variables:
indent-tabs-mode: nil
End:

This directory contains a number of Python programs that are useful
while building or extending Python.

buildbot        Batchfiles for running on Windows buildslaves.

ccbench         A Python threads-based concurrency benchmark. (*)

demo            Several Python programming demos.

freeze          Create a stand-alone executable from a Python program.

gdb             Python code to be run inside gdb, to make it easier to
                debug Python itself (by David Malcolm).

i18n            Tools for internationalization. pygettext.py
                parses Python source code and generates .pot files,
                and msgfmt.py generates a binary message catalog
                from a catalog in text format.

iobench         Benchmark for the new Python I/O system. (*)

msi             Support for packaging Python as an MSI package on Windows.

parser          Un-parsing tool to generate code from an AST.

pybench         Low-level benchmarking for the Python evaluation loop. (*)

pynche          A Tkinter-based color editor.

scripts         A number of useful single-file programs, e.g. tabnanny.py
                by Tim Peters, which checks for inconsistent mixing of
                tabs and spaces, and 2to3, which converts Python 2 code
                to Python 3 code.

stringbench     A suite of micro-benchmarks for various operations on
                strings (both 8-bit and unicode). (*)

test2to3        A demonstration of how to use 2to3 transparently in setup.py.

unicode         Tools for generating unicodedata and codecs from unicode.org
                and other mapping files (by Fredrik Lundh, Marc-Andre Lemburg
                and Martin von Loewis).

unittestgui     A Tkinter based GUI test runner for unittest, with test
                discovery.


(*) A generic benchmark suite is maintained separately at http://hg.python.org/benchmarks/

This directory contains a collection of executable Python scripts that are
useful while building, extending or managing Python.  Some (e.g., dutree or lll)
are also generally useful UNIX tools.

2to3                      Main script for running the 2to3 conversion tool
abitype.py                Converts a C file to use the PEP 384 type definition API
analyze_dxp.py            Analyzes the result of sys.getdxp()
byext.py                  Print lines/words/chars stats of files by extension
byteyears.py              Print product of a file's size and age
checkpyc.py               Check presence and validity of ".pyc" files
cleanfuture.py            Fix redundant Python __future__ statements
combinerefs.py            A helper for analyzing PYTHONDUMPREFS output
copytime.py               Copy one file's atime and mtime to another
crlf.py                   Change CRLF line endings to LF (Windows to Unix)
db2pickle.py              Dump a database file to a pickle
diff.py                   Print file diffs in context, unified, or ndiff formats
dutree.py                 Format du(1) output as a tree sorted by size
eptags.py                 Create Emacs TAGS file for Python modules
finddiv.py                A grep-like tool that looks for division operators
findlinksto.py            Recursively find symbolic links to a given path prefix
findnocoding.py           Find source files which need an encoding declaration
find_recursionlimit.py    Find the maximum recursion limit on this machine
find-uname.py             Look for the given arguments in the sets of all Unicode names
fixcid.py                 Massive identifier substitution on C source files
fixdiv.py                 Tool to fix division operators.
fixheader.py              Add some cpp magic to a C include file
fixnotice.py              Fix the copyright notice in source files
fixps.py                  Fix Python scripts' first line (if #!)
ftpmirror.py              FTP mirror script
get-remote-certificate.py Fetch the certificate that the server(s) are providing in PEM form
google.py                 Open a webbrowser with Google
gprof2html.py             Transform gprof(1) output into useful HTML
h2py.py                   Translate #define's into Python assignments
highlight.py              Python syntax highlighting with HTML output
idle3                     Main program to start IDLE
ifdef.py                  Remove #if(n)def groups from C sources
import_diagnostics.py     Miscellaneous diagnostics for the import system
lfcr.py                   Change LF line endings to CRLF (Unix to Windows)
linktree.py               Make a copy of a tree with links to original files
lll.py                    Find and list symbolic links in current directory
mailerdaemon.py           Parse error messages from mailer daemons (Sjoerd&Jack)
make_ctype.py             Generate ctype.h replacement in stringobject.c
md5sum.py                 Print MD5 checksums of argument files
mkreal.py                 Turn a symbolic link into a real file or directory
ndiff.py                  Intelligent diff between text files (Tim Peters)
nm2def.py                 Create a template for PC/python_nt.def (Marc Lemburg)
objgraph.py               Print object graph from nm output on a library
parseentities.py          Utility for parsing HTML entity definitions
parse_html5_entities.py   Utility for parsing HTML5 entity definitions
patchcheck.py             Perform common checks and cleanup before committing
pathfix.py                Change #!/usr/local/bin/python into something else
pdeps.py                  Print dependencies between Python modules
pickle2db.py              Load a pickle generated by db2pickle.py to a database
pindent.py                Indent Python code, giving block-closing comments
ptags.py                  Create vi tags file for Python modules
pydoc3                    Python documentation browser
pysource.py               Find Python source files
reindent.py               Change .py files to use 4-space indents
reindent-rst.py           Fix-up reStructuredText file whitespace
rgrep.py                  Reverse grep through a file (useful for big logfiles)
run_tests.py              Run the test suite with more sensible default options
serve.py                  Small wsgiref-based web server, used in make serve in Doc
suff.py                   Sort a list of files by suffix
svneol.py                 Set svn:eol-style on all files in directory
texi2html.py              Convert GNU texinfo files into HTML
treesync.py               Synchronize source trees (very idiosyncratic)
untabify.py               Replace tabs with spaces in argument files
which.py                  Find a program in $PATH
win_add2path.py           Add Python to the search path on Windows

stringbench is a set of performance tests comparing byte string
operations with unicode operations.  The two string implementations
are loosely based on each other and sometimes the algorithm for one is
faster than the other.

These test set was started at the Need For Speed sprint in Reykjavik
to identify which string methods could be sped up quickly and to
identify obvious places for improvement.

Here is an example of a benchmark


@bench('"Andrew".startswith("A")', 'startswith single character', 1000)
def startswith_single(STR):
    s1 = STR("Andrew")
    s2 = STR("A")
    s1_startswith = s1.startswith
    for x in _RANGE_1000:
        s1_startswith(s2)

The bench decorator takes three parameters.  The first is a short
description of how the code works.  In most cases this is Python code
snippet.  It is not the code which is actually run because the real
code is hand-optimized to focus on the method being tested.

The second parameter is a group title.  All benchmarks with the same
group title are listed together.  This lets you compare different
implementations of the same algorithm, such as "t in s"
vs. "s.find(t)".

The last is a count.  Each benchmark loops over the algorithm either
100 or 1000 times, depending on the algorithm performance.  The output
time is the time per benchmark call so the reader needs a way to know
how to scale the performance.

These parameters become function attributes.


Here is an example of the output


========== count newlines
38.54   41.60   92.7    ...text.with.2000.newlines.count("\n") (*100)
========== early match, single character
1.14    1.18    96.8    ("A"*1000).find("A") (*1000)
0.44    0.41    105.6   "A" in "A"*1000 (*1000)
1.15    1.17    98.1    ("A"*1000).index("A") (*1000)

The first column is the run time in milliseconds for byte strings.
The second is the run time for unicode strings.  The third is a
percentage; byte time / unicode time.  It's the percentage by which
unicode is faster than byte strings.

The last column contains the code snippet and the repeat count for the
internal benchmark loop.

The times are computed with 'timeit.py' which repeats the test more
and more times until the total time takes over 0.2 seconds, returning
the best time for a single iteration.

The final line of the output is the cumulative time for byte and
unicode strings, and the overall performance of unicode relative to
bytes.  For example

4079.83 5432.25 75.1    TOTAL

However, this has no meaning as it evenly weights every test.


This project demonstrates how a distutils package
can support Python 2.x and Python 3.x from a single
source, using lib2to3.
unittestgui.py is GUI framework and application for use with Python unit 
testing framework. It executes tests written using the framework provided 
by the 'unittest' module.

Based on the original by Steve Purcell, from:

  http://pyunit.sourceforge.net/

Updated for unittest test discovery by Mark Roddy and Python 3
support by Brian Curtin.

For details on how to make your tests work with test discovery,
and for explanations of the configuration options, see the unittest
documentation:

    http://docs.python.org/library/unittest.html#test-discovery


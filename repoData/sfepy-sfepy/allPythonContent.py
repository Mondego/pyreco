__FILENAME__ = build_helpers
"""
Build helpers for setup.py.

Includes package dependency checks and monkey-patch to numpy.distutils
to work with Cython.

Notes
-----
The original version of this file was adapted from NiPy project [1].

[1] http://nipy.sourceforge.net/
"""

# Standard library imports
import os
import shutil
import glob
import fnmatch
from distutils.cmd import Command
from os.path import join as pjoin, dirname
from distutils.command.clean import clean
from distutils.version import LooseVersion
from distutils.dep_util import newer_group
from distutils.errors import DistutilsError

from numpy.distutils.misc_util import appendpath
from numpy.distutils import log

import sfepy.version as INFO

CYTHON_MIN_VERSION = INFO.CYTHON_MIN_VERSION

class NoOptionsDocs(Command):
    user_options = [('None', None, 'this command has no options'),]

    def initialize_options(self):
        pass

    def finalize_options(self):
        pass

def get_sphinx_make_command():
    if os.name in ['posix']:
        return 'make'

    elif os.name in ['nt']:
        return 'make.bat'

    else:
        raise ValueError('unsupported system! (%s)' % os.name)

class SphinxHTMLDocs(NoOptionsDocs):
    description = """generate html docs by Sphinx"""

    def run(self):
        os.chdir('doc')

        try:
            cmd = get_sphinx_make_command()
            os.system(cmd + ' html')

        finally:
            os.chdir('..')

class SphinxPDFDocs(NoOptionsDocs):
    description = """generate pdf docs by Sphinx"""

    def run(self):
        cwd = os.getcwd()

        os.chdir('doc')

        try:
            cmd = get_sphinx_make_command()
            os.system(cmd + ' latex')
            os.chdir('_build/latex')
            os.system(cmd + ' all-pdf')
            os.chdir(cwd)
            try:
                os.remove('doc/sfepy_manual.pdf')

            except:
                pass
            os.rename('doc/_build/latex/SfePy.pdf', 'doc/sfepy_manual.pdf')

        finally:
            os.chdir(cwd)

class DoxygenDocs(NoOptionsDocs):
    description = """generate docs by Doxygen"""

    def run(self):
        try:
            shutil.rmtree('doc/html')

        except OSError:
            pass

        fd_in = open('doc/doxygen.config', 'r')
        fd_out = open('doc/doxygenrc', 'w')
        for line in fd_in:
            aux = line.split('=')
            if len(aux) and (aux[0] == 'PROJECT_NUMBER'):
                line = '='.join([aux[0], INFO.__version__])

            fd_out.write(line)

        fd_out.close()
        fd_in.close()

        os.system('doxygen doc/doxygenrc')

def recursive_glob(top_dir, pattern):
    """
    Utility function working like `glob.glob()`, but working recursively
    and returning generator.

    Parameters
    ----------
    topdir : str
        The top-level directory.
    pattern : str or list of str
        The pattern or list of patterns to match.
    """
    if isinstance(pattern, list):
        for pat in pattern:
            for fn in recursive_glob(top_dir, pat):
                yield fn

    else:
        for dirpath, dirnames, filenames in os.walk(top_dir):
            for fn in [fn for fn in filenames
                       if fnmatch.fnmatchcase(fn, pattern)]:
                yield os.path.join(dirpath, fn)

class Clean(clean):
    """
    Distutils Command class to clean, enhanced to clean also files
    generated during `python setup.py build_ext --inplace`.
    """

    def run(self):
        clean.run(self)

        print 'extra clean:'
        suffixes = ['*.pyc', '*.o', '*.so', '*_wrap.c', '*.bak', '*~', '*%']
        for filename in recursive_glob('sfepy', suffixes):
            print filename
            os.remove(filename)

        for filename in recursive_glob('examples', suffixes):
            print filename
            os.remove(filename)

        for filename in recursive_glob('script', suffixes):
            print filename
            os.remove(filename)

        for filename in recursive_glob('tests', suffixes):
            print filename
            os.remove(filename)

        for filename in glob.glob('*.pyc'):
            print filename
            os.remove(filename)

        for _filename in recursive_glob('sfepy', ['*.pyx']):
            filename = _filename.replace('.pyx', '.c')
            print filename
            try:
                os.remove(filename)
            except OSError:
                pass

            filename = _filename.replace('.pyx', '.html')
            print filename
            try:
                os.remove(filename)
            except OSError:
                pass

# The command classes for distutils, used by setup.py.
cmdclass = {
    'htmldocs' : SphinxHTMLDocs,
    'pdfdocs' : SphinxPDFDocs,
    'doxygendocs' : DoxygenDocs,
    'clean': Clean,
}

def have_good_cython():
    try:
        from Cython.Compiler.Version import version
    except ImportError:
        return False
    return LooseVersion(version) >= LooseVersion(CYTHON_MIN_VERSION)

def generate_a_pyrex_source(self, base, ext_name, source, extension):
    '''
    Monkey patch for numpy build_src.build_src method

    Uses Cython instead of Pyrex.
    '''
    good_cython = have_good_cython()
    if self.inplace or not good_cython:
        target_dir = dirname(base)
    else:
        target_dir = appendpath(self.build_src, dirname(base))
    target_file = pjoin(target_dir, ext_name + '.c')
    depends = [source] + extension.depends
    sources_changed = newer_group(depends, target_file, 'newer')
    if self.force or sources_changed:
        if good_cython:
            # add distribution (package-wide) include directories, in order
            # to pick up needed .pxd files for cython compilation
            incl_dirs = extension.include_dirs[:]
            dist_incl_dirs = self.distribution.include_dirs
            if not dist_incl_dirs is None:
                incl_dirs += dist_incl_dirs
            import Cython.Compiler.Main
            log.info("cythonc:> %s" % (target_file))
            self.mkpath(target_dir)
            options = Cython.Compiler.Main.CompilationOptions(
                defaults=Cython.Compiler.Main.default_options,
                include_path=incl_dirs,
                output_file=target_file)
            cython_result = Cython.Compiler.Main.compile(source,
                                                       options=options)
            if cython_result.num_errors != 0:
                raise DistutilsError("%d errors while compiling "
                                     "%r with Cython"
                                     % (cython_result.num_errors, source))
        elif sources_changed and os.path.isfile(target_file):
            raise DistutilsError("Cython >=%s required for compiling %r"
                                 " because sources (%s) have changed" %
                                 (CYTHON_MIN_VERSION, source,
                                  ','.join(depends)))
        else:
            raise DistutilsError("Cython >=%s required for compiling %r"
                                 " but not available" %
                                 (CYTHON_MIN_VERSION, source))
    return target_file

def package_check(pkg_name, version=None,
                  optional=False,
                  checker=LooseVersion,
                  version_getter=None,
                  messages=None
                  ):
    """
    Check if package `pkg_name` is present, and correct version

    Parameters
    ----------
    pkg_name : str or sequence of str
       name of package as imported into python. Alternative names
       (e.g. for different versions) may be given in a list.
    version : {None, str}, optional
       minimum version of the package that we require. If None, we don't
       check the version.  Default is None
    optional : {False, True}, optional
       If False, raise error for absent package or wrong version;
       otherwise warn
    checker : callable, optional
       callable with which to return comparable thing from version
       string.  Default is ``distutils.version.LooseVersion``
    version_getter : {None, callable}:
       Callable that takes `pkg_name` as argument, and returns the
       package version string - as in::

          ``version = version_getter(pkg_name)``

       If None, equivalent to::

          mod = __import__(pkg_name); version = mod.__version__``
    messages : None or dict, optional
       dictionary giving output messages
    """
    if version_getter is None:
        def version_getter(pkg_name):
            mod = __import__(pkg_name)
            return mod.__version__
    if messages is None:
        messages = {}
    msgs = {
         'missing': 'Cannot import package "%s" - is it installed?',
         'missing opt': 'Missing optional package "%s"',
         'opt suffix' : '; you may get run-time errors',
         'version too old': 'You have version %s of package "%s"'
                            ' but we need version >= %s',
        'no version' : 'cannot determine version of %s!',
    }
    msgs.update(messages)

    if isinstance(pkg_name, str):
        names = [pkg_name]

    else:
        names = pkg_name

    import_ok = False
    for pkg_name in names:
        try:
            __import__(pkg_name)
        except ImportError:
            pass
        else:
            import_ok = True

    if not import_ok:
        if not optional:
            raise RuntimeError(msgs['missing'] % pkg_name)
        log.warn(msgs['missing opt'] % pkg_name +
                 msgs['opt suffix'])
        return

    if not version:
        return
    try:
        have_version = version_getter(pkg_name)
    except AttributeError:
        raise RuntimeError('Cannot find version for %s' % pkg_name)

    if not have_version:
        if optional:
            log.warn(msgs['no version'] % pkg_name)

        else:
            raise RuntimeError(msgs['no version'] % pkg_name)

    elif checker(have_version) < checker(version):
        if optional:
            log.warn(msgs['version too old'] % (have_version,
                                                pkg_name,
                                                version)
                     + msgs['opt suffix'])
        else:
            raise RuntimeError(msgs['version too old'] % (have_version,
                                                          pkg_name,
                                                          version))

########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# SfePy documentation build configuration file, created by
# sphinx-quickstart on Wed Oct 14 00:02:22 2009.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.append(os.path.abspath('.'))

## LS: Below walks parent directory and adds all non-excluded to sys.path
#def add_to_sys_path(arg, dirname, fnames):
#    excludes = ('.git',)
#    for exclude in excludes:
#        if exclude not in dirname:
#            sys.path.append(os.path.abspath(dirname))
#
#doc_dir,conf_file = os.path.split(__file__)
#sfepy_dir = os.path.abspath(os.path.join(doc_dir, os.path.pardir))
#os.path.walk(sfepy_dir, add_to_sys_path, None)
sys.path.append(os.path.abspath('sphinxext'))

# This is needed for gen_term_table.
sys.path.append(os.path.abspath('../script'))

import sfepy

numpydoc_path = sfepy.Config().numpydoc_path()

if numpydoc_path is not None:
    sys.path.append(os.path.abspath(numpydoc_path))

# -- General configuration -----------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autosummary', 'sphinx.ext.autodoc',
              'sphinx.ext.doctest', 'sphinx.ext.pngmath',
              'sphinx.ext.viewcode', 'numpydoc',
              'ipython_console_highlighting', 'gen_term_table']
#extensions = ['sphinx.ext.autodoc']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'SfePy'
copyright = u'2014, Robert Cimrman and Contributors'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = sfepy.__version__
# The full version, including alpha/beta/rc tags.
release = version

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of documents that shouldn't be included in the build.
#unused_docs = []

# List of directories, relative to source directory, that shouldn't be searched
# for source files.
exclude_trees = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  Major themes that come with
# Sphinx are currently 'default' and 'sphinxdoc'.
html_theme = 'sfepy_theme'
# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
html_theme_path = ["."]

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
html_additional_pages = {}
#html_additional_pages = {'index': 'index.html', 'gallery':'gallery.html'}

# If false, no module index is generated.
#html_use_modindex = True

html_domain_indices = ["py-modindex"]

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'SfePydoc'


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'SfePy.tex', u'SfePy Documentation',
   u'Robert Cimrman and Contributors', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''
latex_preamble = r"""
\usepackage{bm}
\usepackage{amsfonts}
\def\dt{{\Delta t}}
\def\pdiff#1#2{\frac{\partial {#1}}{\partial {#2}}}
\def\tdiff#1#2{\frac{{\rm d} {#1}}{{\rm d} {#2}}}
\def\difd#1{\ {\rm d}#1}
\def\intl#1#2{\int \limits_{#1}^{#2}}
\def\eff{^{\rm eff}}
\def\sunm{^{(n-1)}}
\def\suz{^{(0)}}
\newcommand{\dvg}{\mathop{\rm div}}
\newcommand{\tr}{\mathop{\rm tr}}
\newcommand{\ul}[1]{\underline{#1}}
\newcommand{\uld}[1]{\dot{\underline{#1}}}
\newcommand{\ull}[1]{\underline{\underline{#1}}}
\newcommand{\dev}{\mathop{\rm dev}}
\newcommand{\skewop}{\mathop{\rm skew}}
\def\from{\leftarrow}
\def\Vcal{\mathcal{V}}
\def\Tcal{\mathcal{T}}
\def\Ical{\mathcal{I}}
\def\Hcal{\mathcal{H}}
\def\Fcal{\mathcal{F}}
\def\Gcal{\mathcal{G}}
\def\pd{\partial}
\def\ub{\bm{u}}
\def\vb{\bm{v}}
\def\Mb{\bm{M}}
\def\vphib{\bm{\varphi}}
"""
# LS: Are the following needed as well?
#\def\Vcal{\mathcal{V}}
#\def\Tcal{\mathcal{T}}
#\def\figDir{../doc/tex/figures}
#"""

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_use_modindex = True

# Preamble for pngmath images
pngmath_latex_preamble = latex_preamble

# Turn off numpydoc autosummary tables
numpydoc_show_class_members = False

def process_terms(app, what_, name, obj, options, lines):
    """
    Prepend term call signature(s) into term docstrings.
    """
    from types import TypeType
    from sfepy.terms import Term

    if isinstance(obj, TypeType):
        if issubclass(obj, Term) and len(obj.name):
            arg_types = obj.arg_types
            if ((len(arg_types) > 1) and not isinstance(arg_types[0], str)):
                arg_types = [u', '.join(['%s' % arg for arg in arg_type])
                             for arg_type in arg_types]
                at_lines = [u'``(%s)``' % arg_type for arg_type in arg_types]

            else:
                arg_types = u', '.join(['%s' % arg for arg in arg_types])
                at_lines = [u'``(%s)``' % arg_types]

            for ii, line in enumerate(lines):
                if line.startswith(':Arguments'):
                    i0 = ii - 1
                    break

            else:
                i0 = 0

            len0 = len(obj.name) + 4
            lines.insert(i0+0, u'')
            lines.insert(i0+1, u':Call signature:')
            lines.insert(i0+2, u'')
            lines.insert(i0+3, (u'=' * len0) + u' ===')
            lines.insert(i0+4, u'**%s** %s' % (obj.name, at_lines[0]))
            for ii, line in enumerate(at_lines[1:]):
                lines.insert(i0+5+ii, u'..' + (' ' * (len0 - 1)) + line)
            lines.insert(i0+4+len(at_lines), (u'=' * len0) + u' ===')
            lines.insert(i0+5+len(at_lines), u'')

    # make sure there is a blank line at the end
    if lines and lines[-1]:
        lines.append(u'')


def setup(app):
    app.connect('autodoc-process-docstring', process_terms)

########NEW FILE########
__FILENAME__ = ipython_console_highlighting
from pygments.lexer import Lexer, do_insertions
from pygments.lexers.agile import PythonConsoleLexer, PythonLexer, \
    PythonTracebackLexer
from pygments.token import Comment, Generic
from sphinx import highlighting
import re

line_re = re.compile('.*?\n')

class IPythonConsoleLexer(Lexer):
    """
    For IPython console output or doctests, such as:

    Tracebacks are not currently supported.

    .. sourcecode:: ipython

      In [1]: a = 'foo'

      In [2]: a
      Out[2]: 'foo'

      In [3]: print a
      foo

      In [4]: 1 / 0
    """
    name = 'IPython console session'
    aliases = ['ipython']
    mimetypes = ['text/x-ipython-console']
    input_prompt = re.compile("(In \[[0-9]+\]: )|(   \.\.\.+:)")
    output_prompt = re.compile("(Out\[[0-9]+\]: )|(   \.\.\.+:)")
    continue_prompt = re.compile("   \.\.\.+:")
    tb_start = re.compile("\-+")

    def get_tokens_unprocessed(self, text):
        pylexer = PythonLexer(**self.options)
        tblexer = PythonTracebackLexer(**self.options)

        curcode = ''
        insertions = []
        for match in line_re.finditer(text):
            line = match.group()
            input_prompt = self.input_prompt.match(line)
            continue_prompt = self.continue_prompt.match(line.rstrip())
            output_prompt = self.output_prompt.match(line)
            if line.startswith("#"):
                insertions.append((len(curcode),
                                   [(0, Comment, line)]))
            elif input_prompt is not None:
                insertions.append((len(curcode),
                                   [(0, Generic.Prompt, input_prompt.group())]))
                curcode += line[input_prompt.end():]
            elif continue_prompt is not None:
                insertions.append((len(curcode),
                                   [(0, Generic.Prompt, continue_prompt.group())]))
                curcode += line[continue_prompt.end():]
            elif output_prompt is not None:
                insertions.append((len(curcode),
                                   [(0, Generic.Output, output_prompt.group())]))
                curcode += line[output_prompt.end():]
            else:
                if curcode:
                    for item in do_insertions(insertions,
                                              pylexer.get_tokens_unprocessed(curcode)):
                        yield item
                        curcode = ''
                        insertions = []
                yield match.start(), Generic.Output, line
        if curcode:
            for item in do_insertions(insertions,
                                      pylexer.get_tokens_unprocessed(curcode)):
                yield item

highlighting.lexers['ipython'] = IPythonConsoleLexer()

########NEW FILE########
__FILENAME__ = acoustics
r"""
Acoustic pressure distribution.

This example shows how to solve a problem in complex numbers, note the
'accoustic_pressure' field definition.

Find :math:`p` such that:

.. math::
    c^2 \int_{\Omega} \nabla q \cdot \nabla p
    - w^2 \int_{\Omega} q p
    - i w c \int_{\Gamma_{out}} q p
    = i w c^2 \rho v_n \int_{\Gamma_{in}} q
    \;, \quad \forall q \;.
"""
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/2d/special/two_rectangles.mesh'

v_n = 1.0 # m/s
w = 1000.0
c = 343.0 # m/s
rho = 1.55 # kg/m^3

options = {
    'nls' : 'newton',
    'ls' : 'ls',
}

materials = {
    'one' : ({'one' : 1.0},),
}

regions = {
    'Omega' : 'all',
    'Gamma_in' : ('vertices in (x < 0.01)', 'facet'),
    'Gamma_out' : ('vertices in (x > 0.99)', 'facet'),
}

fields = {
    'accoustic_pressure' : ('complex', 1, 'Omega', 1),
}

variables = {
    'p'   : ('unknown field', 'accoustic_pressure', 0),
    'q'   : ('test field',    'accoustic_pressure', 'p'),
}

ebcs = {
}

integrals = {
    'i' : 2,
}

equations = {
    'Acoustic pressure' :
    """%s * dw_laplace.i.Omega( one.one, q, p )
    - %s * dw_volume_dot.i.Omega( q, p )
    - %s * dw_surface_dot.i.Gamma_out( q, p )
    = %s * dw_surface_integrate.i.Gamma_in( q )"""
    % (c*c, w*w, 1j*w*c, 1j*w*c*c*rho*v_n)
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton', {'i_max'      : 1,
                               'eps_a'      : 1e-1,
                               'eps_r'      : 1.0,
                               'macheps'   : 1e-16,
                               # Linear system error < (eps_a * lin_red).
                               'lin_red'    : 1e-1,
                               'ls_red'     : 0.1,
                               'ls_red_warp' : 0.001,
                               'ls_on'      : 1.1,
                               'ls_min'     : 1e-5,
                               'check'     : 0,
                               'delta'     : 1e-6,
                               # 'nonlinear' or 'linear' (ignore i_max)
                               'problem'   : 'nonlinear',
                               } )
}

########NEW FILE########
__FILENAME__ = acoustics3d
r"""
Acoustic pressure distribution in 3D.

Two Laplace equations, one in :math:`\Omega_1`, other in
:math:`\Omega_2`, connected on the interface region :math:`\Gamma_{12}`
using traces of variables.

Find two complex acoustic pressures :math:`p_1`, :math:`p_2` such that:

.. math::
    \int_{\Omega} k^2 q p - \int_{\Omega} \nabla q \cdot \nabla p \\
    - i w/c \int_{\Gamma_{out}} q p
    + i w \rho/Z \int_{\Gamma_2} q (p_2 - p_1)
    + i w \rho/Z \int_{\Gamma_1}  q (p_1 - p_2) \\
    = i w \rho \int_{\Gamma_{in}} v_n q
    \;, \quad \forall q \;.
"""

from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/acoustics_mesh3d.mesh'

freq = 1200
v_n = 1.0 # m/s
c = 343.0 # m/s
rho = 1.55 # kg/m^3
R = 1000
w = 2.0 * freq

k1 = w / c
rhoc1 = rho * c

coef_k = ((1.0 + 0.1472 * (freq / R)**(-0.577))
          + 1j * (-0.1734 * (freq / R)**(-0.595)))
coef_r = ((1.0 + 0.0855 * (freq / R)**(-0.754))
          + 1j * (-0.0765 * (freq / R)**(-0.732)))

k2 = k1 * coef_k
rhoc2 = rhoc1 * coef_r

# perforation geometry parameters
tw = 0.9e-3
dh = 2.49e-3
por = 0.08

# acoustic impedance
Z = rho * c / por * (0.006 + 1j * k1 * (tw + 0.375 * dh
                                        * (1 + rhoc2/rhoc1 * k2/k1)))

regions = {
    'Omega' : 'all',
    'Omega_1' : 'cells of group 1',
    'Omega_2' : 'cells of group 2',
    'Gamma_12' : ('r.Omega_1 *v r.Omega_2', 'facet'),
    'Gamma_12_1' : ('copy r.Gamma_12', 'facet', 'Omega_1'),
    'Gamma_12_2' : ('copy r.Gamma_12', 'facet', 'Omega_2'),
    'Gamma_in' : ('vertices in (z < 0.001)', 'facet'),
    'Gamma_out' : ('vertices in (z > 0.157)', 'facet'),
}

materials = {
}

fields = {
    'accoustic_pressure_1' : ('complex', 'scalar', 'Omega_1', 1),
    'accoustic_pressure_2' : ('complex', 'scalar', 'Omega_2', 1),
}

variables = {
    'p_1'   : ('unknown field', 'accoustic_pressure_1'),
    'q_1'   : ('test field',    'accoustic_pressure_1', 'p_1'),
    'p_2'   : ('unknown field', 'accoustic_pressure_2'),
    'q_2'   : ('test field',    'accoustic_pressure_2', 'p_2'),
}

ebcs = {
}

integrals = {
    'i' : 2,
}

equations = {
    'Acoustic pressure' :
    """%s * dw_volume_dot.i.Omega_1(q_1, p_1)
     + %s * dw_volume_dot.i.Omega_2(q_2, p_2)
     - dw_laplace.i.Omega_1(q_1, p_1)
     - dw_laplace.i.Omega_2(q_2, p_2)
     - %s * dw_surface_dot.i.Gamma_out(q_1, p_1)
     + %s * dw_jump.i.Gamma_12_1(q_1, p_1, tr(p_2))
     + %s * dw_jump.i.Gamma_12_2(q_2, p_2, tr(p_1))
     = %s * dw_surface_integrate.i.Gamma_in(q_1)"""
     % (k1*k1, k2*k2,
       1j*k1,
       1j*k1*rhoc1 / Z, 1j*k2*rhoc2 / Z,
       1j*k1*rhoc1 * v_n)
}

options = {
    'nls': 'newton',
    'ls': 'ls',
    'file_per_var': True,
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton', {'i_max'      : 1,
                               'eps_a'      : 1e-10,
                               'eps_r'      : 1.0,
                               'macheps'   : 1e-16,
                               'lin_red'    : 1e-1,
                               'ls_red'     : 0.1,
                               'ls_red_warp' : 0.001,
                               'ls_on'      : 1.1,
                               'ls_min'     : 1e-5,
                               'check'     : 0,
                               'delta'     : 1e-6,
                               'problem'   : 'nonlinear',
    } )
}

########NEW FILE########
__FILENAME__ = vibro_acoustic3d
r"""
Vibro-acoustic problem

3D acoustic domain with 2D perforated deforming interface.

*Master problem*: defined in 3D acoustic domain (``vibro_acoustic3d.py``)

*Slave subproblem*: 2D perforated interface (``vibro_acoustic3d_mid.py``)

Master 3D problem - find :math:`p` (acoustic pressure)
and :math:`g` (transversal acoustic velocity) such that:

.. math::
    c^2 \int_{\Omega} \nabla q \cdot \nabla p
    - \omega^2 \int_{\Omega} q p
    + i \omega c \int_{\Gamma_{in}} q p
    + i \omega c \int_{\Gamma_{out}} q p
    - i \omega c^2 \int_{\Gamma_0} (q^+ - q^-) g
    = 2i \omega c \int_{\Gamma_{in}} q \bar{p}
    \;, \quad \forall q \;,

    - i \omega \int_{\Gamma_0} f (p^+ - p^-)
    - \omega^2 \int_{\Gamma_0} F f g
    + \omega^2 \int_{\Gamma_0} C f w
    = 0
    \;, \quad \forall f \;,

Slave 2D subproblem - find :math:`w` (plate deflection)
and :math:`\ul{\theta}` (rotation) such that:

.. math::
    \omega^2 \int_{\Gamma_0} C z g
    - \omega^2 \int_{\Gamma_0} S z w
    + \int_{\Gamma_0} \nabla z \cdot \ull{G} \cdot \nabla w
    - \int_{\Gamma_0} \ul{\theta} \cdot \ull{G} \cdot \nabla z
    = 0
    \;, \quad \forall z \;,

    - \omega^2 \int_{\Gamma_0} R\, \ul{\nu} \cdot \ul{\theta}
    + \int_{\Gamma_0} D_{ijkl} e_{ij}(\ul{\nu}) e_{kl}(\ul{\theta})
    - \int_{\Gamma_0} \ul{\nu} \cdot \ull{G} \cdot \nabla w
    + \int_{\Gamma_0} \ul{\nu} \cdot \ull{G} \cdot \ul{\theta}
    = 0
    \;, \quad \forall \ul{\nu} \;,
"""
from sfepy import data_dir
filename_mesh = data_dir + '/meshes/3d/acoustic_wg.vtk'

sound_speed = 343.0
wave_num = 5.5
p_inc = 300

c = sound_speed
c2 = c**2
w = wave_num * c
w2 = w**2
wc = w * c
wc2 = w * c2

regions = {
    'Omega1': 'cells of group 1',
    'Omega2': 'cells of group 2',
    'GammaIn': ('vertices of group 1', 'face'),
    'GammaOut': ('vertices of group 2', 'face'),
    'Gamma_aux': ('r.Omega1 *v r.Omega2', 'face'),
    'Gamma0_1': ('copy r.Gamma_aux', 'face', 'Omega1'),
    'Gamma0_2': ('copy r.Gamma_aux', 'face', 'Omega2'),
    'aux_Left': ('vertices in (x <  0.001)', 'face'),
    'aux_Right': ('vertices in (x > 0.299)', 'face'),
    'Gamma0_1_Left': ('r.Gamma0_1 *v r.aux_Left', 'edge'),
    'Gamma0_1_Right': ('r.Gamma0_1 *v r.aux_Right', 'edge'),
    }

fields = {
    'pressure1': ('complex', 'scalar', 'Omega1', 1),
    'pressure2': ('complex', 'scalar', 'Omega2', 1),
    'tvelocity': ('complex', 'scalar', 'Gamma0_1', 1),
    'deflection': ('complex', 'scalar', 'Gamma0_1', 1),
    }

variables = {
    'p1': ('unknown field', 'pressure1', 0),
    'q1': ('test field', 'pressure1', 'p1'),
    'p2': ('unknown field', 'pressure2', 1),
    'q2': ('test field', 'pressure2', 'p2'),
    'g0': ('unknown field', 'tvelocity', 2),
    'f0': ('test field', 'tvelocity', 'g0'),
    'w': ('unknown field', 'deflection', 3),
    'z': ('test field', 'deflection', 'w'),
    }

ebcs = {
    'fixed_l': ('Gamma0_1_Left', {'w.0': 0.0}),
    'fixed_r': ('Gamma0_1_Right', {'w.0': 0.0}),
    }

options = {
    'file_per_var': True,
    }

functions = {
    }

materials = {
    'ac' : ({'F': -2.064e+00, 'c': -1.064e+00}, ),
    }

equations = {
    'eq_1' : """
        %e * dw_laplace.5.Omega1(q1, p1)
      + %e * dw_laplace.5.Omega2(q2, p2)
      - %e * dw_volume_dot.5.Omega1(q1, p1)
      - %e * dw_volume_dot.5.Omega2(q2, p2)
      + %s * dw_surface_dot.5.GammaIn(q1, p1)
      + %s * dw_surface_dot.5.GammaOut(q2, p2)
      - %s * dw_surface_dot.5.Gamma0_1(q1, g0)
      + %s * dw_surface_dot.5.Gamma0_2(q2, tr(g0))
      = %s * dw_surface_integrate.5.GammaIn(q1)"""\
        % (c2, c2, w2, w2,
           1j * wc, 1j * wc,
           1j * wc2, 1j * wc2,
           2j * wc * p_inc),
    'eq_2' : """
      - %s * dw_surface_dot.5.Gamma0_1(f0, p1)
      + %s * dw_surface_dot.5.Gamma0_1(f0, tr(p2))
      - %e * dw_surface_dot.5.Gamma0_1(ac.F, f0, g0)
      + %e * dw_surface_dot.5.Gamma0_1(ac.c, f0, w)
      = 0"""\
        % (1j * w, 1j * w, w2, w2),
    }

solvers = {
    'ls': ('ls.cm_pb',
           {'others': [data_dir
                       + '/examples/acoustics/vibro_acoustic3d_mid.py'],
            'coupling_variables': ['g0', 'w'],
            'needs_problem_instance': True,
            }),
    'nls': ('nls.newton', {'i_max'   : 1,
                           'eps_a'   : 1e-6,
                           'eps_r'   : 1e-6,
                           'problem' : 'nonlinear', })
    }

########NEW FILE########
__FILENAME__ = vibro_acoustic3d_mid
r"""
Vibro-acoustic problem

3D acoustic domain with 2D perforated deforming interface.

*Master problem*: defined in 3D acoustic domain (``vibro_acoustic3d.py``)

*Slave subproblem*: 2D perforated interface (``vibro_acoustic3d_mid.py``)

Master 3D problem - find :math:`p` (acoustic pressure)
and :math:`g` (transversal acoustic velocity) such that:

.. math::
    c^2 \int_{\Omega} \nabla q \cdot \nabla p
    - \omega^2 \int_{\Omega} q p
    + i \omega c \int_{\Gamma_{in}} q p
    + i \omega c \int_{\Gamma_{out}} q p
    - i \omega c^2 \int_{\Gamma_0} (q^+ - q^-) g
    = 2i \omega c \int_{\Gamma_{in}} q \bar{p}
    \;, \quad \forall q \;,

    - i \omega \int_{\Gamma_0} f (p^+ - p^-)
    - \omega^2 \int_{\Gamma_0} F f g
    + \omega^2 \int_{\Gamma_0} C f w
    = 0
    \;, \quad \forall f \;,

Slave 2D subproblem - find :math:`w` (plate deflection)
and :math:`\ul{\theta}` (rotation) such that:

.. math::
    \omega^2 \int_{\Gamma_0} C z g
    - \omega^2 \int_{\Gamma_0} S z w
    + \int_{\Gamma_0} \nabla z \cdot \ull{G} \cdot \nabla w
    - \int_{\Gamma_0} \ul{\theta} \cdot \ull{G} \cdot \nabla z
    = 0
    \;, \quad \forall z \;,

    - \omega^2 \int_{\Gamma_0} R\, \ul{\nu} \cdot \ul{\theta}
    + \int_{\Gamma_0} D_{ijkl} e_{ij}(\ul{\nu}) e_{kl}(\ul{\theta})
    - \int_{\Gamma_0} \ul{\nu} \cdot \ull{G} \cdot \nabla w
    + \int_{\Gamma_0} \ul{\nu} \cdot \ull{G} \cdot \ul{\theta}
    = 0
    \;, \quad \forall \ul{\nu} \;,
"""
import numpy as nm
from sfepy.mechanics.matcoefs import stiffness_from_lame

filename_mesh = '../../meshes/2d/acoustic_wg_mid.vtk'

sound_speed = 343.0
wave_num = 5.5
thickness = 0.01

c = sound_speed
c2 = c**2
w = wave_num * c
w2 = w**2
wc = w * c
wc2 = w * c2

regions = {
    'Gamma0': 'all',
    'Left': ('vertices in (x < 0.001)', 'facet'),
    'Right': ('vertices in (x > 0.299)', 'facet'),
    }

fields = {
    'deflection': ('complex', 'scalar', 'Gamma0', 1),
    'rotation': ('complex', 'vector', 'Gamma0', 1),
    'tvelocity': ('complex', 'scalar', 'Gamma0', 1),
    }

variables = {
    'w': ('unknown field', 'deflection'),
    'z': ('test field', 'deflection', 'w'),
    'theta': ('unknown field', 'rotation'),
    'nu': ('test field', 'rotation', 'theta'),
    'g0': ('unknown field', 'tvelocity'),
    'f0': ('test field', 'tvelocity', 'g0'),
    }

ebcs = {
    'fixed_l': ('Left', {'w.0': 0.0, 'theta.all': 0.0}),
    'fixed_r': ('Right', {'w.0': 0.0, 'theta.all': 0.0}),
    }

options = {
    }

materials = {
    'ac' : ({'c': -1.064e+00, 'T': 9.202e-01,
            'hG': thickness * 4.5e10 * nm.eye(2),
            'hR': thickness * 0.71,
            'h3R': thickness**3 / 3.0 * 0.71,
            'h3C': thickness**3 / 3.0 * stiffness_from_lame(2, 1e1, 1e0)}, ),
    }

equations = {
    'eq_3': """
        %e * dw_volume_dot.5.Gamma0(ac.c, z, g0)
      - %e * dw_volume_dot.5.Gamma0(ac.T, z, w)
      - %e * dw_volume_dot.5.Gamma0(ac.hR, z, w)
           + dw_diffusion.5.Gamma0(ac.hG, z, w)
           - dw_v_dot_grad_s.5.Gamma0(ac.hG, theta, z)
      = 0"""\
        % (w2, w2, w2),
    'eq_4': """
      - %e * dw_volume_dot.5.Gamma0(ac.h3R, nu, theta)
           + dw_lin_elastic.5.Gamma0(ac.h3C, nu, theta)
           - dw_v_dot_grad_s.5.Gamma0(ac.hG, nu, w)
           + dw_volume_dot.5.Gamma0(ac.hG, nu, theta)
      = 0"""\
        % (w2, ),
    }

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton', {'i_max'   : 1,
                               'eps_a'   : 1e-4,
                               'eps_r'   : 1e-4,
                               'problem' : 'nonlinear', })
    }

########NEW FILE########
__FILENAME__ = biot
r"""
Biot problem - deformable porous medium.

Find :math:`\ul{u}`, :math:`p` such that:

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    - \int_{\Omega}  p\ \alpha_{ij} e_{ij}(\ul{v})
    = 0
    \;, \quad \forall \ul{v} \;,

    \int_{\Omega} q\ \alpha_{ij} e_{ij}(\ul{u})
    + \int_{\Omega} K_{ij} \nabla_i q \nabla_j p
    = 0
    \;, \quad \forall q \;,

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl}+\delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;.
"""
import numpy as nm

from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/cube_medium_hexa.mesh'

regions = {
    'Omega' : 'all',
    'Bottom' : ('vertices in (z < -0.4999999)', 'facet'),
    'Top' : ('vertices in (z > 0.4999999)', 'facet'),
    'Left' : ('vertices in (x < -0.4999999)', 'facet'),
}

field_1 = {
    'name' : 'displacement',
    'dtype' : nm.float64,
    'shape' : (3,),
    'region' : 'Omega',
    'approx_order' : 1,
}

field_2 = {
    'name' : 'pressure',
    'dtype' : nm.float64,
    'shape' : (1,),
    'region' : 'Omega',
    'approx_order' : 1,
}

variables = {
    'u'       : ('unknown field',   'displacement', 0),
    'v'       : ('test field',      'displacement', 'u'),
    'p'       : ('unknown field',   'pressure', 1),
    'q'       : ('test field',      'pressure', 'p'),
}

ebcs = {
    'fix_u' : ('Bottom', {'u.all' : 0.0}),
    'load_u' : ('Top', {'u.2' : 0.2}),
    'load_p' : ('Left', {'p.all' : 1.0}),
}

material_1 = {
    'name' : 'm',
    'values' : {
        'lam' : 1.7,
        'mu' : 0.3,
        'alpha' : nm.array( [[0.132], [0.132], [0.132],
                             [0.092], [0.092], [0.092]],
                            dtype = nm.float64 ),
        'K' : nm.array( [[2.0, 0.2, 0.0], [0.2, 1.0, 0.0], [0.0, 0.0, 0.5]],
                        dtype = nm.float64 ),
    }
}

integral_1 = {
    'name' : 'i1',
    'order' : 1,
}

integral_2 = {
    'name' : 'i2',
    'order' : 2,
}

equations = {
    'eq_1' :
    """dw_lin_elastic_iso.i2.Omega( m.lam, m.mu, v, u )
     - dw_biot.i1.Omega( m.alpha, v, p )
       = 0""",
    'eq_2' :
    """dw_biot.i1.Omega( m.alpha, u, q ) + dw_diffusion.i1.Omega( m.K, q, p )
       = 0""",
}

solver_0 = {
    'name' : 'ls_d',
    'kind' : 'ls.scipy_direct',
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 1,
    'eps_a'      : 1e-10,
    'eps_r'      : 1.0,
    'macheps'   : 1e-16,
    'lin_red'    : 1e-2, # Linear system error < (eps_a * lin_red).
    'ls_red'     : 0.1,
    'ls_red_warp' : 0.001,
    'ls_on'      : 1.1,
    'ls_min'     : 1e-5,
    'check'     : 0,
    'delta'     : 1e-6,
    'problem'   : 'nonlinear', # 'nonlinear' or 'linear' (ignore i_max)
}

########NEW FILE########
__FILENAME__ = biot_npbc
r"""
Biot problem - deformable porous medium with the no-penetration boundary
condition on a boundary region.

Find :math:`\ul{u}`, :math:`p` such that:

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    - \int_{\Omega}  p\ \alpha_{ij} e_{ij}(\ul{v})
    = 0
    \;, \quad \forall \ul{v} \;,

    \int_{\Omega} q\ \alpha_{ij} e_{ij}(\ul{u})
    + \int_{\Omega} K_{ij} \nabla_i q \nabla_j p
    = 0
    \;, \quad \forall q \;,

    \ul{u} \cdot \ul{n} = 0 \mbox{ on } \Gamma_{walls} \;,

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl}+\delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;.
"""
import os
import numpy as nm

from sfepy.linalg import get_coors_in_tube
from sfepy.mechanics.matcoefs import stiffness_from_lame

def define():
    from sfepy import data_dir

    filename = data_dir + '/meshes/3d/cylinder.mesh'
    output_dir = 'output'
    return define_input(filename, output_dir)

def cinc_simple(coors, mode):
    axis = nm.array([1, 0, 0], nm.float64)
    if mode == 0: # In
        centre = nm.array([0.0, 0.0, 0.0], nm.float64)
        radius = 0.019
        length = 0.00002
    elif mode == 1: # Out
        centre = nm.array([0.1, 0.0, 0.0], nm.float64)
        radius = 0.019
        length = 0.00002
    elif mode == 2: # Rigid
        centre = nm.array([0.05, 0.0, 0.0], nm.float64)
        radius = 0.015
        length = 0.03
    else:
        raise ValueError('unknown mode %s!' % mode)

    return get_coors_in_tube(coors,
                             centre, axis, -1, radius, length)

def define_regions(filename):
    if filename.find('simple.mesh'):
        dim = 3
        regions = {
            'Omega' : 'all',
            'Walls' : ('vertices of surface -v (r.Outlet +f r.Inlet)', 'facet'),
            'Inlet' : ('vertices by cinc_simple0', 'facet'),
            'Outlet' : ('vertices by cinc_simple1', 'facet'),
            'Rigid' : 'vertices by cinc_simple2',
        }

    else:
        raise ValueError('unknown mesh %s!' % filename)

    return regions, dim

def get_pars(ts, coor, mode, output_dir='.', **kwargs):
    if mode == 'qp':
        n_nod, dim = coor.shape
        sym = (dim + 1) * dim / 2

        out = {}
        out['D'] = nm.tile(stiffness_from_lame(dim, lam=1.7, mu=0.3),
                           (coor.shape[0], 1, 1))

        aa = nm.zeros((sym, 1), dtype=nm.float64)
        aa[:dim] = 0.132
        aa[dim:sym] = 0.092
        out['alpha'] = nm.tile(aa, (coor.shape[0], 1, 1))

        perm = nm.eye(dim, dtype=nm.float64)
        out['K'] = nm.tile(perm, (coor.shape[0], 1, 1))

        return out

def post_process(out, pb, state, extend=False):
    from sfepy.base.base import Struct

    dvel = pb.evaluate('ev_diffusion_velocity.i.Omega( m.K, p )',
                       mode='el_avg')
    out['dvel'] = Struct(name='output_data',
                         mode='cell', data=dvel, dofs=None)

    stress = pb.evaluate('ev_cauchy_stress.i.Omega( m.D, u )',
                         mode='el_avg')
    out['cauchy_stress'] = Struct(name='output_data',
                                  mode='cell', data=stress, dofs=None)
    return out

def define_input(filename, output_dir):

    filename_mesh = filename
    options = {
        'output_dir' : output_dir,
        'output_format' : 'vtk',
        'post_process_hook' : 'post_process',

        'ls' : 'ls',
        'nls' : 'newton',
    }

    functions = {
        'cinc_simple0' : (lambda coors, domain:
                          cinc_simple(coors, 0),),
        'cinc_simple1' : (lambda coors, domain:
                          cinc_simple(coors, 1),),
        'cinc_simple2' : (lambda coors, domain:
                          cinc_simple(coors, 2),),
        'get_pars' : (lambda ts, coors, mode=None, **kwargs:
                      get_pars(ts, coors, mode,
                               output_dir=output_dir, **kwargs),),
    }
    regions, dim = define_regions(filename_mesh)

    field_1 = {
        'name' : 'displacement',
        'dtype' : nm.float64,
        'shape' : dim,
        'region' : 'Omega',
        'approx_order' : 1,
    }
    field_2 = {
        'name' : 'pressure',
        'dtype' : nm.float64,
        'shape' : 1,
        'region' : 'Omega',
        'approx_order' : 1,
    }

    variables = {
        'u'       : ('unknown field',   'displacement', 0),
        'v'       : ('test field',      'displacement', 'u'),
        'p'       : ('unknown field',   'pressure', 1),
        'q'       : ('test field',      'pressure', 'p'),
    }

    ebcs = {
        'inlet' : ('Inlet', {'p.0' : 1.0, 'u.all' : 0.0}),
        'outlet' : ('Outlet', {'p.0' : -1.0}),
    }

    lcbcs = {
        'rigid' : ('Outlet', {'u.all' : 'rigid'}),
        'no_penetration' : ('Walls', {'u.all' : 'no_penetration'}),
    }

    material_1 = {
        'name' : 'm',
        'function' : 'get_pars',
    }

    integral_1 = {
        'name' : 'i',
        'order' : 2,
    }

    equations = {
        'eq_1' :
        """dw_lin_elastic.i.Omega( m.D, v, u )
         - dw_biot.i.Omega( m.alpha, v, p )
         = 0""",
        'eq_2' :
        """dw_biot.i.Omega( m.alpha, u, q )
         + dw_diffusion.i.Omega( m.K, q, p )
         = 0""",
    }

    solver_0 = {
        'name' : 'ls',
        'kind' : 'ls.scipy_direct', # Direct solver.
    }

    solver_1 = {
        'name' : 'newton',
        'kind' : 'nls.newton',
    }

    return locals()

########NEW FILE########
__FILENAME__ = biot_npbc_lagrange
r"""
Biot problem - deformable porous medium with the no-penetration boundary
condition on a boundary region enforced using Lagrange multipliers.

The non-penetration condition is enforced weakly using the Lagrange
multiplier :math:`\lambda`. There is also a rigid body movement
constraint imposed on the :math:`\Gamma_{outlet}` region using the
linear combination boundary conditions.

Find :math:`\ul{u}`, :math:`p` and :math:`\lambda` such that:

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    - \int_{\Omega}  p\ \alpha_{ij} e_{ij}(\ul{v})
    + \int_{\Gamma_{walls}} \lambda \ul{n} \cdot \ul{v}
    = 0
    \;, \quad \forall \ul{v} \;,

    \int_{\Omega} q\ \alpha_{ij} e_{ij}(\ul{u})
    + \int_{\Omega} K_{ij} \nabla_i q \nabla_j p
    = 0
    \;, \quad \forall q \;,

    \int_{\Gamma_{walls}} \hat\lambda \ul{n} \cdot \ul{u}
    = 0
    \;, \quad \forall \hat\lambda \;,

    \ul{u} \cdot \ul{n} = 0 \mbox{ on } \Gamma_{walls} \;,

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl}+\delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;.
"""
from biot_npbc import cinc_simple, define_regions, get_pars

def define():
    from sfepy import data_dir

    filename = data_dir + '/meshes/3d/cylinder.mesh'
    output_dir = 'output'
    return define_input(filename, output_dir)

def post_process(out, pb, state, extend=False):
    from sfepy.base.base import Struct

    dvel = pb.evaluate('ev_diffusion_velocity.2.Omega( m.K, p )',
                       mode='el_avg')
    out['dvel'] = Struct(name='output_data', var_name='p',
                         mode='cell', data=dvel, dofs=None)

    stress = pb.evaluate('ev_cauchy_stress.2.Omega( m.D, u )',
                         mode='el_avg')
    out['cauchy_stress'] = Struct(name='output_data', var_name='u',
                                  mode='cell', data=stress, dofs=None)
    return out

def define_input(filename, output_dir):

    filename_mesh = filename
    options = {
        'output_dir' : output_dir,
        'output_format' : 'vtk',
        'post_process_hook' : 'post_process',
        ## 'file_per_var' : True,

        'ls' : 'ls',
        'nls' : 'newton',
    }

    functions = {
        'cinc_simple0' : (lambda coors, domain:
                          cinc_simple(coors, 0),),
        'cinc_simple1' : (lambda coors, domain:
                          cinc_simple(coors, 1),),
        'cinc_simple2' : (lambda coors, domain:
                          cinc_simple(coors, 2),),
        'get_pars' : (lambda ts, coors, mode=None, **kwargs:
                      get_pars(ts, coors, mode,
                               output_dir=output_dir, **kwargs),),
    }
    regions, dim = define_regions(filename_mesh)

    fields = {
        'displacement': ('real', 'vector', 'Omega', 1),
        'pressure': ('real', 'scalar', 'Omega', 1),
        'multiplier': ('real', 'scalar', 'Walls', 1),
    }

    variables = {
        'u'  : ('unknown field', 'displacement', 0),
        'v'  : ('test field',    'displacement', 'u'),
        'p'  : ('unknown field', 'pressure', 1),
        'q'  : ('test field',    'pressure', 'p'),
        'ul' : ('unknown field', 'multiplier', 2),
        'vl' : ('test field',    'multiplier', 'ul'),
    }

    ebcs = {
        'inlet' : ('Inlet', {'p.0' : 1.0, 'u.all' : 0.0}),
        'outlet' : ('Outlet', {'p.0' : -1.0}),
    }

    lcbcs = {
        'rigid' : ('Outlet', {'u.all' : 'rigid'}),
    }

    materials = {
        'm' : 'get_pars',
    }

    equations = {
        'eq_1' :
        """dw_lin_elastic.2.Omega( m.D, v, u )
         - dw_biot.2.Omega( m.alpha, v, p )
         + dw_non_penetration.2.Walls( v, ul )
         = 0""",
        'eq_2' :
        """dw_biot.2.Omega( m.alpha, u, q )
         + dw_diffusion.2.Omega( m.K, q, p )
         = 0""",
        'eq_3' :
        """dw_non_penetration.2.Walls( u, vl )
         = 0""",
    }

    solvers = {
        'ls' : ('ls.scipy_direct', {}),
        'newton' : ('nls.newton', {}),
    }

    return locals()

########NEW FILE########
__FILENAME__ = cube
r"""
Laplace equation (e.g. temperature distribution) on a cube geometry with
different boundary condition values on the cube sides. This example was
used to create the SfePy logo.

Find :math:`T` such that:

.. math::
    \int_{\Omega} c \nabla s \cdot \nabla T
    = 0
    \;, \quad \forall s \;.
"""
from sfepy import data_dir

#filename_mesh = data_dir + '/meshes/3d/cube_big_tetra.mesh'
filename_mesh = data_dir + '/meshes/3d/cube_medium_hexa.mesh'

############# Laplace.

material_1 = {
    'name' : 'coef',
    'values' : {'val' : 1.0},
}

field_1 = {
    'name' : 'temperature',
    'dtype' : 'real',
    'shape' : (1,),
    'region' : 'Omega',
    'approx_order' : 1,
}

if filename_mesh.find('cube_medium_hexa.mesh') >= 0:
    region_1000 = {
        'name' : 'Omega',
        'select' : 'cells of group 0',
    }
    integral_1 = {
        'name' : 'i',
        'order' : 1,
    }
    solver_0 = {
        'name' : 'ls',
        'kind' : 'ls.scipy_direct',
    }

elif filename_mesh.find('cube_big_tetra.mesh') >= 0:
    region_1000 = {
        'name' : 'Omega',
        'select' : 'cells of group 6',
    }
    integral_1 = {
        'name' : 'i',
        'quadrature' : 'custom',
        'vals'    : [[1./3., 1./3., 1./3.]],
        'weights' : [0.5]
    }
    solver_0 = {
        'name' : 'ls',
        'kind' : 'ls.scipy_iterative',

        'method' : 'cg',
        'i_max'   : 1000,
        'eps_r'   : 1e-12,
    }

variable_1 = {
    'name' : 'T',
    'kind' : 'unknown field',
    'field' : 'temperature',
    'order' : 0, # order in the global vector of unknowns
}

variable_2 = {
    'name' : 's',
    'kind' : 'test field',
    'field' : 'temperature',
    'dual' : 'T',
}

region_0 = {
    'name' : 'Surface',
    'select' : 'vertices of surface',
    'kind' : 'facet',
}
region_1 = {
    'name' : 'Bottom',
    'select' : 'vertices in (z < -0.4999999)',
    'kind' : 'facet',
}
region_2 = {
    'name' : 'Top',
    'select' : 'vertices in (z > 0.4999999)',
    'kind' : 'facet',
}
region_03 = {
    'name' : 'Left',
    'select' : 'vertices in (x < -0.4999999)',
    'kind' : 'facet',
}

ebc_1 = {
    'name' : 'T0',
    'region' : 'Surface',
    'dofs' : {'T.0' : -3.0},
}
ebc_4 = {
    'name' : 'T1',
    'region' : 'Top',
    'dofs' : {'T.0' : 1.0},
}
ebc_3 = {
    'name' : 'T2',
    'region' : 'Bottom',
    'dofs' : {'T.0' : -1.0},
}
ebc_2 = {
    'name' : 'T3',
    'region' : 'Left',
    'dofs' : {'T.0' : 2.0},
}

equations = {
    'nice_equation' : """dw_laplace.i.Omega( coef.val, s, T ) = 0""",
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 1,
    'eps_a'      : 1e-10,
    'eps_r'      : 1.0,
    'macheps'   : 1e-16,
    'lin_red'    : 1e-2, # Linear system error < (eps_a * lin_red).
    'ls_red'     : 0.1,
    'ls_red_warp' : 0.001,
    'ls_on'      : 1.1,
    'ls_min'     : 1e-5,
    'check'     : 0,
    'delta'     : 1e-6,
    'problem'   : 'nonlinear', # 'nonlinear' or 'linear' (ignore i_max)
}

########NEW FILE########
__FILENAME__ = laplace_time_ebcs
r"""
Example explaining how to change Dirichlet boundary conditions depending
on time. It is shown on the stationary Laplace equation for temperature,
so there is no dynamics, only the conditions change with time.

Five time steps are solved on a cube domain, with the temperature fixed
to zero on the bottom face, and set to other values on the left, right
and top faces in different time steps.

Find :math:`t` such that:

.. math::
    \int_{\Omega} c \nabla s \cdot \nabla t
    = 0
    \;, \quad \forall s \;.
"""
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/cube_medium_tetra.mesh'

options = {
    'nls' : 'newton',
    'ls' : 'ls',
    'ts' : 'ts',
}

regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < -0.499)', 'facet'),
    'Right' : ('vertices in (x > 0.499)', 'facet'),
    'Bottom' : ('vertices in (z < -0.499)', 'facet'),
    'Top' : ('vertices in (z > 0.499)', 'facet'),
}

materials = {
    'one' : ({'val' : 1.0},),
}

fields = {
    'temperature' : ('real', 1, 'Omega', 1),
}

variables = {
    't'   : ('unknown field', 'temperature', 0),
    's'   : ('test field',    'temperature', 't'),
}

ebcs = {
    'fixed' : ('Bottom', {'t.all' : 0}),
    't_t02' : ('Left', [(-0.5, 0.5), (2.5, 3.5)], {'t.all' : 1.0}),
    't_t1' : ('Right', [(0.5, 1.5)], {'t.all' : 2.0}),
    't_t4' : ('Top', 'is_ebc', {'t.all' : 3.0}),
}

def is_ebc(ts):
    if ts.step in (2, 4):
        return True

    else:
        return False

functions = {
    'is_ebc' : (is_ebc,),
}

equations = {
    'eq' : """dw_laplace.2.Omega( one.val, s, t ) = 0""",
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton', {
        'i_max'      : 1,
        'eps_a'      : 1e-10,
        'problem'   : 'nonlinear',
    }),
    'ts' : ('ts.simple', {
        't0'     : 0.0,
        't1'     : 4.0,
        'dt'     : None,
        'n_step' : 5, # has precedence over dt!

        'quasistatic' : True,
    }),
}

########NEW FILE########
__FILENAME__ = octahedron
r"""
Diffusion (Laplace-like) equation with non-isotropic diffusion
coefficient.

Find :math:`t` such that:

.. math::
    \int_{\Omega} K_{ij} \nabla_i s \nabla_j t
    = 0
    \;, \quad \forall s \;.
"""
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/various_formats/octahedron.node'

material_2 = {
    'name' : 'coef',
    'values' : {'K' : [[1.0, 0.0, 0.0], [0.0, 10.0, 0.0], [0.0, 0.0, 1.0]],
                'val' : 1.0},
}

field_1 = {
    'name' : 'temperature',
    'dtype' : 'real',
    'shape' : (1,),
    'region' : 'Omega',
    'approx_order' : 1,
}

variable_1 = {
    'name' : 't',
    'kind' : 'unknown field',
    'field' : 'temperature',
    'order' : 0,
}

variable_2 = {
    'name' : 's',
    'kind' : 'test field',
    'field' : 'temperature',
    'dual' : 't',
}

region_1000 = {
    'name' : 'Omega',
    'select' : 'all',
}

def get_line(x, y, mode):
    import numpy as nm

    if mode == 0:
        val = nm.where((x + y) >= 3.5)[0]
    elif mode == 1:
        val = nm.where((x + y) <= -3.5)[0]
    print mode, val
    return val

functions = {
    'get_line0' : (lambda coors, domain=None:
                   get_line(coors[:,0], coors[:,1], 0),),
    'get_line1' : (lambda coors, domain=None:
                   get_line(coors[:,0], coors[:,1], 1),),
}

region_03 = {
    'name' : 'Gamma_Left',
    'select' : 'vertices by get_line0',
    'kind' : 'facet',
}
region_4 = {
    'name' : 'Gamma_Right',
    'select' : 'vertices by get_line1',
    'kind' : 'facet',
}

ebc_1 = {
    'name' : 't1',
    'region' : 'Gamma_Left',
    'dofs' : {'t.0' : 2.0},
}

ebc_2 = {
    'name' : 't2',
    'region' : 'Gamma_Right',
    'dofs' : {'t.0' : -2.0},
}

integral_1 = {
    'name' : 'i',
    'order' : 1,
}
equations = {
    'Temperature' : """dw_diffusion.i.Omega( coef.K, s, t ) = 0"""
#    'Temperature' : """dw_laplace.i.Omega( coef.val, s, t ) = 0"""
}

solver_0 = {
    'name' : 'ls',
    'kind' : 'ls.scipy_direct',
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 1,
    'eps_a'      : 1e-10,
    'eps_r'      : 1.0,
    'macheps'   : 1e-16,
    'lin_red'    : 1e-2, # Linear system error < (eps_a * lin_red).
    'ls_red'     : 0.1,
    'ls_red_warp' : 0.001,
    'ls_on'      : 1.1,
    'ls_min'     : 1e-5,
    'check'     : 0,
    'delta'     : 1e-6,
    'problem'   : 'nonlinear', # 'nonlinear' or 'linear' (ignore i_max)
}

options = {
    'nls' : 'newton',
    'ls' : 'ls',
}

########NEW FILE########
__FILENAME__ = poisson
r"""
Laplace equation using the long syntax of keywords.

See the tutorial section :ref:`poisson-example-tutorial` for a detailed
explanation. See :ref:`diffusion-poisson_short_syntax` for the short syntax
version.

Find :math:`t` such that:

.. math::
    \int_{\Omega} c \nabla s \cdot \nabla t
    = 0
    \;, \quad \forall s \;.
"""
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/cylinder.mesh'

material_2 = {
    'name' : 'coef',
    'values' : {'val' : 1.0},
}

region_1000 = {
    'name' : 'Omega',
    'select' : 'cells of group 6',
}

region_03 = {
    'name' : 'Gamma_Left',
    'select' : 'vertices in (x < 0.00001)',
    'kind' : 'facet',
}

region_4 = {
    'name' : 'Gamma_Right',
    'select' : 'vertices in (x > 0.099999)',
    'kind' : 'facet',
}

field_1 = {
    'name' : 'temperature',
    'dtype' : 'real',
    'shape' : (1,),
    'region' : 'Omega',
    'approx_order' : 1,
}

variable_1 = {
    'name' : 't',
    'kind' : 'unknown field',
    'field' : 'temperature',
    'order' : 0, # order in the global vector of unknowns
}

variable_2 = {
    'name' : 's',
    'kind' : 'test field',
    'field' : 'temperature',
    'dual' : 't',
}

ebc_1 = {
    'name' : 't1',
    'region' : 'Gamma_Left',
    'dofs' : {'t.0' : 2.0},
}

ebc_2 = {
    'name' : 't2',
    'region' : 'Gamma_Right',
    'dofs' : {'t.0' : -2.0},
}

integral_1 = {
    'name' : 'i',
    'order' : 2,
}

equations = {
    'Temperature' : """dw_laplace.i.Omega( coef.val, s, t ) = 0"""
}

solver_0 = {
    'name' : 'ls',
    'kind' : 'ls.scipy_direct',
    'method' : 'auto',
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 1,
    'eps_a'      : 1e-10,
    'eps_r'      : 1.0,
    'macheps'   : 1e-16,
    'lin_red'    : 1e-2, # Linear system error < (eps_a * lin_red).
    'ls_red'     : 0.1,
    'ls_red_warp' : 0.001,
    'ls_on'      : 1.1,
    'ls_min'     : 1e-5,
    'check'     : 0,
    'delta'     : 1e-6,
    'problem'   : 'nonlinear', # 'nonlinear' or 'linear' (ignore i_max)
}

options = {
    'nls' : 'newton',
    'ls' : 'ls',
}

########NEW FILE########
__FILENAME__ = poisson_field_dependent_material
r"""
Laplace equation with a field-dependent material parameter.

Find :math:`T(t)` for :math:`t \in [0, t_{\rm final}]` such that:

.. math::
   \int_{\Omega} c(T) \nabla s \cdot \nabla T
    = 0
    \;, \quad \forall s \;.

where :math:`c(T)` is the :math:`T` dependent diffusion coefficient.
Each iteration calculates :math:`T` and adjusts :math:`c(T)`.
"""
from sfepy import data_dir
from sfepy.base.base import output

filename_mesh = data_dir + '/meshes/3d/cylinder.mesh'

t0 = 0.0
t1 = 0.1
n_step = 11

def get_conductivity(ts, coors, problem, equations=None, mode=None, **kwargs):
    """
    Calculates the conductivity as 2+10*T and returns it.
    This relation results in larger T gradients where T is small.
    """
    if mode == 'qp':
        # T-field values in quadrature points coordinates given by integral i
        # - they are the same as in `coors` argument.
        T_values = problem.evaluate('ev_volume_integrate.i.Omega(T)',
                                    mode='qp', verbose=False)
        val = 2 + 10 * (T_values + 2)

        output('conductivity: min:', val.min(), 'max:', val.max())

        val.shape = (val.shape[0] * val.shape[1], 1, 1)
        return {'val' : val}

materials = {
    'coef' : 'get_conductivity',
}

fields = {
    'temperature' : ('real', 1, 'Omega', 1),
}

variables = {
    'T' : ('unknown field', 'temperature', 0),
    's' : ('test field',    'temperature', 'T'),
}

regions = {
    'Omega' : 'all',
    'Gamma_Left' : ('vertices in (x < 0.00001)', 'facet'),
    'Gamma_Right' : ('vertices in (x > 0.099999)', 'facet'),
}

ebcs = {
    'T1' : ('Gamma_Left', {'T.0' : 2.0}),
    'T2' : ('Gamma_Right', {'T.0' : -2.0}),
}

functions = {
    'get_conductivity' : (get_conductivity,),
}

ics = {
    'ic' : ('Omega', {'T.0' : 0.0}),
}

integrals = {
    'i' : 1,
}

equations = {
    'Temperature' : """dw_laplace.i.Omega( coef.val, s, T ) = 0"""
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton', {
        'i_max' : 1,
        'eps_a' : 1e-10,
        'eps_r' : 1.0,
        'problem' : 'nonlinear'
    }),
    'ts' : ('ts.simple', {
        't0' : t0,
        't1' : t1,
        'dt' : None,
        'n_step' : n_step, # has precedence over dt!
        'quasistatic' : True,
    }),
}

options = {
    'nls' : 'newton',
    'ls' : 'ls',
    'ts' : 'ts',
    'save_steps' : -1,
}

########NEW FILE########
__FILENAME__ = poisson_functions
r"""
Poisson equation with source term.

Find :math:`u` such that:

.. math::
    \int_{\Omega} c \nabla v \cdot \nabla u
    = - \int_{\Omega_L} b v = - \int_{\Omega_L} f v p
    \;, \quad \forall v \;,

where :math:`b(x) = f(x) p(x)`, :math:`p` is a given FE field and :math:`f` is
a given general function of space.

This example demonstrates use of functions for defining material parameters,
regions, parameter variables or boundary conditions. Notably, it demonstrates
the following:

1. How to define a material parameter by an arbitrary function - see the
   function :func:`get_pars()` that evaluates :math:`f(x)` in quadrature
   points.
2. How to define a known function that belongs to a given FE space (field) -
   this function, :math:`p(x)`, is defined in a FE sense by its nodal values
   only - see the function :func:`get_load_variable()`.

In order to define the load :math:`b(x)` directly, the term ``dw_volume_dot``
should be replaced by ``dw_volume_integrate``.
"""
import numpy as nm
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/cylinder.mesh'

options = {
    'nls' : 'newton',
    'ls' : 'ls',
}

materials = {
    'm' : ({'c' : 1.0},),
    'load' : 'get_pars',
}

regions = {
    'Omega' : 'all',
    'Omega_L' : 'vertices by get_middle_ball',
    'Gamma_Left' : ('vertices in (x < 0.00001)', 'facet'),
    'Gamma_Right' : ('vertices in (x > 0.099999)', 'facet'),
}

fields = {
    'temperature' : ('real', 1, 'Omega', 1),
    'velocity' : ('real', 'vector', 'Omega', 1),
}

variables = {
    'u' : ('unknown field', 'temperature', 0),
    'v' : ('test field',    'temperature', 'u'),
    'p' : ('parameter field', 'temperature',
           {'setter' : 'get_load_variable'}),
    'w' : ('parameter field', 'velocity',
           {'setter' : 'get_convective_velocity'}),
}

ebcs = {
    'u1' : ('Gamma_Left', {'u.0' : 'get_ebc'}),
    'u2' : ('Gamma_Right', {'u.0' : -2.0}),
}

integrals = {
    'i' : 1,
}

equations = {
    'Laplace equation' :
    """dw_laplace.i.Omega( m.c, v, u )
     - dw_convect_v_grad_s.i.Omega( v, w, u )
     = - dw_volume_dot.i.Omega_L( load.f, v, p )"""
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton', {
        'i_max'      : 1,
        'eps_a'      : 1e-10,
    }),
}

def get_pars(ts, coors, mode=None, **kwargs):
    """
    Evaluate the coefficient `load.f` in quadrature points `coors` using a
    function of space.

    For scalar parameters, the shape has to be set to `(coors.shape[0], 1, 1)`.
    """
    if mode == 'qp':
        x = coors[:, 0]

        val = 55.0 * (x - 0.05)

        val.shape = (coors.shape[0], 1, 1)
        return {'f' : val}

def get_middle_ball(coors, domain=None):
    """
    Get the :math:`\Omega_L` region as a function of mesh coordinates.
    """
    x, y, z = coors[:, 0], coors[:, 1], coors[:, 2]

    r1 = nm.sqrt((x - 0.025)**2.0 + y**2.0 + z**2)
    r2 = nm.sqrt((x - 0.075)**2.0 + y**2.0 + z**2)
    flag = nm.where((r1 < 2.3e-2) | (r2 < 2.3e-2))[0]

    return flag

def get_load_variable(ts, coors, region=None):
    """
    Define nodal values of 'p' in the nodal coordinates `coors`.
    """
    y = coors[:,1]

    val = 5e5 * y
    return val

def get_convective_velocity(ts, coors, region=None):
    """
    Define nodal values of 'w' in the nodal coordinates `coors`.
    """
    val = 100.0 * nm.ones_like(coors)

    return val

def get_ebc(coors, amplitude):
    """
    Define the essential boundary conditions as a function of coordinates
    `coors` of region nodes.
    """
    z = coors[:, 2]
    val = amplitude * nm.sin(z * 2.0 * nm.pi)
    return val

functions = {
    'get_pars' : (get_pars,),
    'get_load_variable' : (get_load_variable,),
    'get_convective_velocity' : (get_convective_velocity,),
    'get_middle_ball' : (get_middle_ball,),
    'get_ebc' : (lambda ts, coor, bc, problem, **kwargs: get_ebc(coor, 5.0),),
}

########NEW FILE########
__FILENAME__ = poisson_iga
r"""
Poisson equation solved in a single patch NURBS domain using the isogeometric
analysis (IGA) approach.

Find :math:`t` such that:

.. math::
    \int_{\Omega} c \nabla s \cdot \nabla t
    =  \int_{\Omega_0} f s
    \;, \quad \forall s \;.

Try setting the Dirichlet boundary condition (ebcs) on various sides of the
domain (``'Gamma1'``, ..., ``'Gamma4'``).

View the results using::

  $ ./postproc.py patch2d.vtk --wireframe -b
  $ ./postproc.py patch2d.vtk --wireframe -b -d't,plot_warp_scalar,rel_scaling=1'
"""
from sfepy import data_dir

filename_domain = data_dir + '/meshes/iga/patch2d.iga'

materials = {
    'm' : ({'c' : 1.0, 'f' : -10.0},),
}

regions = {
    'Omega' : 'all',
    'Omega_0' : 'vertices in (x > 1.5)',
    'Gamma1' : ('vertices of set xi00', 'facet'),
    'Gamma2' : ('vertices of set xi01', 'facet'),
    'Gamma3' : ('vertices of set xi10', 'facet'),
    'Gamma4' : ('vertices of set xi11', 'facet'),
}

fields = {
    'temperature' : ('real', 1, 'Omega', None, 'H1', 'iga'),
}

variables = {
    't' : ('unknown field', 'temperature', 0),
    's' : ('test field',    'temperature', 't'),
}

ebcs = {
    't1' : ('Gamma3', {'t.0' : 2.0}),
    't2' : ('Gamma4', {'t.0' : -2.0}),
}

integrals = {
    'i' : 3,
}

equations = {
    'Temperature' : """dw_laplace.i.Omega(m.c, s, t)
                       = dw_volume_lvf.i.Omega_0(m.f, s)"""
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton', {
        'i_max'      : 1,
        'eps_a'      : 1e-10,
    }),
}

options = {
    'nls' : 'newton',
    'ls' : 'ls',
}

########NEW FILE########
__FILENAME__ = poisson_neumann
r"""
Poisson equation with Neumann boundary conditions on a part of the boundary.

Find :math:`t` such that:

.. math::
    \int_{\Omega} c \nabla s \cdot \nabla t
    = \int_{\Gamma_N} s g
    \;, \quad \forall s \;,

where :math:`g` is the given flux, :math:`g = \nabla T \cdot \ul{n}`. See the
tutorial section :ref:`poisson-weak-form-tutorial` for a detailed explanation.
"""
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/cylinder.mesh'

materials = {
    'flux' : ({'val' : -50.0},),
    'coef' : ({'val' : 2.0},),
}

regions = {
    'Omega' : 'all', # or 'cells of group 6'
    'Gamma_Left' : ('vertices in (x < 0.00001)', 'facet'),
    'Gamma_Right' : ('vertices in (x > 0.099999)', 'facet'),
    'Gamma_N' : ('vertices of surface -f (r.Gamma_Left +v r.Gamma_Right)',
                 'facet'),
}

fields = {
    'temperature' : ('real', 1, 'Omega', 1),
}

variables = {
    't' : ('unknown field', 'temperature', 0),
    's' : ('test field',    'temperature', 't'),
}

ebcs = {
    't1' : ('Gamma_Left', {'t.0' : 2.0}),
    't2' : ('Gamma_Right', {'t.0' : -2.0}),
}

equations = {
    'Temperature' : """
           dw_laplace.2.Omega(coef.val, s, t)
         = dw_surface_integrate.2.Gamma_N(flux.val, s)
    """
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton', {
        'i_max' : 1,
        'eps_a' : 1e-10,
    }),
}

options = {
    'nls' : 'newton',
    'ls' : 'ls',
}

########NEW FILE########
__FILENAME__ = poisson_parametric_study
r"""
Poisson equation.

This example demonstrates parametric study capabilities of Application
classes. In particular (written in the strong form):

.. math::
    c \Delta t = f \mbox{ in } \Omega,

    t = 2 \mbox{ on } \Gamma_1 \;,
    t = -2 \mbox{ on } \Gamma_2 \;,
    f = 1 \mbox{ in } \Omega_1 \;,
    f = 0 \mbox{ otherwise,}

where :math:`\Omega` is a square domain, :math:`\Omega_1 \in \Omega` is
a circular domain.

Now let's see what happens if :math:`\Omega_1` diameter changes.

Run::

    $ ./simple.py <this file>

and then look in 'output/r_omega1' directory, try for example::

    $ ./postproc.py output/r_omega1/circles_in_square*.vtk

Remark: this simple case could be achieved also by defining
:math:`\Omega_1` by a time-dependent function and solve the static
problem as a time-dependent problem. However, the approach below is much
more general.

Find :math:`t` such that:

.. math::
    \int_{\Omega} c \nabla s \cdot \nabla t
    = 0
    \;, \quad \forall s \;.
"""
import os
import numpy as nm

from sfepy import data_dir
from sfepy.base.base import output

# Mesh.
filename_mesh = data_dir + '/meshes/2d/special/circles_in_square.vtk'

# Options. The value of 'parametric_hook' is the function that does the
# parametric study.
options = {
    'nls' : 'newton', # Nonlinear solver
    'ls' : 'ls', # Linear solver

    'parametric_hook' : 'vary_omega1_size',
    'output_dir' : 'output/r_omega1',
}

# Domain and subdomains.
default_diameter = 0.25
regions = {
    'Omega' : 'all',
    'Gamma_1' : ('vertices in (x < -0.999)', 'facet'),
    'Gamma_2' : ('vertices in (x > 0.999)', 'facet'),
    'Omega_1' : 'vertices by select_circ',
}

# FE field defines the FE approximation: 2_3_P1 = 2D, P1 on triangles.
field_1 = {
    'name' : 'temperature',
    'dtype' : 'real',
    'shape' : (1,),
    'region' : 'Omega',
    'approx_order' : 1,
}

# Unknown and test functions (FE sense).
variables = {
    't' : ('unknown field', 'temperature', 0),
    's' : ('test field', 'temperature', 't'),
}

# Dirichlet boundary conditions.
ebcs = {
    't1' : ('Gamma_1', {'t.0' : 2.0}),
    't2' : ('Gamma_2', {'t.0' : -2.0}),
}

# Material coefficient c and source term value f.
material_1 = {
    'name' : 'coef',
    'values' : {
        'val' : 1.0,
    }
}
material_2 = {
    'name' : 'source',
    'values' : {
        'val' : 10.0,
    }
}

# Numerical quadrature and the equation.
integral_1 = {
    'name' : 'i',
    'order' : 2,
}

equations = {
    'Poisson' : """dw_laplace.i.Omega( coef.val, s, t )
                 = dw_volume_lvf.i.Omega_1( source.val, s )"""
}

# Solvers.
solver_0 = {
    'name' : 'ls',
    'kind' : 'ls.scipy_direct',
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 1,
    'eps_a'      : 1e-10,
    'eps_r'      : 1.0,
    'macheps'   : 1e-16,
    'lin_red'    : 1e-2, # Linear system error < (eps_a * lin_red).
    'ls_red'     : 0.1,
    'ls_red_warp' : 0.001,
    'ls_on'      : 1.1,
    'ls_min'     : 1e-5,
    'check'     : 0,
    'delta'     : 1e-6,
    'problem'   : 'nonlinear', # 'nonlinear' or 'linear' (ignore i_max)
}

functions = {
    'select_circ': (lambda coors, domain=None: 
                    select_circ(coors[:,0], coors[:,1], 0, default_diameter),),
}

# Functions.
def select_circ( x, y, z, diameter ):
    """Select circular subdomain of a given diameter."""
    r = nm.sqrt( x**2 + y**2 )

    out = nm.where(r < diameter)[0]

    n = out.shape[0]
    if n <= 3:
        raise ValueError( 'too few vertices selected! (%d)' % n )

    return out

def vary_omega1_size( problem ):
    """Vary size of \Omega1. Saves also the regions into options['output_dir'].

    Input:
      problem: Problem instance
    Return:
      a generator object:
      1. creates new (modified) problem
      2. yields the new (modified) problem and output container
      3. use the output container for some logging
      4. yields None (to signal next iteration to Application)
    """
    from sfepy.discrete import Problem
    from sfepy.solvers.ts import get_print_info
    
    output.prefix = 'vary_omega1_size:'

    diameters = nm.linspace( 0.1, 0.6, 7 ) + 0.001
    ofn_trunk, output_format = problem.ofn_trunk, problem.output_format
    output_dir = problem.output_dir
    join = os.path.join

    conf = problem.conf
    cf = conf.get_raw( 'functions' )
    n_digit, aux, d_format = get_print_info( len( diameters ) + 1 )
    for ii, diameter in enumerate( diameters ):
        output( 'iteration %d: diameter %3.2f' % (ii, diameter) )

        cf['select_circ'] = (lambda coors, domain=None: 
                             select_circ(coors[:,0], coors[:,1], 0, diameter),)
        conf.edit('functions', cf)
        problem = Problem.from_conf(conf)

        problem.save_regions( join( output_dir, ('regions_' + d_format) % ii ),
                              ['Omega_1'] )
        region = problem.domain.regions['Omega_1']
        if not region.has_cells():
            raise ValueError('region %s has no cells!' % region.name)

        ofn_trunk = ofn_trunk + '_' + (d_format % ii)
        problem.setup_output(output_filename_trunk=ofn_trunk,
                             output_dir=output_dir,
                             output_format=output_format)

        out = []
        yield problem, out

        out_problem, state = out[-1]

        filename = join( output_dir,
                         ('log_%s.txt' % d_format) % ii )
        fd = open( filename, 'w' )
        log_item = '$r(\Omega_1)$: %f\n' % diameter
        fd.write( log_item )
        fd.write( 'solution:\n' )
        nm.savetxt(fd, state())
        fd.close()

        yield None


########NEW FILE########
__FILENAME__ = poisson_periodic_boundary_condition
r"""
Transient Laplace equation with a localized power source and
periodic boundary conditions.

This example is using a mesh generated by gmsh. Both the
.geo script used by gmsh to generate the file and the .mesh
file can be found in meshes.

The mesh is suitable for periodic boundary conditions. It consists
of a cylinder enclosed by a box in the x and y directions.

The cylinder will act as a power source.

The transient Laplace equation will be solved in time interval
:math:`t \in [0, t_{\rm final}]`.

Find :math:`T(t)` for :math:`t \in [0, t_{\rm final}]` such that:

.. math::
    \int_{\Omega}c s \pdiff{T}{t}
    + \int_{\Omega} \sigma_2 \nabla s \cdot \nabla T
    = \int_{\Omega_2} P_3 T
    \;, \quad \forall s \;.
"""

from sfepy import data_dir
import numpy as nm
import sfepy.discrete.fem.periodic as per

filename_mesh = data_dir + '/meshes/3d/cylinder_in_box.mesh'

t0 = 0.0
t1 = 1.
n_step = 11
power_per_volume =1.e2 # Heating power per volume of the cylinder
capacity_cylinder = 1. # Heat capacity of cylinder
capacity_fill = 1. #  Heat capacity of filling material
conductivity_cylinder = 1. # Heat conductivity of cylinder
conductivity_fill = 1. # Heat conductivity of filling material

def cylinder_material_func(ts, coors, problem, mode=None, **kwargs):
    """
    Returns the thermal conductivity, the thermal mass, and the power of the
    material in the cylinder.
    """
    if mode == 'qp':
        shape = (coors.shape[0], 1, 1)

        power = nm.empty(shape, dtype=nm.float64)
        if ts.step < 5:
            # The power is turned on in the first 5 steps only.
            power.fill(power_per_volume)

        else:
            power.fill(0.0)

        conductivity = nm.ones(shape) * conductivity_cylinder
        capacity = nm.ones(shape) * capacity_cylinder

        return {'power' : power, 'capacity' : capacity,
                'conductivity' : conductivity}

materials = {
    'cylinder' : 'cylinder_material_func',
    'fill' : ({'capacity' : capacity_fill,
               'conductivity' : conductivity_fill,},),
}

fields = {
    'temperature' : ('real', 1, 'Omega', 1),
}

variables = {
    'T' : ('unknown field', 'temperature', 1, 1),
    's' : ('test field', 'temperature', 'T'),
}

regions = {
    'Omega' : 'all',
    'cylinder' : 'cells of group 444',
    'fill' : 'cells of group 555',
    'Gamma_Left' : ('vertices in (x < -2.4999)', 'facet'),
    'y+' : ('vertices in (y >2.4999)', 'facet'),
    'y-' : ('vertices in (y <-2.4999)', 'facet'),
    'z+' : ('vertices in (z >0.4999)', 'facet'),
    'z-' : ('vertices in (z <-0.4999)', 'facet'),
}

ebcs = {
    'T1' : ('Gamma_Left', {'T.0' : 0.0}),
}

# The  matching functions link the elements on each side with that on the
# opposing side.
functions = {
    'cylinder_material_func' : (cylinder_material_func,),
    "match_y_plane" : (per.match_y_plane,),
    "match_z_plane" : (per.match_z_plane,),
}

epbcs = {
    # In the y-direction
    'periodic_y' : (['y+', 'y-'], {'T.0' : 'T.0'}, 'match_y_plane'),
    # and in the z-direction. Due to the symmetry of the problem, this periodic
    # boundary condition is actually not necessary, but we include it anyway.
    'periodic_z' : (['z+', 'z-'], {'T.0' : 'T.0'}, 'match_z_plane'),
}

ics = {
    'ic' : ('Omega', {'T.0' : 0.0}),
}

integrals = {
    'i' : 1,
}

equations = {
    'Temperature' :
    """dw_volume_dot.i.cylinder( cylinder.capacity, ds/dt, dT/dt )
        dw_volume_dot.i.fill( fill.capacity, ds/dt, dT/dt )
        dw_laplace.i.cylinder( cylinder.conductivity, s, T )
        dw_laplace.i.fill( fill.conductivity, s, T )
        = dw_volume_integrate.i.cylinder( cylinder.power, s )"""
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton', {
        'i_max' : 1,
        'eps_a' : 1e-10,
        'eps_r' : 1.0,
        'problem' : 'nonlinear'
    }),
    'ts' : ('ts.simple', {
        't0' : t0,
        't1' : t1,
        'dt' : None,
        'n_step' : n_step, # has precedence over dt!
        'quasistatic' : False,
    }),
}

options = {
    'nls' : 'newton',
    'ls' : 'ls',
    'ts' : 'ts',
    'output_dir' : 'output',
    'save_steps' : -1,
}

########NEW FILE########
__FILENAME__ = poisson_short_syntax
r"""
Laplace equation using the short syntax of keywords.

See :ref:`diffusion-poisson` for the long syntax version.

Find :math:`t` such that:

.. math::
    \int_{\Omega} c \nabla s \cdot \nabla t
    = 0
    \;, \quad \forall s \;.
"""
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/cylinder.mesh'

materials = {
    'coef' : ({'val' : 1.0},),
}

regions = {
    'Omega' : 'all', # or 'cells of group 6'
    'Gamma_Left' : ('vertices in (x < 0.00001)', 'facet'),
    'Gamma_Right' : ('vertices in (x > 0.099999)', 'facet'),
}

fields = {
    'temperature' : ('real', 1, 'Omega', 1),
}

variables = {
    't' : ('unknown field', 'temperature', 0),
    's' : ('test field',    'temperature', 't'),
}

ebcs = {
    't1' : ('Gamma_Left', {'t.0' : 2.0}),
    't2' : ('Gamma_Right', {'t.0' : -2.0}),
}

integrals = {
    'i' : 2,
}

equations = {
    'Temperature' : """dw_laplace.i.Omega( coef.val, s, t ) = 0"""
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton',
                {'i_max'      : 1,
                 'eps_a'      : 1e-10,
    }),
}

options = {
    'nls' : 'newton',
    'ls' : 'ls',
}

########NEW FILE########
__FILENAME__ = sinbc
r"""
Laplace equation with Dirichlet boundary conditions given by a sine function
and constants.

Find :math:`t` such that:

.. math::
    \int_{\Omega} c \nabla s \cdot \nabla t
    = 0
    \;, \quad \forall s \;.

This example demonstrates how to use a hierarchical basis approximation - it
uses the fifth order Lobatto polynomial space for the solution. The adaptive
linearization is applied in order to save viewable results, see both the
options keyword and the ``post_process()`` function that computes the solution
gradient. Use the following commands to view the results (assuming default
output directory and names)::

  $ ./postproc.py -b -d't,plot_warp_scalar,rel_scaling=1' 2_4_2_refined_t.vtk --wireframe
  $ ./postproc.py -b 2_4_2_refined_grad.vtk

The :class:`sfepy.discrete.fem.meshio.UserMeshIO` class is used to refine the original
two-element mesh before the actual solution.
"""
import numpy as nm

from sfepy import data_dir

from sfepy.base.base import output
from sfepy.discrete.fem import Mesh, FEDomain
from sfepy.discrete.fem.meshio import UserMeshIO, MeshIO
from sfepy.homogenization.utils import define_box_regions

base_mesh = data_dir + '/meshes/elements/2_4_2.mesh'

def mesh_hook(mesh, mode):
    """
    Load and refine a mesh here.
    """
    if mode == 'read':
        mesh = Mesh.from_file(base_mesh)
        domain = FEDomain(mesh.name, mesh)
        for ii in range(3):
            output('refine %d...' % ii)
            domain = domain.refine()
            output('... %d nodes %d elements'
                   % (domain.shape.n_nod, domain.shape.n_el))

        domain.mesh.name = '2_4_2_refined'

        return domain.mesh

    elif mode == 'write':
        pass

def post_process(out, pb, state, extend=False):
    """
    Calculate gradient of the solution.
    """
    from sfepy.discrete.fem.fields_base import create_expression_output

    aux = create_expression_output('ev_grad.ie.Elements( t )',
                                   'grad', 'temperature',
                                   pb.fields, pb.get_materials(),
                                   pb.get_variables(), functions=pb.functions,
                                   mode='qp', verbose=False,
                                   min_level=0, max_level=5, eps=1e-3)
    out.update(aux)

    return out

filename_mesh = UserMeshIO(mesh_hook)

# Get the mesh bounding box.
io = MeshIO.any_from_filename(base_mesh)
bbox, dim = io.read_bounding_box(ret_dim=True)

options = {
    'nls' : 'newton',
    'ls' : 'ls',
    'post_process_hook' : 'post_process',
    'linearization' : {
        'kind' : 'adaptive',
        'min_level' : 0, # Min. refinement level to achieve everywhere.
        'max_level' : 5, # Max. refinement level.
        'eps' : 1e-3, # Relative error tolerance.
    },
}

materials = {
    'coef' : ({'val' : 1.0},),
}

regions = {
    'Omega' : 'all',
}
regions.update(define_box_regions(dim, bbox[0], bbox[1], 1e-5))

fields = {
    'temperature' : ('real', 1, 'Omega', 5, 'H1', 'lobatto'),
    # Compare with the Lagrange basis.
    ## 'temperature' : ('real', 1, 'Omega', 5, 'H1', 'lagrange'),
}

variables = {
    't' : ('unknown field', 'temperature', 0),
    's' : ('test field',    'temperature', 't'),
}

amplitude = 1.0
def ebc_sin(ts, coor, **kwargs):
    x0 = 0.5 * (coor[:, 1].min() + coor[:, 1].max())
    val = amplitude * nm.sin( (coor[:, 1] - x0) * 2. * nm.pi )
    return val

ebcs = {
    't1' : ('Left', {'t.0' : 'ebc_sin'}),
    't2' : ('Right', {'t.0' : -0.5}),
    't3' : ('Top', {'t.0' : 1.0}),
}

functions = {
    'ebc_sin' : (ebc_sin,),
}

equations = {
    'Temperature' : """dw_laplace.10.Omega( coef.val, s, t ) = 0"""
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton', {
        'i_max'      : 1,
        'eps_a'      : 1e-10,
    }),
}

########NEW FILE########
__FILENAME__ = time_poisson
r"""
Transient Laplace equation with non-constant initial conditions given by a
function.

Find :math:`T(t)` for :math:`t \in [0, t_{\rm final}]` such that:

.. math::
    \int_{\Omega} s \pdiff{T}{t}
    + \int_{\Omega} c \nabla s \cdot \nabla T
    = 0
    \;, \quad \forall s \;.
"""
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/cylinder.mesh'

t0 = 0.0
t1 = 0.1
n_step = 11

material_2 = {
    'name' : 'coef',
    'values' : {'val' : 0.01},
    'kind' : 'stationary', # 'stationary' or 'time-dependent'
}

field_1 = {
    'name' : 'temperature',
    'dtype' : 'real',
    'shape' : (1,),
    'region' : 'Omega',
    'approx_order' : 1,
}

variable_1 = {
    'name' : 'T',
    'kind' : 'unknown field',
    'field' : 'temperature',
    'order' : 0,
    'history' : 1,
}
variable_2 = {
    'name' : 's',
    'kind' : 'test field',
    'field' : 'temperature',
    'dual' : 'T',
}

regions = {
    'Omega' : 'all',
    'Gamma_Left' : ('vertices in (x < 0.00001)', 'facet'),
    'Gamma_Right' : ('vertices in (x > 0.099999)', 'facet'),
}

ebcs = {
    'T1': ('Gamma_Left', {'T.0' : 2.0}),
    'T2': ('Gamma_Right', {'T.0' : -2.0}),
}

def get_ic(coor, ic):
    """Non-constant initial condition."""
    import numpy as nm
    # Normalize x coordinate.
    mi, ma = coor[:,0].min(), coor[:,0].max()
    nx = (coor[:,0] - mi) / (ma - mi)
    return nm.where( (nx > 0.25) & (nx < 0.75 ), 8.0 * (nx - 0.5), 0.0 )

functions = {
    'get_ic' : (get_ic,),
}
    
ics = {
    'ic' : ('Omega', {'T.0' : 'get_ic'}),
}

integral_1 = {
    'name' : 'i',
    'order' : 1,
}

equations = {
    'Temperature' :
    """dw_volume_dot.i.Omega( s, dT/dt )
     + dw_laplace.i.Omega( coef.val, s, T ) = 0"""
}

solver_0 = {
    'name' : 'ls',
    'kind' : 'ls.scipy_direct',

    'presolve' : True,
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 1,
    'eps_a'      : 1e-10,
    'eps_r'      : 1.0,
    'macheps'   : 1e-16,
    'lin_red'    : 1e-2, # Linear system error < (eps_a * lin_red).
    'ls_red'     : 0.1,
    'ls_red_warp' : 0.001,
    'ls_on'      : 1.1,
    'ls_min'     : 1e-5,
    'check'     : 0,
    'delta'     : 1e-6,
    'problem'   : 'linear', # 'nonlinear' or 'linear' (ignore i_max)
}

solver_2 = {
    'name' : 'ts',
    'kind' : 'ts.simple',

    't0'    : t0,
    't1'    : t1,
    'dt'    : None,
    'n_step' : n_step, # has precedence over dt!
}

options = {
    'nls' : 'newton',
    'ls' : 'ls',
    'ts' : 'ts',
    'save_steps' : -1,
}

########NEW FILE########
__FILENAME__ = time_poisson_explicit
r"""
Transient Laplace equation.

The same example as time_poisson.py, but using the short syntax of keywords,
and explicit time-stepping.

Find :math:`T(t)` for :math:`t \in [0, t_{\rm final}]` such that:

.. math::
    \int_{\Omega} s \pdiff{T}{t}
    + \int_{\Omega} c \nabla s \cdot \nabla T
    = 0
    \;, \quad \forall s \;.
"""
from sfepy import data_dir

from examples.diffusion.time_poisson import get_ic

filename_mesh = data_dir + '/meshes/3d/cylinder.mesh'

materials = {
    'coef' : ({'val' : 0.01},),
}

regions = {
    'Omega' : 'all',
    'Gamma_Left' : ('vertices in (x < 0.00001)', 'facet'),
    'Gamma_Right' : ('vertices in (x > 0.099999)', 'facet'),
}

fields = {
    'temperature' : ('real', 1, 'Omega', 1),
}

variables = {
    'T' : ('unknown field', 'temperature', 0),
    's' : ('test field',    'temperature', 'T'),
}

ebcs = {
    't1' : ('Gamma_Left', {'T.0' : 2.0}),
    't2' : ('Gamma_Right', {'T.0' : -2.0}),
}

ics = {
    'ic' : ('Omega', {'T.0' : 'get_ic'}),
}

functions = {
    'get_ic' : (get_ic,),
}

integrals = {
    'i' : 1,
}

equations = {
    'Temperature' : """dw_laplace.i.Omega( coef.val, s, T ) = 0"""
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'ts' : ('ts.explicit', {
        't0' : 0.0,
        't1' : 1.0,
        'dt' : 0.00002,
        'n_step' : None,
        'mass' : 'dw_volume_dot.i.Omega( s, T )',
        'lumped' : False, # If True, lump mass matrix so that it is diagonal.
    }),
}

options = {
    'ls' : 'ls',
    'ts' : 'ts',
    'save_steps' : 100,
    'output_format' : 'h5',
}

########NEW FILE########
__FILENAME__ = linear_elastic_mM
import os
from sfepy import data_dir
from sfepy.base.base import nm
from sfepy.homogenization.micmac import get_homog_coefs_linear
from sfepy.homogenization.recovery import save_recovery_region, recover_micro_hook

def post_process( out, pb, state, extend = False ):
    from sfepy.base.base import Struct

    if isinstance( state, dict ):
        pass
    else:
        stress = pb.evaluate('ev_cauchy_stress.i.Omega( solid.D, u )',
                             mode='el_avg')
        strain = pb.evaluate('ev_cauchy_strain.i.Omega( u )',
                             mode='el_avg')
        out['cauchy_strain'] = Struct( name = 'output_data',
                                       mode = 'cell', data = strain,
                                       dofs = None )
        out['cauchy_stress'] = Struct( name = 'output_data',
                                       mode = 'cell', data = stress,
                                       dofs = None )

        if pb.conf.options.get('recover_micro', False):

            rname = pb.conf.options.recovery_region
            region = pb.domain.regions[rname]

            filename = os.path.join(os.path.dirname(pb.get_output_name()),
                                    'recovery_region.vtk')
            save_recovery_region(pb, rname, filename=filename);

            rstrain = pb.evaluate('ev_cauchy_strain.i.%s( u )' % rname,
                                  mode='el_avg')

            recover_micro_hook( pb.conf.options.micro_filename,
                                region, {'strain' : rstrain} )

    return out

def get_elements(coors, domain=None):
    return nm.arange(50, domain.shape.n_el, 100)

functions = {
    'get_elements' : (get_elements,),
    'get_homog' : (lambda ts, coors, mode=None, **kwargs:
                   get_homog_coefs_linear(ts, coors, mode,
                                          micro_filename=options['micro_filename']),)
}

filename_mesh = data_dir + '/meshes/3d/cylinder.mesh'

regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < 0.001)', 'facet'),
    'Right' : ('vertices in (x > 0.099)', 'facet'),
    'Recovery' : 'cells by get_elements',
}

materials = {
    'solid' : 'get_homog',
}

fields = {
    '3_displacement': ('real', 3, 'Omega', 1),
}

integrals = {
    'i' : 1,
}

variables = {
    'u' : ('unknown field', '3_displacement', 0),
    'v' : ('test field', '3_displacement', 'u'),
}

ebcs = {
    'Fixed' : ('Left', {'u.all' : 0.0}),
    'PerturbedSurface' : ('Right', {'u.0' : 0.02, 'u.1' : 0.0, 'u.2' : 0.0}),
}

equations = {
    'balance_of_forces' :
    """dw_lin_elastic.i.Omega( solid.D, v, u ) = 0""",
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton',
                { 'i_max'      : 1,
                  'eps_a'      : 1e-6,
                  'problem'   : 'nonlinear'}),
}

micro_filename = data_dir \
                 + '/examples/homogenization/linear_homogenization_up.py'
options = {
    'nls' : 'newton',
    'ls' : 'ls',
    'output_dir' : 'output',
    'post_process_hook' : 'post_process',
    'output_prefix' : 'macro:',
    'recover_micro': True,
    'recovery_region' : 'Recovery',
    'micro_filename' : micro_filename,
}

########NEW FILE########
__FILENAME__ = linear_homogenization
# 04.08.2009
#!
#! Homogenization: Linear Elasticity
#! =================================
#$ \centerline{Example input file, \today}

#! Homogenization of heterogeneous linear elastic material

import sfepy.discrete.fem.periodic as per
from sfepy.mechanics.matcoefs import stiffness_from_youngpoisson
from sfepy.homogenization.utils import define_box_regions
import sfepy.homogenization.coefs_base as cb
from sfepy import data_dir
from sfepy.base.base import Struct
from sfepy.homogenization.recovery import compute_micro_u, compute_stress_strain_u, compute_mac_stress_part

def recovery_le( pb, corrs, macro ):

    out = {}

    dim = corrs['corrs_le']['u_00'].shape[1]
    mic_u = - compute_micro_u( corrs['corrs_le'], macro['strain'], 'u', dim )

    out['u_mic'] = Struct( name = 'output_data',
                           mode = 'vertex', data = mic_u,
                           var_name = 'u', dofs = None )

    stress_Y, strain_Y = compute_stress_strain_u( pb, 'i', 'Y', 'mat.D', 'u', mic_u )
    stress_Y += compute_mac_stress_part( pb, 'i', 'Y', 'mat.D', 'u', macro['strain'] )

    strain = macro['strain'] + strain_Y

    out['cauchy_strain'] = Struct( name = 'output_data',
                                   mode = 'cell', data = strain,
                                   dofs = None )
    out['cauchy_stress'] = Struct( name = 'output_data',
                                   mode = 'cell', data = stress_Y,
                                   dofs = None )
    return out

#! Mesh
#! ----
filename_mesh = data_dir + '/meshes/3d/matrix_fiber.mesh'
dim = 3
region_lbn = (0, 0, 0)
region_rtf = (1, 1, 1)
#! Regions
#! -------
#! Regions, edges, ...
regions = {
    'Y' : 'all',
    'Ym' : 'cells of group 1',
    'Yc' : 'cells of group 2',
}
regions.update( define_box_regions( dim, region_lbn, region_rtf ) )
#! Materials
#! ---------
materials = {
    'mat' : ({'D' : {'Ym': stiffness_from_youngpoisson(dim, 7.0e9, 0.4),
                     'Yc': stiffness_from_youngpoisson(dim, 70.0e9, 0.2)}},),
}
#! Fields
#! ------
#! Scalar field for corrector basis functions.
fields = {
    'corrector' : ('real', dim, 'Y', 1),
}
#! Variables
#! ---------
#! Unknown and corresponding test variables. Parameter fields
#! used for evaluation of homogenized coefficients.
variables = {
    'u'     : ('unknown field', 'corrector', 0),
    'v'     : ('test field', 'corrector', 'u'),
    'Pi'    : ('parameter field', 'corrector', 'u'),
    'Pi1' : ('parameter field', 'corrector', '(set-to-None)'),
    'Pi2' : ('parameter field', 'corrector', '(set-to-None)'),
}
#! Functions
functions = {
    'match_x_plane' : (per.match_x_plane,),
    'match_y_plane' : (per.match_y_plane,),
    'match_z_plane' : (per.match_z_plane,),
}
#! Boundary Conditions
#! -------------------
#! Fixed nodes.
ebcs = {
    'fixed_u' : ('Corners', {'u.all' : 0.0}),
}
if dim == 3:
    epbcs = {
        'periodic_x' : (['Left', 'Right'], {'u.all' : 'u.all'}, 'match_x_plane'),
        'periodic_y' : (['Near', 'Far'], {'u.all' : 'u.all'}, 'match_y_plane'),
        'periodic_z' : (['Top', 'Bottom'], {'u.all' : 'u.all'}, 'match_z_plane'),
}
else:
    epbcs = {
        'periodic_x' : (['Left', 'Right'], {'u.all' : 'u.all'}, 'match_x_plane'),
        'periodic_y' : (['Bottom', 'Top'], {'u.all' : 'u.all'}, 'match_y_plane'),
    }
all_periodic = ['periodic_%s' % ii for ii in ['x', 'y', 'z'][:dim] ]
#! Integrals
#! ---------
#! Define the integral type Volume/Surface and quadrature rule.
integrals = {
    'i' : 2,
}
#! Options
#! -------
#! Various problem-specific options.
options = {
    'coefs' : 'coefs',
    'requirements' : 'requirements',
    'ls' : 'ls', # linear solver to use
    'volume' : { 'variables' : ['u'],
                 'expression' : 'd_volume.i.Y( u )' },
    'output_dir' : 'output',
    'coefs_filename' : 'coefs_le',
    'recovery_hook' : 'recovery_le',
}
#! Equations
#! ---------
#! Equations for corrector functions.
equation_corrs = {
    'balance_of_forces' :
    """dw_lin_elastic.i.Y(mat.D, v, u ) =
     - dw_lin_elastic.i.Y(mat.D, v, Pi )"""
}
#! Expressions for homogenized linear elastic coefficients.
expr_coefs = """dw_lin_elastic.i.Y(mat.D, Pi1, Pi2 )"""
#! Coefficients
#! ------------
#! Definition of homogenized acoustic coefficients.
def set_elastic(variables, ir, ic, mode, pis, corrs_rs):
    mode2var = {'row' : 'Pi1', 'col' : 'Pi2'}

    val = pis.states[ir, ic]['u'] + corrs_rs.states[ir, ic]['u']

    variables[mode2var[mode]].set_data(val)

coefs = {
    'D' : {
        'requires' : ['pis', 'corrs_rs'],
        'expression' : expr_coefs,
        'set_variables' : set_elastic,
        'class' : cb.CoefSymSym,
    },
    'filenames' : {},
}

requirements = {
    'pis' : {
        'variables' : ['u'],
        'class' : cb.ShapeDimDim,
    },
    'corrs_rs' : {
        'requires' : ['pis'],
        'ebcs' : ['fixed_u'],
        'epbcs' : all_periodic,
        'equations' : equation_corrs,
        'set_variables' : [('Pi', 'pis', 'u')],
        'class' : cb.CorrDimDim,
        'save_name' : 'corrs_le',
        'dump_variables' : ['u'],
    },
}
#! Solvers
#! -------
#! Define linear and nonlinear solver.
solvers = {
    'ls' : ('ls.umfpack', {}),
    'newton' : ('nls.newton', {'i_max' : 1,
                               'eps_a' : 1e-4,
                               'problem' : 'nonlinear',
                               })
}

########NEW FILE########
__FILENAME__ = linear_homogenization_up
# mixed formulation

# 07.08.2009
#!
#! Homogenization: Linear Elasticity
#! =================================
#$ \centerline{Example input file, \today}

#! Homogenization of heterogeneous linear elastic material - mixed formulation
import numpy as nm

import sfepy.discrete.fem.periodic as per
from sfepy.mechanics.matcoefs import stiffness_from_youngpoisson_mixed, bulk_from_youngpoisson
from sfepy.homogenization.utils import define_box_regions, get_box_volume
import sfepy.homogenization.coefs_base as cb

from sfepy import data_dir
from sfepy.base.base import Struct
from sfepy.homogenization.recovery import compute_micro_u, compute_stress_strain_u, compute_mac_stress_part, add_stress_p

def recovery_le( pb, corrs, macro ):

    out = {}
    dim = corrs['corrs_le']['u_00'].shape[1]
    mic_u = - compute_micro_u( corrs['corrs_le'], macro['strain'], 'u', dim )
    mic_p = - compute_micro_u( corrs['corrs_le'], macro['strain'], 'p', dim )

    out['u_mic'] = Struct( name = 'output_data',
                           mode = 'vertex', data = mic_u,
                           var_name = 'u', dofs = None )
    out['p_mic'] = Struct( name = 'output_data',
                           mode = 'cell', data = mic_p[:,nm.newaxis,
                                                       :,nm.newaxis],
                           var_name = 'p', dofs = None )

    stress_Y, strain_Y = compute_stress_strain_u( pb, 'i', 'Y', 'mat.D', 'u', mic_u )
    stress_Y += compute_mac_stress_part( pb, 'i', 'Y', 'mat.D', 'u', macro['strain'] )
    add_stress_p( stress_Y, pb, 'i', 'Y', 'p', mic_p )

    strain = macro['strain'] + strain_Y

    out['cauchy_strain'] = Struct( name = 'output_data',
                                   mode = 'cell', data = strain,
                                   dofs = None )
    out['cauchy_stress'] = Struct( name = 'output_data',
                                   mode = 'cell', data = stress_Y,
                                   dofs = None )
    return out

#! Mesh
#! ----
dim = 3
filename_mesh = data_dir + '/meshes/3d/matrix_fiber.mesh'
region_lbn = (0, 0, 0)
region_rtf = (1, 1, 1)
#! Regions
#! -------
#! Regions, edges, ...
regions = {
    'Y' : 'all',
    'Ym' : 'cells of group 1',
    'Yc' : 'cells of group 2',
}
regions.update( define_box_regions( dim, region_lbn, region_rtf ) )
#! Materials
#! ---------
materials = {
    'mat' : ({'D' : {'Ym': stiffness_from_youngpoisson_mixed(dim, 7.0e9, 0.4),
                     'Yc': stiffness_from_youngpoisson_mixed(dim, 70.0e9, 0.2)},
              'gamma': {'Ym': 1.0/bulk_from_youngpoisson(7.0e9, 0.4),
                        'Yc': 1.0/bulk_from_youngpoisson(70.0e9, 0.2)}},),
}
#! Fields
#! ------
#! Scalar field for corrector basis functions.
fields = {
    'corrector_u' : ('real', dim, 'Y', 1),
    'corrector_p' : ('real', 1, 'Y', 0),
}
#! Variables
#! ---------
#! Unknown and corresponding test variables. Parameter fields
#! used for evaluation of homogenized coefficients.
variables = {
    'u'     : ('unknown field', 'corrector_u'),
    'v'     : ('test field', 'corrector_u', 'u'),
    'p'     : ('unknown field', 'corrector_p'),
    'q'     : ('test field', 'corrector_p', 'p'),
    'Pi'    : ('parameter field', 'corrector_u', 'u'),
    'Pi1u' : ('parameter field', 'corrector_u', '(set-to-None)'),
    'Pi2u' : ('parameter field', 'corrector_u', '(set-to-None)'),
    'Pi1p' : ('parameter field', 'corrector_p', '(set-to-None)'),
    'Pi2p' : ('parameter field', 'corrector_p', '(set-to-None)'),
}
#! Functions
functions = {
    'match_x_plane' : (per.match_x_plane,),
    'match_y_plane' : (per.match_y_plane,),
    'match_z_plane' : (per.match_z_plane,),
}
#! Boundary Conditions
#! -------------------
#! Fixed nodes.
ebcs = {
    'fixed_u' : ('Corners', {'u.all' : 0.0}),
}
if dim == 3:
    epbcs = {
        'periodic_x' : (['Left', 'Right'], {'u.all' : 'u.all'}, 'match_x_plane'),
        'periodic_y' : (['Near', 'Far'], {'u.all' : 'u.all'}, 'match_y_plane'),
        'periodic_z' : (['Top', 'Bottom'], {'u.all' : 'u.all'}, 'match_z_plane'),
    }
else:
    epbcs = {
        'periodic_x' : (['Left', 'Right'], {'u.all' : 'u.all'}, 'match_x_plane'),
        'periodic_y' : (['Bottom', 'Top'], {'u.all' : 'u.all'}, 'match_y_plane'),
    }
all_periodic = ['periodic_%s' % ii for ii in ['x', 'y', 'z'][:dim] ]
#! Integrals
#! ---------
#! Define the integral type Volume/Surface and quadrature rule.
integrals = {
    'i' : 2,
}
#! Options
#! -------
#! Various problem-specific options.
options = {
    'coefs' : 'coefs',
    'requirements' : 'requirements',
    'ls' : 'ls', # linear solver to use
    'volume' : { #'variables' : ['u'],
                 #'expression' : 'd_volume.i.Y( u )',
                 'value' : get_box_volume( dim, region_lbn, region_rtf ),
                 },
    'output_dir' : 'output',
    'coefs_filename' : 'coefs_le_up',
    'recovery_hook' : 'recovery_le',
}
#! Equations
#! ---------
#! Equations for corrector functions.
equation_corrs = {
    'balance_of_forces' :
    """  dw_lin_elastic.i.Y( mat.D, v, u )
       - dw_stokes.i.Y( v, p ) =
       - dw_lin_elastic.i.Y( mat.D, v, Pi )""",
    'pressure constraint' :
    """- dw_stokes.i.Y( u, q )
       - dw_volume_dot.i.Y( mat.gamma, q, p ) =
       + dw_stokes.i.Y( Pi, q )""",
}
#! Expressions for homogenized linear elastic coefficients.
expr_coefs = {
    'Q1' : """dw_lin_elastic.i.Y( mat.D, Pi1u, Pi2u )""",
    'Q2' : """dw_volume_dot.i.Y( mat.gamma, Pi1p, Pi2p )""",
}
#! Coefficients
#! ------------
#! Definition of homogenized acoustic coefficients.
def set_elastic_u(variables, ir, ic, mode, pis, corrs_rs):
    mode2var = {'row' : 'Pi1u', 'col' : 'Pi2u'}

    val = pis.states[ir, ic]['u'] + corrs_rs.states[ir, ic]['u']

    variables[mode2var[mode]].set_data(val)

coefs = {
    'elastic_u' : {
        'requires' : ['pis', 'corrs_rs'],
        'expression' : expr_coefs['Q1'],
        'set_variables' : set_elastic_u,
        'class' : cb.CoefSymSym,
    },
    'elastic_p' : {
        'requires' : ['corrs_rs'],
        'expression' : expr_coefs['Q2'],
        'set_variables' : [('Pi1p', 'corrs_rs', 'p'), ('Pi2p', 'corrs_rs', 'p')],
        'class' : cb.CoefSymSym,
    },
    'D' : {
        'requires' : ['c.elastic_u', 'c.elastic_p'],
        'class' : cb.CoefSum,
    },
    'filenames' : {},
}

requirements = {
    'pis' : {
        'variables' : ['u'],
        'class' : cb.ShapeDimDim,
    },
    'corrs_rs' : {
        'requires' : ['pis'],
        'ebcs' : ['fixed_u'],
        'epbcs' : all_periodic,
        'equations' : equation_corrs,
        'set_variables' : [('Pi', 'pis', 'u')],
        'class' : cb.CorrDimDim,
        'save_name' : 'corrs_le',
        'dump_variables' : ['u', 'p'],
        'is_linear' : True,
    },
}
#! Solvers
#! -------
#! Define linear and nonlinear solver.
solvers = {
    'ls' : ('ls.umfpack', {}),
    'newton' : ('nls.newton', {'i_max' : 1,
                               'eps_a' : 1e-4,
                               'problem' : 'nonlinear', })
}

########NEW FILE########
__FILENAME__ = perfusion_micro
r"""
Homogenization of the Darcy flow in a thin porous layer.
The reference cell is composed of the matrix representing the dual porosity
and of two disconnected channels representing the primary porosity,
see paper [1].

[1] http://seth.asc.tuwien.ac.at/proc12/full_paper/Contribution183.pdf
"""

from sfepy.discrete.fem.periodic import match_x_plane, match_y_plane
import sfepy.homogenization.coefs_base as cb
import numpy as nm
from sfepy import data_dir

def get_mats(pk, ph, pe, dim):

    m1 = nm.eye(dim, dtype=nm.float64) * pk
    m1[-1,-1] = pk / ph
    m2 = nm.eye(dim, dtype=nm.float64) * pk
    m2[-1,-1] = pk / ph ** 2

    return m1, m2

def recovery_perf(pb, corrs, macro):

    from sfepy.homogenization.recovery import compute_p_from_macro
    from sfepy.base.base import Struct

    slev = ''

    micro_coors = pb.domain.mesh.coors
    micro_nnod = pb.domain.mesh.n_nod

    centre_Y = nm.sum(pb.domain.mesh.coors, axis=0) / micro_nnod
    nodes_Y = {}

    channels = {}
    for k in macro.iterkeys():
        if 'press' in k:
            channels[k[-1]] = 1

    channels = channels.keys()

    varnames = ['pM']
    for ch in channels:
        nodes_Y[ch] = pb.domain.regions['Y' + ch].vertices
        varnames.append('p' + ch)

    pvars = pb.create_variables(varnames)

    press = {}

    # matrix
    press['M'] = \
       corrs['corrs_%s_gamma_p' % pb_def['name']]['pM'] * macro['g_p'] + \
       corrs['corrs_%s_gamma_m' % pb_def['name']]['pM'] * macro['g_m']

    out = {}
    # channels
    for ch in channels:
        press_mac = macro['press' + ch][0,0]
        press_mac_grad = macro['pressg' + ch]
        nnod = corrs['corrs_%s_pi%s' % (pb_def['name'], ch)]\
          ['p%s_0' % ch].shape[0]

        press_mic = nm.zeros( (nnod,1) )
        for key, val in \
          corrs['corrs_%s_pi%s' % (pb_def['name'], ch)].iteritems():
            kk = int( key[-1] )
            press_mic += val * press_mac_grad[kk,0]

        for key in corrs.iterkeys():
            if ('_gamma_' + ch in key):
                kk = int(key[-1]) - 1
                press_mic += corrs[key]['p' + ch] * macro['g' + ch][kk]

        press_mic += \
          compute_p_from_macro(press_mac_grad[nm.newaxis,nm.newaxis,:,:],
                               micro_coors[nodes_Y[ch]], 0,
                               centre=centre_Y, extdim=-1).reshape((nnod,1))

        press[ch] = press_mac + eps0 * press_mic

        out[slev + 'p' + ch] = Struct(name='output_data',
                                      mode='vertex',
                                      data=press[ch],
                                      var_name='p' + ch,
                                      dofs=None)

        pvars['p' + ch].set_data(press_mic)
        dvel = pb.evaluate('ev_diffusion_velocity.iV.Y%s(mat1%s.k, p%s)'\
                           % (ch, ch, ch),
                           var_dict = {'p' + ch: pvars['p' + ch]},
                           mode='el_avg')

        out[slev + 'w' + ch] = Struct(name='output_data',
                                      mode='cell',
                                      data=dvel,
                                      var_name='w' + ch,
                                      dofs=None)

        press['M'] += corrs['corrs_%s_eta%s' % (pb_def['name'], ch)]['pM']\
          * press_mac

    pvars['pM'].set_data(press['M'])
    dvel = pb.evaluate('%e * ev_diffusion_velocity.iV.YM(mat1M.k, pM)' % eps0,
                       var_dict = {'pM': pvars['pM']}, mode='el_avg')

    out[slev + 'pM'] = Struct(name='output_data',
                              mode='vertex',
                              dat =press['M'],
                              var_name='pM',
                              dofs=None)

    out[slev + 'wM'] = Struct(name='output_data',
                              mode='cell',
                              data=dvel,
                              var_name='wM',
                              dofs=None )

    return out

geoms = {
    '2_4': ['2_4_Q1', '2', 5],
    '3_8': ['3_8_Q1', '4', 5],
    '3_4': ['3_4_P1', '3', 3],
    }

pb_def = {
    'name': '3d_2ch',
    'mesh_filename': data_dir + '/meshes/3d/perfusion_micro3d.mesh',
    'dim': 3,
    'geom': geoms['3_4'],
    'eps0': 1.0e-2,
    'param_h': 1.0,
    'param_kappa_m': 0.1,
    'matrix_mat_el_grp': 3,
    'channels': {
        'A': {
            'mat_el_grp': 1,
            'fix_nd_grp': (4, 1),
            'io_nd_grp': [ 1, 2, 3 ],
            'param_kappa_ch': 1.0,
            },
        'B': {
            'mat_el_grp': 2,
            'fix_nd_grp': (14, 11),
            'io_nd_grp': [ 11, 12, 13 ],
            'param_kappa_ch': 2.0,
            },
    },
}

filename_mesh = pb_def['mesh_filename']
eps0 = pb_def['eps0']
param_h = pb_def['param_h']

# integrals
integrals = {
    'iV' : 2,
    'iS' : 2,
}

functions = {
    'match_x_plane': (match_x_plane,),
    'match_y_plane': (match_y_plane,),
    }

aux = []
for ch, val in pb_def['channels'].iteritems():
    aux.append( 'r.bYM' + ch )

# basic regions
regions = {
    'Y': 'all',
    'YM': 'cells of group %d' % pb_def['matrix_mat_el_grp'],
    # periodic boundaries
    'Pl': ('vertices in (x < 0.001)', 'facet'),
    'Pr': ('vertices in (x > 0.999)', 'facet'),
    'PlYM': ('r.Pl *v r.YM', 'facet'),
    'PrYM': ('r.Pr *v r.YM', 'facet'),
    'bYMp': ('r.bYp *v r.YM', 'facet', 'YM'),
    'bYMm': ('r.bYm *v r.YM', 'facet', 'YM'),
    'bYMpm': ('r.bYMp +v r.bYMm', 'facet', 'YM'),
}

# matrix/channel boundaries
regions.update({
    'bYMchs': (' +v '.join(aux), 'facet', 'YM'),
    'YMmchs': 'r.YM -v r.bYMchs',
    })

# boundary conditions Gamma+/-
ebcs = {
    'gamma_pm_bYMchs': ('bYMchs', {'pM.0': 0.0}),
    'gamma_pm_YMmchs': ('YMmchs', {'pM.0': 1.0}),
    }

# periodic boundary conditions - matrix, X-direction
epbcs = {'periodic_xYM': (['PlYM', 'PrYM'], {'pM.0': 'pM.0'}, 'match_x_plane')}
lcbcs = {}

all_periodicYM = ['periodic_%sYM' % ii for ii in ['x', 'y'][:pb_def['dim']-1] ]
all_periodicY = {}

if pb_def['dim'] == 2:
    regions.update( {
        'bYm': ('vertices in (y < 0.001)', 'facet'),
        'bYp':  ('vertices in (y > 0.999)', 'facet'),
        } )
if pb_def['dim'] == 3:
    regions.update( {
        'Pn': ('vertices in (y < 0.001)', 'facet'),
        'Pf': ('vertices in (y > 0.999)', 'facet'),
        'PnYM': ('r.Pn *v r.YM', 'facet'),
        'PfYM': ('r.Pf *v r.YM', 'facet'),
        'bYm': ('vertices in (z < 0.001)', 'facet'),
        'bYp':  ('vertices in (z > 0.999)', 'facet'),
        } )
    # periodic boundary conditions - matrix, Y-direction
    epbcs.update( {
        'periodic_yYM': (['PnYM', 'PfYM'], {'pM.0': 'pM.0'}, 'match_y_plane'),
        } )

reg_io = {}
ebcs_eta = {}
ebcs_gamma = {}

# generate regions, ebcs, epbcs
for ch, val in pb_def['channels'].iteritems():

    all_periodicY[ch] = ['periodic_%sY%s' % (ii, ch)\
                         for ii in ['x', 'y'][:pb_def['dim']-1] ]

    # channels: YA, fixedYA, bYMA (matrix/channel boundaries)
    regions.update( {
        'Y' + ch: 'cells of group %d' % val['mat_el_grp'],
        'bYM' + ch: ('r.YM *v r.Y' + ch, 'facet', 'YM'),
        'PlY' + ch: ('r.Pl *v r.Y' + ch, 'facet'),
        'PrY' + ch: ('r.Pr *v r.Y' + ch, 'facet'),
        } )

    if 'fix_nd_grp' in val:
        regions.update({
            'fixedY' + ch: 'vertices of group %d' % val['fix_nd_grp'][0],
            })

    ebcs_eta[ch] = []
    for ch2, val2 in pb_def['channels'].iteritems():
        aux = 'eta%s_bYM%s' % (ch, ch2)
        if ch2 == ch:
            ebcs.update( {aux: ('bYM' + ch2, {'pM.0': 1.0})} )
        else:
            ebcs.update( {aux: ('bYM' + ch2, {'pM.0': 0.0})} )

        ebcs_eta[ch].append(aux)

    # boundary conditions
    # periodic boundary conditions - channels, X-direction
    epbcs.update({
            'periodic_xY' + ch: (['PlY' + ch, 'PrY' + ch],
                                 {'p%s.0' % ch: 'p%s.0' % ch},
                                 'match_x_plane'),
            })

    if pb_def['dim'] == 3:
        regions.update({
                'PnY' + ch: ('r.Pn *v r.Y' + ch, 'facet'),
                'PfY' + ch: ('r.Pf *v r.Y' + ch, 'facet'),
                })
        # periodic boundary conditions - channels, Y-direction
        epbcs.update({
                'periodic_yY' + ch: (['PnY' + ch, 'PfY' + ch],
                                     {'p%s.0' % ch: 'p%s.0' % ch},
                                     'match_y_plane'),
            })

    reg_io[ch] = []
    aux_bY = []
    # channel: inputs/outputs
    for i_io in range( len( val['io_nd_grp'] ) ):
        io = '%s_%d' % (ch, i_io+1)

        # regions
        aux = val['io_nd_grp'][i_io]
        if 'fix_nd_grp' in val and val['fix_nd_grp'][1] == aux:
            regions.update( {
                'bY%s' % io : ('vertices of group %d +v r.fixedY%s' % (aux, ch),
                               'facet', 'Y%s' % ch),
                } )
        else:
            regions.update( {
                'bY%s' % io : ('vertices of group %d' % aux,
                               'facet', 'Y%s' % ch),
            } )

        aux_bY.append('r.bY%s' % io)
        reg_io[ch].append('bY%s' % io)

    regions.update({
        'bY' + ch : (' +v '.join(aux_bY), 'facet', 'Y' + ch),
        })

    # channel: inputs/outputs
    for i_io in range( len( val['io_nd_grp'] ) ):
        io = '%s_%d' % (ch, i_io + 1)
        ion = '%s_n%d' % (ch, i_io + 1)
        regions.update({
            'bY%s' % ion: ('r.bY%s -v r.bY%s' % (ch, io), 'facet', 'Y%s' % ch),
            })

        # boundary conditions
        aux = 'fix_p%s_bY%s' % (ch, ion)
        ebcs.update({
            aux: ('bY%s' % ion, {'p%s.0' % ch: 0.0}),
            })

    lcbcs.update({
        'imv' + ch: ('Y' + ch, {'ls%s.all' % ch: 'integral_mean_value'})
        })

###########################################################################
# materials, fields, variables, integrals

matk1, matk2 = get_mats(pb_def['param_kappa_m'], param_h, eps0, pb_def['dim'])

materials = {
    'mat1M': ({'k': matk1},),
    'mat2M': ({'k': matk2},),
}

fields = {
    'corrector_M': ('real', 'scalar', 'YM', 1),
    'vel_M': ('real', 'vector', 'YM', 1),
    'vol_all': ('real', 'scalar', 'Y', 1),
}

variables = {
    'pM': ('unknown field', 'corrector_M'),
    'qM': ('test field', 'corrector_M', 'pM'),
    'Pi_M': ('parameter field', 'corrector_M', '(set-to-None)'),
    'corr_M': ('parameter field', 'corrector_M', '(set-to-None)'),
    'corr1_M': ('parameter field', 'corrector_M', '(set-to-None)'),
    'corr2_M': ('parameter field', 'corrector_M', '(set-to-None)'),
    'wM': ('parameter field', 'vel_M', '(set-to-None)'),
    'vol_all': ('parameter field', 'vol_all', '(set-to-None)'),
}

# generate regions for channel inputs/outputs
for ch, val in pb_def['channels'].iteritems():

    matk1, matk2 = get_mats(val['param_kappa_ch'],  param_h,
                            eps0, pb_def['dim'])
    materials.update({
        'mat1' + ch: ({'k': matk1},),
        'mat2' + ch: ({'k': matk2},),
        })

    fields.update({
        'corrector_' + ch: ('real', 'scalar', 'Y' + ch, 1),
        'vel_' + ch: ('real', 'vector', 'Y' + ch, 1),
        })

    variables.update({
        'p' + ch: ('unknown field', 'corrector_' + ch),
        'q' + ch: ('test field', 'corrector_' + ch, 'p' + ch),
        'Pi_' + ch: ('parameter field', 'corrector_' + ch, '(set-to-None)'),
        'corr1_' + ch: ('parameter field', 'corrector_' + ch, '(set-to-None)'),
        'corr2_' + ch: ('parameter field', 'corrector_' + ch, '(set-to-None)'),
        'w' + ch: ('unknown field', 'vel_' + ch),
        # lagrange mutltipliers - integral mean value
        'ls' + ch: ('unknown field', 'corrector_' + ch),
        'lv' + ch: ('test field', 'corrector_' + ch, 'ls' + ch),
        })

###########################################################################

options = {
    'coefs': 'coefs',
    'requirements': 'requirements',
    'ls': 'ls', # linear solver to use
    'volumes': {
        'total': {
            'variables': ['vol_all'],
            'expression': """d_volume.iV.Y(vol_all)""",
            },
        'one': {
            'value': 1.0,
            }
        },
    'output_dir': './output',
    'file_per_var': True,
    'coefs_filename': 'coefs_perf_' + pb_def['name'],
    'coefs_info': {'eps0': eps0},
    'recovery_hook': 'recovery_perf',
    }

for ipm in ['p', 'm']:
    options['volumes'].update({
        'bYM' + ipm: {
            'variables': ['pM'],
            'expression': "d_surface.iS.bYM%s(pM)" % ipm,
            },
        'bY' + ipm: {
            'variables': ['vol_all'],
            'expression': "d_surface.iS.bY%s(vol_all)" % ipm,
            }
        })

for ch in reg_io.iterkeys():
    for ireg in reg_io[ch]:
        options['volumes'].update({
            ireg: {
                'variables': ['p' + ch],
                'expression': "d_surface.iS.%s(p%s)" % (ireg, ch),
                }
            })

###########################################################################
# equations, correctors, coefficients

coefs = {
    'vol_bYMpm': {
        'regions': ['bYMp', 'bYMm'],
        'expression': 'd_surface.iS.%s(pM)',
        'class': cb.VolumeFractions,
        },
    'filenames': {},
    }

# requirements for perfusion homog. coefficients
requirements = {
    'corrs_one_YM': {
        'variable': ['pM'],
        'ebcs': ['gamma_pm_YMmchs', 'gamma_pm_bYMchs'],
        'epbcs': [],
        'save_name': 'corrs_one_YM',
        'class': cb.CorrSetBCS,
        'dump_variables': ['pM'],
        },
    }

for ipm in ['p', 'm']:
    requirements.update({
        'corrs_gamma_' + ipm: {
            'requires': [],
            'ebcs': ['gamma_pm_bYMchs'],
            'epbcs': all_periodicYM,
            'equations': {
                'eq_gamma_pm': """dw_diffusion.iV.YM(mat2M.k, qM, pM) =
                             %e * dw_surface_integrate.iS.bYM%s(qM)"""\
                             % (1.0/param_h, ipm),
                },
            'class': cb.CorrOne,
            'save_name': 'corrs_%s_gamma_%s' % (pb_def['name'], ipm),
            'dump_variables': ['pM'],
            },
        })

    for ipm2 in ['p', 'm']:
        coefs.update({
            'H' + ipm + ipm2: { # test+
                'requires': ['corrs_gamma_' + ipm],
                'set_variables': [('corr_M', 'corrs_gamma_' + ipm, 'pM')],
                'expression': 'ev_surface_integrate.iS.bYM%s(corr_M)' % ipm2,
                'set_volume': 'bYp',
                'class': cb.CoefOne,
                },
        })

def get_channel(keys, bn):
    for ii in keys:
        if bn in ii:
            return ii[(ii.rfind(bn) + len(bn)):]

    return None

def set_corrpis(variables, ir, ic, mode, **kwargs):
    ch = get_channel(kwargs.keys(), 'pis_')
    pis = kwargs['pis_' + ch]
    corrs_pi = kwargs['corrs_pi' + ch]

    if mode == 'row':
        val = pis.states[ir]['p' + ch] + corrs_pi.states[ir]['p' + ch]
        variables['corr1_' + ch].set_data(val)
    elif mode == 'col':
        val = pis.states[ic]['p' + ch] + corrs_pi.states[ic]['p' + ch]
        variables['corr2_' + ch].set_data(val)

def set_corr_S(variables, ir, **kwargs):
    ch = get_channel(kwargs.keys(), 'pis_')
    io = get_channel(kwargs.keys(), 'corrs_gamma_')

    pis = kwargs['pis_' + ch]
    corrs_gamma = kwargs['corrs_gamma_' + io]

    pi = pis.states[ir]['p' + ch]
    val = corrs_gamma.state['p' + ch]
    variables['corr1_' + ch].set_data(pi)
    variables['corr2_' + ch].set_data(val)

def set_corr_cc(variables, ir, **kwargs):
    ch = get_channel(kwargs.keys(), 'pis_')
    pis = kwargs['pis_' + ch]
    corrs_pi = kwargs['corrs_pi' + ch]

    pi = pis.states[ir]['p' + ch]
    pi = pi - nm.mean(pi)
    val = pi + corrs_pi.states[ir]['p' + ch]
    variables['corr1_' + ch].set_data(val)

for ch, val in pb_def['channels'].iteritems():

    coefs.update({
        'G' + ch: {  # test+
            'requires': ['corrs_one' + ch, 'corrs_eta' + ch],
            'set_variables': [('corr1_M', 'corrs_one' + ch, 'pM'),
                              ('corr2_M', 'corrs_eta' + ch, 'pM')],
            'expression': 'dw_diffusion.iV.YM(mat2M.k, corr1_M, corr2_M)',
            'class': cb.CoefOne,
            },
        'K' + ch: {  # test+
            'requires': ['pis_' + ch, 'corrs_pi' + ch],
            'set_variables': set_corrpis,
            'expression': 'dw_diffusion.iV.Y%s(mat2%s.k, corr1_%s, corr2_%s)'\
                           % ((ch,) * 4),
            'dim': pb_def['dim'] - 1,
            'class': cb.CoefDimDim,
            },
        })

    requirements.update({
        'pis_' + ch: {
            'variables': ['p' + ch],
            'class': cb.ShapeDim,
            },
        'corrs_one' + ch: {
            'variable': ['pM'],
            'ebcs': ebcs_eta[ch],
            'epbcs': [],
            'save_name': 'corrs_%s_one%s' % (pb_def['name'], ch),
            'dump_variables': ['pM'],
            'class': cb.CorrSetBCS,
             },
        'corrs_eta' + ch: {
            'ebcs': ebcs_eta[ch],
            'epbcs': all_periodicYM,
            'equations': {
                'eq_eta': 'dw_diffusion.iV.YM(mat2M.k, qM, pM) = 0',
                },
            'class': cb.CorrOne,
            'save_name': 'corrs_%s_eta%s' % (pb_def['name'], ch),
            'dump_variables': ['pM'],
            },
        'corrs_pi' + ch: {
            'requires': ['pis_' + ch],
            'set_variables': [('Pi_' + ch, 'pis_' + ch, 'p' + ch)],
            'ebcs': [],
            'epbcs': all_periodicY[ch],
            'lcbcs': ['imv' + ch],
            'equations': {
                'eq_pi': """dw_diffusion.iV.Y%s(mat2%s.k, q%s, p%s)
                            + dw_volume_dot.iV.Y%s(q%s, ls%s)
                            = - dw_diffusion.iV.Y%s(mat2%s.k, q%s, Pi_%s)"""\
                            % ((ch,) * 11),
                'eq_imv': 'dw_volume_dot.iV.Y%s(lv%s, p%s) = 0' % ((ch,) * 3),
                },
            'dim': pb_def['dim'] - 1,
            'class': cb.CorrDim,
            'save_name': 'corrs_%s_pi%s' % (pb_def['name'], ch),
            'dump_variables': ['p' + ch],
            },
        })

    for ipm in ['p', 'm']:
        coefs.update({
            'E' + ipm + ch: {  # test+
                'requires': ['corrs_eta' + ch],
                'set_variables': [('corr_M', 'corrs_eta' + ch, 'pM')],
                'expression': 'ev_surface_integrate.iS.bYM%s(corr_M)' % ipm,
                'set_volume': 'bYp',
                'class': cb.CoefOne,
                },
            'F' + ipm + ch: {  # test+
                'requires': ['corrs_one' + ch, 'corrs_gamma_' + ipm],
                'set_variables': [('corr1_M', 'corrs_one' + ch, 'pM'),
                                  ('corr2_M', 'corrs_gamma_' + ipm, 'pM')],
                'expression': """dw_diffusion.iV.YM(mat2M.k, corr1_M, corr2_M)
                          - %e * ev_surface_integrate.iS.bYM%s(corr1_M)"""\
                                 % (1.0/param_h, ipm),
                'class': cb.CoefOne,
                },
            })

    for i_io in range(len(val['io_nd_grp'])):
        io = '%s_%d' % (ch, i_io + 1)

        coefs.update({
            'S' + io: { # [Rohan1] (4.28), test+
                'requires': ['corrs_gamma_' + io, 'pis_' + ch],
                'set_variables': set_corr_S,
                'expression': 'dw_diffusion.iV.Y%s(mat2%s.k,corr1_%s,corr2_%s)'\
                              % ((ch,) * 4),
                'dim': pb_def['dim'] - 1,
                'class': cb.CoefDim,
                },
            'P' + io: {  # test+
                'requires': ['pis_' + ch, 'corrs_pi' + ch],
                'set_variables': set_corr_cc,
                'expression': 'ev_surface_integrate.iS.bY%s(corr1_%s)'\
                              % (io, ch),
                'set_volume': 'bYp',
                'dim': pb_def['dim'] - 1,
                'class': cb.CoefDim,
                },
            'S_test' + io: {
                'requires': ['corrs_pi' + ch],
                'set_variables': [('corr1_' + ch, 'corrs_pi' + ch, 'p' + ch)],
                'expression':  '%e * ev_surface_integrate.iS.bY%s(corr1_%s)'\
                                % (1.0 / param_h, io, ch),
                'dim': pb_def['dim'] - 1,
                'class': cb.CoefDim,
                },
            })

        requirements.update({
            'corrs_gamma_' + io: {
                'requires': [],
                'variables': ['p' + ch, 'q' + ch],
                'ebcs': [],
                'epbcs': all_periodicY[ch],
                'lcbcs': ['imv' + ch],
                'equations': {
                    'eq_gamma': """dw_diffusion.iV.Y%s(mat2%s.k, q%s, p%s)
                                   + dw_volume_dot.iV.Y%s(q%s, ls%s)
                                   = %e * dw_surface_integrate.iS.bY%s(q%s)"""\
                                    % ((ch,) * 7 + (1.0/param_h, io, ch)),
                    'eq_imv': 'dw_volume_dot.iV.Y%s(lv%s, p%s) = 0'\
                              % ((ch,) * 3),
                    },
                'class': cb.CorrOne,
                'save_name': 'corrs_%s_gamma_%s' % (pb_def['name'], io),
                'dump_variables': ['p' + ch],
                },
            })

        for i_io2 in range(len(val['io_nd_grp'])):
            io2 = '%s_%d' % (ch, i_io2 + 1)
            io12 = '%s_%d' % (io, i_io2 + 1)
            coefs.update({
                'R' + io12: {  # test+
                    'requires': ['corrs_gamma_' + io2],
                    'set_variables': [('corr1_' + ch, 'corrs_gamma_' + io2,
                                       'p' + ch)],
                    'expression': 'ev_surface_integrate.iS.bY%s(corr1_%s)'\
                                  % (io, ch),
                    'set_volume': 'bYp',
                    'class': cb.CoefOne,
                    },
                })

###########################################################################
# solver, fe

solvers = {
    'ls': ('ls.umfpack', {}),
    'newton': ('nls.newton', {'i_max': 1,
                              'problem': 'nonlinear', })
}

fe = {
    'chunk_size': 1000
}


# [Rohan1] Rohan E.: HOMOGENIZATION OF THE DARCY FLOW IN A DOUBLE-POROUS LAYER


########NEW FILE########
__FILENAME__ = active_fibres
# -*- coding: utf-8 -*-
r"""
Nearly incompressible hyperelastic material model with active fibres.

Large deformation is described using the total Lagrangian formulation.
Models of this kind can be used in biomechanics to model biological
tissues, e.g. muscles.

Find :math:`\ul{u}` such that:

.. math::
    \intl{\Omega\suz}{} \left( \ull{S}\eff(\ul{u})
    + K(J-1)\; J \ull{C}^{-1} \right) : \delta \ull{E}(\ul{v}) \difd{V}
    = 0
    \;, \quad \forall \ul{v} \;,

where

.. list-table::
   :widths: 20 80

   * - :math:`\ull{F}`
     - deformation gradient :math:`F_{ij} = \pdiff{x_i}{X_j}`
   * - :math:`J`
     - :math:`\det(F)`
   * - :math:`\ull{C}`
     -  right Cauchy-Green deformation tensor :math:`C = F^T F`
   * - :math:`\ull{E}(\ul{u})`
     - Green strain tensor :math:`E_{ij} = \frac{1}{2}(\pdiff{u_i}{x_j} +
       \pdiff{u_j}{x_i} + \pdiff{u_m}{x_i}\pdiff{u_m}{x_j})`
   * - :math:`\ull{S}\eff(\ul{u})`
     - effective second Piola-Kirchhoff stress tensor

The effective stress :math:`\ull{S}\eff(\ul{u})` incorporates also the
effects of the active fibres in two preferential directions:

.. math::
    \ull{S}\eff(\ul{u}) = \mu J^{-\frac{2}{3}}(\ull{I}
    - \frac{1}{3}\tr(\ull{C}) \ull{C}^{-1})
    + \sum_{k=1}^2 \tau^k \ull{\omega}^k
    \;.

The first term is the neo-Hookean term and the sum add contributions of
the two fibre systems. The tensors :math:`\ull{\omega}^k =
\ul{d}^k\ul{d}^k` are defined by the fibre system direction vectors
:math:`\ul{d}^k` (unit).

For the one-dimensional tensions :math:`\tau^k` holds simply (:math:`^k`
omitted):

.. math::
    \tau = A f_{\rm max} \exp{\left\{-(\frac{\epsilon - \varepsilon_{\rm
    opt}}{s})^2\right\}} \mbox{ , } \epsilon = \ull{E} : \ull{\omega}
    \;.
"""
import numpy as nm

from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/cylinder.mesh'

vf_matrix = 0.5
vf_fibres1 = 0.2
vf_fibres2 = 0.3

options = {
    'nls' : 'newton',
    'ls' : 'ls',
    'ts' : 'ts',
    'save_steps' : -1,
    'post_process_hook' : 'stress_strain',
}


fields = {
    'displacement': (nm.float64, 3, 'Omega', 1),
}

materials = {
    'solid' : ({
        'K'  : vf_matrix * 1e3, # bulk modulus
        'mu' : vf_matrix * 20e0, # shear modulus of neoHookean term
    },),
    'f1' : 'get_pars_fibres1',
    'f2' : 'get_pars_fibres2',
}

def get_pars_fibres(ts, coors, mode=None, which=0, vf=1.0, **kwargs):
    """
    Parameters
    ----------
    ts : TimeStepper
        Time stepping info.
    coors : array_like
        The physical domain coordinates where the parameters shound be defined.
    mode : 'qp' or 'special'
        Call mode.
    which : int
        Fibre system id.
    vf : float
        Fibre system volume fraction.
    """
    if mode != 'qp': return

    fmax = 10.0
    eps_opt = 0.01
    s = 1.0

    tt = ts.nt * 2.0 * nm.pi

    if which == 0: # system 1
        fdir = nm.array([1.0, 0.0, 0.0], dtype=nm.float64)
        act = 0.5 * (1.0 + nm.sin(tt - (0.5 * nm.pi)))

    elif which == 1: # system 2
        fdir = nm.array([0.0, 1.0, 0.0], dtype=nm.float64)
        act = 0.5 * (1.0 + nm.sin(tt + (0.5 * nm.pi)))

    else:
        raise ValueError('unknown fibre system! (%d)' % which)

    fdir.shape = (3, 1)
    fdir /= nm.linalg.norm(fdir)

    print act

    shape = (coors.shape[0], 1, 1)
    out = {
        'fmax' : vf * nm.tile(fmax, shape),
        'eps_opt' : nm.tile(eps_opt, shape),
        's' : nm.tile(s, shape),
        'fdir' : nm.tile(fdir, shape),
        'act' : nm.tile(act, shape),
    }

    return out

functions = {
    'get_pars_fibres1' : (lambda ts, coors, mode=None, **kwargs:
                          get_pars_fibres(ts, coors, mode=mode, which=0,
                                          vf=vf_fibres1, **kwargs),),
    'get_pars_fibres2' : (lambda ts, coors, mode=None, **kwargs:
                          get_pars_fibres(ts, coors, mode=mode, which=1,
                                          vf=vf_fibres2, **kwargs),),
}

variables = {
    'u' : ('unknown field', 'displacement', 0),
    'v' : ('test field', 'displacement', 'u'),
}

regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < 0.001)', 'facet'),
    'Right' : ('vertices in (x > 0.099)', 'facet'),
}

##
# Dirichlet BC.
ebcs = {
    'l' : ('Left', {'u.all' : 0.0}),
}

##
# Balance of forces.
integral_1 = {
    'name' : 'i',
    'order' : 1,
}
equations = {
    'balance'
        : """dw_tl_he_neohook.i.Omega( solid.mu, v, u )
           + dw_tl_bulk_penalty.i.Omega( solid.K, v, u )
           + dw_tl_fib_a.i.Omega( f1.fmax, f1.eps_opt, f1.s, f1.fdir, f1.act,
                                  v, u )
           + dw_tl_fib_a.i.Omega( f2.fmax, f2.eps_opt, f2.s, f2.fdir, f2.act,
                                  v, u )
           = 0""",
}

def stress_strain(out, problem, state, extend=False):
    from sfepy.base.base import Struct, debug

    ev = problem.evaluate
    strain = ev('dw_tl_he_neohook.i.Omega( solid.mu, v, u )',
                mode='el_avg', term_mode='strain')
    out['green_strain'] = Struct(name='output_data',
                                 mode='cell', data=strain, dofs=None)

    stress = ev('dw_tl_he_neohook.i.Omega( solid.mu, v, u )',
                mode='el_avg', term_mode='stress')
    out['neohook_stress'] = Struct(name='output_data',
                                   mode='cell', data=stress, dofs=None )

    stress = ev('dw_tl_bulk_penalty.i.Omega( solid.K, v, u )',
                mode='el_avg', term_mode= 'stress')
    out['bulk_stress'] = Struct(name='output_data',
                                mode='cell', data=stress, dofs=None)

    return out

##
# Solvers etc.
solver_0 = {
    'name' : 'ls',
    'kind' : 'ls.scipy_direct',
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 7,
    'eps_a'      : 1e-10,
    'eps_r'      : 1.0,
    'macheps'    : 1e-16,
    'lin_red'    : 1e-2, # Linear system error < (eps_a * lin_red).
    'ls_red'     : 0.1,
    'ls_red_warp': 0.001,
    'ls_on'      : 1.1,
    'ls_min'     : 1e-5,
    'check'      : 0,
    'delta'      : 1e-6,
    'problem'    : 'nonlinear', # 'nonlinear' or 'linear' (ignore i_max)
}

solver_2 = {
    'name' : 'ts',
    'kind' : 'ts.simple',

    't0'    : 0,
    't1'    : 1,
    'dt'    : None,
    'n_step' : 21, # has precedence over dt!
}

########NEW FILE########
__FILENAME__ = hyperelastic
# -*- coding: utf-8 -*-
r"""
Nearly incompressible Mooney-Rivlin hyperelastic material model.

Large deformation is described using the total Lagrangian formulation.
Models of this kind can be used to model e.g. rubber or some biological
materials.

Find :math:`\ul{u}` such that:

.. math::
    \intl{\Omega\suz}{} \left( \ull{S}\eff(\ul{u})
    + K(J-1)\; J \ull{C}^{-1} \right) : \delta \ull{E}(\ul{v}) \difd{V}
    = 0
    \;, \quad \forall \ul{v} \;,

where

.. list-table::
   :widths: 20 80

   * - :math:`\ull{F}`
     - deformation gradient :math:`F_{ij} = \pdiff{x_i}{X_j}`
   * - :math:`J`
     - :math:`\det(F)`
   * - :math:`\ull{C}`
     -  right Cauchy-Green deformation tensor :math:`C = F^T F`
   * - :math:`\ull{E}(\ul{u})`
     - Green strain tensor :math:`E_{ij} = \frac{1}{2}(\pdiff{u_i}{x_j} +
       \pdiff{u_j}{x_i} + \pdiff{u_m}{x_i}\pdiff{u_m}{x_j})`
   * - :math:`\ull{S}\eff(\ul{u})`
     - effective second Piola-Kirchhoff stress tensor

The effective stress :math:`\ull{S}\eff(\ul{u})` is given by:

.. math::
    \ull{S}\eff(\ul{u}) = \mu J^{-\frac{2}{3}}(\ull{I}
    - \frac{1}{3}\tr(\ull{C}) \ull{C}^{-1})
    + \kappa J^{-\frac{4}{3}} (\tr(\ull{C}\ull{I} - \ull{C}
    - \frac{2}{6}((\tr{\ull{C}})^2 - \tr{(\ull{C}^2)})\ull{C}^{-1})
    \;.
"""
import numpy as nm

from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/cylinder.mesh'

options = {
    'nls' : 'newton',
    'ls' : 'ls',
    'ts' : 'ts',
    'save_steps' : -1,
    'post_process_hook' : 'stress_strain',
}


field_1 = {
    'name' : 'displacement',
    'dtype' : nm.float64,
    'shape' : 3,
    'region' : 'Omega',
    'approx_order' : 1,
}

material_1 = {
    'name' : 'solid',
    'values' : {
        'K'  : 1e3, # bulk modulus
        'mu' : 20e0, # shear modulus of neoHookean term
        'kappa' : 10e0, # shear modulus of Mooney-Rivlin term
    }
}

variables = {
    'u' : ('unknown field', 'displacement', 0),
    'v' : ('test field', 'displacement', 'u'),
}

regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < 0.001)', 'facet'),
    'Right' : ('vertices in (x > 0.099)', 'facet'),
}

##
# Dirichlet BC + related functions.
ebcs = {
    'l' : ('Left', {'u.all' : 0.0}),
    'r' : ('Right', {'u.0' : 0.0, 'u.[1,2]' : 'rotate_yz'}),
}

centre = nm.array( [0, 0], dtype = nm.float64 )

def rotate_yz(ts, coor, **kwargs):
    from sfepy.linalg import rotation_matrix2d

    vec = coor[:,1:3] - centre

    angle = 10.0 * ts.step
    print 'angle:', angle

    mtx = rotation_matrix2d( angle )
    vec_rotated = nm.dot( vec, mtx )

    displacement = vec_rotated - vec

    return displacement.T.flat

functions = {
    'rotate_yz' : (rotate_yz,),
}

def stress_strain( out, problem, state, extend = False ):
    from sfepy.base.base import Struct, debug

    ev = problem.evaluate
    strain = ev('dw_tl_he_neohook.i.Omega( solid.mu, v, u )',
                mode='el_avg', term_mode='strain')
    out['green_strain'] = Struct(name='output_data',
                                 mode='cell', data=strain, dofs=None)

    stress = ev('dw_tl_he_neohook.i.Omega( solid.mu, v, u )',
                mode='el_avg', term_mode='stress')
    out['neohook_stress'] = Struct(name='output_data',
                                   mode='cell', data=stress, dofs=None)

    stress = ev('dw_tl_he_mooney_rivlin.i.Omega( solid.kappa, v, u )',
                mode='el_avg', term_mode='stress')
    out['mooney_rivlin_stress'] = Struct(name='output_data',
                                         mode='cell', data=stress, dofs=None)

    stress = ev('dw_tl_bulk_penalty.i.Omega( solid.K, v, u )',
                mode='el_avg', term_mode= 'stress')
    out['bulk_stress'] = Struct(name='output_data',
                                mode='cell', data=stress, dofs=None)

    return out

##
# Balance of forces.
integral_1 = {
    'name' : 'i',
    'order' : 1,
}
equations = {
    'balance' : """dw_tl_he_neohook.i.Omega( solid.mu, v, u )
                 + dw_tl_he_mooney_rivlin.i.Omega( solid.kappa, v, u )
                 + dw_tl_bulk_penalty.i.Omega( solid.K, v, u )
                 = 0""",
}

##
# Solvers etc.
solver_0 = {
    'name' : 'ls',
    'kind' : 'ls.scipy_direct',
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 5,
    'eps_a'      : 1e-10,
    'eps_r'      : 1.0,
    'macheps'    : 1e-16,
    'lin_red'    : 1e-2, # Linear system error < (eps_a * lin_red).
    'ls_red'     : 0.1,
    'ls_red_warp': 0.001,
    'ls_on'      : 1.1,
    'ls_min'     : 1e-5,
    'check'      : 0,
    'delta'      : 1e-6,
    'problem'    : 'nonlinear', # 'nonlinear' or 'linear' (ignore i_max)
}

solver_2 = {
    'name' : 'ts',
    'kind' : 'ts.simple',

    't0'    : 0,
    't1'    : 1,
    'dt'    : None,
    'n_step' : 11, # has precedence over dt!
}

########NEW FILE########
__FILENAME__ = hyperelastic_ul
# -*- coding: utf-8 -*-
r"""
Nearly incompressible Mooney-Rivlin hyperelastic material model.

Large deformation is described using the updated Lagrangian formulation.
Models of this kind can be used to model e.g. rubber or some biological
materials.
"""
import numpy as nm
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/cylinder.mesh'

options = {
    'nls': 'newton',
    'ls': 'ls',
    'ts': 'ts',
    'ulf': True,
    'mesh_update_variables': ['u'],
    'output_dir': 'output',
    'post_process_hook': 'stress_strain',
}

fields = {
    'displacement': ('real', 3, 'Omega', 1),
}

materials = {
    'solid': ({'K': 1e3, # bulk modulus
               'mu': 20e0, # shear modulus of neoHookean term
               'kappa': 10e0, # shear modulus of Mooney-Rivlin term
               },),
}

variables = {
    'u': ('unknown field', 'displacement', 0),
    'v': ('test field', 'displacement', 'u'),
}

regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < 0.001)', 'facet'),
    'Right' : ('vertices in (x > 0.099)', 'facet'),
}

##
# Dirichlet BC + related functions.
ebcs = {
    'l' : ('Left', {'u.all' : 0.0}),
    'r' : ('Right', {'u.0' : 0.0, 'u.[1,2]' : 'rotate_yz'}),
}

centre = nm.array( [0, 0], dtype = nm.float64 )

def rotate_yz(ts, coor, **kwargs):
    from sfepy.linalg import rotation_matrix2d

    vec = coor[:,1:3] - centre

    angle = 10.0 * ts.step
    print 'angle:', angle

    mtx = rotation_matrix2d( angle )
    vec_rotated = nm.dot( vec, mtx )

    displacement = vec_rotated - vec

    return displacement.T.flat

functions = {
    'rotate_yz' : (rotate_yz,),
}

def stress_strain( out, problem, state, extend = False ):
    from sfepy.base.base import Struct

    ev = problem.evaluate
    strain = ev('dw_ul_he_neohook.3.Omega( solid.mu, v, u )',
                mode='el_avg', term_mode='strain')
    out['green_strain'] = Struct(name='output_data',
                                 mode='cell', data=strain, dofs=None)

    stress = ev('dw_ul_he_neohook.3.Omega( solid.mu, v, u )',
                mode='el_avg', term_mode='stress')
    out['neohook_stress'] = Struct(name='output_data',
                                   mode='cell', data=stress, dofs=None)

    stress = ev('dw_ul_he_mooney_rivlin.3.Omega( solid.kappa, v, u )',
                mode='el_avg', term_mode='stress')
    out['mooney_rivlin_stress'] = Struct(name='output_data',
                                         mode='cell', data=stress, dofs=None)

    stress = ev('dw_ul_bulk_penalty.3.Omega( solid.K, v, u )',
                mode='el_avg', term_mode= 'stress')
    out['bulk_stress'] = Struct(name='output_data',
                                mode='cell', data=stress, dofs=None)

    return out

equations = {
    'balance': """dw_ul_he_neohook.3.Omega( solid.mu, v, u )
                + dw_ul_he_mooney_rivlin.3.Omega(solid.kappa, v, u)
                + dw_ul_bulk_penalty.3.Omega( solid.K, v, u )
                = 0""",
    }

##
# Solvers etc.
solvers = {
    'ls': ('ls.scipy_direct', {}),
    'newton': ('nls.newton', {
        'i_max': 25,
        'eps_a': 1e-8,
        'eps_r': 1.0,
        'macheps': 1e-16,
        'lin_red': 1e-2, # Linear system error < (eps_a * lin_red).
        'ls_red': 0.1,
        'ls_red_warp': 0.001,
        'ls_on': 1.1,
        'ls_min': 1e-5,
        'check': 0,
        'delta': 1e-6,
        'problem': 'nonlinear', # 'nonlinear' or 'linear' (ignore i_max),
        }),
    'ts': ('ts.simple', {
        't0': 0,
        't1': 1,
        'dt': None,
        'n_step': 11, # has precedence over dt!
        }),
    }

########NEW FILE########
__FILENAME__ = hyperelastic_ul_up
# -*- coding: utf-8 -*-
r"""
Compressible Mooney-Rivlin hyperelastic material model.

Large deformation is described using the updated Lagrangian formulation.
Incompressibility is treated by mixed displacement-pressure formulation.
Models of this kind can be used to model e.g. rubber or some biological
materials.
"""
import numpy as nm
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/cylinder.mesh'

options = {
    'nls': 'newton',
    'ls': 'ls',
    'ts': 'ts',
    'ulf': True,
    'mesh_update_variables': ['u'],
    'output_dir': 'output',
    'post_process_hook': 'stress_strain',
}

fields = {
    'displacement': ('real', 'vector', 'Omega', 1),
    'pressure': ('real', 'scalar', 'Omega', 0),
}

materials = {
    'solid': ({'iK': 1.0 / 1e3, # bulk modulus
               'mu': 20e0, # shear modulus of neoHookean term
               'kappa': 10e0, # shear modulus of Mooney-Rivlin term
               },),
}

variables = {
    'u': ('unknown field', 'displacement', 0),
    'v': ('test field', 'displacement', 'u'),
    'p': ('unknown field', 'pressure', 1),
    'q': ('test field', 'pressure', 'p'),
}

regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < 0.001)', 'facet'),
    'Right' : ('vertices in (x > 0.099)', 'facet'),
}

##
# Dirichlet BC + related functions.
ebcs = {
    'l' : ('Left', {'u.all' : 0.0}),
    'r' : ('Right', {'u.0' : 0.0, 'u.[1,2]' : 'rotate_yz'}),
}

centre = nm.array( [0, 0], dtype = nm.float64 )

def rotate_yz(ts, coor, **kwargs):
    from sfepy.linalg import rotation_matrix2d

    vec = coor[:,1:3] - centre

    angle = 10.0 * ts.step
    print 'angle:', angle

    mtx = rotation_matrix2d( angle )
    vec_rotated = nm.dot( vec, mtx )

    displacement = vec_rotated - vec

    return displacement.T.flat

functions = {
    'rotate_yz' : (rotate_yz,),
}

def stress_strain( out, problem, state, extend = False ):
    from sfepy.base.base import Struct

    ev = problem.evaluate
    strain = ev('dw_ul_he_neohook.3.Omega( solid.mu, v, u )',
                mode='el_avg', term_mode='strain')
    out['green_strain'] = Struct(name='output_data',
                                 mode='cell', data=strain, dofs=None)

    stress = ev('dw_ul_he_neohook.3.Omega( solid.mu, v, u )',
                mode='el_avg', term_mode='stress')
    out['neohook_stress'] = Struct(name='output_data',
                                   mode='cell', data=stress, dofs=None)

    stress = ev('dw_ul_he_mooney_rivlin.3.Omega( solid.kappa, v, u )',
                mode='el_avg', term_mode='stress')
    out['mooney_rivlin_stress'] = Struct(name='output_data',
                                         mode='cell', data=stress, dofs=None)

    stress = ev('dw_ul_bulk_pressure.3.Omega( v, u, p )',
                mode='el_avg', term_mode= 'stress')
    out['bulk_stress'] = Struct(name='output_data',
                                mode='cell', data=stress, dofs=None)

    return out

equations = {
    'balance': """dw_ul_he_neohook.3.Omega( solid.mu, v, u )
                + dw_ul_he_mooney_rivlin.3.Omega(solid.kappa, v, u)
                + dw_ul_bulk_pressure.3.Omega( v, u, p ) = 0""",
    'volume': """dw_ul_volume.3.Omega( q, u )
               + dw_ul_compressible.3.Omega( solid.iK, q, p, u ) = 0"""
    }

##
# Solvers etc.
solvers = {
    'ls': ('ls.scipy_direct', {}),
    'newton': ('nls.newton', {
        'i_max': 25,
        'eps_a': 1e-8,
        'eps_r': 1.0,
        'macheps': 1e-16,
        'lin_red': 1e-2, # Linear system error < (eps_a * lin_red).
        'ls_red': 0.1,
        'ls_red_warp': 0.001,
        'ls_on': 1.1,
        'ls_min': 1e-5,
        'check': 0,
        'delta': 1e-6,
        'problem': 'nonlinear', # 'nonlinear' or 'linear' (ignore i_max),
        }),
    'ts': ('ts.simple', {
        't0': 0,
        't1': 1,
        'dt': None,
        'n_step': 11, # has precedence over dt!
        }),
    }

########NEW FILE########
__FILENAME__ = perfusion_tl
# -*- coding: utf-8 -*-
r"""
Porous nearly incompressible hyperelastic material with fluid perfusion.

Large deformation is described using the total Lagrangian formulation.
Models of this kind can be used in biomechanics to model biological
tissues, e.g. muscles.

Find :math:`\ul{u}` such that:

(equilibrium equation with boundary tractions)

.. math::
    \intl{\Omega\suz}{} \left( \ull{S}\eff - p J \ull{C}^{-1}
    \right) : \delta \ull{E}(\ul{v}) \difd{V}
    + \intl{\Gamma_0\suz}{} \ul{\nu} \cdot \ull{F}^{-1} \cdot \ull{\sigma}
    \cdot  \ul{v} J \difd{S}
    = 0
    \;, \quad \forall \ul{v} \;,

(mass balance equation (perfusion))

.. math::
    \intl{\Omega\suz}{} q J(\ul{u})
    + \intl{\Omega\suz}{} \ull{K}(\ul{u}\sunm) : \pdiff{q}{X} \pdiff{p}{X}
    = \intl{\Omega\suz}{} q J(\ul{u}\sunm)
    \;, \quad \forall q \;,


where

.. list-table::
   :widths: 20 80

   * - :math:`\ull{F}`
     - deformation gradient :math:`F_{ij} = \pdiff{x_i}{X_j}`
   * - :math:`J`
     - :math:`\det(F)`
   * - :math:`\ull{C}`
     -  right Cauchy-Green deformation tensor :math:`C = F^T F`
   * - :math:`\ull{E}(\ul{u})`
     - Green strain tensor :math:`E_{ij} = \frac{1}{2}(\pdiff{u_i}{x_j} +
       \pdiff{u_j}{x_i} + \pdiff{u_m}{x_i}\pdiff{u_m}{x_j})`
   * - :math:`\ull{S}\eff(\ul{u})`
     - effective second Piola-Kirchhoff stress tensor

The effective (neo-Hookean) stress :math:`\ull{S}\eff(\ul{u})` is given
by:

.. math::
    \ull{S}\eff(\ul{u}) = \mu J^{-\frac{2}{3}}(\ull{I}
    - \frac{1}{3}\tr(\ull{C}) \ull{C}^{-1})
    \;.

The linearized deformation-dependent permeability is defined as
:math:`\ull{K}(\ul{u}) = J \ull{F}^{-1} \ull{k} f(J) \ull{F}^{-T}`,
where :math:`\ul{u}` relates to the previous time step :math:`(n-1)` and
:math:`f(J) = \max\left(0, \left(1 + \frac{(J -
1)}{N_f}\right)\right)^2` expresses the dependence on volume
compression/expansion.
"""
import numpy as nm

from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/cylinder.mesh'

# Time-stepping parameters.
t0 = 0.0
t1 = 1.0
n_step = 21

from sfepy.solvers.ts import TimeStepper
ts = TimeStepper(t0, t1, None, n_step)

options = {
    'nls' : 'newton',
    'ls' : 'ls',
    'ts' : 'ts',
    'save_steps' : -1,
    'post_process_hook' : 'post_process',
}


fields = {
    'displacement': ('real', 3, 'Omega', 1),
    'pressure'    : ('real', 1, 'Omega', 1),
}

materials = {
    # Perfused solid.
    'ps' : ({
        'mu' : 20e0, # shear modulus of neoHookean term
        'k'  : ts.dt * nm.eye(3, dtype=nm.float64), # reference permeability
        'N_f' : 1.0, # reference porosity
    },),
    # Surface pressure traction.
    'traction' : 'get_traction',
}

variables = {
    'u' : ('unknown field', 'displacement', 0, 1),
    'v' : ('test field', 'displacement', 'u'),
    'p' : ('unknown field', 'pressure', 1),
    'q' : ('test field', 'pressure', 'p'),
}

regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < 0.001)', 'facet'),
    'Right' : ('vertices in (x > 0.099)', 'facet'),
}

##
# Dirichlet BC.
ebcs = {
    'l' : ('Left', {'u.all' : 0.0, 'p.0' : 'get_pressure'}),
}

##
# Balance of forces.
integrals = {
    'i1' : 1,
    'i2' : 2,
}

equations = {
    'force_balance'
        : """dw_tl_he_neohook.i1.Omega( ps.mu, v, u )
           + dw_tl_bulk_pressure.i1.Omega( v, u, p )
           + dw_tl_surface_traction.i2.Right( traction.pressure, v, u )
           = 0""",
    'mass_balance'
        : """dw_tl_volume.i1.Omega( q, u )
           + dw_tl_diffusion.i1.Omega( ps.k, ps.N_f, q, p, u[-1])
           = dw_tl_volume.i1.Omega( q, u[-1] )"""
}

def post_process(out, problem, state, extend=False):
    from sfepy.base.base import Struct, debug

    val = problem.evaluate('dw_tl_he_neohook.i1.Omega( ps.mu, v, u )',
                           mode='el_avg', term_mode='strain')
    out['green_strain'] = Struct(name='output_data',
                                 mode='cell', data=val, dofs=None)

    val = problem.evaluate('dw_tl_he_neohook.i1.Omega( ps.mu, v, u )',
                           mode='el_avg', term_mode='stress')
    out['neohook_stress'] = Struct(name='output_data',
                                   mode='cell', data=val, dofs=None)

    val = problem.evaluate('dw_tl_bulk_pressure.i1.Omega( v, u, p )',
                           mode='el_avg', term_mode='stress')
    out['bulk_pressure'] = Struct(name='output_data',
                                  mode='cell', data=val, dofs=None)

    val = problem.evaluate('dw_tl_diffusion.i1.Omega( ps.k, ps.N_f, q, p, u[-1] )',
                           mode='el_avg', term_mode='diffusion_velocity')
    out['diffusion_velocity'] = Struct(name='output_data',
                                       mode='cell', data=val, dofs=None)

    return out

##
# Solvers etc.
solver_0 = {
    'name' : 'ls',
    'kind' : 'ls.scipy_direct',
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 7,
    'eps_a'      : 1e-10,
    'eps_r'      : 1.0,
    'macheps'    : 1e-16,
    'lin_red'    : 1e-2, # Linear system error < (eps_a * lin_red).
    'ls_red'     : 0.1,
    'ls_red_warp': 0.001,
    'ls_on'      : 1.1,
    'ls_min'     : 1e-5,
    'check'      : 0,
    'delta'      : 1e-6,
    'problem'    : 'nonlinear', # 'nonlinear' or 'linear' (ignore i_max)
}

solver_2 = {
    'name' : 'ts',
    'kind' : 'ts.simple',

    't0'    : t0,
    't1'    : t1,
    'dt'    : None,
    'n_step' : n_step, # has precedence over dt!
}

##
# Functions.
def get_traction(ts, coors, mode=None):
    """
    Pressure traction.
    
    Parameters
    ----------
    ts : TimeStepper
        Time stepping info.
    coors : array_like
        The physical domain coordinates where the parameters shound be defined.
    mode : 'qp' or 'special'
        Call mode.
    """
    if mode != 'qp': return

    tt = ts.nt * 2.0 * nm.pi

    dim = coors.shape[1]
    val = 0.05 * nm.sin(tt) * nm.eye(dim, dtype=nm.float64)
    val[1,0] = val[0,1] = 0.5 * val[0,0]

    shape = (coors.shape[0], 1, 1)
    out = {
        'pressure' : nm.tile(val, shape),
    }

    return out

def get_pressure(ts, coor, **kwargs):
    """Internal pressure Dirichlet boundary condition."""
    tt = ts.nt * 2.0 * nm.pi

    val = nm.zeros((coor.shape[0],), dtype=nm.float64)

    val[:] = 1e-2 * nm.sin(tt)

    return val

functions = {
    'get_traction' : (lambda ts, coors, mode=None, **kwargs:
                      get_traction(ts, coors, mode=mode),),
    'get_pressure' : (get_pressure,),
}


########NEW FILE########
__FILENAME__ = elastic_contact_planes
r"""
Elastic contact planes simulating an indentation test.

Four contact planes bounded by polygons (triangles in this case) form a very
rigid pyramid shape simulating an indentor.

Find :math:`\ul{u}` such that:

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    + \sum_{i=1}^4 \int_{\Gamma_i} \ul{v} \cdot f^i(d(\ul{u})) \ul{n^i}
    = 0 \;,

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl} + \delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;.

Notes
-----

Even though the material is linear elastic and small deformations are used, the
problem is highly nonlinear due to contacts with the planes.

Checking the tangent matrix by finite differences by setting 'check' in 'nls'
solver configuration to nonzero is rather tricky - the active contact points
must not change during the test. This can be ensured by a sufficient initial
penetration and large enough contact boundary polygons (hard!), or by tweaking
the dw_contact_plane term to mask points only by undeformed coordinates.
"""
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/cube_medium_hexa.mesh'

k = 1e5 # Elastic plane stiffness for positive penetration.
f0 = 1e2 # Force at zero penetration.
dn = 0.2 # x or y component magnitude of normals.
ds = 0.25 # Boundary polygon size in horizontal directions.
az = 0.4 # Anchor z coordinate.

options = {
    'ts' : 'ts',
    'nls' : 'newton',
    'ls' : 'lsd',

    'output_format': 'vtk',
}

fields = {
    'displacement': ('real', 3, 'Omega', 1),
}

materials = {
    'solid' : ({
        'lam' : 5.769,
        'mu' : 3.846,
    },),
    'cp0' : ({
        'f' : [k, f0],
        '.n' : [dn, 0.0, 1.0],
        '.a' : [0.0, 0.0, az],
        '.bs' : [[0.0, 0.0, az],
                 [-ds, -ds, az],
                 [-ds, ds, az]],
    },),
    'cp1' : ({
        'f' : [k, f0],
        '.n' : [-dn, 0.0, 1.0],
        '.a' : [0.0, 0.0, az],
        '.bs' : [[0.0, 0.0, az],
                 [ds, -ds, az],
                 [ds, ds, az]],
    },),
    'cp2' : ({
        'f' : [k, f0],
        '.n' : [0.0, dn, 1.0],
        '.a' : [0.0, 0.0, az],
        '.bs' : [[0.0, 0.0, az],
                 [-ds, -ds, az],
                 [ds, -ds, az]],
    },),
    'cp3' : ({
        'f' : [k, f0],
        '.n' : [0.0, -dn, 1.0],
        '.a' : [0.0, 0.0, az],
        '.bs' : [[0.0, 0.0, az],
                 [-ds, ds, az],
                 [ds, ds, az]],
    },),
}

variables = {
    'u' : ('unknown field', 'displacement', 0),
    'v' : ('test field', 'displacement', 'u'),
}

regions = {
    'Omega' : 'all',
    'Bottom' : ('vertices in (z < -0.499)', 'facet'),
    'Top' : ('vertices in (z > 0.499)', 'facet'),
}

ebcs = {
    'fixed' : ('Bottom', {'u.all' : 0.0}),
}

equations = {
    'elasticity' :
    """dw_lin_elastic_iso.2.Omega(solid.lam, solid.mu, v, u)
     + dw_contact_plane.2.Top(cp0.f, cp0.n, cp0.a, cp0.bs, v, u)
     + dw_contact_plane.2.Top(cp1.f, cp1.n, cp1.a, cp1.bs, v, u)
     + dw_contact_plane.2.Top(cp2.f, cp2.n, cp2.a, cp2.bs, v, u)
     + dw_contact_plane.2.Top(cp3.f, cp3.n, cp3.a, cp3.bs, v, u)
     = 0""",
}

solvers = {
    'lsd' : ('ls.scipy_direct', {}),
    'lsi' : ('ls.petsc', {
        'method' : 'cg',
        'eps_r' : 1e-8,
        'i_max' : 3000,
    }),
    'newton' : ('nls.newton', {
        'i_max' : 10,
        'eps_a' : 1e-10,
        'problem' : 'nonlinear',
        'check' : 0,
        'delta' : 1e-6,
    }),
}

def main():
    import os

    import numpy as nm
    import matplotlib.pyplot as plt

    from sfepy.discrete.fem import MeshIO
    import sfepy.linalg as la
    from sfepy.mechanics.contact_bodies import (ContactPlane, plot_polygon,
                                                plot_points)

    conf_dir = os.path.dirname(__file__)
    io = MeshIO.any_from_filename(filename_mesh, prefix_dir=conf_dir)
    bb = io.read_bounding_box()
    outline = [vv for vv in la.combine(zip(*bb))]

    ax = plot_points(None, nm.array(outline), 'r*')
    for name in ['cp%d' % ii for ii in range(4)]:
        cpc = materials[name][0]
        cp = ContactPlane(cpc['.a'], cpc['.n'], cpc['.bs'])

        v1, v2 = la.get_perpendiculars(cp.normal)

        ax = plot_polygon(ax, cp.bounds)
        ax = plot_polygon(ax, nm.r_[cp.anchor[None, :],
                                    cp.anchor[None, :] + cp.normal[None, :]])
        ax = plot_polygon(ax, nm.r_[cp.anchor[None, :],
                                    cp.anchor[None, :] + v1])
        ax = plot_polygon(ax, nm.r_[cp.anchor[None, :],
                                    cp.anchor[None, :] + v2])

    plt.show()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = elastic_contact_sphere
r"""
Elastic contact sphere simulating an indentation test.

Find :math:`\ul{u}` such that:

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    + \int_{\Gamma} \ul{v} \cdot f(d(\ul{u})) \ul{n}(\ul{u})
    = 0 \;,

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl} + \delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;.

Notes
-----

Even though the material is linear elastic and small deformations are used, the
problem is highly nonlinear due to contacts with the sphere. See also
elastic_contact_planes.py example.
"""
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/cube_medium_hexa.mesh'

k = 1e5 # Elastic sphere stiffness for positive penetration.
f0 = 1e-2 # Force at zero penetration.

options = {
    'nls' : 'newton',
    'ls' : 'ls',

    'output_format': 'vtk',
}

fields = {
    'displacement': ('real', 3, 'Omega', 1),
}

materials = {
    'solid' : ({
        'lam' : 5.769,
        'mu' : 3.846,
    },),
    'cs' : ({
        'f' : [k, f0],
        '.c' : [0.0, 0.0, 1.2],
        '.r' : 0.8,
    },),
}

variables = {
    'u' : ('unknown field', 'displacement', 0),
    'v' : ('test field', 'displacement', 'u'),
}

regions = {
    'Omega' : 'all',
    'Bottom' : ('vertices in (z < -0.499)', 'facet'),
    'Top' : ('vertices in (z > 0.499)', 'facet'),
}

ebcs = {
    'fixed' : ('Bottom', {'u.all' : 0.0}),
}

equations = {
    'elasticity' :
    """dw_lin_elastic_iso.2.Omega(solid.lam, solid.mu, v, u)
     + dw_contact_sphere.2.Top(cs.f, cs.c, cs.r, v, u)
     = 0""",
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton', {
        'i_max' : 20,
        'eps_a' : 1e-1,
        'ls_on' : 2.0,
        'problem' : 'nonlinear',
        'check' : 0,
        'delta' : 1e-6,
    }),
}

def main():
    import os

    import numpy as nm
    import matplotlib.pyplot as plt

    from sfepy.discrete.fem import MeshIO
    import sfepy.linalg as la
    from sfepy.mechanics.contact_bodies import ContactSphere, plot_points

    conf_dir = os.path.dirname(__file__)
    io = MeshIO.any_from_filename(filename_mesh, prefix_dir=conf_dir)
    bb = io.read_bounding_box()
    outline = [vv for vv in la.combine(zip(*bb))]

    ax = plot_points(None, nm.array(outline), 'r*')
    csc = materials['cs'][0]
    cs = ContactSphere(csc['.c'], csc['.r'])

    pps = (bb[1] - bb[0]) * nm.random.rand(5000, 3) + bb[0]
    mask = cs.mask_points(pps, 0.0)

    ax = plot_points(ax, cs.centre[None, :], 'b*', ms=30)
    ax = plot_points(ax, pps[mask], 'kv')
    ax = plot_points(ax, pps[~mask], 'r.')

    plt.show()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = its2D_1
r"""
Diametrically point loaded 2-D disk. See :ref:`sec-primer`.

Find :math:`\ul{u}` such that:

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    = 0
    \;, \quad \forall \ul{v} \;,

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl}+\delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;.
"""
from sfepy.mechanics.matcoefs import lame_from_youngpoisson
from sfepy.discrete.fem.utils import refine_mesh
from sfepy import data_dir

# Fix the mesh file name if you run this file outside the SfePy directory.
filename_mesh = data_dir + '/meshes/2d/its2D.mesh'

refinement_level = 0
filename_mesh = refine_mesh(filename_mesh, refinement_level)

output_dir = '.' # set this to a valid directory you have write access to

young = 2000.0 # Young's modulus [MPa]
poisson = 0.4  # Poisson's ratio

options = {
    'output_dir' : output_dir,
}

regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < 0.001)', 'facet'),
    'Bottom' : ('vertices in (y < 0.001)', 'facet'),
    'Top' : ('vertex 2', 'vertex'),
}

materials = {
    'Asphalt' : ({
        'lam' : lame_from_youngpoisson(young, poisson)[0],
        'mu' : lame_from_youngpoisson(young, poisson)[1],
    },),
    'Load' : ({'.val' : [0.0, -1000.0]},),
}

fields = {
    'displacement': ('real', 'vector', 'Omega', 1),
}

equations = {
   'balance_of_forces' :
   """dw_lin_elastic_iso.2.Omega(Asphalt.lam, Asphalt.mu, v, u )
      = dw_point_load.0.Top(Load.val, v)""",
}

variables = {
    'u' : ('unknown field', 'displacement', 0),
    'v' : ('test field', 'displacement', 'u'),
}

ebcs = {
    'XSym' : ('Bottom', {'u.1' : 0.0}),
    'YSym' : ('Left', {'u.0' : 0.0}),
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton', {
        'i_max' : 1,
        'eps_a' : 1e-6,
        'problem' : 'nonlinear'
    }),
}

########NEW FILE########
__FILENAME__ = its2D_2
r"""
Diametrically point loaded 2-D disk with postprocessing. See
:ref:`sec-primer`.

Find :math:`\ul{u}` such that:

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    = 0
    \;, \quad \forall \ul{v} \;,

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl}+\delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;.
"""

from its2D_1 import *

from sfepy.mechanics.matcoefs import stiffness_from_youngpoisson

def stress_strain(out, pb, state, extend=False):
    """
    Calculate and output strain and stress for given displacements.
    """
    from sfepy.base.base import Struct

    ev = pb.evaluate
    strain = ev('ev_cauchy_strain.2.Omega(u)', mode='el_avg')
    stress = ev('ev_cauchy_stress.2.Omega(Asphalt.D, u)', mode='el_avg')

    out['cauchy_strain'] = Struct(name='output_data', mode='cell',
                                  data=strain, dofs=None)
    out['cauchy_stress'] = Struct(name='output_data', mode='cell',
                                  data=stress, dofs=None)

    return out

asphalt = materials['Asphalt'][0]
asphalt.update({'D' : stiffness_from_youngpoisson(2, young, poisson)})
options.update({'post_process_hook' : 'stress_strain',})

########NEW FILE########
__FILENAME__ = its2D_3
r"""
Diametrically point loaded 2-D disk with nodal stress calculation. See
:ref:`sec-primer`.

Find :math:`\ul{u}` such that:

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    = 0
    \;, \quad \forall \ul{v} \;,

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl}+\delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;.
"""
from its2D_1 import *

from sfepy.mechanics.matcoefs import stiffness_from_youngpoisson
from sfepy.discrete.fem.geometry_element import geometry_data
from sfepy.discrete import FieldVariable
from sfepy.discrete.fem import Field
import numpy as nm

gdata = geometry_data['2_3']
nc = len(gdata.coors)

def nodal_stress(out, pb, state, extend=False):
    '''
    Calculate stresses at nodal points.
    '''

    # Point load.
    mat = pb.get_materials()['Load']
    P = 2.0 * mat.get_data('special', None, 'val')[1]

    # Calculate nodal stress.
    pb.time_update()

    stress = pb.evaluate('ev_cauchy_stress.ivn.Omega(Asphalt.D, u)', mode='qp')
    sfield = Field.from_args('stress', nm.float64, (3,),
                             pb.domain.regions['Omega'])
    svar = FieldVariable('sigma', 'parameter', sfield,
                         primary_var_name='(set-to-None)')
    svar.set_data_from_qp(stress, pb.integrals['ivn'])

    print '\n=================================================================='
    print 'Given load = %.2f N' % -P
    print '\nAnalytical solution'
    print '==================='
    print 'Horizontal tensile stress = %.5e MPa/mm' % (-2.*P/(nm.pi*150.))
    print 'Vertical compressive stress = %.5e MPa/mm' % (-6.*P/(nm.pi*150.))
    print '\nFEM solution'
    print '============'
    print 'Horizontal tensile stress = %.5e MPa/mm' % (svar()[0][0])
    print 'Vertical compressive stress = %.5e MPa/mm' % (-svar()[0][1])
    print '=================================================================='
    return out

asphalt = materials['Asphalt'][0]
asphalt.update({'D' : stiffness_from_youngpoisson(2, young, poisson)})
options.update({'post_process_hook' : 'nodal_stress',})

integrals = {
    'ivn' : ('custom', gdata.coors, [gdata.volume / nc] * nc),
}

########NEW FILE########
__FILENAME__ = its2D_4
r"""
Diametrically point loaded 2-D disk with postprocessing and probes. See
:ref:`sec-primer`.

Find :math:`\ul{u}` such that:

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    = 0
    \;, \quad \forall \ul{v} \;,

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl}+\delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;.
"""
from its2D_1 import *

from sfepy.mechanics.matcoefs import stiffness_from_youngpoisson

def stress_strain(out, pb, state, extend=False):
    """
    Calculate and output strain and stress for given displacements.
    """
    from sfepy.base.base import Struct

    ev = pb.evaluate
    strain = ev('ev_cauchy_strain.2.Omega(u)', mode='el_avg')
    stress = ev('ev_cauchy_stress.2.Omega(Asphalt.D, u)', mode='el_avg')

    out['cauchy_strain'] = Struct(name='output_data', mode='cell',
                                  data=strain, dofs=None)
    out['cauchy_stress'] = Struct(name='output_data', mode='cell',
                                  data=stress, dofs=None)

    return out

def gen_lines(problem):
    from sfepy.discrete.probes import LineProbe
    ps0 = [[0.0,  0.0], [ 0.0,  0.0]]
    ps1 = [[75.0, 0.0], [ 0.0, 75.0]]

    # Use adaptive probe with 10 inital points.
    n_point = -10

    labels = ['%s -> %s' % (p0, p1) for p0, p1 in zip(ps0, ps1)]
    probes = []
    for ip in xrange(len(ps0)):
        p0, p1 = ps0[ip], ps1[ip]
        probes.append(LineProbe(p0, p1, n_point))

    return probes, labels


def probe_hook(data, probe, label, problem):
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm

    def get_it(name, var_name):
        var = problem.create_variables([var_name])[var_name]
        var.set_data(data[name].data)

        pars, vals = probe(var)
        vals = vals.squeeze()
        return pars, vals

    results = {}
    results['u'] = get_it('u', 'u')
    results['cauchy_strain'] = get_it('cauchy_strain', 's')
    results['cauchy_stress'] = get_it('cauchy_stress', 's')

    fig = plt.figure()
    plt.clf()
    fig.subplots_adjust(hspace=0.4)
    plt.subplot(311)
    pars, vals = results['u']
    for ic in range(vals.shape[1]):
        plt.plot(pars, vals[:,ic], label=r'$u_{%d}$' % (ic + 1),
                 lw=1, ls='-', marker='+', ms=3)
    plt.ylabel('displacements')
    plt.xlabel('probe %s' % label, fontsize=8)
    plt.legend(loc='best', prop=fm.FontProperties(size=10))

    sym_indices = ['11', '22', '12']

    plt.subplot(312)
    pars, vals = results['cauchy_strain']
    for ic in range(vals.shape[1]):
        plt.plot(pars, vals[:,ic], label=r'$e_{%s}$' % sym_indices[ic],
                 lw=1, ls='-', marker='+', ms=3)
    plt.ylabel('Cauchy strain')
    plt.xlabel('probe %s' % label, fontsize=8)
    plt.legend(loc='best', prop=fm.FontProperties(size=8))

    plt.subplot(313)
    pars, vals = results['cauchy_stress']
    for ic in range(vals.shape[1]):
        plt.plot(pars, vals[:,ic], label=r'$\sigma_{%s}$' % sym_indices[ic],
                 lw=1, ls='-', marker='+', ms=3)
    plt.ylabel('Cauchy stress')
    plt.xlabel('probe %s' % label, fontsize=8)
    plt.legend(loc='best', prop=fm.FontProperties(size=8))

    return plt.gcf(), results

materials['Asphalt'][0].update({'D' : stiffness_from_youngpoisson(2, young, poisson)})

# Update fields and variables to be able to use probes for tensors.
fields.update({
    'sym_tensor': ('real', 3, 'Omega', 0),
})

variables.update({
    's' : ('parameter field', 'sym_tensor', None),
})

options.update({
    'output_format'     : 'h5', # VTK reader cannot read cell data yet for probing
    'post_process_hook' : 'stress_strain',
    'gen_probes'        : 'gen_lines',
    'probe_hook'        : 'probe_hook',
})

########NEW FILE########
__FILENAME__ = linear_elastic
r"""
Linear elasticity with comments.

Find :math:`\ul{u}` such that:

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    = 0
    \;, \quad \forall \ul{v} \;,

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl}+\delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;.
"""
#!
#! Linear Elasticity
#! =================
#$ \centerline{Example input file, \today}

#! This file models a cylinder that is fixed at one end while the
#! second end has a specified displacement of 0.01 in the x direction
#! (this boundary condition is named Displaced). There is also a specified
#! displacement of 0.005 in the z direction for points in
#! the region labeled SomewhereTop. This boundary condition is named
#! PerturbedSurface.  The region SomewhereTop is specified as those nodes for
#! which
#!              (z > 0.017) & (x > 0.03) & (x <  0.07).
#! The output is the displacement for each node, saved by default to
#! simple_out.vtk. The material is linear elastic and its properties are
#! specified as Lame parameters (see
#! http://en.wikipedia.org/wiki/Lam%C3%A9_parameters)
#!

#! Mesh
#! ----
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/cylinder.mesh'
#! Regions
#! -------
#! Whole domain 'Omega', left and right ends.
regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < 0.001)', 'facet'),
    'Right' : ('vertices in (x > 0.099)', 'facet'),
    'SomewhereTop' : ('vertices in (z > 0.017) & (x > 0.03) & (x < 0.07)',
                      'vertex'),
}
#! Materials
#! ---------
#! The linear elastic material model is used. Properties are
#! specified as Lame parameters.
materials = {
    'solid' : ({'lam' : 1e1, 'mu' : 1e0},),
}
#! Fields
#! ------
#! A field is used to define the approximation on a (sub)domain
#! A displacement field (three DOFs/node) will be computed on a region
#! called 'Omega' using P1 (four-node tetrahedral) finite elements.
fields = {
    'displacement': ('real', 'vector', 'Omega', 1),
}
#! Integrals
#! ---------
#! Define the integral type Volume/Surface and quadrature rule
#! (here: dim=3, order=1).
integrals = {
    'i' : ('v', 1),
}
#! Variables
#! ---------
#! One field is used for unknown variable (generate discrete degrees
#! of freedom) and the seccond field for the corresponding test variable of
#! the weak formulation.
variables = {
    'u' : ('unknown field', 'displacement', 0),
    'v' : ('test field', 'displacement', 'u'),
}
#! Boundary Conditions
#! -------------------
#! The left end of the cilinder is fixed (all DOFs are zero) and
#! the 'right' end has non-zero displacements only in the x direction.
ebcs = {
    'Fixed' : ('Left', {'u.all' : 0.0}),
    'Displaced' : ('Right', {'u.0' : 0.01, 'u.[1,2]' : 0.0}),
    'PerturbedSurface' : ('SomewhereTop', {'u.2' : 0.005}),
}
#! Equations
#! ---------
#! The weak formulation of the linear elastic problem.
equations = {
    'balance_of_forces' :
    """dw_lin_elastic_iso.i.Omega( solid.lam, solid.mu, v, u ) = 0""",
}
#! Solvers
#! -------
#! Define linear and nonlinear solver.
#! Even linear problems are solved by a nonlinear solver (KISS rule) - only one
#! iteration is needed and the final rezidual is obtained for free.
solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton',
                { 'i_max'      : 1,
                  'eps_a'      : 1e-10,
                  'eps_r'      : 1.0,
                  'macheps'   : 1e-16,
                  # Linear system error < (eps_a * lin_red).
                  'lin_red'    : 1e-2,                
                  'ls_red'     : 0.1,
                  'ls_red_warp' : 0.001,
                  'ls_on'      : 1.1,
                  'ls_min'     : 1e-5,
                  'check'     : 0,
                  'delta'     : 1e-6,
                  # 'nonlinear' or 'linear' (ignore i_max)
                  'problem'   : 'nonlinear'}),
}

########NEW FILE########
__FILENAME__ = linear_elastic_damping
r"""
Time-dependent linear elasticity with a simple damping.

Find :math:`\ul{u}` such that:

.. math::
    \int_{\Omega} c\ \ul{v} \cdot \pdiff{\ul{u}}{t}
    + \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    = 0
    \;, \quad \forall \ul{v} \;,

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl}+\delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;.
"""
import numpy as nm
from linear_elastic import \
     filename_mesh, materials, regions, fields, ebcs, \
     integrals, solvers

def print_times(problem, state):
    print nm.array(problem.ts.times)

options = {
    'ts' : 'ts',
    'save_steps' : -1,
    'post_process_hook_final' : print_times,
}

variables = {
    'u' : ('unknown field', 'displacement', 0, 1),
    'v' : ('test field', 'displacement', 'u'),
}

# Put density to 'solid'.
materials['solid'][0].update({'c' : 1000.0})

# Moving the PerturbedSurface region.
ebcs['PerturbedSurface'][1].update({'u.0' : 'ebc_sin'})

def ebc_sin(ts, coors, **kwargs):
    val = 0.01 * nm.sin(2.0*nm.pi*ts.nt)
    return nm.tile(val, (coors.shape[0],))

equations = {
    'balance_of_forces in time' :
    """dw_volume_dot.i.Omega( solid.c, v, du/dt )
     + dw_lin_elastic_iso.i.Omega( solid.lam, solid.mu, v, u ) = 0""",
}

solvers.update({
    'ts' : ('ts.adaptive', {
        't0' : 0.0,
        't1' : 1.0,
        'dt' : None,
        'n_step' : 101,

        'adapt_fun' : 'adapt_time_step',
    }),
})

def adapt_time_step(ts, status, adt, problem):
    if ts.time > 0.5:
        ts.set_time_step(0.1)

    return True

# Pre-assemble and factorize the matrix prior to time-stepping.
newton = solvers['newton']
newton[1].update({'problem' : 'nonlinear'}) # Change of time step changes
                                            # matrix!

ls = solvers['ls']
ls[1].update({'presolve' : True})

functions = {
    'ebc_sin' : (ebc_sin,),
    'adapt_time_step' : (adapt_time_step,),
}

########NEW FILE########
__FILENAME__ = linear_elastic_iga
r"""
Linear elasticity solved in a single patch NURBS domain using the isogeometric
analysis (IGA) approach.

Find :math:`\ul{u}` such that:

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    = 0
    \;, \quad \forall \ul{v} \;,

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl}+\delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;.

The domain geometry was created by::

  $ ./script/gen_iga_patch.py -d [1,0.5,0.1] -s [11,5,3] --degrees [2,2,2] -o meshes/iga/block3d.iga

View the results using::

  $ ./postproc.py block3d.vtk --wireframe -b
  $ ./postproc.py block3d.vtk --wireframe -b -d 'u,plot_displacements,rel_scaling=1e0'
"""
from sfepy.mechanics.matcoefs import stiffness_from_lame
from sfepy import data_dir

filename_domain = data_dir + '/meshes/iga/block3d.iga'

regions = {
    'Omega' : 'all',
    'Gamma1' : ('vertices of set xi00', 'facet'),
    'Gamma2' : ('vertices of set xi01', 'facet'),
}

materials = {
    'solid' : ({
        'D' : stiffness_from_lame(3, lam=5.769, mu=3.846),
    },),
}

fields = {
    'displacement': ('real', 'vector', 'Omega', None, 'H1', 'iga'),
}

integrals = {
    'i' : 3,
}

variables = {
    'u' : ('unknown field', 'displacement', 0),
    'v' : ('test field', 'displacement', 'u'),
}

ebcs = {
    'u1' : ('Gamma1', {'u.all' : 0.0}),
    'u2' : ('Gamma2', {'u.0' : 0.1, 'u.[1,2]' : -0.05}),
}

equations = {
    'balance_of_forces' : """dw_lin_elastic.i.Omega(solid.D, v, u) = 0""",
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton', {
        'i_max'      : 1,
        'eps_a'      : 1e-10,
    }),
}

########NEW FILE########
__FILENAME__ = linear_elastic_probes
"""
This example shows how to use the post_process_hook and probe_hook options.

Use it as follows (assumes running from the sfepy directory; on Windows, you
may need to prefix all the commands with "python " and remove "./"):

1. solve the problem::

   ./simple.py examples/linear_elasticity/linear_elastic_probes.py

2. optionally, view the results::

   ./postproc.py cylinder.h5 -b

3. optionally, convert results to VTK, and view again::

   ./extractor.py -d cylinder.h5
   ./postproc.py cylinder.vtk -b

4. probe the data::

   ./probe.py examples/linear_elasticity/linear_elastic_probes.py cylinder.h5

Find :math:`\ul{u}` such that:

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    = 0
    \;, \quad \forall \ul{v} \;,

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl}+\delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;.
"""
# Just grab the problem definition of linear_elastic.py.
from linear_elastic import *

# Define options.
options = {
    'output_dir' : '.',
    'output_format' : 'h5', # VTK reader cannot read cell data yet...

    'post_process_hook' : 'post_process',
    'gen_probes' : 'gen_lines',
    'probe_hook' : 'probe_hook',
}

# Update materials, as ev_cauchy_stress below needs the elastic constants in
# the tensor form.
from sfepy.mechanics.matcoefs import stiffness_from_lame

solid = materials['solid'][0]
lam, mu = solid['lam'], solid['mu']
solid.update({
    'D' : stiffness_from_lame(3, lam=lam, mu=mu),
})

# Update fields and variables to be able to use probes for tensors.
fields.update({
    'sym_tensor': ('real', 6, 'Omega', 0),
})

variables.update({
    's' : ('parameter field', 'sym_tensor', None),
})

# Define the function post_process, that will be called after the problem is
# solved.
def post_process(out, problem, state, extend=False):
    """
    This will be called after the problem is solved.

    Parameters
    ----------
    out : dict
        The output dictionary, where this function will store additional data.
    problem : Problem instance
        The current Problem instance.
    state : State instance
        The computed state, containing FE coefficients of all the unknown
        variables.
    extend : bool
        The flag indicating whether to extend the output data to the whole
        domain. It can be ignored if the problem is solved on the whole domain
        already.

    Returns
    -------
    out : dict
        The updated output dictionary.
    """
    from sfepy.base.base import Struct

    # Cauchy strain averaged in elements.
    strain = problem.evaluate('ev_cauchy_strain.i.Omega( u )',
                              mode='el_avg')
    out['cauchy_strain'] = Struct(name='output_data',
                                  mode='cell', data=strain,
                                  dofs=None)
    # Cauchy stress averaged in elements.
    stress = problem.evaluate('ev_cauchy_stress.i.Omega( solid.D, u )',
                              mode='el_avg')
    out['cauchy_stress'] = Struct(name='output_data',
                                  mode='cell', data=stress,
                                  dofs=None)

    return out

# This function will be called by probe.py.
def gen_lines(problem):
    """
    Define three line probes in axial directions.

    Parameters
    ----------
    problem : Problem instance
        The current Problem instance.

    Returns
    -------
    probes : list
        The list of the probes.
    labels : list
        The list of probe labels.
    """
    from sfepy.discrete.probes import LineProbe

    mesh = problem.domain.mesh
    bbox = mesh.get_bounding_box()
    cx, cy, cz = 0.5 * bbox.sum(axis=0)
    print bbox
    print cx, cy, cz

    # Probe end points.
    ps0 = [[bbox[0,0], cy, cz],
           [cx, bbox[0,1], cz],
           [cx, cy, bbox[0,2]]]
    ps1 = [[bbox[1,0], cy, cz],
           [cx, bbox[1,1], cz],
           [cx, cy, bbox[1,2]]]

    # Use adaptive probe with 10 inital points.
    n_point = -10

    labels = ['%s -> %s' % (p0, p1) for p0, p1 in zip(ps0, ps1)]
    probes = []
    for ip in xrange(len(ps0)):
        p0, p1 = ps0[ip], ps1[ip]
        probes.append(LineProbe(p0, p1, n_point))

    return probes, labels

# This function will be called by probe.py.
def probe_hook(data, probe, label, problem):
    """
    Parameters
    ----------
    data : dict
        The output data.
    probe : Probe subclass instance
        The probe to be used on data.
    label : str
        The label describing the probe.
    problem : Problem instance
        The current Problem instance.

    Returns
    -------
    fig : figure
        The matplotlib figure with the probe plot.
    results : dict
        The dict of tuples (pars, vals) of the probe parametrization and the
        corresponding probed data.
    """
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm

    def get_it(name, var_name):
        var = problem.create_variables([var_name])[var_name]
        var.set_data(data[name].data)

        pars, vals = probe(var)
        vals = vals.squeeze()
        return pars, vals

    results = {}
    results['u'] = get_it('u', 'u')
    results['cauchy_strain'] = get_it('cauchy_strain', 's')
    results['cauchy_stress'] = get_it('cauchy_stress', 's')

    fig = plt.figure()
    plt.clf()
    fig.subplots_adjust(hspace=0.4)

    plt.subplot(311)
    pars, vals = results['u']
    for ic in range(vals.shape[1]):
        plt.plot(pars, vals[:,ic], label=r'$u_{%d}$' % (ic + 1),
                 lw=1, ls='-', marker='+', ms=3)
    plt.ylabel('displacements')
    plt.xlabel('probe %s' % label, fontsize=8)
    plt.legend(loc='best', prop=fm.FontProperties(size=10))

    sym_indices = ['11', '22', '33', '12', '13', '23']

    plt.subplot(312)
    pars, vals = results['cauchy_strain']
    for ic in range(vals.shape[1]):
        plt.plot(pars, vals[:,ic], label=r'$e_{%s}$' % sym_indices[ic],
                 lw=1, ls='-', marker='+', ms=3)
    plt.ylabel('Cauchy strain')
    plt.xlabel('probe %s' % label, fontsize=8)
    plt.legend(loc='best', prop=fm.FontProperties(size=8))

    plt.subplot(313)
    pars, vals = results['cauchy_stress']
    for ic in range(vals.shape[1]):
        plt.plot(pars, vals[:,ic], label=r'$\tau_{%s}$' % sym_indices[ic],
                 lw=1, ls='-', marker='+', ms=3)
    plt.ylabel('Cauchy stress')
    plt.xlabel('probe %s' % label, fontsize=8)
    plt.legend(loc='best', prop=fm.FontProperties(size=8))

    return plt.gcf(), results

########NEW FILE########
__FILENAME__ = linear_elastic_tractions
r"""
Linear elasticity with pressure traction load on a surface and constrained to
one-dimensional motion.

Find :math:`\ul{u}` such that:

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    = - \int_{\Gamma_{right}} \ul{v} \cdot \ull{\sigma} \cdot \ul{n}
    \;, \quad \forall \ul{v} \;,

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl}+\delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;.

and :math:`\ull{\sigma} \cdot \ul{n} = \bar{p} \ull{I} \cdot \ul{n}`
with given traction pressure :math:`\bar{p}`.
"""
import numpy as nm

def linear_tension(ts, coor, mode=None, **kwargs):
    if mode == 'qp':
        val = nm.tile(1.0, (coor.shape[0], 1, 1))

        return {'val' : val}

def define():
    """Define the problem to solve."""
    from sfepy import data_dir

    filename_mesh = data_dir + '/meshes/3d/block.mesh'

    options = {
        'nls' : 'newton',
        'ls' : 'ls',
    }

    functions = {
        'linear_tension' : (linear_tension,),
    }

    fields = {
        'displacement': ('real', 3, 'Omega', 1),
    }

    materials = {
        'solid' : ({
            'lam' : 5.769,
            'mu' : 3.846,
        },),
        'load' : (None, 'linear_tension')
    }

    variables = {
        'u' : ('unknown field', 'displacement', 0),
        'v' : ('test field', 'displacement', 'u'),
    }

    regions = {
        'Omega' : 'all',
        'Left' : ('vertices in (x < -4.99)', 'facet'),
        'Right' : ('vertices in (x > 4.99)', 'facet'),
    }

    ebcs = {
        'fixb' : ('Left', {'u.all' : 0.0}),
        'fixt' : ('Right', {'u.[1,2]' : 0.0}),
    }

    ##
    # Balance of forces.
    equations = {
        'elasticity' :
        """dw_lin_elastic_iso.2.Omega( solid.lam, solid.mu, v, u )
         = - dw_surface_ltr.2.Right( load.val, v )""",
    }

    ##
    # Solvers etc.
    solvers = {
        'ls' : ('ls.scipy_direct', {}),
        'newton' : ('nls.newton',
                    { 'i_max'      : 1,
                      'eps_a'      : 1e-10,
                      'eps_r'      : 1.0,
                      'macheps'   : 1e-16,
                      # Linear system error < (eps_a * lin_red).
                      'lin_red'    : 1e-2,                
                      'ls_red'     : 0.1,
                      'ls_red_warp' : 0.001,
                      'ls_on'      : 1.1,
                      'ls_min'     : 1e-5,
                      'check'     : 0,
                      'delta'     : 1e-6,
                      # 'nonlinear' or 'linear' (ignore i_max)
                      'problem'   : 'nonlinear'}),
    }

    return locals()

########NEW FILE########
__FILENAME__ = linear_elastic_up
r"""
Nearly incompressible linear elasticity in mixed displacement-pressure
formulation with comments.

Find :math:`\ul{u}`, :math:`p` such that:

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    - \int_{\Omega} p\ \nabla \cdot \ul{v}
    = 0
    \;, \quad \forall \ul{v} \;,

    - \int_{\Omega} q\ \nabla \cdot \ul{u}
    - \int_{\Omega} \gamma q p
    = 0
    \;, \quad \forall q \;.
"""
#!
#! Linear Elasticity
#! =================
#$ \centerline{Example input file, \today}

#! This file models a cylinder that is fixed at one end while the
#! second end has a specified displacement of 0.02 in the x direction
#! (this boundary condition is named PerturbedSurface).
#! The output is the displacement for each node, saved by default to
#! simple_out.vtk. The material is linear elastic.
from sfepy import data_dir

from sfepy.mechanics.matcoefs import stiffness_from_youngpoisson_mixed, bulk_from_youngpoisson

#! Mesh
#! ----

dim = 3
approx_u = '3_4_P1'
approx_p = '3_4_P0'
order = 2
filename_mesh = data_dir + '/meshes/3d/cylinder.mesh'
#! Regions
#! -------
#! Whole domain 'Omega', left and right ends.
regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < 0.001)', 'facet'),
    'Right' : ('vertices in (x > 0.099)', 'facet'),
}
#! Materials
#! ---------
#! The linear elastic material model is used.
materials = {
    'solid' : ({'D' : stiffness_from_youngpoisson_mixed(dim, 0.7e9, 0.4),
                'gamma' : 1.0/bulk_from_youngpoisson(0.7e9, 0.4)},),
}
#! Fields
#! ------
#! A field is used to define the approximation on a (sub)domain
fields = {
    'displacement': ('real', 'vector', 'Omega', 1),
    'pressure' : ('real', 'scalar', 'Omega', 0),
}
#! Integrals
#! ---------
#! Define the integral type Volume/Surface and quadrature rule.
integrals = {
    'i' : order,
}
#! Variables
#! ---------
#! Define displacement and pressure fields and corresponding fields
#! for test variables.
variables = {
    'u' : ('unknown field', 'displacement'),
    'v' : ('test field', 'displacement', 'u'),
    'p' : ('unknown field', 'pressure'),
    'q' : ('test field', 'pressure', 'p'),
}
#! Boundary Conditions
#! -------------------
#! The left end of the cylinder is fixed (all DOFs are zero) and
#! the 'right' end has non-zero displacements only in the x direction.
ebcs = {
    'Fixed' : ('Left', {'u.all' : 0.0}),
    'PerturbedSurface' : ('Right', {'u.0' : 0.02, 'u.1' : 0.0}),
}
#! Equations
#! ---------
#! The weak formulation of the linear elastic problem.
equations = {
    'balance_of_forces' :
    """  dw_lin_elastic.i.Omega( solid.D, v, u )
       - dw_stokes.i.Omega( v, p )
       = 0 """,
    'pressure constraint' :
    """- dw_stokes.i.Omega( u, q )
       - dw_volume_dot.i.Omega( solid.gamma, q, p )
       = 0""",
}
#! Solvers
#! -------
#! Define linear and nonlinear solver.
#! Even linear problems are solved by a nonlinear solver - only one
#! iteration is needed and the final rezidual is obtained for free.
solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton',
                { 'i_max'      : 1,
                  'eps_a'      : 1e-2,
                  'eps_r'      : 1e-10,
                  'problem'   : 'nonlinear'}),
}
#! Options
#! -------
#! Various problem-specific options.
options = {
    'output_dir' : './output',
    'absolute_mesh_path' : True,
}

########NEW FILE########
__FILENAME__ = linear_viscoelastic
r"""
Linear viscoelasticity with pressure traction load on a surface and constrained
to one-dimensional motion.

The fading memory terms require an unloaded initial configuration, so the load
starts in the second time step. The load is then held for the first half of the
total time interval, and released afterwards.

This example uses exponential fading memory kernel
:math:`\Hcal_{ijkl}(t) = \Hcal_{ijkl}(0) e^{-d t}` with decay
:math:`d`. Two equation kinds are supported - 'th' and 'eth'. In 'th'
mode the tabulated kernel is linearly interpolated to required times
using :func:`interp_conv_mat()`. In 'eth' mode, the computation is exact
for exponential kernels.

Find :math:`\ul{u}` such that:

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u}) \\
    + \int_{\Omega} \left [\int_0^t
    \Hcal_{ijkl}(t-\tau)\,e_{kl}(\pdiff{\ul{u}}{\tau}(\tau)) \difd{\tau}
    \right]\,e_{ij}(\ul{v}) \\
    = - \int_{\Gamma_{right}} \ul{v} \cdot \ull{\sigma} \cdot \ul{n}
    \;, \quad \forall \ul{v} \;,

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl}+\delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;,

:math:`\Hcal_{ijkl}(0)` has the same structure as :math:`D_{ijkl}` and
:math:`\ull{\sigma} \cdot \ul{n} = \bar{p} \ull{I} \cdot \ul{n}` with
given traction pressure :math:`\bar{p}`.

Notes
-----

Because this example is run also as a test, it uses by default very few time
steps. Try changing that.

Visualization
-------------

The output file is assumed to be 'block.h5' in the working directory. Change it
appropriately for your situation.

Deforming mesh
^^^^^^^^^^^^^^

Try to play with the following::

    $ ./postproc.py block.h5 -b --only-names=u -d 'u,plot_displacements,rel_scaling=1e0,opacity=1.0,color_name="viscous_stress",color_kind="tensors"' --wireframe

Use::

    $ ./postproc.py -l block.h5

to see names and kinds of variables.

Time history plots
^^^^^^^^^^^^^^^^^^

Run the following::

    $ python examples/linear_elasticity/linear_viscoelastic.py -h
    $ python examples/linear_elasticity/linear_viscoelastic.py block.h5

Try comparing 'th' and 'eth' versions, e.g., for n_step = 201, and f_n_step =
51. There is a visible notch on viscous stress curves in the 'th' mode, as the
fading memory kernel is cut off before it goes close enough to zero.
"""
import numpy as nm

from sfepy.base.base import output
from sfepy.mechanics.matcoefs import stiffness_from_lame
from sfepy.homogenization.utils import interp_conv_mat
from sfepy import data_dir

def linear_tension(ts, coors, mode=None, verbose=True, **kwargs):
    if mode == 'qp':
        val = 1.0 * ((ts.step > 0) and (ts.nt <= 0.5))

        if verbose:
            output('load:', val)

        val = nm.tile(val, (coors.shape[0], 1, 1))

        return {'val' : val}

def get_exp_fading_kernel(coef0, decay, times):
    val = coef0[None, ...] * nm.exp(-decay * times[:, None, None])
    return val

def get_th_pars(ts, coors, mode=None, times=None, kernel=None, **kwargs):
    out = {}

    if mode == 'special':
        out['H'] = interp_conv_mat(kernel, ts, times)

    elif mode == 'qp':
        out['H0'] = kernel[0]
        out['Hd'] = kernel[1, 0, 0] / kernel[0, 0, 0]

        for key, val in out.iteritems():
            out[key] = nm.tile(val, (coors.shape[0], 1, 1))

    return out

filename_mesh = data_dir + '/meshes/3d/block.mesh'

## Configure below. ##

# Time stepping times.
t0 = 0.0
t1 = 20.0
n_step = 21

# Fading memory times.
f_t0 = 0.0
f_t1 = 5.0
f_n_step = 6

decay = 0.8
mode = 'eth'

## Configure above. ##

times = nm.linspace(f_t0, f_t1, f_n_step)
kernel = get_exp_fading_kernel(stiffness_from_lame(3, lam=1.0, mu=1.0),
                               decay, times)

dt = (t1 - t0) / (n_step - 1)
fading_memory_length = min(int((f_t1 - f_t0) / dt) + 1, n_step)
output('fading memory length:', fading_memory_length)

def post_process(out, pb, state, extend=False):
    """
    Calculate and output strain and stress for given displacements.
    """
    from sfepy.base.base import Struct

    ev = pb.evaluate
    strain = ev('ev_cauchy_strain.2.Omega(u)', mode='el_avg')
    out['cauchy_strain'] = Struct(name='output_data', mode='cell',
                                  data=strain, dofs=None)

    estress = ev('ev_cauchy_stress.2.Omega(solid.D, u)', mode='el_avg')
    out['cauchy_stress'] = Struct(name='output_data', mode='cell',
                                  data=estress, dofs=None)

    ts = pb.get_timestepper()
    if mode == 'th':
        vstress = ev('ev_cauchy_stress_th.2.Omega(ts, th.H, du/dt)',
                     ts=ts, mode='el_avg')
        out['viscous_stress'] = Struct(name='output_data', mode='cell',
                                       data=vstress, dofs=None)

    else:
        # The eth terms require 'preserve_caches=True' in order to have correct
        # fading memory history.
        vstress = ev('ev_cauchy_stress_eth.2.Omega(ts, th.H0, th.Hd, du/dt)',
                     ts=ts, mode='el_avg', preserve_caches=True)
        out['viscous_stress'] = Struct(name='output_data', mode='cell',
                                       data=vstress, dofs=None)

    out['total_stress'] = Struct(name='output_data', mode='cell',
                                 data=estress + vstress, dofs=None)

    return out

options = {
    'ts' : 'ts',
    'nls' : 'newton',
    'ls' : 'ls',

    'output_format'     : 'h5',
    'post_process_hook' : 'post_process',
}

functions = {
    'linear_tension' : (linear_tension,),
    'get_pars' : (lambda ts, coors, mode=None, **kwargs:
                  get_th_pars(ts, coors, mode, times=times, kernel=kernel,
                              **kwargs),),
}

fields = {
    'displacement': ('real', 3, 'Omega', 1),
}

materials = {
    'solid' : ({
        'D' : stiffness_from_lame(3, lam=5.769, mu=3.846),
    },),
    'th' : 'get_pars',
    'load' : 'linear_tension',
}

variables = {
    'u' : ('unknown field', 'displacement', 0, fading_memory_length),
    'v' : ('test field', 'displacement', 'u'),
}

regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < -4.99)', 'facet'),
    'Right' : ('vertices in (x > 4.99)', 'facet'),
}

ebcs = {
    'fixb' : ('Left', {'u.all' : 0.0}),
    'fixt' : ('Right', {'u.[1,2]' : 0.0}),
}

if mode == 'th':
    # General form with tabulated kernel.
    equations = {
        'elasticity' :
        """dw_lin_elastic.2.Omega( solid.D, v, u )
         + dw_lin_elastic_th.2.Omega( ts, th.H, v, du/dt )
         = - dw_surface_ltr.2.Right( load.val, v )""",
    }

else:
    # Fast form that is exact for exponential kernels.
    equations = {
        'elasticity' :
        """dw_lin_elastic.2.Omega( solid.D, v, u )
         + dw_lin_elastic_eth.2.Omega( ts, th.H0, th.Hd, v, du/dt )
         = - dw_surface_ltr.2.Right( load.val, v )""",
    }

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton',
                { 'i_max' : 1,
                  'eps_a' : 1e-10,
                  'problem' : 'nonlinear'}),
    'ts' : ('ts.simple', {
        't0' : t0,
        't1' : t1,
        'dt' : None,
        'n_step' : n_step,
        'quasistatic' : True,
    }),
}

def main():
    """
    Plot the load, displacement, strain and stresses w.r.t. time.
    """
    from optparse import OptionParser
    import matplotlib.pyplot as plt

    import sfepy.postprocess.time_history as th

    usage = """%prog <output file in HDF5 format>"""

    msgs = {'node' : 'plot displacements in given node [default: %default]',
            'element' : 'plot tensors in given element [default: %default]',}

    parser = OptionParser(usage=usage)
    parser.add_option('-n', '--node', type=int, metavar='ii',
                      action='store', dest='node',
                      default=512, help=msgs['node'])
    parser.add_option('-e', '--element', type=int, metavar='ii',
                      action='store', dest='element',
                      default=299, help=msgs['element'])
    options, args = parser.parse_args()

    if len(args) == 1:
        filename = args[0]

    else:
        parser.print_help()
        return

    tensor_names = ['cauchy_strain',
                    'cauchy_stress', 'viscous_stress', 'total_stress']
    extract = ('u n %d, ' % options.node) \
              + ', '.join('%s e %d' % (name, options.element)
                          for name in tensor_names)
    ths, ts = th.extract_time_history(filename, extract)

    load = [linear_tension(ts, nm.array([0]),
                           mode='qp', verbose=False)['val'].squeeze()
            for ii in ts]
    load = nm.array(load)

    normalized_kernel = kernel[:, 0, 0] / kernel[0, 0, 0]

    plt.figure(1, figsize=(8, 10))
    plt.subplots_adjust(hspace=0.3,
                        top=0.95, bottom=0.05, left=0.07, right=0.95)

    plt.subplot(311)
    plt.plot(times, normalized_kernel, lw=3)
    plt.title('fading memory decay')
    plt.xlabel('time')

    plt.subplot(312)
    plt.plot(ts.times, load, lw=3)
    plt.title('load')
    plt.xlabel('time')

    displacements = ths['u'][options.node]

    plt.subplot(313)
    plt.plot(ts.times, displacements, lw=3)
    plt.title('displacement components, node %d' % options.node)
    plt.xlabel('time')

    plt.figure(2, figsize=(8, 10))
    plt.subplots_adjust(hspace=0.35,
                        top=0.95, bottom=0.05, left=0.07, right=0.95)

    for ii, tensor_name in enumerate(tensor_names):
        tensor = ths[tensor_name][options.element]

        plt.subplot(411 + ii)
        plt.plot(ts.times, tensor, lw=3)
        plt.title('%s components, element %d' % (tensor_name, options.element))
        plt.xlabel('time')

    plt.show()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = material_nonlinearity
# -*- coding: utf-8 -*-
r"""
Example demonstrating how a linear elastic term can be used to solve an
elasticity problem with a material nonlinearity.

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    = 0
    \;, \quad \forall \ul{v} \;,

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl}+\delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;.
"""
import numpy as nm

from sfepy.linalg import norm_l2_along_axis
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/cylinder.mesh'

def post_process(out, pb, state, extend=False):
    from sfepy.base.base import Struct

    mu = pb.evaluate('ev_integrate_mat.2.Omega(nonlinear.mu, u)',
                     mode='el_avg', copy_materials=False, verbose=False)
    out['mu'] = Struct(name='mu', mode='cell', data=mu, dofs=None)

    strain = pb.evaluate('ev_cauchy_strain.2.Omega(u)', mode='el_avg')
    out['strain'] = Struct(name='strain', mode='cell', data=strain, dofs=None)

    return out

strains = [None]

def get_pars(ts, coors, mode='qp',
             equations=None, term=None, problem=None, **kwargs):
    """
    The material nonlinearity function - the Lam coefficient `mu`
    depends on the strain.
    """
    if mode != 'qp': return

    val = nm.empty((coors.shape[0], 1, 1), dtype=nm.float64)
    val.fill(1e0)

    order = term.integral.order
    uvar = equations.variables['u']

    strain = problem.evaluate('ev_cauchy_strain.%d.Omega(u)' % order,
                              u=uvar, mode='qp')
    if ts.step > 0:
        strain0 = strains[-1]

    else:
        strain0 = strain

    dstrain = (strain - strain0) / ts.dt
    dstrain.shape = (strain.shape[0] * strain.shape[1], strain.shape[2])

    norm = norm_l2_along_axis(dstrain)

    val += norm[:, None, None]

    # Store history.
    strains[0] = strain

    return {'mu' : val}

def pull(ts, coors, **kwargs):
    val = nm.empty_like(coors[:,0])
    val.fill(0.01 * ts.step)

    return val

functions = {
    'get_pars' : (get_pars,),
    'pull' : (pull,),
}

options = {
    'ts' : 'ts',
    'output_format' : 'h5',
    'save_steps' : -1,

    'post_process_hook' : 'post_process',
}

regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < 0.001)', 'facet'),
    'Right' : ('vertices in (x > 0.099)', 'facet'),
}

materials = {
    'linear' : ({'lam' : 1e1},),
    'nonlinear' : 'get_pars',
}

fields = {
    'displacement': ('real', 'vector', 'Omega', 1),
}

variables = {
    'u' : ('unknown field', 'displacement', 0),
    'v' : ('test field', 'displacement', 'u'),
}

ebcs = {
    'Fixed' : ('Left', {'u.all' : 0.0}),
    'Displaced' : ('Right', {'u.0' : 'pull', 'u.[1,2]' : 0.0}),
}

equations = {
    'balance_of_forces in time' :
    """dw_lin_elastic_iso.2.Omega(linear.lam, nonlinear.mu, v, u) = 0""",
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton',
                { 'i_max'      : 1,
                  'eps_a'      : 1e-10,
                  'eps_r'      : 1.0,
                  'problem'   : 'nonlinear'}),
    'ts' : ('ts.simple',
            {'t0' : 0.0,
             't1' : 1.0,
             'dt' : None,
             'n_step' : 5,
             'quasistatic' : True,
             }),
}

########NEW FILE########
__FILENAME__ = prestress_fibres
r"""
Linear elasticity with a given prestress in one subdomain and a (pre)strain
fibre reinforcement in the other.

Find :math:`\ul{u}` such that:

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    + \int_{\Omega_1} \sigma_{ij} e_{ij}(\ul{v})
    + \int_{\Omega_2} D^f_{ijkl} e_{ij}(\ul{v}) \left(d_k d_l\right)
    = 0
    \;, \quad \forall \ul{v} \;,

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl}+\delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;.

The stiffness of fibres :math:`D^f_{ijkl}` is defined analogously,
:math:`\ul{d}` is the unit fibre direction vector and :math:`\sigma_{ij}` is
the prestress.

Visualization
-------------

Use the following to see the deformed structure with 10x magnified
displacements::

    $ ./postproc.py block.vtk -b --vector-mode=warp_norm -s 1 --wireframe
"""
import numpy as nm
from sfepy.mechanics.matcoefs import stiffness_from_lame
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/block.mesh'

regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < -4.99)', 'facet'),
    'Omega1' : 'vertices in (x < 0.001)',
    'Omega2' : 'vertices in (x > -0.001)',
}

materials = {
    'solid' : ({
        'D' : stiffness_from_lame(3, lam=1e2, mu=1e1),
        'prestress' : 0.1 * nm.array([[1.0], [1.0], [1.0],
                                      [0.5], [0.5], [0.5]],
                                     dtype=nm.float64),
        'DF' : stiffness_from_lame(3, lam=8e0, mu=8e-1),
        'nu' : nm.array([[-0.5], [0.0], [0.5]], dtype=nm.float64),
    },),
}

fields = {
    'displacement': ('real', 'vector', 'Omega', 1),
}

variables = {
    'u' : ('unknown field', 'displacement', 0),
    'v' : ('test field', 'displacement', 'u'),
}

ebcs = {
    'Fixed' : ('Left', {'u.all' : 0.0}),
}

equations = {
    'balance_of_forces' :
    """dw_lin_elastic.2.Omega( solid.D, v, u )
     + dw_lin_prestress.2.Omega1( solid.prestress, v )
     + dw_lin_strain_fib.2.Omega2( solid.DF, solid.nu, v )
     = 0""",
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton',
                { 'i_max' : 1,
                  'eps_a' : 1e-10,
                  'problem' : 'nonlinear'}),
}

########NEW FILE########
__FILENAME__ = compare_scalar_terms
r"""
Example without a physical relevance comparing new and old-style terms
with scalar variables.

Find :math:`p` (new style terms), :math:`r` (old_style terms) such that:

.. math::
    \int_{\Omega} c \delta_{ij} \nabla_i q \nabla_j p
    + \int_{\Omega} q p
    = 0
    \;, \quad \forall q \;,

    \int_{\Omega} c \delta_{ij} \nabla_i s \nabla_j r
    + \int_{\Omega} s r
    = 0
    \;, \quad \forall s \;.

The same values of :math:`p`, :math:`r` should be obtained.
"""
import os

import numpy as nm

from sfepy import data_dir
from sfepy.discrete.fem import MeshIO

filename_mesh = data_dir + '/meshes/3d/cylinder.mesh'
#filename_mesh = data_dir + '/meshes/3d/cube_big_tetra.mesh'

conf_dir = os.path.dirname(__file__)
io = MeshIO.any_from_filename(filename_mesh, prefix_dir=conf_dir)
bbox = io.read_bounding_box()

dd = bbox[1] - bbox[0]

xmin, ymin, zmin = bbox[0, :] + 1e-4 * dd
xmax, ymax, zmax = bbox[1, :] - 1e-4 * dd

def post_process(out, pb, state, extend=False):

    for vn in ['p', 'r']:
        try:
            dd = pb.evaluate('dw_new_diffusion.2.Omega(m.c, %s, %s)'
                             % (vn, vn), verbose=False)
            print 'dw_new_diffusion', vn, dd

            dd = pb.evaluate('dw_diffusion.2.Omega(m.c, %s, %s)'
                             % (vn, vn), verbose=False)
            print 'dw_diffusion', vn, dd

            mass = pb.evaluate('dw_new_mass.2.Omega(%s, %s)'
                               % (vn, vn), verbose=False)
            print 'dw_new_mass', vn, mass

            mass = pb.evaluate('dw_new_mass_scalar.2.Omega(%s, %s)'
                               % (vn, vn), verbose=False)
            print 'dw_new_mass_scalar', vn, mass

            mass = pb.evaluate('dw_volume_dot.2.Omega(%s, %s)'
                               % (vn, vn), verbose=False)
            print 'dw_volume_dot', vn, mass

        except:
            pass

    return out

options = {
    'nls' : 'newton',
    'ls' : 'ls',
    'post_process_hook' : 'post_process',
}

materials = {
    'm' : ({'c' : 0.0001 * nm.eye(3)},),
}

regions = {
    'Omega' : 'all',
    'Gamma_Left' : ('vertices in (x < %f)' % xmin, 'facet'),
    'Gamma_Right' : ('vertices in (x > %f)' % xmax, 'facet'),
}

fields = {
    'temperature' : ('real', 1, 'Omega', 2),
}

variables = {
    'p' : ('unknown field', 'temperature', 0),
    'q' : ('test field',    'temperature', 'p'),
    'r' : ('unknown field', 'temperature', 1),
    's' : ('test field',    'temperature', 'r'),
}

ebcs = {
    'p1' : ('Gamma_Left', {'p.0' : 2.0}),
    'p2' : ('Gamma_Right', {'p.0' : -2.0}),
    'r1' : ('Gamma_Left', {'r.0' : 2.0}),
    'r2' : ('Gamma_Right', {'r.0' : -2.0}),
}

equations = {
    'new equation' :
    """dw_new_diffusion.2.Omega(m.c, q, p)
     + dw_new_mass.2.Omega(q, p)
     = 0""",
    'equation' :
    """dw_diffusion.2.Omega(m.c, s, r)
     + dw_volume_dot.2.Omega(s, r)
     = 0""",
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton',
                {'i_max'      : 1,
                 'eps_a'      : 1e-10,
    }),
}

########NEW FILE########
__FILENAME__ = compare_vector_terms
r"""
Example without a physical relevance comparing new and old-style terms
with vector variables.

Find :math:`\ul{u}` (new style terms), :math:`\ul{r}` (old_style terms)
such that:

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    \int_{\Omega}\ul{v} \cdot \ul{u}
    = 0
    \;, \quad \forall \ul{v} \;,

    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{s}) e_{kl}(\ul{r})
    \int_{\Omega}\ul{s} \cdot \ul{r}
    = 0
    \;, \quad \forall \ul{s} \;.

The same values of :math:`\ul{u}`, :math:`\ul{r}` should be obtained.
"""
import os

from sfepy import data_dir
from sfepy.discrete.fem import MeshIO
from sfepy.mechanics.matcoefs import stiffness_from_lame

filename_mesh = data_dir + '/meshes/3d/cylinder.mesh'
#filename_mesh = data_dir + '/meshes/3d/cube_medium_hexa.mesh'

conf_dir = os.path.dirname(__file__)
io = MeshIO.any_from_filename(filename_mesh, prefix_dir=conf_dir)
bbox = io.read_bounding_box()

dd = bbox[1] - bbox[0]

xmin, ymin, zmin = bbox[0, :] + 1e-4 * dd
xmax, ymax, zmax = bbox[1, :] - 1e-4 * dd

def post_process(out, pb, state, extend=False):

    for vn in ['u', 'r']:
        try:
            val = pb.evaluate('dw_new_mass.2.Omega(%s, %s)'
                              % (vn, vn), verbose=False)
            print 'dw_new_mass', vn, val

            val = pb.evaluate('dw_new_lin_elastic.2.Omega(m.D, %s, %s)'
                              % (vn, vn), verbose=False)
            print 'dw_new_lin_elastic', vn, val

            val = pb.evaluate('dw_lin_elastic.2.Omega(m.D, %s, %s)'
                              % (vn, vn), verbose=False)
            print 'dw_lin_elastic', vn, val

        except:
            pass

    return out

options = {
    'nls' : 'newton',
    'ls' : 'ls',
    'post_process_hook' : 'post_process',
}

materials = {
    'm' : ({'D' : stiffness_from_lame(3, lam=0.0007, mu=0.0003),
            'one' : 1.0},),
}

regions = {
    'Omega' : 'all',
    'Gamma_Left' : ('vertices in (x < %f)' % xmin, 'facet'),
    'Gamma_Right' : ('vertices in (x > %f)' % xmax, 'facet'),
}

fields = {
    'displacements' : ('real', 'vector', 'Omega', 1),
}

variables = {
    'u' : ('unknown field', 'displacements', 0),
    'v' : ('test field',    'displacements', 'u'),
    'r' : ('unknown field', 'displacements', 1),
    's' : ('test field',    'displacements', 'r'),
}

ebcs = {
    'u1' : ('Gamma_Left', {'u.all' : 0.0}),
    'u2' : ('Gamma_Right', {'u.0' : 0.1 * (xmax - xmin),
                            'u.1' : 0.1 * (ymax - ymin)}),
    'r1' : ('Gamma_Left', {'r.all' : 0.0}),
    'r2' : ('Gamma_Right', {'r.0' : 0.1 * (xmax - xmin),
                            'r.1' : 0.1 * (ymax - ymin)}),
}

equations = {
    'new equation' :
    """dw_new_lin_elastic.2.Omega(m.D, v, u)
     + dw_new_mass.2.Omega(v, u)
     = 0""",
    'equation' :
    """dw_lin_elastic.2.Omega(m.D, s, r)
     + dw_volume_dot.2.Omega(m.one, s, r)
     = 0""",
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton',
                {'i_max'      : 1,
                 'eps_a'      : 1e-10,
    }),
}

########NEW FILE########
__FILENAME__ = navier_stokes
r"""
Navier-Stokes equations for incompressible fluid flow.

Find :math:`\ul{u}`, :math:`p` such that:

.. math::
    \int_{\Omega} \nu\ \nabla \ul{v} : \nabla \ul{u}
    + \int_{\Omega} ((\ul{u} \cdot \nabla) \ul{u}) \cdot \ul{v}
    - \int_{\Omega} p\ \nabla \cdot \ul{v}
    = 0
    \;, \quad \forall \ul{v} \;,

    \int_{\Omega} q\ \nabla \cdot \ul{u}
    = 0
    \;, \quad \forall q \;.
"""
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/elbow2.mesh'

options = {
    'nls' : 'newton',
    'ls' : 'ls',
    'post_process_hook' : 'verify_incompressibility',

    # Options for saving higher-order variables.
    # Possible kinds:
    #    'strip' ... just remove extra DOFs (ignores other linearization
    #                options)
    #    'adaptive' ... adaptively refine linear element mesh.
    'linearization' : {
        'kind' : 'strip',
        'min_level' : 1, # Min. refinement level to achieve everywhere.
        'max_level' : 2, # Max. refinement level.
        'eps' : 1e-1, # Relative error tolerance.
    },
}

field_1 = {
    'name' : '3_velocity',
    'dtype' : 'real',
    'shape' : (3,),
    'region' : 'Omega',
    'approx_order' : '1B',
}

field_2 = {
    'name' : 'pressure',
    'dtype' : 'real',
    'shape' : (1,),
    'region' : 'Omega',
    'approx_order' : 1,
}

# Can use logical operations '&' (and), '|' (or).
region_1000 = {
    'name' : 'Omega',
    'select' : 'cells of group 6',
}

region_0 = {
    'name' : 'Walls',
    'select' : 'vertices of surface -v (r.Outlet +v r.Inlet)',
    'kind' : 'facet',
}
region_1 = {
    'name' : 'Inlet',
    'select' : 'vertices by cinc0', # In
    'kind' : 'facet',
}
region_2 = {
    'name' : 'Outlet',
    'select' : 'vertices by cinc1', # Out
    'kind' : 'facet',
}

ebc_1 = {
    'name' : 'Walls',
    'region' : 'Walls',
    'dofs' : {'u.all' : 0.0},
}
ebc_2 = {
    'name' : 'Inlet',
    'region' : 'Inlet',
    'dofs' : {'u.1' : 1.0, 'u.[0,2]' : 0.0},
}

material_1 = {
    'name' : 'fluid',
    'values' : {
        'viscosity' : 1.25e-3,
        'density' : 1e0,
    },
}

variable_1 = {
    'name' : 'u',
    'kind' : 'unknown field',
    'field' : '3_velocity',
    'order' : 0,
}
variable_2 = {
    'name' : 'v',
    'kind' : 'test field',
    'field' : '3_velocity',
    'dual' : 'u',
}
variable_3 = {
    'name' : 'p',
    'kind' : 'unknown field',
    'field' : 'pressure',
    'order' : 1,
}
variable_4 = {
    'name' : 'q',
    'kind' : 'test field',
    'field' : 'pressure',
    'dual' : 'p',
}
variable_5 = {
    'name' : 'pp',
    'kind' : 'parameter field',
    'field' : 'pressure',
    'like' : 'p',
}

integral_1 = {
    'name' : 'i1',
    'order' : 2,
}
integral_2 = {
    'name' : 'i2',
    'order' : 3,
}

##
# Stationary Navier-Stokes equations.
equations = {
    'balance' :
    """+ dw_div_grad.i2.Omega( fluid.viscosity, v, u )
       + dw_convect.i2.Omega( v, u )
       - dw_stokes.i1.Omega( v, p ) = 0""",
    'incompressibility' :
    """dw_stokes.i1.Omega( u, q ) = 0""",
}

solver_0 = {
    'name' : 'ls',
    'kind' : 'ls.scipy_direct',
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 5,
    'eps_a'      : 1e-8,
    'eps_r'      : 1.0,
    'macheps'   : 1e-16,
    'lin_red'    : 1e-2, # Linear system error < (eps_a * lin_red).
    'ls_red'     : 0.1,
    'ls_red_warp' : 0.001,
    'ls_on'      : 0.99999,
    'ls_min'     : 1e-5,
    'check'     : 0,
    'delta'     : 1e-6,
    'problem'   : 'nonlinear', # 'nonlinear' or 'linear' (ignore i_max)
}

def verify_incompressibility( out, problem, state, extend = False ):
    """This hook is normally used for post-processing (additional results can
    be inserted into `out` dictionary), but here we just verify the weak
    incompressibility condition."""
    from sfepy.base.base import nm, output, assert_

    vv = problem.get_variables()
    one = nm.ones( (vv['p'].field.n_nod,), dtype = nm.float64 )
    vv['p'].set_data(one)
    zero = problem.evaluate('dw_stokes.i1.Omega( u, p )', p=one, u=vv['u']())
    output('div( u ) = %.3e' % zero)

    assert_(abs(zero) < 1e-14)

    return out

##
# Functions.
import os.path as op
import sys

sys.path.append(data_dir) # Make installed example work.
import examples.navier_stokes.utils as utils

cinc_name = 'cinc_' + op.splitext(op.basename(filename_mesh))[0]
cinc = getattr(utils, cinc_name)

functions = {
    'cinc0' : (lambda coors, domain=None: cinc(coors, 0),),
    'cinc1' : (lambda coors, domain=None: cinc(coors, 1),),
}

########NEW FILE########
__FILENAME__ = navier_stokes2d
# -*- coding: utf-8 -*-
r"""
Navier-Stokes equations for incompressible fluid flow in 2D.

Find :math:`\ul{u}`, :math:`p` such that:

.. math::
    \int_{\Omega} \nu\ \nabla \ul{v} : \nabla \ul{u}
    + \int_{\Omega} ((\ul{u} \cdot \nabla) \ul{u}) \cdot \ul{v}
    - \int_{\Omega} p\ \nabla \cdot \ul{v}
    = 0
    \;, \quad \forall \ul{v} \;,

    \int_{\Omega} q\ \nabla \cdot \ul{u}
    = 0
    \;, \quad \forall q \;.
"""
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/2d/rectangle_fine_quad.mesh'

regions = {
    'Omega' : 'all',
    'Surface' : ('vertices of surface', 'facet'),
    'Right' : ('vertices in (x > 4.999)', 'facet'),
    'Bottom' : ('vertices in (y < -9.999)', 'facet'),
    'Top' : ('vertices in (y > 9.999)', 'facet'),
    'Walls' : ('r.Top +v r.Right +v r.Bottom', 'facet'),
    'Driven' : ('r.Surface -v r.Walls', 'facet'),
}

materials = {
    'fluid' : ({'viscosity' : 1.00e-2},),
}

fields = {
    'velocity': ('real', 'vector', 'Omega', 2),
    'pressure': ('real', 'scalar', 'Omega', 1),
}

variables = {
    'u' : ('unknown field', 'velocity', 0),
    'v' : ('test field', 'velocity', 'u'),
    'p' : ('unknown field', 'pressure', 1),
    'q' : ('test field', 'pressure', 'p'),
}

ebcs = {
    'Walls' : ('Walls', {'u.all' : 0.0}),
    'Driven' : ('Driven', {'u.1' : 0.1, 'u.0' : 0.0}),
}

equations = {
    'balance' :
    """+ dw_div_grad.5.Omega(fluid.viscosity, v, u)
       + dw_convect.5.Omega(v, u)
       - dw_stokes.5.Omega(v, p) = 0""",

    'incompressibility' :
    """dw_stokes.5.Omega(u, q) = 0""",
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton', {
        'i_max'      : 15,
        'eps_a'      : 1e-10,
        'eps_r'      : 1.0,
        'problem'   : 'nonlinear'
    }),
}

########NEW FILE########
__FILENAME__ = stabilized_navier_stokes
r"""
Stabilized Navier-Stokes problem with grad-div, SUPG and PSPG stabilization
solved by a custom Oseen solver.

The stabilization terms are described in [1].

[1] G. Matthies and G. Lube. On streamline-diffusion methods of inf-sup stable
discretisations of the generalised Oseen problem. Number 2007-02 in Preprint
Series of Institut fuer Numerische und Angewandte Mathematik,
Georg-August-Universitaet Goettingen, 2007.

Find :math:`\ul{u}`, :math:`p` such that:

.. math::
    \begin{array}{l}
    \int_{\Omega} \nu\ \nabla \ul{v} : \nabla \ul{u}
    \int_{\Omega} ((\ul{b} \cdot \nabla) \ul{u}) \cdot \ul{v}
    - \int_{\Omega} p\ \nabla \cdot \ul{v} \\
    + \gamma \int_{\Omega} (\nabla\cdot\ul{u}) \cdot (\nabla\cdot\ul{v}) \\
    + \sum_{K \in \Ical_h}\int_{T_K} \delta_K\ ((\ul{b} \cdot \nabla)
      \ul{u})\cdot ((\ul{b} \cdot \nabla) \ul{v}) \\
    + \sum_{K \in \Ical_h}\int_{T_K} \delta_K\ \nabla p\cdot ((\ul{b} \cdot
      \nabla) \ul{v})
    = 0
    \;, \quad \forall \ul{v} \;,
    \end{array}

    \begin{array}{l}
    \int_{\Omega} q\ \nabla \cdot \ul{u} \\
    + \sum_{K \in \Ical_h}\int_{T_K} \tau_K\ ((\ul{b} \cdot \nabla) \ul{u})
      \cdot \nabla q \\
    + \sum_{K \in \Ical_h}\int_{T_K} \tau_K\ \nabla p \cdot \nabla q
    = 0
    \;, \quad \forall q \;.
    \end{array}
"""
from sfepy.solvers.oseen import StabilizationFunction
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/elbow2.mesh'

options = {
    'solution' : 'steady',
    'nls' : 'oseen',
    'ls' : 'ls',
}

regions = {
    'Omega' : 'all',
    'Walls' : ('vertices of surface -v (r.Outlet +v r.Inlet)', 'facet'),
    'Inlet' : ('vertices by cinc0', 'facet'),
    'Outlet' : ('vertices by cinc1', 'facet'),
}

fields = {
    'velocity' : ('real', 3, 'Omega', 1),
    'pressure' : ('real', 1, 'Omega', 1),
}

variables = {
    'u'   : ('unknown field',   'velocity', 0),
    'v'   : ('test field',      'velocity', 'u'),
    'b'   : ('parameter field', 'velocity', 'u'),
    'p'   : ('unknown field',   'pressure', 1),
    'q'   : ('test field',      'pressure', 'p'),
}

ebcs = {
    'Walls_velocity' : ('Walls', {'u.all' : 0.0}),
    'Inlet_velocity' : ('Inlet', {'u.1' : 1.0, 'u.[0,2]' : 0.0}),
}

materials = {
    'fluid' : ({'viscosity' : 1.25e-5,
                'density' : 1e0},),
    'stabil' : 'stabil',
}

integrals = {
    'i1' : 2,
    'i2' : 3,
}

##
# Stationary Navier-Stokes equations with grad-div, SUPG and PSPG stabilization.
equations = {
    'balance' :
    """  dw_div_grad.i2.Omega( fluid.viscosity, v, u )
       + dw_lin_convect.i2.Omega( v, b, u )
       - dw_stokes.i1.Omega( v, p )
       + dw_st_grad_div.i1.Omega( stabil.gamma, v, u )
       + dw_st_supg_c.i1.Omega( stabil.delta, v, b, u )
       + dw_st_supg_p.i1.Omega( stabil.delta, v, b, p )
       = 0""",
    'incompressibility' :
    """  dw_stokes.i1.Omega( u, q )
       + dw_st_pspg_c.i1.Omega( stabil.tau, q, b, u )
       + dw_st_pspg_p.i1.Omega( stabil.tau, q, p )
       = 0""",
}

solver_1 = {
    'name' : 'oseen',
    'kind' : 'nls.oseen',

    'needs_problem_instance' : True,
    'stabil_mat' : 'stabil',

    'adimensionalize' : False,
    'check_navier_stokes_rezidual' : False,

    'i_max'      : 10,
    'eps_a'      : 1e-8,
    'eps_r'      : 1.0,
    'macheps'    : 1e-16,
    'lin_red'    : 1e-2, # Linear system error < (eps_a * lin_red).

    # Uncomment the following to get a convergence log.
    ## 'log'        : {'text' : 'oseen_log.txt',
    ##                 'plot' : 'oseen_log.png'},
}

solver_2 = {
    'name' : 'ls',
    'kind' : 'ls.scipy_direct',
}

##
# Functions.
import os.path as op
import sys

sys.path.append(data_dir) # Make installed example work.
import examples.navier_stokes.utils as utils

cinc_name = 'cinc_' + op.splitext(op.basename(filename_mesh))[0]
cinc = getattr(utils, cinc_name)

name_map = {'p' : 'p', 'q' : 'q', 'u' : 'u', 'b' : 'b', 'v' : 'v',
            'fluid' : 'fluid', 'omega' : 'omega', 'i1' : 'i1', 'i2' : 'i2',
            'viscosity' : 'viscosity', 'velocity' : 'velocity',
            'gamma' : 'gamma', 'delta' : 'delta', 'tau' : 'tau'}

functions = {
    'cinc0' : (lambda coors, domain=None: cinc(coors, 0),),
    'cinc1' : (lambda coors, domain=None: cinc(coors, 1),),
    'stabil' : (StabilizationFunction(name_map),),
}

########NEW FILE########
__FILENAME__ = stokes
r"""
Stokes equations for incompressible fluid flow.

This example demonstrates fields defined on subdomains as well as use of
periodic boundary conditions.

Find :math:`\ul{u}`, :math:`p` such that:

.. math::
    \int_{Y_1 \cup Y_2} \nu\ \nabla \ul{v} : \nabla \ul{u}
    - \int_{Y_1 \cup Y_2} p\ \nabla \cdot \ul{v}
    = 0
    \;, \quad \forall \ul{v} \;,

    \int_{Y_1 \cup Y_2} q\ \nabla \cdot \ul{u}
    = 0
    \;, \quad \forall q \;.
"""
from sfepy import data_dir
from sfepy.discrete.fem.periodic import match_y_line

filename_mesh = data_dir + '/meshes/2d/special/channels_symm944t.mesh'

if filename_mesh.find( 'symm' ):
    region_1 = {
        'name' : 'Y1',
        'select' : """cells of group 3""",
    }
    region_2 = {
        'name' : 'Y2',
        'select' : """cells of group 4 +c cells of group 6
                      +c cells of group 8""",
    }
    region_4 = {
        'name' : 'Y1Y2',
        'select' : """r.Y1 +c r.Y2""",
    }
    region_5 = {
        'name' : 'Walls',
        'select' : """r.EBCGamma1 +v r.EBCGamma2""",
        'kind' : 'facet',
    }
    region_310 = {
        'name' : 'EBCGamma1',
        'select' : """(cells of group 1 *v cells of group 3)
                      +v
                      (cells of group 2 *v cells of group 3)
                      """,
        'kind' : 'facet',
    }
    region_320 = {
        'name' : 'EBCGamma2',
        'select' : """(cells of group 5 *v cells of group 4)
                      +v
                      (cells of group 1 *v cells of group 4)
                      +v
                      (cells of group 7 *v cells of group 6)
                      +v
                      (cells of group 2 *v cells of group 6)
                      +v
                      (cells of group 9 *v cells of group 8)
                      +v
                      (cells of group 2 *v cells of group 8)
                      """,
        'kind' : 'facet',
    }


w2 = 0.499
# Sides.
region_20 = {
    'name' : 'Left',
    'select' : 'vertices in (x < %.3f)' % -w2,
    'kind' : 'facet',
}
region_21 = {
    'name' : 'Right',
    'select' : 'vertices in (x > %.3f)' % w2,
    'kind' : 'facet',
}
region_22 = {
    'name' : 'Bottom',
    'select' : 'vertices in (y < %.3f)' % -w2,
    'kind' : 'facet',
}
region_23 = {
    'name' : 'Top',
    'select' : 'vertices in (y > %.3f)' % w2,
    'kind' : 'facet',
}

field_1 = {
    'name' : '2_velocity',
    'dtype' : 'real',
    'shape' : (2,),
    'region' : 'Y1Y2',
    'approx_order' : 2,
}

field_2 = {
    'name' : 'pressure',
    'dtype' : 'real',
    'shape' : (1,),
    'region' : 'Y1Y2',
    'approx_order' : 1,
}

variable_1 = {
    'name' : 'u',
    'kind' : 'unknown field',
    'field' : '2_velocity',
    'order' : 0,
}
variable_2 = {
    'name' : 'v',
    'kind' : 'test field',
    'field' : '2_velocity',
    'dual' : 'u',
}
variable_3 = {
    'name' : 'p',
    'kind' : 'unknown field',
    'field' : 'pressure',
    'order' : 1,
}
variable_4 = {
    'name' : 'q',
    'kind' : 'test field',
    'field' : 'pressure',
    'dual' : 'p',
}

integral_1 = {
    'name' : 'i',
    'order' : 2,
}

equations = {
    'balance' :
    """dw_div_grad.i.Y1Y2( fluid.viscosity, v, u )
     - dw_stokes.i.Y1Y2( v, p ) = 0""",
    'incompressibility' :
    """dw_stokes.i.Y1Y2( u, q ) = 0""",
}

material_1 = {
    'name' : 'fluid',
    'values' : {
        'viscosity' : 1.0,
        'density' : 1e0,
    },
}

ebc_1 = {
    'name' : 'walls',
    'region' : 'Walls',
    'dofs' : {'u.all' : 0.0},
}
ebc_2 = {
    'name' : 'top_velocity',
    'region' : 'Top',
    'dofs' : {'u.1' : -1.0, 'u.0' : 0.0},
}
ebc_10 = {
    'name' : 'bottom_pressure',
    'region' : 'Bottom',
    'dofs' : {'p.0' : 0.0},
}

epbc_1 = {
    'name' : 'u_rl',
    'region' : ['Left', 'Right'],
    'dofs' : {'u.all' : 'u.all', 'p.0' : 'p.0'},
    'match' : 'match_y_line',
}

functions = {
    'match_y_line' : (match_y_line,),
}

solver_0 = {
    'name' : 'ls',
    'kind' : 'ls.scipy_direct',
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 2,
    'eps_a'      : 1e-8,
    'eps_r'      : 1e-2,
    'macheps'   : 1e-16,
    'lin_red'    : 1e-2, # Linear system error < (eps_a * lin_red).
    'ls_red'     : 0.1,
    'ls_red_warp' : 0.001,
    'ls_on'      : 1.1,
    'ls_min'     : 1e-5,
    'check'     : 0,
    'delta'     : 1e-6,
    'problem'   : 'nonlinear', # 'nonlinear' or 'linear' (ignore i_max)
}

save_format = 'hdf5' # 'hdf5' or 'vtk'

########NEW FILE########
__FILENAME__ = stokes_slip_bc
r"""
Incompressible Stokes flow with Navier (slip) boundary conditions, flow driven
by a moving wall and a small diffusion for stabilization.

This example demonstrates the use of `no-penetration` boundary conditions as
well as `edge direction` boundary conditions together with Navier or slip
boundary conditions.

Find :math:`\ul{u}`, :math:`p` such that:

.. math::
    \int_{\Omega} \nu\ \nabla \ul{v} : \nabla \ul{u}
    - \int_{\Omega} p\ \nabla \cdot \ul{v}
    + \int_{\Gamma_1} \beta \ul{v} \cdot (\ul{u} - \ul{u}_d)
    + \int_{\Gamma_2} \beta \ul{v} \cdot \ul{u}
    = 0
    \;, \quad \forall \ul{v} \;,

    \int_{\Omega} \mu \nabla q \cdot \nabla p
    + \int_{\Omega} q\ \nabla \cdot \ul{u}
    = 0
    \;, \quad \forall q \;,

where :math:`\nu` is the fluid viscosity, :math:`\beta` is the slip
coefficient, :math:`\mu` is the (small) numerical diffusion coefficient,
:math:`\Gamma_1` is the top wall that moves with the given driving velocity
:math:`\ul{u}_d` and :math:`\Gamma_2` are the remaining walls. The Navier
conditions are in effect on both :math:`\Gamma_1`, :math:`\Gamma_2` and are
expressed by the corresponding integrals in the equations above.

The `no-penetration` boundary conditions are applied on :math:`\Gamma_1`,
:math:`\Gamma_2`, except the vertices of the block edges, where the `edge
direction` boundary conditions are applied. Optionally, Dirichlet boundary
conditions can be applied on the inlet, see the code below.

The mesh is created by ``gen_block_mesh()`` function - try different mesh
dimensions and resolutions below. For large meshes use the ``'ls_i'`` linear
solver - PETSc + petsc4py is needed in that case.
"""
import numpy as nm

from sfepy.discrete.fem.meshio import UserMeshIO
from sfepy.mesh.mesh_generators import gen_block_mesh
from sfepy.homogenization.utils import define_box_regions

# Mesh dimensions.
dims = nm.array([3, 1, 0.5])

# Mesh resolution: increase to improve accuracy.
shape = [11, 15, 15]

def mesh_hook(mesh, mode):
    """
    Generate the block mesh.
    """
    if mode == 'read':
        mesh = gen_block_mesh(dims, shape, [0, 0, 0], name='user_block',
                              verbose=False)
        return mesh

    elif mode == 'write':
        pass

filename_mesh = UserMeshIO(mesh_hook)

regions = define_box_regions(3, 0.5 * dims)
regions.update({
    'Omega' : 'all',
    'Edges_v' : ("""(r.Near *v r.Bottom) +v
                    (r.Bottom *v r.Far) +v
                    (r.Far *v r.Top) +v
                    (r.Top *v r.Near)""", 'edge'),
    'Gamma1_f' : ('copy r.Top', 'face'),
    'Gamma2_f' : ('r.Near +v r.Bottom +v r.Far', 'face'),
    'Gamma_f' : ('r.Gamma1_f +v r.Gamma2_f', 'face'),
    'Gamma_v' : ('r.Gamma_f -v r.Edges_v', 'face'),
    'Inlet_f' : ('r.Left -v r.Gamma_f', 'face'),
})

fields = {
    'velocity' : ('real', 3, 'Omega', 1),
    'pressure' : ('real', 1, 'Omega', 1),
}

def get_u_d(ts, coors, region=None):
    """
    Given stator velocity.
    """
    out = nm.zeros_like(coors)
    out[:] = [1.0, 1.0, 0.0]

    return out

functions = {
    'get_u_d' : (get_u_d,),
}

variables = {
    'u' : ('unknown field', 'velocity', 0),
    'v' : ('test field',    'velocity', 'u'),
    'u_d' : ('parameter field', 'velocity',
             {'setter' : 'get_u_d'}),
    'p' : ('unknown field', 'pressure', 1),
    'q' : ('test field',    'pressure', 'p'),
}

# Try setting the inlet velocity by un-commenting the ebcs.
ebcs = {
    ## 'inlet' : ('Inlet_f', {'u.0' : 1.0, 'u.[1, 2]' : 0.0}),
}

lcbcs = {
    'walls' : ('Gamma_v', {'u.all' : 'no_penetration'},
               'normals_Gamma.vtk'),
    'edges' : ('Edges_v', {'u.all' : 'edge_direction'},
               'edges_Edges.vtk'),
}

materials = {
    'm' : ({
        'nu' : 1e-3,
        'beta' : 1e-2,
        'mu' : 1e-10,
    },),
}

equations = {
    'balance' :
    """dw_div_grad.5.Omega(m.nu, v, u)
     - dw_stokes.5.Omega(v, p)
     + dw_surface_dot.5.Gamma1_f(m.beta, v, u)
     + dw_surface_dot.5.Gamma2_f(m.beta, v, u)
     =
     + dw_surface_dot.5.Gamma1_f(m.beta, v, u_d)""",
    'incompressibility' :
    """dw_laplace.5.Omega(m.mu, q, p)
     + dw_stokes.5.Omega(u, q) = 0""",
}

solvers = {
    'ls_d' : ('ls.scipy_direct', {}),
    ## 'ls_i' : ('ls.petsc', {
    ##     'method' : 'bcgsl', # ksp_type
    ##     'precond' : 'ilu', # pc_type
    ##     'eps_a' : 0.0, # abstol
    ##     'eps_r' : 1e-12, # rtol
    ##     'eps_d' : 1e10, # Divergence tolerance.
    ##     'i_max' : 2500, # maxits
    ## }),
    'newton' : ('nls.newton', {
        'i_max' : 1,
        'eps_a'      : 1e-10,
    }),
}

options = {
    'nls' : 'newton',
    'ls' : 'ls_d',
}

########NEW FILE########
__FILENAME__ = utils
##
# Functions.
import numpy as nm

from sfepy.linalg import get_coors_in_tube

# last revision: 01.08.2007
def cinc_cylinder(coors, mode):
    axis = nm.array([1, 0, 0], nm.float64)
    if mode == 0: # In
        centre = nm.array([-0.00001, 0.0, 0.0], nm.float64)
        radius = 0.019
        length = 0.00002
    elif mode == 1: # Out
        centre = nm.array([0.09999, 0.0, 0.0], nm.float64)
        radius = 0.019
        length = 0.00002
    else:
        centre = nm.array([0.05, 0.0, 0.0], nm.float64)
        radius = 0.012
        length = 0.04

    return get_coors_in_tube(coors, centre, axis, -1.0, radius, length)

def cinc_elbow2(coors, mode):
    if mode == 0: # In
        centre = nm.array([0.0, -0.00001, 0.0], nm.float64)
    else: # Out
        centre = nm.array([0.2, -0.00001, 0.0], nm.float64)
    
    axis = nm.array([0, 1, 0], nm.float64)
    radius = 0.029
    length = 0.00002

    return get_coors_in_tube(coors, centre, axis, -1.0, radius, length)

########NEW FILE########
__FILENAME__ = band_gaps
"""
Acoustic band gaps in a strongly heterogeneous elastic body, detected using
homogenization techniques.

A reference periodic cell contains two domains: the stiff matrix :math:`Y_m`
and the soft (but heavy) inclusion :math:`Y_c`.
"""
from sfepy import data_dir
from sfepy.base.base import Struct
from sfepy.base.ioutils import InDir
from sfepy.homogenization.coefficients import Coefficients

from band_gaps_conf import BandGapsConf, get_pars, clip_sqrt, normalize

clip_sqrt, normalize # Make pyflakes happy...

incwd = InDir(__file__)

filename = data_dir + '/meshes/2d/special/circle_in_square.mesh'

output_dir = incwd('output/band_gaps')

# aluminium, in 1e+10 Pa
D_m = get_pars(2, 5.898, 2.681)
density_m = 0.2799 # in 1e4 kg/m3

# epoxy, in 1e+10 Pa
D_c = get_pars(2, 0.1798, 0.148)
density_c = 0.1142 # in 1e4 kg/m3

mat_pars = Coefficients(D_m=D_m, density_m=density_m,
                        D_c=D_c, density_c=density_c)

region_selects = Struct(matrix='cells of group 1',
                        inclusion='cells of group 2')

corrs_save_names = {'evp' : 'evp', 'corrs_rs' : 'corrs_rs'}

options = {
    'plot_transform_angle' : None,
    'plot_transform_wave' : ('clip_sqrt', (0, 30)),
    'plot_transform' : ('normalize', (-2, 2)),

    'fig_name' : 'band_gaps',
    'fig_name_angle' : 'band_gaps_angle',
    'fig_name_wave' : 'band_gaps_wave',
    'fig_suffix' : '.pdf',

    'coefs_filename' : 'coefs.txt',

    'incident_wave_dir' : [1.0, 1.0],

    'plot_options' : {
        'show' : True,
        'legend' : True,
    },
    'plot_labels' : {
        'band_gaps' : {
            'resonance' : r'$\lambda^r$',
            'masked' : r'masked $\lambda^r$',
            'eig_min' : r'min eig($M$)',
            'eig_max' : r'max eig($M$)',
            'x_axis' : r'$\sqrt{\lambda}$, $\omega$',
            'y_axis' : r'eigenvalues of mass matrix $M$',
        },
    },
    'plot_rsc' : {
        'params' : {'axes.labelsize': 'x-large',
                    'text.fontsize': 'large',
                    'legend.fontsize': 'large',
                    'legend.loc': 1,
                    'xtick.labelsize': 'large',
                    'ytick.labelsize': 'large',
                    'text.usetex': True},
    },
}

evp_options = {
    'eigensolver' : 'eig.sgscipy',
    'save_eig_vectors' : (12, 0),
    'scale_epsilon' : 1.0,
    'elasticity_contrast' : 1.0,
}

eigenmomenta_options = {
    # eigenmomentum threshold,
    'threshold' : 1e-2,
    # eigenmomentum threshold is relative w.r.t. largest one,
    'threshold_is_relative' : True,
}

band_gaps_options = {
    'eig_range' : (0, 30), # -> freq_range
                           # = sqrt(eigs[slice(*eig_range)][[0, -1]])
    'freq_margins' : (10, 10), # % of freq_range
    'freq_eps' : 1e-12, # frequency
    'zezo_eps' : 1e-12, # zero finding
    'freq_step' : 0.0001, # % of freq_range

    'log_save_name' : 'band_gaps.log',
}

conf = BandGapsConf(filename, 1, region_selects, mat_pars, options,
                    evp_options, eigenmomenta_options, band_gaps_options,
                    corrs_save_names=corrs_save_names, incwd=incwd,
                    output_dir=output_dir)

define = lambda: conf.conf.to_dict()

########NEW FILE########
__FILENAME__ = band_gaps_conf
"""
Configuration classes for acoustic band gaps in a strongly heterogeneous
elastic body.
"""
import numpy as nm

from sfepy.base.base import get_default, import_file, Struct
from sfepy.base.conf import ProblemConf
from sfepy.discrete.fem import MeshIO
import sfepy.discrete.fem.periodic as per
from sfepy.mechanics.matcoefs import stiffness_from_lame, TransformToPlane
from sfepy.homogenization.utils import define_box_regions, get_lattice_volume
import sfepy.homogenization.coefs_base as cb
import sfepy.homogenization.coefs_phononic as cp

def get_pars(dim, lam, mu):
    c = stiffness_from_lame(3, lam, mu)
    if dim == 2:
        tr = TransformToPlane()
        try:
            c = tr.tensor_plane_stress(c3=c)
        except:
            sym = (dim + 1) * dim / 2
            c = nm.zeros((sym, sym), dtype=nm.float64)

    return c

def set_coef_d(variables, ir, ic, mode, pis, corrs_rs):
    mode2var = {'row' : 'u1_m', 'col' : 'u2_m'}

    val = pis.states[ir, ic]['u_m'] + corrs_rs.states[ir, ic]['u_m']

    variables[mode2var[mode]].set_data(val)

class BandGapsConf(Struct):
    """
    Configuration class for acoustic band gaps in a strongly heterogeneous
    elastic body.
    """

    def __init__(self, filename, approx, region_selects, mat_pars, options,
                 evp_options, eigenmomenta_options, band_gaps_options,
                 coefs_save_name='coefs',
                 corrs_save_names=None,
                 incwd=None,
                 output_dir=None, **kwargs):
        Struct.__init__(self, approx=approx, region_selects=region_selects,
                        mat_pars=mat_pars, options=options,
                        evp_options=evp_options,
                        eigenmomenta_options=eigenmomenta_options,
                        band_gaps_options=band_gaps_options,
                        **kwargs)
        self.incwd = get_default(incwd, lambda x: x)

        self.conf = Struct()
        self.conf.filename_mesh = self.incwd(filename)

        output_dir = get_default(output_dir, self.incwd('output'))

        default = {'evp' : 'evp', 'corrs_rs' : 'corrs_rs'}
        self.corrs_save_names = get_default(corrs_save_names,
                                            default)

        io = MeshIO.any_from_filename(self.conf.filename_mesh)
        self.bbox, self.dim = io.read_bounding_box(ret_dim=True)
        rpc_axes = nm.eye(self.dim, dtype=nm.float64) \
                   * (self.bbox[1] - self.bbox[0])

        self.conf.options = options
        self.conf.options.update({
            'output_dir' : output_dir,

            'volume' : {
                'value' : get_lattice_volume(rpc_axes),
            },

            'coefs' : 'coefs',
            'requirements' : 'requirements',

            'coefs_filename' : coefs_save_name,
        })

        self.conf.mat_pars = mat_pars

        self.conf.solvers = self.define_solvers()
        self.conf.regions = self.define_regions()
        self.conf.materials = self.define_materials()
        self.conf.fields = self.define_fields()
        self.conf.variables = self.define_variables()
        (self.conf.ebcs, self.conf.epbcs,
         self.conf.lcbcs, self.all_periodic) = self.define_bcs()
        self.conf.functions = self.define_functions()
        self.conf.integrals = self.define_integrals()

        self.equations, self.expr_coefs = self.define_equations()
        self.conf.coefs = self.define_coefs()
        self.conf.requirements = self.define_requirements()

    def __call__(self):
        return ProblemConf.from_dict(self.conf.__dict__,
                                     import_file(__file__))

    def define_solvers(self):
        solvers = {
            'ls_d' : ('ls.umfpack', {}),
            'ls_i' : ('ls.scipy_iterative', {
                'method' : 'cg',
                'i_max'      : 1000,
                'eps_a'      : 1e-12,
            }),
            'newton' : ('nls.newton', {
                'i_max' : 1,
                'eps_a' : 1e-4,
                'problem' : 'nonlinear',
            }),
        }

        return solvers

    def define_regions(self):
        regions = {
            'Y' : 'all',
            'Y_m' : self.region_selects.matrix,
            'Y_c' : self.region_selects.inclusion,
            'Gamma_mc': ('r.Y_m *v r.Y_c', 'facet'),
        }

        regions.update(define_box_regions(self.dim,
                                          self.bbox[0], self.bbox[1], 1e-5))

        return regions

    def define_materials(self):
        materials = {
            'm' : ({
                'D_m' : self.mat_pars.D_m,
                'density_m' : self.mat_pars.density_m,
                'D_c' : self.mat_pars.D_c,
                'density_c' : self.mat_pars.density_c,
            }, None, None, {'special_constant' : True}),
        }
        return materials

    def define_fields(self):
        fields = {
            'vector_Y_m' : ('real', self.dim, 'Y_m', self.approx),
            'vector_Y_c' : ('real', self.dim, 'Y_c', self.approx),

            'scalar_Y' : ('real', 1, 'Y', 1),
        }
        return fields

    def define_variables(self):
        variables = {
            'u_m' : ('unknown field', 'vector_Y_m'),
            'v_m' : ('test field', 'vector_Y_m', 'u_m'),
            'Pi'    : ('parameter field', 'vector_Y_m', '(set-to-None)'),
            'u1_m'   : ('parameter field', 'vector_Y_m', '(set-to-None)'),
            'u2_m'   : ('parameter field', 'vector_Y_m', '(set-to-None)'),

            'u_c' : ('unknown field', 'vector_Y_c'),
            'v_c' : ('test field', 'vector_Y_c', 'u_c'),

            'aux'   : ('parameter field', 'scalar_Y', '(set-to-None)'),
        }
        return variables

    def define_bcs(self):
        ebcs = {
            'fixed_corners' : ('Corners', {'u_m.all' : 0.0}),
            'fixed_gamma_mc' : ('Gamma_mc', {'u_c.all' : 0.0}),
        }

        epbcs = {}
        all_periodic = []
        for vn in ['u_m']:
            val = {'%s.all' % vn : '%s.all' % vn}

            epbcs.update({
                'periodic_%s_x' % vn : (['Left', 'Right'], val,
                                        'match_y_line'),
                'periodic_%s_y' % vn : (['Top', 'Bottom'], val,
                                        'match_x_line'),
            })
            all_periodic.extend(['periodic_%s_x' % vn, 'periodic_%s_y' % vn])

        lcbcs = {}

        return ebcs, epbcs, lcbcs, all_periodic

    def define_functions(self):
        functions = {
            'match_x_line' : (per.match_x_line,),
            'match_y_line' : (per.match_y_line,),
        }

        return functions

    def define_integrals(self):
        integrals = {
            'i' : 2,
        }

        return integrals

    def define_equations(self):
        equations = {}
        equations['corrs_rs'] = {
            'balance_of_forces' :
            """dw_lin_elastic.i.Y_m( m.D_m, v_m, u_m )
             = - dw_lin_elastic.i.Y_m( m.D_m, v_m, Pi )""",
        }
        equations['evp'] = {
            'lhs' : """dw_lin_elastic.i.Y_c( m.D_c, v_c, u_c )""",
            'rhs' : """dw_volume_dot.i.Y_c( m.density_c, v_c, u_c )""",
        }

        expr_coefs = {
            'D' : """dw_lin_elastic.i.Y_m( m.D_m, u1_m, u2_m )""",
            'VF' : """d_volume.i.%s(aux)""",
            'ema' : """ev_volume_integrate.i.Y_c( m.density_c, u_c )""",
        }

        return equations, expr_coefs

    def define_coefs(self):
        from copy import copy

        ema_options = copy(self.eigenmomenta_options)
        ema_options.update({'var_name' : 'u_c'})

        coefs = {
            # Basic.
            'VF' : {
                'regions' : ['Y_m', 'Y_c'],
                'expression' : self.expr_coefs['VF'],
                'class' : cb.VolumeFractions,
            },
            'dv_info' : {
                'requires' : ['c.VF'],
                'region_to_material' : {'Y_m' : ('m', 'density_m'),
                                        'Y_c' : ('m', 'density_c'),},
                'class' : cp.DensityVolumeInfo,
            },

            'eigenmomenta' : {
                'requires' : ['evp', 'c.dv_info'],
                'expression' : self.expr_coefs['ema'],
                'options' : ema_options,
                'class' : cp.Eigenmomenta,
            },
            'M' : {
                'requires' : ['evp', 'c.dv_info', 'c.eigenmomenta'],
                'class' : cp.AcousticMassTensor,
            },
            'band_gaps' : {
                'requires' : ['evp', 'c.eigenmomenta', 'c.M'],
                'options' : self.band_gaps_options,
                'class' : cp.BandGaps,
            },

            # Dispersion.
            'D' : {
                'requires' : ['pis', 'corrs_rs'],
                'expression' : self.expr_coefs['D'],
                'set_variables' : set_coef_d,
                'class' : cb.CoefSymSym,
            },
            'Gamma' : {
                'requires' : ['c.D'],
                'options' : {
                    'mode' : 'simple',
                    'incident_wave_dir' : None,
                },
                'class' : cp.ChristoffelAcousticTensor,
            },
            'dispersion' : {
                'requires' : ['evp', 'c.eigenmomenta', 'c.M', 'c.Gamma'],
                'options' : self.band_gaps_options,
                'class' : cp.BandGaps,
            },
            'polarization_angles' : {
                'requires' : ['c.dispersion'],
                'options' : {
                    'incident_wave_dir' : None,
                },
                'class' : cp.PolarizationAngles,
            },

            # Phase velocity.
            'phase_velocity' : {
                'requires' : ['c.dv_info', 'c.Gamma'],
                'options' : {
                    'eigensolver' : 'eig.sgscipy',
                },
                'class' : cp.PhaseVelocity,
            },
            'filenames' : {},
        }

        return coefs

    def define_requirements(self):
        requirements = {
            # Basic.
            'evp' : {
                'ebcs' : ['fixed_gamma_mc'],
                'epbcs' : None,
                'equations' : self.equations['evp'],
                'save_name' : self.corrs_save_names['evp'],
                'dump_variables' : ['u_c'],
                'options' : self.evp_options,
                'class' : cp.SimpleEVP,
            },

            # Dispersion.
            'pis' : {
                'variables' : ['u_m'],
                'class' : cb.ShapeDimDim,
            },
            'corrs_rs' : {
                'requires' : ['pis'],
                'ebcs' : ['fixed_corners'],
                'epbcs' : self.all_periodic,
                'equations' : self.equations['corrs_rs'],
                'set_variables' : [('Pi', 'pis', 'u_m')],
                'save_name' : self.corrs_save_names['corrs_rs'],
                'dump_variables' : ['u_m'],
                'is_linear' : True,
                'class' : cb.CorrDimDim,
            },
        }
        return requirements

class BandGapsRigidConf(BandGapsConf):
    """
    Configuration class for acoustic band gaps in a strongly heterogeneous
    elastic body with rigid inclusions.
    """

    def define_regions(self):
        regions = BandGapsConf.define_regions(self)
        regions['Y_cr'] = regions['Y_c']
        regions.update({
            'Y_r' : 'vertices by select_yr',
            'Y_c' : 'r.Y_cr -c r.Y_r',
        })
        return regions

    def define_materials(self):
        materials = BandGapsConf.define_materials(self)
        materials['m'][0].update({
                'D_r' : self.mat_pars.D_r,
                'density_r' : self.mat_pars.density_r,
        })
        return materials

    def define_fields(self):
        fields = {
            'vector_Y_cr' : ('real', self.dim, 'Y_cr', self.approx),

            'scalar_Y' : ('real', 1, 'Y', 1),
        }
        return fields

    def define_variables(self):
        variables = {
            'u' : ('unknown field', 'vector_Y_cr'),
            'v' : ('test field', 'vector_Y_cr', 'u'),

            'aux'   : ('parameter field', 'scalar_Y', '(set-to-None)'),
        }
        return variables

    def define_bcs(self):
        ebcs = {
            'fixed_gamma_mc' : ('Gamma_mc', {'u.all' : 0.0}),
        }
        lcbcs ={
            'rigid' : ('Y_r',{'u.all' : 'rigid'}),
        }

        return ebcs, {}, lcbcs, []

    def define_functions(self):
        functions = BandGapsConf.define_functions(self)
        functions.update({
            'select_yr' : (self.select_yr,),
        })

        return functions

    def define_equations(self):
        equations = {}

        # dw_lin_elastic.i.Y_r( m.D_r, v, u ) should have no effect!
        equations['evp'] = {
            'lhs' : """dw_lin_elastic.i.Y_c( m.D_c, v, u )
                     + dw_lin_elastic.i.Y_r( m.D_r, v, u )""",
            'rhs' : """dw_volume_dot.i.Y_c( m.density_c, v, u )
                     + dw_volume_dot.i.Y_r( m.density_r, v, u )""",
        }

        expr_coefs = {
            'VF' : """d_volume.i.%s(aux)""",
            'ema' : """ev_volume_integrate.i.Y_c( m.density_c, u )
                     + ev_volume_integrate.i.Y_r( m.density_r, u )""",
        }

        return equations, expr_coefs

    def define_coefs(self):
        from copy import copy

        ema_options = copy(self.eigenmomenta_options)
        ema_options.update({'var_name' : 'u'})

        coefs = {
            # Basic.
            'VF' : {
                'regions' : ['Y_m', 'Y_cr', 'Y_c', 'Y_r'],
                'expression' : self.expr_coefs['VF'],
                'class' : cb.VolumeFractions,
            },
            'dv_info' : {
                'requires' : ['c.VF'],
                'region_to_material' : {'Y_m' : ('m', 'density_m'),
                                        'Y_c' : ('m', 'density_c'),
                                        'Y_r' : ('m', 'density_r'),},
                'class' : cp.DensityVolumeInfo,
            },

            'eigenmomenta' : {
                'requires' : ['evp', 'c.dv_info'],
                'expression' : self.expr_coefs['ema'],
                'options' : ema_options,
                'class' : cp.Eigenmomenta,
            },
            'M' : {
                'requires' : ['evp', 'c.dv_info', 'c.eigenmomenta'],
                'class' : cp.AcousticMassTensor,
            },
            'band_gaps' : {
                'requires' : ['evp', 'c.eigenmomenta', 'c.M'],
                'options' : self.band_gaps_options,
                'class' : cp.BandGaps,
            },

            'filenames' : {},
        }

        return coefs

    def define_requirements(self):
        requirements = {
            # Basic.
            'evp' : {
                'ebcs' : ['fixed_gamma_mc'],
                'epbcs' : None,
                'lcbcs' : ['rigid'],
                'equations' : self.equations['evp'],
                'save_name' : self.corrs_save_names['evp'],
                'dump_variables' : ['u'],
                'options' : self.evp_options,
                'class' : cp.SimpleEVP,
            },
        }
        return requirements


def clip(data, plot_range):
    return nm.clip(data, *plot_range)

def clip_sqrt(data, plot_range):
    return nm.clip(nm.sqrt(data), *plot_range)

def normalize(data, plot_range):
    aux = nm.arctan(data)
    return clip(aux, plot_range)


########NEW FILE########
__FILENAME__ = band_gaps_rigid
"""
Acoustic band gaps in a strongly heterogeneous elastic body with a rigid
inclusion, detected using homogenization techniques.

A reference periodic cell contains three domains: the stiff matrix :math:`Y_m`
and the soft inclusion :math:`Y_c` enclosing the rigid heavy sub-inclusion
:math:`Y_r`.
"""
import numpy as nm

from sfepy import data_dir
from sfepy.base.base import Struct
from sfepy.base.ioutils import InDir
from sfepy.discrete.fem import extend_cell_data
from sfepy.linalg import norm_l2_along_axis
from sfepy.homogenization.coefficients import Coefficients

from band_gaps_conf import BandGapsRigidConf, get_pars, normalize

normalize # Make pyflakes happy...

incwd = InDir(__file__)

dim = 2

if dim == 3:
    filename = data_dir + '/meshes/3d/special/cube_sphere.mesh'

else:
    filename = data_dir + '/meshes/2d/special/circle_in_square.mesh'


output_dir = incwd('output/band_gaps_rigid')

# Rigid inclusion diameter.
yr_diameter = 0.125

# aluminium, in 1e+10 Pa
D_m = get_pars(dim, 5.898, 2.681)
density_m = 0.2799 # in 1e4 kg/m3

# epoxy, in 1e+10 Pa
D_c = get_pars(dim, 0.1798, 0.148)
density_c = 0.1142 # in 1e4 kg/m3

# lead, in 1e+10 Pa, does not matter
D_r = get_pars(dim, 4.074, 0.5556)
density_r = 1.1340 # in 1e4 kg/m3

mat_pars = Coefficients(D_m=D_m, density_m=density_m,
                        D_c=D_c, density_c=density_c,
                        D_r=D_r, density_r=density_r)

region_selects = Struct(matrix='cells of group 1',
                        inclusion='cells of group 2')

corrs_save_names = {'evp' : 'evp'}

evp_options = {
    'eigensolver' : 'eig.sgscipy',
    'save_eig_vectors' : (12, 0),
    'scale_epsilon' : 1.0,
    'elasticity_contrast' : 1.0,
}

eigenmomenta_options = {
    # eigenmomentum threshold,
    'threshold' : 1e-1,
    # eigenmomentum threshold is relative w.r.t. largest one,
    'threshold_is_relative' : True,
}

band_gaps_options = {
    'fixed_freq_range' : (0., 35.), # overrides eig_range!

    'freq_eps' : 1e-12, # frequency
    'zezo_eps' : 1e-12, # zero finding
    'freq_step' : 0.01, # % of freq_range

    'log_save_name' : 'band_gaps.log',
}

options = {
    'post_process_hook' : 'post_process',

    'plot_transform' : ('normalize', (-2, 2)),

    'fig_name' : 'band_gaps',
    'fig_suffix' : '.pdf',

    'coefs_filename' : 'coefs.txt',

    'plot_options' : {
        'show' : True, # Show figure.
        'legend' : True, # Show legend.
    },
}

def select_yr_circ(coors, diameter=None):
    r = norm_l2_along_axis(coors)
    out = nm.where(r < diameter)[0]

    if out.shape[0] <= 3:
        raise ValueError('too few nodes selected! (%d)' % out.shape[0])

    return out

def _select_yr_circ(coors, domain=None, diameter=None):
    return select_yr_circ(coors, diameter=yr_diameter)

def post_process(out, problem, mtx_phi):
    var = problem.get_variables()['u']

    for key in out.keys():
        ii = int(key[1:])
        vec = mtx_phi[:,ii].copy()
        var.set_data(vec)

        strain = problem.evaluate('ev_cauchy_strain.i.Y_c(u)', u=var,
                                  verbose=False, mode='el_avg')
        strain = extend_cell_data(strain, problem.domain, 'Y_c')
        out['strain%03d' % ii] = Struct(name='output_data',
                                        mode='cell', data=strain,
                                        dofs=None)
    return out

conf = BandGapsRigidConf(filename, 1, region_selects, mat_pars, options,
                         evp_options, eigenmomenta_options, band_gaps_options,
                         corrs_save_names=corrs_save_names, incwd=incwd,
                         output_dir=output_dir, select_yr=_select_yr_circ)

define = lambda: conf.conf.to_dict()

########NEW FILE########
__FILENAME__ = piezo
r"""
Piezo-elasticity problem - linear elastic material with piezoelectric
effects.

Find :math:`\ul{u}`, :math:`\phi` such that:

.. math::
    - \omega^2 \int_{Y} \rho\ \ul{v} \cdot \ul{u}
    + \int_{Y} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    - \int_{Y_2} g_{kij}\ e_{ij}(\ul{v}) \nabla_k \phi
    = 0
    \;, \quad \forall \ul{v} \;,

    \int_{Y_2} g_{kij}\ e_{ij}(\ul{u}) \nabla_k \psi
    + \int_{Y} K_{ij} \nabla_i \psi \nabla_j \phi
    = 0
    \;, \quad \forall \psi \;,

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl}+\delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;.
"""
import os
import numpy as nm

from sfepy import data_dir
from sfepy.discrete.fem import MeshIO

filename_mesh = data_dir + '/meshes/2d/special/circle_in_square.mesh'
## filename_mesh = data_dir + '/meshes/2d/special/circle_in_square_small.mesh'
## filename_mesh = data_dir + '/meshes/3d/special/cube_sphere.mesh'
## filename_mesh = data_dir + '/meshes/2d/special/cube_cylinder.mesh'

omega = 1
omega_squared = omega**2

conf_dir = os.path.dirname(__file__)
io = MeshIO.any_from_filename(filename_mesh, prefix_dir=conf_dir)
bbox, dim = io.read_bounding_box( ret_dim = True )

geom = {3 : '3_4', 2 : '2_3'}[dim]

x_left, x_right = bbox[:,0]

regions = {
    'Y' : 'all',
    'Y1' : 'cells of group 1',
    'Y2' : 'cells of group 2',
    'Y2_Surface': ('r.Y1 *v r.Y2', 'facet'),
    'Left' : ('vertices in (x < %f)' % (x_left + 1e-3), 'facet'),
    'Right' : ('vertices in (x > %f)' % (x_right - 1e-3), 'facet'),
}

material_2 = {
    'name' : 'inclusion',

    # epoxy
    'function' : 'get_inclusion_pars',
}

def get_inclusion_pars(ts, coor, mode=None, **kwargs):
    """TODO: implement proper 3D -> 2D transformation of constitutive
    matrices."""
    if mode == 'qp':
        n_nod, dim = coor.shape
        sym = (dim + 1) * dim / 2

        dielectric = nm.eye( dim, dtype = nm.float64 )
        # !!!
        coupling = nm.ones( (dim, sym), dtype = nm.float64 )
        #    coupling[0,1] = 0.2

        out = {
            # Lame coefficients in 1e+10 Pa.
            'lam' : 0.1798,
            'mu' : 0.148,
            # dielectric tensor
            'dielectric' : dielectric,
            # piezoelectric coupling
            'coupling' : coupling,
            'density' : 0.1142, # in 1e4 kg/m3
        }

        for key, val in out.iteritems():
            out[key] = nm.tile(val, (coor.shape[0], 1, 1))
        return out

functions = {
    'get_inclusion_pars' : (get_inclusion_pars,),
}

field_0 = {
    'name' : 'displacement',
    'dtype' : nm.float64,
    'shape' : dim,
    'region' : 'Y',
    'approx_order' : 1,
}

field_2 = {
    'name' : 'potential',
    'dtype' : nm.float64,
    'shape' : (1,),
    'region' : 'Y',
    'approx_order' : 1,
}

variables = {
    'u' : ('unknown field', 'displacement', 0),
    'v' : ('test field', 'displacement', 'u'),
    'phi' : ('unknown field', 'potential', 1),
    'psi' : ('test field', 'potential', 'phi'),
}

ebcs = {
    'u1' : ('Left', {'u.all' : 0.0}),
    'u2' : ('Right', {'u.0' : 0.1}),
    'phi' : ('Y2_Surface', {'phi.all' : 0.0}),
}

integral_1 = {
    'name' : 'i',
    'order' : 2,
}

equations = {
    '1' : """- %f * dw_volume_dot.i.Y( inclusion.density, v, u )
             + dw_lin_elastic_iso.i.Y( inclusion.lam, inclusion.mu, v, u )
             - dw_piezo_coupling.i.Y2( inclusion.coupling, v, phi )
           = 0""" % omega_squared,
    '2' : """dw_piezo_coupling.i.Y2( inclusion.coupling, u, psi )
           + dw_diffusion.i.Y( inclusion.dielectric, psi, phi )
           = 0""",
}

##
# Solvers etc.
solver_0 = {
    'name' : 'ls',
    'kind' : 'ls.scipy_direct',
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 1,
    'eps_a'      : 1e-10,
    'eps_r'      : 1.0,
    'macheps'    : 1e-16,
    'lin_red'    : 1e-2, # Linear system error < (eps_a * lin_red).
    'ls_red'     : 0.1,
    'ls_red_warp': 0.001,
    'ls_on'      : 1.1,
    'ls_min'     : 1e-5,
    'check'      : 0,
    'delta'      : 1e-6,
    'problem'    : 'nonlinear', # 'nonlinear' or 'linear' (ignore i_max)
}

########NEW FILE########
__FILENAME__ = boron
from sfepy.linalg import norm_l2_along_axis

from quantum_common import common

def fun_v(ts, coor, mode=None, **kwargs):
    if not mode == 'qp': return

    out = {}
    C = 0.5
    r = norm_l2_along_axis(coor, axis=1)
    V = - C * 5.0 / r

    V.shape = (V.shape[0], 1, 1)
    out['V'] = V
    return out

def define(n_eigs=10, tau=-15):
    l = common(fun_v, n_eigs=n_eigs, tau=tau)
    return l

########NEW FILE########
__FILENAME__ = hydrogen
from sfepy.linalg import norm_l2_along_axis

from quantum_common import common

def fun_v(ts, coor, mode=None, **kwargs):
    if not mode == 'qp': return

    out = {}
    C = 0.5
    r = norm_l2_along_axis(coor, axis=1)

    V = - C * 1.0 / r

    V.shape = (V.shape[0], 1, 1)
    out['V'] = V
    return out

def define(n_eigs=5, tau=-1.0):
    l = common(fun_v, n_eigs=n_eigs, tau=tau)
    return l

########NEW FILE########
__FILENAME__ = oscillator
from sfepy.linalg import norm_l2_along_axis

from quantum_common import common

def fun_v(ts, coor, mode=None, **kwargs):
    if not mode == 'qp': return

    out = {}
    C = 0.5
    val = C * norm_l2_along_axis(coor, axis=1, squared=True)

    val.shape = (val.shape[0], 1, 1)
    out['V'] = val
    return out

def define(n_eigs=20, tau=0.0):
    l = common(fun_v, n_eigs=n_eigs, tau=tau)
    return l

########NEW FILE########
__FILENAME__ = quantum_common
"""
Common code for basic electronic structure examples.

Notes
-----

The same code should work also with a 3D (box) mesh, but a very fine mesh would
be required. Also in the 2D case, finer mesh and/or higher approximation order
means higher accuracy.

Try changing C, F and L parameters in square.geo and regenerate the mesh using
gmsh::

  $ gmsh -2 -format mesh meshes/quantum/square.geo -o meshes/quantum/square.mesh
  $ ./script/convert_mesh.py meshes/quantum/square.mesh meshes/quantum/aux.vtk
  $ ./script/convert_mesh.py meshes/quantum/aux.vtk meshes/quantum/square.mesh

The ``script/convert_mesh.py`` calls make the mesh 2D, as gmsh does not save
planar medit meshes.

Also try changing approximation order ('approx_order') of the field below, as
well as the integral order (should be two times the approximation order).
"""
from sfepy import data_dir

def common(fun_v, n_eigs=5, tau=0.0):
    filename_mesh = data_dir + '/meshes/quantum/square.mesh'

    options = {
        'save_eig_vectors' : None,
        'n_eigs' : n_eigs,
        'eigen_solver' : 'eigen1',
    }

    region_1000 = {
        'name' : 'Omega',
        'select' : 'all',
    }

    region_2 = {
        'name' : 'Surface',
        'select' : 'vertices of surface',
        'kind' : 'facet',
    }

    functions = {
        'fun_v' : (fun_v,),
    }

    material_1 = {
        'name' : 'm',

        'values' : {
            'val' : 0.5,
        },
    }

    material_2 = {
        'name' : 'mat_v',

        'function' : 'fun_v',
    }

    field_0 = {
        'name' : 'field_Psi',
        'dtype' : 'real',
        'shape' : 'scalar',
        'region' : 'Omega',
        'approx_order' : 2,
    }

    integral_1 = {
        'name' : 'i',
        'order' : 4,
    }

    variable_1 = {
        'name' : 'Psi',
        'kind' : 'unknown field',
        'field' : 'field_Psi',
        'order' : 0,
    }
    variable_2 = {
        'name' : 'v',
        'kind' : 'test field',
        'field' : 'field_Psi',
        'dual' : 'Psi',
    }
    variable_3 = {
        'name' : 'V',
        'kind' : 'parameter field',
        'field' : 'field_Psi',
        'like' : 'Psi',
    }

    ebc_1 = {
        'name' : 'ZeroSurface',
        'region' : 'Surface',
        'dofs' : {'Psi.0' : 0.0},
    }

    equations = {
        'lhs' : """  dw_laplace.i.Omega( m.val, v, Psi )
                   + dw_volume_dot.i.Omega( mat_v.V, v, Psi )""",
        'rhs' : """dw_volume_dot.i.Omega( v, Psi )""",
    }

    solver_2 = {
        'name' : 'eigen1',
        'kind' : 'eig.pysparse',

        'tau' : tau,
        'eps_a' : 1e-10,
        'i_max' : 150,
        'method' : 'qmrs',
        'verbosity' : 0,
        'strategy' : 1,
    }

    return locals()

########NEW FILE########
__FILENAME__ = well
from quantum_common import common

def fun_v(ts, coor, mode=None, **kwargs):
    from numpy import zeros_like

    if not mode == 'qp': return

    out = {}
    val = zeros_like(coor[:,0])

    val.shape = (val.shape[0], 1, 1)
    out['V'] = val
    return out

def define(n_eigs=10, tau=0.0):
    l = common(fun_v, n_eigs=n_eigs, tau=tau)
    return l

########NEW FILE########
__FILENAME__ = compare_elastic_materials
"""
Compare various elastic materials w.r.t. uniaxial tension/compression test.

Requires Matplotlib.
"""
from optparse import OptionParser
import sys
sys.path.append( '.' )

import numpy as nm

def define():
    """Define the problem to solve."""

    filename_mesh = 'el3.mesh'

    options = {
        'nls' : 'newton',
        'ls' : 'ls',
        'ts' : 'ts',
        'save_steps' : -1,
    }

    functions = {
        'linear_tension' : (linear_tension,),
        'linear_compression' : (linear_compression,),
        'empty' : (lambda ts, coor, mode, region, ig: None,),
    }

    field_1 = {
        'name' : 'displacement',
        'dtype' : nm.float64,
        'shape' : (3,),
        'region' : 'Omega',
        'approx_order' : 1,
    }

    # Coefficients are chosen so that the tangent stiffness is the same for all
    # material for zero strains.
    # Young modulus = 10 kPa, Poisson's ratio = 0.3
    material_1 = {
        'name' : 'solid',

        'values' : {
            'K'  : 8.333, # bulk modulus
            'mu_nh' : 3.846, # shear modulus of neoHookean term
            'mu_mr' : 1.923, # shear modulus of Mooney-Rivlin term
            'kappa' : 1.923, # second modulus of Mooney-Rivlin term
            'lam' : 5.769, # Lame coefficients for LE term
            'mu_le' : 3.846,
        }
    }

    material_2 = {
        'name' : 'load',
        'function' : 'empty'
    }

    variables = {
        'u' : ('unknown field', 'displacement', 0),
        'v' : ('test field', 'displacement', 'u'),
    }

    regions = {
        'Omega' : 'all',
        'Bottom' : ('vertices in (z < 0.1)', 'facet'),
        'Top' : ('vertices in (z > 2.9)', 'facet'),
    }

    ebcs = {
        'fixb' : ('Bottom', {'u.all' : 0.0}),
        'fixt' : ('Top', {'u.[0,1]' : 0.0}),
    }

    ##
    # Balance of forces.
    integral_1 = {
        'name' : 'i',
        'order' : 1,
    }
    integral_3 = {
        'name' : 'isurf',
        'order' : 2,
    }
    equations = {
        'linear' : """dw_lin_elastic_iso.i.Omega( solid.lam, solid.mu_le, v, u )
                      = dw_surface_ltr.isurf.Top( load.val, v )""",
        'neoHookean' : """dw_tl_he_neohook.i.Omega( solid.mu_nh, v, u )
                        + dw_tl_bulk_penalty.i.Omega( solid.K, v, u )
                        = dw_surface_ltr.isurf.Top( load.val, v )""",
        'Mooney-Rivlin' : """dw_tl_he_neohook.i.Omega( solid.mu_mr, v, u )
                           + dw_tl_he_mooney_rivlin.i.Omega( solid.kappa, v, u )
                           + dw_tl_bulk_penalty.i.Omega( solid.K, v, u )
                           = dw_surface_ltr.isurf.Top( load.val, v )""",
    }

    ##
    # Solvers etc.
    solver_0 = {
        'name' : 'ls',
        'kind' : 'ls.scipy_direct',
    }

    solver_1 = {
        'name' : 'newton',
        'kind' : 'nls.newton',

        'i_max'      : 5,
        'eps_a'      : 1e-10,
        'eps_r'      : 1.0,
        'macheps'    : 1e-16,
        'lin_red'    : 1e-2, # Linear system error < (eps_a * lin_red).
        'ls_red'     : 0.1,
        'ls_red_warp': 0.001,
        'ls_on'      : 1.1,
        'ls_min'     : 1e-5,
        'check'      : 0,
        'delta'      : 1e-6,
        'problem'    : 'nonlinear', # 'nonlinear' or 'linear' (ignore i_max)
    }

    solver_2 = {
        'name' : 'ts',
        'kind' : 'ts.simple',

        't0'    : 0,
        't1'    : 1,
        'dt'    : None,
        'n_step' : 101, # has precedence over dt!
    }

    return locals()

##
# Pressure tractions.
def linear_tension(ts, coor, mode=None, **kwargs):
    if mode == 'qp':
        val = nm.tile(0.1 * ts.step, (coor.shape[0], 1, 1))
        return {'val' : val}

def linear_compression(ts, coor, mode=None, **kwargs):
    if mode == 'qp':
        val = nm.tile(-0.1 * ts.step, (coor.shape[0], 1, 1))
        return {'val' : val}


def store_top_u( displacements ):
    """Function _store() will be called at the end of each loading step. Top
    displacements will be stored into `displacements`."""
    def _store( problem, ts, state ):

        top = problem.domain.regions['Top']
        top_u = problem.get_variables()['u'].get_state_in_region( top )
        displacements.append( nm.mean( top_u[:,-1] ) )

    return _store

def solve_branch(problem, branch_function):
    displacements = {}
    for key, eq in problem.conf.equations.iteritems():
        problem.set_equations( {key : eq} )

        load = problem.get_materials()['load']
        load.set_function(branch_function)

        time_solver = problem.get_time_solver()

        out = []
        time_solver(save_results=False, step_hook=store_top_u(out))
        displacements[key] = nm.array(out, dtype=nm.float64)

    return displacements

usage = """%prog [options]"""
helps = {
    'no_plot' : 'do not show plot window',
}

def main():
    from sfepy.base.conf import ProblemConf, get_standard_keywords
    from sfepy.discrete import Problem
    from sfepy.base.plotutils import plt

    parser = OptionParser(usage=usage, version='%prog')
    parser.add_option('-n', '--no-plot',
                      action="store_true", dest='no_plot',
                      default=False, help=helps['no_plot'])
    options, args = parser.parse_args()

    required, other = get_standard_keywords()
    # Use this file as the input file.
    conf = ProblemConf.from_file( __file__, required, other )

    # Create problem instance, but do not set equations.
    problem = Problem.from_conf(conf, init_equations=False)

    # Solve the problem. Output is ignored, results stored by using the
    # step_hook.
    u_t = solve_branch(problem, linear_tension)
    u_c = solve_branch(problem, linear_compression)

    # Get pressure load by calling linear_*() for each time step.
    ts = problem.get_timestepper()
    load_t = nm.array([linear_tension(ts, nm.array([[0.0]]), 'qp')['val']
                       for aux in ts.iter_from( 0 )],
                      dtype=nm.float64).squeeze()
    load_c = nm.array([linear_compression(ts, nm.array([[0.0]]), 'qp')['val']
                       for aux in ts.iter_from( 0 )],
                      dtype=nm.float64).squeeze()

    # Join the branches.
    displacements = {}
    for key in u_t.keys():
        displacements[key] = nm.r_[u_c[key][::-1], u_t[key]]
    load = nm.r_[load_c[::-1], load_t]


    if plt is None:
        print 'matplotlib cannot be imported, printing raw data!'
        print displacements
        print load
    else:
        legend = []
        for key, val in displacements.iteritems():
            plt.plot( load, val )
            legend.append( key )

        plt.legend( legend, loc = 2 )
        plt.xlabel( 'tension [kPa]' )
        plt.ylabel( 'displacement [mm]' )
        plt.grid( True )

        plt.gcf().savefig( 'pressure_displacement.png' )

        if not options.no_plot:
            plt.show()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = rs_correctors
# c: 05.05.2008, r: 05.05.2008
from optparse import OptionParser
import sys
sys.path.append( '.' )

import numpy as nm

import sfepy.discrete.fem.periodic as per
from sfepy.homogenization.utils import define_box_regions

# c: 05.05.2008, r: 05.05.2008
def define_regions( filename ):
    """Define various subdomain for a given mesh file. This function is called
    below."""
    regions = {}
    dim = 2
    
    regions['Y'] = 'all'

    eog = 'cells of group %d'
    if filename.find( 'osteonT1' ) >= 0:
        mat_ids = [11, 39, 6, 8, 27, 28, 9, 2, 4, 14, 12, 17, 45, 28, 15]
        regions['Ym'] = ' +c '.join( (eog % im) for im in  mat_ids )
        wx = 0.865
        wy = 0.499

    regions['Yc'] = 'r.Y -c r.Ym'

    # Sides and corners.
    regions.update( define_box_regions( 2, (wx, wy) ) )

    return dim, regions, mat_ids

def get_pars(ts, coor, mode=None, term=None, group_indx=None,
             mat_ids = [], **kwargs):
    """Define material parameters:
         $D_ijkl$ (elasticity),
       in a given region."""
    if mode == 'qp':
        dim = coor.shape[1]
        sym = (dim + 1) * dim / 2

        m2i = term.region.domain.mat_ids_to_i_gs
        matrix_igs = [m2i[im] for im in mat_ids]

        out = {}

        # in 1e+10 [Pa]
        lam = 1.7
        mu = 0.3
        o = nm.array( [1.] * dim + [0.] * (sym - dim), dtype = nm.float64 )
        oot = nm.outer( o, o )
        out['D'] = lam * oot + mu * nm.diag( o + 1.0 )

        for key, val in out.iteritems():
            out[key] = nm.tile(val, (coor.shape[0], 1, 1))

        for ig, indx in group_indx.iteritems():
            if ig not in matrix_igs: # channels
                out['D'][indx] *= 1e-1

        return out

##
# Mesh file.
filename_mesh = 'meshes/osteonT1_11.mesh'

##
# Define regions (subdomains, boundaries) - $Y$, $Y_i$, ...
# depending on a mesh used.
dim, regions, mat_ids = define_regions( filename_mesh )

functions = {
    'get_pars' : (lambda ts, coors, **kwargs:
                  get_pars(ts, coors, mat_ids=mat_ids, **kwargs),),
    'match_x_plane' : (per.match_x_plane,),
    'match_y_plane' : (per.match_y_plane,),
    'match_z_plane' : (per.match_z_plane,),
    'match_x_line' : (per.match_x_line,),
    'match_y_line' : (per.match_y_line,),
}

##
# Define fields: 'displacement' in $Y$,
# 'pressure_m' in $Y_m$.
field_1 = {
    'name' : 'displacement',
    'dtype' : nm.float64,
    'shape' : dim,
    'region' : 'Y',
    'approx_order' : 1,
}

##
# Define corrector variables: unknown displaements: uc, test: vc
# displacement-like variables: Pi, Pi1, Pi2
variables = {
    'uc'       : ('unknown field',   'displacement', 0),
    'vc'       : ('test field',      'displacement', 'uc'),
    'Pi'       : ('parameter field', 'displacement', 'uc'),
    'Pi1'      : ('parameter field', 'displacement', None),
    'Pi2'      : ('parameter field', 'displacement', None),
}

##
# Periodic boundary conditions.
if dim == 3:
    epbc_10 = {
        'name' : 'periodic_x',
        'region' : ['Left', 'Right'],
        'dofs' : {'uc.all' : 'uc.all'},
        'match' : 'match_x_plane',
    }
    epbc_11 = {
        'name' : 'periodic_y',
        'region' : ['Near', 'Far'],
        'dofs' : {'uc.all' : 'uc.all'},
        'match' : 'match_y_plane',
    }
    epbc_12 = {
        'name' : 'periodic_z',
        'region' : ['Top', 'Bottom'],
        'dofs' : {'uc.all' : 'uc.all'},
        'match' : 'match_z_plane',
    }
else:
    epbc_10 = {
        'name' : 'periodic_x',
        'region' : ['Left', 'Right'],
        'dofs' : {'uc.all' : 'uc.all'},
        'match' : 'match_y_line',
    }
    epbc_11 = {
        'name' : 'periodic_y',
        'region' : ['Top', 'Bottom'],
        'dofs' : {'uc.all' : 'uc.all'},
        'match' : 'match_x_line',
    }
    
##
# Dirichlet boundary conditions.
ebcs = {
    'fixed_u' : ('Corners', {'uc.all' : 0.0}),
}

##
# Material defining constitutive parameters of the microproblem.
material_1 = {
    'name' : 'm',
    'function' : 'get_pars',
}

##
# Numerical quadratures for volume (i3 - order 3) integral terms.
integral_1 = {
    'name' : 'i3',
    'order' : 3,
}

##
# Homogenized coefficients to compute.
def set_elastic(variables, ir, ic, mode, pis, corrs_rs):
    mode2var = {'row' : 'Pi1', 'col' : 'Pi2'}

    val = pis.states[ir, ic]['uc'] + corrs_rs.states[ir, ic]['uc']

    variables[mode2var[mode]].set_data(val)

coefs = {
    'E' : {
        'requires' : ['pis', 'corrs_rs'],
        'expression' : 'dw_lin_elastic.i3.Y( m.D, Pi1, Pi2 )',
        'set_variables' : set_elastic,
    },
}

all_periodic = ['periodic_%s' % ii for ii in ['x', 'y', 'z'][:dim] ]
requirements = {
    'pis' : {
        'variables' : ['uc'],
    },
    ##
    # Steady state correctors $\bar{\omega}^{rs}$.
    'corrs_rs' : {
        'requires' : ['pis'],
        'save_variables' : ['uc'],
        'ebcs' : ['fixed_u'],
        'epbcs' : all_periodic,
        'equations' : {'eq' : """dw_lin_elastic.i3.Y( m.D, vc, uc )
                             = - dw_lin_elastic.i3.Y( m.D, vc, Pi )"""},
        'set_variables' : [('Pi', 'pis', 'uc')],
        'save_name' : 'corrs_elastic',
        'is_linear' : True,
    },
}

##
# Solvers.
solver_0 = {
    'name' : 'ls',
    'kind' : 'ls.scipy_direct', # Direct solver.
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 2,
    'eps_a'      : 1e-8,
    'eps_r'      : 1e-2,
    'macheps'   : 1e-16,
    'lin_red'    : 1e-2, # Linear system error < (eps_a * lin_red).
    'ls_red'     : 0.1,
    'ls_red_warp' : 0.001,
    'ls_on'      : 0.99999,
    'ls_min'     : 1e-5,
    'check'     : 0,
    'delta'     : 1e-6,
    'problem'   : 'nonlinear', # 'nonlinear' or 'linear' (ignore i_max)
}

############################################
# Mini-application below, computing the homogenized elastic coefficients.
usage = """%prog [options]"""
help = {
    'no_pauses' : 'do not make pauses',
}

##
# c: 05.05.2008, r: 28.11.2008
def main():
    import os
    from sfepy.base.base import spause, output
    from sfepy.base.conf import ProblemConf, get_standard_keywords
    from sfepy.discrete import Problem
    import sfepy.homogenization.coefs_base as cb

    parser = OptionParser(usage=usage, version='%prog')
    parser.add_option('-n', '--no-pauses',
                      action="store_true", dest='no_pauses',
                      default=False, help=help['no_pauses'])
    options, args = parser.parse_args()

    if options.no_pauses:
        def spause(*args):
            output(*args)

    nm.set_printoptions( precision = 3 )

    spause( r""">>>
First, this file will be read in place of an input
(problem description) file.
Press 'q' to quit the example, press any other key to continue...""" )
    required, other = get_standard_keywords()
    required.remove( 'equations' )
    # Use this file as the input file.
    conf = ProblemConf.from_file( __file__, required, other )
    print conf.to_dict().keys()
    spause( r""">>>
...the read input as a dict (keys only for brevity).
['q'/other key to quit/continue...]""" )

    spause( r""">>>
Now the input will be used to create a Problem instance.
['q'/other key to quit/continue...]""" )
    problem = Problem.from_conf(conf, init_equations=False)
    # The homogenization mini-apps need the output_dir.
    output_dir = os.path.join(os.path.split(__file__)[0], 'output')
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    problem.output_dir = output_dir
    print problem
    spause( r""">>>
...the Problem instance.
['q'/other key to quit/continue...]""" )


    spause( r""">>>
The homogenized elastic coefficient $E_{ijkl}$ is expressed
using $\Pi$ operators, computed now. In fact, those operators are permuted
coordinates of the mesh nodes.
['q'/other key to quit/continue...]""" )
    req = conf.requirements['pis']
    mini_app = cb.ShapeDimDim( 'pis', problem, req )
    pis = mini_app()
    print pis
    spause( r""">>>
...the $\Pi$ operators.
['q'/other key to quit/continue...]""" )

    spause( r""">>>
Next, $E_{ijkl}$ needs so called steady state correctors $\bar{\omega}^{rs}$,
computed now.
['q'/other key to quit/continue...]""" )
    req = conf.requirements['corrs_rs']

    save_name = req.get( 'save_name', '' )
    name = os.path.join( output_dir, save_name )

    mini_app = cb.CorrDimDim('steady rs correctors', problem, req)
    mini_app.setup_output(save_format='vtk',
                          file_per_var=False)
    corrs_rs = mini_app( data = {'pis': pis} )
    print corrs_rs
    spause( r""">>>
...the $\bar{\omega}^{rs}$ correctors.
The results are saved in: %s.%s

Try to display them with:

   python postproc.py %s.%s

['q'/other key to quit/continue...]""" % (2 * (name, problem.output_format)) )

    spause( r""">>>
Then the volume of the domain is needed.
['q'/other key to quit/continue...]""" )
    volume = problem.evaluate('d_volume.i3.Y( uc )')
    print volume

    spause( r""">>>
...the volume.
['q'/other key to quit/continue...]""" )

    spause( r""">>>
Finally, $E_{ijkl}$ can be computed.
['q'/other key to quit/continue...]""" )
    mini_app = cb.CoefSymSym('homogenized elastic tensor',
                             problem, conf.coefs['E'])
    c_e = mini_app(volume, data={'pis': pis, 'corrs_rs' : corrs_rs})
    print r""">>>
The homogenized elastic coefficient $E_{ijkl}$, symmetric storage
with rows, columns in 11, 22, 12 ordering:"""
    print c_e
    
if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = linear_elasticity
#!/usr/bin/env python
from optparse import OptionParser
import numpy as nm

import sys
sys.path.append('.')

from sfepy.base.base import IndexedStruct
from sfepy.discrete import (FieldVariable, Material, Integral, Function,
                            Equation, Equations, Problem)
from sfepy.discrete.fem import Mesh, FEDomain, Field
from sfepy.terms import Term
from sfepy.discrete.conditions import Conditions, EssentialBC
from sfepy.solvers.ls import ScipyDirect
from sfepy.solvers.nls import Newton
from sfepy.postprocess.viewer import Viewer

def shift_u_fun(ts, coors, bc=None, problem=None, shift=0.0):
    """
    Define a displacement depending on the y coordinate.
    """
    val = shift * coors[:,1]**2

    return val

usage = """%prog [options]"""
help = {
    'show' : 'show the results figure',
}

def main():
    from sfepy import data_dir

    parser = OptionParser(usage=usage, version='%prog')
    parser.add_option('-s', '--show',
                      action="store_true", dest='show',
                      default=False, help=help['show'])
    options, args = parser.parse_args()

    mesh = Mesh.from_file(data_dir + '/meshes/2d/rectangle_tri.mesh')
    domain = FEDomain('domain', mesh)

    min_x, max_x = domain.get_mesh_bounding_box()[:,0]
    eps = 1e-8 * (max_x - min_x)
    omega = domain.create_region('Omega', 'all')
    gamma1 = domain.create_region('Gamma1',
                                  'vertices in x < %.10f' % (min_x + eps),
                                  'facet')
    gamma2 = domain.create_region('Gamma2',
                                  'vertices in x > %.10f' % (max_x - eps),
                                  'facet')

    field = Field.from_args('fu', nm.float64, 'vector', omega, approx_order=2)

    u = FieldVariable('u', 'unknown', field)
    v = FieldVariable('v', 'test', field, primary_var_name='u')

    m = Material('m', lam=1.0, mu=1.0)
    f = Material('f', val=[[0.02], [0.01]])

    integral = Integral('i', order=3)

    t1 = Term.new('dw_lin_elastic_iso(m.lam, m.mu, v, u)',
         integral, omega, m=m, v=v, u=u)
    t2 = Term.new('dw_volume_lvf(f.val, v)', integral, omega, f=f, v=v)
    eq = Equation('balance', t1 + t2)
    eqs = Equations([eq])

    fix_u = EssentialBC('fix_u', gamma1, {'u.all' : 0.0})

    bc_fun = Function('shift_u_fun', shift_u_fun, extra_args={'shift' : 0.01})
    shift_u = EssentialBC('shift_u', gamma2, {'u.0' : bc_fun})

    ls = ScipyDirect({})

    nls_status = IndexedStruct()
    nls = Newton({}, lin_solver=ls, status=nls_status)

    pb = Problem('elasticity', equations=eqs, nls=nls, ls=ls)
    pb.save_regions_as_groups('regions')

    pb.time_update(ebcs=Conditions([fix_u, shift_u]))

    vec = pb.solve()
    print nls_status

    pb.save_state('linear_elasticity.vtk', vec)

    if options.show:
        view = Viewer('linear_elasticity.vtk')
        view(vector_mode='warp_norm', rel_scaling=2,
             is_scalar_bar=True, is_wireframe=True)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = live_plot
import os
import sys
sys.path.append( '.' )

import numpy as nm

from sfepy.base.base import output, pause
from sfepy.base.log import Log

def main():
    cwd = os.path.split(os.path.join(os.getcwd(), __file__))[0]
    
    log = Log((['sin(x)', 'cos(x)'], ['exp(x)']),
              yscales=['linear', 'log'],
              xlabels=['angle', None], ylabels=[None, 'a function'],
              log_filename=os.path.join(cwd, 'live_plot.log'))
    log2 = Log([['x^3']],
               yscales=['linear'],
               xlabels=['x'], ylabels=['a cubic function'],
               log_filename=os.path.join(cwd, 'live_plot2.log'))

    added = 0
    for x in nm.linspace(0, 4.0 * nm.pi, 200):
        output('x: ', x)

        if x < (2.0 * nm.pi):
            log(nm.sin(x), nm.cos(x), nm.exp(x), x = [x, None])

        else:
            if added:
                log(nm.sin(x), nm.cos(x), nm.exp(x), x**2,
                    x=[x, None, x])
            else:
                log.plot_vlines(color='r', linewidth=2)
                log.add_group(['x^2'], 'linear', 'new x', 'square',
                              formats=['%+g'])
            added += 1

        if (added == 20) or (added == 50):
            log.plot_vlines([2], color='g', linewidth=2)

        log2(x*x*x, x=[x])
            
    print log
    print log2
    pause()

    log(finished=True)
    log2(finished=True)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = thermal_electric
#!/usr/bin/env python
"""
First solve the stationary electric conduction problem. Then use its
results to solve the evolutionary heat conduction problem.

Run this example as on a command line::

    $ python <path_to_this_file>/thermal_electric.py
"""
import sys
sys.path.append( '.' )
import os

from sfepy import data_dir

filename_mesh = data_dir + '/meshes/2d/special/circle_in_square.mesh'

# Time stepping for the heat conduction problem.
t0 = 0.0
t1 = 0.5
n_step = 11

# Material parameters.
specific_heat = 1.2

##########

cwd = os.path.split(os.path.join(os.getcwd(), __file__))[0]

options = {
    'absolute_mesh_path' : True,
    'output_dir' : os.path.join(cwd, 'output')
}

regions = {
    'Omega' : 'all',
    'Omega1' : 'cells of group 1',
    'Omega2' : 'cells of group 2',
    'Omega2_Surface': ('r.Omega1 *v r.Omega2', 'facet'),
    'Left' : ('vertices in (x < %f)' % -0.4999, 'facet'),
    'Right' : ('vertices in (x > %f)' % 0.4999, 'facet'),
}

materials = {
    'm' : ({
        'thermal_conductivity' : 2.0,
        'electric_conductivity' : 1.5,
    },),
}

# The fields use the same approximation, so a single field could be used
# instead.
fields = {
    'temperature': ('real', 1, 'Omega', 1),
    'potential' : ('real', 1, 'Omega', 1),
}

variables = {
    'T' : ('unknown field', 'temperature', 0, 1),
    's' : ('test field', 'temperature', 'T'),
    'phi' : ('unknown field', 'potential', 1),
    'psi' : ('test field', 'potential', 'phi'),
    'phi_known' : ('parameter field', 'potential', '(set-to-None)'),
}

ics = {
    'ic' : ('Omega', {'T.0' : 0.0}),
}

ebcs = {
    'left' : ('Left', {'T.0' : 0.0, 'phi.0' : 0.0}),
    'right' : ('Right', {'T.0' : 2.0, 'phi.0' : 0.0}),
    'inside' : ('Omega2_Surface', {'phi.0' : 'set_electric_bc'}),
}

def set_electric_bc(coor):
    y = coor[:,1]
    ymin, ymax = y.min(), y.max()
    val = 2.0 * (((y - ymin) / (ymax - ymin)) - 0.5)
    return val

functions = {
    'set_electric_bc' : (lambda ts, coor, bc, problem, **kwargs:
                         set_electric_bc(coor),),
}

equations = {
    '2' : """%.12e * dw_volume_dot.2.Omega( s, dT/dt )
             + dw_laplace.2.Omega( m.thermal_conductivity, s, T )
             = dw_electric_source.2.Omega( m.electric_conductivity,
                                           s, phi_known ) """ % specific_heat,
    '1' : """dw_laplace.2.Omega( m.electric_conductivity, psi, phi ) = 0""",
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton', {
        'i_max'      : 1,
        'eps_a'      : 1e-10,
        'problem'   : 'nonlinear',
    }),
    'ts' : ('ts.simple', {
        't0'     : t0,
        't1'     : t1,
        'dt'     : None,
        'n_step' : n_step, # has precedence over dt!
    }),
}

def main():
    from sfepy.base.base import output
    from sfepy.base.conf import ProblemConf, get_standard_keywords
    from sfepy.discrete import Problem

    output.prefix = 'therel:'

    required, other = get_standard_keywords()
    conf = ProblemConf.from_file(__file__, required, other)

    problem = Problem.from_conf(conf, init_equations=False)

    # Setup output directory according to options above.
    problem.setup_default_output()

    # First solve the stationary electric conduction problem.
    problem.set_equations({'eq' : conf.equations['1']})
    problem.time_update()
    state_el = problem.solve()
    problem.save_state(problem.get_output_name(suffix = 'el'), state_el)

    # Then solve the evolutionary heat conduction problem, using state_el.
    problem.set_equations({'eq' : conf.equations['2']})
    phi_var = problem.get_variables()['phi_known']
    phi_var.set_data(state_el())
    time_solver = problem.get_time_solver()
    time_solver()

    output('results saved in %s' % problem.get_output_name(suffix = '*'))

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = thermo_elasticity
r"""
Thermo-elasticity with a given temperature distribution.

Uses `dw_biot` term with an isotropic coefficient for thermo-elastic coupling.

For given body temperature :math:`T` and background temperature
:math:`T_0` find :math:`\ul{u}` such that:

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    - \int_{\Omega}  (T - T_0)\ \alpha_{ij} e_{ij}(\ul{v})
    = 0
    \;, \quad \forall \ul{v} \;,

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl}+\delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;, \\

    \alpha_{ij} = (3 \lambda + 2 \mu) \alpha \delta_{ij}

and :math:`\alpha` is the thermal expansion coefficient.
"""
import numpy as np

from sfepy.base.base import Struct
from sfepy.mechanics.matcoefs import stiffness_from_lame
from sfepy.mechanics.tensors import get_von_mises_stress
from sfepy import data_dir

# Material parameters.
lam = 10.0
mu = 5.0
thermal_expandability = 1.25e-5
T0 = 20.0 # Background temperature.

filename_mesh = data_dir + '/meshes/3d/block.mesh'

def get_temperature_load(ts, coors, region=None):
    """
    Temperature load depends on the `x` coordinate.
    """
    x = coors[:, 0]
    return (x - x.min())**2 - T0

def post_process(out, pb, state, extend=False):
    """
    Compute derived quantities: strain, stresses. Store also the loading
    temperature.
    """
    ev = pb.evaluate

    strain = ev('ev_cauchy_strain.2.Omega( u )', mode='el_avg')
    out['cauchy_strain'] = Struct(name='output_data',
                                  mode='cell', data=strain,
                                  dofs=None)

    e_stress = ev('ev_cauchy_stress.2.Omega( solid.D, u )', mode='el_avg')
    out['elastic_stress'] = Struct(name='output_data',
                                   mode='cell', data=e_stress,
                                   dofs=None)

    t_stress = ev('ev_biot_stress.2.Omega( solid.alpha, T )', mode='el_avg')
    out['thermal_stress'] = Struct(name='output_data',
                                   mode='cell', data=t_stress,
                                   dofs=None)

    out['total_stress'] = Struct(name='output_data',
                                 mode='cell', data=e_stress + t_stress,
                                 dofs=None)

    out['von_mises_stress'] = aux = out['total_stress'].copy()
    vms = get_von_mises_stress(aux.data.squeeze())
    vms.shape = (vms.shape[0], 1, 1, 1)
    out['von_mises_stress'].data = vms

    val = pb.get_variables()['T']()
    val.shape = (val.shape[0], 1)
    out['T'] = Struct(name='output_data',
                      mode='vertex', data=val + T0,
                      dofs=None)
    return out

options = {
    'post_process_hook' : 'post_process',

    'nls' : 'newton',
    'ls' : 'ls',
}

functions = {
    'get_temperature_load' : (get_temperature_load,),
}

regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < -4.99)', 'facet'),
}

fields = {
    'displacement': ('real', 3, 'Omega', 1),
    'temperature': ('real', 1, 'Omega', 1),
}

variables = {
    'u' : ('unknown field', 'displacement', 0),
    'v' : ('test field', 'displacement', 'u'),
    'T' : ('parameter field', 'temperature',
           {'setter' : 'get_temperature_load'}),
}

ebcs = {
    'fix_u' : ('Left', {'u.all' : 0.0}),
}

eye_sym = np.array([[1], [1], [1], [0], [0], [0]], dtype=np.float64)
materials = {
    'solid' : ({
        'D' : stiffness_from_lame(3, lam=lam, mu=mu),
        'alpha' : (3.0 * lam + 2.0 * mu) * thermal_expandability * eye_sym
    },),
}

equations = {
    'balance_of_forces' :
    """dw_lin_elastic.2.Omega( solid.D, v, u )
     - dw_biot.2.Omega( solid.alpha, v, T )
     = 0""",
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton', {
        'i_max'      : 1,
        'eps_a'      : 1e-10,
        'problem'   : 'nonlinear',
    }),
}

########NEW FILE########
__FILENAME__ = thermo_elasticity_ess
r"""
Thermo-elasticity with a computed temperature demonstrating equation sequence
solver.

Uses `dw_biot` term with an isotropic coefficient for thermo-elastic coupling.

The equation sequence solver (``'ess'`` in ``solvers``) automatically solves
first the temperature distribution and then the elasticity problem with the
already computed temperature.

Find :math:`\ul{u}`, :math:`T` such that:

.. math::
    \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})
    - \int_{\Omega}  (T - T_0)\ \alpha_{ij} e_{ij}(\ul{v})
    = 0
    \;, \quad \forall \ul{v} \;,

    \int_{\Omega} \nabla s \cdot \nabla T
    = 0
    \;, \quad \forall s \;.

where

.. math::
    D_{ijkl} = \mu (\delta_{ik} \delta_{jl}+\delta_{il} \delta_{jk}) +
    \lambda \ \delta_{ij} \delta_{kl}
    \;, \\

    \alpha_{ij} = (3 \lambda + 2 \mu) \alpha \delta_{ij} \;,

:math:`T_0` is the background temperature and :math:`\alpha` is the thermal
expansion coefficient.

Notes
-----
The gallery image was produced by (plus proper view settings)::

    ./postproc.py block.vtk -d'u,plot_displacements,rel_scaling=1000,color_kind="scalars",color_name="T"' --wireframe --only-names=u -b
"""
import numpy as np

from sfepy.mechanics.matcoefs import stiffness_from_lame
from sfepy import data_dir

# Material parameters.
lam = 10.0
mu = 5.0
thermal_expandability = 1.25e-5
T0 = 20.0 # Background temperature.

filename_mesh = data_dir + '/meshes/3d/block.mesh'

options = {
    'ts' : 'ess',
    'nls' : 'newton',
    'ls' : 'ls',
}

regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < -4.99)', 'facet'),
    'Right' : ('vertices in (x > 4.99)', 'facet'),
    'Bottom' : ('vertices in (z < -0.99)', 'facet'),
}

fields = {
    'displacement': ('real', 3, 'Omega', 1),
    'temperature': ('real', 1, 'Omega', 1),
}

variables = {
    'u' : ('unknown field', 'displacement', 0),
    'v' : ('test field', 'displacement', 'u'),
    'T' : ('unknown field', 'temperature', 1),
    's' : ('test field', 'temperature', 'T'),
}

ebcs = {
    'u0' : ('Left', {'u.all' : 0.0}),
    't0' : ('Left', {'T.0' : 20.0}),
    't2' : ('Bottom', {'T.0' : 0.0}),
    't1' : ('Right', {'T.0' : 30.0}),
}

eye_sym = np.array([[1], [1], [1], [0], [0], [0]], dtype=np.float64)
materials = {
    'solid' : ({
        'D' : stiffness_from_lame(3, lam=lam, mu=mu),
        'alpha' : (3.0 * lam + 2.0 * mu) * thermal_expandability * eye_sym
    },),
}

equations = {
    'balance_of_forces' : """
        + dw_lin_elastic.2.Omega(solid.D, v, u)
        - dw_biot.2.Omega(solid.alpha, v, T)
        = 0
    """,
    'temperature' : """
        + dw_laplace.1.Omega(s, T)
        = 0
    """
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton', {
        'i_max'      : 1,
        'eps_a'      : 1e-10,
        'problem'   : 'nonlinear',
    }),
    'ess' : ('ts.equation_sequence', {}),
}

########NEW FILE########
__FILENAME__ = extractor
#!/usr/bin/env python
"""
Examples
--------

$ ./extractor.py -e "p e 0 1999" bone.h5
$ ./extractor.py -e "p e 0 1999" bone.h5 -a
$ ./extractor.py -e "p e 0 1999" bone.h5 -o extracted.h5
$ ./extractor.py -e "p e 0 1999" bone.h5 -o extracted.h5 -a
"""
import os
from optparse import OptionParser

import sfepy
from sfepy.base.base import nm, dict_to_struct, get_default, Struct
from sfepy.base.ioutils import get_trunk
import sfepy.postprocess.time_history as th

def create_problem(filename):
    from sfepy.discrete import Problem

    problem = Problem.from_conf_file(filename,
                                     init_equations=False, init_solvers=False)
    return problem

def parse_linearization(linearization):
    out = {}
    for item in linearization.split(','):
        key, val = item.split(':')
        if key == 'eps':
            val = float(val)
        elif key in ('min_level', 'max_level'):
            val = int(val)
        elif key == 'kind':
            pass
        else:
            raise ValueError('wrong linearization option key! (%s)'
                             % key)
        out[key] = val

    return dict_to_struct(out)

usage = """%prog [options] [<input file>] <results file>

Extract information from a SfePy multi-time-step results file (HDF5
format) and/or linearize results with stored higher order DOFs.

For the linearization, the original input (problem description) file must
be specified as the first argument. Use the option --linearization below
to override linearization parameters defined in the input file. The
linearization forces --dump option, i.e., output to VTK files.
"""

help = {
    'filename' :
    'basename of output file(s) [default: <basename of input file>]',
    'dump' :
    'dump to sequence of VTK files',
    'same_dir' :
    'store the dumped VTK files in the directory of filename_in',
    'linearization' :
    'linearization options. Default values apply if neither command'
    ' line nor input file options are set.'
    " [default: 'kind:adaptive,min_level:0,max_level:2,eps:1e-2']",
    'times' :
    'extract and print times of individual time steps',
    'from' :
    'start dumping from time step ii [default: %default]',
    'to' :
    'stop dumping at time step ii [default: <last step>]',
    'step' :
    'use every ii-th step for dumping [default: %default]',
    'extract' :
    'extract variables according to extraction list.'
    " Example: 'u n 10 15, p e 0' means variable 'u' in nodes 10, 15"
    " and variable 'p' in element 0",
    'average' :
    'average vertex variable into cells ("e" extraction mode)'
}

def main():
    parser = OptionParser(usage=usage, version="%prog " + sfepy.__version__)
    parser.add_option('-o', '', metavar='filename',
                      action='store', dest='output_filename_trunk',
                      default=None, help=help['filename'])
    parser.add_option('-d', '--dump', action='store_true', dest='dump',
                       default=False, help=help['dump'])
    parser.add_option('', '--same-dir', action='store_true', dest='same_dir',
                      default=False, help=help['same_dir'])
    parser.add_option('-l', '--linearization', metavar='options',
                      action='store', dest='linearization',
                      default=None, help=help['linearization'])
    parser.add_option('', '--times', action='store_true', dest='times',
                      default=False, help=help['times'])
    parser.add_option('-f', '--from', type=int, metavar='ii',
                      action='store', dest='step_from',
                      default=0, help=help['from'])
    parser.add_option('-t', '--to', type=int, metavar='ii',
                      action='store', dest='step_to',
                      default=None, help=help['to'])
    parser.add_option('-s', '--step', type=int, metavar='ii',
                      action='store', dest='step_by',
                      default=1, help=help['step'])
    parser.add_option('-e', '--extract', metavar='list',
                      action='store', dest='extract',
                      default=None, help=help['extract'])
    parser.add_option('-a', '--average', action='store_true', dest='average',
                      default=False, help=help['average'])

    (options, args) = parser.parse_args()

    nargs = len(args)
    if nargs == 1:
        filename_results = args[0]
        linearize = False

    elif nargs == 2:
        filename_in, filename_results = args
        linearize = True
        options.dump = True

    else:
        parser.print_help()
        return

    if options.times:
        steps, times, nts, dts = th.extract_times(filename_results)
        for ii, time in enumerate(times):
            step = steps[ii]
            print '%d %e %e %e' % (step, time, nts[ii], dts[ii])

    if options.dump:
        trunk = get_default(options.output_filename_trunk,
                            get_trunk(filename_results))
        if options.same_dir:
            trunk = os.path.join(os.path.dirname(filename_results),
                                 os.path.basename(trunk))

        args = {}
        if linearize:
            problem = create_problem(filename_in)

            linearization = Struct(kind='adaptive', min_level=0,
                                   max_level=2, eps=1e-2)
            aux = problem.conf.options.get('linearization', None)
            linearization.update(aux)

            if options.linearization is not None:
                aux = parse_linearization(options.linearization)
                linearization.update(aux)

            args.update({'fields' : problem.fields,
                         'linearization' : linearization})

        if options.step_to is None:
            args.update({'step0' : options.step_from})

        else:
            args.update({'steps' : nm.arange(options.step_from,
                                             options.step_to + 1,
                                             options.step_by, dtype=nm.int)})

        th.dump_to_vtk(filename_results, output_filename_trunk=trunk, **args)

    if options.extract:
        ths, ts = th.extract_time_history(filename_results, options.extract)

        if options.average:
            ths = th.average_vertex_var_in_cells(ths)

        if options.output_filename_trunk:
            th.save_time_history(ths, ts, options.output_filename_trunk + '.h5')

        else:
            print dict_to_struct(ths, flag=(1, 1, 1)).str_all()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = homogen
#!/usr/bin/env python
from optparse import OptionParser

import sfepy
from sfepy.base.conf import ProblemConf, get_standard_keywords
from sfepy.homogenization.homogen_app import HomogenizationApp

usage = """%prog [options] filename_in"""

help = {
    'filename' :
    'basename of output file(s) [default: <basename of input file>]',
}

def main():
    parser = OptionParser(usage=usage, version="%prog " + sfepy.__version__)
    parser.add_option("-o", "", metavar='filename', action="store",
                      dest="output_filename_trunk",
                      default=None, help=help['filename'])

    (options, args) = parser.parse_args()

    if (len(args) == 1):
        filename_in = args[0]
    else:
        parser.print_help(),
        return

    required, other = get_standard_keywords()
    required.remove('equations')

    conf = ProblemConf.from_file(filename_in, required, other)

    app = HomogenizationApp(conf, options, 'homogen:')
    opts = conf.options
    if hasattr(opts, 'parametric_hook'):  # Parametric study.
        parametric_hook = conf.get_function(opts.parametric_hook)
        app.parametrize(parametric_hook)
    app()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = phonon
#!/usr/bin/env python
# 12.01.2007, c
from optparse import OptionParser

import sfepy
from sfepy.base.base import output
from sfepy.base.conf import ProblemConf, get_standard_keywords
from sfepy.homogenization.band_gaps_app import AcousticBandGapsApp
from sfepy.base.plotutils import plt

usage = """%prog [options] filename_in"""

help = {
    'filename' :
    'basename of output file(s) [default: <basename of input file>]',
    'detect_band_gaps' :
    'detect frequency band gaps',
    'analyze_dispersion' :
    'analyze dispersion properties (low frequency domain)',
    'plot' :
    'plot frequency band gaps, assumes -b',
    'phase_velocity' :
    'compute phase velocity (frequency-independet mass only)'
}

def main():
    parser = OptionParser(usage=usage, version="%prog " + sfepy.__version__)
    parser.add_option("-o", "", metavar='filename',
                      action="store", dest="output_filename_trunk",
                      default=None, help=help['filename'])
    parser.add_option("-b", "--band-gaps",
                      action="store_true", dest="detect_band_gaps",
                      default=False, help=help['detect_band_gaps'])
    parser.add_option("-d", "--dispersion",
                      action="store_true", dest="analyze_dispersion",
                      default=False, help=help['analyze_dispersion'])
    parser.add_option("-p", "--plot",
                      action="store_true", dest="plot",
                      default=False, help=help['plot'])
    parser.add_option("--phase-velocity",
                      action="store_true", dest="phase_velocity",
                      default=False, help=help['phase_velocity'])

    options, args = parser.parse_args()
    if options.plot:
        if plt is None:
            output('matplotlib.pyplot cannot be imported, ignoring option -p!')
            options.plot = False
        elif options.analyze_dispersion == False:
            options.detect_band_gaps = True

    if (len(args) == 1):
        filename_in = args[0];
    else:
        parser.print_help(),
        return

    required, other = get_standard_keywords()
    required.remove('equations')
    if not options.analyze_dispersion:
        required.remove('solver_[0-9]+|solvers')
    if options.phase_velocity:
        required.remove('ebc_[0-9]+|ebcs')
    conf = ProblemConf.from_file(filename_in, required, other)

    app = AcousticBandGapsApp(conf, options, 'phonon:')
    opts = conf.options
    if hasattr(opts, 'parametric_hook'): # Parametric study.
        parametric_hook = conf.get_function(opts.parametric_hook)
        app.parametrize(parametric_hook)
    app()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = postproc
#!/usr/bin/env python
"""
This is a script for quick Mayavi-based visualizations of finite element
computations results.

Examples
--------
  The examples assume that run_tests.py has been run successfully and the
  resulting data files are present.

  - view data in output-tests/test_navier_stokes.vtk

    $ python postproc.py output-tests/test_navier_stokes.vtk
    $ python postproc.py output-tests/test_navier_stokes.vtk --3d

  - save a snapshot image and exit

    $ python postproc.py output-tests/test_poisson.vtk -o image.png -n

  - save a snapshot image without off-screen rendering and exit

    $ python postproc.py output-tests/test_poisson.vtk -o image.png \
                         -n --no-offscreen

  - create animation (forces offscreen rendering) from
    output-tests/test_time_poisson.*.vtk

    $ python postproc.py output-tests/test_time_poisson.*.vtk -a mov

  - create animation (forces offscreen rendering) from
    output-tests/test_hyperelastic.*.vtk

    The range specification for the displacements 'u' is required, as
    output-tests/test_hyperelastic.00.vtk contains only zero
    displacements which leads to invisible glyph size.

    $ python postproc.py output-tests/test_hyperelastic.*.vtk \
                         --ranges=u,0,0.02 -a mov

  - same as above, but slower frame rate

    $ python postproc.py output-tests/test_hyperelastic_TL.*.vtk \
                         --ranges=u,0,0.02 -a mov --ffmpeg-options="-r 2 -sameq"
"""
from optparse import OptionParser, OptionGroup
import os
import glob

import sfepy
from sfepy.base.base import assert_, get_default, output, nm
from sfepy.postprocess.viewer import (Viewer, get_data_ranges,
                                      create_file_source)
from sfepy.postprocess.domain_specific import DomainSpecificPlot

usage = '%prog [options] filename\n' + __doc__.rstrip()

help = {
    'filename' :
    'view image file name [default: "view.png"]',
    'output_dir' :
    'output directory for saving view images; ignored when -o option is' \
    ' given, as the directory part of the filename is taken instead' \
    ' [default: "."]',
    'no_show' :
    'do not call mlab.show()',
    'no_offscreen' :
    'force no offscreen rendering for --no-show',
    'anim_format' :
    'if set to a ffmpeg-supported format (e.g. mov, avi, mpg), ffmpeg is' \
    ' installed and results of multiple time steps are given, an animation is' \
    ' created in the same directory as the view images',
    'ffmpeg_options' :
    'ffmpeg animation encoding options (enclose in "") [default: "%default"]',

    'step' :
    'set the time step. Negative indices are allowed, -1 means the last step.'
    ' The closest higher step is used if the desired one is not available.'
    ' Has precedence over --time. [default: the first step]',
    'time' :
    'set the time. The closest higher time is used if the desired one is not'
    ' available. [default: None]',
    'watch' :
    'watch the results file for changes (single file mode only)',
    'all' :
    'draw all data (normally, node_groups and mat_id are omitted)',
    'only_names' :
    'draw only named data',
    'list_ranges' :
    'do not plot, only list names and ranges of all data',
    'ranges' :
    'force data ranges [default: automatic from data]',

    'resolution' :
    'image resolution in NxN format [default: shorter axis: 600;'\
    ' depends on layout: for rowcol it is 800x600]',
    'layout' :
    'layout for multi-field plots, one of: rowcol, colrow, row, col, row#n, col#n,' \
    ' where #n is the number of plots in the specified direction [default: %default]',
    'is_3d' :
    '3d plot mode',
    'view' :
    'camera azimuth, elevation angles, and optionally also '
    'distance and focal point coordinates (without []) as in `mlab.view()` '
    '[default: if --3d is True: "45,45", else: "0,0"]',
    'roll' :
    'camera roll angle [default: %default]',
    'fgcolor' :
    'foreground color, that is the color of all text annotation labels'
    ' (axes, orientation axes, scalar bar labels) [default: %default]',
    'bgcolor' :
    'background color [default: %default]',
    'anti_aliasing' :
    'value of anti-aliasing [default: mayavi2 default]',

    'is_scalar_bar' :
    'show scalar bar for each data',
    'is_wireframe' :
    'show wireframe of mesh surface for each data',
    'group_names' :
    'superimpose plots of data in each group',
    'subdomains' :
    'superimpose surfaces of subdomains over each data;' \
    ' example value: mat_id,0,None,True',
    'domain_specific' :
    'domain specific drawing functions and configurations',

    'scalar_mode' :
    'mode for plotting scalars with --3d, one of: cut_plane, iso_surface,'\
    ' both [default: %default]',
    'vector_mode' :
    'mode for plotting vectors, one of: arrows, norm, arrows_norm, warp_norm'\
    ' [default: %default]',
    'rel_scaling' :
    'relative scaling of glyphs (vector field visualization)' \
    ' [default: %default]',
    'clamping' :
    'glyph clamping mode',
    'opacity' :
    'opacity in [0.0, 1.0]. Can be given either globally'
    ' as a single float, or per module, e.g.'
    ' "wireframe=0.1,scalar_cut_plane=0.5". Possible keywords are: wireframe,'
    ' scalar_cut_plane, vector_cut_plane, surface, iso_surface,'
    ' arrows_surface, glyphs. [default: 1.0]',
    'rel_text_width' :
    'relative text annotation width [default: %default]',
}

def parse_view(option, opt, value, parser):
    vals = value.split(',')
    assert_(len(vals) in [2, 3, 6])
    val = tuple(float(ii) for ii in vals)
    if len(vals) == 6:
        val = val[:3] + (list(val[3:]),)
    setattr(parser.values, option.dest, val)

def parse_resolution(option, opt, value, parser):
    if value is not None:
        print value
        setattr(parser.values, option.dest,
                tuple([int(r) for r in value.split('x')]))

def parse_ranges(option, opt, value, parser):
    if value is not None:
        print value
        ranges = {}
        for rng in value.split(':'):
            aux = rng.split(',')
            ranges[aux[0]] = (float(aux[1]), float(aux[2]))
        setattr(parser.values, option.dest, ranges)

def parse_opacity(option, opt, value, parser):
    try:
        opacity = float(value)
        assert_(0.0 <= opacity <= 1.0)

    except:
        opacity = {}

        for vals in value.split(','):
            key, val = vals.split('=')
            val = float(val)
            assert_(0.0 <= val <= 1.0)

            opacity[key] = val

    setattr(parser.values, option.dest, opacity)

def parse_group_names(option, opt, value, parser):
    if value is not None:
        print value
        group_names = [tuple(group.split(',')) for group in value.split(':')]
        setattr(parser.values, option.dest, group_names)

def parse_subdomains(option, opt, value, parser):
    if value is not None:
        print value
        aux = value.split(',')

        try:
            tmin = int(aux[1])

        except ValueError:
            tmin = None

        try:
            tmax = int(aux[2])

        except ValueError:
            tmax = None

        subdomains_args = {'mat_id_name' : aux[0],
                           'threshold_limits' : (tmin, tmax),
                           'single_color' : aux[3] == 'True'}
        setattr(parser.values, option.dest, subdomains_args)

def parse_domain_specific(option, opt, value, parser):
    if value is not None:
        print value
        out = {}
        confs = value.split(':')
        for conf in confs:
            aux = conf.split(',')
            var_name, fun_name = aux[:2]
            args = aux[2:]

            out[var_name] = DomainSpecificPlot(fun_name, args)

        setattr(parser.values, option.dest, out)

def view_file(filename, filter_names, options, view=None):
    if view is None:
        if options.show:
            offscreen = False

        else:
            offscreen = get_default(options.offscreen, True)
        view = Viewer(filename, watch=options.watch,
                      ffmpeg_options=options.ffmpeg_options,
                      output_dir=options.output_dir,
                      offscreen=offscreen)

        if options.only_names is not None:
            options.only_names = options.only_names.split(',')

        view(show=options.show, is_3d=options.is_3d, view=options.view,
             roll=options.roll,
             fgcolor=options.fgcolor, bgcolor=options.bgcolor,
             layout=options.layout,
             scalar_mode=options.scalar_mode,
             vector_mode=options.vector_mode,
             rel_scaling=options.rel_scaling,
             clamping=options.clamping, ranges=options.ranges,
             is_scalar_bar=options.is_scalar_bar,
             is_wireframe=options.is_wireframe,
             opacity=options.opacity,
             subdomains_args=options.subdomains_args,
             rel_text_width=options.rel_text_width,
             fig_filename=options.filename, resolution=options.resolution,
             filter_names=filter_names, only_names=options.only_names,
             group_names=options.group_names,
             step=options.step, time=options.time,
             anti_aliasing=options.anti_aliasing,
             domain_specific=options.domain_specific)

    else:
        view.set_source_filename(filename)
        view.save_image(options.filename)

    return view

def main():
    parser = OptionParser(usage=usage, version='%prog ' + sfepy.__version__)

    group = OptionGroup(parser, 'Output Options')
    group.add_option('-o', '--output', metavar='filename',
                     action='store', dest='filename',
                     default=None, help=help['filename'])
    group.add_option('--output-dir', metavar='directory',
                     action='store', dest='output_dir',
                     default=None, help=help['output_dir'])
    group.add_option('-n', '--no-show',
                     action='store_false', dest='show',
                     default=True, help=help['no_show'])
    group.add_option('', '--no-offscreen',
                     action='store_false', dest='offscreen',
                     default=None, help=help['no_offscreen'])
    group.add_option('-a', '--animation', metavar='<ffmpeg-supported format>',
                     action='store', dest='anim_format',
                     default=None, help=help['anim_format'])
    group.add_option('', '--ffmpeg-options', metavar='<ffmpeg options>',
                     action='store', dest='ffmpeg_options',
                     default='-r 10 -sameq',
                     help=help['ffmpeg_options'])
    parser.add_option_group(group)

    group = OptionGroup(parser, 'Data Options')
    group.add_option('--step', type='int', metavar='step',
                     action='store', dest='step',
                     default=None, help=help['step'])
    group.add_option('--time', type='float', metavar='time',
                     action='store', dest='time',
                     default=None, help=help['time'])
    group.add_option('-w', '--watch',
                     action='store_true', dest='watch',
                     default=False, help=help['watch'])
    group.add_option('--all',
                     action='store_true', dest='all',
                     default=False, help=help['all'])
    group.add_option('--only-names', metavar='list of names',
                     action='store', dest='only_names',
                     default=None, help=help['only_names'])
    group.add_option('-l', '--list-ranges',
                     action='store_true', dest='list_ranges',
                     default=False, help=help['list_ranges'])
    group.add_option('--ranges', type='str',
                     metavar='name1,min1,max1:name2,min2,max2:...',
                     action='callback', dest='ranges',
                     callback=parse_ranges, help=help['ranges'])
    parser.add_option_group(group)

    group = OptionGroup(parser, 'View Options')
    group.add_option('-r', '--resolution', type='str', metavar='resolution',
                     action='callback', dest='resolution',
                     callback=parse_resolution, help=help['resolution'])
    group.add_option('--layout', metavar='layout',
                     action='store', dest='layout',
                     default='rowcol', help=help['layout'])
    group.add_option('--3d',
                     action='store_true', dest='is_3d',
                     default=False, help=help['is_3d'])
    group.add_option('--view', type='str',
                     metavar='angle,angle[,distance[,focal_point]]',
                     action='callback', dest='view',
                     callback=parse_view, help=help['view'])
    group.add_option('--roll', type='float', metavar='angle',
                     action='store', dest='roll',
                     default=0.0, help=help['roll'])
    group.add_option('--fgcolor', metavar='R,G,B',
                     action='store', dest='fgcolor',
                     default='0.0,0.0,0.0', help=help['fgcolor'])
    group.add_option('--bgcolor', metavar='R,G,B',
                     action='store', dest='bgcolor',
                     default='1.0,1.0,1.0', help=help['bgcolor'])
    group.add_option('--anti-aliasing', type='int', metavar='value',
                     action='store', dest='anti_aliasing',
                     default=None, help=help['anti_aliasing'])
    parser.add_option_group(group)

    group = OptionGroup(parser, 'Custom Plots Options')
    group.add_option('-b', '--scalar-bar',
                     action='store_true', dest='is_scalar_bar',
                     default=False, help=help['is_scalar_bar'])
    group.add_option('', '--wireframe',
                     action='store_true', dest='is_wireframe',
                     default=False, help=help['is_wireframe'])
    group.add_option('--group-names', type='str', metavar='name1,...,nameN:...',
                     action='callback', dest='group_names',
                     callback=parse_group_names, help=help['group_names'])
    group.add_option('--subdomains', type='str',
                     metavar='mat_id_name,threshold_limits,single_color',
                     action='callback', dest='subdomains_args',
                     callback=parse_subdomains, default=None,
                     help=help['subdomains'])
    group.add_option('-d', '--domain-specific', type='str',
                     metavar='"var_name0,function_name0,' \
                             'par0=val0,par1=val1,...:var_name1,..."',
                     action='callback', dest='domain_specific',
                     callback=parse_domain_specific, default=None,
                     help=help['domain_specific'])
    parser.add_option_group(group)

    group = OptionGroup(parser, 'Mayavi Options')
    group.add_option('--scalar-mode', metavar='mode',
                     action='store', dest='scalar_mode',
                     default='iso_surface', help=help['scalar_mode'])
    group.add_option('--vector-mode', metavar='mode',
                     action='store', dest='vector_mode',
                     default='arrows_norm', help=help['vector_mode'])
    group.add_option('-s', '--scale-glyphs', type='float', metavar='scale',
                     action='store', dest='rel_scaling',
                     default=0.05, help=help['rel_scaling'])
    group.add_option('--clamping',
                     action='store_true', dest='clamping',
                     default=False, help=help['clamping'])
    group.add_option('--opacity', type='str', metavar='opacity',
                     action='callback', dest='opacity',
                     callback=parse_opacity, help=help['opacity'])
    group.add_option('--rel-text-width', type='float', metavar='width',
                     action='store', dest='rel_text_width',
                     default=0.02, help=help['rel_text_width'])
    parser.add_option_group(group)

    options, args = parser.parse_args()

    if len(args) >= 1:
        if len(args) == 1:
            filenames = glob.glob(args[0])
            filenames.sort()
        else:
            filenames = args
    else:
        parser.print_help(),
        return

    options.fgcolor = tuple([float(ii) for ii in
                             options.fgcolor.split(',')])
    assert_(len(options.fgcolor) == 3)

    options.bgcolor = tuple([float(ii) for ii in
                             options.bgcolor.split(',')])
    assert_(len(options.bgcolor) == 3)

    can_save = not options.show

    # Output dir / file names.
    if options.filename is None:
        can_save = False
        options.filename = 'view.png'
        if options.output_dir is None:
            options.output_dir = '.'

    else:
        options.output_dir, options.filename = os.path.split(options.filename)

    # Data filtering,
    if not options.all:
        filter_names = ['node_groups', 'mat_id']
    else:
        filter_names = []

    if options.anim_format is not None:
        # Do not call show when saving an animation.
        options.show = False

    if options.list_ranges:
        all_ranges = {}
        for ii, filename in enumerate(filenames):
            output('%d: %s' % (ii, filename))

            file_source = create_file_source(filename)
            if (options.step is None) and (options.time is None):
                steps, _ = file_source.get_ts_info()

            else:
                if options.step is not None:
                    step, _ = file_source.get_step_time(step=options.step)

                else:
                    step, _ = file_source.get_step_time(time=options.time)

                steps = [step]

            if not len(steps):
                steps = [0]

            for iis, step in enumerate(steps):
                output('%d: step %d' %(iis, step))
                file_source.get_step_time(step=step)
                source = file_source.create_source()
                ranges = get_data_ranges(source, return_only=True)
                for key, val in ranges.iteritems():
                    all_ranges.setdefault(key, []).append(val[3:])

        if (len(filenames) > 1) or (len(steps) > 1):
            output('union of ranges:')

        else:
            output('ranges:')

        for key, ranges in all_ranges.iteritems():
            aux = nm.array(ranges)
            mins = aux[:, [0, 2]].min(axis=0)
            maxs = aux[:, [1, 3]].max(axis=0)
            output('  items: %s,%e,%e' % (key, mins[0], maxs[0]))
            output('  norms: %s,%e,%e' % (key, mins[1], maxs[1]))

    else:
        if len(filenames) == 1:
            filenames = filenames[0]

        view = view_file(filenames, filter_names, options)
        if can_save:
            view.save_image(options.filename)

    if options.anim_format is not None:
        view.save_animation(options.filename)
        view.encode_animation(options.filename, options.anim_format,
                              options.ffmpeg_options)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = probe
#!/usr/bin/env python
# 12.01.2007, c
"""
Probe finite element solutions in points defined by various geometrical probes.

Generation mode
---------------
Probe the data in the results file corresponding to the problem defined in the
input file. The input file options must contain 'gen_probes' and 'probe_hook'
keys, pointing to proper functions accessible from the input file scope.

For each probe returned by `gen_probes()` a data plot figure and a text
file with the data plotted are saved, see the options below.

Generation options
------------------
-o, --auto-dir, --same-dir, -f, --only-names, -s

Postprocessing mode
-------------------
Read a previously probed data from the probe text file, re-plot them,
and integrate them along the probe.

Postprocessing options
----------------------
--postprocess, --radial, --only-names

Notes
-----
For extremely thin hexahedral elements the Newton's iteration for finding the
reference element coordinates might converge to a spurious solution outside
of the element. To obtain some values even in this case, try increasing the
--close-limit option value.
"""
import os
from optparse import OptionParser

import numpy as nm

import sfepy
from sfepy.base.base import output, assert_
from sfepy.base.ioutils import edit_filename
from sfepy.base.conf import ProblemConf, get_standard_keywords
from sfepy.discrete import Problem
from sfepy.discrete.fem import MeshIO
from sfepy.discrete.probes import write_results, read_results

usage = """%prog [generation options] <input file> <results file>
%prog [postprocessing options] <probe file> <figure file>
""" + __doc__.rstrip()

help = {
    'filename' :
    'basename of output file(s) [default: <basename of input file>]',
    'output_format' :
    'output figure file format (supported by the matplotlib backend used) '\
    '[default: %default]',
    'auto_dir' :
    'the directory of the results file is determined automatically using the '\
    '"output_dir" option in input file options',
    'same_dir' :
    'store the probe figures/data in the directory of the results file',
    'only_names' :
    'probe only named data',
    'step' :
    'probe the given time step',
    'close_limit' :
    'maximum limit distance of a point from the closest element allowed'
    ' for extrapolation. [default: %default]',
    'postprocess' :
    'postprocessing mode',
    'radial' :
    'assume radial integration',
}

def generate_probes(filename_input, filename_results, options,
                    conf=None, problem=None, probes=None, labels=None,
                    probe_hooks=None):
    """
    Generate probe figures and data files.
    """
    if conf is None:
        required, other = get_standard_keywords()
        conf = ProblemConf.from_file(filename_input, required, other)

    opts = conf.options

    if options.auto_dir:
        output_dir = opts.get_('output_dir', '.')
        filename_results = os.path.join(output_dir, filename_results)

    output('results in: %s' % filename_results)

    io = MeshIO.any_from_filename(filename_results)
    step = options.step if options.step >= 0 else io.read_last_step()
    all_data = io.read_data(step)
    output('loaded:', all_data.keys())
    output('from step:', step)

    if options.only_names is None:
        data = all_data
    else:
        data = {}
        for key, val in all_data.iteritems():
            if key in options.only_names:
                data[key] = val

    if problem is None:
        problem = Problem.from_conf(conf,
                                    init_equations=False, init_solvers=False)

    if probes is None:
        gen_probes = conf.get_function(conf.options.gen_probes)
        probes, labels = gen_probes(problem)

    if probe_hooks is None:
        probe_hooks = {None : conf.get_function(conf.options.probe_hook)}

    if options.output_filename_trunk is None:
            options.output_filename_trunk = problem.ofn_trunk

    filename_template = options.output_filename_trunk \
                        + ('_%%d.%s' % options.output_format)
    if options.same_dir:
        filename_template = os.path.join(os.path.dirname(filename_results),
                                         filename_template)

    output_dir = os.path.dirname(filename_results)

    for ip, probe in enumerate(probes):
        output(ip, probe.name)

        probe.set_options(close_limit=options.close_limit)

        for key, probe_hook in probe_hooks.iteritems():

            out = probe_hook(data, probe, labels[ip], problem)
            if out is None: continue
            if isinstance(out, tuple):
                fig, results = out
            else:
                fig = out

            if key is not None:
                filename = filename_template % (key, ip)

            else:
                filename = filename_template % ip

            if fig is not None:
                if isinstance(fig, dict):
                    for fig_name, fig_fig in fig.iteritems():
                        fig_filename = edit_filename(filename,
                                                     suffix='_' + fig_name)
                        fig_fig.savefig(fig_filename)
                        output('figure ->', os.path.normpath(fig_filename))

                else:
                    fig.savefig(filename)
                    output('figure ->', os.path.normpath(filename))

            if results is not None:
                txt_filename = edit_filename(filename, new_ext='.txt')

                write_results(txt_filename, probe, results)

                output('data ->', os.path.normpath(txt_filename))

def integrate_along_line(x, y, is_radial=False):
    """
    Integrate numerically (trapezoidal rule) a function :math:`y=y(x)`.

    If is_radial is True, multiply each :math:`y` by :math:`4 \pi x^2`.
    """
    dx = nm.diff(x)
    ay = 0.5 * (y[:-1] + y[1:])

    if is_radial:
        ax = 0.5 * (x[:-1] + x[1:])
        val = 4.0 * nm.pi * nm.sum(ay * dx * (ax**2))

    else:
        val = nm.sum(ay * dx)

    return val

def postprocess(filename_input, filename_results, options):
    """
    Postprocess probe data files - replot, integrate data.
    """
    from matplotlib import pyplot as plt

    header, results = read_results(filename_input,
                                   only_names=options.only_names)
    output(header)

    fig = plt.figure()
    for name, result in results.iteritems():
        pars, vals = result[:, 0], result[:, 1]

        ii = nm.where(nm.isfinite(vals))[0]
        # Nans only at the edges.
        assert_(nm.diff(ii).sum() == (len(ii)-1))

        val = integrate_along_line(pars[ii], vals[ii], options.radial)

        label = r'%s: $\int\ %s' % (name, name)
        if options.radial:
            label += ' (r)'
        label += '$ = %.5e'% val

        plt.plot(pars, vals, label=label, lw=0.2, marker='+', ms=1)
        plt.ylabel('probed data')
        plt.xlabel('probe coordinate')

        output(label)

    plt.legend()

    fig.savefig(filename_results)

def main():
    parser = OptionParser(usage=usage, version='%prog ' + sfepy.__version__)
    parser.add_option('-o', '', metavar='filename',
                      action='store', dest='output_filename_trunk',
                      default=None, help=help['filename'])
    parser.add_option('', '--auto-dir',
                      action='store_true', dest='auto_dir',
                      default=False, help=help['auto_dir'])
    parser.add_option('', '--same-dir',
                      action='store_true', dest='same_dir',
                      default=False, help=help['same_dir'])
    parser.add_option('-f', '--format', metavar='format',
                      action='store', dest='output_format',
                      default='png', help=help['output_format'])
    parser.add_option('--only-names', metavar='list of names',
                      action='store', dest='only_names',
                      default=None, help=help['only_names'])
    parser.add_option('-s', '--step', type='int', metavar='step',
                      action='store', dest='step',
                      default=0, help=help['step'])
    parser.add_option('-c', '--close-limit', type='float', metavar='distance',
                      action='store', dest='close_limit',
                      default=0.1, help=help['close_limit'])
    parser.add_option('-p', '--postprocess',
                      action='store_true', dest='postprocess',
                      default=False, help=help['postprocess'])
    parser.add_option('--radial',
                      action='store_true', dest='radial',
                      default=False, help=help['radial'])
    options, args = parser.parse_args()

    if (len(args) == 2):
        filename_input, filename_results = args
    else:
        parser.print_help(),
        return

    if options.only_names is not None:
        options.only_names = options.only_names.split(',')

    output.prefix = 'probe:'

    if options.postprocess:
        postprocess(filename_input, filename_results, options)
    else:
        generate_probes(filename_input, filename_results, options)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = run_tests
#!/usr/bin/env python
"""
Notes on writing new test files:
--------------------------------

A test file can contain anything, but usually it is similar to a regular input
file (defining a test problem), with a mandatory Test class. This class holds
all the test_* functions, as well as the from_conf(), which serves to initialize
the test (conf is in fact the test file itself, options are command-line
options).

All variables defined in a test file are collected in 'conf' variable passed to
a Test.__init__(). For example, 'input_name' in test_input_*.py files is
accessible as 'conf.input_name'. This is usefull if the test class is defined
outside the test file, as the classes in tests_basic.py are.

The test_* functions are collected automatically by run_tests.py, with one
exception: if a certain order of their evaluation is required, a class
attribute 'test' of the Test class with a list of the test function names
should be defined (example: test_meshio.py)."""

import sys
import time
import os
import os.path as op
from optparse import OptionParser

import sfepy
from sfepy.base.conf import ProblemConf, get_standard_keywords

##
# 05.06.2007, c
class OutputFilter( object ):

    ##
    # c: 05.06.2007, r: 05.02.2008
    def __init__( self, allowed_lines ):
        self.allowed_lines = allowed_lines
        self.msg_type1 = ['...', '!!!', '+++', '---']
        self.msg_type2 = ['<<<', '>>>']
        self.start()

    ##
    # 05.06.2007, c
    def start( self ):
        self.stdout = sys.stdout
        sys.stdout = self

    ##
    # c: 05.06.2007, r: 05.02.2008
    def write( self, msg ):
        if self.stdout is not None:
            msg_type = msg[:3]
            if msg_type in self.allowed_lines:
                if msg_type in self.msg_type1:
                    msg = ''.join( (msg[:3], '  ', msg[3:]) )
                elif msg_type in self.msg_type2:
                    msg = msg[4:]
                self.stdout.write( msg )
                self.stdout.write( '\n' )

    ##
    # 05.06.2007, c
    def stop( self ):
        sys.stdout = self.stdout
        self.stdout = None

##
# c: 04.06.2007, r: 19.02.2008
def run_test( conf_name, options ):
    try:
        os.makedirs( options.out_dir )
    except OSError, e:
        if e.errno != 17: # [Errno 17] File exists
            raise

    if options.filter_none or options.debug:
        of = None
    elif options.filter_less:
        of = OutputFilter( ['<<<', '>>>', '...', '!!!', '+++', '---'] )
    elif options.filter_more:
        of = OutputFilter( ['+++', '---'] )
    else:
        of = OutputFilter( ['<<<', '+++', '---'] )
        
    print '<<< %s' % conf_name

    _required, other = get_standard_keywords()
    required = ['Test']

    num = 1
    test_time = 0.0
    try:
        conf = ProblemConf.from_file( conf_name, required, _required + other )
        test = conf.funmod.Test.from_conf( conf, options )
        num = test.get_number()
        ok = True
        print '>>> test instance prepared (%d test(s))' % num
    except KeyboardInterrupt:
        print '>>> interrupted'
        sys.exit( 0 )
    except:
        print '--- test instance creation failed'
        if options.debug:
            raise
        ok, n_fail, n_total = False, num, num
        
    if ok:
        try:
            tt = time.clock()
            ok, n_fail, n_total = test.run( options.debug )
            test_time = time.clock() - tt
        except KeyboardInterrupt:
            print '>>> interrupted'
            sys.exit( 0 )
        except Exception, e:
            print '>>> %s' % e.__class__
            if options.debug:
                raise
            ok, n_fail, n_total = False, num, num

    if ok:
        print '>>> all passed in %.2f s' % test_time
    else:
        print '!!! %s test failed' % n_fail

    if of is not None:
        of.stop()
        
    return n_fail, n_total, test_time

##
# 30.05.2007, c
# 01.06.2007
# 04.06.2007
# 19.07.2007
def wrap_run_tests( options ):
    def run_tests( stats, dir_name, filenames ):
        filenames = [filename for filename in sorted(filenames)
                     if (len( filename ) > 8) and
                     filename[:5] == 'test_' and filename[-3:] == '.py']

        print '<<< directory: %s, test files: %d' % (dir_name, len( filenames ))

        for filename in filenames:
            conf_name = op.join( dir_name, filename )

            n_fail, n_total, test_time = run_test( conf_name, options )

            stats[0] += 1
            stats[1] += n_fail
            stats[2] += n_total
            stats[3] += test_time

    return run_tests

def get_dir(default):
    if sfepy.in_source_tree:
        out = default
    else:
        out = op.normpath(op.join(sfepy.data_dir, default))
    return out

usage = """%prog [options] [test_filename[ test_filename ...]]"""

help = {
    'dir' : 'directory with tests [default: %default]',
    'out_dir' : 'directory for storing test results and temporary files'
    ' [default: %default]',
    'debug' : 'raise silenced exceptions to see what was wrong',
    'filter-none' : 'do not filter any messages',
    'filter-less' : 'filter output (suppress all except test messages)',
    'filter-more' : 'filter output (suppress all except test result messages)',
    'print-doc' : 'print the docstring of this file (howto write new tests)',
}

##
# c: 30.05.2007, r: 06.02.2008
def main():
    parser = OptionParser(usage = usage, version = "%prog " + sfepy.__version__)
    parser.add_option( "", "--print-doc",
                       action = "store_true", dest = "print_doc",
                       default = False, help = help['print-doc'] )
    parser.add_option( "-d", "--dir", metavar = 'directory',
                       action = "store", dest = "test_dir",
                       default = get_dir('tests'),
                       help = help['dir'] )
    parser.add_option( "-o", "--output", metavar = 'directory',
                       action = "store", dest = "out_dir",
                       default = get_dir('output-tests'),
                       help = help['out_dir'] )
    parser.add_option( "", "--debug",
                       action = "store_true", dest = "debug",
                       default = False, help = help['debug'] )
    parser.add_option( "", "--filter-none",
                       action = "store_true", dest = "filter_none",
                       default = False, help = help['filter-none'] )
    parser.add_option( "", "--filter-less",
                       action = "store_true", dest = "filter_less",
                       default = False, help = help['filter-less'] )
    parser.add_option( "", "--filter-more",
                       action = "store_true", dest = "filter_more",
                       default = False, help = help['filter-more'] )
    options, args = parser.parse_args()

    if options.print_doc:
        print __doc__
        return

    run_tests = wrap_run_tests(options)
    stats = [0, 0, 0, 0.0]

    if len(args) >= 1:
        for test_filename in args:
            dirname, filename = op.split(test_filename)
            run_tests(stats, dirname, [filename])

    else:
        op.walk( options.test_dir, run_tests, stats )

    print '%d test file(s) executed in %.2f s, %d failure(s) of %d test(s)'\
          % (stats[0], stats[3], stats[1], stats[2])

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = schroedinger
#!/usr/bin/env python
"""
Solver for basic electronic structure problems.

Examples
--------

Compute the 2D predefined problems::

  $ ./schroedinger.py --well
  $ ./schroedinger.py --oscillator
  $ ./schroedinger.py --hydrogen
  $ ./schroedinger.py --boron

See ``examples/quantum/`` directory for the problem description files
corresponding to the above examples.
"""
import os
from optparse import OptionParser

import sfepy
from sfepy.base.conf import ProblemConf, get_standard_keywords
from sfepy.physics.schroedinger_app import SchroedingerApp

def fix_path(filename):
    return os.path.join(sfepy.data_dir, filename)

usage = """%prog [options] [filename_in]\n""" + __doc__.rstrip()

help = {
    'conf' :
    'override problem description file items, written as python'
    ' dictionary without surrounding braces',
    'options' : 'override options item of problem description,'
    ' written as python dictionary without surrounding braces',
    'filename' :
    'basename of output file(s) [default: <basename of input file mesh>]',
    'well' :
    'solve infinite potential well (particle in a box) problem',
    'oscillator' :
    'solve spherically symmetric linear harmonic oscillator '
    '(1 electron) problem',
    'hydrogen' :
    'solve the hydrogen atom',
    'boron' :
    'solve the boron atom with 1 electron',
    'n_eigs' :
    'number of eigenpairs to compute [default: as set in the examples]',
    'tau' :
    'target value of the Pysparse eigenvalue solver. Eigenvalues in the'
    ' vicinity of tau will be computed [default: as set in the examples]',
}

def main():
    parser = OptionParser(usage=usage, version='%prog ' + sfepy.__version__)
    parser.add_option('-c', '--conf', metavar='"key : value, ..."',
                      action='store', dest='conf', type='string',
                      default=None, help= help['conf'])
    parser.add_option('-O', '--options', metavar='"key : value, ..."',
                      action='store', dest='app_options', type='string',
                      default=None, help=help['options'])
    parser.add_option('-o', '', metavar='filename',
                      action='store', dest='output_filename_trunk',
                      default=None, help=help['filename'])
    parser.add_option('--oscillator',
                      action='store_true', dest='oscillator',
                      default=False, help=help['oscillator'])
    parser.add_option('--well',
                      action='store_true', dest='well',
                      default=False, help=help['well'])
    parser.add_option('--hydrogen',
                      action='store_true', dest='hydrogen',
                      default=False, help=help['hydrogen'])
    parser.add_option('--boron',
                      action='store_true', dest='boron',
                      default=False, help=help['boron'])
    parser.add_option('-n', '--n-eigs', type='int', metavar='int',
                      action='store', dest='n_eigs',
                      default=None, help=help['n_eigs'])
    parser.add_option('-t', '--tau', type='float', metavar='float',
                      action='store', dest='tau',
                      default=None, help=help['tau'])

    options, args = parser.parse_args()

    if len(args) == 1:
        filename_in = args[0];

    elif len(args) == 0:
        if options.oscillator:
            filename_in = fix_path("examples/quantum/oscillator.py")

        elif options.well:
            filename_in = fix_path("examples/quantum/well.py")

        elif options.hydrogen:
            filename_in = fix_path("examples/quantum/hydrogen.py")

        elif options.boron:
            filename_in = fix_path("examples/quantum/boron.py")

        else:
            parser.print_help()
            return

    else:
        parser.print_help()
        return

    define_args = {}

    if options.n_eigs is not None:
        define_args['n_eigs'] = options.n_eigs

    if options.tau is not None:
        define_args['tau'] = options.tau

    required, other = get_standard_keywords()
    conf = ProblemConf.from_file_and_options(filename_in, options,
                                             required, other,
                                             define_args=define_args)

    app = SchroedingerApp(conf, options, 'schroedinger:')
    opts = conf.options
    if hasattr(opts, 'parametric_hook'): # Parametric study.
        parametric_hook = conf.get_function(opts.parametric_hook)
        app.parametrize(parametric_hook)
    app()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = blockgen
#!/usr/bin/env python
"""
Block mesh generator.
"""
import sys
sys.path.append('.')
from optparse import OptionParser

import numpy as nm

from sfepy.base.base import output
from sfepy.mesh.mesh_generators import gen_block_mesh
from sfepy.discrete.fem.meshio import MeshIO

usage = '%prog [options]\n' + __doc__.rstrip()

help = {
    'filename' :
    'output file name [default: %default]',
    'format' : 'output mesh format (overrides output file name extension)',
    'dims' :
    'dimensions  of the block [default: %default]',
    'shape' :
    'shape (counts of nodes in x, y, z) of the block [default: %default]',
    'centre' :
    'centre of the block [default: %default]',
    '2d' :
    'generate a 2D rectangular mesh, the third components of the above'
    ' options are ignored',
}

def main():
    parser = OptionParser(usage=usage, version='%prog')
    parser.add_option('-o', '', metavar='filename',
                      action='store', dest='output_filename',
                      default='out.vtk', help=help['filename'])
    parser.add_option('-f', '--format', metavar='format',
                      action='store', type='string', dest='format',
                      default=None, help=help['format'])
    parser.add_option('-d', '--dims', metavar='dims',
                      action='store', dest='dims',
                      default='[1.0, 1.0, 1.0]', help=help['dims'])
    parser.add_option('-s', '--shape', metavar='shape',
                      action='store', dest='shape',
                      default='[11, 11, 11]', help=help['shape'])
    parser.add_option('-c', '--centre', metavar='centre',
                      action='store', dest='centre',
                      default='[0.0, 0.0, 0.0]', help=help['centre'])
    parser.add_option('-2', '--2d',
                      action='store_true', dest='is_2d',
                      default=False, help=help['2d'])
    (options, args) = parser.parse_args()

    dim = 2 if options.is_2d else 3

    dims = nm.array(eval(options.dims), dtype=nm.float64)[:dim]
    shape = nm.array(eval(options.shape), dtype=nm.int32)[:dim]
    centre = nm.array(eval(options.centre), dtype=nm.float64)[:dim]

    output.prefix = 'blockgen:'
    output('dimensions:', dims)
    output('shape:', shape)
    output('centre:', centre)

    mesh = gen_block_mesh(dims, shape, centre, name=options.output_filename)

    io = MeshIO.for_format(options.output_filename, format=options.format,
                           writable=True)

    mesh.write(options.output_filename, io=io)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = convert_mesh
#!/usr/bin/env python
"""
Convert a mesh file from one SfePy-supported format to another.

Examples::

  $ ./script/convert_mesh.py meshes/3d/cylinder.mesh new.vtk
  $ ./script/convert_mesh.py meshes/3d/cylinder.mesh new.vtk -s2.5
  $ ./script/convert_mesh.py meshes/3d/cylinder.mesh new.vtk -s0.5,2,1
  $ ./script/convert_mesh.py meshes/3d/cylinder.mesh new.vtk -s0.5,2,1 -c 0
"""
import sys
sys.path.append('.')

from optparse import OptionParser
from sfepy.base.base import nm, output
from sfepy.discrete.fem import Mesh, FEDomain
from sfepy.discrete.fem.meshio import (output_writable_meshes, MeshIO,
                              supported_cell_types)

usage = '%prog [options] filename_in filename_out\n' + __doc__.rstrip()

help = {
    'scale' : 'scale factor (float or comma-separated list for each axis)'
    ' [default: %default]',
    'center' : 'center of the output mesh (0 for origin or'
    ' comma-separated list for each axis) applied after scaling'
    ' [default: %default]',
    'refine' : 'uniform refinement level [default: %default]',
    'format' : 'output mesh format (overrides filename_out extension)',
    'list' : 'list supported writable output mesh formats',
}

def _parse_val_or_vec(option, name, parser):
    if option is not None:
        try:
            try:
                option = float(option)
            except ValueError:
                option = [float(ii) for ii in option.split(',')]
            option = nm.array(option, dtype=nm.float64, ndmin=1)
        except:
            output('bad %s! (%s)' % (name, option))
            parser.print_help()
            sys.exit(1)

    return option

def main():
    parser = OptionParser(usage=usage)
    parser.add_option('-s', '--scale', metavar='scale',
                      action='store', dest='scale',
                      default=None, help=help['scale'])
    parser.add_option('-c', '--center', metavar='center',
                      action='store', dest='center',
                      default=None, help=help['center'])
    parser.add_option('-r', '--refine', metavar='level',
                      action='store', type=int, dest='refine',
                      default=0, help=help['refine'])
    parser.add_option('-f', '--format', metavar='format',
                      action='store', type='string', dest='format',
                      default=None, help=help['format'])
    parser.add_option('-l', '--list', action='store_true',
                      dest='list', help=help['list'])
    (options, args) = parser.parse_args()

    if options.list:
        output_writable_meshes()
        sys.exit(0)

    if len(args) != 2:
        parser.print_help()
        sys.exit(1)

    scale = _parse_val_or_vec(options.scale, 'scale', parser)
    center = _parse_val_or_vec(options.center, 'center', parser)

    filename_in, filename_out = args

    mesh = Mesh.from_file(filename_in)

    if scale is not None:
        if len(scale) == 1:
            tr = nm.eye(mesh.dim, dtype=nm.float64) * scale
        elif len(scale) == mesh.dim:
            tr = nm.diag(scale)
        else:
            raise ValueError('bad scale! (%s)' % scale)
        mesh.transform_coors(tr)

    if center is not None:
        cc = 0.5 * mesh.get_bounding_box().sum(0)
        shift = center - cc
        tr = nm.c_[nm.eye(mesh.dim, dtype=nm.float64), shift[:, None]]
        mesh.transform_coors(tr)

    if options.refine > 0:
        domain = FEDomain(mesh.name, mesh)
        output('initial mesh: %d nodes %d elements'
               % (domain.shape.n_nod, domain.shape.n_el))

        for ii in range(options.refine):
            output('refine %d...' % ii)
            domain = domain.refine()
            output('... %d nodes %d elements'
                   % (domain.shape.n_nod, domain.shape.n_el))

        mesh = domain.mesh

    io = MeshIO.for_format(filename_out, format=options.format,
                           writable=True)

    cell_types = ', '.join(supported_cell_types[io.format])
    output('writing [%s] %s...' % (cell_types, filename_out))
    mesh.write(filename_out, io=io)
    output('...done')

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = cylindergen
#!/usr/bin/env python
import sys
sys.path.append( '.' )
from optparse import OptionParser
from sfepy.mesh.mesh_generators import gen_cylinder_mesh
from sfepy.discrete.fem.meshio import MeshIO

usage = """%prog [options]

Cylinder mesh generator.
"""
help = {
    'filename' :
    'output file name [default: %default]',
    'format' : 'output mesh format (overrides output file name extension)',
    'axis' :
    'axis of the cylinder, one of x, y, z [default: %default]',
    'dims' :
    'dimensions of the cylinder: inner surface semi-axes a1, b1, outer'\
    ' surface semi-axes a2, b2, length [default: %default]',
    'shape' :
    'shape (counts of nodes in radial, circumferential and longitudinal'\
    ' directions) of the cylinder mesh [default: %default]',
    'centre' :
    'centre of the cylinder [default: %default]',
    'force_hollow' :
    'force hollow mesh even if inner radii a1 = b1 = 0',
    'is_open' :
    'generate an open cylinder segment',
    'open_angle' :
    'opening angle in radians [default: %default]',
    'non_uniform' :
    'space the mesh nodes in radial direction so that the element'\
    ' volumes are (approximately) the same, making thus the elements towards'\
    ' the outer surface thinner',
}

def main():
    parser = OptionParser( usage = usage, version = "%prog" )
    parser.add_option( "-o", "", metavar = 'filename',
                       action = "store", dest = "output_filename",
                       default = 'out.vtk', help = help['filename'] )
    parser.add_option('-f', '--format', metavar='format',
                      action='store', type='string', dest='format',
                      default=None, help=help['format'])
    parser.add_option( "-a", "--axis", metavar = 'axis',
                       action = "store", dest = "axis",
                       default = 'x', help = help['axis'] )
    parser.add_option( "-d", "--dims", metavar = 'dims',
                       action = "store", dest = "dims",
                       default = '[1.0, 1.0, 2.0, 2.0, 3.0]',
                       help = help['dims'] )
    parser.add_option( "-s", "--shape", metavar = 'shape',
                       action = "store", dest = "shape",
                       default = '[11, 11, 11]', help = help['shape'] )
    parser.add_option( "-c", "--centre", metavar = 'centre',
                       action = "store", dest = "centre",
                       default = '[0.0, 0.0, 0.0]', help = help['centre'] )
    parser.add_option( "", "--force-hollow",
                       action = "store_true", dest = "force_hollow",
                       default = False, help = help['force_hollow'] )
    parser.add_option( "", "--is-open",
                       action = "store_true", dest = "is_open",
                       default = False, help = help['is_open'] )
    parser.add_option( "", "--open-angle", metavar = 'angle', type='float',
                       action = "store", dest = "open_angle",
                       default = '0.0', help = help['open_angle'] )
    parser.add_option( "", "--non-uniform",
                       action = "store_true", dest = "non_uniform",
                       default = False, help = help['non_uniform'] )
    (options, args) = parser.parse_args()

    import numpy as nm
    dims = eval( "nm.array( %s, dtype = nm.float64 )" % options.dims )
    shape = eval( "nm.array( %s, dtype = nm.int32 )" % options.shape )
    centre = eval( "nm.array( %s, dtype = nm.float64 )" % options.centre )

    print dims
    print shape
    print centre

    mesh = gen_cylinder_mesh(dims, shape, centre,
                             axis=options.axis,
                             force_hollow=options.force_hollow,
                             is_open=options.is_open,
                             open_angle=options.open_angle,
                             non_uniform=options.non_uniform,
                             name=options.output_filename)

    io = MeshIO.for_format(options.output_filename, format=options.format,
                           writable=True)

    mesh.write(options.output_filename, io=io)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = edit_identifiers
#!/usr/bin/env python
"""
Convert mixedCase identifiers to under_scores.
"""
import sys, re, os

##
# Taken from http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/66009.
def cw2us(x): # capwords to underscore notation
#    return re.sub(r'(?<=[a-z])[A-Z]', r"_\g<0>", x).lower()
    return re.sub(r'(?<=[a-z])[A-Z]|(?<!^)[A-Z](?=[a-z])', r"_\g<0>", x).lower()

def mc2us(x): # mixed case to underscore notation
    return cw2us(x)

def us2mc(x): # underscore to mixed case notation
    return re.sub(r'_([a-z])', lambda m: (m.group(1).upper()), x)

def us2cw(x): # underscore to capwords notation
    s = us2mc(x)
    return s[0].upper()+s[1:]
##

misc = '[\'\"\[\]\(\)\{\}]'
mixed = '^%s*(?=[^A-Z])[a-z]+' % misc
match_candidate = re.compile( mixed ).match

def split_on( token, chars ):
    if len( chars ) == 1:
        return token.split( chars[0] )
    else:
        aux = token.split( chars[0] )
        out = []
        for item in aux:
            out.extend( split_on( item, chars[1:] ) )
        return out

def edit( line ):
    count = 0
    aux = line.split()
    if len( aux ) > 2:
        if aux[0] == 'from' and aux[2] == 'import':
            aux = aux[3:]
        elif aux[0] == 'import':
            aux = aux[3:]
        
    for token in aux:
        for item in split_on( token, '.,=*/+-_' ):
            if match_candidate( item ):
#                print item, cw2us( item )
                line = line.replace( item, cw2us( item ), 1 )
                count += 1
    return line, count

def main():

    write = True
    
    for name in sys.argv[1:]:
        print name
        
        rfd = open( name, 'r' )
        path = os.path.dirname( name )
        base = os.path.basename( name )
        new_name = os.path.join( path, 'new_' + base )
        if write:
            wfd = open( new_name, 'w' )

        n_edit = 0
        for line in rfd:
#            print line
            eline, count = edit( line )
#            print eline
            if write:
                wfd.write( eline )
            n_edit += count
#        raw_input()
        rfd.close()

        print '%d edit candidates' % n_edit

        if write:
            wfd.close()
            os.rename( new_name, name )
        
if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = eval_ns_forms
"""
Operators present in the FE discretization of (adjoint) Navier-Stokes terms.
"""
import sympy as s

def create_scalar(name, n_ep):
    vec = s.zeros(n_ep, 1)
    for ip in range(n_ep):
        vec[ip,0] = '%s%d' % (name, ip)
    return vec

def create_vector(name, n_ep, dim):
    """ordering is DOF-by-DOF"""
    vec = s.zeros(dim * n_ep, 1)
    for ii in range(dim):
        for ip in range(n_ep):
            vec[n_ep*ii+ip,0] = '%s%d%d' % (name, ii, ip)
    return vec

def create_scalar_base(name, n_ep):
    phi = s.zeros(1, n_ep)
    for ip in range(n_ep):
        phi[0,ip] = '%s%d' % (name, ip)
    return phi

def create_vector_base(name, phic, dim):
    n_ep = phic.shape[1]
    phi = s.zeros(dim, dim * n_ep)
    indx = []
    for ii in range(dim):
        phi[ii,n_ep*ii:n_ep*(ii+1)] = phic
        indx.append(ii)
    return phi, indx

def create_scalar_base_grad(name, phic, dim):
    n_ep = phic.shape[1]
    gc = s.zeros(dim, n_ep)
    for ii in range(dim):
        for ip in range(n_ep):
            gc[ii,ip] = '%s%d%d' % (name, ii, ip)
    return gc

def create_vector_base_grad(name, gc, transpose=False):
    dim, n_ep = gc.shape
    g = s.zeros(dim * dim, dim * n_ep)
    indx = []
    if transpose:
        for ir in range(dim):
            for ic in range(dim):
                g[dim*ir+ic,n_ep*ic:n_ep*(ic+1)] = gc[ir,:]
                indx.append((ic, ir))
    else:
        for ir in range(dim):
            for ic in range(dim):
                g[dim*ir+ic,n_ep*ir:n_ep*(ir+1)] = gc[ic,:]
                indx.append((ir, ic))
    return g, indx

def create_u_operator(u, transpose=False):
    dim = u.shape[0]
    op_u = s.zeros(dim * dim, dim)
    if transpose:
        for ir in range(dim):
            for ic in range(dim):
                op_u[dim*ir+ic,ic] = u[ir]
    else:
        for ii in range(dim):
            op_u[dim*ii:dim*(ii+1),ii] = u
    return op_u

def grad_vector_to_matrix(name, gv):
    dim2 = gv.shape[0]
    dim = int(s.sqrt(dim2))
    gm = s.zeros(dim, dim)
    for ir in range(dim):
        for ic in range(dim):
            gm[ir,ic] = gv[dim*ir+ic,0]
    return gm

def substitute_continuous(expr, names, u, phi):
    pu = phi * u
    for ii in range(phi.rows):
        expr = expr.subs(pu[ii,0], names[ii])
    return expr

def create_vector_var_data(name, phi, vindx, g, gt, vgindx, u):
    gu = g * u
    gum = grad_vector_to_matrix('gum', gu)
    print 'g %s:\n' % name, gum

    gut = gt * u
    gutm = grad_vector_to_matrix('gutm', gut)
    print 'gt %s:\n' % name, gutm

    pu = phi * u
    names = ['c%s%d' % (name, indx) for indx in vindx]
    cu = substitute_continuous(pu, names, u, phi)
    print 'continuous %s:\n' % name, cu

    gnames = ['cg%s%d_%d' % (name, indx[0], indx[1]) for indx in vgindx]
    cgu = substitute_continuous(gu, gnames, u, g)
    cgum = grad_vector_to_matrix('gum', cgu)
    print 'continuous g %s:\n' % name, cgum

    cgut = substitute_continuous(gut, gnames, u, g)
    cgutm = grad_vector_to_matrix('gutm', cgut)
    print 'continuous gt %s:\n' % name, cgutm

    op_u = create_u_operator(cu)
    print op_u

    op_ut = create_u_operator(cu, transpose=True)
    print op_ut

    out = {
        'g' : gu,
        'g_m' : gum,
        'q' : pu,
        'c' : cu,
        'cg' : cgu,
        'cg_m' : cgum,
        'cgt' : cgut,
        'cgt_m' : cgutm,
        'op' : op_u,
        'opt' : op_ut,
        'names' : names,
        'gnames' : gnames,
    }

    return out

def create_scalar_var_data(name, phi, g, u):
    gu = g * u

    pu = phi * u
    names = ['c%s' % name]
    cu = substitute_continuous(pu, names, u, phi)
    print 'continuous %s:\n' % name, cu

    gnames = ['cg%s_%d' % (name, ii) for ii in range(g.shape[0])]
    cgu = substitute_continuous(gu, gnames, u, g)
    print 'continuous g %s:\n' % name, cgu

    op_gu = create_u_operator(cgu)
    print op_gu

    out = {
        'g' : gu,
        'q' : pu,
        'c' : cu,
        'cg' : cgu,
        'gop' : op_gu,
        'names' : names,
        'gnames' : gnames,
    }

    return out

def main():
    n_ep = 3
    dim = 2

    u = create_vector('u', n_ep, dim)
    v = create_vector('v', n_ep, dim)
    b = create_vector('b', n_ep, dim)
    p = create_scalar('p', n_ep)
    q = create_scalar('q', n_ep)
    r = create_scalar('r', n_ep)

    print u
    print v

    phic = create_scalar_base('phic', n_ep)
    phi, vindx = create_vector_base('phi', phic, dim)
    gc = create_scalar_base_grad('gc', phic, dim)
    g, vgindx = create_vector_base_grad('g', gc)
    gt, aux = create_vector_base_grad('gt', gc, transpose=True)

    print phi
    print phic
    print gc
    print g
    print gt

    ud = create_vector_var_data('u', phi, vindx, g, gt, vgindx, u)
    vd = create_vector_var_data('v', phi, vindx, g, gt, vgindx, v)
    bd = create_vector_var_data('b', phi, vindx, g, gt, vgindx, b)
    pd = create_scalar_var_data('p', phic, gc, p)
    qd = create_scalar_var_data('q', phic, gc, q)
    rd = create_scalar_var_data('r', phic, gc, r)
    print ud.keys()

    assert bool(bd['op'].T * g == bd['opt'].T * gt)
    assert bool(bd['opt'].T * g == bd['op'].T * gt)
    assert bool(bd['cgt_m'] == bd['cg_m'].T)

    print '((b * grad) u), v)'
    form1 = vd['c'].T * bd['op'].T * ud['cg']
    form2 = vd['c'].T * bd['opt'].T * ud['cgt']
    print form1
    print form2
    print bool(form1 == form2)

    print '((v * grad) u), b)'
    form1 = vd['c'].T * bd['op'].T * ud['cgt']
    form2 = vd['c'].T * bd['opt'].T * ud['cg']
    print form1
    print form2
    print bool(form1 == form2)

    print '((u * grad) v), b)'
    form1 = vd['cgt'].T * bd['op'] * ud['c']
    form2 = vd['cg'].T * bd['opt'] * ud['c']
    print form1
    print form2
    print bool(form1 == form2)

    print '((b * grad) v), u)'
    form1 = vd['cg'].T * bd['op'] * ud['c']
    form2 = vd['cgt'].T * bd['opt'] * ud['c']
    print form1
    print form2
    print bool(form1 == form2)

    print '((v * grad) b), u)'
    form1 = vd['c'].T * bd['cgt_m'] * ud['c']
    form2 = vd['c'].T * bd['cg_m'].T * ud['c']
    print form1
    print form2
    print bool(form1 == form2)

    print '((b * grad) u), (b * grad) v)'
    form1 = vd['cg'].T * bd['op'] * bd['op'].T * ud['cg']
    print form1

    print '((u * grad) b), (b * grad) v)'
    form1 = vd['cg'].T * bd['op'] * bd['cg_m'] * ud['c']
    print form1

    print '(grad p, (b * grad) v)'
    form1 = vd['cg'].T * bd['op'] * pd['cg']
    print form1

    print '(grad q, (b * grad) u)'
    form1 = qd['cg'].T * bd['op'].T * ud['cg']
    print form1

    print '(grad q, (u * grad) b)'
    form1 = qd['cg'].T * bd['cg_m'] * ud['c']
    print form1

    print '(grad r, (u * grad) v)'
    form1 = vd['cgt'].T * rd['gop'] * ud['c']
    print form1

    return ud, vd, bd, pd, qd, rd

if __name__ == '__main__':
    ud, vd, bd, pd, qd, rd = main()

########NEW FILE########
__FILENAME__ = eval_tl_forms
"""
Operators present in the FE discretization of hyperelastic terms in the total
Lagrangian formulation.
"""
import sympy as s

def main():
    u1, u2, u3 = s.symbols(['u1', 'u2', 'u3'], commutative=False)
    u = s.Matrix([[u1], [u2], [u3]])

    g1, g2, g3 = s.symbols(['g1', 'g2', 'g3'], commutative=False)
    gc = s.Matrix([[g1], [g2], [g3]])

    aux = s.symbols(['u11', 'u12', 'u13', 'u21', 'u22', 'u23', 'u31', 'u32', 'u33'],
                    commutative=False)
    u11, u12, u13, u21, u22, u23, u31, u32, u33 = aux

    aux = s.symbols(['f11', 'f12', 'f13', 'f21', 'f22', 'f23', 'f31', 'f32', 'f33'],
                    commutative=False)
    f11, f12, f13, f21, f22, f23, f31, f32, f33 = aux

    print gc

    ## z = s.zeros((3, 1))

    ## g = s.Matrix([[gc, z, z],
    ##               [z, gc, z],
    ##               [z, z, gc]])

    g = s.Matrix([[g1, 0, 0],
                  [g2, 0, 0],
                  [g3, 0, 0],
                  [0, g1, 0],
                  [0, g2, 0],
                  [0, g3, 0],
                  [0, 0, g1],
                  [0, 0, g2],
                  [0, 0, g3]])

    print g
    print g * u


    h = s.Matrix([[1, 0, 0, 0, 0, 0, 0, 0, 0],
                  [0, 0, 0, 0, 1, 0, 0, 0, 0],
                  [0, 0, 0, 0, 0, 0, 0, 0, 1],
                  [0, 1, 0, 1, 0, 0, 0, 0, 0],
                  [0, 0, 1, 0, 0, 0, 1, 0, 0],
                  [0, 0, 0, 0, 0, 1, 0, 1, 0]])

    print h

    print 'linear part:'
    print h * g * u
    print h * g

    a = s.Matrix([[u11, 0, 0, u21, 0, 0, u31, 0, 0],
                  [0, u12, 0, 0, u22, 0, 0, u32, 0],
                  [0, 0, u13, 0, 0, u23, 0, 0, u33],
                  [u12, u11, 0, u22, u21, 0, u32, u31, 0],
                  [u13, 0, u11, u23, 0, u21, u33, 0, u31],
                  [0, u13, u12, 0, u23, u22, 0, u33, u32]])

    print a

    print (h + a) * g * u

    b = (h + a) * g

    s.pprint(b)

    a = s.Matrix([[u11+1, 0, 0, u21, 0, 0, u31, 0, 0],
                  [0, u12, 0, 0, u22+1, 0, 0, u32, 0],
                  [0, 0, u13, 0, 0, u23, 0, 0, u33+1],
                  [u12, u11+1, 0, u22+1, u21, 0, u32, u31, 0],
                  [u13, 0, u11+1, u23, 0, u21, u33+1, 0, u31],
                  [0, u13, u12, 0, u23, u22+1, 0, u33+1, u32]])
    print a

    print a * g * u

    b2 = a * g

    s.pprint(b2)

    print b == b2

    u11p, u22p, u33p = s.symbols(['u11p', 'u22p', 'u33p'], commutative=False)
    a = s.Matrix([[u11p, 0, 0, u21, 0, 0, u31, 0, 0],
                  [0, u12, 0, 0, u22p, 0, 0, u32, 0],
                  [0, 0, u13, 0, 0, u23, 0, 0, u33p],
                  [u12, u11p, 0, u22p, u21, 0, u32, u31, 0],
                  [u13, 0, u11p, u23, 0, u21, u33p, 0, u31],
                  [0, u13, u12, 0, u23, u22p, 0, u33p, u32]])
    print a

    print a * g * u

    b = a * g

    s.pprint(b)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = extract_surface
#!/usr/bin/env python
# 05.10.2005, c
"""
Given a mesh file, this script extracts its surface and prints it to stdout in
form of a list where each row is [element, face, component]. A component
corresponds to a contiguous surface region - for example, a cubical mesh with a
spherical hole has two surface components. Two surface faces sharing a single
node belong to one component.

With '-m' option, a mesh of the surface is created and saved in
'<original path>/surf_<original mesh file name>.mesh'.
"""
import sys
from optparse import OptionParser

import numpy as nm
import scipy.sparse as sp

import sfepy
from sfepy.base.base import output
from sfepy.base.ioutils import edit_filename
from sfepy.discrete.fem import Mesh, FEDomain
from sfepy.discrete.fem.extmods.cmesh import create_mesh_graph, graph_components

def _get_facets(vertices, offsets, ii, n_fp):
    facets = []
    for ic in range(n_fp):
        facets.append(vertices[offsets[ii] + ic][:, None])

    facets = nm.concatenate(facets, axis=1)

    return nm.ascontiguousarray(facets.astype(nm.int32))

def get_surface_faces(domain):
    cmesh = domain.cmesh
    faces = cmesh.get_surface_facets()
    vertices_f, offs_f = cmesh.get_incident(0, faces,
                                            cmesh.dim - 1, ret_offsets=True)

    n_fp = nm.diff(offs_f)
    surf_faces = []

    itri = nm.where(n_fp == 3)[0]
    if itri.size:
        surf_faces.append(_get_facets(vertices_f, offs_f, itri, 3))

    itet = nm.where(n_fp == 4)[0]
    if itet.size:
        surf_faces.append(_get_facets(vertices_f, offs_f, itet, 4))

    cells_c, offs_c = cmesh.get_incident(cmesh.dim, faces, cmesh.dim - 1,
                                         ret_offsets=True)
    ids = cmesh.get_local_ids(faces, cmesh.dim - 1, cells_c, offs_c,
                              cmesh.dim)
    lst = nm.c_[cells_c, ids]

    return lst, surf_faces

def surface_graph(surf_faces, n_nod):
    nnz, prow, icol = create_mesh_graph(n_nod, n_nod, len(surf_faces),
                                        surf_faces, surf_faces)
    data = nm.empty((nnz,), dtype=nm.int32)
    data.fill(2)
    return sp.csr_matrix((data, icol, prow), (n_nod, n_nod))

def surface_components(gr_s, surf_faces):
    """
    Determine surface components given surface mesh connectivity graph.
    """
    n_nod = gr_s.shape[0]
    n_comp, flag = graph_components(n_nod, gr_s.indptr, gr_s.indices)

    comps = []
    for ii, face in enumerate(surf_faces):
        comp = flag[face[:,0]]
        comps.append(comp)

    return n_comp, comps

usage = """%prog [options] filename_in|- filename_out|-

'-' is for stdin, stdout
""" + __doc__.rstrip()

def main():
    parser = OptionParser(usage=usage, version="%prog " + sfepy.__version__)
    parser.add_option("-m", "--mesh",
                      action="store_true", dest="save_mesh",
                      default=False,
                      help="save surface mesh")
    parser.add_option("-n", "--no-surface",
                      action="store_true", dest="no_surface",
                      default=False,
                      help="do not output surface [default: %default]")
    (options, args) = parser.parse_args()

    if (len(args) == 2):
        filename_in = args[0];
        filename_out = args[1];
    else:
        parser.print_help(),
        return

    if (filename_in == '-'):
        file_in = sys.stdin
    else:
        file_in = open(filename_in, "r");

    mesh = Mesh.from_file(filename_in)

    if (filename_in != '-'):
        file_in.close()

    domain = FEDomain('domain', mesh)
    domain.setup_groups()

    if domain.has_faces():
        domain.fix_element_orientation()

        lst, surf_faces = get_surface_faces(domain)

        surf_mesh = Mesh.from_surface(surf_faces, mesh)

        if options.save_mesh:
            aux = edit_filename(filename_in, prefix='surf_', new_ext='.mesh')
            surf_mesh.write(aux, io='auto')

        if options.no_surface:
            return

        gr_s = surface_graph(surf_faces, mesh.n_nod)

        n_comp, comps = surface_components(gr_s, surf_faces)
        output('number of surface components:', n_comp)

        ccs, comps = comps, nm.zeros((0,1), nm.int32)
        for cc in ccs:
            comps = nm.concatenate((comps, cc[:,nm.newaxis]), 0)

        out = nm.concatenate((lst, comps), 1)

        if (filename_out == '-'):
            file_out = sys.stdout
        else:
            file_out = open(filename_out, "w");
        for row in out:
            file_out.write('%d %d %d\n' % (row[0], row[1], row[2]))
        if (filename_out != '-'):
            file_out.close()

if __name__=='__main__':
    main()

########NEW FILE########
__FILENAME__ = gen_gallery
#!/usr/bin/env python
"""
Generate the images and rst files for gallery of SfePy examples.

The following steps need to be made to regenerate the documentation with the
updated example files:

1. Generate the files:

   - for sfepy.org deployment::

     $ ./script/gen_gallery.py -l ../doc-devel

   - for local test build run from ./::

     $ ./script/gen_gallery.py -l doc/_build/html/

2. remove doc/examples/::

   $ rm -rf doc/examples/

3. copy gallery/examples/ to doc/::

   $ cp -a gallery/examples/ doc/

4. regenerate the documentation::

   $ python setup.py htmldocs

Additional steps for sfepy.org deployment:

- copy doc/_build/html/ to <sfepy.org>/doc-devel/
- copy gallery/gallery.html and gallery/images/ to <sfepy.org>/
"""
import sys
sys.path.append( '.' )
import os
import tempfile
import glob
import re
from optparse import OptionParser

import matplotlib.image as image

import sfepy
from sfepy.base.base import (get_default, ordered_iteritems,
                             import_file, output, Struct)
from sfepy.base.ioutils import (ensure_path, locate_files, remove_files,
                                edit_filename)
from sfepy.postprocess.domain_specific import DomainSpecificPlot

omits = [
    'vibro_acoustic3d_mid.py',
    'linear_elastic_mM.py',
    'time_poisson_explicit.py',
    '__init__.py',
]

omit_dirs = [
    re.compile('.*output.*/').match,
]

custom = {
    'acoustics/acoustics3d.py' : {
        '_p_1' : {
            'view' : (-53, 120, 0.225, [0.021, 0.018, 0.066]),
            'roll' : -177,
        },
        '_p_2' : {
            'view' : (-112, 107, 0.32, [0.081, 0.042, 0.082]),
            'roll' : 111,
        },
    },
    'acoustics/vibro_acoustic3d.py' : {
        '_p1' : {
            'view' : (45.0, 54.7, 1.47, [0.325, 0.1, 0.05]),
            'roll' : -120,
        },
        '_p2' : {
            'view' : (45.0, 54.7, 1.47, [0.525, 0.1, 0.15]),
            'roll' : -120,
        },
        '_w' : {
            'view' : (0.0, 0.0, 0.86, [0.315, 0.1, 0.1]),
            'roll' : 0,
        },
        '_g0' : {
            'view' : (0.0, 0.0, 0.86, [0.315, 0.1, 0.1]),
            'roll' : 0,
        },
    },
    'diffusion/poisson_iga.py' : {
        '' : {
            'is_wireframe' : True,
            'domain_specific' : {
                't' : DomainSpecificPlot('plot_warp_scalar',
                                         ['rel_scaling=1']),
            },
            'view' : (55, 39, 7, [0.57,  0.91, -0.09]),
            'roll' : 15,
            'opacity' : {'wireframe' : 0.3},
        },
    },
    'diffusion/sinbc.py' : {
        '_t' : {
            'is_wireframe' : True,
            'domain_specific' : {
                't' : DomainSpecificPlot('plot_warp_scalar',
                                         ['rel_scaling=1']),
            },
            'view' : (-160, 33, 4, [0.5, 1.22, 0.05]),
            'roll' : 68,
            'opacity' : {'wireframe' : 0.3},
        },
        '_grad' : {
            'opacity' : {'surface' : 0.3},
            'view' : (-160, 33, 4, [0.5, 1.22, 0.05]),
            'roll' : 68,
        },
    },
    'linear_elasticity/elastic_contact_planes.py' : {
        '' : {
            'is_wireframe' : True,
            'domain_specific' : {
                'u' : DomainSpecificPlot('plot_displacements',
                                         ['rel_scaling=1']),
            },
            'view' : (-82, 47, 2.8, [-0.01, -0.02, -0.02]),
            'roll' : -8.4,
            'opacity' : {'wireframe' : 0.3},
        },
    },
    'linear_elasticity/elastic_contact_sphere.py' : {
        '' : {
            'is_wireframe' : True,
            'domain_specific' : {
                'u' : DomainSpecificPlot('plot_displacements',
                                         ['rel_scaling=1']),
            },
            'view' : (-82, 47, 2.8, [-0.01, -0.02, -0.02]),
            'roll' : -8.4,
            'opacity' : {'wireframe' : 0.3},
        },
    },
    'linear_elasticity/linear_elastic_iga.py' : {
        '' : {
            'is_wireframe' : True,
            'domain_specific' : {
                'u' : DomainSpecificPlot('plot_displacements',
                                         ['rel_scaling=1']),
            },
            'view' : (-113, 35, 1.7, [0.017, -0.05, -0.05]),
            'roll' : 40,
            'opacity' : {'wireframe' : 0.2},
        },
    },
    'navier_stokes/stokes_slip_bc.py' : {
        '' : {
            'view' : (-63, 52, 5.2, [-0.001,  0.52, -0.026]),
            'roll' : -32,
            'resolution' : (800, 600),
            'layout' : 'col',
            'rel_scaling' : 0.1,
        },
    },
    'thermo_elasticity/thermo_elasticity_ess.py' : {
        '' : {
            'is_wireframe' : True,
            'only_names' : ['u'],
            'domain_specific' : {
                'u' : DomainSpecificPlot('plot_displacements',
                                         ['rel_scaling=1000',
                                          'color_kind="scalars"',
                                          'color_name="T"']),
            },
            'view' : (-51, 71, 15, [1.16, 0.43, -1.56]),
            'roll' : -65,
            'opacity' : {'wireframe' : 0.3},
        },
    }
}

def _omit(filename):
    omit = False

    base = os.path.basename(filename)

    if base in omits:
        omit = True

    for omit_dir in omit_dirs:
        if omit_dir(filename) is not None:
            omit = True
            break

    return omit

def _get_fig_filenames(ebase, images_dir):
    fig_base = os.path.splitext(ebase)[0].replace(os.path.sep, '-')

    yield fig_base

    if ebase in custom:
        suffixes = sorted(custom[ebase].keys())
        for suffix in suffixes:
            fig_filename = os.path.join(images_dir, fig_base + suffix + '.png')
            yield fig_filename

    else:
        fig_filename = os.path.join(images_dir, fig_base + '.png')
        yield fig_filename

def _get_fig_filename(ebase, images_dir, suffix):
    fig_base = os.path.splitext(ebase)[0].replace(os.path.sep, '-')
    fig_filename = os.path.join(images_dir, fig_base + suffix + '.png')

    return fig_filename

def _make_sphinx_path(path, relative=False):
    if relative:
        aux = path.replace(sfepy.data_dir, '')
        prefix = ('..' + os.path.sep) * aux.count(os.path.sep)
        sphinx_path = prefix[:-1] + aux

    else:
        sphinx_path = path.replace(sfepy.data_dir, '/..')

    return sphinx_path

def generate_images(images_dir, examples_dir):
    """
    Generate images from results of running examples found in
    `examples_dir` directory.

    The generated images are stored to `images_dir`,
    """
    from sfepy.applications import solve_pde
    from sfepy.postprocess.viewer import Viewer
    from sfepy.postprocess.utils import mlab

    prefix = output.prefix

    output_dir = tempfile.mkdtemp()
    trunk = os.path.join(output_dir, 'result')
    options = Struct(output_filename_trunk=trunk,
                     output_format='vtk',
                     save_ebc=False,
                     save_ebc_nodes=False,
                     save_regions=False,
                     save_field_meshes=False,
                     save_regions_as_groups=False,
                     solve_not=False)
    default_views = {'' : {}}

    ensure_path(images_dir + os.path.sep)

    view = Viewer('', offscreen=False)

    for ex_filename in locate_files('*.py', examples_dir):
        if _omit(ex_filename): continue

        output.level = 0
        output.prefix = prefix
        ebase = ex_filename.replace(examples_dir, '')[1:]
        output('trying "%s"...' % ebase)

        try:
            problem, state = solve_pde(ex_filename, options=options)

        except KeyboardInterrupt:
            raise

        except:
            problem = None
            output('***** failed! *****')

        if problem is not None:
            if ebase in custom:
                views = custom[ebase]

            else:
                views = default_views

            tsolver = problem.get_time_solver()
            if tsolver.ts is None:
                suffix = None

            else:
                suffix = tsolver.ts.suffix % (tsolver.ts.n_step - 1)

            filename = problem.get_output_name(suffix=suffix)

            for suffix, kwargs in views.iteritems():
                fig_filename = _get_fig_filename(ebase, images_dir, suffix)

                fname = edit_filename(filename, suffix=suffix)
                output('displaying results from "%s"' % fname)
                disp_name = fig_filename.replace(sfepy.data_dir, '')
                output('to "%s"...' % disp_name.lstrip(os.path.sep))

                view.filename = fname
                view(scene=view.scene, show=False, is_scalar_bar=True, **kwargs)
                view.save_image(fig_filename)
                mlab.clf()

                output('...done')

            remove_files(output_dir)

        output('...done')

def generate_thumbnails(thumbnails_dir, images_dir, scale=0.3):
    """
    Generate thumbnails into `thumbnails_dir` corresponding to images in
    `images_dir`.
    """
    ensure_path(thumbnails_dir + os.path.sep)

    output('generating thumbnails...')
    filenames = glob.glob(os.path.join(images_dir, '*.png'))
    for fig_filename in filenames:
        ebase = fig_filename.replace(sfepy.data_dir, '').lstrip(os.path.sep)
        output('"%s"' % ebase)

        base = os.path.basename(fig_filename)
        thumb_filename = os.path.join(thumbnails_dir, base)

        image.thumbnail(fig_filename, thumb_filename, scale=scale)

    output('...done')

_index = """\
.. _%s-index:

%s examples
%s=========

.. toctree::
    :maxdepth: 2

"""

_image = '.. image:: %s'

_include = """\
.. _%s:

%s
%s

**Description**

%s

%s

:download:`source code <%s>`

.. literalinclude:: %s

"""

def generate_rst_files(rst_dir, examples_dir, images_dir):
    """
    Generate Sphinx rst files for examples in `examples_dir` with images
    in `images_dir` and put them into `rst_dir`.

    Returns
    -------
    dir_map : dict
        The directory mapping of examples and corresponding rst files.
    """
    ensure_path(rst_dir + os.path.sep)

    output('generating rst files...')

    dir_map = {}
    for ex_filename in locate_files('*.py', examples_dir):
        if _omit(ex_filename): continue

        ebase = ex_filename.replace(examples_dir, '')[1:]
        base_dir = os.path.dirname(ebase)

        rst_filename = os.path.basename(ex_filename).replace('.py', '.rst')

        dir_map.setdefault(base_dir, []).append((ex_filename, rst_filename))

    for dirname, filenames in dir_map.iteritems():
        filenames = sorted(filenames, cmp=lambda a, b: cmp(a[1], b[1]))
        dir_map[dirname ] = filenames

    # Main index.
    mfd = open(os.path.join(rst_dir, 'index.rst'), 'w')
    mfd.write(_index % ('sfepy', 'SfePy autogenerated gallery', '=' * 27))

    for dirname, filenames in ordered_iteritems(dir_map):
        full_dirname = os.path.join(rst_dir, dirname)
        ensure_path(full_dirname + os.path.sep)

        # Subdirectory index.
        ifd = open(os.path.join(full_dirname, 'index.rst'), 'w')
        ifd.write(_index % (dirname, dirname, '=' * len(dirname)))

        for ex_filename, rst_filename in filenames:
            full_rst_filename = os.path.join(full_dirname, rst_filename)
            output('"%s"' % full_rst_filename.replace(rst_dir, '')[1:])
            rst_filename_ns = rst_filename.replace('.rst', '')
            ebase = ex_filename.replace(examples_dir, '')[1:]

            rst_ex_filename = _make_sphinx_path(ex_filename)
            docstring = get_default(import_file(ex_filename).__doc__,
                                    'missing description!')

            ifd.write('    %s\n' % rst_filename_ns)
            fig_include = ''
            fig_base = _get_fig_filenames(ebase, images_dir).next()
            for fig_filename in _get_fig_filenames(ebase, images_dir):
                rst_fig_filename = _make_sphinx_path(fig_filename)

                if os.path.exists(fig_filename):
                    fig_include += _image % rst_fig_filename + '\n'

            # Example rst file.
            fd = open(full_rst_filename, 'w')
            fd.write(_include % (fig_base, ebase, '=' * len(ebase),
                                 docstring,
                                 fig_include,
                                 rst_ex_filename, rst_ex_filename))
            fd.close()

        ifd.close()

        mfd.write('    %s/index\n' % dirname)

    mfd.close()

    output('...done')

    return dir_map

_gallery_template_file = os.path.join(sfepy.top_dir,
                                      'doc/gallery_template.html')

_link_template = """\
<div class="figure">
<a class="reference external image-reference" href="../%s">
<img alt="%s" src="%s" />
</a>
<p class="caption">
<a class="reference internal" href="../%s"><em>%s</em></a>
</p>
</div>
<div class="toctree-wrapper compound">
</div>
"""
_side_links="<li><a class='reference internal' href='#%s'>%s</a></li>"
_div_line ="""\
<div class="section" id="%s">
<h2>%s<a class="headerlink" href="\#%s" title="Permalink to this headline">
</a></h2>
%s
<div style="clear: both"></div></div>
"""
def generate_gallery_html(examples_dir, output_filename, gallery_dir,
                          rst_dir, thumbnails_dir, dir_map, link_prefix):
    """
    Generate the gallery html file with thumbnail images and links to
    examples.

    Parameters
    ----------
    output_filename : str
        The output html file name.
    gallery_dir : str
        The top level directory of gallery files.
    rst_dir : str
        The full path to rst files of examples within `gallery_dir`.
    thumbnails_dir : str
        The full path to thumbnail images within `gallery_dir`.
    dir_map : dict
        The directory mapping returned by `generate_rst_files()`
    link_prefix : str, optional
        The prefix to prepend to links to individual pages of examples.
    """
    output('generating %s...' % output_filename)

    with open(_gallery_template_file, 'r') as fd:
        gallery_template = fd.read()

    div_lines=[]
    sidebar = []
    for dirname, filenames in ordered_iteritems(dir_map):
        full_dirname = os.path.join(rst_dir, dirname)
        dirnamenew = dirname.replace("_"," ")
        sidebarline = _side_links % (dirname, dirnamenew.title())
        lines = []
        for ex_filename, rst_filename in filenames:
            full_rst_filename = os.path.join(full_dirname, rst_filename)

            ebase = full_rst_filename.replace(rst_dir, '')[1:]
            ebase = edit_filename(ebase, new_ext='.py')

            link_base = full_rst_filename.replace(gallery_dir, '')[1:]
            link = os.path.join(link_prefix,
                                os.path.splitext(link_base)[0] + '.html')

            _get_fig_filenames(ebase, thumbnails_dir).next()
            for thumbnail_filename in _get_fig_filenames(ebase,
                                                         thumbnails_dir):
                if not os.path.isfile(thumbnail_filename):
                    # Skip examples with no image (= failed examples).
                    continue

                thumbnail_name = thumbnail_filename.replace(gallery_dir,
                                                            '')[1:]
                path_to_file = os.path.join(examples_dir,ebase)
                docstring = get_default(import_file(path_to_file).__doc__,
                                        'missing description!')
                docstring = docstring.replace('e.g.', 'eg:')
                docstring = docstring.split('.')
                line = _link_template % (link,os.path.splitext(ebase)[0],
                                         thumbnail_name,link,docstring[0]+'.')
                lines.append(line)

        if(len(lines)!=0):
            div_lines.append(_div_line % (dirname, dirnamenew.title(),
                                          dirname, '\n'.join(lines)))
            sidebar.append(sidebarline)

    fd = open(output_filename, 'w')
    fd.write(gallery_template % ((link_prefix,) * 7
                                 + ('\n'.join(sidebar), '\n'.join(div_lines))))
    fd.close()

    output('...done')

usage = '%prog [options]\n' + __doc__.rstrip()

help = {
    'examples_dir' :
    'directory containing examples [default: %default]',
    'images_dir' :
    'directory where to store gallery images [default: gallery/images]',
    'no_images' :
    'do not (re)generate images and thumbnails',
    'output_filename' :
    'output file name [default: %default]',
    'link_prefix' :
    'prefix to be prepended to links to examples pages in gallery '
    '[default: %default]',
}

def main():
    parser = OptionParser(usage=usage, version='%prog')
    parser.add_option('-e', '--examples-dir', metavar='directory',
                      action='store', dest='examples_dir',
                      default='examples', help=help['examples_dir'])
    parser.add_option('-i', '--images-dir', metavar='directory',
                      action='store', dest='images_dir',
                      default=None, help=help['images_dir'])
    parser.add_option('-n', '--no-images',
                      action='store_true', dest='no_images',
                      default=False, help=help['no_images'])
    parser.add_option('-o', '--output', metavar='output_filename',
                      action='store', dest='output_filename',
                      default='gallery/gallery.html',
                      help=help['output_filename'])
    parser.add_option('-l', '--link-prefix', metavar='prefix',
                      action='store', dest='link_prefix',
                      default='http://sfepy.org/doc-devel',
                      help=help['link_prefix'])
    (options, args) = parser.parse_args()

    examples_dir = os.path.realpath(options.examples_dir)

    output_filename = os.path.realpath(options.output_filename)
    gallery_dir = os.path.dirname(output_filename)

    images_dir = get_default(options.images_dir,
                             os.path.join(gallery_dir, 'images'))

    thumbnails_dir = os.path.join(images_dir, 'thumbnails')
    rst_dir = os.path.join(gallery_dir, 'examples')
    if not options.no_images:
        generate_images(images_dir, examples_dir)
        generate_thumbnails(thumbnails_dir, images_dir)

    dir_map = generate_rst_files(rst_dir, examples_dir, images_dir)

    generate_gallery_html(examples_dir,output_filename, gallery_dir,
                          rst_dir, thumbnails_dir, dir_map,
                          link_prefix=options.link_prefix)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = gen_iga_patch
#!/usr/bin/env python
"""
Generate a single IGA patch block in 2D or 3D of given degrees and continuity
using igakit.

The grid has equally-spaced knot vectors. The generated control points form a
regular grid as well - this prevents coarser resolution inside the block.
"""
from optparse import OptionParser
import numpy as nm

import igakit.cad as cad

from sfepy.base.base import output
import sfepy.discrete.iga as iga
import sfepy.discrete.iga.plot_nurbs as pn
import sfepy.discrete.iga.io as io

usage = '%prog [options]\n' + __doc__.rstrip()

helps = {
    'filename' :
    'output file name [default: block%dd.iga]',
    'dims' :
    'dimensions of the block [default: %default]',
    'centre' :
    'centre of the block [default: %default]',
    'shape' :
    'numbers of unique knot values along each axis [default: %default]',
    'degrees' :
    'NURBS degrees along each axis [default: %default]',
    'continuity' :
    'NURBS continuity along each axis [default: degrees-1]',
    '2d' :
    'generate a 2D block, the third components of the above'
    ' options are ignored',
    'plot' :
    'plot parametric, control and Bezier meshes as well as 1D basis slices',
    'label' :
    'label control and Bezier mesh points in figures',
}

def main():
    parser = OptionParser(usage=usage, version='%prog')
    parser.add_option('-o', '', metavar='filename',
                      action='store', dest='filename',
                      default=None, help=helps['filename'])
    parser.add_option('-d', '--dims', metavar='dims',
                      action='store', dest='dims',
                      default='[1.0, 1.0, 1.0]', help=helps['dims'])
    parser.add_option('-c', '--centre', metavar='centre',
                      action='store', dest='centre',
                      default='[0.0, 0.0, 0.0]', help=helps['centre'])
    parser.add_option('-s', '--shape', metavar='shape',
                      action='store', dest='shape',
                      default='[5, 5, 5]', help=helps['shape'])
    parser.add_option('', '--degrees', metavar='degrees',
                      action='store', dest='degrees',
                      default='[2, 2, 2]', help=helps['degrees'])
    parser.add_option('', '--continuity', metavar='continuity',
                      action='store', dest='continuity',
                      default=None, help=helps['continuity'])
    parser.add_option('-2', '--2d',
                      action='store_true', dest='is_2d',
                      default=False, help=helps['2d'])
    parser.add_option('-p', '--plot',
                      action='store_true', dest='plot',
                      default=False, help=helps['plot'])
    parser.add_option('-l', '--label',
                      action='store_true', dest='label',
                      default=False, help=helps['label'])
    (options, args) = parser.parse_args()

    dim = 2 if options.is_2d else 3

    filename = options.filename
    if filename is None:
        filename = 'block%dd.iga' % dim

    dims = nm.array(eval(options.dims), dtype=nm.float64)[:dim]
    centre = nm.array(eval(options.centre), dtype=nm.float64)[:dim]
    shape = nm.array(eval(options.shape), dtype=nm.int32)[:dim]
    degrees = nm.array(eval(options.degrees), dtype=nm.int32)[:dim]

    if options.continuity is None:
        continuity = degrees - 1

    else:
        continuity = nm.array(eval(options.continuity), dtype=nm.int32)[:dim]

    output('dimensions:', dims)
    output('centre:    ', centre)
    output('shape:     ', shape)
    output('degrees:   ', degrees)
    output('continuity:', continuity)
    output('->        :', filename)

    dd = centre - 0.5 * dims
    block = cad.grid(shape - 1, degree=degrees, continuity=continuity)

    for ia in xrange(dim):
        block.scale(dims[ia], ia)

    for ia in xrange(dim):
        block.translate(dd[ia], ia)

    # Force uniform control points. This prevents coarser resolution inside the
    # block.
    shape = nm.asarray(block.points.shape[:-1])
    n_nod = nm.prod(shape)
    x0 = centre - 0.5 * dims
    dd = dims / (shape - 1)

    ngrid = nm.mgrid[[slice(ii) for ii in shape]]
    ngrid.shape = (dim, n_nod)

    coors = x0 + ngrid.T * dd
    coors.shape = shape.tolist() + [dim]

    block.array[..., :dim] = coors

    # Compute Bezier extraction data.
    cs = iga.compute_bezier_extraction(block.knots, block.degree)
    n_els = [len(ii) for ii in cs]
    conn, bconn = iga.create_connectivity(n_els, block.knots, block.degree)

    ccs = iga.combine_bezier_extraction(cs)

    cps = block.points[..., :dim].copy()
    cps = cps.reshape((-1, dim))
    bcps, bweights = iga.compute_bezier_control(cps, block.weights.ravel(), ccs,
                                                conn, bconn)

    regions = iga.get_patch_box_regions(n_els, block.degree)

    io.write_iga_data(filename, block.knots, block.degree, cps,
                      block.weights.ravel(), cs, conn, bcps, bweights, bconn,
                      regions)

    if options.plot:
        pn.plt.rcParams['lines.linewidth'] = 2

        ax = pn.plot_parametric_mesh(None, block.knots)
        ax.set_title('parametric mesh')
        ax.axis('equal')

        points = block.points[..., :dim]
        ax = pn.plot_control_mesh(None, points, label=options.label)
        ax = pn.plot_iso_lines(ax, block)
        ax.set_title('control mesh and iso lines (blue)'
                     ' in Greville abscissae coordinates')
        ax.axis('equal')

        points = bcps
        ax = pn.plot_bezier_mesh(None, points, bconn, block.degree,
                                 label=options.label)
        ax = pn.plot_iso_lines(ax, block)
        ax.set_title('Bezier mesh and iso lines (blue)'
                     ' in Greville abscissae coordinates')
        ax.axis('equal')

        pn.plt.rcParams['lines.linewidth'] = 3

        line = block.extract(0, 0)
        if dim == 3:
            line = line.extract(0, 0)
        ax = pn.plot_nurbs_basis_1d(None, line, n_points=1000,
                                    legend=options.label)
        ax.set_xlabel('last parametric coordinate')
        ax.set_title('1D NURBS basis')

        ax = pn.plot_nurbs_basis_1d(None, line, n_points=1000, x_axis=dim-1,
                                    legend=options.label)
        ax.set_xlabel('last physical coordinate')
        ax.set_title('1D NURBS basis')

        pn.plt.show()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = gen_lobatto1d_c
#!/usr/bin/env python
"""
Generate lobatto1d.c and lobatto1h.c files.
"""
import sys
sys.path.append('.')
import os
from optparse import OptionParser

import sympy as sm
import numpy as nm
import matplotlib.pyplot as plt

from sfepy import top_dir
from sfepy.base.ioutils import InDir

hdef = 'float64 %s(float64 x);\n'

cdef = """
float64 %s(float64 x)
{
  return(%s);
}
"""

fun_list = """
const fun %s[%d] = {%s};
"""

def gen_lobatto(max_order):
    assert max_order > 2

    x = sm.symbols('x')

    lobs = [0, 1]
    lobs[0] = (1 - x) / 2
    lobs[1] = (1 + x) / 2

    dlobs = [lob.diff('x') for lob in lobs]

    legs = [sm.legendre(0, 'y')]
    clegs = [sm.ccode(legs[0])]
    dlegs = [sm.legendre(0, 'y').diff('y')]
    cdlegs = [sm.ccode(dlegs[0])]

    clobs = [sm.ccode(lob) for lob in lobs]
    cdlobs = [sm.ccode(dlob) for dlob in dlobs]

    denoms = [] # for lobs.

    for ii in range(2, max_order + 1):
        coef = sm.sympify('sqrt(2 * (2 * %s - 1)) / 2' % ii)
        leg = sm.legendre(ii - 1, 'y')

        pleg = leg.as_poly()
        coefs = pleg.all_coeffs()
        denom = max(sm.denom(val) for val in coefs)

        cleg = sm.ccode(sm.horner(leg*denom)/denom)

        dleg = leg.diff('y')
        cdleg = sm.ccode(sm.horner(dleg*denom)/denom)

        lob = sm.simplify(coef * sm.integrate(leg, ('y', -1, x)))
        lobnc = sm.simplify(sm.integrate(leg, ('y', -1, x)))

        plobnc = lobnc.as_poly()
        coefs = plobnc.all_coeffs()
        denom = sm.denom(coef) * max(sm.denom(val) for val in coefs)

        clob = sm.ccode(sm.horner(lob*denom)/denom)

        dlob = lob.diff('x')
        cdlob = sm.ccode(sm.horner(dlob*denom)/denom)

        legs.append(leg)
        clegs.append(cleg)
        dlegs.append(dleg)
        cdlegs.append(cdleg)
        lobs.append(lob)
        clobs.append(clob)
        dlobs.append(dlob)
        cdlobs.append(cdlob)
        denoms.append(denom)

    coef = sm.sympify('sqrt(2 * (2 * %s - 1)) / 2' % (max_order + 1))
    leg = sm.legendre(max_order, 'y')

    pleg = leg.as_poly()
    coefs = pleg.all_coeffs()
    denom = max(sm.denom(val) for val in coefs)

    cleg = sm.ccode(sm.horner(leg*denom)/denom)

    dleg = leg.diff('y')
    cdleg = sm.ccode(sm.horner(dleg*denom)/denom)

    legs.append(leg)
    clegs.append(cleg)
    dlegs.append(dleg)
    cdlegs.append(cdleg)

    kerns = []
    ckerns = []
    dkerns = []
    cdkerns = []
    for ii, lob in enumerate(lobs[2:]):
        kern = sm.simplify(lob / (lobs[0] * lobs[1]))
        dkern = kern.diff('x')

        denom = denoms[ii] / 4
        ckern = sm.ccode(sm.horner(kern*denom)/denom)
        cdkern = sm.ccode(sm.horner(dkern*denom)/denom)

        kerns.append(kern)
        ckerns.append(ckern)
        dkerns.append(dkern)
        cdkerns.append(cdkern)

    return (legs, clegs, dlegs, cdlegs,
            lobs, clobs, dlobs, cdlobs,
            kerns, ckerns, dkerns, cdkerns,
            denoms)

def plot_polys(fig, polys, var_name='x'):
    plt.figure(fig)
    plt.clf()

    x = sm.symbols(var_name)
    vx = nm.linspace(-1, 1, 100)

    for ii, poly in enumerate(polys):
        print ii
        print poly
        print poly.as_poly(x).all_coeffs()

        vy = [float(poly.subs(x, xx)) for xx in vx]
        plt.plot(vx, vy)

def append_declarations(out, cpolys, comment, cvar_name, shift=0):
    names = []
    out.append('\n// %s functions.\n' % comment)
    for ii, cpoly in enumerate(cpolys):
        name = '%s_%03d' % (cvar_name, ii + shift)
        function = hdef % name
        out.append(function)
        names.append(name)

    return names

def append_polys(out, cpolys, comment, cvar_name, var_name='x', shift=0):
    names = []
    out.append('\n// %s functions.\n' % comment)
    for ii, cpoly in enumerate(cpolys):
        name = '%s_%03d' % (cvar_name, ii + shift)
        function = cdef % (name, cpoly.replace(var_name, 'x'))
        out.append(function)
        names.append(name)

    return names

def append_lists(out, names, length):
    args = ', '.join(['&%s' % name for name in names])
    name = names[0][:-4]
    _list = fun_list % (name, length, args)
    out.append(_list)

usage = '%prog [options]\n' + __doc__.rstrip()

help = {
    'max_order' :
    'maximum order of polynomials [default: %default]',
    'plot' :
    'plot polynomials',
}

def main():
    parser = OptionParser(usage=usage, version='%prog')
    parser.add_option('-m', '--max-order', metavar='order', type=int,
                      action='store', dest='max_order',
                      default=10, help=help['max_order'])
    parser.add_option('', '--plot',
                      action='store_true', dest='plot',
                      default=False, help=help['plot'])
    options, args = parser.parse_args()

    max_order = options.max_order

    (legs, clegs, dlegs, cdlegs,
     lobs, clobs, dlobs, cdlobs,
     kerns, ckerns, dkerns, cdkerns,
     denoms) = gen_lobatto(max_order)

    if options.plot:
        plot_polys(1, lobs)
        plot_polys(11, dlobs)

        plot_polys(2, kerns)
        plot_polys(21, dkerns)

        plot_polys(3, legs, var_name='y')
        plot_polys(31, dlegs, var_name='y')

        plt.show()

    indir = InDir(os.path.join(top_dir, 'sfepy.discrete.fem.extmods/'))

    fd = open(indir('lobatto1d_template.h'), 'r')
    template = fd.read()
    fd.close

    fd = open(indir('lobatto1d.h'), 'w')

    out = []

    append_declarations(out, clobs, 'Lobatto', 'lobatto')
    append_declarations(out, cdlobs, 'Derivatives of Lobatto', 'd_lobatto')

    append_declarations(out, ckerns, 'Kernel', 'kernel',
                        shift=2)
    append_declarations(out, cdkerns, 'Derivatives of kernel', 'd_kernel',
                        shift=2)

    append_declarations(out, clegs, 'Legendre', 'legendre')
    append_declarations(out, cdlegs, 'Derivatives of Legendre', 'd_legendre')

    fd.write(template.replace('// REPLACE_TEXT', ''.join(out)))

    fd.close()


    fd = open(indir('lobatto1d_template.c'), 'r')
    template = fd.read()
    fd.close()

    fd = open(indir('lobatto1d.c'), 'w')

    out = []

    names_lobatto = append_polys(out, clobs,
                                 'Lobatto', 'lobatto')
    names_d_lobatto = append_polys(out, cdlobs,
                                   'Derivatives of Lobatto', 'd_lobatto')

    names_kernel = append_polys(out, ckerns,
                                'Kernel', 'kernel',
                                shift=2)
    names_d_kernel = append_polys(out, cdkerns,
                                  'Derivatives of kernel', 'd_kernel',
                                  shift=2)

    names_legendre = append_polys(out, clegs,
                                  'Legendre', 'legendre',
                                  var_name='y')
    names_d_legendre = append_polys(out, cdlegs,
                                    'Derivatives of Legendre', 'd_legendre',
                                    var_name='y')

    out.append('\n// Lists of functions.\n')

    out.append('\nconst int32 max_order = %d;\n' % max_order)

    append_lists(out, names_lobatto, max_order + 1)
    append_lists(out, names_d_lobatto, max_order + 1)

    append_lists(out, names_kernel, max_order - 1)
    append_lists(out, names_d_kernel, max_order - 1)

    append_lists(out, names_legendre, max_order + 1)
    append_lists(out, names_d_legendre, max_order + 1)

    fd.write(template.replace('// REPLACE_TEXT', ''.join(out)))

    fd.close()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = gen_mesh_prev
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Mesh Preview Generator.

Examples
--------

$ ./script/gen_mesh_prev.py meshes/2d/
"""
from optparse import OptionParser
import sys
sys.path.append('.')
import os
import vtk
from sfepy.discrete.fem import Mesh

def gen_shot(vtk_filename, png_filename):
    """
    Generate PNG image of the FE mesh.

    Parameters
    ----------
    vtk_filename : str
        The input mesh filename (file in VTK format).

    png_filename : str
        The name of the output PNG file.
    """

    reader = vtk.vtkUnstructuredGridReader()
    reader.SetFileName(vtk_filename)
    reader.Update()
    bnd = reader.GetOutput().GetPoints().GetBounds()

    surface0 = vtk.vtkDataSetSurfaceFilter()
    surface0.SetInput(reader.GetOutput())
    surface0.Update()

    if abs(bnd[5] - bnd[4]) > 1.0e-12:
        tr = vtk.vtkTransform()
        tr.RotateWXYZ(45,1,1,1)

        trFilter = vtk.vtkTransformPolyDataFilter()
        trFilter.SetTransform(tr)
        trFilter.SetInputConnection(surface0.GetOutputPort())
        trFilter.Update()
        surface = trFilter

    else:
        surface = surface0

    ca,cb = surface.GetOutput().GetCellData().GetScalars().GetRange()

    lut = vtk.vtkLookupTable()
    lut.SetHueRange(0.667, 0.667)
    lut.SetSaturationRange(0.0, 1.0)
    lut.SetValueRange(0.8, 1.0)
    lut.SetAlphaRange(1.0, 1.0)
    lut.SetTableRange(ca,cb)

    gf = vtk.vtkGraphicsFactory()
    gf.SetOffScreenOnlyMode(1)
    gf.SetUseMesaClasses(1)

    ifa = vtk.vtkImagingFactory()
    ifa.SetUseMesaClasses(1)

    mapper = vtk.vtkPolyDataMapper()
    mapper.SetLookupTable(lut)
    mapper.SetScalarRange(ca,cb);
    mapper.SetInput(surface.GetOutput())
    mapper.SetScalarModeToUseCellData()

    actor = vtk.vtkActor()
    actor.SetMapper(mapper)

    mapper2 = vtk.vtkPolyDataMapper()
    mapper2.SetInput(surface.GetOutput())
    actor2 = vtk.vtkActor()
    actor2.SetMapper(mapper2)
    actor2.GetProperty().SetRepresentationToWireframe()

    ren = vtk.vtkRenderer()
    renWin = vtk.vtkRenderWindow()
    renWin.SetOffScreenRendering(1)
    renWin.AddRenderer(ren)
    ren.AddActor(actor)
    ren.AddActor(actor2)
    renWin.Render()

    image = vtk.vtkWindowToImageFilter()
    image.SetInput(renWin)
    image.Update()

    base, _ = os.path.splitext(vtk_filename)
    writer = vtk.vtkPNGWriter()
    writer.SetFileName(png_filename)
    writer.SetInput(image.GetOutput())
    writer.Write()

usage = '%prog [options] mesh_dir\n' + __doc__.rstrip()

def main():
    parser = OptionParser(usage=usage)
    (options, args) = parser.parse_args()

    if len(args) != 1:
        parser.print_help()
        sys.exit(1)

    mesh_dir = args[0]

    mesh_files = []
    for (dirpath, dirnames, filenames) in os.walk(mesh_dir):
        for ii in filenames:
            _, ext = os.path.splitext(ii)
            if ext.lower() in ['.mesh', '.vtk']:
                mesh_files.append(dirpath + os.path.sep + ii)

    for ii in mesh_files:
        base, ext = os.path.splitext(ii)
        fname_out = base + '.png'
        if ext == '.mesh':
            fname_in = 'aux.vtk'
            mesh = Mesh.from_file(ii)
            mesh.write(fname_in, io='auto')

        else:
            fname_in = ii

        print('writing %s...' % fname_out)
        gen_shot(fname_in, fname_out)

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = gen_term_table
#!/usr/bin/env python
import os
import sys
from optparse import OptionParser
import pyparsing as pp

import numpy as nm

sys.path.append( '.' )
import sfepy.discrete.fem # Hack: fix circular dependency, as terms.pyx imports
                          # from sfepy.discrete.fem
from sfepy.terms import term_table

def set_section(sec):
    def action(str, loc, toks):
        if toks:
            sec[0] = toks[0][1:-1]
        return toks
    return action

def to_list(slist, sec):
    def action(str, loc, toks):
        if toks:
            slist.append((sec[0], toks[0]))
        return toks
    return action
    
def create_parser(slist, current_section):
    
    colon = pp.Literal(':')

    section = pp.Combine(colon
                         + pp.Word(pp.alphas, pp.alphanums + '_ ')
                         + colon)
    section.setParseAction(set_section(current_section))
    section.setName('section')

    text = pp.SkipTo(section | pp.StringEnd())
    text.setParseAction(to_list(slist, current_section))
    text.setName('text')

    doc = pp.StringStart()\
           + pp.Optional(text) + pp.ZeroOrMore(section + text)\
           + pp.StringEnd()

    return doc

header = """
.. tabularcolumns:: |p{0.3\linewidth}|p{0.2\linewidth}|p{0.5\linewidth}|
.. list-table:: Table of all terms.
   :widths: 30 30 40
   :header-rows: 1

   * - name/class/link
     - arguments
     - definition
"""

table_row = """   * - %s

         :class:`%s`
         :mod:`%s <%s>`
     - %s
     -
%s
"""

def format_next(text, new_text, pos, can_newline, width, ispaces):
    new_len = len(new_text)

    if (pos + new_len > width) and can_newline:
        text += '\n' + ispaces + new_text
        pos = new_len
        can_newline = False

    else:
        if pos > 0:
            text += ' ' + new_text
            pos += new_len + 1

        else:
            text += new_text
            pos += new_len

        can_newline = True

    return text, pos, can_newline

def typeset_to_indent(txt, indent0, indent, width):
    if not len(txt): return txt

    txt_lines = txt.strip().split('\n')

    ispaces = ' ' * indent
    text = (' ' * indent0) + txt_lines[0] + '\n' + ispaces

    can_newline = False
    pos = indent0
    for line in txt_lines[1:]:
        for word in line.split():
            text, pos, can_newline = format_next(text, word, pos, can_newline,
                                                 width, ispaces)

    return text

def typeset_term_syntax(term_class):
    if ((len(term_class.arg_types) > 1) and not
        isinstance(term_class.arg_types[0], str)):
        arg_types = [', '.join(['<%s>' % arg for arg in arg_type])
                     for arg_type in term_class.arg_types]
        arg_types = [' ``%s``' % arg_type for arg_type in arg_types]
        text = '\n\n       '.join(arg_types)

    else:
        text = ', '.join(['<%s>' % arg for arg in term_class.arg_types])
        text = '``%s``' % text
    return text

def typeset_term_table(fd, table):
    """Terms are sorted by name without the d*_ prefix."""
    sec_list = []
    current_section = ['']
    parser = create_parser(sec_list, current_section)

    fd.write(header)

    keys = table.keys()
    sort_keys = [key[key.find( '_' ):] for key in keys]
    iis = nm.argsort(sort_keys)
    for ii in iis:
        key = keys[ii]
        item_class = table[key]
        doc = item_class.__doc__

        if doc is not None:
            sec_list[:] = []
            current_section[0] = ''
            out = parser.parseString(doc)

            dd = [x[1] for x in sec_list if x[0].lower() == 'definition']
            if len(dd):
                dd = dd[0]
            else:
                dd = ''

            dds = dd.split('\n\n')
            definition = '\n\n'.join(typeset_to_indent(dd, 7, 11, 65)
                                     for dd in dds)

            fd.write(table_row % (item_class.name,
                                  item_class.__name__,
                                  item_class.__module__,
                                  'sfepy.terms.' + item_class.__module__,
                                  typeset_term_syntax(item_class),
                                  definition))

    fd.write('\n')

def typeset(filename):
    """Utility function called by sphinx. """
    fd = open(filename, 'w')
    typeset_term_table(fd, term_table)
    fd.close()

def gen_term_table(app):
    typeset(os.path.join(app.builder.srcdir, 'term_table.rst'))

def setup(app):
    app.connect('builder-inited', gen_term_table)

usage = """%prog [options]

Generate the table of all terms for the sphinx documentation.
"""
help = {
    'output_filename' :
    'output file name',
}

def main():

    parser = OptionParser(usage=usage, version="%prog")
    parser.add_option("-o", "--output", metavar='output_filename',
                      action="store", dest="output_filename",
                      default="term_table.rst", help=help['output_filename'])
    (options, args) = parser.parse_args()

    typeset(options.output_filename)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = plot_condition_numbers
#!/usr/bin/env python
"""
Plot conditions numbers w.r.t. polynomial approximation order of reference
element matrices for various FE polynomial spaces (bases).
"""
from optparse import OptionParser
import time
import numpy as nm
import matplotlib.pyplot as plt

from sfepy import data_dir
from sfepy.base.base import output, assert_
from sfepy.discrete import FieldVariable, Material, Integral
from sfepy.discrete.fem import Mesh, FEDomain, Field
from sfepy.terms import Term
from sfepy.solvers import eig

usage = '%prog [options]\n' + __doc__.rstrip()

help = {
    'basis' :
    'name of the FE basis [default: %default]',
    'max_order' :
    'maximum order of polynomials [default: %default]',
    'matrix_type' :
    'matrix type, one of "elasticity", "laplace" [default: %default]',
    'geometry' :
    'reference element geometry, one of "2_3", "2_4", "3_4", "3_8"'
    ' [default: %default]',
}

def main():
    parser = OptionParser(usage=usage, version='%prog')
    parser.add_option('-b', '--basis', metavar='name',
                      action='store', dest='basis',
                      default='lagrange', help=help['basis'])
    parser.add_option('-n', '--max-order', metavar='order', type=int,
                      action='store', dest='max_order',
                      default=10, help=help['max_order'])
    parser.add_option('-m', '--matrix', metavar='type',
                      action='store', dest='matrix_type',
                      default='laplace', help=help['matrix_type'])
    parser.add_option('-g', '--geometry', metavar='name',
                      action='store', dest='geometry',
                      default='2_4', help=help['geometry'])
    options, args = parser.parse_args()

    dim, n_ep = int(options.geometry[0]), int(options.geometry[2])
    output('reference element geometry:')
    output('  dimension: %d, vertices: %d' % (dim, n_ep))

    n_c = {'laplace' : 1, 'elasticity' : dim}[options.matrix_type]

    output('matrix type:', options.matrix_type)
    output('number of variable components:',  n_c)

    output('polynomial space:', options.basis)

    output('max. order:', options.max_order)

    mesh = Mesh.from_file(data_dir + '/meshes/elements/%s_1.mesh'
                          % options.geometry)
    domain = FEDomain('domain', mesh)
    omega = domain.create_region('Omega', 'all')

    orders = nm.arange(1, options.max_order + 1, dtype=nm.int)
    conds = []

    order_fix = 0 if  options.geometry in ['2_4', '3_8'] else 1

    for order in orders:
        output('order:', order, '...')

        field = Field.from_args('fu', nm.float64, n_c, omega,
                                approx_order=order,
                                space='H1', poly_space_base=options.basis)

        to = field.approx_order
        quad_order = 2 * (max(to - order_fix, 0))
        output('quadrature order:', quad_order)

        integral = Integral('i', order=quad_order)
        qp, _ = integral.get_qp(options.geometry)
        output('number of quadrature points:', qp.shape[0])

        u = FieldVariable('u', 'unknown', field)
        v = FieldVariable('v', 'test', field, primary_var_name='u')

        m = Material('m', lam=1.0, mu=1.0)

        if options.matrix_type == 'laplace':
            term = Term.new('dw_laplace(m.mu, v, u)',
                            integral, omega, m=m, v=v, u=u)
            n_zero = 1

        else:
            assert_(options.matrix_type == 'elasticity')
            term = Term.new('dw_lin_elastic_iso(m.lam, m.mu, v, u)',
                            integral, omega, m=m, v=v, u=u)
            n_zero = (dim + 1) * dim / 2

        term.setup()

        output('assembling...')
        tt = time.clock()
        mtx, iels = term.evaluate(mode='weak', diff_var='u')
        output('...done in %.2f s' % (time.clock() - tt))
        mtx = mtx[0][0, 0]

        try:
            assert_(nm.max(nm.abs(mtx - mtx.T)) < 1e-10)

        except:
            from sfepy.base.base import debug; debug()

        output('matrix shape:', mtx.shape)

        eigs = eig(mtx, method='eig.sgscipy', eigenvectors=False)
        eigs.sort()

        # Zero 'true' zeros.
        eigs[:n_zero] = 0.0

        ii = nm.where(eigs < 0.0)[0]
        if len(ii):
            output('matrix is not positive semi-definite!')

        ii = nm.where(eigs[n_zero:] < 1e-12)[0]
        if len(ii):
            output('matrix has more than %d zero eigenvalues!' % n_zero)

        output('smallest eigs:\n', eigs[:10])

        ii = nm.where(eigs > 0.0)[0]
        emin, emax = eigs[ii[[0, -1]]]

        output('min:', emin, 'max:', emax)

        cond = emax / emin
        conds.append(cond)

        output('condition number:', cond)

        output('...done')

    plt.figure(1)
    plt.semilogy(orders, conds)
    plt.xticks(orders, orders)
    plt.xlabel('polynomial order')
    plt.ylabel('condition number')
    plt.grid()

    plt.figure(2)
    plt.loglog(orders, conds)
    plt.xticks(orders, orders)
    plt.xlabel('polynomial order')
    plt.ylabel('condition number')
    plt.grid()

    plt.show()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = plot_logs
#!/usr/bin/env python
"""
Plot logs of variables saved in a text file by sfepy.base.log.Log class.

The plot should be almost the same as the plot that would be generated by the
Log directly.
"""
from optparse import OptionParser

import matplotlib.pyplot as plt

from sfepy.base.log import read_log, plot_log

usage = '%prog [options] filename\n' + __doc__.rstrip()

def parse_rc(option, opt, value, parser):
    pars = {}
    for pair in value.split(','):
        key, val = pair.split('=')
        pars[key] = eval(val)

    setattr(parser.values, option.dest, pars)

helps = {
    'output_filename' :
    'save the figure using the given file name',
    'rc' : 'matplotlib resources',
    'no_show' :
    'do not show the figure',
}

def main():
    parser = OptionParser(usage=usage)
    parser.add_option('-o', '--output', metavar='filename',
                      action='store', dest='output_filename',
                      default=None, help=helps['output_filename'])
    parser.add_option('--rc', type='str', metavar='key=val,...',
                      action='callback', dest='rc',
                      callback=parse_rc, default={}, help=helps['rc'])
    parser.add_option('-n', '--no-show',
                      action='store_true', dest='no_show',
                      default=False, help=helps['no_show'])
    options, args = parser.parse_args()

    if len(args) == 1:
        filename = args[0]

    else:
        parser.print_help()
        return

    log, info = read_log(filename)

    plt.rcParams.update(options.rc)

    plot_log(1, log, info)

    if options.output_filename:
        plt.savefig(options.output_filename)

    if not options.no_show:
        plt.show()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = plot_mesh
#!/usr/bin/env python
"""
Plot mesh connectivities, facet orientations, global and local DOF ids etc.
"""
from optparse import OptionParser

from sfepy.base.base import output
from sfepy.discrete.fem import Mesh, FEDomain
import sfepy.postprocess.plot_cmesh as pc

usage = '%prog [options] filename\n' + __doc__.rstrip()

helps = {
}

def main():
    parser = OptionParser(usage=usage, version='%prog')
    options, args = parser.parse_args()

    if len(args) == 1:
        filename = args[0]
    else:
        parser.print_help(),
        return

    mesh = Mesh.from_file(filename)
    output('Mesh:')
    output('  dimension: %d, vertices: %d, elements: %d'
           % (mesh.dim, mesh.n_nod, mesh.n_el))

    domain = FEDomain('domain', mesh)
    output(domain.cmesh)
    domain.cmesh.cprint(1)
    dim = domain.cmesh.dim

    ax = pc.plot_wireframe(None, domain.cmesh)

    ax = pc.plot_entities(ax, domain.cmesh, 0, 'k')
    ax = pc.label_global_entities(ax, domain.cmesh, 0, 'k', 12)
    ax = pc.label_local_entities(ax, domain.cmesh, 0, 'k', 8)

    ax = pc.plot_entities(ax, domain.cmesh, 1, 'b')
    ax = pc.label_global_entities(ax, domain.cmesh, 1, 'b', 12)
    ax = pc.label_local_entities(ax, domain.cmesh, 1, 'b', 8)

    if dim == 3:
        ax = pc.plot_entities(ax, domain.cmesh, 2, 'g')
        ax = pc.label_global_entities(ax, domain.cmesh, 2, 'g', 12)
        ax = pc.label_local_entities(ax, domain.cmesh, 2, 'g', 8)

    ax = pc.plot_entities(ax, domain.cmesh, dim, 'r')
    ax = pc.label_global_entities(ax, domain.cmesh, dim, 'r', 12)

    pc.plt.show()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = plot_quadratures
#!/usr/bin/env python
"""
Plot quadrature points for the given geometry and integration order.
"""
from optparse import OptionParser

import sfepy.postprocess.plot_quadrature as pq

usage = '%prog [options]\n' + __doc__.rstrip()

helps = {
    'geometry' :
    'reference element geometry, one of "2_3", "2_4", "3_4", "3_8"'
    ' [default: %default]',
    'order' :
    'quadrature order [default: %default]',
    'min_radius' :
    'min. radius of points corresponding to the min. weight'
    ' [default:  %default]',
    'max_radius' :
    'max. radius of points corresponding to the max. weight'
    ' [default:  %default]',
    'show_colorbar' :
    'show colorbar for quadrature weights'
}

def main():
    parser = OptionParser(usage=usage, version='%prog')
    parser.add_option('-g', '--geometry', metavar='name',
                      action='store', dest='geometry',
                      default='2_4', help=helps['geometry'])
    parser.add_option('-n', '--order', metavar='order', type=int,
                      action='store', dest='order',
                      default=2, help=helps['order'])
    parser.add_option('-r', '--min-radius', metavar='float', type=float,
                      action='store', dest='min_radius',
                      default=10, help=helps['min_radius'])
    parser.add_option('-R', '--max-radius', metavar='float', type=float,
                      action='store', dest='max_radius',
                      default=50, help=helps['max_radius'])
    parser.add_option('-c', '--show-colorbar',
                      action='store_true', dest='show_colorbar',
                      default=False, help=helps['show_colorbar'])
    options, args = parser.parse_args()

    if len(args) != 0:
        parser.print_help(),
        return

    pq.plot_quadrature(None, options.geometry, options.order,
                       options.min_radius, options.max_radius,
                       options.show_colorbar)
    pq.plt.show()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = plot_times
#!/usr/bin/env python
"""
Plot time steps, times of time steps and time deltas in a HDF5 results file.
"""
from optparse import OptionParser

import numpy as nm
import matplotlib.pyplot as plt

from sfepy.postprocess.time_history import extract_times

usage = '%prog [options] filename\n' + __doc__.rstrip()

helps = {
    'logarithmic' :
    'plot time steps in logarithmic scale',
}

def main():
    parser = OptionParser(usage=usage, version='%prog')
    parser.add_option('-l', '--logarithmic',
                      action='store_true', dest='logarithmic',
                      default=False, help=helps['logarithmic'])
    options, args = parser.parse_args()

    if (len(args) == 1):
        filename = args[0]
    else:
        parser.print_help()
        return

    plt.rcParams['lines.linewidth'] = 3
    plt.rcParams['lines.markersize'] = 9
    fontsize = 16

    steps, times, nts, dts = extract_times(filename)
    dts[-1] = nm.nan

    ax = plt.subplot(211)
    if options.logarithmic:
        l1, = ax.semilogy(steps, dts, 'b')
    else:
        l1, = ax.plot(steps, dts, 'b')
    ax.set_xlabel('step', fontsize=fontsize)
    ax.set_ylabel(r'$\Delta t$', fontsize=fontsize)
    ax.grid(True)

    ax = ax.twinx()
    l2, = ax.plot(steps, times, 'g')
    ax.set_ylabel(r'$t$', fontsize=fontsize)
    ax.legend([l1, l2], [r'$\Delta t$', r'$t$'], loc=0)

    ax = plt.subplot(212)
    if options.logarithmic:
        ax.semilogy(times, dts, 'b+')
    else:
        ax.plot(times, dts, 'b+')
    ax.set_xlabel(r'$t$', fontsize=fontsize)
    ax.set_ylabel(r'$\Delta t$', fontsize=fontsize)
    ax.grid(True)

    plt.show()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = save_basis
#!/usr/bin/env python
"""
Save polynomial basis on reference elements or on a mesh for visualization into
a given output directory.
"""
import os
from optparse import OptionParser
import numpy as nm

from sfepy.base.base import output, Struct
from sfepy.base.ioutils import get_print_info, ensure_path
from sfepy.discrete import FieldVariable, Variables
from sfepy.discrete.fem import Mesh, FEDomain, Field
from sfepy.discrete.fem.geometry_element import GeometryElement
from sfepy.discrete.fem.poly_spaces import PolySpace
from sfepy.discrete.fem.linearizer import create_output
from sfepy.discrete.fem.fields_base import create_expression_output

usage = '%prog [options] output_dir\n' + __doc__.rstrip()

help = {
    'basis' :
    'name of the FE basis [default: %default]',
    'derivative' :
    'save d-th derivative of FE basis, can be 0 or 1 [default: %default]',
    'max_order' :
    'maximum order of polynomials [default: %default]',
    'geometry' :
    'reference element geometry, one of "2_3", "2_4", "3_4", "3_8"'
    ' [default: %default]',
    'mesh' :
    'name of the mesh file - alternative to --geometry [default: %default]',
    'permutations' :
    'list of geometry element permutations for each element, e.g. 0,1 is a'
    ' single permutation for two elements, 0,1,0,2,1,0 are three permutations'
    ' for two elements. Special value "all" can be used to save all possible'
    ' permutations for given reference element. Works only with --mesh option'
    ' [default: %default]',
    'dofs' :
    'if given, save only the DOFs specified as a comma-separated list'
    ' [default: %default]',
    'lin_options' :
    'linearizer options [default: %default]',
    'plot_dofs' :
    'plot local and global DOF numberings, with --mesh option',
}

def get_dofs(dofs, n_total):
    if dofs is None:
        dofs = range(n_total)

    else:
        dofs = [int(ii) for ii in dofs.split(',')]

    return dofs

def save_basis_on_mesh(mesh, options, output_dir, lin,
                       permutations=None, suffix=''):
    if permutations is not None:
        mesh = mesh.copy()
        for ig, conn in enumerate(mesh.conns):
            gel = GeometryElement(mesh.descs[ig])
            perms = gel.get_conn_permutations()[permutations]
            n_el, n_ep = conn.shape
            offsets = nm.arange(n_el) * n_ep

            conn[:] = conn.take(perms + offsets[:, None])

    domain = FEDomain('domain', mesh)

    omega = domain.create_region('Omega', 'all')
    field = Field.from_args('f', nm.float64, shape=1, region=omega,
                            approx_order=options.max_order,
                            poly_space_base=options.basis)
    var = FieldVariable('u', 'unknown', field)

    if options.plot_dofs:
        import sfepy.postprocess.plot_dofs as pd
        group = domain.groups[0]
        ax = pd.plot_mesh(None, mesh.coors, mesh.conns[0], group.gel.edges)
        ax = pd.plot_global_dofs(ax, field.get_coor(), field.aps[0].econn)
        ax = pd.plot_local_dofs(ax, field.get_coor(), field.aps[0].econn)
        if options.dofs is not None:
            ax = pd.plot_nodes(ax, field.get_coor(), field.aps[0].econn,
                               field.aps[0].interp.poly_spaces['v'].nodes,
                               get_dofs(options.dofs, var.n_dof))
        pd.plt.show()

    output('dofs: %d' % var.n_dof)

    vec = nm.empty(var.n_dof, dtype=var.dtype)
    n_digit, _format = get_print_info(var.n_dof, fill='0')
    name_template = os.path.join(output_dir,
                                 'dof_%s%s.vtk' % (_format, suffix))
    for ip in get_dofs(options.dofs, var.n_dof):
        output('dof %d...' % ip)

        vec.fill(0.0)
        vec[ip] = 1.0

        var.set_data(vec)

        if options.derivative == 0:
            out = var.create_output(vec, linearization=lin)

        else:
            out = create_expression_output('ev_grad.ie.Elements(u)',
                                           'u', 'f', {'f' : field}, None,
                                           Variables([var]),
                                           mode='qp', verbose=False,
                                           min_level=lin.min_level,
                                           max_level=lin.max_level,
                                           eps=lin.eps)

        name = name_template % ip
        ensure_path(name)
        out['u'].mesh.write(name, out=out)

        output('...done (%s)' % name)

def main():
    parser = OptionParser(usage=usage, version='%prog')
    parser.add_option('-b', '--basis', metavar='name',
                      action='store', dest='basis',
                      default='lagrange', help=help['basis'])
    parser.add_option('-d', '--derivative', metavar='d', type=int,
                      action='store', dest='derivative',
                      default=0, help=help['derivative'])
    parser.add_option('-n', '--max-order', metavar='order', type=int,
                      action='store', dest='max_order',
                      default=2, help=help['max_order'])
    parser.add_option('-g', '--geometry', metavar='name',
                      action='store', dest='geometry',
                      default='2_4', help=help['geometry'])
    parser.add_option('-m', '--mesh', metavar='mesh',
                      action='store', dest='mesh',
                      default=None, help=help['mesh'])
    parser.add_option('', '--permutations', metavar='permutations',
                      action='store', dest='permutations',
                      default=None, help=help['permutations'])
    parser.add_option('', '--dofs', metavar='dofs',
                      action='store', dest='dofs',
                      default=None, help=help['dofs'])
    parser.add_option('-l', '--lin-options', metavar='options',
                      action='store', dest='lin_options',
                      default='min_level=2,max_level=5,eps=1e-3',
                      help=help['lin_options'])
    parser.add_option('', '--plot-dofs',
                      action='store_true', dest='plot_dofs',
                      default=False, help=help['plot_dofs'])
    options, args = parser.parse_args()

    if len(args) == 1:
        output_dir = args[0]
    else:
        parser.print_help(),
        return

    output('polynomial space:', options.basis)
    output('max. order:', options.max_order)

    lin = Struct(kind='adaptive', min_level=2, max_level=5, eps=1e-3)
    for opt in options.lin_options.split(','):
        key, val = opt.split('=')
        setattr(lin, key, eval(val))

    if options.mesh is None:
        dim, n_ep = int(options.geometry[0]), int(options.geometry[2])
        output('reference element geometry:')
        output('  dimension: %d, vertices: %d' % (dim, n_ep))

        gel = GeometryElement(options.geometry)
        gps = PolySpace.any_from_args(None, gel, 1,
                                      base=options.basis)
        ps = PolySpace.any_from_args(None, gel, options.max_order,
                                     base=options.basis)

        n_digit, _format = get_print_info(ps.n_nod, fill='0')
        name_template = os.path.join(output_dir, 'bf_%s.vtk' % _format)
        for ip in get_dofs(options.dofs, ps.n_nod):
            output('shape function %d...' % ip)

            def eval_dofs(iels, rx):
                if options.derivative == 0:
                    bf = ps.eval_base(rx).squeeze()
                    rvals = bf[None, :, ip:ip+1]

                else:
                    bfg = ps.eval_base(rx, diff=True)
                    rvals = bfg[None, ..., ip]

                return rvals

            def eval_coors(iels, rx):
                bf = gps.eval_base(rx).squeeze()
                coors = nm.dot(bf, gel.coors)[None, ...]
                return coors

            (level, coors, conn,
             vdofs, mat_ids) = create_output(eval_dofs, eval_coors, 1,
                                             ps, min_level=lin.min_level,
                                             max_level=lin.max_level,
                                             eps=lin.eps)
            out = {
                'bf' : Struct(name='output_data',
                              mode='vertex', data=vdofs,
                              var_name='bf', dofs=None)
            }

            mesh = Mesh.from_data('bf_mesh', coors, None, [conn], [mat_ids],
                                  [options.geometry])

            name = name_template % ip
            ensure_path(name)
            mesh.write(name, out=out)

            output('...done (%s)' % name)

    else:
        mesh = Mesh.from_file(options.mesh)
        output('mesh geometry:')
        output('  dimension: %d, vertices: %d, elements: %d'
               % (mesh.dim, mesh.n_nod, mesh.n_el))

        if options.permutations:
            if options.permutations == 'all':
                from sfepy.linalg import cycle
                gel = GeometryElement(mesh.descs[0])
                n_perms = gel.get_conn_permutations().shape[0]
                all_permutations = [ii for ii in cycle(mesh.n_el * [n_perms])]

            else:
                all_permutations = [int(ii)
                                    for ii in options.permutations.split(',')]
                all_permutations = nm.array(all_permutations)
                np = len(all_permutations)
                all_permutations.shape = (np / mesh.n_el, mesh.n_el)

            output('using connectivity permutations:\n', all_permutations)

        else:
            all_permutations = [None]

        for ip, permutations in enumerate(all_permutations):
            if permutations is None:
                suffix = ''

            else:
                suffix = '_' + '_'.join('%d' % ii for ii in permutations)

            save_basis_on_mesh(mesh, options, output_dir, lin, permutations,
                               suffix)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = show_authors
#!/usr/bin/env python
import shlex
import subprocess

def main():
    args = shlex.split('git log --pretty=format:"%an <%ae>"')
    p = subprocess.Popen(args, stdout=subprocess.PIPE)
    out = p.communicate()[0].split('\n')

    done = set()
    unique = []
    for line in reversed(out):
        line = line.strip()
        if len(line) and not line in done:
            done.add(line)
            unique.append(line)

    for line in unique:
        print line

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = show_terms_use
#!/usr/bin/env python
"""
Show terms use in problem description files in the given directory.
"""
import os
from optparse import OptionParser

from sfepy.base.base import output, dict_from_keys_init, ordered_iteritems
from sfepy.base.conf import ProblemConf, get_standard_keywords
from sfepy.base.ioutils import locate_files
from sfepy.discrete.equations import parse_definition
from sfepy.terms import term_table

usage = '%prog [options] <directory>\n' + __doc__.rstrip()

helps = {
    'counts' : 'show terms use counts only',
    'unused' : 'show unused terms only',
}

def main():
    parser = OptionParser(usage=usage, version='%prog')
    parser.add_option('-c', '--counts',
                      action='store_true', dest='counts',
                      default=False, help=helps['counts'])
    parser.add_option('-u', '--unused',
                      action='store_true', dest='unused',
                      default=False, help=helps['unused'])
    options, args = parser.parse_args()

    if len(args) > 0:
        pdf_dir = os.path.realpath(args[0])

    else:
        parser.print_help(),
        return

    required, other = get_standard_keywords()

    terms_use = dict_from_keys_init(term_table.keys(), set)

    for filename in locate_files('*.py', pdf_dir):
        base = filename.replace(pdf_dir, '').lstrip(os.path.sep)
        output('trying "%s"...' % base)

        try:
            conf = ProblemConf.from_file(filename, required, other,
                                         verbose=False)

        except:
            output('...failed')
            continue

        use = conf.options.get('use_equations', 'equations')
        eqs_conf = getattr(conf, use)
        for key, eq_conf in eqs_conf.iteritems():
            term_descs = parse_definition(eq_conf)
            for td in term_descs:
                terms_use[td.name].add(base)

        output('...ok')
    output('...done')

    if options.unused:
        output('unused terms:')

        unused = [name for name in terms_use.keys()
                  if len(terms_use[name]) == 0]
        for name in sorted(unused):
            output('  ' + name)

        output('total: %d' % len(unused))

    else:
        output('terms use:')
        for name, ex_names in ordered_iteritems(terms_use):
            output('%s: %d' % (name, len(ex_names)))
            if not options.counts:
                for ex_name in sorted(ex_names):
                    output('  ' + ex_name)

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = sync_module_docs
#!/usr/bin/env python
"""
Synchronize the documentation files in a given directory ``doc_dir`` with the
actual state of the SfePy sources in ``top_dir``. Missing files are created,
files with no corresponding source file are removed, other files are left
untouched.

Notes
-----
The developer guide needs to be edited manually to reflect the changes.
"""
import os
import fnmatch
from optparse import OptionParser

from sfepy.base.base import output
from sfepy.base.ioutils import locate_files, edit_filename, ensure_path

usage = '%prog [options] doc_dir top_dir\n' + __doc__.rstrip()

omits = [
    '__init__.py',
    '__config__.py',
    'debug.py',
    'gitwash_dumper.py',
    'setup.py',
    'site_cfg.py',
    'site_cfg_template.py',
]

omits_pyx = [
    'lobatto_template.pyx',
]

doc_template = """%s
%s

.. automodule:: %s
   :members:
   :undoc-members:
"""

help = {
    'dry_run' :
    'only show what changes would be made',
}

def main():
    parser = OptionParser(usage=usage, version='%prog')
    parser.add_option('-n', '--dry-run',
                      action='store_true', dest='dry_run',
                      default=False, help=help['dry_run'])
    options, args = parser.parse_args()

    if len(args) == 2:
        doc_dir, top_dir = [os.path.realpath(ii) for ii in args]
    else:
        parser.print_help(),
        return

    docs = set(ii for ii in locate_files('*.rst', root_dir=doc_dir))

    sources = set(ii for ii in
                  locate_files('*.py',
                               root_dir=os.path.join(top_dir, 'sfepy'))
                  if os.path.basename(ii) not in omits)
    sources.update(ii for ii in
                   locate_files('*.pyx',
                                root_dir=os.path.join(top_dir, 'sfepy'))
                   if os.path.basename(ii) not in omits_pyx)
    scripts = set(ii for ii in
                  locate_files('*.py',
                               root_dir=os.path.join(top_dir, 'script'))
                  if os.path.basename(ii) not in omits)
    top_scripts = set(os.path.realpath(ii)
                      for ii in fnmatch.filter(os.listdir(top_dir), '*.py')
                      if os.path.basename(ii) not in omits)

    all_sources = set()
    all_sources.update(sources, scripts, top_scripts)

    cwd = os.path.realpath(os.path.curdir) + os.path.sep

    output.prefix = 'smd:'
    output('removing unneeded rst files in "%s"...' % doc_dir)
    for doc in sorted(docs):
        aux = edit_filename(doc, new_ext='.py')
        src1 = os.path.normpath(aux.replace(doc_dir, top_dir))

        aux = edit_filename(doc, new_ext='.pyx')
        src2 = os.path.normpath(aux.replace(doc_dir, top_dir))

        if (src1 not in all_sources) and (src2 not in all_sources):
            output('remove: %s' % doc.replace(cwd, ''))
            if not options.dry_run:
                os.remove(doc)
    output('...done')

    output('creating missing rst files in "%s"...' % doc_dir)
    for src in sorted(all_sources):
        aux = edit_filename(src, new_ext='.rst')
        doc = os.path.normpath(aux.replace(top_dir, doc_dir))

        if doc not in docs:
            output('create: %s' % doc.replace(cwd, ''))
            if not options.dry_run:
                mod_filename = src.replace(top_dir + os.path.sep, '')
                mod_name = mod_filename.replace(os.path.sep, '.')
                mod_name = edit_filename(mod_name, new_ext='')
                if mod_name.startswith('sfepy'): # Module.
                    title = mod_name + ' module'

                else: # Script.
                    title = mod_filename + ' script'
                    mod_name = mod_name.split('.')[-1]

                underlines = '=' * len(title)

                contents = doc_template % (title, underlines, mod_name)

                ensure_path(doc)
                fd = open(doc, 'w')
                fd.write(contents)
                fd.close()

    output('...done')

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = tile_periodic_mesh
#!/usr/bin/env python

from sfepy.base.base import Output
from sfepy.discrete.fem.mesh import Mesh
from sfepy.mesh.mesh_generators import gen_tiled_mesh
from optparse import OptionParser

usage = """%prog [options] filename_in filename_out

The program scales a periodic input mesh (a rectangle or box) in
filename_in by a scale factor and generates a new mesh by repeating the
scaled original mesh in a regular grid (scale x scale [x scale]) if
repeat option is None, or in a grid nx x ny x nz for repeat 'nx,ny,nz',
producing again a periodic rectangle or box mesh.
"""

help = {
    'scale' : 'scale factor [default: %default]',
    'repeat' : 'repetition counts in each axial direction'
               ' [default: %default]',
    'eps'   : 'coordinate precision [default: %default]',
}

def parse_repeat(option, opt, value, parser):
    if value is not None:
        setattr(parser.values, option.dest, [int(r) for r in value.split(',')])

def main():
    parser = OptionParser(usage=usage, version="%prog 42")
    parser.add_option("-s", "--scale", type=int, metavar='scale',
                      action="store", dest="scale",
                      default=2, help=help['scale'])
    parser.add_option("-r", "--repeat", type='str', metavar='nx,ny[,nz]',
                      action="callback", dest="repeat",
                      callback=parse_repeat, default=None, help=help['repeat'])
    parser.add_option("-e", "--eps", type=float, metavar='eps',
                      action="store", dest="eps",
                      default=1e-8, help=help['eps'])
    (options, args) = parser.parse_args()

    if (len( args ) == 2):
        filename_in = args[0]
        filename_out = args[1]
    else:
        parser.print_help()
        return

    output = Output('tpm:')
    output('scale:', options.scale)
    output('repeat:', options.repeat)
    output('eps:', options.eps)

    mesh_in = Mesh.from_file(filename_in)
    mesh_out = gen_tiled_mesh(mesh_in, options.repeat, 1./options.scale,
                              options.eps)
    mesh_out.write(filename_out, io='auto')
    output('done.')

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = application
from sfepy.base.base import Struct, output, insert_as_static_method

class Application(Struct):
    """
    Base class for applications.

    Subclasses should implement: __init__(), call().

    Automates parametric studies, see parametrize().
    """
    def __init__(self, conf, options, output_prefix, **kwargs):
        Struct.__init__(self,
                        conf=conf,
                        options=options,
                        output_prefix=output_prefix)
        output.prefix = self.output_prefix
        self.restore()

    def setup_options(self):
        pass

    def __call__(self, **kwargs):
        """
        This is either call_basic() or call_parametrized().
        """
        pass

    def call_basic(self, **kwargs):
        return self.call(**kwargs)

    def call_parametrized(self, **kwargs):
        generator = self.parametric_hook(self.problem)
        for aux in generator:
            if isinstance(aux, tuple) and (len(aux) == 2):
                problem, container = aux
                mode = 'coroutine'
            else:
                problem = aux
                mode = 'simple'
            self.problem = problem

            generator_prefix = output.prefix
            output.prefix = self.output_prefix # Restore default.

            """Application options have to be re-processed here as they can
            change in the parametric hook."""
            self.setup_options()
            out = self.call(**kwargs)

            output.prefix = generator_prefix

            if mode == 'coroutine':
                # Pass application output to the generator.
                container.append(out)
                generator.next()

    def restore(self):
        """
        Remove parametric_hook, restore __call__() to call_basic().
        """
        self.parametric_hook = None
        insert_as_static_method(self.__class__, '__call__',
                                self.call_basic)

    def parametrize(self, parametric_hook):
        """
        Add parametric_hook, set __call__() to call_parametrized().
        """
        if parametric_hook is None: return

        self.parametric_hook = parametric_hook
        insert_as_static_method(self.__class__, '__call__',
                                self.call_parametrized)

########NEW FILE########
__FILENAME__ = pde_solver_app
import os

from sfepy.base.base import output, dict_to_struct, Struct
from sfepy.base.conf import ProblemConf, get_standard_keywords
import sfepy.base.ioutils as io
from sfepy.discrete import Problem
from sfepy.discrete.fem.meshio import MeshIO
from application import Application

def solve_pde(conf, options=None, nls_status=None, **app_options):
    """
    Solve a system of partial differential equations (PDEs).

    This function is a convenience wrapper that creates and runs an instance of
    :class:`PDESolverApp`.

    Parameters
    ----------
    conf : str or ProblemConf instance
        Either the name of the problem description file defining the PDEs,
        or directly the ProblemConf instance.
    options : options
        The command-line options.
    nls_status : dict-like
        The object for storing the nonlinear solver return status.
    app_options : kwargs
        The keyword arguments that can override application-specific options.
    """
    if not isinstance(conf, ProblemConf):
        required, other = get_standard_keywords()
        conf = ProblemConf.from_file(conf, required, other)

    opts = conf.options = (dict_to_struct(app_options, flag=(1,),
                                          constructor=type(conf.options))
                           + conf.options)

    output_prefix = opts.get('output_prefix', None)
    if output_prefix is None:
        output_prefix = output.prefix

    if options is None:
        options = Struct(output_filename_trunk=None,
                         save_ebc=False,
                         save_ebc_nodes=False,
                         save_regions=False,
                         save_field_meshes=False,
                         save_regions_as_groups=False,
                         solve_not=False)

    app = PDESolverApp(conf, options, output_prefix)
    if hasattr(opts, 'parametric_hook'): # Parametric study.
        parametric_hook = conf.get_function(opts.parametric_hook)
        app.parametrize(parametric_hook)

    return app(nls_status=nls_status)

def save_only(conf, save_names, problem=None):
    """
    Save information available prior to setting equations and
    solving them.
    """
    if problem is None:
        problem = Problem.from_conf(conf, init_equations=False)

    if save_names.regions is not None:
        problem.save_regions(save_names.regions)

    if save_names.regions_as_groups is not None:
        problem.save_regions_as_groups(save_names.regions_as_groups)

    if save_names.field_meshes is not None:
        problem.save_field_meshes(save_names.field_meshes)

    if save_names.ebc is not None:
        problem.save_ebc(save_names.ebc, force=False)

    if save_names.ebc_nodes is not None:
        problem.save_ebc(save_names.ebc_nodes, force=True)

def assign_standard_hooks(obj, get, conf):
    """
    Set standard hook function attributes from `conf` to `obj` using the
    `get` function.
    """
    hook_names = ['step_hook', 'post_process_hook',
                  'post_process_hook_final', 'pre_process_hook']
    for hook_name in hook_names:
        setattr(obj, hook_name, conf.get_function(get(hook_name, None)))

class PDESolverApp(Application):

    @staticmethod
    def process_options(options):
        """
        Application options setup. Sets default values for missing
        non-compulsory options.
        """
        get = options.get

        output_dir = get('output_dir', '.')
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        return Struct(save_results=get('save_results', True),
                      # Save each variable into a separate file, using
                      # the region of its definition only.
                      linearization=dict_to_struct(get('linearization',
                                                       {'kind' : 'strip'})),
                      file_per_var=get('file_per_var', False),
                      output_format=get('output_format', 'vtk'),
                      output_dir=output_dir,
                      # Called after each time step, can do anything, no
                      # return value.
                      step_hook=get('step_hook', None),
                      # Called after each time step.
                      post_process_hook=get('post_process_hook', None),
                      # Called after all time steps, or in the
                      # stationary case.
                      post_process_hook_final=get('post_process_hook_final',
                                                  None),
                      # Called in init process.
                      pre_process_hook=get('pre_process_hook', None),
                      use_equations=get('use_equations', 'equations'))

    def __init__(self, conf, options, output_prefix,
                 init_equations=True, **kwargs):
        """`kwargs` are passed to  Problem.from_conf()

        Command-line options have precedence over conf.options."""
        Application.__init__( self, conf, options, output_prefix )
        self.setup_options()

        is_eqs = init_equations
        if hasattr(options, 'solve_not') and options.solve_not:
            is_eqs = False
        self.problem = Problem.from_conf(conf, init_equations=is_eqs, **kwargs)

        self.setup_output_info( self.problem, self.options )

    def setup_options( self ):
        self.app_options = PDESolverApp.process_options(self.conf.options)

        assign_standard_hooks(self, self.app_options.get, self.conf)

        # Override default equations, if use_equations is set.
        if hasattr(self.conf, 'equations'):
            self.conf.equations = getattr(self.conf,
                                          self.app_options.use_equations)

    def setup_output_info(self, problem, options):
        """Modifies both problem and options!"""
        if options.output_filename_trunk is None:
            if self.conf.get('filename_mesh') is not None:
                filename_mesh = self.conf.filename_mesh
                if isinstance(filename_mesh, MeshIO):
                    ofn_trunk = filename_mesh.get_filename_trunk()

                else:
                    ofn_trunk = io.get_trunk(filename_mesh)

            elif self.conf.get('filename_domain') is not None:
                ofn_trunk = io.get_trunk(self.conf.filename_domain)

            else:
                raise ValueError('missing filename_mesh or filename_domain!')

            options.output_filename_trunk = ofn_trunk

        else:
            ofn_trunk = options.output_filename_trunk

        if hasattr(options, 'output_format') \
               and (options.output_format is not None):
            output_format = options.output_format

        else:
            output_format = self.app_options.output_format

        problem.setup_output(output_filename_trunk=ofn_trunk,
                             output_dir=self.app_options.output_dir,
                             output_format=output_format,
                             file_per_var=self.app_options.file_per_var,
                             linearization=self.app_options.linearization)

    def call(self, nls_status=None):
        problem = self.problem
        options = self.options
        opts = self.app_options

        if self.pre_process_hook is not None: # User pre_processing.
            self.pre_process_hook(problem)

        ofn_trunk = problem.ofn_trunk
        self.save_names = Struct(ebc=ofn_trunk + '_ebc.vtk'
                                 if options.save_ebc else None,

                                 ebc_nodes=ofn_trunk + '_ebc_nodes.vtk'
                                 if options.save_ebc_nodes else None,

                                 regions=ofn_trunk + '_region'
                                 if options.save_regions else None,

                                 regions_as_groups=ofn_trunk + '_regions'
                                 if options.save_regions_as_groups else None,

                                 field_meshes=ofn_trunk + '_field'
                                 if options.save_field_meshes else None)

        if any(self.save_names.to_dict().values()):
            save_only(self.conf, self.save_names, problem=problem)

        if options.solve_not:
            return None, None, None

        time_solver = problem.get_time_solver()

        state = time_solver(save_results=opts.save_results,
                            step_hook=self.step_hook,
                            post_process_hook=self.post_process_hook,
                            nls_status=nls_status)

        if self.post_process_hook_final is not None: # User postprocessing.
            self.post_process_hook_final(problem, state)

        return problem, state

    def save_dict(self, filename, data):
        """
        Utility function to save a dictionary `data` to a HDF5 file
        `filename`.
        """
        io.write_dict_hdf5(filename, data)

    def load_dict(self, filename):
        """
        Utility function to load a dictionary `data` from a HDF5 file
        `filename`.
        """
        data = io.read_dict_hdf5(filename)

        return data

########NEW FILE########
__FILENAME__ = base
import time
import sys
import os
from copy import copy, deepcopy
from types import UnboundMethodType
from getch import getch

import numpy as nm
import scipy.sparse as sp

real_types = [nm.float64]
complex_types = [nm.complex128]

nm.set_printoptions(threshold=100)

from sfepy.base.goptions import goptions

sfepy_config_dir = os.path.expanduser('~/.sfepy')
if not os.path.exists(sfepy_config_dir):
    os.makedirs(sfepy_config_dir)

if sys.version[0] < '3':
    basestr = basestring

else:
    basestr = str

def get_debug():
    """
    Utility function providing ``debug()`` function.
    """
    try:
        import IPython

    except ImportError:
        debug = None

    else:
        old_excepthook = sys.excepthook

        def debug(frame=None):
            if IPython.__version__ >= '0.11':
                from IPython.core.debugger import Pdb

                try:
                    ip = get_ipython()

                except NameError:
                    from IPython.frontend.terminal.embed \
                         import InteractiveShellEmbed
                    ip = InteractiveShellEmbed()

                colors = ip.colors

            else:
                from IPython.Debugger import Pdb
                from IPython.Shell import IPShell
                from IPython import ipapi

                ip = ipapi.get()
                if ip is None:
                    IPShell(argv=[''])
                    ip = ipapi.get()

                colors = ip.options.colors

            sys.excepthook = old_excepthook

            if frame is None:
                frame = sys._getframe().f_back

            Pdb(colors).set_trace(frame)

    if debug is None:
        import pdb
        debug = pdb.set_trace

    debug.__doc__ = """
    Start debugger on line where it is called, roughly equivalent to::

        import pdb; pdb.set_trace()

    First, this function tries to start an `IPython`-enabled
    debugger using the `IPython` API.

    When this fails, the plain old `pdb` is used instead.
    """

    return debug

debug = get_debug()

def mark_time(times, msg=None):
    """
    Time measurement utility.

    Measures times of execution between subsequent calls using
    time.clock(). The time is printed if the msg argument is not None.

    Examples
    --------

    >>> times = []
    >>> mark_time(times)
    ... do something
    >>> mark_time(times, 'elapsed')
    elapsed 0.1
    ... do something else
    >>> mark_time(times, 'elapsed again')
    elapsed again 0.05
    >>> times
    [0.10000000000000001, 0.050000000000000003]
    """
    tt = time.clock()
    times.append(tt)
    if (msg is not None) and (len(times) > 1):
        print msg, times[-1] - times[-2]

def import_file(filename, package_name=None):
    """
    Import a file as a module. The module is explicitly reloaded to
    prevent undesirable interactions.
    """
    path = os.path.dirname(filename)

    if not path in sys.path:
        sys.path.append(path)
        remove_path = True

    else:
        remove_path = False

    name = os.path.splitext(os.path.basename(filename))[0]

    if name in sys.modules:
        force_reload = True
    else:
        force_reload = False

    if package_name is not None:
        mod = __import__('.'.join((package_name, name)), fromlist=[name])

    else:
        mod = __import__(name)

    if force_reload:
        reload(mod)

    if remove_path:
        sys.path.pop(-1)

    return mod

def try_imports(imports, fail_msg=None):
    """
    Try import statements until one succeeds.

    Parameters
    ----------
    imports : list
        The list of import statements.
    fail_msg : str
        If not None and no statement succeeds, a `ValueError` is raised with
        the given message, appended to all failed messages.

    Returns
    -------
    locals : dict
        The dictionary of imported modules.
    """
    msgs = []
    for imp in imports:
        try:
            exec imp
            break

        except Exception, inst:
            msgs.append(str(inst))

    else:
        if fail_msg is not None:
            msgs.append(fail_msg)
            raise ValueError('\n'.join(msgs))

    return locals()

def python_shell():
    import code
    frame = sys._getframe(1)
    code.interact(local=frame.f_locals)

def assert_(condition, msg='assertion failed!'):
    if not condition:
        raise ValueError(msg)

##
# c: 06.04.2005, r: 05.05.2008
def pause(msg=None):
    """
    Prints the line number and waits for a keypress.

    If you press:
    "q" ............. it will call sys.exit()
    any other key ... it will continue execution of the program

    This is useful for debugging.
    """
    f = sys._getframe(1)
    ff = f.f_code
    if (msg):
        print '%s, %d: %s(), %d: %s' % (ff.co_filename, ff.co_firstlineno,
                                        ff.co_name, f.f_lineno, msg)
    else:
        print '%s, %d: %s(), %d' % (ff.co_filename, ff.co_firstlineno,
                                    ff.co_name, f.f_lineno)
    spause()

##
# Silent pause.
# 18.02.2005, c
# 12.02.2007
def spause(msg=None):
    """
    Waits for a keypress.

    If you press:
    "q" ............. it will call sys.exit()
    any other key ... it will continue execution of the program

    This is useful for debugging. This function is called from pause().
    """
    if (msg):
        print msg
    sys.stdout.flush()
    ch = getch()
    if ch == 'q':
        sys.exit()

##
# 02.01.2005
class Struct(object):
    # 03.10.2005, c
    # 26.10.2005
    def __init__(self, **kwargs):
        if kwargs:
            self.__dict__.update(kwargs)

    def _format_sequence(self, seq, threshold):
        threshold_half = threshold / 2

        if len(seq) > threshold:
            out = ', '.join(str(ii) for ii in seq[:threshold_half]) \
                  + ', ..., ' \
                  + ', '.join(str(ii) for ii in seq[-threshold_half:])

        else:
            out = str(seq)

        return out

    # 08.03.2005
    def __str__(self):
        """Print instance class, name and items in alphabetical order.

        If the class instance has '_str_attrs' attribute, only the attributes
        listed there are taken into account. Other attributes are provided only
        as a list of attribute names (no values).

        For attributes that are Struct instances, if
        the listed attribute name ends with '.', the attribute is printed fully
        by calling str(). Otherwise only its class name/name are printed.

        Attributes that are NumPy arrays or SciPy sparse matrices are
        printed in a brief form.

        Only keys of dict attributes are printed. For the dict keys as
        well as list or tuple attributes only several edge items are
        printed if their length is greater than the threshold value 20.
        """
        return self._str()

    def _str(self, keys=None, threshold=20):
        ss = '%s' % self.__class__.__name__
        if hasattr(self, 'name'):
            ss += ':%s' % self.name
        ss += '\n'

        if keys is None:
            keys = self.__dict__.keys()

        str_attrs = sorted(Struct.get(self, '_str_attrs', keys))
        printed_keys = []
        for key in str_attrs:
            if key[-1] == '.':
                key = key[:-1]
                full_print = True
            else:
                full_print = False

            printed_keys.append(key)

            try:
                val = getattr(self, key)

            except AttributeError:
                continue

            if isinstance(val, Struct):
                if not full_print:
                    ss += '  %s:\n    %s' % (key, val.__class__.__name__)
                    if hasattr(val, 'name'):
                        ss += ':%s' % val.name
                    ss += '\n'

                else:
                    aux = '\n' + str(val)
                    aux = aux.replace('\n', '\n    ')
                    ss += '  %s:\n%s\n' % (key, aux[1:])

            elif isinstance(val, dict):
                sval = self._format_sequence(val.keys(), threshold)
                sval = sval.replace('\n', '\n    ')
                ss += '  %s:\n    dict with keys: %s\n' % (key, sval)

            elif isinstance(val, list):
                sval = self._format_sequence(val, threshold)
                sval = sval.replace('\n', '\n    ')
                ss += '  %s:\n    list: %s\n' % (key, sval)

            elif isinstance(val, tuple):
                sval = self._format_sequence(val, threshold)
                sval = sval.replace('\n', '\n    ')
                ss += '  %s:\n    tuple: %s\n' % (key, sval)

            elif isinstance(val, nm.ndarray):
                ss += '  %s:\n    %s array of %s\n' \
                      % (key, val.shape, val.dtype)

            elif isinstance(val, sp.spmatrix):
                ss += '  %s:\n    %s spmatrix of %s, %d nonzeros\n' \
                      % (key, val.shape, val.dtype, val.nnz)

            else:
                aux = '\n' + str(val)
                aux = aux.replace('\n', '\n    ')
                ss += '  %s:\n%s\n' % (key, aux[1:])

        other_keys = sorted(set(keys).difference(set(printed_keys)))
        if len(other_keys):
            ss += '  other attributes:\n    %s\n' \
                  % '\n    '.join(key for key in other_keys)

        return ss.rstrip()

    def __repr__(self):
        ss = "%s" % self.__class__.__name__
        if hasattr(self, 'name'):
            ss += ":%s" % self.name
        return ss

    ##
    # 28.08.2007, c
    def __add__(self, other):
        """Merge Structs. Attributes of new are those of self unless an
        attribute and its counterpart in other are both Structs - these are
        merged then."""
        new = copy(self)
        for key, val in other.__dict__.iteritems():
            if hasattr(new, key):
                sval = getattr(self, key)
                if issubclass(sval.__class__, Struct) and \
                        issubclass(val.__class__, Struct):
                    setattr(new, key, sval + val)
                else:
                    setattr(new, key, sval)
            else:
                setattr(new, key, val)
        return new

    ##
    # 28.08.2007, c
    def __iadd__(self, other):
        """Merge Structs in place. Attributes of self are left unchanged
        unless an attribute and its counterpart in other are both Structs -
        these are merged then."""
        for key, val in other.__dict__.iteritems():
            if hasattr(self, key):
                sval = getattr(self, key)
                if issubclass(sval.__class__, Struct) and \
                       issubclass(val.__class__, Struct):
                    setattr(self, key, sval + val)
            else:
                setattr(self, key, val)
        return self

    def str_class(self):
        """
        As __str__(), but for class attributes.
        """
        return self._str(self.__class__.__dict__.keys())

    # 08.03.2005, c
    def str_all(self):
        ss = "%s\n" % self.__class__
        for key, val in self.__dict__.iteritems():
            if issubclass(self.__dict__[key].__class__, Struct):
                ss += "  %s:\n" % key
                aux = "\n" + self.__dict__[key].str_all()
                aux = aux.replace("\n", "\n    ")
                ss += aux[1:] + "\n"
            else:
                aux = "\n" + str(val)
                aux = aux.replace("\n", "\n    ")
                ss += "  %s:\n%s\n" % (key, aux[1:])
        return(ss.rstrip())

    ##
    # 09.07.2007, c
    def to_dict(self):
        return copy(self.__dict__)

    def get(self, key, default=None, msg_if_none=None):
        """
        A dict-like get() for Struct attributes.
        """
        out = getattr(self, key, default)

        if (out is None) and (msg_if_none is not None):
            raise ValueError(msg_if_none)

        return out

    def update(self, other, **kwargs):
        """
        A dict-like update for Struct attributes.
        """
        if other is None: return

        if not isinstance(other, dict):
            other = other.to_dict()
        self.__dict__.update(other, **kwargs)

    def set_default(self, key, default=None):
        """
        Behaves like dict.setdefault().
        """
        return self.__dict__.setdefault(key, default)

    def copy(self, deep=False, name=None):
        """Make a (deep) copy of self.

        Parameters:

        deep : bool
            Make a deep copy.
        name : str
            Name of the copy, with default self.name + '_copy'.
        """
        if deep:
            other = deepcopy(self)
        else:
            other = copy(self)

        if hasattr(self, 'name'):
            other.name = get_default(name, self.name + '_copy')

        return other
#
# 12.07.2007, c
class IndexedStruct(Struct):

    ##
    # 12.07.2007, c
    def __getitem__(self, key):
        return getattr(self, key)

    ##
    # 12.07.2007, c
    def __setitem__(self, key, val):
        setattr(self, key, val)

##
# 14.07.2006, c
class Container(Struct):

    def __init__(self, objs=None, **kwargs):
        Struct.__init__(self, **kwargs)

        if objs is not None:
            self._objs = objs
            self.update()
        else:
            self._objs = []
            self.names = []

    def update(self, objs=None):
        if objs is not None:
            self._objs = objs

        self.names = [obj.name for obj in self._objs]

    def __setitem__(self, ii, obj):
        try:
            if isinstance(ii, basestr):
                if ii in self.names:
                    ii = self.names.index(ii)
                else:
                    ii = len(self.names)

            elif not isinstance(ii, int):
                    raise ValueError('bad index type! (%s)' % type(ii))

            if ii >= len(self.names):
                self._objs.append(obj)
                self.names.append(obj.name)

            else:
                self._objs[ii] = obj
                self.names[ii] = obj.name

        except (IndexError, ValueError), msg:
            raise IndexError(msg)

    def __getitem__(self, ii):
        try:
            if isinstance(ii, basestr):
                ii = self.names.index(ii)
            elif not isinstance(ii, int):
                raise ValueError('bad index type! (%s)' % type(ii))

            return  self._objs[ii]

        except (IndexError, ValueError), msg:
            raise IndexError(msg)

    def __iter__(self):
        return self._objs.__iter__()

    ##
    # 18.07.2006, c
    def __len__(self):
        return len(self._objs)

    def insert(self, ii, obj):
        self._objs.insert(ii, obj)
        self.names.insert(ii, obj.name)

    def append(self, obj):
        self[len(self.names)] = obj

    def extend(self, objs):
        """
        Extend the container items by the sequence `objs`.
        """
        for obj in objs:
            self.append(obj)

    def get(self, ii, default=None, msg_if_none=None):
        """
        Get an item from Container - a wrapper around
        Container.__getitem__() with defaults and custom error message.

        Parameters
        ----------
        ii : int or str
            The index or name of the item.
        default : any, optional
            The default value returned in case the item `ii` does not exist.
        msg_if_none : str, optional
            If not None, and if `default` is None and the item `ii` does
            not exist, raise ValueError with this message.
        """
        try:
            out = self[ii]

        except (IndexError, ValueError):
            if default is not None:
                out = default

            else:
                if msg_if_none is not None:
                    raise ValueError(msg_if_none)

                else:
                    raise

        return out

    def remove_name(self, name):
        ii = self.names.index[name]
        del self.names[ii]
        del self._objs[ii]

    ##
    # dict-like methods.
    def itervalues(self):
        return self._objs.__iter__()

    def iterkeys(self):
        return self.get_names().__iter__()

    def iteritems(self):
        for obj in self._objs:
            yield obj.name, obj

    ##
    # 20.09.2006, c
    def has_key(self, ii):
        if isinstance(ii, int):
            if (ii < len(self)) and (ii >= (-len(self))):
                return True
            else:
                return False
        elif isinstance(ii, basestr):
            try:
                self.names.index(ii)
                return True
            except:
                return False
        else:
            raise IndexError('unsupported index type: %s' % ii)

    ##
    # 12.06.2007, c
    def print_names(self):
        print [obj.name for obj in self._objs]

    def get_names(self):
        return [obj.name for obj in self._objs]

    def as_dict(self):
        """
        Return stored objects in a dictionary with object names as keys.
        """
        out = {}
        for key, val in self.iteritems():
            out[key] = val

        return out

##
# 30.11.2004, c
# 01.12.2004
# 01.12.2004
class OneTypeList(list):

    def __init__(self, item_class, seq=None):
        self.item_class = item_class

        if seq is not None:
            for obj in seq:
                self.append(obj)

    def __setitem__(self, key, value):
        if (type(value) in (list, tuple)):
            for ii, val in enumerate(value):
                if not isinstance(val, self.item_class):
                    raise TypeError
        else:
            if not isinstance(value, self.item_class):
                raise TypeError
        list.__setitem__(self, key, value)

    ##
    # 21.11.2005, c
    def __getitem__(self, ii):
        if isinstance(ii, int):
            return list.__getitem__(self, ii)
        elif isinstance(ii, basestr):
            ir = self.find(ii, ret_indx=True)
            if ir:
                return list.__getitem__(self, ir[0])
            else:
                raise IndexError, ii
        else:
            raise IndexError, ii

    def __str__(self):
        ss = "[\n"
        for ii in self:
            aux = "\n" + ii.__str__()
            aux = aux.replace("\n", "\n  ")
            ss += aux[1:] + "\n"
        ss += "]"
        return(ss)

    def find(self, name, ret_indx=False):
        for ii, item in enumerate(self):
            if item.name == name:
                if ret_indx:
                    return ii, item
                else:
                    return item
        return None

    ##
    # 12.06.2007, c
    def print_names(self):
        print [ii.name for ii in self]

    def get_names(self):
        return [ii.name for ii in self]

class Output(Struct):
    """
    Factory class providing output (print) functions. All SfePy
    printing should be accomplished by this class.

    Examples
    --------
    >>> from sfepy.base.base import Output
    >>> output = Output('sfepy:')
    >>> output(1, 2, 3, 'hello')
    sfepy: 1 2 3 hello
    >>> output.prefix = 'my_cool_app:'
    >>> output(1, 2, 3, 'hello')
    my_cool_app: 1 2 3 hello
    """

    def __init__(self, prefix, filename=None, quiet=False, combined=False,
                 append=False, **kwargs):
        Struct.__init__(self, **kwargs)

        self.prefix = prefix

        self.set_output(filename=filename, quiet=quiet,
                        combined=combined, append=append)

    def __call__(self, *argc, **argv):
        """Call self.output_function.

        Parameters
        ----------
        argc : positional arguments
            The values to print.
        argv : keyword arguments
            The arguments to control the output behaviour. Supported keywords
            are listed below.
        verbose : bool (in **argv)
            No output if False.
        """
        verbose = argv.get('verbose', goptions['verbose'])
        if verbose:
            self.output_function(*argc, **argv)

    def set_output(self, filename=None, quiet=False, combined=False,
                   append=False):
        """
        Set the output mode.

        If `quiet` is `True`, no messages are printed to screen. If
        simultaneously `filename` is not `None`, the messages are logged
        into the specified file.

        If `quiet` is `False`, more combinations are possible. If
        `filename` is `None`, output is to screen only, otherwise it is
        to the specified file. Moreover, if `combined` is `True`, both
        the ways are used.

        Parameters
        ----------
        filename : str or file object
            Print messages into the specified file.
        quiet : bool
            Do not print anything to screen.
        combined : bool
            Print both on screen and into the specified file.
        append : bool
            Append to an existing file instead of overwriting it. Use with
            `filename`.
        """
        if not isinstance(filename, basestr):
            # filename is a file descriptor.
            append = True

        self.level = 0

        def output_none(*argc, **argv):
            pass

        def output_screen(*argc, **argv):
            format = '%s' + ' %s' * (len(argc) - 1)
            msg = format % argc

            if msg.startswith('...'):
                self.level -= 1

            print self._prefix + ('  ' * self.level) + msg

            if msg.endswith('...'):
                self.level += 1

        def print_to_file(filename, msg):
            if isinstance(filename, basestr):
                fd = open(filename, 'a')

            else:
                fd = filename

            print >>fd, self._prefix + ('  ' * self.level) + msg

            if isinstance(filename, basestr):
                fd.close()

            else:
                fd.flush()

        def output_file(*argc, **argv):
            format = '%s' + ' %s' * (len(argc) - 1)
            msg = format % argc

            if msg.startswith('...'):
                self.level -= 1

            print_to_file(filename, msg)

            if msg.endswith('...'):
                self.level += 1

        def output_combined(*argc, **argv):
            format = '%s' + ' %s' * (len(argc) - 1)
            msg = format % argc

            if msg.startswith('...'):
                self.level -= 1

            print self._prefix + ('  ' * self.level) + msg

            print_to_file(filename, msg)

            if msg.endswith('...'):
                self.level += 1

        def reset_file(filename):
            if isinstance(filename, basestr):
                output_dir = os.path.dirname(filename)
                if output_dir and not os.path.exists(output_dir):
                    os.makedirs(output_dir)

                fd = open(filename, 'w')
                fd.close()

            else:
                raise ValueError('cannot reset a file object!')

        if quiet is True:
            if filename is not None:
                if not append:
                    reset_file(filename)

                self.output_function = output_file

            else:
                self.output_function = output_none

        else:
            if filename is None:
                self.output_function = output_screen

            else:
                if not append:
                    reset_file(filename)

                if combined:
                    self.output_function = output_combined

                else:
                    self.output_function = output_file

    def get_output_function(self):
        return self.output_function

    def set_output_prefix(self, prefix):
        assert_(isinstance(prefix, basestr))
        if len(prefix) > 0:
            prefix += ' '
        self._prefix = prefix

    def get_output_prefix(self):
        return self._prefix[:-1]
    prefix = property(get_output_prefix, set_output_prefix)

output = Output('sfepy:')

def configure_output(options):
    """
    Configure the standard :function:`output()` function using
    `output_log_name` and `output_screen` attributes of `options`.

    Parameters
    ----------
    options : Struct or dict
        The options with `output_screen` and `output_log_name` items. Defaults
        are provided if missing.
    """
    output_screen = options.get('output_screen', True)
    output_log_name = options.get('output_log_name', None)

    output.set_output(filename=output_log_name, quiet=not output_screen,
                      combined=output_screen and (output_log_name is not None))

def print_structs(objs):
    """Print Struct instances in a container, works recursively. Debugging
    utility function."""
    if isinstance(objs, dict):
        for key, vals in objs.iteritems():
            print key
            print_structs(vals)
    elif isinstance(objs, list):
        for vals in objs:
            print_structs(vals)
    else:
        print objs

def iter_dict_of_lists(dol, return_keys=False):
    for key, vals in dol.iteritems():
        for ii, val in enumerate(vals):
            if return_keys:
                yield key, ii, val
            else:
                yield val

##
# 19.07.2005, c
# 26.05.2006
# 17.10.2007
def dict_to_struct(*args, **kwargs):
    """Convert a dict instance to a Struct instance."""
    try:
        level = kwargs['level']
    except:
        level = 0

    try:
        flag = kwargs['flag']
    except:
        flag = (1,)

    # For level 0 only...
    try:
        constructor = kwargs['constructor']
    except:
        constructor = Struct

    out = []
    for arg in args:
        if type(arg) == dict:
            if flag[level]:
                aux = constructor()
            else:
                aux = {}

            for key, val in arg.iteritems():
                if type(val) == dict:
                    try:
                        flag[level + 1]
                    except:
                        flag = flag + (0,)
                    val2 = dict_to_struct(val, level=level + 1, flag=flag)
                    if flag[level]:
                        aux.__dict__[key] = val2
                    else:
                        aux[key] = val2
                else:
                    if flag[level]:
                        aux.__dict__[key] = val
                    else:
                        aux[key] = val
            out.append(aux)
        else:
            out.append(arg)

    if len(out) == 1:
        out = out[0]

    return out

##
# 23.01.2006, c
def is_sequence(var):
    if issubclass(var.__class__, tuple) or issubclass(var.__class__, list):
        return True
    else:
        return False

##
# 17.10.2007, c
def is_derived_class(cls, parent):
    return issubclass(cls, parent) and (cls is not parent)

##
# 23.10.2007, c
def insert_static_method(cls, function):
    setattr(cls, function.__name__, staticmethod(function))

##
# 23.10.2007, c
def insert_method(instance, function):
    setattr(instance, function.__name__,
             UnboundMethodType(function, instance, instance.__class__))

def use_method_with_name(instance, method, new_name):
    setattr(instance, new_name, method)

def insert_as_static_method(cls, name, function):
    setattr(cls, name, staticmethod(function))

def find_subclasses(context, classes, omit_unnamed=False, name_attr='name'):
    """Find subclasses of the given classes in the given context.

    Examples
    --------

    >>> solver_table = find_subclasses(vars().items(),
                                       [LinearSolver, NonlinearSolver,
                                        TimeSteppingSolver, EigenvalueSolver,
                                        OptimizationSolver])
    """
    var_dict = context.items()
    table = {}

    for key, var in var_dict:
        try:
            for cls in classes:
                if is_derived_class(var, cls):
                    if hasattr(var, name_attr):
                        key = getattr(var, name_attr)
                        if omit_unnamed and not key:
                            continue

                    elif omit_unnamed:
                        continue

                    else:
                        key = var.__class__.__name__

                    table[key] = var
                    break

        except TypeError:
            pass
    return table

def load_classes(filenames, classes, package_name=None, ignore_errors=False,
                 name_attr='name'):
    """
    For each filename in filenames, load all subclasses of classes listed.
    """
    table = {}
    for filename in filenames:
        if not ignore_errors:
            mod = import_file(filename, package_name=package_name)

        else:
            try:
                mod = import_file(filename, package_name=package_name)

            except:
                output('WARNING: module %s cannot be imported!' % filename)
                output('reason:\n', sys.exc_info()[1])
                continue

        table.update(find_subclasses(vars(mod), classes, omit_unnamed=True,
                                     name_attr=name_attr))

    return table

def update_dict_recursively(dst, src, tuples_too=False,
                            overwrite_by_none=True):
    """
    Update `dst` dictionary recursively using items in `src` dictionary.

    Parameters
    ----------
    dst : dict
        The destination dictionary.
    src : dict
        The source dictionary.
    tuples_too : bool
        If True, recurse also into dictionaries that are members of tuples.
    overwrite_by_none : bool
        If False, do not overwrite destination dictionary values by None.

    Returns
    -------
    dst : dict
        The destination dictionary.
    """
    def tuplezip(a):
        if isinstance(a[0], dict) and isinstance(a[1], dict):
            return update_dict_recursively(a[0], a[1], True)
        return a[1]

    for key in src:
        if key in dst:
            if isinstance(src[key], dict) and isinstance(dst[key], dict):
                dst[key] = update_dict_recursively(dst[key],
                                                   src[key], tuples_too)
                continue

            if tuples_too and isinstance(dst[key], tuple) \
                   and isinstance(src[key], tuple):
                dst[key] = tuple(map(tuplezip,
                                     zip(src[key], dst[key]))[:len(dst[key])])
                continue

        if overwrite_by_none or not src[key] is None:
            dst[key] = src[key]

    return dst

def edit_tuple_strings(str_tuple, old, new, recur=False):
    """
    Replace substrings `old` with `new` in items of tuple
    `str_tuple`. Non-string items are just copied to the new tuple.

    Parameters
    ----------
    str_tuple : tuple
        The tuple with string values.
    old : str
        The old substring.
    new : str
        The new substring.
    recur : bool
        If True, edit items that are tuples recursively.

    Returns
    -------
    new_tuple : tuple
        The tuple with edited strings.
    """
    new_tuple = []
    for item in str_tuple:
        if isinstance(item, basestr):
            item = item.replace(old, new)

        elif recur and isinstance(item, tuple):
            item = edit_tuple_strings(item, old, new, recur=True)

        new_tuple.append(item)

    return tuple(new_tuple)

def edit_dict_strings(str_dict, old, new, recur=False):
    """
    Replace substrings `old` with `new` in string values of dictionary
    `str_dict`. Both `old` and `new` can be lists of the same length - items
    in `old` are replaced by items in `new` with the same index.

    Parameters
    ----------
    str_dict : dict
        The dictionary with string values or tuples containing strings.
    old : str or list of str
        The old substring or list of substrings.
    new : str or list of str
        The new substring or list of substrings.
    recur : bool
        If True, edit tuple values recursively.

    Returns
    -------
    new_dict : dict
        The dictionary with edited strings.
    """
    if isinstance(old, basestr):
        new_dict = {}
        for key, val in str_dict.iteritems():
            if isinstance(val, basestr):
                new_dict[key] = val.replace(old, new)

            elif isinstance(val, tuple):
                new_dict[key] = edit_tuple_strings(val, old, new, recur=recur)

            else:
                raise ValueError('unsupported value! (%s)' % type(val))

    else:
        assert_(len(old) == len(new))

        new_dict = dict(str_dict)
        for ii, _old in enumerate(old):
            new_dict.update(edit_dict_strings(new_dict, _old, new[ii],
                                              recur=recur))

    return new_dict

def invert_dict(d, is_val_tuple=False, unique=True):
    """
    Invert a dictionary by making its values keys and vice versa.

    Parameters
    ----------
    d : dict
        The input dictionary.
    is_val_tuple : bool
        If True, the `d` values are tuples and new keys are the tuple items.
    unique : bool
        If True, the `d` values are unique and so the mapping is
        one to one. If False, the `d` values (possibly) repeat, so the inverted
        dictionary will have as items lists of corresponding keys.

    Returns
    -------
    di : dict
        The inverted dictionary.
    """
    di = {}

    for key, val in d.iteritems():
        if unique:
            if is_val_tuple:
                for v in val:
                    di[v] = key
            else:
                di[val] = key

        else:
            if is_val_tuple:
                for v in val:
                    item = di.setdefault(v, [])
                    item.append(key)

            else:
                item = di.setdefault(val, [])
                item.append(key)

    return di

def remap_dict(d, map):
    """
    Utility function to remap state dict keys according to var_map.
    """
    out = {}
    for new_key, key in map.iteritems():
        out[new_key] = d[key]

    return out

##
# 24.08.2006, c
# 05.09.2006
def dict_from_keys_init(keys, seq_class=None):

    if seq_class is None:
        return {}.fromkeys(keys)

    out = {}
    for key in keys:
        out[key] = seq_class()
    return out

##
# 16.10.2006, c
def dict_extend(d1, d2):
    for key, val in d1.iteritems():
        val.extend(d2[key])

def get_subdict(adict, keys):
    """
    Get a sub-dictionary of `adict` with given `keys`.
    """
    return dict((key, adict[key]) for key in keys if key in adict)

def set_defaults(dict_, defaults):
    for key, val in defaults.iteritems():
        dict_.setdefault(key, val)

##
# c: 12.03.2007, r: 04.04.2008
def get_default(arg, default, msg_if_none=None):
    if arg is None:
        out = default
    else:
        out = arg

    if (out is None) and (msg_if_none is not None):
        raise ValueError(msg_if_none)

    return out

##
# c: 28.04.2008, r: 28.04.2008
def get_default_attr(obj, attr, default, msg_if_none=None):
    if hasattr(obj, attr):
        out = getattr(obj, attr)
    else:
        out = default

    if (out is None) and (msg_if_none is not None):
        raise ValueError(msg_if_none)

    return out

def get_arguments(omit=None):
    """Get a calling function's arguments.

    Returns:

    args : dict
        The calling function's  arguments.
    """
    from inspect import getargvalues, stack
    if omit is None:
        omit = []

    _args, _, _, _vars = getargvalues(stack()[1][0])

    args = {}
    for name in _args:
        if name in omit: continue
        args[name] = _vars[name]

    return args

def check_names(names1, names2, msg):
    """Check if all names in names1 are in names2, otherwise raise IndexError
    with the provided message msg.
    """
    names = set(names1)
    both = names.intersection(names2)
    if both != names:
        missing = ', '.join(ii for ii in names.difference(both))
        raise IndexError(msg % missing)

##
# c: 27.02.2008, r: 27.02.2008
def select_by_names(objs_all, names, replace=None, simple=True):
    objs = {}
    for key, val in objs_all.iteritems():
        if val.name in names:
            if replace is None:
                objs[key] = val
            else:
                new_val = copy(val)
                old_attr = getattr(val, replace[0])
                if simple:
                    new_attr = old_attr % replace[1]
                    setattr(new_val, replace[0], new_attr)
                else:
                    new_attr = replace[1].get(val.name, old_attr)
                    setattr(new_val, replace[0], new_attr)
                objs[key] = new_val
    return objs

def ordered_iteritems(adict):
    keys = adict.keys()
    order = nm.argsort(keys)
    for ii in order:
        key = keys[ii]
        yield key, adict[key]

def dict_to_array(adict):
    """
    Convert a dictionary of nD arrays of the same shapes with
    non-negative integer keys to a single (n+1)D array.
    """
    keys = adict.keys()
    ik = nm.array(keys, dtype=nm.int32)
    assert_((ik >= 0).all())

    if ik.shape[0] == 0:
        return nm.zeros((0,), dtype=nm.int32)

    aux = nm.asarray(adict[ik[0]])
    out = nm.empty((ik.max() + 1,) + aux.shape, dtype=aux.dtype)
    out.fill(-1)
    for key, val in adict.iteritems():
        out[key] = val

    return out

def as_float_or_complex(val):
    """
    Try to cast val to Python float, and if this fails, to Python
    complex type.
    """
    success = False
    try:
        out = float(val)
    except:
        pass
    else:
        success = True

    if not success:
        try:
            out = complex(val)
        except:
            pass
        else:
            success = True

    if not success:
        raise ValueError('cannot cast %s to float or complex!' % val)

    return out

########NEW FILE########
__FILENAME__ = compat
"""
This module contains functions that have different names or behavior
depending on NumPy and Scipy versions.
"""
import numpy as nm
import scipy as sc

__all__ = ['in1d', 'unique']

try:
    in1d = nm.in1d

except AttributeError:
    in1d = nm.setmember1d

unique = nm.unique
try:
    nm.unique([0], return_index=True, return_inverse=True)

except TypeError:
    unique = nm.unique1d


try:
    factorial = sc.factorial

except AttributeError:
    import scipy.misc as scm

    factorial = scm.factorial

########NEW FILE########
__FILENAME__ = conf
"""
Problem description file handling.

Notes
-----

Short syntax: key is suffixed with '__<number>' to prevent collisions with long
syntax keys -> both cases can be used in a single input.
"""
import re
import numpy as nm

from sfepy.base.base import (Struct, IndexedStruct, dict_to_struct,
                             output, copy, update_dict_recursively,
                             import_file, assert_, get_default, basestr)
from sfepy.base.parse_conf import create_bnf

_required = ['filename_mesh|filename_domain', 'field_[0-9]+|fields',
             'ebc_[0-9]+|ebcs', 'equations',
             'region_[0-9]+|regions', 'variable_[0-9]+|variables',
             'material_[0-9]+|materials',
             'solver_[0-9]+|solvers']
_other = ['epbc_[0-9]+|epbcs', 'lcbc_[0-9]+|lcbcs', 'nbc_[0-9]+|nbcs',
          'ic_[0-9]+|ics', 'function_[0-9]+|functions', 'options',
          'integral_[0-9]+|integrals']

def get_standard_keywords():
    return copy(_required), copy(_other)

def tuple_to_conf(name, vals, order):
    """
    Convert a configuration tuple `vals` into a Struct named `name`, with
    attribute names given in and ordered by `order`.

    Items in `order` at indices outside the length of `vals` are ignored.
    """
    conf = Struct(name=name)
    for ii, key in enumerate(order[:len(vals)]):
        setattr(conf, key, vals[ii])
    return conf

def transform_variables(adict):
    d2 = {}
    for ii, (key, conf) in enumerate(adict.iteritems()):
        if isinstance(conf, tuple):
            c2 = tuple_to_conf(key, conf, ['kind', 'field'])
            if len(conf) >= 3:
                kind = c2.kind.split()[0]
                if kind == 'unknown':
                    c2.order = conf[2]
                elif kind == 'test':
                    c2.dual = conf[2]
                elif kind == 'parameter':
                    if isinstance(conf[2], basestr) or (conf[2] is None):
                        c2.like = conf[2]
                    else:
                        c2.like = None
                        c2.special = conf[2]
                if len(conf) == 4:
                    c2.history = conf[3]
            d2['variable_%s__%d' % (c2.name, ii)] = c2
        else:
            c2 = transform_to_struct_1(conf)
            d2['variable_'+c2.name] = c2
    return d2

def transform_conditions(adict, prefix):
    d2 = {}
    for ii, (key, conf) in enumerate(adict.iteritems()):
        if isinstance(conf, tuple):
            if len(conf) == 2:
                c2 = tuple_to_conf(key, conf, ['region', 'dofs'])

            else:
                c2 = tuple_to_conf(key, conf, ['region'])

                if isinstance(conf[1], dict):
                    c2.dofs, c2.filename = conf[1:]

                else:
                    c2.times, c2.dofs = conf[1:]

            d2['%s_%s__%d' % (prefix, c2.name, ii)] = c2

        else:
            c2 = transform_to_struct_1(conf)
            d2['%s_%s' % (prefix, c2.name)] = c2

    return d2

def transform_ebcs(adict):
    return transform_conditions(adict, 'ebc')

def transform_ics(adict):
    return transform_conditions(adict, 'ic')

def transform_lcbcs(adict):
    return transform_conditions(adict, 'lcbc')

def transform_epbcs(adict):
    d2 = {}
    for ii, (key, conf) in enumerate(adict.iteritems()):
        if isinstance(conf, tuple):
            if len(conf) == 3:
                c2 = tuple_to_conf(key, conf, ['region', 'dofs', 'match'])

            else:
                c2 = tuple_to_conf(key, conf,
                                   ['region', 'times', 'dofs', 'match'])

            d2['epbcs_%s__%d' % (c2.name, ii)] = c2
        else:
            c2 = transform_to_struct_1(conf)
            d2['epbcs_%s' % c2.name] = c2
    return d2

def transform_regions(adict):
    d2 = {}
    for ii, (key, conf) in enumerate(adict.iteritems()):
        if isinstance(conf, basestr):
            c2 = Struct(name=key, select=conf)
            d2['region_%s__%d' % (c2.name, ii)] = c2
        elif isinstance(conf, tuple):
            c2 = tuple_to_conf(key, conf, ['select', 'kind'])
            if len(conf) == 3:
                c2.parent = conf[2]
            d2['region_%s__%d' % (c2.name, ii)] = c2
        else:
            c2 = transform_to_struct_1(conf)
            d2['region_'+c2.name] = c2
    return d2

def transform_integrals(adict):
    d2 = {}
    for ii, (key, conf) in enumerate(adict.iteritems()):
        if isinstance(conf, int):
            c2 = Struct(name=key, order=conf)
            d2['integral_%s__%d' % (c2.name, ii)] = c2

        elif isinstance(conf, tuple):
            if len(conf) == 2: # Old tuple version with now-ignored 'kind'.
                conf = conf[1]
                c2 = Struct(name=key, order=conf)

            elif len(conf) == 3:
                c2 = tuple_to_conf(key, conf, ['order', 'vals', 'weights'])

            d2['integral_%s__%d' % (c2.name, ii)] = c2

        else:
            c2 = transform_to_struct_1(conf)
            d2['integral_'+c2.name] = c2

    return d2

def transform_fields(adict):
    dtypes = {'real' : nm.float64, 'complex' : nm.complex128}
    d2 = {}
    for ii, (key, conf) in enumerate(adict.iteritems()):
        if isinstance(conf, tuple):
            c2 = tuple_to_conf(key, conf,
                               ['dtype', 'shape', 'region', 'approx_order',
                                'space', 'poly_space_base'])
            if c2.dtype in dtypes:
                c2.dtype = dtypes[c2.dtype]
            d2['field_%s__%d' % (c2.name, ii)] = c2
        else:
            c2 = transform_to_struct_1(conf)
            c2.set_default('dtype', nm.float64)
            if c2.dtype in dtypes:
                c2.dtype = dtypes[c2.dtype]
            d2['field_'+c2.name] = c2
    return d2

def transform_materials(adict):
    d2 = {}
    for ii, (key, conf) in enumerate(adict.iteritems()):
        if isinstance(conf, basestr):
            c2 = Struct(name=key, function=conf)
            d2['material_%s__%d' % (c2.name, ii)] = c2

        elif isinstance(conf, tuple):
            c2 = tuple_to_conf(key, conf,
                               ['values', 'function', 'kind'])
            if len(conf) == 4:
                c2.flags = conf[3]
            d2['material_%s__%d' % (c2.name, ii)] = c2

        else:
            c2 = transform_to_struct_1(conf)
            d2['material_'+conf['name']] = c2

    return d2

def transform_solvers(adict):
    d2 = {}
    for ii, (key, conf) in enumerate(adict.iteritems()):
        if isinstance(conf, tuple):
            c2 = tuple_to_conf(key, conf, ['kind','params'])
            for param, val in c2.params.iteritems():
                setattr(c2, param, val)
            delattr(c2, 'params')
            d2['solvers_%s__%d' % (c2.name, ii)] = c2
        else:
            c2 = transform_to_struct_1(conf)
            d2['solvers_'+c2.name] = c2
    return d2

def transform_functions(adict):
    d2 = {}
    for ii, (key, conf) in enumerate(adict.iteritems()):
        if isinstance(conf, tuple):
            c2 = tuple_to_conf(key, conf, ['function'])
            d2['function_%s__%d' % (c2.name, ii)] = c2
        else:
            c2 = transform_to_struct_1(conf)
            d2['function_'+c2.name] = c2
    return d2

def transform_to_struct_1(adict):
    return dict_to_struct(adict, flag=(1,))
def transform_to_i_struct_1(adict):
    return dict_to_struct(adict, flag=(1,), constructor=IndexedStruct)
def transform_to_struct_01(adict):
    return dict_to_struct(adict, flag=(0,1))
def transform_to_struct_10(adict):
    return dict_to_struct(adict, flag=(1,0))

transforms = {
    'options'   : transform_to_i_struct_1,
    'solvers'   : transform_solvers,
    'integrals' : transform_integrals,
    'opt'       : transform_to_struct_1,
    'regions'   : transform_regions,
    'shape_opt' : transform_to_struct_10,
    'fields'    : transform_fields,
    'variables' : transform_variables,
    'ebcs'      : transform_ebcs,
    'epbcs'     : transform_epbcs,
    'nbcs'      : transform_to_struct_01,
    'lcbcs'     : transform_lcbcs,
    'ics'       : transform_ics,
    'materials' : transform_materials,
    'functions' : transform_functions,
}

def dict_from_string(string):
    """
    Parse `string` and return a dictionary that can be used to
    construct/override a ProblemConf instance.
    """
    if string is None:
        return {}

    if isinstance(string, dict):
        return string

    parser = create_bnf()

    out = {}
    for r in parser.parseString(string, parseAll=True):
        out.update(r)

    return out

def dict_from_options(options):
    """
    Return a dictionary that can be used to construct/override a ProblemConf
    instance based on `options`.

    See ``--conf`` and ``--options`` options of the ``simple.py`` script.
    """
    override = dict_from_string(options.conf)
    if options.app_options:
        if not 'options' in override:
            override['options'] = {}

        override_options = dict_from_string(options.app_options)
        override['options'].update(override_options)

    return override

##
# 27.10.2005, c
class ProblemConf(Struct):
    """
    Problem configuration, corresponding to an input (problem description
    file). It validates the input using lists of required and other keywords
    that have to/can appear in the input. Default keyword lists can be obtained
    by sfepy.base.conf.get_standard_keywords().

    ProblemConf instance is used to construct a Problem instance via
    Problem.from_conf(conf).
    """

    @staticmethod
    def from_file(filename, required=None, other=None, verbose=True,
                  define_args=None, override=None, setup=True):
        """
        Loads the problem definition from a file.

        The filename can either contain plain definitions, or it can contain
        the define() function, in which case it will be called to return the
        input definitions.

        The job of the define() function is to return a dictionary of
        parameters. How the dictionary is constructed is not our business, but
        the usual way is to simply have a function define() along these lines
        in the input file::

            def define():
                options = {
                    'save_eig_vectors' : None,
                    'eigen_solver' : 'eigen1',
                }
                region_2 = {
                    'name' : 'Surface',
                    'select' : 'nodes of surface',
                }
                return locals()

        Optionally, the define() function can accept additional arguments
        that should be defined using the `define_args` tuple or dictionary.
        """
        funmod = import_file(filename)

        if "define" in funmod.__dict__:
            if define_args is None:
                define_dict = funmod.__dict__["define"]()

            else:
                if isinstance(define_args, str):
                    define_args = dict_from_string(define_args)

                if isinstance(define_args, dict):
                    define_dict = funmod.__dict__["define"](**define_args)

                else:
                    define_dict = funmod.__dict__["define"](*define_args)

        else:
            define_dict = funmod.__dict__

        obj = ProblemConf(define_dict, funmod=funmod, filename=filename,
                          required=required, other=other, verbose=verbose,
                          override=override, setup=setup)

        return obj

    @staticmethod
    def from_file_and_options(filename, options, required=None, other=None,
                              verbose=True, define_args=None, setup=True):
        """
        Utility function, a wrapper around ProblemConf.from_file() with
        possible override taken from `options`.
        """
        override = dict_from_options(options)
        obj = ProblemConf.from_file(filename, required=required, other=other,
                                    verbose=verbose, define_args=define_args,
                                    override=override, setup=setup)
        return obj

    @staticmethod
    def from_module(module, required=None, other=None, verbose=True,
                    override=None, setup=True):
        obj = ProblemConf(module.__dict__, module, module.__name__,
                          required, other, verbose, override, setup=setup)

        return obj

    @staticmethod
    def from_dict(dict_, funmod, required=None, other=None, verbose=True,
                  override=None, setup=True):
        obj = ProblemConf(dict_, funmod, None, required, other, verbose,
                          override, setup=setup)

        return obj

    def __init__(self, define_dict, funmod=None, filename=None,
                 required=None, other=None, verbose=True, override=None,
                 setup=True):
        if override:
            if isinstance(override, Struct):
                override = override.__dict__
            define_dict = update_dict_recursively(define_dict, override, True)

        self.__dict__.update(define_dict)
        self.verbose = verbose

        if setup:
            self.setup(funmod=funmod, filename=filename,
                       required=required, other=other)


    def setup(self, define_dict=None, funmod=None, filename=None,
              required=None, other=None):

        define_dict = get_default(define_dict, self.__dict__)

        self._filename = filename

        self.validate(required=required, other=other)

        self.transform_input_trivial()
        self._raw = {}
        for key, val in define_dict.iteritems():
            if isinstance(val, dict):
                self._raw[key] = copy(val)

        self.transform_input()
        self.funmod = funmod

    def _validate_helper(self, items, but_nots):
        keys = self.__dict__.keys()
        left_over = keys[:]
        if but_nots is not None:
            for item in but_nots:
                match = re.compile('^' + item + '$').match
                for key in keys:
                    if match(key):
                        left_over.remove(key)

        missing = []
        if items is not None:
            for item in items:
                found = False
                match = re.compile('^' + item + '$').match
                for key in keys:
                    if match(key):
                        found = True
                        left_over.remove(key)
                if not found:
                    missing.append(item)
        return left_over, missing

    def validate(self, required=None, other=None):
        required_left_over, required_missing \
                            = self._validate_helper(required, other)
        other_left_over, other_missing \
                         = self._validate_helper(other, required)

        assert_(required_left_over == other_left_over)

        if other_left_over and self.verbose:
            output('left over:', other_left_over)

        if required_missing:
            raise ValueError('required missing: %s' % required_missing)

        return other_missing

    def transform_input_trivial(self):
        """Trivial input transformations."""

        ##
        # Unordered inputs.
        tr_list = ['([a-zA-Z0-9]+)_[0-9]+']
        # Keywords not in 'required', but needed even empty (e.g. for
        # running tests).
        for key in transforms.keys():
            if not self.__dict__.has_key(key):
                self.__dict__[key] = {}

        keys = self.__dict__.keys()
        for item in tr_list:
            match = re.compile(item).match
            for key in keys:
                obj = match(key)
                if obj:
                    new = obj.group(1) + 's'
                    result = {key : self.__dict__[key]}
                    try:
                        self.__dict__[new].update(result)
                    except:
                        self.__dict__[new] = result

                    del self.__dict__[key]

    def transform_input(self):
        keys = self.__dict__.keys()
        for key, transform in transforms.iteritems():
            if not key in keys: continue
            self.__dict__[key] = transform(self.__dict__[key])

    def get_raw(self, key=None):
        if key is None:
            return self._raw
        else:
            return self._raw[key]

    def get_item_by_name(self, key, item_name):
        """
        Return item with name `item_name` in configuration group given
        by `key`.
        """
        val = getattr(self, key)
        for item in val.itervalues():
            if item.name == item_name:
                return item

    def get_function(self, name):
        """
        Get a function object given its name.

        It can be either in `ProblemConf.funmod`, or a `ProblemConf`
        attribute directly.

        Parameters
        ----------
        name : str or function or None
            The function name or directly the function.

        Returns
        -------
        fun : function or None
            The required function, or None if `name` was `None`.
        """
        if name is None:
            fun = None

        elif callable(name):
            import inspect
            if not (inspect.isfunction(name) or inspect.ismethod(name)):
                msg = '`name` has to have `str` or `function` type! (got %s)'
                raise ValueError(msg % type(name))
            fun = name

        else:
            try:
                fun = getattr(self.funmod, name)

            except AttributeError:
                try:
                    fun = getattr(self, name)

                except AttributeError:
                    raise ValueError('function %s cannot be found!' % name)

        return fun

    def edit(self, key, newval):
        self.__dict__[key] = transforms[key](newval)

    def update_conf(self, conf):
        """
        Update configuration by values in another problem configuration.

        Values that are dictionaries are updated in-place by ``dict.update()``.

        Parameters
        ----------
        conf : ProblemConf instance
            The other configuration.
        """
        for x in conf.__dict__:
            his = conf.__dict__[x]
            my = getattr(self, x, None)
            if isinstance(my, dict) and isinstance(his, dict):
                my.update(his)
            else:
                setattr(self, x, his)

    def add_missing(self, conf):
        """
        Add missing values from another problem configuration.

        Missing keys/values are added also to values that are dictionaries.

        Parameters
        ----------
        conf : ProblemConf instance
            The other configuration.
        """
        for x in conf.__dict__:
            his = conf.__dict__[x]
            my = getattr(self, x, None)
            if isinstance(my, dict) and isinstance(his, dict):
                for key in his:
                    if not my.has_key(key):
                        my[key]=his[key]
            elif my is None:
                setattr(self, x, his)

########NEW FILE########
__FILENAME__ = getch
"""
getch()-like unbuffered character reading from stdin on both Windows and Unix

_Getch classes inspired by Danny Yoo, iskeydown() based on code by Zachary
Pincus.
"""
import os, sys

class _Getch:
    """Gets a single character from standard input. Does not echo to the
    screen."""
    def __init__(self):
        if os.name == 'nt':
            self.impl = _GetchWindows()
        elif os.name == 'posix':
            self.impl = _GetchUnix()
        else:
            self.impl = _GetchDefault()

    def __call__(self): return self.impl()

class _GetchWindows:
    def __init__(self):
        import msvcrt
        self.msvcrt = msvcrt

    def __call__(self):
        msvcrt = self.msvcrt
        
        c = msvcrt.getch()
        if c == '\x00' or c == '\xE0':    #functions keys
            msvcrt.getch()
        return c

    def iskeydown(self):
        msvcrt = self.msvcrt
        return msvcrt.kbhit()

class _GetchUnix:
    def __init__(self):
        import tty, sys, select
        self.mods = (tty, sys, select)

    def __call__(self):
        tty, sys, select = self.mods

        fd = sys.stdin.fileno()
        old = tty.tcgetattr(fd)
        tty.setcbreak(fd, tty.TCSANOW)
        try:
            return sys.stdin.read(1)
        finally:
            tty.tcsetattr(fd, tty.TCSAFLUSH, old)

    def iskeydown(self):
        tty, sys, select = self.mods

        fd = sys.stdin.fileno()
        old = tty.tcgetattr(fd)
        tty.setcbreak(fd, tty.TCSANOW)
        try:
            if select.select([sys.stdin], [], [], 0) == ([sys.stdin], [], []):
                return sys.stdin.read(1)
            else:
                return False
        finally:
            tty.tcsetattr(fd, tty.TCSAFLUSH, old)

class _GetchDefault:
    def __call__(self):
        return raw_input()[0]

    def iskeydown(self):
        raise NotImplementedError

getch = _Getch()

if __name__ == '__main__':
    from base import pause, spause
    
    pause('press a key anytime the script stops!')
    pause()
    spause('last time...')
    print 'done.'

########NEW FILE########
__FILENAME__ = goptions
"""
Various global options/parameters.

Notes
-----
Inspired by rcParams of matplotlib.
"""

def validate_bool(val):
    """
    Convert b to a boolean or raise a ValueError.
    """
    if type(val) is str:
        val = val.lower()

    if val in ('t', 'y', 'yes', 'on', 'true', '1', 1, True):
        return True

    elif val in ('f', 'n', 'no', 'off', 'false', '0', 0, False):
        return False

    else:
        raise ValueError('Could not convert "%s" to boolean!' % val)

default_goptions = {
    'verbose' : [True, validate_bool],
    'check_term_finiteness' : [False, validate_bool],
}

class ValidatedDict(dict):
    """
    A dictionary object including validation.
    """
    validate = dict([(key, validator) for key, (default, validator) in \
                     default_goptions.iteritems()])

    def __setitem__(self, key, val):
        try:
            cval = self.validate[key](val)
            dict.__setitem__(self, key, cval)
        except KeyError:
            raise KeyError('%s is not a valid option.'
                           ' See goptions.keys() for a list of valid'
                           ' options.' % (key,))

    def keys(self):
        """
        Return sorted list of keys.
        """
        ks = dict.keys(self)
        ks.sort()
        return ks

    def values(self):
        """
        Return values in order of sorted keys.
        """
        return [self[key] for key in self.keys()]

goptions = ValidatedDict([(key, val[0])
                          for key, val in default_goptions.iteritems()])

########NEW FILE########
__FILENAME__ = ioutils
import numpy as nm
import os
import os.path as op
import fnmatch
import shutil
from base import output, Struct, basestr
try:
    import tables as pt
except:
    pt = None

class InDir(Struct):
    """
    Store the directory name a file is in, and prepend this name to other
    files.

    Examples
    --------

    >>> indir = InDir('output/file1')
    >>> print indir('file2')
    """
    def __init__(self, filename):
        self.dir = op.split(op.join(os.getcwd(), filename))[0]

    def __call__(self, filename):
        return op.join(self.dir, filename)

def ensure_path(filename):
    """
    Check if path to `filename` exists and if not, create the necessary
    intermediate directories.
    """
    dirname = os.path.dirname(filename)
    if dirname:
        if not os.path.exists(dirname):
            os.makedirs(dirname)

        if not os.path.isdir(dirname):
            raise IOError('cannot ensure path for "%s"!' % filename)

def locate_files(pattern, root_dir=os.curdir):
    """
    Locate all files matching fiven filename pattern in and below
    supplied root directory.
    """
    for dirpath, dirnames, filenames in os.walk(os.path.abspath(root_dir)):
        for filename in fnmatch.filter(filenames, pattern):
            yield os.path.join(dirpath, filename)

def remove_files(root_dir):
    """
    Remove all files and directories in supplied root directory.
    """
    for dirpath, dirnames, filenames in os.walk(os.path.abspath(root_dir)):
        for filename in filenames:
            os.remove(os.path.join(root_dir, filename))

        for dirname in dirnames:
            shutil.rmtree(os.path.join(root_dir, dirname))

##
# 27.04.2006, c
def get_trunk(filename):
    return op.splitext(op.basename(filename))[0]

def edit_filename(filename, prefix='', suffix='', new_ext=None):
    """
    Edit a file name by add a prefix, inserting a suffix in front of a file
    name extension or replacing the extension.

    Parameters
    ----------
    filename : str
        The file name.
    prefix : str
        The prefix to be added.
    suffix : str
        The suffix to be inserted.
    new_ext : str, optional
        If not None, it replaces the original file name extension.

    Returns
    -------
    new_filename : str
        The new file name.
    """
    path, filename = os.path.split(filename)
    base, ext = os.path.splitext(filename)

    if new_ext is None:
        new_filename = prefix + base + suffix + ext

    else:
        new_filename = prefix + base + suffix + new_ext

    return os.path.join(path, new_filename)

def get_print_info(n_step, fill=None):
    """
    Returns the max. number of digits in range(n_step) and the corresponding
    format string.

    Examples:

    >>> get_print_info(11)
    (2, '%2d')
    >>> get_print_info(8)
    (1, '%1d')
    >>> get_print_info(100)
    (2, '%2d')
    >>> get_print_info(101)
    (3, '%3d')
    >>> get_print_info(101, fill='0')
    (3, '%03d')
    """
    if n_step > 1:
        n_digit = int(nm.log10(n_step - 1) + 1)
        if fill is None:
            format = '%%%dd' % n_digit
        else:
            format = '%%%s%dd' % (fill, n_digit)
    else:
        n_digit, format = 0, None
    return n_digit, format

def skip_read_line(fd, no_eof=False):
    """
    Read the first non-empty line (if any) from the given file
    object. Return an empty string at EOF, if `no_eof` is False. If it
    is True, raise the EOFError instead.
    """
    ls = ''
    while 1:
        try:
            line = fd.readline()

        except EOFError:
            break

        if not line:
            if no_eof:
                raise EOFError

            else:
                break

        ls = line.strip()
        if ls and (ls[0] != '#'):
            break

    return ls

def read_token(fd):
    """
    Read a single token (sequence of non-whitespace characters) from the
    given file object.

    Notes
    -----
    Consumes the first whitespace character after the token.
    """
    out = ''
    # Skip initial whitespace.

    while 1:
        ch = fd.read(1)
        if ch.isspace(): continue
        elif len(ch) == 0: return out
        else: break

    while not ch.isspace():
        out = out + ch
        ch = fd.read(1)
        if len(ch) == 0: break

    return out

def read_array(fd, n_row, n_col, dtype):
    """
    Read a NumPy array of shape `(n_row, n_col)` from the given file
    object and cast it to type `dtype`.
    If `n_col` is None, determine the number of columns automatically.
    """
    if n_col is None:
        idx = fd.tell()
        row = fd.readline().split()
        fd.seek(idx)
        n_col = len(row)

    count = n_row * n_col
    val = nm.fromfile(fd, sep=' ', count=count)

    if val.shape[0] < count:
        raise ValueError('(%d, %d) array reading failed!' % (n_row, n_col))

    val = nm.asarray(val, dtype=dtype)
    val.shape = (n_row, n_col)

    return val

##
# c: 05.02.2008, r: 05.02.2008
def read_list(fd, n_item, dtype):
    vals = []
    ii = 0
    while ii < n_item:
        line = [dtype(ic) for ic in fd.readline().split()]
        vals.append(line)
        ii += len(line)
    if ii > n_item:
        output('corrupted row?', line, ii, n_item)
        raise ValueError

    return vals

def write_dict_hdf5(filename, adict, level=0, group=None, fd=None):

    if level == 0:
        fd = pt.openFile(filename, mode='w', title='Recursive dict dump')
        group = '/'

    for key, val in adict.iteritems():
        if isinstance(val, dict):
            group2 = fd.createGroup(group, '_' + str(key), '%s group' % key)
            write_dict_hdf5(filename, val, level + 1, group2, fd)
        else:
            fd.createArray(group, '_' + str(key), val, '%s data' % key)

    if level == 0:
        fd.close()

def read_dict_hdf5(filename, level=0, group=None, fd=None):
    out = {}

    if level == 0:
        fd = pt.openFile(filename, mode='r')
        group = fd.root

    for name, gr in group._v_groups.iteritems():
        name = name.replace('_', '', 1)
        out[name] = read_dict_hdf5(filename, level + 1, gr, fd)

    for name, data in group._v_leaves.iteritems():
        name = name.replace('_', '', 1)
        out[name] = data.read()

    if level == 0:
        fd.close()

    return out

##
# 02.07.2007, c
def write_sparse_matrix_hdf5(filename, mtx, name='a sparse matrix'):
    """Assume CSR/CSC."""
    fd = pt.openFile(filename, mode='w', title=name)
    try:
        info = fd.createGroup('/', 'info')
        fd.createArray(info, 'dtype', mtx.dtype.str)
        fd.createArray(info, 'shape', mtx.shape)
        fd.createArray(info, 'format', mtx.format)

        data = fd.createGroup('/', 'data')
        fd.createArray(data, 'data', mtx.data)
        fd.createArray(data, 'indptr', mtx.indptr)
        fd.createArray(data, 'indices', mtx.indices)

    except:
        print 'matrix must be in SciPy sparse CSR/CSC format!'
        print mtx.__repr__()
        raise

    fd.close()

##
# 02.07.2007, c
# 08.10.2007
def read_sparse_matrix_hdf5(filename, output_format=None):
    import scipy.sparse as sp
    constructors = {'csr' : sp.csr_matrix, 'csc' : sp.csc_matrix}

    fd = pt.openFile(filename, mode='r')
    info = fd.root.info
    data = fd.root.data

    format = info.format.read()
    if not isinstance(format, basestr):
        format = format[0]

    dtype = info.dtype.read()
    if not isinstance(dtype, basestr):
        dtype = dtype[0]

    if output_format is None:
        constructor = constructors[format]
    else:
        constructor = constructors[output_format]

    if format in ['csc', 'csr']:
        mtx = constructor((data.data.read(),
                           data.indices.read(), data.indptr.read()),
                          shape=info.shape.read(), dtype=dtype)
    elif format == 'coo':
        mtx = constructor((data.data.read(),
                           nm.c_[data.rows.read(), data.cols.read()].T),
                          shape=info.shape.read(), dtype=dtype)
    else:
        print format
        raise ValueError
    fd.close()

    if output_format in ['csc', 'csr']:
        mtx.sort_indices()

    return mtx

########NEW FILE########
__FILENAME__ = log
import time
import os
import atexit

try:
    from multiprocessing import Process, Pipe

except ImportError:
    Process = None

import numpy as nm

from sfepy.base.base import sfepy_config_dir, ordered_iteritems
from sfepy.base.base import output, get_default, set_defaults, Output, Struct
from sfepy.base.log_plotter import LogPlotter

_msg_no_live = """warning: log plot is disabled, install matplotlib
         (use GTKAgg backend) and multiprocessing"""

def get_logging_conf(conf, log_name='log'):
    """
    Check for a log configuration ('log' attribute by default) in
    `conf`. Supply default values if necessary.

    Parameters
    ----------
    conf : Struct
        The configuration object.
    log_name : str, optional
        The name of the log configuration attribute in `conf`.

    Returns
    -------
    log : dict
        The dictionary {'plot' : <figure_file>, 'text' : <text_log_file>}. One
        or both values can be None.
    """
    log = conf.get(log_name, None)

    default_log = {'text' : None, 'plot' : None}

    if log is None:
        log = default_log

    else:
        set_defaults(log, default_log)

    return log

def name_to_key(name, ii):
    return name + (':%d' % ii)

def read_log(filename):
    """
    Read data saved by :class:`Log` into a text file.

    Parameters
    ----------
    filename : str
        The name of a text log file.

    Returns
    -------
    log : dict
        The log with data names as keys and ``(xs, ys, vlines)`` as values.
    info : dict
        The log plot configuration with subplot numbers as keys.
    """
    log = {}
    info = {}

    fd = open(filename, 'r')

    last_xval = None
    for line in fd:
        if line[0] == '#':
            ls = line.split(':')
            if ls[0] == '# groups':
                n_gr = int(ls[1])
                for ig in range(n_gr):
                    fd.next()
                    line_info = fd.next()
                    xlabel, ylabel, yscales = line_info.split(',')

                    line_names = fd.next()
                    names = line_names.split(':')[1]

                    info[ig] = (xlabel.split(':')[1].strip().strip('"'),
                                ylabel.split(':')[1].strip().strip('"'),
                                yscales.split(':')[1].strip().strip('"'),
                                [name.strip().strip('"')
                                 for name in names.split(',')])

            continue

        ls = line.split(':')

        key = ls[0]
        xs, ys, vlines = log.setdefault(key, ([], [], []))

        if (len(ls) == 2) and (last_xval is not None):
            vlines.append(last_xval)

        else:
            try:
                xval, yval = float(ls[1]), float(ls[2])

            except ValueError:
                continue

            xs.append(xval)
            ys.append(yval)

            last_xval = xval

    fd.close()

    for key, (xs, ys, vlines) in log.iteritems():
        log[key] = (nm.array(xs), nm.array(ys), nm.array(vlines))

    return log, info

def plot_log(fig_num, log, info, xticks=None, yticks=None):
    """
    Plot log data returned by :func:`read_log()` into a specified figure.

    Parameters
    ----------
    fig_num : int
        The figure number.
    log : dict
        The log with data names as keys and ``(xs, ys, vlines)`` as values.
    info : dict
        The log plot configuration with subplot numbers as keys.
    xticks : list of arrays, optional
        The list of x-axis ticks (array or None) for each subplot.
    yticks : list of arrays, optional
        The list of y-axis ticks (array or None) for each subplot.
    """
    import matplotlib.pyplot as plt

    fig = plt.figure(fig_num)
    fig.clf()

    n_gr = len(info)
    n_col = min(5.0, nm.fix(nm.sqrt(n_gr)))
    if int(n_col) == 0:
        n_row = 0

    else:
        n_row = int(nm.ceil(n_gr / n_col))
        n_col = int(n_col)

    if xticks is None:
        xticks = [None] * n_gr

    if yticks is None:
        yticks = [None] * n_gr

    for ii, (xlabel, ylabel, yscale, names) in info.iteritems():
        ax = fig.add_subplot(n_row, n_col, ii + 1)
        ax.set_yscale(yscale)

        if xlabel:
            ax.set_xlabel(xlabel)

        if ylabel:
            ax.set_ylabel(ylabel)

        for name in names:
            xs, ys, vlines = log[name]
            ax.plot(xs, ys, label=name)

            for x in vlines:
                ax.axvline(x, color='k', alpha=0.3)

        if xticks[ii] is not None:
            ax.set_xticks(xticks[ii])

        else:
            ax.locator_params(axis='x', nbins=10)

        if yticks[ii] is not None:
            ax.set_yticks(yticks[ii])

        ax.legend(loc='best')

    plt.tight_layout(pad=0.5)

class Log(Struct):
    """
    Log data and (optionally) plot them in the second process via
    LogPlotter.
    """
    count = -1

    @staticmethod
    def from_conf(conf, data_names):
        """
        Parameters
        ----------
        data_names : list of lists of str
            The data names grouped by subplots: [[name1, name2, ...], [name3,
            name4, ...], ...], where name<n> are strings to display in
            (sub)plot legends.
        """
        obj = Log(data_names, **conf)

        return obj

    def __init__(self, data_names=None, xlabels=None, ylabels=None,
                 yscales=None, is_plot=True, aggregate=200,
                 log_filename=None, formats=None):
        """
        Parameters
        ----------
        data_names : list of lists of str
            The data names grouped by subplots: [[name1, name2, ...], [name3,
            name4, ...], ...], where name<n> are strings to display in
            (sub)plot legends.
        xlabels : list of str
            The x axis labels of subplots.
        ylabels : list of str
            The y axis labels of subplots.
        yscales : list of 'linear' or 'log'
            The y axis scales of subplots.
        is_plot : bool
            If True, try to use LogPlotter for plotting.
        aggregate : int
            The number of plotting commands to process before a redraw.
        log_filename : str, optional
            If given, save log data into a log file.
        formats : list of lists of number format strings
            The print formats of data to be used in a log file, group in the
            same way as subplots.
        """
        try:
            import matplotlib as mpl
        except:
            mpl = None

        if (mpl is not None) and mpl.rcParams['backend'] == 'GTKAgg':
            can_live_plot = True
        else:
            can_live_plot = False

        Struct.__init__(self, data_names = {},
                        n_arg = 0, n_gr = 0,
                        data = {}, x_values = {}, n_calls = 0,
                        yscales = {}, xlabels = {}, ylabels = {},
                        plot_pipe = None, formats = {}, output = None)

        if data_names is not None:
            n_gr = len(data_names)
        else:
            n_gr = 0
            data_names = []

        yscales = get_default(yscales, ['linear'] * n_gr)
        xlabels = get_default(xlabels, ['iteration'] * n_gr)
        ylabels = get_default(ylabels, [''] * n_gr)

        if formats is None:
            formats = [None] * n_gr

        for ig, names in enumerate(data_names):
            self.add_group(names, yscales[ig], xlabels[ig], ylabels[ig],
                           formats[ig])

        self.is_plot = get_default(is_plot, True)
        self.aggregate = get_default(aggregate, 100)

        self.can_plot = (can_live_plot and (mpl is not None)
                         and (Process is not None))

        if log_filename is not None:
            self.output = Output('', filename=log_filename)
            self.output('# started: %s' % time.asctime())
            self.output('# groups: %d' % n_gr)
            for ig, names in enumerate(data_names):
                self.output('#   %d' % ig)
                self.output('#     xlabel: "%s", ylabel: "%s", yscales: "%s"'
                            % (xlabels[ig], ylabels[ig], yscales[ig]))
                self.output('#     names: "%s"' % ', '.join(names))

        if self.is_plot and (not self.can_plot):
            output(_msg_no_live)

    def add_group(self, names, yscale=None, xlabel=None, ylabel=None,
                  formats=None):
        """
        Add a new data group. Notify the plotting process if it is
        already running.
        """
        ig = self.n_gr
        self.n_gr += 1

        self.x_values[ig] = []

        self.data_names[ig] = names
        self.yscales[ig] = yscale
        self.xlabels[ig] = xlabel
        self.ylabels[ig] = ylabel

        ii = self.n_arg
        for iseq, name in enumerate(names):
            key = name_to_key(name, ii)
            self.data[key] = []
            ii += 1

            if formats is not None:
                self.formats[key] = formats[iseq]
            else:
                self.formats[key] = '%.3e'

        self.n_arg = ii

        if self.plot_pipe is not None:
            send = self.plot_pipe.send
            send(['add_axis', ig, names, yscale, xlabel, ylabel])

    def iter_names(self, igs=None):
        if igs is None:
            igs = nm.arange(self.n_gr)

        ii = iseq = 0
        for ig, names in ordered_iteritems(self.data_names):
            for name in names:
                if ig in igs:
                    yield ig, ii, iseq, name
                    iseq += 1
                ii += 1

    def get_log_name(self):
        return os.path.join(sfepy_config_dir,
                            'plotter_%03d.log' % self.__class__.count)


    def __call__(self, *args, **kwargs):
        """
        Log the data passed via *args, and send them to the plotting
        process, if available.
        """
        finished = False
        save_figure = ''
        x_values = None
        igs = nm.arange(self.n_gr)
        full = True
        if kwargs:
            if 'finished' in kwargs:
                finished = kwargs['finished']
            if 'save_figure' in kwargs:
                save_figure = kwargs['save_figure']
            if 'x' in kwargs:
                x_values = kwargs['x']

            if 'igs' in kwargs:
                igs = nm.array(kwargs['igs'])
                full = False

        if save_figure and (self.plot_pipe is not None):
            self.plot_pipe.send(['save', save_figure])
            self.plot_pipe.recv()

        if finished:
            self.terminate()
            return

        ls = len(args), self.n_arg
        if full and (ls[0] != ls[1]):
            if kwargs:
                return
            else:
                msg = 'log called with wrong number of arguments! (%d == %d)' \
                      % ls
                raise IndexError(msg)

        for ig in igs:
            if (x_values is not None) and (x_values[ig] is not None):
                self.x_values[ig].append(x_values[ig])
            else:
                if len(self.x_values[ig]):
                    ii = self.x_values[ig][-1] + 1
                else:
                    ii = 0
                self.x_values[ig].append(ii)

        for ig, ii, iseq, name in self.iter_names(igs):
            aux = args[iseq]
            if isinstance(aux, nm.ndarray):
                aux = nm.array(aux, ndmin = 1)
                if len(aux) == 1:
                    aux = aux[0]
                else:
                    raise ValueError, 'can log only scalars (%s)' % aux
            key = name_to_key(name, ii)
            self.data[key].append(aux)

            if self.output:
                self.output(('%%s: %%s: %s' % self.formats[key])
                            % (name, self.x_values[ig][-1], aux))

        if self.is_plot and self.can_plot:
            if self.n_calls == 0:
                atexit.register(self.terminate)

                self.__class__.count += 1

                self.plot_pipe, plotter_pipe = Pipe()
                self.plotter = LogPlotter(self.aggregate)
                self.plot_process = Process(target=self.plotter,
                                            args=(plotter_pipe,
                                                  self.get_log_name(),
                                                  self.data_names,
                                                  self.yscales,
                                                  self.xlabels,
                                                  self.ylabels))
                self.plot_process.daemon = True
                self.plot_process.start()

            self.plot_data(igs)

        self.n_calls += 1

    def terminate(self):
        if self.output is not None:
            self.output('# ended: %s' % time.asctime())
            self.output = None

        if self.is_plot and self.can_plot:
            self.plot_pipe.send(None)
            self.plot_process.join()
            self.n_calls = 0
            output('terminated')

    def plot_data(self, igs):
        send = self.plot_pipe.send

        ii = 0
        for ig, names in ordered_iteritems(self.data_names):
            if ig in igs:
                send(['ig', ig])
                send(['clear'])
                for name in names:
                    key = name_to_key(name, ii)
                    try:
                        send(['plot',
                              nm.array(self.x_values[ig]),
                              nm.array(self.data[key])])
                    except:
                        msg = "send failed! (%s, %s, %s)!" \
                              % (ii, name, self.data[key])
                        raise IOError(msg)
                    ii += 1

            else:
                ii += len(names)

        send(['legends'])
        send(['continue'])

    def plot_vlines(self, igs=None, **kwargs):
        """
        Plot vertical lines in axes given by igs at current x locations
        to mark some events.
        """
        if igs is None:
            igs = range(self.n_gr)

        if self.plot_pipe is not None:
            send = self.plot_pipe.send

            for ig in igs:
                x = self.x_values[ig]
                if len(x):
                    send(['ig', ig])
                    send(['vline', x[-1], kwargs])

            send(['continue'])

        if self.output:
            for ig in igs:
                for name in self.data_names[ig]:
                    self.output(name + ': -----')

########NEW FILE########
__FILENAME__ = log_plotter
"""
Plotting class to be used by Log.
"""
import numpy as nm

from sfepy.base.base import Output, Struct

class LogPlotter(Struct):
    """
    LogPlotter to be used by :class:`sfepy.base.log.Log`.
    """
    output = Output('plotter:')
    output = staticmethod(output)

    def __init__(self, aggregate=100):
        Struct.__init__(self, aggregate=aggregate)

    def process_command(self, command):
        from matplotlib.ticker import LogLocator, AutoLocator

        self.output(command[0])

        if command[0] == 'ig':
            self.ig = command[1]

        elif command[0] == 'plot':
            xdata, ydata = command[1:]

            ig = self.ig
            ax = self.ax[ig]
            ax.set_yscale(self.yscales[ig])
            ax.yaxis.grid(True)
            ax.plot(xdata, ydata)

            if self.yscales[ig] == 'log':
                ymajor_formatter = ax.yaxis.get_major_formatter()
                ymajor_formatter.label_minor(True)
                yminor_locator = LogLocator()
            else:
                yminor_locator = AutoLocator()
                self.ax[ig].yaxis.set_minor_locator(yminor_locator)

        elif command[0] == 'vline':
            x, kwargs = command[1:]

            self.vlines[self.ig].append((x, kwargs))

        elif command[0] == 'clear':
            self.ax[self.ig].cla()

        elif command[0] == 'legends':
            for ig, ax in enumerate(self.ax):
                try:
                    ax.legend(self.data_names[ig])
                except:
                    pass

                if self.xlabels[ig]:
                    ax.set_xlabel(self.xlabels[ig])
                if self.ylabels[ig]:
                    ax.set_ylabel(self.ylabels[ig])

                for x, kwargs in self.vlines[ig]:
                    ax.axvline(x, **kwargs)

            self.plt.tight_layout(pad=0.5)

        elif command[0] == 'add_axis':
            ig, names, yscale, xlabel, ylabel = command[1:]
            self.data_names[ig] = names
            self.yscales[ig] = yscale
            self.xlabels[ig] = xlabel
            self.ylabels[ig] = ylabel
            self.n_gr = len(self.data_names)
            self.make_axes()

        elif command[0] == 'save':
            self.fig.savefig(command[1])
            self.pipe.send(True) # Acknowledge save.

    def terminate(self):
        if self.ii:
            self.output('processed %d commands' % self.ii)
        self.output('ended.')
        self.plt.close('all')

    def poll_draw(self):

        def call_back():
            self.ii = 0

            while 1:
                if not self.pipe.poll():
                    break

                command = self.pipe.recv()
                can_break = False

                if command is None:
                    self.terminate()
                    return False
                elif command[0] == 'continue':
                    can_break = True
                else:
                    self.process_command(command)

                if (self.ii >= self.aggregate) and can_break:
                    break

                self.ii += 1

            if self.ii:
                self.fig.canvas.draw()
                self.output('processed %d commands' % self.ii)

            return True

        return call_back

    def make_axes(self):
        from sfepy.linalg import cycle

        self.fig.clf()
        self.ax = []

        n_col = min(5.0, nm.fix(nm.sqrt(self.n_gr)))
        if int(n_col) == 0:
            n_row = 0
        else:
            n_row = int(nm.ceil(self.n_gr / n_col))
            n_col = int(n_col)

        for ii, (ir, ic) in enumerate(cycle((n_col, n_row))):
            if ii == self.n_gr: break
            self.ax.append(self.fig.add_subplot(n_row, n_col, ii + 1))
            self.vlines.setdefault(ii, [])

    def __call__(self, pipe, log_file, data_names, yscales, xlabels, ylabels):
        """
        Sets-up the plotting window, sets GTK event loop timer callback to
        callback() returned by self.poll_draw(). The callback does the actual
        plotting, taking commands out of `pipe`, and is called every second.

        Note that pyplot _must_ be imported here and not in this module so that
        the import occurs _after_ the plotting process is started in that
        process.
        """
        import matplotlib.pyplot as plt
        import gobject
        self.plt = plt

        self.output.set_output(filename=log_file)
        self.output('starting plotter...')

        self.pipe = pipe
        self.data_names = data_names
        self.yscales = yscales
        self.xlabels = xlabels
        self.ylabels = ylabels
        self.n_gr = len(data_names)
        self.vlines = {}

        self.fig = self.plt.figure()
        self.make_axes()
        self.gid = gobject.timeout_add(1000, self.poll_draw())

        self.output('...done')
        self.plt.show()

########NEW FILE########
__FILENAME__ = mem_usage
"""
Memory usage functions.
"""
import sys
import collections

import numpy as nm
import scipy.sparse as sp

from sfepy.base.base import basestr, Struct, Output

def get_mem_usage(obj, usage=None, name=None, traversal_order=None, level=0):
    """
    Get lower bound of memory usage of an object.

    Takes into account strings, numpy arrays and scipy CSR sparse matrices,
    descends into sequences, mappings and objects.

    Parameters
    ----------
    obj : any object
        The object to be measured.
    usage : dict
        The dict with memory usage records, serving also as a cache of already
        traversed objects.
    name : str
        The name to be given to the object in its record.
    traversal_order : list, internal
        The traversal order of the object.
    level : int, internal
        The recurrence level.

    Returns
    -------
    usage : int
        The object's lower bound of memory usage.
    """
    if usage is None:
        usage = {}

    if name is None:
        name = getattr(obj, 'name', '-')

    if traversal_order is None:
        traversal_order = [0]

    to = traversal_order

    key = id(obj)
    if key in usage:
        usage[key].nrefs += 1
        return 0

    else:
        record = usage.setdefault(key, Struct(name=name,
                                              kind=type(obj).__name__,
                                              usage=0, nrefs=1,
                                              traversal_order=to[0],
                                              level=level))
        level += 1

    if isinstance(obj, nm.ndarray):
        record.usage = obj.nbytes

    elif isinstance(obj, sp.csr_matrix):
        record.usage = (get_mem_usage(obj.data, usage, name='data',
                                      traversal_order=to, level=level)
                        + get_mem_usage(obj.indices, usage, name='indices',
                                        traversal_order=to, level=level)
                        + get_mem_usage(obj.indptr, usage, name='indptr',
                                        traversal_order=to, level=level))

    elif isinstance(obj, basestr):
        record.usage = len(obj)

    elif isinstance(obj, Struct):
        for subname, sub in obj.__dict__.iteritems():
            to[0] += 1
            record.usage += get_mem_usage(sub, usage,
                                          name='attribute %s of %s'
                                          % (subname, getattr(obj, 'name',
                                                              record.kind)),
                                          traversal_order=to, level=level)

    elif isinstance(obj, collections.Mapping):
        for subname, sub in obj.iteritems():
            to[0] += 1
            record.usage += get_mem_usage(sub, usage,
                                          name='item %s of %s'
                                          % (subname, record.kind),
                                          traversal_order=to, level=level)

    elif isinstance(obj, collections.Sequence):
        for ii, sub in enumerate(obj):
            to[0] += 1
            record.usage += get_mem_usage(sub, usage,
                                          name='item %d of %s'
                                          % (ii, record.kind),
                                          traversal_order=to, level=level)

    else:
        record.usage = sys.getsizeof(obj)

    return record.usage

def print_mem_usage(usage, order_by='usage', direction='up', print_key=False):
    """
    Print memory usage dictionary.

    Parameters
    ----------
    usage : dict
        The dict with memory usage records.
    order_by : 'usage', 'name', 'kind', 'nrefs', 'traversal_order', or 'level'
        The sorting field name.
    direction : 'up' or 'down'
        The sorting direction.
    print_key : bool
        If True, print also the record key (object's id).
    """
    keys = usage.keys()
    order_vals = nm.array([record.get(order_by)
                           for record in usage.itervalues()])

    order = nm.argsort(order_vals)
    if direction == 'down':
        order = order[::-1]

    output = Output('')
    fmt = '%9s, %s, %s, %d %d %d' + ', %d' * print_key

    for ii in order:
        key = keys[ii]
        record = usage[key]

        if print_key:
            output(fmt % (record.usage, record.name, record.kind, record.nrefs,
                          record.traversal_order, record.level, key))

        else:
            output(fmt % (record.usage, record.name, record.kind, record.nrefs,
                          record.traversal_order, record.level))

########NEW FILE########
__FILENAME__ = parse_conf
"""
Create pyparsing grammar for problem configuration and options.
"""
from pyparsing import (Word, Group, Suppress, Combine, Optional,
                       Forward, Empty, quotedString, oneOf, removeQuotes,
                       delimitedList, nums, alphas, alphas8bit, alphanums,
                       Keyword)

word_free = Word(alphas8bit + '][@_-/.+**' + alphanums)
word_strict = Word(alphas8bit + alphas, alphas8bit + alphanums + '_' )

(lparen, rparen, lbrack, rbrack,
 lbrace, rbrace, colon, equal_sign) = map(Suppress, '()[]{}:=')

integer = Combine(Optional(oneOf('+ -')) + Word(nums)).setName('integer')
cvt_int = lambda toks: int(toks[0])
integer.setParseAction(cvt_int)

boolean_true = Keyword('True', caseless=True)
boolean_true.setParseAction(lambda x: True)
boolean_false = Keyword('False', caseless=True)
boolean_false.setParseAction(lambda x: False)

boolean = boolean_true | boolean_false

none = Keyword('None', caseless=True)

cvt_none = lambda toks: [None]
none.setParseAction(cvt_none)

real = Combine(Optional(oneOf('+ -')) + Word(nums)
               + '.' + Optional(Word(nums))
               + Optional('e' + Optional(oneOf('+ -'))
                          + Word(nums))).setName('real')
cvt_real = lambda toks: float(toks[0])
real.setParseAction(cvt_real)

array_index = integer + Optional(colon + integer
                                 + Optional(colon + integer))
cvt_array_index = lambda toks: int(toks[0]) if len(toks) == 1 \
                  else slice(*toks)
array_index.setParseAction(cvt_array_index)
array_braces = lbrack + array_index + rbrack

def create_bnf(allow_tuple=False, free_word=False):
    word = word_free if free_word else word_strict
    defs = get_standard_type_defs(word)

    if allow_tuple:
        return defs['dict'].inner | defs['tuple'].inner
    else:
        return defs['dict'].inner

def list_of(element, *elements):
    """
    Return lexical element that parses a list of items. The items can be a one
    or several lexical elements. For example, result of ``list_of(real,
    integer)`` parses list of real or integer numbers.
    """
    for e in elements:
        element |= e
    lst = delimitedList(element)
    return lst + Optional(Suppress(','))

def get_standard_type_defs(word=word_free):
    """
    Return dict of the pyparsing base lexical elements.

    The compound types (tuple, list, dict) can contain compound types or simple
    types such as integers, floats and words.

    Parameters
    ----------
    word : lexical element
        A custom lexical element for word.

    Returns
    -------
    defs : dict
        The dictionary with the following items:

        - tuple: (..., ..., ...)
        - list: [..., ...., ...]
        - dict: {...:..., ...:..., ....} or {...=..., ...=..., ....}
        - list_item: any of preceding compound types or simple types
    """
    tuple_str = Forward()
    list_str = Forward()
    dict_str = Forward()
    cvt_tuple = lambda toks : tuple(toks.asList())
    cvt_dict = lambda toks: dict(toks.asList())

    list_item = (none | boolean | real | integer | list_str | tuple_str
                 | dict_str
                 | quotedString.setParseAction(removeQuotes)
                 | word)
    list_item2 = list_item | Empty().setParseAction(lambda: [None])

    tuple_str.inner = list_of(list_item)
    tuple_str.inner.setParseAction(cvt_tuple)
    tuple_str << (lparen + tuple_str.inner + rparen)

    list_str.inner = tuple_str.inner.copy()
    list_str.inner.setParseAction(lambda toks: list(toks))
    list_str << (lbrack + list_str.inner + rbrack)

    dict_entry = Group(list_item + (colon | equal_sign) + list_item2)
    dict_str.inner = list_of(dict_entry)
    dict_str.inner.setParseAction(cvt_dict)
    dict_str << (lbrace + Optional(dict_str.inner) + rbrace)

    defs = {'tuple' : tuple_str,
            'list' : list_str,
            'dict' : dict_str,
            'list_item' : list_item}

    return defs

def list_dict(word=word_free):
    """
    Return the pyparsing lexical element, that parses a string either as a list
    or as a dictionary.

    Parameters
    ----------
    word : lexical element
        A custom lexical element for word.

    Returns
    -------
    ld : lexical element
        The returned lexical element parses a string in the form
        ``..., ..., ...`` or ``key1:..., key2=..., key3: ...``
        where ``...`` is a ``list_item`` from :func:`get_standard_type_defs()`
        and interprets it as a list or a dictionary.
    """
    defs = get_standard_type_defs(word)
    i = defs['list_item']
    arg = i.copy()
    arg.setParseAction(lambda t: (t[0],))
    narg = word_strict + (colon | equal_sign) + i
    narg.setParseAction(lambda t: (t[0], t[1]))

    ld = Group(list_of(narg | arg))
    ld.setParseAction(lambda t: ([x[0] for x in t[0] if len(x) == 1],
                                 dict([x for x in t[0] if len(x) > 1]))
                      )
    return ld

########NEW FILE########
__FILENAME__ = plotutils
import numpy as nm

try:
    import matplotlib.pyplot as plt
    import matplotlib as mpl
except (ImportError, RuntimeError):
    plt = mpl = None
    #print 'matplotlib import failed!'

from sfepy.base.base import output, pause

def spy(mtx, eps=None, color='b', **kwargs):
    """
    Show sparsity structure of a `scipy.sparse` matrix.
    """
    aux = mtx.tocoo()
    ij, val = nm.concatenate((aux.row[:,nm.newaxis],
                              aux.col[:,nm.newaxis]), 1), aux.data
    n_item = aux.getnnz()
    n_row, n_col = aux.shape

    if eps is not None:
        output('using eps =', eps)
        ij = nm.compress(nm.absolute(val) > eps, ij, 0)
        n_item = ij.shape[0]
    else:
        output('showing all')

    output('n_item:', n_item)
    if n_item:
        args = {'marker' : '.', 'markersize' : 0.5, 'markeredgewidth' : 0.5}
        args.update(kwargs)
        plt.plot(ij[:,1] + 0.5, ij[:,0] + 0.5, color, linestyle='None',
                 **args)
    plt.axis([-0.5, n_row+0.5, -0.5, n_col+0.5])
    plt.axis('image')
    plt.xlabel(r'%d x %d: %d nnz, %.2f%% fill'
               % (n_row, n_col, n_item, 100. * n_item /
                  (float(n_row) * float(n_col))))
    ax = plt.gca()
    ax.set_ylim(ax.get_ylim()[::-1])

def spy_and_show(mtx, **kwargs):
    spy(mtx, **kwargs)
    plt.show()

##
# 13.12.2005, c
def print_matrix_diff( title, legend, mtx1, mtx2, mtx_da, mtx_dr, iis ):
    import copy

    print '%s: ir, ic, %s, %s, adiff, rdiff' % ((title,) + tuple( legend ))

    aux = copy.copy(mtx_da)
    aux.data = nm.ones(mtx_da.data.shape[0])
    irs, ics = aux.nonzero()

    for ii in iis:
        ir, ic = irs[ii], ics[ii]
        print '%5d %5d %11.4e %11.4e %9.2e %9.2e'\
              % (ir, ic, mtx1[ir,ic], mtx2[ir,ic], mtx_da[ir,ic], mtx_dr[ir,ic] )

    print 'total: %d' % len( iis )

##
# 13.12.2005, c
# 14.12.2005
# 15.12.2005
# 18.07.2007
def plot_matrix_diff( mtx1, mtx2, delta, legend, mode ):

    eps = 1e-16

    print nm.amin( mtx1.data ), nm.amin( mtx2.data )
    print nm.amax( mtx1.data ), nm.amax( mtx2.data )

    mtx_da = mtx1.copy() # To preserve structure of mtx1.
    mtx_da.data[:] = nm.abs( mtx1.data - mtx2.data )

    mtx_dr = mtx_da.copy()
    mtx_dr.data[:] = -1
    iin = nm.where( nm.abs( mtx1.data ) > eps )[0]
    mtx_dr.data[iin] = mtx_da.data[iin] / nm.abs( mtx1.data[iin] )

    print nm.amin( mtx_da.data ), nm.amax( mtx_da.data )
    print nm.amin( mtx_dr.data ), nm.amax( mtx_dr.data )

    epsilon = max( 1e-5, 10 * delta )

    print 'epsilon:', epsilon
    pause()

    ija = nm.where( mtx_da.data > epsilon )[0]
    print_matrix_diff( '--- absolute diff', legend,
                     mtx1, mtx2, mtx_da, mtx_dr, ija )
    pause()

    iin = nm.where( nm.abs( mtx1.data ) > epsilon )[0]
    ij = nm.where( nm.abs( mtx_dr.data[iin] ) > epsilon )[0]
    ij = iin[ij]
    print_matrix_diff( '--- relative diff', legend,
                     mtx1, mtx2, mtx_da, mtx_dr, ij )
    pause()

    ijb = nm.intersect1d( ija, ij )
    print_matrix_diff( '--- a-r', legend,
                     mtx1, mtx2, mtx_da, mtx_dr, ijb )
    pause()

    ii = nm.argsort( mtx_dr.data[ijb] )
    n_s = min( 20, len( ii ) )
    ijbs = ijb[ii[-1:-n_s-1:-1]]
    print_matrix_diff( '--- a-r 20 biggest (by r)', legend,
                     mtx1, mtx2, mtx_da, mtx_dr, ijbs )
    pause()

    if mode < 2: return
    
    h = 100
    plt.figure( h ); plt.clf()

    plt.axes( [0.04, 0.6, 0.3, 0.3], frameon = True )
    spy( mtx_da, epsilon )
    plt.title( 'absolute diff' )

    plt.axes( [0.68, 0.6, 0.3, 0.3], frameon = True )
    iia = nm.where( mtx_dr.data )[0]
    mtx_dr.data[nm.setdiff1d( iia, iin )] = 0.0
    spy( mtx_dr, epsilon )
    plt.title( 'relative diff' )

    plt.axes( [0.36, 0.6, 0.3, 0.3], frameon = True )
    mtx = mtx_dr.copy()
    mtx.data[:] = 0.0
    ii = nm.intersect1d( nm.where( mtx_dr.data > epsilon )[0],
                           nm.where( mtx_da.data > epsilon )[0] )
    mtx.data[ii] = 1.0
    spy( mtx, epsilon )
    plt.title( 'a-r intersection' )

    plt.axes( [0.04, 0.08, 0.42, 0.42], frameon = True )
    spy( mtx1, epsilon )
    plt.title( legend[0] )

    plt.axes( [0.54, 0.08, 0.42, 0.42], frameon = True )
    spy( mtx2, epsilon )
    plt.title( legend[1] )

    plt.show()

##
# 02.05.2006, c
def set_axes_font_size( ax, size ):
    labels = ax.get_xticklabels() + ax.get_yticklabels()
    for label in labels:
        label.set_size( size )

##
# 27.09.2006, c
def font_size( size ):
    return mpl.font_manager.FontProperties( size = size )

##
# 28.08.2007, c
def iplot( *args, **kwargs ):
    plt.ion()
    plt.plot( *args, **kwargs )
    plt.draw()
    plt.ioff()
    pause()

########NEW FILE########
__FILENAME__ = reader
from base import Struct
import os.path as op

##
# 16.06.2005, c
class Reader( Struct ):
    """
    Reads and executes a Python file as a script with execfile(), storing its
    locals. Then sets the __dict__ of a new instance of obj_class to the stored
    locals.

    Example:

    >>> class A:
    >>>    pass

    >>> read = Reader( '.' )
    >>> instance_of_a = read( A, 'file.py' )

    It is equivalent to:

    >>> mod = __import__( 'file' )
    >>> instance_of_a = A()
    >>> instance_of_a.__dict__.update( mod.__dict__ )

    The first way does not create the 'file.pyc'...
    """
    ##
    # 16.06.2005, c
    def __init__( self, directory ):
        self.directory = directory

    ##
    # 16.06.2005, c
    # 17.10.2005
    # 09.02.2006
    def __call__( self, obj_class, name ):
        filename = op.join( self.directory, name + '.py' )

        aux = {}
        execfile( filename, {}, aux )

        obj = obj_class()
        for key, val in aux.iteritems():
            obj.__dict__[key] = val

        return obj

########NEW FILE########
__FILENAME__ = resolve_deps
"""
Functions for resolving dependencies.
"""
import itertools as it

from sfepy.base.base import basestr

def get_nums(deps):
    """
    Get number of prerequisite names for each name in dependencies.
    """
    nums = {}
    for key, val in deps.iteritems():
        nums[key] = len(val)

    return nums

def solvable(deps, names):
    """
    Return True if `names` form a solvable block, i.e. the set of names equals
    to the set of their prerequisites.
    """
    if not names: return False # Missing self-reference.

    dep_names = set()
    for name in names:
        dep_names.update(deps[name])

    return dep_names == set(names)

def remove_known(deps, known):
    """
    Remove known names from dependencies.
    """
    if isinstance(known, basestr):
        out = {}
        for key, val in deps.iteritems():
            if key == known: continue
            out[key] = [ii for ii in val if ii != known]

        return out

    else:
        out = deps
        for ii in known:
            out = remove_known(out, ii)

        return out

def try_block(deps, num):
    """
    Return generator of lists of solvable blocks of the length `num`.
    """
    keys = deps.keys()
    for ic in it.combinations(keys, num):
        if solvable(deps, ic):
            yield sorted(ic)

def resolve(deps):
    """
    Resolve dependencies among equations so that smaller blocks are solved
    first.

    The dependencies are given in terms of variable names.

    Parameters
    ----------
    deps : dict
        The dependencies as a dictionary with names as keys and sets of
        prerequisite names as values.

    Returns
    -------
    order : list
        The list of blocks in the order of solving. Each block is a list of
        names.
    """
    order = []
    if not(len(deps)): return order

    nums = get_nums(deps)
    ib0 = min(nums.values())
    for ib in range(ib0, len(deps) + 1):
        blocks = [ii for ii in try_block(deps, ib)]
        if len(blocks):
            new_deps = remove_known(deps, blocks[0])
            order.extend([blocks[0]] + resolve(new_deps))

        if len(list(it.chain(*order))) == len(deps):
            break

    return order

########NEW FILE########
__FILENAME__ = testing
import inspect

import numpy as nm
import numpy.linalg as nla

from sfepy.base.base import Struct

##
# 30.05.2007, c
class TestCommon( Struct ):

    @staticmethod
    def xfail(test_method):
        """
        Decorator that allows a test to fail.
        """
        def wrapper(self):
            try:
                ret = test_method(self)
            except:
                if self.debug:
                    raise
                ret = False

            if not ret:
                print '--- test expected to fail.'
                ret = True

            return ret

        return wrapper

    ##
    # 16.07.2007, c
    def get_number( self ):
        methods = inspect.getmembers( self, inspect.ismethod )
        tests = [ii for ii in methods
                 if (len( ii[0] ) > 5) and ii[0][:5] == 'test_']
        return len( tests )

    ##
    # c: 30.05.2007, r: 05.02.2008
    def run( self, debug = False ):
        self.debug = debug

        ok = True
        n_fail = 0

        methods = inspect.getmembers( self, inspect.ismethod )
        if hasattr( self, 'tests' ):
            dmethods = {}
            for key, method in methods:
                dmethods[key] = method
            tests = [(ii, dmethods[ii]) for ii in self.tests]
            print tests
        else:
            tests = [ii for ii in methods
                     if (len( ii[0] ) > 5) and ii[0][:5] == 'test_']

            
        for test_name, test_method in tests:
            aux = ' %s: ' % test_name

            try:
                ret = test_method()
            except:
                if debug:
                    raise
                ret = False
                
            if not ret:
                aux = '---' + aux + 'failed!'
                n_fail += 1
                ok = False
            else:
                aux = '+++' + aux + 'ok'

            print aux

        return ok, n_fail, len( tests )

    ##
    # c: 31.05.2007, r: 02.05.2008
    def report( *argc ):
        """All tests should print via this function."""
        format = '...' + ' %s' * len( argc )
        msg =  format % argc
        print msg
    report = staticmethod( report )

    ##
    # c: 30.05.2007, r: 09.05.2008
    def eval_coor_expression( expression, coor ):

        x = coor[:,0]
        y = coor[:,1]
        if coor.shape[1] == 3:
            z = coor[:,2]
        else:
            z = None

        env = {'x' : x, 'y' : y, 'z' : z}
        out = eval( expression, nm.__dict__, env )

        if isinstance(out, float):
            aux = nm.empty(coor.shape[0], dtype=nm.float64)
            aux.fill(out)
            out = aux
        
        return out
    eval_coor_expression = staticmethod( eval_coor_expression )

    ##
    # c: 30.05.2007, r: 07.05.2008
    def compare_vectors( vec1, vec2, allowed_error = 1e-8,
                        label1 = 'vec1', label2 = 'vec2', norm = None ):

        diff_norm = nla.norm( vec1 - vec2, ord = norm )
        TestCommon.report( '||%s - %s||: %e' % (label1, label2, diff_norm) )
        if diff_norm > allowed_error:
            return False
        else:
            return True
    compare_vectors = staticmethod( compare_vectors )

########NEW FILE########
__FILENAME__ = config
import sys
import os
import shutil

msg_unknown_os = """could not determine operating system!
try setting it in site_cfg.py manually, see site_cfg_template.py"""

msg_numpydoc = """could not find numpydoc!
If it is installed in a non-standard location, try setting it in
site_cfg.py manually."""

if not os.path.exists('site_cfg.py'):
    try:
        shutil.copyfile('site_cfg_template.py', 'site_cfg.py')

    except:
        pass

try:
    import site_cfg

except ImportError:
    site_cfg = None

has_attr = lambda obj, attr: obj and hasattr(obj, attr)

class Config(object):
    def python_version(self):
        if has_attr(site_cfg, 'python_version'):
            if site_cfg.python_version == 'auto':
                return "%d.%d" % tuple(sys.version_info[:2])
            else:
                return site_cfg.python_version
        else:
            return "%d.%d" % tuple(sys.version_info[:2])

    def python_include(self):
        if (has_attr(site_cfg, 'python_include')
            and (site_cfg.python_include != 'auto')):
            return site_cfg.python_include

        else:
            system = self.system()

            if system == 'posix':
                version = self.python_version()
                return os.path.join(sys.prefix, 'include', 'python' + version)

            else:
                return os.path.join(sys.prefix, 'include')


    def system(self):
        if has_attr(site_cfg, 'system') and site_cfg.system is not None:
            return site_cfg.system
        else:
            if os.name in ['posix']:
                return 'posix'
            elif os.name in ['nt']:
                return 'windows'
            else:
                raise ValueError(msg_unknown_os)

    def compile_flags(self):
        if has_attr(site_cfg, 'compile_flags'):
            flags = site_cfg.compile_flags

        else:
            flags = '-g -O2'

        return flags.split()

    def link_flags(self):
        if has_attr(site_cfg, 'link_flags'):
            flags =  site_cfg.link_flags

        else:
            flags = ''

        return flags.split()

    def debug_flags(self):
        if has_attr(site_cfg, 'debug_flags'):
            return site_cfg.debug_flags
        else:
            return ''

    def numpydoc_path(self):
        if (has_attr(site_cfg, 'numpydoc_path') and
            (site_cfg.numpydoc_path is not None)):
            return site_cfg.numpydoc_path

        else:
            try:
                import numpydoc
            except ImportError:
                raise ValueError(msg_numpydoc)

    def is_release(self):
        if has_attr(site_cfg, 'is_release'):
            return site_cfg.is_release
        else:
            return ''

    def tetgen_path(self):
        if has_attr(site_cfg, 'tetgen_path'):
            return site_cfg.tetgen_path
        else:
            return '/usr/bin/tetgen'

########NEW FILE########
__FILENAME__ = dof_info
"""
Classes holding information on global DOFs and mapping of all DOFs -
equations (active DOFs).

Helper functions for the equation mapping.
"""
import numpy as nm
import scipy.sparse as sp

from sfepy.base.base import assert_, Struct, basestr
from sfepy.discrete.functions import Function
from sfepy.discrete.conditions import get_condition_value, EssentialBC

def expand_nodes_to_dofs(nods, n_dof_per_node):
    """
    Expand DOF node indices into DOFs given a constant number of DOFs
    per node.
    """
    dofs = nm.repeat(nods, n_dof_per_node)
    dofs.shape = (nods.shape[0], n_dof_per_node)

    idof = nm.arange(n_dof_per_node, dtype=nm.int32)

    dofs = n_dof_per_node * dofs + idof

    return dofs

def expand_nodes_to_equations(nods, dof_names, all_dof_names):
    """
    Expand vector of node indices to equations (DOF indices) based on
    the DOF-per-node count.

    DOF names must be already canonized.
    """
    dpn = len(all_dof_names)

    eq = nm.array([], dtype=nm.int32)
    for dof in dof_names:
        idof = all_dof_names.index(dof)
        eq = nm.concatenate((eq, dpn * nods + idof))
    return eq

def resolve_chains(master_slave, chains):
    """
    Resolve EPBC chains - e.g. in corner nodes.
    """
    for chain in chains:
        slave = chain[-1]
        master_slave[chain[:-1]] = slave + 1
        master_slave[slave] = - chain[0] - 1 # Any of masters...

def group_chains(chain_list):
    """
    Group EPBC chains.
    """
    chains = []
    while len(chain_list):
        chain = set(chain_list.pop(0))
        ## print ':', chain
        ii = 0
        while ii < len(chain_list):
            c1 = sorted(chain_list[ii])
            ## print '--', ii, c1, chain
            is0 = c1[0] in chain
            is1 = c1[1] in chain

            if is0 and is1:
                chain_list.pop(ii)
            elif is0 or is1:
                chain.update(c1)
                chain_list.pop(ii)
                ii = 0
            else:
                ii += 1
            ## print ii, chain, chain_list
        ## print '->', chain
        ## print chain_list

        chains.append(list(chain))

    ## print 'EPBC chain groups:', chains
    aux = {}
    for chain in chains:
        aux.setdefault(len(chain), [0])[0] += 1
    ## print 'EPBC chain counts:', aux

    return chains

class DofInfo(Struct):
    """
    Global DOF information, i.e. ordering of DOFs of the state (unknown)
    variables in the global state vector.
    """

    def __init__(self, name):
        Struct.__init__(self, name=name)

        self.n_var = 0
        self.var_names = []
        self.n_dof = {}
        self.ptr = [0]
        self.indx = {}
        self.details = {}

    def _update_after_append(self, name):
        self.ptr.append(self.ptr[-1] + self.n_dof[name])

        ii = self.n_var
        self.indx[name] = slice(int(self.ptr[ii]), int(self.ptr[ii+1]))

        self.n_var += 1

    def append_variable(self, var, active=False):
        """
        Append DOFs of the given variable.

        Parameters
        ----------
        var : Variable instance
            The variable to append.
        active : bool, optional
            When True, only active (non-constrained) DOFs are considered.
        """
        name = var.name
        if name in self.var_names:
            raise ValueError('variable %s already present!' % name)

        self.var_names.append(name)

        self.n_dof[name], self.details[name] = var.get_dof_info(active=active)
        self._update_after_append(name)

    def append_raw(self, name, n_dof):
        """
        Append raw DOFs.

        Parameters
        ----------
        name : str
            The name of variable the DOFs correspond to.
        n_dof : int
            The number of DOFs.
        """
        if name in self.var_names:
            raise ValueError('variable %s already present!' % name)

        self.var_names.append(name)

        self.n_dof[name], self.details[name] = n_dof, None
        self._update_after_append(name)

    def update(self, name, n_dof):
        """
        Set the number of DOFs of the given variable.

        Parameters
        ----------
        name : str
            The name of variable the DOFs correspond to.
        n_dof : int
            The number of DOFs.
        """
        if not name in self.var_names:
            raise ValueError('variable %s is not present!' % name)

        ii = self.var_names.index(name)
        delta = n_dof - self.n_dof[name]

        self.n_dof[name] = n_dof

        for iv, nn in enumerate(self.var_names[ii:]):
            self.ptr[ii+iv+1] += delta
            self.indx[nn] = slice(self.ptr[ii+iv], self.ptr[ii+iv+1])

    def get_info(self, var_name):
        """
        Return information on DOFs of the given variable.

        Parameters
        ----------
        var_name : str
            The name of the variable.
        """
        return Struct(name='%s_dof_info' % var_name,
                      var_name=var_name,
                      n_dof=self.n_dof[var_name],
                      indx=self.indx[var_name],
                      details=self.details[var_name])

    def get_subset_info(self, var_names):
        """
        Return global DOF information for selected variables
        only. Silently ignores non-existing variable names.

        Parameters
        ----------
        var_names : list
            The names of the selected variables.
        """
        di = DofInfo(self.name + ':subset')
        for var_name in var_names:
            if var_name not in self.var_names:
                continue

            di.append_raw(var_name, self.n_dof[var_name])

        return di

    def get_n_dof_total(self):
        """
        Return the total number of DOFs of all state variables.
        """
        return self.ptr[-1]

def is_active_bc(bc, ts=None, functions=None):
    """
    Check whether the given boundary condition is active in the current
    time.

    Returns
    -------
    active : bool
        True if the condition `bc` is active.
    """
    if (bc.times is None) or (ts is None):
        active = True

    elif isinstance(bc.times, list):
        for tt in bc.times:
            if tt[0] <= ts.time < tt[1]:
                active = True
                break

        else:
            active = False

    else:
        if isinstance(bc.times, basestr):
            if functions is not None:
                fun = functions[bc.times]

            else:
                raise ValueError('no functions given for bc %s!' % bc.name)

        elif isinstance(bc.times, Function):
            fun = bc.times

        else:
            raise ValueError('unknown times type! (%s)'
                             % type(bc.times))

        active = fun(ts)

    return active

class EquationMap(Struct):
    """
    Map all DOFs to equations for active DOFs.
    """

    def __init__(self, name, dof_names, var_di):
        Struct.__init__(self, name=name, dof_names=dof_names, var_di=var_di)

        self.dpn = len(self.dof_names)
        self.eq = nm.arange(var_di.n_dof, dtype=nm.int32)

    def _init_empty(self):
        self.eqi = nm.arange(self.var_di.n_dof, dtype=nm.int32)
        self.eq_ebc = nm.empty((0,), dtype=nm.int32)

        self.master = nm.empty((0,), dtype=nm.int32)
        self.slave = nm.empty((0,), dtype=nm.int32)

        self.n_eq = self.eqi.shape[0]
        self.n_ebc = self.eq_ebc.shape[0]
        self.n_epbc = self.master.shape[0]

    def map_equations(self, bcs, field, ts, functions, problem=None,
                      warn=False):
        """
        Create the mapping of active DOFs from/to all DOFs.

        Parameters
        ----------
        bcs : Conditions instance
            The Dirichlet or periodic boundary conditions (single
            condition instances). The dof names in the conditions must
            already be canonized.
        field : Field instance
            The field of the variable holding the DOFs.
        ts : TimeStepper instance
            The time stepper.
        functions : Functions instance
            The registered functions.
        problem : Problem instance, optional
            The problem that can be passed to user functions as a context.
        warn : bool, optional
            If True, warn about BC on non-existent nodes.

        Returns
        -------
        active_bcs : set
            The set of boundary conditions active in the current time.

        Notes
        -----
        - Periodic bc: master and slave DOFs must belong to the same
          field (variables can differ, though).
        """
        if bcs is None:
            self.val_ebc = nm.empty((0,), dtype=field.dtype)
            self._init_empty()
            return set()

        eq_ebc = nm.zeros((self.var_di.n_dof,), dtype=nm.int32)
        val_ebc = nm.zeros((self.var_di.n_dof,), dtype=field.dtype)
        master_slave = nm.zeros((self.var_di.n_dof,), dtype=nm.int32)
        chains = []

        active_bcs = set()
        for bc in bcs:
            # Skip conditions that are not active in the current time.
            if not is_active_bc(bc, ts=ts, functions=functions):
                continue

            active_bcs.add(bc.key)

            if isinstance(bc, EssentialBC):
                ntype = 'EBC'
                region = bc.region

            else:
                ntype = 'EPBC'
                region = bc.regions[0]

            if warn:
                clean_msg = ('warning: ignoring nonexistent %s node (%s) in '
                             % (ntype, self.var_di.var_name))
            else:
                clean_msg = None

            # Get master region nodes.
            master_nod_list = field.get_dofs_in_region(region, clean=True,
                                                       warn=clean_msg)
            if len(master_nod_list) == 0:
                continue

            if ntype == 'EBC': # EBC.
                dofs, val = bc.dofs
                ##
                # Evaluate EBC values.
                fun = get_condition_value(val, functions, 'EBC', bc.name)
                if isinstance(fun, Function):
                    aux = fun
                    fun = lambda coors: aux(ts, coors,
                                            bc=bc, problem=problem)

                nods, vv = field.set_dofs(fun, region, len(dofs), clean_msg)

                eq = expand_nodes_to_equations(nods, dofs, self.dof_names)
                # Duplicates removed here...
                eq_ebc[eq] = 1
                if vv is not None: val_ebc[eq] = vv

            else: # EPBC.
                region = bc.regions[1]
                slave_nod_list = field.get_dofs_in_region(region, clean=True,
                                                          warn=clean_msg)

                nmaster = nm.unique(nm.hstack(master_nod_list))
                # Treat fields not covering the whole domain.
                if nmaster[0] == -1:
                    nmaster = nmaster[1:]

                nslave = nm.unique(nm.hstack(slave_nod_list))
                # Treat fields not covering the whole domain.
                if nslave[0] == -1:
                    nslave = nslave[1:]

                ## print nmaster + 1
                ## print nslave + 1
                if nmaster.shape != nslave.shape:
                    msg = 'EPBC list lengths do not match!\n(%s,\n %s)' %\
                          (nmaster, nslave)
                    raise ValueError(msg)

                if (nmaster.shape[0] == 0) and (nslave.shape[0] == 0):
                    continue

                mcoor = field.get_coor(nmaster)
                scoor = field.get_coor(nslave)

                fun = get_condition_value(bc.match, functions, 'EPBC', bc.name)
                if isinstance(fun, Function):
                    i1, i2 = fun(mcoor, scoor)

                else:
                    i1, i2 = fun

                ## print nm.c_[mcoor[i1], scoor[i2]]
                ## print nm.c_[nmaster[i1], nslave[i2]] + 1

                meq = expand_nodes_to_equations(nmaster[i1], bc.dofs[0],
                                                self.dof_names)
                seq = expand_nodes_to_equations(nslave[i2], bc.dofs[1],
                                                self.dof_names)

                m_assigned = nm.where(master_slave[meq] != 0)[0]
                s_assigned = nm.where(master_slave[seq] != 0)[0]
                if m_assigned.size or s_assigned.size: # Chain EPBC.
                    aux = master_slave[meq[m_assigned]]
                    sgn = nm.sign(aux)
                    om_chain = zip(meq[m_assigned], (aux - sgn) * sgn)
                    chains.extend(om_chain)

                    aux = master_slave[seq[s_assigned]]
                    sgn = nm.sign(aux)
                    os_chain = zip(seq[s_assigned], (aux - sgn) * sgn)
                    chains.extend(os_chain)

                    m_chain = zip(meq[m_assigned], seq[m_assigned])
                    chains.extend(m_chain)

                    msd = nm.setdiff1d(s_assigned, m_assigned)
                    s_chain = zip(meq[msd], seq[msd])
                    chains.extend(s_chain)

                    msa = nm.union1d(m_assigned, s_assigned)
                    ii = nm.setdiff1d(nm.arange(meq.size), msa)
                    master_slave[meq[ii]] = seq[ii] + 1
                    master_slave[seq[ii]] = - meq[ii] - 1

                else:
                    master_slave[meq] = seq + 1
                    master_slave[seq] = - meq - 1

        chains = group_chains(chains)
        resolve_chains(master_slave, chains)

        ii = nm.argwhere(eq_ebc == 1)
        self.eq_ebc = nm.atleast_1d(ii.squeeze())
        self.val_ebc = nm.atleast_1d(val_ebc[ii].squeeze())
        self.master = nm.argwhere(master_slave > 0).squeeze()
        self.slave = master_slave[self.master] - 1

        assert_((self.eq_ebc.shape == self.val_ebc.shape))
        self.eq[self.eq_ebc] = -2
        self.eq[self.master] = -1
        self.eqi = nm.compress(self.eq >= 0, self.eq)
        self.eq[self.eqi] = nm.arange(self.eqi.shape[0], dtype=nm.int32)
        self.eq[self.master] = self.eq[self.slave]
        self.n_eq = self.eqi.shape[0]
        self.n_ebc = self.eq_ebc.shape[0]
        self.n_epbc = self.master.shape[0]

        return active_bcs

    def get_operator(self):
        """
        Get the matrix operator :math:`R` corresponding to the equation
        mapping, such that the restricted matrix :math:`A_r` can be
        obtained from the full matrix :math:`A` by :math:`A_r = R^T A
        R`. All the matrices are w.r.t. a single variables that uses
        this mapping.

        Returns
        -------
        mtx : coo_matrix
            The matrix :math:`R`.
        """
        # EBC.
        rows = self.eqi
        cols = nm.arange(self.n_eq, dtype=nm.int32)

        # EPBC.
        ic = self.eq[self.slave]
        ii = ic >= 0
        rows = nm.r_[rows, self.master[ii]]
        cols = nm.r_[cols, ic[ii]]

        ones = nm.ones(rows.shape[0], dtype=nm.float64)
        mtx = sp.coo_matrix((ones, (rows, cols)),
                            shape=(self.eq.shape[0], self.n_eq))

        return mtx

########NEW FILE########
__FILENAME__ = domain
import time

import numpy as nm

from sfepy.base.base import output, assert_, OneTypeList, Struct
from sfepy.discrete.common.region import (Region, get_dependency_graph,
                                          sort_by_dependency, get_parents)
from sfepy.discrete.parse_regions import create_bnf, visit_stack, ParseException

def region_leaf(domain, regions, rdef, functions):
    """
    Create/setup a region instance according to rdef.
    """
    n_coor = domain.shape.n_nod
    dim = domain.shape.dim

    def _region_leaf(level, op):
        token, details = op['token'], op['orig']

        if token != 'KW_Region':
            parse_def = token + '<' + ' '.join(details) + '>'
            region = Region('leaf', rdef, domain, parse_def=parse_def)

        if token == 'KW_Region':
            details = details[1][2:]
            aux = regions.find(details)
            if not aux:
                raise ValueError, 'region %s does not exist' % details
            else:
                if rdef[:4] == 'copy':
                    region = aux.copy()
                else:
                    region = aux

        elif token == 'KW_All':
            region.vertices = nm.arange(n_coor, dtype=nm.uint32)

        elif token == 'E_VIR':
            where = details[2]

            if where[0] == '[':
                vertices = nm.array(eval(where), dtype=nm.uint32)
                assert_(nm.amin(vertices) >= 0)
                assert_(nm.amax(vertices) < n_coor)
            else:
                coors = domain.cmesh.coors
                x = coors[:,0]
                y = coors[:,1]
                if dim == 3:
                    z = coors[:,2]
                else:
                    z = None
                coor_dict = {'x' : x, 'y' : y, 'z': z}

                vertices = nm.where(eval(where, {}, coor_dict))[0]

            region.vertices = vertices

        elif token == 'E_VOS':
            facets = domain.cmesh.get_surface_facets()

            region.set_kind('facet')
            region.facets = facets

        elif token == 'E_VBF':
            where = details[2]

            coors = domain.cmesh.coors

            fun = functions[where]
            vertices = fun(coors, domain=domain)

            region.vertices = vertices

        elif token == 'E_CBF':
            where = details[2]

            coors = domain.get_centroids(dim)

            fun = functions[where]
            cells = fun(coors, domain=domain)

            region.cells = cells

        elif token == 'E_COG':
            group = int(details[3])

            ig = domain.mat_ids_to_i_gs[group]
            region.cells = nm.where(domain.cmesh.cell_groups == ig)[0]

        elif token == 'E_COSET':
            raise NotImplementedError('element sets not implemented!')

        elif token == 'E_VOG':
            group = int(details[3])
            vertices = nm.where(domain.mesh.ngroups == group)[0]

            region.vertices = vertices

        elif token == 'E_VOSET':
            try:
                vertices = domain.vertex_set_bcs[details[3]]

            except KeyError:
                msg = 'undefined vertex set! (%s)' % details[3]
                raise ValueError(msg)

            region.vertices = vertices

        elif token == 'E_OVIR':
            aux = regions[details[3][2:]]
            region.vertices = aux.vertices[0:1]

        elif token == 'E_VI':
            region.vertices = nm.array([int(ii) for ii in details[1:]],
                                       dtype=nm.uint32)

        elif token == 'E_CI1':
            region.cells = nm.array([int(ii) for ii in details[1:]],
                                    dtype=nm.uint32)

        elif token == 'E_CI2':
            num = len(details[1:]) / 2

            cells = []
            for ii in range(num):
                ig, iel = int(details[1+2*ii]), int(details[2+2*ii])
                cells.append(iel + domain.mesh.el_offsets[ig])

            region.cells = cells

        else:
            output('token "%s" unkown - check regions!' % token)
            raise NotImplementedError
        return region

    return _region_leaf

def region_op(level, op_code, item1, item2):
    token = op_code['token']
    op = {'S' : '-', 'A' : '+', 'I' : '*'}[token[3]]

    if token[-1] == 'V':
        return item1.eval_op_vertices(item2, op)

    elif token[-1] == 'E':
        return item1.eval_op_edges(item2, op)

    elif token[-1] == 'F':
        return item1.eval_op_faces(item2, op)

    elif token[-1] == 'S':
        return item1.eval_op_facets(item2, op)

    elif token[-1] == 'C':
        return item1.eval_op_cells(item2, op)

    else:
        raise ValueError('unknown region operator token! (%s)' % token)

class Domain(Struct):

    def __init__(self, name, mesh=None, nurbs=None, bmesh=None, regions=None,
                 verbose=False):
        Struct.__init__(self, name=name, mesh=mesh, nurbs=nurbs, bmesh=bmesh,
                        regions=regions, verbose=verbose)

    def get_centroids(self, dim):
        """
        Return the coordinates of centroids of mesh entities with dimension
        `dim`.
        """
        return self.cmesh.get_centroids(dim)

    def has_faces(self):
        return self.shape.tdim == 3

    def reset_regions(self):
        """
        Reset the list of regions associated with the domain.
        """
        self.regions = OneTypeList(Region)
        self._region_stack = []
        self._bnf = create_bnf(self._region_stack)

    def create_region(self, name, select, kind='cell', parent=None,
                      check_parents=True, functions=None, add_to_regions=True):
        """
        Region factory constructor. Append the new region to
        self.regions list.
        """
        if check_parents:
            parents = get_parents(select)
            for p in parents:
                if p not in [region.name for region in self.regions]:
                    msg = 'parent region %s of %s not found!' % (p, name)
                    raise ValueError(msg)

        stack = self._region_stack
        try:
            self._bnf.parseString(select)
        except ParseException:
            print 'parsing failed:', select
            raise

        region = visit_stack(stack, region_op,
                             region_leaf(self, self.regions, select, functions))
        region.name = name
        region.definition = select
        region.set_kind(kind)
        region.parent = parent
        region.update_shape()

        if add_to_regions:
            self.regions.append(region)

        return region

    def create_regions(self, region_defs, functions=None):
        output('creating regions...')
        tt = time.clock()

        self.reset_regions()

        ##
        # Sort region definitions by dependencies.
        graph, name_to_sort_name = get_dependency_graph(region_defs)
        sorted_regions = sort_by_dependency(graph)

        ##
        # Define regions.
        for name in sorted_regions:
            sort_name = name_to_sort_name[name]
            rdef = region_defs[sort_name]

            region = self.create_region(name, rdef.select,
                                        kind=rdef.get('kind', 'cell'),
                                        parent=rdef.get('parent', None),
                                        check_parents=False,
                                        functions=functions)
            output(' ', region.name)

        output('...done in %.2f s' % (time.clock() - tt))

        return self.regions

    def save_regions(self, filename, region_names=None):
        """
        Save regions as individual meshes.

        Parameters
        ----------
        filename : str
            The output filename.
        region_names : list, optional
            If given, only the listed regions are saved.
        """
        import os

        if region_names is None:
            region_names = self.regions.get_names()

        trunk, suffix = os.path.splitext(filename)

        output('saving regions...')
        for name in region_names:
            region = self.regions[name]
            output(name)
            aux = self.mesh.from_region(region, self.mesh)
            aux.write('%s_%s%s' % (trunk, region.name, suffix),
                      io='auto')
        output('...done')

    def save_regions_as_groups(self, filename, region_names=None):
        """
        Save regions in a single mesh but mark them by using different
        element/node group numbers.

        If regions overlap, the result is undetermined, with exception of the
        whole domain region, which is marked by group id 0.

        Region masks are also saved as scalar point data for output formats
        that support this.

        Parameters
        ----------
        filename : str
            The output filename.
        region_names : list, optional
            If given, only the listed regions are saved.
        """

        output('saving regions as groups...')
        aux = self.mesh.copy()
        n_ig = c_ig = 0
        n_nod = self.shape.n_nod

        # The whole domain region should go first.
        names = (region_names if region_names is not None
                 else self.regions.get_names())
        for name in names:
            region = self.regions[name]
            if region.vertices.shape[0] == n_nod:
                names.remove(region.name)
                names = [region.name] + names
                break

        out = {}
        for name in names:
            region = self.regions[name]
            output(region.name)

            aux.ngroups[region.vertices] = n_ig
            n_ig += 1

            mask = nm.zeros((n_nod, 1), dtype=nm.float64)
            mask[region.vertices] = 1.0
            out[name] = Struct(name='region', mode='vertex', data=mask,
                               var_name=name, dofs=None)

            if region.has_cells():
                for ig in region.igs:
                    ii = region.get_cells(ig)
                    aux.mat_ids[ig][ii] = c_ig
                    c_ig += 1

        aux.write(filename, io='auto', out=out)
        output('...done')

########NEW FILE########
__FILENAME__ = fields
import numpy as nm

from sfepy.base.base import output, iter_dict_of_lists, Struct, basestr

def parse_approx_order(approx_order):
    """
    Parse the uniform approximation order value (str or int).
    """
    ao_msg = 'unsupported approximation order! (%s)'
    force_bubble = False
    discontinuous = False

    if approx_order is None:
        return 'iga', force_bubble, discontinuous

    try:
        ao = int(approx_order)
    except ValueError:
        mode = approx_order[-1].lower()

        if mode == 'b':
            ao = int(approx_order[:-1])
            force_bubble = True

        elif mode == 'd':
            ao = int(approx_order[:-1])
            discontinuous = True

        else:
            raise ValueError(ao_msg % approx_order)

    if ao < 0:
        raise ValueError(ao_msg % approx_order)

    elif ao == 0:
        discontinuous = True

    return ao, force_bubble, discontinuous

def parse_shape(shape, dim):
    if isinstance(shape, basestr):
        try:
            shape = {'scalar' : (1,),
                     'vector' : (dim,)}[shape]
        except KeyError:
            raise ValueError('unsupported field shape! (%s)', shape)

    elif isinstance(shape, int):
        shape = (shape,)

    return shape

def setup_extra_data(conn_info):
    """
    Setup extra data required for non-volume integration.
    """
    for key, ii, info in iter_dict_of_lists(conn_info, return_keys=True):
        for var in info.all_vars:
            field = var.get_field()
            field.setup_extra_data(info.ps_tg, info, info.is_trace)

def fields_from_conf(conf, regions):
    fields = {}
    for key, val in conf.iteritems():
        field = Field.from_conf(val, regions)
        fields[field.name] = field

    return fields

class Field(Struct):
    """
    Base class for fields.
    """
    _all = None

    @staticmethod
    def from_args(name, dtype, shape, region, approx_order=1,
                  space='H1', poly_space_base='lagrange'):
        """
        Create a Field subclass instance corresponding to a given space.

        Parameters
        ----------
        name : str
            The field name.
        dtype : numpy.dtype
            The field data type: float64 or complex128.
        shape : int/tuple/str
            The field shape: 1 or (1,) or 'scalar', space dimension (2, or (2,)
            or 3 or (3,)) or 'vector', or a tuple. The field shape determines
            the shape of the FE base functions and is related to the number of
            components of variables and to the DOF per node count, depending
            on the field kind.
        region : Region
            The region where the field is defined.
        approx_order : int/str
            The FE approximation order, e.g. 0, 1, 2, '1B' (1 with bubble).
        space : str
            The function space name.
        poly_space_base : str
            The name of polynomial space base.

        Notes
        -----
        Assumes one cell type for the whole region!
        """
        conf = Struct(name=name, dtype=dtype, shape=shape, region=region.name,
                      approx_order=approx_order, space=space,
                      poly_space_base=poly_space_base)
        return Field.from_conf(conf, {region.name : region})

    @staticmethod
    def from_conf(conf, regions):
        """
        Create a Field subclass instance based on the configuration.
        """
        if Field._all is None:
            import sfepy
            from sfepy.base.base import load_classes

            field_files = [ii for ii
                           in sfepy.get_paths('sfepy/discrete/fem/fields*.py')
                           if 'fields_base.py' not in ii] \
                           + sfepy.get_paths('sfepy/discrete/iga/fields*.py')
            Field._all = load_classes(field_files, [Field], ignore_errors=True,
                                      name_attr='family_name')
        table = Field._all

        space = conf.get('space', 'H1')
        poly_space_base = conf.get('poly_space_base', 'lagrange')

        key = space + '_' + poly_space_base

        approx_order = parse_approx_order(conf.approx_order)
        ao, force_bubble, discontinuous = approx_order
        region = regions[conf.region]

        if region.kind == 'cell':
            # Volume fields.
            kind = 'volume'

            if discontinuous:
                cls = table[kind + '_' + key + '_discontinuous']

            else:
                cls = table[kind + '_' + key]

            obj = cls(conf.name, conf.dtype, conf.shape, region,
                      approx_order=approx_order[:2])

        else:
            # Surface fields.
            kind = 'surface'

            cls = table[kind + '_' + key]
            obj = cls(conf.name, conf.dtype, conf.shape, region,
                      approx_order=approx_order[:2])

        return obj

    def _setup_kind(self):
        name = self.get('family_name', None,
                        'An abstract Field method called!')
        aux = name.split('_')
        self.space = aux[1]
        self.poly_space_base = aux[2]

    def get_dofs_in_region(self, region, merge=False, clean=False,
                           warn=False, igs=None):
        """
        Return indices of DOFs that belong to the given region.
        """
        if igs is None:
            igs = region.igs

        nods = []
        for ig in self.igs:
            if not ig in igs:
                nods.append(None)
                continue

            nn = self.get_dofs_in_region_group(region, ig)
            nods.append(nn)

        if merge:
            nods = [nn for nn in nods if nn is not None]
            nods = nm.unique(nm.hstack(nods))

        elif clean:
            for nn in nods[:]:
                if nn is None:
                    nods.remove(nn)
                    if warn is not None:
                        output(warn + ('%s' % region.name))

        return nods

    def clear_mappings(self, clear_all=False):
        """
        Clear current reference mappings.
        """
        self.mappings = {}
        if clear_all:
            self.mappings0 = {}

    def save_mappings(self):
        """
        Save current reference mappings to `mappings0` attribute.
        """
        self.mappings0 = self.mappings.copy()

    def get_mapping(self, ig, region, integral, integration,
                    get_saved=False, return_key=False):
        """
        For given region, integral and integration type, get a reference
        mapping, i.e. jacobians, element volumes and base function
        derivatives for Volume-type geometries, and jacobians, normals
        and base function derivatives for Surface-type geometries
        corresponding to the field approximation.

        The mappings are cached in the field instance in `mappings`
        attribute. The mappings can be saved to `mappings0` using
        `Field.save_mappings`. The saved mapping can be retrieved by
        passing `get_saved=True`. If the required (saved) mapping
        is not in cache, a new one is created.

        Returns
        -------
        geo : CMapping instance
            The reference mapping.
        mapping : VolumeMapping or SurfaceMapping instance
            The mapping.
        key : tuple
            The key of the mapping in `mappings` or `mappings0`.
        """
        key = (integral.name, region.name, ig, integration)

        if get_saved:
            out = self.mappings0.get(key, None)

        else:
            out = self.mappings.get(key, None)

        if out is None:
            out = self.create_mapping(ig, region, integral, integration)
            self.mappings[key] = out

        if return_key:
            out = out + (key,)

        return out

########NEW FILE########
__FILENAME__ = mappings
"""
Reference-physical domain mappings.
"""
import numpy as nm

from sfepy.base.base import Struct

class PhysicalQPs(Struct):
    """
    Physical quadrature points in a region.
    """

    def __init__(self, igs, n_total=0, is_uniform=True):
        Struct.__init__(self, igs=igs, n_total=n_total, indx={}, rindx={},
                        n_per_group={}, shape={}, values={},
                        is_uniform=is_uniform)
        for ig in self.igs:
            self.indx[ig] = slice(None)
            self.rindx[ig] = slice(None)
            self.n_per_group[ig] = 0
            self.shape[ig] = (0, 0, 0)
            self.values[ig] = nm.empty(self.shape[ig], dtype=nm.float64)

    def get_merged_values(self):
        qps = nm.concatenate([self.values[ig] for ig in self.igs], axis=0)

        return qps

    def get_shape(self, rshape, ig=None):
        """
        Get shape from raveled shape.
        """
        if ig is None:
            if self.is_uniform:
                n_qp = self.shape[self.igs[0]][1]

            else:
                msg = 'ig argument must be given for non-uniform QPs!'
                raise ValueError(msg)

        else:
            n_qp = self.shape[ig][1]

        if (rshape[0] / n_qp) * n_qp != rshape[0]:
            raise ValueError('incompatible shapes! (n_qp: %d, %s)'
                             % (n_qp, rshape))

        shape = (rshape[0] / n_qp, n_qp) + rshape[1:]

        return shape

class Mapping(Struct):
    """
    Base class for mappings.
    """

    @staticmethod
    def from_args(region, kind='v', ig=None):
        """
        Create mapping from reference to physical entities in a given
        region, given the integration kind ('v' or 's').

        This mapping can be used to compute the physical quadrature
        points.

        Parameters
        ----------
        region : Region instance
            The region defining the entities.
        kind : 'v' or 's'
            The kind of the entities: 'v' - cells, 's' - facets.
        ig : int, optional
            The group index.

        Returns
        -------
        mapping : VolumeMapping or SurfaceMapping instance
            The requested mapping.
        """
        from sfepy.discrete.fem.domain import FEDomain
        from sfepy.discrete.iga.domain import IGDomain

        if isinstance(region.domain, FEDomain):
            import sfepy.discrete.fem.mappings as mm
            coors = region.domain.get_mesh_coors()
            if kind == 's':
                coors = coors[region.vertices]

            gel = region.domain.groups[ig].gel
            conn = region.domain.groups[ig].conn

            if kind == 'v':
                cells = region.get_cells(ig)

                mapping = mm.VolumeMapping(coors, conn[cells], gel=gel)

            elif kind == 's':
                from sfepy.discrete.fem.fe_surface import FESurface

                aux = FESurface('aux', region, gel.get_surface_entities(),
                                conn , ig)
                mapping = mm.SurfaceMapping(coors, aux.leconn,
                                            gel=gel.surface_facet)

        elif isinstance(region.domain, IGDomain):
            import sfepy.discrete.iga.mappings as mm
            mapping = mm.IGMapping(region.domain, region.cells)

        else:
            raise ValueError('unknown domain class! (%s)' % type(region.domain))

        return mapping

def get_physical_qps(region, integral, map_kind=None):
    """
    Get physical quadrature points corresponding to the given region
    and integral.
    """
    phys_qps = PhysicalQPs(region.igs)

    if map_kind is None:
        map_kind = 'v' if region.can_cells else 's'

    ii = 0
    for ig in region.igs:
        gmap = Mapping.from_args(region, map_kind, ig)

        gel = gmap.get_geometry()
        qp_coors, _ = integral.get_qp(gel.name)

        qps = gmap.get_physical_qps(qp_coors)
        n_el, n_qp = qps.shape[0], qps.shape[1]

        phys_qps.n_per_group[ig] = n_per_group = n_el * n_qp
        phys_qps.shape[ig] = qps.shape

        phys_qps.indx[ig] = slice(ii, ii + n_el)
        phys_qps.rindx[ig] = slice(ii * n_qp, (ii + n_el) * n_qp)

        ii += qps.shape[0]

        qps.shape = (n_per_group, qps.shape[2])
        phys_qps.values[ig] = qps
        phys_qps.n_total += n_el * n_qp

    return phys_qps

def get_mapping_data(name, field, integral, region=None, integration='volume'):
    """
    General helper function for accessing reference mapping data.

    Get data attribute `name` from reference mapping corresponding to
    `field` in `region` in quadrature points of the given `integral` and
    `integration` type.

    Parameters
    ----------
    name : str
        The reference mapping attribute name.
    field : Field instance
        The field defining the reference mapping.
    integral : Integral instance
        The integral defining quadrature points.
    region : Region instance, optional
        If given, use the given region instead of `field` region.
    integration : one of ('volume', 'surface', 'surface_extra')
        The integration type.

    Returns
    -------
    data : array
        The required data merged for all element groups.

    Notes
    -----
    Assumes the same element geometry in all element groups of the field!
    """
    data = None
    if region is None:
        region = field.region
    for ig in region.igs:
        geo, _ = field.get_mapping(ig, region, integral, integration)
        _data = getattr(geo, name)
        if data is None:
            data = _data

        else:
            data = nm.concatenate((data, _data), axis=0)

    return data

def get_jacobian(field, integral, region=None, integration='volume'):
    """
    Get the jacobian of reference mapping corresponding to `field`.

    Parameters
    ----------
    field : Field instance
        The field defining the reference mapping.
    integral : Integral instance
        The integral defining quadrature points.
    region : Region instance, optional
        If given, use the given region instead of `field` region.
    integration : one of ('volume', 'surface', 'surface_extra')
        The integration type.

    Returns
    -------
    jac : array
        The jacobian merged for all element groups.

    See Also
    --------
    get_mapping_data()

    Notes
    -----
    Assumes the same element geometry in all element groups of the field!
    """
    jac = get_mapping_data('det', field, integral, region=region,
                           integration=integration)
    return jac

def get_normals(field, integral, region):
    """
    Get the normals of element faces in `region`.

    Parameters
    ----------
    field : Field instance
        The field defining the reference mapping.
    integral : Integral instance
        The integral defining quadrature points.
    region : Region instance
        The given of the element faces.

    Returns
    -------
    normals : array
        The normals merged for all element groups.

    See Also
    --------
    get_mapping_data()

    Notes
    -----
    Assumes the same element geometry in all element groups of the field!
    """
    normals = get_mapping_data('normal', field, integral, region=region,
                               integration='surface')
    return normals

########NEW FILE########
__FILENAME__ = region
import re
from copy import copy

import numpy as nm

from sfepy.base.base import output, assert_, Struct

_depends = re.compile('r\.([a-zA-Z_0-9.]+)').findall

def get_parents(selector):
    """
    Given a region selector, return names of regions it is based on.
    """
    parents = _depends(selector)

    return parents

def get_dependency_graph(region_defs):
    """
    Return a dependency graph and a name-sort name mapping for given
    region definitions.
    """
    graph = {}
    name_to_sort_name = {}
    for sort_name, rdef in region_defs.iteritems():
        name, sel = rdef.name, rdef.select
        if name_to_sort_name.has_key(name):
            msg = 'region %s/%s already defined!' % (sort_name, name)
            raise ValueError(msg)
        name_to_sort_name[name] = sort_name

        if not graph.has_key(name):
            graph[name] = [0]

        for parent in get_parents(sel):
            graph[name].append(parent)

        if rdef.get('parent', None) is not None:
            graph[name].append(rdef.parent)

    return graph, name_to_sort_name

def sort_by_dependency(graph):
    out = []

    n_nod = len(graph)
    idone = 0
    idone0 = -1
    while idone < n_nod:

        dep_removed = 0
        for node, deps in graph.iteritems():

            if (len(deps) == 1) and not deps[0]:
                out.append(node)
                deps[0] = 1
                idone += 1

            elif not deps[0]:

                for ii, dep in enumerate(deps[1:]):
                    if not dep in graph:
                        msg = 'dependency %s of region %s does not exist!'
                        raise ValueError(msg % (dep, node))

                    if graph[dep][0]:
                        ir = deps.index(dep)
                        deps.pop(ir)
                        dep_removed += 1

        if (idone <= idone0) and not dep_removed:
            raise ValueError, 'circular dependency'
        idone0 = idone

    return out

def _join(def1, op, def2):
    return '(' + def1 + ' ' + op + ' ' + def2 + ')'

def _try_delete(obj, ig):
    try:
        del obj[ig]
    except KeyError:
        pass

class Region(Struct):
    """
    Region defines a subset of a FE domain.

    Region kinds:

    - cell_only, facet_only, face_only, edge_only, vertex_only - only the
      specified entities are included, others are empty sets (so that the
      operators are still defined)
    - cell, facet, face, edge, vertex - entities of higher dimension are not
      included

    The 'cell' kind is the most general and it is the default.

    Region set-like operators: + (union), - (difference), * (intersection),
    followed by one of ('v', 'e', 'f', 'c', and 's') for vertices, edges,
    faces, cells, and facets.

    Notes
    -----
    Functions depending on `ig` are adapters for current code that should be
    removed after new assembling is done.

    Created: 31.10.2005
    """
    __can = {
        'cell'        : (1, 1, 1, 1),
        'face'        : (1, 1, 1, 0),
        'edge'        : (1, 1, 0, 0),
        'vertex'      : (1, 0, 0, 0),
        'cell_only'   : (0, 0, 0, 1),
        'face_only'   : (0, 0, 1, 0),
        'edge_only'   : (0, 1, 0, 0),
        'vertex_only' : (1, 0, 0, 0),
    }

    __facet_kinds = {
        2 : {'facet' : 'edge', 'facet_only' : 'edge_only'},
        3 : {'facet' : 'face', 'facet_only' : 'face_only'},
    }

    __op_to_fun = {
        '+' : nm.union1d,
        '-' : nm.setdiff1d,
        '*' : nm.intersect1d,
    }

    @staticmethod
    def from_vertices(vertices, domain, name='region', kind='cell'):
        """
        Create a new region containing given vertices.

        Parameters
        ----------
        vertices : array
            The array of vertices.
        domain : Domain instance
            The domain containing the vertices.
        name : str, optional
            The name of the region.
        kind : str, optional
            The kind of the region.

        Returns
        -------
        obj : Region instance
            The new region.
        """
        obj = Region(name, 'given vertices', domain, '', kind=kind)
        obj.vertices = vertices

        return obj

    @staticmethod
    def from_facets(facets, domain, name='region', kind='facet', parent=None):
        """
        Create a new region containing given facets.

        Parameters
        ----------
        facets : array
            The array with indices to unique facets.
        domain : Domain instance
            The domain containing the facets.
        name : str, optional
            The name of the region.
        kind : str, optional
            The kind of the region.
        parent : str, optional
            The name of the parent region.

        Returns
        -------
        obj : Region instance
            The new region.
        """
        obj = Region(name, 'given faces', domain, '', kind=kind, parent=parent)
        obj.facets = facets

        return obj

    def __init__(self, name, definition, domain, parse_def, kind='cell',
                 parent=None):
        """
        Create region instance.

        Parameters
        ----------
        name : str
            The region name, either given, or automatic for intermediate
            regions.
        definition : str
            The region selector definition.
        domain : Domain instance
            The domain of the region.
        parse_def : str
            The parsed definition of the region.
        kind : str
            The region kind - one of 'cell', 'facet', 'face', 'edge', 'vertex',
            'cell_only', ..., 'vertex_only'.
        parent : str, optional
            The name of the parent region.
        """
        tdim = domain.shape.tdim
        Struct.__init__(self,
                        name=name, definition=definition,
                        domain=domain, parse_def=parse_def,
                        n_v_max=domain.shape.n_nod, dim=domain.shape.dim,
                        tdim=tdim,
                        entities=[None] * (tdim + 1),
                        kind=None, parent=parent, shape=None,
                        mirror_region=None, ig_map=None,
                        ig_map_i=None)
        self.set_kind(kind)

    def set_kind(self, kind):
        if kind == self.kind: return

        self.kind = kind
        if 'facet' in kind:
            self.true_kind = self.__facet_kinds[self.tdim][kind]

        else:
            self.true_kind = kind

        can = [bool(ii) for ii in self.__can[self.true_kind]]

        self.can_vertices = can[0]
        self.can_edges = can[1]

        if self.tdim == 2:
            self.can = (can[0], can[1], can[3])
            self.can_cells = can[2]

        else:
            self.can = can
            self.can_faces = can[2]
            self.can_cells = can[3]

        for ii, ican in enumerate(self.can):
            if not ican:
                self.entities[ii] = nm.empty(0, dtype=nm.uint32)

        self._igs = None

    @property
    def vertices(self):
        if self.entities[0] is None:
            self._access(1)
            self.setup_from_highest(0)

        return self.entities[0]

    @vertices.setter
    def vertices(self, vals):
        if self.can_vertices:
            self.entities[0] = nm.asarray(vals, dtype=nm.uint32)

        else:
            raise ValueError('region "%s" cannot have vertices!' % self.name)

    @property
    def edges(self):
        if self.entities[1] is None:
            if 'edge' in self.true_kind:
                self.setup_from_vertices(1)

            else:
                self._access(2)
                self.setup_from_highest(1)

        return self.entities[1]

    @edges.setter
    def edges(self, vals):
        if self.can_edges:
            self.entities[1] = nm.asarray(vals, dtype=nm.uint32)

        else:
            raise ValueError('region "%s" cannot have edges!' % self.name)

    @property
    def faces(self):
        if self.tdim == 2:
            raise AttributeError('2D region has no faces!')

        if self.entities[2] is None:
            if 'face' in self.true_kind:
                self.setup_from_vertices(2)

            else:
                self._access(3)
                self.setup_from_highest(2)

        return self.entities[2]

    @faces.setter
    def faces(self, vals):
        if self.can_faces:
            self.entities[2] = nm.asarray(vals, dtype=nm.uint32)

        else:
            raise ValueError('region "%s" cannot have faces!' % self.name)

    @property
    def facets(self):
        if self.tdim == 3:
            return self.faces

        else:
            return self.edges

    @facets.setter
    def facets(self, vals):
        if self.tdim == 3:
            self.faces = vals

        else:
            self.edges = vals

    @property
    def cells(self):
        if self.entities[self.tdim] is None:
            self.setup_from_vertices(self.tdim)
        return self.entities[self.tdim]

    @cells.setter
    def cells(self, vals):
        if self.can_cells:
            self.entities[self.tdim] = nm.asarray(vals, dtype=nm.uint32)

        else:
            raise ValueError('region "%s" cannot have cells!' % self.name)

    def _access(self, dim):
        """
        Helper to access region entities of dimension `dim`.
        """
        if dim == 1:
            self.edges

        elif dim == 2:
            if self.tdim == 3:
                self.faces

            else:
                self.cells

        else:
            self.cells

    def setup_from_highest(self, dim):
        """
        Setup entities of topological dimension `dim` using the available
        entities of the highest topological dimension.
        """
        if not self.can[dim]: return

        for idim in range(self.tdim, -1, -1):
            if self.entities[idim] is not None:
                if self.entities[idim].shape[0] > 0:
                    break

        else:
            msg = 'region "%s" has no entities!'
            raise ValueError(msg % self.name)

        if idim <= dim:
            msg = 'setup_from_highest() can be used only with dim < %d'
            raise ValueError(msg % idim)

        cmesh = self.domain.cmesh
        cmesh.setup_connectivity(idim, dim)

        incident = cmesh.get_incident(dim, self.entities[idim], idim)
        self.entities[dim] = nm.unique(incident)

    def setup_from_vertices(self, dim):
        """
        Setup entities of topological dimension `dim` using the region
        vertices.
        """
        if not self.can[dim]: return

        cmesh = self.domain.cmesh
        cmesh.setup_connectivity(dim, 0)
        vv = self.vertices
        self.entities[dim] = cmesh.get_complete(dim, vv, 0)

    def eval_op_vertices(self, other, op):
        parse_def = _join(self.parse_def, '%sv' % op, other.parse_def)
        tmp = self.light_copy('op', parse_def)
        tmp.vertices = self.__op_to_fun[op](self.vertices, other.vertices)

        return tmp

    def eval_op_edges(self, other, op):
        parse_def = _join(self.parse_def, '%se' % op, other.parse_def)
        tmp = self.light_copy('op', parse_def)
        tmp.edges = self.__op_to_fun[op](self.edges, other.edges)

        return tmp

    def eval_op_faces(self, other, op):
        parse_def = _join(self.parse_def, '%sf' % op, other.parse_def)
        tmp = self.light_copy('op', parse_def)
        tmp.faces = self.__op_to_fun[op](self.faces, other.faces)

        return tmp

    def eval_op_facets(self, other, op):
        parse_def = _join(self.parse_def, '%ss' % op, other.parse_def)
        tmp = self.light_copy('op', parse_def)
        tmp.facets = self.__op_to_fun[op](self.facets, other.facets)

        return tmp

    def eval_op_cells(self, other, op):
        parse_def = _join(self.parse_def, '%sc' % op, other.parse_def)
        tmp = self.light_copy('op', parse_def)
        tmp.cells = self.__op_to_fun[op](self.cells, other.cells)

        return tmp

    def light_copy(self, name, parse_def):
        return Region(name, self.definition, self.domain, parse_def,
                      kind=self.kind)

    def copy(self):
        """
        Vertices-based copy.
        """
        tmp = self.light_copy('copy', self.parse_def)
        tmp.vertices = copy(self.vertices)

        return tmp

    def delete_zero_faces(self, eps=1e-14):
        raise NotImplementedError

    @property
    def igs(self):
        """
        Cell group indices according to region kind.
        """
        if self.parent is not None:
            self._igs = self.domain.regions[self.parent].igs

        elif self._igs is None:
            if 'vertex' in self.true_kind:
                self._igs = self.domain.cmesh.get_igs(self.vertices, 0)

            elif 'edge' in self.true_kind:
                self._igs = self.domain.cmesh.get_igs(self.edges, 1)

            elif 'face' in self.true_kind:
                self._igs = self.domain.cmesh.get_igs(self.faces, 2)

            elif 'cell' in self.true_kind:
                self._igs = self.domain.cmesh.get_igs(self.cells, self.tdim)

            if not len(self._igs):
                output('warning: region %s of %s kind has empty group indices!'
                       % (self.name, self.kind))

        return self._igs

    def update_shape(self):
        """
        Update shape of each group according to region vertices, edges,
        faces and cells.
        """
        get = self.domain.cmesh.get_from_cell_group

        self.shape = {}
        for ig in self.igs:
            n_vertex = get(ig, 0, self.vertices).shape[0]
            n_edge = get(ig, 1, self.edges).shape[0]
            n_cell = get(ig, self.tdim, self.cells).shape[0]
            if self.tdim == 3:
                n_face = get(ig, 2, self.faces).shape[0]

            else:
                n_face = 0

            self.shape[ig] = Struct(n_vertex=n_vertex,
                                    n_edge=n_edge,
                                    n_face=n_face,
                                    n_cell=n_cell)

    def get_entities(self, ig, dim):
        """
        Return mesh entities of dimension `dim` with cell group `ig`.
        """
        out = self.domain.cmesh.get_from_cell_group(ig, dim, self.entities[dim])
        return out

    def get_vertices_of_cells(self):
        """
        Return all vertices, that are in some cell of the region.
        """
        vertices = self.domain.cmesh.get_incident(0, self.cells, self.tdim)

        return nm.unique(vertices)

    def get_vertices(self, ig):
        out = self.domain.cmesh.get_from_cell_group(ig, 0, self.vertices)
        return out

    def get_edges(self, ig):
        out = self.domain.cmesh.get_from_cell_group(ig, 1, self.edges)
        return out

    def get_faces(self, ig):
        out = self.domain.cmesh.get_from_cell_group(ig, 2, self.faces)
        return out

    def get_facets(self, ig):
        """
        Return either region edges (in 2D) or faces (in 3D) .
        """
        if self.tdim == 2:
            return self.get_edges(ig)

        else:
            return self.get_faces(ig)

    def get_cells(self, ig, true_cells_only=True, offset=True):
        """
        Get cells of the region.

        Raises ValueError if `true_cells_only` is True and the region kind does
        not allow cells (e.g. surface integration region). For
        `true_cells_only` equal to False, cells incident to facets are returned
        if the region itself contains no cells.

        If `offset` is True, the cell group offset is subtracted from the cell
        ids.
        """
        cmesh = self.domain.cmesh

        if self.cells.shape[0] == 0:
            if true_cells_only:
                msg = 'region %s has not true cells! (has kind: %s)' \
                      % (self.name, self.kind)
                raise ValueError(msg)

            else:
                # Has to be consistent with get_facet_indices()!
                cmesh.setup_connectivity(self.tdim - 1, self.tdim)
                out = cmesh.get_incident(self.tdim, self.facets, self.tdim - 1)

                igs = cmesh.cell_groups[out]
                ic = nm.where(igs == ig)
                out = out[ic]

        else:
            out = cmesh.get_from_cell_group(ig, self.tdim, self.cells)

        if offset:
            out -= self.domain.cell_offsets[ig]

        return out

    def get_facet_indices(self, ig, offset=True, force_ig=True):
        """
        Return an array (per group) of (iel, ifa) for each facet. A facet can
        be in 1 (surface) or 2 (inner) cells.

        If `offset` is True, the cell group offset is subtracted from the cell
        ids.

        If `force_ig` is True, only the cells with the given `ig` are used.
        """
        cmesh = self.domain.cmesh
        facets = self.get_facets(ig)
        cells, offs = cmesh.get_incident(self.tdim, facets, self.tdim - 1,
                                         ret_offsets=True)
        ii = cmesh.get_local_ids(facets, self.tdim - 1, cells, offs, self.tdim)
        fis = nm.c_[cells, ii]

        if force_ig:
            igs = cmesh.cell_groups[cells]
            ic = nm.where(igs == ig)
            fis = fis[ic]

        if offset:
            fis[:, 0] -= self.domain.cell_offsets[ig]

        return fis

    def setup_mirror_region(self):
        """
        Find the corresponding mirror region, set up element mapping.
        """
        regions = self.domain.regions

        parent = regions[self.parent]
        for reg in regions:
            mirror_parent = regions.find(reg.parent)
            if mirror_parent is None: continue
            if ((reg is not self)
                and (len(reg.igs) == len(self.igs))
                and (not len(nm.intersect1d(parent.igs, mirror_parent.igs)))
                and nm.all(self.vertices == reg.vertices)):
                mirror_region = reg
                break
        else:
            raise ValueError('cannot find mirror region! (%s)' % self.name)

        ig_map = {}
        ig_map_i = {}
        for igr in parent.igs:
            v1 = self.get_vertices(igr)
            for igc in mirror_parent.igs:
                v2 = self.get_vertices(igc)
                if nm.all(v1 == v2):
                    ig_map[igc] = igr
                    ig_map_i[igr] = igc
                    break
            else:
                raise ValueError('cannot find mirror region group! (%d)' % igr)

        self._igs = parent._igs
        mirror_region._igs = mirror_parent._igs

        self.mirror_region = mirror_region
        self.ig_map = ig_map
        self.ig_map_i = ig_map_i

    def get_mirror_region(self):
        return self.mirror_region, self.ig_map, self.ig_map_i

    def get_n_cells(self, ig=None, is_surface=False):
        """
        Get number of region cells.

        Parameters
        ----------
        ig : int, optional
            The group index. If None, counts from all groups are added
            together.
        is_surface : bool
            If True, number of edges or faces according to domain
            dimension is returned instead.

        Returns
        -------
        n_cells : int
            The number of cells.
        """
        if ig is not None:
            if is_surface:
                if self.domain.groups[ig].shape.dim == 2:
                    return self.shape[ig].n_edge

                else:
                    return self.shape[ig].n_face

            else:
                return self.shape[ig].n_cell

        else:
            return sum(self.get_n_cells(ig, is_surface=is_surface)
                       for ig in self.igs)

    def iter_cells(self):
        ii = 0
        off = 0
        for ig in self.igs:
            n_cell = self.shape[ig].n_cell
            for iel in self.cells[off : off + n_cell]:
                yield ig, ii, iel - off
                ii += 1

            off += n_cell

    def has_cells(self):
        return self.cells.size > 0

    def contains(self, other):
        """
        Tests only igs for now!!!
        """
        return set(other.igs).issubset(set(self.igs))

    def get_cell_offsets(self):
        offs = {}
        off = 0
        for ig in self.igs:
            offs[ig] = off
            off += self.shape[ig].n_cell
        return offs

    def get_charfun(self, by_cell=False, val_by_id=False):
        """
        Return the characteristic function of the region as a vector of values
        defined either in the mesh vertices (by_cell == False) or cells. The
        values are either 1 (val_by_id == False) or sequential id + 1.
        """
        if by_cell:
            chf = nm.zeros((self.domain.shape.n_el,), dtype=nm.float64)
            if val_by_id:
                chf[self.cells] = self.cells + 1
            else:
                chf[self.cells] = 1.0

        else:
            chf = nm.zeros((self.domain.shape.n_nod,), dtype=nm.float64)
            if val_by_id:
                chf[self.vertices] = self.vertices + 1
            else:
                chf[self.vertices] = 1.0

        return chf

    def get_edge_graph(self):
        """
        Return the graph of region edges as a sparse matrix having uid(k) + 1
        at (i, j) if vertex[i] is connected with vertex[j] by the edge k.

        Degenerate edges are ignored.
        """
        from scipy.sparse import csr_matrix

        cmesh = self.domain.cmesh

        e_verts = cmesh.get_incident(0, self.edges, 1)
        e_verts.shape = (e_verts.shape[0] / 2, 2)

        ii = nm.where(e_verts[:, 0] != e_verts[:, 1])[0]
        edges = self.edges[ii]
        e_verts = e_verts[ii]

        vals = edges + 1
        rows = e_verts[:, 0]
        cols = e_verts[:, 1]

        num = self.vertices.max() + 1
        graph = csr_matrix((vals, (rows, cols)), shape=(num, num))

        nnz = graph.nnz
        # Symmetrize.
        graph = graph + graph.T
        assert_(graph.nnz == 2 * nnz)

        return graph

########NEW FILE########
__FILENAME__ = conditions
"""
The Dirichlet, periodic and linear combination boundary condition
classes, as well as the initial condition class.
"""
import numpy as nm

from sfepy.base.base import Container, Struct
from sfepy.discrete.functions import Function

def get_condition_value(val, functions, kind, name):
    """
    Check a boundary/initial condition value type and return the value or
    corresponding function.
    """
    if type(val) == str:
        if functions is not None:
            try:
                fun = functions[val]

            except IndexError:
                raise ValueError('unknown function %s given for %s %s!'
                                 % (val, kind, name))

        else:
            raise ValueError('no functions given for %s %s!' % (kind, name))

    elif (isinstance(val, Function) or nm.isscalar(val)
          or isinstance(val, nm.ndarray)):
        fun = val

    else:
        raise ValueError('unknown value type for %s %s!'
                         % (kind, name))

    return fun

def _get_region(name, regions, bc_name):
    try:
        region = regions[name]
    except IndexError:
        msg = "no region '%s' used in condition %s!" % (name, bc_name)
        raise IndexError( msg )

    return region

class Conditions(Container):
    """
    Container for various conditions.
    """
    @staticmethod
    def from_conf(conf, regions):
        conds = []
        for key, cc in conf.iteritems():
            times = cc.get('times', None)

            if 'ebc' in key:
                region = _get_region(cc.region, regions, cc.name)
                cond = EssentialBC(cc.name, region, cc.dofs, key=key,
                                   times=times)

            elif 'epbc' in key:
                rs = [_get_region(ii, regions, cc.name) for ii in cc.region]
                cond = PeriodicBC(cc.name, rs, cc.dofs, cc.match, key=key,
                                   times=times)

            elif 'lcbc' in key:
                region = _get_region(cc.region, regions, cc.name)
                filename = cc.get('filename', None)
                cond = LinearCombinationBC(cc.name, region, cc.dofs, key=key,
                                           filename=filename,
                                           times=times)

            elif 'ic' in key:
                region = _get_region(cc.region, regions, cc.name)
                cond = InitialCondition(cc.name, region, cc.dofs, key=key)

            else:
                raise ValueError('unknown condition type! (%s)' % key)

            conds.append(cond)

        obj = Conditions(conds)
        return obj

    def group_by_variables(self, groups=None):
        """
        Group boundary conditions of each variable. Each condition is a
        group is a single condition.

        Parameters
        ----------
        groups : dict, optional
            If present, update the `groups` dictionary.

        Returns
        -------
        out : dict
            The dictionary with variable names as keys and lists of
            single condition instances as values.
        """
        if groups is None:
            out = {}

        else:
            out = groups

        for cond in self:
            for single_cond in cond.iter_single():
                vname = single_cond.dofs[0].split('.')[0]
                out.setdefault(vname, Conditions()).append(single_cond)

        return out

    def canonize_dof_names(self, dofs):
        """
        Canonize the DOF names using the full list of DOFs of a
        variable.
        """
        for cond in self:
            cond.canonize_dof_names(dofs)

    def sort(self):
        """
        Sort boundary conditions by their key.
        """
        self._objs.sort(cmp=lambda i1, i2: cmp(i1.key, i2.key))
        self.update()

    def zero_dofs(self):
        """
        Set all boundary condition values to zero, if applicable.
        """
        for cond in self:
            if isinstance(cond, EssentialBC):
                cond.zero_dofs()

def _canonize(dofs, all_dofs):
    """
    Helper function.
    """
    vname, dd = dofs.split('.')

    if dd == 'all':
        cdofs = all_dofs

    elif dd[0] == '[':
        cdofs = [vname + '.' + ii.strip()
                 for ii in dd[1:-1].split(',')]

    else:
        cdofs = [dofs]

    return cdofs

class Condition(Struct):
    """
    Common boundary condition methods.
    """
    def __init__(self, name, **kwargs):
        Struct.__init__(self, name=name, **kwargs)
        self.is_single = False

    def iter_single(self):
        """
        Create a single condition instance for each item in self.dofs
        and yield it.
        """
        for dofs, val in self.dofs.iteritems():
            single_cond = self.copy(name=self.name)
            single_cond.is_single = True
            single_cond.dofs = [dofs, val]
            yield single_cond

    def canonize_dof_names(self, dofs):
        """
        Canonize the DOF names using the full list of DOFs of a
        variable.
        
        Assumes single condition instance.
        """
        self.dofs[0] = _canonize(self.dofs[0], dofs)

class EssentialBC(Condition):
    """
    Essential boundary condidion.

    Parameters
    ----------
    name : str
        The boundary condition name.
    region : Region instance
        The region where the boundary condition is applied.
    dofs : dict
        The boundary condition specification defining the constrained
        DOFs and their values.
    key : str, optional
        The sorting key.
    times : list or str, optional
        The list of time intervals or a function returning True at time
        steps, when the condition applies.
    """
    def __init__(self, name, region, dofs, key='', times=None):
        Condition.__init__(self, name=name, region=region, dofs=dofs, key=key,
                           times=times)

    def zero_dofs(self):
        """
        Set all essential boundary condition values to zero.
        """
        if self.is_single:
            self.dofs[1] = 0.0

        else:
            new_dofs = {}
            for key in self.dofs.iterkeys():
                new_dofs[key] = 0.0

            self.dofs = new_dofs

class PeriodicBC(Condition):
    """
    Periodic boundary condidion.

    Parameters
    ----------
    name : str
        The boundary condition name.
    regions : list of two Region instances
        The master region and the slave region where the DOFs should match.
    dofs : dict
        The boundary condition specification defining the DOFs in the master
        region and the corresponding DOFs in the slave region.
    match : str
        The name of function for matching corresponding nodes in the
        two regions.
    key : str, optional
        The sorting key.
    times : list or str, optional
        The list of time intervals or a function returning True at time
        steps, when the condition applies.
    """
    def __init__(self, name, regions, dofs, match, key='', times=None):
        Condition.__init__(self, name=name, regions=regions, dofs=dofs,
                           match=match, key=key, times=times)

    def canonize_dof_names(self, dofs):
        """
        Canonize the DOF names using the full list of DOFs of a
        variable.
        
        Assumes single condition instance.
        """
        self.dofs[0] = _canonize(self.dofs[0], dofs)
        self.dofs[1] = _canonize(self.dofs[1], dofs)

class LinearCombinationBC(Condition):
    """
    Linear combination boundary condidion.

    Parameters
    ----------
    name : str
        The boundary condition name.
    region : Region instance
        The region where the boundary condition is applied.
    dofs : dict
        The boundary condition specification defining the constrained
        DOFs and the constraint type.
    key : str, optional
        The sorting key.
    times : list or str, optional
        The list of time intervals or a function returning True at time
        steps, when the condition applies.
    filename : str, optional
        Some conditions can store data (e.g. normal vectors) into a file.
    """
    def __init__(self, name, region, dofs, key='', times=None, filename=None):
        Condition.__init__(self, name=name, region=region, dofs=dofs,
                           key=key, times=times, filename=filename)

class InitialCondition(Condition):
    """
    Initial condidion.

    Parameters
    ----------
    name : str
        The initial condition name.
    region : Region instance
        The region where the initial condition is applied.
    dofs : dict
        The initial condition specification defining the constrained
        DOFs and their values.
    key : str, optional
        The sorting key.
    """
    def __init__(self, name, region, dofs, key=''):
        Condition.__init__(self, name=name, region=region, dofs=dofs, key=key)

########NEW FILE########
__FILENAME__ = equations
"""
Classes of equations composed of terms.
"""
import time
from copy import copy

import numpy as nm
import scipy.sparse as sp

from sfepy.base.base import output, assert_, get_default, iter_dict_of_lists
from sfepy.base.base import debug, OneTypeList, Container, Struct
from sfepy.discrete import Materials, Variables, create_adof_conns
from sfepy.discrete.fem.extmods.cmesh import create_mesh_graph
from sfepy.terms import Terms, Term

def parse_definition(equation_def):
    """
    Parse equation definition string to create term description list.
    """
    from parse_equations import create_bnf

    term_descs = []
    bnf = create_bnf(term_descs)
    try:
        bnf.parseString(equation_def)
    except:
        raise ValueError('cannot parse equation! (%s)' % equation_def)

    return term_descs

def get_expression_arg_names(expression, strip_dots=True):
    """
    Parse expression and return set of all argument names. For arguments
    with attribute-like syntax (e.g. materials), if `strip_dots` is
    True, only base argument names are returned.
    """
    args = ','.join(aux.args for aux in parse_definition(expression))
    args = [arg.strip() for arg in args.split(',')]

    if strip_dots:
        for ii, arg in enumerate(args[:]):
            aux = arg.split('.')
            if len(aux) == 2:
                args[ii] = aux[0]

    return set(args)

class Equations(Container):

    @staticmethod
    def from_conf(conf, variables, regions, materials, integrals,
                  user=None, verbose=True):

        objs = OneTypeList(Equation)

        conf = copy(conf)

        ii = 0
        for name, desc in conf.iteritems():
            if verbose:
                output('equation "%s":' %  name)
                output(desc)
            eq = Equation.from_desc(name, desc, variables, regions,
                                    materials, integrals, user=user)
            objs.append(eq)
            ii += 1

        obj = Equations(objs)

        return obj

    def __init__(self, equations):
        Container.__init__(self, equations)

        self.variables = Variables(self.collect_variables())
        self.materials = Materials(self.collect_materials())

        self.domain = self.get_domain()

        self.active_bcs = set()

        self.collect_conn_info()

    def create_subequations(self, var_names, known_var_names=None):
        """
        Create sub-equations containing only terms with the given virtual
        variables.

        Parameters
        ----------
        var_names : list
            The list of names of virtual variables.
        known_var_names : list
            The list of  names of (already) known state variables.

        Returns
        -------
        subequations : Equations instance
            The sub-equations.
        """
        from sfepy.discrete import FieldVariable

        known_var_names = get_default(known_var_names, [])

        objs = []
        for iv, var_name in enumerate(var_names):
            terms = [term.copy(name=term.name)
                     for eq in self for term in eq.terms
                     if term.get_virtual_name() == var_name]

            # Make parameter variables from known state variables in terms
            # arguments.
            for known_name in known_var_names:
                for term in terms:
                    if known_name in term.arg_names:
                        ii = term.arg_names.index(known_name)
                        state = self.variables[known_name]
                        par = FieldVariable(known_name, 'parameter',
                                            state.field,
                                            primary_var_name='(set-to-None)')
                        term.args[ii] = par
                        term._kwargs[known_name] = par
                        par.set_data(state())

            new_terms = Terms(terms)
            objs.append(Equation('eq_%d' % iv, new_terms))

        subequations = Equations(objs)

        return subequations

    def get_domain(self):
        domain = None

        for eq in self:
            for term in eq.terms:
                if term.has_region:
                    domain = term.region.domain

        return domain

    def collect_materials(self):
        """
        Collect materials present in the terms of all equations.
        """
        materials = []
        for eq in self:
            materials.extend(eq.collect_materials())

        # Make the list items unique.
        materials = list(set(materials))

        return materials

    def reset_materials(self):
        """
        Clear material data so that next materials.time_update() is
        performed even for stationary materials.
        """
        self.materials.reset()

    def collect_variables(self):
        """
        Collect variables present in the terms of all equations.
        """
        variables = []
        for eq in self:
            variables.extend(eq.collect_variables())

        # Make the list items unique.
        variables = list(set(variables))

        return variables

    def get_variable(self, name):
        var = self.variables.get(name,
                                 msg_if_none='unknown variable! (%s)' % name)
        return var

    def collect_conn_info(self):
        """
        Collect connectivity information as defined by the equations.
        """
        self.conn_info = {}

        for eq in self:
            eq.collect_conn_info(self.conn_info)

        return self.conn_info

    def get_variable_names(self):
        """
        Return the list of names of all variables used in equations.
        """
        vns = set()
        for eq in self:
            for term in eq.terms:
                vns.update(term.get_variable_names())
        return list(vns)

    def get_variable_dependencies(self):
        """
        For each virtual variable get names of state/parameter variables that
        are present in terms with that virtual variable.

        The virtual variables define the actual equations and their
        dependencies define the variables needed to evaluate the equations.

        Returns
        -------
        deps : dict
            The dependencies as a dictionary with virtual variable names as
            keys and sets of state/parameter variables as values.
        """
        deps = {}
        for eq in self:
            for term in eq.terms:
                dep_list = deps.setdefault(term.get_virtual_name(), set())
                dep_list.update(term.get_state_names())

        return deps

    def invalidate_term_caches(self):
        """
        Invalidate evaluate caches of variables present in equations.
        """
        for var in self.variables:
            var.invalidate_evaluate_cache()

    def print_terms(self):
        """
        Print names of equations and their terms.
        """
        output('equations:')
        for eq in self:
            output('  %s:' % eq.name)
            for term in eq.terms:
                output('    %+.2e * %s.%d.%s(%s)'
                       % (term.sign, term.name, term.integral.order,
                          term.region.name, term.arg_str))

    def time_update(self, ts, ebcs=None, epbcs=None, lcbcs=None,
                    functions=None, problem=None, verbose=True):
        """
        Update the equations for current time step.

        The update involves creating the mapping of active DOFs from/to
        all DOFs for all state variables, the setup of linear
        combination boundary conditions operators and the setup of
        active DOF connectivities.

        Parameters
        ----------
        ts : TimeStepper instance
            The time stepper.
        ebcs : Conditions instance, optional
            The essential (Dirichlet) boundary conditions.
        epbcs : Conditions instance, optional
            The periodic boundary conditions.
        lcbcs : Conditions instance, optional
            The linear combination boundary conditions.
        functions : Functions instance, optional
            The user functions for boundary conditions, materials, etc.
        problem : Problem instance, optional
            The problem that can be passed to user functions as a context.
        verbose : bool
            If False, reduce verbosity.

        Returns
        -------
        graph_changed : bool
            The flag set to True if the current time step set of active
            boundary conditions differs from the set of the previous
            time step.
        """
        self.variables.time_update(ts, functions, verbose=verbose)

        active_bcs = self.variables.equation_mapping(ebcs, epbcs, ts, functions,
                                                     problem=problem)
        graph_changed = active_bcs != self.active_bcs
        self.active_bcs = active_bcs

        if graph_changed or not self.variables.adof_conns:
            adcs = create_adof_conns(self.conn_info, self.variables.adi.indx)
            self.variables.set_adof_conns(adcs)

        self.variables.setup_lcbc_operators(lcbcs, ts, functions)

        for eq in self:
            for term in eq.terms:
                term.time_update(ts)

        return graph_changed

    def time_update_materials(self, ts, mode='normal', problem=None,
                              verbose=True):
        """
        Update data materials for current time and possibly also state.

        Parameters
        ----------
        ts : TimeStepper instance
            The time stepper.
        mode : 'normal', 'update' or 'force'
            The update mode, see
            :func:`sfepy.discrete.materials.Material.time_update()`.
        problem : Problem instance, optional
            The problem that can be passed to user functions as a context.
        verbose : bool
            If False, reduce verbosity.
        """
        self.materials.time_update(ts, self, mode=mode, problem=problem,
                                   verbose=verbose)

    def setup_initial_conditions(self, ics, functions):
        self.variables.setup_initial_conditions(ics, functions)

    def get_graph_conns(self, any_dof_conn=False, rdcs=None, cdcs=None):
        """
        Get DOF connectivities needed for creating tangent matrix graph.

        Parameters
        ----------
        any_dof_conn : bool
            By default, only volume DOF connectivities are used, with
            the exception of trace surface DOF connectivities. If True,
            any kind of DOF connectivities is allowed.
        rdcs, cdcs : arrays, optional
            Additional row and column DOF connectivities, corresponding
            to the variables used in the equations.

        Returns
        -------
        rdcs, cdcs : arrays
            The row and column DOF connectivities defining the matrix
            graph blocks.
        """
        if rdcs is None:
            rdcs = []
            cdcs = []

        elif cdcs is None:
            cdcs = copy(rdcs)

        else:
            assert_(len(rdcs) == len(cdcs))
            if rdcs is cdcs: # Make sure the lists are not the same object.
                rdcs = copy(rdcs)

        adcs = self.variables.adof_conns

        # Only volume dof connectivities are used, with the exception of trace
        # surface dof connectivities.
        shared = set()
        for key, ii, info in iter_dict_of_lists(self.conn_info,
                                                return_keys=True):
            rvar, cvar = info.virtual, info.state
            if (rvar is None) or (cvar is None):
                continue

            is_surface = rvar.is_surface or cvar.is_surface

            dct = info.dc_type.type
            if not (dct in ('volume', 'scalar', 'plate') or is_surface
                    or info.is_trace or any_dof_conn):
                continue


            rreg_name = info.get_region_name(can_trace=False)
            creg_name = info.get_region_name()

            for rig, cig in info.iter_igs():
                rname = rvar.get_primary_name()
                rkey = (rname, rreg_name, dct, rig, False)
                ckey = (cvar.name, creg_name, dct, cig, info.is_trace)

                dc_key = (rkey, ckey)

                if not dc_key in shared:
                    try:
                        rdcs.append(adcs[rkey])
                        cdcs.append(adcs[ckey])
                    except:
                        debug()
                    shared.add(dc_key)

        return rdcs, cdcs

    def create_matrix_graph(self, any_dof_conn=False, rdcs=None, cdcs=None,
                            shape=None, verbose=True):
        """
        Create tangent matrix graph, i.e. preallocate and initialize the
        sparse storage needed for the tangent matrix. Order of DOF
        connectivities is not important.

        Parameters
        ----------
        any_dof_conn : bool
            By default, only volume DOF connectivities are used, with
            the exception of trace surface DOF connectivities. If True,
            any kind of DOF connectivities is allowed.
        rdcs, cdcs : arrays, optional
            Additional row and column DOF connectivities, corresponding
            to the variables used in the equations.
        shape : tuple, optional
            The required shape, if it is different from the shape
            determined by the equations variables. This may be needed if
            additional row and column DOF connectivities are passed in.
        verbose : bool
            If False, reduce verbosity.

        Returns
        -------
        matrix : csr_matrix
            The matrix graph in the form of a CSR matrix with
            preallocated structure and zero data.
        """
        if not self.variables.has_virtuals():
            output('no matrix (no test variables)!')
            return None

        shape = get_default(shape, self.variables.get_matrix_shape())

        output('matrix shape:', shape, verbose=verbose)
        if nm.prod(shape) == 0:
            output('no matrix (zero size)!')
            return None

        rdcs, cdcs = self.get_graph_conns(any_dof_conn=any_dof_conn,
                                          rdcs=rdcs, cdcs=cdcs)

        if not len(rdcs):
            output('no matrix (empty dof connectivities)!')
            return None

        output('assembling matrix graph...', verbose=verbose)
        tt = time.clock()

        nnz, prow, icol = create_mesh_graph(shape[0], shape[1],
                                            len(rdcs), rdcs, cdcs)

        output('...done in %.2f s' % (time.clock() - tt), verbose=verbose)
        output('matrix structural nonzeros: %d (%.2e%% fill)' \
               % (nnz, float(nnz) / nm.prod(shape)), verbose=verbose)

        data = nm.zeros((nnz,), dtype=self.variables.dtype)
        matrix = sp.csr_matrix((data, icol, prow), shape)

        return matrix

    def init_time(self, ts):
        pass

    def advance(self, ts):
        for eq in self:
            for term in eq.terms:
                term.advance(ts)

        self.variables.advance(ts)

    ##
    # Interface to self.variables.
    def create_state_vector(self):
        return self.variables.create_state_vector()

    def create_stripped_state_vector(self):
        return self.variables.create_stripped_state_vector()

    def strip_state_vector(self, vec, follow_epbc=False):
        """
        Strip a full vector by removing EBC dofs.

        Notes
        -----
        If 'follow_epbc' is True, values of EPBC master dofs are not simply
        thrown away, but added to the corresponding slave dofs, just like when
        assembling. For vectors with state (unknown) variables it should be set
        to False, for assembled vectors it should be set to True.
        """
        return self.variables.strip_state_vector(vec, follow_epbc=follow_epbc)

    def make_full_vec(self, svec, force_value=None):
        """
        Make a full DOF vector satisfying E(P)BCs from a reduced DOF
        vector.
        """
        return self.variables.make_full_vec(svec, force_value)

    def set_variables_from_state(self, vec, step=0):
        """
        Set data (vectors of DOF values) of variables.

        Parameters
        ----------
        data : array
            The state vector.
        step : int
            The time history step, 0 (default) = current.
        """
        self.variables.set_data(vec, step=step)

    def get_state_parts(self, vec=None):
        """
        Return parts of a state vector corresponding to individual state
        variables.

        Parameters
        ----------
        vec : array, optional
            The state vector. If not given, then the data stored in the
            variables are returned instead.

        Returns
        -------
        out : dict
            The dictionary of the state parts.
        """
        return self.variables.get_state_parts(vec)

    def set_data(self, data, step=0, ignore_unknown=False):
        """
        Set data (vectors of DOF values) of variables.

        Parameters
        ----------
        data : array
            The dictionary of {variable_name : data vector}.
        step : int, optional
            The time history step, 0 (default) = current.
        ignore_unknown : bool, optional
            Ignore unknown variable names if `data` is a dict.
        """
        self.variables.set_data(data, step=step,
                                ignore_unknown=ignore_unknown)

    def apply_ebc(self, vec, force_values=None):
        """
        Apply essential (Dirichlet) boundary conditions to a state vector.
        """
        self.variables.apply_ebc(vec, force_values=force_values)

    def apply_ic(self, vec, force_values=None):
        """
        Apply initial conditions to a state vector.
        """
        self.variables.apply_ic(vec, force_values=force_values)

    def state_to_output(self, vec, fill_value=None, var_info=None,
                        extend=True):
        return self.variables.state_to_output(vec,
                                              fill_value=fill_value,
                                              var_info=var_info,
                                              extend=extend)

    def get_lcbc_operator(self):
        return self.variables.get_lcbc_operator()

    def evaluate(self, names=None, mode='eval', dw_mode='vector',
                 term_mode=None, asm_obj=None):
        """
        Evaluate the equations.

        Parameters
        ----------
        mode : one of 'eval', 'el_avg', 'qp', 'weak'
            The evaluation mode.
        names : str or sequence of str, optional
            Evaluate only equations of the given name(s).

        Returns
        -------
        out : dict or result
            The evaluation result. In 'weak' mode it is the
            `asm_obj`. Otherwise, it is a dict of results with equation names
            as keys or a single result for a single equation.
        """
        if names is None:
            eqs = self
            single = (len(eqs) == 1)

        else:
            single = isinstance(names, str)
            if single:
                names = [names]

            eqs = [self[eq] for eq in names]

        if mode == 'weak':
            for eq in eqs:
                eq.evaluate(mode=mode, dw_mode=dw_mode,
                            term_mode=term_mode, asm_obj=asm_obj)

            out = asm_obj

        else:
            out = {}
            for eq in eqs:
                eout = eq.evaluate(mode=mode, dw_mode=dw_mode,
                                   term_mode=term_mode)
                out[eq.name] = eout

            if single:
                out = out.popitem()[1]

        return out

    def eval_residuals(self, state, by_blocks=False, names=None):
        """
        Evaluate (assemble) residual vectors.

        Parameters
        ----------
        state : array
            The vector of DOF values. Note that it is needed only in
            nonlinear terms.
        by_blocks : bool
            If True, return the individual blocks composing the whole
            residual vector. Each equation should then correspond to one
            required block and should be named as `'block_name,
            test_variable_name, unknown_variable_name'`.
        names : list of str, optional
            Optionally, select only blocks with the given `names`, if
            `by_blocks` is True.

        Returns
        -------
        out : array or dict of array
            The assembled residual vector. If `by_blocks` is True, a
            dictionary is returned instead, with keys given by
            `block_name` part of the individual equation names.
        """
        self.set_variables_from_state(state)

        if by_blocks:
            names = get_default(names, self.names)

            out = {}

            get_indx = self.variables.get_indx
            for name in names:
                eq = self[name]
                key, rname, cname = [aux.strip()
                                     for aux in name.split(',')]

                ir = get_indx(rname, stripped=True, allow_dual=True)

                residual = self.create_stripped_state_vector()
                eq.evaluate(mode='weak', dw_mode='vector', asm_obj=residual)

                out[key] = residual[ir]

        else:
            out = self.create_stripped_state_vector()

            self.evaluate(mode='weak', dw_mode='vector', asm_obj=out)

        return out

    def eval_tangent_matrices(self, state, tangent_matrix,
                              by_blocks=False, names=None):
        """
        Evaluate (assemble) tangent matrices.

        Parameters
        ----------
        state : array
            The vector of DOF values. Note that it is needed only in
            nonlinear terms.
        tangent_matrix : csr_matrix
            The preallocated CSR matrix with zero data.
        by_blocks : bool
            If True, return the individual blocks composing the whole
            matrix. Each equation should then correspond to one
            required block and should be named as `'block_name,
            test_variable_name, unknown_variable_name'`.
        names : list of str, optional
            Optionally, select only blocks with the given `names`, if
            `by_blocks` is True.

        Returns
        -------
        out : csr_matrix or dict of csr_matrix
            The assembled matrix. If `by_blocks` is True, a dictionary
            is returned instead, with keys given by `block_name` part
            of the individual equation names.
        """
        self.set_variables_from_state(state)

        if by_blocks:
            names = get_default(names, self.names)

            out = {}

            get_indx = self.variables.get_indx
            for name in names:
                eq = self[name]
                key, rname, cname = [aux.strip()
                                     for aux in eq.name.split(',')]

                ir = get_indx(rname, stripped=True, allow_dual=True)
                ic = get_indx(cname, stripped=True, allow_dual=True)

                tangent_matrix.data[:] = 0.0
                eq.evaluate(mode='weak', dw_mode='matrix',
                            asm_obj=tangent_matrix)

                out[key] = tangent_matrix[ir, ic]

        else:
            tangent_matrix.data[:] = 0.0

            self.evaluate(mode='weak', dw_mode='matrix', asm_obj=tangent_matrix)

            out = tangent_matrix

        return out

class Equation(Struct):

    @staticmethod
    def from_desc(name, desc, variables, regions, materials, integrals,
                  user=None):
        term_descs = parse_definition(desc)
        terms = Terms.from_desc(term_descs, regions, integrals)

        terms.setup()
        terms.assign_args(variables, materials, user)

        obj = Equation(name, terms)

        return obj

    def __init__(self, name, terms):
        Struct.__init__(self, name=name)

        if isinstance(terms, Term): # A single term.
            terms = Terms([terms])

        self.terms = terms

        self.terms.setup()

    def collect_materials(self):
        """
        Collect materials present in the terms of the equation.
        """
        materials = []
        for term in self.terms:
            materials.extend(term.get_materials(join=True))

        return materials

    def collect_variables(self):
        """
        Collect variables present in the terms of the equation.

        Ensures that corresponding primary variables of test/parameter
        variables are always in the list, even if they are not directly
        used in the terms.
        """
        variables = []
        for term in self.terms:
            var_names = term.get_variable_names()

            aux = term.get_args_by_name(var_names)
            for var in aux:
                variables.append(var)
                pvar = var.get_primary()
                if pvar is not None:
                    variables.append(pvar)

        return variables

    def collect_conn_info(self, conn_info):

        for term in self.terms:
            key = (self.name,) + term.get_conn_key()

            conn_info[key] = term.get_conn_info()

    def evaluate(self, mode='eval', dw_mode='vector', term_mode=None,
                 asm_obj=None):
        """
        Parameters
        ----------
        mode : one of 'eval', 'el_avg', 'qp', 'weak'
            The evaluation mode.
        """
        if mode == 'eval':
            val = 0.0
            for term in self.terms:
                aux, status = term.evaluate(mode=mode,
                                            term_mode=term_mode,
                                            standalone=False,
                                            ret_status=True)
                val += aux

            out = val

        elif mode in ('el_avg', 'el', 'qp'):

            vals = []
            for term in self.terms:
                val, iels, status = term.evaluate(mode=mode,
                                                  term_mode=term_mode,
                                                  standalone=False,
                                                  ret_status=True)
                vals.append(val)

            if len(vals) == 1:
                vals = vals[0]

            out = vals

        elif mode == 'weak':

            if dw_mode == 'vector':

                for term in self.terms:
                    val, iels, status = term.evaluate(mode=mode,
                                                      term_mode=term_mode,
                                                      standalone=False,
                                                      ret_status=True)
                    term.assemble_to(asm_obj, val, iels, mode=dw_mode)

            elif dw_mode == 'matrix':

                for term in self.terms:
                    svars = term.get_state_variables(unknown_only=True)

                    for svar in svars:
                        val, iels, status = term.evaluate(mode=mode,
                                                          term_mode=term_mode,
                                                          diff_var=svar.name,
                                                          standalone=False,
                                                          ret_status=True)
                        term.assemble_to(asm_obj, val, iels,
                                         mode=dw_mode, diff_var=svar)

            else:
                raise ValueError('unknown assembling mode! (%s)' % dw_mode)

            out = asm_obj

        else:
            raise ValueError('unknown evaluation mode! (%s)' % mode)

        return out

########NEW FILE########
__FILENAME__ = evaluate
from copy import copy

import numpy as nm

from sfepy.base.base import output, get_default, OneTypeList, Struct, basestr
from sfepy.discrete import Equations, Variables, Region, Integral, Integrals
from sfepy.discrete.common.fields import setup_extra_data

##
# 02.10.2007, c
class Evaluator( Struct ):
    pass

##
# 02.10.2007, c
class BasicEvaluator( Evaluator ):

    def __init__(self, problem, matrix_hook=None):
        Evaluator.__init__(self, problem=problem,
                           matrix_hook=matrix_hook)

    def new_ulf_iteration(self, nls, vec, it, err, err0):

        pb = self.problem

        vec = self.make_full_vec(vec)
        pb.equations.set_variables_from_state(vec)

        upd_vars = pb.conf.options.get('mesh_update_variables', None)
        for varname in upd_vars:
            try:
                state = pb.equations.variables[varname]
            except IndexError:
                msg = 'variable "%s" does not exist!' % varname
                raise KeyError( msg )

        nods = state.field.get_dofs_in_region(state.field.region, merge=True)
        coors = pb.domain.get_mesh_coors().copy()
        vs = state()
        coors[nods,:] = coors[nods,:] + vs.reshape(len(nods), state.n_components)
        if pb.ts.step == 1 and it == 0:
            state.field.save_mappings()

        state.field.clear_mappings()
        pb.set_mesh_coors(coors, update_fields=False, actual=True,
                          clear_all=False)

    def eval_residual( self, vec, is_full = False ):
        if not is_full:
            vec = self.make_full_vec( vec )

        vec_r = self.problem.equations.eval_residuals(vec)

        return vec_r

    def eval_tangent_matrix( self, vec, mtx = None, is_full = False ):
        if isinstance(vec, basestr) and vec == 'linear':
            return get_default(mtx, self.problem.mtx_a)

        if not is_full:
            vec = self.make_full_vec( vec )

        pb = self.problem
        if mtx is None:
            mtx = pb.mtx_a
        mtx = pb.equations.eval_tangent_matrices(vec, mtx)

        if self.matrix_hook is not None:
            mtx = self.matrix_hook(mtx, pb, call_mode='basic')

        return mtx

    def make_full_vec( self, vec ):
        return self.problem.equations.make_full_vec(vec)

##
# 04.10.2007, c
class LCBCEvaluator( BasicEvaluator ):

    ##
    # 04.10.2007, c
    def __init__(self, problem, matrix_hook=None):
        BasicEvaluator.__init__(self, problem, matrix_hook=matrix_hook)
        self.op_lcbc = problem.equations.get_lcbc_operator()

    ##
    # 04.10.2007, c
    def eval_residual( self, vec, is_full = False ):
        if not is_full:
            vec = self.make_full_vec( vec )
        vec_r = BasicEvaluator.eval_residual( self, vec, is_full = True )
        vec_rr = self.op_lcbc.T * vec_r
        return vec_rr

    ##
    # 04.10.2007, c
    def eval_tangent_matrix( self, vec, mtx = None, is_full = False ):
        if isinstance(vec, basestr) and vec == 'linear':
            return get_default(mtx, self.problem.mtx_a)

        if not is_full:
            vec = self.make_full_vec( vec )
        mtx = BasicEvaluator.eval_tangent_matrix( self, vec, mtx = mtx,
                                                  is_full = True )
        mtx_r = self.op_lcbc.T * mtx * self.op_lcbc
        mtx_r = mtx_r.tocsr()
        mtx_r.sort_indices()
##         import pylab
##         from sfepy.base.plotutils import spy
##         spy( mtx_r )
##         pylab.show()
##         print mtx_r.__repr__()

        if self.matrix_hook is not None:
            mtx_r = self.matrix_hook(mtx_r, self.problem, call_mode='lcbc')

        return mtx_r

def create_evaluable(expression, fields, materials, variables, integrals,
                     regions=None,
                     ebcs=None, epbcs=None, lcbcs=None, ts=None, functions=None,
                     auto_init=False, mode='eval', extra_args=None,
                     verbose=True, kwargs=None):
    """
    Create evaluable object (equations and corresponding variables)
    from the `expression` string.

    Parameters
    ----------
    expression : str
        The expression to evaluate.
    fields : dict
        The dictionary of fields used in `variables`.
    materials : Materials instance
        The materials used in the expression.
    variables : Variables instance
        The variables used in the expression.
    integrals : Integrals instance
        The integrals to be used.
    regions : Region instance or list of Region instances
        The region(s) to be used. If not given, the regions defined
        within the fields domain are used.
    ebcs : Conditions instance, optional
        The essential (Dirichlet) boundary conditions for 'weak'
        mode.
    epbcs : Conditions instance, optional
        The periodic boundary conditions for 'weak'
        mode.
    lcbcs : Conditions instance, optional
        The linear combination boundary conditions for 'weak'
        mode.
    ts : TimeStepper instance, optional
        The time stepper.
    functions : Functions instance, optional
        The user functions for boundary conditions, materials
        etc.
    auto_init : bool
        Set values of all variables to all zeros.
    mode : one of 'eval', 'el_avg', 'qp', 'weak'
        The evaluation mode - 'weak' means the finite element
        assembling, 'qp' requests the values in quadrature points,
        'el_avg' element averages and 'eval' means integration over
        each term region.
    extra_args : dict, optional
        Extra arguments to be passed to terms in the expression.
    verbose : bool
        If False, reduce verbosity.
    kwargs : dict, optional
        The variables (dictionary of (variable name) : (Variable
        instance)) to be used in the expression.

    Returns
    -------
    equation : Equation instance
        The equation that is ready to be evaluated.
    variables : Variables instance
        The variables used in the equation.
    """
    if kwargs is None:
        kwargs = {}

    if regions is not None:
        if isinstance(regions, Region):
            regions = [regions]

        regions = OneTypeList(Region, regions)

    else:
        regions = fields[fields.keys()[0]].domain.regions

    # Create temporary variables.
    aux_vars = Variables(variables)

    if extra_args is None:
        extra_args = kwargs

    else:
        extra_args = copy(extra_args)
        extra_args.update(kwargs)

    if ts is not None:
        extra_args.update({'ts' : ts})

    equations = Equations.from_conf({'tmp' : expression},
                                    aux_vars, regions, materials, integrals,
                                    user=extra_args, verbose=verbose)
    equations.collect_conn_info()

    # The true variables used in the expression.
    variables = equations.variables
    if auto_init:
        for var in variables:
            var.init_data(step=0)

    if mode == 'weak':
        equations.time_update(ts, ebcs, epbcs, lcbcs, functions,
                              verbose=verbose)

    else:
        setup_extra_data(equations.conn_info)

    return equations, variables


def eval_equations(equations, variables, names=None, preserve_caches=False,
                   mode='eval', dw_mode='vector', term_mode=None,
                   verbose=True):
    """
    Evaluate the equations.

    Parameters
    ----------
    equations : Equations instance
        The equations returned by :func:`create_evaluable()`.
    variables : Variables instance
        The variables returned by :func:`create_evaluable()`.
    names : str or sequence of str, optional
        Evaluate only equations of the given name(s).
    preserve_caches : bool
        If True, do not invalidate evaluate caches of variables.
    mode : one of 'eval', 'el_avg', 'qp', 'weak'
        The evaluation mode - 'weak' means the finite element
        assembling, 'qp' requests the values in quadrature points,
        'el_avg' element averages and 'eval' means integration over
        each term region.
    dw_mode : 'vector' or 'matrix'
        The assembling mode for 'weak' evaluation mode.
    term_mode : str
        The term call mode - some terms support different call modes
        and depending on the call mode different values are
        returned.
    verbose : bool
        If False, reduce verbosity.

    Returns
    -------
    out : dict or result
        The evaluation result. In 'weak' mode it is the vector or sparse
        matrix, depending on `dw_mode`. Otherwise, it is a dict of results with
        equation names as keys or a single result for a single equation.
    """
    asm_obj = None

    if mode == 'weak':
        if dw_mode == 'vector':
            asm_obj = equations.create_stripped_state_vector()

        else:
            asm_obj = equations.create_matrix_graph(verbose=verbose)

    if not preserve_caches:
        equations.invalidate_term_caches()

    out = equations.evaluate(names=names, mode=mode, dw_mode=dw_mode,
                             term_mode=term_mode, asm_obj=asm_obj)

    if variables.has_lcbc and mode == 'weak':
        op_lcbc = variables.op_lcbc
        if dw_mode == 'vector':
            out = op_lcbc.T * out

        elif dw_mode == 'matrix':
            out = op_lcbc.T * out * op_lcbc
            out = out.tocsr()
            out.sort_indices()

    return out

def eval_in_els_and_qp(expression, ig, iels, coors,
                       fields, materials, variables,
                       functions=None, mode='eval', term_mode=None,
                       extra_args=None, verbose=True, kwargs=None):
    """
    Evaluate an expression in given elements and points.

    Parameters
    ----------
    expression : str
        The expression to evaluate.
    fields : dict
        The dictionary of fields used in `variables`.
    materials : Materials instance
        The materials used in the expression.
    variables : Variables instance
        The variables used in the expression.
    functions : Functions instance, optional
        The user functions for materials etc.
    mode : one of 'eval', 'el_avg', 'qp'
        The evaluation mode - 'qp' requests the values in quadrature points,
        'el_avg' element averages and 'eval' means integration over
        each term region.
    term_mode : str
        The term call mode - some terms support different call modes
        and depending on the call mode different values are
        returned.
    extra_args : dict, optional
        Extra arguments to be passed to terms in the expression.
    verbose : bool
        If False, reduce verbosity.
    kwargs : dict, optional
        The variables (dictionary of (variable name) : (Variable
        instance)) to be used in the expression.

    Returns
    -------
    out : array
        The result of the evaluation.
    """
    weights = nm.ones_like(coors[:, 0])
    integral = Integral('ie', coors=coors, weights=weights)

    domain = fields.values()[0].domain

    region = Region('Elements', 'given elements', domain, '')
    region.cells = iels + domain.mesh.el_offsets[ig]
    region.update_shape()
    domain.regions.append(region)

    for field in fields.itervalues():
        field.clear_mappings(clear_all=True)
        for ap in field.aps.itervalues():
            ap.clear_qp_base()

    aux = create_evaluable(expression, fields, materials,
                           variables.itervalues(), Integrals([integral]),
                           functions=functions,
                           mode=mode, extra_args=extra_args, verbose=verbose,
                           kwargs=kwargs)
    equations, variables = aux

    out = eval_equations(equations, variables,
                         preserve_caches=False,
                         mode=mode, term_mode=term_mode)
    domain.regions.pop()

    return out

def assemble_by_blocks(conf_equations, problem, ebcs=None, epbcs=None,
                       dw_mode='matrix'):
    """Instead of a global matrix, return its building blocks as defined in
    `conf_equations`. The name and row/column variables of each block have to
    be encoded in the equation's name, as in::

        conf_equations = {
          'A,v,u' : "dw_lin_elastic_iso.i1.Y2( inclusion.lame, v, u )",
        }

    Notes
    -----
    `ebcs`, `epbcs` must be either lists of BC names, or BC configuration
    dictionaries.
    """
    if isinstance( ebcs, list ) and isinstance( epbcs, list ):
        bc_mode = 0
    elif isinstance( ebcs, dict ) and isinstance( epbcs, dict ):
        bc_mode = 1
    else:
        raise TypeError('bad BC!')

    matrices = {}
    for key, mtx_term in conf_equations.iteritems():
        ks = key.split( ',' )
        mtx_name, var_names = ks[0], ks[1:]
        output( mtx_name, var_names )

        problem.set_equations({'eq': mtx_term})
        variables = problem.get_variables()
        indx = variables.get_indx

        if bc_mode == 0:
            problem.select_bcs( ebc_names = ebcs, epbc_names = epbcs )

        else:
            problem.time_update(ebcs=ebcs, epbcs=epbcs)

        ir = indx( var_names[0], stripped = True, allow_dual = True )
        ic = indx( var_names[1], stripped = True, allow_dual = True )

        problem.update_materials()
        mtx = problem.evaluate(mtx_term, auto_init=True,
                               mode='weak', dw_mode='matrix',
                               copy_materials=False)
        matrices[mtx_name] = mtx[ir,ic]

    return matrices

########NEW FILE########
__FILENAME__ = evaluate_variable
import numpy as nm

from sfepy.base.base import assert_
from sfepy.terms.extmods import terms

def eval_real(vec, conn, geo, mode, shape, bf=None):
    """
    Evaluate basic derived quantities of a real variable given its DOF
    vector, connectivity and reference mapping.
    """
    n_el, n_qp, dim, n_en, n_comp = shape
    dtype = nm.float64

    if mode == 'val':
        function = terms.dq_state_in_qp

        out = nm.empty((n_el, n_qp, n_comp, 1), dtype=dtype)
        if bf is not None:
            function(out, vec, bf, conn)
        else:
            function(out, vec, geo.bf, conn)

    elif mode == 'grad':
        function = terms.dq_grad

        out = nm.empty((n_el, n_qp, dim, n_comp), dtype=dtype)
        function(out, vec, geo, conn)

    elif mode == 'div':
        assert_(n_comp == dim)
        function = terms.dq_div_vector

        out = nm.empty((n_el, n_qp, 1, 1), dtype=dtype)
        function(out, vec, geo, conn)

    elif mode == 'cauchy_strain':
        assert_(n_comp == dim)
        function = terms.dq_cauchy_strain

        sym = (dim + 1) * dim / 2
        out = nm.empty((n_el, n_qp, sym, 1), dtype=dtype)
        function(out, vec, geo, conn)

    else:
        raise ValueError('unsupported variable evaluation mode! (%s)'
                         % mode)

    return out

def eval_complex(vec, conn, geo, mode, shape, bf=None):
    """
    Evaluate basic derived quantities of a complex variable given its DOF
    vector, connectivity and reference mapping.
    """
    n_el, n_qp, dim, n_en, n_comp = shape

    if mode == 'val':
        function = terms.dq_state_in_qp

        rout = nm.empty((n_el, n_qp, n_comp, 1), dtype=nm.float64)
        iout = nm.empty((n_el, n_qp, n_comp, 1), dtype=nm.float64)
        if bf is not None:
            function(rout, vec.real.copy(), bf, conn)
            function(iout, vec.imag.copy(), bf, conn)
        else:
            function(rout, vec.real.copy(), geo.bf, conn)
            function(iout, vec.imag.copy(), geo.bf, conn)
        out = rout + 1j * iout

    elif mode == 'grad':
        function = terms.dq_grad

        rout = nm.empty((n_el, n_qp, dim, n_comp), dtype=nm.float64)
        iout = nm.empty((n_el, n_qp, dim, n_comp), dtype=nm.float64)
        function(rout, vec.real.copy(), geo, conn)
        function(iout, vec.imag.copy(), geo, conn)
        out = rout + 1j * iout

    elif mode == 'div':
        assert_(n_comp == dim)
        function = terms.dq_div_vector

        rout = nm.empty((n_el, n_qp, 1, 1), dtype=nm.float64)
        iout = nm.empty((n_el, n_qp, 1, 1), dtype=nm.float64)
        function(rout, vec.real.copy(), geo, conn)
        function(iout, vec.imag.copy(), geo, conn)
        out = rout + 1j * iout

    elif mode == 'cauchy_strain':
        assert_(n_comp == dim)
        function = terms.dq_cauchy_strain

        sym = (dim + 1) * dim / 2
        rout = nm.empty((n_el, n_qp, sym, 1), dtype=nm.float64)
        iout = nm.empty((n_el, n_qp, sym, 1), dtype=nm.float64)
        function(rout, vec.real.copy(), geo, conn)
        function(iout, vec.imag.copy(), geo, conn)
        out = rout + 1j * iout

    else:
        raise ValueError('unsupported variable evaluation mode! (%s)'
                         % mode)

    return out

########NEW FILE########
__FILENAME__ = domain
"""
Computational domain, consisting of the mesh and regions.
"""
import time

import numpy as nm

from sfepy.base.base import output, Struct
from geometry_element import GeometryElement
from sfepy.discrete.common.domain import Domain
from sfepy.discrete.fem.refine import refine_2_3, refine_2_4, refine_3_4, refine_3_8
from sfepy.discrete.fem.fe_surface import FESurface
from sfepy.discrete.fem.mesh import make_inverse_connectivity
import fea

class FEDomain(Domain):
    """
    Domain is divided into groups, whose purpose is to have homogeneous
    data shapes.
    """

    def __init__(self, name, mesh, verbose=False, **kwargs):
        """Create a Domain.

        Parameters
        ----------
        name : str
            Object name.
        mesh : Mesh
            A mesh defining the domain.
        """
        Domain.__init__(self, name, mesh=mesh, verbose=verbose, **kwargs)

        self.geom_els = geom_els = {}
        for ig, desc in enumerate(mesh.descs):
            gel = GeometryElement(desc)
            # Create geometry elements of dimension - 1.
            gel.create_surface_facet()

            geom_els[desc] = gel

        self.geom_interps = interps = {}
        for gel in geom_els.itervalues():
            key = gel.get_interpolation_name()

            gel.interp = interps.setdefault(key, fea.Interpolant(key, gel))
            gel = gel.surface_facet
            if gel is not None:
                key = gel.get_interpolation_name()
                gel.interp = interps.setdefault(key, fea.Interpolant(key, gel))

        self.vertex_set_bcs = self.mesh.nodal_bcs

        self.mat_ids_to_i_gs = {}
        for ig, mat_id in enumerate(mesh.mat_ids):
            self.mat_ids_to_i_gs[mat_id[0]] = ig

        n_nod, dim = self.mesh.coors.shape
        self.shape = Struct(n_nod=n_nod, dim=dim, tdim=0,
                            n_el=0,
                            n_gr=len(self.mesh.conns))

        self.setup_groups()
        self.fix_element_orientation()
        self.reset_regions()
        self.clear_surface_groups()

        from sfepy.discrete.fem.geometry_element import create_geometry_elements
        from sfepy.discrete.fem.extmods.cmesh import CMesh
        self.cmesh = CMesh.from_mesh(mesh)
        gels = create_geometry_elements()
        self.cmesh.set_local_entities(gels)
        self.cmesh.setup_entities()

        self.shape.tdim = self.cmesh.tdim
        self.cell_offsets = self.mesh.el_offsets

    def setup_groups(self):
        self.groups = {}
        for ii in range(self.shape.n_gr):
            gel = self.geom_els[self.mesh.descs[ii]] # Shortcut.
            conn = self.mesh.conns[ii]
            vertices = nm.unique(conn)

            n_vertex = vertices.shape[0]
            n_el, n_ep = conn.shape
            n_edge = gel.n_edge
            n_edge_total = n_edge * n_el

            if gel.dim == 3:
                n_face = gel.n_face
                n_face_total = n_face * n_el
            else:
                n_face = n_face_total = 0

            shape = Struct(n_vertex=n_vertex, n_el=n_el, n_ep=n_ep,
                           n_edge=n_edge, n_edge_total=n_edge_total,
                           n_face=n_face, n_face_total=n_face_total,
                           dim=self.mesh.dims[ii])

            self.groups[ii] = Struct(ig=ii, vertices=vertices, conn=conn,
                                      gel=gel, shape=shape)
            self.shape.n_el += n_el

    def iter_groups(self, igs=None):
        if igs is None:
            for ig in xrange(self.shape.n_gr): # sorted by ig.
                yield self.groups[ig]
        else:
            for ig in igs:
                yield ig, self.groups[ig]

    def get_cell_offsets(self):
        offs = {}
        for ig in range(self.shape.n_gr):
            offs[ig] = self.cell_offsets[ig]

        return offs

    def get_mesh_coors(self, actual=False):
        """
        Return the coordinates of the underlying mesh vertices.
        """
        if actual and hasattr(self.mesh, 'coors_act'):
            return self.mesh.coors_act
        else:
            return self.mesh.coors

    def get_mesh_bounding_box(self):
        """
        Return the bounding box of the underlying mesh.

        Returns
        -------
        bbox : ndarray (2, dim)
            The bounding box with min. values in the first row and max. values
            in the second row.
        """
        return self.mesh.get_bounding_box()

    def get_diameter(self):
        """
        Return the diameter of the domain.

        Notes
        -----
        The diameter corresponds to the Friedrichs constant.
        """
        bbox = self.get_mesh_bounding_box()
        return (bbox[1,:] - bbox[0,:]).max()

    def fix_element_orientation(self):
        """
        Ensure element nodes ordering giving positive element volume.

        The groups with elements of lower dimension than the space dimension
        are skipped.
        """
        from extmods.cmesh import orient_elements

        coors = self.mesh.coors
        for ii, group in self.groups.iteritems():
            if group.shape.dim < self.shape.dim: continue

            ori, conn = group.gel.orientation, group.conn

            itry = 0
            while itry < 2:
                flag = -nm.ones(conn.shape[0], dtype=nm.int32)

                # Changes orientation if it is wrong according to swap*!
                # Changes are indicated by positive flag.
                orient_elements(flag, conn, coors,
                                ori.roots, ori.vecs,
                                ori.swap_from, ori.swap_to)

                if nm.alltrue(flag == 0):
                    if itry > 0: output('...corrected')
                    itry = -1
                    break

                output('warning: bad element orientation, trying to correct...')
                itry += 1

            if itry == 2 and flag[0] != -1:
                raise RuntimeError('elements cannot be oriented! (%d, %s)'
                                   % (ii, self.mesh.descs[ii]))
            elif flag[0] == -1:
                output('warning: element orienation not checked')

    def get_element_diameters(self, ig, cells, vg, mode, square=True):
        group = self.groups[ig]
        diameters = nm.empty((len(cells), 1, 1, 1), dtype=nm.float64)
        if vg is None:
            diameters.fill(1.0)
        else:
            vg.get_element_diameters(diameters, group.gel.edges,
                                     self.get_mesh_coors().copy(), group.conn,
                                     cells.astype(nm.int32), mode)
        if square:
            out = diameters.squeeze()
        else:
            out = nm.sqrt(diameters.squeeze())

        return out

    def get_evaluate_cache(self, cache=None, share_geometry=False):
        """
        Get the evaluate cache for :func:`Variable.evaluate_at()
        <sfepy.discrete.variables.Variable.evaluate_at()>`.

        Parameters
        ----------
        cache : Struct instance, optional
            Optionally, use the provided instance to store the cache data.
        share_geometry : bool
            Set to True to indicate that all the probes will work on the same
            domain. Certain data are then computed only for the first probe and
            cached.

        Returns
        -------
        cache : Struct instance
            The evaluate cache.
        """
        try:
            from scipy.spatial import cKDTree as KDTree
        except ImportError:
            from scipy.spatial import KDTree

        if cache is None:
            cache = Struct(name='evaluate_cache')

        tt = time.clock()
        if (cache.get('iconn', None) is None) or not share_geometry:
            mesh = self.mesh
            offsets, iconn = make_inverse_connectivity(mesh.conns, mesh.n_nod,
                                                       ret_offsets=True)
            ii = nm.where(offsets[1:] == offsets[:-1])[0]
            if len(ii):
                raise ValueError('some vertices not in any element! (%s)' % ii)

            cache.offsets = offsets
            cache.iconn = iconn
        output('iconn: %f s' % (time.clock()-tt))

        tt = time.clock()
        if (cache.get('kdtree', None) is None) or not share_geometry:
            cache.kdtree = KDTree(mesh.coors)
        output('kdtree: %f s' % (time.clock()-tt))

        return cache

    def clear_surface_groups(self):
        """
        Remove surface group data.
        """
        self.surface_groups = {}

    def create_surface_group(self, region):
        """
        Create a new surface group corresponding to `region` if it does
        not exist yet.

        Notes
        -----
        Surface groups define surface facet connectivity that is needed
        for :class:`sfepy.discrete.fem.mappings.SurfaceMapping`.
        """
        for ig in region.igs:
            groups = self.surface_groups.setdefault(ig, {})
            if region.name not in groups:
                group = self.groups[ig]
                gel_faces = group.gel.get_surface_entities()

                name = 'surface_group_%s_%d' % (region.name, ig)
                surface_group = FESurface(name, region, gel_faces,
                                          group.conn, ig)

                groups[region.name] = surface_group

    def refine(self):
        """
        Uniformly refine the domain mesh.

        Returns
        -------
        domain : FEDomain instance
            The new domain with the refined mesh.

        Notes
        -----
        Works only for meshes with single element type! Does not
        preserve node groups!
        """

        names = set()
        for group in self.groups.itervalues():
            names.add(group.gel.name)

        if len(names) != 1:
            msg = 'refine() works only for meshes with single element type!'
            raise NotImplementedError(msg)

        el_type = names.pop()
        if el_type == '2_3':
            mesh = refine_2_3(self.mesh, self.cmesh)

        elif el_type == '2_4':
            mesh = refine_2_4(self.mesh, self.cmesh)

        elif el_type == '3_4':
            mesh = refine_3_4(self.mesh, self.cmesh)

        elif el_type == '3_8':
            mesh = refine_3_8(self.mesh, self.cmesh)

        else:
            msg = 'unsupported element type! (%s)' % el_type
            raise NotImplementedError(msg)

        domain = FEDomain(self.name + '_r', mesh)

        return domain

########NEW FILE########
__FILENAME__ = facets
"""
Helper functions related to mesh facets and Lagrange FE approximation.

Line: ori - iter:

0 - iter0
1 - iter1

Triangle: ori - iter:

0 - iter21
1 - iter12
3 - iter02
4 - iter20
6 - iter10
7 - iter01

Possible couples:

1, 4, 7 <-> 0, 3, 6

Square: ori - iter:

 0 - iter10x01y
 7 - iter10y01x
11 - iter01y01x
30 - iter01x10y
33 - iter10x10y
52 - iter01y10x
56 - iter10y10x
63 - iter01x01y

Possible couples:

7, 33, 52, 63 <-> 0, 11, 30, 56
"""
import numpy as nm

_quad_ori_groups = {
    0 : 0,
    1 : 0,
    3 : 7,
    4 : 0,
    6 : 7,
    7 : 7,
    11 : 11,
    15 : 11,
    20 : 52,
    22 : 30,
    30 : 30,
    31 : 63,
    32 : 33,
    33 : 33,
    41 : 33,
    43 : 11,
    48 : 56,
    52 : 52,
    56 : 56,
    57 : 56,
    59 : 63,
    60 : 52,
    62 : 30,
    63 : 63,
}

def build_orientation_map(n_fp):
    """
    The keys are binary masks of the lexicographical ordering of facet
    vertices. A bit i set to one means `v[i] < v[i+1]`.

    The values are `[original_order, permutation]`, where `permutation` can be
    used to sort facet vertices lexicographically. Hence `permuted_facet =
    facet[permutation]`.
    """
    from sfepy.linalg import permutations

    indices = range(n_fp)

    cmps = [(i1, i2) for i2 in indices for i1 in indices[:i2]]
    powers = [2**ii for ii in range(len(cmps))]

    ori_map = {}
    for indx in permutations(indices):
        key = 0
        sign = 1
        for ip, power in enumerate(powers):
            i1, i2 = cmps[ip]
            less = (indx[i1] < indx[i2])
            key += power * less
            if not less:
                sign *= -1

        isort = nm.argsort(indx)
        ori_map[key] = [indx, isort]

    return ori_map, cmps, powers

def iter0(num):
    for ir in xrange(num - 1, -1, -1):
        yield ir

def iter1(num):
    for ir in xrange(num):
        yield ir

ori_line_to_iter = {
    0 : iter0,
    1 : iter1,
}

def make_line_matrix(order):
    if (order < 2):
        return nm.zeros((0, 0), dtype=nm.int32)

    oo = order - 1
    mtx = nm.arange(oo, dtype=nm.int32)

    return mtx

def iter01(num):
    for ir in xrange(num - 1, -1, -1):
        for ic in xrange(ir + 1):
            yield ir, ic

def iter10(num):
    for ir in xrange(num - 1, -1, -1):
        for ic in xrange(ir, -1, -1):
            yield ir, ic

def iter02(num):
    for ic in xrange(num):
        for ir in xrange(num - 1, ic - 1, -1):
            yield ir, ic

def iter20(num):
    for ic in xrange(num):
        for ir in xrange(ic, num):
            yield ir, ic

def iter12(num):
    for idiag in xrange(num):
        irs, ics = nm.diag_indices(num - idiag)
        for ii in xrange(irs.shape[0] - 1, -1, -1):
            yield irs[ii] + idiag, ics[ii]

def iter21(num):
    for idiag in xrange(num):
        irs, ics = nm.diag_indices(num - idiag)
        for ii in xrange(irs.shape[0]):
            yield irs[ii] + idiag, ics[ii]

ori_triangle_to_iter = {
    0 : iter21,
    1 : iter12,
    3 : iter02,
    4 : iter20,
    6 : iter10,
    7 : iter01,
}

def make_triangle_matrix(order):
    if (order < 3):
        return nm.zeros((0, 0), dtype=nm.int32)

    oo = order - 2
    mtx = nm.zeros((oo, oo), dtype=nm.int32)
    for ii, (ir, ic) in enumerate(iter01(oo)):
        mtx[ir, ic] = ii

    return mtx

def iter01x01y(num):
    for ir in xrange(num):
        for ic in xrange(num):
            yield ir, ic

def iter01y01x(num):
    for ir, ic in iter01x01y(num):
        yield ic, ir

def iter10x01y(num):
    for ir in xrange(num - 1, -1, -1):
        for ic in xrange(num):
            yield ir, ic

def iter10y01x(num):
    for ir, ic in iter10x01y(num):
        yield ic, ir

def iter01x10y(num):
    for ir in xrange(num):
        for ic in xrange(num - 1, -1, -1):
            yield ir, ic

def iter01y10x(num):
    for ir, ic in iter01x10y(num):
        yield ic, ir

def iter10x10y(num):
    for ir in xrange(num - 1, -1, -1):
        for ic in xrange(num - 1, -1, -1):
            yield ir, ic

def iter10y10x(num):
    for ir, ic in iter10x10y(num):
        yield ic, ir

ori_square_to_iter = {
    0 : iter10x01y,
    7 : iter10y01x,
    11 : iter01y01x,
    30 : iter01x10y,
    33 : iter10x10y,
    52 : iter01y10x,
    56 : iter10y10x,
    63 : iter01x01y,
}

def make_square_matrix(order):
    if (order < 2):
        return nm.zeros((0, 0), dtype=nm.int32)

    oo = order - 1
    mtx = nm.arange(oo * oo, dtype=nm.int32)
    mtx.shape = (oo, oo)

    return mtx

def get_facet_dof_permutations(n_fp, igs, order):
    """
    Prepare DOF permutation vector for each possible facet orientation.
    """
    from sfepy.base.base import dict_to_array

    if n_fp == 2:
        mtx = make_line_matrix(order)
        ori_map = ori_line_to_iter
        fo = order - 1

    elif n_fp == 3:
        mtx = make_triangle_matrix(order)
        ori_map = ori_triangle_to_iter
        fo = order - 2

    elif n_fp == 4:
        mtx = make_square_matrix(order)
        ori_map = {}
        for key, val in _quad_ori_groups.iteritems():
            ori_map[key] = ori_square_to_iter[val]
        fo = order - 1

    else:
        raise ValueError('unsupported number of facet points! (%d)' % n_fp)

    dof_perms = {}
    for ig in igs:
        dof_perms[ig] = {}
        for key, itfun in ori_map.iteritems():
            dof_perms[ig][key] = [mtx[ii] for ii in itfun(fo)]

        dof_perms[ig] = dict_to_array(dof_perms[ig])

    return dof_perms

if __name__ == '__main__':
    order = 5
    mtx = make_triangle_matrix(order)
    print mtx

    oo = order - 2
    print [mtx[ir, ic] for ir, ic in ori_triangle_to_iter[0](oo)]
    print [mtx[ir, ic] for ir, ic in ori_triangle_to_iter[1](oo)]
    print [mtx[ir, ic] for ir, ic in ori_triangle_to_iter[3](oo)]
    print [mtx[ir, ic] for ir, ic in ori_triangle_to_iter[4](oo)]
    print [mtx[ir, ic] for ir, ic in ori_triangle_to_iter[6](oo)]
    print [mtx[ir, ic] for ir, ic in ori_triangle_to_iter[7](oo)]

    order = 4
    mtx = make_square_matrix(order)
    print mtx

    oo = order - 1
    print [mtx[ir, ic] for ir, ic in ori_square_to_iter[0](oo)]
    print [mtx[ir, ic] for ir, ic in ori_square_to_iter[7](oo)]
    print [mtx[ir, ic] for ir, ic in ori_square_to_iter[11](oo)]
    print [mtx[ir, ic] for ir, ic in ori_square_to_iter[30](oo)]
    print [mtx[ir, ic] for ir, ic in ori_square_to_iter[33](oo)]
    print [mtx[ir, ic] for ir, ic in ori_square_to_iter[52](oo)]
    print [mtx[ir, ic] for ir, ic in ori_square_to_iter[56](oo)]
    print [mtx[ir, ic] for ir, ic in ori_square_to_iter[63](oo)]

########NEW FILE########
__FILENAME__ = fea
import numpy as nm

from sfepy.base.base import Struct, assert_
from sfepy.discrete.fem.mappings import VolumeMapping, SurfaceMapping
from poly_spaces import PolySpace
from fe_surface import FESurface

def set_mesh_coors(domain, fields, coors, update_fields=False, actual=False,
                   clear_all=True):
    if actual:
        domain.mesh.coors_act = coors.copy()
    else:
        domain.mesh.coors = coors.copy()

    if update_fields:
        for field in fields.itervalues():
            field.setup_coors(coors)
            field.clear_mappings(clear_all=clear_all)

def eval_nodal_coors(coors, mesh_coors, region, poly_space, geom_poly_space,
                     econn, ig, only_extra=True):
    """
    Compute coordinates of nodes corresponding to `poly_space`, given
    mesh coordinates and `geom_poly_space`.
    """
    if only_extra:
        iex = (poly_space.nts[:,0] > 0).nonzero()[0]
        if iex.shape[0] == 0: return

        qp_coors = poly_space.node_coors[iex, :]
        econn = econn[:, iex].copy()

    else:
        qp_coors = poly_space.node_coors

    ##
    # Evaluate geometry interpolation base functions in (extra) nodes.
    bf = geom_poly_space.eval_base(qp_coors)
    bf = bf[:,0,:].copy()

    ##
    # Evaluate extra coordinates with 'bf'.
    group = region.domain.groups[ig]
    cells = region.get_cells(ig)

    ecoors = nm.dot(bf, mesh_coors[group.conn[cells]])
    coors[econn] = nm.swapaxes(ecoors, 0, 1)


##
# 04.08.2005, c
def _interp_to_faces( vertex_vals, bfs, faces ):
    dim = vertex_vals.shape[1]
    n_face = faces.shape[0]
    n_qp = bfs.shape[0]
    
    faces_vals = nm.zeros( (n_face, n_qp, dim), nm.float64 )
    for ii, face in enumerate( faces ):
        vals = vertex_vals[face,:dim]
        faces_vals[ii,:,:] = nm.dot( bfs[:,0,:], vals )

    return( faces_vals )

class Interpolant( Struct ):
    """A simple wrapper around PolySpace."""

    def __init__(self, name, gel, space='H1', base='lagrange',
                 approx_order=1, force_bubble=False):
        self.name = name
        self.gel = gel

        self.poly_spaces = poly_spaces = {}
        poly_spaces['v'] = PolySpace.any_from_args(name, gel, approx_order,
                                                   base=base,
                                                   force_bubble=force_bubble)
        gel = gel.surface_facet
        if gel is not None:
            ps = PolySpace.any_from_args(name, gel, approx_order,
                                         base=base,
                                         force_bubble=False)
            skey = 's%d' % ps.n_nod
            poly_spaces[skey] = ps

    def describe_nodes( self ):
        ps = self.poly_spaces['v']
        node_desc = ps.describe_nodes()

        return node_desc

    ##
    # 16.11.2007, c
    def get_n_nodes( self ):
        nn = {}
        for key, ps in self.poly_spaces.iteritems():
            nn[key] = ps.nodes.shape[0]
        return nn

    def get_geom_poly_space(self, key):
        if key == 'v':
            ps = self.gel.interp.poly_spaces['v']

        elif key[0] == 's':
            n_v = self.gel.surface_facet.n_vertex
            ps = self.gel.interp.poly_spaces['s%d' % n_v]

        else:
            raise ValueError('bad polynomial space key! (%s)' % key)

        return ps

class SurfaceInterpolant(Interpolant):
    """
    Like Interpolant, but for use with SurfaceField and
    SurfaceApproximation.
    """

    def __init__(self, name, gel, space='H1', base='lagrange',
                 approx_order=1, force_bubble=False):
        Interpolant.__init__(self, name, gel, space=space, base=base,
                             approx_order=approx_order,
                             force_bubble=force_bubble)

        # Make alias 'v' <-> 's#'.
        ps = self.poly_spaces['v']
        self.poly_spaces['s%d' % ps.n_nod] = ps

    def get_geom_poly_space(self, key):
        assert_(key[0] == 's')

        ps = self.gel.interp.poly_spaces['v']

        return ps

##
# 18.07.2006, c
class Approximation( Struct ):
    ##
    # 18.07.2006, c
    # 10.10.2006
    # 11.07.2007
    # 17.07.2007
    def __init__(self, name, interp, region, ig, is_surface=False):
        """interp, region are borrowed."""

        self.name = name
        self.interp = interp
        self.region = region
        self.ig = ig
        self.is_surface = is_surface
        self.surface_data = {}
        self.edge_data = {}
        self.point_data = {}
        self.n_ep = self.interp.get_n_nodes()
        self.ori = None

        self.clear_qp_base()

    def eval_extra_coor(self, coors, mesh_coors):
        """
        Compute coordinates of extra nodes.
        """
        gps = self.interp.gel.interp.poly_spaces['v']
        ps = self.interp.poly_spaces['v']

        eval_nodal_coors(coors, mesh_coors, self.region, ps, gps, self.econn, self.ig)

    ##
    # c: 05.09.2006, r: 09.05.2008
    def setup_surface_data( self, region ):
        """nodes[leconn] == econn"""
        """nodes are sorted by node number -> same order as region.vertices"""
        sd = FESurface('surface_data_%s' % region.name, region,
                       self.efaces, self.econn, self.ig)
        self.surface_data[region.name] = sd
        return sd

    ##
    # 11.07.2007, c
    def setup_point_data( self, field, region ):
        conn = field.get_dofs_in_region(region, merge=True, igs=region.igs)
##         conn = [nods]\
##                + [nm.empty( (0,), dtype = nm.int32 )]\
##                * (len( region.igs ) - 1)

        conn.shape += (1,)
        self.point_data[region.name] = conn

    def get_connectivity(self, region, integration, is_trace=False):
        """
        Return the DOF connectivity for the given geometry type.

        Parameters
        ----------
        region : Region instance
            The region, used to index surface and volume connectivities.
        integration : one of ('volume', 'plate', 'surface', 'surface_extra')
            The term integration type.
        """
        if integration == 'surface':
            sd = self.surface_data[region.name]
            conn = sd.get_connectivity(self.is_surface, is_trace=is_trace)

        elif integration in ('volume', 'plate', 'surface_extra'):
            if region.name == self.region.name:
                conn = self.econn

            else:
                aux = integration in ('volume', 'plate')
                cells = region.get_cells(self.ig, true_cells_only=aux)
                conn = nm.take(self.econn, cells.astype(nm.int32), axis=0)

        else:
            raise ValueError('unsupported term integration! (%s)' % integration)

        return conn

    def get_poly_space(self, key, from_geometry=False):
        """
        Get the polynomial space.

        Parameters
        ----------
        key : 'v' or 's?'
            The key denoting volume or surface.
        from_geometry : bool
            If True, return the polynomial space for affine geometrical
            interpolation.

        Returns
        -------
        ps : PolySpace instance
            The polynomial space.
        """
        if from_geometry:
            ps = self.interp.get_geom_poly_space(key)

        else:
            ps = self.interp.poly_spaces[key]

        return ps

    def clear_qp_base(self):
        """
        Remove cached quadrature points and base functions.
        """
        self.qp_coors = {}
        self.bf = {}

    def get_qp(self, key, integral):
        """
        Get quadrature points and weights corresponding to the given key
        and integral. The key is 'v' or 's#', where # is the number of
        face vertices.
        """
        qpkey = (integral.name, key)

        if not self.qp_coors.has_key(qpkey):
            interp = self.interp
            if (key[0] == 's'):
                dim = interp.gel.dim - 1
                n_fp = interp.gel.surface_facet.n_vertex
                geometry = '%d_%d' % (dim, n_fp)

            else:
                geometry = interp.gel.name

            vals, weights = integral.get_qp(geometry)
            self.qp_coors[qpkey] = Struct(vals=vals, weights=weights)

        return self.qp_coors[qpkey]

    def get_base(self, key, derivative, integral, iels=None,
                 from_geometry=False, base_only=True):
        qp = self.get_qp(key, integral)

        ps = self.get_poly_space(key, from_geometry=from_geometry)

        _key = key if not from_geometry else 'g' + key
        bf_key = (integral.name, _key, derivative)

        if not self.bf.has_key(bf_key):
            if (iels is not None) and (self.ori is not None):
                ori = self.ori[iels]

            else:
                ori = self.ori

            self.bf[bf_key] = ps.eval_base(qp.vals, diff=derivative, ori=ori)

        if base_only:
            return self.bf[bf_key]
        else:
            return self.bf[bf_key], qp.weights

    def describe_geometry(self, field, gtype, region, integral,
                          return_mapping=False):
        """
        Compute jacobians, element volumes and base function derivatives
        for Volume-type geometries (volume mappings), and jacobians,
        normals and base function derivatives for Surface-type
        geometries (surface mappings).

        Notes
        -----
        - volume mappings can be defined on a part of an element group,
          although the field has to be defined always on the whole group.
        - surface mappings are defined on the surface region
        - surface mappings require field order to be > 0
        """
        domain = field.domain
        group = domain.groups[self.ig]
        coors = domain.get_mesh_coors(actual=True)

        if gtype == 'volume':
            qp = self.get_qp('v', integral)

            iels = region.get_cells(self.ig)

            geo_ps = self.interp.get_geom_poly_space('v')
            ps = self.interp.poly_spaces['v']
            bf = self.get_base('v', 0, integral, iels=iels)

            conn = nm.take(group.conn, iels.astype(nm.int32), axis=0)
            mapping = VolumeMapping(coors, conn, poly_space=geo_ps)
            vg = mapping.get_mapping(qp.vals, qp.weights, poly_space=ps,
                                     ori=self.ori)

            out = vg

        elif gtype == 'plate':
            import sfepy.mechanics.membranes as mm
            from sfepy.linalg import dot_sequences

            qp = self.get_qp('v', integral)
            iels = region.get_cells(self.ig)

            ps = self.interp.poly_spaces['v']
            bf = self.get_base('v', 0, integral, iels=iels)

            conn = nm.take(group.conn, nm.int32(iels), axis=0)
            ccoors = coors[conn]

            # Coordinate transformation matrix (transposed!).
            mtx_t = mm.create_transformation_matrix(ccoors)

            # Transform coordinates to the local coordinate system.
            coors_loc = dot_sequences((ccoors - ccoors[:, 0:1, :]), mtx_t)

            # Mapping from transformed elements to reference elements.
            mapping = mm.create_mapping(coors_loc, field.gel, 1)
            vg = mapping.get_mapping(qp.vals, qp.weights, poly_space=ps,
                                     ori=self.ori)
            vg.mtx_t = mtx_t
            out = vg

        elif (gtype == 'surface') or (gtype == 'surface_extra'):
            assert_(field.approx_order > 0)

            if self.ori is not None:
                msg = 'surface integrals do not work yet with the' \
                      ' hierarchical basis!'
                raise ValueError(msg)

            sd = domain.surface_groups[self.ig][region.name]
            esd = self.surface_data[region.name]

            qp = self.get_qp(sd.face_type, integral)

            geo_ps = self.interp.get_geom_poly_space(sd.face_type)
            ps = self.interp.poly_spaces[esd.face_type]
            bf = self.get_base(esd.face_type, 0, integral)

            conn = sd.get_connectivity()

            mapping = SurfaceMapping(coors, conn, poly_space=geo_ps)
            sg = mapping.get_mapping(qp.vals, qp.weights, poly_space=ps,
                                     mode=gtype)
            if gtype == 'surface_extra':
                sg.alloc_extra_data(self.n_ep['v'])

                self.create_bqp(region.name, integral)
                qp = self.qp_coors[(integral.name, esd.bkey)]

                v_geo_ps = self.interp.get_geom_poly_space('v')
                bf_bg = v_geo_ps.eval_base(qp.vals, diff=True)
                ebf_bg = self.get_base(esd.bkey, 1, integral)

                sg.evaluate_bfbgm(bf_bg, ebf_bg, coors, sd.fis, group.conn)

            out =  sg

        elif gtype == 'point':
            out = mapping = None

        else:
            raise ValueError('unknown geometry type: %s' % gtype)

        if out is not None:
            # Store the integral used.
            out.integral = integral
            out.qp = qp
            out.ps = ps
            # Update base.
            out.bf[:] = bf

        if return_mapping:
            out = (out, mapping)

        return out

    def _create_bqp(self, skey, bf_s, weights, integral_name):
        interp = self.interp
        gel = interp.gel
        bkey = 'b%s' % skey[1:]
        bqpkey = (integral_name, bkey)
        coors, faces = gel.coors, gel.get_surface_entities()

        vals = _interp_to_faces(coors, bf_s, faces)
        self.qp_coors[bqpkey] = Struct(name = 'BQP_%s' % bkey,
                                       vals = vals, weights = weights)
        interp.poly_spaces[bkey] = interp.poly_spaces['v']
        return bkey

    def create_bqp(self, region_name, integral):
        sd = self.surface_data[region_name]
        bqpkey = (integral.name, sd.bkey)
        if not bqpkey in self.qp_coors:
            bf_s = self.get_base(sd.face_type, 0, integral,
                                 from_geometry=True)
            qp = self.get_qp(sd.face_type, integral)

            bkey = self._create_bqp(sd.face_type, bf_s, qp.weights,
                                    integral.name)
            assert_(bkey == sd.bkey)

class DiscontinuousApproximation(Approximation):

    def eval_extra_coor(self, coors, mesh_coors):
        """
        Compute coordinates of extra nodes. For discontinuous
        approximations, all nodes are treated as extra.
        """
        gps = self.interp.gel.interp.poly_spaces['v']
        ps = self.interp.poly_spaces['v']

        eval_nodal_coors(coors, mesh_coors, self.region, ps, gps,
                         self.econn, self.ig, only_extra=False)

class SurfaceApproximation(Approximation):

    def __init__(self, name, interp, region, ig):
        Approximation.__init__(self, name, interp, region, ig, is_surface=True)

    def get_qp(self, key, integral):
        """
        Get quadrature points and weights corresponding to the given key
        and integral. The key is 's#', where # is the number of
        face vertices.
        """
        assert_(key[0] == 's')
        qpkey = (integral.name, key)

        if not self.qp_coors.has_key(qpkey):
            interp = self.interp
            geometry = interp.gel.name

            vals, weights = integral.get_qp(geometry)
            self.qp_coors[qpkey] = Struct(vals=vals, weights=weights)

        return self.qp_coors[qpkey]


########NEW FILE########
__FILENAME__ = fe_surface
import numpy as nm

from sfepy.base.base import get_default, Struct
from sfepy.discrete.fem.facets import build_orientation_map
from sfepy.discrete.fem.utils import prepare_remap

class FESurface(Struct):
    """Description of a surface of a finite element domain."""

    def __init__(self, name, region, efaces, volume_econn, ig):
        """nodes[leconn] == econn"""
        """nodes are sorted by node number -> same order as region.vertices"""
        self.name = get_default(name, 'surface_data_%s' % region.name)

        face_indices = region.get_facet_indices(ig)

        faces = efaces[face_indices[:,1]]
        if faces.size == 0:
            raise ValueError('region with group with no faces! (%s)'
                             % region.name)

        try:
            ee = volume_econn[face_indices[:,0]]

        except:
            raise ValueError('missing region face indices! (%s)'
                             % region.name)

        econn = nm.empty(faces.shape, dtype=nm.int32)
        for ir, face in enumerate( faces ):
            econn[ir] = ee[ir,face]

        nodes = nm.unique(econn)
        remap = prepare_remap(nodes, nodes.max() + 1)
        leconn = remap[econn].copy()

        n_fa, n_fp = face_indices.shape[0], faces.shape[1]
        face_type = 's%d' % n_fp

        # Store bkey in SurfaceData, so that base function can be
        # queried later.
        bkey = 'b%s' % face_type[1:]

        self.ig = ig
        self.econn = econn
        self.fis = nm.ascontiguousarray(face_indices.astype(nm.int32))
        self.n_fa, self.n_fp = n_fa, n_fp
        self.nodes = nodes
        self.leconn = leconn
        self.face_type = face_type
        self.bkey = bkey
        self.meconn = self.mleconn = None

        if self.n_fp <= 4:
            self.ori_map, _, _ = build_orientation_map(self.n_fp)

        else:
            self.ori_map = None

    def setup_mirror_connectivity(self, region):
        """
        Setup mirror surface connectivity required to integrate over a
        mirror region.

        1. Get orientation of the faces:
           a) for elements in group ig -> ooris (own)
           b) for elements in group mig -> moris (mirror)

        2. orientation -> permutation.
        """
        mregion, ig_map, ig_map_i = region.get_mirror_region()
        mig = ig_map_i[self.ig]

        oo = self.ori_map
        ori_map = nm.zeros((nm.max(oo.keys()) + 1, self.n_fp), dtype=nm.int32)
        ori_map[oo.keys()] = nm.array([ii[1] for ii in oo.values()])

        conn = region.domain.cmesh.get_conn_as_graph(region.dim, region.dim - 1)
        oris = region.domain.cmesh.facet_oris

        ofis = region.get_facet_indices(self.ig, offset=False)
        ooris = oris[conn.indptr[ofis[:, 0]] + ofis[:, 1]]
        mfis = mregion.get_facet_indices(mig, offset=False)
        moris = oris[conn.indptr[mfis[:, 0]] + mfis[:, 1]]

        omap = ori_map[ooris]
        mmap = ori_map[moris]

        n_el, n_ep = self.econn.shape
        ii = nm.repeat(nm.arange(n_el)[:, None], n_ep, 1)
        self.meconn = nm.empty_like(self.econn)
        self.meconn[ii, omap] = self.econn[ii, mmap]

        nodes = nm.unique(self.meconn)
        remap = prepare_remap(nodes, nodes.max() + 1)
        self.mleconn = remap[self.meconn].copy()

    def get_connectivity(self, local=False, is_trace=False):
        """
        Return the surface element connectivity.

        Parameters
        ----------
        local : bool
            If True, return local connectivity w.r.t. surface nodes,
            otherwise return global connectivity w.r.t. all mesh nodes.
        is_trace : bool
            If True, return mirror connectivity according to `local`.
        """
        if not is_trace:
            if local:
                return self.leconn

            else:
                return self.econn

        else:
            if local:
                return self.mleconn

            else:
                return self.meconn

########NEW FILE########
__FILENAME__ = fields_base
"""
Notes
-----

Important attributes of continuous (order > 0) :class:`Field` and
:class:`SurfaceField` instances:

- `vertex_remap` : `econn[:, :n_vertex] = vertex_remap[conn]`
- `vertex_remap_i` : `conn = vertex_remap_i[econn[:, :n_vertex]]`

where `conn` is the mesh vertex connectivity, `econn` is the
region-local field connectivity.
"""
import numpy as nm

from sfepy.base.base import output, get_default, assert_
from sfepy.base.base import Struct
import fea
from sfepy.discrete.common.fields import parse_shape, Field
from sfepy.discrete.fem.mesh import Mesh
from sfepy.discrete.fem.meshio import convert_complex_output
from sfepy.discrete.fem.utils import (extend_cell_data, prepare_remap,
                                      invert_remap, get_min_value)
from sfepy.discrete.fem.fe_surface import FESurface
from sfepy.discrete.integrals import Integral
from sfepy.discrete.fem.linearizer import (get_eval_dofs, get_eval_coors,
                                           create_output)

def get_eval_expression(expression, ig,
                        fields, materials, variables,
                        functions=None, mode='eval', term_mode=None,
                        extra_args=None, verbose=True, kwargs=None):
    """
    Get the function for evaluating an expression given a list of elements,
    and reference element coordinates.
    """
    from sfepy.discrete.evaluate import eval_in_els_and_qp

    def _eval(iels, coors):
        val = eval_in_els_and_qp(expression, ig, iels, coors,
                                 fields, materials, variables,
                                 functions=functions, mode=mode,
                                 term_mode=term_mode,
                                 extra_args=extra_args, verbose=verbose,
                                 kwargs=kwargs)
        return val[..., 0]

    return _eval

def create_expression_output(expression, name, primary_field_name,
                             fields, materials, variables,
                             functions=None, mode='eval', term_mode=None,
                             extra_args=None, verbose=True, kwargs=None,
                             min_level=0, max_level=1, eps=1e-4):
    """
    Create output mesh and data for the expression using the adaptive
    linearizer.

    Parameters
    ----------
    expression : str
        The expression to evaluate.
    name : str
        The name of the data.
    primary_field_name : str
        The name of field that defines the element groups and polynomial
        spaces.
    fields : dict
        The dictionary of fields used in `variables`.
    materials : Materials instance
        The materials used in the expression.
    variables : Variables instance
        The variables used in the expression.
    functions : Functions instance, optional
        The user functions for materials etc.
    mode : one of 'eval', 'el_avg', 'qp'
        The evaluation mode - 'qp' requests the values in quadrature points,
        'el_avg' element averages and 'eval' means integration over
        each term region.
    term_mode : str
        The term call mode - some terms support different call modes
        and depending on the call mode different values are
        returned.
    extra_args : dict, optional
        Extra arguments to be passed to terms in the expression.
    verbose : bool
        If False, reduce verbosity.
    kwargs : dict, optional
        The variables (dictionary of (variable name) : (Variable
        instance)) to be used in the expression.
    min_level : int
        The minimum required level of mesh refinement.
    max_level : int
        The maximum level of mesh refinement.
    eps : float
        The relative tolerance parameter of mesh adaptivity.

    Returns
    -------
    out : dict
        The output dictionary.
    """
    field = fields[primary_field_name]
    vertex_coors = field.coors[:field.n_vertex_dof, :]

    coors = []
    vdofs = []
    conns = []
    mat_ids = []
    levels = []
    offset = 0
    for ig, ap in field.aps.iteritems():
        ps = ap.interp.poly_spaces['v']
        gps = ap.interp.gel.interp.poly_spaces['v']
        group = field.domain.groups[ig]
        vertex_conn = ap.econn[:, :group.shape.n_ep]

        eval_dofs = get_eval_expression(expression, ig,
                                        fields, materials, variables,
                                        functions=functions,
                                        mode=mode, extra_args=extra_args,
                                        verbose=verbose, kwargs=kwargs)
        eval_coors = get_eval_coors(vertex_coors, vertex_conn, gps)

        (level, _coors, conn,
         _vdofs, _mat_ids) = create_output(eval_dofs, eval_coors,
                                           group.shape.n_el, ps,
                                           min_level=min_level,
                                           max_level=max_level, eps=eps)

        _mat_ids[:] = field.domain.mesh.mat_ids[ig][0]

        coors.append(_coors)
        vdofs.append(_vdofs)
        conns.append(conn + offset)
        mat_ids.append(_mat_ids)
        levels.append(level)

        offset += _coors.shape[0]

    coors = nm.concatenate(coors, axis=0)
    vdofs = nm.concatenate(vdofs, axis=0)
    mesh = Mesh.from_data('linearized_mesh', coors, None, conns, mat_ids,
                          field.domain.mesh.descs)

    out = {}
    out[name] = Struct(name='output_data', mode='vertex',
                       data=vdofs, var_name=name, dofs=None,
                       mesh=mesh, levels=levels)

    out = convert_complex_output(out)

    return out

class FEField(Field):
    """
    Base class for finite element fields.

    Notes
    -----
    - Region can span over several groups -> different Aproximation
      instances
    - interps and hence node_descs are per region (must have single
      geometry!)
    - no two interps can be in a same group -> no two aps (with
      different regions) can be in a same group -> aps can be uniquely
      indexed with ig

    Field shape information:

    - ``shape`` - the shape of the base functions in a point
    - ``n_components`` - the number of DOFs per FE node
    - ``val_shape`` - the shape of field value (the product of DOFs and
      base functions) in a point
    """

    def __init__(self, name, dtype, shape, region, approx_order=1):
        """
        Create a finite element field.

        Parameters
        ----------
        name : str
            The field name.
        dtype : numpy.dtype
            The field data type: float64 or complex128.
        shape : int/tuple/str
            The field shape: 1 or (1,) or 'scalar', space dimension (2, or (2,)
            or 3 or (3,)) or 'vector', or a tuple. The field shape determines
            the shape of the FE base functions and is related to the number of
            components of variables and to the DOF per node count, depending
            on the field kind.
        region : Region
            The region where the field is defined.
        approx_order : int or tuple
            The FE approximation order. The tuple form is (order, has_bubble),
            e.g. (1, True) means order 1 with a bubble function.

        Notes
        -----
        Assumes one cell type for the whole region!
        """
        shape = parse_shape(shape, region.domain.shape.dim)
        if not self._check_region(region):
            raise ValueError('unsuitable region for field %s! (%s)' %
                             (name, region.name))

        Struct.__init__(self, name=name, dtype=dtype, shape=shape,
                        region=region)
        self.domain = self.region.domain
        self.igs = self.region.igs

        self._set_approx_order(approx_order)
        self._setup_geometry()
        self._setup_kind()
        self._setup_shape()

        self._create_interpolant()
        self._setup_approximations()
        self._setup_global_base()
        self.setup_coors()
        self.clear_mappings(clear_all=True)

    def _set_approx_order(self, approx_order):
        """
        Set a uniform approximation order.
        """
        if isinstance(approx_order, tuple):
            self.approx_order = approx_order[0]
            self.force_bubble = approx_order[1]

        else:
            self.approx_order = approx_order
            self.force_bubble = False

    def _create_interpolant(self):
        name = '%s_%s_%s_%d%s' % (self.gel.name, self.space,
                                  self.poly_space_base, self.approx_order,
                                  'B' * self.force_bubble)
        self.interp = fea.Interpolant(name, self.gel, self.space,
                                      self.poly_space_base, self.approx_order,
                                      self.force_bubble)

    def _setup_approximations(self):
        self.aps = {}
        self.aps_by_name = {}
        for ig in self.igs:
            name = self.interp.name + '_%s_ig%d' % (self.region.name, ig)
            ap = fea.Approximation(name, self.interp, self.region, ig)
            self.aps[ig] = ap
            self.aps_by_name[ap.name] = ap

    def get_true_order(self):
        """
        Get the true approximation order depending on the reference
        element geometry.

        For example, for P1 (linear) approximation the true order is 1,
        while for Q1 (bilinear) approximation in 2D the true order is 2.
        """
        gel = self.gel
        if (gel.dim + 1) == gel.n_vertex:
            order = self.approx_order

        else:
            order = gel.dim * self.approx_order

        if self.force_bubble:
            bubble_order = gel.dim + 1
            order = max(order, bubble_order)

        return order

    def is_higher_order(self):
        """
        Return True, if the field's approximation order is greater than one.
        """
        return self.force_bubble or (self.approx_order > 1)

    def _setup_global_base(self):
        """
        Setup global DOF/base functions, their indices and connectivity of the
        field. Called methods implemented in subclasses.
        """
        self._setup_facet_orientations()

        self._init_econn()

        self.n_vertex_dof, self.vertex_remap = self._setup_vertex_dofs()
        self.vertex_remap_i = invert_remap(self.vertex_remap)

        aux = self._setup_edge_dofs()
        self.n_edge_dof, self.edge_dofs, self.edge_remap = aux

        aux = self._setup_face_dofs()
        self.n_face_dof, self.face_dofs, self.face_remap = aux

        aux = self._setup_bubble_dofs()
        self.n_bubble_dof, self.bubble_dofs, self.bubble_remaps = aux

        self.n_nod = self.n_vertex_dof + self.n_edge_dof \
                     + self.n_face_dof + self.n_bubble_dof

        self._setup_esurface()

    def _setup_esurface(self):
        """
        Setup extended surface entities (edges in 2D, faces in 3D),
        i.e. indices of surface entities into the extended connectivity.
        """
        node_desc = self.node_desc

        for ig, ap in self.aps.iteritems():
            gel = ap.interp.gel
            ap.efaces = gel.get_surface_entities().copy()

            nd = node_desc.edge
            if nd is not None:
                efs = []
                for eof in gel.get_edges_per_face():
                    efs.append(nm.concatenate([nd[ie] for ie in eof]))
                efs = nm.array(efs).squeeze()

                if efs.ndim < 2:
                    efs = efs[:,nm.newaxis]
                ap.efaces = nm.hstack((ap.efaces, efs))

            efs = node_desc.face
            if efs is not None:
                efs = nm.array(efs).squeeze()

                if efs.ndim < 2:
                    efs = efs[:,nm.newaxis]
                ap.efaces = nm.hstack((ap.efaces, efs))

    def setup_coors(self, coors=None):
        """
        Setup coordinates of field nodes.
        """
        mesh = self.domain.mesh
        self.coors = nm.empty((self.n_nod, mesh.dim), nm.float64)

        if coors is None:
            coors = mesh.coors

        # Mesh vertex nodes.
        if self.n_vertex_dof:
            indx = self.vertex_remap_i
            self.coors[:self.n_vertex_dof] = nm.take(coors,
                                                     indx.astype(nm.int32),
                                                     axis=0)

        for ig, ap in self.aps.iteritems():
            ap.eval_extra_coor(self.coors, coors)

    def get_vertices(self):
        """
        Return indices of vertices belonging to the field region.
        """
        return self.vertex_remap_i

    def _get_facet_dofs(self, get_facets, remap, dofs, ig):
        gfacets = get_facets(ig)
        facets = remap[gfacets]

        return dofs[facets[facets >= 0]].ravel()

    def get_data_shape(self, ig, integral,
                       integration='volume', region_name=None):
        """
        Get element data dimensions.

        Parameters
        ----------
        ig : int
            The element group index.
        integral : Integral instance
            The integral describing used numerical quadrature.
        integration : 'volume', 'plate', 'surface', 'surface_extra' or 'point'
            The term integration type.
        region_name : str
            The name of surface region, required when `shape_kind` is
            'surface'.

        Returns
        -------
        data_shape : 4 ints
            The `(n_el, n_qp, dim, n_en)` for volume shape kind,
            `(n_fa, n_qp, dim, n_fn)` for surface shape kind and
            `(n_nod, 0, 0, 1)` for point shape kind.

        Notes
        -----
        - `n_el`, `n_fa` = number of elements/facets
        - `n_qp` = number of quadrature points per element/facet
        - `dim` = spatial dimension
        - `n_en`, `n_fn` = number of element/facet nodes
        - `n_nod` = number of element nodes
        """
        ap = self.aps[ig]

        region = self.domain.regions[region_name]
        shape = region.shape[ig]
        dim = region.dim

        if integration in ('surface', 'surface_extra'):
            sd = ap.surface_data[region_name]

            # This works also for surface fields.
            key = sd.face_type
            weights = ap.get_qp(key, integral).weights
            n_qp = weights.shape[0]

            if integration == 'surface':
                data_shape = (sd.n_fa, n_qp, dim, ap.n_ep[key])

            else:
                data_shape = (sd.n_fa, n_qp, dim, ap.n_ep['v'])

        elif integration in ('volume', 'plate'):
            _, weights = integral.get_qp(self.gel.name)
            n_qp = weights.shape[0]

            data_shape = (shape.n_cell, n_qp, dim, ap.n_ep['v'])

        elif integration == 'point':
            dofs = self.get_dofs_in_region(region, merge=True)
            data_shape = (dofs.shape[0], 0, 0, 1)

        else:
            raise NotImplementedError('unsupported integration! (%s)'
                                      % integration)

        return data_shape

    def get_dofs_in_region_group(self, region, ig, merge=True):
        """
        Return indices of DOFs that belong to the given region and group.
        """
        node_desc = self.node_desc

        dofs = []

        vdofs = nm.empty((0,), dtype=nm.int32)
        if node_desc.vertex is not None:
            ii = region.get_vertices(ig)
            vdofs = self.vertex_remap[ii]
            vdofs = vdofs[vdofs >= 0]
        dofs.append(vdofs)

        edofs = nm.empty((0,), dtype=nm.int32)
        if node_desc.edge is not None:
            edofs = self._get_facet_dofs(region.get_edges,
                                         self.edge_remap,
                                         self.edge_dofs, ig)
        dofs.append(edofs)

        fdofs = nm.empty((0,), dtype=nm.int32)
        if node_desc.face is not None:
            fdofs = self._get_facet_dofs(region.get_faces,
                                         self.face_remap,
                                         self.face_dofs, ig)
        dofs.append(fdofs)

        bdofs = nm.empty((0,), dtype=nm.int32)
        if (node_desc.bubble is not None) and region.has_cells():
            ii = region.get_cells(ig)
            group_els = self.bubble_remaps[ig][ii]
            bdofs = self.bubble_dofs[ig][group_els[group_els >= 0]].ravel()
        dofs.append(bdofs)

        if merge:
            dofs = nm.concatenate(dofs)

        return dofs

    def extend_dofs(self, dofs, fill_value=None):
        """
        Extend DOFs to the whole domain using the `fill_value`, or the
        smallest value in `dofs` if `fill_value` is None.
        """
        if fill_value is None:
            if nm.isrealobj(dofs):
                fill_value = get_min_value(dofs)

            else:
                # Complex values - treat real and imaginary parts separately.
                fill_value = get_min_value(dofs.real)
                fill_value += 1j * get_min_value(dofs.imag)

        if self.approx_order != 0:
            indx = self.get_vertices()

            n_nod = self.domain.shape.n_nod
            new_dofs = nm.empty((n_nod, dofs.shape[1]), dtype=self.dtype)
            new_dofs.fill(fill_value)
            new_dofs[indx] = dofs[:indx.size]

        else:
            new_dofs = extend_cell_data(dofs, self.domain, self.region,
                                        val=fill_value)

        return new_dofs

    def remove_extra_dofs(self, dofs):
        """
        Remove DOFs defined in higher order nodes (order > 1).
        """
        if self.approx_order != 0:
            new_dofs = dofs[:self.n_vertex_dof]

        else:
            new_dofs = dofs

        return new_dofs

    def linearize(self, dofs, min_level=0, max_level=1, eps=1e-4):
        """
        Linearize the solution for post-processing.

        Parameters
        ----------
        dofs : array, shape (n_nod, n_component)
            The array of DOFs reshaped so that each column corresponds
            to one component.
        min_level : int
            The minimum required level of mesh refinement.
        max_level : int
            The maximum level of mesh refinement.
        eps : float
            The relative tolerance parameter of mesh adaptivity.

        Returns
        -------
        mesh : Mesh instance
            The adapted, nonconforming, mesh.
        vdofs : array
            The DOFs defined in vertices of `mesh`.
        levels : array of ints
            The refinement level used for each element group.
        """
        assert_(dofs.ndim == 2)

        n_nod, dpn = dofs.shape

        assert_(n_nod == self.n_nod)
        assert_(dpn == self.shape[0])

        vertex_coors = self.coors[:self.n_vertex_dof, :]

        coors = []
        vdofs = []
        conns = []
        mat_ids = []
        levels = []
        offset = 0
        for ig, ap in self.aps.iteritems():
            ps = ap.interp.poly_spaces['v']
            gps = ap.interp.gel.interp.poly_spaces['v']
            group = self.domain.groups[ig]
            vertex_conn = ap.econn[:, :group.shape.n_ep]

            eval_dofs = get_eval_dofs(dofs, ap.econn, ps, ori=ap.ori)
            eval_coors = get_eval_coors(vertex_coors, vertex_conn, gps)

            (level, _coors, conn,
             _vdofs, _mat_ids) = create_output(eval_dofs, eval_coors,
                                               group.shape.n_el, ps,
                                               min_level=min_level,
                                               max_level=max_level, eps=eps)

            _mat_ids[:] = self.domain.mesh.mat_ids[ig][0]

            coors.append(_coors)
            vdofs.append(_vdofs)
            conns.append(conn + offset)
            mat_ids.append(_mat_ids)
            levels.append(level)

            offset += _coors.shape[0]

        coors = nm.concatenate(coors, axis=0)
        vdofs = nm.concatenate(vdofs, axis=0)
        mesh = Mesh.from_data('linearized_mesh', coors, None, conns, mat_ids,
                              self.domain.mesh.descs)

        return mesh, vdofs, levels

    def get_output_approx_order(self):
        """
        Get the approximation order used in the output file.
        """
        return min(self.approx_order, 1)

    def create_output(self, dofs, var_name, dof_names=None,
                      key=None, extend=True, fill_value=None,
                      linearization=None):
        """
        Convert the DOFs corresponding to the field to a dictionary of
        output data usable by Mesh.write().

        Parameters
        ----------
        dofs : array, shape (n_nod, n_component)
            The array of DOFs reshaped so that each column corresponds
            to one component.
        var_name : str
            The variable name corresponding to `dofs`.
        dof_names : tuple of str
            The names of DOF components.
        key : str, optional
            The key to be used in the output dictionary instead of the
            variable name.
        extend : bool
            Extend the DOF values to cover the whole domain.
        fill_value : float or complex
           The value used to fill the missing DOF values if `extend` is True.
        linearization : Struct or None
            The linearization configuration for higher order approximations.

        Returns
        -------
        out : dict
            The output dictionary.
        """
        linearization = get_default(linearization, Struct(kind='strip'))

        out = {}
        if linearization.kind is None:
            out[key] = Struct(name='output_data', mode='full',
                              data=dofs, var_name=var_name,
                              dofs=dof_names, field_name=self.name)

        elif ((not self.is_higher_order())
            or (linearization.kind == 'strip')):
            if extend:
                ext = self.extend_dofs(dofs, fill_value)

            else:
                ext = self.remove_extra_dofs(dofs)

            if ext is not None:
                approx_order = self.get_output_approx_order()

                if approx_order != 0:
                    # Has vertex data.
                    out[key] = Struct(name='output_data', mode='vertex',
                                      data=ext, var_name=var_name,
                                      dofs=dof_names)

                else:
                    ext.shape = (ext.shape[0], 1, ext.shape[1], 1)
                    out[key] = Struct(name='output_data', mode='cell',
                                      data=ext, var_name=var_name,
                                      dofs=dof_names)

        else:
            mesh, vdofs, levels = self.linearize(dofs,
                                                 linearization.min_level,
                                                 linearization.max_level,
                                                 linearization.eps)
            out[key] = Struct(name='output_data', mode='vertex',
                              data=vdofs, var_name=var_name, dofs=dof_names,
                              mesh=mesh, levels=levels)

        out = convert_complex_output(out)

        return out

    def create_mesh(self, extra_nodes=True):
        """
        Create a mesh from the field region, optionally including the field
        extra nodes.
        """
        mesh = self.domain.mesh

        if self.approx_order != 0:
            conns, mat_ids, descs = [], [], []
            for ig, ap in self.aps.iteritems():
                group = self.domain.groups[ig]
                if extra_nodes:
                    conn = ap.econn
                else:
                    offset = group.shape.n_ep
                    conn = ap.econn[:,:offset]
                conns.append(conn)
                mat_ids.append(mesh.mat_ids[ig])
                descs.append(mesh.descs[ig])

            if extra_nodes:
                coors = self.coors

            else:
                coors = self.coors[:self.n_vertex_dof]

            mesh = Mesh.from_data(self.name, coors, None, conns,
                                  mat_ids, descs)

        return mesh

    def interp_to_qp(self, dofs):
        """
        Interpolate DOFs into quadrature points.

        The quadrature order is given by the field approximation order.

        Parameters
        ----------
        dofs : array
            The array of DOF values of shape `(n_nod, n_component)`.

        Returns
        -------
        data_qp : array
            The values interpolated into the quadrature points.
        integral : Integral
            The corresponding integral defining the quadrature points.
        """
        integral = Integral('i', order=self.approx_order)

        data_qp = []
        for ig, ap in self.aps.iteritems():
            bf = ap.get_base('v', False, integral)
            bf = bf[:,0,:].copy()

            vals = nm.dot(bf, dofs[ap.econn])
            vals = nm.swapaxes(vals, 0, 1)
            vals.shape = vals.shape + (1,)

            data_qp.append(vals)

        data_qp = nm.concatenate(data_qp, axis=0)

        return data_qp, integral

    def get_coor(self, nods=None):
        """
        Get coordinates of the field nodes.

        Parameters
        ----------
        nods : array, optional
           The indices of the required nodes. If not given, the
           coordinates of all the nodes are returned.
        """
        if nods is None:
            return self.coors
        else:
            return self.coors[nods]

    def create_mapping(self, ig, region, integral, integration):
        """
        Create a new reference mapping.
        """
        ap = self.aps[ig]

        out = ap.describe_geometry(self, integration, region, integral,
                                   return_mapping=True)
        return out

class VolumeField(FEField):
    """
    Finite element field base class over volume elements (element dimension
    equals space dimension).
    """

    def _check_region(self, region):
        """
        Check whether the `region` can be used for the
        field. Non-surface fields require the region to span whole
        element groups.

        Returns
        -------
        ok : bool
            True if the region is usable for the field.
        """
        ok = True
        domain = region.domain
        for ig in region.igs:
            if domain.groups[ig].gel.dim != domain.shape.tdim:
                output('cells with a bad topological dimension! (%d == %d)'
                       % (domain.groups[ig].gel.dim, domain.shape.tdim))
                ok = False
                break
            shape = domain.groups[ig].shape
            if region.shape[ig].n_vertex < shape.n_vertex:
                output('region does not span a whole element group!')
                ok = False
                break

        return ok

    def _setup_geometry(self):
        """
        Setup the field region geometry.
        """
        ig = self.region.domain.cmesh.cell_groups[self.region.cells[0]]
        self.gel = self.domain.groups[ig].gel

        self.is_surface = False

    def _create_interpolant(self):
        name = '%s_%s_%s_%d%s' % (self.gel.name, self.space,
                                  self.poly_space_base, self.approx_order,
                                  'B' * self.force_bubble)
        self.interp = fea.Interpolant(name, self.gel, self.space,
                                      self.poly_space_base, self.approx_order,
                                      self.force_bubble)

    def _setup_approximations(self):
        self.aps = {}
        self.aps_by_name = {}
        for ig in self.igs:
            name = self.interp.name + '_%s_ig%d' % (self.region.name, ig)
            ap = fea.Approximation(name, self.interp, self.region, ig)
            self.aps[ig] = ap
            self.aps_by_name[ap.name] = ap

    def _init_econn(self):
        """
        Initialize the extended DOF connectivity.
        """
        for ig, ap in self.aps.iteritems():
            n_ep = ap.n_ep['v']
            n_cell = self.region.get_n_cells(ig)
            ap.econn = nm.zeros((n_cell, n_ep), nm.int32)

    def _setup_vertex_dofs(self):
        """
        Setup vertex DOF connectivity.
        """
        if self.node_desc.vertex is None:
            return 0, None

        region = self.region

        vertices = region.get_vertices_of_cells()
        remap = prepare_remap(vertices, region.n_v_max)
        n_dof = vertices.shape[0]

        ##
        # Remap vertex node connectivity to field-local numbering.
        for ig, ap in self.aps.iteritems():
            group = self.domain.groups[ig]
            offset = group.shape.n_ep
            cells = region.get_cells(ig)
            ap.econn[:,:offset] = nm.take(remap,
                                          nm.take(group.conn,
                                                  cells.astype(nm.int32),
                                                  axis=0))

        return n_dof, remap

    def setup_extra_data(self, geometry, info, is_trace):
        dct = info.dc_type.type

        if geometry != None:
            geometry_flag = 'surface' in geometry
        else:
            geometry_flag = False

        if (dct == 'surface') or (geometry_flag):
            reg = info.get_region()
            self.domain.create_surface_group(reg)
            self._setup_surface_data(reg, is_trace)

        elif dct == 'edge':
            raise NotImplementedError('dof connectivity type %s' % dct)

        elif dct == 'point':
            self._setup_point_data(self, info.region)

        elif dct not in ('volume', 'scalar', 'plate'):
            raise ValueError('unknown dof connectivity type! (%s)' % dct)

    def _setup_surface_data(self, region, is_trace=False):
        for ig, ap in self.aps.iteritems():
            if ig not in region.igs: continue
            if region.name not in ap.surface_data:
                ap.setup_surface_data(region)

        for ig, ap in self.aps.iteritems():
            if region.name in ap.surface_data and is_trace:
                sd = ap.surface_data[region.name]
                sd.setup_mirror_connectivity(region)

    def _setup_point_data(self, field, region):
        # Point data only in the first group to avoid multiple
        # assembling of nodes on group boundaries.
        ap = self.aps[self.igs[0]]
        if region.name not in ap.point_data:
            ap.setup_point_data(field, region)

    def get_econn(self, conn_type, region, ig, is_trace=False,
                  integration=None):
        """
        Get extended connectivity of the given type in the given region.
        """
        ct = conn_type.type if isinstance(conn_type, Struct) else conn_type

        if ((ig not in self.igs) or (ig not in region.igs)
            or (ct == 'point' and (ig > self.igs[0]))):
            # Point data only in the first group to avoid multiple
            # assembling of nodes on group boundaries.
            return None

        ap = self.aps[ig]

        if ct in ('volume', 'plate'):
            if region.name == self.region.name:
                conn = ap.econn

            else:
                aux = integration in ('volume', 'plate')
                cells = region.get_cells(ig, true_cells_only=aux)
                conn = nm.take(ap.econn, cells.astype(nm.int32), axis=0)

        elif ct == 'surface':
            sd = ap.surface_data[region.name]
            conn = sd.get_connectivity(is_trace=is_trace)

        elif ct == 'edge':
            raise NotImplementedError('connectivity type %s' % ct)

        elif ct == 'point':
            conn = ap.point_data[region.name]

        else:
            raise ValueError('unknown connectivity type! (%s)' % ct)

        return conn

    def average_qp_to_vertices(self, data_qp, integral):
        """
        Average data given in quadrature points in region elements into
        region vertices.

        .. math::
           u_n = \sum_e (u_{e,avg} * volume_e) / \sum_e volume_e
               = \sum_e \int_{volume_e} u / \sum volume_e
        """
        region = self.region

        n_cells = region.get_n_cells()
        if n_cells != data_qp.shape[0]:
            msg = 'incomatible shape! (%d == %d)' % (n_cells,
                                                     data_qp.shape[0])
            raise ValueError(msg)

        n_vertex = self.n_vertex_dof
        nc = data_qp.shape[2]

        nod_vol = nm.zeros((n_vertex,), dtype=nm.float64)
        data_vertex = nm.zeros((n_vertex, nc), dtype=nm.float64)
        for ig, ap in self.aps.iteritems():
            vg = ap.describe_geometry(self, 'volume', ap.region, integral)

            volume = nm.squeeze(vg.volume)
            iels = ap.region.get_cells(ig)

            data_e = nm.zeros((volume.shape[0], 1, nc, 1), dtype=nm.float64)
            vg.integrate(data_e, data_qp[iels])

            ir = nm.arange(nc, dtype=nm.int32)

            conn = ap.econn[:, :self.gel.n_vertex]
            for ii, cc in enumerate(conn):
                # Assumes unique nodes in cc!
                ind2, ind1 = nm.meshgrid(ir, cc)
                data_vertex[ind1,ind2] += data_e[iels[ii],0,:,0]
                nod_vol[cc] += volume[ii]
        data_vertex /= nod_vol[:,nm.newaxis]

        return data_vertex

class SurfaceField(FEField):
    """
    Finite element field base class over surface (element dimension is one
    less than space dimension).
    """

    def _check_region(self, region):
        """
        Check whether the `region` can be used for the
        field.

        Returns
        -------
        ok : bool
            True if the region is usable for the field.
        """
        ok = True
        for ig in region.igs:
            n_cell = region.get_n_cells(ig, True)
            if n_cell == 0:
                ok = False
                break

        return ok

    def _setup_geometry(self):
        """
        Setup the field region geometry.
        """
        self.gel = self.domain.groups[self.region.igs[0]].gel.surface_facet
        if self.gel is None:
            raise ValueError('element group has no surface!')

        self.is_surface = True

    def _create_interpolant(self):
        name = '%s_%s_%s_%d%s' % (self.gel.name, self.space,
                                  self.poly_space_base, self.approx_order,
                                  'B' * self.force_bubble)
        self.interp = fea.SurfaceInterpolant(name, self.gel, self.space,
                                             self.poly_space_base,
                                             self.approx_order,
                                             self.force_bubble)

    def _setup_approximations(self):
        self.aps = {}
        self.aps_by_name = {}
        for ig in self.igs:
            name = self.interp.name + '_%s_ig%d' % (self.region.name, ig)
            ap = fea.SurfaceApproximation(name, self.interp, self.region, ig)
            self.aps[ig] = ap
            self.aps_by_name[ap.name] = ap

    def setup_extra_data(self, geometry, info, is_trace):
        dct = info.dc_type.type

        if dct != 'surface':
            msg = "dof connectivity type must be 'surface'! (%s)" % dct
            raise ValueError(msg)

        reg = info.get_region()

        for ig, ap in self.aps.iteritems():
            if ig not in reg.igs: continue

            if reg.name not in ap.surface_data:
                # Defined in setup_vertex_dofs()
                msg = 'no surface data of surface field! (%s)' % reg.name
                raise ValueError(msg)

        for ig, ap in self.aps.iteritems():
            if reg.name in ap.surface_data and is_trace:
                sd = ap.surface_data[reg.name]
                sd.setup_mirror_connectivity(reg)

    def _init_econn(self):
        """
        Initialize the extended DOF connectivity.
        """
        for ig, ap in self.aps.iteritems():
            n_ep = ap.n_ep['v']
            n_cell = self.region.get_n_cells(ig, True)
            ap.econn = nm.zeros((n_cell, n_ep), nm.int32)

    def _setup_vertex_dofs(self):
        """
        Setup vertex DOF connectivity.
        """
        if self.node_desc.vertex is None:
            return 0, None

        region = self.region

        remap = prepare_remap(region.vertices, region.n_v_max)
        n_dof = region.vertices.shape[0]

        ##
        # Remap vertex node connectivity to field-local numbering.
        for ig, ap in self.aps.iteritems():
            group = self.domain.groups[ig]
            faces = group.gel.get_surface_entities()
            aux = FESurface('aux', region, faces, group.conn, ig)
            ap.econn[:,:aux.n_fp] = aux.leconn
            ap.surface_data[region.name] = aux

        return n_dof, remap

    def _setup_bubble_dofs(self):
        """
        Setup bubble DOF connectivity.
        """
        return 0, None, None

    def get_econn(self, conn_type, region, ig, is_trace=False,
                  integration=None):
        """
        Get extended connectivity of the given type in the given region.
        """
        ct = conn_type.type if isinstance(conn_type, Struct) else conn_type

        if ct != 'surface':
            msg = 'connectivity type must be "surface"! (%s)' % ct
            raise ValueError(msg)

        ap = self.aps[ig]

        sd = ap.surface_data[region.name]
        conn = sd.get_connectivity(local=True, is_trace=is_trace)

        return conn

    def average_qp_to_vertices(self, data_qp, integral):
        """
        Average data given in quadrature points in region elements into
        region vertices.

        .. math::
           u_n = \sum_e (u_{e,avg} * area_e) / \sum_e area_e
               = \sum_e \int_{area_e} u / \sum area_e
        """
        region = self.region

        n_cells = region.get_n_cells(None, True)
        if n_cells != data_qp.shape[0]:
            msg = 'incomatible shape! (%d == %d)' % (n_cells,
                                                     data_qp.shape[0])
            raise ValueError(msg)

        n_vertex = len(region.vertices)
        nc = data_qp.shape[2]

        nod_vol = nm.zeros((n_vertex,), dtype=nm.float64)
        data_vertex = nm.zeros((n_vertex, nc), dtype=nm.float64)
        offset = 0
        for ig, ap in self.aps.iteritems():
            sg = ap.describe_geometry(self, 'surface', ap.region, integral)

            area = nm.squeeze(sg.volume)
            n_cells = region.get_n_cells(ig, True)
            iels = offset + nm.arange(n_cells, dtype=nm.int32)
            offset += n_cells

            data_e = nm.zeros((area.shape[0], 1, nc, 1), dtype=nm.float64)
            sg.integrate(data_e, data_qp[iels])

            ir = nm.arange(nc, dtype=nm.int32)

            sd = self.domain.surface_groups[ig][region.name]
            # Should be vertex connectivity!
            conn = sd.get_connectivity(local=True)
            for ii, cc in enumerate(conn):
                # Assumes unique nodes in cc!
                ind2, ind1 = nm.meshgrid(ir, cc)
                data_vertex[ind1,ind2] += data_e[iels[ii],0,:,0]
                nod_vol[cc] += area[ii]
        data_vertex /= nod_vol[:,nm.newaxis]

        return data_vertex

class H1Mixin(Struct):
    """
    Methods of fields specific to H1 space.
    """

    def _setup_shape(self):
        """
        Setup the field's shape-related attributes, see :class:`Field`.
        """
        self.n_components = nm.prod(self.shape)
        self.val_shape = self.shape

########NEW FILE########
__FILENAME__ = fields_hierarchic
import numpy as nm

from sfepy.base.base import assert_
from sfepy.discrete.fem.utils import prepare_remap, prepare_translate
from sfepy.discrete.common.dof_info import expand_nodes_to_dofs
from sfepy.discrete.fem.fields_base import VolumeField, H1Mixin

class H1HierarchicVolumeField(H1Mixin, VolumeField):
    family_name = 'volume_H1_lobatto'

    def _init_econn(self):
        """
        Initialize the extended DOF connectivity and facet orientation array.
        """
        VolumeField._init_econn(self)

        for ig, ap in self.aps.iteritems():
            ap.ori = nm.zeros_like(ap.econn)

    def _setup_facet_orientations(self):
        self.node_desc = self.interp.describe_nodes()

    def _setup_edge_dofs(self):
        """
        Setup edge DOF connectivity.
        """
        if self.node_desc.edge is None:
            return 0, None, None

        return self._setup_facet_dofs(1,
                                      self.node_desc.edge,
                                      self.region.get_edges,
                                      self.n_vertex_dof)

    def _setup_face_dofs(self):
        """
        Setup face DOF connectivity.
        """
        if self.node_desc.face is None:
            return 0, None, None

        return self._setup_facet_dofs(self.domain.shape.tdim - 1,
                                      self.node_desc.face,
                                      self.region.get_faces,
                                      self.n_vertex_dof + self.n_edge_dof)

    def _setup_facet_dofs(self, dim, facet_desc, get_facets, offset):
        """
        Helper function to setup facet DOF connectivity, works for both
        edges and faces.
        """
        facet_desc = nm.array(facet_desc)
        n_dof_per_facet = facet_desc.shape[1]

        cmesh = self.domain.cmesh

        facets = self.region.entities[dim]
        ii = nm.arange(facets.shape[0], dtype=nm.int32)
        all_dofs = offset + expand_nodes_to_dofs(ii, n_dof_per_facet)

        # Prepare global facet id remapping to field-local numbering.
        remap = prepare_remap(facets, cmesh.num[dim])

        cconn = self.region.domain.cmesh.get_conn(self.region.tdim, dim)
        offs = cconn.offsets

        n_f = self.gel.edges.shape[0] if dim == 1 else self.gel.faces.shape[0]
        n_fp = 2 if dim == 1 else self.gel.surface_facet.n_vertex

        oris = cmesh.get_orientations(dim)
        for ig, ap in self.aps.iteritems():
            gcells = self.region.get_cells(ig, offset=False)
            n_el = gcells.shape[0]

            indices = cconn.indices[offs[gcells[0]]:offs[gcells[-1]+1]]
            facets_of_cells = remap[indices]

            # Define global facet dof numbers.
            gdofs = offset + expand_nodes_to_dofs(facets_of_cells,
                                                  n_dof_per_facet)

            # Elements of facets.
            iel = nm.arange(n_el, dtype=nm.int32).repeat(n_f)
            ies = nm.tile(nm.arange(n_f, dtype=nm.int32), n_el)

            # DOF columns in econn for each facet (repeating same values for
            # each element.
            iep = facet_desc[ies]

            ap.econn[iel[:, None], iep] = gdofs

            ori = oris[offs[gcells[0]]:offs[gcells[-1]+1]]

            if (n_fp == 2) and (ap.interp.gel.name in ['2_4', '3_8']):
                tp_edges = ap.interp.gel.edges
                ecs = ap.interp.gel.coors[tp_edges]
                # True = positive, False = negative edge orientation w.r.t.
                # reference tensor product axes.
                tp_edge_ori = (nm.diff(ecs, axis=1).sum(axis=2) > 0).squeeze()
                aux = nm.tile(tp_edge_ori, n_el)
                ori = nm.where(aux, ori, 1 - ori)

            if n_fp == 2: # Edges.
                # ori == 1 means the basis has to be multiplied by -1.
                ps = ap.interp.poly_spaces['v']
                orders = ps.node_orders
                eori = nm.repeat(ori[:, None], n_dof_per_facet, 1)
                eoo = orders[iep] % 2 # Odd orders.
                ap.ori[iel[:, None], iep] = eori * eoo

            elif n_fp == 3: # Triangular faces.
                raise NotImplementedError

            else: # Quadrilateral faces.
                # ori encoding in 3 bits:
                # 0: axis swap, 1: axis 1 sign, 2: axis 2 sign
                # 0 = + or False, 1 = - or True
                # 63 -> 000 = 0
                #  0 -> 001 = 1
                # 30 -> 010 = 2
                # 33 -> 011 = 3
                # 11 -> 100 = 4
                #  7 -> 101 = 5
                # 52 -> 110 = 6
                # 56 -> 111 = 7
                # Special cases:
                # Both orders same and even -> 000
                # Both orders same and odd -> 0??
                # Bits 1, 2 are multiplied by (swapped) axial order % 2.
                new = nm.repeat(nm.arange(8, dtype=nm.int32), 3)
                translate = prepare_translate([31, 59, 63,
                                               0, 1, 4,
                                               22, 30, 62,
                                               32, 33, 41,
                                               11, 15, 43,
                                               3, 6, 7,
                                               20, 52, 60,
                                               48, 56, 57], new)
                ori = translate[ori]
                eori = nm.repeat(ori[:, None], n_dof_per_facet, 1)

                ps = ap.interp.poly_spaces['v']
                orders = ps.face_axes_nodes[iep - ps.face_indx[0]]
                eoo = orders % 2
                eoo0, eoo1 = eoo[..., 0], eoo[..., 1]

                i0 = nm.where(eori < 4)
                i1 = nm.where(eori >= 4)

                eori[i0] = nm.bitwise_and(eori[i0], 2*eoo0[i0] + 5)
                eori[i0] = nm.bitwise_and(eori[i0], eoo1[i0] + 6)

                eori[i1] = nm.bitwise_and(eori[i1], eoo0[i1] + 6)
                eori[i1] = nm.bitwise_and(eori[i1], 2*eoo1[i1] + 5)

                ap.ori[iel[:, None], iep] = eori

        n_dof = n_dof_per_facet * facets.shape[0]
        assert_(n_dof == nm.prod(all_dofs.shape))

        return n_dof, all_dofs, remap

    def _setup_bubble_dofs(self):
        """
        Setup bubble DOF connectivity.
        """
        if self.node_desc.bubble is None:
            return 0, None, None

        offset = self.n_vertex_dof + self.n_edge_dof + self.n_face_dof
        n_dof = 0
        n_dof_per_cell = self.node_desc.bubble.shape[0]
        all_dofs = {}
        remaps = {}
        for ig, ap in self.aps.iteritems():
            ii = self.region.get_cells(ig)
            n_cell = ii.shape[0]
            nd = n_dof_per_cell * n_cell

            group = self.domain.groups[ig]
            remaps[ig] = prepare_remap(ii, group.shape.n_el)

            aux = nm.arange(offset + n_dof, offset + n_dof + nd,
                            dtype=nm.int32)
            aux.shape = (n_cell, n_dof_per_cell)
            iep = self.node_desc.bubble[0]
            ap.econn[:,iep:] = aux
            all_dofs[ig] = aux

            n_dof += nd

        return n_dof, all_dofs, remaps

    def set_dofs(self, fun=0.0, region=None, dpn=None, warn=None):
        """
        Set the values of given DOFs using a function of space coordinates or
        value `fun`.
        """
        if region is None:
            region = self.region

        if dpn is None:
            dpn = self.n_components

        nods = []
        vals = []
        for ig in self.igs:
            if nm.isscalar(fun):
                # Hack - use only vertex DOFs.
                gnods = self.get_dofs_in_region_group(region, ig, merge=False)
                n_dof = dpn * sum([nn.shape[0] for nn in gnods])
                gvals = nm.zeros(n_dof, dtype=nm.dtype(type(fun)))
                gvals[:gnods[0].shape[0] * dpn] = fun

                nods.append(nm.concatenate(gnods))
                vals.append(gvals)

            elif callable(fun):
                # Hack - use only vertex DOFs.
                gnods = self.get_dofs_in_region_group(region, ig, merge=False)
                n_dof = dpn * sum([nn.shape[0] for nn in gnods])

                vv = fun(self.get_coor(gnods[0]))

                gvals = nm.zeros(n_dof, dtype=vv.dtype)
                gvals[:gnods[0].shape[0] * dpn] = vv

                nods.append(nm.concatenate(gnods))
                vals.append(gvals)

            else:
                raise NotImplementedError

        nods, indx = nm.unique(nm.concatenate(nods), return_index=True)
        ii = (nm.tile(dpn * indx, dpn)
              + nm.tile(nm.arange(dpn, dtype=nm.int32), indx.shape[0]))
        vals = nm.concatenate(vals)[ii]

        return nods, vals

    def evaluate_at(self, coors, source_vals, strategy='kdtree',
                    close_limit=0.1, cache=None, ret_cells=False,
                    ret_status=False, ret_ref_coors=False, verbose=True):
        """
        Evaluate source DOF values corresponding to the field in the given
        coordinates using the field interpolation.

        Parameters
        ----------
        coors : array
            The coordinates the source values should be interpolated into.
        source_vals : array
            The source DOF values corresponding to the field.
        strategy : str, optional
            The strategy for finding the elements that contain the
            coordinates. Only 'kdtree' is supported for the moment.
        close_limit : float, optional
            The maximum limit distance of a point from the closest
            element allowed for extrapolation.
        cache : Struct, optional
            To speed up a sequence of evaluations, the field mesh, the inverse
            connectivity of the field mesh and the KDTree instance can
            be cached as `cache.mesh`, `cache.offsets`, `cache.iconn` and
            `cache.kdtree`. Optionally, the cache can also contain the
            reference element coordinates as `cache.ref_coors`,
            `cache.cells` and `cache.status`, if the evaluation occurs
            in the same coordinates repeatedly. In that case the KDTree
            related data are ignored.
        ret_cells : bool, optional
            If True, return also the cell indices the coordinates are in.
        ret_status : bool, optional
            If True, return also the status for each point: 0 is
            success, 1 is extrapolation within `close_limit`, 2 is
            extrapolation outside `close_limit`, 3 is failure.
        ret_ref_coors : bool, optional
            If True, return also the found reference element coordinates.
        verbose : bool
            If False, reduce verbosity.

        Returns
        -------
        vals : array
            The interpolated values.
        cells : array
            The cell indices, if `ret_cells` or `ret_status` are True.
        status : array
            The status, if `ret_status` is True.
        """
        raise NotImplementedError

########NEW FILE########
__FILENAME__ = fields_nodal
"""
Notes
-----

Important attributes of continuous (order > 0) :class:`Field` and
:class:`SurfaceField` instances:

- `vertex_remap` : `econn[:, :n_vertex] = vertex_remap[conn]`
- `vertex_remap_i` : `conn = vertex_remap_i[econn[:, :n_vertex]]`

where `conn` is the mesh vertex connectivity, `econn` is the
region-local field connectivity.
"""
import time
import numpy as nm

from sfepy.base.base import output, assert_
import fea
from sfepy.discrete.fem.utils import prepare_remap
from sfepy.discrete.common.dof_info import expand_nodes_to_dofs
from sfepy.discrete.fem.global_interp import get_ref_coors
from sfepy.discrete.fem.facets import get_facet_dof_permutations
from sfepy.discrete.fem.fields_base import (FEField, VolumeField, SurfaceField,
                                            H1Mixin)
from sfepy.discrete.fem.extmods.bases import evaluate_in_rc

class H1NodalMixin(H1Mixin):

    def _setup_facet_orientations(self):
        order = self.approx_order
        self.node_desc = self.interp.describe_nodes()

        edge_nodes = self.node_desc.edge_nodes
        if edge_nodes is not None:
            n_fp = self.gel.edges.shape[1]
            self.edge_dof_perms = get_facet_dof_permutations(n_fp, self.igs,
                                                             order)

        face_nodes = self.node_desc.face_nodes
        if face_nodes is not None:
            n_fp = self.gel.faces.shape[1]
            self.face_dof_perms = get_facet_dof_permutations(n_fp, self.igs,
                                                             order)

    def _setup_edge_dofs(self):
        """
        Setup edge DOF connectivity.
        """
        if self.node_desc.edge is None:
            return 0, None, None

        return self._setup_facet_dofs(1, self.node_desc.edge,
                                      self.edge_dof_perms,
                                      self.n_vertex_dof)

    def _setup_face_dofs(self):
        """
        Setup face DOF connectivity.
        """
        if self.node_desc.face is None:
            return 0, None, None

        return self._setup_facet_dofs(self.domain.shape.tdim - 1,
                                      self.node_desc.face,
                                      self.face_dof_perms,
                                      self.n_vertex_dof + self.n_edge_dof)

    def _setup_facet_dofs(self, dim, facet_desc, facet_perms, offset):
        """
        Helper function to setup facet DOF connectivity, works for both
        edges and faces.
        """
        facet_desc = nm.array(facet_desc)
        n_dof_per_facet = facet_desc.shape[1]

        cmesh = self.domain.cmesh

        facets = self.region.entities[dim]
        ii = nm.arange(facets.shape[0], dtype=nm.int32)
        all_dofs = offset + expand_nodes_to_dofs(ii, n_dof_per_facet)

        # Prepare global facet id remapping to field-local numbering.
        remap = prepare_remap(facets, cmesh.num[dim])

        cconn = self.region.domain.cmesh.get_conn(self.region.tdim, dim)
        offs = cconn.offsets

        n_f = self.gel.edges.shape[0] if dim == 1 else self.gel.faces.shape[0]

        oris = cmesh.get_orientations(dim)
        for ig, ap in self.aps.iteritems():
            gcells = self.region.get_cells(ig, offset=False)
            n_el = gcells.shape[0]

            indices = cconn.indices[offs[gcells[0]]:offs[gcells[-1]+1]]
            facets_of_cells = remap[indices]

            ori = oris[offs[gcells[0]]:offs[gcells[-1]+1]]
            perms = facet_perms[ig][ori]

            # Define global facet dof numbers.
            gdofs = offset + expand_nodes_to_dofs(facets_of_cells,
                                                  n_dof_per_facet)

            # Elements of facets.
            iel = nm.arange(n_el, dtype=nm.int32).repeat(n_f)
            ies = nm.tile(nm.arange(n_f, dtype=nm.int32), n_el)

            # DOF columns in econn for each facet.
            iep = facet_desc[ies]

            iaux = nm.arange(gdofs.shape[0], dtype=nm.int32)
            ap.econn[iel[:, None], iep] = gdofs[iaux[:, None], perms]

        n_dof = n_dof_per_facet * facets.shape[0]
        assert_(n_dof == nm.prod(all_dofs.shape))

        return n_dof, all_dofs, remap

    def _setup_bubble_dofs(self):
        """
        Setup bubble DOF connectivity.
        """
        if self.node_desc.bubble is None:
            return 0, None, None

        offset = self.n_vertex_dof + self.n_edge_dof + self.n_face_dof
        n_dof = 0
        n_dof_per_cell = self.node_desc.bubble.shape[0]
        all_dofs = {}
        remaps = {}
        for ig, ap in self.aps.iteritems():
            ii = self.region.get_cells(ig)
            n_cell = ii.shape[0]
            nd = n_dof_per_cell * n_cell

            group = self.domain.groups[ig]
            remaps[ig] = prepare_remap(ii, group.shape.n_el)

            aux = nm.arange(offset + n_dof, offset + n_dof + nd,
                            dtype=nm.int32)
            aux.shape = (n_cell, n_dof_per_cell)
            iep = self.node_desc.bubble[0]
            ap.econn[:,iep:] = aux
            all_dofs[ig] = aux

            n_dof += nd

        return n_dof, all_dofs, remaps

    def set_dofs(self, fun=0.0, region=None, dpn=None, warn=None):
        """
        Set the values of DOFs in a given region using a function of space
        coordinates or value `fun`.
        """
        if region is None:
            region = self.region

        if dpn is None:
            dpn = self.n_components

        aux = self.get_dofs_in_region(region, clean=True, warn=warn)
        nods = nm.unique(nm.hstack(aux))

        if callable(fun):
            vals = fun(self.get_coor(nods))

        elif nm.isscalar(fun):
            vals = nm.repeat([fun], nods.shape[0] * dpn)

        elif isinstance(fun, nm.ndarray):
            assert_(len(fun) == dpn)
            vals = nm.repeat(fun, nods.shape[0])

        else:
            raise ValueError('unknown function/value type! (%s)' % type(fun))

        return nods, vals

    def evaluate_at(self, coors, source_vals, strategy='kdtree',
                    close_limit=0.1, cache=None, ret_cells=False,
                    ret_status=False, ret_ref_coors=False, verbose=True):
        """
        Evaluate source DOF values corresponding to the field in the given
        coordinates using the field interpolation.

        Parameters
        ----------
        coors : array
            The coordinates the source values should be interpolated into.
        source_vals : array
            The source DOF values corresponding to the field.
        strategy : str, optional
            The strategy for finding the elements that contain the
            coordinates. Only 'kdtree' is supported for the moment.
        close_limit : float, optional
            The maximum limit distance of a point from the closest
            element allowed for extrapolation.
        cache : Struct, optional
            To speed up a sequence of evaluations, the field mesh, the inverse
            connectivity of the field mesh and the KDTree instance can
            be cached as `cache.mesh`, `cache.offsets`, `cache.iconn` and
            `cache.kdtree`. Optionally, the cache can also contain the
            reference element coordinates as `cache.ref_coors`,
            `cache.cells` and `cache.status`, if the evaluation occurs
            in the same coordinates repeatedly. In that case the KDTree
            related data are ignored.
        ret_cells : bool, optional
            If True, return also the cell indices the coordinates are in.
        ret_status : bool, optional
            If True, return also the status for each point: 0 is
            success, 1 is extrapolation within `close_limit`, 2 is
            extrapolation outside `close_limit`, 3 is failure.
        ret_ref_coors : bool, optional
            If True, return also the found reference element coordinates.
        verbose : bool
            If False, reduce verbosity.

        Returns
        -------
        vals : array
            The interpolated values.
        cells : array
            The cell indices, if `ret_cells` or `ret_status` are True.
        status : array
            The status, if `ret_status` is True.
        """
        output('evaluating in %d points...' % coors.shape[0], verbose=verbose)

        ref_coors, cells, status = get_ref_coors(self, coors,
                                                 strategy=strategy,
                                                 close_limit=close_limit,
                                                 cache=cache,
                                                 verbose=verbose)

        tt = time.clock()
        vertex_coorss, nodess, orders, mtx_is = [], [], [], []
        conns = []
        for ap in self.aps.itervalues():
            ps = ap.interp.poly_spaces['v']

            vertex_coorss.append(ps.geometry.coors)
            nodess.append(ps.nodes)
            mtx_is.append(ps.get_mtx_i())

            orders.append(ps.order)
            conns.append(ap.econn)

        orders = nm.array(orders, dtype=nm.int32)

        # Interpolate to the reference coordinates.
        vals = nm.empty((coors.shape[0], source_vals.shape[1]),
                        dtype=source_vals.dtype)

        evaluate_in_rc(vals, ref_coors, cells, status, source_vals,
                       conns, vertex_coorss, nodess, orders, mtx_is,
                       1e-15)
        output('interpolation: %f s' % (time.clock()-tt),verbose=verbose)

        output('...done',verbose=verbose)

        if ret_ref_coors:
            return vals, ref_coors, cells, status

        elif ret_status:
            return vals, cells, status

        elif ret_cells:
            return vals, cells

        else:
            return vals

class H1NodalVolumeField(H1NodalMixin, VolumeField):
    family_name = 'volume_H1_lagrange'

    def interp_v_vals_to_n_vals(self, vec):
        """
        Interpolate a function defined by vertex DOF values using the FE
        geometry base (P1 or Q1) into the extra nodes, i.e. define the
        extra DOF values.
        """
        if not self.node_desc.has_extra_nodes():
            enod_vol_val = vec.copy()

        else:
            dim = vec.shape[1]
            enod_vol_val = nm.zeros((self.n_nod, dim), dtype=nm.float64)
            for ig, ap in self.aps.iteritems():
                group = self.domain.groups[ig]
                econn = ap.econn

                coors = ap.interp.poly_spaces['v'].node_coors

                ginterp = ap.interp.gel.interp
                bf = ginterp.poly_spaces['v'].eval_base(coors)
                bf = bf[:,0,:].copy()

                evec = nm.dot(bf, vec[group.conn])
                enod_vol_val[econn] = nm.swapaxes(evec, 0, 1)

        return enod_vol_val

class H1DiscontinuousField(H1NodalMixin, VolumeField):
    family_name = 'volume_H1_lagrange_discontinuous'

    def _setup_approximations(self):
        self.aps = {}
        self.aps_by_name = {}
        for ig in self.igs:
            name = self.interp.name + '_%s_ig%d' % (self.region.name, ig)
            ap = fea.DiscontinuousApproximation(name, self.interp,
                                                self.region, ig)
            self.aps[ig] = ap
            self.aps_by_name[ap.name] = ap

    def _setup_global_base( self ):
        """
        Setup global DOF/base function indices and connectivity of the field.
        """
        self._setup_facet_orientations()

        self._init_econn()

        n_dof = 0
        all_dofs = {}
        remaps = {}
        for ig, ap in self.aps.iteritems():
            ii = self.region.get_cells(ig)
            nd = nm.prod(ap.econn.shape)

            group = self.domain.groups[ig]
            remaps[ig] = prepare_remap(ii, group.shape.n_el)

            aux = nm.arange(n_dof, n_dof + nd, dtype=nm.int32)
            aux.shape = ap.econn.shape

            ap.econn[:] = aux
            all_dofs[ig] = aux

            n_dof += nd

        self.n_nod = n_dof

        self.n_bubble_dof = n_dof
        self.bubble_dofs = all_dofs
        self.bubble_remaps = remaps

        self.n_vertex_dof = self.n_edge_dof = self.n_face_dof = 0

        self._setup_esurface()

    def extend_dofs(self, dofs, fill_value=None):
        """
        Extend DOFs to the whole domain using the `fill_value`, or the
        smallest value in `dofs` if `fill_value` is None.
        """
        if self.approx_order != 0:
            dofs = self.average_to_vertices(dofs)

        new_dofs = FEField.extend_dofs(self, dofs)

        return new_dofs

    def remove_extra_dofs(self, dofs):
        """
        Remove DOFs defined in higher order nodes (order > 1).
        """
        if self.approx_order != 0:
            dofs = self.average_to_vertices(dofs)

        new_dofs = FEField.remove_extra_dofs(self, dofs)

        return new_dofs

    def average_to_vertices(self, dofs):
        """
        Average DOFs of the discontinuous field into the field region
        vertices.
        """
        data_qp, integral = self.interp_to_qp(dofs)
        vertex_dofs = self.average_qp_to_vertices(data_qp, integral)

        return vertex_dofs

class H1NodalSurfaceField(H1NodalMixin, SurfaceField):
    """
    A field defined on a surface region.
    """
    family_name = 'surface_H1_lagrange'

    def interp_v_vals_to_n_vals(self, vec):
        """
        Interpolate a function defined by vertex DOF values using the FE
        surface geometry base (P1 or Q1) into the extra nodes, i.e. define the
        extra DOF values.
        """
        if not self.node_desc.has_extra_nodes():
            enod_vol_val = vec.copy()

        else:
            msg = 'surface nodal fields do not support higher order nodes yet!'
            raise NotImplementedError(msg)

        return enod_vol_val

########NEW FILE########
__FILENAME__ = geometry_element
"""
GeometryElement describes the geometric entities of a finite element mesh.

Notes
-----
* geometry_data: surface facets are assumed to be of the same kind for
  each geometry element - wedges or pyramides are not supported.
* the orientation is a tuple:
  (root1, vertices of direction vectors, swap from, swap to, root2, ...)
"""
import numpy as nm

from sfepy.base.base import assert_, Struct

def _get_grid_1_2(n_nod):
    return nm.linspace(0.0, 1.0, n_nod)

def _get_grid_2_3(n_nod):
    from sfepy.mechanics.tensors import sym2dim

    n1d = sym2dim(n_nod)

    ii = nm.linspace(0.0, 1.0, n1d)

    coors = []
    for iy in xrange(n1d):
        for ix in xrange(n1d - iy):
            coors.append([ii[ix], ii[iy]])

    coors = nm.array(coors, dtype=nm.float64)

    return coors

def _get_grid_2_4(n_nod):
    n1d = int(nm.round(nm.sqrt(n_nod)))
    assert_((n1d**2) == n_nod)

    ii = nm.linspace(0.0, 1.0, n1d)

    ix, iy = nm.mgrid[:n1d, :n1d]
    coors = nm.c_[ii[ix].flat, ii[iy].flat]

    return nm.ascontiguousarray(coors)

def _get_grid_3_4(n_nod):
    sqrt = nm.sqrt
    pow = nm.power
    root = (-(-1.0/2.0 + sqrt(3)*1j/2)
            *pow(-3*n_nod + sqrt(9*pow(n_nod, 2) - 1.0/27.0) + 0j, 1.0/3.0)
            - 2 - 1/(3*(-1.0/2.0 + sqrt(3)*1j/2)
                     *pow(-3*n_nod + sqrt(9*pow(n_nod, 2) - 1.0/27.0)+0j,
                          1.0/3.0)))

    n1d = int(nm.round(root.real)) + 1
    assert_(((n1d + 2) * (n1d + 1) * (n1d + 0) / 6.) == n_nod)

    ii = nm.linspace(0.0, 1.0, n1d)

    coors = []
    for iz in xrange(n1d):
        for iy in xrange(n1d - iz):
            for ix in xrange(n1d - iy - iz):
                coors.append([ii[ix], ii[iy], ii[iz]])

    coors = nm.array(coors, dtype=nm.float64)

    return coors

def _get_grid_3_8(n_nod):
    n1d = int(nm.round(n_nod**(1.0 / 3.0)))
    assert_((n1d**3) == n_nod)

    ii = nm.linspace(0.0, 1.0, n1d)

    ix, iy, iz = nm.mgrid[:n1d, :n1d, :n1d]
    coors = nm.c_[ii[ix].flat, ii[iy].flat, ii[iz].flat]

    return nm.ascontiguousarray(coors)

geometry_data = {
    '1_2' : Struct(coors = [[0.0],
                            [1.0]],
                   conn = [0, 1],
                   faces = None,
                   edges = None,
                   volume = 1.0,
                   orientation = None,
                   get_grid = _get_grid_1_2,
                   surface_facet_name = None),

    '2_3' : Struct(coors = [[0.0, 0.0],
                            [1.0, 0.0],
                            [0.0, 1.0]],
                   conn = [0, 1, 2],
                   faces = None,
                   edges = [[0, 1],
                            [1, 2],
                            [2, 0]],
                   volume = 0.5,
                   orientation = (0, (1, 2), 1, 2),
                   get_grid = _get_grid_2_3,
                   surface_facet_name = '1_2'),

    '2_4' : Struct(coors = [[0.0, 0.0],
                            [1.0, 0.0],
                            [1.0, 1.0],
                            [0.0, 1.0]],
                   conn = [0, 1, 2, 3],
                   faces = None,
                   edges = [[0, 1],
                            [1, 2],
                            [2, 3],
                            [3, 0]],
                   volume = 1.0,
                   # Not finished...
                   orientation = (0, (1, 3), (0, 1), (3, 2)),
                   get_grid = _get_grid_2_4,
                   surface_facet_name = '1_2'),

    '3_4' : Struct(coors = [[0.0, 0.0, 0.0],
                            [1.0, 0.0, 0.0],
                            [0.0, 1.0, 0.0],
                            [0.0, 0.0, 1.0]],
                   conn = [0, 1, 2, 3],
                   faces = [[0, 2, 1],
                            [0, 3, 2],
                            [0, 1, 3],
                            [1, 2, 3]],
                   edges = [[0, 1],
                            [1, 2],
                            [2, 0],
                            [0, 3],
                            [1, 3],
                            [2, 3]],
                   volume = 1.0 / 6.0,
                   orientation = (0, (1, 2, 3), 0, 3),
                   get_grid = _get_grid_3_4,
                   surface_facet_name = '2_3'),

    '3_8' : Struct(coors = [[0.0, 0.0, 0.0],
                            [1.0, 0.0, 0.0],
                            [1.0, 1.0, 0.0],
                            [0.0, 1.0, 0.0],
                            [0.0, 0.0, 1.0],
                            [1.0, 0.0, 1.0],
                            [1.0, 1.0, 1.0],
                            [0.0, 1.0, 1.0]],
                   conn = [0, 1, 2, 3, 4, 5, 6, 7],
                   faces = [[0, 3, 2, 1],
                            [0, 4, 7, 3],
                            [0, 1, 5, 4],
                            [4, 5, 6, 7],
                            [1, 2, 6, 5],
                            [3, 7, 6, 2]],
                   edges = [[0, 1],
                            [1, 2],
                            [2, 3],
                            [3, 0],
                            [4, 5],
                            [5, 6],
                            [6, 7],
                            [7, 4],
                            [0, 4],
                            [1, 5],
                            [2, 6],
                            [3, 7]],
                   volume = 1.0,
                   # Not finished...
                   orientation = (0, (1, 3, 4), (0, 1, 2, 3), (4, 5, 6, 7) ),
                   get_grid = _get_grid_3_8,
                   surface_facet_name = '2_4'),
}

def setup_orientation(vecs_tuple):
    cycle = range(len(vecs_tuple) / 4)

    roots = nm.array([vecs_tuple[4*ii] for ii in cycle], dtype=nm.int32)
    vecs = nm.array([vecs_tuple[4*ii+1] for ii in cycle],
                    dtype=nm.int32, ndmin=2)
    swap_from = nm.array([vecs_tuple[4*ii+2] for ii in cycle],
                         dtype=nm.int32, ndmin=2)
    swap_to = nm.array([vecs_tuple[4*ii+3] for ii in cycle],
                       dtype=nm.int32, ndmin=2)

    return roots, vecs, swap_from, swap_to

def create_geometry_elements(names=None):
    """
    Utility function to create GeometryElement instances.

    Parameters
    ----------
    names : str, optional
        The names of the entity, one of the keys in geometry_data
        dictionary. If None, all keys of geometry_data are used.

    Returns
    -------
    gels : dict
        The dictionary of geometry elements with names as keys.
    """
    if names is None:
        names = geometry_data.keys()

    gels = {}
    for name in names:
        gel = GeometryElement(name)
        gels[name] = gel

    return gels

class GeometryElement(Struct):
    """
    The geometric entities of a finite element mesh.
    """

    def __init__(self, name):
        """
        Parameters
        ----------
        name : str
            The name of the entity, one of the keys in geometry_data
            dictionary.
        """
        self.name = name

        gd = geometry_data[name]

        self.coors = nm.array(gd.coors, dtype=nm.float64)

        self.conn = nm.array(gd.conn, dtype=nm.int32)

        self.n_vertex, self.dim = self.coors.shape
        self.is_simplex = self.n_vertex == (self.dim + 1)

        self.vertices = nm.arange(self.n_vertex, dtype=nm.int32)

        if gd.edges is not None:
            self.edges = nm.array(gd.edges, dtype=nm.int32)
            self.n_edge = self.edges.shape[0]
        else:
            self.edges = gd.edges
            self.n_edge = 0

        if gd.faces is not None:
            self.faces = nm.array(gd.faces, dtype=nm.int32)
            self.n_face = self.faces.shape[0]
        else:
            self.faces = gd.faces
            self.n_face = 0

        if gd.orientation is not None:
            aux = setup_orientation(gd.orientation)
            self.orientation = Struct(name='orientation',
                                      roots=aux[0], vecs=aux[1],
                                      swap_from=aux[2], swap_to=aux[3])
        else:
            self.orientation = None

        self.surface_facet_name = gd.surface_facet_name
        self.surface_facet = None

    def get_interpolation_name(self):
        """
        Get the name of corresponding linear interpolant.
        """
        if self.is_simplex:
            suffix = '_P1'
        else:
            suffix = '_Q1'
        return self.name + suffix

    def get_surface_entities(self):
        """
        Return self.vertices in 1D, self.edges in 2D and self.faces in 3D.
        """
        if self.dim == 1:
            return self.vertices
        elif self.dim == 2:
            return self.edges
        else:
            assert_(self.dim == 3)
            return self.faces

    def get_edges_per_face(self):
        """
        Return the indices into self.edges per face.
        """
        if self.dim == 3:
            # Assign edges to a face (in order).
            indx = {3: [[0, 1], [1, 2], [2, 0]],
                    4: [[0, 1], [1, 2], [2, 3], [3, 0]]}
            epf = []
            se = [set(edge) for edge in self.edges]
            iis = indx[self.surface_facet.n_vertex]
            for face in self.faces:
                aux = []

                for ii in iis:
                    edge = set(face[ii])
                    ie = se.index(edge)
                    aux.append(ie)

                epf.append(aux)

        else:
            epf = nm.arange(self.edges.shape[0])[:,nm.newaxis]

        return nm.array(epf, dtype=nm.int32)

    def get_conn_permutations(self):
        """
        Get all possible connectivity permutations corresponding to different
        spatial orientations of the geometry element.
        """
        if self.dim < 3:
            perms = [nm.roll(self.conn, -ii) for ii in range(self.n_vertex)]
            perms = nm.vstack(perms)

        else:
            _perms3d = {
                '3_4' : [[0, 1, 2, 3],
                         [1, 2, 0, 3],
                         [2, 0, 1, 3],
                         [1, 3, 2, 0],
                         [3, 0, 2, 1],
                         [3, 1, 0, 2],
                         [2, 1, 3, 0],
                         [0, 2, 3, 1],
                         [0, 3, 1, 2]],
                '3_8' : [[0, 1, 2, 3, 4, 5, 6, 7],
                         [1, 2, 3, 0, 5, 6, 7, 4],
                         [2, 3, 0, 1, 6, 7, 4, 5],
                         [3, 0, 1, 2, 7, 4, 5, 6],
                         [3, 2, 6, 7, 0, 1, 5, 4],
                         [7, 6, 5, 4, 3, 2, 1, 0],
                         [4, 5, 1, 0, 7, 6, 2, 3],
                         [1, 5, 6, 2, 0, 4, 7, 3],
                         [5, 4, 7, 6, 1, 0, 3, 2],
                         [4, 0, 3, 7, 5, 1, 2, 6]],
            }

            perms = nm.array(_perms3d[self.name], dtype=nm.int32)

        return perms

    def get_grid(self, n_nod):
        """
        Get a grid of `n_nod` interpolation points, including the geometry
        element vertices. The number of points must correspond to a valid
        number of FE nodes for each geometry.
        """
        gd = geometry_data[self.name]
        return gd.get_grid(n_nod)

    def create_surface_facet(self):
        """
        Create a GeometryElement instance corresponding to this instance
        surface facet.
        """
        self.surface_facet = GeometryElement(self.surface_facet_name)

########NEW FILE########
__FILENAME__ = global_interp
"""
Global interpolation functions.
"""
import time
import numpy as nm

from sfepy.base.base import output, get_default_attr
from sfepy.discrete.fem.mesh import make_inverse_connectivity
from sfepy.discrete.fem.extmods.bases import find_ref_coors

def get_ref_coors(field, coors, strategy='kdtree', close_limit=0.1, cache=None,
                  verbose=True):
    """
    Get reference element coordinates and elements corresponding to given
    physical coordinates.

    Parameters
    ----------
    field : Field instance
        The field defining the approximation.
    coors : array
        The physical coordinates.
    strategy : str, optional
        The strategy for finding the elements that contain the
        coordinates. Only 'kdtree' is supported for the moment.
    close_limit : float, optional
        The maximum limit distance of a point from the closest
        element allowed for extrapolation.
    cache : Struct, optional
        To speed up a sequence of evaluations, the field mesh, the inverse
        connectivity of the field mesh and the KDTree instance can be cached as
        `cache.mesh`, `cache.offsets`, `cache.iconn` and
        `cache.kdtree`. Optionally, the cache can also contain the reference
        element coordinates as `cache.ref_coors`, `cache.cells` and
        `cache.status`, if the evaluation occurs in the same coordinates
        repeatedly. In that case the KDTree related data are ignored.
    verbose : bool
        If False, reduce verbosity.

    Returns
    -------
    ref_coors : array
        The reference coordinates.
    cells : array
        The cell indices corresponding to the reference coordinates.
    status : array
        The status: 0 is success, 1 is extrapolation within `close_limit`, 2 is
        extrapolation outside `close_limit`, 3 is failure.
    """
    ref_coors = get_default_attr(cache, 'ref_coors', None)
    if ref_coors is None:
        mesh = get_default_attr(cache, 'mesh', None)
        if mesh is None:
            mesh = field.create_mesh(extra_nodes=False)

        scoors = mesh.coors
        output('reference field: %d vertices' % scoors.shape[0],
               verbose=verbose)

        iconn = get_default_attr(cache, 'iconn', None)
        if iconn is None:
            offsets, iconn = make_inverse_connectivity(mesh.conns,
                                                       mesh.n_nod,
                                                       ret_offsets=True)

            ii = nm.where(offsets[1:] == offsets[:-1])[0]
            if len(ii):
                raise ValueError('some vertices not in any element! (%s)'
                                 % ii)

        else:
            offsets = cache.offsets

        if strategy == 'kdtree':
            kdtree = get_default_attr(cache, 'kdtree', None)
            if kdtree is None:
                from scipy.spatial import cKDTree as KDTree

                tt = time.clock()
                kdtree = KDTree(scoors)
                output('kdtree: %f s' % (time.clock()-tt), verbose=verbose)

            tt = time.clock()
            ics = kdtree.query(coors)[1]
            output('kdtree query: %f s' % (time.clock()-tt), verbose=verbose)

            tt = time.clock()
            ics = nm.asarray(ics, dtype=nm.int32)

            vertex_coorss, nodess, mtx_is = [], [], []
            conns = []
            for ig, ap in field.aps.iteritems():
                ps = ap.interp.gel.interp.poly_spaces['v']

                vertex_coorss.append(ps.geometry.coors)
                nodess.append(ps.nodes)
                mtx_is.append(ps.get_mtx_i())

                conns.append(mesh.conns[ig].copy())

            # Get reference element coordinates corresponding to
            # destination coordinates.
            ref_coors = nm.empty_like(coors)
            cells = nm.empty((coors.shape[0], 2), dtype=nm.int32)
            status = nm.empty((coors.shape[0],), dtype=nm.int32)

            find_ref_coors(ref_coors, cells, status, coors,
                        ics, offsets, iconn,
                        scoors, conns,
                        vertex_coorss, nodess, mtx_is,
                        1, close_limit, 1e-15, 100, 1e-8)
            output('ref. coordinates: %f s' % (time.clock()-tt),
                   verbose=verbose)

        elif strategy == 'crawl':
            raise NotImplementedError

        else:
            raise ValueError('unknown search strategy! (%s)' % strategy)

    else:
        ref_coors = cache.ref_coors
        cells = cache.cells
        status = cache.status

    return ref_coors, cells, status

########NEW FILE########
__FILENAME__ = history
import numpy as nm

from sfepy.base.base import get_default, OneTypeList, Container, Struct
from sfepy.solvers.ts import TimeStepper
from sfepy.discrete.fem.meshio import HDF5MeshIO

##
# 14.06.2007, c
class Histories( Container ):

    def from_file_hdf5( filename, var_names ):
        """TODO: do not read entire file, provide data on demand."""
        io = HDF5MeshIO( filename )
        ts = TimeStepper( *io.read_time_stepper() )
        steps = nm.arange( ts.n_step, dtype = nm.int32 )

        ths = io.read_variables_time_history( var_names, ts )

        objs = OneTypeList( History )
        for name, th in ths.iteritems():
            hist = History( name,
                            steps = steps,
                            times = ts.times,
                            th = th )
            objs.append( hist )
            
        obj = Histories( objs, dt = ts.dt,
                         name = ' '.join( var_names ) )
        return obj
    from_file_hdf5 = staticmethod( from_file_hdf5 )


##
# 14.06.2007, c
class History( Struct ):

    ##
    # 14.06.2007, c
    def from_sequence( seq, name ):
        obj = History( name = name,
                       steps = nm.arange( len( seq ), dtype = nm.int32 ),
                       th = list( seq ) )
        return obj
    from_sequence = staticmethod( from_sequence )

    def __init__( self, name, th = None, steps = None, times = None ):
        Struct.__init__( self, name = name,
                         th = get_default( th, [] ),
                         steps = get_default( steps, [] ),
                         times = get_default( times, [] ) )

##     def __setitem__( self, key, value ):
##         pass

    ##
    # 14.06.2007, c
    def __getitem__( self, ii ):
        return self.th[ii]

    ##
    # 15.06.2007, c
    def __len__( self ):
        return len( self.th )

    def append( self, item, step, time ):
        self.th.append( item )
        self.steps.append( step )
        self.times.append( time )

########NEW FILE########
__FILENAME__ = lcbc_operators
"""
Operators for enforcing linear combination boundary conditions in nodal FEM
setting.
"""
import numpy as nm
import scipy.sparse as sp

from sfepy.base.base import output, assert_, Container, Struct
from sfepy.discrete.common.dof_info import DofInfo, expand_nodes_to_equations
from sfepy.discrete.fem.utils import (compute_nodal_normals,
                                      compute_nodal_edge_dirs)

class LCBCOperator(Struct):
    """
    Base class for LCBC operators.
    """

    def treat_pbcs(self, dofs, master):
        """
        Treat dofs with periodic BC.
        """
        master = nm.intersect1d(dofs, master)
        if len(master) == 0: return

        # Remove rows with master DOFs.
        remove = nm.searchsorted(nm.sort(dofs), master)
        keep = nm.setdiff1d(nm.arange(len(dofs)), remove)

        mtx = self.mtx[keep]

        # Remove empty columns, update new DOF count.
        mtx = mtx.tocsc()
        indptr = nm.unique(mtx.indptr)
        self.mtx = sp.csc_matrix((mtx.data, mtx.indices, indptr),
                                 shape=(mtx.shape[0], indptr.shape[0] - 1))
        self.n_dof = self.mtx.shape[1]

class RigidOperator(LCBCOperator):
    """
    Transformation matrix operator for rigid LCBCs.
    """

    def __init__(self, name, nodes, field, dof_names, all_dof_names):
        Struct.__init__(self, name=name, nodes=nodes, dof_names=dof_names)

        coors = field.get_coor(nodes)
        n_nod, dim = coors.shape

        mtx_e = nm.tile(nm.eye(dim, dtype=nm.float64), (n_nod, 1))

        if dim == 2:
            mtx_r = nm.empty((dim * n_nod, 1), dtype=nm.float64)
            mtx_r[0::dim,0] = -coors[:,1]
            mtx_r[1::dim,0] = coors[:,0]
            n_rigid_dof = 3

        elif dim == 3:
            mtx_r = nm.zeros((dim * n_nod, dim), dtype=nm.float64)
            mtx_r[0::dim,1] = coors[:,2]
            mtx_r[0::dim,2] = -coors[:,1]
            mtx_r[1::dim,0] = -coors[:,2]
            mtx_r[1::dim,2] = coors[:,0]
            mtx_r[2::dim,0] = coors[:,1]
            mtx_r[2::dim,1] = -coors[:,0]
            n_rigid_dof = 6

        else:
            msg = 'dimension in [2, 3]: %d' % dim
            raise ValueError(msg)

        self.n_dof = n_rigid_dof
        self.mtx = nm.hstack((mtx_r, mtx_e))

        # Strip unconstrained dofs.
        aux = dim * nm.arange(n_nod)
        indx = [aux + all_dof_names.index(dof) for dof in dof_names]
        indx = nm.array(indx).T.ravel()

        self.mtx = self.mtx[indx]

def _save_vectors(filename, vectors, region, mesh, data_name):
    """
    Save vectors defined in region nodes as vector data in mesh vertices.
    """
    nv = nm.zeros_like(mesh.coors)
    nmax = region.vertices.shape[0]
    nv[region.vertices] = vectors[:nmax]

    out = {data_name : Struct(name='output_data', mode='vertex', data=nv)}
    mesh.write(filename, out=out, io='auto')

class NoPenetrationOperator(LCBCOperator):
    """
    Transformation matrix operator for no-penetration LCBCs.
    """
    def __init__(self, name, nodes, region, field, dof_names, filename=None):
        Struct.__init__(self, name=name, nodes=nodes, dof_names=dof_names)

        dim = region.dim
        assert_(len(dof_names) == dim)

        normals = compute_nodal_normals(nodes, region, field)

        if filename is not None:
            _save_vectors(filename, normals, region, field.domain.mesh, 'n')

        ii = nm.abs(normals).argmax(1)
        n_nod, dim = normals.shape

        irs = set(range(dim))

        data = []
        rows = []
        cols = []
        for idim in xrange(dim):
            ic = nm.where(ii == idim)[0]
            if len(ic) == 0: continue

            ir = list(irs.difference([idim]))
            nn = nm.empty((len(ic), dim - 1), dtype=nm.float64)
            for ik, il in enumerate(ir):
                nn[:,ik] = - normals[ic,il] / normals[ic,idim]

            irn = dim * ic + idim
            ics = [(dim - 1) * ic + ik for ik in xrange(dim - 1)]
            for ik in xrange(dim - 1):
                rows.append(irn)
                cols.append(ics[ik])
                data.append(nn[:,ik])

            ones = nm.ones((nn.shape[0],), dtype=nm.float64)
            for ik, il in enumerate(ir):
                rows.append(dim * ic + il)
                cols.append(ics[ik])
                data.append(ones)

        rows = nm.concatenate(rows)
        cols = nm.concatenate(cols)
        data = nm.concatenate(data)

        n_np_dof = n_nod * (dim - 1)
        mtx = sp.coo_matrix((data, (rows, cols)), shape=(n_nod * dim, n_np_dof))

        self.n_dof = n_np_dof
        self.mtx = mtx.tocsr()

class NormalDirectionOperator(LCBCOperator):
    """
    Transformation matrix operator for normal direction LCBCs.

    The substitution (in 3D) is:

    .. math::
        [u_1, u_2, u_3]^T = [n_1, n_2, n_3]^T w

    The new DOF is :math:`w`.
    """
    def __init__(self, name, nodes, region, field, dof_names, filename=None):
        Struct.__init__(self, name=name, nodes=nodes, dof_names=dof_names)

        dim = region.dim
        assert_(len(dof_names) == dim)

        vectors = self.get_vectors(nodes, region, field, filename=filename)

        n_nod, dim = vectors.shape

        data = vectors.ravel()
        rows = nm.arange(data.shape[0])
        cols = nm.repeat(nm.arange(n_nod), dim)

        mtx = sp.coo_matrix((data, (rows, cols)), shape=(n_nod * dim, n_nod))

        self.n_dof = n_nod
        self.mtx = mtx.tocsr()

    def get_vectors(self, nodes, region, field, filename=None):
        normals = compute_nodal_normals(nodes, region, field)

        if filename is not None:
            _save_vectors(filename, normals, region, field.domain.mesh, 'n')

        return normals

class EdgeDirectionOperator(NormalDirectionOperator):
    """
    Transformation matrix operator for edges direction LCBCs.

    The substitution (in 3D) is:

    .. math::
        [u_1, u_2, u_3]^T = [d_1, d_2, d_3]^T w,

    where :math:`\ul{d}` is an edge direction vector averaged into a node. The
    new DOF is :math:`w`.
    """
    def get_vectors(self, nodes, region, field, filename=None):
        edirs = compute_nodal_edge_dirs(nodes, region, field)

        if filename is not None:
            _save_vectors(filename, edirs, region, field.domain.mesh, 'e')

        return edirs

class IntegralMeanValueOperator(LCBCOperator):
    """
    Transformation matrix operator for integral mean value LCBCs.
    All node DOFs are sumed to the new one.
    """
    def __init__(self, name, nodes, region, field, dof_names):
        Struct.__init__(self, name=name, nodes=nodes, dof_names=dof_names)

        dpn = len(dof_names)
        n_nod = nodes.shape[0]

        data = nm.ones((n_nod * dpn,))
        rows = nm.arange(data.shape[0])
        cols = nm.zeros((data.shape[0],))

        mtx = sp.coo_matrix((data, (rows, cols)), shape=(n_nod * dpn, dpn))

        self.n_dof = dpn
        self.mtx = mtx.tocsr()

class LCBCOperators(Container):
    """
    Container holding instances of LCBCOperator subclasses for a single
    variable.

    Parameters
    ----------
    name : str
        The object name.
    eq_map : EquationMap instance
        The equation mapping of the variable.
    offset : int
        The offset added to markers distinguishing the individual LCBCs.
    """
    def __init__(self, name, eq_map, offset):
        Container.__init__(self, name=name, eq_map=eq_map, offset=offset)

        self.eq_lcbc = nm.zeros((self.eq_map.n_eq,), dtype=nm.int32)
        self.markers = []
        self.n_transformed_dof = []
        self.n_op = 0
        self.ics = None

    def add_from_bc(self, bc, field):
        """
        Create a new LCBC operator described by `bc`, and add it to the
        container.

        Parameters
        ----------
        bc : LinearCombinationBC instance
            The LCBC condition description.
        field : Field instance
            The field of the variable.
        """
        region = bc.region
        dofs, kind = bc.dofs

        nmaster = field.get_dofs_in_region(region, merge=True)

        if kind == 'rigid':
            op = RigidOperator('%d_rigid' % len(self),
                               nmaster, field, dofs, self.eq_map.dof_names)

        elif kind == 'no_penetration':
            filename = bc.get('filename', None)
            op = NoPenetrationOperator('%d_no_penetration' % len(self),
                                       nmaster, region, field, dofs,
                                       filename=filename)

        elif kind == 'normal_direction':
            filename = bc.get('filename', None)
            op = NormalDirectionOperator('%d_normal_direction' % len(self),
                                         nmaster, region, field, dofs,
                                         filename=filename)

        elif kind == 'edge_direction':
            filename = bc.get('filename', None)
            op = EdgeDirectionOperator('%d_edge_direction' % len(self),
                                       nmaster, region, field, dofs,
                                       filename=filename)

        elif kind == 'integral_mean_value':
            op = IntegralMeanValueOperator('%d_integral_mean_value' % len(self),
                                           nmaster, region, field, dofs)

        self.append(op)

    def append(self, op):
        Container.append(self, op)

        eq = self.eq_map.eq
        dofs = expand_nodes_to_equations(op.nodes, op.dof_names,
                                         self.eq_map.dof_names)
        meq = eq[dofs]
        assert_(nm.all(meq >= 0))

        if self.eq_map.n_epbc:
            op.treat_pbcs(dofs, self.eq_map.master)

        self.markers.append(self.offset + self.n_op + 1)
        self.eq_lcbc[meq] = self.markers[-1]

        self.n_transformed_dof.append(op.n_dof)
        self.n_op = len(self)

    def finalize(self):
        """
        Call this after all LCBCs of the variable have been added.

        Initializes the global column indices.
        """
        self.ics = nm.cumsum(nm.r_[0, self.n_transformed_dof])

def make_global_lcbc_operator(lcbc_ops, adi, new_only=False):
    """
    Assemble all LCBC operators into a single matrix.

    Returns
    -------
    mtx_lc : csr_matrix
        The global LCBC operator in the form of a CSR matrix.
    lcdi : DofInfo
        The global active LCBC-constrained DOF information.
    new_only : bool
        If True, the operator columns will contain only new DOFs.
    """
    n_dof = adi.ptr[-1]
    eq_lcbc = nm.zeros((n_dof,), dtype=nm.int32)

    n_dof_new = 0
    n_free = {}
    n_new = {}
    for var_name, lcbc_op in lcbc_ops.iteritems():
        if lcbc_op is None: continue

        indx = adi.indx[var_name]
        eq_lcbc[indx] = lcbc_op.eq_lcbc

        n_free[var_name] = len(nm.where(lcbc_op.eq_lcbc == 0)[0])
        n_new[var_name] = nm.sum(lcbc_op.n_transformed_dof)

        n_dof_new += n_new[var_name]

    if n_dof_new == 0:
        return None, None

    ii = nm.nonzero(eq_lcbc)[0]
    n_constrained = ii.shape[0]
    n_dof_free = n_dof - n_constrained
    n_dof_reduced = n_dof_free + n_dof_new
    output('dofs: total %d, free %d, constrained %d, new %d'\
            % (n_dof, n_dof_free, n_constrained, n_dof_new))
    output(' -> reduced %d' % (n_dof_reduced))

    lcdi = DofInfo('lcbc_active_state_dof_info')
    fdi = DofInfo('free_dof_info')
    ndi = DofInfo('new_dof_info')
    for var_name in adi.var_names:
        nf = n_free.get(var_name, adi.n_dof[var_name])
        nn = n_new.get(var_name, 0)
        fdi.append_raw(var_name, nf)
        ndi.append_raw(var_name, nn)
        lcdi.append_raw(var_name, nn + nf)

    assert_(lcdi.ptr[-1] == n_dof_reduced)

    rows = []
    cols = []
    data = []
    for var_name, lcbc_op in lcbc_ops.iteritems():
        if lcbc_op is None: continue

        if new_only:
            offset = ndi.indx[var_name].start

        else:
            offset = lcdi.indx[var_name].start + fdi.n_dof[var_name]

        for ii, op in enumerate(lcbc_op):
            indx = nm.where(eq_lcbc == lcbc_op.markers[ii])[0]
            icols = nm.arange(offset + lcbc_op.ics[ii],
                              offset + lcbc_op.ics[ii+1])

            if isinstance(op.mtx, sp.spmatrix):
                lr, lc, lv = sp.find(op.mtx)
                rows.append(indx[lr])
                cols.append(icols[lc])
                data.append(lv)

            else:
                irs, ics = nm.meshgrid(indx, icols)
                rows.append(irs.ravel())
                cols.append(ics.ravel())
                data.append(op.mtx.T.ravel())

    rows = nm.concatenate(rows)
    cols = nm.concatenate(cols)
    data = nm.concatenate(data)

    if new_only:
        mtx_lc = sp.coo_matrix((data, (rows, cols)),
                               shape=(n_dof, n_dof_new))

    else:
        mtx_lc = sp.coo_matrix((data, (rows, cols)),
                               shape=(n_dof, n_dof_reduced))

        ir = nm.where(eq_lcbc == 0)[0]

        ic = nm.empty((n_dof_free,), dtype=nm.int32)
        for var_name in adi.var_names:
            ii = nm.arange(fdi.n_dof[var_name], dtype=nm.int32)
            ic[fdi.indx[var_name]] = lcdi.indx[var_name].start + ii

        mtx_lc2 = sp.coo_matrix((nm.ones((ir.shape[0],)), (ir, ic)),
                                shape=(n_dof, n_dof_reduced), dtype=nm.float64)


        mtx_lc = mtx_lc + mtx_lc2

    mtx_lc = mtx_lc.tocsr()

    return mtx_lc, lcdi

########NEW FILE########
__FILENAME__ = linearizer
"""
Linearization of higher order solutions for the purposes of visualization.
"""
import numpy as nm

from sfepy.linalg import dot_sequences
from sfepy.discrete.fem.refine import refine_reference

def get_eval_dofs(dofs, dof_conn, ps, ori=None):
    """
    Get default function for evaluating field DOFs given a list of elements and
    reference element coordinates.
    """
    def _eval(iels, rx):
        edofs = dofs[dof_conn[iels]]

        if ori is not None:
            eori = ori[iels]

        else:
            eori = None

        bf = ps.eval_base(rx, ori=eori, force_axis=True)[...,0,:]
        rvals = dot_sequences(bf, edofs)

        return rvals

    return _eval

def get_eval_coors(coors, conn, ps):
    """
    Get default function for evaluating physical coordinates given a list of
    elements and reference element coordinates.
    """
    def _eval(iels, rx):
        ecoors = coors[conn[iels]]
        aux = ecoors.transpose((0, 2, 1))

        bf = ps.eval_base(rx).squeeze()
        phys_coors = nm.dot(aux, bf.T).transpose((0, 2, 1))
        return phys_coors

    return _eval

def create_output(eval_dofs, eval_coors, n_el, ps, min_level=0, max_level=2,
                  eps=1e-4):
    """
    Create mesh with linear elements that approximates DOFs returned by
    `eval_dofs()` corresponding to a higher order approximation with a relative
    precision given by `eps`. The DOFs are evaluated in physical coordinates
    returned by `eval_coors()`.
    """

    def _get_msd(iels, rx, ree):
        rvals = eval_dofs(iels, rx)
        rng = rvals.max() - rvals.min()
        n_components = rvals.shape[-1]

        msd = 0.0
        for ic in range(n_components):
            rval = rvals[..., ic]

            sd = rval[:, ree]
            # ~ max. second derivative.
            msd += nm.abs(sd[..., 0] + sd[..., 2]
                          - 2.0 * sd[..., 1]).max(axis=-1)

        msd /= n_components

        return msd, rng

    rx0 = ps.geometry.coors

    rc0 = ps.geometry.conn[None, :]
    rx, rc, ree = refine_reference(ps.geometry, 1)

    factor = rc.shape[0] / rc0.shape[0]

    iels = nm.arange(n_el)
    msd, rng = _get_msd(iels, rx, ree)
    eps_r = rng * eps
    flag = msd > eps_r

    iels0 = flag0 = None

    coors = []
    conns = []
    vdofs = []

    inod = 0
    for level in range(max_level + 1):
        if level < min_level:
            flag.fill(True) # Force refinement everywhere.

        elif level == max_level:
            # Last level - take everything.
            flag.fill(False)

        # Deal with finished elements.
        if flag0 is not None:
            ii = nm.searchsorted(iels0, iels)
            expand_flag0 = flag0[ii].repeat(factor, axis=1)

        else:
            expand_flag0 = nm.ones_like(flag)

        ie, ir = nm.where((flag == False) & (expand_flag0 == True))
        if len(ie):
            uie, iies = nm.unique(ie, return_inverse=True)

            # Each (sub-)element has own coordinates - no shared vertices.
            xes = eval_coors(iels[uie], rx0)
            des = eval_dofs(iels[uie], rx0)

            # Vectorize (how??) or use cython?
            cc = []
            vd = []
            for ii, iie in enumerate(iies):
                ce = rc0[ir[ii]]

                xe = xes[iie]
                cc.append(xe[ce])

                de = des[iie]
                vd.append(de[ce])

            cc = nm.vstack(cc)
            vd = nm.vstack(vd)

            nc = cc.shape[0]
            np = rc0.shape[1]
            conn = nm.arange(nc, dtype=nm.int32).reshape((nc / np, np))

            coors.append(cc)
            conns.append(conn + inod)
            vdofs.append(vd)

            inod += nc

        if not flag.any():
            break

        iels0 = iels
        flag0 = flag

        # Deal with elements to refine.
        if level < max_level:
            eflag = flag.sum(axis=1, dtype=nm.bool)
            iels = iels[eflag]

            rc0 = rc
            rx0 = rx
            rx, rc, ree = refine_reference(ps.geometry, level + 2)

            msd, rng = _get_msd(iels, rx, ree)
            eps_r = rng * eps
            flag = msd > eps_r

    all_coors = nm.concatenate(coors, axis=0)
    conn = nm.concatenate(conns, axis=0)
    all_vdofs = nm.concatenate(vdofs, axis=0)

    mat_ids = nm.zeros(conn.shape[0], dtype=nm.int32)

    return level, all_coors, conn, all_vdofs, mat_ids

########NEW FILE########
__FILENAME__ = mappings
"""
Finite element reference mappings.
"""
import numpy as nm

from sfepy.base.base import get_default, output
from sfepy.discrete.common.mappings import Mapping
from sfepy.discrete.fem.poly_spaces import PolySpace
from sfepy.discrete.fem.extmods.mappings import CMapping

class FEMapping(Mapping):
    """
    Base class for finite element mappings.
    """

    def __init__(self, coors, conn, poly_space=None, gel=None, order=1):
        self.coors = coors
        self.conn = conn

        try:
            nm.take(self.coors, self.conn)

        except IndexError:
            output('coordinates shape: %s' % list(coors.shape))
            output('connectivity: min: %d, max: %d' % (conn.min(), conn.max()))
            msg = 'incompatible connectivity and coordinates (see above)'
            raise IndexError(msg)

        self.n_el, self.n_ep = conn.shape
        self.dim = self.coors.shape[1]

        if poly_space is None:
            poly_space = PolySpace.any_from_args(None, gel, order,
                                                 base='lagrange',
                                                 force_bubble=False)

        self.poly_space = poly_space

    def get_geometry(self):
        """
        Return reference element geometry as a GeometryElement instance.
        """
        return self.poly_space.geometry

    def get_base(self, coors, diff=False):
        """
        Get base functions or their gradient evaluated in given
        coordinates.
        """
        bf = self.poly_space.eval_base(coors, diff=diff)
        return bf

    def get_physical_qps(self, qp_coors):
        """
        Get physical quadrature points corresponding to given reference
        element quadrature points.

        Returns
        -------
        qps : array
            The physical quadrature points ordered element by element,
            i.e. with shape (n_el, n_qp, dim).
        """
        bf = self.get_base(qp_coors)
        qps = nm.dot(nm.atleast_2d(bf.squeeze()), self.coors[self.conn])
        # Reorder so that qps are really element by element.
        qps = nm.ascontiguousarray(nm.swapaxes(qps, 0, 1))

        return qps

class VolumeMapping(FEMapping):
    """
    Mapping from reference domain to physical domain of the same space
    dimension.
    """

    def get_mapping(self, qp_coors, weights, poly_space=None, ori=None):
        """
        Get the mapping for given quadrature points, weights, and
        polynomial space.

        Returns
        -------
        cmap : CMapping instance
            The volume mapping.
        """
        poly_space = get_default(poly_space, self.poly_space)

        bf_g = self.get_base(qp_coors, diff=True)

        ebf_g = poly_space.eval_base(qp_coors, diff=True, ori=ori,
                                     force_axis=True)
        flag = ori is not None

        cmap = CMapping(self.n_el, qp_coors.shape[0], self.dim,
                        poly_space.n_nod, mode='volume', flag=flag)
        cmap.describe(self.coors, self.conn, bf_g, ebf_g, weights)

        return cmap

class SurfaceMapping(FEMapping):
    """
    Mapping from reference domain to physical domain of the space
    dimension higher by one.
    """

    def get_mapping(self, qp_coors, weights, poly_space=None, mode='surface'):
        """
        Get the mapping for given quadrature points, weights, and
        polynomial space.

        Returns
        -------
        cmap : CMapping instance
            The surface mapping.
        """
        poly_space = get_default(poly_space, self.poly_space)

        bf_g = self.get_base(qp_coors, diff=True)

        if nm.allclose(bf_g, 0.0):
            raise ValueError('zero base function gradient!')

        cmap = CMapping(self.n_el, qp_coors.shape[0], self.dim,
                        poly_space.n_nod, mode=mode)
        cmap.describe(self.coors, self.conn, bf_g, None, weights)

        return cmap

########NEW FILE########
__FILENAME__ = mesh
import time
import numpy as nm
import scipy.sparse as sp

from sfepy.base.base import Struct, get_default, output, assert_
from meshio import MeshIO, supported_cell_types

def make_point_cells(indx, dim):
    conn = nm.zeros((indx.shape[0], dim + 1), dtype=nm.int32)
    for ii in range(0, dim + 1):
        conn[:,ii] = indx
    return conn

def find_map(x1, x2, eps=1e-8, allow_double=False, join=True):
    """
    Find a mapping between common coordinates in x1 and x2, such that
    x1[cmap[:,0]] == x2[cmap[:,1]]
    """
    off, dim = x1.shape
    ir = nm.zeros((off + x2.shape[0],), dtype=nm.int32)
    ir[off:] = off

    x1 = nm.round(x1.T / eps) * eps
    x2 = nm.round(x2.T / eps) * eps
    xx = nm.c_[x1, x2]

    keys = [xx[ii] for ii in range(dim)]
    iis = nm.lexsort(keys=keys)

    xs = xx.T[iis]
    xd = nm.sqrt(nm.sum(nm.diff(xs, axis=0)**2.0, axis=1))

    ii = nm.where(xd < eps)[0]
    off1, off2 = ir[iis][ii], ir[iis][ii+1]
    i1, i2 = iis[ii] - off1, iis[ii+1] - off2
    dns = nm.where(off1 == off2)[0]
    if dns.size:
        output('double node(s) in:')
        for dn in dns:
            if off1[dn] == 0:
                output('x1: %d %d -> %s %s' % (i1[dn], i2[dn],
                                               x1[:,i1[dn]], x1[:,i2[dn]]))
            else:
                output('x2: %d %d -> %s %s' % (i1[dn], i2[dn],
                                               x2[:,i1[dn]], x2[:,i2[dn]]))
        if not allow_double:
            raise ValueError('double node(s)! (see above)')

    if join:
        cmap = nm.c_[i1, i2]
        return cmap
    else:
        return i1, i2

def merge_mesh(x1, ngroups1, conns1, mat_ids1, x2, ngroups2, conns2, mat_ids2,
               cmap, eps=1e-8):
    """
    Merge two meshes in common coordinates found in x1, x2.

    Notes
    -----
    Assumes the same number and kind of element groups in both meshes!
    """
    n1 = x1.shape[0]
    n2 = x2.shape[0]

    err = nm.sum(nm.sum(nm.abs(x1[cmap[:,0],:-1] - x2[cmap[:,1],:-1])))
    if abs(err) > (10.0 * eps):
        raise ValueError('nonmatching meshes! (error: %e)' % err)

    mask = nm.ones((n2,), dtype=nm.int32)
    mask[cmap[:,1]] = 0
    remap = nm.cumsum(mask) + n1 - 1
    remap[cmap[:,1]] = cmap[:,0]

    i2 = nm.setdiff1d(nm.arange( n2, dtype=nm.int32), cmap[:,1])
    xx = nm.r_[x1, x2[i2]]
    ngroups = nm.r_[ngroups1, ngroups2[i2]]

    conns = []
    for ii, conn1 in enumerate(conns1):
        conn = nm.vstack((conn1, remap[conns2[ii]]))
        conns.append(conn)

    mat_ids = None
    if (mat_ids1 is not None) and (mat_ids2 is not None):
        mat_ids = []

        for ii, mm1 in enumerate(mat_ids1):
            mm = nm.concatenate((mm1, mat_ids2[ii]))
            mat_ids.append(mm)

    return xx, ngroups, conns, mat_ids

def fix_double_nodes(coor, ngroups, conns, eps):
    """
    Detect and attempt fixing double nodes in a mesh.

    The double nodes are nodes having the same coordinates
    w.r.t. precision given by `eps`.
    """
    n_nod, dim = coor.shape
    cmap = find_map(coor, nm.zeros((0,dim)), eps=eps, allow_double=True)
    if cmap.size:
        output('double nodes in input mesh!')
        output('trying to fix...')

        while cmap.size:
            # Just like in Variable.equation_mapping()...
            ii = nm.argsort(cmap[:,1])
            scmap = cmap[ii]

            eq = nm.arange(n_nod)
            eq[scmap[:,1]] = -1
            eqi = eq[eq >= 0]
            eq[eqi] = nm.arange(eqi.shape[0])
            remap = eq.copy()
            remap[scmap[:,1]] = eq[scmap[:,0]]
            output(coor.shape)
            coor = coor[eqi]
            ngroups = ngroups[eqi]
            output(coor.shape)
            ccs = []
            for conn in conns:
                ccs.append(remap[conn])
            conns = ccs
            cmap = find_map(coor, nm.zeros((0,dim)), eps=eps,
                            allow_double=True)
        output('...done')
    return coor, ngroups, conns

def get_min_edge_size(coor, conns):
    """
    Get the smallest edge length.
    """
    mes = 1e16
    for conn in conns:
        n_ep = conn.shape[1]
        for ir in range(n_ep):
            x1 = coor[conn[:,ir]]
            for ic in range(ir + 1, n_ep):
                x2 = coor[conn[:,ic]]
                aux = nm.sqrt(nm.sum((x2 - x1)**2.0, axis=1).min())
                mes = min(mes, aux)

    return mes

def get_min_vertex_distance(coor, guess):
    """Can miss the minimum, but is enough for our purposes."""
    # Sort by x.
    ix = nm.argsort(coor[:,0])
    scoor = coor[ix]

    mvd = 1e16

    # Get mvd in chunks potentially smaller than guess.
    n_coor = coor.shape[0]

    i0 = i1 = 0
    x0 = scoor[i0,0]
    while 1:
        while ((scoor[i1,0] - x0) < guess) and (i1 < (n_coor - 1)):
            i1 += 1

        ## print i0, i1, x0, scoor[i1,0]
        aim, aa1, aa2, aux = get_min_vertex_distance_naive(scoor[i0:i1+1])
        if aux < mvd:
            im, a1, a2 = aim, aa1 + i0, aa2 + i0
        mvd = min(mvd, aux)
        i0 = i1 = int(0.5 * (i1 + i0)) + 1
        ## i0 += 1
        x0 = scoor[i0,0]
        ## print '-', i0

        if i1 == n_coor - 1: break

    ## print im, ix[a1], ix[a2], a1, a2, scoor[a1], scoor[a2]

    return mvd

def get_min_vertex_distance_naive(coor):

    ii = nm.arange(coor.shape[0])
    i1, i2 = nm.meshgrid(ii, ii)
    i1 = i1.flatten()
    i2 = i2.flatten()

    ii = nm.where(i1 < i2)
    aux = coor[i1[ii]] - coor[i2[ii]]
    aux = nm.sum(aux**2.0, axis=1)

    im = aux.argmin()

    return im, i1[ii][im], i2[ii][im], nm.sqrt(aux[im])

def make_mesh(coor, ngroups, conns, mesh_in):
    """Create a mesh reusing mat_ids and descs of mesh_in."""
    mat_ids = []
    for ii, conn in enumerate(conns):
        mat_id = nm.empty((conn.shape[0],), dtype=nm.int32)
        mat_id.fill(mesh_in.mat_ids[ii][0])
        mat_ids.append(mat_id)

    mesh_out = Mesh.from_data('merged mesh', coor, ngroups, conns,
                              mat_ids, mesh_in.descs)
    return mesh_out

def make_inverse_connectivity(conns, n_nod, ret_offsets=True):
    """
    For each mesh node referenced in the connectivity conns, make a list of
    elements it belongs to.
    """
    from itertools import chain

    iconn = [[] for ii in xrange(n_nod)]
    n_els = [0] * n_nod
    for ig, conn in enumerate(conns):
        for iel, row in enumerate(conn):
            for node in row:
                iconn[node].extend([ig, iel])
                n_els[node] += 1

    n_els = nm.array(n_els, dtype=nm.int32)
    iconn = nm.fromiter(chain(*iconn), nm.int32)

    if ret_offsets:
        offsets = nm.cumsum(nm.r_[0, n_els], dtype=nm.int32)
        return offsets, iconn

    else:
        return n_els, iconn

class Mesh(Struct):
    """
    Contains the FEM mesh together with all utilities related to it.

    Input and output is handled by the MeshIO class and subclasses.
    The Mesh class only contains the real mesh - nodes, connectivity,
    regions, plus methods for doing operations on this mesh.

    Example of creating and working with a mesh::

        In [1]: from sfepy.discrete.fem import Mesh
        In [2]: m = Mesh.from_file("meshes/3d/cylinder.vtk")
        sfepy: reading mesh (meshes/3d/cylinder.vtk)...
        sfepy: ...done in 0.04 s

        In [3]: m.coors
        Out[3]:
        array([[  1.00000000e-01,   2.00000000e-02,  -1.22460635e-18],
               [  1.00000000e-01,   1.80193774e-02,   8.67767478e-03],
               [  1.00000000e-01,   1.24697960e-02,   1.56366296e-02],
               ...,
               [  8.00298527e-02,   5.21598617e-03,  -9.77772215e-05],
               [  7.02544004e-02,   3.61610291e-04,  -1.16903153e-04],
               [  3.19633596e-02,  -1.00335972e-02,   9.60460305e-03]])

        In [4]: m.ngroups
        Out[4]: array([0, 0, 0, ..., 0, 0, 0])

        In [5]: m.conns
        Out[5]:
        [array([[ 28,  60,  45,  29],
               [ 28,  60,  57,  45],
               [ 28,  57,  27,  45],
               ...,
               [353, 343, 260, 296],
               [353, 139, 181, 140],
               [353, 295, 139, 140]])]

        In [6]: m.mat_ids
        Out[6]: [array([6, 6, 6, ..., 6, 6, 6])]

        In [7]: m.descs
        Out[7]: ['3_4']

        In [8]: m
        Out[8]: Mesh:meshes/3d/cylinder

        In [9]: print m
        Mesh:meshes/3d/cylinder
          conns:
            [array([[ 28,  60,  45,  29],
                   [ 28,  60,  57,  45],
                   [ 28,  57,  27,  45],
                   ...,
                   [353, 343, 260, 296],
                   [353, 139, 181, 140],
                   [353, 295, 139, 140]])]
          coors:
            [[  1.00000000e-01   2.00000000e-02  -1.22460635e-18]
             [  1.00000000e-01   1.80193774e-02   8.67767478e-03]
             [  1.00000000e-01   1.24697960e-02   1.56366296e-02]
             ...,
             [  8.00298527e-02   5.21598617e-03  -9.77772215e-05]
             [  7.02544004e-02   3.61610291e-04  -1.16903153e-04]
             [  3.19633596e-02  -1.00335972e-02   9.60460305e-03]]
          descs:
            ['3_4']
          dim:
            3
          el_offsets:
            [   0 1348]
          io:
            None
          mat_ids:
            [array([6, 6, 6, ..., 6, 6, 6])]
          n_e_ps:
            [4]
          n_el:
            1348
          n_els:
            [1348]
          n_nod:
            354
          name:
            meshes/3d/cylinder
          ngroups:
            [0 0 0 ..., 0 0 0]
          setup_done:
            0

    The Mesh().coors is an array of node coordinates and Mesh().conns is the
    list of elements of each type (see Mesh().desc), so for example if you want
    to know the coordinates of the nodes of the fifth finite element of the
    type 3_4 do::

        In [10]: m.descs
        Out[10]: ['3_4']

    So now you know that the finite elements of the type 3_4 are in a.conns[0]::

        In [11]: m.coors[m.conns[0][4]]
        Out[11]:
        array([[  1.00000000e-01,   1.80193774e-02,  -8.67767478e-03],
               [  1.00000000e-01,   1.32888539e-02,  -4.35893200e-04],
               [  1.00000000e-01,   2.00000000e-02,  -1.22460635e-18],
               [  9.22857574e-02,   1.95180454e-02,  -4.36416134e-03]])

    The element ids are of the form "<dimension>_<number of nodes>", i.e.:

    - 2_2 ... line
    - 2_3 ... triangle
    - 2_4 ... quadrangle
    - 3_2 ... line
    - 3_4 ... tetrahedron
    - 3_8 ... hexahedron

    """

    @staticmethod
    def from_surface(surf_faces, mesh_in):
        """
        Create a mesh given a set of surface faces and the original mesh.
        """
        aux = nm.concatenate([faces.ravel() for faces in surf_faces])
        inod = nm.unique(aux)

        n_nod = len(inod)
        n_nod_m, dim = mesh_in.coors.shape

        aux = nm.arange(n_nod, dtype=nm.int32)
        remap = nm.zeros((n_nod_m,), nm.int32)
        remap[inod] = aux

        mesh = Mesh(mesh_in.name + "_surf")

        mesh.coors = mesh_in.coors[inod]
        mesh.ngroups = mesh_in.ngroups[inod]

        sfm = {3 : "2_3", 4 : "2_4"}
        mesh.conns = []
        mesh.descs = []
        mesh.mat_ids = []
        for ii, sf in enumerate(surf_faces):
            n_el, n_fp = sf.shape

            conn = remap[sf]
            mat_id = nm.empty((conn.shape[0],), dtype=nm.int32)
            mat_id.fill(ii)

            mesh.descs.append(sfm[n_fp])
            mesh.conns.append(conn)
            mesh.mat_ids.append(mat_id)

        mesh._set_shape_info()

        return mesh

    @staticmethod
    def from_file(filename=None, io='auto', prefix_dir=None,
                  omit_facets=False):
        """
        Read a mesh from a file.

        Parameters
        ----------
        filename : string or function or MeshIO instance or Mesh instance
            The name of file to read the mesh from. For convenience, a
            mesh creation function or a MeshIO instance or directly a Mesh
            instance can be passed in place of the file name.
        io : *MeshIO instance
            Passing *MeshIO instance has precedence over filename.
        prefix_dir : str
            If not None, the filename is relative to that directory.
        omit_facets : bool
            If True, do not read cells of lower dimension than the space
            dimension (faces and/or edges). Only some MeshIO subclasses
            support this!
        """
        if isinstance(filename, Mesh):
            return filename

        if io == 'auto':
            if filename is None:
                output('filename or io must be specified!')
                raise ValueError
            else:
                io = MeshIO.any_from_filename(filename, prefix_dir=prefix_dir)

        cell_types = ', '.join(supported_cell_types[io.format])
        output('reading mesh [%s] (%s)...' % (cell_types, io.filename))
        tt = time.clock()

        trunk = io.get_filename_trunk()
        mesh = Mesh(trunk)
        mesh = io.read(mesh, omit_facets=omit_facets)

        output('...done in %.2f s' % (time.clock() - tt))

        mesh._set_shape_info()

        return mesh

    @staticmethod
    def from_region(region, mesh_in, save_edges=False, save_faces=False,
                    localize=False, is_surface=False):
        """
        Create a mesh corresponding to a given region.
        """
        mesh = Mesh(mesh_in.name + "_reg")
        mesh.coors = mesh_in.coors.copy()
        mesh.ngroups = mesh_in.ngroups.copy()

        mesh.conns = []
        mesh.descs = []
        mesh.mat_ids = []

        if not is_surface:
            if region.has_cells():
                for ig in region.igs:
                    mesh.descs.append(mesh_in.descs[ig])
                    els = region.get_cells(ig)
                    mesh.mat_ids.append(mesh_in.mat_ids[ig][els].copy())
                    mesh.conns.append(mesh_in.conns[ig][els,:].copy())

            if save_edges:
                cmesh = region.domain.cmesh
                for ig in region.igs:
                    edges = region.get_edges(ig)
                    if not edges.size: continue

                    verts = cmesh.get_incident(0, edges, 1)
                    verts.shape = (verts.shape[0] / 2, 2)

                    mesh.descs.append('1_2')
                    mesh.conns.append(verts)

                    mat_ids = nm.repeat(ig, verts.shape[0])
                    mesh.mat_ids.append(mat_ids)

            if save_faces:
                mesh._append_region_faces(region)

            if save_edges or save_faces:
                mesh.descs.append('1_1')
                mesh.mat_ids.append(-nm.ones_like(region.vertices))
                mesh.conns.append(region.vertices[:, None])

        else:
            mesh._append_region_faces(region, force_faces=True)

        mesh._set_shape_info()

        if localize:
            mesh.localize(region.vertices)

        return mesh

    @staticmethod
    def from_data(name, coors, ngroups, conns, mat_ids, descs,
                  igs=None, nodal_bcs=None):
        """
        Create a mesh from mesh data.
        """
        if igs is None:
            igs = range(len(conns))
        mesh = Mesh(name)
        mesh._set_data(coors=coors,
                       ngroups=ngroups,
                       conns=[conns[ig] for ig in igs],
                       mat_ids=[mat_ids[ig] for ig in igs],
                       descs=[descs[ig] for ig in igs],
                       nodal_bcs=nodal_bcs)
        mesh._set_shape_info()
        return mesh

    def __init__(self, name='mesh', filename=None,
                 prefix_dir=None, **kwargs):
        """Create a Mesh.

        Parameters
        ----------
        name : str
            Object name.
        filename : str
            Loads a mesh from the specified file, if not None.
        prefix_dir : str
            If not None, the filename is relative to that directory.
        """
        Struct.__init__(self, name=name, **kwargs)
        self.nodal_bcs = {}

        if filename is None:
            self.io = None
            self.setup_done = 0

        else:
            io = MeshIO.any_from_filename(filename, prefix_dir=prefix_dir)
            output('reading mesh (%s)...' % (io.filename))
            tt = time.clock()
            io.read(self)
            output('...done in %.2f s' % (time.clock() - tt))
            self._set_shape_info()

    def copy(self, name=None):
        """Make a deep copy of self.

        Parameters
        ----------
        name : str
            Name of the copied mesh.
        """
        return Struct.copy(self, deep=True, name=name)

    def __add__(self, other):
        """
        Merge the two meshes, assuming they have the same number and kind of
        element groups.
        """
        cmap = find_map(self.coors, other.coors)
        aux = merge_mesh(self.coors, self.ngroups, self.conns, self.mat_ids,
                         other.coors, other.ngroups, other.conns, other.mat_ids,
                         cmap)
        coors, ngroups, conns, mat_ids = aux

        mesh = Mesh.from_data(self.name + ' + ' + other.name,
                              coors, ngroups, conns, mat_ids, self.descs)

        return mesh

    def _set_shape_info(self):
        self.n_nod, self.dim = self.coors.shape
        self.n_els = nm.array([conn.shape[0] for conn in self.conns])
        self.n_e_ps = nm.array([conn.shape[1] for conn in self.conns])
        self.el_offsets = nm.cumsum(nm.r_[0, self.n_els])
        self.n_el = nm.sum(self.n_els)
        self.dims = [int(ii[0]) for ii in self.descs]

    def _set_data(self, coors, ngroups, conns, mat_ids, descs, nodal_bcs=None):
        """
        Set mesh data.

        Parameters
        ----------
        coors : array
            Coordinates of mesh nodes.
        ngroups : array
            Node groups.
        conns : list of arrays
            The array of mesh elements (connectivities) for each element group.
        mat_ids : list of arrays
            The array of material ids for each element group.
        descs: list of strings
            The element type for each element group.
        nodal_bcs : dict of arrays, optional
            The nodes defining regions for boundary conditions referred
            to by the dict keys in problem description files.
        """
        self.coors = nm.ascontiguousarray(coors)

        if ngroups is None:
            self.ngroups = nm.zeros((self.coors.shape[0],), dtype=nm.int32)

        else:
            self.ngroups = nm.ascontiguousarray(ngroups)

        self.conns = [nm.asarray(conn, dtype=nm.int32) for conn in conns]
        self.mat_ids = [nm.asarray(mat_id, dtype=nm.int32)
                        for mat_id in mat_ids]
        self.descs = descs
        self.nodal_bcs = get_default(nodal_bcs, {})

    def _append_region_faces(self, region, force_faces=False):
        dim = self.coors.shape[1]
        if (not force_faces) and (dim == 2): return

        cmesh = region.domain.cmesh
        for ig in region.igs:
            faces = region.get_facets(ig)
            if not faces.size: continue

            verts, offs = cmesh.get_incident(0, faces, cmesh.dim - 1,
                                             ret_offsets=True)
            n_fp = offs[1] - offs[0]
            verts.shape = (verts.shape[0] / n_fp, n_fp)

            self.descs.append('%d_%d' % (dim - 1, n_fp))
            self.conns.append(verts)

            mat_ids = nm.repeat(ig, verts.shape[0])
            self.mat_ids.append(mat_ids)

    def write(self, filename=None, io=None,
              coors=None, igs=None, out=None, float_format=None, **kwargs):
        """
        Write mesh + optional results in `out` to a file.

        Parameters
        ----------
        filename : str, optional
            The file name. If None, the mesh name is used instead.
        io : MeshIO instance or 'auto', optional
            Passing 'auto' respects the extension of `filename`.
        coors : array, optional
            The coordinates that can be used instead of the mesh coordinates.
        igs : array_like, optional
            Passing a list of group ids selects only those groups for writing.
        out : dict, optional
            The output data attached to the mesh vertices and/or cells.
        float_format : str, optional
            The format string used to print floats in case of a text file
            format.
        **kwargs : dict, optional
            Additional arguments that can be passed to the `MeshIO` instance.
        """
        if filename is None:
            filename = self.name + '.mesh'

        if io is None:
            io = self.io
            if io is None:
                io = 'auto'

        if io == 'auto':
            io = MeshIO.any_from_filename(filename)

        if coors is None:
            coors = self.coors

        if igs is None:
            igs = range(len(self.conns))

        aux_mesh = Mesh.from_data(self.name, coors, self.ngroups,
                                  self.conns, self.mat_ids, self.descs,
                                  igs=igs, nodal_bcs=self.nodal_bcs)
        io.set_float_format(float_format)
        io.write(filename, aux_mesh, out, **kwargs)

    def get_bounding_box(self):
        return nm.vstack((nm.amin(self.coors, 0), nm.amax(self.coors, 0)))

    def get_element_coors(self, ig=None):
        """
        Get the coordinates of vertices elements in group `ig`.

        Parameters
        ----------
        ig : int, optional
            The element group. If None, the coordinates for all groups
            are returned, filled with zeros at places of missing
            vertices, i.e. where elements having less then the full number
            of vertices (`n_ep_max`) are.

        Returns
        -------
        coors : array
            The coordinates in an array of shape `(n_el, n_ep_max, dim)`.
        """
        cc = self.coors
        n_ep_max = self.n_e_ps.max()

        coors = nm.empty((self.n_el, n_ep_max, self.dim), dtype=cc.dtype)
        for ig, conn in enumerate(self.conns):
            i1, i2 = self.el_offsets[ig], self.el_offsets[ig + 1]
            coors[i1:i2, :conn.shape[1], :] = cc[conn]

        return coors

    def localize(self, inod):
        """
        Strips nodes not in inod and remaps connectivities.
        Omits elements where remap[conn] contains -1...
        """
        n_nod = inod.shape[0]

        remap = nm.empty((self.n_nod,), dtype=nm.int32)
        remap.fill(-1)
        remap[inod] = nm.arange(n_nod, dtype=nm.int32)

        self.coors = self.coors[inod]
        self.ngroups = self.ngroups[inod]
        conns = []
        mat_ids = []
        used_vertices = nm.zeros((0,), dtype=nm.int32)
        for ig, conn in enumerate(self.conns):
            if conn.shape[0] == 0:
                continue

            aux = remap[conn]
            ii = nm.unique(nm.where(aux == -1)[0])
            ii = nm.setdiff1d(nm.arange(conn.shape[0], dtype=nm.int32), ii)
            cc = aux[ii]
            conns.append(cc)
            used_vertices = nm.r_[used_vertices, nm.unique(cc)]
            mat_ids.append(self.mat_ids[ig][ii])

        self.conns = conns
        self.mat_ids = mat_ids

        # Remove nodes not present in any cell.
        used_vertices = nm.unique(used_vertices)
        n_nod_new = used_vertices.shape[0]
        if n_nod_new < n_nod:
            remap = nm.empty((n_nod,), dtype=nm.int32)
            remap.fill(-1)
            remap[used_vertices] = nm.arange(n_nod_new, dtype=nm.int32)
            self.coors = self.coors[used_vertices]
            self.ngroups = self.ngroups[used_vertices]
            # Only renumber cells, no cells should be removed.
            for ig, conn in enumerate(self.conns):
                if conn.shape[0] == 0:
                    continue
                conn[:] = remap[conn]

        self._set_shape_info()

    def transform_coors(self, mtx_t, ref_coors=None):
        """
        Transform coordinates of the mesh by the given transformation matrix.

        Parameters
        ----------
        mtx_t : array
           The transformation matrix `T` (2D array). It is applied
           depending on its shape:

           - `(dim, dim): x = T * x`
           - `(dim, dim + 1): x = T[:, :-1] * x + T[:, -1]`
        ref_coors : array, optional
           Alternative coordinates to use for the transformation instead
           of the mesh coordinates, with the same shape as `self.coors`.
        """
        if ref_coors is None:
            ref_coors = self.coors

        if mtx_t.shape[1] > self.coors.shape[1]:
            self.coors[:] = nm.dot(ref_coors, mtx_t[:,:-1].T) + mtx_t[:,-1]
        else:
            self.coors[:] = nm.dot(ref_coors, mtx_t.T)

    def create_conn_graph(self, verbose=True):
        """
        Create a graph of mesh connectivity.

        Returns
        -------
        graph : csr_matrix
            The mesh connectivity graph as a SciPy CSR matrix.
        """
        from extmods.cmesh import create_mesh_graph

        shape = (self.n_nod, self.n_nod)
        output('graph shape:', shape, verbose=verbose)
        if nm.prod(shape) == 0:
            output('no graph (zero size)!', verbose=verbose)
            return None

        output('assembling mesh graph...', verbose=verbose)
        tt = time.clock()

        nnz, prow, icol = create_mesh_graph(shape[0], shape[1],
                                            len(self.conns),
                                            self.conns, self.conns)
        output('...done in %.2f s' % (time.clock() - tt), verbose=verbose)
        output('graph nonzeros: %d (%.2e%% fill)' \
               % (nnz, float(nnz) / nm.prod(shape)))

        data = nm.ones((nnz,), dtype=nm.bool)
        graph = sp.csr_matrix((data, icol, prow), shape)

        return graph

    def explode_groups(self, eps, return_emap=False):
        """
        Explode the mesh element groups by `eps`, i.e. split group
        interface nodes and shrink each group towards its centre by
        `eps`.

        Parameters
        ----------
        eps : float in `[0.0, 1.0]`
            The group shrinking factor.
        return_emap : bool, optional
            If True, also return the mapping against original mesh
            coordinates that result in the exploded mesh coordinates.
            The mapping can be used to map mesh vertex data to the
            exploded mesh vertices.

        Returns
        -------
        mesh : Mesh
            The new mesh with exploded groups.
        emap : spmatrix, optional
            The maping for exploding vertex values. Only provided if
            `return_emap` is True.
        """
        assert_(0.0 <= eps <= 1.0)

        remap = nm.empty((self.n_nod,), dtype=nm.int32)
        offset = 0

        if return_emap:
            rows, cols = [], []

        coors = []
        ngroups = []
        conns = []
        mat_ids = []
        descs = []
        for ig, conn in enumerate(self.conns):
            nodes = nm.unique(conn)
            group_coors = self.coors[nodes]
            n_nod = group_coors.shape[0]

            centre = group_coors.sum(axis=0) / float(n_nod)
            vectors = group_coors - centre[None, :]
            new_coors = centre + (vectors * eps)

            remap[nodes] = nm.arange(n_nod, dtype=nm.int32) + offset
            new_conn = remap[conn]

            coors.append(new_coors)
            ngroups.append(self.ngroups[nodes])

            conns.append(new_conn)
            mat_ids.append(self.mat_ids[ig])
            descs.append(self.descs[ig])

            offset += n_nod

            if return_emap:
                cols.append(nodes)
                rows.append(remap[nodes])

        coors = nm.concatenate(coors, axis=0)
        ngroups = nm.concatenate(ngroups, axis=0)

        mesh = Mesh.from_data('exploded_' + self.name,
                              coors, ngroups, conns, mat_ids, descs)

        if return_emap:
            rows = nm.concatenate(rows)
            cols = nm.concatenate(cols)
            data = nm.ones(rows.shape[0], dtype=nm.float64)

            emap = sp.coo_matrix((data, (rows, cols)),
                                 shape=(mesh.n_nod, self.n_nod))

            return mesh, emap

        else:
            return mesh

########NEW FILE########
__FILENAME__ = meshio
import sys
from copy import copy

import numpy as nm

from sfepy.base.base import (complex_types, dict_from_keys_init,
                             assert_, is_derived_class,
                             insert_static_method, output, get_default,
                             get_default_attr, Struct, basestr)
from sfepy.base.ioutils \
     import skip_read_line, read_token, read_array, read_list, pt
import os.path as op

supported_formats = {
    '.mesh' : 'medit',
    '.vtk'  : 'vtk',
    '.node' : 'tetgen',
    '.txt'  : 'comsol',
    '.h5'   : 'hdf5',
     # Order is important, avs_ucd does not guess -> it is the default.
    '.inp'  : ('abaqus', 'ansys_cdb', 'avs_ucd'),
    '.dat'  : 'ansys_cdb',
    '.hmascii'  : 'hmascii',
    '.mesh3d'   : 'mesh3d',
    '.bdf'  : 'nastran',
    '.neu'  : 'gambit',
    '.med'  : 'med',
    '.cdb'  : 'ansys_cdb',
}

# Map mesh formats to read and write capabilities.
# 'r' ... read mesh
# 'w' ... write mesh
# 'rn' ... read nodes for boundary conditions
# 'wn' ... write nodes for boundary conditions
supported_capabilities = {
    'medit' : ['r', 'w'],
    'vtk' : ['r', 'w'],
    'tetgen' : ['r'],
    'comsol' : ['r', 'w'],
    'hdf5' : ['r', 'w'],
    'abaqus' : ['r'],
    'avs_ucd' : ['r'],
    'hmascii' : ['r'],
    'mesh3d' : ['r'],
    'nastran' : ['r', 'w'],
    'gambit' : ['r', 'rn'],
    'med' : ['r'],
    'ansys_cdb' : ['r'],
}

supported_cell_types = {
    'medit' : ['line2', 'tri3', 'quad4', 'tetra4', 'hexa8'],
    'vtk' : ['line2', 'tri3', 'quad4', 'tetra4', 'hexa8'],
    'tetgen' : ['tetra4'],
    'comsol' : ['tri3', 'quad4', 'tetra4', 'hexa8'],
    'hdf5' : ['user'],
    'abaqus' : ['tri3', 'quad4', 'tetra4', 'hexa8'],
    'avs_ucd' : ['tetra4', 'hexa8'],
    'hmascii' : ['tri3', 'quad4', 'tetra4', 'hexa8'],
    'mesh3d' : ['tetra4', 'hexa8'],
    'nastran' : ['tri3', 'quad4', 'tetra4', 'hexa8'],
    'gambit' : ['tri3', 'quad4', 'tetra4', 'hexa8'],
    'med' : ['tri3', 'quad4', 'tetra4', 'hexa8'],
    'ansys_cdb' : ['tetra4', 'hexa8'],
    'function' : ['user'],
}

def output_writable_meshes():
    output('Supported writable mesh formats are:')
    for key, val in supported_capabilities.iteritems():
        if 'w' in val:
            output(key)

def sort_by_mat_id(conns_in):
    """
    Sort by mat_id within a group, preserve order.
    """
    conns = []
    mat_ids = []
    for ig, conn in enumerate(conns_in):
        if conn.shape[0] > 0:
            ii = nm.argsort(conn[:,-1], kind='mergesort')
            conn = conn[ii]

            conns.append(conn[:,:-1].copy())
            mat_ids.append(conn[:,-1].copy())
        else:
            conns.append([])
            mat_ids.append([])

    return conns, mat_ids

def sort_by_mat_id2(conns_in, mat_ids_in):
    """
    Sort by mat_id within a group, preserve order.
    """
    conns = []
    mat_ids = []
    for ig, conn in enumerate(conns_in):
        if conn.shape[0] > 0:
            mat_id = mat_ids_in[ig]
            ii = nm.argsort(mat_id, kind='mergesort')
            conns.append(conn[ii])
            mat_ids.append(mat_id[ii])
        else:
            conns.append([])
            mat_ids.append([])

    return conns, mat_ids

def split_by_mat_id(conns_in, mat_ids_in, descs_in):
    """
    Notes
    -----
    conns_in must be sorted by mat_id within a group!
    """
    conns = []
    mat_ids = []
    descs = []

    for ig, conn in enumerate(conns_in):
        one = nm.array([-1], nm.int32)
        aux = nm.concatenate((one, mat_ids_in[ig], one))
        ii = nm.where(aux[1:] != aux[:-1])[0]

        n_gr = len(ii) - 1;
        for igr in range(0, n_gr):
            conns.append(conn[ii[igr]:ii[igr+1],:].copy())
            mat_ids.append(mat_ids_in[ig][ii[igr]:ii[igr+1]])
            descs.append(descs_in[ig])

    return (conns, mat_ids, descs)


def write_bb(fd, array, dtype):

    fd.write('3 %d %d %d\n' % (array.shape[1], array.shape[0], dtype))
    format = ' '.join(['%.5e'] * array.shape[1] + ['\n'])

    for row in array:
        fd.write(format % tuple(row))

def join_conn_groups(conns, descs, mat_ids, concat=False):
    """Join groups of the same element type."""

    el = dict_from_keys_init(descs, list)
    for ig, desc in enumerate(descs):
        el[desc].append(ig)
    groups = [ii for ii in el.values() if ii]

    descs_out, conns_out, mat_ids_out = [], [], []
    for group in groups:
        n_ep = conns[group[0]].shape[1]

        conn = nm.zeros((0, n_ep), nm.int32)
        mat_id = nm.zeros((0,), nm.int32)
        for ig in group:
            conn = nm.concatenate((conn, conns[ig]))
            mat_id = nm.concatenate((mat_id, mat_ids[ig]))

        if concat:
            conn = nm.concatenate((conn, mat_id[:,nm.newaxis]), 1)
        else:
            mat_ids_out.append(mat_id)
        conns_out.append(conn)
        descs_out.append(descs[group[0]])

    if concat:
        return conns_out, descs_out
    else:
        return conns_out, descs_out, mat_ids_out

def convert_complex_output(out_in):
    """
    Convert complex values in the output dictionary `out_in` to pairs of
    real and imaginary parts.
    """
    out = {}
    for key, val in out_in.iteritems():

        if val.data.dtype in  complex_types:
            rval = copy(val)
            rval.data = val.data.real
            out['real(%s)' % key] = rval

            ival = copy(val)
            ival.data = val.data.imag
            out['imag(%s)' % key] = ival

        else:
            out[key] = val

    return out

class MeshIO(Struct):
    """
    The abstract class for importing and exporting meshes.

    Read the docstring of the Mesh() class. Basically all you need to do is to
    implement the read() method::

        def read(self, mesh, **kwargs):
            nodes = ...
            conns = ...
            mat_ids = ...
            descs = ...
            mesh._set_data(nodes, conns, mat_ids, descs)
            return mesh

    See the Mesh class' docstring how the nodes, conns, mat_ids and descs
    should look like. You just need to read them from your specific format from
    disk.

    To write a mesh to disk, just implement the write() method and use the
    information from the mesh instance (e.g. nodes, conns, mat_ids and descs)
    to construct your specific format.

    The methods read_dimension(), read_bounding_box() should be implemented in
    subclasses, as it is often possible to get that kind of information without
    reading the whole mesh file.

    Optionally, subclasses can implement read_data() to read also computation
    results. This concerns mainly the subclasses with implemented write()
    supporting the 'out' kwarg.

    The default implementation od read_last_step() just returns 0. It should be
    reimplemented in subclasses capable of storing several steps.
    """
    format = None
    call_msg = 'called an abstract MeshIO instance!'

    def __init__(self, filename, **kwargs):
        Struct.__init__(self, filename=filename, **kwargs)
        self.set_float_format()

    def get_filename_trunk(self):
        if isinstance(self.filename, file):
            trunk = 'from_descriptor'
        else:
            trunk = op.splitext(self.filename)[0]

        return trunk

    def read_dimension(self, ret_fd=False):
        raise ValueError(MeshIO.call_msg)

    def read_bounding_box(self, ret_fd=False, ret_dim=False):
        raise ValueError(MeshIO.call_msg)

    def read_last_step(self):
        """The default implementation: just return 0 as the last step."""
        return 0

    def read_times(self, filename=None):
        """
        Read true time step data from individual time steps.

        Returns
        -------
        steps : array
            The time steps.
        times : array
            The times of the time steps.
        nts : array
            The normalized times of the time steps, in [0, 1].

        Notes
        -----
        The default implementation returns empty arrays.
        """
        aux = nm.array([0.0], dtype=nm.float64)
        return aux.astype(nm.int32), aux, aux

    def read(self, mesh, omit_facets=False, **kwargs):
        raise ValueError(MeshIO.call_msg)

    def write(self, filename, mesh, **kwargs):
        raise ValueError(MeshIO.call_msg)

    def read_data(self, step, filename=None):
        raise ValueError(MeshIO.call_msg)

    def set_float_format(self, format=None):
        self.float_format = get_default(format, '%e')

    def get_vector_format(self, dim):
        return ' '.join([self.float_format] * dim)

class UserMeshIO(MeshIO):
    """
    Special MeshIO subclass that enables reading and writing a mesh using a
    user-supplied function.
    """
    format = 'function'

    def __init__(self, filename, **kwargs):
        assert_(hasattr(filename, '__call__'))
        self.function = filename

        MeshIO.__init__(self, filename='function:%s' % self.function.__name__,
                        **kwargs)

    def get_filename_trunk(self):
        return self.filename

    def read(self, mesh, *args, **kwargs):
        aux = self.function(mesh, mode='read')
        if aux is not None:
            mesh = aux

        self.filename = mesh.name

        return mesh

    def write(self, filename, mesh, *args, **kwargs):
        self.function(mesh, mode='write')

class MeditMeshIO(MeshIO):
    format = 'medit'

    def read_dimension(self, ret_fd=False):
        fd = open(self.filename, 'r')
        while 1:
            line = skip_read_line(fd, no_eof=True).split()
            if line[0] == 'Dimension':
                if len(line) == 2:
                    dim = int(line[1])
                else:
                    dim = int(fd.readline())
                break

        if ret_fd:
            return dim, fd
        else:
            fd.close()
            return dim

    def read_bounding_box(self, ret_fd=False, ret_dim=False):
        fd = open(self.filename, 'r')

        dim, fd  = self.read_dimension(ret_fd=True)

        while 1:
            line = skip_read_line(fd, no_eof=True).split()
            if line[0] == 'Vertices':
                num = int(read_token(fd))
                nod = read_array(fd, num, dim + 1, nm.float64)
                break

        bbox = nm.vstack((nm.amin(nod[:,:dim], 0),
                          nm.amax(nod[:,:dim], 0)))

        if ret_dim:
            if ret_fd:
                return bbox, dim, fd
            else:
                fd.close()
                return bbox, dim
        else:
            if ret_fd:
                return bbox, fd
            else:
                fd.close()
                return bbox


    def read(self, mesh, omit_facets=False, **kwargs):
        dim, fd  = self.read_dimension(ret_fd=True)

        conns_in = []
        descs = []

        def _read_cells(dimension, size, has_id=True):
            num = int(read_token(fd))
            data = read_array(fd, num, size + 1 * has_id, nm.int32)
            if omit_facets and (dimension < dim): return

            data[:, :-1] -= 1

            conns_in.append(data)
            descs.append('%i_%i' % (dimension, size))

        while 1:
            line = skip_read_line(fd).split()
            if not line:
                break

            ls = line[0]
            if (ls == 'Vertices'):
                num = int(read_token(fd))
                nod = read_array(fd, num, dim + 1, nm.float64)

            elif (ls == 'Corners'):
                _read_cells(1, 1, False)

            elif (ls == 'Edges'):
                _read_cells(1, 2)

            elif (ls == 'Tetrahedra'):
                _read_cells(3, 4)

            elif (ls == 'Hexahedra'):
                _read_cells(3, 8)

            elif (ls == 'Triangles'):
                _read_cells(2, 3)

            elif (ls == 'Quadrilaterals'):
                _read_cells(2, 4)

            elif ls == 'End':
                break

            elif line[0] == '#':
                continue

            else:
                output('skipping unknown entity: %s' % line)
                continue

        fd.close()

        conns_in, mat_ids = sort_by_mat_id(conns_in)

        # Detect wedges and pyramides -> separate groups.
        if ('3_8' in descs):
            ic = descs.index('3_8')

            conn_in = conns_in.pop(ic)
            mat_id_in = mat_ids.pop(ic)

            flag = nm.zeros((conn_in.shape[0],), nm.int32)
            for ii, el in enumerate(conn_in):
                if (el[4] == el[5]):
                    if (el[5] == el[6]):
                        flag[ii] = 2
                    else:
                        flag[ii] = 1

            conn = []
            desc = []
            mat_id = []

            ib = nm.where(flag == 0)[0]
            if (len(ib) > 0):
                conn.append(conn_in[ib])
                mat_id.append(mat_id_in[ib])
                desc.append('3_8')

            iw = nm.where(flag == 1)[0]
            if (len(iw) > 0):
                ar = nm.array([0,1,2,3,4,6], nm.int32)
                conn.append(conn_in[iw[:, None], ar])
                mat_id.append(mat_id_in[iw])
                desc.append('3_6')

            ip = nm.where(flag == 2)[0]
            if (len(ip) > 0):
                ar = nm.array([0,1,2,3,4], nm.int32)
                conn.append(conn_in[ip[:, None], ar])
                mat_id.append(mat_id_in[ip])
                desc.append('3_5')

            conns_in[ic:ic] = conn
            mat_ids[ic:ic] = mat_id
            del(descs[ic])
            descs[ic:ic] = desc

        conns, mat_ids, descs = split_by_mat_id(conns_in, mat_ids, descs)
        mesh._set_data(nod[:,:-1], nod[:,-1], conns, mat_ids, descs)

        return mesh

    def write(self, filename, mesh, out=None, **kwargs):
        fd = open(filename, 'w')

        coors = mesh.coors
        conns, desc = join_conn_groups(mesh.conns, mesh.descs,
                                       mesh.mat_ids, concat=True)

        n_nod, dim = coors.shape

        fd.write("MeshVersionFormatted 1\nDimension %d\n" % dim)

        fd.write("Vertices\n%d\n" % n_nod)
        format = self.get_vector_format(dim) + ' %d\n'
        for ii in range(n_nod):
            nn = tuple(coors[ii]) + (mesh.ngroups[ii],)
            fd.write(format % tuple(nn))

        for ig, conn in enumerate(conns):
            if (desc[ig] == "1_1"):
                fd.write("Corners\n%d\n" % conn.shape[0])
                for ii in range(conn.shape[0]):
                    nn = conn[ii] + 1
                    fd.write("%d\n"
                             % nn[0])
            elif (desc[ig] == "1_2"):
                fd.write("Edges\n%d\n" % conn.shape[0])
                for ii in range(conn.shape[0]):
                    nn = conn[ii] + 1
                    fd.write("%d %d %d\n"
                             % (nn[0], nn[1], nn[2] - 1))
            elif (desc[ig] == "2_4"):
                fd.write("Quadrilaterals\n%d\n" % conn.shape[0])
                for ii in range(conn.shape[0]):
                    nn = conn[ii] + 1
                    fd.write("%d %d %d %d %d\n"
                             % (nn[0], nn[1], nn[2], nn[3], nn[4] - 1))
            elif (desc[ig] == "2_3"):
                fd.write("Triangles\n%d\n" % conn.shape[0])
                for ii in range(conn.shape[0]):
                    nn = conn[ii] + 1
                    fd.write("%d %d %d %d\n" % (nn[0], nn[1], nn[2], nn[3] - 1))
            elif (desc[ig] == "3_4"):
                fd.write("Tetrahedra\n%d\n" % conn.shape[0])
                for ii in range(conn.shape[0]):
                    nn = conn[ii] + 1
                    fd.write("%d %d %d %d %d\n"
                             % (nn[0], nn[1], nn[2], nn[3], nn[4] - 1))
            elif (desc[ig] == "3_8"):
                fd.write("Hexahedra\n%d\n" % conn.shape[0])
                for ii in range(conn.shape[0]):
                    nn = conn[ii] + 1
                    fd.write("%d %d %d %d %d %d %d %d %d\n"
                             % (nn[0], nn[1], nn[2], nn[3], nn[4], nn[5],
                                nn[6], nn[7], nn[8] - 1))
            else:
                raise ValueError('unknown element type! (%s)' % desc[ig])

        fd.close()

        if out is not None:
            for key, val in out.iteritems():
                raise NotImplementedError


vtk_header = r"""x vtk DataFile Version 2.0
step %d time %e normalized time %e, generated by %s
ASCII
DATASET UNSTRUCTURED_GRID
"""
vtk_cell_types = {'1_1' : 1, '1_2' : 3, '2_2' : 3, '3_2' : 3,
                  '2_3' : 5, '2_4' : 9, '3_4' : 10, '3_8' : 12}
vtk_dims = {1 : 1, 3 : 1, 5 : 2, 9 : 2, 10 : 3, 12 : 3}
vtk_inverse_cell_types = {(3, 2) : '1_2', (5, 2) : '2_3',
                          (8, 2) : '2_4', (9, 2) : '2_4',
                          (3, 3) : '1_2', (10, 3) : '3_4',
                          (11, 3) : '3_8', (12, 3) : '3_8' }
vtk_remap = {8 : nm.array([0, 1, 3, 2], dtype=nm.int32),
             11 : nm.array([0, 1, 3, 2, 4, 5, 7, 6], dtype=nm.int32)}
vtk_remap_keys = vtk_remap.keys()

class VTKMeshIO(MeshIO):
    format = 'vtk'

    def read_coors(self, ret_fd=False):
        fd = open(self.filename, 'r')
        while 1:
            line = skip_read_line(fd, no_eof=True).split()
            if line[0] == 'POINTS':
                n_nod = int(line[1])
                coors = read_array(fd, n_nod, 3, nm.float64)
                break

        if ret_fd:
            return coors, fd
        else:
            fd.close()
            return coors

    def get_dimension(self, coors):
        dz = nm.diff(coors[:,2])
        if nm.allclose(dz, 0.0):
            dim = 2
        else:
            dim = 3
        return dim

    def read_dimension(self, ret_fd=False):
        coors, fd = self.read_coors(ret_fd=True)
        dim = self.get_dimension(coors)
        if ret_fd:
            return dim, fd
        else:
            fd.close()
            return dim

    def read_bounding_box(self, ret_fd=False, ret_dim=False):
        coors, fd = self.read_coors(ret_fd=True)
        dim = self.get_dimension(coors)

        bbox = nm.vstack((nm.amin(coors[:,:dim], 0),
                          nm.amax(coors[:,:dim], 0)))

        if ret_dim:
            if ret_fd:
                return bbox, dim, fd
            else:
                fd.close()
                return bbox, dim
        else:
            if ret_fd:
                return bbox, fd
            else:
                fd.close()
                return bbox

    def read(self, mesh, **kwargs):
        fd = open(self.filename, 'r')
        mode = 'header'
        mode_status = 0
        coors = conns = desc = mat_id = node_grps = None
        finished = 0
        while 1:
            line = skip_read_line(fd)
            if not line:
                break

            if mode == 'header':
                if mode_status == 0:
                    if line.strip() == 'ASCII':
                        mode_status = 1
                elif mode_status == 1:
                    if line.strip() == 'DATASET UNSTRUCTURED_GRID':
                        mode_status = 0
                        mode = 'points'

            elif mode == 'points':
                line = line.split()
                if line[0] == 'POINTS':
                    n_nod = int(line[1])
                    coors = read_array(fd, n_nod, 3, nm.float64)
                    mode = 'cells'

            elif mode == 'cells':
                line = line.split()
                if line[0] == 'CELLS':
                    n_el, n_val = map(int, line[1:3])
                    raw_conn = read_list(fd, n_val, int)
                    mode = 'cell_types'

            elif mode == 'cell_types':
                line = line.split()
                if line[0] == 'CELL_TYPES':
                    assert_(int(line[1]) == n_el)
                    cell_types = read_array(fd, n_el, 1, nm.int32)
                    mode = 'cp_data'

            elif mode == 'cp_data':
                line = line.split()
                if line[0] == 'CELL_DATA':
                    assert_(int(line[1]) == n_el)
                    mode_status = 1
                    mode = 'mat_id'
                elif line[0] == 'POINT_DATA':
                    assert_(int(line[1]) == n_nod)
                    mode_status = 1
                    mode = 'node_groups'

            elif mode == 'mat_id':
                if mode_status == 1:
                    if 'SCALARS mat_id int' in line.strip():
                        mode_status = 2
                elif mode_status == 2:
                    if line.strip() == 'LOOKUP_TABLE default':
                        mat_id = read_list(fd, n_el, int)
                        mode_status = 0
                        mode = 'cp_data'
                        finished += 1

            elif mode == 'node_groups':
                if mode_status == 1:
                    if 'SCALARS node_groups int' in line.strip():
                        mode_status = 2
                elif mode_status == 2:
                    if line.strip() == 'LOOKUP_TABLE default':
                        node_grps = read_list(fd, n_nod, int)
                        mode_status = 0
                        mode = 'cp_data'
                        finished += 1

            elif finished >= 2:
                break
        fd.close()

        if mat_id is None:
            mat_id = [[0]] * n_el
        else:
            if len(mat_id) < n_el:
                mat_id = [[ii] for jj in mat_id for ii in jj]

        if node_grps is None:
            node_grps = [0] * n_nod
        else:
            if len(node_grps) < n_nod:
                node_grps = [ii for jj in node_grps for ii in jj]

        dim = self.get_dimension(coors)
        if dim == 2:
            coors = coors[:,:2]
        coors = nm.ascontiguousarray(coors)

        cell_types = cell_types.squeeze()

        dconns = {}
        for iel, row in enumerate(raw_conn):
            ct = cell_types[iel]
            key = (ct, dim)
            if key not in vtk_inverse_cell_types:
                continue
            ct = vtk_inverse_cell_types[key]
            dconns.setdefault(key, []).append(row[1:] + mat_id[iel])

        desc = []
        conns = []
        for key, conn in dconns.iteritems():
            ct = key[0]
            sct = vtk_inverse_cell_types[key]
            desc.append(sct)

            aconn = nm.array(conn, dtype=nm.int32)
            if ct in vtk_remap_keys: # Remap pixels and voxels.
                aconn[:, :-1] = aconn[:, vtk_remap[ct]]

            conns.append(aconn)

        conns_in, mat_ids = sort_by_mat_id(conns)
        conns, mat_ids, descs = split_by_mat_id(conns_in, mat_ids, desc)

        mesh._set_data(coors, node_grps, conns, mat_ids, descs)

        return mesh

    def write(self, filename, mesh, out=None, ts=None, **kwargs):
        def _reshape_tensors(data, dim, sym, nc):
            if dim == 3:
                if nc == sym:
                    aux = data[:, [0,3,4,3,1,5,4,5,2]]
                elif nc == (dim * dim):
                    aux = data[:, [0,3,4,6,1,5,7,8,2]]
                else:
                    aux = data.reshape((data.shape[0], dim*dim))

            else:
                zz = nm.zeros((data.shape[0], 1), dtype=nm.float64)
                if nc == sym:
                    aux = nm.c_[data[:,[0,2]], zz, data[:,[2,1]],
                                zz, zz, zz, zz]
                elif nc == (dim * dim):
                    aux = nm.c_[data[:,[0,2]], zz, data[:,[3,1]],
                                zz, zz, zz, zz]
                else:
                    aux = nm.c_[data[:,0,[0,1]], zz, data[:,1,[0,1]],
                                zz, zz, zz, zz]

            return aux

        def _write_tensors(data):
            format = self.get_vector_format(3)
            format = '\n'.join([format] * 3) + '\n\n'
            for row in aux:
                fd.write(format % tuple(row))

        if ts is None:
            step, time, nt  = 0, 0.0, 0.0
        else:
            step, time, nt = ts.step, ts.time, ts.nt

        fd = open(filename, 'w')
        fd.write(vtk_header % (step, time, nt, op.basename(sys.argv[0])))

        n_nod, dim = mesh.coors.shape
        sym = dim * (dim + 1) / 2

        fd.write('\nPOINTS %d float\n' % n_nod)

        aux = mesh.coors
        if dim == 2:
            aux = nm.hstack((aux, nm.zeros((aux.shape[0], 1), dtype=aux.dtype)))

        format = self.get_vector_format(3) + '\n'
        for row in aux:
            fd.write(format % tuple(row))

        n_el, n_els, n_e_ps = mesh.n_el, mesh.n_els, mesh.n_e_ps
        total_size = nm.dot(n_els, n_e_ps + 1)
        fd.write('\nCELLS %d %d\n' % (n_el, total_size))

        ct = []
        for ig, conn in enumerate(mesh.conns):
            nn = n_e_ps[ig] + 1
            ct += [vtk_cell_types[mesh.descs[ig]]] * n_els[ig]
            format = ' '.join(['%d'] * nn + ['\n'])

            for row in conn:
                fd.write(format % ((nn-1,) + tuple(row)))

        fd.write('\nCELL_TYPES %d\n' % n_el)
        fd.write(''.join(['%d\n' % ii for ii in ct]))

        fd.write('\nPOINT_DATA %d\n' % n_nod)

        # node groups
        fd.write('\nSCALARS node_groups int 1\nLOOKUP_TABLE default\n')
        fd.write(''.join(['%d\n' % ii for ii in mesh.ngroups]))

        if out is not None:
            point_keys = [key for key, val in out.iteritems()
                          if val.mode == 'vertex']
        else:
            point_keys = {}

        for key in point_keys:
            val = out[key]
            nr, nc = val.data.shape

            if nc == 1:
                fd.write('\nSCALARS %s float %d\n' % (key, nc))
                fd.write('LOOKUP_TABLE default\n')

                format = self.float_format + '\n'
                for row in val.data:
                    fd.write(format % row)

            elif nc == dim:
                fd.write('\nVECTORS %s float\n' % key)
                if dim == 2:
                    aux = nm.hstack((val.data,
                                     nm.zeros((nr, 1), dtype=nm.float64)))
                else:
                    aux = val.data

                format = self.get_vector_format(3) + '\n'
                for row in aux:
                    fd.write(format % tuple(row))

            elif (nc == sym) or (nc == (dim * dim)):
                fd.write('\nTENSORS %s float\n' % key)
                aux = _reshape_tensors(val.data, dim, sym, nc)
                _write_tensors(aux)

            else:
                raise NotImplementedError, nc

        if out is not None:
            cell_keys = [key for key, val in out.iteritems()
                         if val.mode == 'cell']
        else:
            cell_keys = {}

        fd.write('\nCELL_DATA %d\n' % n_el)

        # cells - mat_id
        fd.write('SCALARS mat_id int 1\nLOOKUP_TABLE default\n')
        aux = nm.hstack(mesh.mat_ids).tolist()
        fd.write(''.join(['%d\n' % ii for ii in aux]))

        for key in cell_keys:
            val = out[key]
            ne, aux, nr, nc = val.data.shape

            if (nr == 1) and (nc == 1):
                fd.write('\nSCALARS %s float %d\n' % (key, nc))
                fd.write('LOOKUP_TABLE default\n')
                format = self.float_format + '\n'
                aux = val.data.squeeze()
                if len(aux.shape) == 0:
                    fd.write(format % aux)
                else:
                    for row in aux:
                        fd.write(format % row)

            elif (nr == dim) and (nc == 1):
                fd.write('\nVECTORS %s float\n' % key)
                if dim == 2:
                    aux = nm.hstack((val.data.squeeze(),
                                     nm.zeros((ne, 1), dtype=nm.float64)))
                else:
                    aux = val.data

                format = self.get_vector_format(3) + '\n'
                for row in aux:
                    fd.write(format % tuple(row.squeeze()))

            elif (((nr == sym) or (nr == (dim * dim))) and (nc == 1)) \
                     or ((nr == dim) and (nc == dim)):
                fd.write('\nTENSORS %s float\n' % key)
                data = val.data.squeeze()
                aux = _reshape_tensors(data, dim, sym, nr)
                _write_tensors(aux)

            else:
                raise NotImplementedError, (nr, nc)

        fd.close()

        # Mark the write finished.
        fd = open(filename, 'r+')
        fd.write('#')
        fd.close()

    def read_data(self, step, filename=None):
        """Point data only!"""
        filename = get_default(filename, self.filename)

        out = {}

        fd = open(self.filename, 'r')
        while 1:
            line = skip_read_line(fd, no_eof=True).split()
            if line[0] == 'POINT_DATA':
                break

        n_nod = int(line[1])

        while 1:
            line = skip_read_line(fd)
            if not line:
                break

            line = line.split()

            if line[0] == 'SCALARS':
                name, dtype, nc = line[1:]
                assert_(int(nc) == 1)
                fd.readline() # skip lookup table line

                data = nm.zeros((n_nod,), dtype=nm.float64)
                ii = 0
                while ii < n_nod:
                    data[ii] = float(fd.readline())
                    ii += 1

                out[name] = Struct(name=name, mode='vertex', data=data,
                                   dofs=None)

            elif line[0] == 'VECTORS':
                name, dtype = line[1:]
                data = []
                ii = 0
                while ii < n_nod:
                    data.append([float(val) for val in fd.readline().split()])
                    ii += 1

                out[name] = Struct(name=name, mode='vertex',
                                   data=nm.array(data, dtype=nm.float64),
                                   dofs=None)

            elif line[0] == 'CELL_DATA':
                break

            line = fd.readline()

        fd.close()

        return out

class TetgenMeshIO(MeshIO):
    format = "tetgen"

    def read(self, mesh, **kwargs):
        import os
        fname = os.path.splitext(self.filename)[0]
        nodes = self.getnodes(fname+".node")
        etype, elements, regions = self.getele(fname+".ele")
        descs = []
        conns = []
        mat_ids = []
        elements = nm.array(elements, dtype=nm.int32) - 1
        for key, value in regions.iteritems():
            descs.append(etype)
            mat_ids.append(nm.ones_like(value) * key)
            conns.append(elements[nm.array(value)-1].copy())

        mesh._set_data(nodes, None, conns, mat_ids, descs)
        return mesh

    @staticmethod
    def getnodes(fnods):
        """
        Reads t.1.nodes, returns a list of nodes.

        Example:

        >>> self.getnodes("t.1.node")
        [(0.0, 0.0, 0.0), (4.0, 0.0, 0.0), (0.0, 4.0, 0.0), (-4.0, 0.0, 0.0),
        (0.0, 0.0, 4.0), (0.0, -4.0, 0.0), (0.0, -0.0, -4.0), (-2.0, 0.0,
        -2.0), (-2.0, 2.0, 0.0), (0.0, 2.0, -2.0), (0.0, -2.0, -2.0), (2.0,
        0.0, -2.0), (2.0, 2.0, 0.0), ... ]

        """
        f = open(fnods)
        l = [int(x) for x in f.readline().split()]
        npoints, dim, nattrib, nbound = l
        if dim == 2:
            ndapp = [0.0]
        else:
            ndapp = []

        nodes = []
        for line in f:
            if line[0] == "#": continue
            l = [float(x) for x in line.split()]
            l = l[:(dim + 1)]
            assert_(int(l[0]) == len(nodes)+1)
            l = l[1:]
            nodes.append(tuple(l + ndapp))
        assert_(npoints == len(nodes))
        return nodes

    @staticmethod
    def getele(fele):
        """
        Reads t.1.ele, returns a list of elements.

        Example:

        >>> elements, regions = self.getele("t.1.ele")
        >>> elements
        [(20, 154, 122, 258), (86, 186, 134, 238), (15, 309, 170, 310), (146,
        229, 145, 285), (206, 207, 125, 211), (99, 193, 39, 194), (185, 197,
        158, 225), (53, 76, 74, 6), (19, 138, 129, 313), (23, 60, 47, 96),
        (119, 321, 1, 329), (188, 296, 122, 322), (30, 255, 177, 256), ...]
        >>> regions
        {100: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,
        19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,
        37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,
        55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 7, ...],
        ...}

        """
        f = file(fele)
        l = [int(x) for x in f.readline().split()]
        ntetra,nnod,nattrib = l
        #we have either linear or quadratic tetrahedra:
        elem = None
        if nnod in [4,10]:
            elem = '3_4'
            linear = (nnod == 4)
        if nnod in [3, 7]:
            elem = '2_3'
            linear = (nnod == 3)
        if elem is None or not linear:
            raise ValueError("Only linear triangle and tetrahedra reader"
                             " is implemented")

        els = []
        regions = {}
        for line in f:
            if line[0] == "#": continue
            l = [int(x) for x in line.split()]
            if elem == '2_3':
                assert_((len(l) - 1 - nattrib) == 3)
                els.append((l[1],l[2],l[3]))
            if elem == '3_4':
                assert_((len(l) - 1 - nattrib) == 4)
                els.append((l[1],l[2],l[3],l[4]))
            if nattrib == 1:
                regionnum = l[-1]
            else:
                regionnum = 1

            if regionnum == 0:
                msg = "see %s, element # %d\n"%(fele,l[0])
                msg += "there are elements not belonging to any physical entity"
                raise ValueError(msg)

            if regions.has_key(regionnum):
                regions[regionnum].append(l[0])
            else:
                regions[regionnum]=[l[0]]
            assert_(l[0] == len(els))

        return elem, els, regions

    def write(self, filename, mesh, out=None, **kwargs):
        raise NotImplementedError

    def read_dimension(self):
        # TetGen only supports 3D mesh
        return 3

    def read_bounding_box(self):
        raise NotImplementedError

class ComsolMeshIO(MeshIO):
    format = 'comsol'

    def _read_commented_int(self):
        return int(skip_read_line(self.fd).split('#')[0])

    def _skip_comment(self):
        read_token(self.fd)
        self.fd.readline()

    def read(self, mesh, **kwargs):

        self.fd = fd = open(self.filename, 'r')
        mode = 'header'

        coors = conns = None
        while 1:
            if mode == 'header':
                line = skip_read_line(fd)

                n_tags = self._read_commented_int()
                for ii in xrange(n_tags):
                    skip_read_line(fd)
                n_types = self._read_commented_int()
                for ii in xrange(n_types):
                    skip_read_line(fd)

                skip_read_line(fd)
                assert_(skip_read_line(fd).split()[1] == 'Mesh')
                skip_read_line(fd)
                dim = self._read_commented_int()
                assert_((dim == 2) or (dim == 3))
                n_nod = self._read_commented_int()
                i0 = self._read_commented_int()
                mode = 'points'

            elif mode == 'points':
                self._skip_comment()
                coors = read_array(fd, n_nod, dim, nm.float64)
                mode = 'cells'

            elif mode == 'cells':

                n_types = self._read_commented_int()
                conns = []
                descs = []
                mat_ids = []
                for it in xrange(n_types):
                    t_name = skip_read_line(fd).split()[1]
                    n_ep = self._read_commented_int()
                    n_el = self._read_commented_int()

                    self._skip_comment()
                    aux = read_array(fd, n_el, n_ep, nm.int32)
                    if t_name == 'tri':
                        conns.append(aux)
                        descs.append('2_3')
                        is_conn = True
                    elif t_name == 'quad':
                        # Rearrange element node order to match SfePy.
                        aux = aux[:,(0,1,3,2)]
                        conns.append(aux)
                        descs.append('2_4')
                        is_conn = True
                    elif t_name == 'hex':
                        # Rearrange element node order to match SfePy.
                        aux = aux[:,(0,1,3,2,4,5,7,6)]
                        conns.append(aux)
                        descs.append('3_8')
                        is_conn = True
                    elif t_name == 'tet':
                        conns.append(aux)
                        descs.append('3_4')
                        is_conn = True
                    else:
                        is_conn = False

                    # Skip parameters.
                    n_pv = self._read_commented_int()
                    n_par = self._read_commented_int()
                    for ii in xrange(n_par):
                        skip_read_line(fd)

                    n_domain = self._read_commented_int()
                    assert_(n_domain == n_el)
                    if is_conn:
                        self._skip_comment()
                        mat_id = read_array(fd, n_domain, 1, nm.int32)
                        mat_ids.append(mat_id)
                    else:
                        for ii in xrange(n_domain):
                            skip_read_line(fd)

                    # Skip up/down pairs.
                    n_ud = self._read_commented_int()
                    for ii in xrange(n_ud):
                        skip_read_line(fd)
                break

        fd.close()
        self.fd = None

        conns2 = []
        for ii, conn in enumerate(conns):
            conns2.append(nm.c_[conn, mat_ids[ii]])

        conns_in, mat_ids = sort_by_mat_id(conns2)
        conns, mat_ids, descs = split_by_mat_id(conns_in, mat_ids, descs)
        mesh._set_data(coors, None, conns, mat_ids, descs)

        return mesh

    def write(self, filename, mesh, out=None, **kwargs):

        def write_elements(fd, ig, conn, mat_ids, type_name,
                           npe, format, norder, nm_params):
            fd.write("# Type #%d\n\n" % ig)
            fd.write("%s # type name\n\n\n" % type_name)
            fd.write("%d # number of nodes per element\n" % npe)
            fd.write("%d # number of elements\n" % conn.shape[0])
            fd.write("# Elements\n")
            for ii in range(conn.shape[0]):
                nn = conn[ii] # Zero based
                fd.write(format % tuple(nn[norder]))
            fd.write("\n%d # number of parameter values per element\n"
                     % nm_params)
            # Top level always 0?
            fd.write("0 # number of parameters\n")
            fd.write("# Parameters\n\n")
            fd.write("%d # number of domains\n"
                     % sum([mi.shape[0] for mi in mat_ids]))
            fd.write("# Domains\n")
            for mi in mat_ids:
                # Domains in comsol have to be > 0
                if (mi <= 0).any():
                    mi += mi.min() + 1
                for dom in mi:
                    fd.write("%d\n" % abs(dom))
            fd.write("\n0 # number of up/down pairs\n")
            fd.write("# Up/down\n")

        fd = open(filename, 'w')

        coors = mesh.coors
        conns, desc, mat_ids  = join_conn_groups(mesh.conns, mesh.descs,
                                                 mesh.mat_ids)

        n_nod, dim = coors.shape

        # Header
        fd.write("# Created by SfePy\n\n\n")
        fd.write("# Major & minor version\n")
        fd.write("0 1\n")
        fd.write("1 # number of tags\n")
        fd.write("# Tags\n")
        fd.write("2 m1\n")
        fd.write("1 # number of types\n")
        fd.write("# Types\n")
        fd.write("3 obj\n\n")

        # Record
        fd.write("# --------- Object 0 ----------\n\n")
        fd.write("0 0 1\n") # version unused serializable
        fd.write("4 Mesh # class\n")
        fd.write("1 # version\n")
        fd.write("%d # sdim\n" % dim)
        fd.write("%d # number of mesh points\n" % n_nod)
        fd.write("0 # lowest mesh point index\n\n") # Always zero in SfePy

        fd.write("# Mesh point coordinates\n")

        format = self.get_vector_format(dim) + '\n'
        for ii in range(n_nod):
            nn = tuple(coors[ii])
            fd.write(format % tuple(nn))

        fd.write("\n%d # number of element types\n\n\n" % len(conns))

        for ig, conn in enumerate(conns):
            if (desc[ig] == "2_4"):
                write_elements(fd, ig, conn, mat_ids,
                               "4 quad", 4, "%d %d %d %d\n", [0, 1, 3, 2], 8)

            elif (desc[ig] == "2_3"):
                # TODO: Verify number of parameters for tri element
                write_elements(fd, ig, conn, mat_ids,
                               "3 tri", 3, "%d %d %d\n", [0, 1, 2], 4)

            elif (desc[ig] == "3_4"):
                # TODO: Verify number of parameters for tet element
                write_elements(fd, ig, conn, mat_ids,
                               "3 tet", 4, "%d %d %d %d\n", [0, 1, 2, 3], 16)

            elif (desc[ig] == "3_8"):
                write_elements(fd, ig, conn, mat_ids,
                               "3 hex", 8, "%d %d %d %d %d %d %d %d\n",
                               [0, 1, 3, 2, 4, 5, 7, 6], 24)

            else:
                raise ValueError('unknown element type! (%s)' % desc[ig])

        fd.close()

        if out is not None:
            for key, val in out.iteritems():
                raise NotImplementedError

class HDF5MeshIO(MeshIO):
    format = "hdf5"

    import string
    _all = ''.join(map(chr, range(256)))
    _letters = string.letters + string.digits + '_'
    _rubbish = ''.join([ch for ch in set(_all) - set(_letters)])
    _tr = string.maketrans(_rubbish, '_' * len(_rubbish))

    def read_bounding_box(self, ret_fd=False, ret_dim=False):
        fd = pt.openFile(self.filename, mode="r")

        mesh_group = fd.root.mesh

        coors = mesh_group.coors.read()
        bbox = nm.vstack((nm.amin(coors, 0),
                          nm.amax(coors, 0)))

        if ret_dim:
            dim = coors.shape[1]

            if ret_fd:
                return bbox, dim, fd

            else:
                fd.close()
                return bbox, dim

        else:
            if ret_fd:
                return bbox, fd

            else:
                fd.close()
                return bbox

    def read(self, mesh, **kwargs):
        fd = pt.openFile(self.filename, mode="r")

        mesh_group = fd.root.mesh

        mesh.name = mesh_group.name.read()
        coors = mesh_group.coors.read()
        ngroups = mesh_group.ngroups.read()

        n_gr = mesh_group.n_gr.read()

        conns = []
        descs = []
        mat_ids = []
        for ig in xrange(n_gr):
            gr_name = 'group%d' % ig
            group = mesh_group._f_getChild(gr_name)
            conns.append(group.conn.read())
            mat_ids.append(group.mat_id.read())
            descs.append(group.desc.read())

        nodal_bcs = {}
        try:
            node_sets_groups = mesh_group.node_sets

        except:
            pass

        else:
            for group in node_sets_groups:
                key = group.key.read()
                nods = group.nods.read()
                nodal_bcs[key] = nods

        fd.close()
        mesh._set_data(coors, ngroups, conns, mat_ids, descs,
                       nodal_bcs=nodal_bcs)

        return mesh

    def write(self, filename, mesh, out=None, ts=None, **kwargs):
        from time import asctime

        if pt is None:
            raise ValueError('pytables not imported!')

        step = get_default_attr(ts, 'step', 0)
        if step == 0:
            # A new file.
            fd = pt.openFile(filename, mode="w",
                             title="SfePy output file")

            mesh_group = fd.createGroup('/', 'mesh', 'mesh')

            fd.createArray(mesh_group, 'name', mesh.name, 'name')
            fd.createArray(mesh_group, 'coors', mesh.coors, 'coors')
            fd.createArray(mesh_group, 'ngroups', mesh.ngroups, 'ngroups')
            fd.createArray(mesh_group, 'n_gr', len(mesh.conns), 'n_gr')
            for ig, conn in enumerate(mesh.conns):
                conn_group = fd.createGroup(mesh_group, 'group%d' % ig,
                                            'connectivity group')
                fd.createArray(conn_group, 'conn', conn, 'connectivity')
                fd.createArray(conn_group, 'mat_id', mesh.mat_ids[ig],
                               'material id')
                fd.createArray(conn_group, 'desc', mesh.descs[ig],
                               'element Type')

            node_sets_groups = fd.createGroup(mesh_group, 'node_sets',
                                             'node sets groups')
            ii = 0
            for key, nods in mesh.nodal_bcs.iteritems():
                group = fd.createGroup(node_sets_groups, 'group%d' % ii,
                                       'node sets group')
                fd.createArray(group, 'key', key, 'key')
                fd.createArray(group, 'nods', nods, 'nods')
                ii += 1

            if ts is not None:
                ts_group = fd.createGroup('/', 'ts', 'time stepper')
                fd.createArray(ts_group, 't0', ts.t0, 'initial time')
                fd.createArray(ts_group, 't1', ts.t1, 'final time' )
                fd.createArray(ts_group, 'dt', ts.dt, 'time step')
                fd.createArray(ts_group, 'n_step', ts.n_step, 'n_step')

            tstat_group = fd.createGroup('/', 'tstat', 'global time statistics')
            fd.createArray(tstat_group, 'created', asctime(),
                           'file creation time')
            fd.createArray(tstat_group, 'finished', '.' * 24,
                           'file closing time')

            fd.createArray(fd.root, 'last_step', nm.array([0], dtype=nm.int32),
                           'last saved step')

            fd.close()

        if out is not None:
            if ts is None:
                step, time, nt  = 0, 0.0, 0.0
            else:
                step, time, nt = ts.step, ts.time, ts.nt

            # Existing file.
            fd = pt.openFile(filename, mode="r+")

            step_group = fd.createGroup('/', 'step%d' % step, 'time step data')

            ts_group = fd.createGroup(step_group, 'ts', 'time stepper')
            fd.createArray(ts_group, 'step', step, 'step')
            fd.createArray(ts_group, 't', time, 'time')
            fd.createArray(ts_group, 'nt', nt, 'normalized time')

            name_dict = {}
            for key, val in out.iteritems():
                shape = val.get('shape', val.data.shape)
                dofs = val.get('dofs', None)
                if dofs is None:
                    dofs = [''] * nm.squeeze(shape)[-1]
                var_name = val.get('var_name', '')
                name = val.get('name', 'output_data')

                group_name = '__' + key.translate(self._tr)
                data_group = fd.createGroup(step_group, group_name,
                                            '%s data' % key)
                fd.createArray(data_group, 'data', val.data, 'data')
                fd.createArray(data_group, 'mode', val.mode, 'mode')
                fd.createArray(data_group, 'dofs', dofs, 'dofs')
                fd.createArray(data_group, 'shape', shape, 'shape')
                fd.createArray(data_group, 'name', name, 'object name')
                fd.createArray(data_group, 'var_name',
                               var_name, 'object parent name')
                fd.createArray(data_group, 'dname', key, 'data name')
                if val.mode == 'full':
                    fd.createArray(data_group, 'field_name', val.field_name,
                                   'field name')

                name_dict[key] = group_name

            step_group._v_attrs.name_dict = name_dict
            fd.root.last_step[0] = step

            fd.removeNode(fd.root.tstat.finished)
            fd.createArray(fd.root.tstat, 'finished', asctime(),
                           'file closing time')
            fd.close()

    def read_last_step(self, filename=None):
        filename = get_default(filename, self.filename)
        fd = pt.openFile(filename, mode="r")
        last_step = fd.root.last_step[0]
        fd.close()
        return last_step

    def read_time_stepper(self, filename=None):
        filename = get_default(filename, self.filename)
        fd = pt.openFile(filename, mode="r")

        try:
            ts_group = fd.root.ts
            out =  (ts_group.t0.read(), ts_group.t1.read(),
                    ts_group.dt.read(), ts_group.n_step.read())

        except:
            raise ValueError('no time stepper found!')

        finally:
            fd.close()

        return out

    def read_times(self, filename=None):
        """
        Read true time step data from individual time steps.

        Returns
        -------
        steps : array
            The time steps.
        times : array
            The times of the time steps.
        nts : array
            The normalized times of the time steps, in [0, 1].
        """
        filename = get_default(filename, self.filename)
        fd = pt.openFile(filename, mode='r')

        steps = sorted(int(name[4:]) for name in fd.root._v_groups.keys()
                       if name.startswith('step'))
        times = []
        nts = []
        for step in steps:
            ts_group = fd.getNode(fd.root, 'step%d/ts' % step)

            times.append(ts_group.t.read())
            nts.append(ts_group.nt.read())
        fd.close()

        steps = nm.asarray(steps, dtype=nm.int32)
        times = nm.asarray(times, dtype=nm.float64)
        nts = nm.asarray(nts, dtype=nm.float64)

        return steps, times, nts

    def _get_step_group(self, step, filename=None):
        filename = get_default(filename, self.filename)
        fd = pt.openFile(filename, mode="r")

        gr_name = 'step%d' % step
        try:
            step_group = fd.getNode(fd.root, gr_name)
        except:
            output('step %d data not found - premature end of file?' % step)
            fd.close()
            return None, None

        return fd, step_group

    def read_data(self, step, filename=None):
        fd, step_group = self._get_step_group(step, filename=filename)
        if fd is None: return None

        out = {}
        for data_group in step_group:
            try:
                key = data_group.dname.read()

            except pt.exceptions.NoSuchNodeError:
                continue

            name = data_group.name.read()
            mode = data_group.mode.read()
            data = data_group.data.read()
            dofs = tuple(data_group.dofs.read())
            try:
                shape = tuple(data_group.shape.read())

            except pt.exceptions.NoSuchNodeError:
                shape = data.shape

            if mode == 'full':
                field_name = data_group.field_name.read()

            else:
                field_name = None

            out[key] = Struct(name=name, mode=mode, data=data,
                              dofs=dofs, shape=shape, field_name=field_name)

            if out[key].dofs == (-1,):
                out[key].dofs = None

        fd.close()

        return out

    def read_data_header(self, dname, step=0, filename=None):
        fd, step_group = self._get_step_group(step, filename=filename)
        if fd is None: return None

        groups = step_group._v_groups
        for name, data_group in groups.iteritems():
            try:
                key = data_group.dname.read()

            except pt.exceptions.NoSuchNodeError:
                continue

            if key == dname:
                mode = data_group.mode.read()
                fd.close()
                return mode, name

        fd.close()
        raise KeyError('non-existent data: %s' % dname)

    def read_time_history(self, node_name, indx, filename=None):
        filename = get_default(filename, self.filename)
        fd = pt.openFile(filename, mode="r")

        th = dict_from_keys_init(indx, list)
        for step in xrange(fd.root.last_step[0] + 1):
            gr_name = 'step%d' % step

            step_group = fd.getNode(fd.root, gr_name)
            data = step_group._f_getChild(node_name).data

            for ii in indx:
                th[ii].append(nm.array(data[ii]))

        fd.close()

        for key, val in th.iteritems():
            aux = nm.array(val)
            if aux.ndim == 4: # cell data.
                aux = aux[:,0,:,0]
            th[key] = aux

        return th

    def read_variables_time_history(self, var_names, ts, filename=None):
        filename = get_default(filename, self.filename)
        fd = pt.openFile(filename, mode="r")

        assert_((fd.root.last_step[0] + 1) == ts.n_step)

        ths = dict_from_keys_init(var_names, list)

        arr = nm.asarray
        for step in xrange(ts.n_step):
            gr_name = 'step%d' % step
            step_group = fd.getNode(fd.root, gr_name)
            name_dict = step_group._v_attrs.name_dict
            for var_name in var_names:
                data = step_group._f_getChild(name_dict[var_name]).data
                ths[var_name].append(arr(data.read()))

        fd.close()

        return ths

class MEDMeshIO(MeshIO):
    format = "med"

    def read(self, mesh, **kwargs):
        fd = pt.openFile(self.filename, mode="r")

        mesh_root = fd.root.ENS_MAA

        #TODO: Loop through multiple meshes?
        mesh_group = mesh_root._f_getChild(mesh_root._v_groups.keys()[0])

        mesh.name = mesh_group._v_name

        coors = mesh_group.NOE.COO.read()
        n_nodes = mesh_group.NOE.COO.getAttr('NBR')

        # Unflatten the node coordinate array
        coors = coors.reshape(coors.shape[0]/n_nodes,n_nodes).transpose()
        dim = coors.shape[1]

        ngroups = mesh_group.NOE.FAM.read()
        assert_((ngroups >= 0).all())

        # Dict to map MED element names to SfePy descs
        #NOTE: The commented lines are elements which
        #      produce KeyError in SfePy
        med_descs = {
                      'TE4' : '3_4',
                      #'T10' : '3_10',
                      #'PY5' : '3_5',
                      #'P13' : '3_13',
                      'HE8' : '3_8',
                      #'H20' : '3_20',
                      #'PE6' : '3_6',
                      #'P15' : '3_15',
                      #TODO: Polyhedrons (POE) - need special handling
                      'TR3' : '2_3',
                      #'TR6' : '2_6',
                      'QU4' : '2_4',
                      #'QU8' : '2_8',
                      #TODO: Polygons (POG) - need special handling
                      #'SE2' : '1_2',
                      #'SE3' : '1_3',
                    }

        conns = []
        descs = []
        mat_ids = []

        for md, desc in med_descs.iteritems():
            if int(desc[0]) != dim: continue

            try:
                group = mesh_group.MAI._f_getChild(md)

                conn = group.NOD.read()
                n_conns = group.NOD.getAttr('NBR')

                # (0 based indexing in numpy vs. 1 based in MED)
                conn = conn.reshape(conn.shape[0]/n_conns,n_conns).transpose()-1

                conns.append(conn)

                mat_id = group.FAM.read()
                assert_((mat_id <= 0).all())
                mat_id = nm.abs(mat_id)

                mat_ids.append(mat_id)
                descs.append(med_descs[md])

            except pt.exceptions.NoSuchNodeError:
                pass

        fd.close()
        mesh._set_data(coors, ngroups, conns, mat_ids, descs)

        return mesh

class Mesh3DMeshIO(MeshIO):
    format = "mesh3d"

    def read(self, mesh, **kwargs):
        f = open(self.filename)
        # read the whole file:
        vertices = self._read_section(f, integer=False)
        tetras = self._read_section(f)
        hexes = self._read_section(f)
        prisms = self._read_section(f)
        tris = self._read_section(f)
        quads = self._read_section(f)

        # substract 1 from all elements, because we count from 0:
        conns = []
        mat_ids = []
        descs = []
        if len(tetras) > 0:
            conns.append(tetras - 1)
            mat_ids.append([0]*len(tetras))
            descs.append("3_4")
        if len(hexes) > 0:
            conns.append(hexes - 1)
            mat_ids.append([0]*len(hexes))
            descs.append("3_8")
        mesh._set_data(vertices, None, conns, mat_ids, descs)
        return mesh

    def read_dimension(self):
        return 3

    def _read_line(self, f):
        """
        Reads one non empty line (if it's a comment, it skips it).
        """
        l = f.readline().strip()
        while l == "" or l[0] == "#": # comment or an empty line
            l = f.readline().strip()
        return l

    def _read_section(self, f, integer=True):
        """
        Reads one section from the mesh3d file.

        integer ... if True, all numbers are passed to int(), otherwise to
            float(), before returning

        Some examples how a section can look like:

        2
        1 2 5 4 7 8 11 10
        2 3 6 5 8 9 12 11

        or

        5
        1 2 3 4     1
        1 2 6 5     1
        2 3 7 6     1
        3 4 8 7     1
        4 1 5 8     1

        or

        0

        """
        if integer:
            dtype=int
        else:
            dtype=float
        l = self._read_line(f)
        N = int(l)
        rows = []
        for i in range(N):
            l = self._read_line(f)
            row = nm.fromstring(l, sep=" ", dtype=dtype)
            rows.append(row)
        return nm.array(rows)

def mesh_from_groups(mesh, ids, coors, ngroups,
                     tris, mat_tris, quads, mat_quads,
                     tetras, mat_tetras, hexas, mat_hexas, remap=None):
    ids = nm.asarray(ids, dtype=nm.int32)
    coors = nm.asarray(coors, dtype=nm.float64)

    if remap is None:
        n_nod = coors.shape[0]
        remap = nm.zeros((ids.max()+1,), dtype=nm.int32)
        remap[ids] = nm.arange(n_nod, dtype=nm.int32)

    tris = remap[nm.array(tris, dtype=nm.int32)]
    quads = remap[nm.array(quads, dtype=nm.int32)]
    tetras = remap[nm.array(tetras, dtype=nm.int32)]
    hexas = remap[nm.array(hexas, dtype=nm.int32)]

    conns = [tris, quads, tetras, hexas]
    mat_ids = [nm.array(ar, dtype=nm.int32)
               for ar in [mat_tris, mat_quads, mat_tetras, mat_hexas]]
    descs = ['2_3', '2_4', '3_4', '3_8']

    conns, mat_ids = sort_by_mat_id2(conns, mat_ids)
    conns, mat_ids, descs = split_by_mat_id(conns, mat_ids, descs)
    mesh._set_data(coors, ngroups, conns, mat_ids, descs)
    return mesh

class AVSUCDMeshIO(MeshIO):
    format = 'avs_ucd'

    @staticmethod
    def guess(filename):
        return True

    def read(self, mesh, **kwargs):
        fd = open(self.filename, 'r')

        # Skip all comments.
        while 1:
            line = fd.readline()
            if line and (line[0] != '#'):
                break

        header = [int(ii) for ii in line.split()]
        n_nod, n_el = header[0:2]

        ids = nm.zeros((n_nod,), dtype=nm.int32)
        dim = 3
        coors = nm.zeros((n_nod, dim), dtype=nm.float64)
        for ii in xrange(n_nod):
            line = fd.readline().split()
            ids[ii] = int(line[0])
            coors[ii] = [float(coor) for coor in line[1:]]

        mat_tetras = []
        tetras = []
        mat_hexas = []
        hexas = []
        for ii in xrange(n_el):
            line = fd.readline().split()
            if line[2] == 'tet':
                mat_tetras.append(int(line[1]))
                tetras.append([int(ic) for ic in line[3:]])
            elif line[2] == 'hex':
                mat_hexas.append(int(line[1]))
                hexas.append([int(ic) for ic in line[3:]])
        fd.close()

        mesh = mesh_from_groups(mesh, ids, coors, None,
                                [], [], [], [],
                                tetras, mat_tetras, hexas, mat_hexas)
        return mesh

    def read_dimension(self):
        return 3

    def write(self, filename, mesh, out=None, **kwargs):
        raise NotImplementedError

class HypermeshAsciiMeshIO(MeshIO):
    format = 'hmascii'

    def read(self, mesh, **kwargs):
        fd = open(self.filename, 'r')

        ids = []
        coors = []
        tetras = []
        mat_tetras = []
        hexas = []
        mat_hexas = []
        quads = []
        mat_quads = []
        trias = []
        mat_trias = []
        mat_id = 0

        for line in fd:
            if line and (line[0] == '*'):
                if line[1:10] == 'component':
                    line = line.strip()[11:-1].split(',')
                    mat_id = int(line[0])
                if line[1:5] == 'node':
                    line = line.strip()[6:-1].split(',')
                    ids.append(int(line[0]))
                    coors.append([float(coor) for coor in line[1:4]])

                elif line[1:7] == 'tetra4':
                    line = line.strip()[8:-1].split(',')
                    mat_tetras.append(mat_id)
                    tetras.append([int(ic) for ic in line[2:6]])

                elif line[1:6] == 'hexa8':
                    line = line.strip()[7:-1].split(',')
                    mat_hexas.append(mat_id)
                    hexas.append([int(ic) for ic in line[2:10]])

                elif line[1:6] == 'quad4':
                    line = line.strip()[7:-1].split(',')
                    mat_quads.append(mat_id)
                    quads.append([int(ic) for ic in line[2:6]])

                elif line[1:6] == 'tria3':
                    line = line.strip()[7:-1].split(',')
                    mat_trias.append(mat_id)
                    trias.append([int(ic) for ic in line[2:5]])
        fd.close()

        mesh = mesh_from_groups(mesh, ids, coors, None,
                                trias, mat_trias, quads, mat_quads,
                                tetras, mat_tetras, hexas, mat_hexas)

        return mesh

    def read_dimension(self):
        return 3

    def write(self, filename, mesh, out=None, **kwargs):
        raise NotImplementedError

class AbaqusMeshIO(MeshIO):
    format = 'abaqus'

    @staticmethod
    def guess(filename):
        ok = False
        fd = open(filename, 'r')
        for ii in xrange(100):
            try:
                line = fd.readline().strip().split(',')
            except:
                break
            if line[0].lower() == '*node':
                ok = True
                break
        fd.close()

        return ok

    def read(self, mesh, **kwargs):
        fd = open(self.filename, 'r')

        ids = []
        coors = []
        tetras = []
        mat_tetras = []
        hexas = []
        mat_hexas = []
        tris = []
        mat_tris = []
        quads = []
        mat_quads = []
        nsets = {}
        ing = 1
        dim = 0

        line = fd.readline().split(',')
        while 1:
            if not line[0]: break

            token = line[0].strip().lower()
            if token == '*node':
                while 1:
                    line = fd.readline().split(',')
                    if (not line[0]) or (line[0][0] == '*'): break
                    if dim == 0:
                        dim = len(line) - 1
                    ids.append(int(line[0]))
                    if dim == 2:
                        coors.append([float(coor) for coor in line[1:3]])
                    else:
                        coors.append([float(coor) for coor in line[1:4]])

            elif token == '*element':

                if line[1].find('C3D8') >= 0:
                    while 1:
                        line = fd.readline().split(',')
                        if (not line[0]) or (line[0][0] == '*'): break
                        mat_hexas.append(0)
                        hexas.append([int(ic) for ic in line[1:9]])

                elif line[1].find('C3D4') >= 0:
                    while 1:
                        line = fd.readline().split(',')
                        if (not line[0]) or (line[0][0] == '*'): break
                        mat_tetras.append(0)
                        tetras.append([int(ic) for ic in line[1:5]])

                elif line[1].find('CPS') >= 0 or line[1].find('CPE') >= 0:
                    if line[1].find('4') >= 0:
                        while 1:
                            line = fd.readline().split(',')
                            if (not line[0]) or (line[0][0] == '*'): break
                            mat_quads.append(0)
                            quads.append([int(ic) for ic in line[1:5]])
                    elif line[1].find('3') >= 0:
                        while 1:
                            line = fd.readline().split(',')
                            if (not line[0]) or (line[0][0] == '*'): break
                            mat_tris.append(0)
                            tris.append([int(ic) for ic in line[1:4]])
                    else:
                        raise ValueError('unknown element type! (%s)' % line[1])
                else:
                    raise ValueError('unknown element type! (%s)' % line[1])

            elif token == '*nset':

                if line[-1].strip().lower() == 'generate':
                    line = fd.readline()
                    continue

                while 1:
                    line = fd.readline().strip().split(',')
                    if (not line[0]) or (line[0][0] == '*'): break
                    if not line[-1]: line = line[:-1]
                    aux = [int(ic) for ic in line]
                    nsets.setdefault(ing, []).extend(aux)
                ing += 1

            else:
                line = fd.readline().split(',')

        fd.close()

        ngroups = nm.zeros((len(coors),), dtype=nm.int32)
        for ing, ii in nsets.iteritems():
            ngroups[nm.array(ii)-1] = ing

        mesh = mesh_from_groups(mesh, ids, coors, ngroups,
                                tris, mat_tris, quads, mat_quads,
                                tetras, mat_tetras, hexas, mat_hexas)

        return mesh

    def read_dimension(self):
        fd = open(self.filename, 'r')
        line = fd.readline().split(',')
        while 1:
            if not line[0]: break

            token = line[0].strip().lower()
            if token == '*node':
                while 1:
                    line = fd.readline().split(',')
                    if (not line[0]) or (line[0][0] == '*'): break
                    dim = len(line) - 1

        fd.close()
        return dim

    def write(self, filename, mesh, out=None, **kwargs):
        raise NotImplementedError

class BDFMeshIO(MeshIO):
    format = 'nastran'

    def read_dimension(self, ret_fd=False):
        fd = open(self.filename, 'r')
        el3d = 0
        while 1:
            try:
                line = fd.readline()
            except:
                output("reading " + fd.name + " failed!")
                raise
            if len(line) == 1: continue
            if line[0] == '$': continue
            aux = line.split()

            if aux[0] == 'CHEXA':
                el3d += 1
            elif aux[0] == 'CTETRA':
                el3d += 1

        if el3d > 0:
            dim = 3
        else:
            dim = 2

        if ret_fd:
            return dim, fd
        else:
            fd.close()
            return dim

    def read(self, mesh, **kwargs):
        def mfloat(s):
            if len(s) > 3:
                if s[-3] == '-':
                    return float(s[:-3]+'e'+s[-3:])

            return float(s)

        import string
        fd = open(self.filename, 'r')

        el = {'3_8' : [], '3_4' : [], '2_4' : [], '2_3' : []}
        nod = []
        cmd = ''
        dim = 2

        conns_in = []
        descs = []
        node_grp = None
        while 1:
            try:
                line = fd.readline()
            except EOFError:
                break
            except:
                output("reading " + fd.name + " failed!")
                raise

            if (len(line) == 0): break
            if len(line) < 4: continue
            if line[0] == '$': continue

            row = line.strip().split()
            if row[0] == 'GRID':
                cs = line.strip()[-24:]
                aux = [ cs[0:8], cs[8:16], cs[16:24] ]
                nod.append([mfloat(ii) for ii in aux]);
            elif row[0] == 'GRID*':
                aux = row[1:4];
                cmd = 'GRIDX';
            elif row[0] == 'CHEXA':
                aux = [int(ii)-1 for ii in row[3:9]]
                aux2 = int(row[2])
                aux3 = row[9]
                cmd ='CHEXAX'
            elif row[0] == 'CTETRA':
                aux = [int(ii)-1 for ii in row[3:]]
                aux.append(int(row[2]))
                el['3_4'].append(aux)
                dim = 3
            elif row[0] == 'CQUAD4':
                aux = [int(ii)-1 for ii in row[3:]]
                aux.append(int(row[2]))
                el['2_4'].append(aux)
            elif row[0] == 'CTRIA3':
                aux = [int(ii)-1 for ii in row[3:]]
                aux.append(int(row[2]))
                el['2_3'].append(aux)
            elif cmd == 'GRIDX':
                cmd = ''
                aux2 = row[1]
                if aux2[-1] == '0':
                    aux2 = aux2[:-1]
                    aux3 = aux[1:]
                    aux3.append(aux2)
                    nod.append([float(ii) for ii in aux3]);
            elif cmd == 'CHEXAX':
                cmd = ''
                aux4 = row[0]
                aux5 = string.find(aux4, aux3)
                aux.append(int(aux4[(aux5+len(aux3)):])-1)
                aux.extend([int(ii)-1 for ii in row[1:]])
                aux.append(aux2)
                el['3_8'].append(aux)
                dim = 3

            elif row[0] == 'SPC' or row[0] == 'SPC*':
                if node_grp is None:
                    node_grp = [0] * len(nod)

                node_grp[int(row[2]) - 1] = int(row[1])

        for elem in el.keys():
            if len(el[elem]) > 0:
                conns_in.append(el[elem])
                descs.append(elem)

        fd.close()

        nod = nm.array(nod, nm.float64)
        if dim == 2:
            nod = nod[:,:2].copy()
        conns_in = nm.array(conns_in, nm.int32)

        conns_in, mat_ids = sort_by_mat_id(conns_in)
        conns, mat_ids, descs = split_by_mat_id(conns_in, mat_ids, descs)
        mesh._set_data(nod, node_grp, conns, mat_ids, descs)

        return mesh

    @staticmethod
    def format_str(str, idx, n=8):
        out = ''
        for ii, istr in enumerate(str):
            aux = '%d' % istr
            out += aux + ' ' * (n - len(aux))
            if ii == 7:
                out += '+%07d\n+%07d' % (idx, idx)

        return out

    def write(self, filename, mesh, out=None, **kwargs):
        fd = open(filename, 'w')

        coors = mesh.coors
        conns, desc = join_conn_groups(mesh.conns, mesh.descs,
                                       mesh.mat_ids, concat=True)

        n_nod, dim = coors.shape

        fd.write("$NASTRAN Bulk Data File created by SfePy\n")
        fd.write("$\nBEGIN BULK\n")

        fd.write("$\n$ ELEMENT CONNECTIVITY\n$\n")
        iel = 0
        mats = {}
        for ig, conn in enumerate(conns):
            for ii in range(conn.shape[0]):
                iel += 1
                nn = conn[ii][:-1] + 1
                mat = conn[ii][-1]
                if mat in mats:
                    mats[mat] += 1
                else:
                    mats[mat] = 0

                if (desc[ig] == "2_4"):
                    fd.write("CQUAD4  %s\n" %\
                             self.format_str([ii + 1, mat,
                                              nn[0], nn[1], nn[2], nn[3]],
                                             iel))
                elif (desc[ig] == "2_3"):
                    fd.write("CTRIA3  %s\n" %\
                             self.format_str([ii + 1, mat,
                                              nn[0], nn[1], nn[2]], iel))
                elif (desc[ig] == "3_4"):
                    fd.write("CTETRA  %s\n" %\
                             self.format_str([ii + 1, mat,
                                              nn[0], nn[1], nn[2], nn[3]],
                                             iel))
                elif (desc[ig] == "3_8"):
                    fd.write("CHEXA   %s\n" %\
                             self.format_str([ii + 1, mat, nn[0], nn[1], nn[2],
                                              nn[3], nn[4], nn[5], nn[6],
                                              nn[7]], iel))
                else:
                    raise ValueError('unknown element type! (%s)' % desc[ig])

        fd.write("$\n$ NODAL COORDINATES\n$\n")
        format = 'GRID*   %s                           % 08E   % 08E\n'
        if coors.shape[1] == 3:
            format += '*          % 08E0               \n'
        else:
            format += '*          % 08E0               \n' % 0.0
        for ii in range(n_nod):
            sii = str(ii + 1)
            fd.write(format % ((sii + ' ' * (8 - len(sii)),)
                               + tuple(coors[ii])))

        fd.write("$\n$ GEOMETRY\n$\n1                                   ")
        fd.write("0.000000E+00    0.000000E+00\n")
        fd.write("*           0.000000E+00    0.000000E+00\n*       \n")

        fd.write("$\n$ MATERIALS\n$\n")
        matkeys = mats.keys()
        matkeys.sort()
        for ii, imat in enumerate(matkeys):
            fd.write("$ material%d : Isotropic\n" % imat)
            aux = str(imat)
            fd.write("MAT1*   %s            " % (aux + ' ' * (8 - len(aux))))
            fd.write("0.000000E+00                    0.000000E+00\n")
            fd.write("*           0.000000E+00    0.000000E+00\n")

        fd.write("$\n$ GEOMETRY\n$\n")
        for ii, imat in enumerate(matkeys):
            fd.write("$ material%d : solid%d\n" % (imat, imat))
            fd.write("PSOLID* %s\n" % self.format_str([ii + 1, imat], 0, 16))
            fd.write("*       \n")

        fd.write("ENDDATA\n")

        fd.close()


class NEUMeshIO(MeshIO):
    format = 'gambit'

    def read_dimension(self, ret_fd=False):

        fd = open(self.filename, 'r')

        row = fd.readline().split()
        while 1:
            if not row: break
            if len(row) == 0: continue

            if (row[0] == 'NUMNP'):
                row = fd.readline().split()
                n_nod, n_el, dim = row[0], row[1], int(row[4])
                break;

        if ret_fd:
            return dim, fd
        else:
            fd.close()
            return dim

    def read(self, mesh, **kwargs):

        el = {'3_8' : [], '3_4' : [], '2_4' : [], '2_3' : []}
        nod = []

        conns_in = []
        descs = []

        group_ids = []
        group_n_els = []
        groups = []
        nodal_bcs = {}

        fd = open(self.filename, 'r')

        row = fd.readline().split()
        while 1:
            if not row: break
            if len(row) == 0: continue

            if (row[0] == 'NUMNP'):
                row = fd.readline().split()
                n_nod, n_el, dim = row[0], row[1], int(row[4])

            elif (row[0] == 'NODAL'):
                row = fd.readline().split()
                while not(row[0] == 'ENDOFSECTION'):
                    nod.append(row[1:])
                    row = fd.readline().split()

            elif (row[0] == 'ELEMENTS/CELLS'):
                row = fd.readline().split()
                while not(row[0] == 'ENDOFSECTION'):
                    elid = [row[0]]
                    gtype = int(row[1])
                    if gtype == 6:
                        el['3_4'].append(row[3:]+elid)
                    elif gtype == 4:
                        rr = row[3:]
                        if (len(rr) < 8):
                            rr.extend(fd.readline().split())
                        el['3_8'].append(rr+elid)
                    elif gtype == 3:
                        el['2_3'].append(row[3:]+elid)
                    elif gtype == 2:
                        el['2_4'].append(row[3:]+elid)
                    row = fd.readline().split()

            elif (row[0] == 'GROUP:'):
                group_ids.append(row[1])
                g_n_el = int(row[3])
                group_n_els.append(g_n_el)
                name = fd.readline().strip()

                els = []
                row = fd.readline().split()
                row = fd.readline().split()
                while not(row[0] == 'ENDOFSECTION'):
                    els.extend(row)
                    row = fd.readline().split()
                if g_n_el != len(els):
                    msg = 'wrong number of group elements! (%d == %d)'\
                          % (n_el, len(els))
                    raise ValueError(msg)
                groups.append(els)

            elif (row[0] == 'BOUNDARY'):
                row = fd.readline().split()
                key = row[0]
                num = int(row[2])
                inod = read_array(fd, num, None, nm.int32) - 1
                nodal_bcs[key] = inod.squeeze()

                row = fd.readline().split()
                assert_(row[0] == 'ENDOFSECTION')

            else:
                row = fd.readline().split()

        fd.close()

        if int(n_el) != sum(group_n_els):
            print 'wrong total number of group elements! (%d == %d)'\
                  % (int(n_el), len(group_n_els))

        mat_ids = [None] * int(n_el)
        for ii, els in enumerate(groups):
            for iel in els:
                mat_ids[int(iel) - 1] = group_ids[ii]

        for elem in el.keys():
            if len(el[elem]) > 0:
                for iel in el[elem]:
                    for ii in range(len(iel)):
                        iel[ii] = int(iel[ii]) - 1
                    iel[-1] = mat_ids[iel[-1]]
                conns_in.append(el[elem])
                descs.append(elem)

        nod = nm.array(nod, nm.float64)
        conns_in = nm.array(conns_in, nm.int32)

        conns_in, mat_ids = sort_by_mat_id(conns_in)
        conns, mat_ids, descs = split_by_mat_id(conns_in, mat_ids, descs)
        mesh._set_data(nod, None, conns, mat_ids, descs, nodal_bcs=nodal_bcs)

        return mesh

    def write(self, filename, mesh, out=None, **kwargs):
        raise NotImplementedError

class ANSYSCDBMeshIO(MeshIO):
    format = 'ansys_cdb'

    @staticmethod
    def guess(filename):
        fd = open(filename, 'r')

        for ii in xrange(1000):
            row = fd.readline()
            if not row: break
            if len(row) == 0: continue

            row = row.split(',')
            kw = row[0].lower()

            if (kw == 'nblock'):
                ok = True
                break

        else:
            ok = False

        fd.close()

        return ok

    @staticmethod
    def make_format(format):
        idx = [];
        dtype = [];
        start = 0;

        for iform in format:
            ret = iform.partition('i')
            if not ret[1]:
                ret = iform.partition('e')
            if not ret[1]:
                raise ValueError
            aux = ret[2].partition('.')
            step = int(aux[0])
            for j in range(int(ret[0])):
                idx.append((start, start+step))
                start += step
                dtype.append(ret[1])

        return idx, dtype

    def write(self, filename, mesh, out=None, **kwargs):
        raise NotImplementedError

    def read_bounding_box(self):
        raise NotImplementedError

    def read_dimension(self, ret_fd=False):
        return 3

    def read(self, mesh, **kwargs):
        ids = []
        coors = []
        tetras = []
        hexas = []
        qtetras = []
        qhexas = []
        nodal_bcs = {}

        fd = open(self.filename, 'r')

        while True:
            row = fd.readline()
            if not row: break
            if len(row) == 0: continue

            row = row.split(',')
            kw = row[0].lower()

            if (kw == 'nblock'):
                # Solid keyword -> 3, otherwise 1 is the starting coors index.
                ic = 3 if  len(row) == 3 else 1
                fmt = fd.readline()
                fmt = fmt.strip()[1:-1].split(',')
                idx, dtype = self.make_format(fmt)
                ii0, ii1 = idx[0]
                while True:
                    row = fd.readline()
                    if (row[0] == '!') or (row[:2] == '-1'):
                        break
                    line = [float(row[i0:i1]) for i0, i1 in idx[ic:]]
                    ids.append(int(row[ii0:ii1]))
                    coors.append(line)

            elif (kw == 'eblock'):
                if (len(row) <= 2) or row[2] != 'solid': # no solid keyword
                    continue

                fmt = fd.readline()
                fmt = [fmt.strip()[1:-1]]
                idx, dtype = self.make_format(fmt)

                imi0, imi1 = idx[0] # Material id.
                inn0, inn1 = idx[8] # Number of nodes in line.
                ien0, ien1 = idx[10] # Element number.
                ic0 = 11
                while True:
                    row = fd.readline()
                    if (row[0] == '!') or (row[:2] == '-1'):
                        break

                    line = [int(row[imi0:imi1])]
                    n_nod = int(row[inn0:inn1])

                    line.extend(int(row[i0:i1])
                                for i0, i1 in idx[ic0 : ic0 + n_nod])
                    if n_nod == 4:
                        tetras.append(line)

                    elif n_nod == 8:
                        hexas.append(line)

                    elif n_nod == 10:
                        row = fd.readline()
                        line.extend(int(row[i0:i1])
                                    for i0, i1 in idx[:2])
                        qtetras.append(line)

                    elif n_nod == 20:
                        row = fd.readline()
                        line.extend(int(row[i0:i1])
                                    for i0, i1 in idx[:12])
                        qhexas.append(line)

                    else:
                        raise ValueError('unsupported element type! (%d nodes)'
                                         % n_nod)

            elif kw == 'cmblock':
                if row[2].lower() != 'node': # Only node sets support.
                    continue

                n_nod = int(row[3])
                fd.readline() # Format line not needed.

                nods = read_array(fd, n_nod, 1, nm.int32)
                nodal_bcs[row[1].strip()] = nods.ravel()

        fd.close()

        coors = nm.array(coors, dtype=nm.float64)

        tetras = nm.array(tetras, dtype=nm.int32)
        if len(tetras):
            mat_ids_tetras = tetras[:, 0]
            tetras = tetras[:, 1:]

        else:
            mat_ids_tetras = nm.array([])

        hexas = nm.array(hexas, dtype=nm.int32)
        if len(hexas):
            mat_ids_hexas = hexas[:, 0]
            hexas = hexas[:, 1:]

        else:
            mat_ids_hexas = nm.array([])

        if len(qtetras):
            qtetras = nm.array(qtetras, dtype=nm.int32)
            tetras.shape = (max(0, tetras.shape[0]), 4)
            tetras = nm.r_[tetras, qtetras[:, 1:5]]
            mat_ids_tetras = nm.r_[mat_ids_tetras, qtetras[:, 0]]

        if len(qhexas):
            qhexas = nm.array(qhexas, dtype=nm.int32)
            hexas.shape = (max(0, hexas.shape[0]), 8)
            hexas = nm.r_[hexas, qhexas[:, 1:9]]
            mat_ids_hexas = nm.r_[mat_ids_hexas, qhexas[:, 0]]

        if len(qtetras) or len(qhexas):
            ii = nm.union1d(tetras.ravel(), hexas.ravel())
            n_nod = len(ii)

            remap = nm.zeros((ii.max()+1,), dtype=nm.int32)
            remap[ii] = nm.arange(n_nod, dtype=nm.int32)

            ic = nm.searchsorted(ids, ii)
            coors = coors[ic]

        else:
            n_nod = coors.shape[0]
            remap = nm.zeros((nm.array(ids).max() + 1,), dtype=nm.int32)
            remap[ids] = nm.arange(n_nod, dtype=nm.int32)

        ngroups = nm.zeros(len(coors), dtype=nm.int32)

        mesh = mesh_from_groups(mesh, ids, coors, ngroups,
                                [], [], [], [],
                                tetras, mat_ids_tetras,
                                hexas, mat_ids_hexas, remap=remap)

        mesh.nodal_bcs = {}
        for key, nods in nodal_bcs.iteritems():
            mesh.nodal_bcs[key] = remap[nods]

        return mesh

def guess_format(filename, ext, formats, io_table):
    """
    Guess the format of filename, candidates are in formats.
    """
    ok = False
    for format in formats:
        output('guessing %s' % format)
        try:
            ok = io_table[format].guess(filename)
        except AttributeError:
            pass
        if ok: break

    else:
        raise NotImplementedError('cannot guess format of a *%s file!' % ext)

    return format

var_dict = vars().items()
io_table = {}

for key, var in var_dict:
    try:
        if is_derived_class(var, MeshIO):
            io_table[var.format] = var
    except TypeError:
        pass
del var_dict

def any_from_filename(filename, prefix_dir=None):
    """
    Create a MeshIO instance according to the kind of `filename`.

    Parameters
    ----------
    filename : str, function or MeshIO subclass instance
        The name of the mesh file. It can be also a user-supplied function
        accepting two arguments: `mesh`, `mode`, where `mesh` is a Mesh
        instance and `mode` is one of 'read','write', or a MeshIO subclass
        instance.
    prefix_dir : str
        The directory name to prepend to `filename`.

    Returns
    -------
    io : MeshIO subclass instance
        The MeshIO subclass instance corresponding to the kind of `filename`.
    """
    if not isinstance(filename, basestr):
        if isinstance(filename, MeshIO):
            return filename

        else:
            return UserMeshIO(filename)

    ext = op.splitext(filename)[1].lower()
    try:
        format = supported_formats[ext]
    except KeyError:
        raise ValueError('unsupported mesh file suffix! (%s)' % ext)

    if isinstance(format, tuple):
        format = guess_format(filename, ext, format, io_table)

    if prefix_dir is not None:
        filename = op.normpath(op.join(prefix_dir, filename))

    return io_table[format](filename)

insert_static_method(MeshIO, any_from_filename)
del any_from_filename

def for_format(filename, format=None, writable=False, prefix_dir=None):
    """
    Create a MeshIO instance for file `filename` with forced `format`.

    Parameters
    ----------
    filename : str
        The name of the mesh file.
    format : str
        One of supported formats. If None,
        :func:`MeshIO.any_from_filename()` is called instead.
    writable : bool
        If True, verify that the mesh format is writable.
    prefix_dir : str
        The directory name to prepend to `filename`.

    Returns
    -------
    io : MeshIO subclass instance
        The MeshIO subclass instance corresponding to the `format`.
    """
    ext = op.splitext(filename)[1].lower()
    try:
        _format = supported_formats[ext]
    except KeyError:
        _format = None

    format = get_default(format, _format)

    if format is None:
        io = MeshIO.any_from_filename(filename, prefix_dir=prefix_dir)

    else:
        if not isinstance(format, basestr):
            raise ValueError('ambigous suffix! (%s -> %s)' % (ext, format))

        if format not in io_table:
            raise ValueError('unknown output mesh format! (%s)' % format)

        if writable and ('w' not in supported_capabilities[format]):
            output_writable_meshes()
            msg = 'write support not implemented for output mesh format "%s",' \
                  ' see above!' % format
            raise ValueError(msg)

        if prefix_dir is not None:
            filename = op.normpath(op.join(prefix_dir, filename))

        io = io_table[format](filename)

    return io

insert_static_method(MeshIO, for_format)
del for_format

########NEW FILE########
__FILENAME__ = periodic
import numpy as nm

from sfepy.discrete.fem.mesh import find_map

##
# c: 05.05.2008, r: 05.05.2008
eps = 1e-12
def set_accuracy( eps ):
    globals()['eps'] = eps

##
# c: 18.10.2006, r: 05.05.2008
def match_grid_line( coor1, coor2, which ):
    """
    Match coordinates `coor1` with `coor2` along the axis `which`.
    """
    if coor1.shape != coor2.shape:
        raise ValueError, 'incompatible shapes: %s == %s'\
              % ( coor1.shape, coor2.shape)

    c1 = coor1[:,which]
    c2 = coor2[:,which]
    i1 = nm.argsort( c1 )
    i2 = nm.argsort( c2 )

    if not nm.all( nm.abs(c1[i1] - c2[i2]) < eps ):
        print c1[i1]
        print c2[i2]
        print nm.abs(c1[i1] - c2[i2]).max()
        raise ValueError('cannot match nodes!')

    return i1, i2

##
# 18.10.2006, c
# last revision: 18.10.2006
def match_x_line( coor1, coor2 ):
    return match_grid_line( coor1, coor2, 0 )
def match_y_line( coor1, coor2 ):
    return match_grid_line( coor1, coor2, 1 )
def match_z_line( coor1, coor2 ):
    return match_grid_line( coor1, coor2, 2 )

##
# 01.06.2007, c
# last revision: 01.06.2007
def match_grid_plane( coor1, coor2, which ):
    """
    Match coordinates `coor1` with `coor2` along the plane with normal axis
    `which`.
    """
    if coor1.shape != coor2.shape:
        raise ValueError, 'incompatible shapes: %s == %s'\
              % ( coor1.shape, coor2.shape)

    offset = coor1[0,which] - coor2[0,which]
    aux = coor2.copy()
    aux[:,which] += offset
    i1, i2 = find_map( coor1, aux, join = False )

    if i1.shape[0] != coor1.shape[0]:
        print coor1[i1]
        print coor2[i2]
        print nm.abs(coor1[i1] - coor2[i2]).max(0)
        ii = nm.setdiff1d(nm.arange(coor1.shape[0]), i1)
        print coor1[ii]
        print coor2[ii]
        raise ValueError('cannot match nodes!')

    return i1, i2

##
# 01.06.2007, c
# last revision: 01.06.2007
def match_x_plane( coor1, coor2 ):
    return match_grid_plane( coor1, coor2, 0 )
def match_y_plane( coor1, coor2 ):
    return match_grid_plane( coor1, coor2, 1 )
def match_z_plane( coor1, coor2 ):
    return match_grid_plane( coor1, coor2, 2 )

def match_coors(coors1, coors2):
    """
    Match coordinates `coors1` with `coors2`.
    """
    if coors1.shape != coors2.shape:
        raise ValueError('incompatible shapes: %s == %s'
                         % (coors1.shape, coors2.shape))

    i1, i2 = find_map(coors1, coors2, join=False)

    if i1.shape[0] != coors1.shape[0]:
        print coors1[i1]
        print coors2[i2]
        print nm.abs(coors1[i1] - coors2[i2]).max(0)
        ii = nm.setdiff1d(nm.arange(coors1.shape[0]), i1)
        print coors1[ii]
        print coors2[ii]
        raise ValueError('cannot match nodes!')

    return i1, i2

########NEW FILE########
__FILENAME__ = poly_spaces
import numpy as nm
import numpy.linalg as nla

from sfepy.base.base import find_subclasses, assert_, Struct
from sfepy.linalg import combine, insert_strided_axis

# Requires fixed vertex numbering!
vertex_maps = {3 : [[0, 0, 0],
                    [1, 0, 0],
                    [1, 1, 0],
                    [0, 1, 0],
                    [0, 0, 1],
                    [1, 0, 1],
                    [1, 1, 1],
                    [0, 1, 1]],
               2 : [[0, 0],
                    [1, 0],
                    [1, 1],
                    [0, 1]],
               1 : [[0],
                    [1]]}

class LagrangeNodes(Struct):
    """Helper class for defining nodes of Lagrange elements."""

    @staticmethod
    def append_edges(nodes, nts, iseq, nt, edges, order):
        delta = 1.0 / float(order)

        for ii, edge in enumerate(edges):
            n1 = nodes[edge[0],:].copy()
            n2 = nodes[edge[1],:].copy()
            for ie in range(order - 1):
                c2 = ie + 1
                c1 = order - c2
                nts[iseq] = [nt, ii]
                aux = [int(round(tmp)) for tmp in delta * (c1 * n1 + c2 * n2)]
                nodes[iseq,:] = aux
                iseq += 1
        return iseq

    @staticmethod
    def append_faces(nodes, nts, iseq, nt, faces, order):
        delta = 1.0 / float(order)

        for ii, face in enumerate(faces):
            n1 = nodes[face[0],:].copy()
            n2 = nodes[face[1],:].copy()
            n3 = nodes[face[2],:].copy()
            for i1 in range(order - 2):
                for i2 in range(order - 2 - i1):
                    c3 = i1 + 1
                    c2 = i2 + 1
                    c1 = order - c3 - c2
                    nts[iseq] = [nt, ii]
                    aux = [int(round(tmp)) for tmp
                           in delta * (c1 * n1 + c2 * n2 + c3 * n3)]
                    nodes[iseq,:] = aux
                    iseq += 1
        return iseq

    @staticmethod
    def append_bubbles(nodes, nts, iseq, nt, order):
        delta = 1.0 / float(order)

        n1 = nodes[0,:].copy()
        n2 = nodes[1,:].copy()
        n3 = nodes[2,:].copy()
        n4 = nodes[3,:].copy()
        for i1 in range(order - 3):
            for i2 in range(order - 3):
                for i3 in range(order - 3 - i1 - i2):
                    c4 = i1 + 1
                    c3 = i2 + 1
                    c2 = i3 + 1
                    c1 = order - c4 - c3 - c2
                    nts[iseq] = [nt, 0]
                    aux = [int(round(tmp)) for tmp
                           in delta * (c1 * n1 + c2 * n2 + c3 * n3 + c4 * n4)]
                    nodes[iseq,:] = aux
                    iseq += 1
        return iseq

    @staticmethod
    def append_tp_edges(nodes, nts, iseq, nt, edges, ao):
        delta = 1.0 / float(ao)
        for ii, edge in enumerate(edges):
            n1 = nodes[edge[0],:].copy()
            n2 = nodes[edge[1],:].copy()
            for ie in range(ao - 1):
                c2 = ie + 1
                c1 = ao - c2
                nts[iseq] = [nt, ii]
                aux = [int(round(tmp)) for tmp in delta * (c1 * n1 + c2 * n2)]
                nodes[iseq,:] = aux
                iseq += 1
        return iseq

    @staticmethod
    def append_tp_faces(nodes, nts, iseq, nt, faces, ao):
        delta = 1.0 / (float(ao) ** 2)
        for ii, face in enumerate( faces ):
            n1 = nodes[face[0],:].copy()
            n2 = nodes[face[1],:].copy()
            n3 = nodes[face[2],:].copy()
            n4 = nodes[face[3],:].copy()
            for i1 in range(ao - 1):
                for i2 in range(ao - 1):
                    c4 = i1 + 1
                    c3 = i2 + 1
                    c2 = ao - c4
                    c1 = ao - c3
                    nts[iseq] = [nt, ii]
                    aux = [int(round(tmp)) for tmp
                           in delta * (c1 * c2 * n1 + c2 * c3 * n2
                                       + c3 * c4 * n3 + c4 * c1 * n4)]
                    nodes[iseq,:] = aux
                    iseq += 1
        return iseq

    @staticmethod
    def append_tp_bubbles(nodes, nts, iseq, nt, ao):
        delta = 1.0 / (float(ao) ** 3)
        n1 = nodes[0,:].copy()
        n2 = nodes[1,:].copy()
        n3 = nodes[2,:].copy()
        n4 = nodes[3,:].copy()
        n5 = nodes[4,:].copy()
        n6 = nodes[5,:].copy()
        n7 = nodes[6,:].copy()
        n8 = nodes[7,:].copy()
        for i1 in range(ao - 1):
            for i2 in range(ao - 1):
                for i3 in range(ao - 1):
                    c6 = i1 + 1
                    c5 = i2 + 1
                    c4 = i3 + 1
                    c3 = ao - c6
                    c2 = ao - c5
                    c1 = ao - c4
                    nts[iseq] = [nt, 0]
                    aux = [int(round(tmp)) for tmp
                           in delta * (c1 * c2 * c3 * n1 + c4 * c2 * c3 * n2
                                       + c5 * c4 * c3 * n3 + c1 * c3 * c5 * n4
                                       + c1 * c2 * c6 * n5 + c4 * c2 * c6 * n6
                                       + c5 * c4 * c6 * n7 + c1 * c6 * c5 * n8)]
                    nodes[iseq,:] = aux
                    iseq += 1
        return iseq

class NodeDescription(Struct):
    """
    Describe FE nodes defined on different parts of a reference element.
    """

    def _describe_facets(self, ii):
        nts = self.node_types[ii]
        ik = nm.where(nts[1:,1] > nts[:-1,1])[0]

        if len(ik) == 0:
            ifacets = None
            n_dof = 0

        else:
            ii = ii.astype(nm.int32)

            ik = nm.r_[0, ik + 1, nts.shape[0]]
            ifacets = [ii[ik[ir] : ik[ir+1]] for ir in range(len(ik) - 1)]
            n_dof = len(ifacets[0])

        return ifacets, n_dof

    def _describe_other(self, ii):
        if len(ii):
            return ii, len(ii)

        else:
            return None, 0

    def _get_facet_nodes(self, ifacets, nodes):
        if ifacets is None:
            return None

        else:
            return [nodes[ii] for ii in ifacets]

    def _get_nodes(self, ii, nodes):
        if ii is None:
            return None

        else:
            return nodes[ii]

    def __init__(self, node_types, nodes):
        self.node_types = node_types

        # Vertex nodes.
        ii = nm.where(node_types[:,0] == 0)[0]
        self.vertex, self.n_vertex_nod = self._describe_other(ii)
        self.vertex_nodes = self._get_nodes(self.vertex, nodes)

        # Edge nodes.
        ii = nm.where(node_types[:,0] == 1)[0]
        self.edge, self.n_edge_nod = self._describe_facets(ii)
        self.edge_nodes = self._get_facet_nodes(self.edge, nodes)

        # Face nodes.
        ii = nm.where(node_types[:,0] == 2)[0]
        self.face, self.n_face_nod = self._describe_facets(ii)
        self.face_nodes = self._get_facet_nodes(self.face, nodes)

        # Bubble nodes.
        ii = nm.where(node_types[:,0] == 3)[0]
        self.bubble, self.n_bubble_nod = self._describe_other(ii)
        self.bubble_nodes = self._get_nodes(self.bubble, nodes)

    def has_extra_nodes(self):
        """
        Return True if the element has some edge, face or bubble nodes.
        """
        return (self.n_edge_nod + self.n_face_nod + self.n_bubble_nod) > 0

class PolySpace(Struct):
    """Abstract polynomial space class."""
    _all = None

    keys = {
        (1, 2) : 'simplex',
        (2, 3) : 'simplex',
        (3, 4) : 'simplex',
        (2, 4) : 'tensor_product',
        (3, 8) : 'tensor_product',
    }

    @staticmethod
    def any_from_args(name, geometry, order, base='lagrange',
                      force_bubble=False):
        """
        Construct a particular polynomial space classes according to the
        arguments passed in.
        """
        if name is None:
            name = PolySpace.suggest_name(geometry, order, base, force_bubble)

        if PolySpace._all is None:
            PolySpace._all = find_subclasses(globals(), [PolySpace])
        table = PolySpace._all

        key = '%s_%s' % (base, PolySpace.keys[(geometry.dim,
                                               geometry.n_vertex)])
        if (geometry.name == '1_2') and (key not in table):
            key = '%s_%s' % (base, 'tensor_product')

        if force_bubble:
            key += '_bubble'

        return table[key](name, geometry, order)

    @staticmethod
    def suggest_name(geometry, order, base='lagrange',
                     force_bubble=False):
        """
        Suggest the polynomial space name given its constructor parameters.
        """
        aux = geometry.get_interpolation_name()[:-1]
        if force_bubble:
            return aux + ('%dB' % order)
        else:
            return aux + ('%d' % order)

    def __init__(self, name, geometry, order):
        self.name = name
        self.geometry = geometry
        self.order = order

        self.bbox = nm.vstack((geometry.coors.min(0), geometry.coors.max(0)))

    def eval_base(self, coors, diff=False, ori=None, force_axis=False,
                  suppress_errors=False, eps=1e-15):
        """
        Evaluate the basis in points given by coordinates. The real work is
        done in _eval_base() implemented in subclasses.

        Parameters
        ----------
        coors : array_like
            The coordinates of points where the basis is evaluated. See Notes.
        diff : bool
            If True, return the first derivative.
        ori : array_like, optional
            Optional orientation of element facets for per element basis.
        force_axis : bool
            If True, force the resulting array shape to have one more axis even
            when `ori` is None.
        suppress_errors : bool
            If True, do not report points outside the reference domain.
        eps : float
            Accuracy for comparing coordinates.

        Returns
        -------
        base : array
            The basis (shape (n_coor, 1, n_base)) or its derivative (shape
            (n_coor, dim, n_base)) evaluated in the given points. An additional
            axis is pre-pended of length n_cell, if `ori` is given, or of
            length 1, if `force_axis` is True.

        Notes
        -----
        If coors.ndim == 3, several point sets are assumed, with equal number
        of points in each of them. This is the case, for example, of the
        values of the volume base functions on the element facets. The indexing
        (of bf_b(g)) is then (ifa,iqp,:,n_ep), so that the facet can be set in
        C using FMF_SetCell.
        """
        coors = nm.asarray(coors)
        if not coors.ndim in (2, 3):
            raise ValueError('coordinates must have 2 or 3 dimensions! (%d)'
                             % coors.ndim)

        if (coors.ndim == 2):
            base = self._eval_base(coors, diff=diff, ori=ori,
                                   suppress_errors=suppress_errors,
                                   eps=eps)

            if (base.ndim == 3) and force_axis:
                base = base[None, ...]

            if not base.flags['C_CONTIGUOUS']:
                base = nm.ascontiguousarray(base)

        else: # Several point sets.
            if diff:
                bdim = self.geometry.dim
            else:
                bdim = 1

            base = nm.empty((coors.shape[0], coors.shape[1],
                             bdim, self.n_nod), dtype=nm.float64)

            for ii, _coors in enumerate(coors):
                base[ii] = self._eval_base(_coors, diff=diff, ori=ori,
                                           suppress_errors=suppress_errors,
                                           eps=eps)

        return base

    def get_mtx_i(self):
        return self.mtx_i

    def describe_nodes(self):
        return NodeDescription(self.nts, self.nodes)

class LagrangeSimplexPolySpace(PolySpace):
    """Lagrange polynomial space on a simplex domain."""
    name = 'lagrange_simplex'

    def __init__(self, name, geometry, order):
        PolySpace.__init__(self, name, geometry, order)

        n_v = geometry.n_vertex

        mtx = nm.ones((n_v, n_v), nm.float64)
        mtx[0:n_v-1,:] = nm.transpose(geometry.coors)
        self.mtx_i = nm.ascontiguousarray(nla.inv(mtx))
        self.rhs = nm.ones((n_v,), nm.float64)

        self.nodes, self.nts, node_coors = self._define_nodes()
        self.node_coors = nm.ascontiguousarray(node_coors)
        self.n_nod = self.nodes.shape[0]

    def _define_nodes(self):
        # Factorial.
        fac = lambda n : reduce(lambda a, b : a * (b + 1), xrange(n), 1)

        geometry = self.geometry
        n_v, dim = geometry.n_vertex, geometry.dim
        order = self.order

        n_nod = fac(order + dim) / (fac(order) * fac(dim))
        ## print n_nod, gd
        nodes = nm.zeros((n_nod, n_v), nm.int32)
        nts = nm.zeros((n_nod, 2), nm.int32)

        if order == 0:
            nts[0,:] = [3, 0]
            nodes[0,:] = nm.zeros((n_v,), nm.int32)

        else:
            iseq = 0

            # Vertex nodes.
            nts[0:n_v,0] = 0
            nts[0:n_v,1] = nm.arange(n_v, dtype = nm.int32)
            aux = order * nm.identity(n_v, dtype = nm.int32)
            nodes[iseq:iseq+n_v,:] = aux
            iseq += n_v

            if dim == 1:
                iseq = LagrangeNodes.append_edges(nodes, nts, iseq, 3,
                                                  [[0, 1]], order)
            elif dim == 2:
                iseq = LagrangeNodes.append_edges(nodes, nts, iseq, 1,
                                                  geometry.edges, order)
                iseq = LagrangeNodes.append_faces(nodes, nts, iseq, 3,
                                                  [[0, 1, 2]], order)
            elif dim == 3:
                iseq = LagrangeNodes.append_edges(nodes, nts, iseq, 1,
                                                  geometry.edges, order)
                iseq = LagrangeNodes.append_faces(nodes, nts, iseq, 2,
                                                  geometry.faces, order)
                iseq = LagrangeNodes.append_bubbles(nodes, nts, iseq, 3,
                                                    order)
            else:
                raise NotImplementedError

        ## print nm.concatenate((nts, nodes), 1)

        # Check orders.
        orders = nm.sum(nodes, 1)
        if not nm.all(orders == order):
            raise AssertionError('wrong orders! (%d == all of %s)'
                                 % (order, orders))

        # Coordinates of the nodes.
        if order == 0:
            tmp = nm.ones((n_nod, n_v), nm.int32)
            node_coors = nm.dot(tmp, geometry.coors) / n_v

        else:
            node_coors = nm.dot(nodes, geometry.coors) / order

        return nodes, nts, node_coors

    def _eval_base(self, coors, diff=False, ori=None,
                   suppress_errors=False, eps=1e-15):
        """See PolySpace.eval_base()."""
        from extmods.bases import eval_lagrange_simplex

        base = eval_lagrange_simplex(coors, self.mtx_i, self.nodes,
                                     self.order, diff,
                                     eps, not suppress_errors)

        return base

class LagrangeSimplexBPolySpace(LagrangeSimplexPolySpace):
    """Lagrange polynomial space with forced bubble function on a simplex
    domain."""
    name = 'lagrange_simplex_bubble'

    def __init__(self, name, geometry, order):
        LagrangeSimplexPolySpace.__init__(self, name, geometry, order)

        nodes, nts, node_coors = self.nodes, self.nts, self.node_coors

        shape = [nts.shape[0] + 1, 2]
        nts = nm.resize(nts, shape)
        nts[-1,:] = [3, 0]

        shape = [nodes.shape[0] + 1, nodes.shape[1]]
        nodes = nm.resize(nodes, shape)
        # Make a 'hypercubic' (cubic in 2D) node.
        nodes[-1,:] = 1

        n_v = self.geometry.n_vertex
        tmp = nm.ones((n_v,), nm.int32)

        node_coors = nm.vstack((node_coors,
                                nm.dot(tmp, self.geometry.coors) / n_v))

        self.nodes, self.nts = nodes, nts
        self.node_coors = nm.ascontiguousarray(node_coors)

        self.bnode = nodes[-1:,:]

        self.n_nod = self.nodes.shape[0]

    def _eval_base(self, coors, diff=False, ori=None,
                   suppress_errors=False, eps=1e-15):
        """See PolySpace.eval_base()."""
        from extmods.bases import eval_lagrange_simplex

        base = eval_lagrange_simplex(coors, self.mtx_i, self.nodes[:-1],
                                     self.order, diff,
                                     eps, not suppress_errors)
        bubble = eval_lagrange_simplex(coors, self.mtx_i, self.bnode,
                                       int(self.bnode.sum()), diff,
                                       eps, not suppress_errors)

        base -= bubble / (self.n_nod - 1)
        base = nm.ascontiguousarray(nm.dstack((base, bubble)))

        return base

class LagrangeTensorProductPolySpace(PolySpace):
    """Lagrange polynomial space on a tensor product domain."""
    name = 'lagrange_tensor_product'

    def __init__(self, name, geometry, order):
        PolySpace.__init__(self, name, geometry, order)

        g1d = Struct(n_vertex = 2,
                     dim = 1,
                     coors = self.bbox[:,0:1])
        self.ps1d = LagrangeSimplexPolySpace('P_aux', g1d, order)

        self.nodes, self.nts, node_coors = self._define_nodes()
        self.node_coors = nm.ascontiguousarray(node_coors)
        self.n_nod = self.nodes.shape[0]

    def _define_nodes(self):
        geometry = self.geometry
        order = self.order

        n_v, dim = geometry.n_vertex, geometry.dim

        vertex_map = order * nm.array(vertex_maps[dim], dtype=nm.int32)

        n_nod = (order + 1) ** dim
        nodes = nm.zeros((n_nod, 2 * dim), nm.int32)
        nts = nm.zeros((n_nod, 2), nm.int32)

        if order == 0:
            nts[0,:] = [3, 0]
            nodes[0,:] = nm.zeros((n_nod,), nm.int32)

        else:
            iseq = 0

            # Vertex nodes.
            nts[0:n_v,0] = 0
            nts[0:n_v,1] = nm.arange( n_v, dtype = nm.int32 )
            order * nm.identity( n_v, dtype = nm.int32 )
            if dim == 3:
                for ii in range(n_v):
                    i1, i2, i3 = vertex_map[ii]
                    nodes[iseq,:] = [order - i1, i1,
                                     order - i2, i2,
                                     order - i3, i3]
                    iseq += 1
            elif dim == 2:
                for ii in range(n_v):
                    i1, i2 = vertex_map[ii]
                    nodes[iseq,:] = [order - i1, i1, order - i2, i2]
                    iseq += 1
            else:
                for ii in range(n_v):
                    i1 = vertex_map[ii][0]
                    nodes[iseq,:] = [order - i1, i1]
                    iseq += 1

            if dim == 1:
                iseq = LagrangeNodes.append_tp_edges(nodes, nts, iseq, 3,
                                                     [[0, 1]], order)
            elif dim == 2:
                iseq = LagrangeNodes.append_tp_edges(nodes, nts, iseq, 1,
                                                     geometry.edges, order)
                iseq = LagrangeNodes.append_tp_faces(nodes, nts, iseq, 3,
                                                     [[0, 1, 2, 3]], order)
            elif dim == 3:
                iseq = LagrangeNodes.append_tp_edges(nodes, nts, iseq, 1,
                                                     geometry.edges, order)
                iseq = LagrangeNodes.append_tp_faces(nodes, nts, iseq, 2,
                                                     geometry.faces, order)
                iseq = LagrangeNodes.append_tp_bubbles(nodes, nts, iseq, 3,
                                                       order)
            else:
                raise NotImplementedError

        # Check orders.
        orders = nm.sum(nodes, 1)
        if not nm.all(orders == order * dim):
            raise AssertionError('wrong orders! (%d == all of %s)'
                                 % (order * dim, orders))

        # Coordinates of the nodes.
        if order == 0:
            tmp = nm.ones((n_nod, n_v), nm.int32)
            node_coors = nm.dot(tmp, geometry.coors) / n_v

        else:
            c_min, c_max = self.bbox[:,0]
            
            cr = nm.arange(2 * dim)
            node_coors = (nodes[:,cr[::2]] * c_min
                          + nodes[:,cr[1::2]] * c_max) / order

        return nodes, nts, node_coors

    def _eval_base_debug(self, coors, diff=False, ori=None,
                         suppress_errors=False, eps=1e-15):
        """Python version of eval_base()."""
        dim = self.geometry.dim

        ev = self.ps1d.eval_base
        
        if diff:
            base = nm.ones((coors.shape[0], dim, self.n_nod), dtype=nm.float64)

            for ii in xrange(dim):
                self.ps1d.nodes = self.nodes[:,2*ii:2*ii+2].copy()
                self.ps1d.n_nod = self.n_nod

                for iv in xrange(dim):
                    if ii == iv:
                        base[:,iv:iv+1,:] *= ev(coors[:,ii:ii+1].copy(),
                                                diff=True,
                                                suppress_errors=suppress_errors,
                                                eps=eps)
                
                    else:
                        base[:,iv:iv+1,:] *= ev(coors[:,ii:ii+1].copy(),
                                                diff=False,
                                                suppress_errors=suppress_errors,
                                                eps=eps)

        else:
            base = nm.ones((coors.shape[0], 1, self.n_nod), dtype=nm.float64)

            for ii in xrange(dim):
                self.ps1d.nodes = self.nodes[:,2*ii:2*ii+2].copy()
                self.ps1d.n_nod = self.n_nod
                
                base *= ev(coors[:,ii:ii+1].copy(),
                           diff=diff,
                           suppress_errors=suppress_errors,
                           eps=eps)

        return base

    def _eval_base(self, coors, diff=False, ori=None,
                   suppress_errors=False, eps=1e-15):
        """See PolySpace.eval_base()."""
        from extmods.bases import eval_lagrange_tensor_product as ev
        ps1d = self.ps1d

        base = ev(coors, ps1d.mtx_i, self.nodes, self.order, diff,
                  eps, not suppress_errors)

        return base

    def get_mtx_i(self):
        return self.ps1d.mtx_i

class LobattoTensorProductPolySpace(PolySpace):
    """
    Hierarchical polynomial space using Lobatto functions.

    Each row of the `nodes` attribute defines indices of Lobatto functions that
    need to be multiplied together to evaluate the corresponding shape
    function. This defines the ordering of basis functions on the reference
    element.
    """
    name = 'lobatto_tensor_product'

    def __init__(self, name, geometry, order):
        PolySpace.__init__(self, name, geometry, order)

        aux = self._define_nodes()
        self.nodes, self.nts, node_coors, self.face_axes, self.sfnodes = aux
        self.node_coors = nm.ascontiguousarray(node_coors)
        self.n_nod = self.nodes.shape[0]

        aux = nm.where(self.nodes > 0, self.nodes, 1)
        self.node_orders = nm.prod(aux, axis=1)
        self.edge_indx = nm.where(self.nts[:, 0] == 1)[0]
        self.face_indx = nm.where(self.nts[:, 0] == 2)[0]

        self.face_axes_nodes = self._get_face_axes_nodes(self.face_axes)

    def _get_counts(self):
        order = self.order
        dim = self.geometry.dim

        n_nod = (order + 1) ** dim
        n_per_edge = (order - 1)
        n_per_face = (order - 1) ** (dim - 1)
        n_bubble = (order - 1) ** dim

        return n_nod, n_per_edge, n_per_face, n_bubble

    def _define_nodes(self):
        geometry = self.geometry
        order = self.order

        n_v, dim = geometry.n_vertex, geometry.dim

        n_nod, n_per_edge, n_per_face, n_bubble = self._get_counts()

        nodes = nm.zeros((n_nod, dim), nm.int32)
        nts = nm.zeros((n_nod, 2), nm.int32)

        # Vertex nodes.
        nts[0:n_v, 0] = 0
        nts[0:n_v, 1] = nm.arange(n_v, dtype=nm.int32)
        nodes[0:n_v] = nm.array(vertex_maps[dim], dtype=nm.int32)
        ii = n_v

        # Edge nodes.
        if (dim > 1) and (n_per_edge > 0):
            ik = nm.arange(2, order + 1, dtype=nm.int32)
            zo = nm.zeros((n_per_edge, 2), dtype=nm.int32)
            zo[:, 1] = 1
            for ie, edge in enumerate(geometry.edges):
                n1, n2 = nodes[edge]
                ifix = nm.where(n1 == n2)[0]
                irun = nm.where(n1 != n2)[0][0]
                ic = n1[ifix]

                nodes[ii:ii + n_per_edge, ifix] = zo[:, ic]
                nodes[ii:ii + n_per_edge, irun] = ik
                nts[ii:ii + n_per_edge] = [[1, ie]]
                ii += n_per_edge

        # 3D face nodes.
        face_axes = []
        sfnodes = None
        if (dim == 3) and (n_per_face > 0):
            n_face = len(geometry.faces)
            sfnodes = nm.zeros((n_per_face * n_face, dim), nm.int32)
            ii0 = ii

            ik = nm.arange(2, order + 1, dtype=nm.int32)
            zo = nm.zeros((n_per_face, 2), dtype=nm.int32)
            zo[:, 1] = 1

            for ifa, face in enumerate(geometry.faces):
                ns = nodes[face]

                diff = nm.diff(ns, axis=0)
                asum = nm.abs(diff).sum(axis=0)
                ifix = nm.where(asum == 0)[0][0]
                ic = ns[0, ifix]
                irun1 = nm.where(asum == 2)[0][0]
                irun2 = nm.where(asum == 1)[0][0]

                iy, ix = nm.meshgrid(ik, ik)

                nodes[ii:ii + n_per_face, ifix] = zo[:, ic]
                nodes[ii:ii + n_per_face, irun1] = ix.ravel()
                nodes[ii:ii + n_per_face, irun2] = iy.ravel()
                nts[ii:ii + n_per_face] = [[2, ifa]]

                ij = ii - ii0
                sfnodes[ij:ij + n_per_face, ifix] = zo[:, ic]
                sfnodes[ij:ij + n_per_face, irun1] = iy.ravel()
                sfnodes[ij:ij + n_per_face, irun2] = ix.ravel()

                face_axes.append([irun1, irun2])

                ii += n_per_face

        face_axes = nm.array(face_axes)

        # Bubble nodes.
        if n_bubble > 0:
            ik = nm.arange(2, order + 1, dtype=nm.int32)
            nodes[ii:] = nm.array([aux for aux in combine([ik] * dim)])
            nts[ii:ii + n_bubble] = [[3, 0]]
            ii += n_bubble

        assert_(ii == n_nod)

        # Coordinates of the "nodes". All nodes on a facet have the same
        # coordinates - the centre of the facet.
        c_min, c_max = self.bbox[:, 0]

        node_coors = nm.zeros(nodes.shape, dtype=nm.float64)
        node_coors[:n_v] = nodes[:n_v]

        if (dim > 1) and (n_per_edge > 0):
            ie = nm.where(nts[:, 0] == 1)[0]
            node_coors[ie] = node_coors[geometry.edges[nts[ie, 1]]].mean(1)

        if (dim == 3) and (n_per_face > 0):
            ifa = nm.where(nts[:, 0] == 2)[0]
            node_coors[ifa] = node_coors[geometry.faces[nts[ifa, 1]]].mean(1)

        if n_bubble > 0:
            ib = nm.where(nts[:, 0] == 3)[0]
            node_coors[ib] = node_coors[geometry.conn].mean(0)

        return nodes, nts, node_coors, face_axes, sfnodes

    def _get_face_axes_nodes(self, face_axes):
        if not len(face_axes): return None

        nodes = self.nodes[self.face_indx]
        n_per_face = self._get_counts()[2]
        anodes = nm.tile(nodes[:n_per_face, face_axes[0]], (6, 1))

        return anodes

    def _eval_base(self, coors, diff=False, ori=None,
                   suppress_errors=False, eps=1e-15):
        """
        See PolySpace.eval_base().
        """
        from extmods.lobatto_bases import eval_lobatto_tensor_product as ev
        c_min, c_max = self.bbox[:, 0]

        base = ev(coors, self.nodes, c_min, c_max, self.order, diff)

        if ori is not None:
            ebase = nm.tile(base, (ori.shape[0], 1, 1, 1))

            if self.edge_indx.shape[0]:
                # Orient edge functions.
                ie, ii = nm.where(ori[:, self.edge_indx] == 1)
                ii = self.edge_indx[ii]
                ebase[ie, :, :, ii] *= -1.0

            if self.face_indx.shape[0]:
                # Orient face functions.
                fori = ori[:, self.face_indx]

                # ... normal axis order
                ie, ii = nm.where((fori == 1) | (fori == 2))
                ii = self.face_indx[ii]
                ebase[ie, :, :, ii] *= -1.0

                # ... swapped axis order
                sbase = ev(coors, self.sfnodes, c_min, c_max, self.order, diff)
                sbase = insert_strided_axis(sbase, 0, ori.shape[0])

                # ...overwrite with swapped axes basis.
                ie, ii = nm.where(fori >= 4)
                ii2 = self.face_indx[ii]
                ebase[ie, :, :, ii2] = sbase[ie, :, :, ii]

                # ...deal with orientation.
                ie, ii = nm.where((fori == 5) | (fori == 6))
                ii = self.face_indx[ii]
                ebase[ie, :, :, ii] *= -1.0

            base = ebase

        return base

########NEW FILE########
__FILENAME__ = refine
"""
Basic uniform mesh refinement functions.
"""
import numpy as nm

from sfepy.discrete.fem import Mesh

def refine_2_3(mesh_in, cmesh):
    """
    Refines mesh out of triangles by cutting cutting each edge in half
    and making 4 new finer triangles out of one coarser one.
    """
    # Unique edge centres.
    e_centres = cmesh.get_centroids(cmesh.dim - 1)

    # New coordinates after the original ones.
    coors = nm.r_[mesh_in.coors, e_centres]

    o1 = mesh_in.n_nod

    cc = cmesh.get_conn(cmesh.dim, cmesh.dim - 1)
    offs = cc.offsets

    conns = []
    mat_ids = []
    for ig, conn in enumerate(mesh_in.conns):
        off0, off1 = mesh_in.el_offsets[ig : ig + 2]
        n_el  = conn.shape[0]

        e_nodes = cc.indices[offs[off0]:offs[off1]].reshape((n_el, 3)) + o1

        c = nm.c_[conn, e_nodes].T

        new_conn = nm.vstack([c[0], c[3], c[5],
                              c[3], c[4], c[5],
                              c[1], c[4], c[3],
                              c[2], c[5], c[4]]).T
        new_conn = new_conn.reshape((4 * n_el, 3))
        conns.append(new_conn)

        new_mat_id = mesh_in.mat_ids[ig].repeat(4)
        mat_ids.append(new_mat_id)

    mesh = Mesh.from_data(mesh_in.name + '_r', coors, None, conns,
                          mat_ids, mesh_in.descs )

    return mesh

def refine_2_4(mesh_in, cmesh):
    """
    Refines mesh out of quadrilaterals by cutting cutting each edge in
    half and making 4 new finer quadrilaterals out of one coarser one.
    """
    # Unique edge centres.
    e_centres = cmesh.get_centroids(cmesh.dim - 1)

    # Unique element centres.
    centres = cmesh.get_centroids(cmesh.dim)

    # New coordinates after the original ones.
    coors = nm.r_[mesh_in.coors, e_centres, centres]

    o1 = mesh_in.n_nod
    o2 = o1 + e_centres.shape[0]

    cc = cmesh.get_conn(cmesh.dim, cmesh.dim - 1)
    offs = cc.offsets

    conns = []
    mat_ids = []
    for ig, conn in enumerate(mesh_in.conns):
        off0, off1 = mesh_in.el_offsets[ig : ig + 2]
        n_el  = conn.shape[0]

        e_nodes = cc.indices[offs[off0]:offs[off1]].reshape((n_el, 4)) + o1
        nodes = nm.arange(n_el) + off0 + o2

        c = nm.c_[conn, e_nodes, nodes].T

        new_conn = nm.vstack([c[0], c[4], c[8], c[7],
                              c[1], c[5], c[8], c[4],
                              c[2], c[6], c[8], c[5],
                              c[3], c[7], c[8], c[6]]).T
        new_conn = new_conn.reshape((4 * n_el, 4))
        conns.append(new_conn)

        new_mat_id = mesh_in.mat_ids[ig].repeat(4)
        mat_ids.append(new_mat_id)

    mesh = Mesh.from_data(mesh_in.name + '_r', coors, None, conns,
                          mat_ids, mesh_in.descs )

    return mesh

def refine_3_4(mesh_in, cmesh):
    """
    Refines tetrahedra by cutting each edge in half and making 8 new
    finer tetrahedra out of one coarser one. Old nodal coordinates come
    first in `coors`, then the new ones. The new tetrahedra are similar
    to the old one, no degeneration is supposed to occur as at most 3
    congruence classes of tetrahedra appear, even when re-applied
    iteratively (provided that `conns` are not modified between two
    applications - ordering of vertices in tetrahedra matters not only
    for positivity of volumes).

    References:

    - Juergen Bey: Simplicial grid refinement: on Freudenthal s algorithm and 
      the optimal number of congruence classes, Numer.Math. 85 (2000), 
      no. 1, 1--29, or
    - Juergen Bey: Tetrahedral grid refinement, Computing 55 (1995), 
      no. 4, 355--378, or
      http://citeseer.ist.psu.edu/bey95tetrahedral.html
    """
    # Unique edge centres.
    e_centres = cmesh.get_centroids(cmesh.dim - 2)

    # New coordinates after the original ones.
    coors = nm.r_[mesh_in.coors, e_centres]

    o1 = mesh_in.n_nod

    cc = cmesh.get_conn(cmesh.dim, cmesh.dim - 2)
    offs = cc.offsets

    conns = []
    mat_ids = []
    for ig, conn in enumerate(mesh_in.conns):
        off0, off1 = mesh_in.el_offsets[ig : ig + 2]
        n_el  = conn.shape[0]

        e_nodes = cc.indices[offs[off0]:offs[off1]].reshape((n_el, 6)) + o1

        c = nm.c_[conn, e_nodes].T

        new_conn = nm.vstack([c[0], c[4], c[6], c[7],
                              c[4], c[1], c[5], c[8],
                              c[6], c[5], c[2], c[9],
                              c[7], c[8], c[9], c[3],
                              c[4], c[6], c[7], c[8],
                              c[4], c[6], c[8], c[5],
                              c[6], c[7], c[8], c[9],
                              c[6], c[5], c[9], c[8]]).T
        new_conn = new_conn.reshape((8 * n_el, 4))
        conns.append(new_conn)

        new_mat_id = mesh_in.mat_ids[ig].repeat(8)
        mat_ids.append(new_mat_id)

    mesh = Mesh.from_data(mesh_in.name + '_r', coors, None, conns,
                          mat_ids, mesh_in.descs )

    return mesh

def refine_3_8(mesh_in, cmesh):
    """
    Refines hexahedral mesh by cutting cutting each edge in half and
    making 8 new finer hexahedrons out of one coarser one.
    """
    # Unique edge centres.
    e_centres = cmesh.get_centroids(cmesh.dim - 2)

    # Unique face centres.
    f_centres = cmesh.get_centroids(cmesh.dim - 1)

    # Unique element centres.
    coors = mesh_in.get_element_coors()
    centres = 0.125 * nm.sum(coors, axis=1)

    # New coordinates after the original ones.
    coors = nm.r_[mesh_in.coors, e_centres, f_centres, centres]

    o1 = mesh_in.n_nod
    o2 = o1 + e_centres.shape[0]
    o3 = o2 + f_centres.shape[0]

    ecc = cmesh.get_conn(cmesh.dim, cmesh.dim - 2)
    eoffs = ecc.offsets

    fcc = cmesh.get_conn(cmesh.dim, cmesh.dim - 1)
    foffs = fcc.offsets

    st = nm.vstack

    conns = []
    mat_ids = []
    for ig, conn in enumerate(mesh_in.conns):
        off0, off1 = mesh_in.el_offsets[ig : ig + 2]
        n_el  = conn.shape[0]

        e_nodes = ecc.indices[eoffs[off0]:eoffs[off1]].reshape((n_el, 12)) + o1
        f_nodes = fcc.indices[foffs[off0]:foffs[off1]].reshape((n_el, 6)) + o2
        nodes = nm.arange(n_el) + off0 + o3

        c = nm.c_[conn, e_nodes, f_nodes, nodes].T

        new_conn = st([c[0], c[8], c[20], c[11], c[16], c[22], c[26], c[21],
                       c[1], c[9], c[20], c[8], c[17], c[24], c[26], c[22],
                       c[2], c[10], c[20], c[9], c[18], c[25], c[26], c[24],
                       c[3], c[11], c[20], c[10], c[19], c[21], c[26], c[25],
                       c[4], c[15], c[23], c[12], c[16], c[21], c[26], c[22],
                       c[5], c[12], c[23], c[13], c[17], c[22], c[26], c[24],
                       c[6], c[13], c[23], c[14], c[18], c[24], c[26], c[25],
                       c[7], c[14], c[23], c[15], c[19], c[25], c[26], c[21]]).T
        new_conn = new_conn.reshape((8 * n_el, 8))
        conns.append(new_conn)

        new_mat_id = mesh_in.mat_ids[ig].repeat(8)
        mat_ids.append(new_mat_id)

    mesh = Mesh.from_data(mesh_in.name + '_r', coors, None, conns,
                          mat_ids, mesh_in.descs )

    return mesh

def refine_reference(geometry, level):
    """
    Refine reference element given by `geometry`.

    Notes
    -----
    The error edges must be generated in the order of the connectivity
    of the previous (lower) level.
    """
    from sfepy.discrete.fem import FEDomain
    from sfepy.discrete.fem.geometry_element import geometry_data

    gcoors, gconn = geometry.coors, geometry.conn
    if level == 0:
        return gcoors, gconn

    gd = geometry_data[geometry.name]
    conn = nm.array([gd.conn], dtype=nm.int32)
    mat_id = conn[:, 0].copy()
    mat_id[:] = 0

    mesh = Mesh.from_data('aux', gd.coors, None, [conn],
                          [mat_id], [geometry.name])
    domain = FEDomain('aux', mesh)

    for ii in range(level):
        domain = domain.refine()

    coors = domain.mesh.coors
    conn = domain.mesh.conns[0]

    n_el = conn.shape[0]

    if geometry.name == '2_3':
        aux_conn = conn.reshape((n_el / 4, 4, 3))

        ir = [[0, 1, 2], [2, 2, 3], [3, 3, 0]]
        ic = [[0, 0, 0], [0, 1, 0], [0, 1, 0]]

    elif geometry.name == '2_4':
        aux_conn = conn.reshape((n_el / 4, 4, 4))

        ir = [[0, 0, 1], [1, 1, 2], [2, 2, 3], [3, 3, 0], [0, 0, 2], [3, 3, 1]]
        ic = [[0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 2, 1], [1, 2, 1]]

    elif geometry.name == '3_4':
        aux_conn = conn.reshape((n_el / 8, 8, 4))

        ir = [[0, 0, 1], [1, 1, 2], [2, 0, 0], [3, 1, 1], [3, 2, 2], [3, 0, 0]]
        ic = [[0, 1, 1], [1, 2, 2], [2, 2, 0], [3, 3, 1], [3, 3, 2], [3, 3, 0]]

    elif geometry.name == '3_8':
        aux_conn = conn.reshape((n_el / 8, 8, 8))

        ir = [[0, 0, 1], [1, 1, 2], [2, 2, 3], [3, 0, 0], [0, 0, 2], [0, 0, 1],
              [0, 0, 1], [1, 1, 2], [2, 2, 3], [3, 0, 0], [0, 0, 2], [0, 0, 1],
              [4, 4, 5], [5, 5, 6], [6, 6, 7], [7, 4, 4], [4, 4, 6], [4, 4, 5],
              [0, 0, 4], [1, 1, 5], [2, 2, 6], [3, 3, 7],
              [0, 0, 4], [1, 1, 5], [2, 2, 6], [0, 0, 4],
              [0, 0, 4]]
        ic = [[0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 3, 0], [1, 2, 1], [3, 2, 1],
              [4, 5, 4], [4, 5, 4], [4, 5, 4], [4, 7, 4], [5, 6, 5], [7, 6, 5],
              [0, 3, 0], [0, 3, 0], [0, 3, 0], [0, 1, 0], [3, 2, 3], [1, 2, 3],
              [0, 4, 0], [0, 4, 0], [0, 4, 0], [0, 4, 0],
              [1, 5, 3], [1, 5, 3], [1, 5, 3], [3, 7, 1],
              [2, 6, 2]]

    else:
        raise ValueError('unsupported geometry! (%s)' % geometry.name)

    conn = nm.array(conn, dtype=nm.int32)
    error_edges = aux_conn[:, ir, ic]

    return coors, conn, error_edges

########NEW FILE########
__FILENAME__ = utils
import numpy as nm

import sfepy.linalg as la
from sfepy.discrete.integrals import Integral

def prepare_remap(indices, n_full):
    """
    Prepare vector for remapping range `[0, n_full]` to its subset given
    by `indices`.
    """
    remap = nm.empty((n_full,), dtype=nm.int32)
    remap.fill(-1)
    remap[indices] = nm.arange(indices.shape[0], dtype=nm.int32)

    return remap

def invert_remap(remap):
    """
    Return the inverse of `remap`, i.e. a mapping from a sub-range
    indices to a full range, see :func:`prepare_remap()`.
    """
    if remap is not None:
        inverse = nm.where(remap >= 0)[0].astype(nm.int32)

    else:
        inverse = None

    return inverse

def prepare_translate(old_indices, new_indices):
    """
    Prepare vector for translating `old_indices` to `new_indices`.

    Returns
    -------
    translate : array
        The translation vector. Then `new_ar = translate[old_ar]`.
    """
    old_indices = nm.asarray(old_indices)
    new_indices = nm.asarray(new_indices)

    translate = nm.zeros(old_indices.max() + 1, dtype=new_indices.dtype)
    translate[old_indices] = new_indices

    return translate

def compute_nodal_normals(nodes, region, field, return_imap=False):
    """
    Nodal normals are computed by simple averaging of element normals of
    elements every node is contained in.
    """
    dim = region.dim

    field.domain.create_surface_group(region)
    field._setup_surface_data(region)

    # Custom integral with quadrature points very close to facet vertices.
    coors = field.gel.surface_facet.coors
    centre = coors.sum(axis=0) / coors.shape[0]
    qp_coors = coors + 1e-8 * (centre - coors)
    # Unit normals -> weights = ones.
    qp_weights = nm.ones(qp_coors.shape[0], dtype=nm.float64)

    integral = Integral('aux', coors=qp_coors, weights=qp_weights)

    normals = nm.zeros((nodes.shape[0], dim), dtype=nm.float64)
    mask = nm.zeros((nodes.max() + 1,), dtype=nm.int32)
    imap = nm.empty_like(mask)
    imap.fill(nodes.shape[0]) # out-of-range index for normals.
    imap[nodes] = nm.arange(nodes.shape[0], dtype=nm.int32)

    for ig in region.igs:
        cmap, _ = field.get_mapping(ig, region, integral, 'surface')
        e_normals = cmap.normal[..., 0]

        sd = field.domain.surface_groups[ig][region.name]
        econn = sd.get_connectivity()
        mask[econn] += 1

        # normals[imap[econn]] += e_normals
        im = imap[econn]
        for ii, en in enumerate(e_normals):
            normals[im[ii]] += en

    # All nodes must have a normal.
    if not nm.all(mask[nodes] > 0):
        raise ValueError('region %s has not complete faces!' % region.name)

    norm = la.norm_l2_along_axis(normals)[:, nm.newaxis]
    if (norm < 1e-15).any():
        raise ValueError('zero nodal normal! (a node in volume?)')
    normals /= norm

    if return_imap:
        return normals, imap

    else:
        return normals

def _get_edge_path(graph, seed, mask, cycle=False):
    """
    Get a path in an edge graph starting with seed. The mask is incremented by
    one at positions of the path vertices.
    """
    if mask[seed]:
        return []

    path = [seed]
    mask[seed] = 1

    row = graph[seed].indices
    nv = len(row)
    while nv:
        if nv == 2:
            if mask[row[0]]:
                if mask[row[1]]:
                    if cycle:
                        path.append(seed)
                    break

                else:
                    vert = row[1]

            else:
                vert = row[0]

        elif mask[row[0]]:
            break

        else:
            vert = row[0]

        path.append(vert)
        mask[vert] = 1

        row = graph[vert].indices
        nv = len(row)

    path = nm.array(path, dtype=nm.int32)

    return path

def get_edge_paths(graph, mask):
    """
    Get all edge paths in a graph with non-masked vertices. The mask is
    updated.
    """
    nodes = nm.unique(graph.indices)
    npv = nm.diff(graph.indptr)

    if npv.max() > 2:
        raise ValueError('more than 2 edges sharing a vertex!')

    seeds = nm.where(npv == 1)[0]

    # 1. get paths.
    paths = []
    for seed in seeds:
        path = _get_edge_path(graph, seed, mask)
        if len(path):
            paths.append(path)

    # 2. get possible remaing cycles.
    while 1:
        ii = nm.where(mask[nodes] == 0)[0]
        if not len(ii):
            break

        path = _get_edge_path(graph, nodes[ii[0]], mask, cycle=True)
        if len(path):
            paths.append(path)

    return paths

def compute_nodal_edge_dirs(nodes, region, field, return_imap=False):
    """
    Nodal edge directions are computed by simple averaging of direction vectors
    of edges a node is contained in. Edges are assumed to be straight and a
    node must be on a single edge (a border node) or shared by exactly two
    edges.
    """
    coors = region.domain.mesh.coors
    dim = coors.shape[1]

    graph = region.get_edge_graph()

    imap = prepare_remap(nodes, nodes.max() + 1)
    mask = nm.zeros_like(imap)

    try:
        paths = get_edge_paths(graph, mask)

    except ValueError:
        raise ValueError('more than 2 edges sharing a vertex in region %s!'
                         % region.name)

    # All nodes must have an edge direction.
    if not nm.all(mask[nodes]):
        raise ValueError('region %s has not complete edges!' % region.name)

    edge_dirs = nm.zeros((nodes.shape[0], dim), dtype=nm.float64)
    for path in paths:
        pcoors = coors[path]

        edirs = nm.diff(pcoors, axis=0)
        la.normalize_vectors(edirs, eps=1e-12)

        im = imap[nm.c_[path[:-1], path[1:]]]
        for ii, edir in enumerate(edirs):
            edge_dirs[im[ii]] += edir

    la.normalize_vectors(edge_dirs, eps=1e-12)

    if return_imap:
        return edge_dirs, imap

    else:
        return edge_dirs

def get_min_value(dofs):
    """
    Get a reasonable minimal value of DOFs suitable for extending over a
    whole domain.
    """
    if dofs.shape[1] > 1: # Vector.
        val = 0.0

    else: # Scalar.
        val = dofs.min()

    return val

def extend_cell_data(data, domain, rname, val=None, is_surface=False):
    """
    Extend cell data defined in a region to the whole domain.

    Parameters
    ----------
    data : array
        The data defined in the region.
    domain : FEDomain instance
        The FE domain.
    rname : str
        The region name.
    val : float, optional
        The value for filling cells not covered by the region. If not given,
        the smallest value in data is used.
    is_surface : bool
        If True, the data are defined on a surface region. In that case the
        values are averaged into the cells containing the region surface faces.

    Returns
    -------
    edata : array
        The data extended to all domain elements.
    """
    n_el = domain.shape.n_el
    if data.shape[0] == n_el: return data

    if val is None:
        if data.shape[2] > 1: # Vector.
            val = nm.amin(nm.abs(data))
        else: # Scalar.
            val = nm.amin(data)

    edata = nm.empty((n_el,) + data.shape[1:], dtype = nm.float64)
    edata.fill(val)

    region = domain.regions[rname]
    eoffs = domain.get_cell_offsets()

    if not is_surface:
        offs = region.get_cell_offsets()
        for group in domain.iter_groups():
            ig = group.ig
            ii = eoffs[ig]
            if ig in region.igs:
                n_cell = region.shape[ig].n_cell
                ir = offs[ig]
                edata[ii+region.get_cells(ig)] = data[ir:ir+n_cell]

    else:
        for group in domain.iter_groups():
            ig = group.ig
            ii = eoffs[ig]
            if ig in region.igs:
                cells = region.get_cells(ig, true_cells_only=False)
                ucells = nm.unique(cells)

                avg = nm.bincount(cells, minlength=group.shape.n_el)[ucells]
                for ic in xrange(data.shape[2]):
                    evals = nm.bincount(cells, weights=data[:, 0, ic, 0],
                                        minlength=group.shape.n_el)[ucells]

                    edata[ii+ucells, 0, ic, 0] = evals / avg

    return edata

def refine_mesh(filename, level):
    """
    Uniformly refine `level`-times a mesh given by `filename`.

    The refined mesh is saved to a file with name constructed from base
    name of `filename` and `level`-times appended `'_r'` suffix.

    Parameters
    ----------
    filename : str
        The mesh file name.
    level : int
        The refinement level.
    """
    import os
    from sfepy.base.base import output
    from sfepy.discrete.fem import Mesh, FEDomain

    if level > 0:
        mesh = Mesh.from_file(filename)
        domain = FEDomain(mesh.name, mesh)
        for ii in range(level):
            output('refine %d...' % ii)
            domain = domain.refine()
            output('... %d nodes %d elements'
                   % (domain.shape.n_nod, domain.shape.n_el))

        suffix = os.path.splitext(filename)[1]
        filename = domain.name + suffix

        domain.mesh.write(filename, io='auto')

    return filename


########NEW FILE########
__FILENAME__ = functions
import numpy as nm

from sfepy.base.base import OneTypeList, Container, Struct

class Functions(Container):
    """Container to hold all user-defined functions."""

    def from_conf(conf):
        objs = OneTypeList(Function)
        for key, fc in conf.iteritems():
            fun = Function(name = fc.name,
                           function = fc.function,
                           is_constant = False,
                           extra_args = {})
            objs.append(fun)

        obj = Functions(objs)
        return obj
    from_conf = staticmethod(from_conf)

class Function(Struct):
    """Base class for user-defined functions."""

    def __init__(self, name, function, is_constant=False, extra_args=None):
        Struct.__init__(self, name = name, function = function,
                        is_constant = is_constant)
        if extra_args is None:
            extra_args = {}
        self.extra_args = extra_args

    def __call__(self, *args, **kwargs):
        _kwargs = dict(kwargs)
        _kwargs.update(self.extra_args)
        return self.function(*args, **_kwargs)

    def set_function(self, function, is_constant=False):
        self.function = function
        self.is_constant = is_constant

    def set_extra_args(self, **extra_args):
        self.extra_args = extra_args

class ConstantFunction(Function):
    """Function with constant values."""

    def __init__(self, values):
        """Make a function out of a dictionary of constant values. When
        called with coors argument, the values are repeated for each
        coordinate."""

        name = '_'.join(['get_constants'] + values.keys())

        def get_constants(ts=None, coors=None, mode=None, **kwargs):
            out = {}
            if mode == 'special':
                for key, val in values.iteritems():
                    if '.' in key:
                        vkey = key.split('.')[1]
                        out[vkey] = val

            elif (mode == 'qp'):
                for key, val in values.iteritems():
                    if '.' in key: continue

                    val = nm.array(val, dtype=nm.float64, ndmin=3)
                    out[key] = nm.tile(val, (coors.shape[0], 1, 1))

            elif (mode == 'special_constant') or (mode is None):
                for key, val in values.iteritems():
                    if '.' in key: continue

                    out[key] = val

            else:
                raise ValueError('unknown function mode! (%s)' % mode)
            return out

        Function.__init__(self, name = name, function = get_constants,
                          is_constant = True)

class ConstantFunctionByRegion(Function):
    """
    Function with constant values in regions.
    """

    def __init__(self, values):
        """
        Make a function out of a dictionary of constant values per region. When
        called with coors argument, the values are repeated for each
        coordinate in each of the given regions.
        """

        name = '_'.join(['get_constants_by_region'] + values.keys())

        def get_constants(ts=None, coors=None, mode=None,
                          term=None, problem=None, **kwargs):
            out = {}
            if mode == 'qp':
                qps = term.get_physical_qps()

                for key, val in values.iteritems():
                    if '.' in key: continue
                    rval = nm.array(val[val.keys()[0]], dtype=nm.float64,
                                    ndmin=3)
                    matdata = nm.zeros((coors.shape[0], ) + rval.shape[1:],
                                       dtype=nm.float64)

                    for rkey, rval in val.iteritems():
                        region = problem.domain.regions[rkey]
                        rval = nm.array(rval, dtype=nm.float64, ndmin=3)

                        for ig in region.igs:
                            elems = region.get_cells(ig)
                            nqp = qps.shape[ig][1]
                            nel = elems.shape[0]
                            vmap = nm.tile(elems.reshape((nel,1)) * nqp,
                                           (1, nqp)) + nm.arange(nqp)
                            matdata[vmap.reshape(nel * nqp)
                                    + qps.rindx[ig].start] = rval

                    out[key] = matdata

            return out

        Function.__init__(self, name=name, function=get_constants,
                          is_constant=True)

########NEW FILE########
__FILENAME__ = domain
"""
Computational domain for isogeometric analysis.
"""
import os.path as op

import numpy as nm

from sfepy.base.base import Struct
from sfepy.linalg import cycle
from sfepy.discrete.common.domain import Domain
import sfepy.discrete.iga as iga
import sfepy.discrete.iga.io as io

class NurbsPatch(Struct):
    """
    Single NURBS patch data.
    """

    def __init__(self, knots, degrees, cps,
                 weights, cs, conn):
        Struct.__init__(self, name='nurbs', knots=knots, degrees=degrees,
                        cps=cps, weights=weights, cs=cs, conn=conn)
        self.n_els = [len(ii) for ii in cs]
        self.dim = len(self.n_els)

    def _get_ref_coors_1d(self, pars, axis):
        uk = nm.unique(self.knots[axis])
        indices = nm.searchsorted(uk[1:], pars)
        ref_coors = nm.empty_like(pars)
        for ii in xrange(len(uk) - 1):
            ispan = nm.where(indices == ii)[0]
            pp = pars[ispan]
            ref_coors[ispan] = (pp - uk[ii]) / (uk[ii+1] - uk[ii])

        return uk, indices, ref_coors

    def __call__(self, u=None, v=None, w=None, field=None):
        """
        Igakit-like interface for NURBS evaluation.
        """
        pars = [u]
        if v is not None: pars += [v]
        if w is not None: pars += [w]

        indices = []
        uks = []
        rcs = []
        for ia, par in enumerate(pars):
            uk, indx, rc = self._get_ref_coors_1d(par, ia)
            indices.append(indx)
            uks.append(uk)
            rcs.append(rc)

        shape = [len(ii) for ii in pars]
        n_vals = nm.prod(shape)

        if field is None:
            out = nm.zeros((n_vals, self.dim), dtype=nm.float64)

        else:
            out = nm.zeros((n_vals, field.shape[1]), dtype=nm.float64)

        for ip, igrid in enumerate(cycle(shape)):
            iis = [indices[ii][igrid[ii]] for ii in xrange(self.dim)]
            ie = iga.get_raveled_index(iis, self.n_els)

            rc = [rcs[ii][igrid[ii]] for ii in xrange(self.dim)]

            bf, bfg, det = iga.eval_nurbs_basis_tp(rc, ie,
                                                   self.cps, self.weights,
                                                   self.degrees, self.cs,
                                                   self.conn)
            ec = self.conn[ie]

            if field is None:
                out[ip, :] = nm.dot(bf, self.cps[ec])

            else:
                out[ip, :] = nm.dot(bf, field[ec])

        return out

    def evaluate(self, field, u=None, v=None, w=None):
        """
        Igakit-like interface for NURBS evaluation.
        """
        return self(u, v, w, field)

class IGDomain(Domain):
    """
    Bezier extraction based NURBS domain for isogeometric analysis.
    """

    @staticmethod
    def from_file(filename):
        """
        filename : str
            The name of the IGA domain file.
        """
        (knots, degrees, cps, weights, cs, conn,
         bcps, bweights, bconn, regions) = io.read_iga_data(filename)

        nurbs = NurbsPatch(knots, degrees, cps, weights, cs, conn)
        bmesh = Struct(name='bmesh', cps=bcps, weights=bweights, conn=bconn)

        name = op.splitext(filename)[0]
        domain = IGDomain(name, nurbs=nurbs, bmesh=bmesh, regions=regions)
        return domain

    def __init__(self, name, nurbs, bmesh, regions=None, **kwargs):
        """
        Create an IGA domain.

        Parameters
        ----------
        name : str
            The domain name.
        """
        Domain.__init__(self, name, nurbs=nurbs, bmesh=bmesh, regions=regions,
                        **kwargs)
        from sfepy.discrete.fem.geometry_element import create_geometry_elements
        from sfepy.discrete.fem import Mesh
        from sfepy.discrete.fem.extmods.cmesh import CMesh
        from sfepy.discrete.fem.utils import prepare_remap

        self.facets = iga.get_bezier_element_entities(nurbs.degrees)

        tconn = iga.get_bezier_topology(bmesh.conn, nurbs.degrees)
        itc = nm.unique(tconn)

        remap = prepare_remap(itc, bmesh.conn.max() + 1)

        ltcoors = bmesh.cps[itc]
        ltconn = remap[tconn]

        n_nod, dim = ltcoors.shape
        n_el = ltconn.shape[0]
        self.shape = Struct(n_nod=n_nod, dim=dim, tdim=0, n_el=n_el, n_gr=1)

        desc = '%d_%d' % (dim, 2**dim)
        mat_id = nm.zeros(ltconn.shape[0], dtype=nm.int32)
        self.mesh = Mesh.from_data(self.name + '_topo', ltcoors, None, [ltconn],
                                   [mat_id], [desc])

        self.cmesh = CMesh.from_mesh(self.mesh)
        gels = create_geometry_elements()
        self.cmesh.set_local_entities(gels)
        self.cmesh.setup_entities()

        self.shape.tdim = self.cmesh.tdim

        self.gel = gels[desc]

        if regions is not None:
            self.vertex_set_bcs = {}
            for key, val in self.regions.iteritems():
                self.vertex_set_bcs[key] = remap[val]

        self.cell_offsets = {0 : 0}

        self.reset_regions()

########NEW FILE########
__FILENAME__ = fields
"""
Fields for isogeometric analysis.
"""
import numpy as nm

from sfepy.base.base import assert_, Struct
from sfepy.discrete.common.fields import parse_shape, Field
from sfepy.discrete.iga.mappings import IGMapping

class IGField(Field):
    """
    Bezier extraction based NURBS field for isogeometric analysis.

    Notes
    -----
    The field has to cover the whole IGA domain.
    """
    family_name = 'volume_H1_iga'

    def __init__(self, name, dtype, shape, region, **kwargs):
        """
        Create a Bezier element isogeometric analysis field.

        Parameters
        ----------
        name : str
            The field name.
        dtype : numpy.dtype
            The field data type: float64 or complex128.
        shape : int/tuple/str
            The field shape: 1 or (1,) or 'scalar', space dimension (2, or (2,)
            or 3 or (3,)) or 'vector', or a tuple. The field shape determines
            the shape of the FE base functions and is related to the number of
            components of variables and to the DOF per node count, depending
            on the field kind.
        region : Region
            The region where the field is defined.
        **kwargs : dict
            Additional keyword arguments.
        """
        shape = parse_shape(shape, region.domain.shape.dim)
        Struct.__init__(self, name=name, dtype=dtype, shape=shape,
                        region=region)

        self.domain = self.region.domain
        self.nurbs = self.domain.nurbs

        self._setup_kind()

        self.n_components = nm.prod(self.shape)
        self.val_shape = self.shape
        self.n_nod = self.nurbs.weights.shape[0]
        self.n_efun = nm.prod(self.nurbs.degrees + 1)

        self.mappings = {}

        self.igs = self.region.igs
        self.is_surface = False

    def get_true_order(self):
        return nm.prod(self.nurbs.degrees)

    def is_higher_order(self):
        """
        Return True, if the field's approximation order is greater than one.
        """
        return (self.nurbs.degrees > 1).any()

    def get_econn(self, conn_type, region, ig, is_trace=False,
                  integration=None):
        """
        Get DOF connectivity of the given type in the given region.
        """
        ct = conn_type.type if isinstance(conn_type, Struct) else conn_type

        if (ig not in self.igs) or (ig not in region.igs):
            return None

        if ct == 'volume':
            if region.name == self.region.name:
                conn = self.nurbs.conn

            else:
                cells = region.get_cells(ig, true_cells_only=True)
                conn = nm.take(self.nurbs.conn, cells.astype(nm.int32), axis=0)

        else:
            raise ValueError('unsupported connectivity type! (%s)' % ct)

        return conn

    def get_data_shape(self, ig, integral,
                       integration='volume', region_name=None):
        """
        Get element data dimensions.

        Parameters
        ----------
        ig : int
            The element group index.
        integral : Integral instance
            The integral describing used numerical quadrature.
        integration : 'volume', 'plate', 'surface', 'surface_extra' or 'point'
            The term integration type.
        region_name : str
            The name of surface region, required when `shape_kind` is
            'surface'.

        Returns
        -------
        data_shape : 4 ints
            The `(n_el, n_qp, dim, n_en)` for volume shape kind.

        Notes
        -----
        - `n_el` = number of elements
        - `n_qp` = number of quadrature points per element/facet
        - `dim` = spatial dimension
        - `n_en` = number of element nodes
        """
        region = self.domain.regions[region_name]
        shape = region.shape[ig]
        dim = region.dim

        _, weights = integral.get_qp(self.domain.gel.name)
        n_qp = weights.shape[0]

        data_shape = (shape.n_cell, n_qp, dim, self.n_efun)

        return data_shape

    def get_dofs_in_region_group(self, region, ig, merge=True):
        """
        Return indices of DOFs that belong to the given region and group.
        """
        dofs = []

        for idim in xrange(region.tdim, 0, -1):
            if region.can[idim]:
                break

        else:
            raise ValueError('region "%s" has no facets or cells!'
                             % region.name)

        conn = self.nurbs.conn

        if idim == region.tdim: # Cells.
            cells = region.entities[idim]
            dofs.append(nm.unique(conn[cells]))

        else: # Facets.
            assert_(idim > 0)
            fis = region.get_facet_indices(ig, offset=False, force_ig=False)

            facets = self.domain.facets[2 - idim]

            aux = []
            for ii, fi in enumerate(fis):
                aux.append(conn[fi[0], facets[fi[1]]])

            dofs.append(nm.unique(nm.concatenate(aux)))

        if merge:
            dofs = nm.concatenate(dofs)

        return dofs

    def set_dofs(self, fun=0.0, region=None, dpn=None, warn=None):
        """
        Set the values of given DOFs using a function of space coordinates or
        value `fun`.

        Notes
        -----
        Works for a constant value over an entire patch side only.
        """
        if region is None:
            region = self.region

        if dpn is None:
            dpn = self.n_components

        nods = []
        vals = []

        aux = self.get_dofs_in_region(region, clean=True, warn=warn)
        nods = nm.unique(nm.hstack(aux))

        if nm.isscalar(fun):
            vals = nm.repeat([fun], nods.shape[0] * dpn)

        elif isinstance(fun, nm.ndarray):
            assert_(len(fun) == dpn)
            vals = nm.repeat(fun, nods.shape[0])

        else:
            raise ValueError('unknown function/value type! (%s)' % type(fun))

        return nods, vals

    def setup_extra_data(self, geometry, info, is_trace):
        dct = info.dc_type.type

        if dct != 'volume':
            raise ValueError('unknown dof connectivity type! (%s)' % dct)

    def create_mapping(self, ig, region, integral, integration):
        """
        Create a new reference mapping.
        """
        vals, weights = integral.get_qp(self.domain.gel.name)
        mapping = IGMapping(self.domain, region.cells)
        cmap = mapping.get_mapping(vals, weights)

        return cmap, mapping

    def create_output(self, dofs, var_name, dof_names=None,
                      key=None, **kwargs):
        """
        Convert the DOFs corresponding to the field to a dictionary of
        output data usable by Mesh.write().

        Parameters
        ----------
        dofs : array, shape (n_nod, n_component)
            The array of DOFs reshaped so that each column corresponds
            to one component.
        var_name : str
            The variable name corresponding to `dofs`.
        dof_names : tuple of str
            The names of DOF components.
        key : str, optional
            The key to be used in the output dictionary instead of the
            variable name.

        Returns
        -------
        out : dict
            The output dictionary.
        """
        from sfepy.discrete.iga.utils import create_mesh_and_output

        key = key if key is not None else var_name

        num = 25 if self.region.dim == 3 else 101
        pars = (nm.linspace(0, 1, num),) * self.region.dim
        mesh, out = create_mesh_and_output(self.domain.nurbs, pars,
                                           **{key : dofs})
        out[key].var_name = var_name
        out[key].dofs = dof_names
        out['__mesh__'] = mesh

        return out

########NEW FILE########
__FILENAME__ = iga
"""
Isogeometric analysis utilities.

Notes
-----
The functions :func:`compute_bezier_extraction_1d()` and
:func:`eval_nurbs_basis_tp()` implement the algorithms described in [1].

[1] Michael J. Borden, Michael A. Scott, John A. Evans, Thomas J. R. Hughes:
    Isogeometric finite element data structures based on Bezier extraction of
    NURBS, Institute for Computational Engineering and Sciences, The University
    of Texas at Austin, Austin, Texas, March 2010.
"""
import numpy as nm

from sfepy.base.base import assert_

def _get_knots_tuple(knots):
    if isinstance(knots, nm.ndarray) and (knots.ndim == 1):
        knots = (knots,)

    elif not isinstance(knots, tuple):
        raise ValueError('knots must be 1D array or a tuple of 1D arrays!')

    return knots

def get_raveled_index(indices, shape):
    """
    Get a global raveled index corresponding to nD indices into an array of the
    given shape.
    """
    return nm.ravel_multi_index(indices, shape)

def get_unraveled_indices(index, shape):
    """
    Get nD indices into an array of the given shape corresponding to a global
    raveled index.
    """
    return nm.unravel_index(index, shape)

def tensor_product(a, b):
    """
    Compute tensor product of two 2D arrays with possibly different shapes. The
    result has the form::

       c = [[a00 b, a01 b, ...],
            [a10 b, a11 b, ...],
             ...
             ...               ]
    """
    c = nm.empty((a.shape[0] * b.shape[0],
                  a.shape[1] * b.shape[1]), dtype=b.dtype)

    n0 = b.shape[0]
    n1 = b.shape[1]
    for ir in xrange(a.shape[0]):
        for ic in xrange(a.shape[1]):
            c[n1 * ir : n1 * (ir + 1),
              n0 * ic : n0 * (ic + 1)] = a[ir, ic] * b

    return c

def compute_bezier_extraction_1d(knots, degree):
    """
    Compute local (element) Bezier extraction operators for a 1D B-spline
    parametric domain.

    Parameters
    ----------
    knots : array
        The knot vector.
    degree : int
        The curve degree.

    Returns
    -------
    cs : list of 2D arrays
        The element extraction operators.
    """
    knots = nm.asarray(knots, dtype=nm.float64)
    n_knots = knots.shape[0]

    a = degree
    b = a + 1

    # The first element extraction operator.
    cs = [nm.eye(degree + 1, degree + 1, dtype=nm.float64)]
    while (b + 1) < n_knots:
        # The current extraction operator.
        cc = cs[-1]

        # Multiplicity of the knot at location b.
        b0 = b
        while ((b + 1) < n_knots) and (knots[b] == knots[b + 1]):
            b += 1
        mult = b - b0 + 1

        # The next extraction operator.
        if (b + 1) < n_knots:
            cn = nm.eye(degree + 1, degree + 1, dtype=nm.float64)
            cs.append(cn)

        if mult < degree:
            alphas = nm.zeros(degree - mult, dtype=nm.float64)
            numer = knots[b] - knots[a]
            for ij in xrange(degree, mult, -1):
                alphas[ij - mult - 1] = numer / (knots[a + ij] - knots[a])

            r = degree - mult
            for ij in xrange(0, r):
                save = r - ij - 1
                s = mult + ij

                for ik in xrange(degree, s, -1):
                    alpha = alphas[ik - s - 1]
                    cc[:, ik] = (alpha * cc[:, ik]
                                 + (1.0 - alpha) * cc[:, ik - 1])

                if (b + 1) < n_knots:
                    # Update overlapping coefficients for the next operator.
                    cn[save : ij + save + 2,
                       save] = cc[degree - ij - 1: degree + 1, degree]

        if (b + 1) < n_knots:
            # The next knot vector interval.
            a = b
            b = b + 1

    return cs

def compute_bezier_extraction(knots, degrees):
    """
    Compute local (element) Bezier extraction operators for a nD B-spline
    parametric domain.

    Parameters
    ----------
    knots : sequence of array or array
        The knot vectors.
    degrees : sequence of ints or int
        Polynomial degrees in each parametric dimension.

    Returns
    -------
    cs : list of lists of 2D arrays
        The element extraction operators in each parametric dimension.
    """
    if isinstance(degrees, int): degrees = [degrees]

    knots = _get_knots_tuple(knots)
    dim = len(knots)
    assert_(dim == len(degrees))

    cs = []
    for ii, knots1d in enumerate(knots):
        cs1d = compute_bezier_extraction_1d(knots1d, degrees[ii])
        cs.append(cs1d)

    return cs

def combine_bezier_extraction(cs):
    """
    For a nD B-spline parametric domain, combine the 1D element extraction
    operators in each parametric dimension into a single operator for each nD
    element.

    Parameters
    ----------
    cs : list of lists of 2D arrays
        The element extraction operators in each parametric dimension.

    Returns
    -------
    ccs : list of 2D arrays
        The combined element extraction operators.
    """
    dim = len(cs)

    if dim == 3:
        c0, c1, c2 = cs[0], cs[1], cs[2]

        ncc = (len(c0), len(c1), len(c2))
        ccs = [None] * nm.prod(ncc)
        for i0 in xrange(len(c0)):
            for i1 in xrange(len(c1)):
                for i2 in xrange(len(c2)):
                    cc = tensor_product(c0[i0], tensor_product(c1[i1], c2[i2]))
                    ii = get_raveled_index([i0, i1, i2], ncc)
                    ccs[ii] = cc

    elif dim == 2:
        c0, c1 = cs[0], cs[1]

        ncc = (len(c0), len(c1))
        ccs = [None] * nm.prod(ncc)
        for i0 in xrange(len(c0)):
            for i1 in xrange(len(c1)):
                cc = tensor_product(c0[i0], c1[i1])
                ii = get_raveled_index([i0, i1], ncc)
                ccs[ii] = cc

    else:
        ccs = cs[0]

    return ccs

def create_connectivity_1d(n_el, knots, degree):
    """
    Create connectivity arrays of 1D Bezier elements.

    Parameters
    ----------
    n_el : int
        The number of elements.
    knots : array
        The knot vector.
    degree : int
        The basis degree.

    Returns
    -------
    conn : array
        The connectivity of the global NURBS basis.
    bconn : array
        The connectivity of the Bezier basis.
    """
    # Get multiplicities of NURBS knots.
    n_knots = len(knots)
    mul = [0]
    ii = degree + 1
    while ii < (n_knots - degree - 1):
        i0 = ii
        while (ii < (n_knots - degree - 2)) and (knots[ii] == knots[ii + 1]):
            ii += 1
        mul.append(ii - i0 + 1)
        ii += 1

    mul = nm.array(mul)[:, None]

    aux1 = nm.arange(degree + 1)[None, :]
    conn = aux1 + nm.cumsum(mul, 0)

    # Bezier basis knots have multiplicity equal to degree.
    aux2 = nm.arange(n_el)[:, None]
    bconn = aux1 + degree * aux2

    return conn, bconn

def create_connectivity(n_els, knots, degrees):
    """
    Create connectivity arrays of nD Bezier elements.

    Parameters
    ----------
    n_els : sequence of ints
        The number of elements in each parametric dimension.
    knots : sequence of array or array
        The knot vectors.
    degrees : sequence of ints or int
        The basis degrees in each parametric dimension.

    Returns
    -------
    conn : array
        The connectivity of the global NURBS basis.
    bconn : array
        The connectivity of the Bezier basis.
    """
    if isinstance(degrees, int): degrees = [degrees]
    degrees = nm.asarray(degrees)

    knots = _get_knots_tuple(knots)
    dim = len(n_els)
    assert_(dim == len(degrees) == len(knots))

    conns = []
    bconns = []
    n_gfuns = []
    n_gbfuns = []
    for ii, n_el in enumerate(n_els):
        conn1d, bconn1d = create_connectivity_1d(n_el, knots[ii], degrees[ii])
        conns.append(conn1d)
        bconns.append(bconn1d)

        n_gfuns.append(conn1d.max() + 1)
        n_gbfuns.append(bconn1d.max() + 1)

    n_el = nm.prod(n_els)
    n_efuns = degrees + 1
    n_efun = nm.prod(n_efuns)

    if dim == 3:
        def make_conn_3d(conns, n_gfuns):
            conn = nm.empty((n_el, n_efun), dtype=nm.int32)
            for ie0 in xrange(n_els[0]):
                c0 = conns[0][ie0]
                for ie1 in xrange(n_els[1]):
                    c1 = conns[1][ie1]
                    for ie2 in xrange(n_els[2]):
                        c2 = conns[2][ie2]
                        ie = get_raveled_index([ie0, ie1, ie2], n_els)

                        for il0 in xrange(n_efuns[0]):
                            cl0 = c0[il0]
                            for il1 in xrange(n_efuns[1]):
                                cl1 = c1[il1]
                                for il2 in xrange(n_efuns[2]):
                                    cl2 = c2[il2]

                                    iloc = get_raveled_index([il0, il1, il2],
                                                             n_efuns)
                                    ig = get_raveled_index([cl0, cl1, cl2],
                                                           n_gfuns)
                                    conn[ie, iloc] = ig

            return conn

        conn = make_conn_3d(conns, n_gfuns)
        bconn = make_conn_3d(bconns, n_gbfuns)

    elif dim == 2:
        def make_conn_2d(conns, n_gfuns):
            conn = nm.empty((n_el, n_efun), dtype=nm.int32)
            for ie0 in xrange(n_els[0]):
                c0 = conns[0][ie0]
                for ie1 in xrange(n_els[1]):
                    c1 = conns[1][ie1]
                    ie = get_raveled_index([ie0, ie1], n_els)

                    for il0 in xrange(n_efuns[0]):
                        cl0 = c0[il0]
                        for il1 in xrange(n_efuns[1]):
                            cl1 = c1[il1]

                            iloc = get_raveled_index([il0, il1], n_efuns)
                            ig = get_raveled_index([cl0, cl1], n_gfuns)
                            conn[ie, iloc] = ig

            return conn

        conn = make_conn_2d(conns, n_gfuns)
        bconn = make_conn_2d(bconns, n_gbfuns)

    else:
        conn = conns[0]
        bconn = bconns[0]

    return conn, bconn

def compute_bezier_control(control_points, weights, ccs, conn, bconn):
    """
    Compute the control points and weights of the Bezier mesh.

    Parameters
    ----------
    control_points : array
        The NURBS control points.
    weights : array
        The NURBS weights.
    ccs : list of 2D arrays
        The combined element extraction operators.
    conn : array
        The connectivity of the global NURBS basis.
    bconn : array
        The connectivity of the Bezier basis.

    Returns
    -------
    bezier_control_points : array
        The control points of the Bezier mesh.
    bezier_weights : array
        The weights of the Bezier mesh.
    """
    n_bpoints = bconn.max() + 1
    dim = control_points.shape[1]

    bezier_control_points = nm.zeros((n_bpoints, dim), dtype=nm.float64)
    bezier_weights = nm.zeros(n_bpoints, dtype=nm.float64)

    for ie, ec in enumerate(conn):
        cc = ccs[ie]
        bec = bconn[ie]

        ew = weights[ec]
        ecp = control_points[ec]

        bew = nm.dot(cc.T, ew)

        becp = (1.0 / bew[:, None]) * nm.dot(cc.T, ew[:, None] * ecp)
        bezier_control_points[bec] = becp
        bezier_weights[bec] = bew

    return bezier_control_points, bezier_weights

def get_bezier_topology(bconn, degrees):
    """
    Get a topology connectivity corresponding to the Bezier mesh connectivity.

    In the referenced Bezier control points the Bezier mesh is interpolatory.

    Parameters
    ----------
    bconn : array
        The connectivity of the Bezier basis.
    degrees : sequence of ints or int
        The basis degrees in each parametric dimension.

    Returns
    -------
    tconn : array
        The topology connectivity (corner nodes, or vertices, of Bezier
        elements) with vertex ordering suitable for a FE mesh.
    """
    shape = nm.asarray(degrees) + 1
    dim = len(shape)

    ii = nm.arange(bconn.shape[1]).reshape(shape)

    if dim == 3:
        corners = [ii[0, 0, 0], ii[-1, 0, 0], ii[-1, -1, 0], ii[0, -1, 0],
                   ii[0, 0, -1], ii[-1, 0, -1], ii[-1, -1, -1], ii[0, -1, -1]]

    elif dim == 2:
        corners = [ii[0, 0], ii[-1, 0], ii[-1, -1], ii[0, -1]]

    else:
        corners = [ii[0], ii[-1]]

    tconn = bconn[:, corners]

    return tconn

def get_patch_box_regions(n_els, degrees):
    """
    Get box regions of Bezier topological mesh in terms of element corner
    vertices of Bezier mesh.

    Parameters
    ----------
    n_els : sequence of ints
        The number of elements in each parametric dimension.
    degrees : sequence of ints or int
        Polynomial degrees in each parametric dimension.

    Returns
    -------
    regions : dict
        The Bezier mesh vertices of box regions.
    """
    if isinstance(degrees, int): degrees = [degrees]
    degrees = nm.asarray(degrees)

    n_els = nm.asarray(n_els)
    dim = len(n_els)

    shape = n_els * degrees + 1

    regions = {}
    if dim == 3:
        aux0 = nm.arange(0, shape[2], degrees[2], dtype=nm.uint32)
        aux1 = nm.arange(0, shape[2] * shape[1], shape[2] * degrees[1],
                         dtype=nm.uint32)
        aux2 = nm.arange(0, shape[2] * shape[1] * shape[0],
                         shape[2] * shape[1] * degrees[0], dtype=nm.uint32)

        aux01 = (aux0[None, :] + aux1[:, None]).ravel()
        aux02 = (aux0[None, :] + aux2[:, None]).ravel()
        aux12 = (aux1[None, :] + aux2[:, None]).ravel()

        regions.update({
            'xi00' : aux01,
            'xi01' : aux01 + shape[2] * shape[1] * (shape[0] - 1),
            'xi10' : aux02,
            'xi11' : aux02 + shape[2] * (shape[1] - 1),
            'xi20' : aux12,
            'xi21' : aux12 + shape[2] - 1,
        })

    elif dim == 2:
        aux0 = nm.arange(0, shape[1], degrees[1], dtype=nm.uint32)
        aux1 = nm.arange(0, shape[1] * shape[0], shape[1] * degrees[0],
                         dtype=nm.uint32)

        regions.update({
            'xi00' : aux0,
            'xi01' : aux0 + shape[1] * (shape[0] - 1),
            'xi10' : aux1,
            'xi11' : aux1 + shape[1] - 1,
        })

    else:
        regions.update({
            'xi00' : nm.array([0], dtype=nm.uint32),
            'xi01' : nm.array([shape[0] - 1], dtype=nm.uint32),
        })

    return regions

def get_bezier_element_entities(degrees):
    """
    Get faces and edges of a Bezier mesh element in terms of indices into the
    element's connectivity (reference Bezier element entities).

    Parameters
    ----------
    degrees : sequence of ints or int
        Polynomial degrees in each parametric dimension.

    Returns
    -------
    faces : list of arrays
        The indices for each face or None if not 3D.
    edges : list of arrays
        The indices for each edge or None if not at least 2D.

    Notes
    -----
    The ordering of faces and edges has to be the same as in
    :data:`sfepy.discrete.fem.geometry_element.geometry_data`.
    """
    if isinstance(degrees, int): degrees = [degrees]
    degrees = nm.asarray(degrees)

    dim = len(degrees)
    shape = degrees + 1

    n_dof = nm.prod(shape)

    aux = nm.arange(n_dof, dtype=nm.uint32).reshape(shape)
    if dim == 3:
        faces = [aux[:, :, 0],
                 aux[0, :, :],
                 aux[:, 0, :],
                 aux[:, :, -1],
                 aux[-1, :, :],
                 aux[:, -1, :]]
        faces = [ii.ravel() for ii in faces]
        edges = [aux[:, 0, 0],
                 aux[-1, :, 0],
                 aux[:, -1, 0],
                 aux[0, :, 0],
                 aux[:, 0, -1],
                 aux[-1, :, -1],
                 aux[:, -1, -1],
                 aux[0, :, -1],
                 aux[0, 0, :],
                 aux[0, -1, :],
                 aux[-1, -1, :],
                 aux[-1, 0, :]]

    elif dim == 2:
        faces = None
        edges = [aux[:, 0],
                 aux[-1, :],
                 aux[:, -1],
                 aux[0, :]]

    else:
        faces, edges = None, None

    return faces, edges

def eval_bernstein_basis(x, degree):
    """
    Evaluate the Bernstein polynomial basis of the given `degree`, and its
    derivatives, in a point `x` in [0, 1].

    Parameters
    ----------
    x : float
        The point in [0, 1].
    degree : int
        The basis degree.

    Returns
    -------
    funs : array
        The `degree + 1` values of the Bernstein polynomial basis.
    ders : array
        The `degree + 1` values of the Bernstein polynomial basis derivatives.
    """
    n_fun = degree + 1

    funs = nm.zeros(n_fun, dtype=nm.float64)
    ders = nm.zeros(n_fun, dtype=nm.float64)

    funs[0] = 1.0

    if degree == 0: return funs, ders

    for ip in xrange(1, n_fun - 1):
        prev = 0.0
        for ifun in xrange(ip + 1):
            tmp = x * funs[ifun]
            funs[ifun] = (1.0 - x) * funs[ifun] + prev
            prev = tmp

    for ifun in xrange(n_fun):
        ders[ifun] = degree * (funs[ifun - 1] - funs[ifun])

    prev = 0.0
    for ifun in xrange(n_fun):
        tmp = x * funs[ifun]
        funs[ifun] = (1.0 - x) * funs[ifun] + prev
        prev = tmp

    return funs, ders

def eval_nurbs_basis_tp(qp, ie, control_points, weights, degrees, cs, conn):
    """
    Evaluate the tensor-product NURBS shape functions in a quadrature point for
    a given Bezier element.

    Parameters
    ----------
    qp : array
        The quadrature point coordinates with components in [0, 1] reference
        element domain.
    ie : int
        The Bezier element index.
    control_points : array
        The NURBS control points.
    weights : array
        The NURBS weights.
    degrees : sequence of ints or int
        The basis degrees in each parametric dimension.
    cs : list of lists of 2D arrays
        The element extraction operators in each parametric dimension.
    conn : array
        The connectivity of the global NURBS basis.

    Returns
    -------
    R : array
        The NURBS shape functions.
    dR_dx : array
        The NURBS shape functions derivatives w.r.t. the physical coordinates.
    det : array
        The Jacobian of the mapping to the unit reference element.
    """
    if isinstance(degrees, int): degrees = [degrees]
    degrees = nm.asarray(degrees)

    dim = len(degrees)
    assert_(dim == len(qp) == len(cs))

    n_efuns = degrees + 1
    n_efun = nm.prod(n_efuns)
    n_efuns_max = n_efuns.max()

    assert_(n_efun == conn.shape[1])

    # Element connectivity.
    ec = conn[ie]

    # Element control points and weights.
    W = weights[ec]
    P = control_points[ec]

    # 1D Bernstein basis B, dB/dxi.
    B = nm.empty((dim, n_efuns_max), dtype=nm.float64)
    dB_dxi = nm.empty((dim, n_efuns_max), dtype=nm.float64)
    for ii in xrange(dim):
        (B[ii, :n_efuns[ii]],
         dB_dxi[ii, :n_efuns[ii]]) = eval_bernstein_basis(qp[ii], degrees[ii])

    # 1D B-spline basis N = CB, dN/dxi = C dB/dxi.
    N = nm.empty((dim, n_efuns_max), dtype=nm.float64)
    dN_dxi = nm.empty((dim, n_efuns_max), dtype=nm.float64)
    n_els = [len(ii) for ii in cs]
    ic = get_unraveled_indices(ie, n_els)
    for ii in xrange(dim):
        C = cs[ii][ic[ii]]

        N[ii, :n_efuns[ii]] = nm.dot(C, B[ii, :n_efuns[ii]])
        dN_dxi[ii, :n_efuns[ii]] = nm.dot(C, dB_dxi[ii, :n_efuns[ii]])

    # Numerators and denominator for tensor-product NURBS basis R, dR/dxi.
    R = nm.empty(n_efun, dtype=nm.float64)
    dR_dxi = nm.empty((n_efun, dim), dtype=nm.float64)

    w = 0 # w_b
    dw_dxi = nm.zeros(dim, dtype=nm.float64) # dw_b/dxi
    a = 0 # Basis function index.
    if dim == 3:
        for i0 in xrange(n_efuns[0]):
            for i1 in xrange(n_efuns[1]):
                for i2 in xrange(n_efuns[2]):
                    R[a] = N[0, i0] * N[1, i1] * N[2, i2] * W[a]
                    w += R[a]

                    dR_dxi[a, 0] = dN_dxi[0, i0] * N[1, i1] * N[2, i2] * W[a]
                    dw_dxi[0] += dR_dxi[a, 0]

                    dR_dxi[a, 1] = N[0, i0] * dN_dxi[1, i1] * N[2, i2] * W[a]
                    dw_dxi[1] += dR_dxi[a, 1]

                    dR_dxi[a, 2] = N[0, i0] * N[1, i1] * dN_dxi[2, i2] * W[a]
                    dw_dxi[2] += dR_dxi[a, 2]

                    a += 1

    elif dim == 2:
        for i0 in xrange(n_efuns[0]):
            for i1 in xrange(n_efuns[1]):
                R[a] = N[0, i0] * N[1, i1] * W[a]
                w += R[a]

                dR_dxi[a, 0] = dN_dxi[0, i0] * N[1, i1] * W[a]
                dw_dxi[0] += dR_dxi[a, 0]

                dR_dxi[a, 1] = N[0, i0] * dN_dxi[1, i1] * W[a]
                dw_dxi[1] += dR_dxi[a, 1]

                a += 1

    else:
        for i0 in xrange(n_efuns[0]):
            R[a] = N[0, i0] * W[a]
            w += R[a]

            dR_dxi[a, 0] = dN_dxi[0, i0] * W[a]
            dw_dxi[0] += dR_dxi[a, 0]

            a += 1

    # Finish R <- R / w_b.
    R /= w

    # Finish dR/dxi. D == W C dB/dxi, dR/dxi = (D - R dw_b/dxi) / w_b.
    dR_dxi = (dR_dxi - R[:, None] * dw_dxi) / w

    # Mapping reference -> physical domain dxi/dx.
    # x = sum P_a R_a, dx/dxi = sum P_a dR_a/dxi, invert.
    dx_dxi = nm.dot(P.T, dR_dxi)
    det = nm.linalg.det(dx_dxi)

    dxi_dx = nm.linalg.inv(dx_dxi)

    # dR/dx.
    dR_dx = nm.dot(dR_dxi, dxi_dx)

    return R, dR_dx, det

def eval_mapping_data_in_qp(qps, control_points, weights, degrees, cs, conn,
                            cells=None):
    """
    Evaluate data required for the isogeometric domain reference mapping in the
    given quadrature points. The quadrature points are the same for all Bezier
    elements and should correspond to the Bernstein basis degree.

    Parameters
    ----------
    qps : array
        The quadrature points coordinates with components in [0, 1] reference
        element domain.
    control_points : array
        The NURBS control points.
    weights : array
        The NURBS weights.
    degrees : sequence of ints or int
        The basis degrees in each parametric dimension.
    cs : list of lists of 2D arrays
        The element extraction operators in each parametric dimension.
    conn : array
        The connectivity of the global NURBS basis.
    cells : array, optional
        If given, use only the given Bezier elements.

    Returns
    -------
    bfs : array
        The NURBS shape functions in the physical quadrature points of all
        elements.
    bfgs : array
        The NURBS shape functions derivatives w.r.t. the physical coordinates
        in the physical quadrature points of all elements.
    dets : array
        The Jacobians of the mapping to the unit reference element in the
        physical quadrature points of all elements.
    """
    if cells is None:
        cells = nm.arange(conn.shape[0])

    n_el = len(cells)
    n_qp = qps.shape[0]
    dim = control_points.shape[1]
    n_efuns = degrees + 1
    n_efun = nm.prod(n_efuns)

    # Output Jacobians.
    dets = nm.empty((n_el, n_qp, 1, 1), dtype=nm.float64)

    # Output shape functions.
    bfs = nm.empty((n_el, n_qp, 1, n_efun), dtype=nm.float64)

    # Output gradients of shape functions.
    bfgs = nm.empty((n_el, n_qp, dim, n_efun), dtype=nm.float64)

    # Loop over elements.
    for iseq, ie in enumerate(cells):
        # Loop over quadrature points.
        for iqp, qp in enumerate(qps):
            bf, bfg, det = eval_nurbs_basis_tp(qp, ie,
                                               control_points, weights,
                                               degrees, cs, conn)
            bfs[iseq, iqp] = bf
            bfgs[iseq, iqp] = bfg.T
            dets[iseq, iqp] = det

    return bfs, bfgs, dets

def eval_variable_in_qp(variable, qps,
                        control_points, weights, degrees, cs, conn,
                        cells=None):
    """
    Evaluate a field variable in the given quadrature points. The quadrature
    points are the same for all Bezier elements and should correspond to the
    Bernstein basis degree. The field variable is defined by its DOFs - the
    coefficients of the NURBS basis.

    Parameters
    ----------
    variable : array
        The DOF values of the variable with n_c components, shape (:, n_c).
    qps : array
        The quadrature points coordinates with components in [0, 1] reference
        element domain.
    control_points : array
        The NURBS control points.
    weights : array
        The NURBS weights.
    degrees : sequence of ints or int
        The basis degrees in each parametric dimension.
    cs : list of lists of 2D arrays
        The element extraction operators in each parametric dimension.
    conn : array
        The connectivity of the global NURBS basis.
    cells : array, optional
        If given, use only the given Bezier elements.

    Returns
    -------
    coors : array
        The physical coordinates of the quadrature points of all elements.
    vals : array
        The field variable values in the physical quadrature points.
    dets : array
        The Jacobians of the mapping to the unit reference element in the
        physical quadrature points.
    """
    if cells is None:
        cells = nm.arange(conn.shape[0])

    n_el = len(cells)
    n_qp = qps.shape[0]
    dim = control_points.shape[1]
    nc = variable.shape[1]

    # Output values of the variable.
    vals = nm.empty((n_el * n_qp, nc), dtype=nm.float64)

    # Output physical coordinates of QPs.
    coors = nm.empty((n_el * n_qp, dim), dtype=nm.float64)

    # Output Jacobians.
    dets = nm.empty((n_el * n_qp, 1), dtype=nm.float64)

    # Loop over elements.
    for iseq, ie in enumerate(cells):
        ec = conn[ie]
        vals_e = variable[ec]
        cps_e = control_points[ec]

        # Loop over quadrature points.
        for iqp, qp in enumerate(qps):
            ii = n_qp * iseq + iqp
            bf, bfg, det = eval_nurbs_basis_tp(qp, ie,
                                               control_points, weights,
                                               degrees, cs, conn)
            vals_qp = nm.dot(bf, vals_e)
            vals[ii, :] = vals_qp

            coors_qp = nm.dot(bf, cps_e)
            coors[ii, :] = coors_qp

            dets[ii] = det

    return coors, vals, dets

########NEW FILE########
__FILENAME__ = io
"""
IO for NURBS and Bezier extraction data.
"""
import numpy as nm
import tables as pt

def write_iga_data(filename, knots, degrees, control_points, weights, cs, conn,
                   bezier_control_points, bezier_weights, bezier_conn, regions):
    """
    Write IGA-related data into a HDF5 file using pytables.
    """
    if isinstance(degrees, int): degrees = [degrees]
    degrees = nm.asarray(degrees)

    fd = pt.openFile(filename, mode='w', title='SfePy IGA data file')

    nurbs = fd.createGroup('/', 'nurbs', 'nurbs')

    fd.createArray(nurbs, 'dim', control_points.shape[1], 'dim')
    fd.createArray(nurbs, 'tdim', len(degrees), 'tdim')
    for ii, kv in enumerate(knots):
        name = 'knots_%d' % ii
        fd.createArray(nurbs, name, kv, name)
    fd.createArray(nurbs, 'degrees', degrees, 'degrees')
    fd.createArray(nurbs, 'control_points', control_points, 'control_points')
    fd.createArray(nurbs, 'weights', weights, 'weights')

    bezier = fd.createGroup('/', 'bezier', 'bezier')

    fd.createArray(bezier, 'bezier_control_points', bezier_control_points,
                   'bezier_control_points')
    fd.createArray(bezier, 'bezier_weights', bezier_weights, 'bezier_weights')
    for ii, op in enumerate(cs):
        name = 'extraction_%d' % ii
        fd.createArray(bezier, name, op, name)
    fd.createArray(bezier, 'global_connectivity', conn, 'global_connectivity')
    fd.createArray(bezier, 'bezier_connectivity', bezier_conn,
                   'bezier_connectivity')

    regs = fd.createGroup('/', 'regions', 'regions')
    for key, val in regions.iteritems():
        fd.createArray(regs, key, val, key)

    fd.close()

def read_iga_data(filename):
    """
    Read IGA-related data from a HDF5 file using pytables.
    """
    fd = pt.openFile(filename, mode='r')

    nurbs = fd.root.nurbs

    tdim = nurbs.tdim.read()

    knots = []
    for ii in xrange(tdim):
        name = 'knots_%d' % ii
        knots.append(nurbs._f_getChild(name).read())
    knots = tuple(knots)

    degrees = nurbs.degrees.read()
    control_points = nurbs.control_points.read()
    weights = nurbs.weights.read()

    bezier = fd.root.bezier

    cs = []
    for ii in xrange(tdim):
        name = 'extraction_%d' % ii
        cs.append(bezier._f_getChild(name).read())

    conn = bezier.global_connectivity.read()
    bezier_control_points = bezier.bezier_control_points.read()
    bezier_weights = bezier.bezier_weights.read()
    bezier_conn = bezier.bezier_connectivity.read()

    regions = {}
    for region in fd.root.regions:
        regions[region.name] = region.read()

    fd.close()

    return (knots, degrees, control_points, weights, cs, conn,
            bezier_control_points, bezier_weights, bezier_conn, regions)

########NEW FILE########
__FILENAME__ = mappings
"""
Reference mappings for isogeometric analysis.
"""
import numpy as nm

from sfepy.discrete.common.mappings import Mapping
import sfepy.discrete.iga.iga as iga
from sfepy.discrete.fem.extmods.mappings import CMapping

class IGMapping(Mapping):
    """
    Reference mapping for isogeometric analysis based on Bezier extraction.
    """

    def __init__(self, domain, cells):
        self.domain = domain
        self.cells = cells
        self.v_shape = (len(cells), -1, self.domain.shape.dim)
        self.s_shape = (len(cells), -1, 1)

    def get_geometry(self):
        """
        Return reference element geometry as a GeometryElement instance.
        """
        return self.domain.gel

    def get_physical_qps(self, qp_coors):
        """
        Get physical quadrature points corresponding to given reference
        Bezier element quadrature points.

        Returns
        -------
        qps : array
            The physical quadrature points ordered element by element,
            i.e. with shape (n_el, n_qp, dim).
        """
        nurbs = self.domain.nurbs
        variable = nm.ones((nurbs.weights.shape[0], 1), dtype=nm.float64)
        qps, _, _ = iga.eval_variable_in_qp(variable, qp_coors, nurbs.cps,
                                            nurbs.weights, nurbs.degrees,
                                            nurbs.cs, nurbs.conn, self.cells)
        qps = qps.reshape(self.v_shape)

        return qps

    def get_mapping(self, qp_coors, weights):
        """
        Get the mapping for given quadrature points and weights.

        Returns
        -------
        cmap : CMapping instance
            The reference mapping.

        Notes
        -----
        Does not set total volume of the C mapping structure!
        """
        nurbs = self.domain.nurbs
        bfs, bfgs, dets = iga.eval_mapping_data_in_qp(qp_coors, nurbs.cps,
                                                      nurbs.weights,
                                                      nurbs.degrees, nurbs.cs,
                                                      nurbs.conn, self.cells)
        # Weight Jacobians by quadrature point weights.
        dets = nm.abs(dets) * weights[None, :, None, None]

        # Cell volumes.
        volumes = dets.sum(axis=1)[..., None]

        cmap = CMapping(self.v_shape[0], qp_coors.shape[0], self.v_shape[2],
                        bfs.shape[3], mode='volume', flag=1)

        cmap.bf[:] = bfs
        cmap.bfg[:] = bfgs
        cmap.det[:] = dets
        cmap.volume[:] = volumes

        return cmap

########NEW FILE########
__FILENAME__ = plot_nurbs
import numpy as nm
import matplotlib.pyplot as plt

from sfepy.discrete.fem.geometry_element import GeometryElement
from sfepy.mesh.mesh_generators import get_tensor_product_conn
import sfepy.postprocess.plot_dofs as pd
from sfepy.postprocess.plot_dofs import _get_axes

from sfepy.discrete.iga.iga import _get_knots_tuple

def plot_parametric_mesh(ax, knots):
    """
    Plot the parametric mesh of a NURBS given by its knots.
    """
    knots = _get_knots_tuple(knots)
    dim = len(knots)

    ax = _get_axes(ax, dim)

    uknots = [nm.unique(ii) for ii in knots]
    shape = [len(ii) for ii in uknots]

    ngrid = nm.mgrid[[slice(ii) for ii in shape]]
    coors = nm.r_[[uknots[ii][ig].ravel() for ii, ig in enumerate(ngrid)]].T

    conn, desc = get_tensor_product_conn(nm.array(shape))
    gel = GeometryElement(desc)

    ax = pd.plot_mesh(ax, coors, conn, gel.edges)
    pd.plot_points(ax, coors)

    return ax

def plot_control_mesh(ax, control_points, label=False):
    """
    Plot the control mesh of a NURBS given by its control points.
    """
    dim = control_points.shape[-1]
    ax = _get_axes(ax, dim)

    shape = control_points.shape

    conn, desc = get_tensor_product_conn(nm.array(shape[:-1]))
    gel = GeometryElement(desc)

    coors = control_points.reshape((-1, dim))

    ax = pd.plot_mesh(ax, coors, conn, gel.edges)
    pd.plot_points(ax, coors)

    if label:
        for ii, cc in enumerate(coors):
            if dim == 3:
                cx, cy, cz = cc
                ax.text(cx, cy, cz, '%d' % ii,
                        color='g', fontsize=12, weight='bold')

            else:
                cx, cy = cc
                ax.text(cx, cy, '%d' % ii,
                        color='g', fontsize=12, weight='bold')

    return ax

def _get_edges(n_ep, shape):
    dim = len(shape)
    aux = nm.arange(n_ep).reshape(shape)

    edges = []
    if dim == 3:
        for ii in xrange(shape[2] - 1):
            edges.append(aux[0, 0, ii:ii+2])
            edges.append(aux[-1, 0, ii:ii+2])
            edges.append(aux[0, -1, ii:ii+2])
            edges.append(aux[-1, -1, ii:ii+2])

        for ii in xrange(shape[1] - 1):
            edges.append(aux[0, ii:ii+2, 0])
            edges.append(aux[-1, ii:ii+2, 0])
            edges.append(aux[0, ii:ii+2, -1])
            edges.append(aux[-1, ii:ii+2, -1])

        for ii in xrange(shape[0] - 1):
            edges.append(aux[ii:ii+2, 0, 0])
            edges.append(aux[ii:ii+2, -1, 0])
            edges.append(aux[ii:ii+2, 0, -1])
            edges.append(aux[ii:ii+2, -1, -1])

    elif dim == 2:
        for ii in xrange(shape[1] - 1):
            edges.append(aux[0, ii:ii+2])
            edges.append(aux[-1, ii:ii+2])

        for ii in xrange(shape[0] - 1):
            edges.append(aux[ii:ii+2, 0])
            edges.append(aux[ii:ii+2, -1])

    else:
        for ii in xrange(shape[0] - 1):
            edges.append(aux[ii:ii+2])

    return nm.array(edges)

def plot_bezier_mesh(ax, control_points, conn, degrees, label=False):
    """
    Plot the Bezier mesh of a NURBS given by its control points and
    connectivity.
    """
    dim = control_points.shape[-1]
    ax = _get_axes(ax, dim)

    edges = _get_edges(conn.shape[1], nm.asarray(degrees) + 1)
    ax = pd.plot_mesh(ax, control_points, conn, edges)
    pd.plot_points(ax, control_points)

    if label:
        ax = pd.plot_global_dofs(ax, control_points, conn)

    return ax

def plot_iso_lines(ax, nurbs, color='b', n_points=100):
    """
    Plot the NURBS object using iso-lines in Greville abscissae coordinates.
    """
    dim = nurbs.dim
    ax = _get_axes(ax, dim)

    gas = nurbs.greville()

    if dim == 1:
        ga = gas[0]
        line = nm.linspace(ga[0], ga[-1], n_points)

        vals = nurbs(line)[:, 0]
        ax.plot(line, vals, color)

    elif dim == 2:
        ga0 = gas[0]
        ga1 = gas[1]

        x1 = nm.linspace(ga1[0], ga1[-1], n_points)
        for x0 in ga0:
            vals = nurbs(x0, x1)
            ax.plot(vals[:, 0], vals[:, 1], color)

        x0 = nm.linspace(ga0[0], ga0[-1], n_points)
        for x1 in ga0:
            vals = nurbs(x0, x1)
            ax.plot(vals[:, 0], vals[:, 1], color)

    else:
        ga0 = gas[0]
        ga1 = gas[1]
        ga2 = gas[2]

        x2 = nm.linspace(ga2[0], ga2[-1], n_points)
        for x0 in ga0:
            for x1 in ga1:
                vals = nurbs(x0, x1, x2)
                ax.plot(vals[:, 0], vals[:, 1], vals[:, 2], color)

        x1 = nm.linspace(ga1[0], ga1[-1], n_points)
        for x0 in ga0:
            for x2 in ga2:
                vals = nurbs(x0, x1, x2)
                ax.plot(vals[:, 0], vals[:, 1], vals[:, 2], color)

        x0 = nm.linspace(ga0[0], ga0[-1], n_points)
        for x1 in ga1:
            for x2 in ga2:
                vals = nurbs(x0, x1, x2)
                ax.plot(vals[:, 0], vals[:, 1], vals[:, 2], color)

    return ax

def plot_nurbs_basis_1d(ax, nurbs, n_points=100, x_axis='parametric',
                        legend=False):
    """
    Plot a 1D NURBS basis.
    """
    dim = nurbs.dim
    ax = _get_axes(ax, dim)

    ga = nurbs.greville()[0]

    n_fun = nurbs.weights.shape[0]
    line = nm.linspace(ga[0], ga[-1], n_points)
    for ii in xrange(n_fun):
        field = nm.zeros(n_fun)
        field[ii] = 1.0

        vals = nurbs.evaluate(fields=field, u=line)
        if x_axis == 'parametric':
            plt.plot(line, vals, label='%d' % ii)

        else:
            coors = nurbs(u=line)[:, x_axis]
            plt.plot(coors, vals, label='%d' % ii)

    if legend: plt.legend()

    return ax

def plot_bezier_nurbs_basis_1d(ax, control_points, weights, degrees, cs, conn,
                               n_points=20):
    """
    Plot a 1D NURBS basis using the Bezier extraction and local Bernstein
    basis.
    """
    from sfepy.discrete.iga.iga import eval_variable_in_qp
    ax = _get_axes(ax, 2)

    n_fun = weights.shape[0]
    line = nm.linspace(0, 1, n_points)[:, None]
    for ii in xrange(n_fun):
        variable = nm.zeros((n_fun, 1))
        variable[ii] = 1.0

        coors, vals, dets = eval_variable_in_qp(variable, line,
                                                control_points,
                                                weights, degrees,
                                                cs, conn)
        plt.plot(coors[:, 0], vals)

    return ax

########NEW FILE########
__FILENAME__ = utils
"""
Utility functions based on igakit.
"""
import numpy as nm

from sfepy.base.base import Struct
from sfepy.discrete.fem import Mesh
from sfepy.mesh.mesh_generators import get_tensor_product_conn

def create_linear_fe_mesh(nurbs, pars=None):
    """
    Convert a NURBS object into a nD-linear tensor product FE mesh.

    Parameters
    ----------
    nurbs : igakit.nurbs.NURBS instance
        The NURBS object.
    pars : sequence of array, optional
        The values of parameters in each parametric dimension. If not given,
        the values are set so that the resulting mesh has the same number of
        vertices as the number of control points/basis functions of the NURBS
        object.

    Returns
    -------
    coors : array
        The coordinates of mesh vertices.
    conn : array
        The vertex connectivity array.
    desc : str
        The cell kind.
    """
    knots = nurbs.knots
    shape = nurbs.weights.shape

    if pars is None:
        pars = []
        for ii, kv in enumerate(knots):
            par = nm.linspace(kv[0], kv[-1], shape[ii])
            pars.append(par)

    coors = nurbs(*pars)
    coors.shape = (-1, coors.shape[-1])

    conn, desc = get_tensor_product_conn([len(ii) for ii in pars])

    if (coors[:, -1] == 0.0).all():
        coors = coors[:, :-1]

    return coors, conn, desc

def create_mesh_and_output(nurbs, pars=None, **kwargs):
    """
    Create a nD-linear tensor product FE mesh using
    :func:`create_linear_fe_mesh()`, evaluate field variables given as keyword
    arguments in the mesh vertices and create a dictionary of output data
    usable by Mesh.write().

    Parameters
    ----------
    nurbs : igakit.nurbs.NURBS instance
        The NURBS object.
    pars : sequence of array, optional
        The values of parameters in each parametric dimension. If not given,
        the values are set so that the resulting mesh has the same number of
        vertices as the number of control points/basis functions of the NURBS
        object.
    **kwargs : kwargs
        The field variables as keyword arguments. Their names serve as keys in
        the output dictionary.

    Returns
    -------
    mesh : Mesh instance
        The finite element mesh.
    out : dict
        The output dictionary.
    """
    coors, conn, desc = create_linear_fe_mesh(nurbs, pars)
    mat_id = nm.zeros(conn.shape[0], dtype=nm.int32)
    mesh = Mesh.from_data('nurbs', coors, None, [conn], [mat_id], [desc])

    out = {}
    for key, variable in kwargs.iteritems():
        if variable.ndim == 2:
            nc = variable.shape[1]
            field = variable.reshape(nurbs.weights.shape + (nc,))

        else:
            field = variable.reshape(nurbs.weights.shape)
            nc = 1

        vals = nurbs.evaluate(field, *pars)
        out[key] = Struct(name='output_data', mode='vertex',
                          data=vals.reshape((-1, nc)))

    return mesh, out

def save_basis(nurbs, pars):
    """
    Save a NURBS object basis on a FE mesh corresponding to the given
    parametrization in VTK files.

    Parameters
    ----------
    nurbs : igakit.nurbs.NURBS instance
        The NURBS object.
    pars : sequence of array, optional
        The values of parameters in each parametric dimension.
    """
    coors, conn, desc = create_linear_fe_mesh(nurbs, pars)
    mat_id = nm.zeros(conn.shape[0], dtype=nm.int32)
    mesh = Mesh.from_data('nurbs', coors, None, [conn], [mat_id], [desc])

    n_dof = nurbs.weights.ravel().shape[0]
    variable = nm.zeros(n_dof, dtype=nm.float64)
    field = variable.reshape(nurbs.weights.shape)
    for ic in xrange(n_dof):
        variable[ic - 1] = 0.0
        variable[ic] = 1.0

        vals = nurbs.evaluate(field, *pars).reshape((-1))
        out = {}
        out['bf'] = Struct(name='output_data', mode='vertex',
                           data=vals[:, None])
        mesh.write('iga_basis_%03d.vtk' % ic, io='auto', out=out)

########NEW FILE########
__FILENAME__ = integrals
"""
Classes for accessing quadrature points and weights for various reference
element geometries.
"""
import numpy as nm

from sfepy.base.base import OneTypeList, Container, Struct, basestr
from quadratures import QuadraturePoints

class Integrals(Container):
    """
    Container for instances of :class:`Integral`.
    """

    @staticmethod
    def from_conf(conf):
        objs = OneTypeList(Integral)

        for desc in conf.itervalues():
            if hasattr(desc, 'vals'):
                aux = Integral(desc.name,
                               coors=desc.vals,
                               weights=desc.weights)

            else:
                aux = Integral(desc.name,
                               order=desc.order)

            objs.append(aux)

        obj = Integrals(objs)
        return obj

    def get(self, name):
        """
        Return existing or new integral.

        Parameters
        ----------
        name : str
            The name can either be a non-negative integer, a string
            representation of a non-negative integer (the integral
            order) or 'a' (automatic order) or a string beginning with
            'i' (existing custom integral name).
        """
        if name == 'a':
            raise NotImplementedError

        elif isinstance(name, basestr) and (name[0] == 'i'):
            try:
                obj = self[name]

            except IndexError:
                raise ValueError('integral %s is not defined!' % name)

        else:
            try:
                order = int(name)

            except:
                raise ValueError('unsupported integral reference! (%s)' % name)

            name = '__o%d' % order
            if self.has_key(name):
                obj = self[name]

            else:
                # Create new integral, and add it to self.
                obj = Integral(name, order=order)

                self.append(obj)

        return obj

class Integral(Struct):
    """
    Wrapper class around quadratures.
    """

    def __init__(self, name, order=1, coors=None, weights=None):
        self.name = name
        self.qps = {}

        if coors is None:
            self.mode = 'builtin'

        else:
            self.mode = 'custom'
            self.coors = coors
            self.weights = weights

        self.order = 0

        if order in ('auto', 'custom', 'a', 'c'):
            self.order = -1

        else:
            self.order = int(order)

    def get_qp(self, geometry):
        """
        Get quadrature point coordinates and corresponding weights for given
        geometry. For built-in quadratures, the integration order is given by
        `self.order`.

        Parameters
        ----------
        geometry : str
            The geometry key describing the integration domain,
            see the keys of `sfepy.discrete.quadratures.quadrature_tables`.

        Returns
        -------
        coors : array
           The coordinates of quadrature points.
        weights: array
           The quadrature weights.
        """

        if geometry in self.qps:
            qp = self.qps[geometry]

        else:
            if self.mode == 'builtin':
                qp = QuadraturePoints.from_table(geometry, self.order)

            else:
                qp = QuadraturePoints(None,
                                      coors=self.coors, weights=self.weights)

            self.qps[geometry] = qp

        return qp.coors, qp.weights

    def integrate(self, function, order=1, geometry='1_2'):
        """
        Integrate numerically a given scalar function.

        Parameters
        ----------
        function : callable(coors)
            The function of space coordinates to integrate.
        order : int, optional
            The integration order. For tensor product geometries, this is the
            1D (line) order.
        geometry : str
            The geometry key describing the integration domain. Default
            is `'1_2'`, i.e. a line integral in [0, 1]. For other values
            see the keys of `sfepy.discrete.quadratures.quadrature_tables`.

        Returns
        -------
        val : float
            The value of the integral.
        """
        qp = QuadraturePoints.from_table(geometry, order)

        fvals = function(qp.coors)
        val = nm.sum(fvals * qp.weights)

        return val

########NEW FILE########
__FILENAME__ = mass_operator
from sfepy.base.base import Struct
from sfepy.solvers import Solver

class MassOperator(Struct):
    """
    Encapsulation of action and inverse action of a mass matrix operator
    :math:`M`.
    """

    def __init__(self, problem, options):
        self.mtx_mass = problem.evaluate(options.mass, mode='weak',
                                         auto_init=True, dw_mode='matrix')

        if options.lumped:
            raise NotImplementedError

        else:
            # Initialize solvers (and possibly presolve the matrix).
            self.ls = Solver.any_from_conf(problem.ls_conf, mtx=self.mtx_mass,
                                           presolve=True)

    def action(self, vec):
        """
        Action of mass matrix operator on a vector: :math:`M x`.
        """
        return self.mtx_mass * vec

    def inverse_action(self, vec):
        """
        Inverse action of mass matrix operator on a vector: :math:`M^{-1} x`.
        """
        return self.ls(vec)

########NEW FILE########
__FILENAME__ = materials
import time
from copy import copy

from sfepy.base.base import (Struct, Container, OneTypeList, assert_,
                             output, get_default, basestr)
from functions import ConstantFunction, ConstantFunctionByRegion


##
# 21.07.2006, c
class Materials( Container ):

    @staticmethod
    def from_conf(conf, functions, wanted=None):
        """
        Construct Materials instance from configuration.
        """
        if wanted is None:
            wanted = conf.keys()

        objs = OneTypeList(Material)
        for key, mc in conf.iteritems():
            if key not in wanted: continue

            mat = Material.from_conf(mc, functions)
            objs.append(mat)

        obj = Materials( objs )
        return obj

    def semideep_copy(self, reset=True):
        """Copy materials, while external data (e.g. region) remain shared."""
        others = copy(self)
        others.update(OneTypeList(Material))
        for mat in self:
            other = mat.copy(name=mat.name)
            if reset:
                other.reset()
            others.append(other)
        return others

    def reset(self):
        """Clear material data so that next materials.time_update() is
        performed even for stationary materials."""
        for mat in self:
            mat.reset()

    def time_update(self, ts, equations, mode='normal', problem=None,
                    verbose=True):
        """
        Update material parameters for given time, problem, and equations.

        Parameters
        ----------
        ts : TimeStepper instance
            The time stepper.
        equations : Equations instance
            The equations using the materials.
        mode : 'normal', 'update' or 'force'
            The update mode, see :func:`Material.time_update()`.
        problem : Problem instance, optional
            The problem that can be passed to user functions as a context.
        verbose : bool
            If False, reduce verbosity.
        """
        if verbose: output('updating materials...')
        tt = time.clock()
        for mat in self:
            if verbose: output(' ', mat.name)
            mat.time_update(ts, equations, mode=mode, problem=problem)
        if verbose: output('...done in %.2f s' % (time.clock() - tt))

##
# 21.07.2006, c
class Material( Struct ):
    """
    A class holding constitutive and other material parameters.

    Example input::

        material_2 = {
           'name' : 'm',
           'values' : {'E' : 1.0},
        }

    Material parameters are passed to terms using the dot notation,
    i.e. 'm.E' in our example case.
    """
    @staticmethod
    def from_conf(conf, functions):
        """
        Construct Material instance from configuration.
        """
        kind = conf.get('kind', 'time-dependent')
        flags = conf.get('flags', {})

        function = conf.get('function', None)
        values = conf.get('values', None)

        if isinstance(function, basestr):
            function = functions[function]

        obj =  Material(conf.name, kind, function, values, flags)

        return obj

    def __init__(self, name, kind='time-dependent',
                 function=None, values=None, flags=None, **kwargs):
        """
        Parameters
        ----------
        name : str
            The name of the material.
        kind : 'time-dependent' or 'stationary'
            The kind of the material.
        function : function
            The function for setting up the material values.
        values : dict
            Constant material values.
        flags : dict, optional
            Special flags.
        **kwargs : keyword arguments, optional
            Constant material values passed by their names.
        """
        Struct.__init__(self, name=name, kind=kind, is_constant=False)

        if (function is not None) and ((values is not None) or len(kwargs)):
            msg = 'material can have function or values but not both! (%s)' \
                  % self.name
            raise ValueError(msg)

        self.flags = get_default(flags, {})

        if hasattr(function, '__call__'):
            self.function = function

        elif (values is not None) or len(kwargs): # => function is None
            if isinstance(values, dict):
                key0 = values.keys()[0]
                assert_(isinstance(key0, str))

            else:
                key0 = None

            if (key0 and (not key0.startswith('.'))
                and isinstance(values[key0], dict)):
                self.function = ConstantFunctionByRegion(values)
                self.is_constant = True

            else:
                all_values = {}
                if values is not None:
                    all_values.update(values)
                all_values.update(kwargs)

                self.function = ConstantFunction(all_values)
                self.is_constant = True

        else: # => both values and function are None
            msg = 'material %s: neither function nor values given! (%s)' \
                  % self.name
            raise ValueError(msg)

        self.reset()

    def iter_terms(self, equations, only_new=True):
        """
        Iterate terms for which the material data should be evaluated.
        """
        if equations is None: raise StopIteration

        for equation in equations:
            for term in equation.terms:
                names = [ii[0] for ii in term.names.material]
                if self.name not in names: continue

                key = term.get_qp_key()
                if only_new and (key in self.datas): continue

                self.datas.setdefault(key, {})

                yield key, term

    def set_data(self, key, ig, qps, data, indx):
        """
        Set the material data in quadrature points.

        Parameters
        ----------
        key : tuple
            The (region_name, integral_name) data key.
        ig : int
            The element group id.
        qps : Struct
            Information about the quadrature points.
        data : dict
            The material data. Changes the shape of data!
        indx : array
            The indices of quadrature points in the group `ig`.
        """
        datas = self.datas[key]

        # Restore shape to (n_el, n_qp, ...) until the C
        # core is rewritten to work with a bunch of physical
        # point values only.
        group_data = {}
        if qps.is_uniform:
            if data is not None:
                for key, val in data.iteritems():
                    aux = val[indx]
                    aux.shape = qps.get_shape(aux.shape, ig)
                    group_data[key] = aux
        else:
            raise NotImplementedError

        datas[ig] = group_data

    def set_data_from_variable(self, var, name, equations):
        for key, term in self.iter_terms(equations):
            qps = term.get_physical_qps()
            for ig in term.igs():
                data = var.evaluate_at(qps.values[ig])
                data.shape = data.shape + (1,)

                self.set_data(key, ig, qps, {name : data})

    def update_data(self, key, ts, equations, term, problem=None):
        """
        Update the material parameters in quadrature points.

        Parameters
        ----------
        key : tuple
            The (region_name, integral_name) data key.
        ts : TimeStepper
            The time stepper.
        equations : Equations
            The equations for which the update occurs.
        term : Term
            The term for which the update occurs.
        problem : Problem, optional
            The problem definition for which the update occurs.
        """
        self.datas.setdefault(key, {})

        qps = term.get_physical_qps()
        coors = qps.get_merged_values()
        data = self.function(ts, coors, mode='qp',
                             equations=equations, term=term, problem=problem,
                             group_indx=qps.rindx,
                             **self.extra_args)

        for ig, indx in qps.rindx.iteritems():
            if (qps.n_per_group[ig] == 0):
                self.set_data(key, ig, qps, None, None)

            else:
                self.set_data(key, ig, qps, data, indx)

    def update_special_data(self, ts, equations, problem=None):
        """
        Update the special material parameters.

        Parameters
        ----------
        ts : TimeStepper
            The time stepper.
        equations : Equations
            The equations for which the update occurs.
        problem : Problem, optional
            The problem definition for which the update occurs.
        """
        if 'special' in self.datas: return

        # Special function values (e.g. flags).
        datas = self.function(ts, None, mode='special',
                              problem=problem, equations=equations,
                              **self.extra_args)
        if datas is not None:
            self.datas['special'] = datas
            self.special_names.update(datas.keys())

    def update_special_constant_data(self, equations=None, problem=None):
        """
        Update the special constant material parameters.

        Parameters
        ----------
        equations : Equations
            The equations for which the update occurs.
        problem : Problem, optional
            The problem definition for which the update occurs.
        """
        if 'special_constant' in self.datas: return
        if not self.flags.get('special_constant'): return

        # Special constant values.
        datas = self.function(None, None, mode='special_constant',
                              problem=problem, equations=equations)
        self.datas['special_constant'] = datas
        self.constant_names.update(datas.keys())

    def time_update(self, ts, equations, mode='normal', problem=None):
        """
        Evaluate material parameters in physical quadrature points.

        Parameters
        ----------
        ts : TimeStepper instance
            The time stepper.
        equations : Equations instance
            The equations using the materials.
        mode : 'normal', 'update' or 'force'
            The update mode. In 'force' mode, ``self.datas`` is cleared and all
            updates are redone. In 'update' mode, existing data are preserved
            and new can be added. The 'normal' mode depends on other
            attributes: for stationary (``self.kind == 'stationary'``)
            materials and materials in 'user' mode, nothing is done if
            ``self.datas`` is not empty. For time-dependent materials
            (``self.kind == 'time-dependent'``, the default) that are not
            constant, i.e., are given by a user function, 'normal' mode behaves
            like 'force' mode. For constant materials it behaves like 'update'
            mode - existing data are reused.
        problem : Problem instance, optional
            The problem that can be passed to user functions as a context.
        """
        if mode == 'force':
            self.datas = {}

        elif self.datas:
            if mode == 'normal':
                if (self.mode == 'user') or (self.kind == 'stationary'):
                    return

                elif not self.is_constant:
                    self.datas = {}

        for key, term in self.iter_terms(equations):
            self.update_data(key, ts, equations, term, problem=problem)

        self.update_special_data(ts, equations, problem=problem)
        self.update_special_constant_data(equations, problem=problem)

    def get_keys(self, region_name=None):
        """
        Get all data keys.

        Parameters
        ----------
        region_name : str
            If not None, only keys with this region are returned.
        """
        if not self.datas:
            keys = None

        elif region_name is None:
            keys = self.datas.keys()

        else:
            keys = [key for key in self.datas.keys()
                    if (isinstance(key, tuple) and key[0] == region_name)]

        return keys

    def set_all_data( self, datas ):
        """
        Use the provided data, set mode to 'user'.
        """
        self.mode = 'user'
        self.datas = datas

    def set_function(self, function):
        self.function = function
        self.reset()

    def reset(self):
        """
        Clear all data created by a call to ``time_update()``, set ``self.mode``
        to ``None``.
        """
        self.mode = None
        self.datas = {}
        self.special_names = set()
        self.constant_names = set()
        self.extra_args = {}

    ##
    # 01.08.2007, c
    def set_extra_args(self, **extra_args):
        """Extra arguments passed tu the material function."""
        self.extra_args = extra_args

    def get_data( self, key, ig, name ):
        """`name` can be a dict - then a Struct instance with data as
        attributes named as the dict keys is returned."""
##         print 'getting', self.name, name

        if isinstance(name, basestr):
            return self._get_data( key, ig, name )
        else:
            out = Struct()
            for key, item in name.iteritems():
                setattr( out, key, self._get_data( key, ig, item ) )
            return out

    def _get_data( self, key, ig, name ):
        if name is None:
            msg = 'material arguments must use the dot notation!\n'\
                  '(material: %s, key: %s)' % (self.name, key)
            raise ValueError( msg )

        if not self.datas:
            raise ValueError( 'material data not set! (call time_update())' )

        if name in self.special_names:
            # key, ig ignored.
            return self.datas['special'][name]

        else:
            datas = self.datas[key]

            if isinstance( datas[ig], Struct ):
                return getattr( datas[ig], name )
            elif datas[ig]:
                return datas[ig][name]

    def get_constant_data(self, name):
        """Get constant data by name."""
        if name in self.constant_names:
            # no key, ig.
            return self.datas['special_constant'][name]
        else:
            raise ValueError('material %s has no constant %s!'
                             % (self.name, name))

    ##
    # 01.08.2007, c
    def reduce_on_datas( self, reduce_fun, init = 0.0 ):
        """For non-special values only!"""
        out = {}.fromkeys(self.datas[self.datas.keys()[0]][0].keys(), init)
        for datas in self.datas.itervalues():
            for data in datas.itervalues():
                for key, val in data.iteritems():
                    out[key] = reduce_fun(out[key], val)

        return out

########NEW FILE########
__FILENAME__ = parse_equations
from pyparsing import Combine, Literal, Word, delimitedList, Group, Optional,\
     ZeroOrMore, OneOrMore, nums, alphas, alphanums,\
     StringStart, StringEnd, CaselessLiteral, Forward, oneOf

class TermParse(object):
    def __str__(self):
        ss = "%s\n" % self.__class__
        for key, val in self.__dict__.iteritems():
            ss += "  %s:\n    %s\n" % (key, self.__dict__[key])
        return ss

def collect_term(term_descs, lc):
    signs = {'+': 1.0, '-': -1.0}

    def append(str, loc, toks):
        sign = signs[toks.sign] * signs[lc[0]]
        tp = TermParse()
        tp.integral = toks.term_desc.integral
        if not tp.integral:
            tp.integral = 'a'
        tp.region = toks.term_desc.region
        tp.flag = toks.term_desc.flag
        tp.sign = sign * eval(''.join(toks.mul))
        tp.name = toks.term_desc.name
        tp.args = ', '.join(toks.args[0])
        term_descs.append(tp)
    return append

def rhs(lc):
    def aux(str, loc, toks):
        if toks:
            lc[0] = '-'
    return aux

def create_bnf(term_descs):
    """term_descs .. list of TermParse objects
    (sign, term_name, term_arg_names), where sign can be real or complex
    multiplier"""

    lc = ['+']  # Linear combination context.
    equal = Literal("=").setParseAction(rhs(lc))
    zero = Literal("0").suppress()

    point = Literal(".")
    e = CaselessLiteral("E")
    inumber = Word("+-" + nums, nums)
    fnumber = Combine(Word("+-" + nums, nums) +
                      Optional(point + Optional(Word(nums))) +
                      Optional(e + Word("+-" + nums, nums)))
    number = fnumber + Optional(Literal('j'), default='')
    add_op = oneOf('+ -')
    number_expr = Forward()
    number_expr << ZeroOrMore('(') + number \
                + ZeroOrMore(add_op + number_expr) \
                + ZeroOrMore(')')

    ident = Word(alphas, alphanums + "_")

    integral = Combine((Literal('i') + Word(alphanums)) | Literal('i')
                       | Literal('a') | Word(nums))("integral")

    history = Optional('[' + inumber + ']', default='')("history")

    variable = Combine(Word(alphas, alphanums + '._') + history)

    derivative = Combine(Literal('d') + variable \
                         + Literal('/') + Literal('dt'))

    trace = Combine(Literal('tr') + '(' + variable + ')')

    generalized_var = derivative | trace | variable
    args = Group(delimitedList(generalized_var))

    flag = Literal('a')

    term = Optional(Literal('+') | Literal('-'), default='+')("sign") \
                    + Optional(number_expr + Literal('*').suppress(),
                               default=['1.0', ''])("mul") \
                    + Combine(ident("name") \
                              + Optional("." + (integral + "."
                              + ident("region") + "." + flag("flag") |
                              integral + "." + ident("region") |
                              ident("region")
                              )))("term_desc") + "(" \
                    + Optional(args, default=[''])("args") + ")"
    term.setParseAction(collect_term(term_descs, lc))

    rhs1 = equal + OneOrMore(term)
    rhs2 = equal + zero
    equation = StringStart() + OneOrMore(term) \
               + Optional(rhs1 | rhs2) + StringEnd()
    ## term.setDebug()
    return equation

if __name__ == "__main__":

    test_str = """d_term1.Y(fluid, u, w, Nu, dcf, mode)
                  + 5.0 * d_term2.Omega(u, w, Nu, dcf, mode)
                  - d_another_term.Elsewhere(w, p[-1], Nu, dcf, mode)
                  = - dw_rhs.a.Y3(u, q, Nu, dcf, mode)"""

    term_descs = []
    bnf = create_bnf(term_descs)
    out = bnf.parseString(test_str)

    print 'out:', out, '\n'
    for tp in term_descs:
        print tp

########NEW FILE########
__FILENAME__ = parse_regions
"""
Grammar for selecting regions of a domain.

Regions serve for selection of certain parts of the computational domain
represented as a finite element mesh. They are used to define the boundary
conditions, the domains of terms and materials etc.

Notes
-----
History: pre-git versions already from from 13.06.2006.
"""
from pyparsing import Literal, CaselessLiteral, Word, delimitedList,\
     Group, Optional, ZeroOrMore, nums, alphas, alphanums,\
     Combine, StringStart, StringEnd, Forward, oneOf, ParseException

ParseException # Needed for importing elsewhere.

op_codes = ['OA_SubV', 'OA_SubE', 'OA_SubF', 'OA_SubC', 'OA_SubS',
            'OA_AddV', 'OA_AddE', 'OA_AddF', 'OA_AddC', 'OA_AddS',
            'OA_IntersectV', 'OA_IntersectE', 'OA_IntersectF',
            'OA_IntersectC', 'OA_IntersectS']
eval_codes = ['E_VIR', 'E_VOS', 'E_VBF', 'E_VOG', 'E_OVIR', 'E_VI', 'E_VOSET',
              'E_CBF', 'E_COG', 'E_CI1', 'E_CI2', 'E_COSET']
kw_codes = ['KW_All', 'KW_Region']

def to_stack(stack):
    def push_first(str, loc, toks):
        if toks:
            stack.append(toks[0])
        return toks
    return push_first

def replace(what, keep=False):
    def _replace(str, loc, toks):
        ret = {'token' : what, 'orig' : []}
        if keep:
            ret['orig'] = list(toks[0])
        return ret
    return _replace

def replace_with_region(what, r_index):
    def _replace(str, loc, toks):
        ret = {'token' : what, 'orig' : []}

        orig = toks[0]
        r_orig = orig[r_index]
        if isinstance(r_orig, dict) and (r_orig['token'] == 'KW_Region'):
            orig = list(orig[:r_index]) + r_orig['orig']
        ret['orig'] = orig
        return ret
    return _replace

def join_tokens(str, loc, toks):
    return [" ".join(toks[0])]

def visit_stack(stack, op_visitor, leaf_visitor):

    def visit(stack, level):
        op = stack.pop()

        token = op['token']
        if token in op_codes:
            res2 = visit(stack, level + 1)
            res1 = visit(stack, level + 1)
            return op_visitor(level, op, res1, res2)

        elif token in eval_codes:
            return leaf_visitor(level, op)

        elif token in kw_codes:
            return leaf_visitor(level, op)

        else:
            raise ValueError, token

    return visit(stack, 0)

def print_op(level, op, item1, item2):
    print level * '  ' + (': %s' % op)

def print_leaf(level, op):
    print level * '  ' + ('< %s' % op)

def print_stack(stack):
    visit_stack(stack, print_op, print_leaf)

def create_bnf(stack):
    point = Literal(".")
    comma = Literal(",")
    e = CaselessLiteral("E")
    inumber = Word(nums)
    fnumber = Combine(Word("+-"+nums, nums) +
                       Optional(point + Optional(Word(nums))) +
                       Optional(e + Word("+-"+nums, nums)))
    _of = Literal('of')
    _in = Literal('in')
    _by = Literal('by')
    _copy = Literal('copy')

    _mv = Literal('-v').setParseAction(replace('OA_SubV'))
    _me = Literal('-e').setParseAction(replace('OA_SubE'))
    _mf = Literal('-f').setParseAction(replace('OA_SubF'))
    _mc = Literal('-c').setParseAction(replace('OA_SubC'))
    _ms = Literal('-s').setParseAction(replace('OA_SubS'))
    _pv = Literal('+v').setParseAction(replace('OA_AddV'))
    _pe = Literal('+e').setParseAction(replace('OA_AddE'))
    _pf = Literal('+f').setParseAction(replace('OA_AddF'))
    _pc = Literal('+c').setParseAction(replace('OA_AddC'))
    _ps = Literal('+s').setParseAction(replace('OA_AddS'))
    _inv = Literal('*v').setParseAction(replace('OA_IntersectV'))
    _ine = Literal('*e').setParseAction(replace('OA_IntersectE'))
    _inf = Literal('*f').setParseAction(replace('OA_IntersectF'))
    _inc = Literal('*c').setParseAction(replace('OA_IntersectC'))
    _ins = Literal('*s').setParseAction(replace('OA_IntersectS'))
    regop = (_mv | _me | _mf | _mc | _ms |
             _pv | _pe | _pf | _pc | _ps |
             _inv | _ine | _inf | _inc | _ins)

    lpar  = Literal("(").suppress()
    rpar  = Literal(")").suppress()

    _all = Literal('all').setParseAction(replace('KW_All'))
    vertex = Literal('vertex')
    vertices = Literal('vertices')
    cell = Literal('cell')
    cells = Literal('cells')
    group = Literal('group')
    _set = Literal('set')
    surface = Literal('surface')

    ident = Word(alphas + '_.', alphanums + '_.')
    set_name = Word(nums) | ident

    function = Word(alphas + '_', alphanums + '_')
    function = Group(function).setParseAction(join_tokens)

    region = Combine(Literal('r.') + Word(alphas + '_',
                                          '_' + alphas + nums + '.'))
    region = Group(Optional(_copy, default='nocopy') + region)
    region.setParseAction(replace('KW_Region', keep=True))

    coor = oneOf('x y z')
    boolop = oneOf('& |')
    relop = oneOf('< > <= >= != ==')
    bool_term = (ZeroOrMore('(') + (coor | fnumber) + relop + (coor | fnumber)
                 + ZeroOrMore(')'))
    relation = Forward()
    relation << (ZeroOrMore('(')
                 + bool_term + ZeroOrMore(boolop + relation)
                 + ZeroOrMore(')'))
    relation = Group(relation).setParseAction(join_tokens)

    nos = Group(vertices + _of + surface).setParseAction(replace('E_VOS'))
    nir = Group(vertices + _in + relation).setParseAction(
        replace('E_VIR', keep=True))
    nbf = Group(vertices + _by + function).setParseAction(
        replace('E_VBF', keep=True))
    ebf = Group(cells + _by + function).setParseAction(
        replace('E_CBF', keep=True))
    eog = Group(cells + _of + group + Word(nums)).setParseAction(
        replace('E_COG', keep=True))
    nog = Group(vertices + _of + group + Word(nums)).setParseAction(
        replace('E_VOG', keep=True))
    onir = Group(vertex + _in + region).setParseAction(
        replace_with_region('E_OVIR', 2))
    ni = Group(vertex + delimitedList(inumber)).setParseAction(
        replace('E_VI', keep=True))
    ei1 = Group(cell + delimitedList(inumber)).setParseAction(
        replace('E_CI1', keep=True))
    etuple = (lpar.suppress() + inumber + comma.suppress()
              + inumber + rpar.suppress())
    ei2 = Group(cell + delimitedList(etuple)).setParseAction(
        replace('E_CI2', keep=True))
    noset = Group(vertices + _of + _set + set_name).setParseAction(
        replace('E_VOSET', keep=True))
    eoset = Group(cells + _of + _set + set_name).setParseAction(
        replace('E_COSET', keep=True))

    region_expression = Forward()

    atom1 = (_all | region | ni | onir | nos | nir | nbf
             | ei1 | ei2 | ebf | eog | nog | noset | eoset)
    atom1.setParseAction(to_stack(stack))
    atom2 = (lpar + region_expression.suppress() + rpar)
    atom = (atom1 | atom2)

    aux = (regop + region_expression)
    aux.setParseAction(to_stack(stack))
    region_expression << atom + ZeroOrMore(aux)
    region_expression = StringStart() + region_expression + StringEnd()

    return region_expression

########NEW FILE########
__FILENAME__ = probes
"""Classes for probing values of Variables, for example, along a line."""
import numpy as nm
import numpy.linalg as nla

from sfepy.base.base import get_default, basestr, Struct
from sfepy.linalg import make_axis_rotation_matrix, norm_l2_along_axis

def write_results(filename, probe, results):
    """
    Write probing results into a file.

    Parameters
    ----------
    filename : str or file object
        The output file name.
    probe : Probe subclass instance
        The probe used to obtain the results.
    results : dict
        The dictionary of probing results. Keys are data names, values are
        the probed values.
    """
    fd = open(filename, 'w') if isinstance(filename, basestr) else filename

    fd.write('\n'.join(probe.report()) + '\n')
    for key, result in results.iteritems():
        pars, vals = result
        fd.write('\n# %s %d\n' % (key, vals.shape[-1]))

        if vals.ndim == 1:
            aux = nm.hstack((pars[:,None], vals[:,None]))

        else:
            aux = nm.hstack((pars[:,None], vals))

        nm.savetxt(fd, aux)

    if isinstance(filename, basestr):
        fd.close()

def read_results(filename, only_names=None):
    """
    Read probing results from a file.

    Parameters
    ----------
    filename : str or file object
        The probe results file name.

    Returns
    -------
    header : Struct instance
        The probe data header.
    results : dict
        The dictionary of probing results. Keys are data names, values are
        the probed values.
    """
    from sfepy.base.ioutils import read_array

    only_names = get_default(only_names, [])

    fd = open(filename, 'r') if isinstance(filename, basestr) else filename

    header = read_header(fd)
    results = {}
    for name, nc in get_data_name(fd):
        if name not in only_names: continue

        result = read_array(fd, header.n_point, nc + 1, nm.float64)
        results[name] = result

    return header, results

def read_header(fd):
    """
    Read the probe data header from file descriptor fd.

    Returns
    -------
    header : Struct instance
        The probe data header.
    """
    header = Struct(name='probe_data_header')
    header.probe_class = fd.readline().strip()

    aux = fd.readline().strip().split(':')[1]
    header.n_point = int(aux.strip().split()[0])

    details = []
    while 1:
        line = fd.readline().strip()

        if line == '-----':
            break
        else:
            details.append(line)
    header.details = '\n'.join(details)

    return header

def get_data_name(fd):
    """
    Try to read next data name in file fd.

    Returns
    -------
    name : str
        The data name.
    nc : int
        The number of data columns.
    """
    name = None
    while 1:
        try:
            line = fd.readline()
            if (len(line) == 0): break
            if len(line) == 1: continue
        except:
            raise StopIteration

        line = line.strip().split()
        if (len(line) == 3) and (line[0] == '#'):
            name = line[1]
            nc = int(line[2])

            yield name, nc

class Probe(Struct):
    """
    Base class for all point probes. Enforces two points minimum.
    """
    cache = Struct(name='probe_shared_evaluate_cache')
    is_cyclic = False

    def __init__(self, name, share_geometry=True, n_point=None, **kwargs):
        """
        Parameters
        ----------
        name : str
            The probe name, set automatically by the subclasses.
        share_geometry : bool
            Set to True to indicate that all the probes will work on the same
            domain. Certain data are then computed only for the first probe and
            cached.
        n_point : int
           The (fixed) number of probe points, when positive. When non-positive,
           the number of points is adaptively increased starting from -n_point,
           until the neighboring point distance is less than the diameter of the
           elements enclosing the points. When None, it is set to -10.

        For additional parameters see the __init__() docstrings of the
        subclasses.
        """
        Struct.__init__(self, name=name, share_geometry=share_geometry,
                        **kwargs)

        self.set_n_point(n_point)

        self.options = Struct(close_limit=0.1, size_hint=None)
        self.cache = Struct(name='probe_local_evaluate_cache')

        self.is_refined = False

    def set_n_point(self, n_point):
        """
        Set the number of probe points.

        Parameters
        ----------
        n_point : int
           The (fixed) number of probe points, when positive. When non-positive,
           the number of points is adaptively increased starting from -n_point,
           until the neighboring point distance is less than the diameter of the
           elements enclosing the points. When None, it is set to -10.
        """
        if n_point is None:
            n_point = -10

        if n_point <= 0:
            n_point = max(-n_point, 2)
            self.n_point_required = -1

        else:
            n_point = max(n_point, 2)
            self.n_point_required = n_point

        self.n_point0 = self.n_point = n_point

    def set_options(self, close_limit=None, size_hint=None):
        """
        Set the probe options.

        Parameters
        ----------
        close_limit : float
            The maximum limit distance of a point from the closest
            element allowed for extrapolation.
        size_hint : float
            Element size hint for the refinement of probe parametrization.
        """
        if close_limit is not None:
            self.options.close_limit = close_limit

        if size_hint is not None:
            self.options.size_hint = size_hint

    def report(self):
        """Report the probe parameters."""
        out = [self.__class__.__name__]

        if self.n_point_required == -1:
            aux = 'adaptive'
        else:
            aux = 'fixed'
        out.append('number of points: %s (%s)' % (self.n_point, aux))

        return out

    def __call__(self, variable):
        """
        Probe the given variable. The actual implementation is in self.probe(),
        so that it can be overridden in subclasses.

        Parameters
        ----------
        variable : Variable instance
            The variable to be sampled along the probe.
        """
        return self.probe(variable)

    def probe(self, variable):
        """
        Probe the given variable.

        Parameters
        ----------
        variable : Variable instance
            The variable to be sampled along the probe.
        """
        refine_flag = None

        ev = variable.evaluate_at
        domain = variable.field.domain

        if self.share_geometry:
            cache = domain.get_evaluate_cache(cache=Probe.cache,
                                              share_geometry=True)

        else:
            cache = domain.get_evaluate_cache(cache=self.cache,
                                              share_geometry=False)

        self.reset_refinement()

        while True:
            pars, points = self.get_points(refine_flag)
            if not nm.isfinite(points).all():
                raise ValueError('Inf/nan in probe points!')

            vals, cells, status = ev(points, strategy='kdtree',
                                     close_limit=self.options.close_limit,
                                     cache=cache, ret_status=True)
            ii = nm.where(status > 1)[0]
            vals[ii] = nm.nan

            if self.is_refined:
                break

            else:
                refine_flag = self.refine_points(variable, points, cells)
                if (refine_flag == False).all():
                    break

        self.is_refined = True

        return pars, vals

    def reset_refinement(self):
        """
        Reset the probe refinement state.
        """
        self.is_refined = False
        self.n_point = self.n_point0

    def refine_points(self, variable, points, cells):
        """
        Mark intervals between points for a refinement, based on element
        sizes at those points. Assumes the points to be ordered.

        Returns
        -------
        refine_flag : bool array
            True at places corresponding to intervals between subsequent points
            that need to be refined.
        """
        if self.n_point_required == self.n_point:
            refine_flag = nm.array([False])

        else:
            if self.options.size_hint is None:
                ed = variable.get_element_diameters(cells, 0)
                pd = 0.5 * (ed[1:] + ed[:-1])

            else:
                pd = self.options.size_hint

            dist = norm_l2_along_axis(points[1:] - points[:-1])

            refine_flag = dist > pd
            if self.is_cyclic:
                pd1 = 0.5 * (ed[0] + ed[-1])
                dist1 = nla.norm(points[0] - points[-1])

                refine_flag = nm.r_[refine_flag, dist1 > pd1]

        return refine_flag

    @staticmethod
    def refine_pars(pars, refine_flag, cyclic_val=None):
        """
        Refine the probe parametrization based on the refine_flag.
        """
        ii = nm.where(refine_flag)[0]
        ip = ii + 1

        if cyclic_val is not None:
            cpars = nm.r_[pars, cyclic_val]
            pp = 0.5 * (cpars[ip] + cpars[ii])

        else:
            pp = 0.5 * (pars[ip] + pars[ii])

        pars = nm.insert(pars, ip, pp)

        return pars

class PointsProbe(Probe):
    """
    Probe variables in given points.
    """

    def __init__(self, points, share_geometry=True):
        """
        Parameters
        ----------
        points : array_like
            The coordinates of the points.
        """
        points = nm.array(points, dtype=nm.float64)
        if points.ndim == 1:
            points.shape = points.shape + (1,)
        n_point = points.shape[0]
        name = 'points %d' % n_point

        Probe.__init__(self, name=name, points=points, n_point=n_point)

        self.n_point_single = n_point

    def report(self):
        """Report the probe parameters."""
        out = Probe.report(self)
        for ii, point in enumerate(self.points):
            out.append('point %d: %s' % (ii, point))
        out.append('-----')
        return out

    def refine_points(self, variable, points, cache):
        """No refinement for this probe."""
        refine_flag = nm.array([False])
        return refine_flag

    def get_points(self, refine_flag=None):
        """
        Get the probe points.

        Returns
        -------
        pars : array_like
           The independent coordinate of the probe.
        points : array_like
           The probe points, parametrized by pars.
        """
        pars = nm.arange(self.n_point, dtype=nm.float64)
        return pars, self.points

class LineProbe(Probe):
    """
    Probe variables along a line.

    If n_point is positive, that number of evenly spaced points is used. If
    n_point is None or non-positive, an adaptive refinement based on element
    diameters is used and the number of points and their spacing are determined
    automatically. If it is negative, -n_point is used as an initial guess.
    """

    def __init__(self, p0, p1, n_point, share_geometry=True):
        """
        Parameters
        ----------
        p0 : array_like
            The coordinates of the start point.
        p1 : array_like
            The coordinates of the end point.
        """
        p0 = nm.array(p0, dtype=nm.float64)
        p1 = nm.array(p1, dtype=nm.float64)
        name = 'line [%s, %s]' % (p0, p1)

        Probe.__init__(self, name=name, p0=p0, p1=p1, n_point=n_point)

        dirvec = self.p1 - self.p0
        self.length = nm.linalg.norm(dirvec)
        self.dirvec = dirvec / self.length

    def report(self):
        """Report the probe parameters."""
        out = Probe.report(self)
        out.append('point 0: %s' % self.p0)
        out.append('point 1: %s' % self.p1)
        out.append('-----')
        return out

    def get_points(self, refine_flag=None):
        """
        Get the probe points.

        Returns
        -------
        pars : array_like
           The independent coordinate of the probe.
        points : array_like
           The probe points, parametrized by pars.
        """
        if self.is_refined:
            return self.pars, self.points

        if refine_flag is None:
            pars = nm.linspace(0, self.length, self.n_point)

        else:
            pars = Probe.refine_pars(self.pars, refine_flag)

            self.n_point = pars.shape[0]

        self.pars = pars

        self.points = self.p0 + self.dirvec * pars[:,None]

        return pars, self.points

class RayProbe(Probe):
    """
    Probe variables along a ray. The points are parametrized by a function of
    radial coordinates from a given point in a given direction.
    """

    def __init__(self, p0, dirvec, p_fun, n_point, both_dirs,
                 share_geometry=True):
        """
        Parameters
        ----------
        p0 : array_like
            The coordinates of the start point.
        dirvec : array_like
            The probe direction vector.
        p_fun : function
            The function returning the probe parametrization along the dirvec
            direction.
        both_dirs : bool
            If True, the probe works, starting at p0, symmetrically in both
            dirvec and -dirvec directions.
        """
        p0 = nm.array(p0, dtype=nm.float64)
        dirvec = nm.array(dirvec, dtype=nm.float64)
        dirvec /= nla.norm(dirvec)
        name = 'ray %s [%s, %s]' % (p_fun.__name__, p0, dirvec)

        if both_dirs:
            n_point_true  = 2 * n_point
        else:
            n_point_true = n_point

        Probe.__init__(self, name=name, p0=p0, dirvec=dirvec, p_fun=p_fun,
                       n_point=n_point_true, both_dirs=both_dirs)

        self.n_point_single = n_point

    def report(self):
        """Report the probe parameters."""
        out = Probe.report(self)
        out.append('point 0: %s' % self.p0)
        out.append('direction vector: %s' % self.dirvec)
        out.append('both directions: %s' % self.both_dirs)
        out.append('distribution function: %s' % self.p_fun.__name__)
        out.append('-----')
        return out

    def refine_points(self, variable, points, cache):
        """No refinement for this probe."""
        refine_flag = nm.array([False])
        return refine_flag

    def gen_points(self, sign):
        """Generate the probe points and their parametrization."""
        pars = self.p_fun(nm.arange(self.n_point_single, dtype=nm.float64))
        points = self.p0 + sign * self.dirvec * pars[:,None]
        return pars, points

    def get_points(self, refine_flag=None):
        """
        Get the probe points.

        Returns
        -------
        pars : array_like
           The independent coordinate of the probe.
        points : array_like
           The probe points, parametrized by pars.
        """
        pars, points = self.gen_points(1.0)
        if self.both_dirs:
            pars0, points0 = self.gen_points(-1.0)
            pars = nm.concatenate((-pars0[::-1], pars))
            points = nm.concatenate((points0[::-1], points))
        return pars, points

class CircleProbe(Probe):
    """
    Probe variables along a circle.

    If n_point is positive, that number of evenly spaced points is used. If
    n_point is None or non-positive, an adaptive refinement based on element
    diameters is used and the number of points and their spacing are determined
    automatically. If it is negative, -n_point is used as an initial guess.
    """
    is_cyclic = True

    def __init__(self, centre, normal, radius, n_point, share_geometry=True):
        """
        Parameters
        ----------
        centre : array_like
            The coordinates of the circle centre.
        normal : array_like
            The normal vector perpendicular to the circle plane.
        radius : float
            The radius of the circle.
        """
        centre = nm.array(centre, dtype=nm.float64)
        normal = nm.array(normal, dtype=nm.float64)
        normal /= nla.norm(normal)

        name = 'circle [%s, %s, %s]' % (centre, normal, radius)

        Probe.__init__(self, name=name, centre=centre, normal=normal,
                       radius=radius, n_point=n_point)

    def report(self):
        """Report the probe parameters."""
        out = Probe.report(self)
        out.append('centre: %s' % self.centre)
        out.append('normal: %s' % self.normal)
        out.append('radius: %s' % self.radius)
        out.append('-----')
        return out

    def get_points(self, refine_flag=None):
        """
        Get the probe points.

        Returns
        -------
        pars : array_like
           The independent coordinate of the probe.
        points : array_like
           The probe points, parametrized by pars.
        """
        # Vector of angles.
        if self.is_refined:
            return self.pars, self.points

        if refine_flag is None:
            pars = nm.linspace(0.0, 2.0*nm.pi, self.n_point + 1)[:-1]

        else:
            pars = Probe.refine_pars(self.pars, refine_flag,
                                     cyclic_val=2.0 * nm.pi)

            self.n_point = pars.shape[0]

        self.pars = pars

        # Create the points in xy plane, centered at the origin.
        x = self.radius * nm.cos(pars[:,None])
        y = self.radius * nm.sin(pars[:,None])

        if len(self.centre) == 3:
            z = nm.zeros((self.n_point, 1), dtype=nm.float64)
            points = nm.c_[x, y, z]

            # Rotate to satisfy the normal, shift to the centre.
            n1 = nm.array([0.0, 0.0, 1.0], dtype=nm.float64)
            axis = nm.cross(n1, self.normal)
            angle = nm.arccos(nm.dot(n1, self.normal))

            if nla.norm(axis) < 0.1:
                # n1 == self.normal
                rot_mtx = nm.eye(3, dtype=nm.float64)
            else:
                rot_mtx = make_axis_rotation_matrix(axis, angle)

            points = nm.dot(points, rot_mtx)

        else:
            points = nm.c_[x, y]

        points += self.centre

        self.points = points

        return pars, points

class IntegralProbe(Struct):
    """Evaluate integral expressions."""
    def __init__(self, name, problem, expressions, labels):
        Struct.__init__(self, name=name, problem=problem,
                        expressions=expressions, labels=labels)

    def __call__(self, ip, state=None, **kwargs):
        return self.problem.evaluate(self.expressions[ip], state, **kwargs)

########NEW FILE########
__FILENAME__ = problem
import os
import os.path as op
import time
from copy import copy

import numpy as nm

from sfepy.base.base import dict_from_keys_init, select_by_names
from sfepy.base.base import output, get_default, Struct, IndexedStruct
import sfepy.base.ioutils as io
from sfepy.base.conf import ProblemConf, get_standard_keywords
from sfepy.base.conf import transform_variables, transform_materials
from functions import Functions
from sfepy.discrete.fem.mesh import Mesh
from sfepy.discrete.common.fields import fields_from_conf
from variables import Variables, Variable
from materials import Materials, Material
from equations import Equations
from integrals import Integrals
from sfepy.discrete.state import State
from sfepy.discrete.conditions import Conditions
from sfepy.discrete.evaluate import create_evaluable, eval_equations
from sfepy.discrete.fem import fea
from sfepy.solvers.ts import TimeStepper
from sfepy.discrete.evaluate import BasicEvaluator, LCBCEvaluator
from sfepy.solvers import Solver
from sfepy.solvers.ls import ScipyDirect
from sfepy.solvers.nls import Newton

##
# 29.01.2006, c
class Problem(Struct):
    """
    Problem definition, the top-level class holding all data necessary to solve
    a problem.

    It can be constructed from a :class:`ProblemConf` instance using
    `Problem.from_conf()` or directly from a problem description file using
    `Problem.from_conf_file()`

    For interactive use, the constructor requires only the `equations`,
    `nls` and `ls` keyword arguments.
    """

    @staticmethod
    def from_conf_file(conf_filename, required=None, other=None,
                       init_fields=True, init_equations=True,
                       init_solvers=True):

        _required, _other = get_standard_keywords()
        if required is None:
            required = _required
        if other is None:
            other = _other

        conf = ProblemConf.from_file(conf_filename, required, other)

        obj = Problem.from_conf(conf, init_fields=init_fields,
                                init_equations=init_equations,
                                init_solvers=init_solvers)
        return obj

    @staticmethod
    def from_conf(conf, init_fields=True, init_equations=True,
                  init_solvers=True):
        if conf.options.get('absolute_mesh_path', False):
            conf_dir = None
        else:
            conf_dir = op.dirname(conf.funmod.__file__)

        functions = Functions.from_conf(conf.functions)

        if conf.get('filename_mesh') is not None:
            from sfepy.discrete.fem.domain import FEDomain

            mesh = Mesh.from_file(conf.filename_mesh, prefix_dir=conf_dir)
            domain = FEDomain(mesh.name, mesh)
            if conf.options.get('ulf', False):
                domain.mesh.coors_act = domain.mesh.coors.copy()

        elif conf.get('filename_domain') is not None:
            from sfepy.discrete.iga.domain import IGDomain
            domain = IGDomain.from_file(conf.filename_domain)

        else:
            raise ValueError('missing filename_mesh or filename_domain!')

        obj = Problem('problem_from_conf', conf=conf, functions=functions,
                      domain=domain, auto_conf=False, auto_solvers=False)

        obj.set_regions(conf.regions, obj.functions)

        obj.clear_equations()

        if init_fields:
            obj.set_fields(conf.fields)

            if init_equations:
                obj.set_equations(conf.equations, user={'ts' : obj.ts})

        if init_solvers:
            obj.set_solvers(conf.solvers, conf.options)

        return obj

    def __init__(self, name, conf=None, functions=None,
                 domain=None, fields=None, equations=None, auto_conf=True,
                 nls=None, ls=None, ts=None, auto_solvers=True):
        self.name = name
        self.conf = conf
        self.functions = functions

        self.reset()

        self.ts = get_default(ts, self.get_default_ts())

        if auto_conf:
            if equations is None:
                raise ValueError('missing equations in auto_conf mode!')

            if fields is None:
                variables = equations.variables
                fields = {}
                for field in [var.get_field() for var in variables]:
                    fields[field.name] = field

            if domain is None:
                domain = fields.values()[0].domain

            if conf is None:
                self.conf = Struct(ebcs={}, epbcs={}, lcbcs={})

        self.equations = equations
        self.fields = fields
        self.domain = domain

        if auto_solvers:
            if ls is None:
                ls = ScipyDirect({})

            if nls is None:
                nls = Newton({}, lin_solver=ls)

            ev = self.get_evaluator()
            nls.fun = ev.eval_residual
            nls.fun_grad = ev.eval_tangent_matrix

            self.set_solvers_instances(ls=ls, nls=nls)

        self.setup_output()

    def reset(self):
        if hasattr(self.conf, 'options'):
            self.setup_hooks(self.conf.options)

        else:
            self.setup_hooks()

        self.mtx_a = None
        self.solvers = None
        self.clear_equations()

    def setup_hooks(self, options=None):
        """
        Setup various hooks (user-defined functions), as given in `options`.

        Supported hooks:

          - `matrix_hook`

            - check/modify tangent matrix in each nonlinear solver
              iteration

          - `nls_iter_hook`

            - called prior to every iteration of nonlinear solver, if the
              solver supports that
            - takes the Problem instance (`self`) as the first
              argument
        """
        hook_names = ['nls_iter_hook', 'matrix_hook']
        for hook_name in hook_names:
            setattr(self, hook_name, None)
            if options is not None:
                hook = options.get(hook_name, None)
                if hook is not None:
                    hook = self.conf.get_function(hook)
                    setattr(self, hook_name, hook)

        iter_hook = self.nls_iter_hook
        if iter_hook is not None:
            self.nls_iter_hook = lambda *args, **kwargs: \
                                 iter_hook(self, *args, **kwargs)


    def copy(self, name=None):
        """
        Make a copy of Problem.
        """
        if name is None:
            name = self.name + '_copy'
        obj = Problem(name, conf=self.conf, functions=self.functions,
                      domain=self.domain, fields=self.fields,
                      equations=self.equations, auto_conf=False,
                      auto_solvers=False)

        obj.ebcs = self.ebcs
        obj.epbcs = self.epbcs
        obj.lcbcs = self.lcbcs

        obj.set_solvers(self.conf.solvers, self.conf.options)

        obj.setup_output(output_filename_trunk=self.ofn_trunk,
                         output_dir=self.output_dir,
                         output_format=self.output_format,
                         file_per_var=self.file_per_var,
                         linearization=self.linearization)

        return obj

    def create_subproblem(self, var_names, known_var_names):
        """
        Create a sub-problem with equations containing only terms with the
        given virtual variables.

        Parameters
        ----------
        var_names : list
            The list of names of virtual variables.
        known_var_names : list
            The list of  names of (already) known state variables.

        Returns
        -------
        subpb : Problem instance
            The sub-problem.
        """
        subpb = Problem(self.name + '_' + '_'.join(var_names), conf=self.conf,
                        functions=self.functions, domain=self.domain,
                        fields=self.fields, auto_conf=False,
                        auto_solvers=False)
        subpb.set_solvers(self.conf.solvers, self.conf.options)

        subeqs = self.equations.create_subequations(var_names,
                                                    known_var_names)
        subpb.set_equations_instance(subeqs, keep_solvers=True)

        return subpb

    def setup_default_output(self, conf=None, options=None):
        """
        Provide default values to `Problem.setup_output()`
        from `conf.options` and `options`.
        """
        conf = get_default(conf, self.conf)

        if options and getattr(options, 'output_filename_trunk', None):
            default_output_dir, of = op.split(options.output_filename_trunk)
            default_trunk = io.get_trunk(of)

        else:
            default_trunk = None
            default_output_dir = conf.options.get('output_dir', None)

        if options and getattr(options, 'output_format', None):
            default_output_format = options.output_format

        else:
            default_output_format = conf.options.get('output_format', None)

        default_file_per_var = conf.options.get('file_per_var', None)
        default_float_format = conf.options.get('float_format', None)
        default_linearization = Struct(kind='strip')

        self.setup_output(output_filename_trunk=default_trunk,
                          output_dir=default_output_dir,
                          file_per_var=default_file_per_var,
                          output_format=default_output_format,
                          float_format=default_float_format,
                          linearization=default_linearization)

    def setup_output(self, output_filename_trunk=None, output_dir=None,
                     output_format=None, float_format=None,
                     file_per_var=None, linearization=None):
        """
        Sets output options to given values, or uses the defaults for
        each argument that is None.
        """
        self.output_modes = {'vtk' : 'sequence', 'h5' : 'single'}

        self.ofn_trunk = get_default(output_filename_trunk,
                                     io.get_trunk(self.domain.name))

        self.set_output_dir(output_dir)

        self.output_format = get_default(output_format, 'vtk')
        self.float_format = get_default(float_format, None)
        self.file_per_var = get_default(file_per_var, False)
        self.linearization = get_default(linearization, Struct(kind='strip'))

        if ((self.output_format == 'h5') and
            (self.linearization.kind == 'adaptive')):
            self.linearization.kind = None

    def set_output_dir(self, output_dir=None):
        """
        Set the directory for output files.

        The directory is created if it does not exist.
        """
        self.output_dir = get_default(output_dir, os.curdir)

        if self.output_dir and not op.exists(self.output_dir):
            os.makedirs(self.output_dir)

    def set_regions(self, conf_regions=None,
                     conf_materials=None, functions=None):
        conf_regions = get_default(conf_regions, self.conf.regions)
        functions = get_default(functions, self.functions)

        self.domain.create_regions(conf_regions, functions)

    def set_materials(self, conf_materials=None):
        """
        Set definition of materials.
        """
        self.conf_materials = get_default(conf_materials, self.conf.materials)

    def select_materials(self, material_names, only_conf=False):
        if type(material_names) == dict:
            conf_materials = transform_materials(material_names)

        else:
            conf_materials = select_by_names(self.conf.materials, material_names)

        if not only_conf:
            self.set_materials(conf_materials)

        return conf_materials

    def set_fields(self, conf_fields=None):
        conf_fields = get_default(conf_fields, self.conf.fields)
        self.fields = fields_from_conf(conf_fields, self.domain.regions)

    def set_variables(self, conf_variables=None):
        """
        Set definition of variables.
        """
        self.conf_variables = get_default(conf_variables, self.conf.variables)
        self.reset()

    def select_variables(self, variable_names, only_conf=False):
        if type(variable_names) == dict:
            conf_variables = transform_variables(variable_names)

        else:
            conf_variables = select_by_names(self.conf.variables, variable_names)

        if not only_conf:
            self.set_variables(conf_variables)

        return conf_variables

    def clear_equations(self):
        self.integrals = None
        self.equations = None
        self.ebcs = None
        self.epbcs = None
        self.lcbcs = None

    def set_equations(self, conf_equations=None, user=None,
                      keep_solvers=False, make_virtual=False):
        """
        Set equations of the problem using the `equations` problem
        description entry.

        Fields and Regions have to be already set.
        """
        conf_equations = get_default(conf_equations,
                                     self.conf.get('equations', None))

        self.set_variables()
        variables = Variables.from_conf(self.conf_variables, self.fields)

        self.set_materials()
        materials = Materials.from_conf(self.conf_materials, self.functions)

        self.integrals = self.get_integrals()
        equations = Equations.from_conf(conf_equations, variables,
                                        self.domain.regions,
                                        materials, self.integrals,
                                        user=user)

        self.equations = equations

        if not keep_solvers:
            self.solvers = None

    def set_equations_instance(self, equations, keep_solvers=False):
        """
        Set equations of the problem to `equations`.
        """
        self.mtx_a = None
        self.clear_equations()
        self.equations = equations

        if not keep_solvers:
            self.solvers = None

    def set_solvers(self, conf_solvers=None, options=None):
        """
        Choose which solvers should be used. If solvers are not set in
        `options`, use first suitable in `conf_solvers`.
        """
        conf_solvers = get_default(conf_solvers, self.conf.solvers)
        self.solver_confs = {}
        for key, val in conf_solvers.iteritems():
            self.solver_confs[val.name] = val

        def _find_suitable(prefix):
            for key, val in self.solver_confs.iteritems():
                if val.kind.find(prefix) == 0:
                    return val
            return None

        def _get_solver_conf(kind):
            try:
                key = options[kind]
                conf = self.solver_confs[key]
            except:
                conf = _find_suitable(kind + '.')
            return conf

        self.ts_conf = _get_solver_conf('ts')
        if self.ts_conf is None:
            self.ts_conf = Struct(name='no ts', kind='ts.stationary')

        self.nls_conf = _get_solver_conf('nls')
        self.ls_conf = _get_solver_conf('ls')

        info = 'using solvers:'
        if self.ts_conf:
            info += '\n                ts: %s' % self.ts_conf.name
        if self.nls_conf:
            info += '\n               nls: %s' % self.nls_conf.name
        if self.ls_conf:
            info += '\n                ls: %s' % self.ls_conf.name
        if info != 'using solvers:':
            output(info)

    def set_solvers_instances(self, ls=None, nls=None):
        """
        Set the instances of linear and nonlinear solvers that will be
        used in `Problem.solve()` call.
        """
        if (ls is not None) and (nls is not None):
            if not (nls.lin_solver is ls):
                raise ValueError('linear solver not used in nonlinear!')

        self.solvers = Struct(name='solvers', ls=ls, nls=nls)
        if nls is not None:
            self.nls_status = get_default(nls.status, IndexedStruct())

    def get_solver_conf(self, name):
        return self.solver_confs[name]

    def get_default_ts(self, t0=None, t1=None, dt=None, n_step=None,
                       step=None):
        t0 = get_default(t0, 0.0)
        t1 = get_default(t1, 1.0)
        dt = get_default(dt, 1.0)
        n_step = get_default(n_step, 1)

        ts = TimeStepper(t0, t1, dt, n_step, step=step)

        return ts

    def get_integrals(self, names=None):
        """
        Get integrals, initialized from problem configuration if available.

        Parameters
        ----------
        names : list, optional
            If given, only the named integrals are returned.

        Returns
        -------
        integrals : Integrals instance
            The requested integrals.
        """
        conf_integrals = self.conf.get('integrals', {})
        integrals = Integrals.from_conf(conf_integrals)

        if names is not None:
            integrals.update([integrals[ii] for ii in names
                              if ii in integrals.names])

        return integrals

    def update_time_stepper(self, ts):
        if ts is not None:
            self.ts = ts

    def update_materials(self, ts=None, mode='normal', verbose=True):
        """
        Update materials used in equations.

        Parameters
        ----------
        ts : TimeStepper instance
            The time stepper.
        mode : 'normal', 'update' or 'force'
            The update mode, see :func:`Material.time_update()
            <sfepy.discrete.materials.Material.time_update()>`.
        verbose : bool
            If False, reduce verbosity.
        """
        if self.equations is not None:
            self.update_time_stepper(ts)
            self.equations.time_update_materials(self.ts, mode=mode,
                                                 problem=self, verbose=verbose)


    def update_equations(self, ts=None, ebcs=None, epbcs=None,
                         lcbcs=None, functions=None, create_matrix=False):
        """
        Update equations for current time step.

        The tangent matrix graph is automatically recomputed if the set
        of active essential or periodic boundary conditions changed
        w.r.t. the previous time step.

        Parameters
        ----------
        ts : TimeStepper instance, optional
            The time stepper. If not given, `self.ts` is used.
        ebcs : Conditions instance, optional
            The essential (Dirichlet) boundary conditions. If not given,
            `self.ebcs` are used.
        epbcs : Conditions instance, optional
            The periodic boundary conditions. If not given, `self.epbcs`
            are used.
        lcbcs : Conditions instance, optional
            The linear combination boundary conditions. If not given,
            `self.lcbcs` are used.
        functions : Functions instance, optional
            The user functions for boundary conditions, materials,
            etc. If not given, `self.functions` are used.
        """
        self.update_time_stepper(ts)
        functions = get_default(functions, self.functions)

        graph_changed = self.equations.time_update(self.ts,
                                                   ebcs, epbcs, lcbcs,
                                                   functions, self)
        self.graph_changed = graph_changed

        if graph_changed or (self.mtx_a is None) or create_matrix:
            self.mtx_a = self.equations.create_matrix_graph()
            ## import sfepy.base.plotutils as plu
            ## plu.spy(self.mtx_a)
            ## plu.plt.show()

    def set_bcs(self, ebcs=None, epbcs=None, lcbcs=None):
        """
        Update boundary conditions.
        """
        if isinstance(ebcs, Conditions):
            self.ebcs = ebcs

        else:
            conf_ebc = get_default(ebcs, self.conf.ebcs)
            self.ebcs = Conditions.from_conf(conf_ebc, self.domain.regions)

        if isinstance(epbcs, Conditions):
            self.epbcs = epbcs

        else:
            conf_epbc = get_default(epbcs, self.conf.epbcs)
            self.epbcs = Conditions.from_conf(conf_epbc, self.domain.regions)

        if isinstance(lcbcs, Conditions):
            self.lcbcs = lcbcs

        else:
            conf_lcbc = get_default(lcbcs, self.conf.lcbcs)
            self.lcbcs = Conditions.from_conf(conf_lcbc, self.domain.regions)

    def time_update(self, ts=None,
                    ebcs=None, epbcs=None, lcbcs=None,
                    functions=None, create_matrix=False):
        self.set_bcs(ebcs, epbcs, lcbcs)
        self.update_equations(ts, self.ebcs, self.epbcs, self.lcbcs,
                              functions, create_matrix)

    def setup_ic(self, conf_ics=None, functions=None):
        conf_ics = get_default(conf_ics, self.conf.ics)
        ics = Conditions.from_conf(conf_ics, self.domain.regions)

        functions = get_default(functions, self.functions)

        self.equations.setup_initial_conditions(ics, functions)

    def select_bcs(self, ebc_names=None, epbc_names=None,
                   lcbc_names=None, create_matrix=False):

        if ebc_names is not None:
            conf_ebc = select_by_names(self.conf.ebcs, ebc_names)
        else:
            conf_ebc = None

        if epbc_names is not None:
            conf_epbc = select_by_names(self.conf.epbcs, epbc_names)
        else:
            conf_epbc = None

        if lcbc_names is not None:
            conf_lcbc = select_by_names(self.conf.lcbcs, lcbc_names)
        else:
            conf_lcbc = None

        self.set_bcs(conf_ebc, conf_epbc, conf_lcbc)
        self.update_equations(self.ts, self.ebcs, self.epbcs, self.lcbcs,
                              self.functions, create_matrix)

    def get_timestepper(self):
        return self.ts

    def create_state(self):
        return State(self.equations.variables)

    def get_mesh_coors(self):
        return self.domain.get_mesh_coors()

    def set_mesh_coors(self, coors, update_fields=False, actual=False,
                       clear_all=True):
        """
        Set mesh coordinates.

        Parameters
        ----------
        coors : array
            The new coordinates.
        update_fields : bool
            If True, update also coordinates of fields.
        actual : bool
            If True, update the actual configuration coordinates,
            otherwise the undeformed configuration ones.
        """
        fea.set_mesh_coors(self.domain, self.fields, coors,
                           update_fields=update_fields, actual=actual,
                           clear_all=clear_all)

    def refine_uniformly(self, level):
        """
        Refine the mesh uniformly `level`-times.

        Notes
        -----
        This operation resets almost everything (fields, equations, ...)
        - it is roughly equivalent to creating a new Problem
        instance with the refined mesh.
        """
        if level == 0: return

        domain = self.domain
        for ii in range(level):
            domain = domain.refine()

        self.domain = domain
        self.set_regions(self.conf.regions, self.functions)
        self.clear_equations()

        self.set_fields(self.conf.fields)
        self.set_equations(self.conf.equations, user={'ts' : self.ts})

    def get_dim(self, get_sym=False):
        """Returns mesh dimension, symmetric tensor dimension (if `get_sym` is
        True).
        """
        dim = self.domain.mesh.dim
        if get_sym:
            return dim, (dim + 1) * dim / 2
        else:
            return dim

    def init_time(self, ts):
        self.update_time_stepper(ts)
        self.equations.init_time(ts)
        self.update_materials(mode='force')

    def advance(self, ts=None):
        self.update_time_stepper(ts)
        self.equations.advance(self.ts)

    def save_state(self, filename, state=None, out=None,
                   fill_value=None, post_process_hook=None,
                   linearization=None, file_per_var=False, **kwargs):
        """
        Parameters
        ----------
        file_per_var : bool or None
            If True, data of each variable are stored in a separate
            file. If None, it is set to the application option value.
        linearization : Struct or None
            The linearization configuration for higher order
            approximations. If its kind is 'adaptive', `file_per_var` is
            assumed True.
        """
        linearization = get_default(linearization, self.linearization)
        if linearization.kind != 'adaptive':
            file_per_var = get_default(file_per_var, self.file_per_var)

        else:
            file_per_var = True

        extend = not file_per_var
        if (out is None) and (state is not None):
            out = state.create_output_dict(fill_value=fill_value,
                                           extend=extend,
                                           linearization=linearization)

            if post_process_hook is not None:
                out = post_process_hook(out, self, state, extend=extend)

        if linearization.kind == 'adaptive':
            for key, val in out.iteritems():
                mesh = val.get('mesh', self.domain.mesh)
                aux = io.edit_filename(filename, suffix='_' + val.var_name)
                mesh.write(aux, io='auto', out={key : val},
                           float_format=self.float_format, **kwargs)
                if hasattr(val, 'levels'):
                    output('max. refinement per group:', val.levels)

        elif file_per_var:
            meshes = {}

            if self.equations is None:
                varnames = {}
                for key, val in out.iteritems():
                    varnames[val.var_name] = 1
                varnames = varnames.keys()
                outvars = self.create_variables(varnames)
                itervars = outvars.__iter__
            else:
                itervars = self.equations.variables.iter_state

            for var in itervars():
                rname = var.field.region.name
                if meshes.has_key(rname):
                    mesh = meshes[rname]
                else:
                    mesh = Mesh.from_region(var.field.region, self.domain.mesh,
                                            localize=True,
                                            is_surface=var.is_surface)
                    meshes[rname] = mesh

                vout = {}
                for key, val in out.iteritems():
                    try:
                        if val.var_name == var.name:
                            vout[key] = val

                    except AttributeError:
                        msg = 'missing var_name attribute in output!'
                        raise ValueError(msg)

                aux = io.edit_filename(filename, suffix='_' + var.name)
                mesh.write(aux, io='auto', out=vout,
                           float_format=self.float_format, **kwargs)
        else:
            mesh = out.pop('__mesh__', self.domain.mesh)
            mesh.write(filename, io='auto', out=out,
                       float_format=self.float_format, **kwargs)

    def save_ebc(self, filename, force=True, default=0.0):
        """
        Save essential boundary conditions as state variables.
        """
        output('saving ebc...')
        variables = self.get_variables(auto_create=True)

        ebcs = Conditions.from_conf(self.conf.ebcs, self.domain.regions)
        epbcs = Conditions.from_conf(self.conf.epbcs, self.domain.regions)

        try:
            variables.equation_mapping(ebcs, epbcs, self.ts, self.functions,
                                       problem=self)
        except:
            output('cannot make equation mapping!')
            raise

        state = State(variables)
        state.fill(default)

        if force:
            vals = dict_from_keys_init(variables.state)
            for ii, key in enumerate(vals.iterkeys()):
                vals[key] = ii + 1

            state.apply_ebc(force_values=vals)

        else:
            state.apply_ebc()

        out = state.create_output_dict(extend=True)
        self.save_state(filename, out=out, fill_value=default)
        output('...done')

    def save_regions(self, filename_trunk, region_names=None):
        """
        Save regions as meshes.

        Parameters
        ----------
        filename_trunk : str
            The output filename without suffix.
        region_names : list, optional
            If given, only the listed regions are saved.
        """
        filename = '%s.mesh' % filename_trunk
        self.domain.save_regions(filename, region_names=region_names)

    def save_regions_as_groups(self, filename_trunk, region_names=None):
        """
        Save regions in a single mesh but mark them by using different
        element/node group numbers.

        See :func:`Domain.save_regions_as_groups()
        <sfepy.discrete.fem.domain.Domain.save_regions_as_groups()>` for more
        details.

        Parameters
        ----------
        filename_trunk : str
            The output filename without suffix.
        region_names : list, optional
            If given, only the listed regions are saved.
        """
        filename = '%s.%s' % (filename_trunk, self.output_format)
        self.domain.save_regions_as_groups(filename,
                                           region_names=region_names)

    def save_field_meshes(self, filename_trunk):

        output('saving field meshes...')
        for field in self.fields:
            output(field.name)
            field.write_mesh(filename_trunk + '_%s')
        output('...done')

    def get_evaluator(self, reuse=False):
        """
        Either create a new Evaluator instance (reuse == False),
        or return an existing instance, created in a preceding call to
        Problem.init_solvers().
        """
        if reuse:
            try:
                ev = self.evaluator
            except AttributeError:
                raise AttributeError('call Problem.init_solvers() or'\
                                     ' set reuse to False!')
        else:
            if self.equations.variables.has_lcbc:
                ev = LCBCEvaluator(self, matrix_hook=self.matrix_hook)
            else:
                ev = BasicEvaluator(self, matrix_hook=self.matrix_hook)

        self.evaluator = ev

        return ev

    def init_solvers(self, nls_status=None, ls_conf=None, nls_conf=None,
                     mtx=None, presolve=False):
        """Create and initialize solvers."""
        ls_conf = get_default(ls_conf, self.ls_conf,
                              'you must set linear solver!')

        nls_conf = get_default(nls_conf, self.nls_conf,
                               'you must set nonlinear solver!')

        if presolve:
            tt = time.clock()
        if ls_conf.get('needs_problem_instance', False):
            extra_args = {'problem' : self}
        else:
            extra_args = {}

        ls = Solver.any_from_conf(ls_conf, mtx=mtx, presolve=presolve,
                                  **extra_args)
        if presolve:
            tt = time.clock() - tt
            output('presolve: %.2f [s]' % tt)

        if nls_conf.get('needs_problem_instance', False):
            extra_args = {'problem' : self}
        else:
            extra_args = {}
        ev = self.get_evaluator()

        if self.conf.options.get('ulf', False):
            self.nls_iter_hook = ev.new_ulf_iteration

        nls = Solver.any_from_conf(nls_conf, fun=ev.eval_residual,
                                   fun_grad=ev.eval_tangent_matrix,
                                   lin_solver=ls, iter_hook=self.nls_iter_hook,
                                   status=nls_status, **extra_args)

        self.set_solvers_instances(ls=ls, nls=nls)

    def get_solvers(self):
        return getattr(self, 'solvers', None)

    def is_linear(self):
        nls_conf = get_default(None, self.nls_conf,
                               'you must set nonlinear solver!')
        aux = Solver.any_from_conf(nls_conf)
        if aux.conf.problem == 'linear':
            return True
        else:
            return False

    def set_linear(self, is_linear):
        nls_conf = get_default(None, self.nls_conf,
                               'you must set nonlinear solver!')
        if is_linear:
            nls_conf.problem = 'linear'
        else:
            nls_conf.problem = 'nonlinear'

    def solve(self, state0=None, nls_status=None,
              ls_conf=None, nls_conf=None, force_values=None,
              var_data=None):
        """Solve self.equations in current time step.

        Parameters
        ----------
        var_data : dict
            A dictionary of {variable_name : data vector} used to initialize
            parameter variables.
        """
        solvers = self.get_solvers()
        if solvers is None:
            self.init_solvers(nls_status, ls_conf, nls_conf)
            solvers = self.get_solvers()

        if state0 is None:
            state0 = State(self.equations.variables)

        else:
            if isinstance(state0, nm.ndarray):
                state0 = State(self.equations.variables, vec=state0)

        self.equations.set_data(var_data, ignore_unknown=True)

        self.update_materials()
        state0.apply_ebc(force_values=force_values)

        vec0 = state0.get_reduced()

        self.nls_status = get_default(nls_status, self.nls_status)
        vec = solvers.nls(vec0, status=self.nls_status)

        state = state0.copy(preserve_caches=True)
        state.set_reduced(vec, preserve_caches=True)

        return state

    def create_evaluable(self, expression, try_equations=True, auto_init=False,
                         preserve_caches=False, copy_materials=True,
                         integrals=None,
                         ebcs=None, epbcs=None, lcbcs=None,
                         ts=None, functions=None,
                         mode='eval', var_dict=None, strip_variables=True,
                         extra_args=None, verbose=True, **kwargs):
        """
        Create evaluable object (equations and corresponding variables)
        from the `expression` string. Convenience function calling
        :func:`create_evaluable()
        <sfepy.discrete.evaluate.create_evaluable()>` with defaults provided
        by the Problem instance `self`.

        The evaluable can be repeatedly evaluated by calling
        :func:`eval_equations() <sfepy.discrete.evaluate.eval_equations()>`,
        e.g. for different values of variables.

        Parameters
        ----------
        expression : str
            The expression to evaluate.
        try_equations : bool
            Try to get variables from `self.equations`. If this fails,
            variables can either be provided in `var_dict`, as keyword
            arguments, or are created automatically according to the
            expression.
        auto_init : bool
            Set values of all variables to all zeros.
        preserve_caches : bool
            If True, do not invalidate evaluate caches of variables.
        copy_materials : bool
            Work with a copy of `self.equations.materials` instead of
            reusing them. Safe but can be slow.
        integrals : Integrals instance, optional
            The integrals to be used. Automatically created as needed if
            not given.
        ebcs : Conditions instance, optional
            The essential (Dirichlet) boundary conditions for 'weak'
            mode. If not given, `self.ebcs` are used.
        epbcs : Conditions instance, optional
            The periodic boundary conditions for 'weak'
            mode. If not given, `self.epbcs` are used.
        lcbcs : Conditions instance, optional
            The linear combination boundary conditions for 'weak'
            mode. If not given, `self.lcbcs` are used.
        ts : TimeStepper instance, optional
            The time stepper. If not given, `self.ts` is used.
        functions : Functions instance, optional
            The user functions for boundary conditions, materials
            etc. If not given, `self.functions` are used.
        mode : one of 'eval', 'el_avg', 'qp', 'weak'
            The evaluation mode - 'weak' means the finite element
            assembling, 'qp' requests the values in quadrature points,
            'el_avg' element averages and 'eval' means integration over
            each term region.
        var_dict : dict, optional
            The variables (dictionary of (variable name) : (Variable instance))
            to be used in the expression. Use this if the name of a variable
            conflicts with one of the parameters of this method.
        strip_variables : bool
            If False, the variables in `var_dict` or `kwargs` not present in
            the expression are added to the actual variables as a context.
        extra_args : dict, optional
            Extra arguments to be passed to terms in the expression.
        verbose : bool
            If False, reduce verbosity.
        **kwargs : keyword arguments
            Additional variables can be passed as keyword arguments, see
            `var_dict`.

        Returns
        -------
        equations : Equations instance
            The equations that can be evaluated.
        variables : Variables instance
            The corresponding variables. Set their values and use
            :func:`eval_equations() <sfepy.discrete.evaluate.eval_equations()>`.

        Examples
        --------
        `problem` is Problem instance.

        >>> out = problem.create_evaluable('dq_state_in_volume_qp.i1.Omega(u)')
        >>> equations, variables = out

        `vec` is a vector of coefficients compatible with the field
        of 'u' - let's use all ones.

        >>> vec = nm.ones((variables['u'].n_dof,), dtype=nm.float64)
        >>> variables['u'].set_data(vec)
        >>> vec_qp = eval_equations(equations, variables, mode='qp')

        Try another vector:

        >>> vec = 3 * nm.ones((variables['u'].n_dof,), dtype=nm.float64)
        >>> variables['u'].set_data(vec)
        >>> vec_qp = eval_equations(equations, variables, mode='qp')
        """
        from sfepy.discrete.equations import get_expression_arg_names

        variables = get_default(var_dict, {})
        var_context = get_default(var_dict, {})

        if try_equations and self.equations is not None:
            # Make a copy, so that possible variable caches are preserved.
            for key, var in self.equations.variables.as_dict().iteritems():
                if key in variables:
                    continue
                var = var.copy(name=key)
                if not preserve_caches:
                    var.clear_evaluate_cache()
                variables[key] = var

        elif var_dict is None:
            possible_var_names = get_expression_arg_names(expression)
            variables = self.create_variables(possible_var_names)

        materials = self.get_materials()
        if materials is not None:
            if copy_materials:
                materials = materials.semideep_copy()

            else:
                materials = Materials(objs=materials._objs)

        else:
            possible_mat_names = get_expression_arg_names(expression)
            materials = self.create_materials(possible_mat_names)

        _kwargs = copy(kwargs)
        for key, val in kwargs.iteritems():
            if isinstance(val, Variable):
                if val.name != key:
                    msg = 'inconsistent variable name! (%s == %s)' \
                          % (val.name, key)
                    raise ValueError(msg)
                var_context[key] = variables[key] = val.copy(name=key)
                _kwargs.pop(key)

            elif isinstance(val, Material):
                if val.name != key:
                    msg = 'inconsistent material name! (%s == %s)' \
                          % (val.name, key)
                    raise ValueError(msg)
                materials[val.name] = val
                _kwargs.pop(key)

        kwargs = _kwargs

        ebcs = get_default(ebcs, self.ebcs)
        epbcs = get_default(epbcs, self.epbcs)
        lcbcs = get_default(lcbcs, self.lcbcs)
        ts = get_default(ts, self.get_timestepper())
        functions = get_default(functions, self.functions)
        integrals = get_default(integrals, self.get_integrals())

        out = create_evaluable(expression, self.fields, materials,
                               variables.itervalues(), integrals,
                               ebcs=ebcs, epbcs=epbcs, lcbcs=lcbcs,
                               ts=ts, functions=functions,
                               auto_init=auto_init,
                               mode=mode, extra_args=extra_args, verbose=verbose,
                               kwargs=kwargs)

        if not strip_variables:
            variables = out[1]
            variables.extend([var for var in var_context.itervalues()
                              if var not in variables])

        equations = out[0]
        mode = 'update' if not copy_materials else 'normal'
        equations.time_update_materials(self.ts, mode=mode, problem=self,
                                        verbose=verbose)

        return out

    def evaluate(self, expression, try_equations=True, auto_init=False,
                 preserve_caches=False, copy_materials=True, integrals=None,
                 ebcs=None, epbcs=None, lcbcs=None,
                 ts=None, functions=None,
                 mode='eval', dw_mode='vector', term_mode=None,
                 var_dict=None, strip_variables=True, ret_variables=False,
                 verbose=True, extra_args=None, **kwargs):
        """
        Evaluate an expression, convenience wrapper of
        :func:`Problem.create_evaluable` and
        :func:`eval_equations() <sfepy.discrete.evaluate.eval_equations>`.

        Parameters
        ----------
        dw_mode : 'vector' or 'matrix'
            The assembling mode for 'weak' evaluation mode.
        term_mode : str
            The term call mode - some terms support different call modes
            and depending on the call mode different values are
            returned.
        ret_variables : bool
            If True, return the variables that were created to evaluate
            the expression.
        other : arguments
            See docstrings of :func:`Problem.create_evaluable()`.

        Returns
        -------
        out : array
            The result of the evaluation.
        variables : Variables instance
            The variables that were created to evaluate
            the expression. Only provided if `ret_variables` is True.
        """
        aux = self.create_evaluable(expression,
                                    try_equations=try_equations,
                                    auto_init=auto_init,
                                    preserve_caches=preserve_caches,
                                    copy_materials=copy_materials,
                                    integrals=integrals,
                                    ebcs=ebcs, epbcs=epbcs, lcbcs=lcbcs,
                                    ts=ts, functions=functions,
                                    mode=mode, var_dict=var_dict,
                                    strip_variables=strip_variables,
                                    extra_args=extra_args,
                                    verbose=verbose, **kwargs)
        equations, variables = aux

        out = eval_equations(equations, variables,
                             preserve_caches=preserve_caches,
                             mode=mode, dw_mode=dw_mode, term_mode=term_mode)

        if ret_variables:
            out = (out, variables)

        return out

    def eval_equations(self, names=None, preserve_caches=False,
                   mode='eval', dw_mode='vector', term_mode=None,
                   verbose=True):
        """
        Evaluate (some of) the problem's equations, convenience wrapper of
        :func:`eval_equations() <sfepy.discrete.evaluate.eval_equations>`.

        Parameters
        ----------
        names : str or sequence of str, optional
            Evaluate only equations of the given name(s).
        preserve_caches : bool
            If True, do not invalidate evaluate caches of variables.
        mode : one of 'eval', 'el_avg', 'qp', 'weak'
            The evaluation mode - 'weak' means the finite element
            assembling, 'qp' requests the values in quadrature points,
            'el_avg' element averages and 'eval' means integration over
            each term region.
        dw_mode : 'vector' or 'matrix'
            The assembling mode for 'weak' evaluation mode.
        term_mode : str
            The term call mode - some terms support different call modes
            and depending on the call mode different values are
            returned.
        verbose : bool
            If False, reduce verbosity.

        Returns
        -------
        out : dict or result
            The evaluation result. In 'weak' mode it is the vector or sparse
            matrix, depending on `dw_mode`. Otherwise, it is a dict of results
            with equation names as keys or a single result for a single
            equation.
        """
        return eval_equations(self.equations, self.equations.variables,
                              names=names, preserve_caches=preserve_caches,
                              mode=mode, dw_mode=dw_mode, term_mode=term_mode,
                              verbose=verbose)

    def get_time_solver(self, ts_conf=None, **kwargs):
        """
        Create and return a TimeSteppingSolver instance.

        Notes
        -----
        Also sets `self.ts` attribute.
        """
        ts_conf = get_default(ts_conf, self.ts_conf,
                              'you must set time-stepping solver!')
        ts_solver = Solver.any_from_conf(ts_conf, problem=self, **kwargs)
        self.ts = ts_solver.ts

        return ts_solver

    def get_materials(self):
        if self.equations is not None:
            materials = self.equations.materials

        else:
            materials = None

        return materials

    def create_materials(self, mat_names=None):
        """
        Create materials with names in `mat_names`. Their definitions
        have to be present in `self.conf.materials`.

        Notes
        -----
        This method does not change `self.equations`, so it should not
        have any side effects.
        """
        if mat_names is not None:
            conf_materials = self.select_materials(mat_names, only_conf=True)

        else:
            conf_materials = self.conf.materials

        materials = Materials.from_conf(conf_materials, self.functions)

        return materials

    def init_variables(self, state):
        """Initialize variables with history."""
        self.equations.variables.init_state(state)

    def get_variables(self, auto_create=False):
        if self.equations is not None:
            variables = self.equations.variables

        elif auto_create:
            variables = self.create_variables()

        else:
            variables = None

        return variables

    def create_variables(self, var_names=None):
        """
        Create variables with names in `var_names`. Their definitions
        have to be present in `self.conf.variables`.

        Notes
        -----
        This method does not change `self.equations`, so it should not
        have any side effects.
        """
        if var_names is not None:
            conf_variables = self.select_variables(var_names, only_conf=True)

        else:
            conf_variables = self.conf.variables

        variables = Variables.from_conf(conf_variables, self.fields)

        return variables

    def get_output_name(self, suffix=None, extra=None, mode=None):
        """
        Return default output file name, based on the output directory,
        output format, step suffix and mode. If present, the extra
        string is put just before the output format suffix.
        """
        out = op.join(self.output_dir, self.ofn_trunk)

        if suffix is not None:
            if mode is None:
                mode = self.output_modes[self.output_format]

            if mode == 'sequence':
                out = '.'.join((out, suffix))

        if extra is not None:
            out = '.'.join((out, extra, self.output_format))
        else:
            out = '.'.join((out, self.output_format))

        return out

    def remove_bcs(self):
        """
        Convenience function to remove boundary conditions.
        """
        self.time_update(ebcs={}, epbcs={}, lcbcs={})

########NEW FILE########
__FILENAME__ = projections
"""
Construct projections between FE spaces.
"""
from sfepy.base.base import output, IndexedStruct
from sfepy.discrete import FieldVariable, Integral, Equation, Equations, Material
from sfepy.discrete import Problem
from sfepy.terms import Term
from sfepy.solvers.ls import ScipyDirect
from sfepy.solvers.nls import Newton

def create_mass_matrix(field):
    """
    Create scalar mass matrix corresponding to the given field.

    Returns
    -------
    mtx : csr_matrix
        The mass matrix in CSR format.
    """
    u = FieldVariable('u', 'unknown', field)
    v = FieldVariable('v', 'test', field, primary_var_name='u')

    integral = Integral('i', order=field.approx_order * 2)
    term = Term.new('dw_volume_dot(v, u)', integral, field.region, v=v, u=u)
    eq = Equation('aux', term)
    eqs = Equations([eq])
    eqs.time_update(None)

    dummy = eqs.create_state_vector()

    mtx = eqs.create_matrix_graph()
    mtx = eqs.eval_tangent_matrices(dummy, mtx)

    return mtx

def make_l2_projection(target, source):
    """
    Project a scalar `source` field variable to a scalar `target` field
    variable using the :math:`L^2` dot product.
    """
    def eval_variable(ts, coors, mode, **kwargs):
        val = source.evaluate_at(coors)
        val.shape = val.shape + (1,)
        return val

    make_l2_projection_data(target, eval_variable)

def make_l2_projection_data(target, eval_data, order=None):
    """
    Project scalar data to a scalar `target` field variable using the
    :math:`L^2` dot product.

    Parameters
    ----------
    target : FieldVariable instance
        The target variable.
    eval_data : callable or array
        Either a material-like function `eval_data()`, or an array of values in
        quadrature points that has to be reshapable to the shape required by
        `order`.
    order : int, optional
        The quadrature order. If not given, it is set to
        `2 * target.field.approx_order`.
    """
    if order is None:
       order = 2 * target.field.approx_order
    integral = Integral('i', order=order)

    # Use a copy of target for the projection to avoid overwriting
    # target._variables.
    un = target.copy(name=target.name)

    v = FieldVariable('v', 'test', un.field, primary_var_name=un.name)
    lhs = Term.new('dw_volume_dot(v, %s)' % un.name, integral,
                   un.field.region, v=v, **{un.name : un})

    def _eval_data(ts, coors, mode, **kwargs):
        if mode == 'qp':
            if callable(eval_data):
                val = eval_data(ts, coors, mode, **kwargs)

            else:
                val = eval_data.reshape((coors.shape[0], 1, 1))

            return {'val' : val}

    m = Material('m', function=_eval_data)
    rhs = Term.new('dw_volume_lvf(m.val, v)', integral, un.field.region,
                   m=m, v=v)

    eq = Equation('projection', lhs - rhs)
    eqs = Equations([eq])

    ls = ScipyDirect({})

    nls_status = IndexedStruct()
    nls = Newton({}, lin_solver=ls, status=nls_status)

    pb = Problem('aux', equations=eqs, nls=nls, ls=ls)

    pb.time_update()

    # This sets the un variable with the projection solution.
    pb.solve()

    # Copy the projection solution back to target.
    target.set_data(un())

    if nls_status.condition != 0:
        output('L2 projection: solver did not converge!')

def make_h1_projection_data(target, eval_data):
    """
    Project scalar data given by a material-like `eval_data()` function to a
    scalar `target` field variable using the :math:`H^1` dot product.
    """
    order = target.field.approx_order * 2
    integral = Integral('i', order=order)

    un = target.name
    v = FieldVariable('v', 'test', target.field, primary_var_name=un)
    lhs1 = Term.new('dw_volume_dot(v, %s)' % un, integral,
                    target.field.region, v=v, **{un : target})
    lhs2 = Term.new('dw_laplace(v, %s)' % un, integral,
                    target.field.region, v=v, **{un : target})

    def _eval_data(ts, coors, mode, **kwargs):
        if mode == 'qp':
            val = eval_data(ts, coors, mode, 'val', **kwargs)
            gval = eval_data(ts, coors, mode, 'grad', **kwargs)
            return {'val' : val, 'gval' : gval}

    m = Material('m', function=_eval_data)
    rhs1 = Term.new('dw_volume_lvf(m.val, v)', integral, target.field.region,
                    m=m, v=v)
    rhs2 = Term.new('dw_diffusion_r(m.gval, v)', integral, target.field.region,
                    m=m, v=v)

    eq = Equation('projection', lhs1 + lhs2 - rhs1 - rhs2)
    eqs = Equations([eq])

    ls = ScipyDirect({})

    nls_status = IndexedStruct()
    nls = Newton({}, lin_solver=ls, status=nls_status)

    pb = Problem('aux', equations=eqs, nls=nls, ls=ls)

    pb.time_update()

    # This sets the target variable with the projection solution.
    pb.solve()

    if nls_status.condition != 0:
        output('H1 projection: solver did not converge!')

########NEW FILE########
__FILENAME__ = quadratures
"""
`quadrature_tables` are organized as follows::

    quadrature_tables = {
        '<geometry1>' : {
            order1 : QuadraturePoints(args1),
            order2 : QuadraturePoints(args2),
            ...
        },
        '<geometry2>' : {
            order1 : QuadraturePoints(args1),
            order2 : QuadraturePoints(args2),
            ...
        },
        ...
    }

**Note** The order for quadratures on tensor product domains (`'2_4'`,
`'3_8'` geometries) in case of composite Gauss quadratures (products of
1D quadratures) holds for each component separately, so the actual
polynomial order may be much higher (up to `order * dimension`).

Naming conventions in problem description files::

    `<family>_<order>_<dimension>`

Integral 'family' is just an arbitrary name given by user.

Low order quadrature coordinates and weights copied from The Finite Element
Method Displayed by Gouri Dhatt and Gilbert Touzat, Wiley-Interscience
Production, 1984.

The line integral (geometry '1_2') coordinates and weights are from Abramowitz,
M. and Stegun, I.A., Handbook of Mathematical Functions, Dover Publications,
New York, 1972. The triangle (geometry '2_3') coordinates and weights are from
Dunavant, D.A., High Degree Efficient Symmetrical Gaussian Quadrature Rules for
the Triangle, Int. J. Num. Meth. Eng., 21 (1985) pp 1129-1148 - only rules with
points inside the reference triangle are used. The actual values were copied
from PHAML (http://math.nist.gov/phaml/), see also Mitchell, W.F., PHAML User's
Guide, NISTIR 7374, 2006.

Quadrature rules for the quadrilateral (geometry '2_4') and hexahedron
(geometry '3_8') of order higher than 5 are computed as the tensor product of
the line (geometry '1_2') rules.

Quadrature rules for the triangle (geometry '2_3') and tetrahedron (geometry
'3_4') of order higher than 19 and 6, respectively follow A. Grundmann and
H.M. Moeller, Invariant integration formulas for the n-simplex by combinatorial
methods, SIAM J. Numer. Anal.  15 (1978), 282--290. The generating function was
adapted from pytools/hegde codes (http://mathema.tician.de/software/hedge) by
Andreas Kloeckner.
"""
import numpy as nm

from sfepy.base.base import output, assert_, Struct
from sfepy.discrete.simplex_cubature import get_simplex_cubature

simplex_geometries = ['1_2', '2_3', '3_4']
tp_geometries = ['2_4', '3_8']

_msg1 = 'WARNING: quadrature order %s is not available for geometry %s!'
_msg2 = 'WARNING: using %d instead!'

def get_actual_order(geometry, order):
    """
    Return the actual integration order for given geometry.

    Parameters
    ----------
    geometry : str
        The geometry key describing the integration domain,
        see the keys of `quadrature_tables`.

    Returns
    -------
    order : int
        If `order` is in quadrature tables it is this
        value. Otherwise it is the closest higher order. If no
        higher order is available, a warning is printed and the
        highest available order is used.
    """
    table = quadrature_tables[geometry]
    if order not in table:
        orders = table.keys()
        ii = nm.searchsorted(orders, order)
        if ii >= len(orders):
            omax = max(orders)
            output(_msg1 % (order, geometry))
            output(_msg2 % omax)
            order = omax

        else:
            order = orders[ii]

    return order

class QuadraturePoints(Struct):
    """
    Representation of a set of quadrature points.

    Parameters
    ----------
    data : array_like
        The array of shape `(n_point, dim + 1)` of quadrature point
        coordinates (first `dim` columns) and weights (the last column). 
    coors : array_like, optional
        Optionally, instead of using `data`, the coordinates and weights can
        be provided separately - `data` are then ignored.
    weights : array_like, optional
        Optionally, instead of using `data`, the coordinates and weights can
        be provided separately - `data` are then ignored.
    bounds : (float, float), optional
        The coordinates and weights should correspond to a reference
        element in `[0, 1]` x `dim`. Provide the correct bounds if this is
        not the case.
    tp_fix : float, optional
        The value that is used to multiply the tensor product element
        volume (= 1.0) to get the correct volume.
    symmetric : bool
        If True, the integral is 1D and the given coordinates and weights are
        symmetric w.r.t. the centre of bounds; only the non-negative
        coordinates are given.
    """

    @staticmethod
    def from_table(geometry, order):
        """
        Create a new :class:`QuadraturePoints` instance, given reference
        element geometry name and polynomial order. For tensor product
        geometries, the polynomial order is the 1D (line) order.
        """
        table = quadrature_tables[geometry]

        if geometry in simplex_geometries:
            if order > max_orders[geometry]:
                oo = order / 2
                dim = int(geometry[0])
                tp_fix = 0.5 if dim == 2 else 1.0 / 6.0

                coors, weights, exact = get_simplex_cubature(oo, dim)
                qp = QuadraturePoints(None, coors=coors, weights=weights,
                                      bounds=(-1.0, 1.0), tp_fix=tp_fix)
                assert_(exact >= order)

            else:
                order = get_actual_order(geometry, order)
                qp = table[order]

            qp.order = order

        else:
            order1d = order
            dim = int(geometry[0])

            order = dim * order1d
            if order <= max_orders[geometry]:
                order = get_actual_order(geometry, order)
                qp = table[order]
                qp.order = order

            else:
                oo = get_actual_order('1_2', order1d)
                qp1d = quadrature_tables['1_2'][oo]

                weights = nm.outer(qp1d.weights, qp1d.weights)

                nc = qp1d.coors.shape[0]
                if dim == 3:
                    weights = nm.outer(qp1d.weights, weights)

                    iz, iy, ix = nm.mgrid[0:nc, 0:nc, 0:nc]
                    coors = nm.c_[qp1d.coors[ix.ravel()],
                                  qp1d.coors[iy.ravel()],
                                  qp1d.coors[iz.ravel()]].copy()

                else:
                    iy, ix = nm.mgrid[0:nc, 0:nc]
                    coors = nm.c_[qp1d.coors[ix.ravel()],
                                  qp1d.coors[iy.ravel()]].copy()

                weights = weights.ravel()

                qp = QuadraturePoints(None, coors=coors, weights=weights)
                qp.order = oo

        return qp

    def __init__(self, data, coors=None, weights=None, bounds=None, tp_fix=1.0,
                 weight_fix=1.0, symmetric=False):
        if coors is None:
            data = nm.array(data, dtype=nm.float64, ndmin=2)
            self.coors = data[:,:-1].copy()
            self.weights = data[:,-1].copy()

        elif weights is not None:
            self.coors = nm.array(coors, dtype=nm.float64, ndmin=2)
            self.weights = nm.array(weights, dtype=nm.float64)

        else:
            raise ValueError('both "coors" and "weights" have to be provided!')

        self.weights *= weight_fix

        self.n_point, self.dim = self.coors.shape

        self.bounds = (0, 1)
        bbox = nm.array([self.bounds] * self.dim, dtype=nm.float64)
        self.volume = nm.prod(bbox.sum(axis=1)) * tp_fix

        if symmetric:
            isym = 0 if data[0, 0] == 0 else None

        if bounds is not None:
            # Transform from given bounds to self.bounds.
            bbox = nm.array([bounds] * self.dim, dtype=nm.float64)
            volume = nm.prod(nm.diff(bbox, axis=1)) * tp_fix

            a, b = bounds
            c, d = self.bounds

            c1 = (d - c) / (b - a)
            c2 = ((b * c) - (a * d)) / (b - a)

            self.coors = c1 * self.coors + c2
            self.weights *= self.volume / volume

        if symmetric:
            if self.coors.shape[1] != 1:
                msg = 'symmetric mode is allowed for 1D integrals only!'
                raise ValueError(msg)
            origin = 0.5 * (self.bounds[0] + self.bounds[1])

            self.coors = nm.r_[2 * origin - self.coors[:isym:-1], self.coors]
            self.weights = nm.r_[self.weights[:isym:-1], self.weights]

_QP = QuadraturePoints
quadrature_tables = {
    '1_2' : {
        1 : _QP([[0.000000000000000e+00, 2.0]],
                bounds=(-1.0, 1.0), symmetric=True),

        3 : _QP([[0.577350269189626e+00, 1.0]],
                bounds=(-1.0, 1.0), symmetric=True),

        5 : _QP([[0.000000000000000e+00, 0.888888888888889e+00],
                 [0.774596669241483e+00, 0.555555555555556e+00]],
                bounds=(-1.0, 1.0), symmetric=True),

        7 : _QP([[0.339981043584856e+00, 0.652145154862546e+00],
                 [0.861136311594053e+00, 0.347854845137454e+00]],
                bounds=(-1.0, 1.0), symmetric=True),

        9 : _QP([[0.000000000000000e+00, 0.568888888888889e+00],
                 [0.538469310105683e+00, 0.478628670499366e+00],
                 [0.906179845938664e+00, 0.236926885056189e+00]],
                bounds=(-1.0, 1.0), symmetric=True),

        11 : _QP([[0.238619186083197e+00, 0.467913934572691e+00],
                  [0.661209386466265e+00, 0.360761573048139e+00],
                  [0.932469514203152e+00, 0.171324492379170e+00]],
                 bounds=(-1.0, 1.0), symmetric=True),

        13 : _QP([[0.000000000000000e+00, 0.417959183673469e+00],
                  [0.405845151377397e+00, 0.381830050505119e+00],
                  [0.741531185599394e+00, 0.279705391489277e+00],
                  [0.949107912342759e+00, 0.129484966168870e+00]],
                 bounds=(-1.0, 1.0), symmetric=True),

        15 : _QP([[0.183434642495650e+00, 0.362683783378362e+00],
                  [0.525532409916329e+00, 0.313706645877887e+00],
                  [0.796666477413627e+00, 0.222381034453374e+00],
                  [0.960289856497536e+00, 0.101228536290376e+00]],
                 bounds=(-1.0, 1.0), symmetric=True),

        17 : _QP([[0.000000000000000e+00, 0.330239355001260e+00],
                  [0.324253423403809e+00, 0.312347077040003e+00],
                  [0.613371432700590e+00, 0.260610696402935e+00],
                  [0.836031107326636e+00, 0.180648160694857e+00],
                  [0.968160239507626e+00, 0.081274388361574e+00]],
                 bounds=(-1.0, 1.0), symmetric=True),

        19 : _QP([[0.148874338981631e+00, 0.295524224714753e+00],
                  [0.433395394129247e+00, 0.269266719309996e+00],
                  [0.679409568299024e+00, 0.219086362515982e+00],
                  [0.865063366688985e+00, 0.149451349150581e+00],
                  [0.973906528517172e+00, 0.066671344308688e+00]],
                 bounds=(-1.0, 1.0), symmetric=True),

        23 : _QP([[0.125233408511469e+00, 0.249147045813403e+00],
                  [0.367831498998180e+00, 0.233492536538355e+00],
                  [0.587317954286617e+00, 0.203167426723066e+00],
                  [0.769902674194305e+00, 0.160078328543346e+00],
                  [0.904117256370475e+00, 0.106939325995318e+00],
                  [0.981560634246719e+00, 0.047175336386512e+00]],
                 bounds=(-1.0, 1.0), symmetric=True),

        31 : _QP([[0.095012509837637440185e+00, 0.189450610455068496285e+00],
                  [0.281603550779258913230e+00, 0.182603415044923588867e+00],
                  [0.458016777657227386342e+00, 0.169156519395002538189e+00],
                  [0.617876244402643748447e+00, 0.149595988816576732081e+00],
                  [0.755404408355003033895e+00, 0.124628971255533872052e+00],
                  [0.865631202387831743880e+00, 0.095158511682492784810e+00],
                  [0.944575023073232576078e+00, 0.062253523938647892863e+00],
                  [0.989400934991649932596e+00, 0.027152459411754094852e+00]],
                 bounds=(-1.0, 1.0), symmetric=True),

        39 : _QP([[0.076526521133497333755e+00, 0.152753387130725850698e+00],
                  [0.227785851141645078080e+00, 0.149172986472603746788e+00],
                  [0.373706088715419560673e+00, 0.142096109318382051329e+00],
                  [0.510867001950827098004e+00, 0.131688638449176626898e+00],
                  [0.636053680726515025453e+00, 0.118194531961518417312e+00],
                  [0.746331906460150792614e+00, 0.101930119817240435037e+00],
                  [0.839116971822218823395e+00, 0.083276741576704748725e+00],
                  [0.912234428251325905868e+00, 0.062672048334109063570e+00],
                  [0.963971927277913791268e+00, 0.040601429800386941331e+00],
                  [0.993128599185094924786e+00, 0.017614007139152118312e+00]],
                 bounds=(-1.0, 1.0), symmetric=True),

        47 : _QP([[0.064056892862605626085e+00, 0.127938195346752156974e+00],
                  [0.191118867473616309159e+00, 0.125837456346828296121e+00],
                  [0.315042679696163374387e+00, 0.121670472927803391204e+00],
                  [0.433793507626045138487e+00, 0.115505668053725601353e+00],
                  [0.545421471388839535658e+00, 0.107444270115965634783e+00],
                  [0.648093651936975569252e+00, 0.097618652104113888270e+00],
                  [0.740124191578554364244e+00, 0.086190161531953275917e+00],
                  [0.820001985973902921954e+00, 0.073346481411080305734e+00],
                  [0.886415527004401034213e+00, 0.059298584915436780746e+00],
                  [0.938274552002732758524e+00, 0.044277438817419806169e+00],
                  [0.974728555971309498198e+00, 0.028531388628933663181e+00],
                  [0.995187219997021360180e+00, 0.012341229799987199547e+00]],
                 bounds=(-1.0, 1.0), symmetric=True),
    },

    '2_3' : {
        1 : _QP([[1.0/3.0, 1.0/3.0, 0.5]],
                tp_fix=0.5),

        2 : _QP([[1.0/6.0, 1.0/6.0, 1.0/6.0],
                 [2.0/3.0, 1.0/6.0, 1.0/6.0],
                 [1.0/6.0, 2.0/3.0, 1.0/6.0]],
                tp_fix=0.5),

        3 : _QP([[1.0/3.0, 1.0/3.0,-27.0/96.0],
                 [1.0/5.0, 1.0/5.0, 25.0/96.0],
                 [3.0/5.0, 1.0/5.0, 25.0/96.0],
                 [1.0/5.0, 3.0/5.0, 25.0/96.0]],
                tp_fix=0.5),

        4 : _QP([[0.445948490915965e+00, 0.445948490915965e+00, 0.223381589678011e+00],
                 [0.108103018168070e+00, 0.445948490915965e+00, 0.223381589678011e+00],
                 [0.445948490915965e+00, 0.108103018168070e+00, 0.223381589678011e+00],
                 [0.091576213509771e+00, 0.091576213509771e+00, 0.109951743655322e+00],
                 [0.816847572980459e+00, 0.091576213509771e+00, 0.109951743655322e+00],
                 [0.091576213509771e+00, 0.816847572980459e+00, 0.109951743655322e+00]],
                tp_fix=0.5, weight_fix=0.5),
        5 : _QP([[0.333333333333333e+00, 0.333333333333333e+00, 0.225000000000000e+00],
                 [0.470142064105115e+00, 0.470142064105115e+00, 0.132394152788506e+00],
                 [0.059715871789770e+00, 0.470142064105115e+00, 0.132394152788506e+00],
                 [0.470142064105115e+00, 0.059715871789770e+00, 0.132394152788506e+00],
                 [0.101286507323456e+00, 0.101286507323456e+00, 0.125939180544827e+00],
                 [0.797426985353087e+00, 0.101286507323456e+00, 0.125939180544827e+00],
                 [0.101286507323456e+00, 0.797426985353087e+00, 0.125939180544827e+00]],
                tp_fix=0.5, weight_fix=0.5),
        6 : _QP([[0.249286745170910e+00, 0.249286745170910e+00, 0.116786275726379e+00],
                 [0.501426509658179e+00, 0.249286745170910e+00, 0.116786275726379e+00],
                 [0.249286745170910e+00, 0.501426509658179e+00, 0.116786275726379e+00],
                 [0.063089014491502e+00, 0.063089014491502e+00, 0.050844906370207e+00],
                 [0.873821971016996e+00, 0.063089014491502e+00, 0.050844906370207e+00],
                 [0.063089014491502e+00, 0.873821971016996e+00, 0.050844906370207e+00],
                 [0.310352451033784e+00, 0.636502499121399e+00, 0.082851075618374e+00],
                 [0.636502499121399e+00, 0.310352451033784e+00, 0.082851075618374e+00],
                 [0.053145049844817e+00, 0.636502499121399e+00, 0.082851075618374e+00],
                 [0.636502499121399e+00, 0.053145049844817e+00, 0.082851075618374e+00],
                 [0.310352451033784e+00, 0.053145049844817e+00, 0.082851075618374e+00],
                 [0.053145049844817e+00, 0.310352451033784e+00, 0.082851075618374e+00]],
                tp_fix=0.5, weight_fix=0.5),
        7 : _QP([[0.333333333333333e+00, 0.333333333333333e+00,-0.149570044467682e+00],
                 [0.260345966079040e+00, 0.260345966079040e+00, 0.175615257433208e+00],
                 [0.479308067841920e+00, 0.260345966079040e+00, 0.175615257433208e+00],
                 [0.260345966079040e+00, 0.479308067841920e+00, 0.175615257433208e+00],
                 [0.065130102902216e+00, 0.065130102902216e+00, 0.053347235608838e+00],
                 [0.869739794195568e+00, 0.065130102902216e+00, 0.053347235608838e+00],
                 [0.065130102902216e+00, 0.869739794195568e+00, 0.053347235608838e+00],
                 [0.312865496004874e+00, 0.638444188569810e+00, 0.077113760890257e+00],
                 [0.638444188569810e+00, 0.312865496004874e+00, 0.077113760890257e+00],
                 [0.048690315425316e+00, 0.638444188569810e+00, 0.077113760890257e+00],
                 [0.638444188569810e+00, 0.048690315425316e+00, 0.077113760890257e+00],
                 [0.312865496004874e+00, 0.048690315425316e+00, 0.077113760890257e+00],
                 [0.048690315425316e+00, 0.312865496004874e+00, 0.077113760890257e+00]],
                tp_fix=0.5, weight_fix=0.5),
        8 : _QP([[0.333333333333333e+00, 0.333333333333333e+00, 0.144315607677787e+00],
                 [0.459292588292723e+00, 0.459292588292723e+00, 0.095091634267285e+00],
                 [0.081414823414554e+00, 0.459292588292723e+00, 0.095091634267285e+00],
                 [0.459292588292723e+00, 0.081414823414554e+00, 0.095091634267285e+00],
                 [0.170569307751760e+00, 0.170569307751760e+00, 0.103217370534718e+00],
                 [0.658861384496480e+00, 0.170569307751760e+00, 0.103217370534718e+00],
                 [0.170569307751760e+00, 0.658861384496480e+00, 0.103217370534718e+00],
                 [0.050547228317031e+00, 0.050547228317031e+00, 0.032458497623198e+00],
                 [0.898905543365938e+00, 0.050547228317031e+00, 0.032458497623198e+00],
                 [0.050547228317031e+00, 0.898905543365938e+00, 0.032458497623198e+00],
                 [0.263112829634638e+00, 0.728492392955404e+00, 0.027230314174435e+00],
                 [0.728492392955404e+00, 0.263112829634638e+00, 0.027230314174435e+00],
                 [0.008394777409958e+00, 0.728492392955404e+00, 0.027230314174435e+00],
                 [0.728492392955404e+00, 0.008394777409958e+00, 0.027230314174435e+00],
                 [0.263112829634638e+00, 0.008394777409958e+00, 0.027230314174435e+00],
                 [0.008394777409958e+00, 0.263112829634638e+00, 0.027230314174435e+00]],
                tp_fix=0.5, weight_fix=0.5),
        9 : _QP([[0.333333333333333e+00, 0.333333333333333e+00, 0.097135796282799e+00],
                 [0.489682519198738e+00, 0.489682519198738e+00, 0.031334700227139e+00],
                 [0.020634961602525e+00, 0.489682519198738e+00, 0.031334700227139e+00],
                 [0.489682519198738e+00, 0.020634961602525e+00, 0.031334700227139e+00],
                 [0.437089591492937e+00, 0.437089591492937e+00, 0.077827541004774e+00],
                 [0.125820817014127e+00, 0.437089591492937e+00, 0.077827541004774e+00],
                 [0.437089591492937e+00, 0.125820817014127e+00, 0.077827541004774e+00],
                 [0.188203535619033e+00, 0.188203535619033e+00, 0.079647738927210e+00],
                 [0.623592928761935e+00, 0.188203535619033e+00, 0.079647738927210e+00],
                 [0.188203535619033e+00, 0.623592928761935e+00, 0.079647738927210e+00],
                 [0.044729513394453e+00, 0.044729513394453e+00, 0.025577675658698e+00],
                 [0.910540973211095e+00, 0.044729513394453e+00, 0.025577675658698e+00],
                 [0.044729513394453e+00, 0.910540973211095e+00, 0.025577675658698e+00],
                 [0.221962989160766e+00, 0.741198598784498e+00, 0.043283539377289e+00],
                 [0.741198598784498e+00, 0.221962989160766e+00, 0.043283539377289e+00],
                 [0.036838412054736e+00, 0.741198598784498e+00, 0.043283539377289e+00],
                 [0.741198598784498e+00, 0.036838412054736e+00, 0.043283539377289e+00],
                 [0.221962989160766e+00, 0.036838412054736e+00, 0.043283539377289e+00],
                 [0.036838412054736e+00, 0.221962989160766e+00, 0.043283539377289e+00]],
                tp_fix=0.5, weight_fix=0.5),
        10 : _QP([[0.333333333333333e+00, 0.333333333333333e+00, 0.908179903827540e-01],
                  [0.485577633383657e+00, 0.485577633383657e+00, 0.367259577564670e-01],
                  [0.288447332326850e-01, 0.485577633383657e+00, 0.367259577564670e-01],
                  [0.485577633383657e+00, 0.288447332326850e-01, 0.367259577564670e-01],
                  [0.109481575485037e+00, 0.109481575485037e+00, 0.453210594355280e-01],
                  [0.781036849029926e+00, 0.109481575485037e+00, 0.453210594355280e-01],
                  [0.109481575485037e+00, 0.781036849029926e+00, 0.453210594355280e-01],
                  [0.307939838764121e+00, 0.550352941820999e+00, 0.727579168454200e-01],
                  [0.550352941820999e+00, 0.307939838764121e+00, 0.727579168454200e-01],
                  [0.141707219414880e+00, 0.550352941820999e+00, 0.727579168454200e-01],
                  [0.550352941820999e+00, 0.141707219414880e+00, 0.727579168454200e-01],
                  [0.307939838764121e+00, 0.141707219414880e+00, 0.727579168454200e-01],
                  [0.141707219414880e+00, 0.307939838764121e+00, 0.727579168454200e-01],
                  [0.246672560639903e+00, 0.728323904597411e+00, 0.283272425310570e-01],
                  [0.728323904597411e+00, 0.246672560639903e+00, 0.283272425310570e-01],
                  [0.250035347626860e-01, 0.728323904597411e+00, 0.283272425310570e-01],
                  [0.728323904597411e+00, 0.250035347626860e-01, 0.283272425310570e-01],
                  [0.246672560639903e+00, 0.250035347626860e-01, 0.283272425310570e-01],
                  [0.250035347626860e-01, 0.246672560639903e+00, 0.283272425310570e-01],
                  [0.668032510122000e-01, 0.923655933587500e+00, 0.942166696373300e-02],
                  [0.923655933587500e+00, 0.668032510122000e-01, 0.942166696373300e-02],
                  [0.954081540029900e-02, 0.923655933587500e+00, 0.942166696373300e-02],
                  [0.923655933587500e+00, 0.954081540029900e-02, 0.942166696373300e-02],
                  [0.668032510122000e-01, 0.954081540029900e-02, 0.942166696373300e-02],
                  [0.954081540029900e-02, 0.668032510122000e-01, 0.942166696373300e-02]],
                tp_fix=0.5, weight_fix=0.5),
        12 : _QP([[0.488217389773805e+00, 0.488217389773805e+00, 0.257310664404550e-01],
                  [0.235652204523900e-01, 0.488217389773805e+00, 0.257310664404550e-01],
                  [0.488217389773805e+00, 0.235652204523900e-01, 0.257310664404550e-01],
                  [0.439724392294460e+00, 0.439724392294460e+00, 0.436925445380380e-01],
                  [0.120551215411079e+00, 0.439724392294460e+00, 0.436925445380380e-01],
                  [0.439724392294460e+00, 0.120551215411079e+00, 0.436925445380380e-01],
                  [0.271210385012116e+00, 0.271210385012116e+00, 0.628582242178850e-01],
                  [0.457579229975768e+00, 0.271210385012116e+00, 0.628582242178850e-01],
                  [0.271210385012116e+00, 0.457579229975768e+00, 0.628582242178850e-01],
                  [0.127576145541586e+00, 0.127576145541586e+00, 0.347961129307090e-01],
                  [0.744847708916828e+00, 0.127576145541586e+00, 0.347961129307090e-01],
                  [0.127576145541586e+00, 0.744847708916828e+00, 0.347961129307090e-01],
                  [0.213173504532100e-01, 0.213173504532100e-01, 0.616626105155900e-02],
                  [0.957365299093579e+00, 0.213173504532100e-01, 0.616626105155900e-02],
                  [0.213173504532100e-01, 0.957365299093579e+00, 0.616626105155900e-02],
                  [0.275713269685514e+00, 0.608943235779788e+00, 0.403715577663810e-01],
                  [0.608943235779788e+00, 0.275713269685514e+00, 0.403715577663810e-01],
                  [0.115343494534698e+00, 0.608943235779788e+00, 0.403715577663810e-01],
                  [0.608943235779788e+00, 0.115343494534698e+00, 0.403715577663810e-01],
                  [0.275713269685514e+00, 0.115343494534698e+00, 0.403715577663810e-01],
                  [0.115343494534698e+00, 0.275713269685514e+00, 0.403715577663810e-01],
                  [0.281325580989940e+00, 0.695836086787803e+00, 0.223567732023030e-01],
                  [0.695836086787803e+00, 0.281325580989940e+00, 0.223567732023030e-01],
                  [0.228383322222570e-01, 0.695836086787803e+00, 0.223567732023030e-01],
                  [0.695836086787803e+00, 0.228383322222570e-01, 0.223567732023030e-01],
                  [0.281325580989940e+00, 0.228383322222570e-01, 0.223567732023030e-01],
                  [0.228383322222570e-01, 0.281325580989940e+00, 0.223567732023030e-01],
                  [0.116251915907597e+00, 0.858014033544073e+00, 0.173162311086590e-01],
                  [0.858014033544073e+00, 0.116251915907597e+00, 0.173162311086590e-01],
                  [0.257340505483300e-01, 0.858014033544073e+00, 0.173162311086590e-01],
                  [0.858014033544073e+00, 0.257340505483300e-01, 0.173162311086590e-01],
                  [0.116251915907597e+00, 0.257340505483300e-01, 0.173162311086590e-01],
                  [0.257340505483300e-01, 0.116251915907597e+00, 0.173162311086590e-01]],
                tp_fix=0.5, weight_fix=0.5),
        13 : _QP([[0.333333333333333e+00, 0.333333333333333e+00, 0.525209234008020e-01],
                  [0.495048184939705e+00, 0.495048184939705e+00, 0.112801452093300e-01],
                  [0.990363012059100e-02, 0.495048184939705e+00, 0.112801452093300e-01],
                  [0.495048184939705e+00, 0.990363012059100e-02, 0.112801452093300e-01],
                  [0.468716635109574e+00, 0.468716635109574e+00, 0.314235183624540e-01],
                  [0.625667297808520e-01, 0.468716635109574e+00, 0.314235183624540e-01],
                  [0.468716635109574e+00, 0.625667297808520e-01, 0.314235183624540e-01],
                  [0.414521336801277e+00, 0.414521336801277e+00, 0.470725025041940e-01],
                  [0.170957326397447e+00, 0.414521336801277e+00, 0.470725025041940e-01],
                  [0.414521336801277e+00, 0.170957326397447e+00, 0.470725025041940e-01],
                  [0.229399572042831e+00, 0.229399572042831e+00, 0.473635865363550e-01],
                  [0.541200855914337e+00, 0.229399572042831e+00, 0.473635865363550e-01],
                  [0.229399572042831e+00, 0.541200855914337e+00, 0.473635865363550e-01],
                  [0.114424495196330e+00, 0.114424495196330e+00, 0.311675290457940e-01],
                  [0.771151009607340e+00, 0.114424495196330e+00, 0.311675290457940e-01],
                  [0.114424495196330e+00, 0.771151009607340e+00, 0.311675290457940e-01],
                  [0.248113913634590e-01, 0.248113913634590e-01, 0.797577146507400e-02],
                  [0.950377217273082e+00, 0.248113913634590e-01, 0.797577146507400e-02],
                  [0.248113913634590e-01, 0.950377217273082e+00, 0.797577146507400e-02],
                  [0.268794997058761e+00, 0.636351174561660e+00, 0.368484027287320e-01],
                  [0.636351174561660e+00, 0.268794997058761e+00, 0.368484027287320e-01],
                  [0.948538283795790e-01, 0.636351174561660e+00, 0.368484027287320e-01],
                  [0.636351174561660e+00, 0.948538283795790e-01, 0.368484027287320e-01],
                  [0.268794997058761e+00, 0.948538283795790e-01, 0.368484027287320e-01],
                  [0.948538283795790e-01, 0.268794997058761e+00, 0.368484027287320e-01],
                  [0.291730066734288e+00, 0.690169159986905e+00, 0.174014633038220e-01],
                  [0.690169159986905e+00, 0.291730066734288e+00, 0.174014633038220e-01],
                  [0.181007732788070e-01, 0.690169159986905e+00, 0.174014633038220e-01],
                  [0.690169159986905e+00, 0.181007732788070e-01, 0.174014633038220e-01],
                  [0.291730066734288e+00, 0.181007732788070e-01, 0.174014633038220e-01],
                  [0.181007732788070e-01, 0.291730066734288e+00, 0.174014633038220e-01],
                  [0.126357385491669e+00, 0.851409537834241e+00, 0.155217868390450e-01],
                  [0.851409537834241e+00, 0.126357385491669e+00, 0.155217868390450e-01],
                  [0.222330766740900e-01, 0.851409537834241e+00, 0.155217868390450e-01],
                  [0.851409537834241e+00, 0.222330766740900e-01, 0.155217868390450e-01],
                  [0.126357385491669e+00, 0.222330766740900e-01, 0.155217868390450e-01],
                  [0.222330766740900e-01, 0.126357385491669e+00, 0.155217868390450e-01]],
                tp_fix=0.5, weight_fix=0.5),
        14 : _QP([[0.488963910362179e+00, 0.488963910362179e+00, 0.218835813694290e-01],
                  [0.220721792756430e-01, 0.488963910362179e+00, 0.218835813694290e-01],
                  [0.488963910362179e+00, 0.220721792756430e-01, 0.218835813694290e-01],
                  [0.417644719340454e+00, 0.417644719340454e+00, 0.327883535441250e-01],
                  [0.164710561319092e+00, 0.417644719340454e+00, 0.327883535441250e-01],
                  [0.417644719340454e+00, 0.164710561319092e+00, 0.327883535441250e-01],
                  [0.273477528308839e+00, 0.273477528308839e+00, 0.517741045072920e-01],
                  [0.453044943382323e+00, 0.273477528308839e+00, 0.517741045072920e-01],
                  [0.273477528308839e+00, 0.453044943382323e+00, 0.517741045072920e-01],
                  [0.177205532412543e+00, 0.177205532412543e+00, 0.421625887369930e-01],
                  [0.645588935174913e+00, 0.177205532412543e+00, 0.421625887369930e-01],
                  [0.177205532412543e+00, 0.645588935174913e+00, 0.421625887369930e-01],
                  [0.617998830908730e-01, 0.617998830908730e-01, 0.144336996697770e-01],
                  [0.876400233818255e+00, 0.617998830908730e-01, 0.144336996697770e-01],
                  [0.617998830908730e-01, 0.876400233818255e+00, 0.144336996697770e-01],
                  [0.193909612487010e-01, 0.193909612487010e-01, 0.492340360240000e-02],
                  [0.961218077502598e+00, 0.193909612487010e-01, 0.492340360240000e-02],
                  [0.193909612487010e-01, 0.961218077502598e+00, 0.492340360240000e-02],
                  [0.172266687821356e+00, 0.770608554774996e+00, 0.246657532125640e-01],
                  [0.770608554774996e+00, 0.172266687821356e+00, 0.246657532125640e-01],
                  [0.571247574036480e-01, 0.770608554774996e+00, 0.246657532125640e-01],
                  [0.770608554774996e+00, 0.571247574036480e-01, 0.246657532125640e-01],
                  [0.172266687821356e+00, 0.571247574036480e-01, 0.246657532125640e-01],
                  [0.571247574036480e-01, 0.172266687821356e+00, 0.246657532125640e-01],
                  [0.336861459796345e+00, 0.570222290846683e+00, 0.385715107870610e-01],
                  [0.570222290846683e+00, 0.336861459796345e+00, 0.385715107870610e-01],
                  [0.929162493569720e-01, 0.570222290846683e+00, 0.385715107870610e-01],
                  [0.570222290846683e+00, 0.929162493569720e-01, 0.385715107870610e-01],
                  [0.336861459796345e+00, 0.929162493569720e-01, 0.385715107870610e-01],
                  [0.929162493569720e-01, 0.336861459796345e+00, 0.385715107870610e-01],
                  [0.298372882136258e+00, 0.686980167808088e+00, 0.144363081135340e-01],
                  [0.686980167808088e+00, 0.298372882136258e+00, 0.144363081135340e-01],
                  [0.146469500556540e-01, 0.686980167808088e+00, 0.144363081135340e-01],
                  [0.686980167808088e+00, 0.146469500556540e-01, 0.144363081135340e-01],
                  [0.298372882136258e+00, 0.146469500556540e-01, 0.144363081135340e-01],
                  [0.146469500556540e-01, 0.298372882136258e+00, 0.144363081135340e-01],
                  [0.118974497696957e+00, 0.879757171370171e+00, 0.501022883850100e-02],
                  [0.879757171370171e+00, 0.118974497696957e+00, 0.501022883850100e-02],
                  [0.126833093287200e-02, 0.879757171370171e+00, 0.501022883850100e-02],
                  [0.879757171370171e+00, 0.126833093287200e-02, 0.501022883850100e-02],
                  [0.118974497696957e+00, 0.126833093287200e-02, 0.501022883850100e-02],
                  [0.126833093287200e-02, 0.118974497696957e+00, 0.501022883850100e-02]],
                tp_fix=0.5, weight_fix=0.5),
        17 : _QP([[0.333333333333333e+00, 0.333333333333333e+00, 0.334371992908030e-01],
                  [0.497170540556774e+00, 0.497170540556774e+00, 0.509341544050700e-02],
                  [0.565891888645200e-02, 0.497170540556774e+00, 0.509341544050700e-02],
                  [0.497170540556774e+00, 0.565891888645200e-02, 0.509341544050700e-02],
                  [0.482176322624625e+00, 0.482176322624625e+00, 0.146708645276380e-01],
                  [0.356473547507510e-01, 0.482176322624625e+00, 0.146708645276380e-01],
                  [0.482176322624625e+00, 0.356473547507510e-01, 0.146708645276380e-01],
                  [0.450239969020782e+00, 0.450239969020782e+00, 0.243508783536720e-01],
                  [0.995200619584370e-01, 0.450239969020782e+00, 0.243508783536720e-01],
                  [0.450239969020782e+00, 0.995200619584370e-01, 0.243508783536720e-01],
                  [0.400266239377397e+00, 0.400266239377397e+00, 0.311075508689690e-01],
                  [0.199467521245206e+00, 0.400266239377397e+00, 0.311075508689690e-01],
                  [0.400266239377397e+00, 0.199467521245206e+00, 0.311075508689690e-01],
                  [0.252141267970953e+00, 0.252141267970953e+00, 0.312571112186200e-01],
                  [0.495717464058095e+00, 0.252141267970953e+00, 0.312571112186200e-01],
                  [0.252141267970953e+00, 0.495717464058095e+00, 0.312571112186200e-01],
                  [0.162047004658461e+00, 0.162047004658461e+00, 0.248156543396650e-01],
                  [0.675905990683077e+00, 0.162047004658461e+00, 0.248156543396650e-01],
                  [0.162047004658461e+00, 0.675905990683077e+00, 0.248156543396650e-01],
                  [0.758758822607460e-01, 0.758758822607460e-01, 0.140560730705570e-01],
                  [0.848248235478508e+00, 0.758758822607460e-01, 0.140560730705570e-01],
                  [0.758758822607460e-01, 0.848248235478508e+00, 0.140560730705570e-01],
                  [0.156547269678220e-01, 0.156547269678220e-01, 0.319467617377900e-02],
                  [0.968690546064356e+00, 0.156547269678220e-01, 0.319467617377900e-02],
                  [0.156547269678220e-01, 0.968690546064356e+00, 0.319467617377900e-02],
                  [0.334319867363658e+00, 0.655493203809423e+00, 0.811965531899300e-02],
                  [0.655493203809423e+00, 0.334319867363658e+00, 0.811965531899300e-02],
                  [0.101869288269190e-01, 0.655493203809423e+00, 0.811965531899300e-02],
                  [0.655493203809423e+00, 0.101869288269190e-01, 0.811965531899300e-02],
                  [0.334319867363658e+00, 0.101869288269190e-01, 0.811965531899300e-02],
                  [0.101869288269190e-01, 0.334319867363658e+00, 0.811965531899300e-02],
                  [0.292221537796944e+00, 0.572337590532020e+00, 0.268057422831630e-01],
                  [0.572337590532020e+00, 0.292221537796944e+00, 0.268057422831630e-01],
                  [0.135440871671036e+00, 0.572337590532020e+00, 0.268057422831630e-01],
                  [0.572337590532020e+00, 0.135440871671036e+00, 0.268057422831630e-01],
                  [0.292221537796944e+00, 0.135440871671036e+00, 0.268057422831630e-01],
                  [0.135440871671036e+00, 0.292221537796944e+00, 0.268057422831630e-01],
                  [0.319574885423190e+00, 0.626001190286228e+00, 0.184599932108220e-01],
                  [0.626001190286228e+00, 0.319574885423190e+00, 0.184599932108220e-01],
                  [0.544239242905830e-01, 0.626001190286228e+00, 0.184599932108220e-01],
                  [0.626001190286228e+00, 0.544239242905830e-01, 0.184599932108220e-01],
                  [0.319574885423190e+00, 0.544239242905830e-01, 0.184599932108220e-01],
                  [0.544239242905830e-01, 0.319574885423190e+00, 0.184599932108220e-01],
                  [0.190704224192292e+00, 0.796427214974071e+00, 0.847686853432800e-02],
                  [0.796427214974071e+00, 0.190704224192292e+00, 0.847686853432800e-02],
                  [0.128685608336370e-01, 0.796427214974071e+00, 0.847686853432800e-02],
                  [0.796427214974071e+00, 0.128685608336370e-01, 0.847686853432800e-02],
                  [0.190704224192292e+00, 0.128685608336370e-01, 0.847686853432800e-02],
                  [0.128685608336370e-01, 0.190704224192292e+00, 0.847686853432800e-02],
                  [0.180483211648746e+00, 0.752351005937729e+00, 0.182927967700250e-01],
                  [0.752351005937729e+00, 0.180483211648746e+00, 0.182927967700250e-01],
                  [0.671657824135240e-01, 0.752351005937729e+00, 0.182927967700250e-01],
                  [0.752351005937729e+00, 0.671657824135240e-01, 0.182927967700250e-01],
                  [0.180483211648746e+00, 0.671657824135240e-01, 0.182927967700250e-01],
                  [0.671657824135240e-01, 0.180483211648746e+00, 0.182927967700250e-01],
                  [0.807113136795640e-01, 0.904625504095608e+00, 0.666563200416500e-02],
                  [0.904625504095608e+00, 0.807113136795640e-01, 0.666563200416500e-02],
                  [0.146631822248280e-01, 0.904625504095608e+00, 0.666563200416500e-02],
                  [0.904625504095608e+00, 0.146631822248280e-01, 0.666563200416500e-02],
                  [0.807113136795640e-01, 0.146631822248280e-01, 0.666563200416500e-02],
                  [0.146631822248280e-01, 0.807113136795640e-01, 0.666563200416500e-02]],
                tp_fix=0.5, weight_fix=0.5),
        19 : _QP([[0.333333333333333e+00, 0.333333333333333e+00, 0.329063313889190e-01],
                  [0.489609987073006e+00, 0.489609987073006e+00, 0.103307318912720e-01],
                  [0.207800258539870e-01, 0.489609987073006e+00, 0.103307318912720e-01],
                  [0.489609987073006e+00, 0.207800258539870e-01, 0.103307318912720e-01],
                  [0.454536892697893e+00, 0.454536892697893e+00, 0.223872472630160e-01],
                  [0.909262146042150e-01, 0.454536892697893e+00, 0.223872472630160e-01],
                  [0.454536892697893e+00, 0.909262146042150e-01, 0.223872472630160e-01],
                  [0.401416680649431e+00, 0.401416680649431e+00, 0.302661258694680e-01],
                  [0.197166638701138e+00, 0.401416680649431e+00, 0.302661258694680e-01],
                  [0.401416680649431e+00, 0.197166638701138e+00, 0.302661258694680e-01],
                  [0.255551654403098e+00, 0.255551654403098e+00, 0.304909678021980e-01],
                  [0.488896691193805e+00, 0.255551654403098e+00, 0.304909678021980e-01],
                  [0.255551654403098e+00, 0.488896691193805e+00, 0.304909678021980e-01],
                  [0.177077942152130e+00, 0.177077942152130e+00, 0.241592127416410e-01],
                  [0.645844115695741e+00, 0.177077942152130e+00, 0.241592127416410e-01],
                  [0.177077942152130e+00, 0.645844115695741e+00, 0.241592127416410e-01],
                  [0.110061053227952e+00, 0.110061053227952e+00, 0.160508035868010e-01],
                  [0.779877893544096e+00, 0.110061053227952e+00, 0.160508035868010e-01],
                  [0.110061053227952e+00, 0.779877893544096e+00, 0.160508035868010e-01],
                  [0.555286242518400e-01, 0.555286242518400e-01, 0.808458026178400e-02],
                  [0.888942751496321e+00, 0.555286242518400e-01, 0.808458026178400e-02],
                  [0.555286242518400e-01, 0.888942751496321e+00, 0.808458026178400e-02],
                  [0.126218637772290e-01, 0.126218637772290e-01, 0.207936202748500e-02],
                  [0.974756272445543e+00, 0.126218637772290e-01, 0.207936202748500e-02],
                  [0.126218637772290e-01, 0.974756272445543e+00, 0.207936202748500e-02],
                  [0.395754787356943e+00, 0.600633794794645e+00, 0.388487690498100e-02],
                  [0.600633794794645e+00, 0.395754787356943e+00, 0.388487690498100e-02],
                  [0.361141784841200e-02, 0.600633794794645e+00, 0.388487690498100e-02],
                  [0.600633794794645e+00, 0.361141784841200e-02, 0.388487690498100e-02],
                  [0.395754787356943e+00, 0.361141784841200e-02, 0.388487690498100e-02],
                  [0.361141784841200e-02, 0.395754787356943e+00, 0.388487690498100e-02],
                  [0.307929983880436e+00, 0.557603261588784e+00, 0.255741606120220e-01],
                  [0.557603261588784e+00, 0.307929983880436e+00, 0.255741606120220e-01],
                  [0.134466754530780e+00, 0.557603261588784e+00, 0.255741606120220e-01],
                  [0.557603261588784e+00, 0.134466754530780e+00, 0.255741606120220e-01],
                  [0.307929983880436e+00, 0.134466754530780e+00, 0.255741606120220e-01],
                  [0.134466754530780e+00, 0.307929983880436e+00, 0.255741606120220e-01],
                  [0.264566948406520e+00, 0.720987025817365e+00, 0.888090357333800e-02],
                  [0.720987025817365e+00, 0.264566948406520e+00, 0.888090357333800e-02],
                  [0.144460257761150e-01, 0.720987025817365e+00, 0.888090357333800e-02],
                  [0.720987025817365e+00, 0.144460257761150e-01, 0.888090357333800e-02],
                  [0.264566948406520e+00, 0.144460257761150e-01, 0.888090357333800e-02],
                  [0.144460257761150e-01, 0.264566948406520e+00, 0.888090357333800e-02],
                  [0.358539352205951e+00, 0.594527068955871e+00, 0.161245467617310e-01],
                  [0.594527068955871e+00, 0.358539352205951e+00, 0.161245467617310e-01],
                  [0.469335788381780e-01, 0.594527068955871e+00, 0.161245467617310e-01],
                  [0.594527068955871e+00, 0.469335788381780e-01, 0.161245467617310e-01],
                  [0.358539352205951e+00, 0.469335788381780e-01, 0.161245467617310e-01],
                  [0.469335788381780e-01, 0.358539352205951e+00, 0.161245467617310e-01],
                  [0.157807405968595e+00, 0.839331473680839e+00, 0.249194181749100e-02],
                  [0.839331473680839e+00, 0.157807405968595e+00, 0.249194181749100e-02],
                  [0.286112035056700e-02, 0.839331473680839e+00, 0.249194181749100e-02],
                  [0.839331473680839e+00, 0.286112035056700e-02, 0.249194181749100e-02],
                  [0.157807405968595e+00, 0.286112035056700e-02, 0.249194181749100e-02],
                  [0.286112035056700e-02, 0.157807405968595e+00, 0.249194181749100e-02],
                  [0.750505969759110e-01, 0.701087978926173e+00, 0.182428401189510e-01],
                  [0.701087978926173e+00, 0.750505969759110e-01, 0.182428401189510e-01],
                  [0.223861424097916e+00, 0.701087978926173e+00, 0.182428401189510e-01],
                  [0.701087978926173e+00, 0.223861424097916e+00, 0.182428401189510e-01],
                  [0.750505969759110e-01, 0.223861424097916e+00, 0.182428401189510e-01],
                  [0.223861424097916e+00, 0.750505969759110e-01, 0.182428401189510e-01],
                  [0.142421601113383e+00, 0.822931324069857e+00, 0.102585637361990e-01],
                  [0.822931324069857e+00, 0.142421601113383e+00, 0.102585637361990e-01],
                  [0.346470748167600e-01, 0.822931324069857e+00, 0.102585637361990e-01],
                  [0.822931324069857e+00, 0.346470748167600e-01, 0.102585637361990e-01],
                  [0.142421601113383e+00, 0.346470748167600e-01, 0.102585637361990e-01],
                  [0.346470748167600e-01, 0.142421601113383e+00, 0.102585637361990e-01],
                  [0.654946280829380e-01, 0.924344252620784e+00, 0.379992885530200e-02],
                  [0.924344252620784e+00, 0.654946280829380e-01, 0.379992885530200e-02],
                  [0.101611192962780e-01, 0.924344252620784e+00, 0.379992885530200e-02],
                  [0.924344252620784e+00, 0.101611192962780e-01, 0.379992885530200e-02],
                  [0.654946280829380e-01, 0.101611192962780e-01, 0.379992885530200e-02],
                  [0.101611192962780e-01, 0.654946280829380e-01, 0.379992885530200e-02]],
                tp_fix=0.5, weight_fix=0.5),
    },

    '2_4' : {

        2 : _QP([[ nm.sqrt(2.0/3.0), 0.0         , 4.0/3.0],
                 [-1/nm.sqrt(6)    , 1/nm.sqrt(2), 4.0/3.0],
                 [-1/nm.sqrt(6)    ,-1/nm.sqrt(2), 4.0/3.0]], bounds=(-1.0, 1.0)),

        3 : _QP([[-1/nm.sqrt(3),-1/nm.sqrt(3), 1.0],
                 [ 1/nm.sqrt(3),-1/nm.sqrt(3), 1.0],
                 [ 1/nm.sqrt(3), 1/nm.sqrt(3), 1.0],
                 [-1/nm.sqrt(3), 1/nm.sqrt(3), 1.0]], bounds=(-1.0, 1.0)),

        5 : _QP([[ nm.sqrt(7.0/15.0), 0.0              , 0.816326530612245],
                 [-nm.sqrt(7.0/15.0), 0.0              , 0.816326530612245],
                 [ 0.0              , nm.sqrt(7.0/15.0), 0.816326530612245],
                 [ 0.0              ,-nm.sqrt(7.0/15.0), 0.816326530612245],
                 [ 0.881917103688197, 0.881917103688197, 0.183673469387755],
                 [ 0.881917103688197,-0.881917103688197, 0.183673469387755],
                 [-0.881917103688197, 0.881917103688197, 0.183673469387755],
                 [-0.881917103688197,-0.881917103688197, 0.183673469387755]], bounds=(-1.0, 1.0)),

    },

    '3_4' : {
        1 : _QP([[ 1.0/4.0, 1.0/4.0, 1.0/4.0, 1.0/6.0]], tp_fix=1.0/6.0),

        2 : _QP([[ (5-nm.sqrt(5))/20  , (5-nm.sqrt(5))/20  , (5-nm.sqrt(5))/20  , 1.0/24.0],
                 [ (5-nm.sqrt(5))/20  , (5-nm.sqrt(5))/20  , (5+3*nm.sqrt(5))/20, 1.0/24.0],
                 [ (5-nm.sqrt(5))/20  , (5+3*nm.sqrt(5))/20, (5-nm.sqrt(5))/20  , 1.0/24.0],
                 [ (5+3*nm.sqrt(5))/20, (5-nm.sqrt(5))/20  , (5-nm.sqrt(5))/20  , 1.0/24.0]], tp_fix=1.0/6.0),

        3 : _QP([[ 1.0/4.0, 1.0/4.0, 1.0/4.0,-2.0/15.0],
                 [ 1.0/6.0, 1.0/6.0, 1.0/6.0, 3.0/40.0],
                 [ 1.0/6.0, 1.0/6.0, 1.0/2.0, 3.0/40.0],
                 [ 1.0/6.0, 1.0/2.0, 1.0/6.0, 3.0/40.0],
                 [ 1.0/2.0, 1.0/6.0, 1.0/6.0, 3.0/40.0]], tp_fix=1.0/6.0),

        4 : _QP([[-0.5000000000000000, -0.5000000000000000, -0.5000000000000000, -0.1052444444444440],
                 [-0.8571428571428570, -0.8571428571428570, -0.8571428571428570,  0.0609777777777780],
                 [-0.8571428571428570, -0.8571428571428570,  0.5714285714285710,  0.0609777777777780],
                 [-0.8571428571428570,  0.5714285714285710, -0.8571428571428570,  0.0609777777777780],
                 [ 0.5714285714285710, -0.8571428571428570, -0.8571428571428570,  0.0609777777777780],
                 [-0.2011928476664020, -0.2011928476664020, -0.7988071523335980,  0.1991111111111110],
                 [-0.2011928476664020, -0.7988071523335980, -0.2011928476664020,  0.1991111111111110],
                 [-0.7988071523335980, -0.2011928476664020, -0.2011928476664020,  0.1991111111111110],
                 [-0.2011928476664020, -0.7988071523335980, -0.7988071523335980,  0.1991111111111110],
                 [-0.7988071523335980, -0.2011928476664020, -0.7988071523335980,  0.1991111111111110],
                 [-0.7988071523335980, -0.7988071523335980, -0.2011928476664020,  0.1991111111111110]],
                bounds=(-1.0, 1.0), tp_fix=1.0/6.0),

        6 : _QP([[-0.5707942574816960, -0.5707942574816960, -0.5707942574816960, 0.0532303336775570],
                 [-0.2876172275549120, -0.5707942574816960, -0.5707942574816960, 0.0532303336775570],
                 [-0.5707942574816960, -0.2876172275549120, -0.5707942574816960, 0.0532303336775570],
                 [-0.5707942574816960, -0.5707942574816960, -0.2876172275549120, 0.0532303336775570],
                 [-0.9186520829307770, -0.9186520829307770, -0.9186520829307770, 0.0134362814070940],
                 [0.7559562487923320, -0.9186520829307770, -0.9186520829307770, 0.0134362814070940],
                 [-0.9186520829307770, 0.7559562487923320, -0.9186520829307770, 0.0134362814070940],
                 [-0.9186520829307770, -0.9186520829307770, 0.7559562487923320, 0.0134362814070940],
                 [-0.3553242197154490, -0.3553242197154490, -0.3553242197154490, 0.0738095753915400],
                 [-0.9340273408536530, -0.3553242197154490, -0.3553242197154490, 0.0738095753915400],
                 [-0.3553242197154490, -0.9340273408536530, -0.3553242197154490, 0.0738095753915400],
                 [-0.3553242197154490, -0.3553242197154490, -0.9340273408536530, 0.0738095753915400],
                 [-0.8726779962499650, -0.8726779962499650, -0.4606553370833680, 0.0642857142857140],
                 [-0.8726779962499650, -0.4606553370833680, -0.8726779962499650, 0.0642857142857140],
                 [-0.8726779962499650, -0.8726779962499650, 0.2060113295832980, 0.0642857142857140],
                 [-0.8726779962499650, 0.2060113295832980, -0.8726779962499650, 0.0642857142857140],
                 [-0.8726779962499650, -0.4606553370833680, 0.2060113295832980, 0.0642857142857140],
                 [-0.8726779962499650, 0.2060113295832980, -0.4606553370833680, 0.0642857142857140],
                 [-0.4606553370833680, -0.8726779962499650, -0.8726779962499650, 0.0642857142857140],
                 [-0.4606553370833680, -0.8726779962499650, 0.2060113295832980, 0.0642857142857140],
                 [-0.4606553370833680, 0.2060113295832980, -0.8726779962499650, 0.0642857142857140],
                 [0.2060113295832980, -0.8726779962499650, -0.4606553370833680, 0.0642857142857140],
                 [0.2060113295832980, -0.8726779962499650, -0.8726779962499650, 0.0642857142857140],
                 [0.2060113295832980, -0.4606553370833680, -0.8726779962499650, 0.0642857142857140]],
                bounds=(-1.0, 1.0), tp_fix=1.0/6.0),

    },

    '3_8' : {

        2 : _QP([[ 0.0             , nm.sqrt(2.0/3.0),-1/nm.sqrt(3), 2.0],
                 [ 0.0             ,-nm.sqrt(2.0/3.0),-1/nm.sqrt(3), 2.0],
                 [ nm.sqrt(2.0/3.0), 0.0             , 1/nm.sqrt(3), 2.0],
                 [-nm.sqrt(2.0/3.0), 0.0             , 1/nm.sqrt(3), 2.0]], bounds=(-1.0, 1.0)),

        3 : _QP([[-1.0, 0.0, 0.0, 4.0/3.0],
                 [ 1.0, 0.0, 0.0, 4.0/3.0],
                 [ 0.0,-1.0, 0.0, 4.0/3.0],
                 [ 0.0, 1.0, 0.0, 4.0/3.0],
                 [ 0.0, 0.0,-1.0, 4.0/3.0],
                 [ 0.0, 0.0, 1.0, 4.0/3.0]], bounds=(-1.0, 1.0)),

        5 : _QP([[-nm.sqrt(19.0/30.0), 0.0               , 0.0               , 320.0/361.0],
                 [ nm.sqrt(19.0/30.0), 0.0               , 0.0               , 320.0/361.0],
                 [ 0.0               ,-nm.sqrt(19.0/30.0), 0.0               , 320.0/361.0],
                 [ 0.0               , nm.sqrt(19.0/30.0), 0.0               , 320.0/361.0],
                 [ 0.0               , 0.0               ,-nm.sqrt(19.0/30.0), 320.0/361.0],
                 [ 0.0               , 0.0               , nm.sqrt(19.0/30.0), 320.0/361.0],
                 [ nm.sqrt(19.0/33.0), nm.sqrt(19.0/33.0), nm.sqrt(19.0/33.0), 121.0/361.0],
                 [ nm.sqrt(19.0/33.0), nm.sqrt(19.0/33.0),-nm.sqrt(19.0/33.0), 121.0/361.0],
                 [ nm.sqrt(19.0/33.0),-nm.sqrt(19.0/33.0), nm.sqrt(19.0/33.0), 121.0/361.0],
                 [ nm.sqrt(19.0/33.0),-nm.sqrt(19.0/33.0),-nm.sqrt(19.0/33.0), 121.0/361.0],
                 [-nm.sqrt(19.0/33.0), nm.sqrt(19.0/33.0), nm.sqrt(19.0/33.0), 121.0/361.0],
                 [-nm.sqrt(19.0/33.0), nm.sqrt(19.0/33.0),-nm.sqrt(19.0/33.0), 121.0/361.0],
                 [-nm.sqrt(19.0/33.0),-nm.sqrt(19.0/33.0), nm.sqrt(19.0/33.0), 121.0/361.0],
                 [-nm.sqrt(19.0/33.0),-nm.sqrt(19.0/33.0),-nm.sqrt(19.0/33.0), 121.0/361.0]], bounds=(-1.0, 1.0)),

    },
}
del _QP

def _get_max_orders():
    max_orders = {}
    for key, table in quadrature_tables.iteritems():
        orders = table.keys()
        max_orders[key] = max(orders)

    return max_orders

max_orders = _get_max_orders()

########NEW FILE########
__FILENAME__ = simplex_cubature
"""
Generate simplex quadrature points. Code taken and adapted from pytools/hedge
by Andreas Kloeckner.
"""
from __future__ import division

import numpy as nm

def generate_decreasing_nonnegative_tuples_summing_to(n, length, min=0,
                                                      max=None):
    if length == 0:
        yield ()
    elif length == 1:
        if n <= max:
            #print "MX", n, max
            yield (n,)
        else:
            return
    else:
        if max is None or n < max:
            max = n

        for i in range(min, max+1):
            #print "SIG", sig, i
            for remainder in generate_decreasing_nonnegative_tuples_summing_to(
                    n-i, length-1, min, i):
                yield (i,) + remainder

def generate_permutations(original):
    """
    Generate all permutations of the list `original'.

    Nicked from http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/252178
    """
    if len(original) <=1:
        yield original
    else:
        for perm in generate_permutations(original[1:]):
            for i in range(len(perm)+1):
                #nb str[0:1] works in both string and list contexts
                yield perm[:i] + original[0:1] + perm[i:]

def generate_unique_permutations(original):
    """
    Generate all unique permutations of the list `original'.
    """

    had_those = set()

    for perm in generate_permutations(original):
        if perm not in had_those:
            had_those.add(perm)
            yield perm

def wandering_element(length, wanderer=1, landscape=0):
    for i in range(length):
        yield i*(landscape,) + (wanderer,) + (length-1-i)*(landscape,)

def factorial(n):
    from operator import mul
    assert n == int(n)
    return reduce(mul, (i for i in xrange(1,n+1)), 1)

def _extended_euclidean(q, r):
    """
    Return a tuple (p, a, b) such that p = aq + br,
    where p is the greatest common divisor.
    """

    # see [Davenport], Appendix, p. 214

    if abs(q) < abs(r):
        p, a, b = _extended_euclidean(r, q)
        return p, b, a

    Q = 1, 0
    R = 0, 1

    while r:
        quot, t = divmod(q, r)
        T = Q[0] - quot*R[0], Q[1] - quot*R[1]
        q, r = r, t
        Q, R = R, T

    return q, Q[0], Q[1]

def _gcd(q, r):
    return _extended_euclidean(q, r)[0]

def _simplify_fraction((a, b)):
    gcd = _gcd(a,b)
    return (a//gcd, b//gcd)

def get_simplex_cubature(order, dimension):
    """
    Cubature on an M{n}-simplex.

    cf.
    A. Grundmann and H.M. Moeller,
    Invariant integration formulas for the n-simplex by combinatorial methods,
    SIAM J. Numer. Anal.  15 (1978), 282--290.

    This cubature rule has both negative and positive weights.  It is exact for
    polynomials up to order :math:`2s+1`, where :math:`s` is given as *order*.
    The integration domain is the unit simplex

    .. math::

        T_n := \{(x_1, \dots, x_n): x_i \ge -1, \sum_i x_i \le -1\}
    """
    s = order
    n = dimension
    d = exact_to = 2*s+1

    points_to_weights = {}

    for i in xrange(s+1):
        weight = (-1)**i * 2**(-2*s) \
                * (d + n-2*i)**d \
                / factorial(i) \
                / factorial(d+n-i)

        for t in generate_decreasing_nonnegative_tuples_summing_to(s-i, n+1):
            for beta in generate_unique_permutations(t):
                denominator = d+n-2*i
                point = tuple(
                        _simplify_fraction((2*beta_i+1, denominator))
                        for beta_i in beta)

                points_to_weights[point] = points_to_weights.get(point, 0) \
                                           + weight
    from operator import add

    vertices = [-1 * nm.ones((n,))] \
               + [nm.array(x)
                  for x in wandering_element(n, landscape=-1, wanderer=1)]

    pos_points = []
    pos_weights = []
    neg_points = []
    neg_weights = []

    dim_factor = 2**n
    for p, w in points_to_weights.iteritems():
        real_p = reduce(add, (a/b*v for (a,b),v in zip(p, vertices)))
        if w > 0:
            pos_points.append(real_p)
            pos_weights.append(dim_factor*w)
        else:
            neg_points.append(real_p)
            neg_weights.append(dim_factor*w)

    points = nm.array(pos_points + neg_points)
    weights = nm.array(pos_weights + neg_weights)

    return points, weights, exact_to

########NEW FILE########
__FILENAME__ = state
"""
Module for handling state variables.
"""
import numpy as nm

from sfepy.base.base import Struct

class State(Struct):
    """
    Class holding/manipulating the state variables and corresponding DOF
    vectors.

    Manipulating the state class changes the underlying variables, and
    hence also the corresponding equations/terms (if any).

    Notes
    -----
    This class allows working with LCBC conditions in time-dependent
    problems, as it keeps track of the reduced DOF vector that cannot
    be reconstructed from the full DOF vector by using the usual
    `variables.strip_state_vector()`.
    """

    @staticmethod
    def from_variables(variables):
        """
        Create a State instance for the given variables.

        The DOF vector is created using the DOF data in `variables`.

        Parameters
        ----------
        variables : Variables instance
            The variables.
        """
        parts = variables.get_state_parts()
        vec = variables.create_state_vector()

        for key, part in parts.iteritems():
            indx = variables.get_indx(key)
            vec[indx] = part

        return State(variables, vec)

    def __init__(self, variables, vec=None, preserve_caches=False):
        """
        Create a State instance for the given variables.

        Parameters
        ----------
        variables : Variables instance
            The variables.
        vec : array, optional
            The (initial) DOF vector corresponding to the variables.
        preserve_caches : bool
            If True, do not invalidate evaluate caches of variables.
        """
        Struct.__init__(self, variables=variables, vec=vec, r_vec=None)

        if self.vec is None:
            self.vec = variables.create_state_vector()

        self.variables.set_data(self.vec, preserve_caches=preserve_caches)

    def copy(self, deep=False, preserve_caches=False):
        """
        Copy the state. By default, the new state contains the same
        variables, and creates new DOF vectors. If `deep` is True, also
        the DOF vectors are copied.

        Parameters
        ----------
        deep : bool
            If True, make a copy of the DOF vectors.
        preserve_caches : bool
            If True, do not invalidate evaluate caches of variables.
        """
        if deep:
            other = State(self.variables, self.vec.copy(), preserve_caches=True)
            if self.r_vec is not None:
                other.r_vec = self.r_vec.copy()

        else:
            other = State(self.variables, preserve_caches=True)

        return other

    def fill(self, value):
        """
        Fill the DOF vector with given value.
        """
        if self.r_vec is not None:
            self.r_vec.fill(value)

        self.vec.fill(value)

    def init_history(self):
        """
        Initialize variables with history.
        """
        self.variables.init_history()

    def apply_ebc(self, force_values=None):
        """
        Apply essential (Dirichlet) boundary conditions to the state.
        """
        self.variables.apply_ebc(self.vec, force_values=force_values)

    def has_ebc(self):
        """
        Test whether the essential (Dirichlet) boundary conditions have
        been applied to the DOF vector.
        """
        return self.variables.has_ebc(self.vec)

    def apply_ic(self, force_values=None):
        """
        Apply initial conditions to the state.
        """
        if self.r_vec is not None:
            raise ValueError('cannot re-apply initial conditions with LCBCs!')

        self.variables.apply_ic(self.vec, force_values=force_values)

    def get_reduced(self, follow_epbc=False):
        """
        Get the reduced DOF vector, with EBC and PBC DOFs removed.
        """
        strip = self.variables.strip_state_vector

        if self.variables.has_lcbc:
            if self.r_vec is None:
                r_vec = strip(self.vec, follow_epbc=follow_epbc)
                r_vec = self.variables.op_lcbc.T * r_vec

            else:
                r_vec = self.r_vec

        else:
            r_vec = strip(self.vec, follow_epbc=follow_epbc)

        return r_vec

    def set_reduced(self, r_vec, preserve_caches=False):
        """
        Set the reduced DOF vector, with EBC and PBC DOFs removed.

        Parameters
        ----------
        r_vec : array
            The reduced DOF vector corresponding to the variables.
        preserve_caches : bool
            If True, do not invalidate evaluate caches of variables.
        """
        self.vec = self.variables.make_full_vec(r_vec)

        if self.variables.has_lcbc:
            self.r_vec = r_vec

        self.variables.set_data(self.vec, preserve_caches=preserve_caches)

    def set_full(self, vec, var_name=None, force=False):
        """
        Set the full DOF vector (including EBC and PBC DOFs). If
        `var_name` is given, set only the DOF sub-vector corresponding
        to the given variable. If `force` is True, setting variables
        with LCBC DOFs is allowed.
        """
        if var_name is None:
            if self.variables.has_lcbc and not force:
                raise ValueError('cannot set full DOF vector with LCBCs!')

            self.vec = vec
            self.variables.set_data(self.vec)

        else:
            var = self.variables[var_name]

            if var.has_lcbc and not force:
                raise ValueError('cannot set full DOF vector with LCBCs!')

            self.variables.set_state_part(self.vec, vec, var_name)
            var.set_data(self.vec, self.variables.get_indx(var_name))

    def __call__(self, var_name=None):
        """
        Get the full DOF vector (including EBC and PBC DOFs). If
        `var_name` is given, return only the DOF vector corresponding to
        the given variable.
        """
        if var_name is None:
            out = self.vec

        else:
            out = self.variables.get_state_part_view(self.vec, var_name)

        return out

    def set_parts(self, parts, force=False):
        """
        Set parts of the DOF vector corresponding to individual state
        variables.

        Parameters
        ----------
        parts : dict
            The dictionary of the DOF vector parts.
        """
        if self.variables.has_lcbc and not force:
            raise ValueError('cannot set full DOF vector with LCBCs!')

        self.variables.set_data(parts)
        for key, part in parts.iteritems():
            indx = self.variables.get_indx(key)
            self.vec[indx] = part

    def get_parts(self):
        """
        Return parts of the DOF vector corresponding to individual state
        variables.

        Returns
        -------
        out : dict
            The dictionary of the DOF vector parts.
        """
        return self.variables.get_state_parts(self.vec)

    def create_output_dict(self, fill_value=None, var_info=None,
                           extend=True, linearization=None):
        """
        Transforms state to an output dictionary, that can be
        passed as 'out' kwarg to Mesh.write().

        Then the dictionary entries are formed by components of the
        state vector corresponding to unknown variables according to
        kind of linearization given by `linearization`.

        Examples
        --------
        >>> out = state.create_output_dict()
        >>> problem.save_state('file.vtk', out=out)
        """
        return self.variables.state_to_output(self.vec, fill_value,
                                              var_info, extend,
                                              linearization=linearization)

    def get_weighted_norm(self, vec, weights=None, return_weights=False):
        """
        Return the weighted norm of DOF vector `vec`.

        By default, each component of `vec` is weighted by the 1/norm of the
        corresponding state part, or 1 if the norm is zero. Alternatively, the
        weights can be provided explicitly using `weights` argument.

        Parameters
        ----------
        vec : array
            The DOF vector corresponding to the variables.
        weights : dict, optional
            If given, the weights are used instead of the norms of the state
            parts. Keys of the dictionary must be equal to the names of
            variables comprising the DOF vector.
        return_weights: bool
            If True, return also the used weights.

        Returns
        -------
        norm : float
            The weighted norm.
        weights : dict, optional
            If `return_weights` is True, the used weights.

        Examples
        --------
        >>> err = state0.get_weighted_norm(state() - state0())
        """
        if weights is None:
            parts = self.get_parts()

            weights = {}
            for key, part in parts.iteritems():
                pnorm = nm.linalg.norm(part)
                if pnorm < 10.0 * nm.finfo(nm.float64).eps:
                    pnorm = 1.0
                weights[key] = 1.0 / pnorm

        else:
            if set(weights.keys()) != self.variables.state:
                raise ValueError('weights keys have to be in %s!'
                                 % self.variables.state)

        wvec = vec.copy()
        for key in weights.iterkeys():
            indx = self.variables.get_indx(key)
            wvec[indx] *= weights[key]

        norm = nm.linalg.norm(wvec)

        if return_weights:
            return norm, weights

        else:
            return norm

########NEW FILE########
__FILENAME__ = variables
"""
Classes of variables for equations/terms.
"""
import time
from collections import deque

import numpy as nm

from sfepy.base.base import (real_types, complex_types, assert_, get_default,
                             output, OneTypeList, Container, Struct, basestr,
                             iter_dict_of_lists)
import sfepy.linalg as la
from sfepy.discrete.functions import Function
from sfepy.discrete.conditions import get_condition_value
from sfepy.discrete.integrals import Integral
from sfepy.discrete.common.dof_info import (DofInfo, EquationMap,
                                            expand_nodes_to_equations,
                                            is_active_bc)
from sfepy.discrete.fem.lcbc_operators import (LCBCOperators,
                                               make_global_lcbc_operator)
from sfepy.discrete.common.mappings import get_physical_qps
from sfepy.discrete.evaluate_variable import eval_real, eval_complex

is_state = 0
is_virtual = 1
is_parameter = 2
is_field = 10

def create_adof_conns(conn_info, var_indx=None, verbose=True):
    """
    Create active DOF connectivities for all variables referenced in
    `conn_info`.

    If a variable has not the equation mapping, a trivial mapping is assumed
    and connectivity with all DOFs active is created.

    DOF connectivity key is a tuple ``(primary variable name, region name,
    type, ig, is_trace flag)``.
    """
    var_indx = get_default(var_indx, {})

    def _create(var, econn):
        if var.eq_map is None:
            eq = nm.arange(var.n_dof, dtype=nm.int32)

        else:
            eq = var.eq_map.eq

        offset = var_indx.get(var.name, slice(0, 0)).start
        adc = create_adof_conn(eq, econn, var.n_components, offset)

        return adc

    def _iter_igs(adof_conns, info, region, var, field, is_trace):
        for ig in region.igs:
            key = (var.name, region.name, info.dc_type.type, ig, is_trace)
            if not key in adof_conns:
                econn = field.get_econn(info.dc_type, region,
                                        ig, is_trace=is_trace)
                if econn is None: continue

                adof_conns[key] = _create(var, econn)

            if info.is_trace:
                key = (var.name, region.name, info.dc_type.type, ig, False)
                if not key in adof_conns:
                    econn = field.get_econn(info.dc_type, region,
                                            ig, is_trace=False)

                    adof_conns[key] = _create(var, econn)

    if verbose:
        output('setting up dof connectivities...')
        tt = time.clock()

    adof_conns = {}

    for key, ii, info in iter_dict_of_lists(conn_info, return_keys=True):
        if info.primary is not None:
            var = info.primary
            field = var.get_field()
            field.setup_extra_data(info.ps_tg, info, info.is_trace)

            region = info.get_region()
            _iter_igs(adof_conns, info, region, var, field, info.is_trace)

        if info.has_virtual and not info.is_trace:
            var = info.virtual
            field = var.get_field()
            field.setup_extra_data(info.v_tg, info, False)

            aux = var.get_primary()
            var = aux if aux is not None else var

            region = info.get_region(can_trace=False)
            _iter_igs(adof_conns, info, region, var, field, False)

    if verbose:
        output('...done in %.2f s' % (time.clock() - tt))

    return adof_conns

def create_adof_conn(eq, conn, dpn, offset):
    """
    Given a node connectivity, number of DOFs per node and equation mapping,
    create the active dof connectivity.

    Locally (in a connectivity row), the DOFs are stored DOF-by-DOF (u_0 in all
    local nodes, u_1 in all local nodes, ...).

    Globally (in a state vector), the DOFs are stored node-by-node (u_0, u_1,
    ..., u_X in node 0, u_0, u_1, ..., u_X in node 1, ...).
    """
    if dpn == 1:
        aux = nm.take(eq, conn)
        adc = aux + nm.asarray(offset * (aux >= 0), dtype=nm.int32)

    else:
        n_el, n_ep = conn.shape
        adc = nm.empty((n_el, n_ep * dpn), dtype=conn.dtype)
        ii = 0
        for idof in xrange(dpn):
            aux = nm.take(eq, dpn * conn + idof)
            adc[:, ii : ii + n_ep] = aux + nm.asarray(offset * (aux >= 0),
                                                      dtype=nm.int32)
            ii += n_ep

    return adc

class Variables(Container):
    """
    Container holding instances of Variable.
    """

    @staticmethod
    def from_conf(conf, fields):
        """
        This method resets the variable counters for automatic order!
        """
        Variable.reset()

        obj = Variables()
        for key, val in conf.iteritems():
            var = Variable.from_conf(key, val, fields)

            obj[var.name] = var

        obj.setup_dtype()
        obj.setup_ordering()

        return obj

    def __init__(self, variables=None):
        Container.__init__(self, OneTypeList(Variable),
                           state=set(),
                           virtual=set(),
                           parameter=set(),
                           has_virtual_dcs=False,
                           has_lcbc=False,
                           has_eq_map=False,
                           ordered_state=[],
                           ordered_virtual=[])

        if variables is not None:
            for var in variables:
                self[var.name] = var

            self.setup_ordering()

        self.setup_dtype()

        self.adof_conns = {}

    def __setitem__(self, ii, var):
        Container.__setitem__(self, ii, var)

        if var.is_state():
            self.state.add(var.name)

        elif var.is_virtual():
            self.virtual.add(var.name)

        elif var.is_parameter():
            self.parameter.add(var.name)

        var._variables = self

        self.setup_ordering()
        self.setup_dof_info()

    def setup_dtype(self):
        """
        Setup data types of state variables - all have to be of the same
        data type, one of nm.float64 or nm.complex128.
        """
        dtypes = {nm.complex128 : 0, nm.float64 : 0}
        for var in self.iter_state(ordered=False):
            dtypes[var.dtype] += 1

        if dtypes[nm.float64] and dtypes[nm.complex128]:
            raise ValueError("All variables must have the same dtype!")

        elif dtypes[nm.float64]:
            self.dtype = nm.float64

        elif dtypes[nm.complex128]:
            self.dtype = nm.complex128

        else:
            self.dtype = None

    def link_duals(self):
        """
        Link state variables with corresponding virtual variables,
        and assign link to self to each variable instance.

        Usually, when solving a PDE in the weak form, each state
        variable has a corresponding virtual variable.
        """
        for ii in self.state:
            self[ii].dual_var_name = None

        for ii in self.virtual:
            vvar = self[ii]
            try:
                self[vvar.primary_var_name].dual_var_name = vvar.name
            except IndexError:
                pass

    def get_dual_names(self):
        """
        Get names of pairs of dual variables.

        Returns
        -------
        duals : dict
           The dual names as virtual name : state name pairs.
        """
        duals = {}
        for name in self.virtual:
            duals[name] = self[name].primary_var_name

        return duals

    def setup_ordering(self):
        """
        Setup ordering of variables.
        """
        self.link_duals()

        orders = []
        for var in self:
            try:
                orders.append(var._order)
            except:
                pass
        orders.sort()

        self.ordered_state = [None] * len(self.state)
        for var in self.iter_state(ordered=False):
            ii = orders.index(var._order)
            self.ordered_state[ii] = var.name

        self.ordered_virtual = [None] * len(self.virtual)
        ii = 0
        for var in self.iter_state(ordered=False):
            if var.dual_var_name is not None:
                self.ordered_virtual[ii] = var.dual_var_name
                ii += 1

    def has_virtuals(self):
        return len(self.virtual) > 0

    def setup_dof_info(self, make_virtual=False):
        """
        Setup global DOF information.
        """
        self.di = DofInfo('state_dof_info')
        for var_name in self.ordered_state:
            self.di.append_variable(self[var_name])

        if make_virtual:
            self.vdi = DofInfo('virtual_dof_info')
            for var_name in self.ordered_virtual:
                self.vdi.append_variable(self[var_name])

        else:
            self.vdi = self.di

    def setup_lcbc_operators(self, lcbcs, ts=None, functions=None):
        """
        Prepare linear combination BC operator matrix.
        """
        if lcbcs is None:
            self.lcdi = self.adi
            return

        self.lcbcs = lcbcs
        lcbc_of_vars = self.lcbcs.group_by_variables()

        # Assume disjoint regions.
        lcbc_ops = {}
        offset = 0
        for var_name, bcs in lcbc_of_vars.iteritems():
            var = self[var_name]

            lcbc_op = var.create_lcbc_operators(bcs, offset,
                                                ts=ts, functions=functions)
            lcbc_ops[var_name] = lcbc_op

            if lcbc_op is not None:
                offset += lcbc_op.n_op

        self.op_lcbc, self.lcdi = make_global_lcbc_operator(lcbc_ops, self.adi)

        self.has_lcbc = self.op_lcbc is not None

    def get_lcbc_operator(self):
        if self.has_lcbc:
            return self.op_lcbc
        else:
            raise ValueError('no LCBC defined!')

    def equation_mapping(self, ebcs, epbcs, ts, functions, problem=None):
        """
        Create the mapping of active DOFs from/to all DOFs for all state
        variables.

        Returns
        -------
        active_bcs : set
            The set of boundary conditions active in the current time.
        """
        self.ebcs = ebcs
        self.epbcs = epbcs

        ##
        # Assing EBC, PBC to variables and regions.
        if ebcs is not None:
            self.bc_of_vars = self.ebcs.group_by_variables()

        else:
            self.bc_of_vars = {}

        if epbcs is not None:
            self.bc_of_vars = self.epbcs.group_by_variables(self.bc_of_vars)

        ##
        # List EBC nodes/dofs for each variable.
        active_bcs = set()
        for var_name in self.di.var_names:
            var = self[var_name]
            bcs = self.bc_of_vars.get(var.name, None)

            var_di = self.di.get_info(var_name)
            active = var.equation_mapping(bcs, var_di, ts, functions,
                                          problem=problem)
            active_bcs.update(active)

            if self.has_virtual_dcs:
                vvar = self[var.dual_var_name]
                vvar_di = self.vdi.get_info(var_name)
                active = vvar.equation_mapping(bcs, vvar_di, ts, functions,
                                               problem=problem)
                active_bcs.update(active)

        self.adi = DofInfo('active_state_dof_info')
        for var_name in self.ordered_state:
            self.adi.append_variable(self[var_name], active=True)

        if self.has_virtual_dcs:
            self.avdi = DofInfo('active_virtual_dof_info')
            for var_name in self.ordered_virtual:
                self.avdi.append_variable(self[var_name], active=True)

        else:
            self.avdi = self.adi

        self.has_eq_map = True

        return active_bcs

    def get_matrix_shape(self):
        if not self.has_eq_map:
            raise ValueError('call equation_mapping() first!')

        return (self.avdi.ptr[-1], self.adi.ptr[-1])

    def setup_initial_conditions(self, ics, functions):
        self.ics = ics
        self.ic_of_vars = self.ics.group_by_variables()

        for var_name in self.di.var_names:
            var = self[var_name]

            ics = self.ic_of_vars.get(var.name, None)
            if ics is None: continue

            var.setup_initial_conditions(ics, self.di, functions)

    def set_adof_conns(self, adof_conns):
        """
        Set all active DOF connectivities to `self` as well as relevant
        sub-dicts to the individual variables.
        """
        self.adof_conns = adof_conns

        for var in self:
            var.adof_conns = {}

        for key, val in adof_conns.iteritems():
            if key[0] in self.names:
                var = self[key[0]]
                var.adof_conns[key] = val

                var = var.get_dual()
                if var is not None:
                    var.adof_conns[key] = val

    def create_state_vector(self):
        vec = nm.zeros((self.di.ptr[-1],), dtype=self.dtype)
        return vec

    def create_stripped_state_vector(self):
        vec = nm.zeros((self.adi.ptr[-1],), dtype=self.dtype)
        return vec

    def apply_ebc(self, vec, force_values=None):
        """
        Apply essential (Dirichlet) and periodic boundary conditions
        defined for the state variables to vector `vec`.
        """
        for var in self.iter_state():
            var.apply_ebc(vec, self.di.indx[var.name].start, force_values)

    def apply_ic(self, vec, force_values=None):
        """
        Apply initial conditions defined for the state variables to
        vector `vec`.
        """
        for var in self.iter_state():
            var.apply_ic(vec, self.di.indx[var.name].start, force_values)

    def strip_state_vector(self, vec, follow_epbc=False):
        """
        Get the reduced DOF vector, with EBC and PBC DOFs removed.

        Notes
        -----
        If 'follow_epbc' is True, values of EPBC master dofs are not simply
        thrown away, but added to the corresponding slave dofs, just like when
        assembling. For vectors with state (unknown) variables it should be set
        to False, for assembled vectors it should be set to True.
        """
        svec = nm.empty((self.adi.ptr[-1],), dtype=self.dtype)
        for var in self.iter_state():
            aindx = self.adi.indx[var.name]
            svec[aindx] = var.get_reduced(vec, self.di.indx[var.name].start,
                                          follow_epbc)
        return svec

    def make_full_vec(self, svec, force_value=None):
        """
        Make a full DOF vector satisfying E(P)BCs from a reduced DOF
        vector.

        Passing a `force_value` overrides the EBC values.
        """
        self.check_vector_size(svec, stripped=True)

        if self.has_lcbc:
            svec = self.op_lcbc * svec

        vec = self.create_state_vector()
        for var in self.iter_state():
            indx = self.di.indx[var.name]
            aindx = self.adi.indx[var.name]
            var.get_full(svec, aindx.start, force_value, vec, indx.start)

        return vec

    def has_ebc(self, vec, force_values=None):
        for var_name in self.di.var_names:
            eq_map = self[var_name].eq_map
            i0 = self.di.indx[var_name].start
            ii = i0 + eq_map.eq_ebc
            if force_values is None:
                if not nm.allclose(vec[ii], eq_map.val_ebc):
                    return False
            else:
                if isinstance(force_values, dict):
                    if not nm.allclose(vec[ii], force_values[var_name]):
                        return False
                else:
                    if not nm.allclose(vec[ii], force_values):
                        return False
            # EPBC.
            if not nm.allclose(vec[i0+eq_map.master], vec[i0+eq_map.slave]):
                return False
        return True

    def get_indx(self, var_name, stripped=False, allow_dual=False):
        var = self[var_name]

        if not var.is_state():
            if allow_dual and var.is_virtual():
                var_name = var.primary_var_name
            else:
                msg = '%s is not a state part' % var_name
                raise IndexError(msg)

        if stripped:
            return self.adi.indx[var_name]
        else:
            return self.di.indx[var_name]

    def check_vector_size(self, vec, stripped=False):
        """
        Check whether the shape of the DOF vector corresponds to the
        total number of DOFs of the state variables.

        Parameters
        ----------
        vec : array
            The vector of DOF values.
        stripped : bool
            If True, the size of the DOF vector should be reduced,
            i.e. without DOFs fixed by boundary conditions.
        """
        if not stripped:
            n_dof = self.di.get_n_dof_total()

            if vec.size != n_dof:
                msg = 'incompatible data size!' \
                      ' (%d (variables) == %d (DOF vector))' \
                      % (n_dof, vec.size)
                raise ValueError(msg)

        else:
            if self.has_lcbc:
                n_dof = self.lcdi.get_n_dof_total()

            else:
                n_dof = self.adi.get_n_dof_total()

            if vec.size != n_dof:
                msg = 'incompatible data size!' \
                      ' (%d (active variables) == %d (reduced DOF vector))' \
                      % (n_dof, vec.size)
                raise ValueError(msg)

    def get_state_part_view(self, state, var_name, stripped=False):
        self.check_vector_size(state, stripped=stripped)
        return state[self.get_indx(var_name, stripped)]

    def set_state_part(self, state, part, var_name, stripped=False):
        self.check_vector_size(state, stripped=stripped)
        state[self.get_indx(var_name, stripped)] = part

    def get_state_parts(self, vec=None):
        """
        Return parts of a state vector corresponding to individual state
        variables.

        Parameters
        ----------
        vec : array, optional
            The state vector. If not given, then the data stored in the
            variables are returned instead.

        Returns
        -------
        out : dict
            The dictionary of the state parts.
        """
        if vec is not None:
            self.check_vector_size(vec)

        out = {}
        for var in self.iter_state():
            if vec is None:
                out[var.name] = var()

            else:
                out[var.name] = vec[self.di.indx[var.name]]

        return out

    def set_data(self, data, step=0, ignore_unknown=False,
                 preserve_caches=False):
        """
        Set data (vectors of DOF values) of variables.

        Parameters
        ----------
        data : array
            The state vector or dictionary of {variable_name : data vector}.
        step : int, optional
            The time history step, 0 (default) = current.
        ignore_unknown : bool, optional
            Ignore unknown variable names if `data` is a dict.
        preserve_caches : bool
            If True, do not invalidate evaluate caches of variables.
        """
        if data is None: return

        if isinstance(data, dict):

            for key, val in data.iteritems():
                try:
                    var = self[key]

                except (ValueError, IndexError):
                    if ignore_unknown:
                        pass

                    else:
                        raise KeyError('unknown variable! (%s)' % key)

                else:
                    var.set_data(val, step=step,
                                 preserve_caches=preserve_caches)

        elif isinstance(data, nm.ndarray):
            self.check_vector_size(data)

            for ii in self.state:
                var = self[ii]
                var.set_data(data, self.di.indx[var.name],
                             preserve_caches=preserve_caches)

        else:
            raise ValueError('unknown data class! (%s)' % data.__class__)

    def set_data_from_state(self, var_names, state, var_names_state):
        """
        Set variables with names in `var_names` from state variables with names
        in `var_names_state` using DOF values in the state vector `state`.
        """
        self.check_vector_size(state)

        if isinstance(var_names, basestr):
            var_names = [var_names]
            var_names_state = [var_names_state]

        for ii, var_name in enumerate(var_names):
            var_name_state = var_names_state[ii]

            if self[var_name_state].is_state():
                self[var_name].set_data(state, self.di.indx[var_name_state])

            else:
                msg = '%s is not a state part' % var_name_state
                raise IndexError(msg)

    def state_to_output(self, vec, fill_value=None, var_info=None,
                        extend=True, linearization=None):
        """
        Convert a state vector to a dictionary of output data usable by
        Mesh.write().
        """
        di = self.di

        if var_info is None:
            self.check_vector_size(vec)

            var_info = {}
            for name in di.var_names:
                var_info[name] = (False, name)

        out = {}
        for key, indx in di.indx.iteritems():
            var = self[key]

            if key not in var_info.keys(): continue
            is_part, name = var_info[key]

            if is_part:
                aux = vec

            else:
                aux = vec[indx]

            out.update(var.create_output(aux, extend=extend,
                                         fill_value=fill_value,
                                         linearization=linearization))

        return out

    def iter_state(self, ordered=True):

        if ordered:
            for ii in self.ordered_state:
                yield self[ii]

        else:
            for ii in self.state:
                yield self[ii]

    def init_history(self):
        for var in self.iter_state():
            var.init_history()

    def time_update(self, ts, functions, verbose=True):
        if verbose:
            output('updating variables...')

        for var in self:
            var.time_update(ts, functions)

        if verbose:
            output('...done')

    def advance(self, ts):
        for var in self.iter_state():
            var.advance(ts)

class Variable(Struct):
    _count = 0
    _orders = []
    _all_var_names = set()

    @staticmethod
    def reset():
        Variable._count = 0
        Variable._orders = []
        Variable._all_var_names = set()

    @staticmethod
    def from_conf(key, conf, fields):
        aux = conf.kind.split()
        if len(aux) == 2:
            kind, family = aux

        elif len(aux) == 3:
            kind, family = aux[0], '_'.join(aux[1:])

        else:
            raise ValueError('variable kind is 2 or 3 words! (%s)' % conf.kind)

        history = conf.get('history', None)
        if history is not None:
            try:
                history = int(history)
                assert_(history >= 0)

            except (ValueError, TypeError):
                raise ValueError('history must be integer >= 0! (got "%s")'
                                 % history)

        order = conf.get('order', None)
        if order is not None:
            order = int(order)

        primary_var_name = conf.get('dual', None)
        if primary_var_name is None:
            if hasattr(conf, 'like'):
                primary_var_name = get_default(conf.like, '(set-to-None)')

            else:
                primary_var_name = None

        special = conf.get('special', None)

        if family == 'field':
            try:
                fld = fields[conf.field]
            except IndexError:
                msg = 'field "%s" does not exist!' % conf.field
                raise KeyError(msg)

            obj = FieldVariable(conf.name, kind, fld, order, primary_var_name,
                                special=special, key=key, history=history)

        else:
            raise ValueError('unknown variable family! (%s)' % family)

        return obj

    def __init__(self, name, kind, order=None, primary_var_name=None,
                 special=None, flags=None, **kwargs):
        Struct.__init__(self, name=name, **kwargs)

        self.flags = set()
        if flags is not None:
            for flag in flags:
                self.flags.add(flag)

        self.indx = slice(None)
        self.n_dof = None
        self.step = 0
        self.dt = 1.0
        self.initial_condition = None
        self.dual_var_name = None
        self.eq_map = None

        if self.is_virtual():
            self.data = None

        else:
            self.data = deque()
            self.data.append(None)

        self._set_kind(kind, order, primary_var_name, special=special)
        Variable._all_var_names.add(name)

    def _set_kind(self, kind, order, primary_var_name, special=None):
        if kind == 'unknown':
            self.flags.add(is_state)
            if order is not None:
                if order in Variable._orders:
                    raise ValueError('order %d already used!' % order)
                else:
                    self._order = order
                    Variable._orders.append(order)

            else:
                self._order = Variable._count
                Variable._orders.append(self._order)
            Variable._count += 1

            self.dof_name = self.name

        elif kind == 'test':
            if primary_var_name == self.name:
                raise ValueError('primary variable for %s cannot be %s!'
                                 % (self.name, primary_var_name))

            self.flags.add(is_virtual)
            msg = 'test variable %s: related unknown missing' % self.name
            self.primary_var_name = get_default(primary_var_name, None, msg)
            self.dof_name = self.primary_var_name

        elif kind == 'parameter':
            self.flags.add(is_parameter)
            msg = 'parameter variable %s: related unknown missing' % self.name
            self.primary_var_name = get_default(primary_var_name, None, msg)
            if self.primary_var_name == '(set-to-None)':
                self.primary_var_name = None
            self.dof_name = self.primary_var_name

            if special is not None:
                self.special = special

        else:
            raise NotImplementedError('unknown variable kind: %s' % kind)

        self.kind = kind

    def _setup_dofs(self, n_nod, n_components, val_shape):
        """
        Setup number of DOFs and  DOF names.
        """
        self.n_nod = n_nod
        self.n_components = n_components
        self.val_shape = val_shape

        self.n_dof = self.n_nod * self.n_components

        if self.dof_name is None:
            dof_name = 'aux'
        else:
            dof_name = self.dof_name
        self.dofs = [dof_name + ('.%d' % ii) for ii in range(self.n_components)]

    def get_primary(self):
        """
        Get the corresponding primary variable.

        Returns
        -------
        var : Variable instance
            The primary variable, or `self` for state
            variables or if `primary_var_name` is None, or None if no other
            variables are defined.
        """
        if self.is_state():
            var = self

        elif self.primary_var_name is not None:
            if ((self._variables is not None)
                and (self.primary_var_name in self._variables.names)):
                var = self._variables[self.primary_var_name]

            else:
                var = None

        else:
            var = self

        return var

    def get_dual(self):
        """
        Get the dual variable.

        Returns
        -------
        var : Variable instance
            The primary variable for non-state variables, or the dual
            variable for state variables.
        """
        if self.is_state():
            if ((self._variables is not None)
                and (self.dual_var_name in self._variables.names)):
                var = self._variables[self.dual_var_name]

            else:
                var = None

        else:
            if ((self._variables is not None)
                and (self.primary_var_name in self._variables.names)):
                var = self._variables[self.primary_var_name]

            else:
                var = None

        return var

    def is_state(self):
        return is_state in self.flags

    def is_virtual(self):
        return is_virtual in self.flags

    def is_parameter(self):
        return is_parameter in self.flags

    def is_state_or_parameter(self):
        return (is_state in self.flags) or (is_parameter in self.flags)

    def is_kind(self, kind):
        return eval('self.is_%s()' % kind)

    def is_real(self):
        return self.dtype in real_types

    def is_complex(self):
        return self.dtype in complex_types

    def is_finite(self, step=0, derivative=None, dt=None):
        return nm.isfinite(self(step=step, derivative=derivative, dt=dt)).all()

    def get_primary_name(self):
        if self.is_state():
            name = self.name

        else:
            name = self.primary_var_name

        return name

    def init_history(self):
        """Initialize data of variables with history."""
        if self.history is None: return

        self.data = deque((self.history + 1) * [None])
        self.step = 0

    def time_update(self, ts, functions):
        """Implemented in subclasses."""
        pass

    def advance(self, ts):
        """
        Advance in time the DOF state history. A copy of the DOF vector
        is made to prevent history modification.
        """
        if self.history is None: return

        self.step = ts.step + 1
        self.data.rotate()

        if self.history > 0:
            # Advance evaluate cache.
            for step_cache in self.evaluate_cache.itervalues():
                steps = sorted(step_cache.keys())
                for step in steps:
                    if step is None:
                        # Special caches with possible custom advance()
                        # function.
                        for key, val in step_cache[step].iteritems():
                            if hasattr(val, '__advance__'):
                                val.__advance__(ts, val)

                    elif -step < self.history:
                        step_cache[step-1] = step_cache[step]

                if len(steps) and (steps[0] is not None):
                    step_cache.pop(steps[-1])

    def init_data(self, step=0):
        """
        Initialize the dof vector data of time step `step` to zeros.
        """
        if self.is_state_or_parameter():
            data = nm.zeros((self.n_dof,), dtype=self.dtype)
            self.set_data(data, step=step)

    def set_constant(self, val):
        """
        Set the variable to a constant value.
        """
        data = nm.empty((self.n_dof,), dtype=self.dtype)
        data.fill(val)
        self.set_data(data)

    def set_data(self, data=None, indx=None, step=0,
                 preserve_caches=False):
        """
        Set data (vector of DOF values) of the variable.

        Parameters
        ----------
        data : array
            The vector of DOF values.
        indx : int, optional
            If given, `data[indx]` is used.
        step : int, optional
            The time history step, 0 (default) = current.
        preserve_caches : bool
            If True, do not invalidate evaluate caches of the variable.
        """
        data = data.ravel()

        if indx is None:
            indx = slice(0, len(data))
        else:
            indx = slice(int(indx.start), int(indx.stop))
        n_data_dof = indx.stop - indx.start

        if self.n_dof != n_data_dof:
            msg = 'incompatible data shape! (%d (variable) == %d (data))' \
                  % (self.n_dof, n_data_dof)
            raise ValueError(msg)

        else:
            self.data[step] = data
            self.indx = indx

        if not preserve_caches:
            self.invalidate_evaluate_cache(step=step)

    def __call__(self, step=0, derivative=None, dt=None):
        """
        Return vector of degrees of freedom of the variable.

        Parameters
        ----------
        step : int, default 0
            The time step (0 means current, -1 previous, ...).
        derivative : None or 'dt'
            If not None, return time derivative of the DOF vector,
            approximated by the backward finite difference.

        Returns
        -------
        vec : array
            The DOF vector. If `derivative` is None: a view of the data vector,
             otherwise: required derivative of the DOF vector
             at time step given by `step`.

        Notes
        -----
        If the previous time step is requested in step 0, the step 0
        DOF vector is returned instead.
        """
        if derivative is None:
            if (self.step == 0) and (step == -1):
                data = self.data[0]

            else:
                data = self.data[-step]

            if data is None:
                raise ValueError('data of variable are not set! (%s, step %d)' \
                                 % (self.name, step))

            return data[self.indx]

        else:
            if self.history is None:
                msg = 'set history type of variable %s to use derivatives!'\
                      % self.name
                raise ValueError(msg)
            dt = get_default(dt, self.dt)

            return (self(step=step) - self(step=step-1)) / dt

    def get_initial_condition(self):
        if self.initial_condition is None:
            return 0.0
        else:
            return self.initial_condition

class CloseNodesIterator(Struct):

    def __init__(self, field, create_mesh=True, create_graph=True,
                 strategy=None):
        self.field = field
        self.coors = self.field.get_coor()

        if create_mesh or create_graph:
            self.mesh = self.field.create_mesh()

        if create_graph:
            self.graph = self.mesh.create_conn_graph()
            self.perm = self.get_permutation(strategy=strategy)
            self.strategy = strategy

        else:
            self.graph = None
            self.strategy = None

    def __call__(self, strategy=None):
        if strategy is None or (strategy != self.strategy):
            self.perm = self.get_permutation(strategy=strategy)
            self.strategy = strategy

        self.ii = 0
        return self

    def get_permutation(self, strategy=None):
        graph = self.graph

        n_nod = self.coors.shape[0]
        dtype = nm.int32

        if strategy is None:
            perm = nm.arange(n_nod, dtype=dtype)

        elif strategy == 'rcm':
            from sfepy.linalg import rcm
            perm = rcm(graph)

        elif 'greedy' in strategy:
            ipop, iin = {'00' : (0, 0),
                         'e0' : (-1, 0),
                         '0e' : (0, -1),
                         'ee' : (-1, -1),
                         '01' : (0, 1),
                         }[strategy[-2:]]

            perm_i = nm.empty((n_nod,), dtype=dtype)
            perm_i.fill(-1)

            n_nod = perm_i.shape[0]
            num = graph.indptr[1:] - graph.indptr[:-1]

            ir = nm.argmin(num)
            perm_i[ir] = 0
            active = [ir]
            ii = 1
            while ii < n_nod:
                ir = active.pop(ipop)
                row = graph.indices[graph.indptr[ir]:graph.indptr[ir+1]]

                ips = []
                for ip in row:
                    if perm_i[ip] < 0:
                        perm_i[ip] = ii
                        ii += 1
                        ips.append(ip)
                if iin >= 0:
                    active[iin:iin] = ips
                else:
                    active.extend(ips)

            perm = nm.empty_like(perm_i)
            perm[perm_i] = nm.arange(perm_i.shape[0], dtype=perm.dtype)

        return perm

    def test_permutations(self, strategy='rcm'):
        from sfepy.linalg import permute_in_place, save_sparse_txt

        save_sparse_txt('graph', self.graph, fmt='%d %d %d\n')
        graph = self.graph.copy()

        perm = self.get_permutation('rcm')

        g_types = ['00', 'e0', '0e', 'ee', '01']
        g_names = ['greedy_%s' % ii for ii in g_types]
        g_perms = [self.get_permutation('greedy_%s' % ii) for ii in g_types]

        c1 = self.mesh.coors
        d1 = la.norm_l2_along_axis(c1[1:] - c1[:-1])
        d2 = la.norm_l2_along_axis(c1[perm][1:] - c1[perm][:-1])
        print d1.min(), d1.mean(), d1.max(), d1.std(), d1.var()
        print d2.min(), d2.mean(), d2.max(), d2.std(), d2.var()
        ds = []
        for g_perm in g_perms:
            d3 = la.norm_l2_along_axis(c1[g_perm][1:] - c1[g_perm][:-1])
            ds.append(d3)
            print d3.min(), d3.mean(), d3.max(), d3.std(), d3.var()

        permute_in_place(graph, perm)
        save_sparse_txt('graph_rcm', graph, fmt='%d %d %d\n')

        for ii, g_name in enumerate(g_names):
            graph = self.graph.copy()
            permute_in_place(graph, g_perms[ii])
            save_sparse_txt('graph_%s' % g_name, graph, fmt='%d %d %d\n')

        from matplotlib import pyplot as plt
        n_bins = 30
        plt.figure()
        plt.subplot(311)
        _, bins, ps = plt.hist(d1, n_bins, histtype='bar')
        plt.legend(ps[0:1], ['default'])
        plt.subplot(312)
        plt.hist(d2, bins, histtype='bar')
        plt.legend(ps[0:1], ['RCM'])
        plt.subplot(313)
        _, _, ps = plt.hist(nm.array(ds).T, bins, histtype='bar')
        plt.legend([ii[0] for ii in ps], g_names)
        plt.savefig('hist_distances_sub.pdf', transparent=True)

        plt.figure()
        _, _, ps = plt.hist(nm.array([d1, d2] + ds).T, n_bins, histtype='bar')
        plt.legend([ii[0] for ii in ps], ['default', 'RCM'] + g_names)
        plt.savefig('hist_distances.pdf', transparent=True)
        plt.show()

    def __iter__(self):
        return self

    def next(self):
        try:
            ii = self.perm[self.ii]
            val = self.coors[ii]
        except IndexError:
            raise StopIteration

        self.ii += 1

        return ii, val

class FieldVariable(Variable):
    """
    A finite element field variable.

    field .. field description of variable (borrowed)
    """

    def __init__(self, name, kind, field, order=None, primary_var_name=None,
                 special=None, flags=None, **kwargs):
        Variable.__init__(self, name, kind, order, primary_var_name,
                          special, flags, **kwargs)

        self._set_field(field)

        self.has_field = True
        self.has_bc = True
        self.has_lcbc = False
        self._variables = None

        self.clear_bases()
        self.clear_current_group()
        self.clear_evaluate_cache()

    def _set_field(self, field):
        """
        Set field of the variable.

        Takes reference to a Field instance. Sets dtype according to
        field.dtype. Sets `dim` attribute to spatial dimension.
        """
        self.is_surface = field.is_surface

        self.field = field
        self._setup_dofs(field.n_nod, field.n_components, field.val_shape)

        self.flags.add(is_field)
        self.dtype = field.dtype

        self.dim = field.domain.shape.dim

    def get_field(self):
        return self.field

    def get_mapping(self, ig, region, integral, integration,
                    get_saved=False, return_key=False):
        """
        Get the reference element mapping of the underlying field.

        See Also
        --------
        sfepy.discrete.fem.fields.Field.get_mapping()
        """
        if region is None:
            region = self.field.region

        out = self.field.get_mapping(ig, region, integral, integration,
                                     get_saved=get_saved,
                                     return_key=return_key)
        return out

    def get_dof_conn(self, dc_type, ig, is_trace=False):
        """
        Get active dof connectivity of a variable.

        Notes
        -----
        The primary and dual variables must have the same Region.
        """
        if self.is_virtual():
            var_name = self.get_primary().name

        else:
            var_name = self.name

        if not is_trace:
            region_name = dc_type.region_name
            aig = ig

        else:
            aux = self.field.domain.regions[dc_type.region_name]
            region, _, ig_map = aux.get_mirror_region()
            region_name = region.name
            aig = ig_map[ig]

        key = (var_name, region_name, dc_type.type, aig, is_trace)
        dc = self.adof_conns[key]

        return dc

    def get_dof_info(self, active=False):
        details = Struct(name='field_var_dof_details',
                         n_nod=self.n_nod,
                         dpn=self.n_components)
        if active:
            n_dof = self.n_adof

        else:
            n_dof = self.n_dof

        return n_dof, details

    def time_update(self, ts, functions):
        """
        Store time step, set variable data for variables with the setter
        function.
        """
        if ts is not None:
            self.dt = ts.dt

        if hasattr(self, 'special') and ('setter' in self.special):
            setter_name = self.special['setter']
            setter = functions[setter_name]

            region = self.field.region
            nod_list = self.field.get_dofs_in_region(region, clean=True)
            nods = nm.unique(nm.hstack(nod_list))

            coor = self.field.get_coor(nods)
            self.set_data(setter(ts, coor, region=region))
            output('data of %s set by %s()' % (self.name, setter_name))

    def set_data_from_qp(self, data_qp, integral, step=0):
        """
        Set DOFs of variable using values in quadrature points
        corresponding to the given integral.
        """
        data_vertex = self.field.average_qp_to_vertices(data_qp, integral)

        # Field nodes values.
        data = self.field.interp_v_vals_to_n_vals(data_vertex)
        data = data.squeeze()
        self.indx = slice(0, len(data))

        self.data[step] = data

    def create_lcbc_operators(self, bcs, offset, ts=None, functions=None):
        if len(bcs) == 0: return None

        bcs.canonize_dof_names(self.dofs)
        bcs.sort()

        ops = LCBCOperators('lcbc:%s' % self.name, self.eq_map, offset)
        for bc in bcs:
            # Skip conditions that are not active in the current time.
            if not is_active_bc(bc, ts=ts, functions=functions):
                continue

            output('lcbc:', self.name, bc.name)

            if ts is not None and ts.step > 0:
                # Save LCBC data only in the initial time step of the LCBC
                # application.
                import os
                if os.path.exists(get_default(bc.filename, '')):
                    bc = bc.copy()
                    bc.filename = None

            ops.add_from_bc(bc, self.field)

        ops.finalize()

        self.has_lcbc = True

        return ops

    def equation_mapping(self, bcs, var_di, ts, functions, problem=None,
                         warn=False):
        """
        Create the mapping of active DOFs from/to all DOFs.

        Sets n_adof.

        Returns
        -------
        active_bcs : set
            The set of boundary conditions active in the current time.
        """
        self.eq_map = EquationMap('eq_map', self.dofs, var_di)
        if bcs is not None:
            bcs.canonize_dof_names(self.dofs)
            bcs.sort()

        active_bcs = self.eq_map.map_equations(bcs, self.field, ts, functions,
                                               problem=problem, warn=warn)
        self.n_adof = self.eq_map.n_eq

        return active_bcs

    def setup_initial_conditions(self, ics, di, functions, warn=False):
        """
        Setup of initial conditions.
        """
        ics.canonize_dof_names(self.dofs)
        ics.sort()

        for ic in ics:
            region = ic.region
            dofs, val = ic.dofs

            if warn:
                clean_msg = ('warning: ignoring nonexistent' \
                             ' IC node (%s) in ' % self.name)
            else:
                clean_msg = None

            nod_list = self.field.get_dofs_in_region(region, clean=True,
                                                     warn=clean_msg)
            if len(nod_list) == 0:
                continue

            fun = get_condition_value(val, functions, 'IC', ic.name)
            if isinstance(fun, Function):
                aux = fun
                fun = lambda coors: aux(coors, ic=ic)

            nods, vv = self.field.set_dofs(fun, region, len(dofs), clean_msg)

            eq = expand_nodes_to_equations(nods, dofs, self.dofs)

            ic_vec = nm.zeros((di.n_dof[self.name],), dtype=self.dtype)
            ic_vec[eq] = vv

            self.initial_condition = ic_vec

    def get_approximation(self, ig):
        return self.field.aps[ig]

    def get_data_shape(self, ig, integral,
                       integration='volume', region_name=None):
        """
        Get element data dimensions for given approximation.

        Parameters
        ----------
        ig : int
            The element group index.
        integral : Integral instance
            The integral describing used numerical quadrature.
        integration : 'volume', 'plate', 'surface', 'surface_extra' or 'point'
            The term integration type.
        region_name : str
            The name of surface region, required when `shape_kind` is
            'surface'.

        Returns
        -------
        data_shape : 5 ints
            The `(n_el, n_qp, dim, n_en, n_comp)` for volume shape kind,
            `(n_fa, n_qp, dim, n_fn, n_comp)` for surface shape kind and
            `(n_nod, 0, 0, 1, n_comp)` for point shape kind.

        Notes
        -----
        - `n_el`, `n_fa` = number of elements/facets
        - `n_qp` = number of quadrature points per element/facet
        - `dim` = spatial dimension
        - `n_en`, `n_fn` = number of element/facet nodes
        - `n_comp` = number of variable components in a point/node
        - `n_nod` = number of element nodes
        """
        aux = self.field.get_data_shape(ig, integral, integration=integration,
                                        region_name=region_name)
        data_shape = aux + (self.n_components,)

        return data_shape

    def clear_bases(self):
        """
        Clear base functions, base function gradients and element data
        dimensions.
        """
        self.bfs = {}
        self.bfgs = {}
        self.data_shapes = {}

    def setup_bases(self, geo_key, ig, geo, integral, shape_kind='volume'):
        """
        Setup and cache base functions and base function gradients for
        given geometry. Also cache element data dimensions.
        """
        if geo_key not in self.bfs:
            ap = self.field.aps[ig]

            region_name = geo_key[1]

            self.data_shapes[geo_key] = self.get_data_shape(ig, integral,
                                                            shape_kind,
                                                            region_name)

            if shape_kind == 'surface':
                sd = ap.surface_data[region_name]
                key = sd.face_type

            elif shape_kind == 'volume':
                key = 'v'

            ebf = ap.get_base(key, 0, integral)
            bf = la.insert_strided_axis(ebf, 0, ap.econn.shape[0])
            self.bfs[geo_key] = bf

            self.bfgs[geo_key] = geo.bfg

    def clear_current_group(self):
        """
        Clear current group data.
        """
        self._ap = None
        self._data_shape = None
        self._bf = self._bfg = None

    def set_current_group(self, geo_key, ig):
        """
        Set current group data, initialize current DOF counter to `None`.

        The current group data are the approximation, element data
        dimensions, base functions and base function gradients.
        """
        self._ap = self.field.aps[ig]
        self._data_shape = self.data_shapes[geo_key]
        self._bf = self.bfs[geo_key]
        self._bfg = self.bfgs[geo_key]

        self._idof = None
        self._inod = None
        self._ic = None

    def val(self, ic=None):
        """
        Return base function values in quadrature points.

        Parameters
        ----------
        ic : int, optional
            The index of variable component.
        """
        if self._inod is None:
            # Evaluation mode.
            out = self.val_qp(ic=ic)

        else:
            out = self._bf[..., self._inod : self._inod + 1]

        return out

    def val_qp(self, ic=None):
        """
        Return variable evaluated in quadrature points.

        Parameters
        ----------
        ic : int, optional
            The index of variable component.
        """
        vec = self()[ic::self.n_components]
        evec = vec[self._ap.econn]

        aux = la.insert_strided_axis(evec, 1, self._bf.shape[1])[..., None]

        out = la.dot_sequences(aux, self._bf, 'ATBT')

        return out

    def grad(self, ic=None, ider=None):
        """
        Return base function gradient (space elements) values in
        quadrature points.

        Parameters
        ----------
        ic : int, optional
            The index of variable component.
        ider : int, optional
            The spatial derivative index. If not given, the whole
            gradient is returned.
        """
        if ider is None:
            iders = slice(None)

        else:
            iders = slice(ider, ider + 1)

        if self._inod is None:
            out = self.grad_qp(ic=ic, ider=ider)

        else:
            out = self._bfg[..., iders, self._inod : self._inod + 1]

        return out

    def grad_qp(self, ic=None, ider=None):
        """
        Return variable gradient evaluated in quadrature points.

        Parameters
        ----------
        ic : int, optional
            The index of variable component.
        ider : int, optional
            The spatial derivative index. If not given, the whole
            gradient is returned.
        """
        if ider is None:
            iders = slice(None)

        else:
            iders = slice(ider, ider + 1)

        vec = self()[ic::self.n_components]
        evec = vec[self._ap.econn]

        aux = la.insert_strided_axis(evec, 1, self._bfg.shape[1])[..., None]
        out = la.dot_sequences(self._bfg[:, :, iders, :], aux)

        return out

    def iter_dofs(self):
        """
        Iterate over element DOFs (DOF by DOF).
        """
        n_en, n_c = self._data_shape[3:]

        for ii in xrange(n_en):
            self._inod = ii
            for ic in xrange(n_c):
                self._ic = ic
                self._idof = n_en * ic + ii
                yield self._idof

    def get_element_zeros(self):
        """
        Return array of zeros with correct shape and type for term
        evaluation.
        """
        n_el, n_qp = self._data_shape[:2]

        return nm.zeros((n_el, n_qp, 1,  1), dtype=self.dtype)

    def get_component_indices(self):
        """
        Return indices of variable components according to current term
        evaluation mode.

        Returns
        -------
        indx : list of tuples
            The list of `(ii, slice(ii, ii + 1))` of the variable
            components. The first item is the index itself, the second
            item is a convenience slice to index components of material
            parameters.
        """
        if self._ic is None:
            indx = [(ii, slice(ii, ii + 1)) for ii in range(self.n_components)]

        else:
            indx = [(ii, slice(ii, ii + 1)) for ii in [self._ic]]

        return indx

    def clear_evaluate_cache(self):
        """
        Clear current evaluate cache.
        """
        self.evaluate_cache = {}

    def invalidate_evaluate_cache(self, step=0):
        """
        Invalidate variable data in evaluate cache for time step given
        by `step`  (0 is current, -1 previous, ...).

        This should be done, for example, prior to every nonlinear
        solver iteration.
        """
        for step_cache in self.evaluate_cache.itervalues():
            for key in step_cache.keys():
                if key == step: # Given time step to clear.
                    step_cache.pop(key)

    def evaluate(self, ig, mode='val',
                 region=None, integral=None, integration=None,
                 step=0, time_derivative=None, is_trace=False,
                 dt=None, bf=None):
        """
        Evaluate various quantities related to the variable according to
        `mode` in quadrature points defined by `integral`.

        The evaluated data are cached in the variable instance in
        `evaluate_cache` attribute.

        Parameters
        ----------
        ig : int
            The element group index.
        mode : one of 'val', 'grad', 'div', 'cauchy_strain'
            The evaluation mode.
        region : Region instance, optional
            The region where the evaluation occurs. If None, the
            underlying field region is used.
        integral : Integral instance, optional
            The integral defining quadrature points in which the
            evaluation occurs. If None, the first order volume integral
            is created. Must not be None for surface integrations.
        integration : one of 'volume', 'plate', 'surface', 'surface_extra'
            The term integration type. If None, it is derived from
            `integral`.
        step : int, default 0
            The time step (0 means current, -1 previous, ...).
        derivative : None or 'dt'
            If not None, return time derivative of the data,
            approximated by the backward finite difference.
        is_trace : bool, default False
            Indicate evaluation of trace of the variable on a boundary
            region.
        dt : float, optional
            The time step to be used if `derivative` is `'dt'`. If None,
            the `dt` attribute of the variable is used.
        bf : Base function, optional
            The base function to be used in 'val' mode.

        Returns
        -------
        out : array
            The 4-dimensional array of shape
            `(n_el, n_qp, n_row, n_col)` with the requested data,
            where `n_row`, `n_col` depend on `mode`.
        """
        step_cache = self.evaluate_cache.setdefault(mode, {})
        cache = step_cache.setdefault(step, {})

        field = self.field
        if region is None:
            region = field.region

        if is_trace:
            region, ig_map, ig_map_i = region.get_mirror_region()
            ig = ig_map_i[ig]

        if region is not field.region:
            assert_(field.region.contains(region))

        if integral is None:
            integral = Integral('aux_1', 1)

        if integration is None:
            integration = 'volume' if region.can_cells else 'surface'

        geo, _, key = field.get_mapping(ig, region, integral, integration,
                                        return_key=True)
        key += (time_derivative, is_trace)

        if key in cache:
            out = cache[key]

        else:
            vec = self(step=step, derivative=time_derivative, dt=dt)
            ct = integration
            if integration == 'surface_extra':
                ct = 'volume'
            conn = field.get_econn(ct, region, ig, is_trace, integration)

            shape = self.get_data_shape(ig, integral, integration, region.name)

            if self.dtype == nm.float64:
                out = eval_real(vec, conn, geo, mode, shape, bf)

            else:
                out = eval_complex(vec, conn, geo, mode, shape, bf)

            cache[key] = out

        return out

    def get_state_in_region(self, region, igs=None, reshape=True,
                             step=0):
        nods = self.field.get_dofs_in_region(region, merge=True, igs=igs)

        eq = nm.empty((len(nods) * self.n_components,), dtype=nm.int32)
        for idof in range(self.n_components):
            eq[idof::self.n_components] = self.n_components * nods \
                                          + idof + self.indx.start

        out = self.data[step][eq]
        if reshape:
            out.shape = (len(nods), self.n_components)

        return out

    def apply_ebc(self, vec, offset=0, force_values=None):
        """
        Apply essential (Dirichlet) and periodic boundary conditions to
        vector `vec`, starting at `offset`.
        """
        eq_map = self.eq_map
        ii = offset + eq_map.eq_ebc

        # EBC,
        if force_values is None:
            vec[ii] = eq_map.val_ebc

        else:
            if isinstance(force_values, dict):
                vec[ii] = force_values[self.name]

            else:
                vec[ii] = force_values

        # EPBC.
        vec[offset+eq_map.master] = vec[offset+eq_map.slave]

    def apply_ic(self, vec, offset=0, force_values=None):
        """
        Apply initial conditions conditions to vector `vec`, starting at
        `offset`.
        """
        ii = slice(offset, offset + self.n_dof)

        if force_values is None:
            vec[ii] = self.get_initial_condition()

        else:
            if isinstance(force_values, dict):
                vec[ii] = force_values[self.name]

            else:
                vec[ii] = force_values

    def get_reduced(self, vec, offset=0, follow_epbc=False):
        """
        Get the reduced DOF vector, with EBC and PBC DOFs removed.

        Notes
        -----
        The full vector starts in `vec` at `offset`. If 'follow_epbc' is True,
        values of EPBC master DOFs are not simply thrown away, but added to the
        corresponding slave DOFs, just like when assembling. For vectors with
        state (unknown) variables it should be set to False, for assembled
        vectors it should be set to True.
        """
        eq_map = self.eq_map
        ii = offset + eq_map.eqi

        r_vec = vec[ii]

        if follow_epbc:
            master = offset + eq_map.master
            slave = eq_map.eq[eq_map.slave]
            ii = slave >= 0
            la.assemble1d(r_vec, slave[ii], vec[master[ii]])

        return r_vec

    def get_full(self, r_vec, r_offset=0, force_value=None,
                 vec=None, offset=0):
        """
        Get the full DOF vector satisfying E(P)BCs from a reduced DOF
        vector.

        Notes
        -----
        The reduced vector starts in `r_vec` at `r_offset`.
        Passing a `force_value` overrides the EBC values. Optionally,
        `vec` argument can be provided to store the full vector (in
        place) starting at `offset`.
        """
        if vec is None:
            vec = nm.empty(self.n_dof, dtype=r_vec.dtype)

        else:
            vec = vec[offset:offset+self.n_dof]

        eq_map = self.eq_map
        r_vec = r_vec[r_offset:r_offset+eq_map.n_eq]

        # EBC.
        vec[eq_map.eq_ebc] = get_default(force_value, eq_map.val_ebc)

        # Reduced vector values.
        vec[eq_map.eqi] = r_vec

        # EPBC.
        vec[eq_map.master] = vec[eq_map.slave]

        return vec

    def create_output(self, vec=None, key=None, extend=True, fill_value=None,
                      linearization=None):
        """
        Convert the DOF vector to a dictionary of output data usable by
        Mesh.write().

        Parameters
        ----------
        vec : array, optional
            An alternative DOF vector to be used instead of the variable
            DOF vector.
        key : str, optional
            The key to be used in the output dictionary instead of the
            variable name.
        extend : bool
            Extend the DOF values to cover the whole domain.
        fill_value : float or complex
           The value used to fill the missing DOF values if `extend` is True.
        linearization : Struct or None
            The linearization configuration for higher order approximations.
        """
        linearization = get_default(linearization, Struct(kind='strip'))

        if vec is None:
            vec = self()

        key = get_default(key, self.name)

        aux = nm.reshape(vec,
                         (self.n_dof / self.n_components, self.n_components))

        out = self.field.create_output(aux, self.name, dof_names=self.dofs,
                                       key=key, extend=extend,
                                       fill_value=fill_value,
                                       linearization=linearization)

        return out

    def get_element_diameters(self, cells, mode, square=False):
        """Get diameters of selected elements."""
        field = self.field
        domain = field.domain

        cells = nm.array(cells)

        diameters = nm.empty((cells.shape[0],), dtype=nm.float64)

        integral = Integral('i_tmp', 1)

        igs = nm.unique(cells[:,0])
        for ig in igs:
            ap = field.aps[ig]
            vg = ap.describe_geometry(field, 'volume', field.region, integral)

            ii = nm.where(cells[:,0] == ig)[0]
            aux = domain.get_element_diameters(ig, cells[ii,1].copy(), vg,
                                               mode, square=square)
            diameters[ii] = aux

        return diameters

    def save_as_mesh(self, filename):
        """
        Save the field mesh and the variable values into a file for
        visualization. Only the vertex values are stored.
        """
        mesh = self.field.create_mesh(extra_nodes=False)
        vec = self()

        n_nod, n_dof, dpn = mesh.n_nod, self.n_dof, self.n_components
        aux = nm.reshape(vec, (n_dof / dpn, dpn))

        ext = self.field.extend_dofs(aux, 0.0)

        out = {}
        if self.field.approx_order != 0:
            out[self.name] = Struct(name='output_data',
                                    mode='vertex', data=ext,
                                    var_name=self.name, dofs=self.dofs)
        else:
            ext.shape = (ext.shape[0], 1, ext.shape[1], 1)
            out[self.name] = Struct(name='output_data',
                                    mode='cell', data=ext,
                                    var_name=self.name, dofs=self.dofs)

        mesh.write(filename, io='auto', out=out)

    def set_from_mesh_vertices(self, data):
        """
        Set the variable using values at the mesh vertices.
        """
        ndata = self.field.interp_v_vals_to_n_vals(data)
        self.set_data(ndata)

    def has_same_mesh(self, other):
        """
        Returns
        -------
        flag : int
            The flag can be either 'different' (different meshes), 'deformed'
            (slightly deformed same mesh), or 'same' (same).
        """
        f1 = self.field
        f2 = other.field

        c1 = f1.get_coor()
        c2 = f2.get_coor()

        if c1.shape != c2.shape:
            flag = 'different'

        else:
            eps = 10.0 * nm.finfo(nm.float64).eps

            if nm.allclose(c1, c2, rtol=eps, atol=0.0):
                flag = 'same'

            elif nm.allclose(c1, c2, rtol=0.1, atol=0.0):
                flag = 'deformed'

            else:
                flag = 'different'

        return flag

    def get_interp_coors(self, strategy='interpolation', interp_term=None):
        """
        Get the physical coordinates to interpolate into, based on the strategy
        used.
        """
        if strategy == 'interpolation':
            coors = self.field.get_coor()

        elif strategy == 'projection':
            region = self.field.region
            integral = Integral(term=interp_term)
            coors = get_physical_qps(region, integral)

        else:
            raise ValueError('unknown interpolation strategy! (%s)' % strategy)

        return coors

    def evaluate_at(self, coors, strategy='kdtree',
                    close_limit=0.1, cache=None, ret_cells=False,
                    ret_status=False, ret_ref_coors=False, verbose=True):
        """
        Evaluate the variable in the given physical coordinates. Convenience
        wrapper around :func:`Field.evaluate_at()
        <sfepy.discrete.fem.fields_nodal.H1NodalMixin.evaluate_at()>`, see its
        docstring for more details.
        """
        source_vals = self().reshape((self.n_nod, self.n_components))
        out = self.field.evaluate_at(coors, source_vals, strategy=strategy,
                                     close_limit=close_limit, cache=cache,
                                     ret_cells=ret_cells,
                                     ret_status=ret_status,
                                     ret_ref_coors=ret_ref_coors,
                                     verbose=verbose)

        return out

    def set_from_other(self, other, strategy='projection',
                       search_strategy='kdtree', ordering_strategy='rcm',
                       close_limit=0.1):
        """
        Set the variable using another variable. Undefined values (e.g. outside
        the other mesh) are set to numpy.nan, or extrapolated.

        Parameters
        ----------
        strategy : 'projection' or 'interpolation'
            The strategy to set the values: the L^2 orthogonal projection, or
            a direct interpolation to the nodes (nodal elements only!)

        Notes
        -----
        If the other variable uses the same field mesh, the coefficients are
        set directly.

        If the other variable uses the same field mesh, only deformed slightly,
        it is advisable to provide directly the node ids as a hint where to
        start searching for a containing element; the order of nodes does not
        matter then.

        Otherwise (large deformation, unrelated meshes, ...) there are
        basically two ways:
        a) query each node (its coordinates) using a KDTree of the other nodes
        - this completely disregards the connectivity information;
        b) iterate the mesh nodes so that the subsequent ones are close to each
        other - then also the elements of the other mesh should be close to each
        other: the previous one can be used as a start for the directional
        neighbour element crawling to the target point.

        Not sure which way is faster, depends on implementation efficiency and
        the particular meshes.
        """
        flag_same_mesh = self.has_same_mesh(other)

        if flag_same_mesh == 'same':
            self.set_data(other())
            return

        if strategy == 'interpolation':
            coors = self.get_interp_coors(strategy)

        elif strategy == 'projection':
            ## interp_term = Term() # TODO
            ## coors = self.get_interp_coors(strategy, interp_term)
            pass

        else:
            raise ValueError('unknown interpolation strategy! (%s)' % strategy)

        if search_strategy == 'kdtree':
            tt = time.clock()
            iter_nodes = CloseNodesIterator(self.field, create_graph=False)
            output('iterator: %f s' % (time.clock()-tt))

        elif search_strategy == 'crawl':
            tt = time.clock()
            iter_nodes = CloseNodesIterator(self.field, strategy='rcm')
            output('iterator: %f s' % (time.clock()-tt))

            iter_nodes.test_permutations()

        else:
            raise ValueError('unknown search strategy! (%s)' % search_strategy)

        perm = iter_nodes.get_permutation(iter_nodes.strategy)

        vals = other.evaluate_at(coors[perm], strategy=search_strategy,
                                 close_limit=close_limit)

        if strategy == 'interpolation':
            self.set_data(vals)

        elif strategy == 'projection':
            raise NotImplementedError('unsupported strategy! (%s)' % strategy)

        else:
            raise ValueError('unknown interpolation strategy! (%s)' % strategy)

########NEW FILE########
__FILENAME__ = band_gaps_app
import os.path as op
import shutil

import numpy as nm

from sfepy.base.base import output, set_defaults, assert_
from sfepy.base.base import Struct
from sfepy.homogenization.engine import HomogenizationEngine
from sfepy.homogenization.homogen_app import get_volume_from_options
from sfepy.homogenization.homogen_app import HomogenizationApp
from sfepy.homogenization.coefficients import Coefficients
from sfepy.homogenization.coefs_base import CoefDummy
from sfepy.applications import PDESolverApp
from sfepy.base.plotutils import plt

def try_set_defaults(obj, attr, defaults, recur=False):
    try:
        values = getattr(obj, attr)

    except:
        values = defaults

    else:
        if recur and isinstance(values, dict):
            for key, val in values.iteritems():
                set_defaults(val, defaults)

        else:
            set_defaults(values, defaults)

    return values

def transform_plot_data(datas, plot_transform, conf):
    if plot_transform is not None:
        fun = conf.get_function(plot_transform[0])

    dmin, dmax = 1e+10, -1e+10
    tdatas = []
    for data in datas:
        tdata = data.copy()
        if plot_transform is not None:
            tdata = fun(tdata, *plot_transform[1:])
        dmin = min(dmin, nm.nanmin(tdata))
        dmax = max(dmax, nm.nanmax(tdata))
        tdatas.append(tdata)
    dmin, dmax = min(dmax - 1e-8, dmin), max(dmin + 1e-8, dmax)
    return (dmin, dmax), tdatas

def plot_eigs(fig_num, plot_rsc, plot_labels, valid, freq_range, plot_range,
              show=False, clear=False, new_axes=False):
    """
    Plot resonance/eigen-frequencies.

    `valid` must correspond to `freq_range`

    resonances : red
    masked resonances: dotted red
    """
    if plt is None: return
    assert_(len(valid) == len(freq_range))

    fig = plt.figure(fig_num)
    if clear:
        fig.clf()
    if new_axes:
        ax = fig.add_subplot(111)
    else:
        ax = fig.gca()

    l0 = l1 = None
    for ii, f in enumerate(freq_range):
        if valid[ii]:
            l0 = ax.plot([f, f], plot_range, **plot_rsc['resonance'])[0]
        else:
            l1 = ax.plot([f, f], plot_range, **plot_rsc['masked'])[0]

    if l0:
        l0.set_label(plot_labels['resonance'])
    if l1:
        l1.set_label(plot_labels['masked'])

    if new_axes:
        ax.set_xlim([freq_range[0], freq_range[-1]])
        ax.set_ylim(plot_range)

    if show:
        plt.show()
    return fig

def plot_logs(fig_num, plot_rsc, plot_labels,
              freqs, logs, valid, freq_range, plot_range,
              draw_eigs=True, show_legend=True, show=False,
              clear=False, new_axes=False):
    """
    Plot logs of min/middle/max eigs of a mass matrix.
    """
    if plt is None: return

    fig = plt.figure(fig_num)
    if clear:
        fig.clf()
    if new_axes:
        ax = fig.add_subplot(111)
    else:
        ax = fig.gca()

    if draw_eigs:
        plot_eigs(fig_num, plot_rsc, plot_labels, valid, freq_range,
                  plot_range)

    for ii, log in enumerate(logs):
        l1 = ax.plot(freqs[ii], log[:, -1], **plot_rsc['eig_max'])

        if log.shape[1] >= 2:
            l2 = ax.plot(freqs[ii], log[:, 0], **plot_rsc['eig_min'])
        else:
            l2 = None

        if log.shape[1] == 3:
            l3 = ax.plot(freqs[ii], log[:, 1], **plot_rsc['eig_mid'])
        else:
            l3 = None

    l1[0].set_label(plot_labels['eig_max'])
    if l2:
        l2[0].set_label(plot_labels['eig_min'])
    if l3:
        l3[0].set_label(plot_labels['eig_mid'])

    fmin, fmax = freqs[0][0], freqs[-1][-1]
    ax.plot([fmin, fmax], [0, 0], **plot_rsc['x_axis'])

    ax.set_xlabel(plot_labels['x_axis'])
    ax.set_ylabel(plot_labels['y_axis'])

    if new_axes:
        ax.set_xlim([fmin, fmax])
        ax.set_ylim(plot_range)

    if show_legend:
        ax.legend()

    if show:
        plt.show()
    return fig

def plot_gap(ax, ii, f0, f1, kind, kind_desc, gmin, gmax, plot_range, plot_rsc):
    """
    Plot a single band gap as a rectangle.
    """
    def draw_rect(ax, x, y, rsc):
        ax.fill(nm.asarray(x)[[0,1,1,0]],
                 nm.asarray(y)[[0,0,1,1]],
                 **rsc)

    # Colors.
    strong = plot_rsc['strong_gap']
    weak = plot_rsc['weak_gap']
    propagation = plot_rsc['propagation']

    if kind == 'p':
        draw_rect(ax, (f0, f1), plot_range, propagation)
        info = [(f0, f1)]
    elif kind == 'w':
        draw_rect(ax, (f0, f1), plot_range, weak)
        info = [(f0, f1)]
    elif kind == 'wp':
        draw_rect(ax, (f0, gmin[1]), plot_range, weak)
        draw_rect(ax, (gmin[1], f1), plot_range, propagation)
        info = [(f0, gmin[1]), (gmin[1], f1)]
    elif kind == 's':
        draw_rect(ax, (f0, f1), plot_range, strong)
        info = [(f0, f1)]
    elif kind == 'sw':
        draw_rect(ax, (f0, gmax[1]), plot_range, strong)
        draw_rect(ax, (gmax[1], f1), plot_range, weak)
        info = [(f0, gmax[1]), (gmax[1], f1)]
    elif kind == 'swp':
        draw_rect(ax, (f0, gmax[1]), plot_range, strong)
        draw_rect(ax, (gmax[1], gmin[1]), plot_range, weak)
        draw_rect(ax, (gmin[1], f1), plot_range, propagation)
        info = [(f0, gmax[1]), (gmax[1], gmin[1]), (gmin[1], f1)]
    elif kind == 'is':
        draw_rect(ax, (gmin[1], gmax[1]), plot_range, strong)
        info = [(gmin[1], gmax[1])]
    elif kind == 'iw':
        draw_rect(ax, (gmin[1], gmax[1]), plot_range, weak)
        info = [(gmin[1], gmax[1])]
    else:
        output('impossible band gap combination:')
        output(gmin, gmax)
        raise ValueError

    output(ii, gmin[0], gmax[0], '%.8f' % f0, '%.8f' % f1)
    output(' -> %s\n    %s' %(kind_desc, info))

def plot_gaps(fig_num, plot_rsc, gaps, kinds, freq_range,
              plot_range, show=False, clear=False, new_axes=False):
    """
    Plot band gaps as rectangles.
    """
    if plt is None: return

    fig = plt.figure(fig_num)
    if clear:
        fig.clf()
    if new_axes:
        ax = fig.add_subplot(111)
    else:
        ax = fig.gca()

    for ii in xrange(len(freq_range) - 1):
        f0, f1 = freq_range[[ii, ii+1]]
        gap = gaps[ii]

        if isinstance(gap, list):
            for ig, (gmin, gmax) in enumerate(gap):
                kind, kind_desc = kinds[ii][ig]
                plot_gap(ax, ii, f0, f1, kind, kind_desc, gmin, gmax,
                         plot_range, plot_rsc)
        else:
            gmin, gmax = gap
            kind, kind_desc = kinds[ii]
            plot_gap(ax, ii, f0, f1, kind, kind_desc, gmin, gmax,
                     plot_range, plot_rsc)

    if new_axes:
        ax.set_xlim([freq_range[0], freq_range[-1]])
        ax.set_ylim(plot_range)

    if show:
        plt.show()
    return fig

def _get_fig_name(output_dir, fig_name, key, common, fig_suffix):
    """
    Construct the complete name of figure file.
    """
    name = key.replace(common, '')
    if name and (not name.startswith('_')):
        name = '_' + name

    fig_name = fig_name + name + fig_suffix
    return op.join(output_dir, fig_name)

class AcousticBandGapsApp(HomogenizationApp):
    """
    Application for computing acoustic band gaps.
    """

    @staticmethod
    def process_options(options):
        """
        Application options setup. Sets default values for missing
        non-compulsory options.
        """
        get = options.get

        default_plot_options = {'show' : True,'legend' : False,}

        aux = {
            'resonance' : 'eigenfrequencies',
            'masked' : 'masked eigenfrequencies',
            'eig_min' : r'min eig($M^*$)',
            'eig_mid' : r'mid eig($M^*$)',
            'eig_max' : r'max eig($M^*$)',
            'x_axis' : r'$\sqrt{\lambda}$, $\omega$',
            'y_axis' : r'eigenvalues of mass matrix $M^*$',
        }
        plot_labels = try_set_defaults(options, 'plot_labels', aux, recur=True)

        aux = {
            'resonance' : 'eigenfrequencies',
            'masked' : 'masked eigenfrequencies',
            'eig_min' : r'$\kappa$(min)',
            'eig_mid' : r'$\kappa$(mid)',
            'eig_max' : r'$\kappa$(max)',
            'x_axis' : r'$\sqrt{\lambda}$, $\omega$',
            'y_axis' : 'polarization angles',
        }
        plot_labels_angle = try_set_defaults(options, 'plot_labels_angle', aux)

        aux = {
            'resonance' : 'eigenfrequencies',
            'masked' : 'masked eigenfrequencies',
            'eig_min' : r'wave number (min)',
            'eig_mid' : r'wave number (mid)',
            'eig_max' : r'wave number (max)',
            'x_axis' : r'$\sqrt{\lambda}$, $\omega$',
            'y_axis' : 'wave numbers',
        }
        plot_labels_wave = try_set_defaults(options, 'plot_labels_wave', aux)

        plot_rsc =  {
            'resonance' : {'linewidth' : 0.5, 'color' : 'r', 'linestyle' : '-'},
            'masked' : {'linewidth' : 0.5, 'color' : 'r', 'linestyle' : ':'},
            'x_axis' : {'linewidth' : 0.5, 'color' : 'k', 'linestyle' : '--'},
            'eig_min' : {'linewidth' : 2.0, 'color' : (0.0, 0.0, 1.0),
                         'linestyle' : ':' },
            'eig_mid' : {'linewidth' : 2.0, 'color' : (0.0, 0.0, 0.8),
                         'linestyle' : '--' },
            'eig_max' : {'linewidth' : 2.0, 'color' : (0.0, 0.0, 0.6),
                         'linestyle' : '-' },
            'strong_gap' : {'linewidth' : 0, 'facecolor' : (0.2, 0.4, 0.2)},
            'weak_gap' : {'linewidth' : 0, 'facecolor' : (0.6, 0.8, 0.6)},
            'propagation' : {'linewidth' : 0, 'facecolor' : (1, 1, 1)},
            'params' : {'axes.labelsize': 'x-large',
                        'text.fontsize': 'large',
                        'legend.fontsize': 'large',
                        'legend.loc': 0,
                        'xtick.labelsize': 'large',
                        'ytick.labelsize': 'large',
                        'text.usetex': True},
        }
        plot_rsc = try_set_defaults(options, 'plot_rsc', plot_rsc)

        return Struct(incident_wave_dir=get('incident_wave_dir', None),

                      plot_transform=get('plot_transform', None),
                      plot_transform_wave=get('plot_transform_wave', None),
                      plot_transform_angle=get('plot_transform_angle', None),

                      plot_options=get('plot_options', default_plot_options),

                      fig_name=get('fig_name', None),
                      fig_name_wave=get('fig_name_wave', None),
                      fig_name_angle=get('fig_name_angle', None),
                      fig_suffix=get('fig_suffix', '.pdf'),

                      plot_labels=plot_labels,
                      plot_labels_angle=plot_labels_angle,
                      plot_labels_wave=plot_labels_wave,
                      plot_rsc=plot_rsc)

    @staticmethod
    def process_options_pv(options):
        """
        Application options setup for phase velocity computation. Sets default
        values for missing non-compulsory options.
        """
        get = options.get

        incident_wave_dir=get('incident_wave_dir', None,
                              'missing "incident_wave_dir" in options!')

        return Struct(incident_wave_dir=incident_wave_dir)

    def __init__(self, conf, options, output_prefix, **kwargs):
        PDESolverApp.__init__(self, conf, options, output_prefix,
                              init_equations=False)

        self.setup_options()

        output_dir = self.problem.output_dir
        shutil.copyfile(conf._filename,
                        op.join(output_dir, op.basename(conf._filename)))

    def setup_options(self):
        HomogenizationApp.setup_options(self)

        if self.options.phase_velocity:
            process_options = AcousticBandGapsApp.process_options_pv
        else:
            process_options = AcousticBandGapsApp.process_options
        self.app_options += process_options(self.conf.options)

    def call(self):
        """
        Construct and call the homogenization engine accoring to options.
        """
        options = self.options

        opts = self.app_options
        conf = self.problem.conf
        coefs_name = opts.coefs
        coef_info = conf.get(opts.coefs, None,
                             'missing "%s" in problem description!'
                             % opts.coefs)

        if options.detect_band_gaps:
            # Compute band gaps coefficients and data.
            keys = [key for key in coef_info if key.startswith('band_gaps')]

        elif options.analyze_dispersion or options.phase_velocity:

            # Insert incident wave direction to coefficients that need it.
            for key, val in coef_info.iteritems():
                coef_opts = val.get('options', None)
                if coef_opts is None: continue

                if (('incident_wave_dir' in coef_opts)
                    and (coef_opts['incident_wave_dir'] is None)):
                    coef_opts['incident_wave_dir'] = opts.incident_wave_dir

            if options.analyze_dispersion:
                # Compute dispersion coefficients and data.
                keys = [key for key in coef_info
                        if key.startswith('dispersion')
                        or key.startswith('polarization_angles')]

            else:
                # Compute phase velocity and its requirements.
                keys = [key for key in coef_info
                        if key.startswith('phase_velocity')]

        else:
            # Compute only the eigenvalue problems.
            names = [req for req in conf.get(opts.requirements, [''])
                     if req.startswith('evp')]
            coefs = {'dummy' : {'requires' : names,
                                'class' : CoefDummy,}}
            conf.coefs_dummy = coefs
            coefs_name = 'coefs_dummy'
            keys = ['dummy']

        he_options = Struct(coefs=coefs_name, requirements=opts.requirements,
                            compute_only=keys,
                            post_process_hook=self.post_process_hook)
        volume = get_volume_from_options(opts, self.problem)

        he = HomogenizationEngine(self.problem, options,
                                  app_options=he_options,
                                  volume=volume)
        coefs = he()

        coefs = Coefficients(**coefs.to_dict())
        coefs.volume = volume

        coefs_filename = op.join(opts.output_dir, opts.coefs_filename)
        coefs.to_file_txt(coefs_filename + '.txt',
                          opts.tex_names,
                          opts.float_format)

        bg_keys = [key for key in coefs.to_dict()
                   if key.startswith('band_gaps')
                   or key.startswith('dispersion')]
        for ii, key in enumerate(bg_keys):
            bg = coefs.get(key)
            log_save_name = bg.get('log_save_name', None)
            if log_save_name is not None:
                filename = op.join(self.problem.output_dir, log_save_name)
                bg.save_log(filename, opts.float_format, bg)

        if options.plot:
            if options.detect_band_gaps:
                self.plot_band_gaps(coefs)

            elif options.analyze_dispersion:
                self.plot_dispersion(coefs)

        elif options.phase_velocity:
            keys = [key for key in coefs.to_dict()
                    if key.startswith('phase_velocity')]
            for key in keys:
                output('%s:' % key, coefs.get(key))

        return coefs

    def plot_band_gaps(self, coefs):
        opts = self.app_options

        bg_keys = [key for key in coefs.to_dict()
                   if key.startswith('band_gaps')]

        plot_opts =  opts.plot_options
        plot_rsc = opts.plot_rsc

        plt.rcParams.update(plot_rsc['params'])

        for ii, key in enumerate(bg_keys):
            bg = coefs.get(key)

            plot_labels =  opts.plot_labels.get(key, opts.plot_labels)

            plot_range, teigs = transform_plot_data(bg.logs.eigs,
                                                    opts.plot_transform,
                                                    self.conf)
            fig = plot_gaps(ii, plot_rsc, bg.gaps, bg.kinds,
                            bg.freq_range_margins, plot_range,
                            clear=True)
            fig = plot_logs(ii, plot_rsc, plot_labels, bg.logs.freqs, teigs,
                            bg.valid[bg.eig_range],
                            bg.freq_range_initial,
                            plot_range,
                            show_legend=plot_opts['legend'],
                            new_axes=True)

            if opts.fig_name is not None:
                fig_name = _get_fig_name(self.problem.output_dir, opts.fig_name,
                                         key, 'band_gaps', opts.fig_suffix)
                fig.savefig(fig_name)

        if plot_opts['show']:
            plt.show()

    def plot_dispersion(self, coefs):
        opts = self.app_options

        bg_keys = [key for key in coefs.to_dict()
                   if key.startswith('dispersion')]

        plot_rsc = opts.plot_rsc
        plot_opts =  opts.plot_options
        plt.rcParams.update(plot_rsc['params'])

        plot_labels =  opts.plot_labels_angle

        for ii, key in enumerate(bg_keys):
            pas_key = key.replace('dispersion', 'polarization_angles')
            pas = coefs.get(pas_key)

            aux = transform_plot_data(pas,
                                      opts.plot_transform_angle,
                                      self.conf)
            plot_range, pas = aux


            bg = coefs.get(key)

            fig = plot_gaps(1, plot_rsc, bg.gaps, bg.kinds,
                            bg.freq_range_margins, plot_range,
                            clear=True)
            fig = plot_logs(1, plot_rsc, plot_labels, bg.logs.freqs, pas,
                            bg.valid[bg.eig_range],
                            bg.freq_range_initial,
                            plot_range,
                            show_legend=plot_opts['legend'],
                            new_axes=True)

            fig_name = opts.fig_name_angle
            if fig_name is not None:
                fig_name = _get_fig_name(self.problem.output_dir, fig_name,
                                         key, 'dispersion', opts.fig_suffix)
                fig.savefig(fig_name)

            aux = transform_plot_data(bg.logs.eigs,
                                      opts.plot_transform_wave,
                                      self.conf)
            plot_range, teigs = aux

            plot_labels =  opts.plot_labels_wave

            fig = plot_gaps(2, plot_rsc, bg.gaps, bg.kinds,
                            bg.freq_range_margins, plot_range,
                            clear=True)
            fig = plot_logs(2, plot_rsc, plot_labels, bg.logs.freqs, teigs,
                            bg.valid[bg.eig_range],
                            bg.freq_range_initial,
                            plot_range,
                            show_legend=plot_opts['legend'],
                            new_axes=True)

            fig_name = opts.fig_name_wave
            if fig_name is not None:
                fig_name = _get_fig_name(self.problem.output_dir, fig_name,
                                         key, 'dispersion', opts.fig_suffix)
                fig.savefig(fig_name)

        if plot_opts['show']:
            plt.show()

########NEW FILE########
__FILENAME__ = coefficients
import numpy as nm

from sfepy.base.base import ordered_iteritems, Struct, basestr
from sfepy.base.ioutils import read_dict_hdf5, write_dict_hdf5
from sfepy.homogenization.utils import iter_sym

class Coefficients(Struct):
    """
    Class for storing (homogenized) material coefficients.
    """

    def from_file_hdf5( filename ):
        obj = Coefficients()
        obj.__dict__ = read_dict_hdf5( filename )
        for key, val in obj.__dict__.iteritems():
            if type( val ) == list:
                for ii, vv in enumerate( val ):
                    val[ii] = nm.array( vv, dtype = nm.float64 )

        return obj
    from_file_hdf5 = staticmethod( from_file_hdf5 )

    def to_file_hdf5( self, filename ):
        write_dict_hdf5( filename, self.__dict__ )

    def _escape_latex(self, txt):
        return txt.replace('_', '\_').replace('%', '\%')

    def _format(self, val):
        out = self._a_format % val
        if self._a_cdot:
            a1, a2 = out.split('e')
            if (self._a_filter is not None) and (int(a2) < self._a_filter):
                out = '0'

            else:
                out = '%s \cdot 10^{%s}' % (a1, int(a2))

        return out

    def _write1d(self, fd, val):
        fd.write( r'  \begin{equation}' )
        fd.write( '\n' )
        fd.write( r'    \left[' )
        fd.write( '\n' )
        fd.write(', '.join([self._format(vv) for vv in val]))
        fd.write( '\n' )
        fd.write( r'    \right]' )
        fd.write( '\n' )
        fd.write( r'  \end{equation}' )
        fd.write( '\n' )

    def _write2d(self, fd, val):
        fd.write( r'  \begin{equation}' )
        fd.write( '\n' )
        fd.write( r'    \left[\begin{array}{%s}' % ('c' * val.shape[0]) )
        fd.write( '\n' )
        for ir in xrange( val.shape[1] ):
            for ic in xrange( val.shape[0] ):
                fd.write('    ' + self._format(val[ir,ic]))
                if ic < (val.shape[0] - 1):
                    fd.write( r' & ' )
                elif ir < (val.shape[1] - 1):
                    fd.write( r' \\' )
                    fd.write( '\n' )
        fd.write( '\n' )
        fd.write( r'    \end{array}\right]' )
        fd.write( '\n' )
        fd.write( r'  \end{equation}' )
        fd.write( '\n' )

    def _save_dict_latex(self, adict, fd, names):
        fd.write( r'\begin{itemize}' )
        fd.write( '\n' )
        for key, val in ordered_iteritems(adict):
            if key.startswith('_a_'): continue

            try:
                lname = names[key]
            except:
                lname = self._escape_latex(key)
            fd.write( '\item %s:' % lname )
            fd.write( '\n' )

            if isinstance(val, dict):
                self._save_dict_latex(val, fd, names)

            elif isinstance(val, basestr):
                fd.write(self._escape_latex(val) + '\n')

            elif val.ndim == 0:
                fd.write('$' + self._format(val) + '$\n')

            elif val.ndim == 1:
                self._write1d(fd, val)

            elif val.ndim == 2:
                self._write2d(fd, val)

        fd.write( r'\end{itemize}' )
        fd.write( '\n\n' )


    def to_file_latex(self, filename, names, format='%.2e',
                      cdot=False, filter=None):
        r"""
        Save the coefficients to a file in LaTeX format.

        Parameters
        ----------
        filename : str
            The name of the output file.
        names : dict
            Mapping of attribute names to LaTeX names.
        format : str
            Format string for numbers.
        cdot : bool
            For '%.e' formats only. If True, replace 'e'  by LaTeX '\cdot
            10^{exponent}' format.
        filter : int
            For '%.e' formats only. Typeset as 0, if exponent is less than
            `filter`.
        """
        self._a_format = format
        self._a_cdot = cdot
        self._a_filter = filter

        fd = open(filename, 'w')
        self._save_dict_latex(self.__dict__, fd, names)
        fd.close()

    def _save_dict(self, adict, fd, names, format):
        for key, val in ordered_iteritems(adict):
            try:
                lname = names[key]
            except:
                lname = key
            fd.write('%s:\n' % lname)

            if hasattr(val, 'to_file_txt'):
                if val.to_file_txt is not None:
                    val.to_file_txt(fd, format, val)

                else:
                    fd.write('--\n')

            elif isinstance(val, dict):
                self._save_dict(val, fd, names, format)
                fd.write('\n')

            elif isinstance(val, basestr):
                fd.write(val + '\n')

            elif isinstance(val, float):
                fd.write('%e\n' % val)

            elif isinstance(val, nm.ndarray):
                if val.ndim == 0:
                    fd.write(format % val)
                    fd.write('\n')

                elif val.ndim == 1:
                    for ic in xrange(val.shape[0]):
                        fd.write(format % val[ic])
                        if ic < (val.shape[0] - 1):
                            fd.write(', ')
                        else:
                            fd.write('\n')

                elif val.ndim == 2:
                    for ir in xrange(val.shape[0]):
                        for ic in xrange(val.shape[1]):
                            fd.write(format % val[ir,ic])
                            if ic < (val.shape[1] - 1):
                                fd.write(', ')
                            elif ir < (val.shape[0] - 1):
                                fd.write(';\n')
                    fd.write('\n')

                elif val.ndim == 3:
                    for ii in xrange(val.shape[0]):
                        fd.write('  step %d:\n' % ii)
                        for ir in xrange(val.shape[1]):
                            for ic in xrange(val.shape[2]):
                                fd.write('  ' + format % val[ii,ir,ic])
                                if ic < (val.shape[2] - 1):
                                    fd.write(', ')
                                elif ir < (val.shape[1] - 1):
                                    fd.write(';\n')
                        fd.write('\n')
                    fd.write('\n')

            else:
                fd.write('--\n')

            fd.write('\n')

    def to_file_txt( self, filename, names, format ):

        fd = open( filename, 'w' )
        self._save_dict( self.__dict__, fd, names, format )
        fd.close()

    _table_vector = r"""
\begin{center}
\begin{tabular}{cc}
i & value \\
%s
\end{tabular}
\end{center}
    """

    _table_matrix_1 = r"""
\begin{center}
\begin{tabular}{cc}
ij & value \\
%s
\end{tabular}
\end{center}
    """

    _table_matrix_2 = r"""
\begin{center}
\begin{tabular}{cc}
ijkl & value \\
%s
\end{tabular}
\end{center}
    """

    _itemize = r"""
\begin{itemize}
%s
\end{itemize}
    """

    ##
    # c: 09.07.2008, r: 09.07.2008
    def _typeset( self, val, dim, style = 'table', format = '%f',
                  step = None ):
        sym = (dim + 1) * dim / 2

        mode = None
        if val.ndim == 0:
            mode = 'scalar'

        elif val.ndim == 1:
            if val.shape[0] == 1:
                mode = 'scalar'
            elif val.shape[0] == dim:
                mode = 'vector'
            elif val.shape[0] == sym:
                mode = 'matrix_t1d'

        elif val.ndim == 2:
            if val.shape[0] == dim:
                mode = 'matrix_2D'
            elif val.shape[0] == sym:
                mode = 'matrix_t2d'

        out = ''
        if mode == 'scalar':
            out = format % val
        elif mode == 'vector':
            aux = ' \\\\\n'.join( [r'$_%d$ & %s' % (ir + 1, format % val[ir])
                                   for ir in xrange( dim )] )
            out = self._table_vector % aux
        elif mode == 'matrix_t1d':
            aux = ' \\\\\n'.join( [r'$_{%d%d}$ & %s' % (ir + 1, ic + 1,
                                                        format % val[ii])
                                  for ii, (ir, ic) \
                                  in enumerate( iter_sym( dim ) )] )
            out = self._table_matrix_1 % aux
        elif mode == 'matrix_2D':
            aux = ' \\\\\n'.join( [r'$_{%d%d}$ & %s' % (ir + 1, ic + 1,
                                                        format % val[ir,ic])
                                  for ir in xrange( dim )
                                  for ic in xrange( dim )] )
            out = self._table_matrix_1 % aux
        elif mode == 'matrix_t2d':
            aux = ' \\\\\n'.join( [r'$_{%d%d%d%d}$ & %s' % (irr + 1, irc + 1,
                                                            icr + 1, icc + 1,
                                                            format % val[ii,jj])
                                  for ii, (irr, irc) \
                                  in enumerate( iter_sym( dim ) )
                                  for jj, (icr, icc) \
                                  in enumerate( iter_sym( dim ) )] )
            out = self._table_matrix_2 % aux
        return out

    def to_latex( self, attr_name, dim, style = 'table', format = '%f',
                 step = None ):

        val = getattr( self, attr_name )
        if step is not None:
            val = val[step]

        if isinstance( val, dict ):
            aux = ''
            for key, dval in val.iteritems():
                aux2 = r'\item %s : %s' % (key,
                                           self._typeset( dval, dim, style,
                                                          format, step ))
                aux = '\n'.join( (aux, aux2) )
            out = self._itemize % aux
        else:
            out = self._typeset( val, dim, style, format, step )

        return out

########NEW FILE########
__FILENAME__ = coefs_base
import os
import time

import numpy as nm
import numpy.linalg as nla
import scipy as sc

from sfepy.base.base import output, assert_, get_default, debug, Struct
from sfepy.discrete.evaluate import eval_equations
from sfepy.solvers.ts import TimeStepper
from sfepy.discrete.fem.meshio import HDF5MeshIO
from sfepy.solvers import Solver, eig
from sfepy.linalg import MatrixAction
from utils import iter_sym, create_pis, create_scalar_pis

class MiniAppBase(Struct):
    def any_from_conf(name, problem, kwargs):
        try:
            cls = kwargs['class']
        except KeyError:
            raise KeyError("set 'class' for MiniApp %s!" % name)
        obj = cls(name, problem, kwargs)
        return obj
    any_from_conf = staticmethod(any_from_conf)

    def __init__(self, name, problem, kwargs):
        Struct.__init__(self, name=name, problem=problem, **kwargs)

        self.problem.clear_equations()
        self.set_default('requires', [])
        self.set_default('is_linear', False)
        self.set_default('dtype', nm.float64)
        self.set_default('term_mode', None)
        self.set_default('set_volume', 'total')

        # Application-specific options.
        self.app_options = self.process_options()

    def process_options(self):
        """
        Setup application-specific options.

        Subclasses should implement this method as needed.

        Returns
        -------
        app_options : Struct instance
            The application options.
        """

    def init_solvers(self, problem):
        """
        Setup solvers. Use local options if these are defined,
        otherwise use the global ones.

        For linear problems, assemble the matrix and try to presolve the
        linear system.
        """

        if hasattr(self, 'solvers'):
            opts = self.solvers

        else:
            opts = problem.conf.options

        problem.set_solvers(problem.conf.solvers, opts)

        if self.is_linear:
            output('linear problem, trying to presolve...')
            tt = time.clock()

            ev = problem.get_evaluator()

            state = problem.create_state()
            try:
                mtx_a = ev.eval_tangent_matrix(state(), is_full=True)
            except ValueError:
                output('matrix evaluation failed, giving up...')
                raise

            problem.set_linear(True)
            problem.init_solvers(mtx=mtx_a, presolve=True)

            output('...done in %.2f s' % (time.clock() - tt))

        else:
            problem.set_linear(False)

    def _get_volume(self, volume):
        if isinstance(volume, dict):
            return volume[self.set_volume]

        else:
            return volume

class CorrSolution(Struct):
    """
    Class for holding solutions of corrector problems.
    """

    def iter_solutions(self):
        if hasattr(self, 'components'):
            for indx in self.components:
                key = ('%d' * len(indx)) % indx
                yield key, self.states[indx]

        else:
            yield '', self.state

class CorrMiniApp(MiniAppBase):

    def __init__(self, name, problem, kwargs):
        MiniAppBase.__init__(self, name, problem, kwargs)
        self.output_dir = self.problem.output_dir
        self.set_default('save_name', '(not_set)')
        self.set_default('dump_name', self.save_name)
        self.set_default('dump_variables', [])
        self.set_default('save_variables', self.dump_variables)

        self.save_name = os.path.normpath(os.path.join(self.output_dir,
                                                         self.save_name))
        self.dump_name = os.path.normpath(os.path.join(self.output_dir,
                                                         self.dump_name))

    def setup_output(self, save_format=None, dump_format=None,
                      post_process_hook=None, file_per_var=None):
        """Instance attributes have precedence!"""
        self.set_default('dump_format', dump_format)
        self.set_default('save_format', save_format)
        self.set_default('post_process_hook', post_process_hook)
        self.set_default('file_per_var', file_per_var)

    def get_save_name_base(self):
        return self.save_name

    def get_dump_name_base(self):
        return self.get_save_name_base()

    def get_save_name(self):
        return '.'.join((self.get_save_name_base(), self.save_format))

    def get_dump_name(self):
        if self.dump_format is not None:
            return '.'.join((self.get_dump_name_base(), self.dump_format))
        else:
            return None

    def get_output(self, corr_sol, is_dump=False, extend=True, variables=None):
        if variables is None:
            variables = self.problem.get_variables()
        to_output = variables.state_to_output

        if is_dump:
            var_names = self.dump_variables
            extend = False

        else:
            var_names = self.save_variables

        out = {}
        for key, sol in corr_sol.iter_solutions():
            for var_name in var_names:
                if key:
                    skey = var_name + '_' + key

                else:
                    skey = var_name

                dof_vector = sol[var_name]

                if is_dump:
                        var = variables[var_name]
                        shape = (var.n_dof / var.n_components,
                                 var.n_components)
                        out[skey] = Struct(name = 'dump', mode = 'nodes',
                                           data = dof_vector,
                                           dofs = var.dofs,
                                           shape = shape,
                                           var_name = var_name)

                else:
                    aux = to_output(dof_vector,
                                    var_info={var_name: (True, var_name)},
                                    extend=extend)
                    if self.post_process_hook is not None:
                        aux = self.post_process_hook(aux, self.problem,
                                                     None,
                                                     extend=extend)

                    for _key, val in aux.iteritems():
                        if key:
                            new_key = _key + '_' + key

                        else:
                            new_key = _key
                        out[new_key] = val

        return out

    def save(self, state, problem, variables=None):
        save_name = self.get_save_name()
        if save_name is not None:
            extend = not self.file_per_var
            out = self.get_output(state, extend=extend,
                                  variables=variables)

            problem.save_state(save_name, out=out,
                               file_per_var=self.file_per_var)

        dump_name = self.get_dump_name()
        if dump_name is not None:
            problem.save_state(dump_name,
                               out=self.get_output(state, is_dump=True,
                                                   variables=variables),
                               file_per_var=False)

class ShapeDimDim(CorrMiniApp):

    def __call__(self, problem=None, data=None):
        problem = get_default(problem, self.problem)

        clist, pis = create_pis(problem, self.variables[0])

        corr_sol = CorrSolution(name=self.name,
                                states=pis,
                                components=clist)
        return corr_sol

class ShapeDim(CorrMiniApp):

    def __call__(self, problem=None, data=None):
        problem = get_default(problem, self.problem)

        clist, pis = create_scalar_pis(problem, self.variables[0])

        corr_sol = CorrSolution(name=self.name,
                                states=pis,
                                components=clist)
        return corr_sol

class OnesDim(CorrMiniApp):

    def __call__(self, problem=None, data=None):
        problem = get_default(problem, self.problem)
        var_name = self.variables[0]
        var = problem.get_variables(auto_create=True)[var_name]

        dim = problem.domain.mesh.dim
        nnod = var.n_nod
        e00 = nm.zeros((nnod, dim), dtype=nm.float64)
        e1 = nm.ones((nnod,), dtype=nm.float64)

        ones = nm.zeros((dim,), dtype=nm.object)
        clist = []
        for ir in range(dim):
            aux = e00.copy()
            aux[:,ir] = e1
            ones[ir] = {var_name : nm.ascontiguousarray(aux)}
            clist.append('pi_%d' % (ir,))

        corr_sol = CorrSolution(name=self.name,
                                states=ones,
                                components=clist)
        return corr_sol

class CopyData(CorrMiniApp):

    def __call__(self, problem=None, data=None):
        problem = get_default(problem, self.problem)
        var_name = self.variable
        clist = ['data']
        dn = self.data

        if type(dn) is list:
            data = problem
            for ii in dn:
                data = data.get(ii, 'None')
        else:
            data = problem.get(dn, 'None')

        if type(data) is dict:
            corr_sol = CorrSolution(name=self.name,
                                    state=data,
                                    components=clist)
        else:
            if data.dtype == nm.object:
                corr_sol = CorrSolution(name=self.name,
                                        states=data,
                                        components=clist)


            else:
                ndof, ndim = data.shape
                state = {var_name: data.reshape((ndof * ndim,))}
                corr_sol = CorrSolution(name=self.name,
                                        state=state,
                                        components=clist)

        return corr_sol

class CorrNN(CorrMiniApp):
    """ __init__() kwargs:
        {
             'ebcs' : [],
             'epbcs' : [],
             'equations' : {},
             'set_variables' : None,
        },
    """

    def set_variables_default(variables, ir, ic, set_var, data):
        for (var, req, comp) in set_var:
            variables[var].set_data(data[req].states[ir,ic][comp])

    set_variables_default = staticmethod(set_variables_default)

    def __init__(self, name, problem, kwargs):
        """When dim is not in kwargs, problem dimension is used."""
        CorrMiniApp.__init__(self, name, problem, kwargs)
        self.set_default('dim', problem.get_dim())

    def __call__(self, problem=None, data=None):
        problem = get_default(problem, self.problem)

        problem.set_equations(self.equations)

        problem.select_bcs(ebc_names=self.ebcs, epbc_names=self.epbcs,
                           lcbc_names=self.get('lcbcs', []))

        problem.update_materials(problem.ts)

        self.init_solvers(problem)

        variables = problem.get_variables()

        states = nm.zeros((self.dim, self.dim), dtype=nm.object)
        clist = []
        for ir in range(self.dim):
            for ic in range(self.dim):
                if isinstance(self.set_variables, list):
                    self.set_variables_default(variables, ir, ic,
                                               self.set_variables, data)
                else:
                    self.set_variables(variables, ir, ic, **data)

                state = problem.solve()
                assert_(state.has_ebc())
                states[ir,ic] = state.get_parts()

                clist.append((ir, ic))

        corr_sol = CorrSolution(name=self.name,
                                states=states,
                                components=clist)

        self.save(corr_sol, problem)

        return corr_sol

class CorrN(CorrMiniApp):

    def set_variables_default(variables, ir, set_var, data):
        for (var, req, comp) in set_var:
            variables[var].set_data(data[req].states[ir][comp])

    set_variables_default = staticmethod(set_variables_default)

    def __init__(self, name, problem, kwargs):
        """When dim is not in kwargs, problem dimension is used."""
        CorrMiniApp.__init__(self, name, problem, kwargs)
        self.set_default('dim', problem.get_dim())

    def __call__(self, problem=None, data=None):
        problem = get_default(problem, self.problem)

        problem.set_equations(self.equations)

        problem.select_bcs(ebc_names=self.ebcs, epbc_names=self.epbcs,
                           lcbc_names=self.get('lcbcs', []))

        problem.update_materials(problem.ts)

        self.init_solvers(problem)

        variables = problem.get_variables()

        states = nm.zeros((self.dim,), dtype=nm.object)
        clist = []
        for ir in range(self.dim):
            if isinstance(self.set_variables, list):
                self.set_variables_default(variables, ir,
                                           self.set_variables, data)
            else:
                self.set_variables(variables, ir, **data)
            state = problem.solve()
            assert_(state.has_ebc())
            states[ir] = state.get_parts()

            clist.append((ir,))

        corr_sol = CorrSolution(name=self.name,
                                states=states,
                                components=clist)

        self.save(corr_sol, problem)

        return corr_sol

class CorrDimDim(CorrNN):
    pass

class CorrDim(CorrN):
    pass

class CorrOne(CorrMiniApp):

    def set_variables_default(variables, set_var, data):
        for (var, req, comp) in set_var:
            variables[var].set_data(data[req].state[comp])

    set_variables_default = staticmethod(set_variables_default)

    def __call__(self, problem=None, data=None):
        problem = get_default(problem, self.problem)

        problem.set_equations(self.equations)

        problem.select_bcs(ebc_names=self.ebcs, epbc_names=self.epbcs,
                           lcbc_names=self.get('lcbcs', []))

        problem.update_materials(problem.ts)

        self.init_solvers(problem)

        variables = problem.get_variables()

        if hasattr(self, 'set_variables'):
            if isinstance(self.set_variables, list):
                self.set_variables_default(variables, self.set_variables,
                                           data)
            else:
                self.set_variables(variables, **data)

        state = problem.solve()
        assert_(state.has_ebc())

        corr_sol = CorrSolution(name=self.name,
                                state=state.get_parts())

        self.save(corr_sol, problem)

        return corr_sol

class CorrSetBCS(CorrMiniApp):

    def __call__(self, problem=None, data=None):
        from sfepy.base.base import select_by_names
        from sfepy.discrete.variables import Variables
        from sfepy.discrete.state import State
        from sfepy.discrete.conditions import Conditions

        problem = get_default(problem, self.problem)

        conf_ebc = select_by_names(problem.conf.ebcs, self.ebcs)
        conf_epbc = select_by_names(problem.conf.epbcs, self.epbcs)
        ebcs = Conditions.from_conf(conf_ebc, problem.domain.regions)
        epbcs = Conditions.from_conf(conf_epbc, problem.domain.regions)

        conf_variables = select_by_names(problem.conf.variables, self.variable)
        problem.set_variables(conf_variables)
        variables = Variables.from_conf(conf_variables, problem.fields)
        variables.equation_mapping(ebcs, epbcs, problem.ts, problem.functions)
        state = State(variables)
        state.fill(0.0)
        state.apply_ebc()

        corr_sol = CorrSolution(name=self.name,
                                state=state.get_parts())

        self.save(corr_sol, problem, variables)

        return corr_sol

class CorrEqPar(CorrOne):
    """
    The corrector which equation can be parametrized via 'eq_pars',
    the dimension is given by the number of parameters.

    Example:

        'equations': 'dw_diffusion.5.Y(mat.k, q, p) =
                      dw_surface_integrate.5.%s(q)',
        'eq_pars': ('bYMp', 'bYMm'),
        'class': cb.CorrEqPar,

    """

    def __init__(self, name, problem, kwargs):
        """When dim is not in kwargs, problem dimension is used."""
        CorrMiniApp.__init__(self, name, problem, kwargs)
        self.set_default('dim', len(self.eq_pars))

    def __call__(self, problem=None, data=None):
        problem = get_default(problem, self.problem)

        states = nm.zeros((self.dim,), dtype=nm.object)
        clist = []

        eqns ={}
        for ir in range(self.dim):
            for key_eq, val_eq in self.equations.iteritems():
                eqns[key_eq] = val_eq % self.eq_pars[ir]

            problem.set_equations(eqns)

            problem.select_bcs(ebc_names=self.ebcs, epbc_names=self.epbcs,
                               lcbc_names=self.get('lcbcs', []))

            problem.update_materials(problem.ts)

            self.init_solvers(problem)

            variables = problem.get_variables()

            if hasattr(self, 'set_variables'):
                if isinstance(self.set_variables, list):
                    self.set_variables_default(variables, self.set_variables,
                                               data)
                else:
                    self.set_variables(variables, **data)

            state = problem.solve()
            assert_(state.has_ebc())

            states[ir] = state.get_parts()
            clist.append((ir,))

        corr_sol = CorrSolution(name=self.name,
                                states=states,
                                components=clist)

        self.save(corr_sol, problem)

        return corr_sol

class PressureEigenvalueProblem(CorrMiniApp):
    """Pressure eigenvalue problem solver for time-dependent correctors."""

    def presolve(self, mtx):
        """Prepare A^{-1} B^T for the Schur complement."""

        mtx_a = mtx['A']
        mtx_bt = mtx['BT']
        output('full A size: %.3f MB' % (8.0 * nm.prod(mtx_a.shape) / 1e6))
        output('full B size: %.3f MB' % (8.0 * nm.prod(mtx_bt.shape) / 1e6))

        ls = Solver.any_from_conf(self.problem.ls_conf,
                                   presolve=True, mtx=mtx_a)
        if self.mode == 'explicit':
            tt = time.clock()
            mtx_aibt = nm.zeros(mtx_bt.shape, dtype=mtx_bt.dtype)
            for ic in xrange(mtx_bt.shape[1]):
                mtx_aibt[:,ic] = ls(mtx_bt[:,ic].toarray().squeeze())
            output('mtx_aibt: %.2f s' % (time.clock() - tt))
            action_aibt = MatrixAction.from_array(mtx_aibt)
        else:
            ##
            # c: 30.08.2007, r: 13.02.2008
            def fun_aibt(vec):
                # Fix me for sparse mtx_bt...
                rhs = sc.dot(mtx_bt, vec)
                out = ls(rhs)
                return out
            action_aibt = MatrixAction.from_function(fun_aibt,
                                                    (mtx_a.shape[0],
                                                     mtx_bt.shape[1]),
                                                    nm.float64)
        mtx['action_aibt'] = action_aibt

    def solve_pressure_eigenproblem(self, mtx, eig_problem=None,
                                    n_eigs=0, check=False):
        """G = B*AI*BT or B*AI*BT+D"""

        def get_slice(n_eigs, nn):
            if n_eigs > 0:
                ii = slice(0, n_eigs)
            elif n_eigs < 0:
                ii = slice(nn + n_eigs, nn)
            else:
                ii = slice(0, 0)
            return ii

        eig_problem = get_default(eig_problem, self.eig_problem)
        n_eigs = get_default(n_eigs, self.n_eigs)
        check = get_default(check, self.check)

        mtx_c, mtx_b, action_aibt = mtx['C'], mtx['B'], mtx['action_aibt']
        mtx_g = mtx_b * action_aibt.to_array() # mtx_b must be sparse!
        if eig_problem == 'B*AI*BT+D':
            mtx_g += mtx['D'].toarray()

        mtx['G'] = mtx_g
        output(mtx_c.shape, mtx_g.shape)

        eigs, mtx_q = eig(mtx_c.toarray(), mtx_g, method='eig.sgscipy')

        if check:
            ee = nm.diag(sc.dot(mtx_q.T * mtx_c, mtx_q)).squeeze()
            oo = nm.diag(sc.dot(sc.dot(mtx_q.T,  mtx_g), mtx_q)).squeeze()
            try:
                assert_(nm.allclose(ee, eigs))
                assert_(nm.allclose(oo, nm.ones_like(eigs)))
            except ValueError:
                debug()

        nn = mtx_c.shape[0]
        if isinstance(n_eigs, tuple):
            output('required number of eigenvalues: (%d, %d)' % n_eigs)
            if sum(n_eigs) < nn:
                ii0 = get_slice(n_eigs[0], nn)
                ii1 = get_slice(-n_eigs[1], nn)
                eigs = nm.concatenate((eigs[ii0], eigs[ii1]))
                mtx_q = nm.concatenate((mtx_q[:,ii0], mtx_q[:,ii1]), 1) 
        else:
            output('required number of eigenvalues: %d' % n_eigs)
            if (n_eigs != 0) and (abs(n_eigs) < nn):
                ii = get_slice(n_eigs, nn)
                eigs = eigs[ii]
                mtx_q = mtx_q[:,ii]

##         from sfepy.base.plotutils import pylab, iplot
##         pylab.semilogy(eigs)
##         pylab.figure(2)
##         iplot(eigs)
##         pylab.show()
##         debug()

        out = Struct(eigs=eigs, mtx_q=mtx_q)
        return out

    def __call__(self, problem=None, data=None):
        problem = get_default(problem, self.problem)

        problem.set_equations(self.equations)
        problem.select_bcs(ebc_names=self.ebcs, epbc_names=self.epbcs,
                           lcbc_names=self.get('lcbcs', []))
        problem.update_materials()

        mtx = problem.equations.eval_tangent_matrices(problem.create_state()(),
                                                      problem.mtx_a,
                                                      by_blocks=True)
        self.presolve(mtx)

        evp = self.solve_pressure_eigenproblem(mtx)
        return Struct(name=self.name, ebcs=self.ebcs, epbcs=self.epbcs,
                      mtx=mtx, evp=evp)

class TCorrectorsViaPressureEVP(CorrMiniApp):
    """
    Time correctors via the pressure eigenvalue problem.
    """

    def setup_equations(self, equations, problem=None):
        """
        Set equations, update boundary conditions and materials.
        """
        problem = get_default(problem, self.problem)

        problem.set_equations(equations)
        problem.select_bcs(ebc_names=self.ebcs, epbc_names=self.epbcs,
                           lcbc_names=self.get('lcbcs', []))
        problem.update_materials() # Assume parameters constant in time.

    def compute_correctors(self, evp, sign, state0, ts, dump_name, save_name,
                           problem=None, vec_g=None):
        problem = get_default(problem, self.problem)

        eigs = evp.evp.eigs
        mtx_q = evp.evp.mtx_q
        mtx = evp.mtx

        nr, nc = mtx_q.shape

        if vec_g is not None:
            output('nonzero pressure EBC: max = %e, min = %e' \
                    % (vec_g.max(), vec_g.min()))
            one = nm.ones((nc,), dtype=nm.float64)

        vu, vp = self.dump_variables

        variables = problem.get_variables()
        var_u = variables[vu]
        var_p = variables[vp]

        ##
        # follow_epbc = False -> R1 = - R2 as required. ? for other correctors?
        vec_p0 = sign * var_p.get_reduced(state0[vp], follow_epbc=False)
##         print state0
##         print vec_p0
##         print vec_p0.min(), vec_p0.max(), nla.norm(vec_p0)
##         debug()

        # xi0 = Q^{-1} p(0) = Q^T G p(0)
        vec_xi0 = sc.dot(mtx_q.T, sc.dot(mtx['G'],
                                           vec_p0[:,nm.newaxis])).squeeze()
        action_aibt = mtx['action_aibt']

        e_e_qg = 0.0
        iee_e_qg = 0.0
        format = '====== time %%e (step %%%dd of %%%dd) ====='\
                 % ((ts.n_digit,) * 2)
        for step, time in ts:
            output(format % (time, step + 1, ts.n_step))

            e_e = nm.exp(- eigs * time)
            e_e_qp = e_e * vec_xi0 # exp(-Et) Q^{-1} p(0)

            if vec_g is not None:
                Qg = sc.dot(mtx_q.T, vec_g)
                e_e_qg = e_e * Qg
                iee_e_qg = ((one - e_e) / eigs) * Qg

            vec_p = sc.dot(mtx_q, e_e_qp + iee_e_qg)
            vec_dp = - sc.dot(mtx_q, (eigs * e_e_qp - e_e_qg))
            vec_u = action_aibt(vec_dp)
##             bbb = sc.dot(vec_dp.T, - mtx['C'] * vec_p0)

            vec_u = var_u.get_full(vec_u)
            vec_p = var_p.get_full(vec_p)
            # BC nodes - time derivative of constant is zero!
            vec_dp = var_p.get_full(vec_dp, force_value=0.0)
##             aaa = sc.dot(vec_xi0.T, eigs * (eigs * e_e_qp))
##             print aaa
##             print bbb

            self.save(dump_name, save_name, vec_u, vec_p, vec_dp, ts, problem)

    def save(self, dump_name, save_name, vec_u, vec_p, vec_dp, ts, problem):
        """
        1. saves raw correctors into hdf5 files (filename)
        2. saves correctors transformed to output for visualization
        """
        vu, vp = self.dump_variables
        out = {vu : Struct(name='dump', mode='nodes', data=vec_u,
                           dofs=None, var_name=vu),
               vp : Struct(name='dump', mode='nodes', data=vec_p,
                           dofs=None, var_name=vp),
               'd'+vp : Struct(name='dump', mode='nodes', data=vec_dp,
                               dofs=None, var_name=vp)}

        problem.save_state(dump_name, out=out, file_per_var=False, ts=ts)

        # For visualization...
        out = {}
        extend = not self.file_per_var

        variables = self.problem.get_variables()
        to_output = variables.state_to_output

        out.update(to_output(vec_u, var_info={vu : (True, vu)},
                             extend=extend))
        out.update(to_output(vec_p, var_info={vp : (True, vp)},
                             extend=extend))
        out.update(to_output(vec_dp, var_info={vp : (True, 'd'+vp)},
                             extend=extend))
        if self.post_process_hook is not None:
            out = self.post_process_hook(out, problem,
                                          {vu : vec_u,
                                           vp : vec_p, 'd'+vp : vec_dp},
                                          extend=extend)
        problem.save_state(save_name, out=out,
                            file_per_var=self.file_per_var, ts=ts)

    def verify_correctors(self, sign, state0, filename, problem=None):

        problem = get_default(problem, self.problem)

        io = HDF5MeshIO(filename)
        ts = TimeStepper(*io.read_time_stepper())

        ts.set_step(0)
        problem.equations.init_time(ts)

        variables = self.problem.get_variables()

        vu, vp = self.dump_variables
        vdp = self.verify_variables[-1]

        p0 = sign * state0[vp]

        format = '====== time %%e (step %%%dd of %%%dd) ====='\
                 % ((ts.n_digit,) * 2)
        vv = variables
        ok = True
        for step, time in ts:
            output(format % (time, step + 1, ts.n_step))

            data = io.read_data(step)
            if step == 0:
                assert_(nm.allclose(data[vp].data, p0))

            state0 = problem.create_state()
            state0.set_full(data[vu].data, vu)
            state0.set_full(data[vp].data, vp)
            vv[vdp].set_data(data['d'+vp].data)

            problem.update_time_stepper(ts)
            state = problem.solve(state0)
            state, state0 = state(), state0()
            err = nla.norm(state - state0) / nla.norm(state0)
            output(state.min(), state.max())
            output(state0.min(), state0.max())
            output('>>>>>', err)

            ok = ok and (err < 1e-12)
            problem.advance(ts)

        return ok

class CoefDummy(MiniAppBase):
    """
    Dummy class serving for computing and returning its requirements.
    """

    def __call__(self, volume=None, problem=None, data=None):
        return data

class TSTimes(MiniAppBase):
    """Coefficient-like class, returns times of the time stepper."""
    def __call__(self, volume=None, problem=None, data=None):
        problem = get_default(problem, self.problem)
        return problem.get_time_solver().ts.times

class VolumeFractions(MiniAppBase):
    """Coefficient-like class, returns volume fractions of given regions within
    the whole domain."""
    def __call__(self, volume=None, problem=None, data=None):
        problem = get_default(problem, self.problem)

        vf = {}
        for region_name in self.regions:
            vkey = 'volume_%s' % region_name
            key = 'fraction_%s' % region_name

            equations, variables = problem.create_evaluable(self.expression % region_name)
            val = eval_equations(equations, variables)

            vf[vkey] = nm.asarray(val, dtype=nm.float64)
            vf[key] = vf[vkey] / self._get_volume(volume)

        return vf

class CoefSymSym(MiniAppBase):

    def set_variables_default(variables, ir, ic, mode, set_var, data):
        def get_corr_state(corr, ir, ic):
            if hasattr(corr, 'states'):
                return corr.states[ir,ic]

            else:
                return corr.state

        mode2var = {'row' : 0, 'col' : 1}
        idx = mode2var[mode]

        for (var, req, comp) in [set_var[idx]] + set_var[2:]:
            if type(req) is tuple:
                val = get_corr_state(data[req[0]], ir, ic)[comp].copy()
                for ii in req[1:]:
                    val += get_corr_state(data[ii], ir, ic)[comp]

            else:
                val = get_corr_state(data[req], ir, ic)[comp]

            variables[var].set_data(val)

    set_variables_default = staticmethod(set_variables_default)

    def __call__(self, volume, problem=None, data=None):
        problem = get_default(problem, self.problem)

        dim, sym = problem.get_dim(get_sym=True)
        coef = nm.zeros((sym, sym), dtype=self.dtype)

        term_mode = self.term_mode
        equations, variables = problem.create_evaluable(self.expression,
                                                        term_mode=term_mode)

        for ir, (irr, icr) in enumerate(iter_sym(dim)):
            if isinstance(self.set_variables, list):
                self.set_variables_default(variables, irr, icr, 'row',
                                           self.set_variables, data)
            else:
                self.set_variables(variables, irr, icr, 'row', **data)

            for ic, (irc, icc) in enumerate(iter_sym(dim)):
                if isinstance(self.set_variables, list):
                    self.set_variables_default(variables, irc, icc, 'col',
                                               self.set_variables, data)
                else:
                    self.set_variables(variables, irc, icc, 'col', **data)

                val = eval_equations(equations, variables,
                                     term_mode=term_mode)

                coef[ir,ic] = val

        coef /= self._get_volume(volume)

        return coef

class CoefFMSymSym(MiniAppBase):
    """
    Fading memory sym x sym coefficients.
    """

    def __call__(self, volume, problem=None, data=None):
        problem = get_default(problem, self.problem)

        dim, sym = problem.get_dim(get_sym=True)

        filename = self.set_variables(None, None, None, 0, 0,
                                      'filename', **data)
        ts = TimeStepper(*HDF5MeshIO(filename).read_time_stepper())

        coef = nm.zeros((ts.n_step, sym, sym), dtype=self.dtype)

        term_mode = self.term_mode
        equations, variables = problem.create_evaluable(self.expression,
                                                        term_mode=term_mode)

        for ir, (irr, icr) in enumerate(iter_sym(dim)):
            filename = self.set_variables(None, None, None, irr, icr,
                                          'filename', **data)
            io = HDF5MeshIO(filename)

            for step, time in ts:
                self.set_variables(variables, io, step, None, None,
                                   'row', **data)

                for ic, (irc, icc) in enumerate(iter_sym(dim)):
                    self.set_variables(variables, None, None, irc, icc,
                                       'col', **data)

                    val = eval_equations(equations, variables,
                                         term_mode=term_mode)

                    coef[step,ir,ic] = val

        coef /= self._get_volume(volume)

        return coef

class CoefDimSym(MiniAppBase):

    def __call__(self, volume, problem=None, data=None):
        problem = get_default(problem, self.problem)

        dim, sym = problem.get_dim(get_sym=True)
        coef = nm.zeros((dim, sym), dtype=self.dtype)

        term_mode = self.term_mode
        equations, variables = problem.create_evaluable(self.expression,
                                                        term_mode=term_mode)

        for ir in range(dim):
            self.set_variables(variables, ir, None, 'row', **data)

            for ic, (irc, icc) in enumerate(iter_sym(dim)):
                self.set_variables(variables, irc, icc, 'col', **data)

                val = eval_equations(equations, variables,
                                     term_mode=term_mode)

                coef[ir,ic] = val

        coef /= self._get_volume(volume)

        return coef

class CoefNN(MiniAppBase):

    def set_variables_default(variables, ir, ic, mode, set_var, data):
        def get_corr_state(corr, ii):
            if hasattr(corr, 'states'):
                return corr.states[ii]

            else:
                return corr.state

        mode2var = {'row' : 0, 'col' : 1}

        if mode == 'col':
            ir = ic
        idx = mode2var[mode]

        for (var, req, comp) in [set_var[idx]] + set_var[2:]:
            if type(req) is tuple:
                val = get_corr_state(data[req[0]], ir)[comp].copy()
                for ii in req[1:]:
                    val += get_corr_state(data[ii], ir)[comp]

            else:
                val = get_corr_state(data[req], ir)[comp]

            variables[var].set_data(val)

    set_variables_default = staticmethod(set_variables_default)

    def __init__(self, name, problem, kwargs):
        """When dim is not in kwargs, problem dimension is used."""
        MiniAppBase.__init__(self, name, problem, kwargs)
        self.set_default('dim', problem.get_dim())

    def __call__(self, volume, problem=None, data=None):
        problem = get_default(problem, self.problem)

        coef = nm.zeros((self.dim, self.dim), dtype=self.dtype)

        term_mode = self.term_mode
        equations, variables = problem.create_evaluable(self.expression,
                                                        term_mode=term_mode)

        if isinstance(self.set_variables, list):
            for ir in range(self.dim):
                self.set_variables_default(variables, ir, None, 'row',
                                           self.set_variables, data)
                for ic in range(self.dim):
                    self.set_variables_default(variables, None, ic, 'col',
                                               self.set_variables, data)
                    val = eval_equations(equations, variables,
                                         term_mode=term_mode)
                    coef[ir,ic] = val
        else:
            for ir in range(self.dim):
                self.set_variables(variables, ir, None, 'row', **data)
                for ic in range(self.dim):
                    self.set_variables(variables, None, ic, 'col', **data)
                    val = eval_equations(equations, variables,
                                         term_mode=term_mode)
                    coef[ir,ic] = val

        coef /= self._get_volume(volume)

        return coef

class CoefN(MiniAppBase):

    def set_variables_default(variables, ir, set_var, data):
        def get_corr_state(corr, ii):
            if hasattr(corr, 'states'):
                return corr.states[ii]

            else:
                return corr.state

        for (var, req, comp) in set_var:
            if type(req) is tuple:
                val = get_corr_state(data[req[0]], ir)[comp].copy()
                for ii in req[1:]:
                    val += get_corr_state(data[ii], ir)[comp]

            else:
                val = get_corr_state(data[req], ir)[comp]

            variables[var].set_data(val)

    set_variables_default = staticmethod(set_variables_default)

    def __init__(self, name, problem, kwargs):
        """When dim is not in kwargs, problem dimension is used."""
        MiniAppBase.__init__(self, name, problem, kwargs)
        self.set_default('dim', problem.get_dim())

    def __call__(self, volume, problem=None, data=None):
        problem = get_default(problem, self.problem)

        coef = nm.zeros((self.dim,), dtype=self.dtype)
        term_mode = self.term_mode
        equations, variables = problem.create_evaluable(self.expression,
                                                        term_mode=term_mode)

        for ir in range(self.dim):
            if isinstance(self.set_variables, list):
                self.set_variables_default(variables, ir, self.set_variables,
                                           data)
            else:
                self.set_variables(variables, ir, **data)

            val = eval_equations(equations, variables,
                                 term_mode=term_mode)
            coef[ir] = val

        coef /= self._get_volume(volume)

        return coef

class CoefDimDim(CoefNN):
    pass

class CoefDim(CoefN):
    pass

class CoefSym(MiniAppBase):
    def set_variables_default(variables, ir, ic, mode, set_var, data):
        def get_corr_state(corr, ir, ic):
            if hasattr(corr, 'states'):
                return corr.states[ir,ic]

            else:
                return corr.state

        if mode == 'row':

            for (var, req, comp) in set_var:
                if type(req) is tuple:
                    val = get_corr_state(data[req[0]], ir, ic)[comp].copy()
                    for ii in req[1:]:
                        val += get_corr_state(data[ii], ir, ic)[comp]

                else:
                    val = get_corr_state(data[req], ir, ic)[comp]

                variables[var].set_data(val)

    set_variables_default = staticmethod(set_variables_default)

    def __call__(self, volume, problem=None, data=None):
        problem = get_default(problem, self.problem)

        dim, sym = problem.get_dim(get_sym=True)
        coef = nm.zeros((sym,), dtype=self.dtype)

        term_mode = self.term_mode
        equations, variables = problem.create_evaluable(self.expression,
                                                        term_mode=term_mode)

        if isinstance(self.set_variables, list):
            self.set_variables_default(variables, None, None, 'col',
                                       self.set_variables, data)

        else:
            self.set_variables(variables, None, None, 'col', **data)

        for ii, (ir, ic) in enumerate(iter_sym(dim)):
            if isinstance(self.set_variables, list):
                self.set_variables_default(variables, ir, ic, 'row',
                                           self.set_variables, data)

            else:
                self.set_variables(variables, ir, ic, 'row', **data)

            val = eval_equations(equations, variables,
                                 term_mode=term_mode)
            coef[ii] = val

        coef /= self._get_volume(volume)

        return coef

class CoefFMSym(MiniAppBase):
    """
    Fading memory sym coefficients.
    """

    def __call__(self, volume, problem=None, data=None):
        problem = get_default(problem, self.problem)

        dim, sym = problem.get_dim(get_sym=True)

        filename = self.set_variables(None, 0, 0, 'filename', **data)
        ts = TimeStepper(*HDF5MeshIO(filename).read_time_stepper())

        coef = nm.zeros((ts.n_step, sym), dtype=self.dtype)

        term_mode = self.term_mode
        equations, variables = problem.create_evaluable(self.expression,
                                                        term_mode=term_mode)

        self.set_variables(variables, None, None, 'col', **data)

        for ii, (ir, ic) in enumerate(iter_sym(dim)):
            filename = self.set_variables(None, ir, ic, 'filename', **data)
            io = HDF5MeshIO(filename)
            for step, time in ts:
                self.set_variables(variables, io, step, 'row', **data)

                val = eval_equations(equations, variables,
                                     term_mode=term_mode)

                coef[step,ii] = val

        coef /= self._get_volume(volume)

        return coef

class CoefOne(MiniAppBase):

    def set_variables_default(variables, set_var, data):
        for (var, req, comp) in set_var:
            variables[var].set_data(data[req].state[comp])

    set_variables_default = staticmethod(set_variables_default)

    def __call__(self, volume, problem=None, data=None):
        problem = get_default(problem, self.problem)

        term_mode = self.term_mode
        equations, variables = problem.create_evaluable(self.expression,
                                                        term_mode=term_mode)

        if isinstance(self.set_variables, list):
            self.set_variables_default(variables, self.set_variables,
                                       data)
        else:
            self.set_variables(variables, **data)

        val = eval_equations(equations, variables,
                             term_mode=term_mode)

        coef = val / self._get_volume(volume)

        return coef

class CoefFMOne(MiniAppBase):
    """
    Fading memory scalar coefficients.
    """

    def __call__(self, volume, problem=None, data=None):
        problem = get_default(problem, self.problem)

        filename = self.set_variables(None, None, None, 'filename', **data)
        io = HDF5MeshIO(filename)
        ts = TimeStepper(*io.read_time_stepper())

        coef = nm.zeros((ts.n_step, 1), dtype=self.dtype)

        term_mode = self.term_mode
        equations, variables = problem.create_evaluable(self.expression,
                                                        term_mode=term_mode)

        self.set_variables(variables, None, None, 'col', **data)

        for step, time in ts:
            self.set_variables(variables, io, step, 'row', **data)

            val = eval_equations(equations, variables,
                                 term_mode=term_mode)

            coef[step] = val

        coef /= self._get_volume(volume)

        return coef

class CoefSum(MiniAppBase):

    def __call__(self, volume, problem=None, data=None):

        coef = nm.zeros_like(data[self.requires[0]])
        for i in range(len(self.requires)):
            coef += data[self.requires[i]]

        return coef

class CoefEval(MiniAppBase):
    """
    Evaluate expression.
    """
    def __call__(self, volume, problem=None, data=None):

        expr = self.expression
        for i in range(len(self.requires)):
            expr = expr.replace(self.requires[i],
                                "data['%s']" % self.requires[i])

        coef = eval(expr)

        return coef

class CoefNone(MiniAppBase):

    def __call__(self, volume, problem=None, data=None):

        coef = 0.0

        return coef

class CoefExprPar(MiniAppBase):
    """
    The coefficient which expression can be parametrized via 'expr_pars',
    the dimension is given by the number of parameters.

    Example:

        'expression': 'dw_surface_ndot.5.Ys(mat_norm.k%d, corr1)',
        'expr_pars': [ii for ii in range(dim)],
        'class': cb.CoefExprPar,

    """
    def set_variables_default(variables, ir, set_var, data):
        for (var, req, comp) in set_var:
            if hasattr(data[req], 'states'):
                variables[var].set_data(data[req].states[ir][comp])

            else:
                variables[var].set_data(data[req].state[comp])

    set_variables_default = staticmethod(set_variables_default)

    def __init__(self, name, problem, kwargs):
        """When dim is not in kwargs, problem dimension is used."""
        MiniAppBase.__init__(self, name, problem, kwargs)
        dim = len(self.expr_pars)
        self.set_default('dim', dim)

    def __call__(self, volume, problem=None, data=None):
        problem = get_default(problem, self.problem)

        coef = nm.zeros((self.dim,), dtype=self.dtype)
        term_mode = self.term_mode

        for ir in range(self.dim):
            expression = self.expression % self.expr_pars[ir]
            equations, variables = \
              problem.create_evaluable(expression, term_mode=term_mode)

            if isinstance(self.set_variables, list):
                self.set_variables_default(variables, ir, self.set_variables,
                                           data)
            else:
                self.set_variables(variables, ir, **data)

            val = eval_equations(equations, variables,
                                 term_mode=term_mode)
            coef[ir] = val

        coef /= self._get_volume(volume)

        return coef

########NEW FILE########
__FILENAME__ = coefs_elastic
import numpy as nm

from sfepy.base.base import output, assert_, get_default, Struct
from sfepy.homogenization.coefs_base import CoefOne, CorrDim, \
     TCorrectorsViaPressureEVP, \
     CoefFMSymSym, CoefFMSym, CoefFMOne, \
     CorrMiniApp, CorrSolution
from sfepy.discrete.fem.meshio import HDF5MeshIO
from sfepy.solvers.ts import TimeStepper

class CorrectorsPermeability( CorrDim ):

    def __call__( self, problem = None, data = None, save_hook = None ):
        problem = get_default( problem, self.problem )

        equations = {}
        for key, eq in self.equations.iteritems():
            equations[key] = eq % tuple( self.regions )

        index = [0]
        problem.set_equations( equations, user={self.index_name : index} )

        problem.select_bcs( ebc_names = self.ebcs, epbc_names = self.epbcs )
        problem.update_materials(problem.ts)

        self.init_solvers(problem)

        variables = problem.get_variables()

        dim = problem.get_dim()
        states = nm.zeros( (dim,), dtype = nm.object )
        clist = []
        for ir in range( dim ):
            index[0] = ir # Set the index - this is visible in the term.

            state = problem.solve()
            assert_(state.has_ebc())
            states[ir] = variables.get_state_parts()

            clist.append( (ir,) )

        corr_sol = CorrSolution(name = self.name,
                                states = states,
                                components = clist)

        self.save(corr_sol, problem)

        return corr_sol

class PressureRHSVector( CorrMiniApp ):
    
    def __call__( self, problem = None, data = None ):
        problem = get_default( problem, self.problem )

        problem.select_variables( self.variables )
        problem.set_equations( self.equations )
        problem.select_bcs( ebc_names = self.ebcs, epbc_names = self.epbcs )

        state = problem.create_state()
        state.apply_ebc()

        vec = eval_term_op( state, self.equations.values()[0],
                            problem, dw_mode = 'vector' )
##         print vec.max(), vec.min()

        return vec

class TCorrectorsRSViaPressureEVP( TCorrectorsViaPressureEVP ):

    def get_save_name_base( self ):
        return self.save_name + '_%d%d'

    def get_dump_name_base( self ):
        return self.dump_name + '_%d%d'

    def __call__( self, problem = None, data = None ):
        """data: corrs_rs, evp"""
        problem = get_default( problem, self.problem )
        ts = problem.get_time_solver().ts

        corrs, evp = [data[ii] for ii in self.requires]

        assert_( evp.ebcs == self.ebcs )
        assert_( evp.epbcs == self.epbcs )

        dim = problem.get_dim()

        filenames = nm.zeros( (dim, dim), dtype = nm.object )

        self.setup_equations(self.equations)

        solve = self.compute_correctors
        for ir in range( dim ):
            for ic in range( dim ):
                filenames[ir,ic] = self.get_dump_name() % (ir,ic)
                savename = self.get_save_name() % (ir,ic)
                solve(evp, -1.0, corrs.states[ir,ic], ts,
                      filenames[ir,ic], savename)


        if self.check:
            self.setup_equations(self.verify_equations)
            self.init_solvers(problem)

            output( 'verifying correctors %s...' % self.name )
            verify = self.verify_correctors
            ok = True
            for ir in range( dim ):
                for ic in range( dim ):
                    oo = verify(-1.0, corrs.states[ir,ic], filenames[ir,ic])
                    ok = ok and oo
            output( '...done, ok: %s' % ok )

        return Struct( name = self.name,
                       filenames = filenames )

class TCorrectorsPressureViaPressureEVP( TCorrectorsViaPressureEVP ):

    def get_save_name_base( self ):
        return self.save_name

    def get_dump_name_base( self ):
        return self.dump_name

    def __call__( self, problem = None, data = None, save_hook = None ):
        """data: corrs_pressure, evp, optionally vec_g"""
        problem = get_default( problem, self.problem )
        ts = problem.get_time_solver().ts

        corrs, evp = [data[ii] for ii in self.requires[:2]]
        if len(self.requires) == 3:
            vec_g = data[self.requires[2]]
        else:
            vec_g = None

        assert_( evp.ebcs == self.ebcs )
        assert_( evp.epbcs == self.epbcs )

        filename = self.get_dump_name()
        savename = self.get_save_name()

        self.setup_equations(self.equations)

        solve = self.compute_correctors
        solve(evp, 1.0, corrs.state, ts, filename, savename, vec_g=vec_g)

        if self.check:
            self.setup_equations(self.verify_equations)
            self.init_solvers(problem)

            output( 'verifying correctors %s...' % self.name )
            verify = self.verify_correctors
            ok = verify(1.0, corrs.state, filename)
            output( '...done, ok: %s' % ok )

        return Struct( name = self.name,
                       filename = filename )

class RBiotCoef( CoefFMSym ):
    """Homogenized fading memory Biot-like coefficient."""

    def get_filename( self, data, ir, ic ):
        tcorrs = data[self.requires[1]]
        self.ir, self.ic = ir, ic
        return tcorrs.filename

    def get_variables( self, problem, io, step, data, mode ):

        if mode == 'col':
            return
        
        else:
            pis = data[self.requires[0]]
            step_data = io.read_data( step )

            # omega.
            var_name = self.variables[0]
            c_name = problem.variables[var_name].primary_var_name
            yield var_name, step_data[c_name].data

            var_name = self.variables[1]
            yield var_name, pis.states[self.ir,self.ic]

            var_name = self.variables[2]
            c_name = problem.variables[var_name].primary_var_name
            pc = step_data['d'+c_name].data

            if self.ir == self.ic:
                yield (var_name, pc)
            else:
                yield (var_name, nm.zeros_like(pc))

class GBarCoef( CoefOne ):
    """
    Asymptotic Barenblatt coefficient.

    data = [p^{\infty}]

    Note:

    solving "dw_diffusion.i1.Y3( m.K, qc, pc ) = 0" solve, in fact
    "C p^{\infty} = \hat{C} \hat{\pi}" with the result "\hat{p^{\infty}}",
    where the rhs comes from E(P)BC.
    - it is preferable to computing directly by
    "\hat{p^{\infty}} = \hat{C^-1 \strip(\hat{C} \hat{\pi})}", as it checks
    explicitly the rezidual.
    """

    def __call__( self, volume, problem = None, data = None ):
        expression, region_name = self.expression

        problem = get_default( problem, self.problem )
        problem.select_variables( self.variables )
        problem.set_equations( {'eq' : expression} )
        problem.time_update( conf_ebc = {}, conf_epbc = {}, conf_lcbc = {} )

        pi_inf = data[self.requires[0]]

        coef = nm.zeros( (1,), dtype = nm.float64 )

        vec = eval_term_op( pi_inf.state, expression,
                            problem, new_geometries = False, dw_mode = 'vector' )

        reg = problem.domain.regions[region_name]
        field = problem.variables[self.variables[1]].field
        nods = field.get_dofs_in_region(reg, merge=True)
        coef[0] = vec[nods].sum()

        coef /= volume

        return coef

def eval_boundary_diff_vel_grad( problem, uc, pc, equation, region_name,
                                 pi = None ):
    get_state = problem.variables.get_state_part_view

    problem.set_equations( {'eq_2' : equation} )

    state = problem.create_state()
    state.set_full(uc, 'uc')
    state.set_full(pc, 'pc')
    if pi is not None:
        problem.variables['Pi'].set_data(pi)

    problem.time_update( conf_ebc = {}, conf_epbc = {} )

    state.apply_ebc()
    aux = problem.get_evaluator().eval_residual(state())

    pc = get_state( aux, 'pc', True )
    pc = problem.variables.make_full_vec( pc, 'pc', 0 )

    field = problem.variables['pc'].field

    reg = problem.domain.regions[region_name]
    nods = field.get_dofs_in_region(reg, merge=True)
    val = pc[nods].sum()

#    assert nm.all( problem.variables.di.ptr == problem.variables.adi.ptr )
#    problem.time_update() # Restore EBC.

    return val

class GPlusCoef( CoefFMOne ):

    def get_filename( self, data ):
        tcorrs = data[self.requires[0]]
        return tcorrs.filename

    def __call__( self, volume, problem = None, data = None ):
        expression, region_name, aux_eq = self.expression

        problem = get_default( problem, self.problem )
        problem.select_variables( self.variables )

        aux = self.get_filename( data )
        io = HDF5MeshIO( self.get_filename( data ) )
        ts = TimeStepper( *io.read_time_stepper() )

        coef = nm.zeros( (ts.n_step, 1), dtype = nm.float64 )
        
        for step, time in ts:
            step_data = io.read_data( step )

            var_name = self.variables[0]
            c_name = problem.variables[var_name].primary_var_name
            omega = step_data[c_name].data
            problem.variables[var_name].set_data(omega)

            val1 = eval_term_op( None, expression,
                                problem, call_mode = 'd_eval' )

            pc = step_data[self.variables[3]].data
            val2 = eval_boundary_diff_vel_grad( problem, omega, pc, aux_eq,
                                                region_name )
            coef[step] = val1 + val2

        coef /= volume

        return coef

########NEW FILE########
__FILENAME__ = coefs_perfusion
import numpy as nm

from sfepy.base.base import assert_, get_default, Struct
from sfepy.homogenization.coefs_base import CorrMiniApp, CoefN

class CorrRegion( CorrMiniApp ):

    def __init__( self, name, problem, kwargs ):
        CorrMiniApp.__init__( self, name, problem, kwargs )
        self.set_default('Nreg', len(self.regions.values()[0]))
        self.set_default('ebcs_list', False)

    def get_variables( self, ir, data ):
        return iter([])
        
    def __call__( self, problem = None, data = None ):
        problem = get_default( problem, self.problem )

        states = nm.zeros( (self.Nreg,), dtype = nm.object )
        clist = []
        for ir in range( self.Nreg ):

            problem.select_variables( self.variables )
            
            for name, val in self.get_variables( ir, data ):
                problem.variables[name].set_data(val)

            equations = {}
        
            for keye, vale in self.equations.iteritems():
                for keyr, valr in self.regions.iteritems():
                    vale = vale.replace( keyr, valr[ir] )
            
                equations[keye] = vale    

            problem.set_equations( equations )

            if ( self.ebcs_list ):
                problem.select_bcs( ebc_names = self.ebcs[ir], epbc_names = self.epbcs )
            else:
                problem.select_bcs( ebc_names = self.ebcs, epbc_names = self.epbcs )

            self.init_solvers(problem)

            state = problem.solve()
            assert_(state.has_ebc())
            states[ir] = state()
            clist.append( (ir,) )

        self.save( states, problem, clist )

        return Struct( name = self.name,
                       states = states,
                       di = problem.variables.di )

class CoefRegion( CoefN ):

    def __init__( self, name, problem, kwargs ):
        CoefN.__init__( self, name, problem, kwargs )
        self.corr_dim = len(kwargs['regions'].values()[0])
        
    def get_variables( self, problem, ir, data ):

        corr = data[self.requires[-1]]
        yield (self.variables[0], corr.states[ir])


########NEW FILE########
__FILENAME__ = coefs_phononic
import time

import numpy as nm
import numpy.linalg as nla
import scipy as sc

from sfepy.base.base import output, get_default, dict_to_struct, assert_, Struct
from sfepy.solvers import eig, Solver
from sfepy.linalg import norm_l2_along_axis
from sfepy.discrete.evaluate import eval_equations
from sfepy.homogenization.coefs_base import MiniAppBase, CorrMiniApp
from sfepy.homogenization.utils import coor_to_sym

def compute_eigenmomenta(em_equation, var_name, problem, eig_vectors,
                         transform=None):
    """
    Compute the eigenmomenta corresponding to given eigenvectors.
    """
    n_dof, n_eigs = eig_vectors.shape

    equations, variables = problem.create_evaluable(em_equation)
    var = variables[var_name]

    n_c = var.n_components
    eigenmomenta = nm.empty((n_eigs, n_c), dtype=nm.float64)

    for ii in xrange(n_eigs):

        if transform is None:
            vec_phi, is_zero = eig_vectors[:,ii], False

        else:
            vec_phi, is_zero = transform(eig_vectors[:,ii], (n_dof / n_c, n_c))

        if is_zero:
            eigenmomenta[ii, :] = 0.0

        else:
            var.set_data(vec_phi.copy())

            val = eval_equations(equations, variables)

            eigenmomenta[ii, :] = val

    return eigenmomenta

def get_ranges(freq_range, eigs):
    """
    Get an eigenvalue range slice and a corresponding initial frequency range
    within a given frequency range.
    """
    mine, maxe = freq_range
    ii = nm.where((eigs > (mine**2.)) & (eigs < (maxe**2.)))[0]
    freq_range_initial = nm.sqrt(eigs[ii])
    eig_range = (ii[0], ii[-1] + 1) # +1 as it is a slice.
    eig_range = slice(*eig_range)

    return freq_range_initial, eig_range

def cut_freq_range(freq_range, eigs, valid, freq_margins, eig_range,
                   fixed_freq_range, freq_eps):
    """
    Cut off masked resonance frequencies. Margins are preserved, like no
    resonances were cut.

    Returns
    -------
    freq_range : array
        The new range of frequencies.
    freq_range_margins : array
        The range of frequencies with prepended/appended margins equal to
        `fixed_freq_range` if it is not None.
    """
    n_eigs = eigs.shape[0]

    output('masked resonance frequencies in range:')
    output(nm.where(valid[eig_range] == False)[0])

    if fixed_freq_range is None:
        min_freq, max_freq = freq_range[0], freq_range[-1]
        margins = freq_margins * (max_freq - min_freq)
        prev_freq = min_freq - margins[0]
        next_freq = max_freq + margins[1]

        if eig_range.start > 0:
            prev_freq = max(nm.sqrt(eigs[eig_range.start - 1]) + freq_eps,
                            prev_freq)

        if eig_range.stop < n_eigs:
            next_freq = min(nm.sqrt(eigs[eig_range.stop]) - freq_eps,
                            next_freq)

        prev_freq = max(freq_eps, prev_freq)
        next_freq = max(freq_eps, next_freq, prev_freq + freq_eps)

    else:
        prev_freq, next_freq = fixed_freq_range

    freq_range = freq_range[valid[eig_range]]
    freq_range_margins = nm.r_[prev_freq, freq_range, next_freq]

    return freq_range, freq_range_margins

def split_chunks(indx):
    """Split index vector to chunks of consecutive numbers."""
    if not len(indx): return []

    delta = nm.ediff1d(indx, to_end=2)
    ir = nm.where(delta > 1)[0]

    chunks = []
    ic0 = 0
    for ic in ir:
        chunk = indx[ic0:ic+1]
        ic0 = ic + 1
        chunks.append(chunk)
    return chunks

def get_log_freqs(f0, f1, df, freq_eps, n_point_min, n_point_max):
    """
    Get logging frequencies.

    The frequencies get denser towards the interval boundaries.
    """
    f_delta = f1 - f0
    f_mid = 0.5 * (f0 + f1)

    if (f1 - f0) > (2.0 * freq_eps):
        num = min(n_point_max, max(n_point_min, (f1 - f0) / df))
        a = nm.linspace(0., 1., num)
        log_freqs = f0 + freq_eps \
                    + 0.5 * (nm.sin((a - 0.5) * nm.pi) + 1.0) \
                    * (f1 - f0 - 2.0 * freq_eps)

    else:
        log_freqs = nm.array([f_mid - 1e-8 * f_delta,
                              f_mid + 1e-8 * f_delta])

    return log_freqs

def detect_band_gaps(mass, freq_info, opts, gap_kind='normal', mtx_b=None):
    """
    Detect band gaps given solution to eigenproblem (eigs,
    eig_vectors). Only valid resonance frequencies (e.i. those for which
    corresponding eigenmomenta are above a given threshold) are taken into
    account.

    Notes
    -----
    - make freq_eps relative to ]f0, f1[ size?
    """
    output('eigensolver:', opts.eigensolver)

    fm = freq_info.freq_range_margins
    min_freq, max_freq = fm[0], fm[-1]
    output('freq. range with margins: [%8.3f, %8.3f]'
           % (min_freq, max_freq))

    df = opts.freq_step * (max_freq - min_freq)

    fz_callback = get_callback(mass.evaluate, opts.eigensolver,
                               mtx_b=mtx_b, mode='find_zero')
    trace_callback = get_callback(mass.evaluate, opts.eigensolver,
                                  mtx_b=mtx_b, mode='trace')

    n_col = 1 + (mtx_b is not None)
    logs = [[] for ii in range(n_col + 1)]
    gaps = []

    for ii in xrange(freq_info.freq_range.shape[0] + 1):

        f0, f1 = fm[[ii, ii+1]]
        output('interval: ]%.8f, %.8f[...' % (f0, f1))

        log_freqs = get_log_freqs(f0, f1, df, opts.freq_eps, 100, 1000)

        output('n_logged: %d' % log_freqs.shape[0])

        log_mevp = [[] for ii in range(n_col)]
        for f in log_freqs:
            for ii, data in enumerate(trace_callback(f)):
                log_mevp[ii].append(data)

        # Get log for the first and last f in log_freqs.
        lf0 = log_freqs[0]
        lf1 = log_freqs[-1]

        log0, log1 = log_mevp[0][0], log_mevp[0][-1]
        min_eig0 = log0[0]
        max_eig1 = log1[-1]
        if gap_kind == 'liquid':
            mevp = nm.array(log_mevp, dtype=nm.float64).squeeze()
            si = nm.where(mevp[:,0] < 0.0)[0]
            li = nm.where(mevp[:,-1] < 0.0)[0]
            wi = nm.setdiff1d(si, li)

            if si.shape[0] == 0: # No gaps.
                gap = ([2, lf0, log0[0]], [2, lf0, log0[-1]])
                gaps.append(gap)

            elif li.shape[0] == mevp.shape[0]: # Full interval strong gap.
                gap = ([1, lf1, log1[0]], [1, lf1, log1[-1]])
                gaps.append(gap)

            else:
                subgaps = []
                for chunk in split_chunks(li): # Strong gaps.
                    i0, i1 = chunk[0], chunk[-1]
                    fmin, fmax = log_freqs[i0], log_freqs[i1]
                    gap = ([1, fmin, mevp[i0,-1]], [1, fmax, mevp[i1,-1]])
                    subgaps.append(gap)

                for chunk in split_chunks(wi): # Weak gaps.
                    i0, i1 = chunk[0], chunk[-1]
                    fmin, fmax = log_freqs[i0], log_freqs[i1]
                    gap = ([0, fmin, mevp[i0,-1]], [2, fmax, mevp[i1,-1]])
                    subgaps.append(gap)
                gaps.append(subgaps)

        else:
            if min_eig0 > 0.0: # No gaps.
                gap = ([2, lf0, log0[0]], [2, lf0, log0[-1]])

            elif max_eig1 < 0.0: # Full interval strong gap.
                gap = ([1, lf1, log1[0]], [1, lf1, log1[-1]])

            else:
                llog_freqs = list(log_freqs)

                # Insert fmin, fmax into log.
                output('finding zero of the largest eig...')
                smax, fmax, vmax = find_zero(lf0, lf1, fz_callback,
                                             opts.freq_eps, opts.zero_eps, 1)
                im = nm.searchsorted(log_freqs, fmax)
                llog_freqs.insert(im, fmax)
                for ii, data in enumerate(trace_callback(fmax)):
                    log_mevp[ii].insert(im, data)

                output('...done')
                if smax in [0, 2]:
                    output('finding zero of the smallest eig...')
                    # having fmax instead of f0 does not work if freq_eps is
                    # large.
                    smin, fmin, vmin = find_zero(lf0, lf1, fz_callback,
                                                 opts.freq_eps, opts.zero_eps, 0)
                    im = nm.searchsorted(log_freqs, fmin)
                    # +1 due to fmax already inserted before.
                    llog_freqs.insert(im+1, fmin)
                    for ii, data in enumerate(trace_callback(fmin)):
                        log_mevp[ii].insert(im+1, data)

                    output('...done')

                elif smax == 1:
                    smin = 1 # both are negative everywhere.
                    fmin, vmin = fmax, vmax

                gap = ([smin, fmin, vmin], [smax, fmax, vmax])

                log_freqs = nm.array(llog_freqs)

            output(gap[0])
            output(gap[1])

            gaps.append(gap)

        logs[0].append(log_freqs)
        for ii, data in enumerate(log_mevp):
            logs[ii+1].append(nm.array(data, dtype = nm.float64))

        output('...done')

    kinds = describe_gaps(gaps)

    slogs = Struct(freqs=logs[0], eigs=logs[1])
    if n_col == 2:
        slogs.eig_vectors = logs[2]

    return slogs, gaps, kinds

def get_callback(mass, method, mtx_b=None, mode='trace'):
    """
    Return callback to solve band gaps or dispersion eigenproblem P.

    Notes
    -----
    Find zero callbacks return:
      eigenvalues

    Trace callbacks return:
      (eigenvalues,)
    or
      (eigenvalues, eigenvectors) (in full (dispoersion) mode)

    If `mtx_b` is None, the problem P is
      M w = \lambda w,
    otherwise it is
      omega^2 M w = \eta B w"""

    def find_zero_callback(f):
        meigs = eig(mass(f), eigenvectors=False, method=method)
        return meigs

    def find_zero_full_callback(f):
        meigs = eig((f**2) * mass(f), mtx_b=mtx_b,
                    eigenvectors=False, method=method)
        return meigs

    def trace_callback(f):
        meigs = eig(mass(f), eigenvectors=False, method=method)
        return meigs,

    def trace_full_callback(f):
        meigs, mvecs = eig((f**2) * mass(f), mtx_b=mtx_b,
                           eigenvectors=True, method=method)

        return meigs, mvecs

    if mtx_b is not None:
        mode += '_full'

    return eval(mode + '_callback')

def find_zero(f0, f1, callback, freq_eps, zero_eps, mode):
    """
    For f \in ]f0, f1[ find frequency f for which either the smallest (`mode` =
    0) or the largest (`mode` = 1) eigenvalue of problem P given by `callback`
    is zero.

    Returns
    -------
    flag : 0, 1, or 2
        The flag, see Notes below.
    frequency : float
        The found frequency.
    eigenvalue : float
        The eigenvalue corresponding to the found frequency.

    Notes
    -----
    Meaning of the return value combinations:

    =====  ======  ========
    mode    flag    meaning
    =====  ======  ========
    0, 1    0       eigenvalue -> 0 for f \in ]f0, f1[
    0       1       f -> f1, smallest eigenvalue < 0
    0       2       f -> f0, smallest eigenvalue > 0 and -> -\infty
    1       1       f -> f1, largest eigenvalue < 0 and  -> +\infty
    1       2       f -> f0, largest eigenvalue > 0
    =====  ======  ========
    """
    fm, fp = f0, f1
    ieig = {0 : 0, 1 : -1}[mode]
    while 1:
        f = 0.5 * (fm + fp)
        meigs = callback(f)

        val = meigs[ieig]
        ## print f, f0, f1, fm, fp, val
        ## print '%.16e' % f, '%.16e' % fm, '%.16e' % fp, '%.16e' % val

        if ((abs(val) < zero_eps)
            or ((fp - fm) < (abs(fm) * nm.finfo(float).eps))):
            return 0, f, val

        if mode == 0:
            if (f - f0) < freq_eps:
                return 2, f0, val

            elif (f1 - f) < freq_eps:
                return 1, f1, val

        elif mode == 1:
            if (f1 - f) < freq_eps:
                return 1, f1, val

            elif (f - f0) < freq_eps:
                return 2, f0, val

        if val > 0.0:
            fp = f

        else:
            fm = f

def describe_gaps(gaps):
    kinds = []
    for ii, gap in enumerate(gaps):
        if isinstance(gap, list):
            subkinds = []
            for gmin, gmax in gap:
                if (gmin[0] == 2) and (gmax[0] == 2):
                    kind = ('p', 'propagation zone')
                elif (gmin[0] == 1) and (gmax[0] == 1):
                    kind = ('is', 'inner strong band gap')
                elif (gmin[0] == 0) and (gmax[0] == 2):
                    kind = ('iw', 'inner weak band gap')
                subkinds.append(kind)
            kinds.append(subkinds)

        else:
            gmin, gmax = gap

            if (gmin[0] == 2) and (gmax[0] == 2):
                kind = ('p', 'propagation zone')
            elif (gmin[0] == 1) and (gmax[0] == 2):
                kind = ('w', 'full weak band gap')
            elif (gmin[0] == 0) and (gmax[0] == 2):
                kind = ('wp', 'weak band gap + propagation zone')
            elif (gmin[0] == 1) and (gmax[0] == 1):
                kind = ('s', 'full strong band gap (due to end of freq.'
                        ' range or too large thresholds)')
            elif (gmin[0] == 1) and (gmax[0] == 0):
                kind = ('sw', 'strong band gap + weak band gap')
            elif (gmin[0] == 0) and (gmax[0] == 0):
                kind = ('swp', 'strong band gap + weak band gap +'
                        ' propagation zone')
            else:
                msg = 'impossible band gap combination: %d, %d' % (gmin, gmax)
                raise ValueError(msg)
            kinds.append(kind)

    return kinds

def compute_cat_sym_sym(coef, iw_dir):
    """
    Christoffel acoustic tensor (part) of elasticity tensor dimension.
    """
    dim = iw_dir.shape[0]

    cat = nm.zeros((dim, dim), dtype=nm.float64)
    for ii in range(dim):
        for ij in range(dim):
            ir = coor_to_sym(ii, ij, dim)
            for ik in range(dim):
                for il in range(dim):
                    ic = coor_to_sym(ik, il, dim)
                    cat[ii,ik] += coef[ir,ic] * iw_dir[ij] * iw_dir[il]

    return cat

def compute_cat_dim_sym(coef, iw_dir):
    """
    Christoffel acoustic tensor part of piezo-coupling tensor dimension.
    """
    dim = iw_dir.shape[0]

    cat = nm.zeros((dim,), dtype=nm.float64)
    for ii in range(dim):
        for ij in range(dim):
            ir = coor_to_sym(ii, ij, dim)
            for ik in range(dim):
                cat[ii] += coef[ik,ir] * iw_dir[ij] * iw_dir[ik]

    return cat

def compute_cat_dim_dim(coef, iw_dir):
    """
    Christoffel acoustic tensor part of dielectric tensor dimension.
    """
    cat = nm.dot(nm.dot(coef, iw_dir), iw_dir)

    return cat

class SimpleEVP(CorrMiniApp):
    """
    Simple eigenvalue problem.
    """

    def process_options(self):
        get = self.options.get

        return Struct(eigensolver=get('eigensolver', 'eig.sgscipy'),
                      elasticity_contrast=get('elasticity_contrast', 1.0),
                      scale_epsilon=get('scale_epsilon', 1.0),
                      save_eig_vectors=get('save_eig_vectors', (0, 0)))

    def __call__(self, problem=None, data=None):
        problem = get_default(problem, self.problem)
        opts = self.app_options

        problem.set_equations(self.equations)
        problem.select_bcs(ebc_names=self.ebcs, epbc_names=self.epbcs,
                           lcbc_names=self.get('lcbcs', []))
        problem.update_materials(problem.ts)

        self.init_solvers(problem)

        mtx_a, mtx_m, data = self.prepare_matrices(problem)

        output('computing resonance frequencies...')
        tt = [0]

        if isinstance(mtx_a, sc.sparse.spmatrix):
            mtx_a = mtx_a.toarray()
        if isinstance(mtx_m, sc.sparse.spmatrix):
            mtx_m = mtx_m.toarray()

        eigs, mtx_s_phi = eig(mtx_a, mtx_m, return_time=tt,
                              method=opts.eigensolver)
        eigs[eigs<0.0] = 0.0
        output('...done in %.2f s' % tt[0])
        output('original eigenfrequencies:')
        output(eigs)
        opts = self.app_options
        epsilon2 = opts.scale_epsilon * opts.scale_epsilon
        eigs_rescaled = (opts.elasticity_contrast / epsilon2)  * eigs
        output('rescaled eigenfrequencies:')
        output(eigs_rescaled)
        output('number of eigenfrequencies: %d' % eigs.shape[0])

        try:
            assert_(nm.isfinite(eigs).all())
        except ValueError:
            from sfepy.base.base import debug; debug()

        mtx_phi, eig_vectors = self.post_process(eigs, mtx_s_phi, data,
                                                 problem)

        self.save(eigs, mtx_phi, problem)

        evp = Struct(name='evp', eigs=eigs, eigs_rescaled=eigs_rescaled,
                     eig_vectors=eig_vectors)

        return evp

    def prepare_matrices(self, problem):
        mtx_a = problem.evaluate(self.equations['lhs'], mode='weak',
                                 auto_init=True, dw_mode='matrix')

        mtx_m = problem.evaluate(self.equations['rhs'], mode='weak',
                                 dw_mode='matrix')

        return mtx_a, mtx_m, None

    def post_process(self, eigs, mtx_s_phi, data, problem):
        n_eigs = eigs.shape[0]

        variables = problem.get_variables()

        mtx_phi = nm.empty((variables.di.ptr[-1], mtx_s_phi.shape[1]),
                           dtype=nm.float64)

        make_full = variables.make_full_vec
        for ii in xrange(n_eigs):
            mtx_phi[:,ii] = make_full(mtx_s_phi[:,ii])

        return mtx_phi, mtx_phi

    def save(self, eigs, mtx_phi, problem):
        save = self.app_options.save_eig_vectors

        n_eigs = eigs.shape[0]

        out = {}
        state = problem.create_state()
        for ii in xrange(n_eigs):
            if (ii >= save[0]) and (ii < (n_eigs - save[1])): continue
            state.set_full(mtx_phi[:,ii], force=True)
            aux = state.create_output_dict()
            for name, val in aux.iteritems():
                out[name+'%03d' % ii] = val

        if self.post_process_hook is not None:
            out = self.post_process_hook(out, problem, mtx_phi)

        problem.domain.mesh.write(self.save_name + '.vtk', io='auto', out=out)

        fd = open(self.save_name + '_eigs.txt', 'w')
        eigs.tofile(fd, ' ')
        fd.close()

class SchurEVP(SimpleEVP):
    """
    Schur complement eigenvalue problem.
    """

    def prepare_matrices(self, problem):
        """
        A = K + B^T D^{-1} B
        """
        equations = problem.equations
        mtx = equations.eval_tangent_matrices(None, problem.mtx_a,
                                              by_blocks=True)

        ls = Solver.any_from_conf(problem.ls_conf,
                                  presolve=True, mtx=mtx['D'])

        mtx_b, mtx_m = mtx['B'], mtx['M']
        mtx_dib = nm.empty(mtx_b.shape, dtype=mtx_b.dtype)
        for ic in xrange(mtx_b.shape[1]):
            mtx_dib[:,ic] = ls(mtx_b[:,ic].toarray().squeeze())
        mtx_a = mtx['K'] + mtx_b.T * mtx_dib

        return mtx_a, mtx_m, mtx_dib

    def post_process(self, eigs, mtx_s_phi, mtx_dib, problem):
        n_eigs = eigs.shape[0]

        variables = problem.get_variables()

        mtx_phi = nm.empty((variables.di.ptr[-1], mtx_s_phi.shape[1]),
                           dtype=nm.float64)

        make_full = variables.make_full_vec

        # Update also eliminated variables.
        schur = self.app_options.schur
        primary_var = schur['primary_var']
        eliminated_var = schur['eliminated_var']

        mtx_s_phi_schur = - sc.dot(mtx_dib, mtx_s_phi)
        aux = nm.empty((variables.adi.ptr[-1],), dtype=nm.float64)
        set = variables.set_state_part
        for ii in xrange(n_eigs):
            set(aux, mtx_s_phi[:,ii], primary_var, stripped=True)
            set(aux, mtx_s_phi_schur[:,ii], eliminated_var,
                stripped=True)

            mtx_phi[:,ii] = make_full(aux)

        indx = variables.get_indx(primary_var)
        eig_vectors = mtx_phi[indx,:]

        return mtx_phi, eig_vectors

class DensityVolumeInfo(MiniAppBase):
    """
    Determine densities of regions specified in `region_to_material`, and
    compute average density based on region volumes.
    """

    def __call__(self, volume=None, problem=None, data=None):
        problem = get_default(problem, self.problem)

        vf = data[self.requires[0]]

        average_density = 0.0
        total_volume = 0.0
        volumes = {}
        densities = {}
        for region_name, aux in self.region_to_material.iteritems():
            vol = vf['volume_' + region_name]

            mat_name, item_name = aux
            conf = problem.conf.get_item_by_name('materials', mat_name)
            density = conf.values[item_name]

            output('region %s: volume %f, density %f' % (region_name,
                                                         vol, density))

            volumes[region_name] = vol
            densities[region_name] = density

            average_density += vol * density
            total_volume += vol

        true_volume = self._get_volume(volume)
        assert_(abs(total_volume - true_volume) / true_volume < 1e-14)

        output('total volume:', true_volume)

        average_density /= true_volume

        return Struct(name='density_volume_info',
                      average_density=average_density,
                      total_volume=total_volume,
                      volumes=volumes,
                      densities=densities,
                      to_file_txt=self.to_file_txt)

    @staticmethod
    def to_file_txt(fd, float_format, dv_info):
        ff = float_format + '\n'

        fd.write('total volume:\n')
        fd.write(ff % dv_info.total_volume)
        fd.write('average density:\n')
        fd.write(ff % dv_info.average_density)

        for key, val in dv_info.volumes.iteritems():
            fd.write('%s volume:\n' % key)
            fd.write(ff % val)
            fd.write('%s density:\n' % key)
            fd.write(ff % dv_info.densities[key])

class Eigenmomenta(MiniAppBase):
    """
    Eigenmomenta corresponding to eigenvectors.

    Parameters
    ----------
    var_name : str
        The name of the variable used in the integral.
    threshold : float
        The threshold under which an eigenmomentum is considered zero.
    threshold_is_relative : bool
        If True, the `threshold` is relative w.r.t. max. norm of eigenmomenta.
    transform : callable, optional
        Optional function for transforming the eigenvectors before computing
        the eigenmomenta.

    Returns
    -------
    eigenmomenta : Struct
        The resulting eigenmomenta. An eigenmomentum above threshold is marked
        by the attribute 'valid' set to True.
    """

    def process_options(self):
        options = dict_to_struct(self.options)
        get = options.get

        return Struct(var_name=get('var_name', None,
                                   'missing "var_name" in options!'),
                      threshold=get('threshold', 1e-4),
                      threshold_is_relative=get('threshold_is_relative', True),
                      transform=get('transform', None))

    def __call__(self, volume=None, problem=None, data=None):
        problem = get_default(problem, self.problem)
        opts = self.app_options

        evp, dv_info = [data[ii] for ii in self.requires]

        output('computing eigenmomenta...')

        if opts.transform is not None:
            fun = problem.conf.get_function(opts.transform[0])
            def wrap_transform(vec, shape):
                return fun(vec, shape, *opts.eig_vector_transform[1:])

        else:
            wrap_transform = None

        tt = time.clock()
        eigenmomenta = compute_eigenmomenta(self.expression, opts.var_name,
                                            problem, evp.eig_vectors,
                                            wrap_transform)
        output('...done in %.2f s' % (time.clock() - tt))

        n_eigs = evp.eigs.shape[0]

        mag = norm_l2_along_axis(eigenmomenta)

        if opts.threshold_is_relative:
            tol = opts.threshold * mag.max()
        else:
            tol = opts.threshold

        valid = nm.where(mag < tol, False, True)
        mask = nm.where(valid == False)[0]
        eigenmomenta[mask, :] = 0.0
        n_zeroed = mask.shape[0]

        output('%d of %d eigenmomenta zeroed (under %.2e)'\
                % (n_zeroed, n_eigs, tol))

        out = Struct(name='eigenmomenta', n_zeroed=n_zeroed,
                     eigenmomenta=eigenmomenta, valid=valid,
                     to_file_txt=None)
        return out

class AcousticMassTensor(MiniAppBase):
    """
    The acoustic mass tensor for a given frequency.

    Returns
    -------
    self : AcousticMassTensor instance
        This class instance whose `evaluate()` method computes for a given
        frequency the required tensor.

    Notes
    -----
    `eigenmomenta`, `eigs` should contain only valid resonances.
    """
    to_file_txt = None

    def __call__(self, volume=None, problem=None, data=None):
        evp, self.dv_info, ema = [data[ii] for ii in self.requires]

        self.eigs = evp.eigs[ema.valid]
        self.eigenmomenta = ema.eigenmomenta[ema.valid, :]

        return self

    def evaluate(self, freq):
        ema = self.eigenmomenta

        n_c = ema.shape[1]
        fmass = nm.zeros((n_c, n_c), dtype=nm.float64)

        num, denom = self.get_coefs(freq)
        de = 1.0 / denom
        if not nm.isfinite(de).all():
            raise ValueError('frequency %e too close to resonance!' % freq)

        for ir in range(n_c):
            for ic in range(n_c):
                if ir <= ic:
                    val = nm.sum(num * de * (ema[:, ir] * ema[:, ic]))
                    fmass[ir, ic] += val
                else:
                    fmass[ir, ic] = fmass[ic, ir]

        eye = nm.eye(n_c, n_c, dtype=nm.float64)
        mtx_mass = (eye * self.dv_info.average_density) \
                   - (fmass / self.dv_info.total_volume)

        return mtx_mass

    def get_coefs(self, freq):
        """
        Get frequency-dependent coefficients.
        """
        f2 = freq*freq
        de = f2 - self.eigs
        return f2, de

class AcousticMassLiquidTensor(AcousticMassTensor):

    def get_coefs(self, freq):
        """
        Get frequency-dependent coefficients.
        """
        eigs = self.eigs

        f2 = freq*freq
        aux = (f2 - self.gamma * eigs)
        num = f2 * aux
        denom = aux*aux + f2*(self.eta*self.eta)*nm.power(eigs, 2.0)
        return num, denom

class AppliedLoadTensor(AcousticMassTensor):
    """
    The applied load tensor for a given frequency.

    Returns
    -------
    self : AppliedLoadTensor instance
        This class instance whose `evaluate()` method computes for a given
        frequency the required tensor.

    Notes
    -----
    `eigenmomenta`, `ueigenmomenta`, `eigs` should contain only valid
    resonances.
    """
    to_file_txt = None

    def __call__(self, volume=None, problem=None, data=None):
        evp, self.dv_info, ema, uema = [data[ii] for ii in self.requires]

        self.eigs = evp.eigs[ema.valid]
        self.eigenmomenta = ema.eigenmomenta[ema.valid, :]
        self.ueigenmomenta = uema.eigenmomenta[uema.valid, :]

        return self

    def evaluate(self, freq):
        ema, uema = self.eigenmomenta, self.ueigenmomenta

        n_c = ema.shape[1]
        fload = nm.zeros((n_c, n_c), dtype=nm.float64)

        num, denom = self.get_coefs(freq)
        de = 1.0 / denom
        if not nm.isfinite(de).all():
            raise ValueError('frequency %e too close to resonance!' % freq)

        for ir in range(n_c):
            for ic in range(n_c):
                val = nm.sum(num * de * (ema[:, ir] * uema[:, ic]))
                fload[ir, ic] += val

        eye = nm.eye(n_c, n_c, dtype=nm.float64)

        mtx_load = eye - (fload / self.dv_info.total_volume)

        return mtx_load

class BandGaps(MiniAppBase):
    """
    Band gaps detection.

    Parameters
    ----------
    eigensolver : str
        The name of the eigensolver for mass matrix eigenvalues.
    eig_range : (int, int)
        The eigenvalues range (squared frequency) to consider.
    freq_margins : (float, float)
        Margins in percents of initial frequency range given by
        `eig_range` by which the range is increased.
    fixed_freq_range : (float, float)
        The frequency range to consider. Has precedence over `eig_range`
        and `freq_margins`.
    freq_step : float
        The frequency step for tracing, in percent of the frequency range.
    freq_eps : float
        The frequency difference smaller than `freq_eps` is considered zero.
    zero_eps : float
        The tolerance for finding zeros of mass matrix eigenvalues.
    detect_fun : callable
        The function for detecting the band gaps. Default is
        :func:`detect_band_gaps()`.
    log_save_name : str
        If not None, the band gaps log is to be saved under the given name.
    """

    def process_options(self):
        get = self.options.get

        freq_margins = get('freq_margins', (5, 5))
        # Given per cent.
        freq_margins = 0.01 * nm.array(freq_margins, dtype=nm.float64)

        # Given in per cent.
        freq_step = 0.01 * get('freq_step', 5)

        return Struct(eigensolver=get('eigensolver', 'eig.sgscipy'),
                      eig_range=get('eig_range', None),
                      freq_margins=freq_margins,
                      fixed_freq_range=get('fixed_freq_range', None),
                      freq_step=freq_step,

                      freq_eps=get('freq_eps', 1e-8),
                      zero_eps=get('zero_eps', 1e-8),
                      detect_fun=get('detect_fun', detect_band_gaps),
                      log_save_name=get('log_save_name', None))

    def __call__(self, volume=None, problem=None, data=None):
        problem = get_default(problem, self.problem)
        opts = self.app_options

        evp, ema, mass = [data[ii] for ii in self.requires[:3]]
        if len(self.requires) == 4:
            mtx_b = data[self.requires[3]]

        else:
            mtx_b = None

        eigs = evp.eigs

        self.fix_eig_range(eigs.shape[0])

        if opts.fixed_freq_range is not None:
            (freq_range_initial,
             opts.eig_range) = get_ranges(opts.fixed_freq_range, eigs)

        else:
            opts.eig_range = slice(*opts.eig_range)
            freq_range_initial = nm.sqrt(eigs[opts.eig_range])

        output('initial freq. range     : [%8.3f, %8.3f]'
               % tuple(freq_range_initial[[0, -1]]))

        aux = cut_freq_range(freq_range_initial, eigs, ema.valid,
                             opts.freq_margins, opts.eig_range,
                             opts.fixed_freq_range,
                             opts.freq_eps)
        freq_range, freq_range_margins = aux
        if len(freq_range):
            output('freq. range             : [%8.3f, %8.3f]'
                   % tuple(freq_range[[0, -1]]))

        else:
            # All masked.
            output('freq. range             : all masked!')

        freq_info = Struct(name='freq_info',
                           freq_range_initial=freq_range_initial,
                           freq_range=freq_range,
                           freq_range_margins=freq_range_margins)

        logs, gaps, kinds = opts.detect_fun(mass, freq_info, opts, mtx_b=mtx_b)

        bg = Struct(name='band_gaps', logs=logs, gaps=gaps, kinds=kinds,
                    valid=ema.valid, eig_range=opts.eig_range,
                    n_eigs=eigs.shape[0], n_zeroed=ema.n_zeroed,
                    freq_range_initial=freq_info.freq_range_initial,
                    freq_range=freq_info.freq_range,
                    freq_range_margins=freq_info.freq_range_margins,
                    opts=opts, to_file_txt=self.to_file_txt,
                    log_save_name=opts.log_save_name, save_log=self.save_log)

        return bg

    def fix_eig_range(self, n_eigs):
        eig_range = get_default(self.app_options.eig_range, (0, n_eigs))
        if eig_range[-1] < 0:
            eig_range[-1] += n_eigs + 1

        assert_(eig_range[0] < (eig_range[1] - 1))
        assert_(eig_range[1] <= n_eigs)
        self.app_options.eig_range = eig_range

    @staticmethod
    def to_file_txt(fd, float_format, bg):
        if bg.log_save_name is not None:
            fd.write(bg.log_save_name + '\n')

        else:
            fd.write('--\n')

    @staticmethod
    def save_log(filename, float_format, bg):
        """
        Save band gaps, valid flags and eigenfrequencies.
        """
        fd = open(filename, 'w')
        freq_range = bg.freq_range_margins
        fd.write('n_zeroed: %d\n' % bg.n_zeroed)
        fd.write('n_eigs: %d\n' % bg.n_eigs)
        fd.write('f0 f1 flag_min f_min v_min flag_max f_max v_max'
                  ' kind\ndesc\n')

        ff = float_format
        format = "%s %s %%d %s %s %%d %s %s %%s\n%%s\n" % (6 * (ff,))

        n_row = len(freq_range) - 1
        fd.write('%d\n' % n_row)
        for ir in xrange(n_row):
            f0, f1 = freq_range[[ir, ir+1]]
            gmin, gmax = bg.gaps[ir]
            fd.write(format % ((f0, f1) + tuple(gmin) + tuple(gmax)
                                + bg.kinds[ir]))

        fd.write('valid resonance\n')
        freq_range = bg.freq_range_initial
        n_row = len(freq_range)
        fd.write('%d\n' % n_row)
        valid_in_range = bg.valid[bg.eig_range]
        format = "%%d %s\n" % ff
        for ir in xrange(n_row):
            fd.write(format % (valid_in_range[ir], freq_range[ir]))
        fd.close()

class ChristoffelAcousticTensor(MiniAppBase):

    def process_options(self):
        get = self.options.get
        return Struct(mode=get('mode', 'simple'),
                      incident_wave_dir=get('incident_wave_dir', None))

    r"""
    Compute Christoffel acoustic tensor (cat) given the incident wave
    direction (unit vector).

    Parameters
    ----------
    mode : 'simple' or 'piezo'
        The call mode.
    incident_wave_dir : array
        The incident wave direction vector.

    Returns
    -------
    cat : array
        The Christoffel acoustic tensor.

    Notes
    -----
    - If mode == 'simple', only the elasticity tensor :math:`C_{ijkl}` is used
      and cat := :math:`\Gamma_{ik} = C_{ijkl} n_j n_l`.

    - If mode == 'piezo', also the piezo-coupling :math:`G_{ijk}` and
      dielectric :math:`D_{ij}` tensors are used and cat := :math:`H_{ik} =
      \Gamma_{ik} + \frac{1}{\xi} \gamma_i \gamma_j`, where :math:`\gamma_i =
      G_{kij} n_j n_k`, :math:`\xi = D_{kl} n_k n_l`.
    """

    def __call__(self, volume=None, problem=None, data=None):
        problem = get_default(problem, self.problem)
        opts = self.app_options

        iw_dir = nm.array(opts.incident_wave_dir, dtype=nm.float64)
        dim = problem.get_dim()
        assert_(dim == iw_dir.shape[0])

        iw_dir = iw_dir / nla.norm(iw_dir)

        elastic = data[self.requires[0]]

        cat = compute_cat_sym_sym(elastic, iw_dir)

        if opts.mode =='piezo':
            dielectric, coupling = [data[ii] for ii in self.requires[1:]]
            xi = compute_cat_dim_dim(dielectric, iw_dir)
            gamma = compute_cat_dim_sym(coupling, iw_dir)
            cat += nm.outer(gamma, gamma) / xi

        return cat

class PolarizationAngles(MiniAppBase):
    """
    Compute polarization angles, i.e., angles between incident wave direction
    and wave vectors. Vector length does not matter - eigenvectors are used
    directly.
    """

    def process_options(self):
        get = self.options.get
        return Struct(incident_wave_dir=get('incident_wave_dir', None))

    def __call__(self, volume=None, problem=None, data=None):
        problem = get_default(problem, self.problem)
        opts = self.app_options

        iw_dir = nm.array(opts.incident_wave_dir, dtype=nm.float64)
        dim = problem.get_dim()
        assert_(dim == iw_dir.shape[0])

        iw_dir = iw_dir / nla.norm(iw_dir)

        dispersion = data[self.requires[0]]

        wave_vectors = dispersion.logs.eig_vectors

        pas = []

        iw_dir = iw_dir / nla.norm(iw_dir)
        idims = range(iw_dir.shape[0])
        pi2 = 0.5 * nm.pi
        for vecs in wave_vectors:
            pa = nm.empty(vecs.shape[:-1], dtype=nm.float64)
            for ir, vec in enumerate(vecs):
                for ic in idims:
                    vv = vec[:,ic]
                    # Ensure the angle is in [0, pi/2].
                    val = nm.arccos(nm.dot(iw_dir, vv) / nla.norm(vv))
                    if val > pi2:
                        val = nm.pi - val
                    pa[ir,ic] = val

            pas.append(pa)

        return pas

class PhaseVelocity(MiniAppBase):
    """
    Compute phase velocity.
    """

    def process_options(self):
        get = self.options.get

        return Struct(eigensolver=get('eigensolver', 'eig.sgscipy'))

    def __call__(self, volume=None, problem=None, data=None):
        problem = get_default(problem, self.problem)
        opts = self.app_options

        dv_info, cat = [data[ii] for ii in self.requires]

        output('average density:', dv_info.average_density)

        dim = problem.get_dim()
        eye = nm.eye(dim, dim, dtype=nm.float64)
        mtx_mass = eye * dv_info.average_density

        meigs, mvecs = eig(mtx_mass, mtx_b=cat,
                           eigenvectors=True, method=opts.eigensolver)
        phase_velocity = 1.0 / nm.sqrt(meigs)

        return phase_velocity

########NEW FILE########
__FILENAME__ = convolutions
import numpy as nm

from sfepy.base.base import pause, Struct
from sfepy.homogenization.utils import integrate_in_time

def compute_mean_decay(coef):
    r"""
    Compute mean decay approximation of a non-scalar fading memory
    coefficient.
    """
    n_step = coef.shape[0]
    weights = nm.abs(coef[int(nm.fix(n_step / 2))])
    weights /= weights.sum()

    ## coef_flat = nm.reshape(coef, (n_step, nm.prod(coef.shape[1:])))
    ## maxs = nm.abs(coef_flat).max(axis=-1)
    ## decay = avgs + maxs

    aux = weights[None,...] * coef
    aux_flat = nm.reshape(aux, (n_step, nm.prod(coef.shape[1:])))
    avgs = aux_flat.sum(axis=-1)

    decay = avgs # Gives better results than the above.
    decay /= decay[0]

    return decay

def eval_exponential(coefs, x):
    c1, c2 = coefs
    return c1 * nm.exp(-c2 * x)

def approximate_exponential(x, y):
    r"""
    Approximate :math:`y = f(x)` by :math:`y_a = c_1 exp(- c_2 x)`.

    Initial guess is given by assuming y has already the required exponential
    form.
    """
    from scipy.optimize import leastsq

##     weights = nm.abs(y)
##     weights = weights / weights.sum()

    weights = nm.ones_like(y)
    
    def fun(c, x, y):
        val = weights * (y - eval_exponential(c, x))
        return val

    c1 = y[0]
    c2 = - nm.log(y[1] / c1) / (x[1] - x[0])

    coefs, ier = leastsq(fun, nm.array([c1, c2]), args=(x, y))

    if ier != 1:
        print c1, c2
        print coefs, ier
        pause('exponential fit failed!')

    return coefs

def fit_exponential(x, y, return_coefs=False):
    """
    Evaluate :math:`y = f(x)` after approximating :math:`f` by an exponential.
    """
    coefs = approximate_exponential(x, y)

    if return_coefs:
        return coefs, eval_exponential(coefs, x)
    else:
        return eval_exponential(coefs, x)

class ConvolutionKernel(Struct):
    r"""
    The convolution kernel with exponential synchronous decay approximation
    approximating the original kernel represented by the array :math:`c[i]`,
    :math:`i = 0, 1, \dots`.

    .. math::
        \begin{split}
        & c_0 \equiv c[0] \;, c_{e0} \equiv c_0 c^e_0 \;, \\
        & c(t) \approx c_0 d(t) \approx c_0 e(t) = c_{e0} e_n(t) \;,
        \end{split}
        
    where :math:`d(0) = e_n(0) = 1`, :math:`d` is the synchronous decay and
    :math:`e` its
    exponential approximation, :math:`e = c^e_0 exp(-c^e_1 t)`.
    """
    def __init__(self, name, times, kernel, decay=None,
                 exp_coefs=None, exp_decay=None):
        Struct.__init__(self, name=name, times=times, c=kernel,
                        d=decay, ec=exp_coefs, e=exp_decay)

        if decay is None:
            self.d = compute_mean_decay(kernel)

        if exp_decay is None:
            self.ec, self.e = fit_exponential(times, self.d,
                                              return_coefs=True)

        self.en = self.e / self.e[0]
        self.c0 = self.c[0]
        self.e_c0 = self.c0 * self.e[0]
        self.e_d1 = self.e[1] / self.e[0]

        self.c_slice = (slice(None),) + ((None,) * (self.c.ndim - 1))

    def diff_dt(self, use_exp=False):
        """
        The derivative of the kernel w.r.t. time.
        """
        if use_exp:
            val = - self.ec[1] * self.c0 * self.e[self.c_slice]

        else:
            zz = nm.zeros(self.c[0:1].shape, dtype=self.c.dtype)
            val = nm.r_[nm.diff(self.c, axis=0) /
                        nm.diff(self.times, axis=0)[self.c_slice], zz]

        return val

    def int_dt(self, use_exp=False):
        """
        The integral of the kernel in time.
        """
        if use_exp:
            val = (self.e_c0/self.ec[1]) * (1.0 - self.en[-1])
        else:
            val = integrate_in_time(self.c, self)

        return val

    def get_exp(self):
        """
        Get the exponential synchronous decay kernel approximation.
        """
        return self.c0 * self.e[self.c_slice]

    def get_full(self):
        """
        Get the original (full) kernel.
        """
        return self.c

    def __call__(self, use_exp=False):
        """
        Get the kernel or its approximation.
        """
        if use_exp:
            return self.get_exp()

        else:
            return self.get_full()

########NEW FILE########
__FILENAME__ = engine
from copy import copy

from sfepy.base.base import output, get_default, Struct
from sfepy.applications import PDESolverApp, Application
from coefs_base import MiniAppBase

def insert_sub_reqs(reqs, levels, req_info):
    """Recursively build all requirements in correct order."""
    all_reqs = []
    for _, req in enumerate(reqs):
        # Coefficients are referenced as 'c.<name>'...
        areq = req
        if req.startswith('c.'):
            areq = req[2:]

        try:
            rargs = req_info[areq]
        except KeyError:
            raise ValueError('requirement "%s" is not defined!' % req)

        sub_reqs = rargs.get('requires', [])

        if req in levels:
            raise ValueError('circular requirement "%s"!' % (req))

        if sub_reqs:
            levels.append(req)
            all_reqs.extend(insert_sub_reqs(sub_reqs, levels, req_info))
            levels.pop()

        if req in all_reqs:
            raise ValueError('circular requirement "%s"!' % (req))
        else:
            all_reqs.append(req)

    return all_reqs

class HomogenizationEngine(PDESolverApp):

    @staticmethod
    def process_options(options):
        get = options.get

        return Struct(coefs=get('coefs', None,
                                'missing "coefs" in options!'),
                      requirements=get('requirements', None,
                                       'missing "requirements" in options!'),
                      compute_only=get('compute_only', None),
                      save_format=get('save_format', 'vtk'),
                      dump_format=get('dump_format', 'h5'),
                      coefs_info=get('coefs_info', None))

    def __init__(self, problem, options, app_options=None,
                 volume=None, output_prefix='he:', **kwargs):
        """Bypasses PDESolverApp.__init__()!"""
        Application.__init__(self, problem.conf, options, output_prefix,
                             **kwargs)
        self.problem = problem
        self.setup_options(app_options=app_options)
        self.setup_output_info(self.problem, self.options)

        if volume is None:
            self.volume = self.problem.evaluate(self.app_options.total_volume)

        else:
            self.volume = volume

    def setup_options(self, app_options=None):
        PDESolverApp.setup_options(self)
        app_options = get_default(app_options, self.conf.options)

        po = HomogenizationEngine.process_options
        self.app_options += po(app_options)

    def compute_requirements(self, requirements, dependencies, store):
        problem = self.problem

        opts = self.app_options
        req_info = getattr(self.conf, opts.requirements)

        requires = insert_sub_reqs(copy(requirements), [], req_info)

        for req in requires:
            if req in dependencies and (dependencies[req] is not None):
                continue

            output('computing dependency %s...' % req)

            rargs = req_info[req]

            mini_app = MiniAppBase.any_from_conf(req, problem, rargs)
            mini_app.setup_output(save_format=opts.save_format,
                                  dump_format=opts.dump_format,
                                  post_process_hook=self.post_process_hook,
                                  file_per_var=opts.file_per_var)
            store(mini_app)

            problem.clear_equations()

            # Pass only the direct dependencies, not the indirect ones.
            dep_requires = rargs.get('requires', [])
            data = {}
            for key in dep_requires:
                data[key] = dependencies[key]

            dep = mini_app(data=data)

            dependencies[req] = dep
            output('...done')

        return dependencies

    def call(self, ret_all=False):
        problem = self.problem

        opts = self.app_options
        coef_info = getattr(self.conf, opts.coefs)

        compute_names = set(get_default(opts.compute_only, coef_info.keys()))
        compute_names = ['c.' + key for key in compute_names]

        is_store_filenames = coef_info.pop('filenames', None) is not None
        try:
            compute_names.remove('c.filenames')
        except:
            pass

        dependencies = {}
        save_names = {}
        dump_names = {}

        def store_filenames(app):
            if not '(not_set)' in app.get_save_name_base():
                save_names[app.name] = app.get_save_name_base()
            if not '(not_set)' in app.get_dump_name_base():
                dump_names[app.name] = app.get_dump_name_base()

        # Some coefficients can require other coefficients - resolve their
        # order here.
        req_info = self.conf.get(opts.requirements, {})
        info = copy(coef_info)
        info.update(req_info)
        all_deps = set(compute_names)
        sorted_names = []
        for coef_name in compute_names:
            cargs = coef_info[coef_name[2:]]
            requires = cargs.get('requires', [])
            deps = insert_sub_reqs(copy(requires), [], info)
            all_deps.update(deps)

            aux = [key for key in deps if key.startswith('c.')] + [coef_name]
            sorted_names.extend(aux)

        sorted_coef_names = []
        for name in sorted_names:
            if name[2:] not in sorted_coef_names:
                sorted_coef_names.append(name[2:])

        coefs = Struct()
        for coef_name in sorted_coef_names:
            cargs = coef_info[coef_name]
            output('computing %s...' % coef_name)
            requires = cargs.get('requires', [])
            requirements = [name for name in requires if not
                            name.startswith('c.')]

            self.compute_requirements(requirements, dependencies,
                                      store_filenames)

            for name in requires:
                if name.startswith('c.'):
                    dependencies[name] = getattr(coefs, name[2:])

            mini_app = MiniAppBase.any_from_conf(coef_name, problem, cargs)

            problem.clear_equations()

            # Pass only the direct dependencies, not the indirect ones.
            data = {}
            for key in requires:
                data[key] = dependencies[key]

            val = mini_app(self.volume, data=data)
            setattr(coefs, coef_name, val)
            output('...done')

        # remove "auxiliary" coefs
        for coef_name in sorted_coef_names:
            cstat = coef_info[coef_name].get('status', 'main')
            if cstat == 'auxiliary':
                delattr(coefs, coef_name)

        # Store filenames of all requirements as a "coefficient".
        if is_store_filenames:
            coefs.save_names = save_names
            coefs.dump_names = dump_names

        if opts.coefs_info is not None:
            coefs.info = opts.coefs_info

        if ret_all:
            return coefs, dependencies
        else:
            return coefs

########NEW FILE########
__FILENAME__ = homogen_app
import os.path as op
import shutil

import numpy as nm

from sfepy.base.base import output, get_default, Struct
from sfepy.homogenization.coefficients import Coefficients
from sfepy.homogenization.coefs_base import MiniAppBase
from sfepy.homogenization.engine import HomogenizationEngine
from sfepy.applications import PDESolverApp

class Volume(MiniAppBase):

    def __call__(self, problem=None):
        problem = get_default(problem, self.problem)
        problem.select_variables(self.variables)

        volume = problem.evaluate(self.expression)

        return volume

def get_volume_from_options(options, problem):
    volume = {}

    if hasattr(options, 'volumes') and (options.volumes is not None):
        for vk, vv in options.volumes.iteritems():
            if 'value' in vv:
                volume[vk] = nm.float64(vv['value'])
            else:
                volume[vk] = Volume('volume', problem, vv)()

    elif hasattr(options, 'volume') and (options.volume is not None):
            if 'value' in options.volume:
                vol = nm.float64(options.volume['value'])
            else:
                vol = Volume('volume', problem, options.volume)()
            volume['total'] = vol

    return volume

class HomogenizationApp( HomogenizationEngine ):

    @staticmethod
    def process_options(options):
        """
        Application options setup. Sets default values for missing
        non-compulsory options.
        """
        get = options.get

        volume = get('volume', None)
        volumes = get('volumes', None)
        if volume is None and volumes is None:
            raise ValueError('missing "volume" in options!')

        return Struct(print_digits=get('print_digits', 3),
                      float_format=get('float_format', '%8.3e'),
                      coefs_filename=get('coefs_filename', 'coefs'),
                      tex_names=get('tex_names', None),
                      coefs=get('coefs', None, 'missing "coefs" in options!'),
                      requirements=get('requirements', None,
                                       'missing "requirements" in options!'),
                      return_all=get('return_all', False),
                      volume=volume,
                      volumes=volumes)

    def __init__(self, conf, options, output_prefix, **kwargs):
        PDESolverApp.__init__(self, conf, options, output_prefix,
                              init_equations=False)

        self.setup_options()
        self.cached_coefs = None

        output_dir = self.problem.output_dir

        if conf._filename is not None:
            shutil.copyfile(conf._filename,
                            op.join(output_dir, op.basename(conf._filename)))

    def setup_options( self ):
        PDESolverApp.setup_options(self)
        po = HomogenizationApp.process_options
        self.app_options += po( self.conf.options )

    def call(self, verbose=False, ret_all=None):
        """
        Call the homogenization engine and compute the homogenized
        coefficients.

        Parameters
        ----------
        verbose : bool
            If True, print the computed coefficients.
        ret_all : bool or None
            If not None, it can be used to override the 'return_all' option.
            If True, also the dependencies are returned.

        Returns
        -------
        coefs : Coefficients instance
            The homogenized coefficients.
        dependencies : dict
            The dependencies, if `ret_all` is True.
        """
        opts = self.app_options

        ret_all = get_default(ret_all, opts.return_all)

        volume = get_volume_from_options(opts, self.problem)

        for vk, vv in volume.iteritems():
            output('volume: %s = %.2f' % (vk, vv))

        he = HomogenizationEngine( self.problem, self.options, volume = volume )

        aux = he( ret_all = ret_all)
        if ret_all:
            coefs, dependencies = aux
        else:
            coefs = aux

        coefs = Coefficients( **coefs.to_dict() )
        coefs.volume = volume

        if verbose:
            prec = nm.get_printoptions()[ 'precision']
            if hasattr(opts, 'print_digits'):
                nm.set_printoptions(precision=opts.print_digits)
            print coefs
            nm.set_printoptions(precision=prec)

        coef_save_name = op.join( opts.output_dir, opts.coefs_filename )
        coefs.to_file_hdf5( coef_save_name + '.h5' )
        coefs.to_file_txt( coef_save_name + '.txt',
                           opts.tex_names,
                           opts.float_format )

        if ret_all:
            return coefs, dependencies
        else:
            return coefs

########NEW FILE########
__FILENAME__ = micmac
import numpy as nm

from sfepy.base.base import output, Struct
from sfepy.base.conf import ProblemConf, get_standard_keywords
from sfepy.homogenization.homogen_app import HomogenizationApp
from sfepy.homogenization.coefficients import Coefficients
import tables as pt
from sfepy.discrete.fem.meshio import HDF5MeshIO
import os.path as op

def get_homog_coefs_linear(ts, coor, mode,
                           micro_filename=None, regenerate=False,
                           coefs_filename=None):

    oprefix = output.prefix
    output.prefix = 'micro:'

    required, other = get_standard_keywords()
    required.remove( 'equations' )

    conf = ProblemConf.from_file(micro_filename, required, other, verbose=False)
    if coefs_filename is None:
        coefs_filename = conf.options.get('coefs_filename', 'coefs')
        coefs_filename = op.join(conf.options.get('output_dir', '.'),
                                 coefs_filename) + '.h5'

    if not regenerate:
        if op.exists( coefs_filename ):
            if not pt.isHDF5File( coefs_filename ):
                regenerate = True
        else:
            regenerate = True

    if regenerate:
        options = Struct( output_filename_trunk = None )

        app = HomogenizationApp( conf, options, 'micro:' )
        coefs = app()
        if type(coefs) is tuple:
            coefs = coefs[0]

        coefs.to_file_hdf5( coefs_filename )
    else:
        coefs = Coefficients.from_file_hdf5( coefs_filename )

    out = {}
    if mode == None:
        for key, val in coefs.__dict__.iteritems():
            out[key] = val

    elif mode == 'qp':
        for key, val in coefs.__dict__.iteritems():
            if type( val ) == nm.ndarray or type(val) == nm.float64:
                out[key] = nm.tile( val, (coor.shape[0], 1, 1) )
            elif type(val) == dict:
                for key2, val2 in val.iteritems():
                    if type(val2) == nm.ndarray or type(val2) == nm.float64:
                        out[key+'_'+key2] = \
                                          nm.tile(val2, (coor.shape[0], 1, 1))

    else:
        out = None

    output.prefix = oprefix

    return out

def get_correctors_from_file( coefs_filename = 'coefs.h5',
                              dump_names = None ):

    if dump_names == None:
        coefs = Coefficients.from_file_hdf5( coefs_filename )
        if hasattr( coefs, 'dump_names' ):
            dump_names = coefs.dump_names
        else:
            raise ValueError( ' "filenames" coefficient must be used!' )

    out = {}

    for key, val in dump_names.iteritems():
        corr_name = op.split( val )[-1]
        io = HDF5MeshIO( val+'.h5' )
        data = io.read_data( 0 )
        dkeys = data.keys()
        corr = {}
        for dk in dkeys:
            corr[dk] = data[dk].data.reshape(data[dk].shape)

        out[corr_name] = corr

    return out

########NEW FILE########
__FILENAME__ = recovery
import os

import numpy as nm

from sfepy.base.base import get_default, Struct
from sfepy.base.ioutils import get_print_info
from sfepy.discrete.fem import extend_cell_data
from sfepy.homogenization.utils import coor_to_sym
from sfepy.base.conf import get_standard_keywords
from sfepy.discrete import Problem
from sfepy.homogenization.coefficients import Coefficients
from sfepy.homogenization.micmac import get_correctors_from_file
import os.path as op

shared = Struct()

#
# TODO : interpolate fvars to macro times. ?mid-points?
#
# TODO : clean-up!
#

def get_output_suffix(ig, iel, ts, naming_scheme, format, output_format):
    if output_format != 'h5':
        if naming_scheme == 'step_iel':
            suffix = '.'.join( (ts.suffix % ts.step, format % (ig, iel)) )
        else:
            suffix = '.'.join( (format % (ig, iel), ts.suffix % ts.step) )

    else:
        suffix = format % (ig, iel)

    return suffix

def convolve_field_scalar( fvars, pvars, iel, ts ):
    r"""
    .. math::
      \int_0^t f(t-s) p(s) ds

    Notes
    -----
    - t is given by step
    - f: fvars
      scalar field variables, defined in a micro domain, have shape [step][fmf
      dims]
    - p: pvars
      scalar point variables, a scalar in a point of macro-domain, FMField
      style have shape [n_step][var dims]
    """

    step0 = max( 0, ts.step - fvars.steps[-1] )
##     print step0, ts.step

    val = nm.zeros_like( fvars[0] )
    for ik in xrange( step0, ts.step + 1 ):
##         print ' ', ik, ts.step-ik
        vf = fvars[ts.step-ik]
        vp = pvars[ik][iel,0,0,0]
        val += vf * vp * ts.dt

    return val

def convolve_field_sym_tensor( fvars, pvars, var_name, dim, iel, ts ):
    r"""
    .. math::
      \int_0^t f^{ij}(t-s) p_{ij}(s) ds

    Notes
    -----
    - t is given by step
    - f: fvars
      field variables, defined in a micro domain, have shape [step][fmf dims]
    - p: pvars
      sym. tensor point variables, a scalar in a point of
      macro-domain, FMField style, have shape [dim, dim][var_name][n_step][var
      dims]
    """

    step0 = max( 0, ts.step - fvars[0,0][var_name].steps[-1] )

    val = nm.zeros_like( fvars[0,0][var_name][0] )
    for ik in xrange( step0, ts.step + 1 ):
##         print ' ', ik, ts.step-ik
        for ir in range( dim ):
            for ic in range( dim ):
                ii = coor_to_sym( ir, ic, dim )
                vf = fvars[ir,ic][var_name][ts.step-ik]
                vp = pvars[ik][iel,0,ii,0]
                val += vf * vp * ts.dt
    return val

def add_strain_rs( corrs_rs, strain, vu, dim, iel, out = None ):
    if out is None:
        out = nm.zeros_like( corrs_rs[0,0][vu][0] )

    for ir in range( dim ):
        for ic in range( dim ):
            ii = coor_to_sym( ir, ic, dim )
            out += corrs_rs[ir,ic][vu].data * strain[iel,0,ii,0]
    return out

def combine_scalar_grad(corrs, grad, vn, ii, shift_coors=None):
    r"""
    .. math::
      \eta_k \partial_k^x p

    or

    .. math::
      (y_k + \eta_k) \partial_k^x p
    """
    dim = grad.shape[2]
    
    if shift_coors is None:
        out = corrs[0][vn].data * grad[ii,0,0,0]
        for ir in range(1, dim):
            out += corrs[ir][vn].data * grad[ii,0,ir,0]

    else:        
        out = (shift_coors[:,0] + corrs[0][vn].data) * grad[ii,0,0,0]
        for ir in range(1, dim):
            out += (shift_coors[:,ir] + corrs[ir][vn].data) * grad[ii,0,ir,0]

    return out


def compute_u_corr_steady( corrs_rs, strain, vu, dim, iel ):
    r"""
    .. math::
      \sum_{ij} \bm{\omega}^{ij}\, e_{ij}(\bm{u})

    Notes
    -----
    - iel = element number
    """
    u_corr = add_strain_rs( corrs_rs, strain, vu, dim, iel )
    return u_corr

def compute_u_corr_time( corrs_rs, dstrains, corrs_pressure, pressures,
                         vu, dim, iel, ts ):
    r"""
    .. math::
      \sum_{ij} \left[ \int_0^t \bm{\omega}^{ij}(t-s) {\mathrm{d} \over
      \mathrm{d} s} e_{ij}(\bm{u}(s))\,ds\right] + \int_0^t
      \widetilde{\bm{\omega}}^P(t-s)\,p(s)\,ds
    """
    u_corr = convolve_field_scalar( corrs_pressure[vu], pressures,
                                    iel, ts )
    u_corr += convolve_field_sym_tensor( corrs_rs, dstrains, vu,
                                         dim, iel, ts )
    return u_corr

def compute_p_corr_steady( corrs_pressure, pressure, vp, iel ):
    r"""
    .. math::
      \widetilde\pi^P\,p
    """
    p_corr = corrs_pressure[vp].data * pressure[iel,0,0,0]
    return p_corr

def compute_p_corr_time( corrs_rs, dstrains, corrs_pressure, pressures,
                         vdp, dim, iel, ts ):
    r"""
    .. math::
      \sum_{ij} \int_0^t {\mathrm{d} \over \mathrm{d} t}
      \widetilde\pi^{ij}(t-s)\, {\mathrm{d} \over \mathrm{d} s}
      e_{ij}(\bm{u}(s))\,ds
      + \int_0^t {\mathrm{d} \over \mathrm{d} t}\widetilde\pi^P(t-s)\,p(s)\,ds
    """
    p_corr = convolve_field_scalar( corrs_pressure[vdp], pressures,
                                    iel, ts )
    p_corr += convolve_field_sym_tensor( corrs_rs, dstrains, vdp,
                                         dim, iel, ts )
    return p_corr

def compute_u_from_macro(strain, coor, iel, centre=None):
    r"""
    Macro-induced displacements.
    
    .. math::
      e_{ij}^x(\bm{u})\,(y_j - y_j^c)
    """
    n_nod, dim = coor.shape

    if centre is None:
        centre = nm.zeros((dim,), dtype=nm.float64)

    n_nod, dim = coor.shape
    um = nm.zeros((n_nod * dim,), dtype=nm.float64)
    for ir in range(dim):
        for ic in range(dim):
            ii = coor_to_sym(ir, ic, dim)
            um[ir::dim] += strain[iel,0,ii,0] * (coor[:,ic] - centre[ic])
    return um



def compute_p_from_macro(p_grad, coor, iel, centre=None, extdim=0):
    r"""
    Macro-induced pressure.
    
    .. math::
      \partial_j^x p\,(y_j - y_j^c)
    """
    n_nod, dim = coor.shape

    if centre is None:
        centre = nm.zeros((dim,), dtype=nm.float64)

    n_nod, dim = coor.shape
    pm = nm.zeros((n_nod,), dtype=nm.float64)
    for ic in range(dim + extdim):
        pm += p_grad[iel,0,ic,0] * (coor[:,ic] - centre[ic])
    return pm

###
def compute_micro_u( corrs, strain, vu, dim, out = None ):
    r"""
    Micro displacements.
    
    .. math::
      \bm{u}^1 = \bm{\chi}^{ij}\, e_{ij}^x(\bm{u}^0)
    """

    if out is None:
        out = nm.zeros_like( corrs[vu+'_00'] )

    for ir in range( dim ):
        for ic in range( dim ):
            ii = coor_to_sym( ir, ic, dim )
            out += corrs[vu+'_%d%d' % (ir, ic)] * strain[ii]
    return out

def compute_stress_strain_u( pb, integral, region, material, vu, data ):

    var = pb.create_variables([vu])[vu]
    var.set_data(data)

    stress = pb.evaluate('ev_cauchy_stress.%s.%s( %s, %s )'
                         % (integral, region, material, vu), verbose=False,
                         mode='el_avg', **{vu : var})
    strain = pb.evaluate('ev_cauchy_strain.%s.%s( %s )'
                         % (integral, region, vu), verbose=False,
                         mode='el_avg', **{vu : var})

    return extend_cell_data( stress, pb.domain, region ), \
           extend_cell_data( strain, pb.domain, region )

def add_stress_p( out, pb, integral, region, vp, data ):

    var = pb.create_variables([vp])[vp]
    var.set_data(data)

    press0 = pb.evaluate('ev_volume_integrate.%s.%s( %s )' \
                         % (integral, region, vp), verbose=False,
                         mode='el_avg', **{vp : var})
    press = extend_cell_data( press0, pb.domain, region )
    
    dim = pb.domain.mesh.dim
    nn = out.shape[0]
    for ii in range( nn ):
        for j in range( dim ):
            out[ii,0,j,0] += press[ii,0,0,0]

def compute_mac_stress_part( pb, integral, region, material, vu, mac_strain ):

    avgmat = pb.evaluate('ev_integrate_mat.%s.%s( %s, %s )' \
                         % (integral, region, material, vu), verbose=False,
                         mode='el_avg')

    return extend_cell_data( nm.dot( avgmat, mac_strain ), pb.domain, region )


###

def recover_bones( problem, micro_problem, region, eps0,
                   ts, strain, dstrains, p_grad, pressures,
                   corrs_permeability, corrs_rs, corrs_time_rs,
                   corrs_pressure, corrs_time_pressure,
                   var_names, naming_scheme = 'step_iel' ):
    r"""
    Notes
    -----
    - note that

      .. math::
        \widetilde{\pi}^P

      is in corrs_pressure -> from time correctors only 'u', 'dp' are needed.
    """

    dim = problem.domain.mesh.dim

    vu, vp, vn, vpp1, vppp1 = var_names
    vdp = 'd' + vp

    variables = micro_problem.create_variables()
    to_output = variables.state_to_output

    micro_u, micro_p = variables[vu], variables[vp]

    micro_coor = micro_u.field.get_coor()

    nodes_yc = micro_problem.domain.regions['Yc'].vertices

    join = os.path.join
    aux = max(problem.domain.shape.n_gr, 2)
    format = get_print_info( aux, fill = '0' )[1] \
             + '_' + get_print_info( problem.domain.mesh.n_el, fill = '0' )[1]

    for ig, ii, iel in region.iter_cells():
        print 'ig: %d, ii: %d, iel: %d' % (ig, ii, iel)

        pressure = pressures[-1][ii,0,0,0]

        us = corrs_pressure[vu].data * pressure
        add_strain_rs(corrs_rs, strain, vu, dim, ii, out=us)

        ut = convolve_field_scalar(corrs_time_pressure[vu], pressures, ii, ts)
        ut += convolve_field_sym_tensor(corrs_time_rs, dstrains, vu,
                                        dim, ii, ts)
        u1 = us  + ut

        u_mic = compute_u_from_macro(strain, micro_coor, ii ) + u1

        ps = corrs_pressure[vp].data * pressure

        pt = convolve_field_scalar(corrs_time_pressure[vdp], pressures, ii, ts)
        pt += convolve_field_sym_tensor(corrs_time_rs, dstrains, vdp,
                                        dim, ii, ts)
##     print us
##     print ut
##     print ps
##     print pt

        p_hat = ps  + pt

        # \eta_k \partial_k^x p
        p1 = combine_scalar_grad(corrs_permeability, p_grad, vn, ii)

        p_hat_e = micro_p.field.extend_dofs(p_hat[:,None], fill_value=0.0)
        p_mic = compute_p_from_macro(p_grad, micro_coor, ii)[:,None] \
                + p_hat_e / eps0
        p_mic[nodes_yc] = p1[:,None]
        
##         print u_mic
##         print p_mic

        # (y_k + \eta_k) \partial_k^x p
        p_aux = combine_scalar_grad(corrs_permeability, p_grad, vn, ii,
                                    shift_coors=micro_coor[nodes_yc])

        meval = micro_problem.evaluate

        var_p = variables[vppp1]
        var_p.set_data(p_aux)
        dvel_m1 = meval('ev_diffusion_velocity.i1.Yc( m.K, %s )' % vppp1,
                        verbose=False, mode='el_avg', **{vppp1 : var_p})

        var_p = variables[vpp1]
        var_p.set_data(p_hat)
        dvel_m2 = meval('ev_diffusion_velocity.i1.Ym( m.K, %s )' % vpp1,
                        verbose=False, mode='el_avg',
                        **{vpp1 : var_p}) * eps0
        
        out = {}
        out.update( to_output( u_mic, var_info = {vu : (True, vu)},
                               extend = True ) )
        out[vp] = Struct(name = 'output_data',
                         mode = 'vertex', data = p_mic,
                         var_name = vp, dofs = micro_p.dofs)

        aux = extend_cell_data(dvel_m1, micro_problem.domain, 'Yc')
        out['dvel_m1'] = Struct(name = 'output_data',
                                mode = 'cell', data = aux,
                                dofs = None)

        aux = extend_cell_data(dvel_m2, micro_problem.domain, 'Ym')
        out['dvel_m2'] = Struct(name = 'output_data',
                                mode = 'cell', data = aux,
                                dofs = None)

        suffix = get_output_suffix(ig, iel, ts, naming_scheme, format,
                                   micro_problem.output_format)
        micro_name = micro_problem.get_output_name(extra=suffix)
        filename = join(problem.output_dir,
                        'recovered_' + os.path.basename(micro_name))

        micro_problem.save_state(filename, out=out, ts=ts)

def recover_paraflow( problem, micro_problem, region,
                      ts, strain, dstrains, pressures1, pressures2,
                      corrs_rs, corrs_time_rs,
                      corrs_alpha1, corrs_time_alpha1,
                      corrs_alpha2, corrs_time_alpha2,
                      var_names, naming_scheme = 'step_iel' ):

    dim = problem.domain.mesh.dim

    vu, vp = var_names
    vdp = 'd' + vp

    micro_u = micro_problem.variables[vu]
    micro_coor = micro_u.field.get_coor()

    micro_p = micro_problem.variables[vp]

    nodes_y1 = micro_problem.domain.regions['Y1'].vertices
    nodes_y2 = micro_problem.domain.regions['Y2'].vertices

    to_output = micro_problem.variables.state_to_output

    join = os.path.join
    aux = max(problem.domain.shape.n_gr, 2)
    format = get_print_info( aux, fill = '0' )[1] \
             + '_' + get_print_info( problem.domain.mesh.n_el, fill = '0' )[1]

    for ig, ii, iel in region.iter_cells():
        print 'ig: %d, ii: %d, iel: %d' % (ig, ii, iel)

        p1, p2 = pressures1[-1][ii,0,0,0], pressures2[-1][ii,0,0,0]

        us = corrs_alpha1[vu].data * p1 + corrs_alpha2[vu].data * p2
        add_strain_rs( corrs_rs, strain, vu, dim, ii, out = us )

        ut = convolve_field_scalar( corrs_time_alpha1[vu], pressures1, ii, ts )
        ut += convolve_field_scalar( corrs_time_alpha2[vu], pressures2, ii, ts )
        ut += convolve_field_sym_tensor( corrs_time_rs, dstrains, vu,
                                         dim, ii, ts )

        u_corr = us + ut
        u_mic = compute_u_from_macro( strain, micro_coor, ii ) + u_corr

        ps = corrs_alpha1[vp].data * p1 + corrs_alpha2[vp].data * p2


        pt = convolve_field_scalar( corrs_time_alpha1[vdp], pressures1,
                                    ii, ts )
        pt += convolve_field_scalar( corrs_time_alpha2[vdp], pressures2,
                                     ii, ts )
        pt += convolve_field_sym_tensor( corrs_time_rs, dstrains, vdp,
                                         dim, ii, ts )

        p_corr = ps + pt

        p_mic = micro_p.field.extend_dofs(p_corr[:,nm.newaxis])
        p_mic[nodes_y1] = p1
        p_mic[nodes_y2] = p2
        
        out = {}
        out.update( to_output( u_mic, var_info = {vu : (True, vu)},
                               extend = True ) )
        out[vp] = Struct( name = 'output_data',
                          mode = 'vertex', data = p_mic,
                          var_name = vp, dofs = micro_p.dofs )

        suffix = get_output_suffix(ig, iel, ts, naming_scheme, format,
                                   micro_problem.output_format)
        micro_name = micro_problem.get_output_name(extra=suffix)
        filename = join( problem.output_dir, 'recovered_' + micro_name )

        micro_problem.save_state(filename, out=out, ts=ts)

def save_recovery_region(mac_pb, rname, filename=None):
    filename = get_default(filename, os.path.join(mac_pb.output_dir,
                                                  'recovery_region.vtk'))

    region = mac_pb.domain.regions[rname]

    # Save recovery region characteristic function.
    out = {}
    mask = region.get_charfun( by_cell = False, val_by_id = False )
    out['vmask'] = Struct(name='output_data',
                          mode='vertex', data=mask[:,nm.newaxis],
                          dofs=None)
    mask = region.get_charfun( by_cell = True, val_by_id = False )
    out['cmask'] = Struct(name='output_data',
                          mode='cell',
                          data=mask[:,nm.newaxis,nm.newaxis,nm.newaxis],
                          dofs=None)

    mac_pb.save_state(filename, out=out)


def recover_micro_hook( micro_filename, region, macro,
                        naming_scheme = 'step_iel',
                        recovery_file_tag='' ):

    # Create a micro-problem instance.
    required, other = get_standard_keywords()
    required.remove( 'equations' )
    pb = Problem.from_conf_file(micro_filename, required=required, other=other,
                                init_equations=False, init_solvers=False)

    coefs_filename = pb.conf.options.get('coefs_filename', 'coefs')
    output_dir = pb.conf.options.get('output_dir', '.')
    coefs_filename = op.join(output_dir, coefs_filename) + '.h5'

    # Coefficients and correctors
    coefs = Coefficients.from_file_hdf5( coefs_filename )
    corrs = get_correctors_from_file( dump_names = coefs.dump_names ) 

    recovery_hook = pb.conf.options.get('recovery_hook', None)

    if recovery_hook is not None:
        recovery_hook = pb.conf.get_function(recovery_hook)

        aux = max(pb.domain.shape.n_gr, 2)
        format = get_print_info( aux, fill = '0' )[1] \
            + '_' + get_print_info( pb.domain.mesh.n_el, fill = '0' )[1]

        for ig, ii, iel in region.iter_cells():
            print 'ig: %d, ii: %d, iel: %d' % (ig, ii, iel)

            local_macro = {}
            for k, v in macro.iteritems():
                local_macro[k] = v[ii,0]

            out = recovery_hook( pb, corrs, local_macro )

            # save data
            suffix = format % (ig, iel)
            micro_name = pb.get_output_name(extra='recovered_'\
                                            + recovery_file_tag + suffix)
            filename = op.join(output_dir, op.basename(micro_name))
            fpv = pb.conf.options.get('file_per_var', False)
            pb.save_state(filename, out=out,
                          file_per_var=fpv)


########NEW FILE########
__FILENAME__ = utils
import numpy as nm

def build_op_pi(var, ir, ic):
    """\Pi_i^{rs} = y_s \delta_{ir} for r = `ir`, s = `ic`."""
    coor = var.field.get_coor()

    pi = nm.zeros_like( coor )
    pi[:,ir] = coor[:,ic]
    pi.shape = (pi.shape[0] * pi.shape[1],)

    return pi

def create_pis(problem, var_name):
    """\Pi_i^{rs} = y_s \delta_{ir}, \ul{y} \in Y coordinates."""
    var = problem.get_variables(auto_create=True)[var_name]

    dim = problem.domain.mesh.dim
    pis = nm.zeros( (dim, dim), dtype = nm.object )
    names = []
    for ir in range( dim ):
        for ic in range( dim ):
            pi = build_op_pi(var, ir, ic)
            pis[ir,ic] = {var_name : pi}
            names.append('pi_%d%d' % (ir, ic))
    return names, pis

def create_scalar_pis( problem, var_name ):
    """\Pi^k = y_k, \ul{y} \in Y coordinates."""
    var = problem.get_variables(auto_create=True)[var_name]
    coor = var.field.get_coor()

    dim = problem.domain.mesh.dim
    pis = nm.zeros( (dim,), dtype = nm.object )
    names = []
    for ir in range( dim ):
        pis[ir] = {var_name : nm.ascontiguousarray( coor[:,ir] )}
        names.append('pi_%d' % (ir,))
    return names, pis

def iter_sym( dim ):
    for ii in xrange( dim ):
        yield ii, ii
    for ir in xrange( 0, dim ):
        for ic in xrange( ir + 1, dim ):
            yield ir, ic
c2s = {
    2 : [0, 2, 2, 1],
    3 : [0, 3, 4, 3, 1, 5, 4, 5, 2],
}
def coor_to_sym( ir, ic, dim ):
    return c2s[dim][dim*ir+ic]

##
# c: 14.09.2006, r: 04.04.2008
def interp_conv_mat( mat, ts, tdiff ):
    n_t = mat.shape[0]
    out = []
    tn = ts.time
    for ii, step in enumerate( xrange( ts.step, 0, -1 ) ):
        if ii == 0:
            out.append( mat[0] )
            continue
        
        td = tn - ts.times[step]
        if (td - 1e-12) > tdiff[-1]: break
        
        i1 = (tdiff >= td).argmax()
        i0 = i1 - 1

        td0, td1 = tdiff[[i0, i1]]
        dt = (td1 - td0)
        c1, c0 = (td - td0) / dt, (td1 - td) / dt
        out.append( c0 * mat[i0] + c1 * mat[i1] )

##         print ii, step, td
##        print i0, i1
##         print tn, ts.times[step]
##         print td0, td1, c0, c1

##     print out
##     print tdiff[-1], len( out )
##     pause()

    if not out: # For step == 0 matrix evaluation.
        out.append( mat[0] )

    return out

##
# c: 09.06.2008, r: 16.06.2008
def integrate_in_time( coef, ts, scheme = 'forward' ):
    """Forward difference or trapezoidal rule. 'ts' can be anything with
    'times' attribute."""
    dt = nm.diff(ts.times)
    dt = dt.reshape((dt.shape[0],) + (1,) * (coef.ndim-1))
    
    if scheme == 'trapezoid':
        icoef = nm.sum(0.5 * (coef[1:,...] + coef[:-1,...]) * dt, axis=0)
    elif scheme == 'forward':
        icoef = nm.sum(coef[:-1,...] * dt, axis=0)
    else:
        raise ValueError( 'unsupported scheme: %s' % scheme )
    
    return icoef

def define_box_regions(dim, lbn, rtf=None, eps=1.0e-3, kind='facet'):
    """
    Define sides and corner regions for a box aligned with coordinate
    axes.

    Parameters
    ----------
    dim : int
        Space dimension
    lbn : tuple
        Left bottom near point coordinates if rtf is not None. If rtf is
        None, lbn are the (positive) distances from the origin.
    rtf : tuple
        Right top far point coordinates.
    eps : float
        A parameter, that should be smaller than the smallest mesh node
        distance.
    kind : bool, optional
       The region kind.

    Returns
    -------
    regions : dict
        The box regions.
    """
    if rtf is None:
        lbn, rtf = -nm.array(lbn), lbn

    if dim == 3:
        lbnx, lbny, lbnz = lbn
        rtfx, rtfy, rtfz = rtf
        dx = abs(rtfx-lbnx)
        dy = abs(rtfy-lbny)
        dz = abs(rtfz-lbnz)
        lbnx, lbny, lbnz = (lbnx+dx*eps, lbny+dy*eps, lbnz+dz*eps)
        rtfx, rtfy, rtfz = (rtfx-dx*eps, rtfy-dy*eps, rtfz-dz*eps)
        regions = {
            'Near' : ('vertices in (y < %f)' % lbny, kind),
            'Far' : ('vertices in (y > %f)' % rtfy, kind),
            'Bottom' : ('vertices in (z < %f)' % lbnz, kind),
            'Top' : ('vertices in (z > %f)' % rtfz, kind),
            'Left' : ('vertices in (x < %f)' % lbnx, kind),
            'Right' : ('vertices in (x > %f)' % rtfx, kind),
            'Corners' : ("""vertices in
                            ((x < %f) & (y < %f) & (z < %f))
                          | ((x > %f) & (y < %f) & (z < %f))
                          | ((x > %f) & (y > %f) & (z < %f))
                          | ((x < %f) & (y > %f) & (z < %f))
                          | ((x < %f) & (y < %f) & (z > %f))
                          | ((x > %f) & (y < %f) & (z > %f))
                          | ((x > %f) & (y > %f) & (z > %f))
                          | ((x < %f) & (y > %f) & (z > %f))
                          """ % ( lbnx, lbny, lbnz,
                                  rtfx, lbny, lbnz,
                                  rtfx, rtfy, lbnz,
                                  lbnx, rtfy, lbnz,
                                  lbnx, lbny, rtfz,
                                  rtfx, lbny, rtfz,
                                  rtfx, rtfy, rtfz,
                                  lbnx, rtfy, rtfz ), 'vertex'),
        }
    else:
        lbnx, lbny, = lbn
        rtfx, rtfy, = rtf
        dx = abs(rtfx-lbnx)
        dy = abs(rtfy-lbny)
        lbnx, lbny = (lbnx+dx*eps, lbny+dy*eps,)
        rtfx, rtfy = (rtfx-dx*eps, rtfy-dy*eps,)
        regions = {
            'Bottom' : ('vertices in (y < %f)' % lbny, kind),
            'Top' : ('vertices in (y > %f)' % rtfy, kind),
            'Left' : ('vertices in (x < %f)' % lbnx, kind),
            'Right' : ('vertices in (x > %f)' % rtfx, kind),
            'Corners' : ("""vertices in
                              ((x < %f) & (y < %f))
                            | ((x > %f) & (y < %f))
                            | ((x > %f) & (y > %f))
                            | ((x < %f) & (y > %f))
                            """ % ( lbnx, lbny,
                                    rtfx, lbny,
                                    rtfx, rtfy,
                                    lbnx, rtfy ), 'vertex'),
        }

    return regions

def get_box_volume(dim, lbn, rtf=None):
    """Volume of a box aligned with coordinate axes.

    Parameters:

    dim : int
        Space dimension
    lbn : tuple
        Left bottom near point coordinates if rtf is not None. If rtf is
        None, lbn are the (positive) distances from the origin.
    rtf : tuple
        Right top far point coordinates.

    Returns:

    volume : float
        The box volume.
    """
    if rtf is None:
        lbn, rtf = -nm.array(lbn), lbn
    
    if dim == 3:
        lbnx, lbny, lbnz = lbn
        rtfx, rtfy, rtfz = rtf
        return abs(rtfx-lbnx)*abs(rtfy-lbny)*abs(rtfz-lbnz)
    else:
        lbnx, lbny, = lbn
        rtfx, rtfy, = rtf
        return abs(rtfx-lbnx)*abs(rtfy-lbny)

def get_lattice_volume(axes):
    r"""
    Volume of a periodic cell in a rectangular 3D (or 2D) lattice.

    Parameters
    ----------
    axes : array
        The array with the periodic cell axes :math:`a_1, \dots, a_3` as rows.

    Returns
    -------
    volume : float
        The periodic cell volume :math:`V = (a_1 \times a_2) \cdot a_3`. In 2D
        :math:`V = |(a_1 \times a_2)|` with zeros as the third components of
        vectors :math:`a_1`, :math:`a_2`.
    """
    axes = nm.asarray(axes)

    dim = axes.shape[0]

    if dim == 2:
        volume = nm.abs(nm.cross(axes[0], axes[1]))

    elif dim == 3:
        volume = nm.dot(nm.cross(axes[0], axes[1]), axes[2])

    else:
        raise ValueError('wrong axes shape! (%s)' % axes.shape)

    return volume

def get_volume(problem, field_name, region_name, quad_order=1):
    """
    Get volume of a given region using integration defined by a given
    field. Both the region and the field have to be defined in
    `problem`.
    """
    from sfepy.discrete import FieldVariable

    field = problem.fields[field_name]
    var = FieldVariable('u', 'parameter', field, 1,
                        primary_var_name='(set-to-None)')

    vol = problem.evaluate('d_volume.%d.%s( u )' % (quad_order, region_name),
                           u=var)

    return vol

def set_nonlin_states(variables, nl_state, problem):
    """
    Setup reference state for nonlinear homogenization

    Parameters
    ----------
    variables : dict
        All problem variables
    nl_state : reference state
    problem : problem description
    """

    if nl_state is not None:
        var_names = nl_state['variables']
        var_fun = nl_state['set_states']
        pvar_names = []
        for ivar in var_names:
            if ivar in variables:
                pvar_names.append(ivar)
        states = var_fun(problem, pvar_names, variables)
        for ivar in pvar_names:
            variables[ivar].set_data(states[ivar])

########NEW FILE########
__FILENAME__ = check_derivatives
"""
Utilities for checking derivatives of functions.
"""

import numpy as nm

def check_fx(x0, fx, fx_args, dfx, dfx_args=None, delta=1e-5):
    """
    Check derivatives of a (vectorized) scalar function of a scalar variable.
    """
    if dfx_args is None:
        dfx_args = fx_args

    dfx_a = dfx(x0, *dfx_args)

    x = x0 + delta
    f1 = fx(x, *fx_args)

    x = x0 - delta
    f2 = fx(x, *fx_args)

    dfx_d = 0.5 * (f1 - f2) / delta

    error = nm.linalg.norm(dfx_a - dfx_d, nm.inf)

    print 'analytical:', dfx_a
    print 'difference:', dfx_d
    print 'error:', error

    return dfx_a, dfx_d, error

def check_vfvx(x0, fx, fx_args, dfx, dfx_args=None, delta=1e-5):
    """
    Check derivatives of a (vectorized) vector or scalar function of a vector
    variable.
    """
    if x0.ndim != 2:
        raise ValueError('The variable must have two dimensions!')

    if dfx_args is None:
        dfx_args = fx_args

    dfx_a = dfx(x0, *dfx_args)
    dfx_d = nm.zeros_like(dfx_a)

    for ic in xrange(x0.shape[1]):
        x = x0.copy()
        x[:, ic] += delta
        f1 = fx(x, *fx_args)

        x = x0.copy()
        x[:, ic] -= delta
        f2 = fx(x, *fx_args)

        dfx_d[:, ic] = 0.5 * (f1 - f2) / delta

    error = nm.linalg.norm((dfx_a - dfx_d).ravel(), nm.inf)

    print 'analytical:', dfx_a
    print 'difference:', dfx_d
    print 'error:', error

    return dfx_a, dfx_d, error

########NEW FILE########
__FILENAME__ = eigen
import numpy as nm
import scipy.sparse as sp
from scipy.sparse.linalg import aslinearoperator
from scipy.linalg import eigvals_banded

from sfepy.base.base import get_default, output
from sfepy.linalg import infinity_norm

def sym_tri_eigen(diags, select_indices=None):
    """
    Compute eigenvalues of a symmetric tridiagonal matrix using
    `scipy.linalg.eigvals_banded()`.
    """
    if select_indices is not None:
        n = diags.shape[1]
        select_indices = nm.minimum(select_indices, n)
        eigs = eigvals_banded(diags, lower=True, select='i',
                              select_range=select_indices)

    else:
        eigs = eigvals_banded(diags, lower=True, select='a')

    return eigs

def cg_eigs(mtx, rhs=None, precond=None, i_max=None, eps_r=1e-10,
            shift=None, select_indices=None, verbose=False, report_step=10):
    r"""
    Make several iterations of the conjugate gradients and estimate so
    the eigenvalues of a (sparse SPD) matrix (Lanczos algorithm).

    Parameters
    ----------
    mtx : spmatrix or array
        The sparse matrix :math:`A`.
    precond : spmatrix or array, optional
        The preconditioner matrix. Any object that can be multiplied by
        vector can be passed.
    i_max : int
        The maximum number of the Lanczos algorithm iterations.
    eps_r : float
        The relative stopping tolerance.
    shift : float, optional
        Eigenvalue shift for non-SPD matrices. If negative, the shift is
        computed as :math:`|shift| ||A||_{\infty}`.
    select_indices : (min, max), optional
        If given, computed only the eigenvalues with indices `min <= i <= max`.
    verbose : bool
        Verbosity control.
    report_step : int
        If `verbose` is True, report in every `report_step`-th step.

    Returns
    -------
    vec : array
        The approximate solution to the linear system.
    n_it : int
        The number of CG iterations used.
    norm_rs : array
        Convergence history of residual norms.
    eigs : array
        The approximate eigenvalues sorted in ascending order.
    """
    n_row = mtx.shape[0]
    norm = nm.linalg.norm

    rhs = get_default(rhs, nm.random.rand(n_row))
    i_max = get_default(i_max, min(n_row, 100))

    if shift is not None:
        if shift < 0.0:
            mtx_norm = infinity_norm(mtx)
            output('matrix max norm:', mtx_norm, verbose=verbose)
            shift = abs(shift) * mtx_norm

        output('eigenvalue shift:', shift, verbose=verbose)
        mtx = mtx + shift * sp.eye(n_row, n_row, dtype=mtx.dtype)

    mtx = aslinearoperator(mtx)

    lambda_max = 0
    lambda_min = 0
    econd = 1.0

    x0 = nm.zeros_like(rhs)
    r = r0 = rhs

    # The diagonals (0, 1) in two rows.
    diags = nm.empty((2, i_max + 1), dtype=mtx.dtype)
    diags[0, 0] = 0

    if precond is None:
        z0 = r0

    else:
        z0 = precond * r0

    x = x0
    z = z0

    p = nm.zeros_like(rhs)
    beta = 0.0

    rho0 = nm.dot(z0, r0)
    norm_r0 = norm(r0);

    if verbose:
        output('%5d lambda: %e %e cond: %e |R|: %e\n' % (0, 0, 0, 0, norm_r0))

    norm_rs = [norm_r0]

    for ii in xrange(i_max):
        p = z + beta * p
        q = mtx * p

        alpha = rho0 / nm.dot(p, q)
        if (not nm.isfinite(alpha)) or abs(alpha) < 1e-16:
            output('precision limit reached!')
            ii -= 1
            break

        x = x + alpha * p
        r = r - alpha * q

        if precond is None:
            z = r

        else:
            z = precond * r

        norm_r = norm(r)
        norm_rs.append(norm_r)

        rho1 = nm.dot(z, r)
        beta = rho1 / rho0

        # Lanczos
        diags[0, ii] += 1.0 / alpha
        diags[1, ii] = - nm.sqrt(beta) / alpha
        diags[0, ii+1] = beta / alpha

        if verbose and (ii > 0):
            if (ii % report_step) == 0:
                eigs = sym_tri_eigen(diags[:, :ii+1],
                                     select_indices=select_indices)
                if select_indices is None:
                    lambda_min, lambda_max = eigs[0], eigs[-1]
                    econd = lambda_max / lambda_min
                    output('%5d lambda: %e %e cond: %e |R|: %e\n'
                           % (ii, lambda_min, lambda_max, econd, norm_r))

                else:
                    output('%5d |R|: %e\n'
                           % (ii, norm_r))

        if (norm_r / norm_r0) < eps_r:
            output('converged on %d iters, |Ri|/|R0|: %e, econd: %e\n'
                   % (ii, norm_r / norm_r0, econd), verbose=verbose)
            break

        rho0 = rho1

    eigs = sym_tri_eigen(diags[:, :ii+1], select_indices=select_indices)
    if verbose and (select_indices is None):
        lambda_min, lambda_max = eigs[0], eigs[-1]
        econd = lambda_max / lambda_min
        output('min: %e  max: %e  cond: %e\n'
               % (lambda_min, lambda_max, econd))

    if shift is not None:
        eigs -= shift

    return x, ii, nm.array(norm_rs), eigs

def arpack_eigs(mtx, nev=1, which='SM'):
    """
    Calculate several eigenvalues and corresponding eigenvectors of a
    matrix using ARPACK from SciPy. The eigenvalues are sorted in
    ascending order.
    """
    from scipy.sparse.linalg.interface import aslinearoperator
    from scipy.sparse.linalg.eigen.arpack import speigs

    matvec = aslinearoperator(mtx).matvec
    eigs, vecs = speigs.ARPACK_eigs(matvec, mtx.shape[0], nev=nev, which=which)

    ii = nm.argsort(eigs)
    eigs = eigs[ii]
    vecs = vecs[:,ii]

    return eigs, vecs

########NEW FILE########
__FILENAME__ = geometry
import numpy as nm
import numpy.linalg as nla

from scipy.misc import factorial

from sfepy.base.base import assert_, output
from sfepy.linalg.utils import norm_l2_along_axis as norm
from sfepy.linalg.utils import mini_newton, dets_fast

def transform_bar_to_space_coors(bar_coors, coors):
    """
    Transform barycentric coordinates `bar_coors` within simplices with
    vertex coordinates `coors` to space coordinates.
    """
    space_coors = nm.zeros((coors.shape[0], coors.shape[2]), dtype=coors.dtype)
    for iv in range(bar_coors.shape[1]):
        space_coors += bar_coors[:,iv:iv+1] * coors[:,iv,:]

    return space_coors

def get_simplex_circumcentres(coors, force_inside_eps=None):
    """
    Compute the circumcentres of `n_s` simplices in 1D, 2D and 3D.

    Parameters
    ----------
    coors : array
        The coordinates of the simplices with `n_v` vertices given in an
        array of shape `(n_s, n_v, dim)`, where `dim` is the space
        dimension and `2 <= n_v <= (dim + 1)`.
    force_inside_eps : float, optional
        If not None, move the circumcentres that are outside of their
        simplices or closer to their boundary then `force_inside_eps` so
        that they are inside the simplices at the distance given by
        `force_inside_eps`. It is ignored for edges.

    Returns
    -------
    centres : array
        The circumcentre coordinates as an array of shape `(n_s, dim)`.
    """
    n_s, n_v, dim = coors.shape

    assert_(2 <= n_v <= (dim + 1))
    assert_(1 <= dim <= 3)

    if n_v == 2: # Edges.
        centres = 0.5 * nm.sum(coors, axis=1)

    else:
        if n_v == 3: # Triangles.
            a2 = norm(coors[:,1,:] - coors[:,2,:], squared=True)
            b2 = norm(coors[:,0,:] - coors[:,2,:], squared=True)
            c2 = norm(coors[:,0,:] - coors[:,1,:], squared=True)

            bar_coors = nm.c_[a2 * (-a2 + b2 + c2),
                              b2 * (a2 - b2 + c2),
                              c2 * (a2 + b2 - c2)]

        elif n_v == 4: # Tetrahedrons.
            a2 = norm(coors[:,2,:] - coors[:,1,:], squared=True)
            b2 = norm(coors[:,2,:] - coors[:,0,:], squared=True)
            c2 = norm(coors[:,1,:] - coors[:,0,:], squared=True)
            d2 = norm(coors[:,3,:] - coors[:,0,:], squared=True)
            e2 = norm(coors[:,3,:] - coors[:,1,:], squared=True)
            f2 = norm(coors[:,3,:] - coors[:,2,:], squared=True)
            bar_coors = nm.c_[(d2 * a2 * (f2 + e2 - a2)
                               + b2 * e2 * (a2 + f2 - e2)
                               + c2 * f2 * (e2 + a2 - f2)
                               - 2 * a2 * e2 * f2),
                              (e2 * b2 * (f2 + d2 - b2)
                               +  c2 * f2 * (d2 + b2 - f2)
                               +  a2 * d2 * (b2 + f2 - d2)
                               - 2 * b2 * d2 * f2),
                              (f2 * c2 * (e2 + d2 - c2)
                               +  b2 * e2 * (d2 + c2 - e2)
                               +  a2 * d2 * (c2 + e2 - d2)
                               - 2 * c2 * e2 * d2),
                              (d2 * a2 * (b2 + c2 - a2)
                               +  e2 * b2 * (c2 + a2 - b2)
                               +  f2 * c2 * (a2 + b2 - c2)
                               - 2 * a2 * b2 * c2)]

        else:
            raise ValueError('unsupported simplex! (%d vertices)' % n_v)

        bar_coors /= nm.sum(bar_coors, axis=1)[:,None]
        if force_inside_eps is not None:
            bc = 1.0 / n_v
            limit = 0.9 * bc
            bar_centre = nm.array([bc] * n_v, dtype=nm.float64)

            eps = float(force_inside_eps)
            if eps > limit:
                output('force_inside_eps is too big, adjusting! (%e -> %e)'
                       % (eps, limit))
                eps = limit

            # Flag is True where the barycentre is closer to the simplex
            # boundary then eps, or outside of the simplex.
            mb = nm.min(bar_coors, axis=1)
            flag = nm.where(mb < eps)[0]

            # Move the bar_coors[flag] towards bar_centre so that it is
            # inside at the eps distance.
            mb = mb[flag]
            alpha = ((eps - mb) / (bar_centre[0] - mb))[:,None]

            bar_coors[flag] = (1.0 - alpha) * bar_coors[flag] \
                              + alpha * bar_centre[None,:]

        centres = transform_bar_to_space_coors(bar_coors, coors)

    return centres

def get_simplex_volumes(cells, coors):
    """
    Get volumes of simplices in nD.

    Parameters
    ----------
    cells : array, shape (n, d)
        The indices of `n` simplices with `d` vertices into `coors`.
    coors : array
        The coordinates of simplex vertices.

    Returns
    -------
    volumes : array
        The volumes of the simplices.
    """
    scoors = coors[cells]
    deltas = scoors[:, 1:] - scoors[:, :1]
    dim = coors.shape[1]
    volumes = dets_fast(deltas) / factorial(dim)

    return volumes

def barycentric_coors(coors, s_coors):
    """
    Get barycentric (area in 2D, volume in 3D) coordinates of points
    with coordinates `coors` w.r.t. the simplex given by `s_coors`.

    Returns
    -------
    bc : array
        The barycentric coordinates. Then reference element coordinates
        `xi = dot(bc.T, ref_coors)`.
    """
    n_v, dim = s_coors.shape
    n_c, dim2 = coors.shape
    assert_(dim == dim2)
    assert_(n_v == (dim + 1))

    mtx = nm.ones((n_v, n_v), nm.float64)
    mtx[0:n_v-1,:] = s_coors.T

    rhs = nm.empty((n_v,n_c), nm.float64)
    rhs[0:n_v-1,:] = coors.T
    rhs[n_v-1,:] = 1.0

    bc = nla.solve(mtx, rhs)

    return bc

def points_in_simplex(coors, s_coors, eps=1e-8):
    """
    Test if points with coordinates `coors` are in the simplex given by
    `s_coors`.
    """
    n_c, dim = coors.shape
    bc = barycentric_coors(coors, s_coors)
    flag = nm.ones((n_c,), dtype=nm.bool)
    for idim in xrange(dim + 1):
        flag &= nm.where((bc[idim,:] > -eps)
                         & (bc[idim,:] < (1.0 + eps)), True, False)
    return flag

def flag_points_in_polygon2d(polygon, coors):
    """
    Test if points are in a 2D polygon.

    Parameters
    ----------
    polygon : array, (:, 2)
        The polygon coordinates.
    coors: array, (:, 2)
        The coordinates of points.

    Returns
    -------
    flag : bool array
        The flag that is True for points that are in the polygon.

    Notes
    -----
    This is a semi-vectorized version of [1].

    [1] PNPOLY - Point Inclusion in Polygon Test, W. Randolph Franklin (WRF)
    """
    flag = nm.zeros(coors.shape[0], dtype=nm.bool)
    nv = polygon.shape[0]
    px, py = coors[:, 0], coors[:, 1]
    for ii in xrange(nv):
        vix, viy = polygon[ii, 0], polygon[ii, 1]
        vjx, vjy = polygon[ii-1, 0], polygon[ii-1, 1]
        aux = nm.where((viy > py) != (vjy > py))
        flag[aux] = nm.where((px[aux] < (vjx - vix)
                              * (py[aux] - viy) / (vjy - viy) + vix),
                             ~flag[aux], flag[aux])
    return flag

def inverse_element_mapping(coors, e_coors, eval_base, ref_coors,
                            suppress_errors=False):
    """
    Given spatial element coordinates, find the inverse mapping for
    points with coordinats X = X(xi), i.e. xi = xi(X).

    Returns
    -------
    xi : array
        The reference element coordinates.
    """
    n_v, dim = e_coors.shape
    if coors.ndim == 2:
        n_c, dim2 = coors.shape
    else:
        n_c, dim2 = 1, coors.shape[0]

    assert_(dim == dim2)

    if n_v == (dim + 1): # Simplex.
        bc = barycentric_coors(coors, e_coors)
        xi = nm.dot(bc.T, ref_coors)

    else: # Tensor-product and other.
        def residual(xi):
            bf = eval_base(xi[nm.newaxis,:],
                           suppress_errors=suppress_errors).squeeze()
            res = coors - nm.dot(bf, e_coors)
            return res.squeeze()

        def matrix(xi):
            bfg = eval_base(xi[nm.newaxis,:], diff=True,
                            suppress_errors=suppress_errors).squeeze()
            mtx = - nm.dot(bfg, e_coors)
            return mtx

        xi0 = nm.array([0.0, 0.0, 0.0])
        xi = mini_newton(residual, xi0, matrix)

    return xi

def get_perpendiculars(vec):
    """
    For a given vector, get a unit vector perpendicular to it in 2D, or get two
    mutually perpendicular unit vectors perpendicular to it in 3D.
    """
    nvec = nm.linalg.norm(vec)
    vec /= nvec

    if vec.shape[0] == 2:
        out = nm.array([vec[1], -vec[0]], dtype=nm.float64)

    else:
        aux = nm.array([0.0, 0.0, 1.0], dtype=nm.float64)

        v1 = nm.cross(vec, aux)
        if nm.linalg.norm(v1) < 0.1:
            # vec and aux close to being co-linear.
            aux = nm.array([0.0, 1.0, 0.0], dtype=nm.float64)

            v1 = nm.cross(vec, aux)

        v1 /= nm.linalg.norm(v1)

        v2 = nm.cross(vec, v1)
        v2 /= nm.linalg.norm(v2)

        out = (v1, v2)

    return out

def get_face_areas(faces, coors):
    """
    Get areas of planar convex faces in 2D and 3D.

    Parameters
    ----------
    faces : array, shape (n, m)
        The indices of `n` faces with `m` vertices into `coors`.
    coors : array
        The coordinates of face vertices.

    Returns
    -------
    areas : array
        The areas of the faces.
    """
    faces = nm.asarray(faces)
    coors = nm.asarray(coors)

    n_v = faces.shape[1]

    if n_v == 3:
        aux = coors[faces]
        v1 = aux[:, 1, :] - aux[:, 0, :]
        v2 = aux[:, 2, :] - aux[:, 0, :]
        if coors.shape[1] == 3:
            areas = 0.5 * norm(nm.cross(v1, v2))

        else:
            areas = 0.5 * nm.abs(nm.cross(v1, v2))

    elif n_v == 4:
        areas1 = get_face_areas(faces[:, [0, 1, 2]], coors)
        areas2 = get_face_areas(faces[:, [0, 2, 3]], coors)

        areas = areas1 + areas2

    else:
        raise ValueError('unsupported faces! (%d vertices)' % n_v)

    return areas

def rotation_matrix2d(angle):
    """
    Construct a 2D (plane) rotation matrix corresponding to `angle`.
    """
    angle *= nm.pi / 180.0
    mtx = nm.array([[nm.cos(angle), -nm.sin(angle)],
                    [nm.sin(angle), nm.cos(angle)]], dtype=nm.float64)
    return mtx

def make_axis_rotation_matrix(direction, angle):
    r"""
    Create a rotation matrix :math:`\ull{R}` corresponding to the
    rotation around a general axis :math:`\ul{d}` by a specified angle
    :math:`\alpha`.

    .. math::
        \ull{R} = \ul{d}\ul{d}^T + \cos(\alpha) (I - \ul{d}\ul{d}^T) +
        \sin(\alpha) \skewop(\ul{d})

    Parameters
    ----------
    direction : array
        The rotation axis direction vector :math:`\ul{d}`.
    angle : float
        The rotation angle :math:`\alpha`.

    Returns
    -------
    mtx : array
        The rotation matrix :math:`\ull{R}`.

    Notes
    -----
    The matrix follows the right hand rule: if the right hand thumb
    points along the axis vector :math:`\ul{d}` the fingers show the
    positive angle rotation direction.

    Examples
    --------
    Make transformation matrix for rotation of coordinate system by 90
    degrees around 'z' axis.

    >>> mtx = make_axis_rotation_matrix([0., 0., 1.], nm.pi/2)
    >>> mtx
    array([[ 0.,  1.,  0.],
           [-1.,  0.,  0.],
           [ 0.,  0.,  1.]])

    Coordinates of vector :math:`[1, 0, 0]^T` w.r.t. the original system
    in the rotated system. (Or rotation of the vector by -90 degrees in
    the original system.)

    >>> nm.dot(mtx, [1., 0., 0.])
    >>> array([ 0., -1.,  0.])

    Coordinates of vector :math:`[1, 0, 0]^T` w.r.t. the rotated system
    in the original system. (Or rotation of the vector by +90 degrees in
    the original system.)

    >>> nm.dot(mtx.T, [1., 0., 0.])
    >>> array([ 0.,  1.,  0.])
    """
    d = nm.array(direction, dtype=nm.float64)
    d /= nm.linalg.norm(d)

    eye = nm.eye(3, dtype=nm.float64)
    ddt = nm.outer(d, d)
    skew = nm.array([[    0,  d[2],  -d[1]],
                     [-d[2],     0,  d[0]],
                     [d[1], -d[0],    0]], dtype=nm.float64)

    mtx = ddt + nm.cos(angle) * (eye - ddt) + nm.sin(angle) * skew
    return mtx

def get_coors_in_tube(coors, centre, axis, radius_in, radius_out, length,
                      inside_radii=True):
    """
    Return indices of coordinates inside a tube given by
    centre, axis vector, inner and outer radii and length.

    Parameters
    ----------
    inside_radii : bool, optional
        If False, select points outside the radii, but within the tube
        length.

    Notes
    -----
    All float comparisons are done using `<=` or `>=` operators,
    i.e. the points on the boundaries are taken into account.
    """
    coors = nm.asarray(coors)
    centre = nm.asarray(centre)

    vec = coors - centre[None, :]

    drv = nm.cross(axis, vec, axisb=1)
    dr = nm.sqrt(nm.sum(drv * drv, 1))
    dl = nm.dot(vec, axis)

    l2 = 0.5 * length

    if inside_radii:
        out = nm.where((dl >= -l2) & (dl <= l2) &
                       (dr >= radius_in) & (dr <= radius_out))[0]

    else:
        out = nm.where((dl >= -l2) & (dl <= l2) &
                       (dr <= radius_in) & (dr >= radius_out))[0]

    return out

def get_coors_in_ball(coors, centre, radius, inside=True):
    """
    Return indices of coordinates inside or outside a ball given by
    centre and radius.

    Notes
    -----
    All float comparisons are done using `<=` or `>=` operators,
    i.e. the points on the boundaries are taken into account.
    """
    coors = nm.asarray(coors)
    centre = nm.asarray(centre)

    vec = coors - centre[None, :]

    if inside:
        out = nm.where(norm(vec) <= radius)[0]

    else:
        out = nm.where(norm(vec) >= radius)[0]

    return out

########NEW FILE########
__FILENAME__ = sparse
"""Some sparse matrix utilities missing in scipy."""
import numpy as nm
import scipy.sparse as sp

from sfepy.base.base import assert_

def save_sparse_txt(filename, mtx, fmt='%d %d %f\n'):
    """Save a CSR/CSC sparse matrix into a text file"""
    fd = open(filename, 'w')

    fd.write('%d %d\n' % mtx.shape)
    fd.write('%d\n' % mtx.size)

    if mtx.format == 'csr':
        indptr, indices, data = mtx.indptr, mtx.indices, mtx.data
        for ir in xrange(mtx.shape[0]):
            for ii in xrange(indptr[ir], indptr[ir+1]):
                fd.write(fmt % (ir, indices[ii], data[ii]))

    elif mtx.format == 'csc':
        indptr, indices, data = mtx.indptr, mtx.indices, mtx.data
        for ic in xrange(mtx.shape[0]):
            for ii in xrange(indptr[ir], indptr[ir+1]):
                fd.write(fmt % (indices[ii], ic, data[ii]))

    else:
        raise ValueError('matrix format not supported! (%s)' % mtx.format)

def insert_sparse_to_csr(mtx1, mtx2, irs, ics):
    """
    Insert a sparse matrix `mtx2` into a CSR sparse matrix `mtx1` at
    rows `irs` and columns `ics`. The submatrix `mtx1[irs,ics]` must
    already be preallocated and have the same structure as `mtx2`.
    """
    import sfepy.discrete.fem.extmods.assemble as asm

    if isinstance(irs, slice):
        irs = nm.arange(irs.start, irs.stop, irs.step, dtype=nm.int32)

    if isinstance(ics, slice):
        ics = nm.arange(ics.start, ics.stop, ics.step, dtype=nm.int32)

    n_row, n_col = mtx1.shape

    assert_((irs.min() >= 0) and (irs.max() < n_row))
    assert_((ics.min() >= 0) and (ics.max() < n_col))

    aux = mtx2.tocoo()
    data = nm.ascontiguousarray(aux.data[:,None,None,None])
    rows = irs[aux.row[:,None]]
    cols = ics[aux.col[:,None]]

    iels = nm.arange(rows.shape[0], dtype=nm.int32)
    asm.assemble_matrix(mtx1.data, mtx1.indptr, mtx1.indices, data,
                        iels, 1.0, rows, cols)

def _normalize_sizes(sizes):
    """
    Checks whether all the sizes are either slices or not. Transforms
    slices into their sizes.
    """
    out = []
    ns = 0
    for size in sizes:
        if isinstance(size, slice):
            size = size.stop - size.start
            ns += 1

        else:
            size = int(size)

        out.append(size)

    if ns:
        if ns != len(sizes):
            raise ValueError('cannot mix sizes with slices! (%s)' % (sizes,))

        is_slice = True

    else:
        is_slice = False

    return out, is_slice

def compose_sparse(blocks, row_sizes=None, col_sizes=None):
    """
    Compose sparse matrices into a global sparse matrix.

    Parameters
    ----------
    blocks : sequence of sequences
        The sequence of sequences of equal lengths - the individual
        sparse matrix blocks. The integer 0 can be used to mark an all-zero
        block, if its size can be determined from the other blocks.
    row_sizes : sequence, optional
        The required row sizes of the blocks. It can be either a
        sequence of non-negative integers, or a sequence of slices with
        non-negative limits. In any case the sizes have to be compatible
        with the true block sizes. This allows to extend the matrix
        shape as needed and to specify sizes of all-zero blocks.
    col_sizes : sequence, optional
        The required column sizes of the blocks. See `row_sizes`.

    Returns
    -------
    mtx : coo_matrix
        The sparse matrix (COO format) composed from the given blocks.

    Examples
    --------
    Stokes-like problem matrix.

    >>> import scipy.sparse as sp
    >>> A = sp.csr_matrix([[1, 0], [0, 1]])
    >>> B = sp.coo_matrix([[1, 1]])
    >>> K = compose_sparse([[A, B.T], [B, 0]])
    >>> print K.todense()
    [[1 0 1]
     [0 1 1]
     [1 1 0]]
    """
    if not len(blocks):
        raise ValueError('no matrix blocks!')

    if row_sizes is None:
        row_sizes = nm.array([-1] * len(blocks))

    else:
        assert_(len(row_sizes) == len(blocks))

    if col_sizes is None:
        col_sizes = nm.array([-1] * len(blocks[0]))

    else:
        assert_(len(col_sizes) == len(blocks[0]))

    rs, is_slice_r = _normalize_sizes(row_sizes)
    cs, is_slice_c = _normalize_sizes(col_sizes)

    for ir, row in enumerate(blocks):
        for ic, mtx in enumerate(row):
            if isinstance(mtx, int) and (mtx == 0):
                continue

            if ic >= len(col_sizes):
                raise ValueError('invalid row size at (%d, %d)!' % (ir, ic))

            if rs[ir] == -1:
                rs[ir] = mtx.shape[0]

            elif rs[ir] != mtx.shape[0]:
                msg = 'incompatible matrix block row size at (%d, %d)!' \
                      % (ir, ic)
                raise ValueError(msg)

            if cs[ic] == -1:
                cs[ic] = mtx.shape[1]

            elif cs[ic] != mtx.shape[1]:
                msg = 'incompatible matrix block column size at (%d, %d)!' \
                      % (ic, ic)
                raise ValueError(msg)

    if nm.any(rs == -1):
        raise ValueError('incomplete row block sizes! (%s)' % row_sizes)

    if nm.any(cs == -1):
        raise ValueError('incomplete column block sizes! (%s)' % col_sizes)

    if is_slice_r:
        n_row = row_sizes[-1].stop
        row_offsets = nm.r_[[ii.start for ii in row_sizes], n_row]

    else:
        row_offsets = nm.cumsum(nm.r_[0, rs])
        n_row = row_offsets[-1]

    if is_slice_c:
        n_col = col_sizes[-1].stop
        col_offsets = nm.r_[[ii.start for ii in col_sizes], n_col]

    else:
        col_offsets = nm.cumsum(nm.r_[0, cs])
        n_col = col_offsets[-1]

    rows = []
    cols = []
    datas = []
    for ir, row in enumerate(blocks):
        for ic, mtx in enumerate(row):
            if isinstance(mtx, int) and (mtx == 0):
                continue

            aux = sp.coo_matrix(mtx)

            rows.append(aux.row + row_offsets[ir])
            cols.append(aux.col + col_offsets[ic])
            datas.append(aux.data)

    rows = nm.concatenate(rows)
    cols = nm.concatenate(cols)
    datas = nm.concatenate(datas)

    mtx = sp.coo_matrix((datas, (rows, cols)), shape=(n_row, n_col))

    return mtx

def infinity_norm(mtx):
    """
    Infinity norm of a sparse matrix (maximum absolute row sum).  

    Parameters
    ----------
    mtx : spmatrix or array
        The sparse matrix.
    
    Returns
    -------
    norm : float
        Infinity norm of the matrix.
    
    Notes
    -----
    - This serves as an upper bound on spectral radius.
    - CSR and CSC avoid copying `indices` and `indptr` arrays.
    - inspired by PyAMG

    See Also
    --------
    scipy.linalg.norm : dense matrix norms
    """
    ones = nm.ones(mtx.shape[1], dtype=mtx.dtype)

    if sp.isspmatrix_csr(mtx) or sp.isspmatrix_csc(mtx):
        # Avoid copying index and ptr arrays.
        abs_mtx = mtx.__class__((nm.abs(mtx.data), mtx.indices ,mtx.indptr),
                                shape=mtx.shape)
        norm = (abs_mtx * ones).max()

    elif sp.isspmatrix(mtx):
        norm = (abs(mtx) * ones).max()

    else:
        norm = nm.dot(nm.abs(mtx), ones).max()

    return norm

########NEW FILE########
__FILENAME__ = utils
import numpy as nm
from numpy.lib.stride_tricks import as_strided
import numpy.linalg as nla
import scipy as sc

try:
    from numpy.core.umath_tests import matrix_multiply

except:
    matrix_multiply = None

from sfepy.base.base import assert_, insert_method, Struct

def norm_l2_along_axis(ar, axis=1, n_item=None, squared=False):
    """Compute l2 norm of rows (axis=1) or columns (axis=0) of a 2D array.

    n_item ... use only the first n_item columns/rows
    squared ... if True, return the norm squared
    """
    assert_(axis in [0, 1])
    assert_(ar.ndim == 2)

    other = 1 - axis
    vec = nm.zeros((ar.shape[other],), dtype=nm.float64)

    if n_item is None:
        n_item = ar.shape[axis]
    else:
        n_item = min( n_item, ar.shape[axis] )

    if axis == 1:
        for ii in xrange( n_item ):
            vec += ar[:,ii]**2
    else:
        for ii in xrange( n_item ):
            vec += ar[ii,:]**2

    if not squared:
        vec = nm.sqrt( vec )

    return vec

def normalize_vectors(vecs, eps=1e-8):
    """
    Normalize an array of vectors in place.

    Parameters
    ----------
    vecs : array
        The 2D array of vectors in rows.
    eps : float
        The tolerance for considering a vector to have zero norm. Such
        vectors are left unchanged.
    """
    norms = norm_l2_along_axis(vecs, axis=1)
    ii = norms > eps
    vecs[ii] = vecs[ii] / norms[ii][:, None]

def dets_fast(a):
    """
    Fast determinant calculation of 3-dimensional array.

    Parameters
    ----------
    a : array
        The input array with shape (m, n, n).

    Returns
    -------
    out : array
        The output array with shape (m,): out[i] = det(a[i, :, :]).
    """
    from numpy.linalg import lapack_lite
    from numpy.core import intc

    m = a.shape[0]
    n = a.shape[1]
    lapack_routine = lapack_lite.dgetrf
    pivots = nm.zeros((m, n), intc)
    flags = nm.arange(1, n + 1).reshape(1, -1)
    for i in xrange(m):
        tmp = a[i]
        lapack_routine(n, n, tmp, n, pivots[i], 0)
    sign = 1. - 2. * (nm.add.reduce(pivots != flags, axis=1) % 2)
    idx = nm.arange(n)
    d = a[:, idx, idx]
    absd = nm.absolute(d)
    sign *= nm.multiply.reduce(d / absd, axis=1)
    nm.log(absd, absd)
    logdet = nm.add.reduce(absd, axis=-1)

    return sign * nm.exp(logdet)

def print_array_info(ar):
    """
    Print array shape and other basic information.
    """
    ar = nm.asanyarray(ar)

    print ar.shape, 'c_contiguous:', ar.flags.c_contiguous, \
          'f_contiguous:', ar.flags.f_contiguous
    print 'min:', ar.min(), 'mean:', ar.mean(), 'max:', ar.max()

##
# 21.11.2005, c
def split_range( n_item, step ):
    num = n_item / step
    out = [step] * num
    aux = sum( out )
    if aux < n_item:
        out.append( n_item - aux )

    return out

##
# Inspired on net (ASPN Recipec).
# 14.12.2005, c
def permutations( seq ):

    ls = len( seq )

    if ls <= 1:
        yield seq
    else:
        for ii in xrange( ls ):
            for perm in permutations( seq[:ii] + seq[ii+1:] ):
                yield [seq[ii]] + perm

##
# 14.12.2005, c
def cycle( bounds ):
    """
    Cycles through all combinations of bounds, returns a generator.

    More specifically, let bounds=[a, b, c, ...], so cycle returns all
    combinations of lists [0<=i<a, 0<=j<b, 0<=k<c, ...] for all i,j,k,...

    Examples:
    In [9]: list(cycle([3, 2]))
    Out[9]: [[0, 0], [0, 1], [1, 0], [1, 1], [2, 0], [2, 1]]

    In [14]: list(cycle([3, 4]))
    [[0, 0], [0, 1], [0, 2], [0, 3], [1, 0], [1, 1], [1, 2], [1, 3], [2, 0],
    [2, 1], [2, 2], [2, 3]]

    """

    nb  = len( bounds )
    if nb == 1:
        for ii in xrange( bounds[0] ):
            yield [ii]
    else:
        for ii in xrange( bounds[0] ):
            for perm in cycle( bounds[1:] ):
                yield [ii] + perm

def combine( seqs ):
    """Same as cycle, but with general sequences.

    Example:

    In [19]: c = combine( [['a', 'x'], ['b', 'c'], ['dd']] )

    In [20]: list(c)
    Out[20]: [['a', 'b', 'dd'], ['a', 'c', 'dd'], ['x', 'b', 'dd'],
    ['x', 'c', 'dd']]
    """
    nb  = len( seqs )
    if nb == 1:
        for ii in seqs[0]:
            yield [ii]
    else:
        for ii in seqs[0]:
            for perm in combine( seqs[1:] ):
                yield [ii] + perm

def assemble1d(ar_out, indx, ar_in):
    """
    Perform `ar_out[indx] += ar_in`, where items of `ar_in`
    corresponding to duplicate indices in `indx` are summed together.
    """
    if len(indx) > 0:
        zz = nm.zeros_like(indx)
        aux = sc.sparse.coo_matrix((ar_in, (indx, zz)), dtype=ar_in.dtype)
        aux = aux.tocsr().tocoo() # This sums the duplicates.

        ar_out[aux.row] += aux.data

def unique_rows(ar, return_index=False, return_inverse=False):
    """
    Return unique rows of a two-dimensional array `ar`. The arguments follow
    `numpy.unique()`.
    """
    ar = nm.ascontiguousarray(ar)

    # View the rows as a 1D structured array.
    arv = ar.view(ar.shape[1] * [('', ar.dtype)])
    out = nm.unique(arv, return_index=return_index,
                    return_inverse=return_inverse)
    if isinstance(out, tuple):
        uarv = out[0]

    else:
        uarv = out

    # Restore the original dimensions.
    uar = uarv.view(ar.dtype).reshape((-1, ar.shape[1]))

    if isinstance(out, tuple):
        out = (uar,) + out[1:]

    else:
        out = uar

    return out

def argsort_rows(seq):
    """
    Returns an index array that sorts the sequence `seq`. Works along
    rows if `seq` is two-dimensional.
    """
    seq = nm.asanyarray(seq)
    if seq.ndim == 1:
        ii = nm.argsort(seq)

    else:
        ii = nm.lexsort(seq.T[::-1])

    return ii

def map_permutations(seq1, seq2, check_same_items=False):
    """
    Returns an index array `imap` such that `seq1[imap] == seq2`, if
    both sequences have the same items - this is not checked by default!

    In other words, finds the indices of items of `seq2` in `seq1`.
    """
    assert_(len(seq1) == len(seq2))

    seq1 = nm.asanyarray(seq1)
    seq2 = nm.asanyarray(seq2)

    i1 = argsort_rows(seq1)
    i2 = argsort_rows(seq2)

    if check_same_items:
        assert_(seq1.shape == seq2.shape)
        assert_((seq1[i1] == seq2[i2]).all())

    ii = nm.argsort(i2)

    imap = i1[ii]

    return imap

def mini_newton( fun, x0, dfun, i_max = 100, eps = 1e-8 ):
    x = x0
    ii = 0
    while ii < i_max:
        r = fun( x )
        err = nla.norm( r )
##         print ii, x, r, err
        if err < eps: break

        mtx = dfun( x )
        try:
            dx = nm.dot( nla.inv( mtx.T ), r )
        except:
            break
        x = x - dx
        ii += 1
    return x

def insert_strided_axis(ar, axis, length):
    """
    Insert a new axis of given length into an array using numpy stride
    tricks, i.e. no copy is made.

    Parameters
    ----------
    ar : array
        The input array.
    axis : int
        The axis before which the new axis will be inserted.
    length : int
        The length of the inserted axis. 

    Returns
    -------
    out : array
        The output array sharing data with `ar`.

    Examples
    --------
    >>> import numpy as nm
    >>> from sfepy.linalg import insert_strided_axis
    >>> ar = nm.random.rand(2, 1, 2)
    >>> ar
    array([[[ 0.18905119,  0.44552425]],

           [[ 0.78593989,  0.71852473]]])
    >>> ar.shape
    (2, 1, 2)
    >>> ar2 = insert_strided_axis(ar, 1, 3)
    >>> ar2
    array([[[[ 0.18905119,  0.44552425]],

            [[ 0.18905119,  0.44552425]],

            [[ 0.18905119,  0.44552425]]],


           [[[ 0.78593989,  0.71852473]],

            [[ 0.78593989,  0.71852473]],

            [[ 0.78593989,  0.71852473]]]])
    >>> ar2.shape
    (2, 3, 1, 2)
    """
    shape = list(ar.shape)
    shape.insert(axis, length)

    strides = list(ar.strides)
    strides.insert(axis, 0)

    out = as_strided(ar, shape=shape, strides=strides)
    return out

def dot_sequences(mtx, vec, mode='AB'):
    """
    Computes dot product for each pair of items in the two sequences.

    Equivalent to

    >>> out = nm.empty((vec.shape[0], mtx.shape[1], vec.shape[2]),
    >>>                dtype=vec.dtype)
    >>> for ir in range(mtx.shape[0]):
    >>>     out[ir] = nm.dot(mtx[ir], vec[ir])

    Parameters
    ----------
    mtx : array
        The array of matrices with shape `(n_item, m, n)`.
    vec : array
        The array of vectors with shape `(n_item, a)` or matrices with shape
        `(n_item, a, b)`.
    mode : one of 'AB', 'ATB', 'ABT', 'ATBT'

        The mode of the dot product - the corresponding axes are dotted
        together:

        'AB'   : `a = n`
        'ATB'  : `a = m`
        'ABT'  : `b = n` (*)
        'ATBT' : `b = m` (*)

        (*) The 'BT' part is ignored for the vector second argument.

    Returns
    -------
    out : array
       The resulting array.

    Notes
    -----
    Uses `numpy.core.umath_tests.matrix_multiply()` if available, which is much 
    faster than the default implementation.

    The default implementation uses `numpy.sum()` and element-wise
    multiplication. For r-D arrays `(n_1, ..., n_r, ?, ?)` the arrays
    are first reshaped to `(n_1 * ... * n_r, ?, ?)`, then the dot is
    performed, and finally the shape is restored to `(n_1, ..., n_r, ?, ?)`.
    """
    if matrix_multiply is not None:
        if vec.ndim == mtx.ndim:
            squeeze = False

        else:
            squeeze = True
            vec = vec[..., None]

        if 'BT' in mode:
            ax = range(vec.ndim)
            vec = vec.transpose((ax[:-2]) + [ax[-1], ax[-2]])

        if 'AT' in mode:
            ax = range(mtx.ndim)
            mtx = mtx.transpose((ax[:-2]) + [ax[-1], ax[-2]])

        out = matrix_multiply(mtx, vec)
        if squeeze:
            out = out[..., 0]

    else:
        if (vec.ndim == 2) and (mtx.ndim == 3):
            if mode in ('AB', 'ABT'):
                out = nm.sum(mtx * vec[:, None, :], axis=2)

            else:
                out = nm.sum(mtx * vec[:, :, None], axis=1)

        elif (vec.ndim == 3) and (mtx.ndim == 3):

            if mode == 'AB':
                out = nm.empty((vec.shape[0], mtx.shape[1], vec.shape[2]),
                               dtype=vec.dtype)

                for ic in range(vec.shape[2]):
                    out[:, :, ic] = dot_sequences(mtx, vec[:, :, ic], mode=mode)

            elif mode == 'ABT':
                out = nm.empty((vec.shape[0], mtx.shape[1], vec.shape[1]),
                               dtype=vec.dtype)

                for ic in range(vec.shape[1]):
                    out[:, :, ic] = dot_sequences(mtx, vec[:, ic, :], mode=mode)


            elif mode == 'ATB':
                out = nm.empty((vec.shape[0], mtx.shape[2], vec.shape[2]),
                               dtype=vec.dtype)

                for ic in range(vec.shape[2]):
                    out[:, :, ic] = dot_sequences(mtx, vec[:, :, ic], mode=mode)

            elif mode == 'ATBT':
                out = nm.empty((vec.shape[0], mtx.shape[2], vec.shape[1]),
                               dtype=vec.dtype)

                for ic in range(vec.shape[1]):
                    out[:, :, ic] = dot_sequences(mtx, vec[:, ic, :], mode=mode)

            else:
                raise ValueError('unknown dot mode! (%s)' % mode)

        elif (vec.ndim >= 4) and (mtx.ndim >= 4) and (vec.ndim == mtx.ndim):
            mtx_seq = nm.reshape(mtx,
                                 (nm.prod(mtx.shape[0:-2], dtype=int),)
                                 + mtx.shape[-2:])

            vec_seq = nm.reshape(vec,
                                 (nm.prod(vec.shape[0:-2], dtype=int),)
                                 + vec.shape[-2:])

            out_seq = dot_sequences(mtx_seq, vec_seq, mode=mode)
            out = nm.reshape(out_seq, mtx.shape[0:-2] + out_seq.shape[-2:])

        else:
            raise ValueError('unsupported operand shape')

    return out

def apply_to_sequence(seq, fun, ndim, out_item_shape):
    """
    Applies function `fun()` to each item of the sequence `seq`. An item
    corresponds to the last `ndim` dimensions of `seq`.

    Parameters
    ----------
    seq : array
        The sequence array with shape `(n_1, ..., n_r, m_1, ..., m_{ndim})`.
    fun : function
        The function taking an array argument of shape of length `ndim`.
    ndim : int
        The number of dimensions of an item in `seq`.
    out_item_shape : tuple
        The shape an output item.

    Returns
    -------
    out : array
       The resulting array of shape `(n_1, ..., n_r) + out_item_shape`. The
       `out_item_shape` must be compatible with the `fun`.
    """
    n_seq = nm.prod(seq.shape[0:-ndim], dtype=int)
    aux = nm.reshape(seq, (n_seq,) + seq.shape[-ndim:])

    out = nm.empty((n_seq,) + out_item_shape, dtype=seq.dtype)
    for ii, item in enumerate(aux):
        out[ii,:] = fun(item)

    out = nm.reshape(out, seq.shape[0:-ndim] + out_item_shape)

    return out

##
# 30.08.2007, c
class MatrixAction( Struct ):

    ##
    # 30.08.2007, c
    def from_function( fun, expected_shape, dtype ):
        def call( self, vec ):
            aux = fun( vec )
            assert_( aux.shape[0] == self.shape[0] )
            return nm.asanyarray( aux, dtype = self.dtype )
        obj = MatrixAction( shape = expected_shape,
                            dtype = dtype,
                            kind = 'function' )
        insert_method( obj, call )
        return obj
    from_function = staticmethod( from_function )

    ##
    # 30.08.2007, c
    def from_array( arr ):
        def call( self, vec ):
            return nm.asarray( sc.dot( self.arr, vec ) )
        obj = MatrixAction( shape = arr.shape,
                            dtype = arr.dtype,
                            arr = arr,
                            kind = 'array' )
        insert_method( obj, call )
        return obj
    from_array = staticmethod( from_array )
    
    ##
    # 30.08.2007, c
    def __call__( self, vec ):
        return self.call( vec )

    ##
    # 30.08.2007, c
    def to_array( self ):
        if self.kind == 'array':
            return self.arr
        else:
            print 'cannot make array from MatrixAction of kind %s!' % self.kind
            raise ValueError

########NEW FILE########
__FILENAME__ = contact_bodies
import numpy as nm

from sfepy.base.base import assert_, Struct
import sfepy.linalg as la

class ContactPlane(Struct):

    def __init__(self, anchor, normal, bounds):
        Struct.__init__(self, anchor=nm.array(anchor, dtype=nm.float64),
                        bounds=nm.asarray(bounds, dtype=nm.float64))
        self.normal = nm.asarray(normal, dtype=nm.float64)

        norm = nm.linalg.norm
        self.normal /= norm(self.normal)

        e3 = [0.0, 0.0, 1.0]
        dd = nm.dot(e3, self.normal)
        rot_angle = nm.arccos(dd)

        if nm.abs(rot_angle) < 1e-14:
            mtx = nm.eye(3, dtype=nm.float64)
            bounds2d = self.bounds[:, :2]

        else:
            rot_axis = nm.cross([0.0, 0.0, 1.0], self.normal)
            mtx = la.make_axis_rotation_matrix(rot_axis, rot_angle)

            mm = la.insert_strided_axis(mtx, 0, self.bounds.shape[0])
            rbounds = la.dot_sequences(mm, self.bounds)
            bounds2d = rbounds[:, :2]

        assert_(nm.allclose(nm.dot(mtx, self.normal), e3,
                            rtol=0.0, atol=1e-12))

        self.adotn = nm.dot(self.anchor, self.normal)

        self.rot_angle = rot_angle
        self.mtx = mtx
        self.bounds2d = bounds2d

    def mask_points(self, points):
        mm = la.insert_strided_axis(self.mtx, 0, points.shape[0])
        points2d = la.dot_sequences(mm, points)[:, :2]

        return la.flag_points_in_polygon2d(self.bounds2d, points2d)

    def get_distance(self, points):
        dist = la.dot_sequences(points, self.normal) - self.adotn

        return dist

class ContactSphere(Struct):

    def __init__(self, centre, radius):
        self.centre = nm.asarray(centre)
        self.radius = radius

    def mask_points(self, points, eps):
        dist2 = la.norm_l2_along_axis(points - self.centre, squared=True)
        radius2 = self.radius**2
        mask = dist2 <= ((1 + eps)**2) * radius2
        return mask

    def get_distance(self, points):
        """
        Get the penetration distance and normals of points w.r.t. the sphere
        surface.

        Returns
        -------
        d : array
            The penetration distance.
        normals : array
            The normals from the points to the sphere centre.
        """
        vecs = self.centre - points
        dist = la.norm_l2_along_axis(vecs)
        # Prevent zero division.
        ii = dist > 1e-8
        normals = nm.where(ii[:, None], vecs[ii] / dist[ii][:, None],
                           vecs[ii])
        return self.radius - dist, normals

    def _get_derivatives(self, points):
        vecs = self.centre - points
        dist = la.norm_l2_along_axis(vecs)

        # Distance derivative w.r.t. point coordinates.
        dd = vecs / dist[:, None]

        normals = dd
        # Unit normal derivative w.r.t. point coordinates.
        dim = points.shape[1]
        ee = nm.eye(dim)[None, ...]
        nnt = normals[..., None] * normals[..., None, :]
        dn = - (ee - nnt) / dist[:, None, None]

        return dd, dn

def plot_polygon(ax, polygon):
    from sfepy.postprocess.plot_dofs import _get_axes

    dim = polygon.shape[1]
    ax = _get_axes(ax, dim)

    pp = nm.r_[polygon, polygon[:1]]
    px, py = pp[:, 0], pp[:, 1]
    if dim == 2:
        ax.plot(px, py)

    else:
        pz = pp[:, 2]
        ax.plot(px, py, pz)

    return ax

def plot_points(ax, points, marker, **kwargs):
    from sfepy.postprocess.plot_dofs import _get_axes

    dim = points.shape[1]
    ax = _get_axes(ax, dim)

    px, py = points[:, 0], points[:, 1]
    if dim == 2:
        ax.plot(px, py, marker, **kwargs)

    else:
        pz = points[:, 2]
        ax.plot(px, py, pz, marker, **kwargs)

    return ax

if __name__ == '__main__':
    import matplotlib.pyplot as plt

    # Test and draw the plane.
    anchor = [1, 1, 1]
    normal = [2, -1, 1]
    bounds = [[-2, 0, 0],
              [2, 1, 0],
              [4, 3, 1],
              [1, 3, 1],
              [2, 2, 1]]
    cp = ContactPlane(anchor, normal, bounds)

    pps = 2 * nm.random.rand(20, 3)
    mask = cp.mask_points(pps)

    dist = cp.get_distance(pps)

    v1, v2 = la.get_perpendiculars(cp.normal)

    ax = plot_polygon(None, cp.bounds)
    ax = plot_polygon(ax, nm.r_[cp.anchor[None, :],
                                cp.anchor[None, :] + cp.normal[None, :]])
    ax = plot_polygon(ax, nm.r_[cp.anchor[None, :],
                                cp.anchor[None, :] + v1])
    ax = plot_polygon(ax, nm.r_[cp.anchor[None, :],
                                cp.anchor[None, :] + v2])
    ax = plot_points(ax, cp.anchor[None, :], 'r*')
    ax = plot_points(ax, pps[mask], 'bs', ms=10, mec='None')
    ax = plot_points(ax, pps[~mask], 'go', ms=10, mec='None')

    mask = dist >= 0.0
    ax = plot_points(ax, pps[mask], 'r^', mec='None')
    ax = plot_points(ax, pps[~mask], 'kv', mec='None')

    # Test and draw the sphere.
    pps = nm.random.rand(5000, 3)

    centre = [0, 0.5, 0.5]
    radius = 0.8
    cs = ContactSphere(centre, radius)
    mask = cs.mask_points(pps, 0.0)
    dist = cs.get_distance(pps)

    ax = plot_points(None, cs.centre[None, :], 'b*', ms=30)
    ax = plot_points(ax, pps[mask], 'kv')
    ax = plot_points(ax, pps[~mask], 'r.')

    plt.show()

########NEW FILE########
__FILENAME__ = elastic_constants

from __future__ import division

import sympy as sm

names = ['bulk', 'lam', 'mu', 'young', 'poisson', 'p_wave']
bulk, lam, mu, young, poisson, p_wave = sm.symbols(names, real=True)

relations = {
    ('mu', 'poisson', 'young') : 2*mu + 2*mu*poisson,
    ('p_wave', 'poisson', 'mu') : (p_wave - 2*p_wave*poisson)/(2 - 2*poisson),
    ('bulk', 'lam', 'poisson') : lam/(-lam + 3*bulk),
    ('bulk', 'poisson', 'lam') : 3*bulk*poisson/(1 + poisson),
    ('lam', 'poisson', 'p_wave') : (lam - lam*poisson)/poisson,
    ('p_wave', 'poisson', 'lam') : p_wave*poisson/(1 - poisson),
    ('bulk', 'young', 'p_wave') : (3*bulk*young + 9*bulk**2)/(-young + 9*bulk),
    ('p_wave', 'poisson', 'bulk') : (p_wave + p_wave*poisson)/(3 - 3*poisson),
    ('bulk', 'mu', 'p_wave') : bulk + 4*mu/3,
    ('lam', 'mu', 'poisson') : lam/(2*lam + 2*mu),
    ('bulk', 'mu', 'poisson') : (-2*mu + 3*bulk)/(2*mu + 6*bulk),
    ('lam', 'mu', 'p_wave') : lam + 2*mu,
    ('bulk', 'p_wave', 'poisson') : (-p_wave + 3*bulk)/(p_wave + 3*bulk),
    ('bulk', 'p_wave', 'lam') : -p_wave/2 + 3*bulk/2,
    ('lam', 'p_wave', 'mu') : p_wave/2 - lam/2,
    ('bulk', 'p_wave', 'young') : (-9*bulk*p_wave + 9*bulk**2)/(-p_wave - 3*bulk),
    ('bulk', 'poisson', 'p_wave') : (3*bulk - 3*bulk*poisson)/(1 + poisson),
    ('p_wave', 'young', 'bulk') : p_wave/2 - young/6 + (-90*p_wave*young + 9*young**2 + 81*p_wave**2)**(1/2)/18,
    ('bulk', 'poisson', 'young') : 3*bulk - 6*bulk*poisson,
    ('lam', 'mu', 'young') : (3*lam*mu + 2*mu**2)/(lam + mu),
    ('lam', 'p_wave', 'young') : (lam*p_wave + p_wave**2 - 2*lam**2)/(lam + p_wave),
    ('lam', 'young', 'poisson') : (-lam - young + (2*lam*young + young**2 + 9*lam**2)**(1/2))/(4*lam),
    ('bulk', 'mu', 'young') : 9*bulk*mu/(mu + 3*bulk),
    ('mu', 'young', 'p_wave') : (-mu*young + 4*mu**2)/(-young + 3*mu),
    ('bulk', 'poisson', 'mu') : (3*bulk - 6*bulk*poisson)/(2 + 2*poisson),
    ('mu', 'young', 'poisson') : (young - 2*mu)/(2*mu),
    ('mu', 'p_wave', 'bulk') : p_wave - 4*mu/3,
    ('lam', 'young', 'mu') : -3*lam/4 + young/4 + (2*lam*young + young**2 + 9*lam**2)**(1/2)/4,
    ('lam', 'poisson', 'bulk') : (lam + lam*poisson)/(3*poisson),
    ('lam', 'poisson', 'young') : (lam - lam*poisson - 2*lam*poisson**2)/poisson,
    ('mu', 'p_wave', 'lam') : p_wave - 2*mu,
    ('mu', 'p_wave', 'poisson') : (-p_wave + 2*mu)/(-2*p_wave + 2*mu),
    ('poisson', 'young', 'bulk') : young/(3 - 6*poisson),
    ('mu', 'p_wave', 'young') : (-3*mu*p_wave + 4*mu**2)/(mu - p_wave),
    ('bulk', 'young', 'lam') : (-3*bulk*young + 9*bulk**2)/(-young + 9*bulk),
    ('lam', 'p_wave', 'poisson') : lam/(lam + p_wave),
    ('bulk', 'young', 'mu') : 3*bulk*young/(-young + 9*bulk),
    ('bulk', 'lam', 'p_wave') : -2*lam + 3*bulk,
    ('lam', 'young', 'p_wave') : young/2 - lam/2 + (2*lam*young + young**2 + 9*lam**2)**(1/2)/2,
    ('bulk', 'young', 'poisson') : (-2*young + 6*bulk)/(12*bulk),
    ('poisson', 'young', 'lam') : poisson*young/(1 - poisson - 2*poisson**2),
    ('mu', 'young', 'lam') : (-mu*young + 2*mu**2)/(young - 3*mu),
    ('lam', 'poisson', 'mu') : (lam - 2*lam*poisson)/(2*poisson),
    ('bulk', 'lam', 'mu') : -3*lam/2 + 3*bulk/2,
    ('poisson', 'young', 'mu') : young/(2 + 2*poisson),
    ('lam', 'p_wave', 'bulk') : p_wave/3 + 2*lam/3,
    ('p_wave', 'young', 'poisson') : (young - p_wave + (-10*p_wave*young + young**2 + 9*p_wave**2)**(1/2))/(4*p_wave),
    ('p_wave', 'young', 'mu') : young/8 + 3*p_wave/8 - (-10*p_wave*young + young**2 + 9*p_wave**2)**(1/2)/8,
    ('mu', 'poisson', 'p_wave') : (2*mu - 2*mu*poisson)/(1 - 2*poisson),
    ('lam', 'mu', 'bulk') : lam + 2*mu/3,
    ('p_wave', 'young', 'lam') : -young/4 + p_wave/4 + (-10*p_wave*young + young**2 + 9*p_wave**2)**(1/2)/4,
    ('bulk', 'mu', 'lam') : bulk - 2*mu/3,
    ('mu', 'poisson', 'bulk') : (2*mu + 2*mu*poisson)/(3 - 6*poisson),
    ('poisson', 'young', 'p_wave') : (young - poisson*young)/(1 - poisson - 2*poisson**2),
    ('mu', 'young', 'bulk') : mu*young/(-3*young + 9*mu),
    ('p_wave', 'poisson', 'young') : (p_wave - p_wave*poisson - 2*p_wave*poisson**2)/(1 - poisson),
    ('bulk', 'lam', 'young') : (-9*bulk*lam + 9*bulk**2)/(-lam + 3*bulk),
    ('mu', 'poisson', 'lam') : 2*mu*poisson/(1 - 2*poisson),
    ('bulk', 'p_wave', 'mu') : -3*bulk/4 + 3*p_wave/4,
    ('lam', 'young', 'bulk') : lam/2 + young/6 + (18*lam*young + 9*young**2 + 81*lam**2)**(1/2)/18
}
        
########NEW FILE########
__FILENAME__ = matcoefs
# -*- coding: utf-8 -*-
"""
Conversion of material parameters and other utilities.
"""
import os

import numpy as nm

from sfepy.base.base import Struct

def lame_from_youngpoisson(young, poisson, plane='strain'):
    r"""
    Compute Lam parameters from Young's modulus and Poisson's ratio.

    The relationship between Lam parameters and Young's modulus, Poisson's
    ratio (see [1],[2]):

    .. math::
        \lambda = {\nu E \over (1+\nu)(1-2\nu)},\qquad \mu = {E \over 2(1+\nu)}

    The plain stress hypothesis:

    .. math::
       \bar\lambda = {2\lambda\mu \over \lambda + 2\mu}

    [1] I.S. Sokolnikoff: Mathematical Theory of Elasticity. New York, 1956.

    [2] T.J.R. Hughes: The Finite Element Method, Linear Static and Dynamic
    Finite Element Analysis. New Jersey, 1987.
    """
    mu = young/(2.0*(1.0 + poisson))
    lam = young*poisson/((1.0 + poisson)*(1.0 - 2.0*poisson))

    if plane == 'stress':
        lam = 2*lam*mu/(lam + 2*mu)

    return lam, mu

def stiffness_from_lame(dim, lam, mu):
    r"""
    Compute stiffness tensor corresponding to Lam parameters.

    .. math::
        {\bm D}_{(2D)} = \begin{bmatrix} \lambda + 2\mu & \lambda & 0\\
        \lambda & \lambda + 2\mu & 0\\ 0 & 0 & \mu \end{bmatrix}

    .. math::
        {\bm D}_{(3D)} = \begin{bmatrix} \lambda + 2\mu & \lambda &
        \lambda & 0 & 0 & 0\\ \lambda & \lambda + 2\mu & \lambda & 0 & 0 & 0 \\
        \lambda & \lambda & \lambda + 2\mu & 0 & 0 & 0 \\ 0 & 0 & 0 & \mu & 0 &
        0 \\ 0 & 0 & 0 & 0 & \mu & 0 \\ 0 & 0 & 0 & 0 & 0 & \mu\\ \end{bmatrix}
    """
    sym = (dim + 1) * dim / 2
    o = nm.array([1.] * dim + [0.] * (sym - dim), dtype=nm.float64)
    oot = nm.outer(o, o)[None, ...]
    do1 = nm.diag(o + 1.0)[None, ...]

    lam = nm.array(lam, ndmin=1)[:, None, None]
    mu = nm.array(mu, ndmin=1)[:, None, None]

    return (lam * oot + mu * do1).squeeze()

def stiffness_from_youngpoisson(dim, young, poisson, plane='strain'):
    """
    Compute stiffness tensor corresponding to Young's modulus and Poisson's
    ratio.
    """
    lam, mu = lame_from_youngpoisson(young, poisson, plane)

    return stiffness_from_lame(dim, lam, mu)

def stiffness_from_lame_mixed(dim, lam, mu):
    r"""
    Compute stiffness tensor corresponding to Lam parameters for mixed
    formulation.

    .. math::
        {\bm D}_{(2D)} = \begin{bmatrix} \widetilde\lambda + 2\mu &
        \widetilde\lambda & 0\\ \widetilde\lambda & \widetilde\lambda + 2\mu &
        0\\ 0 & 0 & \mu \end{bmatrix}

    .. math::
        {\bm D}_{(3D)} = \begin{bmatrix} \widetilde\lambda + 2\mu &
        \widetilde\lambda & \widetilde\lambda & 0 & 0 & 0\\ \widetilde\lambda &
        \widetilde\lambda + 2\mu & \widetilde\lambda & 0 & 0 & 0 \\
        \widetilde\lambda & \widetilde\lambda & \widetilde\lambda + 2\mu & 0 &
        0 & 0 \\ 0 & 0 & 0 & \mu & 0 & 0 \\ 0 & 0 & 0 & 0 & \mu & 0 \\ 0 & 0 &
        0 & 0 & 0 & \mu\\ \end{bmatrix}

    where

    .. math::
       \widetilde\lambda = -{2\over 3} \mu
    """
    lam = - 2.0 / 3.0 * mu

    return stiffness_from_lame(dim, lam, mu)

def stiffness_from_youngpoisson_mixed(dim, young, poisson, plane='strain'):
    """
    Compute stiffness tensor corresponding to Young's modulus and Poisson's
    ratio for mixed formulation.
    """
    lam, mu = lame_from_youngpoisson(young, poisson, plane)

    return stiffness_from_lame_mixed(dim, lam, mu)

def bulk_from_lame(lam, mu):
    r"""
    Compute bulk modulus from Lam parameters.

    .. math::
        \gamma = \lambda + {2 \over 3} \mu
    """
    return lam + 2.0 * mu / 3.0

def bulk_from_youngpoisson(young, poisson, plane='strain'):
    """
    Compute bulk modulus corresponding to Young's modulus and Poisson's ratio.
    """
    lam, mu = lame_from_youngpoisson(young, poisson, plane)

    return bulk_from_lame(lam, mu)

elastic_constants_relations = {
}

class ElasticConstants(Struct):
    r"""
    Conversion formulas for various groups of elastic constants. The elastic
    constants supported are:

      - :math:`E` : Young's modulus
      - :math:`\nu` : Poisson's ratio
      - :math:`K` : bulk modulus
      - :math:`\lambda` : Lam's first parameter
      - :math:`\mu, G` : shear modulus, Lam's second parameter
      - :math:`M` : P-wave modulus, longitudinal wave modulus

    The elastic constants are referred to by the following keyword arguments:
    young, poisson, bulk, lam, mu, p_wave.

    Exactly two of them must be provided to the __init__() method.

    Examples
    --------

     - basic usage::

        >>> from sfepy.mechanics.matcoefs import ElasticConstants
        >>> ec = ElasticConstants(lam=1.0, mu=1.5)
        >>> ec.young
        3.6000000000000001
        >>> ec.poisson
        0.20000000000000001
        >>> ec.bulk
        2.0
        >>> ec.p_wave
        4.0
        >>> ec.get(['bulk', 'lam', 'mu', 'young', 'poisson', 'p_wave'])
        [2.0, 1.0, 1.5, 3.6000000000000001, 0.20000000000000001, 4.0]

     - reinitialize existing instance::

        >>> ec.init(p_wave=4.0, bulk=2.0)
        >>> ec.get(['bulk', 'lam', 'mu', 'young', 'poisson', 'p_wave'])
        [2.0, 1.0, 1.5, 3.6000000000000001, 0.20000000000000001, 4.0]
    """
    def __init__(self, young=None, poisson=None, bulk=None, lam=None,
                 mu=None, p_wave=None, _regenerate_relations=False):
        """
        Set exactly two of the elastic constants, and compute the remaining.
        """
        self.names = ['bulk', 'lam', 'mu', 'young', 'poisson', 'p_wave']

        if _regenerate_relations:
            self.relations = self._construct_relations()

        else:
            import elastic_constants as ec
            self.relations = ec.relations
            self.ec = ec

        ## print sorted(self.relations.keys())
        ## print len(self.relations)

        self.init(young=young, poisson=poisson, bulk=bulk, lam=lam,
                  mu=mu, p_wave=p_wave)

    def _construct_relations(self):
        """
        Construct the dictionary of all relations among the six elastic
        constants and save it as `elastic_constants.py` module, that can be
        imported for reuse. Users should not call this!
        """
        import sympy as sm

        relations = {}

        def _expand_keys(sols):
            for key, val in sols.iteritems():
                if len(val) == 2 and (key.name == 'poisson'):
                    val = val[0]
                else:
                    val = val[-1]
                skey = tuple(sorted([ii.name for ii in val.atoms()
                                     if ii.is_Symbol])) + (key.name,)
                if skey in relations:
                    print '!', skey
                relations[skey] = val

        bulk, lam, mu, young, poisson, p_wave = sm.symbols(self.names, real=True)

        _expand_keys(sm.solve(bulk - (lam + 2 * mu / 3)))
        _expand_keys(sm.solve(young - (mu * (3 * lam + 2 * mu) / (lam + mu))))
        _expand_keys(sm.solve(poisson - (lam / (2 * (lam + mu)))))
        _expand_keys(sm.solve(p_wave - (lam + 2 * mu)))

        _expand_keys(sm.solve(bulk - (young / (3 * (1 - 2 * poisson)))))
        _expand_keys(sm.solve(p_wave - ((young * (1 - poisson))
                                        / ((1 + poisson) * (1 - 2 * poisson)))))
        # Choose the correct root manually.
        ## relations[('p_wave', 'young', 'poisson')] \
        ##                 = (young - p_wave + (-10*p_wave*young + young**2 +
        ##                                      9*p_wave**2)**(0.5))/(4*p_wave)
        _expand_keys(sm.solve(lam - (young * poisson
                                     / ((1 + poisson) * (1 - 2 * poisson)))))
        # Choose the correct root.
        ## relations[('lam', 'young', 'poisson')] \
        ##                   = (lam + young - (2*lam*young + young**2 +
        ##                                     9*(lam**2))**(0.5))/(-4*lam)
        _expand_keys(sm.solve(mu - (young / (2 * (1 + poisson)))))

        _expand_keys(sm.solve(bulk - (young * mu / (3 * (3 * mu - young)))))
        _expand_keys(sm.solve(p_wave - (mu * (4 * mu - young)
                                        / (3 * mu - young))))

        _expand_keys(sm.solve(young - (9 * bulk * (bulk - lam)
                                       / (3 * bulk - lam))))
        _expand_keys(sm.solve(poisson - (lam / (3 * bulk - lam))))
        _expand_keys(sm.solve(p_wave - (3 * bulk - 2 * lam)))

        _expand_keys(sm.solve(poisson - ((3 * bulk - 2 * mu)
                                         / (2 * (3 * bulk + mu)))))
        _expand_keys(sm.solve(p_wave - (bulk + 4 * mu / 3)))

        _expand_keys(sm.solve(p_wave - (lam * (1 - poisson) / poisson)))

        _expand_keys(sm.solve(p_wave - (2 * mu * (1 - poisson)
                                        / (1 - 2 * poisson))))

        _expand_keys(sm.solve(p_wave - (3 * bulk * (1 - poisson)
                                        / (1 + poisson))))

        _expand_keys(sm.solve(p_wave - (3 * bulk * (3 * bulk + young)
                                        / (9 * bulk - young))))

        _expand_keys(sm.solve(young - ((lam*p_wave + p_wave**2 - 2*lam**2)
                                       / (lam + p_wave))))

        fd = open(os.path.join(os.path.dirname(__file__),
                               'elastic_constants.py'), 'w')
        fd.write("""
from __future__ import division

import sympy as sm

names = ['bulk', 'lam', 'mu', 'young', 'poisson', 'p_wave']
bulk, lam, mu, young, poisson, p_wave = sm.symbols(names, real=True)

relations = {
%s
}
        """ % ',\n'.join(['    %s : %s' % (key, val)
                         for key, val in relations.iteritems()]))
        fd.close()

        return relations

    def init(self, young=None, poisson=None, bulk=None, lam=None,
             mu=None, p_wave=None):
        """
        Set exactly two of the elastic constants, and compute the
        remaining. (Re)-initializes the existing instance of ElasticConstants.
        """
        Struct.__init__(self, young=young, poisson=poisson, bulk=bulk, lam=lam,
                        mu=mu, p_wave=p_wave)

        values = {}
        for key, val in self.__dict__.iteritems():
            if (key in self.names) and (val is not None):
                sym = getattr(self.ec, key)
                values[sym] = val

        known = values.keys()
        if len(known) != 2:
            raise ValueError('exactly two elastic constants must be provided!')
        known = [ii.name for ii in known]

        unknown = set(self.names).difference(known)

        for name in unknown:
            key = tuple(sorted(known)) + (name,)
            val = float(self.relations[key].n(subs=values))
            setattr(self, name, val)

    def get(self, names):
        """
        Get the named elastic constants.
        """
        out = [getattr(self, name) for name in names]
        return out

class TransformToPlane(Struct):
    """
    Transformations of constitutive law coefficients of 3D problems to 2D.
    """

    def __init__(self, iplane=None):
        """
        Parameters
        ----------
        iplane : list
            The vector of indices denoting the plane, e.g.: [0, 1]
        """
        if iplane is None:
            iplane = [0, 1]

        # Choose the "master" variables and the "slave" ones
        # ... for vectors
        i_m = nm.sort(iplane)
        i_s = nm.setdiff1d(nm.arange(3), i_m)

        # ... for second order tensors (symmetric storage)
        i_ms = {(0, 1) : [0, 1, 3],
                (0, 2) : [0, 2, 4],
                (1, 2) : [1, 2, 5]}[tuple(i_m)]
        i_ss = nm.setdiff1d(nm.arange(6), i_ms)

        Struct.__init__(self, iplane=iplane,
                        i_m=i_m, i_s=i_s, i_ms=i_ms, i_ss=i_ss)

    def tensor_plane_stress(self, c3=None, d3=None, b3=None):
       """
       Transforms all coefficients of the piezoelectric constitutive law
       from 3D to plane stress problem in 2D: strain/stress ordering: 11 22
       33 12 13 23. If `d3` is None, uses only the stiffness tensor `c3`.

       Parameters
       ----------
       c3 : array
           The stiffness tensor.
       d3 : array
           The dielectric tensor.
       b3 : array
           The piezoelectric coupling tensor.
       """
       mg = nm.meshgrid

       cs = c3[mg(self.i_ss,self.i_ss)]
       cm = c3[mg(self.i_ss,self.i_ms)].T
       if d3 is None: # elasticity only.
           A = cs
           Feps = cm

           Ainv = nm.linalg.inv(A)
           c2 = c3[mg(self.i_ms,self.i_ms)] \
                - nm.dot(Feps.T, nm.dot(Ainv, Feps))

           return c2

       else:
           dm = d3[mg(self.i_s,self.i_m)].T
           ds = d3[mg(self.i_s,self.i_s)]

           ii = mg(self.i_s, self.i_ss)
           A = nm.r_[nm.c_[cs, b3[ii]],
                     nm.c_[b3[ii].T, -ds]] #=> sym !!!
           F = nm.r_[nm.c_[cm, b3[mg(self.i_m,self.i_ss)]],
                     nm.c_[b3[mg(self.i_s,self.i_ms)].T, -dm ]]

           Feps = F[:,:3]
           FE = F[:,3:]
           Ainv = nm.linalg.inv(A)
           c2 = c3[mg(self.i_ms,self.i_ms)] \
                - nm.dot(Feps.T, nm.dot(Ainv, Feps))
           d2 = d3[mg(self.i_m,self.i_m)] \
                - nm.dot(FE.T, nm.dot(Ainv, FE))
           b2 = b3[mg(self.i_m,self.i_ms)].T \
                - nm.dot(FE.T, nm.dot(Ainv, Feps))

           return c2, d2, b2

########NEW FILE########
__FILENAME__ = membranes
import numpy as nm

from sfepy.base.base import assert_
from sfepy.linalg import norm_l2_along_axis as norm
from sfepy.linalg import dot_sequences, insert_strided_axis
from sfepy.discrete.fem.poly_spaces import PolySpace
from sfepy.discrete.fem.mappings import VolumeMapping
from sfepy.mechanics.tensors import dim2sym

def create_transformation_matrix(coors):
    """
    Create a transposed coordinate transformation matrix, that
    transforms 3D coordinates of element face nodes so that the
    transformed nodes are in the `x-y` plane. The rotation is performed
    w.r.t. the first node of each face.

    Parameters
    ----------
    coors : array
        The coordinates of element nodes, shape `(n_el, n_ep, dim)`.

    Returns
    -------
    mtx_t : array
        The transposed transformation matrix :math:`T`, i.e.
        :math:`X_{inplane} = T^T X_{3D}`.

    Notes
    -----
    :math:`T = [t_1, t_2, n]`, where :math:`t_1`, :math:`t_2`, are unit
    in-plane (column) vectors and :math:`n` is the unit normal vector,
    all mutually orthonormal.
    """
    # Local coordinate system.
    t1 = coors[:, 1, :] - coors[:, 0, :]
    t2 = coors[:, -1, :] - coors[:, 0, :]
    n = nm.cross(t1, t2)
    t2 = nm.cross(n, t1)

    t1 = t1 / norm(t1)[:, None]
    t2 = t2 / norm(t2)[:, None]
    n = n / norm(n)[:, None]

    # Coordinate transformation matrix (transposed!).
    mtx_t = nm.concatenate((t1[:, :, None],
                            t2[:, :, None],
                            n[:, :, None]), axis=2)

    return mtx_t

def transform_asm_vectors(out, mtx_t):
    """
    Transform vector assembling contributions to global coordinate system, one
    node at a time.

    Parameters
    ----------
    out : array
        The array of vectors, transformed in-place.
    mtx_t : array
        The transposed transformation matrix :math:`T`, see
        :func:`create_transformation_matrix`.
    """
    n_ep = out.shape[2] / mtx_t.shape[2]
    for iep in range(n_ep):
        ir = slice(iep, None, n_ep)
        fn = out[:, 0, ir, 0]
        fn[:] = dot_sequences(mtx_t, fn, 'AB')

def transform_asm_matrices(out, mtx_t):
    """
    Transform matrix assembling contributions to global coordinate system, one
    node at a time.

    Parameters
    ----------
    out : array
        The array of matrices, transformed in-place.
    mtx_t : array
        The transposed transformation matrix :math:`T`, see
        :func:`create_transformation_matrix`.
    """
    n_ep = out.shape[-1] / mtx_t.shape[-1]
    for iepr in range(n_ep):
        ir = slice(iepr, None, n_ep)
        for iepc in range(n_ep):
            ic = slice(iepc, None, n_ep)
            fn = out[:, 0, ir, ic]
            fn[:] = dot_sequences(dot_sequences(mtx_t, fn, 'AB'), mtx_t, 'ABT')

def create_mapping(coors, gel, order):
    """
    Create mapping from transformed (in `x-y` plane) element faces to
    reference element faces.

    Parameters
    ----------
    coors : array
        The transformed coordinates of element nodes, shape `(n_el,
        n_ep, dim)`. The function verifies that the all `z` components
        are zero.
    gel : GeometryElement instance
        The geometry element corresponding to the faces.
    order : int
        The polynomial order of the mapping.

    Returns
    -------
    mapping : VolumeMapping instance
        The reference element face mapping.
    """
    # Strip 'z' component (should be 0 now...).
    assert_(nm.allclose(coors[:, :, -1], 0.0, rtol=1e-12, atol=1e-12))
    coors = coors[:, :, :-1].copy()

    # Mapping from transformed element to reference element.
    sh = coors.shape
    seq_coors = coors.reshape((sh[0] * sh[1], sh[2]))
    seq_conn = nm.arange(seq_coors.shape[0], dtype=nm.int32)
    seq_conn.shape = sh[:2]

    mapping = VolumeMapping(seq_coors, seq_conn, gel=gel, order=1)

    return mapping

def describe_geometry(ig, field, region, integral):
    """
    Describe membrane geometry in a given region.

    Parameters
    ----------
    ig : int
        The element group index.
    field : Field instance
        The field defining the FE approximation.
    region : Region instance
        The surface region to describe.
    integral : Integral instance
        The integral defining the quadrature points.

    Returns
    -------
    mtx_t : array
        The transposed transformation matrix :math:`T`, see
        :func:`create_transformation_matrix`.
    membrane_geo : CMapping instance
        The mapping from transformed elements to a reference elements.
    """
    # Coordinates of element vertices.
    sg, _ = field.get_mapping(ig, region, integral, 'surface')
    sd = field.aps[ig].surface_data[region.name]
    coors = field.coors[sd.econn[:, :sg.n_ep]]

    # Coordinate transformation matrix (transposed!).
    mtx_t = create_transformation_matrix(coors)

    # Transform coordinates to the local coordinate system.
    coors_loc = dot_sequences((coors - coors[:, 0:1, :]), mtx_t)

    # Mapping from transformed elements to reference elements.
    gel = field.gel.surface_facet
    vm = create_mapping(coors_loc, gel, 1)

    qp = integral.get_qp(gel.name)
    ps = PolySpace.any_from_args(None, gel, field.approx_order)
    membrane_geo = vm.get_mapping(qp[0], qp[1], poly_space=ps)

    return mtx_t, membrane_geo

def describe_deformation(el_disps, bfg):
    """
    Describe deformation of a thin incompressible 2D membrane in 3D
    space, composed of flat finite element faces.

    The coordinate system of each element (face), i.e. the membrane
    mid-surface, should coincide with the `x`, `y` axes of the `x-y`
    plane.

    Parameters
    ----------
    el_disps : array
        The displacements of element nodes, shape `(n_el, n_ep, dim)`.
    bfg : array
        The in-plane base function gradients, shape `(n_el, n_qp, dim-1,
        n_ep)`.

    Returns
    -------
    mtx_c ; array
        The in-plane right Cauchy-Green deformation tensor
        :math:`C_{ij}`, :math:`i, j = 1, 2`.
    c33 : array
        The component :math:`C_{33}` computed from the incompressibility
        condition.
    mtx_b : array
        The discrete Green strain variation operator.
    """
    sh = bfg.shape
    n_ep = sh[3]

    dim = el_disps.shape[2]
    sym2 = dim2sym(dim-1)

    # Repeat el_disps by number of quadrature points.
    el_disps_qp = insert_strided_axis(el_disps, 1, bfg.shape[1])

    # Transformed (in-plane) displacement gradient with
    # shape (n_el, n_qp, 2 (-> a), 3 (-> i)), du_i/dX_a.
    du = dot_sequences(bfg, el_disps_qp)

    # Deformation gradient F w.r.t. in plane coordinates.
    # F_{ia} = dx_i / dX_a,
    # a \in {1, 2} (rows), i \in {1, 2, 3} (columns).
    mtx_f = du + nm.eye(dim - 1, dim, dtype=du.dtype)

    # Right Cauchy-Green deformation tensor C.
    # C_{ab} = F_{ka} F_{kb}, a, b \in {1, 2}.
    mtx_c = dot_sequences(mtx_f, mtx_f, 'ABT')

    # C_33 from incompressibility.
    c33 = 1.0 / (mtx_c[..., 0, 0] * mtx_c[..., 1, 1]
                 - mtx_c[..., 0, 1]**2)

    # Discrete Green strain variation operator.
    mtx_b = nm.empty((sh[0], sh[1], sym2, dim * n_ep), dtype=nm.float64)
    mtx_b[..., 0, 0*n_ep:1*n_ep] = bfg[..., 0, :] * mtx_f[..., 0, 0:1]
    mtx_b[..., 0, 1*n_ep:2*n_ep] = bfg[..., 0, :] * mtx_f[..., 0, 1:2]
    mtx_b[..., 0, 2*n_ep:3*n_ep] = bfg[..., 0, :] * mtx_f[..., 0, 2:3]
    mtx_b[..., 1, 0*n_ep:1*n_ep] = bfg[..., 1, :] * mtx_f[..., 1, 0:1]
    mtx_b[..., 1, 1*n_ep:2*n_ep] = bfg[..., 1, :] * mtx_f[..., 1, 1:2]
    mtx_b[..., 1, 2*n_ep:3*n_ep] = bfg[..., 1, :] * mtx_f[..., 1, 2:3]
    mtx_b[..., 2, 0*n_ep:1*n_ep] = bfg[..., 1, :] * mtx_f[..., 0, 0:1] \
                                   + bfg[..., 0, :] * mtx_f[..., 1, 0:1]
    mtx_b[..., 2, 1*n_ep:2*n_ep] = bfg[..., 0, :] * mtx_f[..., 1, 1:2] \
                                   + bfg[..., 1, :] * mtx_f[..., 0, 1:2]
    mtx_b[..., 2, 2*n_ep:3*n_ep] = bfg[..., 0, :] * mtx_f[..., 1, 2:3] \
                                   + bfg[..., 1, :] * mtx_f[..., 0, 2:3]

    return mtx_c, c33, mtx_b

def get_tangent_stress_matrix(stress, bfg):
    """
    Get the tangent stress matrix of a thin incompressible 2D membrane
    in 3D space, given a stress.

    Parameters
    ----------
    stress : array
        The components `11, 22, 12` of the second Piola-Kirchhoff stress
        tensor, shape `(n_el, n_qp, 3, 1)`.
    bfg : array
        The in-plane base function gradients, shape `(n_el, n_qp, dim-1,
        n_ep)`.

    Returns
    -------
    mtx : array
        The tangent stress matrix, shape `(n_el, n_qp, dim*n_ep, dim*n_ep)`.
    """
    n_el, n_qp, dim, n_ep = bfg.shape
    dim += 1

    mtx = nm.zeros((n_el, n_qp, dim * n_ep, dim * n_ep), dtype=nm.float64)

    g1tg1 = dot_sequences(bfg[..., 0:1, :], bfg[..., 0:1, :], 'ATB')
    g1tg2 = dot_sequences(bfg[..., 0:1, :], bfg[..., 1:2, :], 'ATB')
    g2tg1 = dot_sequences(bfg[..., 1:2, :], bfg[..., 0:1, :], 'ATB')
    g2tg2 = dot_sequences(bfg[..., 1:2, :], bfg[..., 1:2, :], 'ATB')

    aux = stress[..., 0:1, :] * g1tg1 + stress[..., 2:3, :] * g1tg2 \
          + stress[..., 2:3, :] * g2tg1 + stress[..., 1:2, :] * g2tg2

    mtx[..., 0 * n_ep : 1 * n_ep, 0 * n_ep : 1 * n_ep] = aux
    mtx[..., 1 * n_ep : 2 * n_ep, 1 * n_ep : 2 * n_ep] = aux
    mtx[..., 2 * n_ep : 3 * n_ep, 2 * n_ep : 3 * n_ep] = aux

    return mtx

def get_invariants(mtx_c, c33):
    """
    Get the first and second invariants of the right Cauchy-Green
    deformation tensor describing deformation of an incompressible
    membrane.
    
    Parameters
    ----------
    mtx_c ; array
        The in-plane right Cauchy-Green deformation tensor
        :math:`C_{ij}`, :math:`i, j = 1, 2`, shape `(n_el, n_qp, dim-1,
        dim-1)`.
    c33 : array
        The component :math:`C_{33}` computed from the incompressibility
        condition, shape `(n_el, n_qp)`.

    Returns
    -------
    i1 : array
        The first invariant of :math:`C_{ij}`.
    i2 : array
        The second invariant of :math:`C_{ij}`.
    """
    i1 = mtx_c[..., 0, 0] + mtx_c[..., 1, 1] + c33

    i2 = mtx_c[..., 0, 0] * mtx_c[..., 1,1] \
         + mtx_c[..., 1, 1] * c33 \
         + mtx_c[..., 0, 0] * c33 \
         - mtx_c[..., 0, 1]**2

    return i1, i2

def get_green_strain_sym3d(mtx_c, c33):
    r"""
    Get the 3D Green strain tensor in symmetric storage.

    Parameters
    ----------
    mtx_c ; array
        The in-plane right Cauchy-Green deformation tensor
        :math:`C_{ij}`, :math:`i, j = 1, 2`, shape `(n_el, n_qp, dim-1,
        dim-1)`.
    c33 : array
        The component :math:`C_{33}` computed from the incompressibility
        condition, shape `(n_el, n_qp)`.

    Returns
    -------
    mtx_e : array
        The membrane Green strain :math:`E_{ij} = \frac{1}{2} (C_{ij}) -
        \delta_{ij}`, symmetric storage: items (11, 22, 33, 12, 13, 23),
        shape `(n_el, n_qp, sym, 1)`.
    """
    n_el, n_qp, dm, _ = mtx_c.shape
    dim = dm + 1
    sym = dim2sym(dim)

    mtx_e = nm.empty((n_el, n_qp, sym, 1), dtype=mtx_c.dtype)

    mtx_e[..., 0, 0] = 0.5 * (mtx_c[..., 0, 0] - 1.0)
    mtx_e[..., 1, 0] = 0.5 * (mtx_c[..., 1, 1] - 1.0)
    mtx_e[..., 2, 0] = 0.5 * (c33 - 1.0)
    mtx_e[..., 3, 0] = 0.5 * mtx_c[..., 0, 1]
    mtx_e[..., 4:, 0] = 0.0

    return mtx_e

########NEW FILE########
__FILENAME__ = tensors
"""
Functions to compute some tensor-related quantities usual in continuum mechanics.
"""
import numpy as nm
import numpy.linalg as nla

from sfepy.base.base import assert_, Struct
from sfepy.linalg \
     import apply_to_sequence, dot_sequences, make_axis_rotation_matrix

def dim2sym(dim):
    """
    Given the space dimension, return the symmetric storage size.
    """
    return (dim + 1) * dim / 2

def sym2dim(sym):
    """
    Given the symmetric storage size, return the space dimension.

    Notes
    -----
    This function works for any space dimension.
    """
    val = int(-0.5 + nm.sqrt(2 * sym + 0.25))
    assert_(dim2sym(val) == sym)

    return val

def get_full_indices(dim):
    """
    The indices for converting the symmetric storage to the full storage.
    """
    return {
        2 : [[0, 2], [2, 1]],
        3 : [[0, 3, 4], [3, 1, 5], [4, 5, 2]],
    }[dim]

def get_sym_indices(dim):
    """
    The indices for converting the full storage to the symmetric storage.
    """
    return {
        2 : [0, 3, 1],
        3 : [0, 4, 8, 1, 2, 5],
    }[dim]

def get_non_diagonal_indices(dim):
    """
    The non_diagonal indices for the full vector storage.
    """
    return {
        2 : ([1], [2]),
        3 : ([1, 2, 5], [3, 6, 7]),
    }[dim]

def get_trace(tensor, sym_storage=True):
    """
    The trace of a tensor.
    """
    if sym_storage:
        dim = sym2dim(tensor.shape[1])
        trace = nm.sum(tensor[:,:dim], axis=1)

    else:
        trace = nm.trace(tensor, axis1=1, axis2=2)

    return trace

def get_volumetric_tensor(tensor, sym_storage=True):
    """
    The volumetric part of a tensor.
    """
    dim = tensor.shape[1]
    if sym_storage:
        dim = sym2dim(dim)

    trace = get_trace(tensor, sym_storage=sym_storage)
    val = trace / float(dim)

    if sym_storage:
        vt = nm.zeros_like(tensor)
        vt[:,:dim] = val[:,None]

    else:
        vt = val[:,None,None] * nm.eye(dim, dtype=nm.float64)

    return vt

def get_deviator(tensor, sym_storage=True):
    """
    The deviatoric part (deviator) of a tensor.
    """
    vt = get_volumetric_tensor(tensor, sym_storage=sym_storage)
    dev = tensor - vt

    return dev

def get_von_mises_stress(stress, sym_storage=True):
    r"""
    Given a symmetric stress tensor, compute the von Mises stress (also known
    as Equivalent tensile stress).

    Notes
    -----
    .. math::
        \sigma_V = \sqrt{\frac{(\sigma_{11} - \sigma_{22})^2 +
        (\sigma_{22} - \sigma_{33})^2 + (\sigma_{11} - \sigma_{33})^2 + 6
        (\sigma_{12}^2 + \sigma_{13}^2 + \sigma_{23}^2)}{2}}
    """
    dim = stress.shape[1]
    if sym_storage:
        dim = sym2dim(dim)

    if dim == 2:

        if sym_storage:
            s11 = stress[:,0]
            s22 = stress[:,1]
            s12 = stress[:,2]

        else:
            s11 = stress[:,0,0]
            s22 = stress[:,1,1]
            s12 = stress[:,0,1]

        vms = nm.sqrt(s11**2 - s11*s22 + s22**2 + 3*s12**2)[:,None]

    else:

        if sym_storage:
            s11 = stress[:,0]
            s22 = stress[:,1]
            s33 = stress[:,2]
            s12 = stress[:,3]
            s13 = stress[:,4]
            s23 = stress[:,5]

        else:
            s11 = stress[:,0,0]
            s22 = stress[:,1,1]
            s33 = stress[:,2,2]
            s12 = stress[:,0,1]
            s13 = stress[:,0,2]
            s23 = stress[:,1,2]

        vms = nm.sqrt(0.5 * ((s11 - s22)**2 + (s22 - s33)**2 + (s11 - s33)**2
                             + 6.0 * (s12**2 + s13**2 + s23**2)))[:,None]

    return vms

def get_t4_from_t2s(t2s):
    """
    Get the full 4D tensor with major/minor symmetries from its 2D matrix
    representation.

    Parameters
    ----------
    t2s : array
        The symmetrically-stored tensor of shape (S, S), where S it the
        symmetric storage size.

    Returns
    -------
    t4 : array
        The full 4D tensor of shape (D, D, D, D), where D is the space
        dimension.
    """
    dim = sym2dim(t2s.shape[0])
    iif = get_full_indices(dim)

    t4 = t2s[:, iif][iif, ...]

    return t4

def prepare_cylindrical_transform(coors, origin, mode='axes'):
    """
    Prepare matrices for transforming tensors into cylindrical coordinates with
    the axis 'z' in a given origin.

    Parameters
    ----------
    coors : array
        The Cartesian coordinates.
    origin : array of length 3
        The origin.
    mode : 'axes' or 'data'
        In 'axes' (default) mode the matrix transforms data to different
        coordinate system, while in 'data' mode the matrix transforms
        the data in the same coordinate system and is transpose of the
        matrix in the 'axes' mode.

    Returns
    -------
    mtx : array
        The array of transformation matrices for each coordinate in `coors`.
    """
    assert_(mode in ['axes', 'data'])

    x, y = coors[:,0] - origin[0], coors[:,1] - origin[1]
    theta = nm.arctan2(y, x)
    if mode == 'data':
        theta = -theta

    mtx = nm.zeros((coors.shape[0], 3, 3), dtype=nm.float64)
    for ii, th in enumerate(theta):
        mtx[ii] = make_axis_rotation_matrix([0.0, 0.0, 1.0], th)

    return mtx

def transform_data(data, coors=None, mode='cylindrical', mtx=None):
    r"""
    Transform vector or tensor data components between orthogonal
    coordinate systems in 3D using transformation matrix :math:`M`, that
    should express rotation of the original coordinate system to the new
    system denoted by :math:`\bullet'` below.

    For vectors:

    .. math::
        \ul{v}' = M \cdot \ul{v}

    For second order tensors:

    .. math::
        \ull{t}' = M \cdot \ull{t} \cdot M^T

        \mbox{or}

        t_{ij}' = M_{ip} M_{jq} t_{pq}

    For fourth order tensors:

    .. math::

        t_{ijkl}' = M_{ip} M_{jq} M_{kr} M_{ls} t_{pqrs}

    Parameters
    ----------
    data : array, shape (num, n_r) or (num, n_r, n_c)
        The vectors (`n_r` is 3) or tensors (symmetric storage, `n_r` is 6,
        `n_c`, if available, is 1 or 6) to be transformed.
    coors : array
        The Cartesian coordinates of the data. Not needed when `mtx` argument
        is given.
    mode : one of ['cylindrical']
        The requested coordinate system. Not needed when `mtx` argument
        is given.
    mtx : array
        The array of transformation matrices :math:`M` for each data row.

    Returns
    -------
    new_data : array
        The transformed data.
    """
    if (coors is None) and (mtx is None):
        raise ValueError('one of (coors, mtx) arguments must be set!')

    if mtx is None:
        if mode == 'cylindrical':
            mtx = prepare_cylindrical_transform(coors, [0.0, 0.0, 0.0])

        else:
            raise ValueError('transformation mode %s is not supported!' % mode)

    shape = data.shape

    if shape[0] != mtx.shape[0]:
        raise ValueError('incompatible numbers of points! (data: %d, mtx: %d)'
                         % (shape[0], mtx.shape[0]))

    if shape[1] == 3: # Vectors.
        new_data = dot_sequences(mtx, data)

    elif shape[1] == 6: # Symmetric tensors.
        iif = get_full_indices(3)
        iis = get_sym_indices(3)

        if ((data.ndim == 2)
            or ((data.ndim == 3) and (shape[2] == 1))): # Second order.
            if data.ndim == 3:
                aux = data[:, iif, 0]

            else:
                aux = data[:, iif]

            aux2 = dot_sequences(dot_sequences(mtx, aux, 'AB'), mtx, 'ABT')
            assert nm.allclose(aux2[0],
                               nm.dot(nm.dot(mtx[0], aux[0]), mtx[0].T))

            aux3 = aux2.reshape((aux2.shape[0], 9))

            new_data = aux3[:, iis]
            if data.ndim == 3:
                new_data = new_data[..., None]

        elif (data.ndim == 3) and (shape[2] == 6): # Fourth order.
            # Note: nm.einsum() is much slower than dot_sequences().
            df = data[:, iif][..., iif]
            tdf = nm.einsum('apqrs,aip,ajq,akr,als->aijkl',
                            df, mtx, mtx, mtx, mtx)
            tdf2 = tdf.reshape(tdf.shape[0], 9, 9)
            new_data = tdf2[:, :, iis][:, iis]

        else:
            raise ValueError('unsupported data shape! (%s)' % str(shape))


    else:
        raise ValueError('unsupported data shape! (%s)' % str(shape))

    assert_(new_data.shape == shape)

    return new_data

class StressTransform(Struct):
    """
    Encapsulates functions to convert various stress tensors in the symmetric
    storage given the deformation state.
    """

    def __init__(self, def_grad, jacobian=None):
        """
        Set :math:`\ull{F} = \pdiff{\ul{x}}{\ul{X}}` and optionally also
        :math:`J = \det(\ull{F})`.
        """
        self.def_grad = nm.asarray(def_grad, dtype=nm.float64)
        self.n_el, self.n_qp, self.dim = self.def_grad.shape[:3]

        self.s2f = get_full_indices(self.dim)
        self.f2s = get_sym_indices(self.dim)

        if jacobian is None:
            self.jacobian = apply_to_sequence(self.def_grad, nla.det,
                                              2, (1, 1))

        else:
            self.jacobian = nm.asarray(jacobian, dtype=nm.float64)

    def _assert_symmetry(self, stress):
        i1, i2 = get_non_diagonal_indices(self.dim)
        assert_(nm.allclose(stress[:,:,i1], stress[:,:,i2]))

    def get_cauchy_from_2pk(self, stress_in):
        """
        Get the Cauchy stress given the second Piola-Kirchhoff stress.

        .. math::

            \sigma_{ij} = J^{-1} F_{ik} S_{kl} F_{jl}
        """
        stress_in = nm.asarray(stress_in, dtype=nm.float64)

        stress_in_full = stress_in[:,:,self.s2f,0]

        val_il = dot_sequences(self.def_grad, stress_in_full)
        val_ij = dot_sequences(val_il, self.def_grad, mode='ABT')

        stress_out_full = val_ij / self.jacobian

        sh = stress_out_full.shape
        stress_out_full.shape = (sh[0], sh[1], sh[2] * sh[3])

        self._assert_symmetry(stress_out_full)

        stress_out = nm.empty_like(stress_in)
        stress_out[...,0] = stress_out_full[:,:,self.f2s]
        return stress_out

########NEW FILE########
__FILENAME__ = units
"""
Some utilities for work with units of physical quantities.
"""
try:
    import sympy as sm
except ImportError:
    sm = None

import numpy as nm

from sfepy.base.base import get_default, invert_dict, Struct

default_units_of_basic_quantities = {
    'length' : 'm',
    'time' : 's',
    'mass' : 'kg',
    'temperature' : 'C',
}

# Cannot use N for Newton as it is a sympy function...
derived_units = {
    'Newton' : '(kg * m) / s**2',
    'Pa' : 'kg / (m * s**2)', # N / m**2
    'J' : '(kg * m**2) / s**2', # N m
}

units_of_quantities = {
    'force' : 'Newton',
    'stress' : 'Pa',
    'energy' : 'J',
    'thermal_expandability' : 'Pa / C',
}

prefixes = {
    'n'  : 1e-9,
    'mu' : 1e-6,
    'm'  : 1e-3,
    'c'  : 1e-2,
    ''   : 1e0,
    'd'  : 1e1,
    'k'  : 1e3,
    'M'  : 1e6,
    'G'  : 1e9,
}

inv_prefixes = invert_dict(prefixes)

class Unit(Struct):
    """
    A unit of a physical quantity. The prefix and coefficient of the unit
    are determined from to its name.

    Examples
    --------
    Construct some units:

    >>> from sfepy.mechanics.units import Unit
    >>> unit = Unit('mm')
    >>> print unit
    Unit:mm
      coef:
        0.001
      name:
        mm
      prefix:
        m
      prefix_length:
        1
      unit:
        m
    >>> unit = Unit('kg')
    >>> print unit
    Unit:kg
      coef:
        1000.0
      name:
        kg
      prefix:
        k
      prefix_length:
        1
      unit:
        g

    Get prefixes for a coefficient:

    >>> Unit.get_prefix(100.0)
    ('d', 10.0)
    >>> Unit.get_prefix(100.0, omit=('d',))
    ('k', 0.10000000000000001)
    """
    
    @staticmethod
    def get_prefix(coef, bias=0.1, omit=()):
        """
        Get the prefix and numerical multiplier corresponding to a numerical
        coefficient, omitting prefixes in omit tuple.
        """
        values = [val for key, val in prefixes.iteritems() if key not in omit]
        coefs = nm.array(values, dtype=nm.float64)
        coefs.sort()
        ii = nm.searchsorted(coefs, bias*coef, side='left')

        cc = coefs[ii]
        prefix = inv_prefixes[cc]
        mul = coef / cc

        return prefix, mul

    def __init__(self, name):
        self.name = name

        aux = sorted(prefixes.keys(), reverse=True)
        for prefix in aux:
            lp = len(prefix)
            if (prefix == name[:lp]) and (lp < len(name)): break

        self.prefix = prefix
        self.prefix_length = len(prefix)
        self.unit = name[self.prefix_length:]

        self.coef = prefixes[self.prefix]

class Quantity(Struct):
    """
    A physical quantity in a given set of basic units.

    Examples
    --------

    Construct the stress quantity:
    
    >>> from sfepy.mechanics.units import Unit, Quantity
    >>> units = ['m', 's', 'kg', 'C']
    >>> unit_set = [Unit(key) for key in units]
    >>> q1 = Quantity('stress', unit_set)
    >>> q1()
    '1.0 Pa'

    Show its unit using various prefixes:
    
    >>> q1('m')
    '1000.0 mPa'
    >>> q1('')
    '1.0 Pa'
    >>> q1('k')
    '0.001 kPa'
    >>> q1('M')
    '1e-06 MPa'

    Construct the stress quantity in another unit set:

    >>> units = ['mm', 's', 'kg', 'C']
    >>> unit_set = [Unit(key) for key in units]
    >>> q2 = Quantity('stress', unit_set)
    >>> q2()
    '1.0 kPa'

    Show its unit using various prefixes:

    >>> q2('m')
    '1000000.0 mPa'
    >>> q2('')
    '1000.0 Pa'
    >>> q2('k')
    '1.0 kPa'
    >>> q2('M')
    '0.001 MPa'
    """

    def __init__(self, name, unit_set):
        """
        Create a quantity in the given unit set. The name must be listed in the
        units_of_quantities dictionary."""
        self.name = name
        self.unit_set = unit_set

        self.unit_name = units_of_quantities[self.name]

        unit_expr = sm.sympify(self.unit_name)
        unit_expr = unit_expr.subs(derived_units)

        self.symbolic_value = sm.sympify(unit_expr)
        atoms = self.symbolic_value.atoms(sm.Symbol)
        self.def_units = [Unit(atom.name) for atom in atoms]

        self.def_names, self.def_units = self._get_dicts(self.def_units)
        self.names, self.units = self._get_dicts(self.unit_set)

        self.def_coef = float(self.symbolic_value.subs(self.def_names))

        coef_dict = {}
        for key, val in self.def_units.iteritems():
            coef_dict[val.name] = self.units[key].coef
        self.coef_dict = coef_dict
        
        self.raw_coef = float(self.symbolic_value.subs(self.coef_dict))
        self.coef = self.raw_coef / self.def_coef

    def _get_dicts(self, unit_set):
        """Get auxiliary dictionaries for a unit set."""
        name_dict = {}
        unit_dict = {}
        for unit in unit_set:
            name_dict[unit.name] = unit.coef
            unit_dict[unit.unit] = unit

        return name_dict, unit_dict

    def __call__(self, prefix=None, omit=('c', 'd')):
        """Get the quantity units."""
        if prefix is None:
            prefix, mul = Unit.get_prefix(self.coef, omit=omit)

        else:
            coef = prefixes[prefix]
            mul = self.coef / coef
            
        return '%s %s%s' % (mul, prefix, self.unit_name)

def get_consistent_unit_set(length=None, time=None, mass=None, temperature=None):
    """Given a set of basic units, return a consistent set of derived units for
    quantities listed in the units_of_quantities dictionary."""
    defaults = default_units_of_basic_quantities
    length = get_default(length, defaults['length'])
    time = get_default(time, defaults['time'])
    mass = get_default(mass, defaults['mass'])
    temperature = get_default(temperature, defaults['temperature'])

    unit_set = [Unit(length), Unit(time), Unit(mass), Unit(temperature)]

    derived_units = {}

    for quantity_name in units_of_quantities.keys():
        quantity = Quantity(quantity_name, unit_set)

        derived_units[quantity_name] = quantity()

    return derived_units

########NEW FILE########
__FILENAME__ = geom_tools
import numpy as nm

class geometry(object):
    """The geometry is given by a sets of points (d0), lines (d1), surfaces
    (d2) and volumes (d3). A lines are constructed from 2 points, a surface from
    any number of lines, a volume from any number of surfaces.

    Physical volumes are contruted from any number of volumes.


    The self.d0, self.d1, self.d2 and self.d3 are dictionaries holding a map

    geometry element number  ->   instance of point,line,surface of volume

    Examples
    --------

    To get all the points which define a surface 5, use:

    self.d2[5].getpoints()

    This would give you a list [..] of point() instances.
    """
    def __init__(self, dim=3):
        self.dim = dim
        self.d0={}
        self.d1={}
        self.d2={}
        self.d3={}
        self.phys2={}
        self.phys3={}
    def addpoint(self,n,p):
        "p=[x,y,z]"
        o=point(self,n,p)
        self.d0[o.getn()]=o
    def addpoints(self,ps,off=1):
        "ps=[p1, p2, ...]"
        for i, p in enumerate(ps):
            self.addpoint(i + off, p)
    def addline(self,n,l):
        "l=[p1,p2]"
        o=line(self,n,l)
        self.d1[o.getn()]=o
    def addlines(self,ls,off=1):
        "ls=[l1, l2, ...]"
        for i, l in enumerate(ls):
            self.addline(i + off, l)
    def addsurface(self,n,s, is_hole=False):
        "s=[l1,l2,l3,...]"
        o=surface(self,n,s, is_hole)
        self.d2[o.getn()]=o
    def addsurfaces(self,ss,off=1):
        "s=[s1,s2,s3,...]"
        for i, s in enumerate(ss):
            self.addsurface(i + off, s)
    def addvolume(self,n,v):
        "v=[s1,s2,s3,...]"
        o=volume(self,n,v)
        self.d3[o.getn()]=o
    def addvolumes(self,vs,off=1):
        "v=[v1,v2,v3,...]"
        for i, v in enumerate(vs):
            self.addvolume(i + off, v)
    def addphysicalsurface(self,n,surfacelist):
        "surfacelist=[s1,s2,s3,...]"
        o=physicalsurface(self,n,surfacelist)
        self.phys2[o.getn()]=o
    def addphysicalvolume(self,n,volumelist):
        "volumelist=[v1,v2,v3,...]"
        o=physicalvolume(self,n,volumelist)
        self.phys3[o.getn()]=o
    def getBCnum(self,snum):
        for x in self.phys2:
            if snum in self.phys2[x].surfaces:
                return x
        return 0
    def printinfo(self, verbose=False):
        print "General geometry information:"
        print "  dimension:", self.dim
        print "  points:", len(self.d0)
        if verbose:
            for k, v in self.d0.iteritems():
                print "    %d - %s" % (k, v.getstr())
        print "  lines:", len(self.d1)
        if verbose:
            for k, v in self.d1.iteritems():
                print "    %d - " % k, v.points
        print "  surfaces:", len(self.d2)
        if verbose:
            for k, v in self.d2.iteritems():
                if v.is_hole:
                    aux = '(hole)'
                else:
                    aux = ''
                print "    %d%s - " % (k, aux), v.lines
        print "  volumes:", len(self.d3)
        if verbose:
            for k, v in self.d3.iteritems():
                print "    %d - " % k, v.surfaces
        print "Physical entities:"
        if self.dim == 2:
            print "  surfaces (regions):"
            for d in self.phys2.values():
                print "    %d: surface numbers %r"%(d.getn(),d.surfaces)
        elif self.dim == 3:
            print "  surfaces (boundary conditions):"
            for d in self.phys2.values():
                print "    %d: surface numbers %r"%(d.getn(),d.surfaces)
            print "  volumes (regions):"
            for d in self.phys3.values():
                print "    %d: volume numbers %r"%(d.getn(),d.volumes)

    def leaveonlyphysicalsurfaces(self):
        points={}
        lines={}
        surfaces={}
        volumes={}
        for e in self.phys2:
            for s in self.phys2[e].getsurfaces():
                surfaces[s.getn()]=s
                for l in s.getlines():
                    lines[l.getn()]=l
                    for p in l.getpoints():
                        points[p.getn()]=p
        self.d0=points
        self.d1=lines
        self.d2=surfaces
        self.d3=volumes

    def leaveonlyphysicalvolumes(self):
        points={}
        lines={}
        surfaces={}
        volumes={}
        for e in self.phys3:
            for v in self.phys3[e].getvolumes():
                volumes[v.getn()]=v
                for s in v.getsurfaces():
                    surfaces[s.getn()]=s
                    for l in s.getlines():
                        lines[l.getn()]=l
                        for p in l.getpoints():
                            points[p.getn()]=p
        self.d0=points
        self.d1=lines
        self.d2=surfaces
        self.d3=volumes

    def splitlines(self, ls, n):
        repl = {}
        for il in ls:
            l = self.d1[il]
            pts = l.getpoints()
            dp = pts[1] - pts[0]
            t = nm.linspace(0, 1, n + 1)
            points = [pts[0].n, pts[1].n]
            for ii, it in enumerate(t[1:-1]):
                pid = il * 1000 + ii
                self.addpoint(pid, (pts[0] + dp * it).getxyz())
                points.insert(-1, pid)

            lines = []
            for ii in range(n):
                lid = il * 1000 + ii
                self.addline(lid, [points[ii], points[ii + 1]])
                lines.append(lid)

            for s in self.d2.itervalues():
                try:
                    idx = s.lines.index(l.n)
                except ValueError:
                    continue

                s.lines.pop(idx)
                for ii, j in enumerate(lines):
                    s.lines.insert(idx + ii, j)

            repl[l.n] = lines
            self.d1.pop(l.n)

    def to_poly_file(self, filename):
        """
        Export geometry to poly format (tetgen and triangle geometry format).

        Parameters
        ----------
        geo : geometry
            geometry description
        filename : string
            file name
        """

        def getinsidepoint(pts):
            direct = (pts[0] + pts[1] + pts[2]) / 3 - pts[0]
            return pts[0] + 0.001 * direct

        if self.dim == 2:
            self.leaveonlyphysicalsurfaces()
        if self.dim == 3:
            self.leaveonlyphysicalvolumes()

        # write nodes
        nodes = []
        map = {}
        for x in self.d0.values():
            assert isinstance(x, point)
            nodes.append(x.getxyz())
            map[x.getn()] = len(nodes)


        s = "# nodes\n%d %d 0 0\n" % (len(nodes), self.dim)
        if self.dim == 2:
            ptstr = " %d %f %f\n"
        else:
            ptstr = " %d %f %f %f\n"

        for n, x in enumerate(nodes):
            s += ptstr % tuple([n + 1] + list(x))

        # facets
        # first write external polygon, then hole polygons and then point in each
        # hole polygon
        facets = []
        if self.dim == 2:

            hole_pts = []
            regions=[]
            for x2 in self.d2.values():
                assert isinstance(x2, surface)
                for x1 in x2.getlines():
                    assert isinstance(x1, line)
                    p = [map[y.getn()] for y in x1.getpoints()]
                    bc = self.getBCnum(x1.getn())
                    facets.append((p, bc))

                for hole in x2.getholepoints():
                    hole_pts.append(hole.getxyz())

        # regions
        for x in self.phys2.values():
            assert isinstance(x, physicalsurface)
            for x2 in x.getsurfaces():
                if not x2.is_hole:
                    regions.append(x2.getinsidepoint().getxyz() + [x.getn()])

            # number of facets, boundary markers=yes
            s += "# segments\n%d 1\n" % len(facets)
            for ii, (p, bc) in enumerate(facets):
                # number of corners, corner 1, corner 2, ...
                s += " %d %s %d\n" % (ii + 1, ' '.join([str(ii) for ii in p]), bc)
            # holes
            s += "# holes\n%d\n" % len(hole_pts)
            for ii, x0 in enumerate(hole_pts):
                # number of corners, corner 1, corner 2, ...
                s += " %d %s\n" % (ii + 1, ' '.join([str(ii) for ii in x0]))
            # regions
            s += "# regions\n%d\n" % len(regions)
            for ii, x0 in enumerate(regions):
                s += " %d %f %f %d\n" % tuple([ii + 1] + x0)

        if self.dim == 3:

            for x in self.d2.values():
                assert isinstance(x, surface)
                p = [map[y.getn()] for y in x.getpoints()]
                h = []
                pts = []
                for hole in x.getholepoints():
                    h.append([map[y.getn()] for y in hole])
                    pts.append(getinsidepoint(hole).getxyz())
                bc = self.getBCnum(x.getn())
                facets.append((p, bc, h, pts))
            # number of facets, boundary markers=yes
            s += "# segments\n%d 1\n" % len(facets)
            for p, bc, h, holes in facets:
                # number of polygons, # of holes, boundary marker
                s += " %d %d %d\n" % (1 + len(h), len(h), bc)
                # number of corners, corner 1, corner 2, ...
                s += " %d %s\n" % (len(p), ' '.join([str(ii) for ii in p]))
                for x in h:
                    # number of corners, corner 1, corner 2, ...
                    s += " %d %s\n" % (len(x), ' '.join([str(ii) for ii in p]))
                for i, pt in enumerate(holes):
                    # hole #, x, y, z
                    s += ptstr % tuple([i + 1] + list(pt))

            # volume holes
            s += "# holes\n0\n"
            # regions
            regions=[]
            for x in self.phys3.values():
                assert isinstance(x, physicalvolume)
                for v in x.getvolumes():
                    regions.append(v.getinsidepoint().getxyz()+[x.getn()])
            s += "# regions\n%d\n" % len(regions)
            for i, x in enumerate(regions):
                s += ptstr % tuple([i + 1], list(x))

        open(filename, "w").write(s)

    @staticmethod
    def from_gmsh_file(filename):
        """
        Import geometry - Gmsh geometry format.

        Parameters
        ----------
        filename : string
            file name

        Returns
        -------
        geo : geometry
            geometry description

        """

        from pyparsing import Word, Optional, nums, Combine, Literal, \
             CaselessLiteral, Group, OneOrMore, StringEnd, restOfLine, \
             ParseException, alphanums, Keyword, ZeroOrMore

        e = CaselessLiteral("E")
        inum = Word("+-"+nums)
        fnum = Combine(
            Word( "+-"+nums, nums ) + Optional("."+Optional(Word(nums))) +
            Optional(e+Word("+-"+nums,nums))
            )

        semi  = Literal(";").suppress()
        colon  = Literal(",").suppress()
        lpar  = Literal("(").suppress()
        rpar  = Literal(")").suppress()
        lbrace  = Literal("{").suppress()
        rbrace  = Literal("}").suppress()
        eq  = Literal("=").suppress()

        point = Group(
                Keyword("Point")+lpar+inum+rpar+eq+
                Group(lbrace+fnum+colon+fnum+colon+fnum+colon+fnum+rbrace)+semi
                )
        line = Group(
                Keyword("Line")+lpar+inum+rpar+eq+
                Group(lbrace+inum+colon+inum+rbrace)+semi
                )
        lineloop = Group(
                Keyword("Line Loop")+lpar+inum+rpar+eq+
                Group(lbrace+inum+OneOrMore(colon+inum)+rbrace)+semi
                )
        circle = Group(
                Keyword("Circle")+lpar+inum+rpar+eq+
                Group(lbrace+inum+colon+inum+colon+inum+rbrace)+semi
                )
        planesurface = Group(
                Keyword("Plane Surface")+lpar+inum+rpar+eq+
                Group(lbrace+inum+rbrace)+semi
                )
        ruledsurface = Group(
                Keyword("Ruled Surface")+lpar+inum+rpar+eq+
                Group(lbrace+inum+rbrace)+semi
                )
        surfaceloop = Group(
                Keyword("Surface Loop")+lpar+inum+rpar+eq+
                Group(lbrace+inum+OneOrMore(colon+inum)+rbrace)+semi
                )
        volume = Group(
                Keyword("Volume")+lpar+inum+rpar+eq+
                Group(lbrace+inum+rbrace)+semi
                )
        physicalsurface = Group(
                Keyword("Physical Surface")+lpar+inum+rpar+eq+
                Group(lbrace+inum+ZeroOrMore(colon+inum)+rbrace)+semi
                )
        physicalvolume = Group(
                Keyword("Physical Volume")+lpar+inum+rpar+eq+
                Group(lbrace+inum+ZeroOrMore(colon+inum)+rbrace)+semi
                )
        skip1 = Group(
                Word(alphanums)+eq+fnum+semi
                )

        comment = Group( Literal("//")+restOfLine).suppress()

        command = point | line | lineloop | circle | planesurface | ruledsurface | \
                surfaceloop | volume | physicalsurface | physicalvolume | comment \
                | skip1

        grammar= OneOrMore(command)+StringEnd()

        try:
            tokens= grammar.parseFile(filename)
        except ParseException, err:
            print err.line
            print " "*(err.column-1) + "^"
            print err
            raise err

        lineloops={}
        surfaceloops={}
        geo=geometry()
        for x in tokens:
            if x[0]=="Point":
                geo.addpoint(int(x[1]),[float(x[2][0]),float(x[2][1]),float(x[2][2])])
            elif x[0]=="Line":
                assert len(x[2])==2
                geo.addline(int(x[1]),[int(x[2][0]),int(x[2][1])])
            elif x[0]=="Circle":
                assert len(x[2])==3
                geo.addline(int(x[1]),[int(x[2][0]),int(x[2][2])])
                #geo.add1(geom.circle(int(x[1]),int(x[2][0]),int(x[2][1]),
                #    int(x[2][2])))
            elif x[0]=="Line Loop":
                lineloops[int(x[1])]=[int(y) for y in x[2]]
            elif x[0]=="Plane Surface":
                assert len(x[2])==1
                geo.addsurface(int(x[1]),lineloops[int(x[2][0])])
            elif x[0]=="Ruled Surface":
                assert len(x[2])==1
                geo.addsurface(int(x[1]),lineloops[int(x[2][0])])
            elif x[0]=="Surface Loop":
                surfaceloops[int(x[1])]=[int(y) for y in x[2]]
            elif x[0]=="Volume":
                assert len(x[2])==1
                geo.addvolume(int(x[1]),surfaceloops[int(x[2][0])])
            elif x[0]=="Physical Surface":
                geo.addphysicalsurface(int(x[1]),[int(y) for y in x[2]])
            elif x[0]=="Physical Volume":
                geo.addphysicalvolume(int(x[1]),[int(y) for y in x[2]])
            else:
                raise "Unsupported entity: "+x[0]

        return geo

class geomobject(object):
    def getn(self):
        return self.n

class point(geomobject):
    def __init__(self,g,n,p):
        self.geom=g
        self.n=n
        self.p=p
    def __add__(self,p):
        return point(self.geom,-1,[a+b for a,b in zip(self.p,p.p)])
    def __sub__(self,p):
        return point(self.geom,-1,[a-b for a,b in zip(self.p,p.p)])
    def __div__(self,num):
        return point(self.geom,-1,[a/num for a in self.p])
    def __mul__(self,num):
        return point(self.geom,-1,[a*num for a in self.p])
    def __rmul__(self,num):
        return self.__mul__(num)
    def getxyz(self):
        return self.p
    def getstr(self):
        if self.geom.dim == 2:
            return "%f, %f" % tuple(self.getxyz())
        elif self.geom.dim == 3:
            return "%f, %f, %f" % tuple(self.getxyz())
        else:
            return None

class line(geomobject):
    def __init__(self,g,n,l):
        self.geom=g
        self.n=n
        self.points=l
    def getpoints(self):
        return [self.geom.d0[x] for x in self.points]

class surface(geomobject):
    def __init__(self,g,n,s, is_hole=False):
        self.geom=g
        self.n=n
        self.lines,self.holes=self.separate(s)
        self.is_hole = is_hole
    def separate(self,s):
        #FIXME - this is just a quick hack to satisfy all the examples
        if len(s)<=4:
            return s,[]
        elif len(s)==8:
            return s[:4],[s[4:]]
        else:
            return s,[]
    def getlines(self):
        return [self.geom.d1[abs(x)] for x in self.lines]
    def getpoints(self):
        #self.lines contains the numbers of all the lines
        def p(idx):
            "Return the correct point of the line 'idx'"
            if idx>0:
                return self.geom.d1[idx].getpoints()[0]
            else:
                return self.geom.d1[-idx].getpoints()[1]
        return [p(x) for x in self.lines]
    def getholepoints(self):
        def p(idx):
            "Return the correct point of the line 'idx'"
            if idx>0:
                return self.geom.d1[idx].getpoints()[0]
            else:
                return self.geom.d1[-idx].getpoints()[1]
        r=[]
        if self.is_hole:
            r.append(self.getinsidepoint())
        else:
            for hole in self.holes:
                r.append([p(x) for x in hole])
        return r
    def getcenterpoint(self):
        pts=self.getpoints()
        return sum(pts[1:], pts[0] * 0) / float(len(pts))
    def getinsidepoint(self):
        p0 = self.getpoints()[0]
        pc = self.getcenterpoint()
        return p0 + (pc - p0) * 0.001

class volume(geomobject):
    def __init__(self,g,n,v):
        self.geom=g
        self.n=n
        self.surfaces=v
    def getsurfaces(self):
        return [self.geom.d2[abs(x)] for x in self.surfaces]
    def getinsidepoint(self):
        sfs=self.getsurfaces()[:3]
        pts=[s.getinsidepoint() for s in sfs]
        p0=sfs[0].getpoints()[0]
        direct=(pts[0]+pts[1]+pts[2]) / 3.0 - p0
        return p0+0.001*direct

class physicalsurface(geomobject):
    def __init__(self,g,n,s):
        self.geom=g
        self.n=n
        self.surfaces=s
    def getsurfaces(self):
        return [self.geom.d2[x] for x in self.surfaces]

class physicalvolume(geomobject):
    def __init__(self,g,n,v):
        self.geom=g
        self.n=n
        self.volumes=v
    def getvolumes(self):
        return [self.geom.d3[x] for x in self.volumes]

########NEW FILE########
__FILENAME__ = mesh_generators
import numpy as nm

from sfepy.base.base import output, assert_
from sfepy.base.ioutils import ensure_path
from sfepy.linalg import cycle
from sfepy.discrete.fem.mesh import Mesh
from sfepy.mesh.mesh_tools import elems_q2t

def get_tensor_product_conn(shape):
    """
    Generate vertex connectivity for cells of a tensor-product mesh of the
    given shape.

    Parameters
    ----------
    shape : array of 2 or 3 ints
        Shape (counts of nodes in x, y, z) of the mesh.

    Returns
    -------
    conn : array
        The vertex connectivity array.
    desc : str
        The cell kind.
    """
    shape = nm.asarray(shape)
    dim = len(shape)
    assert_(1 <= dim <= 3)

    n_nod = nm.prod(shape)
    n_el = nm.prod(shape - 1)

    grid = nm.arange(n_nod, dtype=nm.int32)
    grid.shape = shape

    if dim == 1:
        conn = nm.zeros((n_el, 2), dtype=nm.int32)
        conn[:, 0] = grid[:-1]
        conn[:, 1] = grid[1:]
        desc = '1_2'

    elif dim == 2:
        conn = nm.zeros((n_el, 4), dtype=nm.int32)
        conn[:, 0] = grid[:-1, :-1].flat
        conn[:, 1] = grid[1:, :-1].flat
        conn[:, 2] = grid[1:, 1:].flat
        conn[:, 3] = grid[:-1, 1:].flat
        desc = '2_4'

    else:
        conn = nm.zeros((n_el, 8), dtype=nm.int32)
        conn[:, 0] = grid[:-1, :-1, :-1].flat
        conn[:, 1] = grid[1:, :-1, :-1].flat
        conn[:, 2] = grid[1:, 1:, :-1].flat
        conn[:, 3] = grid[:-1, 1:, :-1].flat
        conn[:, 4] = grid[:-1, :-1, 1:].flat
        conn[:, 5] = grid[1:, :-1, 1:].flat
        conn[:, 6] = grid[1:, 1:, 1:].flat
        conn[:, 7] = grid[:-1, 1:, 1:].flat
        desc = '3_8'

    return conn, desc

def gen_block_mesh(dims, shape, centre, mat_id=0, name='block',
                   coors=None, verbose=True):
    """
    Generate a 2D or 3D block mesh. The dimension is determined by the
    lenght of the shape argument.

    Parameters
    ----------
    dims : array of 2 or 3 floats
        Dimensions of the block.
    shape : array of 2 or 3 ints
        Shape (counts of nodes in x, y, z) of the block mesh.
    centre : array of 2 or 3 floats
        Centre of the block.
    mat_id : int, optional
        The material id of all elements.
    name : string
        Mesh name.
    verbose : bool
        If True, show progress of the mesh generation.

    Returns
    -------
    mesh : Mesh instance
    """
    dims = nm.asarray(dims, dtype=nm.float64)
    shape = nm.asarray(shape, dtype=nm.int32)
    centre = nm.asarray(centre, dtype=nm.float64)

    dim = shape.shape[0]

    centre = centre[:dim]
    dims = dims[:dim]

    n_nod = nm.prod(shape)
    output('generating %d vertices...' % n_nod, verbose=verbose)

    x0 = centre - 0.5 * dims
    dd = dims / (shape - 1)

    ngrid = nm.mgrid[[slice(ii) for ii in shape]]
    ngrid.shape = (dim, n_nod)

    coors = x0 + ngrid.T * dd
    output('...done', verbose=verbose)

    n_el = nm.prod(shape - 1)
    output('generating %d cells...' % n_el, verbose=verbose)

    mat_ids = nm.empty((n_el,), dtype=nm.int32)
    mat_ids.fill(mat_id)

    conn, desc = get_tensor_product_conn(shape)
    output('...done', verbose=verbose)

    mesh = Mesh.from_data(name, coors, None, [conn], [mat_ids], [desc])
    return mesh

def gen_cylinder_mesh(dims, shape, centre, axis='x', force_hollow=False,
                      is_open=False, open_angle=0.0, non_uniform=False,
                      name='cylinder', verbose=True):
    """
    Generate a cylindrical mesh along an axis. Its cross-section can be
    ellipsoidal.

    Parameters
    ----------
    dims : array of 5 floats
        Dimensions of the cylinder: inner surface semi-axes a1, b1, outer
        surface semi-axes a2, b2, length.
    shape : array of 3 ints
        Shape (counts of nodes in radial, circumferential and longitudinal
        directions) of the cylinder mesh.
    centre : array of 3 floats
        Centre of the cylinder.
    axis: one of 'x', 'y', 'z'
        The axis of the cylinder.
    force_hollow : boolean
        Force hollow mesh even if inner radii a1 = b1 = 0.
    is_open : boolean
        Generate an open cylinder segment.
    open_angle : float
        Opening angle in radians.
    non_uniform : boolean
        If True, space the mesh nodes in radial direction so that the element
        volumes are (approximately) the same, making thus the elements towards
        the outer surface thinner.
    name : string
        Mesh name.
    verbose : bool
        If True, show progress of the mesh generation.

    Returns
    -------
    mesh : Mesh instance
    """
    dims = nm.asarray(dims, dtype=nm.float64)
    shape = nm.asarray(shape, dtype=nm.int32)
    centre = nm.asarray(centre, dtype=nm.float64)

    a1, b1, a2, b2, length = dims
    nr, nfi, nl = shape
    origin = centre - nm.array([0.5 * length, 0.0, 0.0])

    dfi = 2.0 * (nm.pi - open_angle) / nfi
    if is_open:
        nnfi = nfi + 1
    else:
        nnfi = nfi

    is_hollow = force_hollow or not (max(abs(a1), abs(b1)) < 1e-15)

    if is_hollow:
        mr = 0
    else:
        mr = (nnfi - 1) * nl

    grid = nm.zeros((nr, nnfi, nl), dtype=nm.int32)

    n_nod = nr * nnfi * nl - mr
    coors = nm.zeros((n_nod, 3), dtype=nm.float64)

    angles = nm.linspace(open_angle, open_angle+(nfi)*dfi, nfi+1)
    xs = nm.linspace(0.0, length, nl)
    if non_uniform:
        ras = nm.zeros((nr,), dtype=nm.float64)
        rbs = nm.zeros_like(ras)
        advol = (a2**2 - a1**2) / (nr - 1)
        bdvol = (b2**2 - b1**2) / (nr - 1)
        ras[0], rbs[0] = a1, b1
        for ii in range(1, nr):
            ras[ii] = nm.sqrt(advol + ras[ii-1]**2)
            rbs[ii] = nm.sqrt(bdvol + rbs[ii-1]**2)
    else:
        ras = nm.linspace(a1, a2, nr)
        rbs = nm.linspace(b1, b2, nr)

    # This is 3D only...
    output('generating %d vertices...' % n_nod, verbose=verbose)
    ii = 0
    for ix in range(nr):
        a, b = ras[ix], rbs[ix]
        for iy, fi in enumerate(angles[:nnfi]):
            for iz, x in enumerate(xs):
                grid[ix,iy,iz] = ii
                coors[ii] = origin + [x, a * nm.cos(fi), b * nm.sin(fi)]
                ii += 1

                if not is_hollow and (ix == 0):
                    if iy > 0:
                        grid[ix,iy,iz] = grid[ix,0,iz]
                        ii -= 1
    assert_(ii == n_nod)
    output('...done', verbose=verbose)

    n_el = (nr - 1) * nnfi * (nl - 1)
    conn = nm.zeros((n_el, 8), dtype=nm.int32)

    output('generating %d cells...' % n_el, verbose=verbose)
    ii = 0
    for (ix, iy, iz) in cycle([nr-1, nnfi, nl-1]):
        if iy < (nnfi - 1):
            conn[ii,:] = [grid[ix  ,iy  ,iz  ], grid[ix+1,iy  ,iz  ],
                          grid[ix+1,iy+1,iz  ], grid[ix  ,iy+1,iz  ],
                          grid[ix  ,iy  ,iz+1], grid[ix+1,iy  ,iz+1],
                          grid[ix+1,iy+1,iz+1], grid[ix  ,iy+1,iz+1]]
            ii += 1
        elif not is_open:
            conn[ii,:] = [grid[ix  ,iy  ,iz  ], grid[ix+1,iy  ,iz  ],
                          grid[ix+1,0,iz  ], grid[ix  ,0,iz  ],
                          grid[ix  ,iy  ,iz+1], grid[ix+1,iy  ,iz+1],
                          grid[ix+1,0,iz+1], grid[ix  ,0,iz+1]]
            ii += 1

    mat_id = nm.zeros((n_el,), dtype = nm.int32)
    desc = '3_8'

    assert_(n_nod == (conn.max() + 1))
    output('...done', verbose=verbose)

    if axis == 'z':
        coors = coors[:,[1,2,0]]
    elif axis == 'y':
        coors = coors[:,[2,0,1]]

    mesh = Mesh.from_data(name, coors, None, [conn], [mat_id], [desc])
    return mesh

def _spread_along_axis(axis, coors, tangents, grading_fun):
    """
    Spread the coordinates along the given axis using the grading function, and
    the tangents in the other two directions.
    """
    oo = list(set([0, 1, 2]).difference([axis]))
    c0, c1, c2 = coors[:, axis], coors[:, oo[0]], coors[:, oo[1]]

    out = nm.empty_like(coors)

    mi, ma = c0.min(), c0.max()
    nc0 = (c0 - mi) / (ma - mi)
    out[:, axis] = oc0 = grading_fun(nc0) * (ma - mi) + mi

    nc = oc0 - oc0.min()

    mi, ma = c1.min(), c1.max()
    n1 = 2 * (c1 - mi) / (ma - mi) - 1
    out[:, oo[0]] = c1 + n1 * nc * tangents[oo[0]]

    mi, ma = c2.min(), c2.max()
    n2 = 2 * (c2 - mi) / (ma - mi) - 1
    out[:, oo[1]] = c2 + n2 * nc * tangents[oo[1]]

    return out

def _get_extension_side(side, grading_fun, mat_id,
                        b_dims, b_shape, e_dims, e_shape, centre):
    """
    Get a mesh extending the given side of a block mesh.
    """
    # Pure extension dimensions.
    pe_dims = 0.5 * (e_dims - b_dims)
    coff = 0.5 * (b_dims + pe_dims)
    cc = centre + coff * nm.eye(3)[side]

    if side == 0: # x axis.
        dims = [pe_dims[0], b_dims[1], b_dims[2]]
        shape = [e_shape, b_shape[1], b_shape[2]]
        tangents = [0, pe_dims[1] / pe_dims[0], pe_dims[2] / pe_dims[0]]

    elif side == 1: # y axis.
        dims = [b_dims[0], pe_dims[1], b_dims[2]]
        shape = [b_shape[0], e_shape, b_shape[2]]
        tangents = [pe_dims[0] / pe_dims[1], 0, pe_dims[2] / pe_dims[1]]

    elif side == 2: # z axis.
        dims = [b_dims[0], b_dims[1], pe_dims[2]]
        shape = [b_shape[0], b_shape[1], e_shape]
        tangents = [pe_dims[0] / pe_dims[2], pe_dims[1] / pe_dims[2], 0]

    e_mesh = gen_block_mesh(dims, shape, cc, mat_id=mat_id, verbose=False)
    e_mesh.coors[:] = _spread_along_axis(side, e_mesh.coors, tangents,
                                         grading_fun)

    return e_mesh, shape

def gen_extended_block_mesh(b_dims, b_shape, e_dims, e_shape, centre,
                            grading_fun=None, name=None):
    """
    Generate a 3D mesh with a central block and (coarse) extending side meshes.

    The resulting mesh is again a block. Each of the components has a different
    material id.

    Parameters
    ----------
    b_dims : array of 3 floats
        The dimensions of the central block.
    b_shape : array of 3 ints
        The shape (counts of nodes in x, y, z) of the central block mesh.
    e_dims : array of 3 floats
        The dimensions of the complete block (central block + extensions).
    e_shape : int
        The count of nodes of extending blocks in the direction from the
        central block.
    centre : array of 3 floats
        The centre of the mesh.
    grading_fun : callable, optional
        A function of :math:`x \in [0, 1]` that can be used to shift nodes in
        the extension axis directions to allow smooth grading of element sizes
        from the centre. The default function is :math:`x**p` with :math:`p`
        determined so that the element sizes next to the central block have the
        size of the shortest edge of the central block.
    name : string, optional
        The mesh name.

    Returns
    -------
    mesh : Mesh instance
    """
    b_dims = nm.asarray(b_dims, dtype=nm.float64)
    b_shape = nm.asarray(b_shape, dtype=nm.int32)
    e_dims = nm.asarray(e_dims, dtype=nm.float64)
    centre = nm.asarray(centre, dtype=nm.float64)

    # Pure extension dimensions.
    pe_dims = 0.5 * (e_dims - b_dims)
    # Central block element sizes.
    dd = (b_dims / (b_shape - 1))
    # The "first x" going to grading_fun.
    nc = 1.0 / (e_shape - 1)
    # Grading power and function.
    power = nm.log(dd.min() / pe_dims.min()) / nm.log(nc)
    grading_fun = (lambda x: x**power) if grading_fun is None else grading_fun

    # Central block mesh.
    b_mesh = gen_block_mesh(b_dims, b_shape, centre, mat_id=0, verbose=False)

    # 'x' extension.
    e_mesh, xs = _get_extension_side(0, grading_fun, 10,
                                     b_dims, b_shape, e_dims, e_shape, centre)
    mesh = b_mesh + e_mesh

    # Mirror by 'x'.
    e_mesh.coors[:, 0] = (2 * centre[0]) - e_mesh.coors[:, 0]
    e_mesh.mat_ids[0].fill(11)
    mesh = mesh + e_mesh

    # 'y' extension.
    e_mesh, ys = _get_extension_side(1, grading_fun, 20,
                                     b_dims, b_shape, e_dims, e_shape, centre)
    mesh = mesh + e_mesh

    # Mirror by 'y'.
    e_mesh.coors[:, 1] = (2 * centre[1]) - e_mesh.coors[:, 1]
    e_mesh.mat_ids[0].fill(21)
    mesh = mesh + e_mesh

    # 'z' extension.
    e_mesh, zs = _get_extension_side(2, grading_fun, 30,
                                     b_dims, b_shape, e_dims, e_shape, centre)
    mesh = mesh + e_mesh

    # Mirror by 'z'.
    e_mesh.coors[:, 2] = (2 * centre[2]) - e_mesh.coors[:, 2]
    e_mesh.mat_ids[0].fill(31)
    mesh = mesh + e_mesh

    if name is not None:
        mesh.name = name

    # Verify merging by checking the number of nodes.
    n_nod = (nm.prod(nm.maximum(b_shape - 2, 0)) + 2 * nm.prod(xs)
             + 2 * (max(ys[0] - 2, 0) * ys[1] * ys[2])
             + 2 * (max(zs[0] - 2, 0) * max(zs[1] - 2, 0) * zs[2]))
    if n_nod != mesh.n_nod:
        raise ValueError('Merge of meshes failed! (%d == %d)'
                         % (n_nod, mesh.n_nod))

    return mesh

def tiled_mesh1d(conns, coors, ngrps, idim, n_rep, bb, eps=1e-6, ndmap=False):
    from sfepy.discrete.fem.periodic import match_grid_plane

    s1 = nm.nonzero(coors[:,idim] < (bb[0] + eps))[0]
    s2 = nm.nonzero(coors[:,idim] > (bb[1] - eps))[0]

    if s1.shape != s2.shape:
        raise ValueError, 'incompatible shapes: %s == %s'\
              % (s1.shape, s2.shape)

    (nnod0, dim) = coors.shape
    nnod = nnod0 * n_rep - s1.shape[0] * (n_rep - 1)
    (nel0, nnel) = conns.shape
    nel = nel0 * n_rep

    dd = nm.zeros((dim,), dtype=nm.float64)
    dd[idim] = bb[1] - bb[0]

    m1, m2 = match_grid_plane(coors[s1], coors[s2], idim)

    oconns = nm.zeros((nel, nnel), dtype=nm.int32)
    ocoors = nm.zeros((nnod, dim), dtype=nm.float64)
    ongrps = nm.zeros((nnod,), dtype=nm.int32)

    if type(ndmap) is bool:
        ret_ndmap = ndmap

    else:
        ret_ndmap= True
        ndmap_out = nm.zeros((nnod,), dtype=nm.int32)

    el_off = 0
    nd_off = 0

    for ii in range(n_rep):
        if ii == 0:
            oconns[0:nel0,:] = conns
            ocoors[0:nnod0,:] = coors
            ongrps[0:nnod0] = ngrps.squeeze()
            nd_off += nnod0

            mapto = s2[m2]
            mask = nm.ones((nnod0,), dtype=nm.int32)
            mask[s1] = 0
            remap0 = nm.cumsum(mask) - 1
            nnod0r = nnod0 - s1.shape[0]
            cidx = nm.where(mask)
            if ret_ndmap:
                ndmap_out[0:nnod0] = nm.arange(nnod0)

        else:
            remap = remap0 + nd_off
            remap[s1[m1]] = mapto
            mapto = remap[s2[m2]]

            ocoors[nd_off:(nd_off + nnod0r),:] =\
              (coors[cidx,:] + ii * dd)
            ongrps[nd_off:(nd_off + nnod0r)] = ngrps[cidx].squeeze()
            oconns[el_off:(el_off + nel0),:] = remap[conns]
            if ret_ndmap:
                ndmap_out[nd_off:(nd_off + nnod0r)] = cidx[0]

            nd_off += nnod0r

        el_off += nel0

    if ret_ndmap:
        if ndmap is not None:
            max_nd_ref = nm.max(ndmap)
            idxs = nm.where(ndmap_out > max_nd_ref)
            ndmap_out[idxs] = ndmap[ndmap_out[idxs]]

        return oconns, ocoors, ongrps, ndmap_out

    else:
        return oconns, ocoors, ongrps

def gen_tiled_mesh(mesh, grid=None, scale=1.0, eps=1e-6, ret_ndmap=False):
    """
    Generate a new mesh by repeating a given periodic element
    along each axis.

    Parameters
    ----------
    mesh : Mesh instance
        The input periodic FE mesh.
    grid : array
        Number of repetition along each axis.
    scale : float, optional
        Scaling factor.
    eps : float, optional
        Tolerance for boundary detection.
    ret_ndmap : bool, optional
        If True, return global node map.

    Returns
    -------
    mesh_out : Mesh instance
        FE mesh.
    ndmap : array
        Maps: actual node id --> node id in the reference cell.
    """
    bbox = mesh.get_bounding_box()

    if grid is None:
        iscale = max(int(1.0 / scale), 1)
        grid = [iscale] * mesh.dim

    conns = mesh.conns[0]
    for ii in mesh.conns[1:]:
        conns = nm.vstack((conns, ii))
    mat_ids = mesh.mat_ids[0]
    for ii in mesh.mat_ids[1:]:
        mat_ids = nm.hstack((mat_ids, ii))

    coors = mesh.coors
    ngrps = mesh.ngroups
    nrep = nm.prod(grid)
    ndmap = None

    output('repeating %s ...' % grid)
    nblk = 1
    for ii, gr in enumerate(grid):
        if ret_ndmap:
            (conns, coors,
             ngrps, ndmap0) = tiled_mesh1d(conns, coors, ngrps,
                                           ii, gr, bbox.transpose()[ii],
                                           eps=eps, ndmap=ndmap)
            ndmap = ndmap0

        else:
            conns, coors, ngrps = tiled_mesh1d(conns, coors, ngrps,
                                               ii, gr, bbox.transpose()[ii],
                                               eps=eps)
        nblk *= gr

    output('...done')

    mat_ids = nm.tile(mat_ids, (nrep,))
    mesh_out = Mesh.from_data('tiled mesh', coors * scale, ngrps,
                              [conns], [mat_ids], [mesh.descs[0]])

    if ret_ndmap:
        return mesh_out, ndmap
    else:
        return mesh_out

def gen_misc_mesh(mesh_dir, force_create, kind, args, suffix='.mesh',
                  verbose=False):
    """
    Create sphere or cube mesh according to `kind` in the given
    directory if it does not exist and return path to it.
    """
    import os
    from sfepy import data_dir

    defdir = os.path.join(data_dir, 'meshes')
    if mesh_dir is None:
        mesh_dir = defdir

    def retype(args, types, defaults):
        args=list(args)
        args.extend(defaults[len(args):len(defaults)])
        return tuple([type(value) for type, value in zip(types, args) ])

    if kind == 'sphere':
        default = [5, 41, args[0]]
        args = retype(args, [float, int, float], default)
        mesh_pattern = os.path.join(mesh_dir, 'sphere-%.2f-%.2f-%i')

    else:
        assert_(kind == 'cube')

        args = retype(args,
                      (int, float, int, float, int, float),
                      (args[0], args[1], args[0], args[1], args[0], args[1]))
        mesh_pattern = os.path.join(mesh_dir, 'cube-%i_%.2f-%i_%.2f-%i_%.2f')

    if verbose:
        output(args)

    filename = mesh_pattern % args
    if not force_create:
        if os.path.exists(filename): return filename
        if os.path.exists(filename + '.mesh') : return filename + '.mesh'
        if os.path.exists(filename + '.vtk'): return filename + '.vtk'

    if kind == 'cube':
        filename = filename + suffix
        ensure_path(filename)

        output('creating new cube mesh')
        output('(%i nodes in %.2f) x (%i nodes in %.2f) x (%i nodes in %.2f)'
               % args)
        output('to file %s...' % filename)

        mesh = gen_block_mesh(args[1::2], args[0::2],
                              (0.0, 0.0, 0.0), name=filename)
        mesh.write(filename, io='auto')
        output('...done')

    else:
        import subprocess, shutil, tempfile
        filename = filename + '.mesh'
        ensure_path(filename)

        output('creating new sphere mesh (%i nodes, r=%.2f) and gradation %d'
               % args)
        output('to file %s...' % filename)

        f = open(os.path.join(defdir, 'quantum', 'sphere.geo'))
        tmp_dir = tempfile.mkdtemp()
        tmpfile = os.path.join(tmp_dir, 'sphere.geo.temp')
        ff = open(tmpfile, "w")
        ff.write("""
R = %i.0;
n = %i.0;
dens = %f;
""" % args)
        ff.write(f.read())
        f.close()
        ff.close()
        subprocess.call(['gmsh', '-3', tmpfile, '-format', 'mesh',
                         '-o', filename])
        shutil.rmtree(tmp_dir)
        output('...done')

    return filename

def gen_mesh_from_string(mesh_name, mesh_dir):
    import re
    result = re.match('^\\s*([a-zA-Z]+)[:\\(]([^\\):]*)[:\\)](\\*)?\\s*$',
                      mesh_name)

    if result is None:
        return mesh_name

    else:
        args = re.split(',', result.group(2))
        kind = result.group(1)
        return gen_misc_mesh(mesh_dir, result.group(3)=='*', kind, args)

def gen_mesh_from_goem(geo, a=None, quadratic=False, verbose=True,
                       refine=False, polyfilename='./meshgen.poly',
                       out='mesh', **kwargs):
    """
    Runs mesh generator - tetgen for 3D or triangle for 2D meshes.

    Parameters
    ----------
    geo : geometry
        geometry description
    a : int, optional
        a maximum area/volume constraint
    quadratic : bool, optional
        set True for quadratic elements
    verbose : bool, optional
        detailed information
    refine : bool, optional
        refines mesh

    Returns
    -------
    mesh : Mesh instance
        triangular or tetrahedral mesh
    """

    import os.path as op
    import pexpect

    # write geometry to poly file
    geo.to_poly_file(polyfilename)

    if not refine:
        params = "-Apq"
    else:
        params = "-Arq"
    if verbose:
        params = params + " -Q"
    if a != None and not refine:
        params = params + " -a%f" % (a)
    if refine:
        params = params + " -a"
    if quadratic:
        params = params + " -o2"
    params = params + " %s" % (polyfilename)

    meshgen_call = {2: 'triangle', 3: 'tetgen'}
    cmd = "%s %s" % (meshgen_call[geo.dim], params)
    if verbose: print "Generating mesh using", cmd
    if geo.dim == 2:
        p=pexpect.run(cmd, timeout=None)
        bname, ext = op.splitext(polyfilename)
        mesh = Mesh.from_file(bname + '.1.node')
        mesh.write(bname + '.' + out)
    if geo.dim == 3:
        p=pexpect.spawn(cmd, timeout=None)
        if not refine:
            p.expect("Opening %s." % (polyfilename))
        else:
            p.expect("Opening %s.node.\r\n" % (polyfilename))
            p.expect("Opening %s.ele.\r\n" % (polyfilename))
            p.expect("Opening %s.face.\r\n" % (polyfilename))
            p.expect("Opening %s.vol." % (polyfilename))
        assert p.before == ""
        p.expect(pexpect.EOF)
        if p.before != "\r\n":
            print p.before
            raise "Error when running mesh generator (see above for output): %s" % cmd

# http://www.cs.cmu.edu/~quake/triangle.html
#
# triangle [-prq__a__uAcDjevngBPNEIOXzo_YS__iFlsCQVh] input_file
#     -p  Triangulates a Planar Straight Line Graph (.poly file).
#     -r  Refines a previously generated mesh.
#     -q  Quality mesh generation.  A minimum angle may be specified.
#     -a  Applies a maximum triangle area constraint.
#     -u  Applies a user-defined triangle constraint.
#     -A  Applies attributes to identify triangles in certain regions.
#     -c  Encloses the convex hull with segments.
#     -D  Conforming Delaunay:  all triangles are truly Delaunay.
#     -j  Jettison unused vertices from output .node file.
#     -e  Generates an edge list.
#     -v  Generates a Voronoi diagram.
#     -n  Generates a list of triangle neighbors.
#     -g  Generates an .off file for Geomview.
#     -B  Suppresses output of boundary information.
#     -P  Suppresses output of .poly file.
#     -N  Suppresses output of .node file.
#     -E  Suppresses output of .ele file.
#     -I  Suppresses mesh iteration numbers.
#     -O  Ignores holes in .poly file.
#     -X  Suppresses use of exact arithmetic.
#     -z  Numbers all items starting from zero (rather than one).
#     -o2 Generates second-order subparametric elements.
#     -Y  Suppresses boundary segment splitting.
#     -S  Specifies maximum number of added Steiner points.
#     -i  Uses incremental method, rather than divide-and-conquer.
#     -F  Uses Fortune's sweepline algorithm, rather than d-and-c.
#     -l  Uses vertical cuts only, rather than alternating cuts.
#     -s  Force segments into mesh by splitting (instead of using CDT).
#     -C  Check consistency of final mesh.
#     -Q  Quiet:  No terminal output except errors.
#     -V  Verbose:  Detailed information on what I'm doing.
#     -h  Help:  Detailed instructions for Triangle.

# http://tetgen.berlios.de/
#
# tetgen [-prq_a_AiMYS_T_dzo_fenvgGOJBNEFICQVh] input_file
#     -p  Tetrahedralizes a piecewise linear complex (PLC).
#     -r  Reconstructs a previously generated mesh.
#     -q  Refines mesh (to improve mesh quality).
#     -a  Applies a maximum tetrahedron volume constraint.
#     -A  Assigns attributes to tetrahedra in different regions.
#     -i  Inserts a list of additional points into mesh.
#     -M  No merge of coplanar facets.
#     -Y  No splitting of input boundaries (facets and segments).
#     -S  Specifies maximum number of added points.
#     -T  Sets a tolerance for coplanar test (default 1e-8).
#     -d  Detects self-intersections of facets of the PLC.
#     -z  Numbers all output items starting from zero.
#     -o2 Generates second-order subparametric elements.
#     -f  Outputs all faces to .face file.
#     -e  Outputs all edges to .edge file.
#     -n  Outputs tetrahedra neighbors to .neigh file.
#     -v  Outputs Voronoi diagram to files.
#     -g  Outputs mesh to .mesh file for viewing by Medit.
#     -G  Outputs mesh to .msh file for viewing by Gid.
#     -O  Outputs mesh to .off file for viewing by Geomview.
#     -K  Outputs mesh to .vtk file for viewing by Paraview.
#     -J  No jettison of unused vertices from output .node file.
#     -B  Suppresses output of boundary information.
#     -N  Suppresses output of .node file.
#     -E  Suppresses output of .ele file.
#     -F  Suppresses output of .face file.
#     -I  Suppresses mesh iteration numbers.
#     -C  Checks the consistency of the final mesh.
#     -Q  Quiet:  No terminal output except errors.
#     -V  Verbose:  Detailed information, more terminal output.
#     -h  Help:  A brief instruction for using TetGen.

def gen_mesh_from_voxels(voxels, dims, etype='q'):
    """
    Generate FE mesh from voxels (volumetric data).

    Parameters
    ----------
    voxels : array
        Voxel matrix, 1=material.
    dims : array
        Size of one voxel.
    etype : integer, optional
        'q' - quadrilateral or hexahedral elements
        't' - triangular or tetrahedral elements
    Returns
    -------
    mesh : Mesh instance
        Finite element mesh.
    """

    dims = dims.squeeze()
    dim = len(dims)
    nddims = nm.array(voxels.shape) + 2

    nodemtx = nm.zeros(nddims, dtype=nm.int32)

    if dim == 2:
        #iy, ix = nm.where(voxels.transpose())
        iy, ix = nm.where(voxels)
        nel = ix.shape[0]

        if etype == 'q':
            nodemtx[ix,iy] += 1
            nodemtx[ix + 1,iy] += 1
            nodemtx[ix + 1,iy + 1] += 1
            nodemtx[ix,iy + 1] += 1

        elif etype == 't':
            nodemtx[ix,iy] += 2
            nodemtx[ix + 1,iy] += 1
            nodemtx[ix + 1,iy + 1] += 2
            nodemtx[ix,iy + 1] += 1
            nel *= 2

    elif dim == 3:
        #iy, ix, iz = nm.where(voxels.transpose(1, 0, 2))
        iy, ix, iz = nm.where(voxels)
        nel = ix.shape[0]

        if etype == 'q':
            nodemtx[ix,iy,iz] += 1
            nodemtx[ix + 1,iy,iz] += 1
            nodemtx[ix + 1,iy + 1,iz] += 1
            nodemtx[ix,iy + 1,iz] += 1
            nodemtx[ix,iy,iz + 1] += 1
            nodemtx[ix + 1,iy,iz + 1] += 1
            nodemtx[ix + 1,iy + 1,iz + 1] += 1
            nodemtx[ix,iy + 1,iz + 1] += 1

        elif etype == 't':
            nodemtx[ix,iy,iz] += 6
            nodemtx[ix + 1,iy,iz] += 2
            nodemtx[ix + 1,iy + 1,iz] += 2
            nodemtx[ix,iy + 1,iz] += 2
            nodemtx[ix,iy,iz + 1] += 2
            nodemtx[ix + 1,iy,iz + 1] += 2
            nodemtx[ix + 1,iy + 1,iz + 1] += 6
            nodemtx[ix,iy + 1,iz + 1] += 2
            nel *= 6

    else:
        msg = 'incorrect voxel dimension! (%d)' % dim
        raise ValueError(msg)

    ndidx = nm.where(nodemtx)
    coors = nm.array(ndidx).transpose() * dims
    nnod = coors.shape[0]

    nodeid = -nm.ones(nddims, dtype=nm.int32)
    nodeid[ndidx] = nm.arange(nnod)

    # generate elements
    if dim == 2:
        elems = nm.array([nodeid[ix,iy],
                          nodeid[ix + 1,iy],
                          nodeid[ix + 1,iy + 1],
                          nodeid[ix,iy + 1]]).transpose()

    elif dim == 3:
        elems = nm.array([nodeid[ix,iy,iz],
                          nodeid[ix + 1,iy,iz],
                          nodeid[ix + 1,iy + 1,iz],
                          nodeid[ix,iy + 1,iz],
                          nodeid[ix,iy,iz + 1],
                          nodeid[ix + 1,iy,iz + 1],
                          nodeid[ix + 1,iy + 1,iz + 1],
                          nodeid[ix,iy + 1,iz + 1]]).transpose()

    if etype == 't':
        elems = elems_q2t(elems)

    eid = etype + str(dim)
    eltab = {'q2': 4, 'q3': 8, 't2': 3, 't3': 4}

    mesh = Mesh.from_data('voxel_data',
                          coors, nm.ones((nnod,), dtype=nm.int32),
                          {0: nm.ascontiguousarray(elems)},
                          {0: nm.ones((nel,), dtype=nm.int32)},
                          {0: '%d_%d' % (dim, eltab[eid])})

    return mesh

def gen_mesh_from_poly(filename, verbose=True):
    """
    Import mesh generated by tetgen or triangle.

    Parameters
    ----------
    filename : string
        file name

    Returns
    -------
    mesh : Mesh instance
        triangular or tetrahedral mesh
    """

    def getnodes(fnods,up):
        f=file(fnods)
        l=[int(x) for x in f.readline().split()]
        npoints,dim,nattrib,nbound=l
        if verbose: up.init(npoints)
        nodes=[]
        for line in f:
            if line[0]=="#": continue
            l=[float(x) for x in line.split()]
            l = l[:(dim + 1)]
            l[0]=int(l[0])
            nodes.append(tuple(l))
            assert l[0]==len(nodes)
        assert npoints==len(nodes)
        return nodes

    def getele(fele,up):
        f=file(fele)
        l=[int(x) for x in f.readline().split()]
        nele,nnod,nattrib=l
        #we have either linear or quadratic tetrahedra:
        if nnod in [4,10]:
            elem = 'tetra'
            linear = (nnod == 4)
        if nnod in [3, 7]:
            elem = 'tri'
            linear = (nnod == 3)

        # if nattrib!=1:
        #     raise "tetgen didn't assign an entity number to each element (option -A)"
        els=[]
        regions={}
        for line in f:
            if line[0]=="#": continue
            l=[int(x) for x in line.split()]
            if elem == 'tri':
                if linear:
                    assert (len(l) - 1 - nattrib) == 3
                    els.append((l[0],l[1],l[2],l[3]))
                    regionnum=l[5]
                else:
                    assert len(l)-2 == 10
                    els.append((l[0],54,l[1],l[2],l[3],l[4],
                                l[5],l[6],l[7],l[8],l[9],l[10]))
                    regionnum=l[11]
            if elem == 'tetra':
                if linear:
                    assert len(l)-2 == 4
                    els.append((l[0],54,l[1],l[2],l[3],l[4]))
                    regionnum=l[5]
                else:
                    assert len(l)-2 == 10
                    els.append((l[0],54,l[1],l[2],l[3],l[4],
                                l[5],l[6],l[7],l[8],l[9],l[10]))
                    regionnum=l[11]
            if regionnum==0:
                print "see %s, element # %d"%(fele,l[0])
                raise "there are elements not belonging to any physical entity"
            if regions.has_key(regionnum):
                regions[regionnum].append(l[0])
            else:
                regions[regionnum]=[l[0]]
            assert l[0]==len(els)
            if verbose: up.update(l[0])
        return els,regions,linear

    def getBCfaces(ffaces,up):
        f=file(ffaces)
        l=[int(x) for x in f.readline().split()]
        nfaces,nattrib=l
        if nattrib!=1:
            raise "tetgen didn't assign an entity number to each face \
(option -A)"
        if verbose: up.init(nfaces)
        faces={}
        for line in f:
            if line[0]=="#": continue
            l=[int(x) for x in line.split()]
            assert len(l)==5
            regionnum=l[4]
            if regionnum==0: continue
            if faces.has_key(regionnum):
                faces[regionnum].append((l[1],l[2],l[3]))
            else:
                faces[regionnum]=[(l[1],l[2],l[3])]
            if verbose: up.update(l[0])
        return faces

    def calculatexyz(nodes, els):
        """Calculate the missing xyz values in place"""
        def avg(i,j,n4,nodes):
            a=nodes[n4[i-1]-1]
            b=nodes[n4[j-1]-1]
            return (a[1]+b[1])/2, (a[2]+b[2])/2, (a[3]+b[3])/2
        def getxyz(i,n4,nodes):
            if i+5==5: return avg(1,2,n4,nodes)
            if i+5==6: return avg(2,3,n4,nodes)
            if i+5==7: return avg(1,3,n4,nodes)
            if i+5==8: return avg(1,4,n4,nodes)
            if i+5==9: return avg(2,4,n4,nodes)
            if i+5==10: return avg(3,4,n4,nodes)
            raise "wrong topology"
        for e in els:
            n4=e[2:2+4]
            n6=e[2+4:2+4+10]
            for i,n in enumerate(n6):
                x,y,z=getxyz(i,n4,nodes)
                nodes[n-1]=(n,x,y,z)

    if verbose: print "Reading geometry from poly file..."
    m=Mesh()
    m.nodes=getnodes(filename+".node")
    m.elements,m.regions, lin=getele(filename+".ele")
    if not lin:
        #tetgen doesn't compute xyz coordinates of the aditional 6 nodes
        #(only of the 4 corner nodes) in tetrahedra.
        calculatexyz(m.nodes,m.elements)
    m.faces=getBCfaces(filename+".face")

    return m

def main():
    mesh = gen_block_mesh(nm.array((1.0, 2.0, 3.0)),
                          nm.array((10,10,10)), nm.array((1.0, 2.0, 3.0)),
                          name='')
    mesh.write('0.mesh', io = 'auto')

    mesh = gen_cylinder_mesh(nm.array((1.0, 1.0, 2.0, 2.0, 3)),
                             nm.array((10,10,10)), nm.array((1.0, 2.0, 3.0)),
                             is_open=False, open_angle = 0.0,
                             name='')
    mesh.write('1.mesh', io = 'auto')
    mesh = gen_cylinder_mesh(nm.array((1.0, 1.0, 2.0, 2.0, 3)),
                             nm.array((10,10,10)), nm.array((1.0, 2.0, 3.0)),
                             is_open=True, open_angle = 0.0,
                             name='')
    mesh.write('2.mesh', io = 'auto')
    mesh = gen_cylinder_mesh(nm.array((1.0, 1.0, 2.0, 2.0, 3)),
                             nm.array((10,10,10)), nm.array((1.0, 2.0, 3.0)),
                             is_open=True, open_angle = 0.5,
                             name='')
    mesh.write('3.mesh', io = 'auto')

    mesh = gen_cylinder_mesh(nm.array((0.0, 0.0, 2.0, 2.0, 3)),
                             nm.array((10,10,10)), nm.array((1.0, 2.0, 3.0)),
                             is_open=False, open_angle = 0.0,
                             name='')
    mesh.write('4.mesh', io = 'auto')

    mesh = gen_cylinder_mesh(nm.array((0.0, 0.0, 1.0, 2.0, 3)),
                             nm.array((10,10,10)), nm.array((1.0, 2.0, 3.0)),
                             is_open=True, open_angle = 0.5,
                             name='')
    mesh.write('5.mesh', io = 'auto')

    mesh = gen_cylinder_mesh(nm.array((0.0, 0.0, 1.0, 2.0, 3)),
                             nm.array((10,10,10)), nm.array((1.0, 2.0, 3.0)),
                             is_open=True, open_angle = 0.5, non_uniform=True,
                             name='')
    mesh.write('6.mesh', io = 'auto')

    mesh = gen_cylinder_mesh(nm.array((0.5, 0.5, 1.0, 2.0, 3)),
                             nm.array((10,10,10)), nm.array((1.0, 2.0, 3.0)),
                             is_open=True, open_angle = 0.5, non_uniform=True,
                             name='')
    mesh.write('7.mesh', io = 'auto')

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = mesh_tools
from sfepy.discrete.fem import FEDomain
import scipy.sparse as sps
import numpy as nm
from sfepy.base.compat import factorial
from sfepy.base.base import output
from numpy.core import intc
from numpy.linalg import lapack_lite

def elems_q2t(el):

    nel, nnd = el.shape
    if nnd > 4:
        q2t = nm.array([[0, 2, 3, 6],
                        [0, 3, 7, 6],
                        [0, 7, 4, 6],
                        [0, 5, 6, 4],
                        [1, 5, 6, 0],
                        [1, 6, 2, 0]])

    else:
        q2t = nm.array([[0, 1, 2],
                        [0, 2, 3]])

    ns, nn = q2t.shape
    nel *= ns

    out = nm.zeros((nel, nn), dtype=nm.int32);

    for ii in range(ns):
        idxs = nm.arange(ii, nel, ns)

        out[idxs,:] = el[:, q2t[ii,:]]

    return nm.ascontiguousarray(out)

def smooth_mesh(mesh, n_iter=4, lam=0.6307, mu=-0.6347,
                weights=None, bconstr=True,
                volume_corr=False):
    """
    FE mesh smoothing.

    Based on:

    [1] Steven K. Boyd, Ralph Muller, Smooth surface meshing for automated
    finite element model generation from 3D image data, Journal of
    Biomechanics, Volume 39, Issue 7, 2006, Pages 1287-1295,
    ISSN 0021-9290, 10.1016/j.jbiomech.2005.03.006.
    (http://www.sciencedirect.com/science/article/pii/S0021929005001442)

    Parameters
    ----------
    mesh : mesh
        FE mesh.
    n_iter : integer, optional
        Number of iteration steps.
    lam : float, optional
        Smoothing factor, see [1].
    mu : float, optional
        Unshrinking factor, see [1].
    weights : array, optional
        Edge weights, see [1].
    bconstr: logical, optional
        Boundary constraints, if True only surface smoothing performed.
    volume_corr: logical, optional
        Correct volume after smoothing process.

    Returns
    -------
    coors : array
        Coordinates of mesh nodes.
    """

    def laplacian(coors, weights):

        n_nod = coors.shape[0]
        displ = (weights - sps.identity(n_nod)) * coors

        return displ

    def taubin(coors0, weights, lam, mu, n_iter):

        coors = coors0.copy()

        for ii in range(n_iter):
            displ = laplacian(coors, weights)
            if nm.mod(ii, 2) == 0:
                coors += lam * displ
            else:
                coors += mu * displ

        return coors

    def get_volume(el, nd):
        from sfepy.linalg.utils import dets_fast

        dim = nd.shape[1]
        nnd = el.shape[1]

        etype = '%d_%d' % (dim, nnd)
        if etype == '2_4' or etype == '3_8':
            el = elems_q2t(el)

        nel = el.shape[0]

        #bc = nm.zeros((dim, ), dtype=nm.double)
        mul = 1.0 / factorial(dim)
        if dim == 3:
            mul *= -1.0

        mtx = nm.ones((nel, dim + 1, dim + 1), dtype=nm.double)
        mtx[:,:,:-1] = nd[el,:]
        vols = mul * dets_fast(mtx.copy())
        vol = vols.sum()
        bc = nm.dot(vols, mtx.sum(1)[:,:-1] / nnd)

        bc /= vol

        return vol, bc

    import time

    output('smoothing...')
    tt = time.clock()

    if weights is None:
        n_nod = mesh.n_nod
        domain = FEDomain('mesh', mesh)
        cmesh = domain.cmesh

        # initiate all vertices as inner - hierarchy = 2
        node_group = nm.ones((n_nod,), dtype=nm.int16) * 2
        # boundary vertices - set hierarchy = 4
        if bconstr:
            # get "vertices of surface"
            facets = cmesh.get_surface_facets()
            f_verts = cmesh.get_incident(0, facets, cmesh.dim - 1)
            node_group[f_verts] = 4

        # generate costs matrix
        e_verts = cmesh.get_conn(1, 0).indices
        fc1, fc2 = e_verts[0::2], e_verts[1::2]
        idxs = nm.where(node_group[fc2] >= node_group[fc1])
        rows1 = fc1[idxs]
        cols1 = fc2[idxs]
        idxs = nm.where(node_group[fc1] >= node_group[fc2])
        rows2 = fc2[idxs]
        cols2 = fc1[idxs]
        crows = nm.concatenate((rows1, rows2))
        ccols = nm.concatenate((cols1, cols2))
        costs = sps.coo_matrix((nm.ones_like(crows), (crows, ccols)),
                               shape=(n_nod, n_nod),
                               dtype=nm.double)

        # generate weights matrix
        idxs = range(n_nod)
        aux = sps.coo_matrix((1.0 / nm.asarray(costs.sum(1)).squeeze(),
                              (idxs, idxs)),
                             shape=(n_nod, n_nod),
                             dtype=nm.double)

        #aux.setdiag(1.0 / costs.sum(1))
        weights = (aux.tocsc() * costs.tocsc()).tocsr()

    coors = taubin(mesh.coors, weights, lam, mu, n_iter)

    output('...done in %.2f s' % (time.clock() - tt))

    if volume_corr:
        output('rescaling...')
        volume0, bc = get_volume(mesh.conns[0], mesh.coors)
        volume, _ = get_volume(mesh.conns[0], coors)

        scale = volume0 / volume
        output('scale factor: %.2f' % scale)

        coors = (coors - bc) * scale + bc

        output('...done in %.2f s' % (time.clock() - tt))

    return coors

########NEW FILE########
__FILENAME__ = splinebox
import numpy as nm
from sfepy.base.base import Struct

class SplineBox(Struct):
    """
    B-spline geometry parametrization. Geometry can be modified
    by moving spline control points.
    """

    @staticmethod
    def mmax(x, y):
        n = len(x)
        aux = nm.zeros((2,n), dtype=nm.int)
        aux[0,:] = x[:]
        aux[1,:] = nm.ones((1,n)) * y

        out = nm.max(aux, axis=0)
        return out

    @staticmethod
    def spsorted(meshsites, sites):
        n1 = len(meshsites)
        n2 = len(sites)
        aux = nm.zeros((n1 + n2,), dtype=nm.double)
        aux[:n1] = meshsites
        aux[n1:] = sites

        inx = nm.argsort(aux)
        out = nm.where(inx >= len(meshsites)) - nm.arange(len(sites))

        return out[0]

    @staticmethod
    def augknt(knots, k, mults=1):

        if mults > 1:
            aux = []
            for j in k[1:-1]:
                aux += [j] * mults;

        else:
            aux = knots[1:-1]

        augknot = [knots[0]] * k + list(aux) + [knots[-1]] * k

        return augknot

    @staticmethod
    def spcol(knots, k, tau):
        npk = len(knots)
        n = npk - k
        nrows = tau.shape[0]
        pts = tau
        km1 = k - 1

        savl = SplineBox.mmax(SplineBox.spsorted(knots[:n], pts), k)
        b = nm.zeros((nrows, k), dtype=nm.double)
        b[:,0] = nm.ones((nrows,), dtype=nm.double)

        for j in range(km1):
            saved = nm.zeros((nrows,), dtype=nm.double);
            for r in range(j+1):
                tr = knots[savl + r] - pts
                tl = pts - knots[savl + r - j - 1]
                term = nm.double(b[:,r]) / (tr + tl)
                b[:,r] = saved + tr * term
                saved = tl * term
            b[:,j+1] = saved

        idx = nm.where((tau < knots[0]) | (tau > knots[npk-1]))[0]

        if len(idx) > 0:
            b[idx,:] = 0

        idx1 = nm.tile(nm.arange(1-nrows, 1), (k,))
        idx2 = nm.tile(nrows * savl, (k,))
        idx3 = nm.tile(nrows * nm.arange(-km1, 1), (nrows, 1))
        idx3 = nm.reshape(idx3, (nrows * k,), order='F')

        width = n + km1 + km1
        nn = nrows * width
        cc = nm.zeros((nn,), dtype=nm.double)
        idx = idx1 + idx2 + idx3 - 1
        cc[idx] = b.reshape((len(idx),), order='F')

        idx1 = nm.tile(nm.arange(1-nrows, 1), (n,))
        idx2 = nm.tile(nrows * nm.arange(1, n + 1), (nrows, 1))
        idx2 = nm.reshape(idx2, (nrows * n,), order='F')
        idx = idx1 + idx2 - 1
        colloc = cc[idx].reshape((nrows, n), order='F')

        return colloc

    @staticmethod
    def create_spb(bbox, coors, nsg=None):
        if type(bbox) is not nm.ndarray:
            bbox = nm.array(bbox)

        if type(coors) is not nm.ndarray:
            coors = nm.array(coors)

        dim = coors.shape[1]
        axes =  []

        if nsg is None:
            nsg = nm.ones((dim,), dtype=nm.int)
        else:
            nsg = nm.array(nsg)

        cpt = []
        ncpt = []
        for idim in range(dim):
            axes.append({})
            aux = nm.linspace(bbox[idim,0], bbox[idim,1], nsg[idim] + 1)
            knots = nm.array(SplineBox.augknt(aux, 4))
            axes[idim]['knots'] = knots

            nn = 4 + nsg[idim] - 1
            ncpt.append(nn)
            cpt.append(nm.zeros((nn,), dtype=nm.double))
            for j in range(nn):
                cpt[idim][j] = nm.sum(knots[(j+1):(j+4)]) / 3.0

            inx = nm.argsort(coors[:,idim])
            aux = SplineBox.spcol(knots, 4, coors[inx,idim])
            axes[idim]['bsc'] = nm.zeros_like(aux)
            axes[idim]['bsc'][inx,:] = aux[:,:]

        ncpts = nm.prod(ncpt)
        cpoints = nm.zeros((ncpts, dim), dtype=nm.double)
        cpoints_idx = nm.zeros(ncpt, dtype=nm.int)
        n2 = nm.prod(ncpt[:2])
        aux = nm.arange(n2).reshape(ncpt[:2], order='F')
        if dim == 2:
            idx = nm.mgrid[0:ncpt[1],0:ncpt[0]]
            cpoints[:,0] = cpt[0][idx[1].reshape((ncpts,))]
            cpoints[:,1] = cpt[1][idx[0].reshape((ncpts,))]
            cpoints_idx[:,:] = aux
        elif dim == 3:
            idx = nm.mgrid[0:ncpt[2],0:ncpt[1],0:ncpt[0]]
            cpoints[:,0] = cpt[0][idx[2].reshape((ncpts,))]
            cpoints[:,1] = cpt[1][idx[1].reshape((ncpts,))]
            cpoints[:,2] = cpt[2][idx[0].reshape((ncpts,))]
            for j in range(ncpt[2]):
                cpoints_idx[:,:,j] = aux + j * n2

        return axes, cpoints, cpoints_idx

    def __init__(self, bbox, coors,
                 name='spbox', **kwargs):
        """
        Create a SplineBox.

        Parameters
        ----------
        bbox : array
            Mesh bounding box.
        coors : array
            Coordinates of mesh nodes.
        name : str
            Object name.
        """
        Struct.__init__(self, name=name, **kwargs)

        self.axes, self.control_points, self.control_points_idx\
            = self.create_spb(bbox, coors)
        self.dim = self.control_points.shape[1]
        self.coors_shape = (self.axes[0]['bsc'].shape[0], self.dim)
        self.control_points0 = self.control_points.copy()

    def get_control_points(self, init=False):
        """
        Get spline control points coordinates.

        Return
        ------
        cpt_coors : array
            The coordinates of the spline control points.
        init : bool
            If True, return initial state.
        """

        if init:
            return self.control_points0

        else:
            return self.control_points

    def set_control_points(self, cpt_coors, add=False):
        """
        Set spline control points position.

        Parameters
        ----------
        cpt_coors : array
            The coordinates of the spline control points.
        add : bool
            If True, coors += cpt_coors
        """

        if add:
            self.control_points += cpt_coors

        else:
            self.control_points = cpt_coors.copy()

    def change_shape(self, cpoint, val):
        """
        Change shape of spline parametrization.

        Parameters
        ----------
        cpoint : list
            The indices of the spline control point.
        val : array
            Displacement.

        """

        idx = self.control_points_idx[cpoint]
        self.control_points[idx] += val

    def evaluate(self, cp_coors=None):
        """
        Evaluate SplineBox.

        Returns
        -------
        coors : array
            The coordinates corresponding to the actual spline control points
            position.
        cp_coors : array
            If is not None, use as control points cooardinates.
        """

        coors = nm.zeros((self.axes[0]['bsc'].shape[0], self.dim),
                         dtype=nm.double)

        if cp_coors is None:
            cp_coors = self.control_points

        cptidx = self.control_points_idx
        nn = self.control_points_idx.shape
        if self.dim == 2:
            for i in range(nn[0]):
                for j in range(nn[1]):
                    aux = self.axes[0]['bsc'][:,i] * self.axes[1]['bsc'][:,j]
                    inx = cptidx[i,j]
                    coors += nm.dot(aux[:,nm.newaxis],
                                    cp_coors[inx,:][nm.newaxis,:])
        elif self.dim == 3:
            for i in range(nn[0]):
                for j in range(nn[1]):
                    aux = self.axes[0]['bsc'][:,i] * self.axes[1]['bsc'][:,j]
                    for k in range(nn[2]):
                        inx = cptidx[i,j,k]
                        aux2 = aux * self.axes[2]['bsc'][:,k]
                        coors += nm.dot(aux2[:,nm.newaxis],
                                        cp_coors[inx,:][nm.newaxis,:])

        return coors

    def dvelocity(self, cpoint, dir):
        """
        Evaluate derivative of spline in a given control point and direction.

        Parameters
        ----------
        cpoint : list
            The indices of the spline control point.
        dir : array
            The directional vector.

        Returns
        -------
        dvel : array
            The design velocity field.
        """

        if type(dir) is not nm.ndarray:
            dir = nm.array(dir)

        dvel = nm.zeros((self.axes[0]['bsc'].shape[0], self.dim),
                        dtype=nm.double)

        ax = self.axes
        if self.dim == 2:
            aux = ax[0]['bsc'][:,cpoint[0]] * ax[1]['bsc'][:,cpoint[1]]
            dvel = nm.dot(aux[:,nm.newaxis], dir[nm.newaxis,:])
        elif self.dim == 3:
            aux = ax[0]['bsc'][:,cpoint[0]] * ax[1]['bsc'][:,cpoint[1]]\
                * ax[2]['bsc'][:,cpoint[2]]
            dvel = nm.dot(aux[:,nm.newaxis], dir[nm.newaxis,:])

        return dvel

    def write_vtk(self, filename):

        cptidx = self.control_points_idx
        ncpt = cptidx.shape
        nnd = nm.prod(ncpt)

        f = open(filename, 'w')
        f.write("# vtk DataFile Version 2.6\nspbox file\n"
                "ASCII\nDATASET UNSTRUCTURED_GRID\n\n")
        f.write("POINTS %d float\n" % nnd)

        if self.dim == 2:

            nel = (ncpt[0] - 1) * ncpt[1] + (ncpt[1] - 1) * ncpt[0]

            for cpt in self.control_points:
                f.write("%e %e 0.0\n" % (cpt[0], cpt[1]))
            f.write("\nCELLS %d %d\n" % (nel, 3 * nel))

            for i in range(ncpt[0]):
                for j in range(ncpt[1] - 1):
                    inx1 = cptidx[i,j]
                    inx2 = cptidx[i,j + 1]
                    f.write("2 %d %d\n" % (inx1, inx2))
            for i in range(ncpt[0] - 1):
                for j in range(ncpt[1]):
                    inx1 = cptidx[i,j]
                    inx2 = cptidx[i + 1,j]
                    f.write("2 %d %d\n" % (inx1, inx2))

        elif self.dim == 3:

            nel = ((ncpt[0] - 1) * ncpt[1] + (ncpt[1] - 1) * ncpt[0]) * ncpt[2]
            nel += ncpt[0] * ncpt[1] * (ncpt[2] - 1)

            for cpt in self.control_points:
                f.write("%e %e %e\n" % (cpt[0], cpt[1], cpt[2]))
            f.write("\nCELLS %d %d\n" % (nel, 3 * nel))

            for k in range(ncpt[2]):
                for i in range(ncpt[0]):
                    for j in range(ncpt[1] - 1):
                        inx1 = cptidx[i, j, k]
                        inx2 = cptidx[i, j + 1, k]
                        f.write("2 %d %d\n" % (inx1, inx2))
                for i in range(ncpt[0] - 1):
                    for j in range(ncpt[1]):
                        inx1 = cptidx[i, j, k]
                        inx2 = cptidx[i + 1, j, k]
                        f.write("2 %d %d\n" % (inx1, inx2))

            for k in range(ncpt[2] - 1):
                for i in range(ncpt[0]):
                    for j in range(ncpt[1]):
                        inx1 = cptidx[i, j, k]
                        inx2 = cptidx[i, j, k + 1]
                        f.write("2 %d %d\n" % (inx1, inx2))

            f.write("\nCELL_TYPES %d\n" % nel )
            f.write("3\n" * nel)
            f.close()

########NEW FILE########
__FILENAME__ = free_form_def
import tables as pt

import numpy as nm
import numpy.linalg as nla

from sfepy.base.base import assert_, OneTypeList, Struct
from sfepy.linalg import cycle
from sfepy.discrete.fem.mesh import Mesh

##
# 11.01.2006, c
# 20.01.2006
# 23.01.2006
# 24.01.2006
# 12.04.2006
def read_spline_box_hdf5( filename ):
    if not pt.isHDF5File( filename ):
        raise ValueError, 'not a HDF5 file! (%s)' % filename

    fd = pt.openFile( filename, mode = 'r' )
    boxes = fd.listNodes( '/box' )
    n_box = len( boxes )
    dim = len( fd.listNodes( boxes[0].ax ) )

    sp_boxes = SplineBoxes( dim = dim, n_box = n_box, n_vertex = 0,
                           spbs = OneTypeList( SplineBox ) )
    for box in boxes:
        spb = SplineBox()
        sp_boxes.spbs.append( spb )

        spb.ib = int( box._v_name )
        spb.cpi = nm.asarray( box.cpi.read() ) - 1
        spb.gpi = nm.asarray( box.gpi.read() ) - 1
        spb.cxyz = nm.asarray( box.cxyz.read() ).transpose()
        spb.cxyz0 = spb.cxyz.copy()
        spb.ax = []
        for axi in fd.listNodes( box.ax ):
            spb.ax.append( nm.asarray( axi.bsc.read() ) )

        sp_boxes.n_vertex = max( sp_boxes.n_vertex, nm.amax( spb.gpi ) + 1 )
        print nm.amin( spb.gpi ), nm.amax( spb.gpi )

        ##
        # Fix cpi by rebuilding :).
        off = 0
        n0, n1, n2 = spb.cpi.shape
        aux = nm.arange( n0 * n1 ).reshape( n1, n0 ).transpose()
        for ii in xrange( n2 ):
            spb.cpi[:,:,ii] = aux + off
            off += n0 * n1

    fd.close()

    for perm in cycle( [n_box] * 2 ):
        if perm[0] == perm[1]: continue
        gpi1 = sp_boxes.spbs[perm[0]].gpi
        gpi2 = sp_boxes.spbs[perm[1]].gpi
        assert_( len( nm.intersect1d( gpi1, gpi2 ) ) == 0 )
    
    return sp_boxes

##
# 20.01.2006, c
# 17.02.2006
# 12.04.2006
# 13.04.2006
def read_dsg_vars_hdf5( filename ):
    if not pt.isHDF5File( filename ):
        raise ValueError, 'not a HDF5 file! (%s)' % filename

    fd = pt.openFile( filename, mode = 'r' )
    aux1 = fd.getNode( '/dsgvar/inx' ).read()
    aux2 = fd.getNode( '/dsgvar/val' ).read()
    aux3 = fd.getNode( '/dsgvar/nsbdsg' ).read()
    dsg_vars = DesignVariables( indx = nm.asarray( aux1, dtype = nm.int32 ),
                               cxyz =  nm.asarray( aux2 ),
                               null_space_b = nm.asarray( aux3 ) )
    dsg_vars.indx = dsg_vars.indx.transpose()
    dsg_vars.indx[:,1] -= 1

    # No. of design variables.
    dsg_vars.n_dsg = dsg_vars.null_space_b.shape[1]
    # No. of control points.
    dsg_vars.n_cp = dsg_vars.indx.shape[0]
    # Design vector and initial design vector. 
    dsg_vars.val0 = nm.zeros( (dsg_vars.n_dsg,), dtype = nm.float64 )
    dsg_vars.val = dsg_vars.val0.copy()
    
    fd.close()

    return dsg_vars

##
# 12.04.2006, c
def interp_box_coordinates( spb, cxyz = None ):
    if cxyz is None:
        cxyz = spb.cxyz
        
    dim = len( spb.ax )
    pp = nm.zeros( (spb.ax[0].shape[0], dim), dtype = nm.float64 )
    for ii in xrange( spb.cpi.shape[0] ):
        for ij in xrange( spb.cpi.shape[1] ):
            aux = spb.ax[0][:,ii] * spb.ax[1][:,ij]
            for ik in xrange( spb.cpi.shape[2] ):
                aux2 = aux * spb.ax[2][:,ik]
                ip = spb.cpi[ii,ij,ik]
                pp += aux2[:,nm.newaxis] * cxyz[ip,:]
##                         print ii, ij, ik, ip, cxyz[:,ip]
##                         pause()
    return pp

class DesignVariables( Struct ):
    ##
    # 20.01.2006, c
    def renumber_by_boxes( self, sp_boxes ):
        bmap = nm.array( [[ii, spb.ib + 1]
                          for ii, spb in enumerate( sp_boxes.spbs )],
                         dtype = nm.int32 )
        bpos = nm.zeros( (nm.amax( bmap[:,1] ) + 1,), dtype = nm.int32 )
        bpos[bmap[:,1]] = bmap[:,0]
        print bmap, bpos
        self.indx[:,0] = bpos[self.indx[:,0]]

    ##
    # 24.04.2006, c
    def normalize_null_space_base( self, magnitude = 1.0 ):
        for ic in xrange( self.n_dsg ):
            col_norm = nla.norm( self.null_space_b[:,ic] )
            self.null_space_b[:,ic] /= magnitude * col_norm

class SplineBox( Struct ):
    pass

class SplineBoxes( Struct ):
    ##
    # 11.01.2006, c
    # 20.01.2006
    # 12.04.2006
    def interp_mesh_velocity( self, shape, dsg_vars, idsg ):
        n_nod, dim = shape
        cv = dsg_vars.null_space_b[:,idsg]
        cv = nm.reshape( cv, (dsg_vars.n_cp, dim) )

        vel = nm.zeros( shape = shape, dtype = nm.float64 )

        for ib, spb in enumerate( self.spbs ):
            ii = nm.where( ib == dsg_vars.indx[:,0] )[0]
#            print ib, ii
            if ii.shape[0] > 0:
                delta = nm.zeros_like( spb.cxyz )
                delta[dsg_vars.indx[ii,1],:] = cv[ii,:]

                vv = interp_box_coordinates( spb, delta )
##                 print delta
##                 print vv
##                 pause()
                vel[spb.gpi,:] = vv
        return vel

    ##
    # 23.01.2006, c
    # 24.01.2006
    # 12.04.2006
    def interp_coordinates( self ):
        coors = nm.zeros( (self.n_vertex, self.dim), nm.float64 )
        # Body using the (design-deformed) Greville abscisae spb.cxyz.
        for spb in self.spbs:
            coors[spb.gpi] = interp_box_coordinates( spb )

        return coors

    ##
    # 13.04.2006, c
    # 19.04.2006
    def set_control_points( self, dsg_vars = None ):
        if dsg_vars is None:
            # Restore Greville abscisae spb.cxyz0.
            for spb in self.spbs:
                spb.cxyz = spb.cxyz0.copy()
        else:
            # Set design-deformed Greville abscisae spb.cxyz.
            delta = nm.dot( dsg_vars.null_space_b, dsg_vars.val )
            delta = nm.reshape( delta, (dsg_vars.n_cp, self.dim) )
            for ib, spb in enumerate( self.spbs ):
                spb.cxyz = spb.cxyz0.copy()

                ii = nm.where( ib == dsg_vars.indx[:,0] )[0]
                if ii.shape[0] > 0:
                    ir = dsg_vars.indx[ii,1]
                    spb.cxyz[ir,:] = spb.cxyz0[ir,:] + delta[ii,:]

    ##
    # 27.03.2007, c
    def create_mesh_from_control_points( self ):
        offset = 0
        dim = self.spbs[0].cxyz.shape[1]
        coors = nm.empty((0, dim), dtype=nm.float64)
        conns = []
        mat_ids = []
        descs = []
        for ib, spb in enumerate( self.spbs ):
            n_nod = spb.cxyz.shape[0]
            coors = nm.concatenate( (coors, spb.cxyz), 0 )
            descs.append( '3_2' )

            conn = []
            for ij in xrange( spb.cpi.shape[1] ):
                for ik in xrange( spb.cpi.shape[2] ):
                    inx = spb.cpi[:,ij,ik]
                    row = [[p1, p2] for p1, p2 in zip( inx[:-1], inx[1:] )]
                    conn.extend( row )
            for ij in xrange( spb.cpi.shape[0] ):
                for ik in xrange( spb.cpi.shape[2] ):
                    inx = spb.cpi[ij,:,ik]
                    row = [[p1, p2] for p1, p2 in zip( inx[:-1], inx[1:] )]
                    conn.extend( row )
            for ij in xrange( spb.cpi.shape[0] ):
                for ik in xrange( spb.cpi.shape[1] ):
                    inx = spb.cpi[ij,ik,:]
                    row = [[p1, p2] for p1, p2 in zip( inx[:-1], inx[1:] )]
                    conn.extend( row )

            aux = nm.empty(len(conn), dtype=nm.int32)
            aux.fill(ib)
            mat_ids.append(aux)

            conns.append( offset + nm.array( conn, dtype = nm.int32 ) )
            offset += n_nod

        mesh = Mesh.from_data('control_points', coors, None, conns,
                              mat_ids, descs)
        return mesh

if __name__ == '__main__':
    filename = 'klik.h5'
    spboxes = read_spline_box_hdf5( filename )
    dsg_vars = read_dsg_vars_hdf5( filename )

########NEW FILE########
__FILENAME__ = shape_optim
import os.path as op

import numpy as nm

from sfepy.base.base import output, assert_, remap_dict, pause, Struct
from sfepy.discrete.equations import get_expression_arg_names
from sfepy.discrete.evaluate import eval_equations
import free_form_def as ffd

##
# c: 15.10.2007, r: 15.04.2008
def update_mesh( shape_opt, pb, design ):
    output( 'mesh update (%d)!' % shape_opt.cache.i_mesh )
    ##
    # Update mesh.
    shape_opt.dsg_vars.val = design
    shape_opt.sp_boxes.set_control_points( shape_opt.dsg_vars )
    coors = shape_opt.sp_boxes.interp_coordinates()

    pb.set_mesh_coors(coors, update_fields=True)

    pb.domain.mesh.write( op.join( shape_opt.save_dir, 'design.%03d.mesh' )\
                          % shape_opt.cache.i_mesh, io = 'auto' )
    shape_opt.cache.i_mesh += 1

##
# c: 19.04.2006, r: 15.04.2008
def solve_problem_for_design(problem, design, shape_opt, opts,
                             var_data=None,
                             use_cache=True, is_mesh_update=True):
    """use_cache == True means direct problem..."""
    pb = problem

    if use_cache and nm.allclose( design, shape_opt.cache.design,
                                 rtol = opts.eps_r, atol = opts.eps_a ):
        output( 'cache!' )
        state = shape_opt.cache.state
    else:
        if is_mesh_update:
            update_mesh( shape_opt, pb, design )

        # Solve direct/adjoint problem.
        try:
            state = problem.solve(var_data=var_data)

        except ValueError:
            output('failed!')
            return None

        if use_cache:
            shape_opt.cache.state = state.copy(deep=True)
            shape_opt.cache.design = design.copy()

            if shape_opt.save_iter_sols:
                pb.save_state( op.join( shape_opt.save_dir, 'direct.%03d.vtk' )\
                              % (shape_opt.cache.i_mesh - 1), state )

            if shape_opt.save_control_points:
                aux = shape_opt.sp_boxes.create_mesh_from_control_points()
                aux.write( op.join( shape_opt.save_dir, 'cp.%03d.vtk' )\
                           % (shape_opt.cache.i_mesh - 1), io = 'auto' )

            if shape_opt.save_dsg_vars:
                filename = op.join( shape_opt.save_dir, 'dsg.%03d.txt' )\
                           % (shape_opt.cache.i_mesh - 1)
                shape_opt.dsg_vars.val.tofile( filename, ' ' )

    return state

def obj_fun(design, shape_opt, opts):
    """
    The objective function evaluation.
    """
    state_dp = solve_problem_for_design(shape_opt.dpb, design, shape_opt, opts)
    if state_dp is None:
        return None

    val = shape_opt.obj_fun(state_dp)

    return val

def obj_fun_grad(design, shape_opt, opts):
    """
    The objective function gradient evaluation.
    """
    state_dp = solve_problem_for_design(shape_opt.dpb, design, shape_opt, opts)

    var_data = state_dp.get_parts()
    var_data = remap_dict(var_data, shape_opt.var_map)

    state_ap = solve_problem_for_design(shape_opt.apb, design, shape_opt, opts,
                                        var_data=var_data,
                                        use_cache=False, is_mesh_update=False)

    vec_sa = shape_opt.sensitivity(var_data, state_ap)

    return vec_sa

def test_terms(idsgs, delta, shape_opt, dp_var_data, state_ap):
    """
    Test individual shape derivative terms.
    """

    def ccs(*args):
        try:
            shape_opt.check_custom_sensitivity(*args)

        except:
            output('running test failed!')

    ccs('d_sd_st_grad_div.i2.Omega_D( stabil.gamma, w, u, Nu )',
        idsgs, delta, dp_var_data, state_ap)

    ccs('d_sd_st_supg_c.i1.Omega_D( stabil.delta, w, u, w, Nu )',
        idsgs, delta, dp_var_data, state_ap)
    ccs('d_sd_st_supg_c.i1.Omega_D( stabil.delta, u, w, w, Nu )',
        idsgs, delta, dp_var_data, state_ap)

    ccs('d_sd_st_pspg_c.i1.Omega_D( stabil.tau, w, u, r, Nu )',
        idsgs, delta, dp_var_data, state_ap)

    ccs('d_sd_st_pspg_p.i1.Omega_D( stabil.tau, p, r, Nu )',
        idsgs, delta, dp_var_data, state_ap)
    ccs('d_sd_st_pspg_p.i1.Omega_D( stabil.tau, r, r, Nu )',
        idsgs, delta, dp_var_data, state_ap)

    ccs('d_sd_volume_dot.i1.Omega_D( p, r, Nu )',
        idsgs, delta, dp_var_data, state_ap)
    ccs('d_sd_volume_dot.i1.Omega_D( u, w, Nu )',
        idsgs, delta, dp_var_data, state_ap)

    ccs('d_sd_div.i1.Omega_D( u, r, Nu )',
        idsgs, delta, dp_var_data, state_ap)
    ccs('d_sd_div.i1.Omega_D( w, p, Nu )',
        idsgs, delta, dp_var_data, state_ap)

    ccs('d_sd_div_grad.i2.Omega_D( one.val, fluid.viscosity, u, w, Nu )',
        idsgs, delta, dp_var_data, state_ap)

    ccs('d_sd_convect.i2.Omega_D( u, w, Nu )',
        idsgs, delta, dp_var_data, state_ap)

##
# 25.01.2006, c
class ShapeOptimFlowCase( Struct ):

    def from_conf(conf, dpb, apb):

        opts = conf.options
        regions = dpb.domain.regions
        if opts.use_mesh_velocity:
            import tables as pt
            fd = pt.open_file( opts.mesh_velocity_filename, mode = 'r' )
            aux = fd.get_node( '/u' ).read()
            nu = nm.asarray( aux, dtype = nm.float64 )
            fd.close()

        else:
            nu = None

        sp_boxes = ffd.read_spline_box_hdf5( opts.ffd_spline_data )
        dsg_vars = ffd.read_dsg_vars_hdf5( opts.ffd_spline_data )
        dsg_vars.renumber_by_boxes( sp_boxes )
        dsg_vars.normalize_null_space_base()
        print dsg_vars.indx.shape
        print dsg_vars.null_space_b.shape

        control_region = regions[opts.control_domain]
        design_region = regions[opts.design_domain]

        from sfepy.discrete.fem.mesh import Mesh
        cmm = Mesh.from_region(control_region, dpb.domain.mesh)
        dmm = Mesh.from_region(design_region, dpb.domain.mesh)
        cmm.write( 'control.mesh', io = 'auto' )
        dmm.write( 'design.mesh', io = 'auto' )

        SOFC = ShapeOptimFlowCase
        obj = SOFC(dpb=dpb, apb=apb,
                   sp_boxes=sp_boxes,
                   dsg_vars=dsg_vars,
                   problem_type=opts.problem,
                   objective_function_type=opts.objective_function,
                   var_map=opts.var_map,
                   nu=nu,
                   use_mesh_velocity=opts.use_mesh_velocity,
                   save_dir=opts.save_dir,
                    save_iter_sols=opts.save_iter_sols,
                   save_control_points=opts.save_control_points,
                   save_dsg_vars=opts.save_dsg_vars,
                   test_terms_if_test=opts.test_terms_if_test)

        equations = getattr( conf, '_'.join( ('equations_sensitivity',
                                              opts.problem,
                                              opts.objective_function) ) )

        obj.obj_fun_term = equations['objective']
        obj.sens_terms = equations['sensitivity']

        obj.n_var = dsg_vars.val.shape[0]

        obj.create_evaluables()

        return obj
    from_conf = staticmethod( from_conf )

    def create_evaluables(self):
        variables = self.dpb.create_variables().as_dict()

        possible_mat_names = get_expression_arg_names(self.obj_fun_term)
        materials = self.dpb.create_materials(possible_mat_names).as_dict()

        aux = self.dpb.create_evaluable(self.obj_fun_term,
                                        try_equations=False,
                                        var_dict=variables,
                                        **materials)
        self.of_equations, self.of_variables = aux

        possible_mat_names = get_expression_arg_names(self.sens_terms)
        materials = self.apb.create_materials(possible_mat_names).as_dict()

        aux = self.apb.create_evaluable(self.sens_terms,
                                        try_equations=False,
                                        var_dict=variables,
                                        **materials)
        self.ofg_equations, self.ofg_variables = aux

    ##
    # 07.03.2006, c
    # 12.04.2006
    def generate_mesh_velocity( self, shape, idsgs = None ):
        if self.use_mesh_velocity:
            assert_( shape == self.nu.shape )
            yield self.nu

        else:
            for idsg in idsgs:
#                print 'idsg:', idsg
                yield self.sp_boxes.interp_mesh_velocity( shape, self.dsg_vars,
                                                       idsg )

    def obj_fun(self, state_dp):
        """
        Objective function evaluation for given direct problem state.
        """
        var_data = state_dp.get_parts()
        var_data = remap_dict(var_data, self.var_map)

        self.of_equations.set_data(var_data, ignore_unknown=True)

        val = eval_equations(self.of_equations, self.of_variables)

        return nm.squeeze( val )

    def sensitivity(self, dp_var_data, state_ap, select=None):
        """
        Sensitivity of objective function evaluation for given direct
        and adjoint problem states.
        """
        apb = self.apb

        var_data = state_ap.get_parts()
        var_data.update(dp_var_data)

        self.ofg_equations.set_data(var_data, ignore_unknown=True)

        dim = self.sp_boxes.dim
        n_mesh_nod = apb.domain.shape.n_nod

        if select is None:
            idsgs = nm.arange( self.dsg_vars.n_dsg, dtype = nm.int32 )
        else:
            idsgs = select

        sa = []

        output('computing sensitivity of %d variables...' % idsgs)

        shape = (n_mesh_nod, dim)
        for ii, nu in enumerate(self.generate_mesh_velocity(shape, idsgs)):
            self.ofg_variables['Nu'].set_data(nu.ravel())

            ## from sfepy.base.ioutils import write_vtk
            ## cc = nla.norm( vec_nu )
            ## nun = nu / cc
            ## out = {'v' : Struct( mode = 'vertex', data = nun,
            ##                      ap_name = 'nic', dof_types = (0,1,2) )}
            ## fd = open( 'anim/pert_%03d.pvtk' % (ii+1), 'w' )
            ## write_vtk( fd, domain.mesh, out )
            ## fd.close()
            ## print ii

            val = eval_equations(self.ofg_equations, self.ofg_variables,
                                 term_mode=1, preserve_caches=True)

            sa.append( val )
        output('...done')

        vec_sa = nm.array( sa, nm.float64 )
        return vec_sa

    def check_custom_sensitivity(self, term_desc, idsg, delta,
                                 dp_var_data, state_ap):
        pb = self.apb

        domain = pb.domain

        possible_mat_names = get_expression_arg_names(term_desc)
        materials = self.dpb.create_materials(possible_mat_names).as_dict()

        variables = self.ofg_equations.variables
        aux = self.dpb.create_evaluable(term_desc,
                                        try_equations=False,
                                        var_dict=variables,
                                        verbose=False,
                                        **materials)
        check0_equations, check0_variables = aux

        aux = self.dpb.create_evaluable(term_desc,
                                        try_equations=False,
                                        var_dict=variables,
                                        verbose=False,
                                        **materials)
        check1_equations, check1_variables = aux

        var_data = state_ap.get_parts()
        var_data.update(dp_var_data)

        check0_equations.set_data(var_data, ignore_unknown=True)
        check1_equations.set_data(var_data, ignore_unknown=True)

        dim = self.sp_boxes.dim
        n_mesh_nod = domain.shape.n_nod

        a_grad = []
        d_grad = []

        coors0 = domain.mesh.coors

        for nu in self.generate_mesh_velocity( (n_mesh_nod, dim), [idsg] ):
            check1_variables['Nu'].set_data(nu.ravel())

            aux = eval_equations(check1_equations, check1_variables,
                                 term_mode=1)
            a_grad.append( aux )

            coorsp = coors0 + delta * nu
            pb.set_mesh_coors( coorsp, update_fields=True )
            valp = eval_equations(check0_equations, check0_variables,
                                  term_mode=0)

            coorsm = coors0 - delta * nu
            pb.set_mesh_coors( coorsm, update_fields=True )
            valm = eval_equations(check0_equations, check0_variables,
                                  term_mode=0)

            d_grad.append( 0.5 * (valp - valm) / delta )

        pb.set_mesh_coors( coors0, update_fields=True )

        a_grad = nm.array( a_grad, nm.float64 )
        d_grad = nm.array( d_grad, nm.float64 )

        output( term_desc + ':' )
        output( '       a: %.8e' % a_grad )
        output( '       d: %.8e' % d_grad )
        output( '-> ratio:', a_grad / d_grad )
        pause()

    def check_sensitivity(self, idsgs, delta, dp_var_data, state_ap):
        a_grad = self.sensitivity(dp_var_data, state_ap, select=idsgs)
        output( a_grad )

        d_grad = []

        dpb = self.dpb
        dim = self.sp_boxes.dim
        n_mesh_nod = dpb.domain.shape.n_nod

        coors0 = dpb.get_mesh_coors().copy()
        for nu in self.generate_mesh_velocity( (n_mesh_nod, dim), idsgs ):

            coorsp = coors0 + delta * nu
            dpb.set_mesh_coors( coorsp, update_fields=True )

            dpb.time_update()
            state_dp = dpb.solve()

            valp = self.obj_fun(state_dp)
            output( 'obj_fun+:', valp )

            coorsm = coors0 - delta * nu
            dpb.set_mesh_coors( coorsm, update_fields=True )

            dpb.time_update()
            state_dp = dpb.solve()

            valm = self.obj_fun(state_dp)
            output( 'obj_fun-:', valm )

            d_grad.append( 0.5 * (valp - valm) / delta )

        ##
        # Restore original mesh coordinates.
        dpb.set_mesh_coors( coors0, update_fields=True )

        d_grad = nm.array( d_grad, nm.float64 )

        output( 'a: %.8e' % a_grad )
        output( 'd: %.8e' % d_grad )
        output( a_grad / d_grad )
        pause()

########NEW FILE########
__FILENAME__ = energy
import numpy as nm
import numpy.linalg as nla

from sfepy.discrete.fem.mappings import get_physical_qps

def eval_ion_ion_energy(centres, charges):
    """
    Compute the ion-ion nergy.
    """
    val = 0.0
    for ir, rcentre in enumerate(centres):
        for ic, ccentre in enumerate(centres):
            if ir == ic: continue

            val += charges[ir] * charges[ic] / nla.norm(rcentre - ccentre)

    val *= 0.5

    return val

def eval_non_local_interaction(problem, region_name, var_name,
                               integral_name, f1, f2, kernel_function):
    """
    Single element group only!
    """
    var = problem.get_variables()[var_name]

    region = problem.domain.regions[region_name]

    integral = problem.integrals[integral_name]

    qps = get_physical_qps(region, integral)
    igs = qps.values.keys()

    ig = igs[0]
    qp = qps.values[ig]

    n_el, n_qp = f1.shape[:2]

    vg, _ = var.get_mapping(ig, region, integral, 'volume')

    # Weighted jacobian.
    det = vg.det

    shape = (n_el * n_qp, 1, 1)

    val1 = f1 * det
    val2 = f2 * det

    val1.shape = shape
    val2.shape = shape

    coef = nm.zeros(shape, dtype=val1.dtype)

    for ii, coor in enumerate(qp):
        ## tt = time.clock()
        kernel = kernel_function(coor, qp)
        kernel.shape = val2.shape
        ## print'aa',   time.clock() - tt

        ## tt = time.clock()
        coef[ii] = (kernel * val2).sum()
        ## print 'bb', time.clock() - tt

    val = (val1 * coef).sum()

    return val

########NEW FILE########
__FILENAME__ = potentials
"""
Classes for constructing potentials of atoms and molecules.
"""
import numpy as nm

from sfepy.base.base import as_float_or_complex, Container, Struct
from sfepy.linalg import norm_l2_along_axis

class CompoundPotential(Container):
    """
    Sum of several potentials.
    """

    def __init__(self, objs=None):
        Container.__init__(self, objs=objs)

        self.update_expression()

    def insert(self, ii, obj):
        Container.insert(self, ii, obj)
        self.update_expression()

    def append(self, obj):
        Container.append(self, obj)
        self.update_expression()

    def update_expression(self):
        self.expression = []
        for pot in self:
            aux = [pot.sign, pot.name, pot.centre]
            self.expression.append(aux)

    def __mul__(self, other):
        out = CompoundPotential()
        for name, pot in self.iteritems():
            out.append(pot * other)

        return out

    def __rmul__(self, other):
        return self * other

    def __add__(self, other):
        if isinstance(other, PotentialBase):
            out = self.copy()
            out.append(other)

        elif isinstance(other, CompoundPotential):
            out = CompoundPotential(self._objs + other._objs)

        else:
            raise ValueError('cannot add CompoundPotential with %s!' % other)

        return out

    def __radd__(self, other):
        return self + other

    def __sub__(self, other):
        if isinstance(other, PotentialBase):
            out = self + (-other)

        elif isinstance(other, CompoundPotential):
            out = self + (-other)

        else:
            raise ValueError('cannot subtract CompoundPotential with %s!' \
                             % other)

        return out

    def __rsub__(self, other):
        return -self + other

    def __pos__(self):
        return self

    def __neg__(self):
        return -1.0 * self

    def __call__(self, coors):
        val = 0.0
        for pot in self:
            val += pot(coors)

        return val

class PotentialBase(Struct):
    """
    Base class for potentials.
    """

    def __mul__(self, other):
        try:
            mul = as_float_or_complex(other)

        except ValueError:
            raise ValueError('cannot multiply PotentialBase with %s!' % other)

        out = self.copy(name=self.name)

        out.sign = mul * self.sign

        return out

    def __rmul__(self, other):
        return self * other

    def __add__(self, other):
        if isinstance(other, PotentialBase):
            out = CompoundPotential([self, other])

        elif nm.isscalar(other):
            if other == 0:
                out = self

            else:
                out = NotImplemented

        else:
            out = NotImplemented

        return out

    def __radd__(self, other):
        return self + other

    def __sub__(self, other):
        if isinstance(other, PotentialBase):
            out = CompoundPotential([self, -1.0 * other])

        elif nm.isscalar(other):
            if other == 0:
                out = self

            else:
                out = NotImplemented

        else:
            out = NotImplemented

        return out

    def __rsub__(self, other):
        return -self + other

    def __pos__(self):
        return self

    def __neg__(self):
        out = -1.0 * self
        return out

class Potential(PotentialBase):
    """
    Single spherically symmetric potential.
    """

    def __init__(self, name, function, centre=None, dim=3, args=None):
        self.name = name
        self.function = function
        self.args = args if args is not None else ()

        if centre is None:
            centre = nm.array([0.0] * dim, dtype=nm.float64)

        self.centre = nm.asarray(centre, dtype=nm.float64)

        self.sign = 1.0

    def __call__(self, coors):
        r = self.get_distance(coors)

        pot = self.sign * self.function(r, *self.args)

        return pot

    def __iter__( self ):
        """
        Allow iteration even over a single potential.
        """
        yield self

    def __len__(self):
        """
        Allow length even of a single potential.
        """
        return 1

    def get_distance(self, coors):
        """
        Get the distance of points with coordinates `coors` of the
        potential centre.
        """
        return norm_l2_along_axis(coors - self.centre)

    def get_charge(self, coors, eps=1e-6):
        """
        Get charge corresponding to the potential by numerically
        applying Laplacian in spherical coordinates.
        """
        r = self.get_distance(coors)

        args = self.args

        f0 = self.function(r, *args)
        fp1 = self.function(r + eps, *args)
        fp2 = self.function(r + 2.0 * eps, *args)
        fm1 = self.function(r - eps, *args)
        fm2 = self.function(r - 2.0 * eps, *args)

        # Second derivative w.r.t. r.
        d2 = (fp2 - 2.0 * f0 + fm2) / (4.0 * eps * eps)
        # First derivative w.r.t. r.
        d1 = (fp1 - fm1) / (2.0 * eps)

        charge = - self.sign / (4.0 * nm.pi) * (d2 + 2.0 * d1 / r)

        return charge

########NEW FILE########
__FILENAME__ = schroedinger_app
import os
import time
from math import pi

import numpy as nm

from sfepy.base.base import Struct, output, get_default
from sfepy.applications import PDESolverApp
from sfepy.solvers import Solver

def guess_n_eigs(n_electron, n_eigs=None):
    """
    Guess the number of eigenvalues (energies) to compute so that the smearing
    iteration converges. Passing n_eigs overrides the guess.
    """
    if n_eigs is not None: return n_eigs

    if n_electron > 2:
        n_eigs = int(1.2 * ((0.5 * n_electron) + 5))
    else:
        n_eigs = n_electron
    return n_eigs

class SchroedingerApp(PDESolverApp):
    """
    Base application for electronic structure calculations.

    Subclasses should typically override `solve_eigen_problem()` method.

    This class allows solving only simple single electron problems,
    e.g. well, oscillator, hydrogen atom and boron atom with 1 electron.
    """

    @staticmethod
    def process_options(options):
        """
        Application options setup. Sets default values for missing
        non-compulsory options.

        Options:

        save_eig_vectors : (from_largest, from_smallest) or None
            If None, save all.
        """
        get = options.get

        n_electron = get('n_electron', 5)
        n_eigs = guess_n_eigs(n_electron, n_eigs=get('n_eigs', None))

        return Struct(eigen_solver=get('eigen_solver', None,
                                       'missing "eigen_solver" in options!'),
                      n_electron=n_electron,
                      n_eigs=n_eigs,
                      save_eig_vectors=get('save_eig_vectors', None))

    def __init__(self, conf, options, output_prefix, **kwargs):
        PDESolverApp.__init__(self, conf, options, output_prefix,
                              init_equations=False)

    def setup_options(self):
        PDESolverApp.setup_options(self)
        opts = SchroedingerApp.process_options(self.conf.options)

        self.app_options += opts

    def setup_output(self):
        """
        Setup various file names for the output directory given by
        `self.problem.output_dir`.
        """
        output_dir = self.problem.output_dir

        opts = self.app_options
        opts.output_dir = output_dir
        self.mesh_results_name = os.path.join(opts.output_dir,
                                              self.problem.get_output_name())
        self.eig_results_name = os.path.join(opts.output_dir,
                                             self.problem.ofn_trunk
                                             + '_eigs.txt')

    def call(self):
        # This cannot be in __init__(), as parametric calls may change
        # the output directory.
        self.setup_output()

        evp = self.solve_eigen_problem()

        output("solution saved to %s" % self.problem.get_output_name())
        output("in %s" % self.app_options.output_dir)

        if self.post_process_hook_final is not None: # User postprocessing.
            self.post_process_hook_final(self.problem, evp=evp)

        return evp

    def solve_eigen_problem(self):
        options = self.options
        opts = self.app_options
        pb = self.problem

        dim = pb.domain.mesh.dim

        pb.set_equations(pb.conf.equations)
        pb.time_update()

        output('assembling lhs...')
        tt = time.clock()
        mtx_a = pb.evaluate(pb.conf.equations['lhs'], mode='weak',
                            auto_init=True, dw_mode='matrix')
        output('...done in %.2f s' % (time.clock() - tt))

        output('assembling rhs...')
        tt = time.clock()
        mtx_b = pb.evaluate(pb.conf.equations['rhs'], mode='weak',
                            dw_mode='matrix')
        output('...done in %.2f s' % (time.clock() - tt))

        n_eigs = get_default(opts.n_eigs, mtx_a.shape[0])

        output('computing resonance frequencies...')
        eig = Solver.any_from_conf(pb.get_solver_conf(opts.eigen_solver))
        eigs, mtx_s_phi = eig(mtx_a, mtx_b, n_eigs)
        output('...done')

        bounding_box = pb.domain.mesh.get_bounding_box()
        # this assumes a box (3D), or a square (2D):
        a = bounding_box[1][0] - bounding_box[0][0]
        E_exact = None
        if options.hydrogen or options.boron:
            if options.hydrogen:
                Z = 1
            elif options.boron:
                Z = 5
            if dim == 2:
                E_exact = [-float(Z)**2/2/(n-0.5)**2/4
                           for n in [1]+[2]*3+[3]*5 + [4]*8 + [5]*15]
            elif dim == 3:
                E_exact = [-float(Z)**2/2/n**2 for n in [1]+[2]*2**2+[3]*3**2 ]
        if options.well:
            if dim == 2:
                E_exact = [pi**2/(2*a**2)*x
                           for x in [2, 5, 5, 8, 10, 10, 13, 13,
                                     17, 17, 18, 20, 20 ] ]
            elif dim == 3:
                E_exact = [pi**2/(2*a**2)*x
                           for x in [3, 6, 6, 6, 9, 9, 9, 11, 11,
                                     11, 12, 14, 14, 14, 14, 14,
                                     14, 17, 17, 17] ]
        if options.oscillator:
            if dim == 2:
                E_exact = [1] + [2]*2 + [3]*3 + [4]*4 + [5]*5 + [6]*6
            elif dim == 3:
                E_exact = [float(1)/2+x for x in [1]+[2]*3+[3]*6+[4]*10 ]
        if E_exact is not None:
            output("a=%f" % a)
            output("Energies:")
            output("n      exact         FEM      error")

            for i, e in enumerate(eigs):
                from numpy import NaN
                if i < len(E_exact):
                    exact = E_exact[i]
                    err = 100*abs((exact - e)/exact)
                else:
                    exact = NaN
                    err = NaN
                output("%d:  %.8f   %.8f  %5.2f%%" % (i, exact, e, err))
        else:
            output(eigs)

        mtx_phi = self.make_full(mtx_s_phi)
        self.save_results(eigs, mtx_phi)

        return Struct(pb=pb, eigs=eigs, mtx_phi=mtx_phi)

    def make_full(self, mtx_s_phi):
        variables = self.problem.get_variables()

        mtx_phi = nm.empty((variables.di.ptr[-1], mtx_s_phi.shape[1]),
                           dtype=nm.float64)
        for ii in xrange(mtx_s_phi.shape[1]):
            mtx_phi[:,ii] = variables.make_full_vec(mtx_s_phi[:,ii])

        return mtx_phi

    def save_results(self, eigs, mtx_phi, out=None,
                     mesh_results_name=None, eig_results_name=None):
        mesh_results_name = get_default(mesh_results_name,
                                        self.mesh_results_name)
        eig_results_name = get_default(eig_results_name,
                                       self.eig_results_name)
        pb = self.problem

        save = self.app_options.save_eig_vectors
        n_eigs = self.app_options.n_eigs
        out = get_default(out, {})
        state = pb.create_state()
        for ii in xrange(eigs.shape[0]):
            if save is not None:
                if (ii > save[0]) and (ii < (n_eigs - save[1])): continue
            state.set_full(mtx_phi[:,ii])
            aux = state.create_output_dict()
            key = aux.keys()[0]
            out[key+'%03d' % ii] = aux[key]

        pb.save_state(mesh_results_name, out=out)

        fd = open(eig_results_name, 'w')
        eigs.tofile(fd, ' ')
        fd.close()

########NEW FILE########
__FILENAME__ = dataset_manager
"""
Code to help with managing a TVTK data set in Pythonic ways.
"""

# Author: Prabhu Ramachandran <prabhu@aero.iitb.ac.in>
# Copyright (c) 2008, Enthought, Inc.
# License: BSD Style.

try:
    from enthought.traits.api import (HasTraits, Instance, Array, Str, 
                            Property, Dict)
    from enthought.tvtk.api import tvtk
    from enthought.tvtk.array_handler import array2vtk

except:
    from traits.api import (HasTraits, Instance, Array, Str, 
                            Property, Dict)
    from tvtk.api import tvtk
    from tvtk.array_handler import array2vtk


######################################################################
# Utility functions.
######################################################################
def get_array_type(arr):    
    """Returns if the array is a scalar ('scalars'), vector
    ('vectors') or tensor ('tensors').  It looks at the number of
    components to decide.  If it has a wierd number of components it
    returns the empty string.
    """    
    n = arr.number_of_components
    ret = {1: 'scalars', 3: 'vectors', 4: 'scalars', 9:'tensors'}
    return ret.get(n) or ''


def get_attribute_list(data):
    """ Gets scalar, vector and tensor information from the given data
    (either cell or point data).
    """    
    attr = {'scalars':[], 'vectors':[], 'tensors':[]}
    if data is not None:
        n = data.number_of_arrays
        for i in range(n):
            name = data.get_array_name(i)
            t = get_array_type(data.get_array(i))
            if len(t) > 0 and name is not None:
                attr[t].extend([name])
    
    def _mk_first(lst, value):
        """Makes the specified `value` the first item in `lst`."""
        lst.remove(value)
        lst.insert(0, value)
    
    attr1 = attr.copy()
    for a in attr:
        v = getattr(data, a)
        if v is not None:
            name = v.name
            if name is not None:
                try:
                    _mk_first(attr[a], v.name)
                except ValueError:
                    # Sometimes we have a multi-component scalar.
                    attr1[a].insert(0, name)
    return attr1


def get_all_attributes(obj):
    """Gets the scalar, vector and tensor attributes that are
    available in the given VTK data object.
    """
    point_attr = get_attribute_list(obj.point_data)
    cell_attr = get_attribute_list(obj.cell_data)
    return point_attr, cell_attr    
    

################################################################################
# `DatasetManager` class.
################################################################################ 
class DatasetManager(HasTraits):

    # The TVTK dataset we manage.
    dataset = Instance(tvtk.DataSet)

    # Our output, this is the dataset modified by us with different
    # active arrays.
    output = Property(Instance(tvtk.DataSet))

    # The point scalars for the dataset.  You may manipulate the arrays
    # in-place.  However adding new keys in this dict will not set the
    # data in the `dataset` for that you must explicitly call
    # `add_array`.
    point_scalars = Dict(Str, Array)
    # Point vectors.
    point_vectors = Dict(Str, Array)
    # Point tensors.
    point_tensors = Dict(Str, Array)

    # The cell scalars for the dataset.
    cell_scalars = Dict(Str, Array)
    cell_vectors = Dict(Str, Array)
    cell_tensors = Dict(Str, Array)

    # This filter allows us to change the attributes of the data
    # object and will ensure that the pipeline is properly taken care
    # of.  Directly setting the array in the VTK object will not do
    # this.
    _assign_attribute = Instance(tvtk.AssignAttribute, args=(),
                                 allow_none=False)


    ######################################################################
    # Public interface.
    ######################################################################
    def add_array(self, array, name, category='point'):
        """
        Add an array to the dataset to specified category ('point' or
        'cell').
        """
        assert len(array.shape) <= 2, "Only 2D arrays can be added."
        data = getattr(self.dataset, '%s_data'%category)
        if len(array.shape) == 2:
            assert array.shape[1] in [1, 3, 4, 9], \
                    "Only Nxm arrays where (m in [1,3,4,9]) are supported"
            va = tvtk.to_tvtk(array2vtk(array))
            va.name = name
            data.add_array(va)
            mapping = {1:'scalars', 3: 'vectors', 4: 'scalars', 
                       9: 'tensors'}
            dict = getattr(self, '%s_%s'%(category,
                                          mapping[array.shape[1]]))
            dict[name] = array
        else:
            va = tvtk.to_tvtk(array2vtk(array))
            va.name = name
            data.add_array(va)
            dict = getattr(self, '%s_scalars'%(category))
            dict[name] = array

    def remove_array(self, name, category='point'):
        """Remove an array by its name and optional category (point and
        cell).  Returns the removed array.
        """
        type = self._find_array(name, category)
        data = getattr(self.dataset, '%s_data'%category)
        data.remove_array(name)
        d = getattr(self, '%s_%s'%(category, type))
        return d.pop(name)

    def rename_array(self, name1, name2, category='point'):
        """Rename a particular array from `name1` to `name2`.
        """
        type = self._find_array(name1, category)
        data = getattr(self.dataset, '%s_data'%category)
        arr = data.get_array(name1)
        arr.name = name2
        d = getattr(self, '%s_%s'%(category, type))
        d[name2] = d.pop(name1)

    def activate(self, name, category='point'):
        """Make the specified array the active one.
        """
        type = self._find_array(name, category)
        self._activate_data_array(type, category, name)

    def update(self):
        """Update the dataset when the arrays are changed.
        """
        self.dataset.modified()
        self._assign_attribute.update()

    ######################################################################
    # Non-public interface.
    ######################################################################
    def _dataset_changed(self, value):
        self._setup_data()
        self._assign_attribute.input = value

    def _get_output(self):
        return self._assign_attribute.output

    def _setup_data(self):
        """Updates the arrays from what is available in the input data.
        """
        input = self.dataset
        pnt_attr, cell_attr = get_all_attributes(input)

        self._setup_data_arrays(cell_attr, 'cell')
        self._setup_data_arrays(pnt_attr, 'point')
     
    def _setup_data_arrays(self, attributes, d_type):
        """Given the dict of the attributes from the
        `get_all_attributes` function and the data type (point/cell)
        data this will setup the object and the data.  
        """
        attrs = ['scalars', 'vectors', 'tensors']
        aa = self._assign_attribute
        input = self.dataset
        data = getattr(input, '%s_data'%d_type)
        for attr in attrs:
            values = attributes[attr]
            # Get the arrays from VTK, create numpy arrays and setup our
            # traits.
            arrays = {}
            for name in values:
                va = data.get_array(name)
                npa = va.to_array()
                # Now test if changes to the numpy array are reflected
                # in the VTK array, if they are we are set, else we
                # have to set the VTK array back to the numpy array.
                if len(npa.shape) > 1:
                    old = npa[0,0]
                    npa[0][0] = old - 1
                    if abs(va[0][0] - npa[0,0]) > 1e-8:
                        va.from_array(npa)
                    npa[0][0] = old
                else:
                    old = npa[0]
                    npa[0] = old - 1
                    if abs(va[0] - npa[0]) > 1e-8:
                        va.from_array(npa)
                    npa[0] = old
                arrays[name] = npa

            setattr(self, '%s_%s'%(d_type, attr), arrays)

    def _activate_data_array(self, data_type, category, name):
        """Activate (or deactivate) a particular array.

        Given the nature of the data (scalars, vectors etc.) and the
        type of data (cell or points) it activates the array given by
        its name.

        Parameters:
        -----------

        data_type: one of 'scalars', 'vectors', 'tensors'
        category: one of 'cell', 'point'.
        name: string of array name to activate.
        """
        input = self.dataset
        data = None
        data = getattr(input, category + '_data')
        method = getattr(data, 'set_active_%s'%data_type)
        if len(name) == 0:
            # If the value is empty then we deactivate that attribute.
            method(None)
        else:
            aa = self._assign_attribute
            method(name)
            aa.assign(name, data_type.upper(), category.upper() +'_DATA')
            aa.update()

    def _find_array(self, name, category='point'):
        """Return information on which kind of attribute contains the
        specified named array in a particular category."""
        types = ['scalars', 'vectors', 'tensors']
        for type in types:
            attr = '%s_%s'%(category, type)
            d = getattr(self, attr)
            if name in d.keys():
                return type
        raise KeyError('No %s array named %s available in dataset'
                        %(category, name))


########NEW FILE########
__FILENAME__ = domain_specific
"""
Domain-specific plot functions.

All the plot functions accept the following parameters:

- `source` : Mayavi source
- `ctp` : Mayavi cell-to-point filter
- `position` : `(x, y, z)`
- `family` : 'point' or 'cell'
- `kind` : 'scalars', 'vectors' or 'tensors'
- `name` : name of a variable

All the plot functions return:
- `kind` : 'scalars', 'vectors' or 'tensors'
- `name` : name of a variable
- `active` : Mayavi module
"""
from copy import copy

from sfepy.base.base import assert_, Struct
from sfepy.postprocess.utils import mlab

class DomainSpecificPlot(Struct):
    """
    Class holding domain-specific plot function and its parameters.
    """
    def __init__(self, fun_name, args):
        Struct.__init__(self, fun_name=fun_name)

        ii = fun_name.rfind('.')
        if ii >= 0:
            # Function in a user module.
            mod_name, fun_name = fun_name[:ii], fun_name[ii+1:]
            mod = __import__(mod_name, fromlist=[fun_name])
            self.fun = getattr(mod, fun_name)

        else:
            self.fun = globals()[self.fun_name]

        kwargs = {}
        for arg in args:
            key, val = arg.split('=')
            kwargs[key] = eval(val)

        self.kwargs = kwargs

    def __call__(self, *args, **kwargs):
        _kwargs = copy(kwargs)
        _kwargs.update(self.kwargs)

        return self.fun(*args, **_kwargs)

def _get_scalars(color_name, color_kind, active):
    """
    Get scalars out of active data.
    """
    if color_kind == 'tensors':
        new_name = '|%s|' % color_name
        active = mlab.pipeline.set_active_attribute(active)
        active.point_tensors_name = color_name
        active = mlab.pipeline.extract_tensor_components(active)

    elif color_kind == 'vectors':
        new_name = '|%s|' % color_name
        active = mlab.pipeline.set_active_attribute(active)
        active.point_vectors_name = color_name
        active = mlab.pipeline.extract_vector_norm(active)

    elif color_kind == 'scalars':
        new_name = '%s' % color_name
        active = mlab.pipeline.set_active_attribute(active)
        active.point_scalars_name = color_name

    return new_name, active

def plot_displacements(source, ctp, bbox, position, family, kind, name,
                       rel_scaling=1.0,
                       color_kind=None, color_name=None, opacity=1.0):
    """
    Show displacements by displaying a colormap given by quantity
    `color_name` on the deformed mesh.

    Parameters
    ----------
    rel_scaling : float
        The relative scaling of displacements.
    color_kind : str, optional
        The kind of data determining the colormap.
    color_name : str, optional
        The name of data determining the colormap.
    opacity : float
        The surface plot opacity.
    """
    assert_(kind == 'vectors')

    if color_name is None:
        active = mlab.pipeline.set_active_attribute(source)

    else:
        active = mlab.pipeline.set_active_attribute(ctp)

    active.point_vectors_name = name
    active = mlab.pipeline.warp_vector(active)
    active.filter.scale_factor = rel_scaling

    if color_name is None:
        new_kind = kind
        new_name = name

        active = mlab.pipeline.extract_vector_norm(active)

    else:
        new_kind = 'scalars'
        new_name, active = _get_scalars(color_name, color_kind, active)

    surf = mlab.pipeline.surface(active, opacity=opacity)
    surf.actor.actor.position = position

    return new_kind, new_name, active

def plot_warp_scalar(source, ctp, bbox, position, family, kind, name,
                     rel_scaling=1.0,
                     color_kind=None, color_name=None, opacity=1.0):
    """
    Show a 2D scalar field by displaying a colormap given by quantity
    `color_name` on the deformed mesh deformed by the scalar in the third
    dimension.

    Parameters
    ----------
    rel_scaling : float
        The relative scaling of scalar warp.
    color_kind : str, optional
        The kind of data determining the colormap.
    color_name : str, optional
        The name of data determining the colormap.
    opacity : float
        The surface plot opacity.
    """
    assert_(kind == 'scalars')

    if color_name is None:
        active = mlab.pipeline.set_active_attribute(source)

    else:
        active = mlab.pipeline.set_active_attribute(ctp)

    active.point_scalars_name = name
    active = mlab.pipeline.warp_scalar(active)
    active.filter.scale_factor = rel_scaling

    if color_name is None:
        new_kind = kind
        new_name = name

    else:
        new_kind = 'scalars'
        new_name, active = _get_scalars(color_name, color_kind, active)

    surf = mlab.pipeline.surface(active, opacity=opacity)
    surf.actor.actor.position = position

    return new_kind, new_name, active

def plot_velocity(source, ctp, bbox, position, family, kind, name,
                  seed='sphere', type='ribbon', integration_direction='both',
                  seed_scale=1.0, seed_resolution=20,
                  widget_enabled=True,
                  color_kind=None, color_name=None, opacity=1.0,
                  **kwargs):
    """
    Show velocity field by displaying streamlines and optionally a
    surface plot given by quantity `color_name`.

    Parameters
    ----------
    seed : one of ('sphere', 'point', 'line', 'plane')
        The streamline seed name.
    type : one of ('line', 'ribbon', 'tube')
        The streamline seed line type.
    integration_direction : one of ('forward', 'backward', 'both')
        The stream tracer integration direction.
    seed_scale : float
        The seed size scale.
    seed_resolution : int
        The number of seed points in a direction (depends on `seed`).
    widget_enabled : bool
        It True, the seed widget is visible and can be interacted with.
    color_kind : str, optional
        The kind of data determining the colormap.
    color_name : str, optional
        The name of data determining the colormap.
    opacity : float
        The surface plot opacity.
    **kwargs : dict
        Additional keyword arguments for attributes of
        `streamline.seed.widget`.
    """
    assert_(kind == 'vectors')

    active_v = mlab.pipeline.set_active_attribute(source)
    active_v.point_vectors_name = name

    active_n = mlab.pipeline.extract_vector_norm(active_v)

    s = mlab.pipeline.streamline
    streamline = s(active_n, seedtype=seed,
                   linetype=type,
                   seed_visible=True,
                   seed_scale=seed_scale,
                   integration_direction=integration_direction,
                   seed_resolution=seed_resolution)
    streamline.update_streamlines = True
    streamline.seed.widget.enabled = widget_enabled

    for key, val in kwargs.iteritems():
        setattr(streamline.seed.widget, key, val)

    if color_name is None:
        active = active_n

    else:
        new_name, active = _get_scalars(color_name, color_kind, source)

    surf = mlab.pipeline.surface(active, opacity=opacity)
    surf.actor.actor.position = position

    if color_name is not None:
        mm = active.children[0]
        lm = mm.scalar_lut_manager
        scalar_bars = [['point', new_name, lm]]

        active_v.point_vectors_name = name # This is needed to have
                                           # colors by velocity!
        return kind, name, active_n, scalar_bars

    else:
        return kind, name, active_n


########NEW FILE########
__FILENAME__ = plot_cmesh
"""
Functions to visualize the CMesh geometry and topology.
"""
import matplotlib.pyplot as plt

from sfepy.postprocess.plot_dofs import _get_axes

def plot_wireframe(ax, cmesh, color='k', show=False):
    """
    Plot a finite element mesh as a wireframe using edges connectivity.
    """
    coors = cmesh.coors
    dim = cmesh.dim

    edges = cmesh.get_conn(1, 0)

    ax = _get_axes(ax, dim)

    for edge_vertices in edges.indices.reshape((edges.num, 2)):
        cc = coors[edge_vertices]
        if dim == 3:
            ax.plot(cc[:, 0], cc[:, 1], cc[:, 2], color)

        else:
            ax.plot(cc[:, 0], cc[:, 1], color)

    if show:
        plt.show()

    return ax

def plot_entities(ax, cmesh, edim, color='b', size=10, show=False):
    """
    Plot mesh topology entities using scatter plot.
    """
    coors = cmesh.get_centroids(edim)
    dim = cmesh.dim

    ax = _get_axes(ax, dim)

    if dim == 3:
        ax.scatter(coors[:, 0], coors[:, 1], coors[:, 2], s=size, c=color)

    else:
        ax.scatter(coors[:, 0], coors[:, 1], s=size, c=color)

    if show:
        plt.show()

    return ax

def label_global_entities(ax, cmesh, edim, color='b', fontsize=10, show=False):
    """
    Label mesh topology entities using global ids.
    """
    coors = cmesh.get_centroids(edim)
    dim = cmesh.dim

    ax = _get_axes(ax, dim)

    for ii, cc in enumerate(coors):
        if dim == 3:
            ax.text(cc[0], cc[1], cc[2], ii,
                    color=color, fontsize=fontsize)

        else:
            ax.text(cc[0], cc[1], ii,
                    color=color, fontsize=fontsize)

    if show:
        plt.show()

    return ax

def label_local_entities(ax, cmesh, edim, color='b', fontsize=10, show=False):
    """
    Label mesh topology entities using cell-local ids.
    """
    coors = cmesh.get_centroids(edim)
    dim = cmesh.dim
    centres = cmesh.get_centroids(dim)

    conn = cmesh.get_conn(dim, edim)
    off = conn.offsets

    ax = _get_axes(ax, dim)

    eps = 0.1
    oeps = 1.0 - eps
    for ii in xrange(conn.num):
        for ic, ie in enumerate(conn.indices[off[ii]:off[ii+1]]):
            # Shift labels towards the cell centre.
            cc = oeps * coors[ie] + eps * centres[ii]
            if dim == 3:
                ax.text(cc[0], cc[1], cc[2], ic,
                        color=color, fontsize=fontsize)

            else:
                ax.text(cc[0], cc[1], ic,
                        color=color, fontsize=fontsize)

    if show:
        plt.show()

    return ax

########NEW FILE########
__FILENAME__ = plot_dofs
"""
Functions to visualize the mesh connectivity with global and local DOF
numberings.
"""
import numpy as nm

import matplotlib.pyplot as plt

def _get_axes(ax, dim):
    if ax is None:
        fig = plt.figure()
        if dim == 3:
            from mpl_toolkits.mplot3d import axes3d
            axes3d # Make pyflakes happy...

            ax = fig.add_subplot(111, projection='3d')

        else:
            ax = fig.add_subplot(111)

    return ax

def _to2d(coors):
    if coors.shape[1] == 1:
        coors = nm.c_[coors, nm.zeros_like(coors)]

    return coors

def plot_points(ax, coors, vals=None, point_size=20,
                show_colorbar=False, show=False):
    """
    Plot points with given coordinates, optionally colored using `vals` values.
    """
    dim = coors.shape[1]
    ax = _get_axes(ax, dim)

    colors = 'b' if vals is None else vals

    if dim == 3:
        sc = ax.scatter(coors[:, 0], coors[:, 1], coors[:, 2],
                        s=point_size, c=colors, alpha=1)

    else:
        coors = _to2d(coors)
        sc = ax.scatter(coors[:, 0], coors[:, 1], s=point_size, c=colors)

    if show_colorbar and (vals is not None):
        plt.colorbar(sc)

    if show:
        plt.show()

    return ax

def plot_mesh(ax, coors, conn, edges, show=False):
    """
    Plot a finite element mesh as a wireframe.
    """
    dim = coors.shape[1]
    ax = _get_axes(ax, dim)
    coors = _to2d(coors)

    for el in conn:
        eds = el[edges]

        for ed in eds:
            cc = coors[ed]

            if dim == 3:
                ax.plot(cc[:, 0], cc[:, 1], cc[:, 2], 'k')

            else:
                ax.plot(cc[:, 0], cc[:, 1], 'k')

    if show:
        plt.show()

    return ax

def plot_global_dofs(ax, coors, econn, show=False):
    """
    Plot global DOF numbers given in an extended connectivity.

    The DOF numbers are plotted for each element, so on common facets they are
    plotted several times - this can be used to check the consistency of the
    global DOF connectivity.
    """
    dim = coors.shape[1]
    ax = _get_axes(ax, dim)
    coors = _to2d(coors)

    for el in econn:
        for gdof in el:
            if dim == 3:
                cx, cy, cz = coors[gdof]
                ax.text(cx, cy, cz, '%d' % gdof,
                        color='g', fontsize=12, weight='bold')

            else:
                cx, cy = coors[gdof]
                ax.text(cx, cy, '%d' % gdof,
                        color='g', fontsize=12, weight='bold')

    if show:
        plt.show()

    return ax

def plot_local_dofs(ax, coors, econn, show=False):
    """
    Plot local DOF numbers corresponding to an extended connectivity.
    """
    dim = coors.shape[1]
    ax = _get_axes(ax, dim)
    coors = _to2d(coors)

    eps = 0.1
    oeps = 1.0 - eps
    for el in econn:
        # Element centre.
        centre = coors[el].sum(0) / el.shape[0]

        for ldof, gdof in enumerate(el):
            # Shift labels towards the centre.
            cc = oeps * coors[gdof] + eps * centre

            if dim == 3:
                cx, cy, cz = cc
                ax.text(cx, cy, cz, '%d' % ldof,
                        color='b', fontsize=10, weight='light')

            else:
                cx, cy = cc
                ax.text(cx, cy, '%d' % ldof,
                        color='b', fontsize=10, weight='light')

    if show:
        plt.show()

    return ax

def plot_nodes(ax, coors, econn, ref_nodes, dofs, show=False):
    """
    Plot Lagrange reference element nodes corresponding to global DOF numbers
    given in an extended connectivity.
    """
    dim = coors.shape[1]
    ax = _get_axes(ax, dim)
    coors = _to2d(coors)

    eps = 0.2
    oeps = 1.0 - eps
    for el in econn:
        # Element centre.
        centre = coors[el].sum(0) / el.shape[0]

        for gdof in dofs:
            if not gdof in el:
                continue
            ldof = nm.where(el == gdof)[0]
            node = ref_nodes[ldof]

            # Shift labels towards the centre.
            cc = oeps * coors[gdof] + eps * centre

            if dim == 3:
                cx, cy, cz = cc
                ax.text(cx, cy, cz, '%s' % node,
                        color='r', fontsize=8, weight='light')

            else:
                cx, cy = cc
                ax.text(cx, cy, '%d' % node,
                        color='r', fontsize=8, weight='light')

    if show:
        plt.show()

    return ax

########NEW FILE########
__FILENAME__ = plot_facets
"""
Functions to visualize the geometry elements and numbering and orientation of
their facets (edges and faces).

The standard geometry elements can be plotted by running::

  $ python sfepy/postprocess/plot_facets.py
"""
import numpy as nm
import matplotlib.pyplot as plt

from sfepy.linalg import (get_perpendiculars, normalize_vectors,
                          make_axis_rotation_matrix)
from sfepy.postprocess.plot_dofs import _get_axes, plot_mesh, plot_global_dofs

def plot_geometry(ax, gel, show=False):
    """
    Plot a geometry element as a wireframe.
    """
    ax = plot_mesh(ax, gel.coors, [gel.conn], gel.edges, show=False)
    ax = plot_global_dofs(ax, gel.coors, [gel.conn], show=show)

    return ax

def plot_edges(ax, gel, length, show=False):
    """
    Plot edges of a geometry element as numbered arrows.
    """
    dim = gel.dim
    ax = _get_axes(ax, dim)

    l2 = 0.5 * length
    for ii, edge in enumerate(gel.edges):
        cc = gel.coors[edge]
        centre = 0.5 * cc.sum(axis=0)

        vdir = (cc - centre)
        normalize_vectors(vdir)

        cc = l2 * vdir + centre
        draw_arrow(ax, cc, length=0.3*length, linewidth=3, color='b')

        if dim == 3:
            cx, cy, cz = centre
            ax.text(cx, cy, cz, ii,
                    color='b', fontsize=10, weight='light')

        else:
            cx, cy = centre
            ax.text(cx, cy, ii,
                    color='b', fontsize=10, weight='light')

    return ax

def plot_faces(ax, gel, radius, n_point, show=False):
    """
    Plot faces of a 3D geometry element as numbered oriented arcs. An arc
    centre corresponds to the first node of a face. It points from the first
    edge towards the last edge of the face.
    """
    dim = gel.dim
    ax = _get_axes(ax, dim)

    if dim < 3: return ax

    for ii, face in enumerate(gel.faces):
        cc = gel.coors[face]

        t1 = cc[1, :] - cc[0, :]
        t2 = cc[-1, :] - cc[0, :]
        n = nm.cross(t1, t2)

        nt1 = nm.linalg.norm(t1)
        nt2 = nm.linalg.norm(t2)
        angle = nm.arccos(nm.dot(t1, t2) / (nt1 * nt2))

        da = angle / (n_point - 1)

        mtx = make_axis_rotation_matrix(n, da)

        rt = cc[0] + radius * t1 / nt1
        coors = [rt]
        for ip in range(n_point - 1):
            rt = nm.dot(mtx.T, (rt - cc[0])) + cc[0]
            coors.append(rt)

        coors = nm.array(coors, dtype=nm.float64)
        centre = coors.sum(axis=0) / coors.shape[0]

        draw_arrow(ax, coors, length=0.3*radius, linewidth=3, color='r')

        if dim == 3:
            cx, cy, cz = centre
            ax.text(cx, cy, cz, ii,
                    color='r', fontsize=10, weight='light')

        else:
            cx, cy = centre
            ax.text(cx, cy, ii,
                    color='r', fontsize=10, weight='light')

    return ax

def draw_arrow(ax, coors, angle=20.0, length=0.3, **kwargs):
    """
    Draw a line ended with an arrow head, in 2D or 3D.
    """
    color = kwargs.get('color', 'b')

    c0 = coors[-2]
    c1 = coors[-1]

    vd = c1 - c0
    nvd = nm.linalg.norm(vd)
    vd /= nvd

    c0 = c1 - length * vd

    ps = get_perpendiculars(vd)

    rangle = nm.deg2rad(min(angle, 60.0))
    plength = length * nm.arctan(rangle)

    if coors.shape[1] == 2:
        from matplotlib.patches import Polygon

        cx, cy = coors[:, 0], coors[:, 1]

        ax.plot(cx, cy, **kwargs)

        p0 = c0 + plength * ps
        p1 = c0 - plength * ps

        pol = Polygon([p0, p1, c1], color=color)
        ax.add_artist(pol)

    else:
        import mpl_toolkits.mplot3d as plt3

        cx, cy, cz = coors[:, 0], coors[:, 1], coors[:, 2]

        ax.plot(cx, cy, cz, **kwargs)

        p00 = c0 + plength * ps[0]
        p01 = c0 - plength * ps[0]

        p10 = c0 + plength * ps[1]
        p11 = c0 - plength * ps[1]

        arr = plt3.art3d.Poly3DCollection([[p00, p01, c1],
                                           [p10, p11, c1]], color=color)
        ax.add_collection3d(arr)

if __name__ == '__main__':
    from sfepy.discrete.fem.geometry_element import GeometryElement, geometry_data

    for key, gd in geometry_data.iteritems():
        if key == '1_2' : continue

        gel = GeometryElement(key)

        ax = plot_geometry(None, gel)
        ax = plot_edges(ax, gel, length=0.2)
        ax = plot_faces(ax, gel, radius=0.3, n_point=5)

        dd = 0.05
        ax.set_xlim([-dd, 1.0 + dd])
        ax.set_ylim([-dd, 1.0 + dd])
        if gel.dim == 3:
            ax.set_zlim([-dd, 1.0 + dd])

        plt.show()

########NEW FILE########
__FILENAME__ = plot_quadrature
"""
Functions to visualize quadrature points in reference elements.
"""
import matplotlib.pyplot as plt

from sfepy.base.base import output

from sfepy.postprocess.plot_dofs import _get_axes
from sfepy.postprocess.plot_facets import plot_geometry

def _get_qp(geometry, order):
    from sfepy.discrete import Integral
    from sfepy.discrete.fem.geometry_element import GeometryElement

    aux = Integral('aux', order=order)
    coors, weights = aux.get_qp(geometry)
    true_order = aux.qps[geometry].order

    output('geometry:', geometry, 'order:', order, 'num. points:',
           coors.shape[0], 'true_order:', true_order)
    output('min. weight:', weights.min())
    output('max. weight:', weights.max())

    return GeometryElement(geometry), coors, weights

def plot_weighted_points(ax, coors, weights, min_radius=10, max_radius=50,
                         show_colorbar=False, show=False):
    """
    Plot points with given coordinates as circles/spheres with radii given by
    weights.
    """
    dim = coors.shape[1]
    ax = _get_axes(ax, dim)

    wmin, wmax = weights.min(), weights.max()
    if (wmax - wmin) < 1e-12:
        nweights = weights * max_radius / wmax

    else:
        nweights = ((weights - wmin) * (max_radius - min_radius)
                    / (wmax - wmin) + min_radius)

    if dim == 3:
        sc = ax.scatter(coors[:, 0], coors[:, 1], coors[:, 2],
                        s=nweights, c=weights, alpha=1)

    else:
        sc = ax.scatter(coors[:, 0], coors[:, 1],
                        s=nweights, c=weights, alpha=1)

    if show_colorbar:
        plt.colorbar(sc)

    if show:
        plt.show()

    return ax

def plot_quadrature(ax, geometry, order, min_radius=10, max_radius=50,
                    show_colorbar=False, show=False):
    """
    Plot quadrature points for the given geometry and integration order.

    The points are plotted as circles/spheres with radii given by quadrature
    weights - the weights are mapped to [`min_radius`, `max_radius`] interval.
    """
    gel, coors, weights = _get_qp(geometry, order)
    dim = coors.shape[1]

    ax = _get_axes(ax, dim)

    plot_geometry(ax, gel, show=False)
    plot_weighted_points(ax, coors, weights,
                         min_radius=min_radius, max_radius=max_radius,
                         show_colorbar=show_colorbar, show=show)

    return ax

########NEW FILE########
__FILENAME__ = sources
import os

import numpy as nm

try:
    from enthought.tvtk.api import tvtk
    from enthought.mayavi.sources.vtk_data_source import VTKDataSource
    from enthought.pyface.timer.api import Timer

except:
    from tvtk.api import tvtk
    from mayavi.sources.vtk_data_source import VTKDataSource
    from pyface.timer.api import Timer

from dataset_manager import DatasetManager

from sfepy.base.base import Struct, basestr
from sfepy.postprocess.utils import mlab
from sfepy.discrete.fem import Mesh
from sfepy.discrete.fem.meshio import MeshIO, vtk_cell_types, supported_formats

def create_file_source(filename, watch=False, offscreen=True):
    """Factory function to create a file source corresponding to the
    given file format."""
    kwargs = {'watch' : watch, 'offscreen' : offscreen}

    if isinstance(filename, basestr):
        fmt = os.path.splitext(filename)[1]
        is_sequence = False

    else: # A sequence.
        fmt = os.path.splitext(filename[0])[1]
        is_sequence = True

    fmt = fmt.lower()

    if fmt == '.vtk':
        # VTK is supported directly by Mayavi, no need to use MeshIO.
        if is_sequence:
            return VTKSequenceFileSource(filename, **kwargs)
        else:
            return VTKFileSource(filename, **kwargs)

    elif fmt in supported_formats.keys():
        if is_sequence:
            if fmt == '.h5':
                raise ValueError('format .h5 does not support file sequences!')
            else:
                return GenericSequenceFileSource(filename, **kwargs)
        else:
            return GenericFileSource(filename, **kwargs)

    else:
        raise ValueError('unknown file format! (%s)' % fmt)

class FileSource(Struct):
    """General file source."""

    def __init__(self, filename, watch=False, offscreen=True):
        """Create a file source using the given file name."""
        mlab.options.offscreen = offscreen
        self.watch = watch
        self.filename = filename
        self.reset()

    def __call__(self, step=0):
        """Get the file source."""
        if self.source is None:
            self.source = self.create_source()
            if self.watch:
                self.timer = Timer(1000, self.poll_file)

        return self.source

    def reset(self):
        """Reset."""
        self.mat_id_name = None
        self.source = None
        self.notify_obj = None
        self.steps = []
        self.times = []
        self.step = 0
        self.time = 0.0
        if self.watch:
            self.last_stat = os.stat(self.filename)

    def setup_mat_id(self, mat_id_name='mat_id', single_color=False):
        self.mat_id_name = mat_id_name
        self.single_color = single_color

    def get_step_time(self, step=None, time=None):
        """
        Set current step and time to the values closest greater or equal to
        either step or time. Return the found values.
        """
        if (step is not None) and len(self.steps):
            step = step if step >= 0 else self.steps[-1] + step + 1
            ii = nm.searchsorted(self.steps, step)
            ii = nm.clip(ii, 0, len(self.steps) - 1)

            self.step = self.steps[ii]
            if len(self.times):
                self.time = self.times[ii]

        elif (time is not None) and len(self.times):
            ii = nm.searchsorted(self.times, time)
            ii = nm.clip(ii, 0, len(self.steps) - 1)

            self.step = self.steps[ii]
            self.time = self.times[ii]

        return self.step, self.time

    def get_ts_info(self):
        return self.steps, self.times

    def get_mat_id(self, mat_id_name='mat_id'):
        """
        Get material ID numbers of the underlying mesh elements.
        """
        if self.source is not None:
            dm = DatasetManager(dataset=self.source.outputs[0])

            mat_id = dm.cell_scalars[mat_id_name]
            return mat_id

    def file_changed(self):
        pass

    def setup_notification(self, obj, attr):
        """The attribute 'attr' of the object 'obj' will be set to True
        when the source file is watched and changes."""
        self.notify_obj = obj
        self.notify_attr = attr

    def poll_file(self):
        """Check the source file's time stamp and notify the
        self.notify_obj in case it changed. Subclasses should implement
        the file_changed() method."""
        if not self.notify_obj:
            return

        s = os.stat(self.filename)
        if s[-2] == self.last_stat[-2]:
            setattr(self.notify_obj, self.notify_attr, False)
        else:
            self.file_changed()
            setattr(self.notify_obj, self.notify_attr, True)
            self.last_stat = s

class VTKFileSource(FileSource):
    """A thin wrapper around mlab.pipeline.open()."""

    def create_source(self):
        """Create a VTK file source """
        return mlab.pipeline.open(self.filename)

    def get_bounding_box(self):
        bbox = nm.array(self.source.reader.unstructured_grid_output.bounds)
        return bbox.reshape((3,2)).T

    def set_filename(self, filename, vis_source):
        self.filename = filename
        vis_source.base_file_name = filename

        # Force re-read.
        vis_source.reader.modified()
        vis_source.update()
        # Propagate changes in the pipeline.
        vis_source.data_changed = True

class VTKSequenceFileSource(VTKFileSource):
    """A thin wrapper around mlab.pipeline.open() for VTK file sequences."""

    def __init__(self, *args, **kwargs):
        FileSource.__init__(self, *args, **kwargs)

        self.steps = nm.arange(len(self.filename), dtype=nm.int32)

    def create_source(self):
        """Create a VTK file source """
        return mlab.pipeline.open(self.filename[0])

    def set_filename(self, filename, vis_source):
        self.filename = filename
        vis_source.base_file_name = filename[self.step]

class GenericFileSource(FileSource):
    """File source usable with any format supported by MeshIO classes."""

    def __init__(self, *args, **kwargs):
        FileSource.__init__(self, *args, **kwargs)

        self.read_common(self.filename)

    def read_common(self, filename):
        self.io = MeshIO.any_from_filename(filename)
        self.steps, self.times, _ = self.io.read_times()

        self.mesh = Mesh.from_file(filename)
        self.n_nod, self.dim = self.mesh.coors.shape

    def create_source(self):
        """
        Create a VTK source from data in a SfePy-supported file.

        Notes
        -----
        All data need to be set here, otherwise time stepping will not
        work properly - data added by user later will be thrown away on
        time step change.
        """
        if self.io is None:
            self.read_common(self.filename)

        dataset = self.create_dataset()

        try:
            out = self.io.read_data(self.step)
        except ValueError:
            out = None

        if out is not None:
            self.add_data_to_dataset(dataset, out)

        if self.mat_id_name is not None:
            mat_id = nm.concatenate(self.mesh.mat_ids)
            if self.single_color:
                rm = mat_id.min(), mat_id.max()
                mat_id[mat_id > rm[0]] = rm[1]

            dm = DatasetManager(dataset=dataset)
            dm.add_array(mat_id, self.mat_id_name, 'cell')

        src = VTKDataSource(data=dataset)
#        src.print_traits()
#        debug()
        return src

    def get_bounding_box(self):
        bbox = self.mesh.get_bounding_box()
        if self.dim == 2:
            bbox = nm.c_[bbox, [0.0, 0.0]]
        return bbox

    def set_filename(self, filename, vis_source):
        self.filename = filename
        self.source = self.create_source()
        vis_source.data = self.source.data

    def get_mat_id(self, mat_id_name='mat_id'):
        """
        Get material ID numbers of the underlying mesh elements.
        """
        if self.source is not None:
            mat_id = nm.concatenate(self.mesh.mat_ids)
            return mat_id

    def file_changed(self):
        self.steps, self.times, _ = self.io.read_times()

    def create_dataset(self):
        """Create a tvtk.UnstructuredGrid dataset from the Mesh instance of the
        file source."""
        mesh = self.mesh
        n_nod, dim = self.n_nod, self.dim
        n_el, n_els, n_e_ps = mesh.n_el, mesh.n_els, mesh.n_e_ps

        if dim == 2:
            nod_zz = nm.zeros((n_nod, 1), dtype=mesh.coors.dtype)
            points = nm.c_[mesh.coors, nod_zz]
        else:
            points = mesh.coors

        dataset = tvtk.UnstructuredGrid(points=points)

        cell_types = []
        cells = []
        offset = [0]
        for ig, conn in enumerate(mesh.conns):
            cell_types += [vtk_cell_types[mesh.descs[ig]]] * n_els[ig]

            nn = nm.array([conn.shape[1]] * n_els[ig])
            aux = nm.c_[nn[:,None], conn]
            cells.extend(aux.ravel())

            offset.extend([aux.shape[1]] * n_els[ig])

        cells = nm.array(cells)
        cell_types = nm.array(cell_types)
        offset = nm.cumsum(offset)[:-1]
        
        cell_array = tvtk.CellArray()
        cell_array.set_cells(n_el, cells)

        dataset.set_cells(cell_types, offset, cell_array)

        return dataset

    def add_data_to_dataset(self, dataset, data):
        """Add point and cell data to the dataset."""
        dim = self.dim
        sym = (dim + 1) * dim / 2

        dm = DatasetManager(dataset=dataset)
        for key, val in data.iteritems():
            vd = val.data
##             print vd.shape
            if val.mode == 'vertex':
                if vd.shape[1] == 1:
                    aux = vd.reshape((vd.shape[0],))

                elif vd.shape[1] == 2:
                    zz = nm.zeros((vd.shape[0], 1), dtype=vd.dtype)
                    aux = nm.c_[vd, zz]

                elif vd.shape[1] == 3:
                    aux = vd

                else:
                    raise ValueError('unknown vertex data format! (%s)'\
                                     % vd.shape)

                dm.add_array(aux, key, 'point')

            elif val.mode == 'cell':
                ne, aux, nr, nc = vd.shape
                if (nr == 1) and (nc == 1):
                    aux = vd.reshape((ne,))

                elif (nr == dim) and (nc == 1):
                    if dim == 3:
                        aux = vd.reshape((ne, dim))
                    else:
                        zz = nm.zeros((vd.shape[0], 1), dtype=vd.dtype);
                        aux = nm.c_[vd.squeeze(), zz]

                elif (((nr == sym) or (nr == (dim * dim))) and (nc == 1)) \
                         or ((nr == dim) and (nc == dim)):
                    vd = vd.squeeze()

                    if dim == 3:
                        if nr == sym:
                            aux = vd[:,[0,3,4,3,1,5,4,5,2]]
                        elif nr == (dim * dim):
                            aux = vd[:,[0,3,4,6,1,5,7,8,2]]
                        else:
                            aux = vd.reshape((vd.shape[0], dim*dim))
                    else:
                        zz = nm.zeros((vd.shape[0], 1), dtype=vd.dtype);
                        if nr == sym:
                            aux = nm.c_[vd[:,[0,2]], zz, vd[:,[2,1]],
                                        zz, zz, zz, zz]
                        elif nr == (dim * dim):
                            aux = nm.c_[vd[:,[0,2]], zz, vd[:,[3,1]],
                                        zz, zz, zz, zz]
                        else:
                            aux = nm.c_[vd[:,0,[0,1]], zz, vd[:,1,[0,1]],
                                        zz, zz, zz, zz]

                dm.add_array(aux, key, 'cell')

class GenericSequenceFileSource(GenericFileSource):
    """File source usable with any format supported by MeshIO classes, with
    exception of HDF5 (.h5), for file sequences."""

    def read_common(self, filename):
        self.steps = nm.arange(len(self.filename), dtype=nm.int32)

    def create_source(self):
        """Create a VTK source from data in a SfePy-supported file."""
        if self.io is None:
            self.read_common(self.filename[self.step])

        dataset = self.create_dataset()

        src = VTKDataSource(data=dataset)
        return src

    def set_filename(self, filename, vis_source):
        self.filename = filename
        self.io = None
        self.source = self.create_source()
        vis_source.data = self.source.data

########NEW FILE########
__FILENAME__ = time_history
import numpy as nm

from sfepy.base.base import output, OneTypeList, Struct
from sfepy.discrete.fem.mesh import Mesh
from sfepy.discrete.fem.meshio import MeshIO
from sfepy.solvers.ts import TimeStepper
from sfepy.base.ioutils import get_trunk, write_dict_hdf5

def _linearize(out, fields, linearization):
    new = {}
    for key, val in out.iteritems():
        field = fields[val.field_name]
        new.update(field.create_output(val.data, var_name=key,
                                       dof_names=val.dofs, key=key,
                                       linearization=linearization))

    return new


def dump_to_vtk(filename, output_filename_trunk=None, step0=0, steps=None,
                fields=None, linearization=None):
    """Dump a multi-time-step results file into a sequence of VTK files."""
    def _save_step(suffix, out, mesh):
        if linearization is not None:
            output('linearizing...')
            out = _linearize(out, fields, linearization)
            output('...done')
            for key, val in out.iteritems():
                lmesh = val.get('mesh', mesh)
                lmesh.write(output_filename_trunk + '_' + key + suffix,
                            io='auto', out={key : val})
                if hasattr(val, 'levels'):
                    output('max. refinement per group:', val.levels)

        else:
            mesh.write(output_filename_trunk + suffix, io='auto', out=out)

    output('dumping to VTK...')

    io = MeshIO.any_from_filename(filename)
    mesh = Mesh.from_file(filename, io=io)

    if output_filename_trunk is None:
        output_filename_trunk = get_trunk(filename)

    try:
        ts = TimeStepper(*io.read_time_stepper())
        all_steps, times, nts, dts = extract_times(filename)

    except ValueError:
        output('no time stepping info found, assuming single step')

        out = io.read_data(0)
        if out is not None:
            _save_step('.vtk', out, mesh)

        ret = None

    else:
        ts.times = times
        ts.n_step = times.shape[0]

        if steps is None:
            ii0 = nm.searchsorted(all_steps, step0)
            iterator = ((all_steps[ii], times[ii])
                        for ii in xrange(ii0, len(times)))

        else:
            iterator = [(step, ts.times[step]) for step in steps]

        max_step = all_steps.max()
        for step, time in iterator:
            output(ts.format % (step, max_step))
            out = io.read_data(step)
            if out is None: break

            _save_step('.' + ts.suffix % step + '.vtk', out, mesh)

        ret = ts.suffix

    output('...done')
    return ret

def extract_times(filename):
    """
    Read true time step data from individual time steps.

    Returns
    -------
    steps : array
        The time steps.
    times : array
        The times of the time steps.
    nts : array
        The normalized times of the time steps, in [0, 1].
    dts : array
        The true time deltas.
    """
    io = MeshIO.any_from_filename(filename)
    steps, times, nts = io.read_times()

    dts = nm.ediff1d(times, to_end=0)

    return steps, times, nts, dts

def extract_time_history(filename, extract, verbose=True):
    """Extract time history of a variable from a multi-time-step results file.

    Parameters
    ----------
    filename : str
        The name of file to extract from.
    extract : str
        The description of what to extract in a string of comma-separated
        description items. A description item consists of: name of the variable
        to extract, mode ('e' for elements, 'n' for nodes), ids of the nodes or
        elements (given by the mode). Example: 'u n 10 15, p e 0' means
        variable 'u' in nodes 10, 15 and variable 'p' in element 0.
    verbose : bool
        Verbosity control.

    Returns
    -------
    ths : dict
        The time histories in a dict with variable names as keys. If a
        nodal variable is requested in elements, its value is a dict of histories
        in the element nodes.
    ts : TimeStepper instance
        The time stepping information.
    """
    output('extracting selected data...', verbose=verbose)

    output('selection:', extract, verbose=verbose)

    ##
    # Parse extractions.
    pes = OneTypeList(Struct)
    for chunk in extract.split(','):
        aux = chunk.strip().split()
        pes.append(Struct(var = aux[0],
                          mode = aux[1],
                          indx = map(int, aux[2:]),
                          igs = None))

    ##
    # Verify array limits, set igs for element data, shift indx.
    mesh = Mesh.from_file(filename)
    n_el, n_els, offs = mesh.n_el, mesh.n_els, mesh.el_offsets
    for pe in pes:
        if pe.mode == 'n':
            for ii in pe.indx:
                if (ii < 0) or (ii >= mesh.n_nod):
                    raise ValueError('node index 0 <= %d < %d!'
                                     % (ii, mesh.n_nod))

        if pe.mode == 'e':
            pe.igs = []
            for ii, ie in enumerate(pe.indx[:]):
                if (ie < 0) or (ie >= n_el):
                    raise ValueError('element index 0 <= %d < %d!'
                                     % (ie, n_el))
                ig = (ie < n_els).argmax()
                pe.igs.append(ig)
                pe.indx[ii] = ie - offs[ig]

##     print pes

    ##
    # Extract data.
    # Assumes only one element group (ignores igs)!
    io = MeshIO.any_from_filename(filename)
    ths = {}
    for pe in pes:
        mode, nname = io.read_data_header(pe.var)
        output(mode, nname, verbose=verbose)

        if ((pe.mode == 'n' and mode == 'vertex') or
            (pe.mode == 'e' and mode == 'cell')):
            th = io.read_time_history(nname, pe.indx)

        elif pe.mode == 'e' and mode == 'vertex':
            conn = mesh.conns[0]
            th = {}
            for iel in pe.indx:
                ips = conn[iel]
                th[iel] = io.read_time_history(nname, ips)
        else:
            raise ValueError('cannot extract cell data %s in nodes!' % pe.var)
            
        ths[pe.var] = th

    output('...done', verbose=verbose)

    ts = TimeStepper(*io.read_time_stepper())

    return ths, ts

def average_vertex_var_in_cells(ths_in):
    """Average histories in the element nodes for each nodal variable
        originally requested in elements."""
    ths = dict.fromkeys(ths_in.keys())
    for var, th in ths_in.iteritems():
        aux = dict.fromkeys(th.keys())
        for ir, data in th.iteritems():
            if isinstance(data, dict):
                for ic, ndata in data.iteritems():
                    if aux[ir] is None:
                        aux[ir] = ndata
                    else:
                        aux[ir] += ndata
                aux[ir] /= float(len(data))
            else:
                aux[ir] = data
        ths[var] = aux

    return ths

def save_time_history(ths, ts, filename_out):
    """Save time history and time-stepping information in a HDF5 file."""
    ths.update({'times' : ts.times, 'dt' : ts.dt})
    write_dict_hdf5(filename_out, ths)

def guess_time_units(times):
    """
    Given a vector of times in seconds, return suitable time units and
    new vector of times suitable for plotting.

    Parameters
    ----------
    times : array
        The vector of times in seconds.

    Returns
    -------
    new_times : array
        The vector of times in `units`.
    units : str
        The time units.
    """
    times = nm.asarray(times)

    if (times[-1] / 60.0 / 60.0) > 10.0:
        units = 'hours'
        new_times = times / 60.0 / 60.0

    elif (times[-1] / 60.0) > 10.0:
        units = 'min.'
        new_times = times / 60.0

    else:
        units = 's'
        new_times = times

    return new_times, units

########NEW FILE########
__FILENAME__ = utils
import numpy as np

try:
    import enthought.mayavi as mayavi

except ImportError:
    try:
        import mayavi

    except ImportError:
        mayavi = None

if mayavi:
    from mayavi import mlab
    from mayavi.core.source import Source
    from mayavi.core.filter import Filter
    import dataset_manager

from sfepy.base.base import basestr

def get_data_ranges(obj, return_only=False, use_names=None, filter_names=None):
    """Collect and print information on ranges of data in a dataset.

    Parameters
    ----------
    obj : a mayavi pipeline object
        The object to probe for data.
        
    return_only : boolean
        If True, do not print the information, just return it to the caller.

    use_names : list of strings
        Consider only data with names in the list.

    filter_names : list of strings
        Consider only data with names not in the list.

    Returns
    -------
    ranges : dict
        The requested data ranges.
    """
    if isinstance(obj, basestr):
        mlab.options.offscreen = True
        scene = mlab.figure(bgcolor=(1,1,1), fgcolor=(0, 0, 0), size=(1, 1))
        obj = mlab.pipeline.open(obj)
    
    if filter_names is None:
        filter_names = []

    source = obj
    while not (isinstance(source, Source)
               and not isinstance(source, Filter)):
        source = source.parent

    dm = dataset_manager.DatasetManager(dataset=source.outputs[0])

    ranges = {}
    for attr in ['point_scalars', 'point_vectors', 'point_tensors',
                 'cell_scalars', 'cell_vectors', 'cell_tensors']:
        family, kind = attr.split('_')

        data = getattr(dm, attr)

        if use_names is None:
            names = data.keys()
        else:
            names = use_names

        if not return_only and (set(data.keys()).intersection(names)):
            print family, kind

        for key, arr in data.iteritems():
            if (key in filter_names) or (key not in names): continue

            shape = arr.shape
            if arr.ndim == 1:
                arr = arr[:,np.newaxis]
            norm = np.sqrt(np.sum(arr*arr, axis=1))

            ranges[key] = (family, kind, shape, np.min(arr), np.max(arr),
                           np.min(norm), np.max(norm))

            if not return_only:
                aux = (key,) + ranges[key][2:]
                print '"%s" %s range: %s %s l2 norm range: %s %s' % aux

    return ranges

########NEW FILE########
__FILENAME__ = viewer
import os
import shutil, tempfile

import numpy as nm

try:
    from enthought.traits.api \
         import HasTraits, Instance, Button, Int, Float, Bool, on_trait_change
    from enthought.traits.ui.api \
         import View, Item, Heading, Group, HGroup, VGroup, Handler, spring
    from  enthought.traits.ui.editors.range_editor import RangeEditor
    from enthought.tvtk.pyface.scene_editor import SceneEditor
    from enthought.mayavi.tools.mlab_scene_model import MlabSceneModel
    from enthought.mayavi.core.ui.mayavi_scene import MayaviScene

except ImportError:
    from traits.api \
         import HasTraits, Instance, Button, Int, Float, Bool, on_trait_change
    from traitsui.api \
         import View, Item, Heading, Group, HGroup, VGroup, Handler, spring
    from traitsui.editors.range_editor import RangeEditor
    from tvtk.pyface.scene_editor import SceneEditor
    from mayavi.tools.mlab_scene_model import MlabSceneModel
    from mayavi.core.ui.mayavi_scene import MayaviScene


from dataset_manager import DatasetManager

from sfepy.base.base import (insert_as_static_method, output, assert_,
                             get_arguments, get_default, Struct, basestr)
from sfepy.base.ioutils import ensure_path
from sfepy.linalg import cycle
from sfepy.solvers.ts import get_print_info
from sfepy.postprocess.utils import mlab, get_data_ranges
from sfepy.postprocess.sources import create_file_source, FileSource

def get_glyphs_scale_factor(rng, rel_scaling, bbox):
    delta = rng[1] - rng[0]
    dx = nm.max((bbox[1,:] - bbox[0,:]))
    if rel_scaling is None:
        rel_scaling = 0.02 # -> delta fits 50x into dx.
    return rel_scaling * dx / delta

def add_surf(obj, position, opacity=1.0):
    surf = mlab.pipeline.surface(obj, opacity=opacity)
    surf.actor.actor.position = position
    return surf

def add_scalar_cut_plane(obj, position, normal, opacity=1.0):
    scp = mlab.pipeline.scalar_cut_plane(obj, opacity=opacity)
    scp.actor.actor.position = position
    scp.implicit_plane.visible = False
    scp.implicit_plane.normal = normal

    return scp

def add_vector_cut_plane(obj, position, normal, bbox, rel_scaling=None,
                         scale_factor='auto', clamping=False,
                         opacity=1.0):
    vcp = mlab.pipeline.vector_cut_plane(obj, opacity=opacity)
    if scale_factor == 'auto':
        rng = vcp.glyph.glyph.range
        scale_factor = get_glyphs_scale_factor(rng, rel_scaling, bbox)

    vcp.glyph.color_mode = 'color_by_vector'
    vcp.glyph.scale_mode = 'scale_by_vector'
    vcp.glyph.glyph.clamping = clamping
    vcp.glyph.glyph.scale_factor = scale_factor
    vcp.glyph.glyph_source.glyph_position = 'tail'

    vcp.actor.actor.position = position
    vcp.implicit_plane.normal = normal
    vcp.implicit_plane.widget.enabled = False

    return vcp

def add_iso_surface(obj, position, contours=10, opacity=1.0):
    obj = mlab.pipeline.iso_surface(obj, contours=contours, opacity=opacity)
    obj.actor.actor.position = position
    return obj

def add_subdomains_surface(obj, position, mat_id_name='mat_id',
                           threshold_limits=(None, None), **kwargs):
    dm = DatasetManager(dataset=obj.outputs[0])
    mat_id = dm.cell_scalars[mat_id_name]

    rm = mat_id.min(), mat_id.max()

    active = mlab.pipeline.set_active_attribute(obj)
    active.cell_scalars_name = mat_id_name

    aa = mlab.pipeline.set_active_attribute(obj)
    aa.cell_scalars_name = mat_id_name

    threshold = mlab.pipeline.threshold(aa)
    threshold.threshold_filter.progress = 1.0
    if threshold_limits[0] is not None:
        threshold.lower_threshold = threshold_limits[0] + 0.1
    if threshold_limits[1] is not None:
        threshold.upper_threshold = threshold_limits[1] - 0.1

    threshold.auto_reset_lower = False
    threshold.auto_reset_upper = False

    surface = mlab.pipeline.surface(threshold, opacity=0.3)
    surface.actor.actor.position = position

    module_manager = surface.parent
    lm = module_manager.scalar_lut_manager
    lm.lut_mode = 'Blues'
    if (rm[1] - rm[0]) == 1:
        lm.reverse_lut = True

    surface2 = mlab.pipeline.surface(active, opacity=0.2)
    surface2.actor.actor.position = position

    module_manager = surface2.parent
    module_manager.scalar_lut_manager.lut_mode = 'Blues'

    return surface, surface2

def add_glyphs(obj, position, bbox, rel_scaling=None,
               scale_factor='auto', clamping=False, color=None):

    glyphs = mlab.pipeline.glyph(obj, mode='2darrow', scale_mode='vector',
                                 color=color, opacity=1.0)
    if scale_factor == 'auto':
        rng = glyphs.glyph.glyph.range
        scale_factor = get_glyphs_scale_factor(rng, rel_scaling, bbox)

    glyphs.glyph.color_mode = 'color_by_vector'
    glyphs.glyph.scale_mode = 'scale_by_vector'
    glyphs.glyph.glyph.clamping = clamping
    glyphs.glyph.glyph.scale_factor = scale_factor
    glyphs.glyph.glyph_source.glyph_position = 'tail'
    glyphs.actor.actor.position = position
    return glyphs

def add_text(obj, position, text, width=None, color=(0, 0, 0)):
    if width is None:
        width = 0.02 * len(text)
    t = mlab.text(x=position[0], y=position[1], text=text,
                  z=position[2], color=color, width=width)
    return t

def get_position_counts(n_data, layout):
    n_col = max(1.0, min(5.0, nm.fix(nm.sqrt(n_data))))
    n_row = int(nm.ceil(n_data / n_col))
    n_col = int(n_col)
    if layout == 'rowcol':
        n_row, n_col = n_col, n_row
    elif layout == 'row':
        n_row, n_col = 1, n_data
    elif layout == 'col':
        n_row, n_col = n_data, 1
    elif layout == 'colrow':
        pass
    else:
        num = int(layout[3:])
        n_col = num
        n_row = int(nm.ceil(n_data / float(n_col)))
        if layout[:3] == 'col':
            n_row, n_col = n_col, n_row

    return n_row, n_col

def get_opacities(opacity):
    """
    Provide defaults for all supported opacity settings.
    """
    defaults = {
        'wireframe' : 0.05,
        'scalar_cut_plane' : 0.5,
        'vector_cut_plane' : 0.5,
        'surface' : 1.0,
        'iso_surface' : 0.3,
        'arrows_surface' : 0.3,
        'glyphs' : 1.0
    }
    if isinstance(opacity, dict):
        opacities = opacity
        default = None

    else:
        opacities = {}
        default = opacity

    for key in ['wireframe', 'scalar_cut_plane', 'vector_cut_plane',
                'surface', 'iso_surface', 'arrows_surface', 'glyphs']:
        if default is None:
            opacities.setdefault(key, defaults[key])

        else:
            opacities.setdefault(key, default)

    return opacities

class Viewer(Struct):
    """
    Class to automate visualization of various data using Mayavi.

    It can use any format that mlab.pipeline.open() handles, e.g. a VTK format.
    After opening a data file, all data (point, cell, scalars, vectors,
    tensors) are plotted in a grid layout.

    Parameters:

    watch : bool
        If True, watch the file for changes and update the mayavi
        pipeline automatically.
    ffmpeg_options : str
        The ffmpeg animation encoding options.
    output_dir : str
        The output directory, where view snapshots will be saved.

    Examples:

    >>> view = Viewer('file.vtk')
    >>> view() # view with default parameters
    >>> view(layout='col') # use column layout
    """
    def __init__(self, filename, watch=False, ffmpeg_options=None,
                 output_dir='.', offscreen=False):
        Struct.__init__(self,
                        filename=filename,
                        watch=watch,
                        ffmpeg_options=ffmpeg_options,
                        output_dir=output_dir,
                        offscreen=offscreen,
                        scene=None,
                        gui=None)
        self.options = get_arguments(omit=['self'])

        if mlab is None:
            output('mlab cannot be imported, check your installation!')
            insert_as_static_method(self.__class__, '__call__', self.call_empty)
        else:
            insert_as_static_method(self.__class__, '__call__', self.call_mlab)

    def get_data_names(self, source=None, detailed=False):
        if source is None:
            mlab.options.offscreen = self.offscreen
            mlab.figure(fgcolor=self.fgcolor, bgcolor=self.bgcolor,
                        size=(1, 1))
            source = mlab.pipeline.open(self.filename)
        point_scalar_names = sorted(source._point_scalars_list[:-1])
        point_vector_names = sorted(source._point_vectors_list[:-1])
        point_tensor_names = sorted(source._point_tensors_list[:-1])
        cell_scalar_names = sorted(source._cell_scalars_list[:-1])
        cell_vector_names = sorted(source._cell_vectors_list[:-1])
        cell_tensor_names = sorted(source._cell_tensors_list[:-1])

        p_names = [['point', 'scalars', name] for name in point_scalar_names]
        p_names += [['point', 'vectors', name] for name in point_vector_names]
        p_names += [['point', 'tensors', name] for name in point_tensor_names]
        c_names = [['cell', 'scalars', name] for name in cell_scalar_names]
        c_names += [['cell', 'vectors', name] for name in cell_vector_names]
        c_names += [['cell', 'tensors', name] for name in cell_tensor_names]

        if detailed:
            return p_names, c_names
        else:
            return p_names + c_names

    def set_source_filename(self, filename):
        self.filename = filename
        try:
            self.file_source.set_filename(filename, self.scene.children[0])
        except (AttributeError, IndexError): # No sources yet.
            pass

    def save_image(self, filename):
        """Save a snapshot of the current scene."""
        name = os.path.join(self.output_dir, filename)
        ensure_path(name)

        output('saving %s...' % name)
        self.scene.scene.save(name)
        output('...done')

    def get_animation_info(self, filename, add_output_dir=True, last_step=None):
        if last_step is None:
            steps, _ = self.file_source.get_ts_info()
            last_step = steps[-1]

        base, ext = os.path.splitext(filename)
        if add_output_dir:
            base = os.path.join(self.output_dir, base)

        _, _, suffix1 = get_print_info(last_step + 1)
        return base, suffix1 + '_%.5e', ext

    def save_animation(self, filename, steps=None, times=None):
        """
        Animate the current scene view for the selected time steps or times and
        save a snapshot of each view.
        """
        all_steps, all_times = self.file_source.get_ts_info()

        if steps is None:
            if times is None:
                iis = nm.arange(len(all_steps), dtype=nm.int32)

            else:
                iis = nm.searchsorted(all_times, times)
                iis = nm.clip(iis, 0, len(all_steps) - 1)
                if not len(iis): iis = [0]

        else:
            iis = nm.searchsorted(all_steps, steps)
            iis = nm.clip(iis, 0, len(all_steps) - 1)

        last_step = all_steps[iis[-1]]
        base, suffix, ext = self.get_animation_info(filename,
                                                    add_output_dir=False,
                                                    last_step=last_step)
        for ii in iis:
            step = all_steps[ii]
            if len(all_times):
                time = all_times[ii]
                aux = (suffix % (step, time)).replace('.', '_')

            else:
                aux = (suffix % (step, 0.0)).replace('.', '_')

            name = '.'.join((base, aux, ext[1:]))
            output('%d: %s' % (step, name))

            self.set_step.step = step
            self.save_image(name)

    def encode_animation(self, filename, format, ffmpeg_options=None):
        if ffmpeg_options is None:
            ffmpeg_options = '-r 10 -sameq'

        base, _, ext = self.get_animation_info(filename)
        anim_name = '.'.join((base, format))
        cmd = 'ffmpeg %s -i %s %s' \
              % (ffmpeg_options, '.'.join((base, '%*', ext[1:])), anim_name)
        output('creating animation "%s"...' % anim_name)
        output('using command:')
        output(cmd)
        try:
            os.system(cmd)
        except:
            output('...warning: animation not created, is ffmpeg installed?')
        else:
            output('...done')

        return anim_name

    def get_size_hint(self, layout, resolution=None):
        if resolution is not None:
            size = resolution
        elif layout == 'rowcol':
            size = (800, 600)
        elif layout == 'row':
            size = (1000, 600)
        elif layout == 'col':
            size = (600, 1000)
        elif layout == 'colrow':
            size = (600, 800)
        else:
            size = (800, 800)

        return size

    def build_mlab_pipeline(self, file_source=None, is_3d=False,
                            layout='rowcol',
                            scalar_mode='iso_surface',
                            vector_mode='arrows_norm',
                            rel_scaling=None, clamping=False,
                            ranges=None, is_scalar_bar=False,
                            is_wireframe=False, opacity=None,
                            subdomains_args=None,
                            rel_text_width=None,
                            filter_names=None, group_names=None,
                            only_names=None,
                            domain_specific=None, **kwargs):
        """Sets self.source, self.is_3d_data """
        opacities = get_opacities(opacity)

        file_source = get_default(file_source, self.file_source,
                                  'file_source not set!')
        filter_names = get_default(filter_names, [])
        domain_specific = get_default(domain_specific, {})

        if subdomains_args is not None:
            is_subdomains = True
            file_source.setup_mat_id(subdomains_args['mat_id_name'],
                                     subdomains_args['single_color'])

        else:
            is_subdomains = False

        self.source = source = self.file_source()
        data_ranges = get_data_ranges(source, return_only=True)

        # Hack to prevent mayavi switching to point scalar on source
        # change.
        if len(source._point_scalars_list):
            source.point_scalars_name = ''

        bbox = file_source.get_bounding_box()
        dx = 1.1 * (bbox[1,:] - bbox[0,:])

        float_eps = nm.finfo(nm.float64).eps
        self.is_3d_data = abs(dx[2]) > (10.0 * float_eps)

        p_names, c_names = self.get_data_names(source, detailed=True)
        names = p_names + c_names
        if only_names is None:
            names = [ii for ii in names if ii[2] not in filter_names]

        else:
            # Use order of only_names.
            _names = []
            aux = [ii[2] for ii in names]
            for name in only_names:
                try:
                    ii = aux.index(name)
                except ValueError:
                    output('ignoring nonexistent name: %s' % name)
                    continue
                _names.append(names[ii])

            if len(_names) != len(only_names):
                output('warning: some names were not found!')
            if not len(_names):
                raise ValueError('no names were found! (%s not in %s)'
                                 % (only_names, [name[2] for name in names]))
            names = _names

        if group_names is not None:
            ndict = {}
            for name in names:
                ndict[name[2]] = name

            repeat = []
            _names = []
            aux = set(name[2] for name in names)
            for group in group_names:
                aux.difference_update(group)
                repeat.append(len(group))
                for name in group:
                    _names.append(ndict[name])

            repeat.extend([1] * len(aux))
            n_pos = len(repeat)

            names = _names
            n_data = len(names)

        else:
            n_pos = n_data = len(names)
            repeat = [1] * n_data

        def _make_iterator(repeat, n_row, n_col):
            ii = 0
            for ij, iric in enumerate(cycle((n_row, n_col))):
                ir, ic = iric
                if ij < len(repeat):
                    for ik in xrange(repeat[ij]):
                        yield ii, ir, ic
                        ii += 1

        n_row, n_col = get_position_counts(n_pos, layout)
        if layout[:3] == 'col':
            iterator = _make_iterator(repeat, n_col, n_row)
        else:
            iterator = _make_iterator(repeat, n_row, n_col)

        max_label_width = nm.max([len(ii[2]) for ii in names] + [5]) + 2

        if c_names:
            ctp = mlab.pipeline.cell_to_point_data(source)

        else:
            ctp = None

        self.scalar_bars = []

        for ii, ir, ic in iterator:
            if layout[:3] == 'col':
                ir, ic = ic, ir
            if ii == n_data: break
            family, kind, name = names[ii]
            data_range = data_ranges[name]

            is_magnitude = False
            position = nm.array([dx[0] * ic, dx[1] * (n_row - ir - 1), 0])
            output(family, kind, name, 'at', position)
            output('range: %.2e %.2e l2 norm range: %.2e %.2e' % data_range[3:])

            if name in domain_specific:
                if ctp is None:
                    ctp = mlab.pipeline.cell_to_point_data(source)

                ds = domain_specific[name]
                out = ds(source, ctp, bbox, position, family, kind, name)
                if len(out) == 4:
                    kind, name, active, bars = out
                    self.scalar_bars.extend(bars)

                else:
                    kind, name, active = out

            elif kind == 'scalars':
                if family == 'point':
                    active = mlab.pipeline.set_active_attribute(source)

                else:
                    if is_3d and ('iso_surface' in scalar_mode):
                        active = mlab.pipeline.set_active_attribute(ctp)

                    else:
                        active = mlab.pipeline.set_active_attribute(source)

                setattr(active, '%s_%s_name' % (family, kind), name)

                if is_3d:
                    if 'cut_plane' in scalar_mode:
                        op = opacities['scalar_cut_plane']
                        add_scalar_cut_plane(active, position, [1, 0, 0],
                                             opacity=op)
                        add_scalar_cut_plane(active, position, [0, 1, 0],
                                             opacity=op)
                        add_scalar_cut_plane(active, position, [0, 0, 1],
                                             opacity=op)
                    if 'iso_surface' in scalar_mode:
                        active.point_scalars_name = name
                        add_iso_surface(active, position,
                                        opacity=opacities['iso_surface'])
                else:
                    add_surf(active, position, opacity=opacities['surface'])

            elif kind == 'vectors':
                if family == 'point':
                    active = mlab.pipeline.set_active_attribute(source)
                else:
                    active = mlab.pipeline.set_active_attribute(ctp)
                active.point_vectors_name = name

                if (ranges is not None) and (name in ranges):
                    sf = get_glyphs_scale_factor(ranges[name],
                                                 rel_scaling, bbox)
                else:
                    sf = None

                if 'arrows' in vector_mode:
                    glyphs = add_glyphs(active, position, bbox,
                                        rel_scaling=rel_scaling,
                                        clamping=clamping)
                    if sf is not None:
                        glyphs.glyph.glyph.scale_factor = sf

                if 'warp' in vector_mode:
                    active = mlab.pipeline.warp_vector(active)
                    active.filter.scale_factor = rel_scaling

                if 'norm' in vector_mode:
                    active = mlab.pipeline.extract_vector_norm(active)
                    if 'arrows' in vector_mode:
                        op = opacities['arrows_surface']
                    else:
                        op = opacities['surface']
                    add_surf(active, position, opacity=op)

                if is_3d:
                    if 'cut_plane' in vector_mode:
                        op = opacities['vector_cut_plane']
                        for normal in [[1, 0, 0], [0, 1, 0], [0, 0, 1]]:
                            vcp = add_vector_cut_plane(active,
                                                       position, normal, bbox,
                                                       rel_scaling=rel_scaling,
                                                       clamping=clamping,
                                                       opacity=op)
                            if sf is not None:
                                vcp.glyph.glyph.scale_factor = sf

            elif kind == 'tensors':
                if family == 'point':
                    active = mlab.pipeline.set_active_attribute(source)
                else:
                    active = mlab.pipeline.set_active_attribute(ctp)
                active.point_tensors_name = name

                active = mlab.pipeline.extract_tensor_components(active)
                is_magnitude = True
                if is_3d:
                    if 'cut_plane' in scalar_mode:
                        op = opacities['scalar_cut_plane']
                        add_scalar_cut_plane(active, position, [1, 0, 0],
                                             opacity=op)
                        add_scalar_cut_plane(active, position, [0, 1, 0],
                                             opacity=op)
                        add_scalar_cut_plane(active, position, [0, 0, 1],
                                             opacity=op)
                    if 'iso_surface' in scalar_mode:
                        add_iso_surface(active, position,
                                        opacity=opacities['iso_surface'])
                else:
                    add_surf(active, position, opacity=opacities['surface'])

            else:
                raise ValueError('bad kind! (%s)' % kind)

            if (ranges is not None) and (name in ranges):
                mm = active.children[0]
                if (kind == 'scalars') or (kind == 'tensors'):
                    lm = mm.scalar_lut_manager

                else: # kind == 'vectors':
                    lm = mm.vector_lut_manager

                lm.use_default_range = False
                lm.data_range = ranges[name]

            if is_subdomains:
                add_subdomains_surface(source, position, **subdomains_args)

            if is_wireframe:
                surf = add_surf(source, position,
                                opacity=opacities['wireframe'])
                surf.actor.property.representation = 'wireframe'
                surf.actor.mapper.scalar_visibility = False

            if is_scalar_bar:
                mm = active.children[0]
                if (kind == 'scalars') or (kind == 'tensors'):
                    lm = mm.scalar_lut_manager

                else: # kind == 'vectors':
                    lm = mm.vector_lut_manager

                self.scalar_bars.append((family, name, lm))

            if rel_text_width > (10 * float_eps):
                position[2] = 0.5 * dx[2]
                if is_magnitude:
                    name = '|%s|' % name
                add_text(active, position, name,
                         float(rel_text_width * len(name))
                         / float(max_label_width), color=self.fgcolor)

        if not names:
            # No data, so just show the mesh.
            surf = add_surf(source, (0.0, 0.0, 0.0),
                            opacity=opacities['surface'])
            surf.actor.property.color = (0.8, 0.8, 0.8)

            if is_subdomains:
                add_subdomains_surface(source, (0.0, 0.0, 0.0),
                                       **subdomains_args)

            if is_wireframe:
                surf = add_surf(source, (0.0, 0.0, 0.0),
                                opacity=opacities['wireframe'])
                surf.actor.property.representation = 'wireframe'
                surf.actor.mapper.scalar_visibility = False

    def render_scene(self, scene, options):
        """
        Render the scene, preferably after it has been activated.
        """
        scene.scene.disable_render = True

        self.build_mlab_pipeline(**options)
        self.source.update() # Force source update to see e.g. streamlines.
        scene.scene.reset_zoom()

        view = options['view']
        if view is None:
            if options['is_3d'] or self.is_3d_data:
                self.view = (45, 45)
            else:
                self.view = (0, 0)
        else:
            self.view = view
        self.roll = options['roll']

        scene.scene.disable_render = False

        anti_aliasing = options['anti_aliasing']
        if anti_aliasing is not None:
            scene.scene.anti_aliasing_frames = anti_aliasing

    def show_scalar_bars(self, scalar_bars):
        for ii, (family, name, lm) in enumerate(scalar_bars):
            x0, y0 = 0.03, 1.0 - 0.01 - float(ii) / 15.0 - 0.07
            x1, y1 = 0.4, 0.07
            lm.scalar_bar_representation.position = [x0, y0]
            lm.scalar_bar_representation.position2 = [x1, y1]
            lm.number_of_labels = 5
            lm.scalar_bar.orientation = 'horizontal'
            lm.data_name = '%s: %s' % (family, name)
            lm.scalar_bar.title_text_property.font_size = 20
            lm.scalar_bar.label_text_property.font_size = 16
            lm.show_scalar_bar = True

    def reset_view(self):
        mlab.view(*self.view)
        mlab.roll(self.roll)
        self.scene.scene.camera.zoom(1.0)

    def __call__(self, *args, **kwargs):
        """
        This is either call_mlab() or call_empty().
        """
        pass

    def call_empty(self, *args, **kwargs):
        pass

    def call_mlab(self, scene=None, show=True, is_3d=False,
                  view=None, roll=None,
                  fgcolor=(0.0, 0.0, 0.0), bgcolor=(1.0, 1.0, 1.0),
                  layout='rowcol', scalar_mode='iso_surface',
                  vector_mode='arrows_norm', rel_scaling=None, clamping=False,
                  ranges=None, is_scalar_bar=False, is_wireframe=False,
                  opacity=None, subdomains_args=None, rel_text_width=None,
                  fig_filename='view.png', resolution=None,
                  filter_names=None, only_names=None, group_names=None,
                  step=None, time=None,
                  anti_aliasing=None, domain_specific=None):
        """
        By default, all data (point, cell, scalars, vectors, tensors)
        are plotted in a grid layout, except data named 'node_groups',
        'mat_id' which are usually not interesting.

        Parameters
        ----------
        show : bool
            Call mlab.show().
        is_3d : bool
            If True, use scalar cut planes instead of surface for certain
            datasets. Also sets 3D view mode.
        view : tuple
            Azimuth, elevation angles, distance and focal point as in
            `mlab.view()`.
        roll : float
            Roll angle tuple as in mlab.roll().
        fgcolor : tuple of floats (R, G, B)
            The foreground color, that is the color of all text
            annotation labels (axes, orientation axes, scalar bar
            labels).
        bgcolor : tuple of floats (R, G, B)
            The background color.
        layout : str
            Grid layout for placing the datasets. Possible values are:
            'row', 'col', 'rowcol', 'colrow'.
        scalar_mode : str
             Mode for plotting scalars and tensor magnitudes, one of
             'cut_plane', 'iso_surface', 'both'.
        vector_mode : str
             Mode for plotting vectors, one of 'arrows', 'norm', 'arrows_norm',
             'warp_norm'.
        rel_scaling : float
            Relative scaling of glyphs for vector datasets.
        clamping : bool
            Clamping for vector datasets.
        ranges : dict
            List of data ranges in the form {name : (min, max), ...}.
        is_scalar_bar : bool
            If True, show a scalar bar for each data.
        is_wireframe : bool
            If True, show a wireframe of mesh surface bar for each data.
        opacity : float
            Global surface and wireframe opacity setting in [0.0, 1.0],
        subdomains_args : tuple
            Tuple of (mat_id_name, threshold_limits, single_color), see
            :func:`add_subdomains_surface`, or None.
        rel_text_width : float
            Relative text width.
        fig_filename : str
            File name for saving the resulting scene figure.
        resolution : tuple
            Scene and figure resolution. If None, it is set
            automatically according to the layout.
        filter_names : list of strings
            Omit the listed datasets. If None, it is initialized to
            ['node_groups', 'mat_id']. Pass [] if you need no filtering.
        only_names : list of strings
            Draw only the listed datasets. If None, it is initialized all names
            besides those in filter_names.
        group_names : list of tuples
            List of data names in the form [(name1, ..., nameN), (...)]. Plots
            of data named in each group are superimposed. Repetitions of names
            are possible.
        step : int, optional
            If not None, the time step to display. The closest higher step is
            used if the desired one is not available. Has precedence over
            `time`.
        time : float, optional
            If not None, the time of the time step to display. The closest
            higher time is used if the desired one is not available.
        anti_aliasing : int
            Value of anti-aliasing.
        domain_specific : dict
            Domain-specific drawing functions and configurations.
        """
        self.fgcolor = fgcolor
        self.bgcolor = bgcolor

        if filter_names is None:
            filter_names = ['node_groups', 'mat_id']

        if rel_text_width is None:
            rel_text_width = 0.02

        if isinstance(scalar_mode, basestr):
            if scalar_mode == 'both':
                scalar_mode = ('cut_plane', 'iso_surface')
            elif scalar_mode in ('cut_plane', 'iso_surface'):
                scalar_mode = (scalar_mode,)
            else:
                raise ValueError('bad value of scalar_mode parameter! (%s)'
                                 % scalar_mode)
        else:
            for sm in scalar_mode:
                if not sm in ('cut_plane', 'iso_surface'):
                    raise ValueError('bad value of scalar_mode parameter! (%s)'
                                     % sm)

        if isinstance(vector_mode, basestr):
            if vector_mode == 'arrows_norm':
                vector_mode = ('arrows', 'norm')
            elif vector_mode == 'warp_norm':
                vector_mode = ('warp', 'norm')
            elif vector_mode in ('arrows', 'norm'):
                vector_mode = (vector_mode,)
            elif vector_mode == 'cut_plane':
                if is_3d:
                    vector_mode = ('cut_plane',)
                else:
                    vector_mode = ('arrows',)
            else:
                raise ValueError('bad value of vector_mode parameter! (%s)'
                                 % vector_mode)
        else:
            for vm in vector_mode:
                if not vm in ('arrows', 'norm', 'warp'):
                    raise ValueError('bad value of vector_mode parameter! (%s)'
                                     % vm)

        mlab.options.offscreen = self.offscreen

        self.size_hint = self.get_size_hint(layout, resolution=resolution)

        is_new_scene = False

        if scene is not None:
            if scene is not self.scene:
                is_new_scene = True
                self.scene = scene
            gui = None

        else:
            if (self.scene is not None) and (not self.scene.running):
                self.scene = None

            if self.scene is None:
                if self.offscreen:
                    gui = None
                    scene = mlab.figure(fgcolor=fgcolor, bgcolor=bgcolor,
                                        size=self.size_hint)

                else:
                    gui = ViewerGUI(viewer=self,
                                    fgcolor=fgcolor, bgcolor=bgcolor)
                    scene = gui.scene.mayavi_scene

                if scene is not self.scene:
                    is_new_scene = True
                    self.scene = scene

            else:
                gui = self.gui
                scene = self.scene

        self.engine = mlab.get_engine()
        self.engine.current_scene = self.scene

        self.gui = gui

        self.file_source = create_file_source(self.filename, watch=self.watch,
                                              offscreen=self.offscreen)
        steps, times = self.file_source.get_ts_info()
        has_several_times = len(times) > 0
        has_several_steps = has_several_times or (len(steps) > 0)

        if gui is not None:
            gui.has_several_steps = has_several_steps

        self.reload_source = reload_source = ReloadSource()
        reload_source._viewer = self
        reload_source._source = self.file_source

        if has_several_steps:
            self.set_step = set_step = SetStep()
            set_step._viewer = self
            set_step._source = self.file_source
            if step is not None:
                step = step if step >= 0 else steps[-1] + step + 1
                assert_(steps[0] <= step <= steps[-1],
                        msg='invalid time step! (%d <= %d <= %d)'
                        % (steps[0], step, steps[-1]))
                set_step.step = step

            elif time is not None:
                assert_(times[0] <= time <= times[-1],
                        msg='invalid time! (%e <= %e <= %e)'
                        % (times[0], time, times[-1]))
                set_step.time = time

            else:
                set_step.step = steps[0]

            if self.watch:
                self.file_source.setup_notification(set_step, 'file_changed')

            if gui is not None:
                gui.set_step = set_step

        else:
            if self.watch:
                self.file_source.setup_notification(reload_source,
                                                    'reload_source')

        self.options.update(get_arguments(omit = ['self', 'file_source']))

        if gui is None:
            self.render_scene(scene, self.options)
            self.reset_view()
            if is_scalar_bar:
                self.show_scalar_bars(self.scalar_bars)

        else:
            traits_view = View(
                Item('scene', editor=SceneEditor(scene_class=MayaviScene),
                     show_label=False,
                     width=self.size_hint[0], height=self.size_hint[1],
                     style='custom',
                ),
                Group(Item('set_step', defined_when='set_step is not None',
                           show_label=False, style='custom'),
                ),
                HGroup(spring,
                       Item('button_make_snapshots_steps', show_label=False,
                            enabled_when='has_several_steps == True'),
                       Item('button_make_animation_steps', show_label=False,
                            enabled_when='has_several_steps == True'),
                       spring,
                       Item('button_make_snapshots_times', show_label=False,
                            enabled_when='has_several_steps == True'),
                       Item('button_make_animation_times', show_label=False,
                            enabled_when='has_several_steps == True'),
                       spring,),
                HGroup(spring,
                       Item('button_reload', show_label=False),
                       Item('button_view', show_label=False),
                       Item('button_quit', show_label=False)),
                resizable=True,
                buttons=[],
                handler=ClosingHandler(),
            )

            if is_new_scene:
                if show:
                    gui.configure_traits(view=traits_view)

                else:
                    gui.edit_traits(view=traits_view)

        return gui

class SetStep(HasTraits):

    _viewer = Instance(Viewer)
    _source = Instance(FileSource)

    seq_start = Int(0)
    seq_stop = Int(-1)
    seq_step = Int(1)

    seq_t0 = Float
    seq_t1 = Float
    seq_dt = Float
    seq_n_step = Int

    _step_editor = RangeEditor(low_name='step_low',
                               high_name='step_high',
                               label_width=28,
                               auto_set=True,
                               mode='slider')
    step = None
    step_low = Int
    step_high = Int

    _time_editor = RangeEditor(low_name='time_low',
                               high_name='time_high',
                               label_width=28,
                               auto_set=True,
                               mode='slider')
    time = None
    time_low = Float
    time_high = Float

    file_changed = Bool(False)

    is_adjust = False

    traits_view = View(
        Item('step', defined_when='step is not None',
             editor=_step_editor),
        Item('time', defined_when='time is not None',
             editor=_time_editor),
        HGroup(Heading('steps:'),
               Item('seq_start', label='start'),
               Item('seq_stop', label='stop'),
               Item('seq_step', label='step'),
               Heading('times:'),
               Item('seq_t0', label='t0'),
               Item('seq_t1', label='t1'),
               Item('seq_dt', label='dt'),
               Item('seq_n_step', label='n_step')),
    )

    def __source_changed(self, old, new):
        steps = self._source.steps
        if len(steps):
            self.add_trait('step', Int(0))
            self.step_low, self.step_high = steps[0], steps[-1]

        times = self._source.times
        if len(times):
            self.add_trait('time', Float(0.0))
            self.time_low, self.time_high = times[0], times[-1]

    def _step_changed(self, old, new):
        if new == old: return
        if not self.is_adjust:
            step, time = self._source.get_step_time(step=new)
            self.is_adjust = True
            self.step = step
            self.time = time
            self.is_adjust = False

            self._viewer.set_source_filename(self._source.filename)

    def _time_changed(self, old, new):
        if new == old: return
        if not self.is_adjust:
            step, time = self._source.get_step_time(time=new)
            self.is_adjust = True
            self.step = step
            self.time = time
            self.is_adjust = False

            self._viewer.set_source_filename(self._source.filename)

    def _file_changed_changed(self, old, new):
        if new == True:
            steps = self._source.steps
            if len(steps):
                self.step_low, self.step_high = steps[0], steps[-1]

            times = self._source.times
            if len(times):
                self.time_low, self.time_high = times[0], times[-1]

        self.file_changed = False

    @on_trait_change('step_high, time_high')
    def init_seq_selection(self, name, new):
        self.seq_t0 = self.time_low
        self.seq_t1 = self.time_high
        self.seq_n_step = self.step_high - self.step_low + 1
        self.seq_dt = (self.seq_t1 - self.seq_t0) / self.seq_n_step

        self.seq_start = self.step_low
        self.seq_stop = self.step_high + 1

        if name == 'time_high':
            self.on_trait_change(self.init_seq_selection, 'time_high',
                                 remove=True)

    def _seq_n_step_changed(self, old, new):
        if new == old: return
        self.seq_dt = (self.seq_t1 - self.seq_t0) / self.seq_n_step

    def _seq_dt_changed(self, old, new):
        if new == old: return
        if self.seq_dt == 0.0: return
        n_step = int(round((self.seq_t1 - self.seq_t0) / self.seq_dt))
        self.seq_n_step = max(1, n_step)

class ReloadSource(HasTraits):

    _viewer = Instance(Viewer)
    _source = Instance(FileSource)

    reload_source = Bool(False)

    def _reload_source_changed(self, old, new):
        if new == True:
            ext = os.path.splitext(self._source.filename)[1]
            if ext == 'vtk':
                while 1:
                    # Wait for the file to be completely written.
                    fd = open(self._source.filename, 'r')
                    ch = fd.read(1)
                    fd.close()
                    if ch == '#':
                        break

            self._viewer.set_source_filename(self._source.filename)

        self.reload_source = False

def make_animation(filename, view, roll, anim_format, options,
                   steps=None, times=None, reuse_viewer=None):
    output_dir = tempfile.mkdtemp()

    viewer = Viewer(filename, watch=options.watch,
                    output_dir=output_dir,
                    offscreen=True)

    if reuse_viewer is None:
        viewer(show=False, is_3d=options.is_3d, view=view,
               roll=roll, layout=options.layout,
               scalar_mode=options.scalar_mode,
               vector_mode=options.vector_mode,
               rel_scaling=options.rel_scaling,
               clamping=options.clamping, ranges=options.ranges,
               is_scalar_bar=options.is_scalar_bar,
               is_wireframe=options.is_wireframe,
               opacity=options.opacity,
               subdomains_args=options.subdomains_args,
               rel_text_width=options.rel_text_width,
               fig_filename=options.fig_filename, resolution=options.resolution,
               filter_names=options.filter_names, only_names=options.only_names,
               group_names=options.group_names,
               anti_aliasing=options.anti_aliasing,
               domain_specific=options.domain_specific)

    else:
        viewer.file_source = reuse_viewer.file_source
        viewer.scene = reuse_viewer.scene
        viewer.set_step = reuse_viewer.set_step

    viewer.save_animation(options.fig_filename, steps=steps, times=times)

    op = os.path
    if anim_format != 'png':
        anim_name = viewer.encode_animation(options.fig_filename, anim_format,
                                            options.ffmpeg_options)

        shutil.move(anim_name, op.join(options.output_dir,
                                       op.split(anim_name)[1]))
        shutil.rmtree(output_dir)

    else:
        shutil.move(output_dir, op.join(options.output_dir, 'snapshots'))

    if reuse_viewer is None:
        mlab.close(viewer.scene)

class ClosingHandler(Handler):
    def object_button_quit_changed(self, info):
        info.ui.dispose()

class ViewerGUI(HasTraits):

    scene = Instance(MlabSceneModel, ())

    has_several_steps = Bool(False)
    viewer = Instance(Viewer)
    set_step = Instance(SetStep)
    button_reload = Button('reload')
    button_view = Button('print view')
    button_quit = Button('quit')
    button_make_animation_steps = Button('make animation')
    button_make_snapshots_steps = Button('make snapshots')
    button_make_animation_times = Button('make animation')
    button_make_snapshots_times = Button('make snapshots')

    @on_trait_change('scene.activated')
    def _post_init(self, name, old, new):
        viewer = self.viewer
        viewer.render_scene(self.scene, viewer.options)
        viewer.reset_view()
        viewer.show_scalar_bars(viewer.scalar_bars)

    def __init__(self, fgcolor=(0.0, 0.0, 0.0), bgcolor=(1.0, 1.0, 1.0),
                 **traits):
        HasTraits.__init__(self, **traits)
        scene = self.scene.scene

        scene.foreground = fgcolor
        scene.background = bgcolor

    def _button_reload_fired(self):
        self.viewer.reload_source.reload_source = True

    def _button_view_fired(self):
        self.scene.camera.print_traits()
        view = mlab.view()
        roll = mlab.roll()
        print 'view:', view
        print 'roll:', roll
        print 'as args: --view=%.2e,%.2e,%.2e,%.2e,%.2e,%.2e --roll=%.2e' \
              % (view[:3] + tuple(view[3]) + (roll,))

    def _button_make_animation_steps_fired(self):
        view = mlab.view()
        roll = mlab.roll()

        steps = nm.arange(self.set_step.seq_start,
                          self.set_step.seq_stop,
                          self.set_step.seq_step, dtype=nm.int32)
        make_animation(self.viewer.filename,
                       view, roll, 'avi',
                       Struct(**self.viewer.options), steps=steps,
                       reuse_viewer=self.viewer)

    def _button_make_snapshots_steps_fired(self):
        view = mlab.view()
        roll = mlab.roll()

        steps = nm.arange(self.set_step.seq_start,
                          self.set_step.seq_stop,
                          self.set_step.seq_step, dtype=nm.int32)
        make_animation(self.viewer.filename,
                       view, roll, 'png',
                       Struct(**self.viewer.options), steps=steps,
                       reuse_viewer=self.viewer)

    def _button_make_animation_times_fired(self):
        view = mlab.view()
        roll = mlab.roll()

        times = nm.arange(self.set_step.seq_t0,
                          self.set_step.seq_t1 + 0.01 * self.set_step.seq_dt,
                          self.set_step.seq_dt, dtype=nm.float64)
        make_animation(self.viewer.filename,
                       view, roll, 'avi',
                       Struct(**self.viewer.options), times=times,
                       reuse_viewer=self.viewer)

    def _button_make_snapshots_times_fired(self):
        view = mlab.view()
        roll = mlab.roll()

        times = nm.arange(self.set_step.seq_t0,
                          self.set_step.seq_t1 + 0.01 * self.set_step.seq_dt,
                          self.set_step.seq_dt, dtype=nm.float64)
        make_animation(self.viewer.filename,
                       view, roll, 'png',
                       Struct(**self.viewer.options), times=times,
                       reuse_viewer=self.viewer)

########NEW FILE########
__FILENAME__ = eigen
import time

import numpy as nm
import scipy.linalg as sla

from sfepy.base.base import output, get_default, try_imports, Struct
from sfepy.solvers.solvers import make_get_conf, Solver, EigenvalueSolver

def eig(mtx_a, mtx_b=None, n_eigs=None, eigenvectors=True,
        return_time=None, method='eig.scipy', **ckwargs):
    """
    Utility function that constructs an eigenvalue solver given by
    `method`, calls it and returns solution.
    """
    kwargs = {'name' : 'aux', 'kind' : method}
    kwargs.update(ckwargs)
    conf = Struct(**kwargs)
    solver = Solver.any_from_conf(conf)

    status = {}
    out = solver(mtx_a, mtx_b, n_eigs, eigenvectors, status)
    if return_time is not None:
        return_time[0] = status['time']

    return out

def standard_call(call):
    """
    Decorator handling argument preparation and timing for eigensolvers.
    """
    def _standard_call(self, mtx_a, mtx_b=None, n_eigs=None,
                       eigenvectors=None, status=None, conf=None, **kwargs):
        tt = time.clock()

        conf = get_default(conf, self.conf)
        mtx_a = get_default(mtx_a, self.mtx_a)
        mtx_b = get_default(mtx_b, self.mtx_b)
        n_eigs = get_default(n_eigs, self.n_eigs)
        eigenvectors = get_default(eigenvectors, self.eigenvectors)
        status = get_default(status, self.status)

        result = call(self, mtx_a, mtx_b, n_eigs, eigenvectors, status, conf,
                      **kwargs)

        ttt = time.clock() - tt
        if status is not None:
            status['time'] = ttt

        return result

    return _standard_call

class ScipyEigenvalueSolver(EigenvalueSolver):
    """
    SciPy-based solver for both dense and sparse problems (if `n_eigs`
    is given).
    """
    name = 'eig.scipy'

    def __init__(self, conf, **kwargs):
        EigenvalueSolver.__init__(self, conf, **kwargs)

    @standard_call
    def __call__(self, mtx_a, mtx_b=None, n_eigs=None, eigenvectors=None,
                 status=None, conf=None):

        if n_eigs is None:
            mtx_a, mtx_b = self._to_array(mtx_a, mtx_b)
            out = sla.eig(mtx_a, mtx_b, right=eigenvectors)
            if eigenvectors:
                eigs = out[0]
            else:
                eigs = out
            ii = nm.argsort(eigs)
            if eigenvectors:
                mtx_ev = out[1][:,ii]
                out = (eigs[ii], mtx_ev)
            else:
                out = (eigs,)
        else:
            try:
                from scipy.splinalg import eigen_symmetric
            except ImportError:
                eigen_symmetric = None

            try:
                from scipy.sparse.linalg.eigen.arpack import eigen_symmetric
            except ImportError:
                eigen_symmetric = None

            if eigen_symmetric is None:
                raise ImportError('cannot import eigen_symmetric!')

            out = eigen_symmetric(mtx_a, k=n_eigs, M=mtx_b)

        return out

class ScipySGEigenvalueSolver(ScipyEigenvalueSolver):
    """
    SciPy-based solver for dense symmetric problems.
    """
    name = 'eig.sgscipy'

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Missing items are set to default values.

        Example configuration, all items::

            solver_20 = {
                'name' : 'eigen2',
                'kind' : 'eig.sgscipy',

                'force_n_eigs' : True,
            }
        """
        get = make_get_conf(conf, kwargs)
        common = EigenvalueSolver.process_conf(conf)

        return Struct(force_n_eigs=get('force_n_eigs', False)) + common

    @standard_call
    def __call__(self, mtx_a, mtx_b=None, n_eigs=None, eigenvectors=None,
                 status=None, conf=None):
        """
        Notes
        -----
        Eigenvectors argument is ignored, as they are computed always.
        """
        import scipy.lib.lapack as ll

        if (n_eigs is None) or (conf.force_n_eigs):
            mtx_a, mtx_b = self._to_array(mtx_a, mtx_b)
            if nm.iscomplexobj(mtx_a):
                if mtx_b is None:
                    fun = ll.get_lapack_funcs(['heev'], arrays=(mtx_a,))[0]
                else:
                    fun = ll.get_lapack_funcs(['hegv'], arrays=(mtx_a,))[0]
            else:
                if mtx_b is None:
                    fun = ll.get_lapack_funcs(['syev'], arrays=(mtx_a,))[0]
                else:
                    fun = ll.get_lapack_funcs(['sygv'], arrays=(mtx_a,))[0]

            if mtx_b is None:
                out = fun(mtx_a)
            else:
                out = fun(mtx_a, mtx_b)

            if not eigenvectors:
                if n_eigs is None:
                    out = out[0]
                else:
                    out = out[0][:n_eigs]
            else:
                if n_eigs is None:
                    out = out[:-1]
                else:
                    out = (out[0][:n_eigs], out[1][:, :n_eigs])

        else:
            out = ScipyEigenvalueSolver.call(self, mtx_a, mtx_b, n_eigs,
                                             eigenvectors, status=status)
        return out

class LOBPCGEigenvalueSolver(EigenvalueSolver):
    """
    SciPy-based LOBPCG solver for sparse symmetric problems.
    """
    name = 'eig.scipy_lobpcg'

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Missing items are set to default values.

        Example configuration, all items::

            solver_2 = {
                'name' : 'lobpcg',
                'kind' : 'eig.scipy_lobpcg',

                'i_max' : 20,
                'n_eigs' : 5,
                'eps_a' : None,
                'largest' : True,
                'precond' : None,
                'verbosity' : 0,
            }
        """
        get = make_get_conf(conf, kwargs)
        common = EigenvalueSolver.process_conf(conf)

        return Struct(i_max=get('i_max', 20),
                      n_eigs=get('n_eigs', None),
                      eps_a=get('eps_a', None),
                      largest=get('largest', True),
                      precond=get('precond', None),
                      verbosity=get('verbosity', 0)) + common

    def __init__(self, conf, **kwargs):
        EigenvalueSolver.__init__(self, conf, **kwargs)

        from scipy.sparse.linalg.eigen import lobpcg
        self.lobpcg = lobpcg

    @standard_call
    def __call__(self, mtx_a, mtx_b=None, n_eigs=None, eigenvectors=None,
                 status=None, conf=None):

        if n_eigs is None:
            n_eigs = mtx_a.shape[0]
        else:
            n_eigs = min(n_eigs, mtx_a.shape[0])

        x = nm.zeros((mtx_a.shape[0], n_eigs), dtype=nm.float64)
        x[:n_eigs] = nm.eye(n_eigs, dtype=nm.float64)

        out = self.lobpcg(mtx_a, x, mtx_b,
                          M=conf.precond,
                          tol=conf.eps_a, maxiter=conf.i_max,
                          largest=conf.largest,
                          verbosityLevel=conf.verbosity)

        if not eigenvectors:
            out = out[0]

        return out

class PysparseEigenvalueSolver(EigenvalueSolver):
    """
    Pysparse-based eigenvalue solver for sparse symmetric problems.
    """
    name = 'eig.pysparse'

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Missing items are set to default values.

        Example configuration, all items::

            solver_2 = {
                'name' : 'eigen1',
                'kind' : 'eig.pysparse',

                'i_max' : 150,
                'eps_a' : 1e-5,
                'tau' : -10.0,
                'method' : 'qmrs',
                'verbosity' : 0,
                'strategy' : 1,
            }
        """
        get = make_get_conf(conf, kwargs)
        common = EigenvalueSolver.process_conf(conf)

        return Struct(i_max=get('i_max', 100),
                      n_eigs=get('n_eigs', 5),
                      eps_a=get('eps_a', 1e-5),
                      tau=get('tau', 0.0),
                      method=get('method', 'qmrs'),
                      verbosity=get('verbosity', 0),
                      strategy=get('strategy', 1)) + common

    @staticmethod
    def _convert_mat(mtx):
        from pysparse import spmatrix
        A = spmatrix.ll_mat(*mtx.shape)
        for i in xrange(mtx.indptr.shape[0] - 1):
            ii = slice(mtx.indptr[i], mtx.indptr[i+1])
            n_in_row = ii.stop - ii.start
            A.update_add_at(mtx.data[ii], [i] * n_in_row, mtx.indices[ii])
        return A

    def __init__(self, conf, **kwargs):
        EigenvalueSolver.__init__(self, conf, **kwargs)

    @standard_call
    def __call__(self, mtx_a, mtx_b=None, n_eigs=None,
                 eigenvectors=None, status=None, conf=None):
        imp = try_imports(['from pysparse import jdsym, itsolvers, precon',
                           'from pysparse.eigen import jdsym;'
                           ' from pysparse import itsolvers, precon'],
                          'cannot import pysparse eigensolvers!')

        jdsym = imp['jdsym']
        itsolvers = imp['itsolvers']
        precon = imp['precon']

        output("solving...")

        A = self._convert_mat(mtx_a)
        Atau = A.copy()

        if mtx_b is not None:
            M = self._convert_mat(mtx_b)
            Atau.shift(-conf.tau, M)

        K = precon.jacobi(Atau)
        A = A.to_sss()

        if mtx_b is not None:
            M = M.to_sss()

        else:
            M = None

        method = getattr(itsolvers, conf.method)
        kconv, lmbd, Q, it, it_in = jdsym.jdsym(A, M, K, n_eigs, conf.tau,
                                                conf.eps_a, conf.i_max,
                                                method,
                                                clvl=conf.verbosity,
                                                strategy=conf.strategy)

        output("number of converged eigenvalues:", kconv)

        output("...done")

        if status is not None:
            status['q'] = Q
            status['it'] = it
            status['it_in'] = it_in

        return lmbd, Q

########NEW FILE########
__FILENAME__ = ls
import time

import numpy as nm
import warnings

import scipy.sparse as sps

warnings.simplefilter('ignore', sps.SparseEfficiencyWarning)

from sfepy.base.base import output, get_default, assert_, try_imports, Struct
from sfepy.solvers.solvers import make_get_conf, LinearSolver

def standard_call(call):
    """
    Decorator handling argument preparation and timing for linear solvers.
    """
    def _standard_call(self, rhs, x0=None, conf=None, eps_a=None, eps_r=None,
                       i_max=None, mtx=None, status=None, **kwargs):
        tt = time.clock()

        conf = get_default(conf, self.conf)
        mtx = get_default(mtx, self.mtx)
        status = get_default(status, self.status)

        assert_(mtx.shape[0] == mtx.shape[1] == rhs.shape[0])
        if x0 is not None:
            assert_(x0.shape[0] == rhs.shape[0])

        result = call(self, rhs, x0, conf, eps_a, eps_r, i_max, mtx, status,
                      **kwargs)

        ttt = time.clock() - tt
        if status is not None:
            status['time'] = ttt

        return result

    return _standard_call

class ScipyDirect(LinearSolver):
    name = 'ls.scipy_direct'

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Missing items are set to default values.

        Example configuration, all items::

            solver_1100 = {
                'name' : 'dls1100',
                'kind' : 'ls.scipy_direct',

                'method' : 'superlu',
                'presolve' : False,
                'warn' : True,
            }
        """
        get = make_get_conf(conf, kwargs)
        common = LinearSolver.process_conf(conf)

        return Struct(method=get('method', 'auto'),
                      presolve=get('presolve', False),
                      warn=get('warn', True),
                      i_max=None, eps_a=None, eps_r=None) + common

    def __init__(self, conf, **kwargs):
        LinearSolver.__init__(self, conf, **kwargs)
        um = self.sls = None

        aux = try_imports(['import scipy.linsolve as sls',
                           'import scipy.splinalg.dsolve as sls',
                           'import scipy.sparse.linalg.dsolve as sls'],
                          'cannot import scipy sparse direct solvers!')
        self.sls = aux['sls']
        aux = try_imports(['import scipy.linsolve.umfpack as um',
                           'import scipy.splinalg.dsolve.umfpack as um',
                           'import scipy.sparse.linalg.dsolve.umfpack as um',
                           'import scikits.umfpack as um'])
        if 'um' in aux:
            um = aux['um']

        if um is not None:
            is_umfpack = hasattr(um, 'UMFPACK_OK')
        else:
            is_umfpack = False

        method = self.conf.method
        if method == 'superlu':
            self.sls.use_solver(useUmfpack=False)
        elif method == 'umfpack':
            if not is_umfpack and self.conf.warn:
                output('umfpack not available, using superlu!')
        elif method != 'auto':
            raise ValueError('uknown solution method! (%s)' % method)

        if method != 'superlu' and is_umfpack:
            self.sls.use_solver(useUmfpack=True,
                                assumeSortedIndices=True)

        self.solve = None
        if self._presolve() and hasattr(self, 'mtx'):
            if self.mtx is not None:
                self.solve = self.sls.factorized(self.mtx)

    @standard_call
    def __call__(self, rhs, x0=None, conf=None, eps_a=None, eps_r=None,
                 i_max=None, mtx=None, status=None, **kwargs):

        if self.solve is not None:
            # Matrix is already prefactorized.
            return self.solve(rhs)
        else:
            return self.sls.spsolve(mtx, rhs)

    def _presolve(self):
        if hasattr(self, 'presolve'):
            return self.presolve
        else:
            return self.conf.presolve

class Umfpack(ScipyDirect):
    """This class stays for compatability with old input files. Use ScipyDirect
    isntead."""
    name = 'ls.umfpack'

    def __init__(self, conf, **kwargs):
        conf.method = 'umfpack'
        ScipyDirect.__init__(self, conf, **kwargs)

##
# c: 22.02.2008
class ScipyIterative( LinearSolver ):
    """
    Interface to SciPy iterative solvers.

    Notes
    -----
    The `eps_r` tolerance is both absolute and relative - the solvers
    stop when either the relative or the absolute residual is below it.

    A preconditioner can be anything that the SciPy solvers accept (sparse
    matrix, dense matrix, LinearOperator).
    """
    name = 'ls.scipy_iterative'

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Missing items are set to default values.

        Example configuration, all items::

            solver_110 = {
                'name' : 'ls110',
                'kind' : 'ls.scipy_iterative',

                'method' : 'cg',
                'precond' : None,
                'callback' : None,
                'i_max' : 1000,
                'eps_r' : 1e-12,
            }
        """
        get = make_get_conf(conf, kwargs)
        common = LinearSolver.process_conf(conf)

        return Struct(method=get('method', 'cg'),
                      precond=get('precond', None),
                      callback=get('callback', None),
                      i_max=get('i_max', 100),
                      eps_a=None,
                      eps_r=get('eps_r', 1e-8)) + common

    def __init__(self, conf, **kwargs):
        import scipy.sparse.linalg.isolve as la

        LinearSolver.__init__(self, conf, **kwargs)

        try:
            solver = getattr( la, self.conf.method )
        except AttributeError:
            output( 'scipy solver %s does not exist!' % self.conf.method )
            output( 'using cg instead' )
            solver = la.cg
        self.solver = solver
        self.converged_reasons = {
            0 : 'successful exit',
            1 : 'number of iterations',
            -1 : 'illegal input or breakdown',
        }

    @standard_call
    def __call__(self, rhs, x0=None, conf=None, eps_a=None, eps_r=None,
                 i_max=None, mtx=None, status=None, **kwargs):

        eps_r = get_default(eps_r, self.conf.eps_r)
        i_max = get_default(i_max, self.conf.i_max)

        precond = get_default(kwargs.get('precond', None), self.conf.precond)
        callback = get_default(kwargs.get('callback', None), self.conf.callback)

        if conf.method == 'qmr':
            prec_args = {'M1' : precond, 'M2' : precond}

        else:
            prec_args = {'M' : precond}

        sol, info = self.solver(mtx, rhs, x0=x0, tol=eps_r, maxiter=i_max,
                                callback=callback, **prec_args)
        output('%s convergence: %s (%s)'
               % (self.conf.method,
                  info, self.converged_reasons[nm.sign(info)]))

        return sol

##
# c: 02.05.2008, r: 02.05.2008
class PyAMGSolver( LinearSolver ):
    """
    Interface to PyAMG solvers.

    Notes
    -----
    Uses relative convergence tolerance, i.e. eps_r is scaled by `||b||`.
    """
    name = 'ls.pyamg'

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Missing items are set to default values.

        Example configuration, all items::

            solver_102 = {
                'name' : 'ls102',
                'kind' : 'ls.pyamg',

                'method' : 'smoothed_aggregation_solver',
                'accel' : 'cg'
                'eps_r' : 1e-12,
            }
        """
        get = make_get_conf(conf, kwargs)
        common = LinearSolver.process_conf(conf)

        return Struct(method=get('method', 'smoothed_aggregation_solver'),
                      accel = get('accel', None),
                      i_max=None, eps_a=None,
                      eps_r=get('eps_r', 1e-8)) + common

    ##
    # c: 02.05.2008, r: 02.05.2008
    def __init__( self, conf, **kwargs ):
        try:
            import pyamg
        except ImportError:
            msg =  'cannot import pyamg!'
            raise ImportError( msg )

        LinearSolver.__init__(self, conf, mg=None, **kwargs)

        try:
            solver = getattr( pyamg, self.conf.method )
        except AttributeError:
            output( 'pyamg.%s does not exist!' % self.conf.method )
            output( 'using pyamg.smoothed_aggregation_solver instead' )
            solver = pyamg.smoothed_aggregation_solver
        self.solver = solver

        if hasattr( self, 'mtx' ):
            if self.mtx is not None:
                self.mg = self.solver( self.mtx )

    @standard_call
    def __call__(self, rhs, x0=None, conf=None, eps_a=None, eps_r=None,
                 i_max=None, mtx=None, status=None, **kwargs):

        eps_r = get_default(eps_r, self.conf.eps_r)

        if (self.mg is None) or (mtx is not self.mtx):
            self.mg = self.solver(mtx)
            self.mtx = mtx

        sol = self.mg.solve(rhs, x0=x0, accel=conf.accel, tol=eps_r)

        return sol

class PETScKrylovSolver( LinearSolver ):
    """
    PETSc Krylov subspace solver.

    The solver and preconditioner types are set upon the solver object
    creation. Tolerances can be overriden when called by passing a `conf`
    object.

    Notes
    -----
    Convergence is reached when `rnorm < max(eps_r * rnorm_0, eps_a)`,
    where, in PETSc, `rnorm` is by default the norm of *preconditioned*
    residual.
    """
    name = 'ls.petsc'

    _precond_sides = {None : None, 'left' : 0, 'right' : 1, 'symmetric' : 2}

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Missing items are set to default values.

        Example configuration, all items::

            solver_120 = {
                'name' : 'ls120',
                'kind' : 'ls.petsc',

                'method' : 'cg', # ksp_type
                'precond' : 'icc', # pc_type
                'precond_side' : 'left', # ksp_pc_side
                'eps_a' : 1e-12, # abstol
                'eps_r' : 1e-12, # rtol
                'eps_d' : 1e5, # divtol
                'i_max' : 1000, # maxits
            }
        """
        get = make_get_conf(conf, kwargs)
        common = LinearSolver.process_conf(conf)

        return Struct(method=get('method', 'cg'),
                      precond=get('precond', 'icc'),
                      precond_side=get('precond_side', None),
                      i_max=get('i_max', 100),
                      eps_a=get('eps_a', 1e-8),
                      eps_r=get('eps_r', 1e-8),
                      eps_d=get('eps_d', 1e5)) + common

    def __init__( self, conf, **kwargs ):
        try:
            import petsc4py
            petsc4py.init([])
            from petsc4py import PETSc
        except ImportError:
            msg = 'cannot import petsc4py!'
            raise ImportError( msg )

        LinearSolver.__init__(self, conf, petsc=PETSc, pmtx=None, **kwargs)

        ksp = PETSc.KSP().create()

        ksp.setType( self.conf.method )
        ksp.getPC().setType( self.conf.precond )
        side = self._precond_sides[self.conf.precond_side]
        if side is not None:
            ksp.setPCSide(side)
        self.ksp = ksp

        self.converged_reasons = {}
        for key, val in ksp.ConvergedReason.__dict__.iteritems():
            if isinstance(val, int):
                self.converged_reasons[val] = key

    def set_matrix( self, mtx ):
        mtx = sps.csr_matrix(mtx)

        pmtx = self.petsc.Mat().createAIJ( mtx.shape,
                                           csr = (mtx.indptr,
                                                  mtx.indices,
                                                  mtx.data) )
        sol, rhs = pmtx.getVecs()
        return pmtx, sol, rhs

    @standard_call
    def __call__(self, rhs, x0=None, conf=None, eps_a=None, eps_r=None,
                 i_max=None, mtx=None, status=None, **kwargs):

        eps_a = get_default(eps_a, self.conf.eps_a)
        eps_r = get_default(eps_r, self.conf.eps_r)
        i_max = get_default(i_max, self.conf.i_max)
        eps_d = self.conf.eps_d

        # There is no use in caching matrix in the solver - always set as new.
        pmtx, psol, prhs = self.set_matrix(mtx)

        ksp = self.ksp
        ksp.setOperators(pmtx)
        ksp.setFromOptions() # PETSc.Options() not used yet...
        ksp.setTolerances(atol=eps_a, rtol=eps_r, divtol=eps_d, max_it=i_max)

        # Set PETSc rhs, solve, get solution from PETSc solution.
        if x0 is not None:
            psol[...] = x0
            ksp.setInitialGuessNonzero(True)
        prhs[...] = rhs
        ksp.solve(prhs, psol)
        sol = psol[...].copy()
        output('%s(%s) convergence: %s (%s)'
               % (self.conf.method, self.conf.precond,
                  ksp.reason, self.converged_reasons[ksp.reason]))

        return sol

class PETScParallelKrylovSolver(PETScKrylovSolver):
    """
    PETSc Krylov subspace solver able to run in parallel by storing the
    system to disk and running a separate script via `mpiexec`.

    The solver and preconditioner types are set upon the solver object
    creation. Tolerances can be overriden when called by passing a `conf`
    object.

    Notes
    -----
    Convergence is reached when `rnorm < max(eps_r * rnorm_0, eps_a)`,
    where, in PETSc, `rnorm` is by default the norm of *preconditioned*
    residual.
    """
    name = 'ls.petsc_parallel'

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Missing items are set to default values.

        Example configuration, all items::

            solver_1 = {
                'name' : 'ls',
                'kind' : 'ls.petsc_parallel',

                'log_dir' : '.', # Store logs here.
                'n_proc' : 5, # Number of processes to run.

                'method' : 'cg', # ksp_type
                'precond' : 'bjacobi', # pc_type
                'sub_precond' : 'icc', # sub_pc_type
                'eps_a' : 1e-12, # abstol
                'eps_r' : 1e-12, # rtol
                'eps_d' : 1e5, # divtol
                'i_max' : 1000, # maxits
            }
        """
        get = make_get_conf(conf, kwargs)
        common = PETScKrylovSolver.process_conf(conf, kwargs)

        return Struct(log_dir=get('log_dir', '.'),
                      n_proc=get('n_proc', 1),
                      sub_precond=get('sub_precond', 'icc')) + common

    @standard_call
    def __call__(self, rhs, x0=None, conf=None, eps_a=None, eps_r=None,
                 i_max=None, mtx=None, status=None, **kwargs):
        import os, sys, shutil, tempfile
        from sfepy import base_dir
        from sfepy.base.ioutils import ensure_path

        eps_a = get_default(eps_a, self.conf.eps_a)
        eps_r = get_default(eps_r, self.conf.eps_r)
        i_max = get_default(i_max, self.conf.i_max)
        eps_d = self.conf.eps_d

        petsc = self.petsc

        # There is no use in caching matrix in the solver - always set as new.
        pmtx, psol, prhs = self.set_matrix(mtx)

        ksp = self.ksp
        ksp.setOperators(pmtx)
        ksp.setFromOptions() # PETSc.Options() not used yet...
        ksp.setTolerances(atol=eps_a, rtol=eps_r, divtol=eps_d, max_it=i_max)

        output_dir = tempfile.mkdtemp()

        # Set PETSc rhs, solve, get solution from PETSc solution.
        if x0 is not None:
            psol[...] = x0
            sol0_filename = os.path.join(output_dir, 'sol0.dat')

        else:
            sol0_filename = ''

        prhs[...] = rhs

        script_filename = os.path.join(base_dir, 'solvers/petsc_worker.py')

        mtx_filename = os.path.join(output_dir, 'mtx.dat')
        rhs_filename = os.path.join(output_dir, 'rhs.dat')
        sol_filename = os.path.join(output_dir, 'sol.dat')
        status_filename = os.path.join(output_dir, 'status.txt')

        log_filename = os.path.join(self.conf.log_dir, 'sol.log')
        ensure_path(log_filename)

        output('storing system to %s...' % output_dir)
        tt = time.clock()
        view_mtx = petsc.Viewer().createBinary(mtx_filename, mode='w')
        view_rhs = petsc.Viewer().createBinary(rhs_filename, mode='w')
        pmtx.view(view_mtx)
        prhs.view(view_rhs)
        if sol0_filename:
            view_sol0 = petsc.Viewer().createBinary(sol0_filename, mode='w')
            psol.view(view_sol0)
        output('...done in %.2f s' % (time.clock() - tt))

        command = [
            'mpiexec -n %d' % self.conf.n_proc,
            sys.executable, script_filename,
            '-mtx %s' % mtx_filename, '-rhs %s' % rhs_filename,
            '-sol0 %s' % sol0_filename, '-sol %s' % sol_filename,
            '-status %s' % status_filename,
            '-ksp_type %s' % self.conf.method,
            '-pc_type %s' % self.conf.precond,
            '-sub_pc_type %s' % self.conf.sub_precond,
            '-ksp_atol %.3e' % self.conf.eps_a,
            '-ksp_rtol %.3e' % self.conf.eps_r,
            '-ksp_max_it %d' % self.conf.i_max,
            '-ksp_monitor %s' % log_filename,
            '-ksp_view %s' % log_filename,
        ]
        if self.conf.precond_side is not None:
            command.append('-ksp_pc_side %s' % self.conf.precond_side)

        out = os.system(" ".join(command))
        assert_(out == 0)

        output('reading solution...')
        tt = time.clock()
        view_sol = self.petsc.Viewer().createBinary(sol_filename, mode='r')
        psol = petsc.Vec().load(view_sol)

        fd = open(status_filename, 'r')
        line = fd.readline().split()
        reason = int(line[0])
        elapsed = float(line[1])
        fd.close()
        output('...done in %.2f s' % (time.clock() - tt))

        sol = psol[...].copy()
        output('%s(%s, %s/proc) convergence: %s (%s)'
               % (self.conf.method, self.conf.precond, self.conf.sub_precond,
                  reason, self.converged_reasons[reason]))
        output('elapsed: %.2f [s]' % elapsed)

        shutil.rmtree(output_dir)

        return sol

class SchurGeneralized(ScipyDirect):
    r"""
    Generalized Schur complement.

    Defines the matrix blocks and calls user defined function.
    """
    name = 'ls.schur_generalized'

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Setup solver configuration options.

        Example configuration::

            solvers = {
                'ls': ('ls.schur_generalized',
                       {'blocks':
                        {'u': ['displacement1', 'displacement2'],
                         'v': ['velocity1', 'velocity2'],
                         'w': ['pressure1', 'pressure2'],
                         },
                        'function': my_schur,
                        'needs_problem_instance': True,
                        })
            }
        """
        get = make_get_conf(conf, kwargs)
        common = ScipyDirect.process_conf(conf, kwargs)

        return Struct(blocks=get('blocks', None,
                                 'missing "blocks" in options!'),
                      function=get('function', None,
                                   'missing "function" in options!'),
                      needs_problem_instance=True) + common

    def __init__(self, conf, **kwargs):
        from sfepy.discrete.state import State

        ScipyDirect.__init__(self, conf, **kwargs)

        equations = self.problem.equations
        aux_state = State(equations.variables)

        conf.idxs = {}
        for bk, bv in conf.blocks.iteritems():
            aux_state.fill(0.0)
            for jj in bv:
                idx = equations.variables.di.indx[jj]
                aux_state.vec[idx] = nm.nan

            aux_state.apply_ebc()
            vec0 = aux_state.get_reduced()
            conf.idxs[bk] = nm.where(nm.isnan(vec0))[0]

    @standard_call
    def __call__(self, rhs, x0=None, conf=None, eps_a=None, eps_r=None,
                 i_max=None, mtx=None, status=None, **kwargs):

        mtxi= self.orig_conf.idxs
        mtxslc_s = {}
        mtxslc_f = {}
        nn = {}

        for ik, iv in mtxi.iteritems():
            ptr = 0
            nn[ik] = len(iv)
            mtxslc_s[ik] = []
            mtxslc_f[ik] = []
            while ptr < nn[ik]:
                idx0 = iv[ptr:]
                idxrange = nm.arange(idx0[0], idx0[0] + len(idx0))
                aux = nm.where(idx0 == idxrange)[0]
                mtxslc_s[ik].append(slice(ptr + aux[0], ptr + aux[-1] + 1))
                mtxslc_f[ik].append(slice(idx0[aux][0], idx0[aux][-1] + 1))
                ptr += aux[-1] + 1

        mtxs = {}
        rhss = {}
        ress = {}
        for ir in mtxi.iterkeys():
            rhss[ir] = nm.zeros((nn[ir],), dtype=nm.float64)
            ress[ir] = nm.zeros((nn[ir],), dtype=nm.float64)
            for jr, idxr in enumerate(mtxslc_f[ir]):
                rhss[ir][mtxslc_s[ir][jr]] = rhs[idxr]

            for ic in mtxi.iterkeys():
                mtxid = '%s%s' % (ir, ic)
                mtxs[mtxid] = nm.zeros((nn[ir], nn[ic]), dtype=nm.float64)
                for jr, idxr in enumerate(mtxslc_f[ir]):
                    for jc, idxc in enumerate(mtxslc_f[ic]):
                        iir = mtxslc_s[ir][jr]
                        iic = mtxslc_s[ic][jc]
                        mtxs[mtxid][iir, iic] = mtx._get_submatrix(idxr, idxc).todense()

        self.orig_conf.function(ress, mtxs, rhss, nn)

        res = nm.zeros_like(rhs)
        for ir in mtxi.iterkeys():
            for jr, idxr in enumerate(mtxslc_f[ir]):
                res[idxr] = ress[ir][mtxslc_s[ir][jr]]

        return res

    def _presolve(self):
        if hasattr(self, 'presolve'):
            return self.presolve
        else:
            return self.conf.presolve

class SchurComplement(SchurGeneralized):
    r"""
    Schur complement.

    Solution of the linear system

    .. math::
       \left[ \begin{array}{cc}
       A & B \\
       C & D \end{array} \right]
       \cdot
       \left[ \begin{array}{c}
       u \\
       v \end{array} \right]
       =
       \left[ \begin{array}{c}
       f \\
       g \end{array} \right]

    is obtained by solving the following equation:

    .. math::
       (D - C A^{-1} B) \cdot v = g - C A^{-1} f

    variable(s) :math:`u` are specified in "eliminate" list,
    variable(s) :math:`v` are specified in "keep" list,

    See: http://en.wikipedia.org/wiki/Schur_complement
    """
    name = 'ls.schur_complement'

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Setup solver configuration options.

        Example configuration::

            solvers = {
                'ls': ('ls.schur_complement',
                       {'eliminate': ['displacement'],
                        'keep': ['pressure'],
                        'needs_problem_instance': True,
                        })
            }
        """
        get = make_get_conf(conf, kwargs)
        conf.blocks = {'1': get('eliminate', None,
                                'missing "eliminate" in options!'),
                       '2': get('keep', None,
                                'missing "keep" in options!'),}
        conf.function = SchurComplement.schur_fun
        common = SchurGeneralized.process_conf(conf, kwargs)

        return common

    @staticmethod
    def schur_fun(res, mtx, rhs, nn):
        import scipy.sparse as scs
        import scipy.sparse.linalg as sls

        invA = sls.splu(scs.csc_matrix(mtx['11']))
        invAB = nm.zeros_like(mtx['12'])
        for j, b in enumerate(mtx['12'].T):
            invAB[:,j] = invA.solve(b)

        invAf = invA.solve(rhs['1'])

        spC = scs.csc_matrix(mtx['21'])
        k_rhs = rhs['2'] - spC * invAf
        res['2'] = sls.spsolve(scs.csc_matrix(mtx['22'] - spC * invAB), k_rhs)
        res['1'] = invAf - nm.dot(invAB, res['2'])

class MultiProblem(ScipyDirect):
    r"""
    Conjugate multiple problems.

    Allows to define conjugate multiple problems.
    """
    name = 'ls.cm_pb'

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Setup solver configuration options.

        Example configuration::

            solvers = {
                'ls': ('ls.cm_pb',
                       {'others': ['acoustic_subproblem.py'],
                        'coupling_variables': ['g'],
                        'needs_problem_instance': True,
                        })
            }
        """
        get = make_get_conf(conf, kwargs)
        common = ScipyDirect.process_conf(conf, kwargs)

        return Struct(others=get('others', None,
                                 'missing "others" in options!'),
                      coupling_variables=get('coupling_variables', None,
                                             'missing "coupling_variables"!'),
                      needs_problem_instance=True) + common

    def __init__(self, conf, problem, **kwargs):
        from sfepy.discrete.state import State
        from sfepy.discrete import Problem
        from sfepy.base.conf import ProblemConf, get_standard_keywords
        from scipy.spatial import cKDTree as KDTree

        ScipyDirect.__init__(self, conf, **kwargs)

        # init subproblems
        pb_vars = problem.get_variables()
        # get "master" DofInfo and last index
        pb_adi_indx = problem.equations.variables.adi.indx
        self.adi_indx = pb_adi_indx.copy()
        last_indx = -1
        for ii in self.adi_indx.itervalues():
            last_indx = nm.max([last_indx, ii.stop])

        # coupling variables
        self.cvars_to_pb = {}
        for jj in conf.coupling_variables:
            self.cvars_to_pb[jj] = [None, None]
            if jj in pb_vars.names:
                if pb_vars[jj].dual_var_name is not None:
                    self.cvars_to_pb[jj][0] = -1

                else:
                    self.cvars_to_pb[jj][1] = -1

        # init subproblems
        self.subpb = []
        required, other = get_standard_keywords()
        master_prefix = output.get_output_prefix()
        for ii, ifname in enumerate(conf.others):
            sub_prefix = master_prefix[:-1] + '-sub%d:' % (ii + 1)
            output.set_output_prefix(sub_prefix)
            kwargs['master_problem'] = problem
            confi = ProblemConf.from_file(ifname, required, other,
                                          define_args=kwargs)
            pbi = Problem.from_conf(confi, init_equations=True)
            sti = State(pbi.equations.variables)
            pbi.equations.set_data(None, ignore_unknown=True)
            pbi.time_update()
            pbi.update_materials()
            sti.apply_ebc()
            pbi_vars = pbi.get_variables()
            output.set_output_prefix(master_prefix)
            self.subpb.append([pbi, sti, None])

            # append "slave" DofInfo
            for jj in pbi_vars.names:
                if not(pbi_vars[jj].is_state()):
                    continue

                didx = pbi.equations.variables.adi.indx[jj]
                ndof = didx.stop - didx.start
                if jj in self.adi_indx:
                    if ndof != \
                      (self.adi_indx[jj].stop - self.adi_indx[jj].start):
                        raise ValueError('DOFs do not match!')

                else:
                    self.adi_indx.update({
                        jj: slice(last_indx, last_indx + ndof, None)})
                    last_indx += ndof

            for jj in conf.coupling_variables:
                if jj in pbi_vars.names:
                    if pbi_vars[jj].dual_var_name is not None:
                        self.cvars_to_pb[jj][0] = ii

                    else:
                        self.cvars_to_pb[jj][1] = ii

        self.subpb.append([problem, None, None])

        self.cvars_to_pb_map = {}
        for varname, pbs in self.cvars_to_pb.iteritems():
            # match field nodes
            coors = []
            for ii in pbs:
                pbi = self.subpb[ii][0]
                pbi_vars = pbi.get_variables()
                fcoors = pbi_vars[varname].field.coors
                dc = nm.abs(nm.max(fcoors, axis=0)\
                            - nm.min(fcoors, axis=0))
                ax = nm.where(dc > 1e-9)[0]
                coors.append(fcoors[:,ax])

            if len(coors[0]) != len(coors[1]):
                raise ValueError('number of nodes does not match!')

            kdtree = KDTree(coors[0])
            map_12 = kdtree.query(coors[1])[1]

            pbi1 = self.subpb[pbs[0]][0]
            pbi1_vars = pbi1.get_variables()
            eq_map_1 = pbi1_vars[varname].eq_map

            pbi2 = self.subpb[pbs[1]][0]
            pbi2_vars = pbi2.get_variables()
            eq_map_2 = pbi2_vars[varname].eq_map

            dpn = eq_map_2.dpn
            nnd = map_12.shape[0]

            map_12_nd = nm.zeros((nnd * dpn,), dtype=nm.int32)
            if dpn > 1:
                for ii in range(dpn):
                    map_12_nd[ii::dpn] = map_12 * dpn + ii
            else:
                map_12_nd = map_12

            idx = nm.where(eq_map_2.eq >= 0)[0]
            self.cvars_to_pb_map[varname] = eq_map_1.eq[map_12[idx]]

    def sparse_submat(self, Ad, Ar, Ac, gr, gc, S):
        """
        A[gr,gc] = S
        """

        if type(gr) is slice:
            gr = nm.arange(gr.start, gr.stop)

        if type(gc) is slice:
            gc = nm.arange(gc.start, gc.stop)

        for ii, lrow in enumerate(S):
            m = lrow.indices.shape[0]
            idxrow = nm.ones((m, ), dtype=nm.int32) * gr[ii]
            Ar = nm.hstack([Ar, idxrow])
            Ac = nm.hstack([Ac, gc[lrow.indices]])
            Ad = nm.hstack([Ad, lrow.data])

        return Ad, Ar, Ac

    @standard_call
    def __call__(self, rhs, x0=None, conf=None, eps_a=None, eps_r=None,
                 i_max=None, mtx=None, status=None, **kwargs):

        max_indx = 0
        hst = nm.hstack
        for ii in self.adi_indx.itervalues():
            max_indx = nm.max([max_indx, ii.stop])

        new_rhs = nm.zeros((max_indx,), dtype=rhs.dtype)
        new_rhs[:rhs.shape[0]] = rhs

        # copy "master" matrices
        pbi = self.subpb[-1][0]
        adi_indxi = pbi.equations.variables.adi.indx
        mtxc = mtx.tocsc()
        aux_data = nm.array([], dtype=mtxc.dtype)
        aux_rows = nm.array([], dtype=nm.int32)
        aux_cols = nm.array([], dtype=nm.int32)

        for jk, jv in adi_indxi.iteritems():
            if jk in self.cvars_to_pb:
                if not(self.cvars_to_pb[jk][0] == -1):
                    continue

            gjv = self.adi_indx[jk]
            ii = gjv.start
            for jj in nm.arange(jv.start, jv.stop):
                ptr = mtxc.indptr[jj]
                nn = mtxc.indptr[jj + 1] - ptr
                sl = slice(ptr, ptr + nn, None)
                aux_data = hst([aux_data, mtxc.data[sl]])
                aux_rows = hst([aux_rows, mtxc.indices[sl]])
                aux_cols = hst([aux_cols, nm.ones((nn,), dtype=nm.int32) * ii])
                ii += 1

        # copy "slave" (sub)matricies
        mtxs = []
        for kk, (pbi, sti0, _) in enumerate(self.subpb[:-1]):
            x0i = sti0.get_reduced()
            evi = pbi.get_evaluator()
            mtxi = evi.eval_tangent_matrix(x0i, mtx=pbi.mtx_a)
            rhsi = evi.eval_residual(x0i)
            mtxs.append(mtxi)

            adi_indxi = pbi.equations.variables.adi.indx
            for ik, iv in adi_indxi.iteritems():
                if ik in self.cvars_to_pb:
                    if not(self.cvars_to_pb[ik][0] == kk):
                        continue

                giv = self.adi_indx[ik]
                for jk, jv in adi_indxi.iteritems():
                    gjv = self.adi_indx[jk]
                    if jk in self.cvars_to_pb:
                        if not(self.cvars_to_pb[jk][0] == kk):
                            continue

                    aux_data, aux_rows, aux_cols =\
                        self.sparse_submat(aux_data, aux_rows, aux_cols,
                                           giv, gjv, mtxi[iv, jv])

                new_rhs[giv] = rhsi[iv]

        mtxs.append(mtx)
        # copy "coupling" (sub)matricies
        for varname, pbs in self.cvars_to_pb.iteritems():
            idx = pbs[1]
            pbi = self.subpb[idx][0]
            mtxi = mtxs[idx]
            gjv = self.adi_indx[varname]
            jv = pbi.equations.variables.adi.indx[varname]
            adi_indxi = pbi.equations.variables.adi.indx
            for ik, iv in adi_indxi.iteritems():
                if ik == varname:
                    continue

                giv = self.adi_indx[ik]
                aux_mtx = mtxi[iv,:].tocsc()
                for ll, jj in enumerate(nm.arange(jv.start, jv.stop)):
                    ptr = aux_mtx.indptr[jj]
                    nn = aux_mtx.indptr[jj + 1] - ptr
                    if nn < 1:
                        continue
                    sl = slice(ptr, ptr + nn, None)
                    aux_data = hst([aux_data, aux_mtx.data[sl]])
                    aux_rows = hst([aux_rows, aux_mtx.indices[sl] + giv.start])
                    jjr = gjv.start + self.cvars_to_pb_map[varname][ll]
                    aux_cols = hst([aux_cols,
                                    nm.ones((nn,), dtype=nm.int32) * jjr])

        # create new matrix
        new_mtx = sps.coo_matrix((aux_data, (aux_rows, aux_cols))).tocsr()

        res0 = ScipyDirect.__call__(self, new_rhs, mtx=new_mtx)

        res = []
        for kk, (pbi, sti0, _) in enumerate(self.subpb):
            adi_indxi = pbi.equations.variables.adi.indx
            max_indx = 0
            for ii in adi_indxi.itervalues():
                max_indx = nm.max([max_indx, ii.stop])

            resi = nm.zeros((max_indx,), dtype=res0.dtype)
            for ik, iv in adi_indxi.iteritems():
                giv = self.adi_indx[ik]
                if ik in self.cvars_to_pb:
                    if pbi is self.subpb[self.cvars_to_pb[ik][1]][0]:
                        giv = self.cvars_to_pb_map[ik] + giv.start

                resi[iv] = res0[giv]

            if sti0 is not None:
                sti = sti0.copy()
                sti.set_reduced(-resi)
                pbi.setup_default_output()
                pbi.save_state(pbi.get_output_name(), sti)
                self.subpb[kk][-1] = sti

            res.append(resi)

        return res[-1]

    def _presolve(self):
        if hasattr(self, 'presolve'):
            return self.presolve
        else:
            return self.conf.presolve

########NEW FILE########
__FILENAME__ = nls
"""
Nonlinear solvers.
"""
import time

import numpy as nm
import numpy.linalg as nla

from sfepy.base.base import output, get_default, debug, Struct
from sfepy.base.log import Log, get_logging_conf
from sfepy.solvers.solvers import make_get_conf, NonlinearSolver

def check_tangent_matrix(conf, vec_x0, fun, fun_grad):
    """
    Verify the correctness of the tangent matrix as computed by `fun_grad()` by
    comparing it with its finite difference approximation evaluated by
    repeatedly calling `fun()` with `vec_x0` items perturbed by a small delta.
    """
    vec_x = vec_x0.copy()
    delta = conf.delta

    vec_r = fun(vec_x) # Update state.
    mtx_a0 = fun_grad(vec_x)

    mtx_a = mtx_a0.tocsc()
    mtx_d = mtx_a.copy()
    mtx_d.data[:] = 0.0

    vec_dx = nm.zeros_like(vec_r)

    for ic in range(vec_dx.shape[0]):
        vec_dx[ic] = delta
        xx = vec_x.copy() - vec_dx
        vec_r1 = fun(xx)

        vec_dx[ic] = -delta
        xx = vec_x.copy() - vec_dx
        vec_r2 = fun(xx)

        vec_dx[ic] = 0.0;

        vec = 0.5 * (vec_r2 - vec_r1) / delta

        ir = mtx_a.indices[mtx_a.indptr[ic]:mtx_a.indptr[ic+1]]
        mtx_d.data[mtx_a.indptr[ic]:mtx_a.indptr[ic+1]] = vec[ir]

    vec_r = fun(vec_x) # Restore.

    tt = time.clock()
    output(mtx_a, '.. analytical')
    output(mtx_d, '.. difference')
    import sfepy.base.plotutils as plu
    plu.plot_matrix_diff(mtx_d, mtx_a, delta, ['difference', 'analytical'],
                         conf.check)

    return time.clock() - tt

def conv_test(conf, it, err, err0):
    """
    Nonlinear solver convergence test.

    Parameters
    ----------
    conf : Struct instance
        The nonlinear solver configuration.
    it : int
        The current iteration.
    err : float
        The current iteration error.
    err0 : float
        The initial error.

    Returns
    -------
    status : int
        The convergence status: -1 = no convergence (yet), 0 = solver converged
        - tolerances were met, 1 = max. number of iterations reached.
    """
    status = -1
    if (abs(err0) < conf.macheps):
        err_r = 0.0
    else:
        err_r = err / err0

    output('nls: iter: %d, residual: %e (rel: %e)' % (it, err, err_r))

    conv_a = err < conf.eps_a
    if it > 0:
        conv_r = err_r < conf.eps_r

        if conv_a and conv_r:
            status = 0

        elif (conf.get('eps_mode', '') == 'or') and (conv_a or conv_r):
            status = 0

    else:
        if conv_a:
            status = 0

    if (status == -1) and (it >= conf.i_max):
        status = 1

    return status

class Newton(NonlinearSolver):
    r"""
    Solves a nonlinear system :math:`f(x) = 0` using the Newton method with
    backtracking line-search, starting with an initial guess :math:`x^0`.

    For common configuration parameters, see :class:`Solver
    <sfepy.solvers.solvers.Solver>`.

    Parameters
    ----------
    i_max : int
        The maximum number of iterations.
    eps_a : float
        The absolute tolerance for the residual, i.e. :math:`||f(x^i)||`.
    eps_r : float
        The relative tolerance for the residual, i.e. :math:`||f(x^i)|| /
        ||f(x^0)||`.
    eps_mode : 'and' or 'or'
        The logical operator to use for combining the absolute and relative
        tolerances.
    macheps : float
        The float considered to be machine "zero".
    lin_red : float
        The linear system solution error should be smaller than (`eps_a` *
        `lin_red`), otherwise a warning is printed.
    lin_precision : float or None
        If not None, the linear system solution tolerances are set in each
        nonlinear iteration relative to the current residual norm by the
        `lin_precision` factor. Ignored for direct linear solvers.
    ls_on : float
        Start the backtracking line-search by reducing the step, if
        :math:`||f(x^i)|| / ||f(x^{i-1})||` is larger than `ls_on`.
    ls_red : 0.0 < float < 1.0
        The step reduction factor in case of correct residual assembling.
    ls_red_warp : 0.0 < float < 1.0
        The step reduction factor in case of failed residual assembling
        (e.g. the "warp violation" error caused by a negative volume element
        resulting from too large deformations).
    ls_min : 0.0 < float < 1.0
        The minimum step reduction factor.
    give_up_warp : bool
        If True, abort on the "warp violation" error.
    check : 0, 1 or 2
        If >= 1, check the tangent matrix using finite differences. If 2, plot
        the resulting sparsity patterns.
    delta : float
        If `check >= 1`, the finite difference matrix is taken as :math:`A_{ij}
        = \frac{f_i(x_j + \delta) - f_i(x_j - \delta)}{2 \delta}`.
    log : dict or None
        If not None, log the convergence according to the configuration in the
        following form::

            {'text' : 'log.txt', 'plot' : 'log.pdf'}

        Each of the dict items can be None.
    problem : 'nonlinear' or 'linear'
        Specifies if the problem is linear or non-linear.
    """
    name = 'nls.newton'

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Missing items are set to default values for a linear problem.

        Example configuration, all items::

            solver_1 = {
                'name' : 'newton',
                'kind' : 'nls.newton',

                'i_max' : 2,
                'eps_a' : 1e-8,
                'eps_r' : 1e-2,
                'eps_mode' : 'or',
                'macheps' : 1e-16,
                'lin_red' : 1e-2, # Linear system error < (eps_a * lin_red).
                'lin_precision' : None,
                'ls_on' : 0.99999,
                'ls_red' : 0.1,
                'ls_red_warp' : 0.001,
                'ls_min' : 1e-5,
                'give_up_warp' : False,
                'check' : 0,
                'delta' : 1e-6,
                'log' : None, # 'nonlinear' or 'linear' (ignore i_max)
                'problem' : 'nonlinear',
            }
        """
        get = make_get_conf(conf, kwargs)
        common = NonlinearSolver.process_conf(conf)

        log = get_logging_conf(conf)
        log = Struct(name='log_conf', **log)
        is_any_log = (log.text is not None) or (log.plot is not None)

        return Struct(i_max=get('i_max', 1),
                      eps_a=get('eps_a', 1e-10),
                      eps_r=get('eps_r', 1.0),
                      eps_mode=get('eps_mode', 'and'),
                      macheps=get('macheps', nm.finfo(nm.float64).eps),
                      lin_red=get('lin_red', 1.0),
                      lin_precision=get('lin_precision', None),
                      ls_red=get('ls_red', 0.1),
                      ls_red_warp=get('ls_red_warp', 0.001),
                      ls_on=get('ls_on', 0.99999),
                      ls_min=get('ls_min', 1e-5),
                      give_up_warp=get('give_up_warp', False),
                      check=get('check', 0),
                      delta=get('delta', 1e-6),
                      problem=get('problem', 'nonlinear'),
                      log=log,
                      is_any_log=is_any_log) + common

    def __init__(self, conf, **kwargs):
        NonlinearSolver.__init__(self, conf, **kwargs)

        conf = self.conf
        if conf.is_any_log:
            self.log = Log([[r'$||r||$'], ['iteration']],
                           xlabels=['', 'all iterations'],
                           ylabels=[r'$||r||$', 'iteration'],
                           yscales=['log', 'linear'],
                           log_filename=conf.log.text,
                           formats=[['%.8e'], ['%d']])

        else:
            self.log = None

    def __call__(self, vec_x0, conf=None, fun=None, fun_grad=None,
                 lin_solver=None, iter_hook=None, status=None):
        """
        Nonlinear system solver call.

        Solves a nonlinear system :math:`f(x) = 0` using the Newton method with
        backtracking line-search, starting with an initial guess :math:`x^0`.

        Parameters
        ----------
        vec_x0 : array
            The initial guess vector :math:`x_0`.
        conf : Struct instance, optional
            The solver configuration parameters,
        fun : function, optional
            The function :math:`f(x)` whose zero is sought - the residual.
        fun_grad : function, optional
            The gradient of :math:`f(x)` - the tangent matrix.
        lin_solver : LinearSolver instance, optional
            The linear solver for each nonlinear iteration.
        iter_hook : function, optional
            User-supplied function to call before each iteration.
        status : dict-like, optional
            The user-supplied object to hold convergence statistics.

        Notes
        -----
        * The optional parameters except `iter_hook` and `status` need
          to be given either here or upon `Newton` construction.
        * Setting `conf.problem == 'linear'` means a pre-assembled and possibly
          pre-solved matrix. This is mostly useful for linear time-dependent
          problems.
        """
        conf = get_default(conf, self.conf)
        fun = get_default(fun, self.fun)
        fun_grad = get_default(fun_grad, self.fun_grad)
        lin_solver = get_default(lin_solver, self.lin_solver)
        iter_hook = get_default(iter_hook, self.iter_hook)
        status = get_default(status, self.status)

        ls_eps_a, ls_eps_r = lin_solver.get_tolerance()
        eps_a = get_default(ls_eps_a, 1.0)
        eps_r = get_default(ls_eps_r, 1.0)
        lin_red = conf.eps_a * conf.lin_red

        time_stats = {}

        vec_x = vec_x0.copy()
        vec_x_last = vec_x0.copy()
        vec_dx = None

        if self.log is not None:
            self.log.plot_vlines(color='r', linewidth=1.0)

        err = err0 = -1.0
        err_last = -1.0
        it = 0
        while 1:
            if iter_hook is not None:
                iter_hook(self, vec_x, it, err, err0)

            ls = 1.0
            vec_dx0 = vec_dx;
            while 1:
                tt = time.clock()

                try:
                    vec_r = fun(vec_x)

                except ValueError:
                    if (it == 0) or (ls < conf.ls_min):
                        output('giving up!')
                        raise

                    else:
                        ok = False

                else:
                    ok = True

                time_stats['rezidual'] = time.clock() - tt
                if ok:
                    try:
                        err = nla.norm(vec_r)
                    except:
                        output('infs or nans in the residual:', vec_r)
                        output(nm.isfinite(vec_r).all())
                        debug()

                    if self.log is not None:
                        self.log(err, it)

                    if it == 0:
                        err0 = err;
                        break
                    if err < (err_last * conf.ls_on): break
                    red = conf.ls_red;
                    output('linesearch: iter %d, (%.5e < %.5e) (new ls: %e)'
                           % (it, err, err_last * conf.ls_on, red * ls))
                else: # Failure.
                    if conf.give_up_warp:
                        output('giving up!')
                        break

                    red = conf.ls_red_warp;
                    output('rezidual computation failed for iter %d'
                           ' (new ls: %e)!' % (it, red * ls))

                if ls < conf.ls_min:
                    output('linesearch failed, continuing anyway')
                    break

                ls *= red;

                vec_dx = ls * vec_dx0;
                vec_x = vec_x_last.copy() - vec_dx
            # End residual loop.

            if self.log is not None:
                self.log.plot_vlines([1], color='g', linewidth=0.5)

            err_last = err;
            vec_x_last = vec_x.copy()

            condition = conv_test(conf, it, err, err0)
            if condition >= 0:
                break

            if (not ok) and conf.give_up_warp:
                condition = 2
                break

            tt = time.clock()
            if conf.problem == 'nonlinear':
                mtx_a = fun_grad(vec_x)

            else:
                mtx_a = fun_grad('linear')

            time_stats['matrix'] = time.clock() - tt

            if conf.check:
                tt = time.clock()
                wt = check_tangent_matrix(conf, vec_x, fun, fun_grad)
                time_stats['check'] = time.clock() - tt - wt

            if conf.lin_precision is not None:
                if ls_eps_a is not None:
                    eps_a = max(err * conf.lin_precision, ls_eps_a)

                elif ls_eps_r is not None:
                    eps_r = max(conf.lin_precision, ls_eps_r)

                lin_red = max(eps_a, err * eps_r)

            if conf.verbose:
                output('solving linear system...')

            tt = time.clock()
            vec_dx = lin_solver(vec_r, x0=vec_x,
                                eps_a=eps_a, eps_r=eps_r, mtx=mtx_a)
            time_stats['solve'] = time.clock() - tt

            if conf.verbose:
                output('...done')

            for kv in time_stats.iteritems():
                output('%10s: %7.2f [s]' % kv)

            vec_e = mtx_a * vec_dx - vec_r
            lerr = nla.norm(vec_e)
            if lerr > lin_red:
                output('warning: linear system solution precision is lower')
                output('then the value set in solver options! (err = %e < %e)'
                       % (lerr, lin_red))

            vec_x -= vec_dx
            it += 1

        if status is not None:
            status['time_stats'] = time_stats
            status['err0'] = err0
            status['err'] = err
            status['n_iter'] = it
            status['condition'] = condition

        if conf.log.plot is not None:
            if self.log is not None:
                self.log(save_figure=conf.log.plot)

        return vec_x

class ScipyBroyden(NonlinearSolver):
    """
    Interface to Broyden and Anderson solvers from scipy.optimize.
    """
    name = 'nls.scipy_broyden_like'

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Missing items are left to scipy defaults. Unused options are ignored.

        Example configuration, all items::

            solver_1 = {
                'name' : 'broyden',
                'kind' : 'nls.scipy_broyden_like',

                'method'  : 'broyden3',
                'i_max'   : 10,
                'alpha'   : 0.9,
                'M'       : 5,
                'w0'      : 0.1,
                'f_tol'   : 6e-6,
                'verbose' : True,
            }
        """
        get = make_get_conf(conf, kwargs)
        common = NonlinearSolver.process_conf(conf)

        return Struct(method=get('method', 'broyden3'),
                      i_max=get('i_max', 10),
                      alpha=get('alpha', 0.9),
                      M=get('M', 5),
                      f_tol=get('f_tol', 6e-6),
                      w0=get('w0', 0.1),
                      verbose=get('verbose', False)) + common

    def __init__(self, conf, **kwargs):
        NonlinearSolver.__init__(self, conf, **kwargs)
        self.set_method(self.conf)

    def set_method(self, conf):
        import scipy.optimize as so

        try:
            solver = getattr(so, conf.method)
        except AttributeError:
            output('scipy solver %s does not exist!' % conf.method)
            output('using broyden3 instead')
            solver = so.broyden3
        self.solver = solver

    def __call__(self, vec_x0, conf=None, fun=None, fun_grad=None,
                 lin_solver=None, iter_hook=None, status=None):
        if conf is not None:
            self.set_method(conf)
        else:
            conf = self.conf
        fun = get_default(fun, self.fun)
        status = get_default(status, self.status)

        tt = time.clock()

        kwargs = {'iter' : conf.i_max,
                  'alpha' : conf.alpha,
                  'verbose' : conf.verbose}

        if conf.method == 'broyden_generalized':
            kwargs.update({'M' : conf.M})

        elif conf.method in ['anderson', 'anderson2']:
            kwargs.update({'M' : conf.M, 'w0' : conf.w0})

        if conf.method in ['anderson', 'anderson2',
                           'broyden', 'broyden2' , 'newton_krylov']:
            kwargs.update({'f_tol' : conf.f_tol })

        vec_x = self.solver(fun, vec_x0, **kwargs)
        vec_x = nm.asarray(vec_x)

        if status is not None:
            status['time_stats'] = time.clock() - tt

        return vec_x

########NEW FILE########
__FILENAME__ = optimize
import time

import numpy as nm
import numpy.linalg as nla

from sfepy.base.base import output, get_default, pause, Struct
from sfepy.base.log import Log, get_logging_conf
from sfepy.solvers.solvers import make_get_conf, OptimizationSolver

import scipy.optimize as sopt
import scipy.optimize.linesearch as linesearch

##
# 19.04.2006, c
# 26.04.2006
# 28.04.2006
def conv_test( conf, it, of, of0, ofg_norm = None ):
    """
    Returns
    -------
    flag : int
        * -1 ... continue
        *  0 ... small OF -> stop
        *  1 ... i_max reached -> stop
        *  2 ... small OFG -> stop
        *  3 ... small relative decrase of OF
     """

    status = -1
    output( 'opt: iter: %d, of: %e (||ofg||: %e)' % (it, of, ofg_norm) )
#    print (of0 - of), (conf.eps_rd * of0)

    if (abs( of ) < conf.eps_of):
        status = 0
    elif ofg_norm and (ofg_norm < conf.eps_ofg):
        status = 2
    elif (it > 0) and (abs(of0 - of) < (conf.eps_rd * abs( of0 ))):
        status = 3
        
    if (status == -1) and (it >= conf.i_max):
        status = 1

    return status

##
# 19.04.2006, from scipy.optimize
# 21.04.2006
# 27.03.2007
def wrap_function( function, args ):
    ncalls = [0]
    times = []
    def function_wrapper( x ):
        ncalls[0] += 1
        tt = time.time()
        out = function( x, *args )
        tt2 = time.time()
        if tt2 < tt:
            raise RuntimeError, '%f >= %f' % (tt, tt2)
        times.append( tt2 - tt )
        return out
    return ncalls, times, function_wrapper

##
# 20.04.2006, c
def check_gradient( xit, aofg, fn_of, delta, check ):

    dofg = nm.zeros_like( aofg )
    xd = xit.copy()
    for ii in xrange( xit.shape[0] ):
        xd[ii] = xit[ii] + delta
        ofp = fn_of( xd )

        xd[ii] = xit[ii] - delta
        ofm = fn_of( xd )

        xd[ii] = xit[ii]

        dofg[ii] = 0.5 * (ofp - ofm) / delta

        output( '**********', ii, aofg[ii], dofg[ii] )

    diff = abs( aofg - dofg )
    aux = nm.concatenate( (aofg[:,nm.newaxis], dofg[:,nm.newaxis],
                           diff[:,nm.newaxis]), 1 )
    output( aux )
    output( nla.norm( diff, nm.Inf ) )
    aofg.tofile( 'aofg.txt', ' ' )
    dofg.tofile( 'dofg.txt', ' ' )
    diff.tofile( 'diff.txt', ' ' )
    if check == 2:
        import pylab
        pylab.plot( aofg )
        pylab.plot( dofg )
        pylab.legend( ('analytical', 'finite difference') )
        pylab.show()
    pause( 'gradient checking done' )

##
# 17.10.2007, c
class FMinSteepestDescent( OptimizationSolver ):
    name = 'opt.fmin_sd'

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Missing items are set to default values.

        Example configuration, all items::

            solver_0 = {
                'name'      : 'fmin_sd',
                'kind'      : 'opt.fmin_sd',

                'i_max'      : 10,
                'eps_rd'     : 1e-5, # Relative delta of objective function
                'eps_of'     : 1e-4,
                'eps_ofg'    : 1e-8,
                'norm'      : nm.Inf,
                'ls'        : True, # Linesearch.
                'ls_method'  : 'backtracking', # 'backtracking' or 'full'
                'ls0'       : 0.25,
                'ls_red'     : 0.5,
                'ls_red_warp' : 0.1,
                'ls_on'      : 0.99999,
                'ls_min'     : 1e-5,
                'check'     : 0,
                'delta'     : 1e-6,
                'output'    : None, # 'itc'
                'log'       : {'text' : 'output/log.txt',
                               'plot' : 'output/log.png'},
                'yscales'   : ['linear', 'log', 'log', 'linear'],
            }
        """
        get = make_get_conf(conf, kwargs)
        common = OptimizationSolver.process_conf(conf)

        log = get_logging_conf(conf)
        log = Struct(name='log_conf', **log)
        is_any_log = (log.text is not None) or (log.plot is not None)

        return Struct(i_max=get('i_max', 10),
                      eps_rd=get('eps_rd', 1e-5),
                      eps_of=get('eps_of', 1e-4),
                      eps_ofg=get('eps_ofg', 1e-8),
                      norm=get('norm', nm.Inf),
                      ls=get('ls', True),
                      ls_method=get('ls_method', 'backtracking'),
                      ls0=get('ls0', 0.25),
                      ls_red=get('ls_red', 0.5),
                      ls_red_warp=get('ls_red_warp', 0.1),
                      ls_on=get('ls_on', 0.99999),
                      ls_min=get('ls_min', 1e-5),
                      check=get('check', 0),
                      delta=get('delta', 1e-6),
                      output=get('output', None),
                      yscales=get('yscales',
                                  ['linear', 'log', 'log', 'linear']),
                      log=log,
                      is_any_log=is_any_log) + common

    ##
    # 17.10.2007, c
    def __init__( self, conf, **kwargs ):
        OptimizationSolver.__init__( self, conf, **kwargs )

        conf = self.conf
        if conf.is_any_log:
            self.log = Log([[r'$||\Psi||$'], [r'$||\nabla \Psi||$'],
                            [r'$\alpha$'], ['iteration']],
                           xlabels=['', '', 'all iterations', 'all iterations'],
                           yscales=conf.yscales,
                           is_plot=conf.log.plot is not None,
                           log_filename=conf.log.text,
                           formats=[['%.8e'], ['%.3e'], ['%.3e'], ['%d']])
        else:
            self.log = None

    ##
    # 19.04.2006, c
    # 20.04.2006
    # 21.04.2006
    # 26.04.2006
    # 06.06.2006
    # 07.06.2006
    # 04.09.2006
    # 21.03.2007
    # 17.10.2007, from fmin_sd()
    def __call__( self, x0, conf = None, obj_fun = None, obj_fun_grad = None,
                  status = None, obj_args = None ):
#    def fmin_sd( conf, x0, fn_of, fn_ofg, args = () ):

        conf = get_default( conf, self.conf )
        obj_fun = get_default( obj_fun, self.obj_fun )
        obj_fun_grad = get_default( obj_fun_grad, self.obj_fun_grad )
        status = get_default( status, self.status )
        obj_args = get_default( obj_args, self.obj_args )

        if conf.output:
            globals()['output'] = conf.output

        output( 'entering optimization loop...' )

        nc_of, tt_of, fn_of = wrap_function( obj_fun, obj_args )
        nc_ofg, tt_ofg, fn_ofg = wrap_function( obj_fun_grad, obj_args )

        time_stats = {'of' : tt_of, 'ofg': tt_ofg, 'check' : []}

        ofg = None

        it = 0
        xit = x0.copy()
        while 1:

            of = fn_of( xit )

            if it == 0:
                of0 = ofit0 = of_prev = of
                of_prev_prev = of + 5000.0

            if ofg is None:
                ofg = fn_ofg( xit )

            if conf.check:
                tt = time.clock()
                check_gradient( xit, ofg, fn_of, conf.delta, conf.check )
                time_stats['check'].append( time.clock() - tt )

            ofg_norm = nla.norm( ofg, conf.norm )

            ret = conv_test( conf, it, of, ofit0, ofg_norm )
            if ret >= 0:
                break
            ofit0 = of

            ##
            # Backtrack (on errors).
            alpha = conf.ls0
            can_ls = True
            while 1:
                xit2 = xit - alpha * ofg
                aux = fn_of( xit2 )

                if self.log is not None:
                    self.log(of, ofg_norm, alpha, it)

                if aux is None:
                    alpha *= conf.ls_red_warp
                    can_ls = False
                    output( 'warp: reducing step (%f)' % alpha )
                elif conf.ls and conf.ls_method == 'backtracking':
                    if aux < of * conf.ls_on: break
                    alpha *= conf.ls_red
                    output( 'backtracking: reducing step (%f)' % alpha )
                else:
                    of_prev_prev = of_prev
                    of_prev = aux
                    break

                if alpha < conf.ls_min:
                    if aux is None:
                        raise RuntimeError, 'giving up...'
                    output( 'linesearch failed, continuing anyway' )
                    break

            # These values are modified by the line search, even if it fails
            of_prev_bak = of_prev
            of_prev_prev_bak = of_prev_prev

            if conf.ls and can_ls and conf.ls_method == 'full':
                output( 'full linesearch...' )
                alpha, fc, gc, of_prev, of_prev_prev, ofg1 = \
                       linesearch.line_search(fn_of,fn_ofg,xit,
                                              -ofg,ofg,of_prev,of_prev_prev,
                                              c2=0.4)
                if alpha is None:  # line search failed -- use different one.
                    alpha, fc, gc, of_prev, of_prev_prev, ofg1 = \
                           sopt.line_search(fn_of,fn_ofg,xit,
                                            -ofg,ofg,of_prev_bak,
                                            of_prev_prev_bak)
                    if alpha is None or alpha == 0:
                        # This line search also failed to find a better solution.
                        ret = 3
                        break
                output( ' -> alpha: %.8e' % alpha )
            else:
                if conf.ls_method == 'full':
                    output( 'full linesearch off (%s and %s)' % (conf.ls,
                                                                 can_ls) )
                ofg1 = None

            if self.log is not None:
                self.log.plot_vlines(color='g', linewidth=0.5)

            xit = xit - alpha * ofg
            if ofg1 is None:
                ofg = None
            else:
                ofg = ofg1.copy()

            for key, val in time_stats.iteritems():
                if len( val ):
                    output( '%10s: %7.2f [s]' % (key, val[-1]) )

            it = it + 1

        output( 'status:               %d' % ret )
        output( 'initial value:        %.8e' % of0 )
        output( 'current value:        %.8e' % of )
        output( 'iterations:           %d' % it )
        output( 'function evaluations: %d in %.2f [s]' \
              % (nc_of[0], nm.sum( time_stats['of'] ) ) )
        output( 'gradient evaluations: %d in %.2f [s]' \
              % (nc_ofg[0], nm.sum( time_stats['ofg'] ) ) )

        if self.log is not None:
            self.log(of, ofg_norm, alpha, it)

            if conf.log.plot is not None:
                self.log(save_figure=conf.log.plot,
                         finished=True)
            else:
                self.log(finished=True)
                
        if status is not None:
            status['log'] = self.log
            status['status'] = status
            status['of0'] = of0
            status['of'] = of
            status['it'] = it
            status['nc_of'] = nc_of[0]
            status['nc_ofg'] = nc_ofg[0]
            status['time_stats'] = time_stats

        return xit

class ScipyFMinSolver(OptimizationSolver):
    """
    Interface to SciPy optimization solvers scipy.optimize.fmin_*.
    """
    name = 'nls.scipy_fmin_like'

    _i_max_name  = {
        'fmin' : 'maxiter',
        'fmin_bfgs' : 'maxiter',
        'fmin_cg' : 'maxiter',
        'fmin_cobyla' : 'maxfun',
        'fmin_l_bfgs_b' : 'maxfun',
        'fmin_ncg' : 'maxiter',
        'fmin_powell' : 'maxiter',
        'fmin_slsqp' : 'iter',
        'fmin_tnc' : 'maxfun',
    }
    _omit = ('name', 'kind', 'method', 'i_max', 'verbose')
    _has_grad = ('fmin_bfgs', 'fmin_cg', 'fmin_l_bfgs_b', 'fmin_ncg',
                 'fmin_slsqp', 'fmin_tnc')

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Missing items are left to SciPy defaults. Unused options are ignored.

        Besides 'i_max', use option names according to scipy.optimize
        function arguments. The 'i_max' translates either to 'maxiter'
        or 'maxfun' as available.

        Example configuration::

            solver_1 = {
                'name' : 'fmin',
                'kind' : 'nls.scipy_fmin_like',

                'method'  : 'bfgs',
                'i_max'   : 10,
                'verbose' : True,

                'gtol' : 1e-7
            }
        """
        get = make_get_conf(conf, kwargs)
        common = OptimizationSolver.process_conf(conf)

        opts = Struct(method=get('method', 'fmin'),
                      i_max=get('i_max', 10),
                      verbose=get('verbose', False)) + common

        other = {}
        keys = opts.to_dict().keys()

        for key, val in conf.to_dict().iteritems():
            if key not in keys:
                other[key] = val

        return opts + Struct(**other)

    def __init__(self, conf, **kwargs):
        OptimizationSolver.__init__(self, conf, **kwargs)
        self.set_method(self.conf)

    def set_method(self, conf):
        import scipy.optimize as so

        try:
            solver = getattr(so, conf.method)
        except AttributeError:
            raise ValueError('scipy solver %s does not exist!' % conf.method)

        self.solver = solver

    def __call__(self, x0, conf=None, obj_fun=None, obj_fun_grad=None,
                 status=None, obj_args=None):
        import inspect

        if conf is not None:
            self.set_method(conf)

        else:
            conf = self.conf

        obj_fun = get_default(obj_fun, self.obj_fun)
        obj_fun_grad = get_default(obj_fun_grad, self.obj_fun_grad)
        status = get_default(status, self.status)
        obj_args = get_default(obj_args, self.obj_args)

        tt = time.clock()

        kwargs = {self._i_max_name[conf.method] : conf.i_max,
                  'args' : obj_args}

        if conf.method in self._has_grad:
            kwargs['fprime'] = obj_fun_grad

        if 'disp' in inspect.getargspec(self.solver)[0]:
            kwargs['disp'] = conf.verbose

        for key, val in conf.to_dict().iteritems():
            if key not in self._omit:
                kwargs[key] = val

        out = self.solver(obj_fun, x0, **kwargs)

        if status is not None:
            status['time_stats'] = time.clock() - tt

        return out

########NEW FILE########
__FILENAME__ = oseen
import time

import numpy as nm
import numpy.linalg as nla

from sfepy.base.base import output, get_default, Struct
from sfepy.base.log import Log, get_logging_conf
from sfepy.solvers.solvers import make_get_conf, NonlinearSolver
from nls import conv_test

class StabilizationFunction(Struct):
    """
    Definition of stabilization material function for the Oseen solver.

    Notes
    -----
    - tau_red <= 1.0; if tau is None: tau = tau_red * delta
    - diameter mode: 'edge': longest edge 'volume': volume-based, 'max': max. of
      previous
    """

    def __init__(self, name_map, gamma=None, delta=None, tau=None, tau_red=1.0,
                 tau_mul=1.0, delta_mul=1.0, gamma_mul=1.0,
                 diameter_mode='max'):
        Struct.__init__(self, name_map=name_map,
                        gamma=gamma, delta=delta, tau=tau,
                        tau_red=tau_red, tau_mul=tau_mul, delta_mul=delta_mul,
                        gamma_mul=gamma_mul, diameter_mode=diameter_mode)

    def setup(self, problem):
        """
        Setup common problem-dependent data.
        """
        variables = problem.get_variables()

        ns = self.name_map

        # Indices to the state vector.
        ii = {}
        ii['u'] = variables.get_indx(ns['u'])
        ii['us'] = variables.get_indx(ns['u'], stripped=True)
        ii['ps'] = variables.get_indx(ns['p'], stripped=True)
        self.indices = ii

        materials = problem.get_materials()

        # The viscosity.
        fluid_mat = materials[ns['fluid']]
        self.viscosity = fluid_mat.function()[ns['viscosity']]

        # The Friedrich's constant.
        self.c_friedrichs = problem.domain.get_diameter()
        self.sigma = 1e-12 # 1 / dt.

        self.b_norm = 1.0

    def get_maps(self):
        """
        Get the maps of names and indices of variables in state vector.
        """
        return self.name_map, self.indices

    def __call__(self, ts, coor, mode=None, term=None, problem=None,
                 b_norm=None, **kwargs):
        """
        The actual material function.
        """
        if mode != 'qp': return

        if not hasattr(self, 'viscosity'):
            self.setup(problem)

        ns = self.name_map

        # Update stored b_norm.
        self.b_norm = get_default(b_norm, self.b_norm)

        output('|b|_max (mat_fun):', self.b_norm)
        gamma = self.viscosity + self.b_norm * self.c_friedrichs

        data = {}
        if self.gamma is None:
            _gamma = self.gamma_mul * gamma

        else:
            _gamma = nm.asarray(self.gamma_mul * self.gamma, dtype=nm.float64)
        _gamma = nm.tile(_gamma, (coor.shape[0], 1, 1))

        if self.delta is None:
            # Element diameter modes.
            dm = {'edge': 0, 'volume': 1, 'max': 2}[self.diameter_mode]

            field = problem.fields[ns['velocity']]
            region = term.region
            diameters2 = []
            for ig in term.iter_groups():
                vg, _ = field.get_mapping(ig, region, term.integral, 'volume')
                cells = region.get_cells(ig)
                d2 = problem.domain.get_element_diameters(ig, cells, vg, dm)
                diameters2.append(d2)
            self.diameters2 = nm.concatenate(diameters2)

            val1 = min(1.0, 1.0 / self.sigma)
            val2 = self.sigma * self.c_friedrichs**2
            val3 = (self.b_norm**2) \
                   * min((self.c_friedrichs**2) / self.viscosity,
                         1.0 / self.sigma)

            n_qp = coor.shape[0] / self.diameters2.shape[0]
            diameters2 = nm.repeat(self.diameters2, n_qp)
            diameters2.shape = diameters2.shape + (1, 1)

            _delta = self.delta_mul * val1 * diameters2 / (_gamma + val2 + val3)

        else:
            val = nm.asarray(self.delta_mul * self.delta, dtype=nm.float64)
            _delta = nm.tile(val, (coor.shape[0], 1, 1))

        if self.tau is None:
            _tau = self.tau_red * _delta

        else:
            _tau = nm.asarray(self.tau_mul * self.tau, dtype=nm.float64)
            _tau = nm.tile(_tau, (coor.shape[0], 1, 1))

        data[ns['gamma']] = _gamma
        data[ns['delta']] = _delta
        data[ns['tau']] = _tau

        return data

##
# 26.07.2007, c
def are_close( a, b, rtol = 0.2, atol = 1e-8 ):
    return False
#    return abs( a - b ) <= max( atol, rtol * abs( b ) )

##
# 26.07.2007, c
def scale_matrix( mtx, indx, factor ):
    ptr0 = mtx.indptr[indx.start]
    ptr1 = mtx.indptr[indx.stop]
    mtx.data[ptr0:ptr1] *= factor

##
# 11.10.2007, c
class Oseen( NonlinearSolver ):
    name = 'nls.oseen'

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Missing items are set to default values.

        Example configuration, all items::

            solver_1 = {
                'name' : 'oseen',
                'kind' : 'nls.oseen',

                'needs_problem_instance' : True,
                'stabil_mat' : 'stabil',

                'adimensionalize' : False,
                'check_navier_stokes_rezidual' : False,

                'i_max'      : 10,
                'eps_a'      : 1e-8,
                'eps_r'      : 1.0,
                'macheps'    : 1e-16,
                'lin_red'    : 1e-2, # Linear system error < (eps_a * lin_red).
                'log'        : {'text' : 'oseen_log.txt',
                                'plot' : 'oseen_log.png'},
            }
        """
        get = make_get_conf(conf, kwargs)
        common = NonlinearSolver.process_conf(conf)

        # Compulsory.
        needs_problem_instance = get('needs_problem_instance', True)
        if not needs_problem_instance:
            msg = 'set solver option "needs_problem_instance" to True!'
            raise ValueError(msg)

        stabil_mat = get('stabil_mat', None, 'missing "stabil_mat" in options!')

        # With defaults.
        adimensionalize = get('adimensionalize', False)
        if adimensionalize:
            raise NotImplementedError

        check = get('check_navier_stokes_rezidual', False)

        log = get_logging_conf(conf)
        log = Struct(name='log_conf', **log)
        is_any_log = (log.text is not None) or (log.plot is not None)

        return Struct(needs_problem_instance=needs_problem_instance,
                      stabil_mat=stabil_mat,
                      adimensionalize=adimensionalize,
                      check_navier_stokes_rezidual=check,
                      i_max=get('i_max', 1),
                      eps_a=get('eps_a', 1e-10),
                      eps_r=get('eps_r', 1.0),
                      macheps=get('macheps', nm.finfo(nm.float64).eps),
                      lin_red=get('lin_red', 1.0),
                      lin_precision=get('lin_precision', None),
                      log=log,
                      is_any_log=is_any_log) + common

    def __init__( self, conf, **kwargs ):
        NonlinearSolver.__init__( self, conf, **kwargs )

        conf = self.conf
        if conf.is_any_log:
            self.log = Log([[r'$||r||$'], ['iteration'],
                            [r'$\gamma$', r'$\max(\delta)$', r'$\max(\tau)$']],
                           xlabels=['', '', 'all iterations'],
                           ylabels=[r'$||r||$', 'iteration', 'stabilization'],
                           yscales=['log', 'linear', 'log'],
                           log_filename=conf.log.text,
                           formats=[['%.8e'], ['%d'],
                                    ['%.8e', '%.8e', '%.8e']])

        else:
            self.log = None

    def __call__( self, vec_x0, conf = None, fun = None, fun_grad = None,
                  lin_solver = None, status = None, problem = None ):
        """
        Oseen solver is problem-specific - it requires a Problem instance.
        """
        conf = get_default( conf, self.conf )
        fun = get_default( fun, self.fun )
        fun_grad = get_default( fun_grad, self.fun_grad )
        lin_solver = get_default( lin_solver, self.lin_solver )
        status = get_default( status, self.status )
        problem = get_default( problem, self.problem )

        if problem is None:
            msg = 'set solver option "needs_problem_instance" to True!'
            raise ValueError(msg)

        time_stats = {}

        stabil = problem.get_materials()[conf.stabil_mat]
        ns, ii = stabil.function.function.get_maps()

        variables = problem.get_variables()
        update_var = variables.set_data_from_state
        make_full_vec = variables.make_full_vec

        print 'problem size:'
        print '    velocity: %s' % ii['us']
        print '    pressure: %s' % ii['ps']

        vec_x = vec_x0.copy()
        vec_x_prev = vec_x0.copy()
        vec_dx = None

        if self.log is not None:
            self.log.plot_vlines(color='r', linewidth=1.0)

        err0 = -1.0
        it = 0
        while 1:
            vec_x_prev_f = make_full_vec( vec_x_prev )
            update_var( ns['b'], vec_x_prev_f, ns['u'] )

            vec_b = vec_x_prev_f[ii['u']]
            b_norm = nla.norm( vec_b, nm.inf )
            print '|b|_max: %.12e' % b_norm

            vec_x_f = make_full_vec( vec_x )
            vec_u = vec_x_f[ii['u']]
            u_norm = nla.norm( vec_u, nm.inf )
            print '|u|_max: %.2e' % u_norm

            stabil.function.set_extra_args(b_norm=b_norm)
            stabil.time_update(None, problem.equations, mode='force',
                               problem=problem)
            max_pars = stabil.reduce_on_datas( lambda a, b: max( a, b.max() ) )
            print 'stabilization parameters:'
            print '                   gamma: %.12e' % max_pars[ns['gamma']]
            print '            max( delta ): %.12e' % max_pars[ns['delta']]
            print '              max( tau ): %.12e' % max_pars[ns['tau']]

            if (not are_close( b_norm, 1.0 )) and conf.adimensionalize:
                adimensionalize = True
            else:
                adimensionalize = False

            tt = time.clock()
            try:
                vec_r = fun( vec_x )
            except ValueError:
                ok = False
            else:
                ok = True
            time_stats['rezidual'] = time.clock() - tt
            if ok:
                err = nla.norm( vec_r )
                if it == 0:
                    err0 = err;
                else:
                    err += nla.norm( vec_dx )
            else: # Failure.
                output( 'rezidual computation failed for iter %d!' % it )
                raise RuntimeError( 'giving up...' )

            if self.log is not None:
                self.log(err, it,
                         max_pars[ns['gamma']], max_pars[ns['delta']],
                         max_pars[ns['tau']])

            condition = conv_test( conf, it, err, err0 )
            if condition >= 0:
                break

            if adimensionalize:
                output( 'adimensionalizing' )
                ## mat.viscosity = viscosity / b_norm
                ## vec_r[indx_us] /= b_norm

            tt = time.clock()
            try:
                mtx_a = fun_grad( vec_x )
            except ValueError:
                ok = False
            else:
                ok = True
            time_stats['matrix'] = time.clock() - tt
            if not ok:
                raise RuntimeError( 'giving up...' )

            tt = time.clock() 
            vec_dx = lin_solver(vec_r, x0=vec_x, mtx=mtx_a)
            time_stats['solve'] = time.clock() - tt

            vec_e = mtx_a * vec_dx - vec_r
            lerr = nla.norm( vec_e )
            if lerr > (conf.eps_a * conf.lin_red):
                output( 'linear system not solved! (err = %e)' % lerr )

            if adimensionalize:
                output( 'restoring pressure...' )
                ## vec_dx[indx_ps] *= b_norm

            dx_norm = nla.norm( vec_dx )
            output( '||dx||: %.2e' % dx_norm )

            for kv in time_stats.iteritems():
                output( '%10s: %7.2f [s]' % kv )

            vec_x_prev = vec_x.copy()
            vec_x -= vec_dx
            it += 1

        if conf.check_navier_stokes_rezidual:

            t1 = '+ dw_div_grad.%s.%s( %s.viscosity, %s, %s )' \
                 % (ns['i2'], ns['omega'], ns['fluid'], ns['v'], ns['u'])
##             t2 = '+ dw_lin_convect.%s( %s, %s, %s )' % (ns['omega'],
##                                                         ns['v'], b_name, ns['u'])
            t2 = '+ dw_convect.%s.%s( %s, %s )' % (ns['i2'], ns['omega'],
                                                   ns['v'], ns['u'])
            t3 = '- dw_stokes.%s.%s( %s, %s )' % (ns['i1'], ns['omega'],
                                                  ns['v'], ns['p'])
            t4 = 'dw_stokes.%s.%s( %s, %s )' % (ns['i1'], ns['omega'],
                                                ns['u'], ns['q'])
            equations = {
                'balance' : ' '.join( (t1, t2, t3) ),
                'incompressibility' : t4,
            }
            problem.set_equations( equations )
            try:
                vec_rns0 = fun( vec_x0 )
                vec_rns = fun( vec_x )
            except ValueError:
                ok = False
            else:
                ok = True
            if not ok:
                print 'Navier-Stokes rezidual computation failed!'
                err_ns = err_ns0 = None
            else:
                err_ns0 = nla.norm( vec_rns0 )
                err_ns = nla.norm( vec_rns )
            print 'Navier-Stokes rezidual0: %.8e' % err_ns0
            print 'Navier-Stokes rezidual : %.8e' % err_ns
            print 'b - u: %.8e' % nla.norm( vec_b - vec_u )
            print condition

        else:
            err_ns = None

        if status is not None:
            status['time_stats'] = time_stats
            status['err0'] = err0
            status['err'] = err
            status['err_ns'] = err_ns
            status['condition'] = condition

        if conf.log.plot is not None:
            if self.log is not None:
                self.log(save_figure=conf.log.plot)
                
        return vec_x

########NEW FILE########
__FILENAME__ = petsc_worker
#!/usr/bin/env python
"""
PETSc solver worker process.
"""
import time
import sys

try:
    import petsc4py
    petsc4py.init(sys.argv)

    from petsc4py import PETSc

except ImportError:
    pass

def solve():
    opts = PETSc.Options()

    mat_filename = opts.getString('-mtx', 'mtx.dat')
    rhs_filename = opts.getString('-rhs', 'rhs.dat')
    sol0_filename = opts.getString('-sol0', '')
    sol_filename = opts.getString('-sol', 'sol.dat')
    status_filename = opts.getString('-status', 'status.txt')

    view_mtx = PETSc.Viewer().createBinary(mat_filename, mode='r')
    view_rhs = PETSc.Viewer().createBinary(rhs_filename, mode='r')

    mtx = PETSc.Mat().load(view_mtx)
    rhs = PETSc.Vec().load(view_rhs)

    ksp = PETSc.KSP().create()
    ksp.setOperators(mtx)
    ksp.setFromOptions()

    if not sol0_filename:
        sol = rhs.duplicate()

    else:
        view_sol0 = PETSc.Viewer().createBinary(sol0_filename, mode='r')
        sol = PETSc.Vec().load(view_sol0)
        ksp.setInitialGuessNonzero(True)

    tt = time.clock()
    ksp.solve(rhs, sol)
    elapsed = time.clock() - tt

    view_sol = PETSc.Viewer().createBinary(sol_filename, mode='w')
    sol.view(view_sol)

    fd = open(status_filename, 'w')
    fd.write('%d %.2f' % (ksp.reason, elapsed))
    fd.close()

if __name__ == '__main__':
    solve()

########NEW FILE########
__FILENAME__ = semismooth_newton
import time

import numpy as nm
import numpy.linalg as nla
import scipy.sparse as sp

from sfepy.base.base import output, get_default, debug, Struct
from sfepy.base.log import get_logging_conf
from sfepy.solvers.solvers import make_get_conf, NonlinearSolver
from sfepy.solvers.nls import Newton, conv_test
from sfepy.linalg import compose_sparse

class SemismoothNewton(Newton):
    r"""
    The semi-smooth Newton method for solving problems of the following
    structure:

    .. math::
        \begin{split}
          & F(y) = 0 \\
          & A(y) \ge 0 \;,\ B(y) \ge 0 \;,\ \langle A(y), B(y) \rangle = 0
        \end{split}

    The function :math:`F(y)` represents the smooth part of the problem.

    Regular step: :math:`y \leftarrow y - J(y)^{-1} \Phi(y)`

    Steepest descent step: :math:`y \leftarrow y - \beta J(y) \Phi(y)`

    Notes
    -----
    Although fun_smooth_grad() computes the gradient of the smooth part only,
    it should return the global matrix, where the non-smooth part is
    uninitialized, but pre-allocated.
    """
    name = 'nls.semismooth_newton'

    _colors = {'regular' : 'g', 'steepest_descent' : 'k'}

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Missing items are set to default values.

        Example configuration, all items::

            solver_1 = {
                'name' : 'semismooth_newton',
                'kind' : 'nls.semismooth_newton',

                'semismooth' : True,

                'i_max'      : 10,
                'eps_a'      : 1e-8,
                'eps_r'      : 1e-2,
                'macheps'   : 1e-16,
                'lin_red'    : 1e-2, # Linear system error < (eps_a * lin_red).
                'ls_red_reg' : 0.1,
                'ls_red_alt' : 0.01,
                'ls_red_warp' : 0.001,
                'ls_on'      : 0.9,
                'ls_min'     : 1e-10,
                'log'        : {'plot' : 'convergence.png'},
            }
        """
        get = make_get_conf(conf, kwargs)
        common = NonlinearSolver.process_conf(conf)

        log = get_logging_conf(conf)
        log = Struct(name='log_conf', **log)
        is_any_log = (log.text is not None) or (log.plot is not None)

        return Struct(semismooth=get('semismooth', True),
                      i_max=get('i_max', 1),
                      eps_a=get('eps_a', 1e-10),
                      eps_r=get('eps_r', 1.0),
                      macheps=get('macheps', nm.finfo(nm.float64).eps),
                      lin_red=get('lin_red', 1.0),
                      ls_red=get('ls_red', 0.1),
                      ls_red_warp=get('ls_red_warp', 0.001),
                      ls_on=get('ls_on', 0.99999),
                      ls_min=get('ls_min', 1e-5),
                      log=log,
                      is_any_log=is_any_log) + common

    def __call__(self, vec_x0, conf=None, fun_smooth=None, fun_smooth_grad=None,
                 fun_a=None, fun_a_grad=None, fun_b=None, fun_b_grad=None,
                 lin_solver=None, status=None):

        conf = get_default(conf, self.conf)

        fun_smooth = get_default(fun_smooth, self.fun_smooth)
        fun_smooth_grad = get_default(fun_smooth_grad, self.fun_smooth_grad)
        fun_a = get_default(fun_a, self.fun_a)
        fun_a_grad = get_default(fun_a_grad, self.fun_a_grad)
        fun_b = get_default(fun_b, self.fun_b)
        fun_b_grad = get_default(fun_b_grad, self.fun_b_grad)

        lin_solver = get_default(lin_solver, self.lin_solver)
        status = get_default(status, self.status)

        time_stats = {}

        vec_x = vec_x0.copy()
        vec_x_last = vec_x0.copy()
        vec_dx = None

        if self.log is not None:
            self.log.plot_vlines(color='r', linewidth=1.0)

        err0 = -1.0
        err_last = -1.0
        it = 0
        step_mode = 'regular'
        r_last = None
        reuse_matrix = False
        while 1:

            ls = 1.0
            vec_dx0 = vec_dx;
            i_ls = 0
            while 1:
                tt = time.clock()

                try:
                    vec_smooth_r = fun_smooth(vec_x)
                    vec_a_r = fun_a(vec_x)
                    vec_b_r = fun_b(vec_x)

                except ValueError:
                    vec_smooth_r = vec_semismooth_r = None
                    if (it == 0) or (ls < conf.ls_min):
                        output('giving up!')
                        raise

                    else:
                        ok = False

                else:
                    if conf.semismooth:
                        # Semi-smooth equation.
                        vec_semismooth_r = nm.sqrt(vec_a_r**2.0 + vec_b_r**2.0) \
                                           - (vec_a_r + vec_b_r)

                    else:
                        # Non-smooth equation (brute force).
                        vec_semismooth_r = nm.where(vec_a_r < vec_b_r,
                                                    vec_a_r, vec_b_r)

                    r_last = (vec_smooth_r, vec_a_r, vec_b_r, vec_semismooth_r)

                    ok = True

                time_stats['rezidual'] = time.clock() - tt

                if ok:
                    vec_r = nm.r_[vec_smooth_r, vec_semismooth_r]

                    try:
                        err = nla.norm(vec_r)
                    except:
                        output('infs or nans in the residual:', vec_semismooth_r)
                        output(nm.isfinite(vec_semismooth_r).all())
                        debug()

                    if self.log is not None:
                        self.log(err, it)

                    if it == 0:
                        err0 = err;
                        break

                    if err < (err_last * conf.ls_on):
                        step_mode = 'regular'
                        break

                    else:
                        output('%s step line search' % step_mode)

                        red = conf.ls_red[step_mode];
                        output('iter %d, (%.5e < %.5e) (new ls: %e)'\
                               % (it, err, err_last * conf.ls_on, red * ls))

                else: # Failed to compute rezidual.
                    red = conf.ls_red_warp;
                    output('rezidual computation failed for iter %d'
                           ' (new ls: %e)!' % (it, red * ls))

                if ls < conf.ls_min:
                    if step_mode == 'regular':
                        output('restore previous state')
                        vec_x = vec_x_last.copy()
                        vec_smooth_r, vec_a_r, vec_b_r, vec_semismooth_r = r_last
                        err = err_last
                        reuse_matrix = True

                        step_mode = 'steepest_descent'

                    else:
                        output('linesearch failed, continuing anyway')

                    break

                ls *= red;

                vec_dx = ls * vec_dx0;
                vec_x = vec_x_last.copy() - vec_dx

                i_ls += 1

            # End residual loop.

            output('%s step' % step_mode)

            if self.log is not None:
                self.log.plot_vlines([1],
                                     color=self._colors[step_mode],
                                     linewidth=0.5)

            err_last = err;
            vec_x_last = vec_x.copy()

            condition = conv_test(conf, it, err, err0)
            if condition >= 0:
                break

            tt = time.clock()

            if not reuse_matrix:
                mtx_jac = self.compute_jacobian(vec_x, fun_smooth_grad,
                                                fun_a_grad, fun_b_grad,
                                                vec_smooth_r,
                                                vec_a_r, vec_b_r)

            else:
                reuse_matrix = False

            time_stats['matrix'] = time.clock() - tt

            tt = time.clock() 

            if step_mode == 'regular':
                vec_dx = lin_solver(vec_r, mtx=mtx_jac)

                vec_e = mtx_jac * vec_dx - vec_r
                lerr = nla.norm(vec_e)
                if lerr > (conf.eps_a * conf.lin_red):
                    output('linear system not solved! (err = %e)' % lerr)

                    output('switching to steepest descent step')
                    step_mode = 'steepest_descent'
                    vec_dx = mtx_jac.T * vec_r

            else:
                vec_dx = mtx_jac.T * vec_r

            time_stats['solve'] = time.clock() - tt

            for kv in time_stats.iteritems():
                output('%10s: %7.2f [s]' % kv)

            vec_x -= vec_dx
            it += 1

        if status is not None:
            status['time_stats'] = time_stats
            status['err0'] = err0
            status['err'] = err
            status['condition'] = condition

        if conf.log.plot is not None:
            if self.log is not None:
                self.log(save_figure=conf.log.plot)

        return vec_x

    def compute_jacobian(self, vec_x, fun_smooth_grad, fun_a_grad, fun_b_grad,
                         vec_smooth_r, vec_a_r, vec_b_r):
        conf = self.conf

        mtx_s = fun_smooth_grad(vec_x)
        mtx_a = fun_a_grad(vec_x)
        mtx_b = fun_b_grad(vec_x)

        n_s = vec_smooth_r.shape[0]
        n_ns = vec_a_r.shape[0]

        if conf.semismooth:
            aa = nm.abs(vec_a_r)
            ab = nm.abs(vec_b_r)
            iz = nm.where((aa < (conf.macheps * max(aa.max(), 1.0)))
                          & (ab < (conf.macheps * max(ab.max(), 1.0))))[0]
            inz = nm.setdiff1d(nm.arange(n_ns), iz)

            output('non_active/active: %d/%d' % (len(inz), len(iz)))

            mul_a = nm.empty_like(vec_a_r)
            mul_b = nm.empty_like(mul_a)

            # Non-active part of the jacobian.
            if len(inz) > 0:
                a_r_nz = vec_a_r[inz]
                b_r_nz = vec_b_r[inz]

                sqrt_ab = nm.sqrt(a_r_nz**2.0 + b_r_nz**2.0)
                mul_a[inz] = (a_r_nz / sqrt_ab) - 1.0
                mul_b[inz] = (b_r_nz / sqrt_ab) - 1.0

            # Active part of the jacobian.
            if len(iz) > 0:
                vec_z = nm.zeros_like(vec_x)
                vec_z[n_s+iz] = 1.0

                mtx_a_z = mtx_a[iz]
                mtx_b_z = mtx_b[iz]

                sqrt_ab = nm.empty((iz.shape[0],), dtype=vec_a_r.dtype)
                for ir in range(len(iz)):
                    row_a_z = mtx_a_z[ir]
                    row_b_z = mtx_b_z[ir]
                    sqrt_ab[ir] = nm.sqrt((row_a_z * row_a_z.T).todense()
                                          + (row_b_z * row_b_z.T).todense())
                mul_a[iz] = ((mtx_a_z * vec_z) / sqrt_ab) - 1.0
                mul_b[iz] = ((mtx_b_z * vec_z) / sqrt_ab) - 1.0

        else:
            iz = nm.where(vec_a_r > vec_b_r)[0]
            mul_a = nm.zeros_like(vec_a_r)
            mul_b = nm.ones_like(mul_a)

            mul_a[iz] = 1.0
            mul_b[iz] = 0.0

        mtx_ns = sp.spdiags(mul_a, 0, n_ns, n_ns) * mtx_a \
                 + sp.spdiags(mul_b, 0, n_ns, n_ns) * mtx_b

        mtx_jac = compose_sparse([[mtx_s], [mtx_ns]]).tocsr()
        mtx_jac.sort_indices()

        return mtx_jac

########NEW FILE########
__FILENAME__ = solvers
"""
Base (abstract) solver classes.
"""
from sfepy.base.base import get_default, Struct

def make_get_conf(conf, kwargs):
    def _get_conf_item(name, default=None, msg_if_none=None):
        return kwargs.get(name, conf.get(name, default=default,
                                         msg_if_none=msg_if_none))

    return _get_conf_item

class Solver(Struct):
    """
    Base class for all solver kinds. Takes care of processing of common
    configuration options.

    The factory method any_from_conf() can be used to create an instance of any
    subclass.

    The subclasses have to reimplement __init__() and __call__(). The
    subclasses that implement process_conf() have to call Solver.process_conf().

    All solvers use the following configuration parameters:

    Parameters
    ----------
    name : str
        The name referred to in problem description options.
    kind : str
        The solver kind, as given by the `name` class attribute of the Solver
        subclasses.
    verbose : bool
        If True, the solver can print more information about the solution.
    """

    @staticmethod
    def process_conf(conf, kwargs=None):
        """
        Ensures conf contains 'name' and 'kind'.
        """
        get = conf.get
        name = get('name', None, 'missing "name" in options!')
        kind = get('kind', None, 'missing "kind" in options!')
        verbose = get('verbose', False)

        return Struct(name=name, kind=kind, verbose=verbose)

    def __init__(self, conf=None, **kwargs):
        if conf is None:
            conf = Struct()

        elif isinstance(conf, dict):
            conf = Struct(**conf)

        if conf.get('name', None) is None:
            conf.name = 'auto_' + self.__class__.__name__

        if conf.get('kind', None) is None:
            if hasattr(self.__class__, 'name'):
                conf.kind = self.__class__.name

            else:
                raise ValueError('solver kind cannot be determined!')

        new_conf = self.process_conf(conf, kwargs)
        Struct.__init__(self, conf=new_conf, orig_conf=conf, **kwargs)

    def __call__(self, **kwargs):
        raise ValueError('called an abstract Solver instance!')

class LinearSolver(Solver):
    """
    Abstract linear solver class.
    """
    def __init__(self, conf, mtx=None, status=None, **kwargs):
        Solver.__init__(self, conf=conf, mtx=mtx, status=status, **kwargs)

    def __call__(self, rhs, x0=None, conf=None, eps_a=None, eps_r=None,
                 i_max=None, mtx=None, status=None, **kwargs):
        raise ValueError('called an abstract LinearSolver instance!')

    def get_tolerance(self):
        """
        Return tuple `(eps_a, eps_r)` of absolute and relative tolerance
        settings. Either value can be `None`, meaning that the solver
        does not use that setting.
        """
        return self.conf.eps_a, self.conf.eps_r

class NonlinearSolver(Solver):
    """
    Abstract nonlinear solver class.
    """

    def __init__(self, conf, fun=None, fun_grad=None, lin_solver=None,
                 iter_hook=None, status=None, **kwargs):
        Solver.__init__(self, conf=conf, fun=fun, fun_grad=fun_grad,
                        lin_solver=lin_solver, iter_hook=iter_hook,
                        status=status, **kwargs)

    def __call__(self, state0, conf=None, fun=None, fun_grad=None,
                 lin_solver=None, iter_hook=None, status=None):
        raise ValueError('called an abstract NonlinearSolver instance!')

class TimeSteppingSolver(Solver):
    """
    Abstract time stepping solver class.
    """

    def __init__(self, conf, **kwargs):
        Solver.__init__(self, conf=conf, **kwargs)

    def __call__(self, state0=None, save_results=True, step_hook=None,
                 post_process_hook=None, nls_status=None):
        raise ValueError('called an abstract TimeSteppingSolver instance!')

class OptimizationSolver(Solver):
    """
    Abstract optimization solver class.
    """

    def __init__(self, conf, obj_fun=None, obj_fun_grad=None, status=None,
                 obj_args=None,  **kwargs ):
        Solver.__init__(self, conf=conf, obj_fun=obj_fun,
                        obj_fun_grad=obj_fun_grad, status=status,
                        obj_args=obj_args, **kwargs)

    def __call__(self, state0, conf=None, obj_fun=None, obj_fun_grad=None,
                 status=None, obj_args=None):
        raise ValueError('called an abstract OptimizationSolver instance!')

class EigenvalueSolver(Solver):
    """
    Abstract eigenvalue solver class.
    """

    def __init__(self, conf, mtx_a=None, mtx_b=None, n_eigs=None,
                 eigenvectors=None, status=None):
        Solver.__init__(self, conf=conf, mtx_a=mtx_a, mtx_b=mtx_b,
                        n_eigs=n_eigs, eigenvectors=eigenvectors,
                        status=status)

    def __call__(self, mtx_a, mtx_b=None, n_eigs=None,
                 eigenvectors=None, status=None, conf=None):
        raise ValueError('called an abstract EigenvalueSolver instance!')

    def _to_array(self, mtx_a, mtx_b=None):
        if hasattr(mtx_a, 'toarray'):
            mtx_a = mtx_a.toarray()
        if mtx_b is not None:
            if hasattr(mtx_b, 'toarray'):
                mtx_b = mtx_b.toarray()
        return mtx_a, mtx_b

########NEW FILE########
__FILENAME__ = ts
import numpy as nm

from sfepy.base.base import output, get_default, Struct

def get_print_info(n_step):
    if n_step > 1:
        n_digit = int(nm.log10(n_step - 1) + 1)

    else:
        n_digit = 1

    format = '%%%dd of %%%dd' % (n_digit, n_digit)
    suffix = '%%0%dd' % n_digit

    return n_digit, format, suffix

class TimeStepper(Struct):
    """
    Time stepper class.
    """

    @staticmethod
    def from_conf(conf):
        return TimeStepper(conf.t0, conf.t1, dt=conf.dt, n_step=conf.n_step,
                           is_quasistatic=conf.quasistatic)

    def __init__(self, t0, t1, dt=None, n_step=None, step=None,
                 is_quasistatic=False):
        self.set_from_data(t0, t1, dt=dt, n_step=n_step, step=step)
        self.is_quasistatic = is_quasistatic

    def _get_n_step(self, t0, t1, dt):
        n_step = int(round(nm.floor(((t1 - t0) / dt) + 0.5) + 1.0))
        return n_step

    def set_from_data(self, t0, t1, dt=None, n_step=None, step=None):
        self.t0, self.t1 = t0, t1

        dt = get_default(dt, t1 - t0)
        self.n_step = get_default(n_step,
                                  self._get_n_step(self.t0, self.t1, dt))

        if self.n_step > 1:
            self.times, self.dt = nm.linspace(self.t0, self.t1, self.n_step,
                                              endpoint=True, retstep=True)
        else:
            self.times = nm.array((self.t0,), dtype=nm.float64)
            self.dt = self.t1 - self.t0

        self.n_digit, self.format, self.suffix = get_print_info(self.n_step)

        self.set_step(step)

    def set_from_ts(self, ts, step=None):
        step = get_default(step, ts.step)
        self.set_from_data(ts.t0, ts.t1, ts.dt, ts.n_step, step=step)

    def __iter__(self):
        """ts.step, ts.time is consistent with step, time returned here
        ts.nt is normalized time in [0, 1]"""
        return self.iter_from(0)

    def iter_from(self, step):
        self.step = step - 1

        for time in self.times[step:]:

            self.time = time
            self.step += 1
            self.normalize_time()

            yield self.step, self.time

    def normalize_time(self):
        self.nt = (self.time - self.t0) / (self.t1 - self.t0)

    def set_step(self, step=0, nt=0.0):
        nm1 = self.n_step - 1
        if step is None:
            step = int(round(nt * nm1))
        if step < 0:
            step = self.n_step + step
        if (step >= self.n_step) or (step < 0):
            output('time step must be in [%d, %d]' % (-nm1, nm1) )
            raise ValueError

        self.step = step
        self.time = self.times[step]
        self.normalize_time()

    def __eq__(self, other):

        if type(other) == type(self):
            return (abs(self.t0 == other.t0) < 1e-15) and \
                   (abs(self.t1 == other.t1) < 1e-15) and \
                   (self.n_step == other.n_step)
        else:
            raise ValueError

class VariableTimeStepper(TimeStepper):
    """
    Time stepper class with a variable time step.
    """

    @staticmethod
    def from_conf(conf):
        return VariableTimeStepper(conf.t0, conf.t1, dt=conf.dt,
                                   n_step=conf.n_step,
                                   is_quasistatic=conf.quasistatic)

    def set_from_data(self, t0, t1, dt=None, n_step=None, step=None):
        self.t0, self.t1 = t0, t1

        self.dtime = self.t1 - self.t0
        dt = get_default(dt, self.dtime)

        self.n_step0 = get_default(n_step,
                                   self._get_n_step(self.t0, self.t1, dt))

        if self.n_step0 > 1:
            self.dt = self.dtime / (self.n_step0 - 1)

        else:
            self.dt = self.dtime

        self.dt0 = self.dt

        self.n_digit, self.format, self.suffix = get_print_info(5)

        self.set_step(step)

    def set_from_ts(self, ts, step=None):
        self.set_from_data(ts.t0, ts.t1, ts.dt, ts.n_step0, step=0)

    def set_n_digit_from_min_dt(self, dt):
        n_step = self._get_n_step(self.t0, self.t1, dt)
        self.n_digit, self.format, self.suffix = get_print_info(n_step)

    def set_step(self, step=0, nt=0.0):
        if step > 0:
            raise ValueError('cannot set step > 0 in VariableTimeStepper!')

        self.step = 0
        self.nt = 0.0
        self.dts = []
        self.times = [self.t0]
        self.n_step = 1

    def get_default_time_step(self):
        return self.dt0

    def set_time_step(self, dt, update_time=False):
        self.dt = dt

        if update_time:
            self.time = self.times[self.step - 1] + self.dt
            self.times[self.step] = self.time
            self.normalize_time()

    def __iter__(self):
        """
        ts.step, ts.time is consistent with step, time returned here
        ts.nt is normalized time in [0, 1].
        """
        self.set_step(0)

        while 1:
            self.time = self.times[self.step]

            yield self.step, self.time

            if self.nt >= 1.0:
                break

            self.step += 1
            self.time += self.dt
            self.normalize_time()

            self.times.append(self.time)
            self.dts.append(self.dt)
            self.n_step = self.step + 1

########NEW FILE########
__FILENAME__ = ts_solvers
"""
Time stepping solvers.
"""
import numpy as nm

from sfepy.base.base import output, Struct, IndexedStruct, basestr
from sfepy.solvers.solvers import make_get_conf, TimeSteppingSolver
from sfepy.discrete.mass_operator import MassOperator
from sfepy.solvers.ts import TimeStepper, VariableTimeStepper

class StationarySolver(TimeSteppingSolver):
    """
    Solver for stationary problems without time stepping.

    This class is provided to have a unified interface of the time stepping
    solvers also for stationary problems.
    """
    name = 'ts.stationary'

    def __init__(self, conf, **kwargs):
        TimeSteppingSolver.__init__(self, conf, ts=None, **kwargs)

    def __call__(self, state0=None, save_results=True, step_hook=None,
                 post_process_hook=None, nls_status=None):
        problem = self.problem

        problem.time_update()

        state = problem.solve(state0=state0, nls_status=nls_status)

        if step_hook is not None:
            step_hook(problem, None, state)

        if save_results:
            problem.save_state(problem.get_output_name(), state,
                               post_process_hook=post_process_hook,
                               file_per_var=None)

        return state

def replace_virtuals(deps, pairs):
    out = {}
    for key, val in deps.iteritems():
        out[pairs[key]] = val

    return out

class EquationSequenceSolver(TimeSteppingSolver):
    """
    Solver for stationary problems with an equation sequence.
    """
    name = 'ts.equation_sequence'

    def __init__(self, conf, **kwargs):
        TimeSteppingSolver.__init__(self, conf, ts=None, **kwargs)

    def __call__(self, state0=None, save_results=True, step_hook=None,
                 post_process_hook=None, nls_status=None):
        from sfepy.base.base import invert_dict, get_subdict
        from sfepy.base.resolve_deps import resolve

        problem = self.problem

        if state0 is None:
            state0 = problem.create_state()

        variables = problem.get_variables()
        vtos = variables.get_dual_names()
        vdeps = problem.equations.get_variable_dependencies()
        sdeps = replace_virtuals(vdeps, vtos)

        sorder = resolve(sdeps)

        stov = invert_dict(vtos)
        vorder = [[stov[ii] for ii in block] for block in sorder]

        parts0 = state0.get_parts()
        state = state0.copy()
        solved = []
        for ib, block in enumerate(vorder):
            output('solving for %s...' % sorder[ib])

            subpb = problem.create_subproblem(block, solved)

            subpb.equations.print_terms()

            subpb.time_update()
            substate0 = subpb.create_state()

            vals = get_subdict(parts0, block)
            substate0.set_parts(vals)

            substate = subpb.solve(state0=substate0, nls_status=nls_status)

            state.set_parts(substate.get_parts())

            solved.extend(sorder[ib])
            output('...done')

        if step_hook is not None:
            step_hook(problem, None, state)

        if save_results:
            problem.save_state(problem.get_output_name(), state,
                               post_process_hook=post_process_hook,
                               file_per_var=None)

        return state

def get_initial_state(problem):
    """
    Create a zero state vector and apply initial conditions.
    """
    state = problem.create_state()

    problem.setup_ic()
    state.apply_ic()

    return state

def prepare_save_data(ts, conf):
    """
    Given a time stepper configuration, return a list of time steps when the
    state should be saved.
    """
    try:
        save_steps = conf.options.save_steps
    except:
        save_steps = -1

    if save_steps == -1:
        save_steps = ts.n_step

    is_save = nm.linspace(0, ts.n_step - 1, save_steps).astype(nm.int32)
    is_save = nm.unique(is_save)

    return ts.suffix, is_save

def prepare_matrix(problem, state):
    """
    Pre-assemble tangent system matrix.
    """
    problem.update_materials()

    ev = problem.get_evaluator()
    try:
        mtx = ev.eval_tangent_matrix(state(), is_full=True)

    except ValueError:
        output('matrix evaluation failed, giving up...')
        raise

    return mtx

def make_implicit_step(ts, state0, problem, nls_status=None):
    """
    Make a step of an implicit time stepping solver.
    """
    problem.time_update(ts)

    if ts.step == 0:
        state0.apply_ebc()
        state = state0.copy(deep=True)

        if not ts.is_quasistatic:
            problem.init_time(ts)

            ev = problem.get_evaluator()
            try:
                vec_r = ev.eval_residual(state(), is_full=True)
            except ValueError:
                output('initial residual evaluation failed, giving up...')
                raise
            else:
                err = nm.linalg.norm(vec_r)
                output('initial residual: %e' % err)

        if problem.is_linear():
            mtx = prepare_matrix(problem, state)

        else:
            mtx = None

        # Initialize solvers (and possibly presolve the matrix).
        presolve = mtx is not None
        problem.init_solvers(nls_status=nls_status, mtx=mtx, presolve=presolve)

        # Initialize variables with history.
        state0.init_history()
        if ts.is_quasistatic:
            # Ordinary solve.
            state = problem.solve(state0=state0, nls_status=nls_status)

    else:
        if (ts.step == 1) and ts.is_quasistatic and problem.is_linear():
            mtx = prepare_matrix(problem, state0)
            problem.init_solvers(nls_status=nls_status, mtx=mtx)

        state = problem.solve(state0=state0, nls_status=nls_status)

    return state

def make_explicit_step(ts, state0, problem, mass, nls_status=None):
    """
    Make a step of an explicit time stepping solver.
    """
    problem.time_update(ts)

    if ts.step == 0:
        state0.apply_ebc()
        state = state0.copy(deep=True)

        problem.init_time(ts)

        # Initialize variables with history.
        state0.init_history()

    ev = problem.get_evaluator()
    try:
        vec_r = ev.eval_residual(state0(), is_full=True)
    except ValueError:
        output('residual evaluation failed, giving up...')
        raise
    else:
        err = nm.linalg.norm(vec_r)
        output('residual: %e' % err)

    if ts.step > 0:
        variables = problem.get_variables()
        vec_rf = variables.make_full_vec(vec_r, force_value=0.0)

        rhs = -ts.dt * vec_rf + mass.action(state0())

        vec = mass.inverse_action(rhs)

        state = state0.copy(preserve_caches=True)
        state.set_full(vec)
        state.apply_ebc()

    return state

def get_min_dt(adt):
    red = adt.red
    while red >= adt.red_max:
        red *= adt.red_factor

    dt = adt.dt0 * red

    return dt

def adapt_time_step(ts, status, adt, problem=None):
    """
    Adapt the time step of `ts` according to the exit status of the
    nonlinear solver.

    The time step dt is reduced, if the nonlinear solver did not converge. If it
    converged in less then a specified number of iterations for several time
    steps, the time step is increased. This is governed by the following
    parameters:

    - red_factor : time step reduction factor
    - red_max : maximum time step reduction factor
    - inc_factor : time step increase factor
    - inc_on_iter : increase time step if the nonlinear solver converged in
      less than this amount of iterations...
    - inc_wait : ...for this number of consecutive time steps

    Parameters
    ----------
    ts : VariableTimeStepper instance
        The time stepper.
    status : IndexedStruct instance
        The nonlinear solver exit status.
    adt : Struct instance
        The adaptivity parameters of the time solver:
    problem : Problem instance, optional
        This canbe used in user-defined adaptivity functions. Not used here.

    Returns
    -------
    is_break : bool
        If True, the adaptivity loop should stop.
    """
    is_break = False

    if status.condition == 0:
        if status.n_iter <= adt.inc_on_iter:
            adt.wait += 1

            if adt.wait > adt.inc_wait:
                if adt.red < 1.0:
                    adt.red = adt.red * adt.inc_factor
                    ts.set_time_step(adt.dt0 * adt.red)
                    output('+++++ new time step: %e +++++' % ts.dt)
                adt.wait = 0

        else:
            adt.wait = 0

        is_break = True

    else:
        adt.red = adt.red * adt.red_factor
        if adt.red < adt.red_max:
            is_break = True

        else:
            ts.set_time_step(adt.dt0 * adt.red, update_time=True)
            output('----- new time step: %e -----' % ts.dt)
            adt.wait = 0

    return is_break

class SimpleTimeSteppingSolver(TimeSteppingSolver):
    """
    Implicit time stepping solver with a fixed time step.
    """
    name = 'ts.simple'

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Process configuration options.
        """
        get = make_get_conf(conf, kwargs)
        common = TimeSteppingSolver.process_conf(conf)

        return Struct(t0=get('t0', 0.0),
                      t1=get('t1', 1.0),
                      dt=get('dt', None),
                      n_step=get('n_step', 10),
                      quasistatic=get('quasistatic', False)) + common

    def __init__(self, conf, **kwargs):
        TimeSteppingSolver.__init__(self, conf, **kwargs)

        self.ts = TimeStepper.from_conf(self.conf)

        nd = self.ts.n_digit
        format = '====== time %%e (step %%%dd of %%%dd) =====' % (nd, nd)

        self.format = format

    def __call__(self, state0=None, save_results=True, step_hook=None,
                 post_process_hook=None, nls_status=None):
        """
        Solve the time-dependent problem.
        """
        problem = self.problem
        ts = self.ts

        suffix, is_save = prepare_save_data(ts, problem.conf)

        if state0 is None:
            state0 = get_initial_state(problem)

        ii = 0
        for step, time in ts:
            output(self.format % (time, step + 1, ts.n_step))

            state = self.solve_step(ts, state0, nls_status=nls_status)
            state0 = state.copy(deep=True)

            if step_hook is not None:
                step_hook(problem, ts, state)

            if save_results and (is_save[ii] == ts.step):
                filename = problem.get_output_name(suffix=suffix % ts.step)
                problem.save_state(filename, state,
                                   post_process_hook=post_process_hook,
                                   file_per_var=None,
                                   ts=ts)
                ii += 1

            problem.advance(ts)

        return state

    def solve_step(self, ts, state0, nls_status=None):
        """
        Solve a single time step.
        """
        state = make_implicit_step(ts, state0, self.problem,
                                   nls_status=nls_status)

        return state

class ExplicitTimeSteppingSolver(SimpleTimeSteppingSolver):
    """
    Explicit time stepping solver with a fixed time step.
    """
    name = 'ts.explicit'

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Process configuration options.
        """
        get = make_get_conf(conf, kwargs)
        common = SimpleTimeSteppingSolver.process_conf(conf, kwargs)

        return Struct(mass=get('mass', None,
                               'missing "mass" in options!'),
                      lumped=get('lumped', False)) + common

    def __init__(self, conf, **kwargs):
        SimpleTimeSteppingSolver.__init__(self, conf, **kwargs)

        self.mass = MassOperator(self.problem, self.conf)

    def solve_step(self, ts, state0, nls_status=None):
        """
        Solve a single time step.
        """
        state = make_explicit_step(ts, state0, self.problem, self.mass,
                                   nls_status=nls_status)

        return state

class AdaptiveTimeSteppingSolver(SimpleTimeSteppingSolver):
    """
    Implicit time stepping solver with an adaptive time step.

    Either the built-in or user supplied function can be used to adapt the time
    step.
    """
    name = 'ts.adaptive'

    @staticmethod
    def process_conf(conf, kwargs):
        """
        Process configuration options.
        """
        get = make_get_conf(conf, kwargs)
        common = SimpleTimeSteppingSolver.process_conf(conf, kwargs)

        adt = Struct(red_factor=get('dt_red_factor', 0.2),
                     red_max=get('dt_red_max', 1e-3),
                     inc_factor=get('dt_inc_factor', 1.25),
                     inc_on_iter=get('dt_inc_on_iter', 4),
                     inc_wait=get('dt_inc_wait', 5),
                     red=1.0, wait=0, dt0=0.0)

        return Struct(adapt_fun=get('adapt_fun', adapt_time_step),
                      adt=adt) + common

    def __init__(self, conf, **kwargs):
        TimeSteppingSolver.__init__(self, conf, **kwargs)

        self.ts = VariableTimeStepper.from_conf(self.conf)

        self.adt = adt = self.conf.adt
        adt.dt0 = self.ts.get_default_time_step()
        self.ts.set_n_digit_from_min_dt(get_min_dt(adt))

        self.format = '====== time %e (dt %e, wait %d, step %d of %d) ====='

        if isinstance(self.conf.adapt_fun, basestr):
            self.adapt_time_step = self.problem.functions[self.conf.adapt_fun]

        else:
            self.adapt_time_step = self.conf.adapt_fun

    def __call__(self, state0=None, save_results=True, step_hook=None,
                 post_process_hook=None, nls_status=None):
        """
        Solve the time-dependent problem.
        """
        problem = self.problem
        ts = self.ts

        if state0 is None:
            state0 = get_initial_state(problem)

        ii = 0
        for step, time in ts:
            output(self.format % (time, ts.dt, self.adt.wait,
                                  step + 1, ts.n_step))

            state = self.solve_step(ts, state0, nls_status=nls_status)
            state0 = state.copy(deep=True)

            if step_hook is not None:
                step_hook(problem, ts, state)

            if save_results:
                filename = problem.get_output_name(suffix=ts.suffix % ts.step)
                problem.save_state(filename, state,
                                   post_process_hook=post_process_hook,
                                   file_per_var=None,
                                   ts=ts)
                ii += 1

            problem.advance(ts)

        return state

    def solve_step(self, ts, state0, nls_status=None):
        """
        Solve a single time step.
        """
        status = IndexedStruct(n_iter=0, condition=0)
        while 1:
            state = make_implicit_step(ts, state0, self.problem,
                                       nls_status=status)

            is_break = self.adapt_time_step(ts, status, self.adt, self.problem)
            if is_break:
                break

        if nls_status is not None:
            nls_status.update(status)

        return state

########NEW FILE########
__FILENAME__ = terms
import re
from copy import copy

import numpy as nm

from sfepy.base.base import (as_float_or_complex, get_default, assert_,
                             Container, Struct, basestr, goptions)
from sfepy.base.compat import in1d

# Used for imports in term files.
from sfepy.terms.extmods import terms

from sfepy.linalg import split_range

_match_args = re.compile('^([^\(\}]*)\((.*)\)$').match
_match_virtual = re.compile('^virtual$').match
_match_state = re.compile('^state(_[_a-zA-Z0-9]+)?$').match
_match_parameter = re.compile('^parameter(_[_a-zA-Z0-9]+)?$').match
_match_material = re.compile('^material(_[_a-zA-Z0-9]+)?$').match
_match_material_opt = re.compile('^opt_material(_[_a-zA-Z0-9]+)?$').match
_match_material_root = re.compile('(.+)\.(.*)').match

def get_arg_kinds(arg_types):
    """
    Translate `arg_types` of a Term to a canonical form.

    Parameters
    ----------
    arg_types : tuple of strings
        The term argument types, as given in the `arg_types` attribute.

    Returns
    -------
    arg_kinds : list of strings
        The argument kinds - one of 'virtual_variable', 'state_variable',
        'parameter_variable', 'opt_material', 'user'.
    """
    arg_kinds = []
    for ii, arg_type in enumerate(arg_types):
        if _match_virtual(arg_type):
            arg_kinds.append('virtual_variable')

        elif _match_state(arg_type):
            arg_kinds.append('state_variable')

        elif _match_parameter(arg_type):
            arg_kinds.append('parameter_variable')

        elif _match_material(arg_type):
            arg_kinds.append('material')

        elif _match_material_opt(arg_type):
            arg_kinds.append('opt_material')
            if ii > 0:
                msg = 'opt_material at position %d, must be at 0!' % ii
                raise ValueError(msg)
        else:
            arg_kinds.append('user')

    return arg_kinds

def get_shape_kind(integration):
    """
    Get data shape kind for given integration type.
    """
    if integration == 'surface':
        shape_kind = 'surface'

    elif integration in ('volume', 'plate', 'surface_extra'):
        shape_kind = 'volume'

    elif integration == 'point':
        shape_kind = 'point'

    else:
        raise NotImplementedError('unsupported term integration! (%s)'
                                  % integration)

    return shape_kind

def split_complex_args(args):
    """
    Split complex arguments to real and imaginary parts.

    Returns
    -------
    newargs : dictionary
        Dictionary with lists corresponding to `args` such that each
        argument of numpy.complex128 data type is split to its real and
        imaginary part. The output depends on the number of complex
        arguments in 'args':

          - 0: list (key 'r') identical to input one

          - 1: two lists with keys 'r', 'i' corresponding to real
            and imaginary parts

          - 2: output dictionary contains four lists:

            - 'r' - real(arg1), real(arg2)
            - 'i' - imag(arg1), imag(arg2)
            - 'ri' - real(arg1), imag(arg2)
            - 'ir' - imag(arg1), real(arg2)
    """
    newargs = {}
    cai = []

    for ii, arg in enumerate(args):
        if isinstance(arg, nm.ndarray) and (arg.dtype == nm.complex128):
            cai.append(ii)

    if len(cai) > 0:
        newargs['r'] = list(args[:])
        newargs['i'] = list(args[:])

        arg1 = cai[0]
        newargs['r'][arg1] = args[arg1].real.copy()
        newargs['i'][arg1] = args[arg1].imag.copy()

        if len(cai) == 2:
            arg2 = cai[1]
            newargs['r'][arg2] = args[arg2].real.copy()
            newargs['i'][arg2] = args[arg2].imag.copy()

            newargs['ri'] = list(args[:])
            newargs['ir'] = list(args[:])
            newargs['ri'][arg1] = newargs['r'][arg1]
            newargs['ri'][arg2] = newargs['i'][arg2]
            newargs['ir'][arg1] = newargs['i'][arg1]
            newargs['ir'][arg2] = newargs['r'][arg2]

        elif len(cai) > 2:
            raise NotImplementedError('more than 2 complex arguments! (%d)'
                                      % len(cai))

    else:
        newargs['r'] = args[:]

    return newargs

def vector_chunk_generator(total_size, chunk_size, shape_in,
                           zero=False, set_shape=True, dtype=nm.float64):
    if not chunk_size:
        chunk_size = total_size
    shape = list(shape_in)

    sizes = split_range(total_size, chunk_size)
    ii = nm.array(0, dtype=nm.int32)
    for size in sizes:
        chunk = nm.arange(size, dtype=nm.int32) + ii
        if set_shape:
            shape[0] = size
        if zero:
            out = nm.zeros(shape, dtype=dtype)
        else:
            out = nm.empty(shape, dtype=dtype)
        yield out, chunk
        ii += size

def create_arg_parser():
    from pyparsing import Literal, Word, delimitedList, Group, \
         StringStart, StringEnd, Optional, nums, alphas, alphanums

    inumber = Word("+-" + nums, nums)

    history = Optional(Literal('[').suppress() + inumber
                       + Literal(']').suppress(), default=0)("history")
    history.setParseAction(lambda str, loc, toks: int(toks[0]))

    variable = Group(Word(alphas, alphanums + '._') + history)

    derivative = Group(Literal('d') + variable\
                       + Literal('/').suppress() + Literal('dt'))

    trace = Group(Literal('tr') + Literal('(').suppress() + variable \
                  + Literal(')').suppress())

    generalized_var = derivative | trace | variable

    args = StringStart() + delimitedList(generalized_var) + StringEnd()

    return args

# 22.01.2006, c
class CharacteristicFunction(Struct):

    def __init__(self, region):
        self.igs = region.igs
        self.region = region
        self.local_chunk = None
        self.ig = None

    def __call__(self, chunk_size, shape_in, zero=False, set_shape=True,
                 ret_local_chunk=False, dtype=nm.float64):
        els = self.region.get_cells(self.ig)
        for out, chunk in vector_chunk_generator(els.shape[0], chunk_size,
                                                 shape_in, zero, set_shape,
                                                 dtype):
            self.local_chunk = chunk

            if ret_local_chunk:
                yield out, chunk
            else:
                yield out, els[chunk]

        self.local_chunk = None

    def set_current_group(self, ig):
        self.ig = ig

    def get_local_chunk(self):
        return self.local_chunk

class ConnInfo(Struct):

    def get_region(self, can_trace=True):
        if self.is_trace and can_trace:
            return self.region.get_mirror_region()[0]
        else:
            return self.region

    def get_region_name(self, can_trace=True):
        if self.is_trace and can_trace:
            reg = self.region.get_mirror_region()[0]
        else:
            reg = self.region

        if reg is not None:
            return reg.name
        else:
            return None

    def iter_igs(self):
        if self.region is not None:
            for ig in self.region.igs:
                if self.virtual_igs is not None:
                    ir = self.virtual_igs.tolist().index(ig)
                    rig = self.virtual_igs[ir]
                else:
                    rig = None

                if not self.is_trace:
                    ii = ig
                else:
                    ig_map_i = self.region.get_mirror_region()[2]
                    ii = ig_map_i[ig]

                if self.state_igs is not None:
                    ic = self.state_igs.tolist().index(ii)
                    cig = self.state_igs[ic]
                else:
                    cig = None

                yield rig, cig

        else:
            yield None, None

class Terms(Container):

    @staticmethod
    def from_desc(term_descs, regions, integrals=None):
        """
        Create terms, assign each term its region.
        """
        from sfepy.terms import term_table

        terms = Terms()
        for td in term_descs:
            try:
                constructor = term_table[td.name]
            except:
                msg = "term '%s' is not in %s" % (td.name,
                                                  sorted(term_table.keys()))
                raise ValueError(msg)

            try:
                region = regions[td.region]
            except IndexError:
                raise KeyError('region "%s" does not exist!' % td.region)

            term = Term.from_desc(constructor, td, region, integrals=integrals)
            terms.append(term)

        return terms

    def __init__(self, objs=None):
        Container.__init__(self, objs=objs)

        self.update_expression()

    def insert(self, ii, obj):
        Container.insert(self, ii, obj)
        self.update_expression()

    def append(self, obj):
        Container.append(self, obj)
        self.update_expression()

    def update_expression(self):
        self.expression = []
        for term in self:
            aux = [term.sign, term.name, term.arg_str,
                   term.integral_name, term.region.name]
            self.expression.append(aux)

    def __mul__(self, other):
        out = Terms()
        for name, term in self.iteritems():
            out.append(term * other)

        return out

    def __rmul__(self, other):
        return self * other

    def __add__(self, other):
        if isinstance(other, Term):
            out = self.copy()
            out.append(other)

        elif isinstance(other, Terms):
            out = Terms(self._objs + other._objs)

        else:
            raise ValueError('cannot add Terms with %s!' % other)

        return out

    def __radd__(self, other):
        return self + other

    def __sub__(self, other):
        if isinstance(other, Term):
            out = self + (-other)

        elif isinstance(other, Terms):
            out = self + (-other)

        else:
            raise ValueError('cannot subtract Terms with %s!' % other)

        return out

    def __rsub__(self, other):
        return -self + other

    def __pos__(self):
        return self

    def __neg__(self):
        return -1.0 * self

    def setup(self):
        for term in self:
            term.setup()

    def assign_args(self, variables, materials, user=None):
        """
        Assign all term arguments.
        """
        for term in self:
            term.assign_args(variables, materials, user)

    def get_variable_names(self):
        out = []
        for term in self:
            out.extend(term.get_variable_names())
        return list(set(out))

    def get_material_names(self):
        out = []
        for term in self:
            out.extend(term.get_material_names())
        return list(set(out))

    def get_user_names(self):
        out = []
        for term in self:
            out.extend(term.get_user_names())
        return list(set(out))

    def set_current_group(self, ig):
        for term in self:
            term.char_fun.set_current_group(ig)

class Term(Struct):
    name = ''
    arg_types = ()
    arg_shapes = {}
    integration = 'volume'
    geometries = ['2_3', '2_4', '3_4', '3_8']

    @staticmethod
    def new(name, integral, region, **kwargs):
        from sfepy.terms import term_table

        arg_str = _match_args(name)
        if arg_str is not None:
            name, arg_str = arg_str.groups()

        else:
            raise ValueError('bad term syntax! (%s)' % name)

        if name in term_table:
            constructor = term_table[name]

        else:
            msg = "term '%s' is not in %s" % (name, sorted(term_table.keys()))
            raise ValueError(msg)

        obj = constructor(name, arg_str, integral, region, **kwargs)
        return obj

    @staticmethod
    def from_desc(constructor, desc, region, integrals=None):
        from sfepy.discrete import Integrals

        if integrals is None:
            integrals = Integrals()

        obj = constructor(desc.name, desc.args, None, region)
        obj.set_integral(integrals.get(desc.integral))
        obj.sign = desc.sign

        return obj

    def __init__(self, name, arg_str, integral, region, **kwargs):
        self.name = name
        self.arg_str = arg_str
        self.region = region
        self._kwargs = kwargs
        self._integration = self.integration
        self.sign = 1.0

        self.set_integral(integral)

    def __mul__(self, other):
        try:
            mul = as_float_or_complex(other)

        except ValueError:
            raise ValueError('cannot multiply Term with %s!' % other)

        out = self.copy(name=self.name)

        out.sign = mul * self.sign

        return out

    def __rmul__(self, other):
        return self * other

    def __add__(self, other):
        if isinstance(other, Term):
            out = Terms([self, other])

        else:
            out = NotImplemented

        return out

    def __sub__(self, other):
        if isinstance(other, Term):
            out = Terms([self, -1.0 * other])

        else:
            out = NotImplemented

        return out

    def __pos__(self):
        return self

    def __neg__(self):
        out = -1.0 * self
        return out

    def set_integral(self, integral):
        """
        Set the term integral.
        """
        self.integral = integral
        if self.integral is not None:
            self.integral_name = self.integral.name

    def setup(self):
        self.char_fun = CharacteristicFunction(self.region)
        self.function = Struct.get(self, 'function', None)

        self.step = 0
        self.dt = 1.0
        self.is_quasistatic = False
        self.has_region = True

        self.setup_formal_args()

        if self._kwargs:
            self.setup_args(**self._kwargs)

        else:
            self.args = []

    def setup_formal_args(self):
        self.arg_names = []
        self.arg_steps = {}
        self.arg_derivatives = {}
        self.arg_traces = {}

        parser = create_arg_parser()
        self.arg_desc = parser.parseString(self.arg_str)

        for arg in self.arg_desc:
            trace = False
            derivative = None

            if isinstance(arg[1], int):
                name, step = arg

            else:
                kind = arg[0]
                name, step = arg[1]
                if kind == 'd':
                    derivative = arg[2]
                elif kind == 'tr':
                    trace = True

            match = _match_material_root(name)
            if match:
                name = (match.group(1), match.group(2))

            self.arg_names.append(name)
            self.arg_steps[name] = step
            self.arg_derivatives[name] = derivative
            self.arg_traces[name] = trace

    def setup_args(self, **kwargs):
        self._kwargs = kwargs

        self.args = []
        for arg_name in self.arg_names:
            if isinstance(arg_name, basestr):
                self.args.append(self._kwargs[arg_name])

            else:
                self.args.append((self._kwargs[arg_name[0]], arg_name[1]))

        self.classify_args()
        self.check_args()

    def __call__(self, diff_var=None, chunk_size=None, **kwargs):
        """
        Subclasses either implement __call__ or plug in a proper _call().
        """
        return self._call(diff_var, chunk_size, **kwargs)

    def _call(self, diff_var=None, chunk_size=None, **kwargs):
        msg = 'base class method "_call" called for %s' \
              % self.__class__.__name__
        raise RuntimeError(msg)

    def assign_args(self, variables, materials, user=None):
        """
        Check term argument existence in variables, materials, user data
        and assign the arguments to terms. Also check compatibility of
        field and term subdomain lists (igs).
        """
        if user is None:
            user = {}

        kwargs = {}
        for arg_name in self.arg_names:
            if isinstance(arg_name, basestr):
                if arg_name in variables.names:
                    kwargs[arg_name] = variables[arg_name]

                elif arg_name in user:
                    kwargs[arg_name] = user[arg_name]

                else:
                    raise ValueError('argument %s not found!' % arg_name)

            else:
                arg_name = arg_name[0]
                if arg_name in materials.names:
                    kwargs[arg_name] = materials[arg_name]

                else:
                    raise ValueError('material argument %s not found!'
                                     % arg_name)

        self.setup_args(**kwargs)

    def classify_args(self):
        """
        Classify types of the term arguments and find matching call
        signature.

        A state variable can be in place of a parameter variable and
        vice versa.
        """
        self.names = Struct(name='arg_names',
                            material=[], variable=[], user=[],
                            state=[], virtual=[], parameter=[])

        # Prepare for 'opt_material' - just prepend a None argument if needed.
        if isinstance(self.arg_types[0], tuple):
            arg_types = self.arg_types[0]

        else:
            arg_types = self.arg_types

        if len(arg_types) == (len(self.args) + 1):
            self.args.insert(0, (None, None))
            self.arg_names.insert(0, (None, None))

        if isinstance(self.arg_types[0], tuple):
            assert_(len(self.modes) == len(self.arg_types))
            # Find matching call signature using variable arguments - material
            # and user arguments are ignored!
            matched = []
            for it, arg_types in enumerate(self.arg_types):
                arg_kinds = get_arg_kinds(arg_types)
                if self._check_variables(arg_kinds):
                    matched.append((it, arg_kinds))

            if len(matched) == 1:
                i_match, arg_kinds = matched[0]
                arg_types = self.arg_types[i_match]
                self.mode = self.modes[i_match]

            elif len(matched) == 0:
                msg = 'cannot match arguments! (%s)' % self.arg_names
                raise ValueError(msg)

            else:
                msg = 'ambiguous arguments! (%s)' % self.arg_names
                raise ValueError(msg)

        else:
            arg_types = self.arg_types
            arg_kinds = get_arg_kinds(self.arg_types)
            self.mode = Struct.get(self, 'mode', None)

            if not self._check_variables(arg_kinds):
                raise ValueError('cannot match variables! (%s)'
                                 % self.arg_names)

        # Set actual argument types.
        self.ats = list(arg_types)

        for ii, arg_kind in enumerate(arg_kinds):
            name = self.arg_names[ii]
            if arg_kind.endswith('variable'):
                names = self.names.variable

                if arg_kind == 'virtual_variable':
                    self.names.virtual.append(name)

                elif arg_kind == 'state_variable':
                    self.names.state.append(name)

                elif arg_kind == 'parameter_variable':
                    self.names.parameter.append(name)

            elif arg_kind.endswith('material'):
                names = self.names.material

            else:
                names = self.names.user

            names.append(name)

        self.n_virtual = len(self.names.virtual)
        if self.n_virtual > 1:
            raise ValueError('at most one virtual variable is allowed! (%d)'
                             % self.n_virtual)

        self.set_arg_types()

        self.setup_integration()

    def _check_variables(self, arg_kinds):
        for ii, arg_kind in enumerate(arg_kinds):
            if arg_kind.endswith('variable'):
                var = self.args[ii]
                check = {'virtual_variable' : var.is_virtual,
                         'state_variable' : var.is_state_or_parameter,
                         'parameter_variable' : var.is_state_or_parameter}
                if not check[arg_kind]():
                    return False

        else:
            return True

    def set_arg_types(self):
        pass

    def check_args(self):
        """
        Common checking to all terms.

        Check compatibility of field and term subdomain lists (igs).
        """
        vns = self.get_variable_names()
        for name in vns:
            field = self._kwargs[name].get_field()
            if field is None:
                continue

            if not nm.all(in1d(self.region.vertices,
                               field.region.vertices)):
                msg = ('%s: incompatible regions: (self, field %s)'
                       + '(%s in %s)') %\
                       (self.name, field.name,
                        self.region.vertices, field.region.vertices)
                raise ValueError(msg)

    def get_variable_names(self):
        return self.names.variable

    def get_material_names(self):
        out = []
        for aux in self.names.material:
            if aux[0] is not None:
                out.append(aux[0])
        return out

    def get_user_names(self):
        return self.names.user

    def get_virtual_name(self):
        if not self.names.virtual:
            return None

        var = self.get_virtual_variable()
        return var.name

    def get_state_names(self):
        """
        If variables are given, return only true unknowns whose data are of
        the current time step (0).
        """
        variables = self.get_state_variables()
        return [var.name for var in variables]

    def get_parameter_names(self):
        return copy(self.names.parameter)

    def get_conn_key(self):
        """The key to be used in DOF connectivity information."""
        key = (self.name,) + tuple(self.arg_names)
        key += (self.integral_name, self.region.name)

        return key

    def get_conn_info(self):
        vvar = self.get_virtual_variable()
        svars = self.get_state_variables()
        pvars = self.get_parameter_variables()

        all_vars = self.get_variables()

        dc_type = self.get_dof_conn_type()
        tgs = self.get_geometry_types()

        v_igs = v_tg = None
        if vvar is not None:
            field = vvar.get_field()
            if field is not None:
                v_igs = field.igs
                if vvar.name in tgs:
                    v_tg = tgs[vvar.name]

                else:
                    v_tg = None

        else:
            # No virtual variable -> all unknowns are in fact known parameters.
            pvars += svars
            svars = []

        region = self.get_region()
        if region is not None:
            is_any_trace = reduce(lambda x, y: x or y,
                                  self.arg_traces.values())
            if is_any_trace:
                region.setup_mirror_region()
                self.char_fun.igs = region.igs

        vals = []
        aux_pvars = []
        for svar in svars:
            # Allow only true state variables.
            if not svar.is_state():
                aux_pvars.append(svar)
                continue

            field = svar.get_field()
            if field is not None:
                s_igs = field.igs
            else:
                s_igs = None
            is_trace = self.arg_traces[svar.name]

            if svar.name in tgs:
                ps_tg = tgs[svar.name]
            else:
                ps_tg = v_tg

            val = ConnInfo(virtual=vvar, virtual_igs=v_igs,
                           state=svar, state_igs=s_igs,
                           primary=svar, primary_igs=s_igs,
                           has_virtual=True,
                           has_state=True,
                           is_trace=is_trace,
                           dc_type=dc_type,
                           v_tg=v_tg,
                           ps_tg=ps_tg,
                           region=region,
                           all_vars=all_vars)
            vals.append(val)

        pvars += aux_pvars
        for pvar in pvars:
            field = pvar.get_field()
            if field is not None:
                p_igs = field.igs
            else:
                p_igs = None
            is_trace = self.arg_traces[pvar.name]

            if pvar.name in tgs:
                ps_tg = tgs[pvar.name]
            else:
                ps_tg = v_tg

            val = ConnInfo(virtual=vvar, virtual_igs=v_igs,
                           state=None, state_igs=[],
                           primary=pvar.get_primary(), primary_igs=p_igs,
                           has_virtual=vvar is not None,
                           has_state=False,
                           is_trace=is_trace,
                           dc_type=dc_type,
                           v_tg=v_tg,
                           ps_tg=ps_tg,
                           region=region,
                           all_vars=all_vars)
            vals.append(val)

        if vvar and (len(vals) == 0):
            # No state, parameter variables, just the virtual one.
            val = ConnInfo(virtual=vvar, virtual_igs=v_igs,
                           state=vvar.get_primary(), state_igs=v_igs,
                           primary=vvar.get_primary(), primary_igs=v_igs,
                           has_virtual=True,
                           has_state=False,
                           is_trace=False,
                           dc_type=dc_type,
                           v_tg=v_tg,
                           ps_tg=v_tg,
                           region=region,
                           all_vars=all_vars)
            vals.append(val)

        return vals

    def get_args_by_name(self, arg_names):
        """
        Return arguments by name.
        """
        out = []
        for name in arg_names:
            try:
                ii = self.arg_names.index(name)

            except ValueError:
                raise ValueError('non-existing argument! (%s)' % name)

            out.append(self.args[ii])

        return out

    def get_args(self, arg_types=None, **kwargs):
        """
        Return arguments by type as specified in arg_types (or
        self.ats). Arguments in **kwargs can override the ones assigned
        at the term construction - this is useful for passing user data.
        """
        ats = self.ats
        if arg_types is None:
            arg_types = ats
        args = []

        iname, region_name, ig = self.get_current_group()
        for at in arg_types:
            ii = ats.index(at)
            arg_name = self.arg_names[ii]
            if isinstance(arg_name, basestr):
                if arg_name in kwargs:
                    args.append(kwargs[arg_name])

                else:
                    args.append(self.args[ii])

            else:
                mat, par_name = self.args[ii]
                if mat is not None:
                    mat_data = mat.get_data((region_name, self.integral_name),
                                            ig, par_name)
                else:
                    mat_data = None

                args.append(mat_data)

        return args

    def get_kwargs(self, keys, **kwargs):
        """Extract arguments from **kwargs listed in keys (default is
        None)."""
        return [kwargs.get(name) for name in keys]

    def get_arg_name(self, arg_type, full=False, join=None):
        """
        Get the name of the argument specified by `arg_type.`

        Parameters
        ----------
        arg_type : str
            The argument type string.
        full : bool
            If True, return the full name. For example, if the name of a
            variable argument is 'u' and its time derivative is
            requested, the full name is 'du/dt'.
        join : str, optional
            Optionally, the material argument name tuple can be joined
            to a single string using the `join` string.

        Returns
        -------
        name : str
            The argument name.
        """
        try:
            ii = self.ats.index(arg_type)
        except ValueError:
            return None

        name = self.arg_names[ii]
        if full:
            # Include derivatives.
            if self.arg_derivatives[name]:
                name = 'd%s/%s' % (name, self.arg_derivatives[name])

        if (join is not None) and isinstance(name, tuple):
            name = join.join(name)

        return name

    def setup_integration(self):
        self.has_geometry = True

        self.geometry_types = {}
        if isinstance(self.integration, basestr):
            for var in self.get_variables():
                self.geometry_types[var.name] = self.integration

        else:
            if self.mode is not None:
                self.integration = self._integration[self.mode]

            if self.integration is not None:
                for arg_type, gtype in self.integration.iteritems():
                    var = self.get_args(arg_types=[arg_type])[0]
                    self.geometry_types[var.name] = gtype

        gtypes = list(set(self.geometry_types.itervalues()))

        if 'surface_extra' in gtypes:
            self.dof_conn_type = 'volume'

        elif len(gtypes):
            self.dof_conn_type = gtypes[0]

    def get_region(self):
        return self.region

    def get_geometry_types(self):
        """
        Returns
        -------
        out : dict
            The required geometry types for each variable argument.
        """
        return self.geometry_types

    def get_current_group(self):
        return (self.integral_name, self.region.name, self.char_fun.ig)


    def get_dof_conn_type(self):
        return Struct(name='dof_conn_info', type=self.dof_conn_type,
                      region_name=self.region.name)

    def set_current_group(self, ig):
        self.char_fun.set_current_group(ig)

    def igs(self):
        return self.char_fun.igs

    def get_assembling_cells(self, shape=None):
        """
        Return the assembling cell indices into a DOF connectivity.
        """
        cells = nm.arange(shape[0], dtype=nm.int32)

        return cells

    def iter_groups(self):
        if self.dof_conn_type == 'point':
            igs = self.igs()[0:1]
        else:
            igs = self.igs()

        for ig in igs:
            if self.integration in ('volume', 'plate'):
                if not len(self.region.get_cells(ig)): continue
            self.set_current_group(ig)
            yield ig

    def time_update(self, ts):
        if ts is not None:
            self.step = ts.step
            self.dt = ts.dt
            self.is_quasistatic = ts.is_quasistatic

    def advance(self, ts):
        """
        Advance to the next time step. Implemented in subclasses.
        """

    def get_vector(self, variable):
        """Get the vector stored in `variable` according to self.arg_steps
        and self.arg_derivatives. Supports only the backward difference w.r.t.
        time."""

        name = variable.name
        return variable(step=self.arg_steps[name],
                        derivative=self.arg_derivatives[name])

    def get_approximation(self, variable, get_saved=False):
        """
        Return approximation corresponding to `variable`. Also return
        the corresponding geometry (actual or saved, according to
        `get_saved`).
        """
        geo, _, key = self.get_mapping(variable, get_saved=get_saved,
                                       return_key=True)
        ig = key[2]
        ap = variable.get_approximation(ig)

        return ap, geo

    def get_variables(self, as_list=True):

        if as_list:
            variables = self.get_args_by_name(self.names.variable)

        else:
            variables = {}
            for var in self.get_args_by_name(self.names.variable):
                variables[var.name] = var

        return variables

    def get_virtual_variable(self):
        aux = self.get_args_by_name(self.names.virtual)
        if len(aux) == 1:
            var = aux[0]

        else:
            var = None

        return var

    def get_state_variables(self, unknown_only=False):
        variables = self.get_args_by_name(self.names.state)

        if unknown_only:
            variables = [var for var in variables
                         if (var.kind == 'unknown') and
                         (self.arg_steps[var.name] == 0)]

        return variables

    def get_parameter_variables(self):
        return self.get_args_by_name(self.names.parameter)

    def get_materials(self, join=False):
        materials = self.get_args_by_name(self.names.material)

        for mat in materials:
            if mat[0] is None:
                materials.remove(mat)

        if join:
            materials = list(set(mat[0] for mat in materials))

        return materials

    def get_qp_key(self):
        """
        Return a key identifying uniquely the term quadrature points.
        """
        return (self.region.name, self.integral.name)

    def get_physical_qps(self):
        """
        Get physical quadrature points corresponding to the term region
        and integral.
        """
        from sfepy.discrete.common.mappings import get_physical_qps, PhysicalQPs

        if self.integration == 'point':
            phys_qps = PhysicalQPs(self.region.igs)

        elif self.integration == 'plate':
            phys_qps = get_physical_qps(self.region, self.integral,
                                        map_kind='v')

        else:
            phys_qps = get_physical_qps(self.region, self.integral)

        return phys_qps

    def get_mapping(self, variable, get_saved=False, return_key=False):
        """
        Get the reference mapping from a variable.

        Notes
        -----
        This is a convenience wrapper of Field.get_mapping() that
        initializes the arguments using the term data.
        """
        integration = self.geometry_types[variable.name]
        is_trace = self.arg_traces[variable.name]

        if is_trace:
            region, ig_map, ig_map_i = self.region.get_mirror_region()
            ig = ig_map_i[self.char_fun.ig]

        else:
            region = self.region
            ig = self.char_fun.ig

        out = variable.field.get_mapping(ig, region,
                                         self.integral, integration,
                                         get_saved=get_saved,
                                         return_key=return_key)

        return out

    def get_data_shape(self, variable):
        """
        Get data shape information from variable.

        Notes
        -----
        This is a convenience wrapper of FieldVariable.get_data_shape() that
        initializes the arguments using the term data.
        """
        integration = self.geometry_types[variable.name]
        is_trace = self.arg_traces[variable.name]

        if is_trace:
            region, ig_map, ig_map_i = self.region.get_mirror_region()
            ig = ig_map_i[self.char_fun.ig]

        else:
            region = self.region
            ig = self.char_fun.ig

        out = variable.get_data_shape(ig, self.integral,
                                      integration, region.name)
        return out

    def get(self, variable, quantity_name, bf=None, integration=None,
            step=None, time_derivative=None):
        """
        Get the named quantity related to the variable.

        Notes
        -----
        This is a convenience wrapper of Variable.evaluate() that
        initializes the arguments using the term data.
        """
        name = variable.name

        step = get_default(step, self.arg_steps[name])
        time_derivative = get_default(time_derivative,
                                      self.arg_derivatives[name])
        integration = get_default(integration, self.geometry_types[name])

        data = variable.evaluate(self.char_fun.ig, mode=quantity_name,
                                 region=self.region, integral=self.integral,
                                 integration=integration,
                                 step=step, time_derivative=time_derivative,
                                 is_trace=self.arg_traces[name], bf=bf)
        return data

    def check_shapes(self, *args, **kwargs):
        """
        Default implementation of function to check term argument shapes
        at run-time.
        """
        pass

    def standalone_setup(self):
        from sfepy.discrete import create_adof_conns, Variables

        conn_info = {'aux' : self.get_conn_info()}
        adcs = create_adof_conns(conn_info, None)

        variables = Variables(self.get_variables())
        variables.set_adof_conns(adcs)

        materials = self.get_materials(join=True)

        for mat in materials:
            mat.time_update(None, [Struct(terms=[self])])

    def call_get_fargs(self, args, kwargs):
        try:
            fargs = self.get_fargs(*args, **kwargs)

        except RuntimeError:
            terms.errclear()
            raise ValueError

        return fargs

    def call_function(self, out, fargs):
        try:
            status = self.function(out, *fargs)

        except RuntimeError:
            terms.errclear()
            raise ValueError

        if status:
            terms.errclear()
            raise ValueError('term evaluation failed! (%s)' % self.name)

        return status

    def eval_real(self, shape, fargs, mode='eval', term_mode=None,
                  diff_var=None, **kwargs):
        out = nm.empty(shape, dtype=nm.float64)

        if mode == 'eval':
            status = self.call_function(out, fargs)
            # Sum over elements but not over components.
            out1 = nm.sum(out, 0).squeeze()
            return out1, status

        else:
            status = self.call_function(out, fargs)

            return out, status

    def eval_complex(self, shape, fargs, mode='eval', term_mode=None,
                     diff_var=None, **kwargs):
        rout = nm.empty(shape, dtype=nm.float64)

        fargsd = split_complex_args(fargs)

        # Assuming linear forms. Then the matrix is the
        # same both for real and imaginary part.
        rstatus = self.call_function(rout, fargsd['r'])
        if (diff_var is None) and len(fargsd) >= 2:
            iout = nm.empty(shape, dtype=nm.float64)
            istatus = self.call_function(iout, fargsd['i'])

            if mode == 'eval' and len(fargsd) >= 4:
                irout = nm.empty(shape, dtype=nm.float64)
                irstatus = self.call_function(irout, fargsd['ir'])
                riout = nm.empty(shape, dtype=nm.float64)
                ristatus = self.call_function(riout, fargsd['ri'])

                out = (rout - iout) + (riout + irout) * 1j
                status = rstatus or istatus or ristatus or irstatus

            else:
                out = rout + 1j * iout
                status = rstatus or istatus

        else:
            out, status = rout, rstatus

        if mode == 'eval':
            out1 = nm.sum(out, 0).squeeze()
            return out1, status

        else:
            return out, status

    def evaluate(self, mode='eval', diff_var=None,
                 standalone=True, ret_status=False, **kwargs):
        """
        Evaluate the term.

        Parameters
        ----------
        mode : 'eval' (default), or 'weak'
            The term evaluation mode.

        Returns
        -------
        val : float or array
            In 'eval' mode, the term returns a single value (the
            integral, it does not need to be a scalar), while in 'weak'
            mode it returns an array for each element.
        status : int, optional
            The flag indicating evaluation success (0) or failure
            (nonzero). Only provided if `ret_status` is True.
        iels : array of ints, optional
            The local elements indices in 'weak' mode. Only provided in
            non-'eval' modes.
        """
        if standalone:
            self.standalone_setup()

        kwargs = kwargs.copy()
        term_mode = kwargs.pop('term_mode', None)

        if mode == 'eval':
            val = 0.0
            status = 0
            for ig in self.iter_groups():
                args = self.get_args(**kwargs)
                self.check_shapes(*args)

                _args = tuple(args) + (mode, term_mode, diff_var)
                fargs = self.call_get_fargs(_args, kwargs)

                shape, dtype = self.get_eval_shape(*_args, **kwargs)

                if dtype == nm.float64:
                    _v, stat = self.eval_real(shape, fargs, mode, term_mode,
                                               **kwargs)

                elif dtype == nm.complex128:
                    _v, stat = self.eval_complex(shape, fargs, mode, term_mode,
                                                 **kwargs)

                else:
                    raise ValueError('unsupported term dtype! (%s)' % dtype)

                val += _v
                status += stat

            val *= self.sign

        elif mode in ('el_avg', 'el', 'qp'):
            vals = None
            iels = nm.empty((0, 2), dtype=nm.int32)
            status = 0
            for ig in self.iter_groups():
                args = self.get_args(**kwargs)
                self.check_shapes(*args)

                _args = tuple(args) + (mode, term_mode, diff_var)
                fargs = self.call_get_fargs(_args, kwargs)

                shape, dtype = self.get_eval_shape(*_args, **kwargs)

                if dtype == nm.float64:
                    val, stat = self.eval_real(shape, fargs, mode,
                                               term_mode, **kwargs)

                elif dtype == nm.complex128:
                    val, stat = self.eval_complex(shape, fargs, mode,
                                                  term_mode, **kwargs)

                if vals is None:
                    vals = val

                else:
                    vals = nm.r_[vals, val]

                _iels = self.get_assembling_cells(val.shape)
                aux = nm.c_[nm.repeat(ig, _iels.shape[0])[:, None],
                            _iels[:, None]]
                iels = nm.r_[iels, aux]
                status += stat

            vals *= self.sign

        elif mode == 'weak':
            vals = []
            iels = []
            status = 0

            varr = self.get_virtual_variable()
            if diff_var is not None:
                varc = self.get_variables(as_list=False)[diff_var]

            for ig in self.iter_groups():
                args = self.get_args(**kwargs)
                self.check_shapes(*args)

                _args = tuple(args) + (mode, term_mode, diff_var)
                fargs = self.call_get_fargs(_args, kwargs)

                n_elr, n_qpr, dim, n_enr, n_cr = self.get_data_shape(varr)
                n_row = n_cr * n_enr

                if diff_var is None:
                    shape = (n_elr, 1, n_row, 1)

                else:
                    n_elc, n_qpc, dim, n_enc, n_cc = self.get_data_shape(varc)
                    n_col = n_cc * n_enc

                    shape = (n_elr, 1, n_row, n_col)

                if varr.dtype == nm.float64:
                    val, stat = self.eval_real(shape, fargs, mode, term_mode,
                                               diff_var, **kwargs)

                elif varr.dtype == nm.complex128:
                    val, stat = self.eval_complex(shape, fargs, mode, term_mode,
                                                  diff_var, **kwargs)

                else:
                    raise ValueError('unsupported term dtype! (%s)'
                                     % varr.dtype)

                vals.append(self.sign * val)
                iels.append((ig, self.get_assembling_cells(val.shape)))
                status += stat

        # Setup return value.
        if mode == 'eval':
            out = (val,)

        else:
            out = (vals, iels)

        if goptions['check_term_finiteness']:
            assert_(nm.isfinite(out[0]).all(),
                    msg='%+.2e * %s.%d.%s(%s) term values not finite!'
                    % (self.sign, self.name, self.integral.order,
                       self.region.name, self.arg_str))

        if ret_status:
            out = out + (status,)

        if len(out) == 1:
            out = out[0]

        return out

    def assemble_to(self, asm_obj, val, iels, mode='vector', diff_var=None):
        import sfepy.discrete.fem.extmods.assemble as asm

        vvar = self.get_virtual_variable()
        dc_type = self.get_dof_conn_type()

        if mode == 'vector':
            if asm_obj.dtype == nm.float64:
                assemble = asm.assemble_vector

            else:
                assert_(asm_obj.dtype == nm.complex128)
                assemble = asm.assemble_vector_complex
                for ii in range(len(val)):
                    if not(val[ii].dtype == nm.complex128):
                        val[ii] = nm.complex128(val[ii])

            for ii, (ig, _iels) in enumerate(iels):
                vec_in_els = val[ii]
                dc = vvar.get_dof_conn(dc_type, ig)
                assert_(vec_in_els.shape[2] == dc.shape[1])

                assemble(asm_obj, vec_in_els, _iels, 1.0, dc)

        elif mode == 'matrix':
            if asm_obj.dtype == nm.float64:
                assemble = asm.assemble_matrix

            else:
                assert_(asm_obj.dtype == nm.complex128)
                assemble = asm.assemble_matrix_complex

            svar = diff_var
            tmd = (asm_obj.data, asm_obj.indptr, asm_obj.indices)
            for ii, (ig, _iels) in enumerate(iels):
                mtx_in_els = val[ii]
                if ((asm_obj.dtype == nm.complex128)
                    and (mtx_in_els.dtype == nm.float64)):
                    mtx_in_els = mtx_in_els.astype(nm.complex128)

                rdc = vvar.get_dof_conn(dc_type, ig)

                is_trace = self.arg_traces[svar.name]
                cdc = svar.get_dof_conn(dc_type, ig, is_trace=is_trace)
                assert_(mtx_in_els.shape[2:] == (rdc.shape[1], cdc.shape[1]))

                sign = 1.0
                if self.arg_derivatives[svar.name]:
                    if not self.is_quasistatic or (self.step > 0):
                        sign *= 1.0 / self.dt

                    else:
                        sign = 0.0

                assemble(tmd[0], tmd[1], tmd[2], mtx_in_els,
                         _iels, sign, rdc, cdc)

        else:
            raise ValueError('unknown assembling mode! (%s)' % mode)

########NEW FILE########
__FILENAME__ = terms_acoustic
import numpy as nm

from sfepy.terms.terms import Term, terms

class DiffusionSATerm(Term):
    r"""
    Diffusion sensitivity analysis term.

    :Definition:

    .. math::
        \int_{\Omega} \left[ (\dvg \ul{\Vcal}) K_{ij} \nabla_i q\, \nabla_j p -
        K_{ij} (\nabla_j \ul{\Vcal} \nabla q) \nabla_i p - K_{ij} \nabla_j q
        (\nabla_i \ul{\Vcal} \nabla p)\right]

    :Arguments:
        - material:    :math:`K_{ij}`
        - parameter_q: :math:`q`
        - parameter_p: :math:`p`
        - parameter_v: :math:`\ul{\Vcal}`
    """
    name = 'd_diffusion_sa'
    arg_types = ('material', 'parameter_q', 'parameter_p', 'parameter_v')
    arg_shapes = {'material' : 'D, D',
                  'parameter_q' : 1, 'parameter_p' : 1, 'parameter_v' : 'D'}

    function = staticmethod(terms.d_diffusion_sa)

    def get_fargs(self, mat, parameter_q, parameter_p, parameter_v,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(parameter_p)

        grad_q = self.get(parameter_q, 'grad')
        grad_p = self.get(parameter_p, 'grad')
        grad_v = self.get(parameter_v, 'grad')
        div_v = self.get(parameter_v, 'div')

        return grad_q, grad_p, grad_v, div_v, mat, vg

    def get_eval_shape(self, mat, parameter_q, parameter_p, parameter_v,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(parameter_q)

        return (n_el, 1, 1, 1), parameter_q.dtype

class SurfaceLaplaceLayerTerm(Term):
    r"""
    Acoustic 'layer' term - derivatives in surface directions.

    :Definition:

    .. math::
        \int_{\Gamma} c \partial_\alpha \ul{q}\,\partial_\alpha \ul{p}, \alpha
        = 1,\dots,N-1

    :Arguments 1:
        - material: :math:`c`
        - virtual:  :math:`q`
        - state:    :math:`p`

    :Arguments 2:
        - material:    :math:`c`
        - parameter_1: :math:`q`
        - parameter_2: :math:`p`
    """
    name = 'dw_surface_laplace'
    arg_types = [('material', 'virtual', 'state'),
                 ('material', 'parameter_2', 'parameter_1')]
    arg_shapes = {'material' : '1, 1', 'virtual' : (1, 'state'),
                  'state' : 1, 'parameter_1' : 1, 'parameter_2' : 1}
    geometries = ['2_3', '2_4']
    modes = ('weak', 'eval')
    integration = 'surface'

    def get_fargs(self, mat, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        ap, sg = self.get_approximation(virtual)
        aps, sgs = self.get_approximation(state)

        sd = aps.surface_data[self.region.name]
        bfg = ap.get_base(sd.face_type, 1, self.integral)
        bfg4 = bfg.reshape((1,) + bfg.shape)

        elvol = nm.sum(sgs.det, axis=1)
        vol = nm.tile(elvol[:,nm.newaxis], (1, sgs.det.shape[1], 1, 1))

        if mode == 'weak':
            if diff_var is None:
                vec = self.get(state, 'val', bf=bfg4) / vol
                fmode = 0

            else:
                vec = nm.array([0], ndmin=4, dtype=nm.float64)
                fmode = 1

            return vec, mat, bfg, sgs, fmode

        elif mode == 'eval':
            vec1 = self.get(state, 'val', bf=bfg4) / vol
            vec2 = self.get(virtual, 'val', bf=bfg4) / vol

            return vec1, vec2, mat, sgs

    def get_eval_shape(self, mat, virtual, state,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)

        return (n_el, 1, 1, 1), state.dtype

    def set_arg_types(self):

        if self.mode == 'weak':
            self.function = terms.dw_surf_laplace
        else:
            self.function = terms.d_surf_laplace

class SurfaceCoupleLayerTerm(Term):
    r"""
    Acoustic 'layer' term - derivatives in surface directions.

    :Definition:

    .. math::
        \int_{\Gamma} c q\,\partial_\alpha p,
        \int_{\Gamma} c \partial_\alpha p\, q,
        \int_{\Gamma} c \partial_\alpha r\, s,\alpha = 1,\dots,N-1

    :Arguments 1:
        - material: :math:`c`
        - virtual:  :math:`q`
        - state:    :math:`p`

    :Arguments 2:
        - material: :math:`c`
        - virtual:  :math:`q`
        - state:    :math:`p`

    :Arguments 3:
        - material:    :math:`c`
        - parameter_1: :math:`s`
        - parameter_2: :math:`r`
    """
    name = 'dw_surface_lcouple'
    arg_types = (('material', 'virtual', 'state'),
                 ('material', 'state', 'virtual'),
                 ('material', 'parameter_1', 'parameter_2'))
    arg_shapes = {'material' : '1, 1', 'virtual' : (1, 'state'),
                  'state' : 1, 'parameter_1' : 1, 'parameter_2' : 1}
    geometries = ['2_3', '2_4']
    modes = ('bv_ns', 'nv_bs', 'eval')
    integration = 'surface'

    def get_fargs(self, mat, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        if self.mode == 'nv_bs':
            state, virtual = virtual, state

        ap, sg = self.get_approximation(virtual)
        aps, sgs = self.get_approximation(state)

        sd = aps.surface_data[self.region.name]
        bf = ap.get_base(sd.face_type, 0, self.integral)
        bfg = ap.get_base(sd.face_type, 1, self.integral)
        bfg4 = bfg.reshape((1,) + bfg.shape)

        elvol = nm.sum(sgs.det, axis=1)
        vol = nm.tile(elvol[:,nm.newaxis], (1, sgs.det.shape[1], 1, 1))

        if mode == 'weak':
            if self.mode == 'nv_bs':
                bf, bfg = bfg, bf
                nmat = mat.reshape((mat.shape[0], mat.shape[1],
                                    1, nm.max(mat.shape[2:])))

            else:
                nmat = mat.reshape((mat.shape[0], mat.shape[1],
                                    nm.max(mat.shape[2:]), 1))

            if diff_var is None:
                if self.mode == 'nv_bs':
                    vec = self.get(state, 'val', bf=bfg4) / vol
                else:
                    vec = self.get(state, 'val')
                fmode = 0

            else:
                vec = nm.array([0], ndmin=4, dtype=nm.float64)
                fmode = 1

            return vec, nmat, bf, bfg, sgs, fmode

        elif mode == 'eval':
            vec1 = self.get(state, 'val')
            vec2 = self.get(virtual, 'val', bf=bfg4) / vol
            nmat = mat.reshape((mat.shape[0], mat.shape[1],
                                1, nm.max(mat.shape[2:])))

            return vec1, vec2, nmat, sgs

    def get_eval_shape(self, mat, virtual, state,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)

        return (n_el, 1, 1, 1), state.dtype

    def set_arg_types(self):
        if self.mode == 'bv_ns' or self.mode == 'nv_bs':
            self.function = terms.dw_surf_lcouple
        else:
            self.function = terms.d_surf_lcouple

########NEW FILE########
__FILENAME__ = terms_adj_navier_stokes
import numpy as nm

from sfepy.terms.terms import Term, terms
from sfepy.base.base import get_default

def grad_as_vector(grad):
    grad = grad.transpose((0, 1, 3, 2))
    sh = grad.shape
    return grad.reshape((sh[0], sh[1], sh[2] * sh[3], 1))

class AdjDivGradTerm(Term):
    r"""
    Gateaux differential of :math:`\Psi(\ul{u}) = \int_{\Omega} \nu\
    \nabla \ul{v} : \nabla \ul{u}` w.r.t. :math:`\ul{u}` in the direction
    :math:`\ul{v}` or adjoint term to `dw_div_grad`.

    :Definition:

    .. math::
        w \delta_{u} \Psi(\ul{u}) \circ \ul{v}

    :Arguments:
        - material_1 : :math:`w` (weight)
        - material_2 : :math:`\nu` (viscosity)
        - virtual    : :math:`\ul{v}`
        - state      : :math:`\ul{u}`
    """
    name = 'dw_adj_div_grad'
    arg_types = ('material_1', 'material_2', 'virtual', 'parameter')
    arg_shapes = {'material_1' : '1, 1', 'material_2' : '1, 1',
                  'virtual' : ('D', None), 'parameter' : 'D'}

    function = staticmethod(terms.term_ns_asm_div_grad)

    def get_fargs(self, mat1, mat2, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(state)

        if diff_var is None:
            grad = grad_as_vector(self.get(state, 'grad'))
            fmode = 0

        else:
            grad = nm.array([0], ndmin=4, dtype=nm.float64)
            fmode = 1

        return grad, mat1 * mat2, vg, fmode

class AdjConvect1Term(Term):
    r"""
    The first adjoint term to nonlinear convective term `dw_convect`.

    :Definition:

    .. math::
        \int_{\Omega} ((\ul{v} \cdot \nabla) \ul{u}) \cdot \ul{w}

    :Arguments:
        - virtual   : :math:`\ul{v}`
        - state     : :math:`\ul{w}`
        - parameter : :math:`\ul{u}`
    """
    name = 'dw_adj_convect1'
    arg_types = ('virtual', 'state', 'parameter' )
    arg_shapes = {'virtual' : ('D', 'state'), 'state' : 'D', 'parameter' : 'D'}

    function = staticmethod(terms.dw_adj_convect1)

    def get_fargs(self, virtual, state, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(state)

        val_w = self.get(state, 'val')
        grad_u = self.get(parameter, 'grad') # No transposition here!

        fmode = diff_var is not None

        return val_w, grad_u, vg, fmode

class AdjConvect2Term(Term):
    r"""
    The second adjoint term to nonlinear convective term `dw_convect`.

    :Definition:

    .. math::
        \int_{\Omega} ((\ul{u} \cdot \nabla) \ul{v}) \cdot \ul{w}

    :Arguments:
        - virtual   : :math:`\ul{v}`
        - state     : :math:`\ul{w}`
        - parameter : :math:`\ul{u}`
    """
    name = 'dw_adj_convect2'
    arg_types = ('virtual', 'state', 'parameter' )
    arg_shapes = {'virtual' : ('D', 'state'), 'state' : 'D', 'parameter' : 'D'}

    function = staticmethod(terms.dw_adj_convect2)

    def get_fargs(self, virtual, state, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(state)

        val_w = self.get(state, 'val')
        val_u = self.get(parameter, 'val')

        fmode = diff_var is not None

        return val_w, val_u, vg, fmode

class SUPGCAdjStabilizationTerm(Term):
    r"""
    Adjoint term to SUPG stabilization term `dw_st_supg_c`.

    :Definition:

    .. math::
        \sum_{K \in \Ical_h}\int_{T_K} \delta_K\ [ ((\ul{v} \cdot \nabla)
        \ul{u}) ((\ul{u} \cdot \nabla) \ul{w}) + ((\ul{u} \cdot \nabla)
        \ul{u}) ((\ul{v} \cdot \nabla) \ul{w}) ]

    :Arguments:
        - material  : :math:`\delta_K`
        - virtual   : :math:`\ul{v}`
        - state     : :math:`\ul{w}`
        - parameter : :math:`\ul{u}`
    """
    name = 'dw_st_adj_supg_c'
    arg_types = ('material', 'virtual', 'parameter', 'state')
    arg_shapes = {'material' : '1, 1', 'virtual' : ('D', 'state'),
                  'state' : 'D', 'parameter' : 'D'}

    function = staticmethod(terms.dw_st_adj_supg_c)

    def get_fargs(self, mat, virtual, state, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        ap, vg = self.get_approximation(state)

        val_u = self.get(parameter, 'val')
        grad_u = self.get(parameter, 'grad').transpose((0, 1, 3, 2)).copy()
        conn = ap.get_connectivity(self.region, self.integration)

        fmode = diff_var is not None

        return state(), val_u, grad_u, mat, vg, conn, fmode

class SUPGPAdj1StabilizationTerm(Term):
    r"""
    The first adjoint term to SUPG stabilization term `dw_st_supg_p`.

    :Definition:

    .. math::
        \sum_{K \in \Ical_h}\int_{T_K} \delta_K\ \nabla p (\ul{v} \cdot
        \nabla \ul{w})

    :Arguments:
        - material  : :math:`\delta_K`
        - virtual   : :math:`\ul{v}`
        - state     : :math:`\ul{w}`
        - parameter : :math:`p`
    """
    name = 'dw_st_adj1_supg_p'
    arg_types = ('material', 'virtual', 'state', 'parameter')
    arg_shapes = {'material' : '1, 1', 'virtual' : ('D', 'state'),
                  'state' : 'D', 'parameter' : 1}

    function = staticmethod(terms.dw_st_adj1_supg_p)

    def get_fargs(self, mat, virtual, state, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        ap_w, vg_w = self.get_approximation(state)

        grad_p = self.get(parameter, 'grad')
        conn_w = ap_w.get_connectivity(self.region, self.integration)

        fmode = diff_var is not None

        return state(), grad_p, mat, vg_w, conn_w, fmode

class SUPGPAdj2StabilizationTerm(Term):
    r"""
    The second adjoint term to SUPG stabilization term `dw_st_supg_p`
    as well as adjoint term to PSPG stabilization term `dw_st_pspg_c`.

    :Definition:

    .. math::
        \sum_{K \in \Ical_h}\int_{T_K} \tau_K\ \nabla r (\ul{v} \cdot \nabla
        \ul{u})

    :Arguments:
        - material  : :math:`\tau_K`
        - virtual   : :math:`\ul{v}`
        - parameter : :math:`\ul{u}`
        - state     : :math:`r`
    """
    name = 'dw_st_adj2_supg_p'
    arg_types = ('material', 'virtual', 'parameter', 'state')
    arg_shapes = {'material' : '1, 1', 'virtual' : ('D', 'state'),
                  'state' : 1, 'parameter' : 'D'}

    function = staticmethod(terms.dw_st_adj2_supg_p)

    def get_fargs(self, mat, virtual, parameter, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        ap_r, vg_r = self.get_approximation(state)
        vg_u, _ = self.get_mapping(parameter)

        grad_u = self.get(parameter, 'grad').transpose((0, 1, 3, 2)).copy()
        conn_r = ap_r.get_connectivity(self.region, self.integration)

        fmode = diff_var is not None

        return grad_u, state(), mat, vg_u, vg_r, conn_r, fmode

class SDDotVolumeTerm(Term):
    r"""
    Sensitivity (shape derivative) of dot product of scalars or vectors.

    :Definition:

    .. math::
        \int_{\Omega_D} p q (\nabla \cdot \ul{\Vcal}) \mbox{ , }
        \int_{\Omega_D} (\ul{u} \cdot \ul{w}) (\nabla \cdot \ul{\Vcal})

    :Arguments:
        - parameter_1 : :math:`p` or :math:`\ul{u}`
        - parameter_2 : :math:`q` or :math:`\ul{w}`
        - parameter_mesh_velocity : :math:`\ul{\Vcal}`
    """
    name = 'd_sd_volume_dot'
    arg_types = ('parameter_1', 'parameter_2', 'parameter_mesh_velocity')
    arg_shapes = {'parameter_1' : 'D', 'parameter_2' : 'D',
                  'parameter_mesh_velocity' : 'D'}

    function = staticmethod(terms.d_sd_volume_dot)

    def get_fargs(self, par1, par2, par_mv,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(par1)

        val1 = self.get(par1, 'val')
        val2 = self.get(par2, 'val')
        div_mv = self.get(par_mv, 'div')

        return val1, val2, div_mv, vg, get_default(term_mode, 1)

    def get_eval_shape(self, par1, par2, par_mv,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(par1)
        return (n_el, 1, 1, 1), par1.dtype

class SDDivTerm(Term):
    r"""
    Sensitivity (shape derivative) of Stokes term `dw_stokes` in 'div' mode.

    Supports the following term modes: 1 (sensitivity) or 0 (original term
    value).

    :Definition:

    .. math::
        \int_{\Omega_D} p [ (\nabla \cdot \ul{w}) (\nabla \cdot \ul{\Vcal})
        - \pdiff{\Vcal_k}{x_i} \pdiff{w_i}{x_k} ]

    :Arguments:
        - parameter_u : :math:`\ul{u}`
        - parameter_p : :math:`p`
        - parameter_mesh_velocity : :math:`\ul{\Vcal}`
    """
    name = 'd_sd_div'
    arg_types = ('parameter_u', 'parameter_p', 'parameter_mesh_velocity')
    arg_shapes = {'parameter_u' : 'D', 'parameter_p' : 1,
                  'parameter_mesh_velocity' : 'D'}

    function = staticmethod(terms.d_sd_div)

    def get_fargs(self, par_u, par_p, par_mv,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(par_u)

        div_u = self.get(par_u, 'div')
        grad_u = grad_as_vector(self.get(par_u, 'grad'))
        val_p = self.get(par_p, 'val')
        div_mv = self.get(par_mv, 'div')
        grad_mv = grad_as_vector(self.get(par_mv, 'grad'))

        return (div_u, grad_u, val_p, div_mv, grad_mv, vg,
                get_default(term_mode, 1))

    def get_eval_shape(self, par_u, par_p, par_mv,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(par_u)
        return (n_el, 1, 1, 1), par_u.dtype

class SDDivGradTerm(Term):
    r"""
    Sensitivity (shape derivative) of diffusion term `dw_div_grad`.

    Supports the following term modes: 1 (sensitivity) or 0 (original term
    value).

    :Definition:

    .. math::
        w \nu \int_{\Omega_D} [ \pdiff{u_i}{x_k} \pdiff{w_i}{x_k}
        (\nabla \cdot \ul{\Vcal})
        - \pdiff{\Vcal_j}{x_k} \pdiff{u_i}{x_j} \pdiff{w_i}{x_k}
        - \pdiff{u_i}{x_k} \pdiff{\Vcal_l}{x_k} \pdiff{w_i}{x_k} ]

    :Arguments:
        - material_1  : :math:`w` (weight)
        - material_2  : :math:`\nu` (viscosity)
        - parameter_u : :math:`\ul{u}`
        - parameter_w : :math:`\ul{w}`
        - parameter_mesh_velocity : :math:`\ul{\Vcal}`
    """
    name = 'd_sd_div_grad'
    arg_types = ('material_1', 'material_2', 'parameter_u', 'parameter_w',
                 'parameter_mesh_velocity')
    arg_shapes = {'material_1' : '1, 1', 'material_2' : '1, 1',
                  'parameter_u' : 'D', 'parameter_w' : 'D',
                  'parameter_mesh_velocity' : 'D'}

    function = staticmethod(terms.d_sd_div_grad)

    def get_fargs(self, mat1, mat2, par_u, par_w, par_mv,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(par_u)

        grad_u = grad_as_vector(self.get(par_u, 'grad'))
        grad_w = grad_as_vector(self.get(par_w, 'grad'))
        div_mv = self.get(par_mv, 'div')
        grad_mv = grad_as_vector(self.get(par_mv, 'grad'))

        return (grad_u, grad_w, div_mv, grad_mv, mat1 * mat2, vg,
                get_default(term_mode, 1))

    def get_eval_shape(self, mat1, mat2, par_u, par_w, par_mv,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(par_u)
        return (n_el, 1, 1, 1), par_u.dtype

class SDConvectTerm(Term):
    r"""
    Sensitivity (shape derivative) of convective term `dw_convect`.

    Supports the following term modes: 1 (sensitivity) or 0 (original term
    value).

    :Definition:

    .. math::
        \int_{\Omega_D} [ u_k \pdiff{u_i}{x_k} w_i (\nabla \cdot \Vcal)
        - u_k \pdiff{\Vcal_j}{x_k} \pdiff{u_i}{x_j} w_i ]

    :Arguments:
        - parameter_u : :math:`\ul{u}`
        - parameter_w : :math:`\ul{w}`
        - parameter_mesh_velocity : :math:`\ul{\Vcal}`
    """
    name = 'd_sd_convect'
    arg_types = ('parameter_u', 'parameter_w', 'parameter_mesh_velocity')
    arg_shapes = {'parameter_u' : 'D', 'parameter_w' : 'D',
                  'parameter_mesh_velocity' : 'D'}

    function = staticmethod(terms.d_sd_convect)

    def get_fargs(self, par_u, par_w, par_mv,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(par_u)

        val_u = self.get(par_u, 'val')
        grad_u = grad_as_vector(self.get(par_u, 'grad'))
        val_w = self.get(par_w, 'val')
        div_mv = self.get(par_mv, 'div')
        grad_mv = grad_as_vector(self.get(par_mv, 'grad'))

        return (val_u, grad_u, val_w, div_mv, grad_mv, vg,
                get_default(term_mode, 1))

    def get_eval_shape(self, par_u, par_w, par_mv,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(par_u)
        return (n_el, 1, 1, 1), par_u.dtype

class NSOFMinGradTerm(Term):
    name = 'd_of_ns_min_grad'
    arg_types = ('material_1', 'material_2', 'parameter')
    arg_shapes = {'material_1' : '1, 1', 'material_2' : '1, 1',
                  'parameter' : 1}

    function = staticmethod(terms.d_of_nsMinGrad)

    def get_fargs(self, weight, mat, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(parameter)

        grad = grad_as_vector(self.get(parameter, 'grad'))

        return grad, weight * mat, vg

    def get_eval_shape(self, weight, mat, parameter,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        return (1, 1, 1, 1), parameter.dtype

class NSOFSurfMinDPressTerm(Term):
    r"""
    Sensitivity of :math:`\Psi(p)`.

    :Definition:

    .. math::
        \delta \Psi(p) = \delta \left( \int_{\Gamma_{in}}p -
        \int_{\Gamma_{out}}bpress \right)

    :Arguments:
        - material_1 : :math:`w` (weight)
        - material_2 : :math:`bpress` (given pressure)
        - parameter  : :math:`p`
    """
    name = 'd_of_ns_surf_min_d_press'
    arg_types = ('material_1', 'material_2', 'parameter')
    arg_shapes = {'material_1' : 1, 'material_2' : 1,
                  'parameter' : 1}
    integration = 'surface'

    function = staticmethod(terms.d_of_nsSurfMinDPress)

    def get_fargs(self, weight, bpress, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        sg, _ = self.get_mapping(parameter)

        val_p = self.get(parameter, 'val')

        return val_p, weight, bpress, sg, 0

    def get_eval_shape(self, weight, bpress, parameter,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        return (1, 1, 1, 1), parameter.dtype

class NSOFSurfMinDPressDiffTerm(NSOFSurfMinDPressTerm):
    r"""
    Gateaux differential of :math:`\Psi(p)` w.r.t. :math:`p` in the
    direction :math:`q`.

    :Definition:

    .. math::
        w \delta_{p} \Psi(p) \circ q

    :Arguments:
        - material : :math:`w` (weight)
        - virtual  : :math:`q`
    """
    name = 'dw_of_ns_surf_min_d_press_diff'
    arg_types = ('material', 'virtual')
    arg_shapes = {'material' : 1, 'virtual' : (1, None)}

    def get_fargs(self, weight, virtual,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        sg, _ = self.get_mapping(virtual)

        aux = nm.array([0], ndmin=4, dtype=nm.float64)

        return aux, weight, 0.0, sg, 1

class SDGradDivStabilizationTerm(Term):
    r"""
    Sensitivity (shape derivative) of stabilization term `dw_st_grad_div`.

    :Definition:

    .. math::
        \gamma \int_{\Omega_D} [ (\nabla \cdot \ul{u}) (\nabla \cdot \ul{w})
        (\nabla \cdot \ul{\Vcal})
        - \pdiff{u_i}{x_k} \pdiff{\Vcal_k}{x_i} (\nabla \cdot \ul{w})
        - (\nabla \cdot \ul{u}) \pdiff{w_i}{x_k} \pdiff{\Vcal_k}{x_i} ]

    :Arguments:
        - material    : :math:`\gamma`
        - parameter_u : :math:`\ul{u}`
        - parameter_w : :math:`\ul{w}`
        - parameter_mesh_velocity : :math:`\ul{\Vcal}`
        - mode        : 1 (sensitivity) or 0 (original term value)
    """
    name = 'd_sd_st_grad_div'
    arg_types = ('material', 'parameter_u', 'parameter_w',
                 'parameter_mesh_velocity')
    arg_shapes = {'material' : '1, 1',
                  'parameter_u' : 'D', 'parameter_w' : 'D',
                  'parameter_mesh_velocity' : 'D'}

    function = staticmethod(terms.d_sd_st_grad_div)

    def get_fargs(self, mat, par_u, par_w, par_mv,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(par_u)

        div_u = self.get(par_u, 'div')
        grad_u = grad_as_vector(self.get(par_u, 'grad'))
        div_w = self.get(par_w, 'div')
        grad_w = grad_as_vector(self.get(par_w, 'grad'))
        div_mv = self.get(par_mv, 'div')
        grad_mv = grad_as_vector(self.get(par_mv, 'grad'))

        return (div_u, grad_u, div_w, grad_w, div_mv, grad_mv, mat, vg,
                get_default(term_mode, 1))

    def get_eval_shape(self, mat, par_u, par_w, par_mv,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(par_u)
        return (n_el, 1, 1, 1), par_u.dtype

class SDSUPGCStabilizationTerm(Term):
    r"""
    Sensitivity (shape derivative) of stabilization term `dw_st_supg_c`.

    :Definition:

    .. math::
        \sum_{K \in \Ical_h}\int_{T_K} \delta_K\ [ (\ul{b} \cdot \nabla u_k)
        (\ul{b} \cdot \nabla w_k) (\nabla \cdot \Vcal) -
        (\ul{b} \cdot \nabla \Vcal_i) \pdiff{u_k}{x_i}
        (\ul{b} \cdot \nabla w_k) - (\ul{u} \cdot \nabla u_k)
        (\ul{b} \cdot \nabla \Vcal_i) \pdiff{w_k}{x_i} ]

    :Arguments:
        - material    : :math:`\delta_K`
        - parameter_b : :math:`\ul{b}`
        - parameter_u : :math:`\ul{u}`
        - parameter_w : :math:`\ul{w}`
        - parameter_mesh_velocity : :math:`\ul{\Vcal}`
        - mode        : 1 (sensitivity) or 0 (original term value)
    """
    name = 'd_sd_st_supg_c'
    arg_types = ('material', 'parameter_b', 'parameter_u', 'parameter_w',
                'parameter_mesh_velocity')
    arg_shapes = {'material' : '1, 1',
                  'parameter_b' : 'D', 'parameter_u' : 'D', 'parameter_w' : 'D',
                  'parameter_mesh_velocity' : 'D'}

    function = staticmethod(terms.d_sd_st_supg_c)

    def get_fargs(self, mat, par_b, par_u, par_w, par_mv,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(par_u)

        val_b = self.get(par_b, 'val')
        grad_u = self.get(par_u, 'grad').transpose((0, 1, 3, 2)).copy()
        grad_w = self.get(par_w, 'grad').transpose((0, 1, 3, 2)).copy()
        div_mv = self.get(par_mv, 'div')
        grad_mv = self.get(par_mv, 'grad').transpose((0, 1, 3, 2)).copy()

        return (val_b, grad_u, grad_w, div_mv, grad_mv, mat, vg,
                get_default(term_mode, 1))

    def get_eval_shape(self, mat, par_b, par_u, par_w, par_mv,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(par_u)
        return (n_el, 1, 1, 1), par_u.dtype

class SDPSPGCStabilizationTerm(Term):
    r"""
    Sensitivity (shape derivative) of stabilization terms `dw_st_supg_p` or
    `dw_st_pspg_c`.

    :Definition:

    .. math::
        \sum_{K \in \Ical_h}\int_{T_K} \delta_K\
        [ \pdiff{r}{x_i} (\ul{b} \cdot \nabla u_i) (\nabla \cdot \Vcal) -
        \pdiff{r}{x_k} \pdiff{\Vcal_k}{x_i} (\ul{b} \cdot \nabla u_i)
        - \pdiff{r}{x_k} (\ul{b} \cdot \nabla \Vcal_k) \pdiff{u_i}{x_k} ]

    :Arguments:
        - material    : :math:`\delta_K`
        - parameter_b : :math:`\ul{b}`
        - parameter_u : :math:`\ul{u}`
        - parameter_r : :math:`r`
        - parameter_mesh_velocity : :math:`\ul{\Vcal}`
        - mode        : 1 (sensitivity) or 0 (original term value)
    """
    name = 'd_sd_st_pspg_c'
    arg_types = ('material', 'parameter_b', 'parameter_u', 'parameter_r',
                'parameter_mesh_velocity')
    arg_shapes = {'material' : '1, 1',
                  'parameter_b' : 'D', 'parameter_u' : 'D', 'parameter_r' : 1,
                  'parameter_mesh_velocity' : 'D'}

    function = staticmethod(terms.d_sd_st_pspg_c)

    def get_fargs(self, mat, par_b, par_u, par_r, par_mv,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(par_u)

        val_b = self.get(par_b, 'val')
        grad_u = self.get(par_u, 'grad').transpose((0, 1, 3, 2)).copy()
        grad_r = self.get(par_r, 'grad')
        div_mv = self.get(par_mv, 'div')
        grad_mv = self.get(par_mv, 'grad').transpose((0, 1, 3, 2)).copy()

        return (val_b, grad_u, grad_r, div_mv, grad_mv, mat, vg,
                get_default(term_mode, 1))

    def get_eval_shape(self, mat, par_b, par_u, par_r, par_mv,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(par_u)
        return (n_el, 1, 1, 1), par_u.dtype

class SDPSPGPStabilizationTerm(Term):
    r"""
    Sensitivity (shape derivative) of stabilization term `dw_st_pspg_p`.

    :Definition:

    .. math::
        \sum_{K \in \Ical_h}\int_{T_K} \tau_K\ [ (\nabla r \cdot \nabla p)
        (\nabla \cdot \Vcal) - \pdiff{r}{x_k} (\nabla \Vcal_k \cdot \nabla p) -
        (\nabla r \cdot \nabla \Vcal_k) \pdiff{p}{x_k} ]

    :Arguments:
        - material    : :math:`\tau_K`
        - parameter_r : :math:`r`
        - parameter_p : :math:`p`
        - parameter_mesh_velocity : :math:`\ul{\Vcal}`
        - mode        : 1 (sensitivity) or 0 (original term value)
    """
    name = 'd_sd_st_pspg_p'
    arg_types = ('material', 'parameter_r', 'parameter_p',
                'parameter_mesh_velocity')
    arg_shapes = {'material' : '1, 1',
                  'parameter_r' : 1, 'parameter_p' : 1,
                  'parameter_mesh_velocity' : 'D'}

    function = staticmethod(terms.d_sd_st_pspg_p)

    def get_fargs(self, mat, par_r, par_p, par_mv,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(par_p)

        grad_r = self.get(par_r, 'grad')
        grad_p = self.get(par_p, 'grad')
        div_mv = self.get(par_mv, 'div')
        grad_mv = self.get(par_mv, 'grad').transpose((0, 1, 3, 2)).copy()

        return (grad_r, grad_p, div_mv, grad_mv, mat, vg,
                get_default(term_mode, 1))

    def get_eval_shape(self, mat, par_r, par_p, par_mv,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(par_p)
        return (n_el, 1, 1, 1), par_p.dtype

########NEW FILE########
__FILENAME__ = terms_basic
import numpy as nm

from sfepy.base.base import assert_
from sfepy.linalg import dot_sequences
from sfepy.terms.terms import Term, terms

class IntegrateVolumeTerm(Term):
    r"""
    Evaluate (weighted) variable in a volume region.

    Depending on evaluation mode, integrate a variable over a volume region
    ('eval'), average it in elements ('el_avg') or interpolate it into volume
    quadrature points ('qp').

    Supports 'eval', 'el_avg' and 'qp' evaluation modes.

    :Definition:

    .. math::
        \int_\Omega y \mbox{ , } \int_\Omega \ul{y} \\
        \int_\Omega c y \mbox{ , } \int_\Omega c \ul{y}

    .. math::
        \mbox{vector for } K \from \Ical_h:
        \int_{T_K} y / \int_{T_K} 1 \mbox{ , }
        \int_{T_K} \ul{y} / \int_{T_K} 1 \\
        \mbox{vector for } K \from \Ical_h:
        \int_{T_K} c y / \int_{T_K} 1 \mbox{ , }
        \int_{T_K} c \ul{y} / \int_{T_K} 1

    .. math::
        y|_{qp} \mbox{ , } \ul{y}|_{qp} \\
        c y|_{qp} \mbox{ , } c \ul{y}|_{qp}

    :Arguments:
        - material : :math:`c` (optional)
        - parameter : :math:`y` or :math:`\ul{y}`
    """
    name = 'ev_volume_integrate'
    arg_types = ('opt_material', 'parameter')
    arg_shapes = [{'opt_material' : '1, 1', 'parameter' : 1},
                  {'opt_material' : None},
                  {'opt_material' : '1, 1', 'parameter' : 'D'},
                  {'opt_material' : None}]

    @staticmethod
    def function(out, val_qp, vg, fmode):
        if fmode == 2:
            out[:] = val_qp
            status = 0

        else:
            status = vg.integrate(out, val_qp, fmode)

        return status

    def get_fargs(self, material, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(parameter)

        val_qp = self.get(parameter, 'val')
        if material is not None:
            val_qp *= material

        fmode = {'eval' : 0, 'el_avg' : 1, 'qp' : 2}.get(mode, 1)

        return val_qp, vg, fmode

    def get_eval_shape(self, material, parameter,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(parameter)

        if mode != 'qp':
            n_qp = 1

        return (n_el, n_qp, n_c, 1), parameter.dtype

class IntegrateSurfaceTerm(Term):
    r"""
    Evaluate (weighted) variable in a surface region.

    Depending on evaluation mode, integrate a variable over a surface region
    ('eval'), average it in element faces ('el_avg') or interpolate it into
    surface quadrature points ('qp'). For vector variables, setting `term_mode`
    to `'flux'` leads to computing corresponding fluxes for the three modes
    instead.

    Supports 'eval', 'el_avg' and 'qp' evaluation modes.

    :Definition:

    .. math::
        \int_\Gamma y \mbox{ , } \int_\Gamma \ul{y}
        \mbox{ , } \int_\Gamma \ul{y} \cdot \ul{n} \\
        \int_\Gamma c y \mbox{ , } \int_\Gamma c \ul{y}
        \mbox{ , } \int_\Gamma c \ul{y} \cdot \ul{n} \mbox{ flux }

    .. math::
        \mbox{vector for } K \from \Ical_h:
        \int_{T_K} y / \int_{T_K} 1 \mbox{ , }
        \int_{T_K} \ul{y} / \int_{T_K} 1 \mbox{ , }
        \int_{T_K} (\ul{y} \cdot \ul{n}) / \int_{T_K} 1 \\
        \mbox{vector for } K \from \Ical_h:
        \int_{T_K} c y / \int_{T_K} 1 \mbox{ , }
        \int_{T_K} c \ul{y} / \int_{T_K} 1 \mbox{ , }
        \int_{T_K} (c \ul{y} \cdot \ul{n}) / \int_{T_K} 1

    .. math::
        y|_{qp} \mbox{ , } \ul{y}|_{qp}
        \mbox{ , } (\ul{y} \cdot \ul{n})|_{qp} \mbox{ flux } \\
        c y|_{qp} \mbox{ , } c \ul{y}|_{qp}
        \mbox{ , } (c \ul{y} \cdot \ul{n})|_{qp} \mbox{ flux }

    :Arguments:
        - material : :math:`c` (optional)
        - parameter : :math:`y` or :math:`\ul{y}`
    """
    name = 'ev_surface_integrate'
    arg_types = ('opt_material', 'parameter')
    arg_shapes = [{'opt_material' : '1, 1', 'parameter' : 1},
                  {'opt_material' : None},
                  {'opt_material' : '1, 1', 'parameter' : 'D'},
                  {'opt_material' : None}]
    integration = 'surface'

    @staticmethod
    def function(out, val_qp, sg, fmode):
        if fmode == 2:
            out[:] = val_qp
            status = 0

        elif fmode == 5:
            normal = sg.normal
            out[:] = dot_sequences(val_qp, normal)
            status = 0

        else:
            status = sg.integrate(out, val_qp, fmode)

        return status

    def get_fargs(self, material, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        sg, _ = self.get_mapping(parameter)

        val_qp = self.get(parameter, 'val')
        if material is not None:
            val_qp *= material

        fmode = {'eval' : 0, 'el_avg' : 1, 'qp' : 2}.get(mode, 1)
        if term_mode == 'flux':
            n_fa, n_qp, dim, n_fn, n_c = self.get_data_shape(parameter)
            if n_c == dim:
                fmode += 3

        return val_qp, sg, fmode

    def get_eval_shape(self, material, parameter,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_fa, n_qp, dim, n_fn, n_c = self.get_data_shape(parameter)

        if mode != 'qp':
            n_qp = 1

        if term_mode == 'flux':
            n_c = 1

        return (n_fa, n_qp, n_c, 1), parameter.dtype

class IntegrateVolumeOperatorTerm(Term):
    r"""
    Volume integral of a test function weighted by a scalar function
    :math:`c`.

    :Definition:

    .. math::
        \int_\Omega q \mbox{ or } \int_\Omega c q

    :Arguments:
        - material : :math:`c` (optional)
        - virtual  : :math:`q`
    """
    name = 'dw_volume_integrate'
    arg_types = ('opt_material', 'virtual')
    arg_shapes = [{'opt_material' : '1, 1', 'virtual' : (1, None)},
                  {'opt_material' : None}]

    @staticmethod
    def function(out, material, bf, geo):
        bf_t = nm.tile(bf.transpose((0, 1, 3, 2)), (out.shape[0], 1, 1, 1))
        bf_t = nm.ascontiguousarray(bf_t)
        if material is not None:
            status = geo.integrate(out, material * bf_t)
        else:
            status = geo.integrate(out, bf_t)
        return status

    def get_fargs(self, material, virtual,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        assert_(virtual.n_components == 1)
        geo, _ = self.get_mapping(virtual)

        return material, geo.bf, geo

class IntegrateSurfaceOperatorTerm(IntegrateVolumeOperatorTerm):
    r"""
    Surface integral of a test function weighted by a scalar function
    :math:`c`.

    :Definition:

    .. math::
        \int_{\Gamma} q \mbox{ or } \int_\Gamma c q

    :Arguments:
        - material : :math:`c` (optional)
        - virtual  : :math:`q`
    """
    name = 'dw_surface_integrate'
    arg_types = ('opt_material', 'virtual')
    arg_shapes = [{'opt_material' : '1, 1', 'virtual' : (1, None)},
                  {'opt_material' : None}]
    integration = 'surface'

class VolumeTerm(Term):
    r"""
    Volume of a domain. Uses approximation of the parameter variable.

    :Definition:

    .. math::
        \int_\Omega 1

    :Arguments:
        - parameter : any variable
    """
    name = 'd_volume'
    arg_types = ('parameter',)
    arg_shapes = {'parameter' : 1}

    @staticmethod
    def function(out, geo):
        out[:] = geo.volume

        return 0

    def get_fargs(self, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        geo, _ = self.get_mapping(parameter)

        return geo,

    def get_eval_shape(self, parameter,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_cell, n_qp, dim, n_n, n_c = self.get_data_shape(parameter)

        return (n_cell, 1, 1, 1), parameter.dtype

class SurfaceTerm(VolumeTerm):
    r"""
    Surface of a domain. Uses approximation of the parameter variable.

    :Definition:

    .. math::
        \int_\Gamma 1

    :Arguments:
        - parameter : any variable
    """
    name = 'd_surface'
    arg_types = ('parameter',)
    arg_shapes = {'parameter' : 1}
    integration = 'surface'

class VolumeSurfaceTerm(Term):
    r"""
    Volume of a :math:`D`-dimensional domain, using a surface integral. Uses
    approximation of the parameter variable.

    :Definition:

    .. math::
        1 / D \int_\Gamma \ul{x} \cdot \ul{n}

    :Arguments:
        - parameter : any variable
    """
    name = 'd_volume_surface'
    arg_types = ('parameter',)
    arg_shapes = {'parameter' : 1}
    integration = 'surface'

    function = staticmethod(terms.d_volume_surface)

    def get_fargs(self, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        ap, sg = self.get_approximation(parameter)

        sd = ap.surface_data[self.region.name]
        coor = parameter.field.get_coor()

        return coor, sg, sd.econn.copy()

    def get_eval_shape(self, parameter,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_fa, n_qp, dim, n_fn, n_c = self.get_data_shape(parameter)

        return (n_fa, 1, 1, 1), parameter.dtype

class SurfaceMomentTerm(Term):
    r"""
    Surface integral of the outer product of the unit outward normal
    :math:`\ul{n}` and the coordinate :math:`\ul{x}` shifted by :math:`\ul{x}_0`

    :Definition:

    .. math::
        \int_{\Gamma} \ul{n} (\ul{x} - \ul{x}_0)

    :Arguments:
        - parameter : any variable
        - shift     : :math:`\ul{x}_0`
    """
    name = 'di_surface_moment'
    arg_types = ('parameter', 'shift')
    integration = 'surface'

    function = staticmethod(terms.di_surface_moment)

    def get_fargs(self, parameter, shift,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        ap, sg = self.get_approximation(parameter)

        sd = ap.surface_data[self.region.name]
        coor = parameter.field.get_coor() \
               - nm.asarray(shift, dtype=nm.float64)[None,:]

        return coor, sg, sd.econn.copy()

    def get_eval_shape(self, parameter, shift,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_fa, n_qp, dim, n_fn, n_c = self.get_data_shape(parameter)

        return (n_fa, 1, dim, dim), parameter.dtype

class IntegrateMatTerm(Term):
    r"""
    Evaluate material parameter :math:`m` in a volume/surface region.

    Depending on evaluation mode, integrate a material parameter over a
    volume/surface region ('eval'), average it in elements/faces ('el_avg') or
    interpolate it into volume/surface quadrature points ('qp').

    Uses reference mapping of :math:`y` variable.

    Supports 'eval', 'el_avg' and 'qp' evaluation modes.

    :Definition:

    .. math::
        \int_\Omega m

    .. math::
        \mbox{vector for } K \from \Ical_h: \int_{T_K} m / \int_{T_K} 1

    .. math::
        m|_{qp}

    :Arguments:
        - material  : :math:`m` (can have up to two dimensions)
        - parameter : :math:`y`
    """
    name = 'ev_integrate_mat'
    arg_types = ('material', 'parameter')
    arg_shapes = [{'material' : '1, 1', 'parameter' : 1},
                  {'material' : 'D, D'},
                  {'material' : 'S, S'},
                  {'material' : 'D, S'}]

    @staticmethod
    def function(out, mat, geo, fmode):
        if fmode == 2:
            out[:] = mat
            status = 0

        else:
            status = geo.integrate(out, mat, fmode)

        return status

    def get_fargs(self, mat, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        geo, _ = self.get_mapping(parameter)

        fmode = {'eval' : 0, 'el_avg' : 1, 'qp' : 2}.get(mode, 1)

        return mat, geo, fmode

    def get_eval_shape(self, mat, parameter,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(parameter)
        n_row, n_col = mat.shape[-2:]

        if mode != 'qp':
            n_qp = 1

        return (n_el, n_qp, n_row, n_col), mat.dtype

class SumNodalValuesTerm(Term):
    r"""
    Sum nodal values.

    :Arguments:
        - parameter : :math:`p` or :math:`\ul{u}`
    """
    name = 'd_sum_vals'
    arg_types = ('parameter',)
    arg_shapes = [{'parameter' : 1}, {'parameter' : 'D'}]

    @staticmethod
    def function(out, vec):
        out[:] = nm.sum(vec, 0)

        return 0

    def get_fargs(self, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vec = parameter.get_state_in_region(self.region)

        return vec,

    def get_eval_shape(self, parameter,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(parameter)

        return (n_el, n_c), parameter.dtype

########NEW FILE########
__FILENAME__ = terms_biot
import numpy as nm

from sfepy.linalg import dot_sequences
from sfepy.terms.terms import Term, terms
from sfepy.terms.terms_th import THTerm, ETHTerm
from sfepy.terms.terms_elastic import CauchyStressTerm

class BiotTerm(Term):
    r"""
    Biot coupling term with :math:`\alpha_{ij}`
    given in vector form exploiting symmetry: in 3D it has the
    indices ordered as :math:`[11, 22, 33, 12, 13, 23]`, in 2D it has
    the indices ordered as :math:`[11, 22, 12]`. Corresponds to weak
    forms of Biot gradient and divergence terms. Can be evaluated. Can
    use derivatives.

    :Definition:

    .. math::
        \int_{\Omega}  p\ \alpha_{ij} e_{ij}(\ul{v}) \mbox{ , } \int_{\Omega}
        q\ \alpha_{ij} e_{ij}(\ul{u})

    :Arguments 1:
        - material : :math:`\alpha_{ij}`
        - virtual  : :math:`\ul{v}`
        - state    : :math:`p`

    :Arguments 2:
        - material : :math:`\alpha_{ij}`
        - state    : :math:`\ul{u}`
        - virtual  : :math:`q`

    :Arguments 3:
        - material    : :math:`\alpha_{ij}`
        - parameter_v : :math:`\ul{u}`
        - parameter_s : :math:`p`
    """
    name = 'dw_biot'
    arg_types = (('material', 'virtual', 'state'),
                 ('material', 'state', 'virtual'),
                 ('material', 'parameter_v', 'parameter_s'))
    arg_shapes = {'material' : 'S, 1',
                  'virtual/grad' : ('D', None), 'state/grad' : 1,
                  'virtual/div' : (1, None), 'state/div' : 'D',
                  'parameter_v' : 'D', 'parameter_s' : 1}

    modes = ('grad', 'div', 'eval')

    def get_fargs(self, mat, vvar, svar,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        if self.mode == 'grad':
            qp_var, qp_name = svar, 'val'

        else:
            qp_var, qp_name = vvar, 'cauchy_strain'

        if mode == 'weak':
            vvg, _ = self.get_mapping(vvar)
            svg, _ = self.get_mapping(svar)

            if diff_var is None:
                val_qp = self.get(qp_var, qp_name)
                fmode = 0

            else:
                val_qp = nm.array([0], ndmin=4, dtype=nm.float64)
                fmode = 1

            return 1.0, val_qp, mat, svg, vvg, fmode

        elif mode == 'eval':
            vvg, _ = self.get_mapping(vvar)

            strain = self.get(vvar, 'cauchy_strain')
            pval = self.get(svar, 'val')

            return 1.0, pval, strain, mat, vvg

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

    def get_eval_shape(self, mat, vvar, svar,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(vvar)

        return (n_el, 1, 1, 1), vvar.dtype

    def set_arg_types(self):
        self.function = {
            'grad' : terms.dw_biot_grad,
            'div' : terms.dw_biot_div,
            'eval' : terms.d_biot_div,
        }[self.mode]

class BiotStressTerm(CauchyStressTerm):
    r"""
    Evaluate Biot stress tensor.

    It is given in the usual vector form exploiting symmetry: in 3D it has 6
    components with the indices ordered as :math:`[11, 22, 33, 12, 13, 23]`, in
    2D it has 3 components with the indices ordered as :math:`[11, 22, 12]`.

    Supports 'eval', 'el_avg' and 'qp' evaluation modes.

    :Definition:

    .. math::
        - \int_{\Omega} \alpha_{ij} \bar{p}

    .. math::
        \mbox{vector for } K \from \Ical_h:
        - \int_{T_K} \alpha_{ij} \bar{p} / \int_{T_K} 1

    .. math::
        - \alpha_{ij} \bar{p}|_{qp}

    :Arguments:
        - material  : :math:`\alpha_{ij}`
        - parameter : :math:`\bar{p}`
    """
    name = 'ev_biot_stress'
    arg_types = ('material', 'parameter')
    arg_shapes = {'material' : 'S, 1', 'parameter' : 1}

    @staticmethod
    def function(out, val_qp, mat, vg, fmode):
        if fmode == 2:
            out[:] = dot_sequences(mat, val_qp)
            status = 0

        else:
            status = terms.de_cauchy_stress(out, val_qp, mat, vg, fmode)

        out *= -1.0

        return status

    def get_fargs(self, mat, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(parameter)

        val_qp = self.get(parameter, 'val')

        fmode = {'eval' : 0, 'el_avg' : 1, 'qp' : 2}.get(mode, 1)

        return val_qp, mat, vg, fmode

class BiotTHTerm(BiotTerm, THTerm):
    r"""
    Fading memory Biot term. Can use derivatives.

    :Definition:

    .. math::
        \begin{array}{l}
        \int_{\Omega} \left [\int_0^t \alpha_{ij}(t-\tau)\,p(\tau)) \difd{\tau}
        \right]\,e_{ij}(\ul{v}) \mbox{ ,} \\
        \int_{\Omega} \left [\int_0^t
        \alpha_{ij}(t-\tau) e_{kl}(\ul{u}(\tau)) \difd{\tau} \right] q
        \end{array}

    :Arguments 1:
        - ts       : :class:`TimeStepper` instance
        - material : :math:`\alpha_{ij}(\tau)`
        - virtual  : :math:`\ul{v}`
        - state    : :math:`p`

    :Arguments 2:
        - ts       : :class:`TimeStepper` instance
        - material : :math:`\alpha_{ij}(\tau)`
        - state    : :math:`\ul{u}`
        - virtual  : :math:`q`
    """
    name = 'dw_biot_th'
    arg_types = (('ts', 'material', 'virtual', 'state'),
                 ('ts', 'material', 'state', 'virtual'))
    arg_shapes = {}
    modes = ('grad', 'div')

    def get_fargs(self, ts, mats, vvar, svar,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        if self.mode == 'grad':
            qp_var, qp_name = svar, 'val'

        else:
            qp_var, qp_name = vvar, 'cauchy_strain'

        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(svar)

        if mode == 'weak':
            vvg, _ = self.get_mapping(vvar)
            svg, _ = self.get_mapping(svar)

            if diff_var is None:
                def iter_kernel():
                    for ii, mat in enumerate(mats):
                        val_qp = self.get(qp_var, qp_name, step=-ii)
                        mat = nm.tile(mat, (n_el, n_qp, 1, 1))
                        yield ii, (ts.dt, val_qp, mat, svg, vvg, 0)
                fargs = iter_kernel

            else:
                val_qp = nm.array([0], ndmin=4, dtype=nm.float64)
                mat = nm.tile(mats[0], (n_el, n_qp, 1, 1))
                fargs = ts.dt, val_qp, mat, svg, vvg, 1

            return fargs

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

class BiotETHTerm(BiotTerm, ETHTerm):
    r"""
    This term has the same definition as dw_biot_th, but assumes an
    exponential approximation of the convolution kernel resulting in much
    higher efficiency. Can use derivatives.

    :Definition:

    .. math::
        \begin{array}{l}
        \int_{\Omega} \left [\int_0^t \alpha_{ij}(t-\tau)\,p(\tau)) \difd{\tau}
        \right]\,e_{ij}(\ul{v}) \mbox{ ,} \\
        \int_{\Omega} \left [\int_0^t
        \alpha_{ij}(t-\tau) e_{kl}(\ul{u}(\tau)) \difd{\tau} \right] q
        \end{array}

    :Arguments 1:
        - ts         : :class:`TimeStepper` instance
        - material_0 : :math:`\alpha_{ij}(0)`
        - material_1 : :math:`\exp(-\lambda \Delta t)` (decay at :math:`t_1`)
        - virtual    : :math:`\ul{v}`
        - state      : :math:`p`

    :Arguments 2:
        - ts         : :class:`TimeStepper` instance
        - material_0 : :math:`\alpha_{ij}(0)`
        - material_1 : :math:`\exp(-\lambda \Delta t)` (decay at :math:`t_1`)
        - state      : :math:`\ul{u}`
        - virtual    : :math:`q`
    """
    name = 'dw_biot_eth'
    arg_types = (('ts', 'material_0', 'material_1', 'virtual', 'state'),
                 ('ts', 'material_0', 'material_1', 'state', 'virtual'))
    arg_shapes = {}
    modes = ('grad', 'div')

    def get_fargs(self, ts, mat0, mat1, vvar, svar,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        if self.mode == 'grad':
            qp_var, qp_name, iv = svar, 'val', 4

        else:
            qp_var, qp_name, iv = vvar, 'cauchy_strain', 3

        if mode == 'weak':
            vvg, _, key = self.get_mapping(vvar, return_key=True)
            svg, _ = self.get_mapping(svar)

            if diff_var is None:
                val_qp = self.get(qp_var, qp_name)

                key += tuple(self.arg_names[ii] for ii in [1, 2, iv])
                data = self.get_eth_data(key, qp_var, mat1, val_qp)

                val = data.history + data.values
                fargs = (ts.dt, val, mat0, svg, vvg, 0)

            else:
                val_qp = nm.array([0], ndmin=4, dtype=nm.float64)
                fargs = (ts.dt, val_qp, mat0, svg, vvg, 1)

            return fargs

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

########NEW FILE########
__FILENAME__ = terms_constraints
import numpy as nm

from sfepy.terms.terms import Term
from sfepy.linalg import dot_sequences

class NonPenetrationTerm(Term):
    r"""
    Non-penetration condition in the weak sense.

    :Definition:

    .. math::
        \int_{\Gamma} c \lambda \ul{n} \cdot \ul{v} \mbox{ , }
        \int_{\Gamma} c \hat\lambda \ul{n} \cdot \ul{u} \\
        \int_{\Gamma} \lambda \ul{n} \cdot \ul{v} \mbox{ , }
        \int_{\Gamma} \hat\lambda \ul{n} \cdot \ul{u}

    :Arguments 1:
        - material : :math:`c` (optional)
        - virtual  : :math:`\ul{v}`
        - state    : :math:`\lambda`

    :Arguments 2:
        - material : :math:`c` (optional)
        - state    : :math:`\ul{u}`
        - virtual  : :math:`\hat\lambda`
    """
    name = 'dw_non_penetration'
    arg_types = (('opt_material', 'virtual', 'state'),
                 ('opt_material', 'state', 'virtual'))
    arg_shapes = [{'opt_material' : '1, 1',
                   'virtual/grad' : ('D', None), 'state/grad' : 1,
                   'virtual/div' : (1, None), 'state/div' : 'D'},
                  {'opt_material' : None}]
    modes = ('grad', 'div')
    integration = 'surface'

    @staticmethod
    def function(out, val_qp, ebf, bf, mat, sg, diff_var, mode):
        """
        `ebf` belongs to vector variable, `bf` to scalar variable.
        """
        normals = sg.normal
        n_fa = out.shape[0]

        if diff_var is None:
            if mode == 'grad':
                ebf_t = nm.tile(ebf.transpose((0, 1, 3, 2)), (n_fa, 1, 1, 1))

                nl = normals * val_qp
                eftnl = mat * dot_sequences(ebf_t, nl)
                status = sg.integrate(out, eftnl, 0)

            else:
                bf_t = nm.tile(bf.transpose((0, 1, 3, 2)), (n_fa, 1, 1, 1))

                ntu = (normals * val_qp).sum(axis=-2)[...,None]
                ftntu = mat * (bf_t * ntu)

                status = sg.integrate(out, ftntu, 0)

        else:
            ebf_t = nm.tile(ebf.transpose((0, 1, 3, 2)), (n_fa, 1, 1, 1))
            bf_ = nm.tile(bf, (n_fa, 1, 1, 1))

            eftn = mat * dot_sequences(ebf_t, normals)
            eftnf = eftn * bf_

            if mode == 'grad':
                status = sg.integrate(out, eftnf, 0)

            else:
                ftntef = nm.ascontiguousarray(eftnf.transpose((0, 1, 3, 2)))
                status = sg.integrate(out, ftntef, 0)

        return status

    def get_fargs(self, mat, vvar, svar,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        if self.mode == 'grad':
            qp_var = svar

        else:
            qp_var = vvar

        val_qp = self.get(qp_var, 'val')

        vsg, _ = self.get_mapping(vvar)
        ssg, _ = self.get_mapping(svar)
        n_fa, n_qp, dim, n_fn, n_c = self.get_data_shape(vvar)

        if mat is None:
            mat = nm.ones((1, n_qp, 1, 1), dtype=nm.float64)

        # Expand base corresponding to \ul{u} for all dofs.
        bf = vsg.bf
        ebf = nm.zeros(bf.shape[:2] + (dim, n_fn * dim), dtype=nm.float64)
        for ir in xrange(dim):
            ebf[..., ir, ir*n_fn:(ir+1)*n_fn] = bf[..., 0, :]

        return val_qp, ebf, ssg.bf, mat, vsg, diff_var, self.mode

########NEW FILE########
__FILENAME__ = terms_diffusion
import numpy as nm

from sfepy.base.base import assert_
from sfepy.linalg import dot_sequences
from sfepy.terms.terms import Term, terms

class DiffusionTerm(Term):
    r"""
    General diffusion term with permeability :math:`K_{ij}`. Can be
    evaluated. Can use derivatives.

    :Definition:

    .. math::
        \int_{\Omega} K_{ij} \nabla_i q \nabla_j p \mbox{ , } \int_{\Omega}
        K_{ij} \nabla_i \bar{p} \nabla_j r

    :Arguments 1:
        - material : :math:`K_{ij}`
        - virtual  : :math:`q`
        - state    : :math:`p`

    :Arguments 2:
        - material    : :math:`K_{ij}`
        - parameter_1 : :math:`\bar{p}`
        - parameter_2 : :math:`r`
    """
    name = 'dw_diffusion'
    arg_types = (('material', 'virtual', 'state'),
                 ('material', 'parameter_1', 'parameter_2'))
    arg_shapes = {'material' : 'D, D', 'virtual' : (1, 'state'),
                  'state' : 1, 'parameter_1' : 1, 'parameter_2' : 1}
    modes = ('weak', 'eval')
    symbolic = {'expression': 'div( K * grad( u ) )',
                'map' : {'u' : 'state', 'K' : 'material'}}

    def get_fargs(self, mat, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(state)

        if mat is None:
            if self.name in ('dw_laplace', 'dw_st_pspg_p'):
                n_el, n_qp, _, _, _ = self.get_data_shape(state)
                mat = nm.ones((1, n_qp, 1, 1), dtype=nm.float64)

        if mode == 'weak':
            if diff_var is None:
                grad = self.get(state, 'grad')
                fmode = 0

            else:
                grad = nm.array([0], ndmin=4, dtype=nm.float64)
                fmode = 1

            return grad, mat, vg, fmode

        elif mode == 'eval':
            grad1 = self.get(virtual, 'grad')
            grad2 = self.get(state, 'grad')

            return grad1, grad2, mat, vg

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

    def get_eval_shape(self, mat, virtual, state,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)

        return (n_el, 1, 1, 1), state.dtype

    def set_arg_types(self):
        if self.mode == 'weak':
            self.function = terms.dw_diffusion

        else:
            self.function = terms.d_diffusion

class LaplaceTerm(DiffusionTerm):
    r"""
    Laplace term with :math:`c` coefficient. Can be
    evaluated. Can use derivatives.

    :Definition:

    .. math::
        \int_{\Omega} c \nabla q \cdot \nabla p \mbox{ , } \int_{\Omega}
        c \nabla \bar{p} \cdot \nabla r

    :Arguments 1:
        - material : :math:`c`
        - virtual  : :math:`q`
        - state    : :math:`p`

    :Arguments 2:
        - material    : :math:`c`
        - parameter_1 : :math:`\bar{p}`
        - parameter_2 : :math:`r`
    """
    name = 'dw_laplace'
    arg_types = (('opt_material', 'virtual', 'state'),
                 ('opt_material', 'parameter_1', 'parameter_2'))
    arg_shapes = [{'opt_material' : '1, 1', 'virtual' : (1, 'state'),
                   'state' : 1, 'parameter_1' : 1, 'parameter_2' : 1},
                  {'opt_material' : None}]
    modes = ('weak', 'eval')
    symbolic = {'expression': 'c * div( grad( u ) )',
                'map' : {'u' : 'state', 'c' : 'opt_material'}}

    def set_arg_types(self):
        if self.mode == 'weak':
            self.function = terms.dw_laplace

        else:
            self.function = terms.d_laplace

class DiffusionRTerm(Term):
    r"""
    Diffusion-like term with material parameter :math:`K_{j}` (to
    use on the right-hand side).

    :Definition:

    .. math::
        \int_{\Omega} K_{j} \nabla_j q

    :Arguments:
        - material : :math:`K_j`
        - virtual  : :math:`q`
    """
    name = 'dw_diffusion_r'
    arg_types = ('material', 'virtual')
    arg_shapes = {'material' : 'D, 1', 'virtual' : (1, None)}
    function = staticmethod(terms.dw_diffusion_r)

    def get_fargs(self, mat, virtual,
                  mode=None, term_mode=None, diff_var=None, **kwargs):

        vg, _ = self.get_mapping(virtual)
        return mat, vg

class DiffusionCoupling(Term):
    r"""
    Diffusion copupling term with material parameter :math:`K_{j}`.

    :Definition:

    .. math::
        \int_{\Omega}  p K_{j} \nabla_j q

    :Arguments:
        - material : :math:`K_{j}`
        - virtual  : :math:`q`
        - state    : :math:`p`
    """
    name = 'dw_diffusion_coupling'
    arg_types = (('material', 'virtual', 'state'),
                 ('material', 'state', 'virtual'),
                 ('material', 'parameter_1', 'parameter_2'))
    arg_shapes = {'material' : 'D, 1', 'virtual' : (1, 'state'),
                  'state' : 1, 'parameter_1' : 1, 'parameter_2' : 1}
    modes = ('weak0', 'weak1', 'eval')

    @staticmethod
    def d_fun(out, mat, val, grad, vg):
        out_qp = val * dot_sequences(mat, grad, 'ATB')

        status = vg.integrate(out, out_qp)

        return status

    @staticmethod
    def dw_fun(out, val, mat, bf, vg, fmode):

        if fmode == 0:
            status = terms.mulATB_integrate(out, vg.bfg, mat * val, vg)

        elif fmode == 1:
            status = terms.mulATB_integrate(out, bf * mat, val, vg)

        elif fmode == 2:
            status = terms.mulATB_integrate(out, vg.bfg, mat * bf, vg)

        elif fmode == 3:
            status = terms.mulATB_integrate(out, mat * bf, vg.bfg, vg)

        return status

    def get_fargs( self, mat, virtual, state,
                   mode=None, term_mode=None, diff_var=None, **kwargs):
        ap, vg = self.get_approximation(virtual)

        if mode == 'weak':

            aps, vgs = self.get_approximation(state)
            bf = aps.get_base('v', 0, self.integral)

            if diff_var is None:
                if self.mode == 'weak0':
                    val = self.get(state, 'val')
                    fmode = 0

                else:
                    val = self.get(virtual, 'grad')
                    fmode = 1

            else:
                val = nm.array([0], ndmin=4, dtype=nm.float64)
                if self.mode == 'weak0':
                    fmode = 2

                else:
                    fmode = 3

            return val, mat, bf, vg, fmode

        elif mode == 'eval':

            grad = self.get(virtual, 'grad')
            val = self.get(state, 'val')

            return mat, val, grad, vg

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

    def get_eval_shape(self, mat, virtual, state,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)

        return (n_el, 1, 1, 1), state.dtype

    def set_arg_types( self ):
        if self.mode[:-1] == 'weak':
            self.function = self.dw_fun

        else:
            self.function = self.d_fun

class DiffusionVelocityTerm( Term ):
    r"""
    Evaluate diffusion velocity.

    Supports 'eval', 'el_avg' and 'qp' evaluation modes.

    :Definition:

    .. math::
        - \int_{\Omega} K_{ij} \nabla_j \bar{p}

    .. math::
        \mbox{vector for } K \from \Ical_h: - \int_{T_K} K_{ij} \nabla_j \bar{p}
        / \int_{T_K} 1

    .. math::
        - K_{ij} \nabla_j \bar{p}

    :Arguments:
        - material  : :math:`K_{ij}`
        - parameter : :math:`\bar{p}`
    """
    name = 'ev_diffusion_velocity'
    arg_types = ('material', 'parameter')
    arg_shapes = {'material' : 'D, D', 'parameter' : 1}

    @staticmethod
    def function(out, grad, mat, vg, fmode):
        dvel = dot_sequences(mat, grad)

        if fmode == 2:
            out[:] = dvel
            status = 0

        else:
            status = vg.integrate(out, dvel, fmode)

        out *= -1.0

        return status

    def get_fargs(self, mat, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(parameter)
        grad = self.get(parameter, 'grad')
        fmode = {'eval' : 0, 'el_avg' : 1, 'qp' : 2}.get(mode, 1)

        return grad, mat, vg, fmode

    def get_eval_shape(self, mat, parameter,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(parameter)

        if mode != 'qp':
            n_qp = 1

        return (n_el, n_qp, dim, 1), parameter.dtype

class SurfaceFluxTerm(Term):
    r"""
    Surface flux term.

    Supports 'eval', 'el_avg' and 'el' evaluation modes.

    :Definition:

    .. math::
        \int_{\Gamma} \ul{n} \cdot K_{ij} \nabla_j \bar{p}

    .. math::
        \mbox{vector for } K \from \Ical_h: \int_{T_K} \ul{n}
        \cdot K_{ij} \nabla_j \bar{p}\ / \int_{T_K} 1

    .. math::
        \mbox{vector for } K \from \Ical_h: \int_{T_K} \ul{n}
        \cdot K_{ij} \nabla_j \bar{p}

    :Arguments:
        - material: :math:`\ul{K}`
        - parameter:  :math:`\bar{p}`,
    """
    name = 'd_surface_flux'
    arg_types = ('material', 'parameter')
    arg_shapes = {'material' : 'D, D', 'parameter' : 1}
    integration = 'surface_extra'

    function = staticmethod(terms.d_surface_flux)

    def get_fargs(self, mat, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        sg, _ = self.get_mapping(parameter)

        grad = self.get(parameter, 'grad')

        fmode = {'eval' : 0, 'el_avg' : 1, 'el' : 0}.get(mode, 0)

        return grad, mat, sg, fmode

    def get_eval_shape(self, mat, parameter,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_fa, n_qp, dim, n_en, n_c = self.get_data_shape(parameter)

        return (n_fa, 1, 1, 1), parameter.dtype

class SurfaceFluxOperatorTerm(Term):
    r"""
    Surface flux operator term.

    :Definition:

    .. math::
        \int_{\Gamma} q \ul{n} \cdot \ull{K} \cdot \nabla p

    :Arguments:
        - material : :math:`\ull{K}`
        - virtual  : :math:`q`
        - state    : :math:`p`
    """
    name = 'dw_surface_flux'
    arg_types = ('opt_material', 'virtual', 'state')
    arg_shapes = [{'opt_material' : 'D, D', 'virtual' : (1, 'state'),
                   'state' : 1},
                  {'opt_material' : None}]
    integration = 'surface_extra'
    function = terms.dw_surface_flux

    def get_fargs(self, mat, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        ap, sg = self.get_approximation(state)
        sd = ap.surface_data[self.region.name]
        bf = ap.get_base(sd.bkey, 0, self.integral)

        if mat is None:
            _, n_qp, dim, _, _ = self.get_data_shape(state)
            mat = nm.empty((1, n_qp, dim, dim), dtype=nm.float64)
            mat[..., :, :] = nm.eye(dim, dtype=nm.float64)

        if diff_var is None:
            grad = self.get(state, 'grad', integration='surface_extra')
            fmode = 0

        else:
            grad = nm.array([0], ndmin=4, dtype=nm.float64)
            fmode = 1

        return grad, mat,  bf, sg, sd.fis, fmode

class ConvectVGradSTerm(Term):
    r"""
    Scalar gradient term with convective velocity.

    :Definition:

    .. math::
        \int_{\Omega} q (\ul{u} \cdot \nabla p)

    :Arguments:
        - virtual  : :math:`q`
        - state_v  : :math:`\ul{u}`
        - state_s  : :math:`p`
    """
    name = 'dw_convect_v_grad_s'
    arg_types = ('virtual', 'state_v', 'state_s')
    arg_shapes = [{'virtual' : (1, 'state_s'), 'state_v' : 'D', 'state_s' : 1}]
    function = terms.dw_convect_v_grad_s

    def get_fargs(self, virtual, state_v, state_s,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vvg, _ = self.get_mapping(state_v)
        svg, _ = self.get_mapping(state_s)

        if diff_var is None:
            grad_s = self.get(state_s, 'grad')
            val_v = self.get(state_v, 'val')
            fmode = 0

        elif diff_var == state_s.name:
            grad_s = nm.array([0], ndmin=4, dtype=nm.float64)
            val_v = self.get(state_v, 'val')
            fmode = 1

        else:
            assert_(diff_var == state_v.name)
            grad_s = self.get(state_s, 'grad')
            val_v = nm.array([0], ndmin=4, dtype=nm.float64)
            fmode = 2

        return val_v, grad_s, vvg, svg, fmode

########NEW FILE########
__FILENAME__ = terms_dot
import numpy as nm

from sfepy.base.base import assert_
from sfepy.linalg import dot_sequences
from sfepy.terms.terms import Term, terms
from sfepy.terms.terms_th import THTerm, ETHTerm

class DotProductVolumeTerm(Term):
    r"""
    Volume :math:`L^2(\Omega)` weighted dot product for both scalar and vector
    fields. Can be evaluated. Can use derivatives.

    :Definition:

    .. math::
        \int_\Omega q p \mbox{ , } \int_\Omega \ul{v} \cdot \ul{u}
        \mbox{ , }
        \int_\Omega p r \mbox{ , } \int_\Omega \ul{u} \cdot \ul{w} \\
        \int_\Omega c q p \mbox{ , } \int_\Omega c \ul{v} \cdot \ul{u}
        \mbox{ , }
        \int_\Omega c p r \mbox{ , } \int_\Omega c \ul{u} \cdot \ul{w} \\
        \int_\Omega \ul{v} \cdot \ull{M} \cdot \ul{u}
        \mbox{ , }
        \int_\Omega \ul{u} \cdot \ull{M} \cdot \ul{w}

    :Arguments 1:
        - material : :math:`c` or :math:`\ull{M}` (optional)
        - virtual  : :math:`q` or :math:`\ul{v}`
        - state    : :math:`p` or :math:`\ul{u}`

    :Arguments 2:
        - material    : :math:`c` or :math:`\ull{M}` (optional)
        - parameter_1 : :math:`p` or :math:`\ul{u}`
        - parameter_2 : :math:`r` or :math:`\ul{w}`
    """
    name = 'dw_volume_dot'
    arg_types = (('opt_material', 'virtual', 'state'),
                 ('opt_material', 'parameter_1', 'parameter_2'))
    arg_shapes = [{'opt_material' : '1, 1', 'virtual' : (1, 'state'),
                   'state' : 1, 'parameter_1' : 1, 'parameter_2' : 1},
                  {'opt_material' : None},
                  {'opt_material' : '1, 1', 'virtual' : ('D', 'state'),
                   'state' : 'D', 'parameter_1' : 'D', 'parameter_2' : 'D'},
                  {'opt_material' : 'D, D'},
                  {'opt_material' : None}]
    modes = ('weak', 'eval')

    @staticmethod
    def dw_dot(out, mat, val_qp, vgeo, sgeo, fun, fmode):
        status = fun(out, mat, val_qp, vgeo, sgeo, fmode)
        return status

    @staticmethod
    def d_dot(out, mat, val1_qp, val2_qp, geo):
        if mat is None:
            if val1_qp.shape[2] > 1:
                if val2_qp.shape[2] == 1:
                    aux = dot_sequences(val1_qp, geo.normal, mode='ATB')
                    vec = dot_sequences(aux, val2_qp, mode='AB')

                else:
                    vec = dot_sequences(val1_qp, val2_qp, mode='ATB')

            else:
                vec = val1_qp * val2_qp

        elif mat.shape[-1] == 1:
            if val1_qp.shape[2] > 1:
                vec = mat * dot_sequences(val1_qp, val2_qp, mode='ATB')

            else:
                vec = mat * val1_qp * val2_qp

        else:
            aux = dot_sequences(mat, val2_qp, mode='AB')
            vec = dot_sequences(val1_qp, aux, mode='ATB')

        status = geo.integrate(out, vec)

        return status

    def check_shapes(self, mat, virtual, state):
        is_vector_scalar = ((virtual.n_components == 1)
            and (state.n_components == state.dim))\
            or ((virtual.n_components == virtual.dim)
            and (state.n_components == 1))

        assert_((virtual.n_components == state.n_components)
                or is_vector_scalar)

        if mat is not None:
            n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)
            assert_((mat.shape[1:] == (n_qp, 1, 1))
                    or ((mat.shape[1:] == (n_qp, dim, dim)) and (n_c == dim)))
            assert_((mat.shape[0] == 1) or (mat.shape[0] == n_el))

    def get_fargs(self, mat, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vgeo, _ = self.get_mapping(virtual)

        if mode == 'weak':
            if mat is None:
                n_cell, n_qp, dim, n_n, n_c = self.get_data_shape(state)
                mat = nm.ones((n_cell, n_qp, 1, 1), dtype=nm.float64)

            sgeo, _ = self.get_mapping(state)

            if diff_var is None:
                val_qp = self.get(state, 'val')
                fmode = 0

            else:
                val_qp = nm.array([0], ndmin=4, dtype=nm.float64)
                fmode = 1

            if state.n_components > 1:
                if ((self.integration == 'volume')
                    or (virtual.n_components > 1)):
                    fun = terms.dw_volume_dot_vector

                else:
                    fun = terms.dw_surface_s_v_dot_n

            else:
                if ((self.integration == 'volume')
                    or (virtual.n_components == 1)):
                    fun = terms.dw_volume_dot_scalar

                else:
                    fun = terms.dw_surface_v_dot_n_s

            return mat, val_qp, vgeo, sgeo, fun, fmode

        elif mode == 'eval':
            val1_qp = self.get(virtual, 'val')
            val2_qp = self.get(state, 'val')

            return mat, val1_qp, val2_qp, vgeo

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

    def get_eval_shape(self, mat, virtual, state,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_cell, n_qp, dim, n_n, n_c = self.get_data_shape(state)

        return (n_cell, 1, 1, 1), state.dtype

    def set_arg_types(self):
        if self.mode == 'weak':
            self.function = self.dw_dot

        else:
            self.function = self.d_dot

class DotProductSurfaceTerm(DotProductVolumeTerm):
    r"""
    Surface :math:`L^2(\Gamma)` dot product for both scalar and vector
    fields.

    :Definition:

    .. math::
        \int_\Gamma q p \mbox{ , } \int_\Gamma \ul{v} \cdot \ul{u}
        \mbox{ , }
        \int_\Gamma \ul{v} \cdot \ul{n} p \mbox{ , }
        \int_\Gamma q \ul{n} \cdot \ul{u} \mbox{ , }
        \int_\Gamma p r \mbox{ , } \int_\Gamma \ul{u} \cdot \ul{w}
        \mbox{ , } \int_\Gamma \ul{w} \cdot \ul{n} p \\
        \int_\Gamma c q p \mbox{ , } \int_\Gamma c \ul{v} \cdot \ul{u}
        \mbox{ , }
        \int_\Gamma c p r \mbox{ , } \int_\Gamma c \ul{u} \cdot \ul{w} \\
        \int_\Gamma \ul{v} \cdot \ull{M} \cdot \ul{u}
        \mbox{ , }
        \int_\Gamma \ul{u} \cdot \ull{M} \cdot \ul{w}

    :Arguments 1:
        - material : :math:`c` or :math:`\ull{M}` (optional)
        - virtual  : :math:`q` or :math:`\ul{v}`
        - state    : :math:`p` or :math:`\ul{u}`

    :Arguments 2:
        - material    : :math:`c` or :math:`\ull{M}` (optional)
        - parameter_1 : :math:`p` or :math:`\ul{u}`
        - parameter_2 : :math:`r` or :math:`\ul{w}`
    """
    name = 'dw_surface_dot'
    arg_types = (('opt_material', 'virtual', 'state'),
                 ('opt_material', 'parameter_1', 'parameter_2'))
    modes = ('weak', 'eval')
    integration = 'surface'

class BCNewtonTerm(DotProductSurfaceTerm):
    r"""
    Newton boundary condition term.

    :Definition:

    .. math::
        \int_{\Gamma} \alpha q (p - p_{\rm outer})

    :Arguments:
        - material_1 : :math:`\alpha`
        - material_2 : :math:`p_{\rm outer}`
        - virtual    : :math:`q`
        - state      : :math:`p`
    """
    name = 'dw_bc_newton'
    arg_types = ('material_1', 'material_2', 'virtual', 'state')
    arg_shapes = {'material_1' : '1, 1', 'material_2' : '1, 1',
                  'virtual' : (1, 'state'), 'state' : 1}
    mode = 'weak'

    def check_shapes(self, alpha, p_outer, virtual, state):
        pass

    def get_fargs(self, alpha, p_outer, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        fargs = DotProductSurfaceTerm.get_fargs(self, alpha, virtual, state,
                                                mode, term_mode, diff_var,
                                                **kwargs)
        fargs = fargs[:1] + (fargs[1] - p_outer,) + fargs[2:]

        return fargs

class DotSProductVolumeOperatorWTHTerm(THTerm):
    r"""
    Fading memory volume :math:`L^2(\Omega)` weighted dot product for
    scalar fields. Can use derivatives.

    :Definition:

    .. math::
        \int_\Omega \left [\int_0^t \Gcal(t-\tau) p(\tau) \difd{\tau} \right] q

    :Arguments:
        - ts       : :class:`TimeStepper` instance
        - material : :math:`\Gcal(\tau)`
        - virtual  : :math:`q`
        - state    : :math:`p`
    """
    name = 'dw_volume_dot_w_scalar_th'
    arg_types = ('ts', 'material', 'virtual', 'state')

    function = staticmethod(terms.dw_volume_dot_scalar)

    def get_fargs(self, ts, mats, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(state)

        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)

        if diff_var is None:
            def iter_kernel():
                for ii, mat in enumerate(mats):
                    val_qp = self.get(state, 'val', step=-ii)
                    mat = nm.tile(mat, (n_el, n_qp, 1, 1))
                    yield ii, (ts.dt * mat, val_qp, vg, vg, 0)
            fargs = iter_kernel

        else:
            val_qp = nm.array([0], ndmin=4, dtype=nm.float64)
            mat = nm.tile(mats[0], (n_el, n_qp, 1, 1))
            fargs = ts.dt * mat, val_qp, vg, vg, 1

        return fargs

class DotSProductVolumeOperatorWETHTerm(ETHTerm):
    r"""
    Fading memory volume :math:`L^2(\Omega)` weighted dot product for
    scalar fields. This term has the same definition as
    dw_volume_dot_w_scalar_th, but assumes an exponential approximation of
    the convolution kernel resulting in much higher efficiency. Can use
    derivatives.

    :Definition:

    .. math::
        \int_\Omega \left [\int_0^t \Gcal(t-\tau) p(\tau) \difd{\tau} \right] q

    :Arguments:
        - ts         : :class:`TimeStepper` instance
        - material_0 : :math:`\Gcal(0)`
        - material_1 : :math:`\exp(-\lambda \Delta t)` (decay at :math:`t_1`)
        - virtual    : :math:`q`
        - state      : :math:`p`
    """
    name = 'dw_volume_dot_w_scalar_eth'
    arg_types = ('ts', 'material_0', 'material_1', 'virtual', 'state')

    function = staticmethod(terms.dw_volume_dot_scalar)

    def get_fargs(self, ts, mat0, mat1, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _, key = self.get_mapping(state, return_key=True)

        if diff_var is None:
            val_qp = self.get(state, 'val')

            key += tuple(self.arg_names[ii] for ii in [1, 2, 4])
            data = self.get_eth_data(key, state, mat1, val_qp)

            fargs = (ts.dt * mat0, data.history + data.values, vg, vg, 0)

        else:
            aux = nm.array([0], ndmin=4, dtype=nm.float64)
            fargs = ts.dt * mat0, aux, vg, vg, 1

        return fargs

class VectorDotGradScalarTerm(Term):
    r"""
    Volume dot product of a vector and a gradient of scalar.
    Can be evaluated.

    :Definition:

    .. math::
        \int_{\Omega} \ul{v} \cdot \nabla p \mbox{ , }
        \int_{\Omega} \ul{u} \cdot \nabla q \\
        \int_{\Omega} c \ul{v} \cdot \nabla p \mbox{ , }
        \int_{\Omega} c \ul{u} \cdot \nabla q \\
        \int_{\Omega} \ul{v} \cdot \ull{M} \cdot \nabla p \mbox{ , }
        \int_{\Omega} \ul{u} \cdot \ull{M} \cdot \nabla q

    :Arguments 1:
        - material : :math:`c` or :math:`\ull{M}` (optional)
        - virtual  : :math:`\ul{v}`
        - state    : :math:`p`

    :Arguments 2:
        - material : :math:`c` or :math:`\ull{M}` (optional)
        - state    : :math:`\ul{u}`
        - virtual  : :math:`q`

    :Arguments 3:
        - material    : :math:`c` or :math:`\ull{M}` (optional)
        - parameter_v : :math:`\ul{u}`
        - parameter_s : :math:`p`
    """
    name = 'dw_v_dot_grad_s'
    arg_types = (('opt_material', 'virtual', 'state'),
                 ('opt_material', 'state', 'virtual'),
                 ('opt_material', 'parameter_v', 'parameter_s'))
    arg_shapes = [{'opt_material' : '1, 1',
                   'virtual/v_weak' : ('D', None), 'state/v_weak' : 1,
                   'virtual/s_weak' : (1, None), 'state/s_weak' : 'D',
                   'parameter_v' : 'D', 'parameter_s' : 1},
                  {'opt_material' : 'D, D'},
                  {'opt_material' : None}]
    modes = ('v_weak', 's_weak', 'eval')

    def check_shapes(self, coef, vvar, svar):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(vvar)
        assert_(n_c == dim)
        assert_(svar.n_components == 1)

        if coef is not None:
            assert_((coef.shape[1:] == (n_qp, 1, 1))
                    or (coef.shape[1:] == (n_qp, dim, dim)))
            assert_((coef.shape[0] == 1) or (coef.shape[0] == n_el))

    def get_fargs(self, coef, vvar, svar,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(vvar)
        if coef is None:
            coef = nm.ones((1, n_qp, 1, 1), dtype=nm.float64)

        if mode == 'weak':
            if self.mode == 'v_weak':
                qp_var, qp_name = svar, 'grad'

            else:
                qp_var, qp_name = vvar, 'val'

            vvg, _ = self.get_mapping(vvar)
            svg, _ = self.get_mapping(svar)

            if diff_var is None:
                val_qp = self.get(qp_var, qp_name)
                fmode = 0

            else:
                val_qp = nm.array([0], ndmin=4, dtype=nm.float64)
                fmode = 1

            return coef, val_qp, vvg, svg, fmode

        elif mode == 'eval':
            vvg, _ = self.get_mapping(vvar)

            grad = self.get(svar, 'grad')
            val = self.get(vvar, 'val')

            return coef, grad, val, vvg

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

    def get_eval_shape(self, coef, vvar, svar,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(vvar)

        return (n_el, 1, 1, 1), vvar.dtype

    def set_arg_types(self):
        self.function = {
            'v_weak' : terms.dw_v_dot_grad_s_vw,
            's_weak' : terms.dw_v_dot_grad_s_sw,
            'eval' : DotProductVolumeTerm.d_dot,
        }[self.mode]

class ScalarDotGradIScalarTerm(Term):
    r"""
    Dot product of a scalar and the :math:`i`-th component of gradient of a
    scalar. The index should be given as a 'special_constant' material
    parameter.

    :Definition:

    .. math::
        Z^i = \int_{\Omega} q \nabla_i p

    :Arguments:
        - material : :math:`i`
        - virtual  : :math:`q`
        - state    : :math:`p`
    """
    name = 'dw_s_dot_grad_i_s'
    arg_types = ('material', 'virtual', 'state')
    arg_shapes = {'material' : '1, 1', 'virtual' : (1, 'state'), 'state' : 1}

    @staticmethod
    def dw_fun(out, bf, vg, grad, idx, fmode):
        cc = nm.ascontiguousarray
        bft = cc(nm.tile(bf, (out.shape[0], 1, 1, 1)))

        if fmode == 0:
            status = terms.mulATB_integrate(out, bft,
                                            cc(grad[..., idx:idx+1, :]), vg)

        else:
            status = terms.mulATB_integrate(out, bft,
                                            cc(vg.bfg[:,:,idx:(idx + 1),:]), vg)

        return status

    def get_fargs(self, material, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        if mode == 'weak':
            if diff_var is None:
                grad = self.get(state, 'grad')
                fmode = 0

            else:
                grad = nm.array([0], ndmin=4, dtype=nm.float64)
                fmode = 1

            ap, vg = self.get_approximation(virtual)
            aps, vgs = self.get_approximation(state)

            bf = aps.get_base('v', 0, self.integral)
            idx = int(material[0, 0, 0, 0])

            return bf, vg, grad, idx, fmode

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

    def set_arg_types(self):
        self.function = self.dw_fun

########NEW FILE########
__FILENAME__ = terms_elastic
import numpy as nm

from sfepy.base.base import use_method_with_name, assert_
from sfepy.linalg import dot_sequences
from sfepy.homogenization.utils import iter_sym
from sfepy.terms.terms import Term, terms
from sfepy.terms.terms_th import THTerm, ETHTerm

## expr = """
## e = 1/2 * (grad( vec( u ) ) + grad( vec( u ) ).T)
## D = map( D_sym )
## s = D * e
## div( s )
## """

## """
## e[i,j] = 1/2 * (der[j]( u[i] ) + der[i]( u[j] ))
## map =
## D[i,j,k,l]
## s[i,j] = D[i,j,k,l] * e[k,l]
## """

class LinearElasticTerm(Term):
    r"""
    General linear elasticity term, with :math:`D_{ijkl}` given in
    the usual matrix form exploiting symmetry: in 3D it is :math:`6\times6`
    with the indices ordered as :math:`[11, 22, 33, 12, 13, 23]`, in 2D it is
    :math:`3\times3` with the indices ordered as :math:`[11, 22, 12]`. Can be
    evaluated. Can use derivatives.

    :Definition:

    .. math::
        \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})

    :Arguments 1:
        - material : :math:`D_{ijkl}`
        - virtual  : :math:`\ul{v}`
        - state    : :math:`\ul{u}`

    :Arguments 2:
        - material    : :math:`D_{ijkl}`
        - parameter_1 : :math:`\ul{w}`
        - parameter_2 : :math:`\ul{u}`
    """
    name = 'dw_lin_elastic'
    arg_types = (('material', 'virtual', 'state'),
                 ('material', 'parameter_1', 'parameter_2'))
    arg_shapes = {'material' : 'S, S', 'virtual' : ('D', 'state'),
                  'state' : 'D', 'parameter_1' : 'D', 'parameter_2' : 'D'}
    modes = ('weak', 'eval')
##     symbolic = {'expression': expr,
##                 'map' : {'u' : 'state', 'D_sym' : 'material'}}

    def check_shapes(self, mat, virtual, state):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)
        sym = (dim + 1) * dim / 2
        assert_(mat.shape == (n_el, n_qp, sym, sym))

    def get_fargs(self, mat, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(state)

        if mode == 'weak':
            if diff_var is None:
                strain = self.get(state, 'cauchy_strain')
                fmode = 0

            else:
                strain = nm.array([0], ndmin=4, dtype=nm.float64)
                fmode = 1

            return 1.0, strain, mat, vg, fmode

        elif mode == 'eval':
            strain1 = self.get(virtual, 'cauchy_strain')
            strain2 = self.get(state, 'cauchy_strain')

            return 1.0, strain1, strain2, mat, vg

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

    def get_eval_shape(self, mat, virtual, state,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)

        return (n_el, 1, 1, 1), state.dtype

    def set_arg_types(self):
        if self.mode == 'weak':
            self.function = terms.dw_lin_elastic

        else:
            self.function = terms.d_lin_elastic

class SDLinearElasticTerm(Term):
    r"""
    Sensitivity analysis of the linear elastic term.

    :Definition:

    .. math::
        \int_{\Omega} \hat{D}_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u})

    .. math::
        \hat{D}_{ijkl} = D_{ijkl}(\nabla \cdot \ul{\Vcal})
        - D_{ijkq}{\partial \Vcal_l \over \partial x_q}
        - D_{iqkl}{\partial \Vcal_j \over \partial x_q}

    :Arguments:
        - material    : :math:`D_{ijkl}`
        - parameter_w : :math:`\ul{w}`
        - parameter_u : :math:`\ul{u}`
        - parameter_mesh_velocity : :math:`\ul{\Vcal}`
    """
    name = 'd_sd_lin_elastic'
    arg_types = ('material', 'parameter_w', 'parameter_u',
                 'parameter_mesh_velocity')
    arg_shapes = {'material' : 'S, S',
                  'parameter_w' : 'D', 'parameter_u' : 'D',
                  'parameter_mesh_velocity' : 'D'}
    function = terms.d_lin_elastic

    @staticmethod
    def op_dv(vgrad):
        nel, nlev, dim, _ = vgrad.shape
        sd = nm.zeros((nel, nlev, dim**2, dim**2), dtype=vgrad.dtype)

        if dim == 2:
            sd[:,:,0:2,0:2] = vgrad[:,:]
            sd[:,:,2:4,2:4] = vgrad[:,:]

        elif dim == 3:
            sd[:,:,0:3,0:3] = vgrad[:,:]
            sd[:,:,3:6,3:6] = vgrad[:,:]
            sd[:,:,6:9,6:9] = vgrad[:,:]
        else:
            exit('not yet implemented!')

        return sd

    def get_fargs(self, mat, par_w, par_u, par_mv,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(par_u)

        grad_w = self.get(par_w, 'grad').transpose((0,1,3,2))
        grad_u = self.get(par_u, 'grad').transpose((0,1,3,2))
        nel, nqp, nr, nc = grad_u.shape
        strain_w = grad_w.reshape((nel, nqp, nr * nc, 1))
        strain_u = grad_u.reshape((nel, nqp, nr * nc, 1))

        mat_map = {3: nm.array([0, 2, 2, 1]),
                   6: nm.array([0, 3, 4, 3, 1, 5, 4, 5, 2])}

        mmap = mat_map[mat.shape[-1]]
        mat_ns = mat[nm.ix_(nm.arange(nel), nm.arange(nqp),
                            mmap, mmap)]

        div_mv = self.get(par_mv, 'div')
        grad_mv = self.get(par_mv, 'grad')
        opd_mv = self.op_dv(grad_mv)

        aux = dot_sequences(mat_ns, opd_mv)
        mat_mv = mat_ns * div_mv - (aux + aux.transpose((0,1,3,2)))

        return 1.0, strain_w, strain_u, mat_mv, vg

    def get_eval_shape(self, mat, par_w, par_u, par_mv,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(par_u)

        return (n_el, 1, 1, 1), par_u.dtype

class LinearElasticIsotropicTerm(Term):
    r"""
    Isotropic linear elasticity term.

    :Definition:

    .. math::
        \int_{\Omega} D_{ijkl}\ e_{ij}(\ul{v}) e_{kl}(\ul{u}) \mbox{ with }
        D_{ijkl} = \mu (\delta_{ik} \delta_{jl}+\delta_{il} \delta_{jk}) +
        \lambda \ \delta_{ij} \delta_{kl}

    :Arguments:
        - material_1 : :math:`\lambda`
        - material_2 : :math:`\mu`
        - virtual    : :math:`\ul{v}`
        - state      : :math:`\ul{u}`
    """
    name = 'dw_lin_elastic_iso'
    arg_types = ('material_1', 'material_2', 'virtual', 'state')
    arg_shapes = {'material_1' : '1, 1', 'material_2' : '1, 1',
                  'virtual' : ('D', 'state'), 'state' : 'D'}

    function = staticmethod(terms.dw_lin_elastic_iso)

    def check_shapes(self, lam, mu, virtual, state):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)
        assert_(lam.shape == (n_el, n_qp, 1, 1))
        assert_(mu.shape == (n_el, n_qp, 1, 1))

    def get_fargs(self, lam, mu, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(state)

        if mode == 'weak':
            if diff_var is None:
                strain = self.get(state, 'cauchy_strain')
                fmode = 0

            else:
                strain = nm.array([0], ndmin=4, dtype=nm.float64)
                fmode = 1

            return strain, lam, mu, vg, fmode

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

class LinearElasticTHTerm(THTerm):
    r"""
    Fading memory linear elastic (viscous) term. Can use derivatives.

    :Definition:

    .. math::
        \int_{\Omega} \left [\int_0^t
        \Hcal_{ijkl}(t-\tau)\,e_{kl}(\ul{u}(\tau)) \difd{\tau}
        \right]\,e_{ij}(\ul{v})

    :Arguments:
        - ts       : :class:`TimeStepper` instance
        - material : :math:`\Hcal_{ijkl}(\tau)`
        - virtual  : :math:`\ul{v}`
        - state    : :math:`\ul{u}`
    """
    name = 'dw_lin_elastic_th'
    arg_types = ('ts', 'material', 'virtual', 'state')

    function = staticmethod(terms.dw_lin_elastic)

    def get_fargs(self, ts, mats, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(state)

        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)

        if mode == 'weak':
            if diff_var is None:
                def iter_kernel():
                    for ii, mat in enumerate(mats):
                        strain = self.get(state, 'cauchy_strain',
                                          step=-ii)
                        mat = nm.tile(mat, (n_el, n_qp, 1, 1))
                        yield ii, (ts.dt, strain, mat, vg, 0)
                fargs = iter_kernel

            else:
                strain = nm.array([0], ndmin=4, dtype=nm.float64)
                mat = nm.tile(mats[0], (n_el, n_qp, 1, 1))
                fargs = ts.dt, strain, mat, vg, 1

            return fargs

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

class LinearElasticETHTerm(ETHTerm):
    r"""
    This term has the same definition as dw_lin_elastic_th, but assumes an
    exponential approximation of the convolution kernel resulting in much
    higher efficiency. Can use derivatives.

    :Definition:

    .. math::
        \int_{\Omega} \left [\int_0^t
        \Hcal_{ijkl}(t-\tau)\,e_{kl}(\ul{u}(\tau)) \difd{\tau}
        \right]\,e_{ij}(\ul{v})

    :Arguments:
        - ts         : :class:`TimeStepper` instance
        - material_0 : :math:`\Hcal_{ijkl}(0)`
        - material_1 : :math:`\exp(-\lambda \Delta t)` (decay at :math:`t_1`)
        - virtual    : :math:`\ul{v}`
        - state      : :math:`\ul{u}`
    """
    name = 'dw_lin_elastic_eth'
    arg_types = ('ts', 'material_0', 'material_1', 'virtual', 'state')

    function = staticmethod(terms.dw_lin_elastic)

    def get_fargs(self, ts, mat0, mat1, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _, key = self.get_mapping(state, return_key=True)

        if diff_var is None:
            strain = self.get(state, 'cauchy_strain')

            key += tuple(self.arg_names[ii] for ii in [1, 2, 4])
            data = self.get_eth_data(key, state, mat1, strain)

            fargs = (ts.dt, data.history + data.values, mat0, vg, 0)

        else:
            aux = nm.array([0], ndmin=4, dtype=nm.float64)
            fargs = (ts.dt, aux, mat0, vg, 1)

        return fargs

class LinearPrestressTerm(Term):
    r"""
    Linear prestress term, with the prestress :math:`\sigma_{ij}` given in
    the usual vector form exploiting symmetry: in 3D it has 6 components
    with the indices ordered as :math:`[11, 22, 33, 12, 13, 23]`, in 2D it has
    3 components with the indices ordered as :math:`[11, 22, 12]`. Can be
    evaluated.

    :Definition:

    .. math::
        \int_{\Omega} \sigma_{ij} e_{ij}(\ul{v})

    :Arguments 1:
        - material : :math:`\sigma_{ij}`
        - virtual  : :math:`\ul{v}`

    :Arguments 2:
        - material : :math:`\sigma_{ij}`
        - parameter : :math:`\ul{u}`
    """
    name = 'dw_lin_prestress'
    arg_types = (('material', 'virtual'),
                 ('material', 'parameter'))
    arg_shapes = {'material' : 'S, 1', 'virtual' : ('D', None),
                  'parameter' : 'D'}
    modes = ('weak', 'eval')

    def check_shapes(self, mat, virtual):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(virtual)
        sym = (dim + 1) * dim / 2
        assert_(mat.shape == (n_el, n_qp, sym, 1))

    def get_fargs(self, mat, virtual,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(virtual)

        if mode == 'weak':
            return mat, vg

        else:
            strain = self.get(virtual, 'cauchy_strain')

            fmode = {'eval' : 0, 'el_avg' : 1, 'qp' : 2}.get(mode, 1)
            return strain, mat, vg, fmode

    def get_eval_shape(self, mat, virtual,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(virtual)

        if mode != 'qp':
            n_qp = 1

        return (n_el, n_qp, 1, 1), virtual.dtype

    def d_lin_prestress(self, out, strain, mat, vg, fmode):
        aux = dot_sequences(mat, strain, mode='ATB')
        if fmode == 2:
            out[:] = aux
            status = 0

        else:
            status = vg.integrate(out, aux, fmode)

        return status

    def set_arg_types(self):
        if self.mode == 'weak':
            self.function = terms.dw_lin_prestress

        else:
            self.function = self.d_lin_prestress

class LinearStrainFiberTerm(Term):
    r"""
    Linear (pre)strain fiber term with the unit direction vector
    :math:`\ul{d}`.

    :Definition:

    .. math::
        \int_{\Omega} D_{ijkl} e_{ij}(\ul{v}) \left(d_k d_l\right)

    :Arguments:
        - material_1 : :math:`D_{ijkl}`
        - material_2 : :math:`\ul{d}`
        - virtual  : :math:`\ul{v}`
    """
    name = 'dw_lin_strain_fib'
    arg_types = ('material_1', 'material_2', 'virtual')
    arg_shapes = {'material_1' : 'S, S', 'material_2' : 'D, 1',
                  'virtual' : ('D', None)}

    function = staticmethod(terms.dw_lin_strain_fib)

    def check_shapes(self, mat1, mat2, virtual):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(virtual)
        sym = (dim + 1) * dim / 2
        assert_(mat1.shape == (n_el, n_qp, sym, sym))
        assert_(mat2.shape == (n_el, n_qp, dim, 1))

    def get_fargs(self, mat1, mat2, virtual,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(virtual)

        omega = nm.empty(mat1.shape[:3] + (1,), dtype=nm.float64)
        for ii, (ir, ic) in enumerate(iter_sym(mat2.shape[2])):
            omega[..., ii, 0] = mat2[..., ir, 0] * mat2[..., ic, 0]

        return mat1, omega, vg

class CauchyStrainTerm(Term):
    r"""
    Evaluate Cauchy strain tensor.

    It is given in the usual vector form exploiting symmetry: in 3D it has 6
    components with the indices ordered as :math:`[11, 22, 33, 12, 13, 23]`, in
    2D it has 3 components with the indices ordered as :math:`[11, 22,
    12]`. The last three (non-diagonal) components are doubled so that it is
    energetically conjugate to the Cauchy stress tensor with the same storage.

    Supports 'eval', 'el_avg' and 'qp' evaluation modes.

    :Definition:

    .. math::
        \int_{\Omega} \ull{e}(\ul{w})

    .. math::
        \mbox{vector for } K \from \Ical_h: \int_{T_K} \ull{e}(\ul{w}) /
        \int_{T_K} 1

    .. math::
        \ull{e}(\ul{w})|_{qp}

    :Arguments:
        - parameter : :math:`\ul{w}`
    """
    name = 'ev_cauchy_strain'
    arg_types = ('parameter',)
    arg_shapes = {'parameter' : 'D'}

    @staticmethod
    def function(out, strain, vg, fmode):
        if fmode == 2:
            out[:] = strain
            status = 0

        else:
            status = terms.de_cauchy_strain(out, strain, vg, fmode)

        return status

    def get_fargs(self, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(parameter)

        strain = self.get(parameter, 'cauchy_strain')

        fmode = {'eval' : 0, 'el_avg' : 1, 'qp' : 2}.get(mode, 1)

        return strain, vg, fmode

    def get_eval_shape(self, parameter,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(parameter)

        if mode != 'qp':
            n_qp = 1

        return (n_el, n_qp, dim * (dim + 1) / 2, 1), parameter.dtype

class CauchyStrainSTerm(CauchyStrainTerm):
    r"""
    Evaluate Cauchy strain tensor on a surface region.

    See :class:`CauchyStrainTerm`.

    Supports 'eval', 'el_avg' and 'qp' evaluation modes.

    :Definition:

    .. math::
        \int_{\Gamma} \ull{e}(\ul{w})

    .. math::
        \mbox{vector for } K \from \Ical_h: \int_{T_K} \ull{e}(\ul{w}) /
        \int_{T_K} 1

    .. math::
        \ull{e}(\ul{w})|_{qp}

    :Arguments:
        - parameter : :math:`\ul{w}`
    """
    name = 'ev_cauchy_strain_s'
    arg_types = ('parameter',)
    integration = 'surface_extra'

class CauchyStressTerm(Term):
    r"""
    Evaluate Cauchy stress tensor.

    It is given in the usual vector form exploiting symmetry: in 3D it has 6
    components with the indices ordered as :math:`[11, 22, 33, 12, 13, 23]`, in
    2D it has 3 components with the indices ordered as :math:`[11, 22, 12]`.

    Supports 'eval', 'el_avg' and 'qp' evaluation modes.

    :Definition:

    .. math::
        \int_{\Omega} D_{ijkl} e_{kl}(\ul{w})

    .. math::
        \mbox{vector for } K \from \Ical_h:
        \int_{T_K} D_{ijkl} e_{kl}(\ul{w}) / \int_{T_K} 1

    .. math::
        D_{ijkl} e_{kl}(\ul{w})|_{qp}

    :Arguments:
        - material  : :math:`D_{ijkl}`
        - parameter : :math:`\ul{w}`
    """
    name = 'ev_cauchy_stress'
    arg_types = ('material', 'parameter')
    arg_shapes = {'material' : 'S, S', 'parameter' : 'D'}

    @staticmethod
    def function(out, coef, strain, mat, vg, fmode):
        if fmode == 2:
            out[:] = dot_sequences(mat, strain)
            status = 0

        else:
            status = terms.de_cauchy_stress(out, strain, mat, vg, fmode)

        if coef is not None:
            out *= coef

        return status

    def get_fargs(self, mat, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(parameter)

        strain = self.get(parameter, 'cauchy_strain')

        fmode = {'eval' : 0, 'el_avg' : 1, 'qp' : 2}.get(mode, 1)

        return None, strain, mat, vg, fmode

    def get_eval_shape(self, mat, parameter,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(parameter)

        if mode != 'qp':
            n_qp = 1

        return (n_el, n_qp, dim * (dim + 1) / 2, 1), parameter.dtype

class CauchyStressTHTerm(CauchyStressTerm, THTerm):
    r"""
    Evaluate fading memory Cauchy stress tensor.

    It is given in the usual vector form exploiting symmetry: in 3D it has 6
    components with the indices ordered as :math:`[11, 22, 33, 12, 13, 23]`, in
    2D it has 3 components with the indices ordered as :math:`[11, 22, 12]`.

    Supports 'eval', 'el_avg' and 'qp' evaluation modes.

    :Definition:

    .. math::
        \int_{\Omega} \int_0^t \Hcal_{ijkl}(t-\tau)\,e_{kl}(\ul{w}(\tau))
        \difd{\tau}

    .. math::
        \mbox{vector for } K \from \Ical_h:
        \int_{T_K} \int_0^t \Hcal_{ijkl}(t-\tau)\,e_{kl}(\ul{w}(\tau))
        \difd{\tau} / \int_{T_K} 1

    .. math::
        \int_0^t \Hcal_{ijkl}(t-\tau)\,e_{kl}(\ul{w}(\tau)) \difd{\tau}|_{qp}

    :Arguments:
        - ts        : :class:`TimeStepper` instance
        - material  : :math:`\Hcal_{ijkl}(\tau)`
        - parameter : :math:`\ul{w}`
    """
    name = 'ev_cauchy_stress_th'
    arg_types = ('ts', 'material', 'parameter')
    arg_shapes = {}

    def get_fargs(self, ts, mats, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(state)

        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)

        fmode = {'eval' : 0, 'el_avg' : 1, 'qp' : 2}.get(mode, 1)
        def iter_kernel():
            for ii, mat in enumerate(mats):
                strain = self.get(state, 'cauchy_strain',
                                  step=-ii)
                mat = nm.tile(mat, (n_el, n_qp, 1, 1))
                yield ii, (ts.dt, strain, mat, vg, fmode)

        return iter_kernel

    def get_eval_shape(self, ts, mats, parameter,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        out = CauchyStressTerm.get_eval_shape(self, mats, parameter, mode,
                                              term_mode, diff_var, **kwargs)
        return out

class CauchyStressETHTerm(CauchyStressTerm, ETHTerm):
    r"""
    Evaluate fading memory Cauchy stress tensor.

    It is given in the usual vector form exploiting symmetry: in 3D it has 6
    components with the indices ordered as :math:`[11, 22, 33, 12, 13, 23]`, in
    2D it has 3 components with the indices ordered as :math:`[11, 22, 12]`.

    Assumes an exponential approximation of the convolution kernel resulting in
    much higher efficiency.

    Supports 'eval', 'el_avg' and 'qp' evaluation modes.

    :Definition:

    .. math::
        \int_{\Omega} \int_0^t \Hcal_{ijkl}(t-\tau)\,e_{kl}(\ul{w}(\tau))
        \difd{\tau}

    .. math::
        \mbox{vector for } K \from \Ical_h:
        \int_{T_K} \int_0^t \Hcal_{ijkl}(t-\tau)\,e_{kl}(\ul{w}(\tau))
        \difd{\tau} / \int_{T_K} 1

    .. math::
        \int_0^t \Hcal_{ijkl}(t-\tau)\,e_{kl}(\ul{w}(\tau)) \difd{\tau}|_{qp}

    :Arguments:
        - ts         : :class:`TimeStepper` instance
        - material_0 : :math:`\Hcal_{ijkl}(0)`
        - material_1 : :math:`\exp(-\lambda \Delta t)` (decay at :math:`t_1`)
        - parameter  : :math:`\ul{w}`
    """
    name = 'ev_cauchy_stress_eth'
    arg_types = ('ts', 'material_0', 'material_1', 'parameter')
    arg_shapes = {}

    def get_fargs(self, ts, mat0, mat1, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _, key = self.get_mapping(state, return_key=True)

        strain = self.get(state, 'cauchy_strain')

        key += tuple(self.arg_names[1:])
        data = self.get_eth_data(key, state, mat1, strain)

        fmode = {'eval' : 0, 'el_avg' : 1, 'qp' : 2}.get(mode, 1)

        return ts.dt, data.history + data.values, mat0, vg, fmode

    def get_eval_shape(self, ts, mat0, mat1, parameter,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        out = CauchyStressTerm.get_eval_shape(self, mat0, parameter, mode,
                                              term_mode, diff_var, **kwargs)
        return out

########NEW FILE########
__FILENAME__ = terms_electric
from sfepy.terms.terms import Term, terms

class ElectricSourceTerm( Term ):
    r"""
    Electric source term.

    :Definition:

    .. math::
        \int_{\Omega} c s (\nabla \phi)^2

    :Arguments:
        - material : :math:`c` (electric conductivity)
        - virtual : :math:`s` (test function)
        - parameter : :math:`\phi` (given electric potential)
    """
    name = 'dw_electric_source'
    arg_types = ('material', 'virtual', 'parameter')
    arg_shapes = {'material' : '1, 1', 'virtual' : (1, None), 'parameter' : 1}

    function = staticmethod(terms.dw_electric_source)

    def get_fargs(self, mat, virtual, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(virtual)

        grad = self.get(parameter, 'grad')

        return grad, mat, vg

########NEW FILE########
__FILENAME__ = terms_fibres
import numpy as nm

from sfepy.terms.terms_hyperelastic_tl import HyperElasticTLBase
from sfepy.homogenization.utils import iter_sym

def fibre_function(out, pars, green_strain, fmode):
    """
    Depending on `fmode`, compute fibre stress (0) or tangent modulus (!= 0).
    """
    fmax, eps_opt, s, fdir, act = pars

    eps = nm.zeros_like(fmax)
    omega = nm.empty_like(green_strain)
    for ii, (ir, ic) in enumerate(iter_sym(fdir.shape[2])):
        omega[..., ii, 0] = fdir[..., ir, 0] * fdir[..., ic, 0]
        eps[..., 0, 0] += omega[..., ii, 0] * green_strain[..., ii, 0]

    tau = act * fmax * nm.exp(-((eps - eps_opt) / s)**2.0)

    if fmode == 0:
        out[:] = omega * tau

    else:
        for ir in range(omega.shape[2]):
            for ic in range(omega.shape[2]):
                out[..., ir, ic] = omega[..., ir, 0] * omega[..., ic, 0]

        out[:] *= -2.0 * ((eps - eps_opt) / (s**2.0)) * tau

    return out

class FibresActiveTLTerm(HyperElasticTLBase):
    r"""
    Hyperelastic active fibres term. Effective stress
    :math:`S_{ij} = A f_{\rm max} \exp{\left\{-(\frac{\epsilon -
    \varepsilon_{\rm opt}}{s})^2\right\}} d_i d_j`,
    where :math:`\epsilon = E_{ij} d_i d_j` is the Green strain
    :math:`\ull{E}` projected to the fibre direction :math:`\ul{d}`.

    :Definition:

    .. math::
        \int_{\Omega} S_{ij}(\ul{u}) \delta E_{ij}(\ul{u};\ul{v})

    :Arguments:
        - material_1 : :math:`f_{\rm max}`
        - material_2 : :math:`\varepsilon_{\rm opt}`
        - material_3 : :math:`s`
        - material_4 : :math:`\ul{d}`
        - material_5 : :math:`A`
        - virtual    : :math:`\ul{v}`
        - state      : :math:`\ul{u}`
    """
    name = 'dw_tl_fib_a'
    arg_types = ('material_1', 'material_2', 'material_3',
                 'material_4', 'material_5', 'virtual', 'state')
    arg_shapes = {'material_1' : '1, 1', 'material_2' : '1, 1',
                  'material_3' : '1, 1', 'material_4' : 'D, 1',
                  'material_5' : '1, 1',
                  'virtual' : ('D', 'state'), 'state' : 'D'}
    family_data_names = ['green_strain']

    def get_fargs(self, mat1, mat2, mat3, mat4, mat5, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        fargs = HyperElasticTLBase.get_fargs(self,
                                             (mat1, mat2, mat3, mat4, mat5),
                                             virtual, state,
                                             mode, term_mode, diff_var,
                                             **kwargs)
        return fargs

    @staticmethod
    def stress_function(out, pars, green_strain):
        fibre_function(out, pars, green_strain, 0)

    @staticmethod
    def tan_mod_function(out, pars, green_strain):
        fibre_function(out, pars, green_strain, 1)

    def get_eval_shape(self, mat1, mat2, mat3, mat4, mat5, virtual, state,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)
        sym = dim * (dim + 1) / 2

        return (n_el, 1, sym, 1), state.dtype

########NEW FILE########
__FILENAME__ = terms_hyperelastic_base
import numpy as nm
from sfepy.terms.terms import Term, terms

_msg_missing_data = 'missing family data!'

class HyperElasticBase(Term):
    """
    Base class for all hyperelastic terms in TL/UL formulation.

    `HyperElasticBase.__call__()` computes element contributions given either
    stress (-> rezidual) or tangent modulus (-> tangent sitffnes matrix),
    i.e. constitutive relation type (CRT) related data. The CRT data are
    computed in subclasses implementing particular CRT (e.g. neo-Hookean
    material), in self.compute_crt_data().

    Modes:

      - 0: total formulation
      - 1: updated formulation

    Notes
    -----
    This is not a proper Term!
    """
    arg_types = ('material', 'virtual', 'state')
    arg_shapes = {'material' : '1, 1', 'virtual' : ('D', 'state'),
                  'state' : 'D'}

    @staticmethod
    def integrate(out, val_qp, vg, fmode):
        if fmode == 2:
            out[:] = val_qp
            status = 0

        else:
            status = vg.integrate(out, val_qp, fmode)

        return status

    @staticmethod
    def function(out, fun, *args):
        return fun(out, *args)

    def __init__(self, *args, **kwargs):
        Term.__init__(self, *args, **kwargs)

        igs = self.region.igs
        self.stress_cache = {}.fromkeys(igs, None)

    def get_family_data(self, state, cache_name, data_names):
        """
        Notes
        -----
        `data_names` argument is ignored for now.
        """
        name = state.name

        step_cache = state.evaluate_cache.setdefault(cache_name, {})
        cache = step_cache.setdefault(self.arg_steps[name], {})

        vg, _, key = self.get_mapping(state, return_key=True)

        data_key = key + (self.arg_derivatives[name],)

        if data_key in cache:
            out = cache[data_key]

        else:
            out = self.compute_family_data(state)
            cache[data_key] = out

        return out

    def compute_stress(self, mat, family_data, **kwargs):
        out = nm.empty_like(family_data.green_strain)

        get = family_data.get
        fargs = [get(name, msg_if_none=_msg_missing_data)
                 for name in self.family_data_names]

        self.stress_function(out, mat, *fargs)

        return out

    def compute_tan_mod(self, mat, family_data, **kwargs):
        shape = list(family_data.green_strain.shape)
        shape[-1] = shape[-2]
        out = nm.empty(shape, dtype=nm.float64)

        get = family_data.get
        fargs = [get(name, msg_if_none=_msg_missing_data)
                 for name in self.family_data_names]

        self.tan_mod_function(out, mat, *fargs)

        return out

    def get_fargs(self, mat, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(state)

        fd = self.get_family_data(state, self.fd_cache_name,
                                  self.family_data_names)

        if mode == 'weak':
            ig = self.char_fun.ig

            if diff_var is None:
                stress = self.compute_stress(mat, fd, **kwargs)
                self.stress_cache[ig] = stress
                tan_mod = nm.array([0], ndmin=4, dtype=nm.float64)

                fmode = 0

            else:
                stress = self.stress_cache[ig]
                if stress is None:
                    stress = self.compute_stress(mat, fd, **kwargs)

                tan_mod = self.compute_tan_mod(mat, fd, **kwargs)
                fmode = 1

            return (self.weak_function,
                    stress, tan_mod, fd.mtx_f, fd.det_f, vg, fmode,
                    self.hyperelastic_mode)

        elif mode in ('el_avg', 'qp'):
            if term_mode == 'strain':
                out_qp = fd.green_strain

            elif term_mode == 'stress':
                out_qp = self.compute_stress(mat, fd, **kwargs)

            else:
                raise ValueError('unsupported term mode in %s! (%s)'
                                 % (self.name, term_mode))

            fmode = {'el_avg' : 1, 'qp' : 2}[mode]

            return self.integrate, out_qp, vg, fmode

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

    def get_eval_shape(self, mat, virtual, state,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)
        sym = dim * (dim + 1) / 2

        if mode != 'qp':
            n_qp = 1

        return (n_el, n_qp, sym, 1), state.dtype

    # def _call_hmode( self, diff_var = None, chunk_size = None, **kwargs ):
    #     term_mode, = self.get_kwargs( ['term_mode'], **kwargs )
    #     virtual, state, state_u = self.get_args( ['virtual', 'state', 'state_u'], **kwargs )
    #     ap, vg = self.get_approximation(virtual)

    #     self.set_data_shape(ap)
    #     shape, mode = self.get_shape(diff_var, chunk_size)

    #     cache = self.get_cache( self.function['finite_strain'][self.mode_ul], 0 )
    #     family_data = cache(self.family_data_names, self, 0, state=state_u)

    #     ig = self.char_fun.ig
    #     if term_mode is None:
    #         stress = self.crt_data.stress[ig]
    #         if stress is None:
    #             stress = self.compute_crt_data(family_data, 0, **kwargs)
    #             self.crt_data.stress[ig] = stress
    #         tan_mod = self.crt_data.tan_mod[ig]
    #         if tan_mod is None:
    #             tan_mod = self.compute_crt_data(family_data, 1, **kwargs)
    #             self.crt_data.tan_mod[ig] = tan_mod

    #         fun = self.function['element_contribution']
    #         mtxF, detF = cache(['F', 'detF'], self, 0, state=state_u)

    #         if mode == 0:
    #             vec = self.get_vector(state)
    #             for out, chunk in self.char_fun( chunk_size, shape ):
    #                 out2 = nm.zeros(out.shape[:-1] + (out.shape[-2],),
    #                                 dtype=nm.float64)
    #                 status1 = fun( out2, stress, tan_mod,
    #                               mtxF, detF, vg, chunk, 1, self.mode_ul )
    #                 status2 = terms.he_residuum_from_mtx( out, out2, vec, ap.econn, chunk )
    #                 yield out, chunk, status1 or status2
    #         else:
    #             for out, chunk in self.char_fun( chunk_size, shape ):
    #                 status = fun( out, stress, tan_mod,
    #                               mtxF, detF, vg, chunk, 1, self.mode_ul )
    #                 yield out, chunk, status

    #     elif term_mode == 'd_eval':
    #         raise NotImplementedError

    # def _call_emode( self, diff_var = None, chunk_size = None, **kwargs ):
    #     term_mode, = self.get_kwargs( ['term_mode'], **kwargs )
    #     par1, par2, state_u = self.get_args(['parameter_1', 'parameter_2', 'state_u'], **kwargs)
    #     ap, vg = self.get_approximation(par1)

    #     self.set_data_shape(ap)
    #     n_el, n_qp, dim, n_ep = self.data_shape
    #     shape0 = (1, dim * n_ep, dim * n_ep)
    #     shape = (chunk_size, 1, 1, 1)

    #     cache = self.get_cache( self.function['finite_strain'][self.mode_ul], 0 )
    #     family_data = cache(self.family_data_names, self, 0, state=state_u)

    #     ig = self.char_fun.ig

    #     stress = self.crt_data.stress[ig]
    #     if stress is None:
    #         stress = self.compute_crt_data(family_data, 0, **kwargs)
    #         self.crt_data.stress[ig] = stress
    #     tan_mod = self.crt_data.tan_mod[ig]
    #     if tan_mod is None:
    #         tan_mod = self.compute_crt_data(family_data, 1, **kwargs)
    #         self.crt_data.tan_mod[ig] = tan_mod

    #     fun = self.function['element_contribution']
    #     mtxF, detF = cache(['F', 'detF'], self, 0, state=state_u)

    #     p1 = self.get_vector(par1)
    #     p2 = self.get_vector(par2)
    #     for out, chunk in self.char_fun( chunk_size, shape ):
    #         out2 = nm.zeros((out.shape[0],) + shape0, dtype=nm.float64)
    #         status1 = fun( out2, stress, tan_mod,
    #                        mtxF, detF, vg, chunk, 1, self.mode_ul )
    #         status2 = terms.he_eval_from_mtx(out, out2, p1, p2, ap.econn, chunk)
    #         out0 = nm.sum(out)

    #         yield out0, chunk, status1 or status2

    # def __call__( self, diff_var = None, chunk_size = None, **kwargs ):
    #     if self.call_mode is 0:
    #         return self._call_smode( diff_var, chunk_size, **kwargs )
    #     # elif self.call_mode is 1:
    #     #     return self._call_hmode( diff_var, chunk_size, **kwargs )
    #     # elif self.call_mode is 2:
    #     #     return self._call_emode( diff_var, chunk_size, **kwargs )
    #     else:
    #         raise NotImplementedError

class DeformationGradientTerm(Term):
    r"""
    Deformation gradient :math:`\ull{F}` in quadrature points for
    `term_mode='def_grad'` (default) or the jacobian :math:`J` if
    `term_mode='jacobian'`.

    Supports 'eval', 'el_avg' and 'qp' evaluation modes.

    :Definition:

    .. math::
        \ull{F} = \pdiff{\ul{x}}{\ul{X}}|_{qp}
        = \ull{I} + \pdiff{\ul{u}}{\ul{X}}|_{qp} \;, \\
        \ul{x} = \ul{X} + \ul{u} \;, J = \det{(\ull{F})}

    :Arguments:
        - parameter : :math:`\ul{u}`
    """
    name = 'ev_def_grad'
    arg_types = ('parameter',)
    arg_shapes = {'parameter' : 'D'}

    @staticmethod
    def function(out, vec, vg, econn, term_mode, fmode):
        d = 1 if term_mode == 'jacobian' else vg.dim
        out_qp = nm.empty((out.shape[0], vg.n_qp, d, d), dtype=out.dtype)

        mode = 1 if term_mode == 'jacobian' else 0
        terms.dq_def_grad(out_qp, vec, vg, econn, mode)

        if fmode == 2:
            out[:] = out_qp
            status = 0

        else:
            status = vg.integrate(out, out_qp, fmode)

        return status

    def get_fargs(self, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        ap, vg = self.get_approximation(parameter)

        vec = self.get_vector(parameter)

        fmode = {'eval' : 0, 'el_avg' : 1, 'qp' : 2}.get(mode, 1)

        return vec, vg, ap.econn, term_mode, fmode

    def get_eval_shape(self, parameter,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(parameter)

        if mode != 'qp':
            n_qp = 1

        if term_mode == 'jacobian':
            return (n_el, n_qp, 1, 1), parameter.dtype

        else: # 'def_grad'
            return (n_el, n_qp, dim, dim), parameter.dtype

########NEW FILE########
__FILENAME__ = terms_hyperelastic_tl
import numpy as nm

from sfepy.base.base import assert_, Struct
from sfepy.terms.terms import terms
from sfepy.terms.terms_hyperelastic_base import HyperElasticBase

class HyperElasticTLBase(HyperElasticBase):
    """
    Base class for all hyperelastic terms in TL formulation family.

    The subclasses should have the following static method attributes:
    - `stress_function()` (the stress)
    - `tan_mod_function()` (the tangent modulus)

    The common (family) data are cached in the evaluate cache of state
    variable.
    """
    family_function = staticmethod(terms.dq_finite_strain_tl)
    weak_function = staticmethod(terms.dw_he_rtm)
    fd_cache_name = 'tl_common'
    hyperelastic_mode = 0

    def compute_family_data(self, state):
        ap, vg = self.get_approximation(state)

        vec = self.get_vector(state)

        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)
        sym = dim * (dim + 1) / 2

        shapes = {
            'mtx_f' : (n_el, n_qp, dim, dim),
            'det_f' : (n_el, n_qp, 1, 1),
            'sym_c' : (n_el, n_qp, sym, 1),
            'tr_c' : (n_el, n_qp, 1, 1),
            'in2_c' : (n_el, n_qp, 1, 1),
            'sym_inv_c' : (n_el, n_qp, sym, 1),
            'green_strain' : (n_el, n_qp, sym, 1),
        }
        data = Struct(name='tl_family_data')
        for key, shape in shapes.iteritems():
            setattr(data, key, nm.zeros(shape, dtype=nm.float64))

        self.family_function(data.mtx_f,
                             data.det_f,
                             data.sym_c,
                             data.tr_c,
                             data.in2_c,
                             data.sym_inv_c,
                             data.green_strain,
                             vec, vg, ap.econn)
        return data

class NeoHookeanTLTerm(HyperElasticTLBase):
    r"""
    Hyperelastic neo-Hookean term. Effective stress
    :math:`S_{ij} = \mu J^{-\frac{2}{3}}(\delta_{ij} -
    \frac{1}{3}C_{kk}C_{ij}^{-1})`.

    :Definition:

    .. math::
        \int_{\Omega} S_{ij}(\ul{u}) \delta E_{ij}(\ul{u};\ul{v})

    :Arguments:
        - material : :math:`\mu`
        - virtual  : :math:`\ul{v}`
        - state    : :math:`\ul{u}`
    """
    name = 'dw_tl_he_neohook'
    family_data_names = ['det_f', 'tr_c', 'sym_inv_c']

    stress_function = staticmethod(terms.dq_tl_he_stress_neohook)
    tan_mod_function = staticmethod(terms.dq_tl_he_tan_mod_neohook)

class MooneyRivlinTLTerm(HyperElasticTLBase):
    r"""
    Hyperelastic Mooney-Rivlin term. Effective stress
    :math:`S_{ij} = \kappa J^{-\frac{4}{3}} (C_{kk} \delta_{ij} - C_{ij}
    - \frac{2}{3 } I_2 C_{ij}^{-1})`.

    :Definition:

    .. math::
        \int_{\Omega} S_{ij}(\ul{u}) \delta E_{ij}(\ul{u};\ul{v})

    :Arguments:
        - material : :math:`\kappa`
        - virtual  : :math:`\ul{v}`
        - state    : :math:`\ul{u}`
    """
    name = 'dw_tl_he_mooney_rivlin'
    family_data_names = ['det_f', 'tr_c', 'sym_inv_c', 'sym_c', 'in2_c']

    stress_function = staticmethod(terms.dq_tl_he_stress_mooney_rivlin)
    tan_mod_function = staticmethod(terms.dq_tl_he_tan_mod_mooney_rivlin)

class BulkPenaltyTLTerm(HyperElasticTLBase):
    r"""
    Hyperelastic bulk penalty term. Stress
    :math:`S_{ij} = K(J-1)\; J C_{ij}^{-1}`.

    :Definition:

    .. math::
        \int_{\Omega} S_{ij}(\ul{u}) \delta E_{ij}(\ul{u};\ul{v})

    :Arguments:
        - material : :math:`K`
        - virtual  : :math:`\ul{v}`
        - state    : :math:`\ul{u}`
    """

    name = 'dw_tl_bulk_penalty'
    family_data_names = ['det_f', 'sym_inv_c']

    stress_function = staticmethod(terms.dq_tl_he_stress_bulk)
    tan_mod_function = staticmethod(terms.dq_tl_he_tan_mod_bulk)

class BulkActiveTLTerm(HyperElasticTLBase):
    r"""
    Hyperelastic bulk active term. Stress :math:`S_{ij} = A J C_{ij}^{-1}`,
    where :math:`A` is the activation in :math:`[0, F_{\rm max}]`.

    :Definition:

    .. math::
        \int_{\Omega} S_{ij}(\ul{u}) \delta E_{ij}(\ul{u};\ul{v})

    :Arguments:
        - material : :math:`A`
        - virtual  : :math:`\ul{v}`
        - state    : :math:`\ul{u}`
    """

    name = 'dw_tl_bulk_active'
    family_data_names = ['det_f', 'sym_inv_c']

    stress_function = staticmethod(terms.dq_tl_he_stress_bulk_active)
    tan_mod_function = staticmethod(terms.dq_tl_he_tan_mod_bulk_active)

class BulkPressureTLTerm(HyperElasticTLBase):
    r"""
    Hyperelastic bulk pressure term. Stress
    :math:`S_{ij} = -p J C_{ij}^{-1}`.

    :Definition:

    .. math::
        \int_{\Omega} S_{ij}(p) \delta E_{ij}(\ul{u};\ul{v})

    :Arguments:
        - virtual : :math:`\ul{v}`
        - state   : :math:`\ul{u}`
        - state_p : :math:`p`
    """
    name = 'dw_tl_bulk_pressure'
    arg_types = ('virtual', 'state', 'state_p')
    arg_shapes = {'virtual' : ('D', 'state'), 'state' : 'D', 'state_p' : 1}
    family_data_names = ['det_f', 'sym_inv_c']

    family_function = staticmethod(terms.dq_finite_strain_tl)
    weak_function = staticmethod(terms.dw_he_rtm)
    weak_dp_function = staticmethod(terms.dw_tl_volume)

    stress_function = staticmethod(terms.dq_tl_stress_bulk_pressure)
    tan_mod_u_function = staticmethod(terms.dq_tl_tan_mod_bulk_pressure_u)

    def compute_data(self, family_data, mode, **kwargs):
        det_f, sym_inv_c = family_data.det_f, family_data.sym_inv_c
        p_qp = family_data.p_qp

        if mode == 0:
            out = nm.empty_like(sym_inv_c)
            fun = self.stress_function

        elif mode == 1:
            shape = list(sym_inv_c.shape)
            shape[-1] = shape[-2]
            out = nm.empty(shape, dtype=nm.float64)
            fun = self.tan_mod_u_function

        else:
            raise ValueError('bad mode! (%d)' % mode)

        fun(out, p_qp, det_f, sym_inv_c)

        return out

    def get_fargs(self, virtual, state, state_p,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vgv, _ = self.get_mapping(state)

        fd = self.get_family_data(state, 'tl_common', self.family_data_names)
        fd.p_qp = self.get(state_p, 'val')

        if mode == 'weak':
            ig = self.char_fun.ig

            if diff_var != state_p.name:
                if diff_var is None:
                    stress = self.compute_data(fd, 0, **kwargs)
                    self.stress_cache[ig] = stress
                    tan_mod = nm.array([0], ndmin=4, dtype=nm.float64)

                    fmode = 0

                else:
                    stress = self.stress_cache[ig]
                    if stress is None:
                        stress = self.compute_data(fd, 0, **kwargs)

                    tan_mod = self.compute_data(fd, 1, **kwargs)
                    fmode = 1

                fargs = (self.weak_function,
                         stress, tan_mod, fd.mtx_f, fd.det_f, vgv, fmode, 0)

            else:
                vgs, _ = self.get_mapping(state_p)

                fargs =  (self.weak_dp_function,
                          fd.mtx_f, fd.sym_inv_c, fd.det_f, vgs, vgv, 1, -1)

            return fargs

        elif mode == 'el_avg':
            if term_mode == 'strain':
                out_qp = fd.green_strain

            elif term_mode == 'stress':
                out_qp = self.compute_data(fd, 0, **kwargs)

            else:
                raise ValueError('unsupported term mode in %s! (%s)'
                                 % (self.name, term_mode))

            return self.integrate, out_qp, vgv, 1

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

    def get_eval_shape(self, virtual, state, state_p,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)
        sym = dim * (dim + 1) / 2

        return (n_el, 1, sym, 1), state.dtype

class VolumeTLTerm(HyperElasticTLBase):
    r"""
    Volume term (weak form) in the total Lagrangian formulation.

    :Definition:

    .. math::
         \begin{array}{l}
         \int_{\Omega} q J(\ul{u}) \\
         \mbox{volume mode: vector for } K \from \Ical_h: \int_{T_K}
         J(\ul{u}) \\
         \mbox{rel\_volume mode: vector for } K \from \Ical_h:
         \int_{T_K} J(\ul{u}) / \int_{T_K} 1
         \end{array}

    :Arguments:
        - virtual : :math:`q`
        - state   : :math:`\ul{u}`
    """
    name = 'dw_tl_volume'
    arg_types = ('virtual', 'state')
    arg_shapes = {'virtual' : (1, None), 'state' : 'D'}
    family_data_names = ['mtx_f', 'det_f', 'sym_inv_c']

    function = staticmethod(terms.dw_tl_volume)

    def get_fargs(self, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vgs, _ = self.get_mapping(virtual)
        vgv, _ = self.get_mapping(state)

        fd = self.get_family_data(state, 'tl_common', self.family_data_names)

        if mode == 'weak':
            if diff_var is None:
                fmode = 0

            else:
                fmode = 1

        elif (mode == 'eval') or (mode == 'el_avg'):
            if term_mode == 'volume':
                fmode = 2

            elif term_mode == 'rel_volume':
                fmode = 3

            else:
                raise ValueError('unsupported term evaluation mode in %s! (%s)'
                                 % (self.name, term_mode))

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

        return fd.mtx_f, fd.sym_inv_c, fd.det_f, vgs, vgv, 0, fmode

    def get_eval_shape(self, virtual, state,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)

        return (n_el, 1, 1, 1), state.dtype

class DiffusionTLTerm(HyperElasticTLBase):
    r"""
    Diffusion term in the total Lagrangian formulation with
    linearized deformation-dependent permeability
    :math:`\ull{K}(\ul{u}) = J \ull{F}^{-1} \ull{k} f(J) \ull{F}^{-T}`,
    where :math:`\ul{u}` relates to the previous time step :math:`(n-1)`
    and
    :math:`f(J) = \max\left(0, \left(1 + \frac{(J - 1)}{N_f}\right)\right)^2`
    expresses the dependence on volume compression/expansion.

    :Definition:

    .. math::
        \int_{\Omega} \ull{K}(\ul{u}^{(n-1)}) : \pdiff{q}{\ul{X}}
        \pdiff{p}{\ul{X}}

    :Arguments:
        - material_1 : :math:`\ull{k}`
        - material_2 : :math:`N_f`
        - virtual    : :math:`q`
        - state      : :math:`p`
        - parameter  : :math:`\ul{u}^{(n-1)}`
    """
    name = 'dw_tl_diffusion'
    arg_types = ('material_1', 'material_2', 'virtual', 'state', 'parameter')
    arg_shapes = {'material_1' : 'D, D', 'material_2' : '1, 1',
                  'virtual' : (1, 'state'), 'state' : 1, 'parameter' : 'D'}
    family_data_names = ['mtx_f', 'det_f']

    function = staticmethod(terms.dw_tl_diffusion)

    def get_fargs(self, perm, ref_porosity, virtual, state, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vgv, _ = self.get_mapping(parameter)

        fd = self.get_family_data(parameter, 'tl_common',
                                  self.family_data_names)
        grad = self.get(state, 'grad')

        if mode == 'weak':
            if diff_var is None:
                fmode = 0

            else:
                fmode = 1

        elif mode == 'el_avg':
            if term_mode == 'diffusion_velocity':
                fmode = 2

            else:
                raise ValueError('unsupported term evaluation mode in %s! (%s)'
                                 % (self.name, term_mode))

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

        return grad, perm, ref_porosity, fd.mtx_f, fd.det_f, vgv, fmode

    def get_eval_shape(self, perm, ref_porosity, virtual, state, parameter,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)

        return (n_el, 1, dim, 1), state.dtype

class HyperElasticSurfaceTLBase(HyperElasticBase):
    """
    Base class for all hyperelastic surface terms in TL formulation family.
    """
    family_function = staticmethod(terms.dq_tl_finite_strain_surface)
    fd_cache_name = 'tl_surface_common'

    def compute_family_data(self, state):
        ap, sg = self.get_approximation(state)
        sd = ap.surface_data[self.region.name]

        vec = self.get_vector(state)

        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)

        shapes = {
            'mtx_f' : (n_el, n_qp, dim, dim),
            'det_f' : (n_el, n_qp, 1, 1),
            'inv_f' : (n_el, n_qp, dim, dim),
        }
        data = Struct(name='tl_surface_family_data')
        for key, shape in shapes.iteritems():
            setattr(data, key, nm.zeros(shape, dtype=nm.float64))

        self.family_function(data.mtx_f,
                             data.det_f,
                             data.inv_f,
                             vec, sg, sd.fis, ap.econn)
        return data

class SurfaceFluxTLTerm(HyperElasticSurfaceTLBase):
    r"""
    Surface flux term in the total Lagrangian formulation, consistent with
    :class:`DiffusionTLTerm`.

    :Definition:

    .. math::
        \int_{\Gamma} \ul{\nu} \cdot \ull{K}(\ul{u}^{(n-1)}) \pdiff{p}{\ul{X}}

    :Arguments:
        - material_1 : :math:`\ull{k}`
        - material_2 : :math:`N_f`
        - parameter_1 : :math:`p`
        - parameter_2 : :math:`\ul{u}^{(n-1)}`
    """
    name = 'd_tl_surface_flux'
    arg_types = ('material_1', 'material_2', 'parameter_1', 'parameter_2')
    arg_shapes = {'material_1' : 'D, D', 'material_2' : '1, 1',
                  'parameter_1' : 1, 'parameter_2' : 'D'}
    family_data_names = ['det_f', 'inv_f']
    integration = 'surface_extra'

    function = staticmethod(terms.d_tl_surface_flux)

    def get_fargs(self, perm, ref_porosity, pressure, displacement,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        ap, sg = self.get_approximation(displacement)
        fd = self.get_family_data(displacement, 'tl_surface_common',
                                  self.family_data_names)
        grad = self.get(pressure, 'grad')

        fmode = {'eval' : 0, 'el_avg' : 1, 'el' : 0}.get(mode, 0)

        return grad, perm, ref_porosity, fd.inv_f, fd.det_f, sg, fmode

    def get_eval_shape(self, perm, ref_porosity, pressure, displacement,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_fa, n_qp, dim, n_en, n_c = self.get_data_shape(displacement)

        return (n_fa, 1, 1, 1), pressure.dtype

class SurfaceTractionTLTerm(HyperElasticSurfaceTLBase):
    r"""
    Surface traction term in the total Lagrangian formulation, expressed
    using :math:`\ul{\nu}`, the outward unit normal vector w.r.t. the
    undeformed surface, :math:`\ull{F}(\ul{u})`, the deformation gradient,
    :math:`J = \det(\ull{F})`, and :math:`\ull{\sigma}` a given traction,
    often equal to a given pressure, i.e.
    :math:`\ull{\sigma} = \pi \ull{I}`.

    :Definition:

    .. math::
        \int_{\Gamma} \ul{\nu} \cdot \ull{F}^{-1} \cdot \ull{\sigma} \cdot
        \ul{v} J

    :Arguments:
        - material : :math:`\ull{\sigma}`
        - virtual  : :math:`\ul{v}`
        - state    : :math:`\ul{u}`
    """
    name = 'dw_tl_surface_traction'
    arg_types = ('opt_material', 'virtual', 'state')
    arg_shapes = [{'opt_material' : 'D, D', 'virtual' : ('D', 'state'),
                   'state' : 'D'},
                  {'opt_material' : None}]
    family_data_names = ['det_f', 'inv_f']
    integration = 'surface_extra'

    function = staticmethod(terms.dw_tl_surface_traction)

    def check_shapes(self, mat, virtual, state):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)

        if mat is not None:
            assert_(mat.shape == (n_el, n_qp, dim, dim))

    def get_fargs(self, mat, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        ap, sg = self.get_approximation(virtual)
        sd = ap.surface_data[self.region.name]
        bf = ap.get_base(sd.bkey, 0, self.integral)

        fd = self.get_family_data(state, 'tl_surface_common',
                                  self.family_data_names)

        if mat is None:
            eye = nm.eye(sg.dim, dtype=nm.float64)
            mat = nm.tile(eye, ((1, sg.n_qp, 1, 1)))

        if diff_var is None:
            fmode = 0

        else:
            fmode = 1

        return mat, fd.det_f, fd.inv_f, bf, sg, sd.fis, fmode

class VolumeSurfaceTLTerm(HyperElasticSurfaceTLBase):
    r"""
    Volume of a :math:`D`-dimensional domain, using a surface integral in the
    total Lagrangian formulation, expressed using :math:`\ul{\nu}`, the outward
    unit normal vector w.r.t. the undeformed surface, :math:`\ull{F}(\ul{u})`,
    the deformation gradient, and :math:`J = \det(\ull{F})`. Uses the
    approximation of :math:`\ul{u}` for the deformed surface coordinates
    :math:`\ul{x}`.

    :Definition:

    .. math::
        1 / D \int_{\Gamma} \ul{\nu} \cdot \ull{F}^{-1} \cdot \ul{x} J

    :Arguments:
        - parameter : :math:`\ul{u}`
    """
    name = 'd_tl_volume_surface'
    arg_types = ('parameter',)
    arg_shapes = {'parameter' : 'D'}
    family_data_names = ['det_f', 'inv_f']
    integration = 'surface_extra'

    function = staticmethod(terms.d_tl_volume_surface)

    def check_shapes(self, parameter):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(parameter)

        assert_(dim == n_c)

    def get_fargs(self, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        ap, sg = self.get_approximation(parameter)
        sd = ap.surface_data[self.region.name]
        bf = ap.get_base(sd.bkey, 0, self.integral)

        fd = self.get_family_data(parameter, 'tl_surface_common',
                                  self.family_data_names)

        asc = nm.ascontiguousarray

        coors0 = parameter.field.get_coor()
        coors = asc(coors0 + parameter().reshape(coors0.shape))

        return coors, fd.det_f, fd.inv_f, bf, sg, asc(sd.econn)

    def get_eval_shape(self, parameter,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(parameter)

        return (n_el, 1, 1, 1), parameter.dtype

########NEW FILE########
__FILENAME__ = terms_hyperelastic_ul
import numpy as nm

from sfepy.base.base import Struct
from sfepy.terms.terms import terms
from sfepy.terms.terms_hyperelastic_base import HyperElasticBase

_msg_missing_data = 'missing family data!'

class HyperElasticULBase(HyperElasticBase):
    """
    Base class for all hyperelastic terms in UL formulation family.

    The subclasses should have the following static method attributes:
    - `stress_function()` (the stress)
    - `tan_mod_function()` (the tangent modulus)

    The common (family) data are cached in the evaluate cache of state
    variable.
    """
    family_function = staticmethod(terms.dq_finite_strain_ul)
    weak_function = staticmethod(terms.dw_he_rtm)
    fd_cache_name = 'ul_common'
    hyperelastic_mode = 1

    def compute_family_data(self, state):
        ap, vg = self.get_approximation(state, get_saved=True)

        vec = self.get_vector(state)

        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)
        sym = dim * (dim + 1) / 2

        shapes = {
            'mtx_f' : (n_el, n_qp, dim, dim),
            'det_f' : (n_el, n_qp, 1, 1),
            'sym_b' : (n_el, n_qp, sym, 1),
            'tr_b' : (n_el, n_qp, 1, 1),
            'in2_b' : (n_el, n_qp, 1, 1),
            'green_strain' : (n_el, n_qp, sym, 1),
        }
        data = Struct(name='ul_family_data')
        for key, shape in shapes.iteritems():
            setattr(data, key, nm.zeros(shape, dtype=nm.float64))

        self.family_function(data.mtx_f,
                             data.det_f,
                             data.sym_b,
                             data.tr_b,
                             data.in2_b,
                             data.green_strain,
                             vec, vg, ap.econn)

        return data

class NeoHookeanULTerm(HyperElasticULBase):
    r"""
    Hyperelastic neo-Hookean term. Effective stress :math:`\tau_{ij} = \mu
    J^{-\frac{2}{3}}(b_{ij} - \frac{1}{3}b_{kk}\delta_{ij})`.

    :Definition:

    .. math::
        \int_{\Omega} \mathcal{L}\tau_{ij}(\ul{u}) e_{ij}(\delta\ul{v})/J

    :Arguments:
        - material : :math:`\mu`
        - virtual  : :math:`\ul{v}`
        - state    : :math:`\ul{u}`
    """
    name = 'dw_ul_he_neohook'
    family_data_names = ['det_f', 'tr_b', 'sym_b']

    stress_function = staticmethod(terms.dq_ul_he_stress_neohook)
    tan_mod_function = staticmethod(terms.dq_ul_he_tan_mod_neohook)

# class NeoHookeanULHTerm(NeoHookeanULTerm):
#     r"""
#     Hyperelastic neo-Hookean term.  Geometrical configuration given by
#     parameter :math:`\ul{w}`.  Effective stress :math:`\tau_{ij} = \mu
#     J^{-\frac{2}{3}}(b_{ij} - \frac{1}{3}b_{kk}\delta_{ij})`.

#     :Definition:

#     .. math::
#         \int_{\Omega} \mathcal{L}\tau_{ij}(\ul{u}) e_{ij}(\delta\ul{v})/J

#     :Arguments 1:
#         - material : :math:`\mu`
#         - virtual  : :math:`\ul{v}`
#         - state    : :math:`\ul{u}`
#         - state_u  : :math:`\ul{w}`
#     """
#     name = 'dw_ul_he_neohook_h'
#     arg_types = ('material', 'virtual', 'state', 'state_u')
#     use_caches = {'finite_strain_ul' : [['state_u']]}

#     def __init__(self, *args, **kwargs):
#         HyperElasticULBase.__init__(self, *args, **kwargs)
#         self.call_mode = 1

# class NeoHookeanULEHTerm(NeoHookeanULTerm):
#     r"""
#     Hyperelastic neo-Hookean term.
#     Geometrical configuration given by parameter :math:`\ul{w}`.
#     Effective stress :math:`\tau_{ij} = \mu J^{-\frac{2}{3}}(b_{ij} - \frac{1}{3}b_{kk}\delta_{ij})`.

#     :Definition:

#     .. math::
#         \int_{\Omega} \mathcal{L}\tau_{ij}(\ul{u}) e_{ij}(\delta\ul{v})/J

#     :Arguments:
#         - material    : :math:`\mu`
#         - parameter_1 : :math:`\ul{v}`
#         - parameter_2 : :math:`\ul{u}`
#         - state_u     : :math:`\ul{w}`
#     """
#     name = 'd_ul_he_neohook_h'
#     arg_types = ('material', 'parameter_1', 'parameter_2', 'state_u')
#     use_caches = {'finite_strain_ul' : [['state_u']]}

#     def __init__(self, *args, **kwargs):
#         HyperElasticULBase.__init__(self, *args, **kwargs)
#         self.call_mode = 2

# class NeoHookeanULEvalTerm(Term):

#     name = 'de_ul_he_neohook'
#     arg_types = ('material', 'state_u')
#     use_caches = {'finite_strain_ul' : [['state_u']]}
#     function = {'stress': terms.dq_ul_he_stress_neohook,
#                 'element_contribution' : terms.de_he_rtm}

#     def __call__( self, diff_var = None, chunk_size = None, **kwargs ):
#         mat, state_u = self.get_args( ['material', 'state_u'], **kwargs )
#         ap, vg = self.get_approximation(state_u)

#         dim = ap.dim[0]
#         sym = (dim + 1) * dim / 2
#         shape = (chunk_size, 1, sym, 1)

#         cache = self.get_cache('finite_strain_ul', 0)
#         detF, trB, B = cache(['detF', 'trB', 'B'], self, 0, state=state_u)

#         stress = nm.empty_like(B)
#         fun = self.function['stress']
#         fun(stress, mat, detF, trB, B)

#         fun = self.function['element_contribution']
#         for out, chunk in self.char_fun(chunk_size, shape):
#             status = fun(out, stress, detF, vg, chunk, 1)
#             out1 = nm.sum(out,0).reshape((sym,))

#             yield out1, chunk, status

# class BulkPenaltyULHTerm(BulkPenaltyULTerm):
#     r"""
#     Hyperelastic bulk penalty term.
#     Geometrical configuration given by parameter :math:`\ul{w}`.
#     Stress :math:`\tau_{ij} = K(J-1)\; J \delta_{ij}`.

#     :Definition:

#     .. math::
#         \int_{\Omega} \mathcal{L}\tau_{ij}(\ul{u}) e_{ij}(\delta\ul{v})/J

#     :Arguments:
#         - material : :math:`K`
#         - virtual  : :math:`\ul{v}`
#         - state    : :math:`\ul{u}`
#         - state_u  : :math:`\ul{w}`
#     """
#     name = 'dw_ul_bulk_penalty_h'
#     arg_types = ('material', 'virtual', 'state', 'state_u')
#     use_caches = {'finite_strain_ul' : [['state_u']]}

#     def __init__(self, *args, **kwargs):
#         HyperElasticULBase.__init__(self, *args, **kwargs)
#         self.call_mode = 1

# class BulkPenaltyULEHTerm(BulkPenaltyULTerm):
#     r"""
#     Hyperelastic bulk penalty term.
#     Geometrical configuration given by parameter :math:`\ul{w}`.
#     Stress :math:`\tau_{ij} = K(J-1)\; J \delta_{ij}`.

#     :Definition:

#     .. math::
#         \int_{\Omega} \mathcal{L}\tau_{ij}(\ul{u}) e_{ij}(\delta\ul{v})/J

#     :Arguments:
#         - material    : :math:`K`
#         - parameter_1 : :math:`\ul{v}`
#         - parameter_2 : :math:`\ul{u}`
#         - state_u  : :math:`\ul{w}`
#     """
#     name = 'd_ul_bulk_penalty_h'
#     arg_types = ('material', 'parameter_1', 'parameter_2', 'state_u')
#     use_caches = {'finite_strain_ul' : [['state_u']]}

#     def __init__(self, *args, **kwargs):
#         HyperElasticULBase.__init__(self, *args, **kwargs)
#         self.call_mode = 2

class MooneyRivlinULTerm(HyperElasticULBase):
    r"""
    Hyperelastic Mooney-Rivlin term.

    :Definition:

    .. math::
        \int_{\Omega} \mathcal{L}\tau_{ij}(\ul{u}) e_{ij}(\delta\ul{v})/J

    :Arguments:
        - material : :math:`\kappa`
        - virtual  : :math:`\ul{v}`
        - state    : :math:`\ul{u}`
    """
    name = 'dw_ul_he_mooney_rivlin'
    family_data_names = ['det_f', 'tr_b', 'sym_b', 'in2_b']

    stress_function = staticmethod(terms.dq_ul_he_stress_mooney_rivlin)
    tan_mod_function = staticmethod(terms.dq_ul_he_tan_mod_mooney_rivlin)

class BulkPenaltyULTerm(HyperElasticULBase):
    r"""
    Hyperelastic bulk penalty term. Stress :math:`\tau_{ij} = K(J-1)\; J
    \delta_{ij}`.

    :Definition:

    .. math::
        \int_{\Omega} \mathcal{L}\tau_{ij}(\ul{u}) e_{ij}(\delta\ul{v})/J

    :Arguments:
        - material : :math:`K`
        - virtual  : :math:`\ul{v}`
        - state    : :math:`\ul{u}`
    """
    name = 'dw_ul_bulk_penalty'
    family_data_names = ['det_f']

    stress_function = staticmethod(terms.dq_ul_he_stress_bulk)
    tan_mod_function = staticmethod(terms.dq_ul_he_tan_mod_bulk)

class BulkPressureULTerm(HyperElasticULBase):
    r"""
    Hyperelastic bulk pressure term. Stress :math:`S_{ij} = -p J \delta_{ij}`.

    :Definition:

    .. math::
        \int_{\Omega} \mathcal{L}\tau_{ij}(\ul{u}) e_{ij}(\delta\ul{v})/J

    :Arguments:
        - virtual : :math:`\ul{v}`
        - state   : :math:`\ul{u}`
        - state_p : :math:`p`
    """

    name = 'dw_ul_bulk_pressure'
    arg_types = ('virtual', 'state', 'state_p')
    arg_shapes = {'virtual' : ('D', 'state'), 'state' : 'D', 'state_p' : 1}
    family_data_names = ['det_f', 'sym_b']

    family_function = staticmethod(terms.dq_finite_strain_ul)
    weak_function = staticmethod(terms.dw_he_rtm)
    weak_dp_function = staticmethod(terms.dw_ul_volume)

    stress_function = staticmethod(terms.dq_ul_stress_bulk_pressure)
    tan_mod_u_function = staticmethod(terms.dq_ul_tan_mod_bulk_pressure_u)

    def compute_data(self, family_data, mode, **kwargs):
        det_f, sym_b = family_data.det_f, family_data.sym_b
        p_qp = family_data.p_qp

        if mode == 0:
            out = nm.empty_like(sym_b)
            fun = self.stress_function

        elif mode == 1:
            shape = list(sym_b.shape)
            shape[-1] = shape[-2]
            out = nm.empty(shape, dtype=nm.float64)
            fun = self.tan_mod_u_function

        else:
            raise ValueError('bad mode! (%d)' % mode)

        fun(out, p_qp, det_f)

        return out

    def get_fargs(self, virtual, state, state_p,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vgv, _ = self.get_mapping(state)

        fd = self.get_family_data(state, 'ul_common', self.family_data_names)
        fd.p_qp = self.get(state_p, 'val')

        if mode == 'weak':
            ig = self.char_fun.ig

            if diff_var != state_p.name:
                if diff_var is None:
                    stress = self.compute_data(fd, 0, **kwargs)
                    self.stress_cache[ig] = stress
                    tan_mod = nm.array([0], ndmin=4, dtype=nm.float64)

                    fmode = 0

                else:
                    stress = self.stress_cache[ig]
                    if stress is None:
                        stress = self.compute_data(fd, 0, **kwargs)

                    tan_mod = self.compute_data(fd, 1, **kwargs)
                    fmode = 1

                fargs = (self.weak_function,
                         stress, tan_mod, fd.mtx_f, fd.det_f, vgv, fmode, 1)

            else:
                vgs, _ = self.get_mapping(state_p)

                fargs =  (self.weak_dp_function, fd.det_f, vgs, vgv, 1, -1)

            return fargs

        elif mode == 'el_avg':
            if term_mode == 'strain':
                out_qp = fd.green_strain

            elif term_mode == 'stress':
                out_qp = self.compute_data(fd, 0, **kwargs)

            else:
                raise ValueError('unsupported term mode in %s! (%s)'
                                 % (self.name, term_mode))

            return self.integrate, out_qp, vgv, 1

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

    def get_eval_shape(self, virtual, state, state_p,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)
        sym = dim * (dim + 1) / 2

        return (n_el, 1, sym, 1), state.dtype

# class BulkPressureULHTerm(BulkPressureULTerm):
#     r"""
#     Hyperelastic bulk pressure term. Stress
#     :math:`S_{ij} = -p J \delta_{ij}`.

#     :Definition:

#     .. math::
#         \int_{\Omega} \mathcal{L}\tau_{ij}(\ul{u}) e_{ij}(\delta\ul{v})/J

#     :Arguments:
#         - virtual : :math:`\ul{v}`
#         - state   : :math:`\ul{u}`
#         - state_p : :math:`p`
#         - state_u : :math:`w`
#     """
#     name = 'dw_ul_bulk_pressure_h'
#     arg_types = ('virtual', 'state', 'state_p', 'state_u')
#     use_caches = {'finite_strain_ul' : [['state_u']],
#                   'state_in_volume_qp' : [['state_p']]}

#     def __call__(self, diff_var=None, chunk_size=None, **kwargs):
#         term_mode, = self.get_kwargs(['term_mode'], **kwargs)
#         virtual, state, state_p, state_u = self.get_args(**kwargs)
#         apv, vgv = self.get_approximation(virtual)
#         aps, vgs = self.get_approximation(state_p)

#         self.set_data_shape(apv, aps)
#         shape, mode = self.get_shape_grad(diff_var, chunk_size)

#         cache = self.get_cache('finite_strain_ul', 0)
#         family_data = cache(['detF', 'B'], self, 0, state=state_u)

#         ig = self.char_fun.ig

#         if term_mode is None:

#             if mode < 2:
#                 stress = self.crt_data.stress[ig]
#                 if stress is None:
#                     stress = self.compute_crt_data(family_data, 0, **kwargs)
#                     self.crt_data.stress[ig] = stress
#                 tan_mod = self.crt_data.tan_mod[ig]
#                 if tan_mod is None:
#                     tan_mod = self.compute_crt_data(family_data, 1, **kwargs)
#                     self.crt_data.tan_mod[ig] = tan_mod

#                 fun = self.function['element_contribution']

#                 mtxF, detF = cache(['F', 'detF'], self, 0, state=state_u)

#                 if mode == 0:
#                     vec = self.get_vector(state)
#                     for out, chunk in self.char_fun(chunk_size, shape):
#                         out2 = nm.zeros(out.shape[:-1] + (out.shape[-2],),
#                                         dtype=nm.float64)
#                         status1 = fun(out2, stress, tan_mod,
#                                       mtxF, detF, vgv, chunk, 1, 1)
#                         status2 = terms.he_residuum_from_mtx(out, out2, vec, apv.econn, chunk)
#                         yield out, chunk, status1 or status2

#                 else:
#                     for out, chunk in self.char_fun(chunk_size, shape):
#                         status = fun(out, stress, tan_mod,
#                                      mtxF, detF, vgv, chunk, 1, 1)
#                         yield out, chunk, status

#             else:
#                 from sfepy.base.base import debug
#                 debug()
#                 # fun = self.function['element_contribution_dp']

#                 # mtxF, B, detF = cache(['F', 'B', 'detF'],
#                 #                       self, 0, state=state_u)

#                 # bf = aps.get_base('v', 0, self.integral)
#                 # for out, chunk in self.char_fun(chunk_size, shape):
#                 #     status = fun(out, bf, mtxF, B, detF, vgv, 1, chunk, 1)
#                 #     yield -out, chunk, status

#         elif term_mode == 'd_eval':
#             raise NotImplementedError

# class BulkPressureULEHTerm(BulkPressureULTerm):
#     r"""
#     Hyperelastic bulk pressure term. Stress
#     :math:`S_{ij} = -p J \delta_{ij}`.

#     :Definition:

#     .. math::
#         \int_{\Omega} \mathcal{L}\tau_{ij}(\ul{u}) e_{ij}(\delta\ul{v})/J

#     :Arguments:
#         - virtual : :math:`\ul{v}`
#         - state   : :math:`\ul{u}`
#         - state_p : :math:`p`
#         - state_u : :math:`w`
#     """
#     name = 'd_ul_bulk_pressure_h'
#     arg_types = ('virtual', 'state', 'state_p', 'state_u')
#     use_caches = {'finite_strain_ul' : [['state_u']],
#                   'state_in_volume_qp' : [['state_p']]}

#     def __call__(self, diff_var=None, chunk_size=None, **kwargs):
#         term_mode, = self.get_kwargs(['term_mode'], **kwargs)
#         par1, par2, state_p, state_u = self.get_args(**kwargs)
#         apv, vgv = self.get_approximation(par1)
#         aps, vgs = self.get_approximation(state_p)

#         self.set_data_shape(apv, aps)
#         n_el, n_qp, dim, n_ep = self.data_shape_v
#         shape0 = (1, dim * n_ep, dim * n_ep)
#         shape = (chunk_size, 1, 1, 1)

#         cache = self.get_cache('finite_strain_ul', 0)
#         family_data = cache(['detF', 'B'], self, 0, state=state_u)

#         ig = self.char_fun.ig
#         p1 = self.get_vector(par1)
#         p2 = self.get_vector(par2)

#         stress = self.crt_data.stress[ig]
#         if stress is None:
#             stress = self.compute_crt_data(family_data, 0, **kwargs)
#             self.crt_data.stress[ig] = stress
#         tan_mod = self.crt_data.tan_mod[ig]
#         if tan_mod is None:
#             tan_mod = self.compute_crt_data(family_data, 1, **kwargs)
#             self.crt_data.tan_mod[ig] = tan_mod

#         fun = self.function['element_contribution']
#         mtxF, detF = cache(['F', 'detF'], self, 0, state=state_u)

#         for out, chunk in self.char_fun( chunk_size, shape ):
#             out2 = nm.zeros((out.shape[0],) + shape0, dtype=nm.float64)
#             status1 = fun(out2, stress, tan_mod,
#                           mtxF, detF, vgv, chunk, 1, 1)
#             status2 = terms.he_eval_from_mtx(out, out2, p1, p2, apv.econn, chunk)
#             out0 = nm.sum(out)

#             yield out0, chunk, status1 or status2

# class BulkPressureULEvalTerm(Term):

#     name = 'de_ul_bulk_pressure'
#     arg_types = ('state_u', 'state_p')
#     use_caches = {'finite_strain_ul' : [['state_u']],
#                   'state_in_volume_qp' : [['state_p']]}

#     function = {'stress': terms.dq_ul_stress_bulk_pressure,
#                 'element_contribution' : terms.de_he_rtm}

#     def __call__( self, diff_var = None, chunk_size = None, **kwargs ):
#         state_u, state_p = self.get_args( ['state_u', 'state_p'], **kwargs )
#         ap, vg = self.get_approximation(state_u)

#         dim = ap.dim[0]
#         sym = (dim + 1) * dim / 2
#         shape = (chunk_size, 1, sym, 1)

#         cache = self.get_cache( 'finite_strain_ul', 0 )
#         detF, B = cache(['detF', 'B'], self, 0, state=state_u)
#         cache = self.get_cache('state_in_volume_qp', 0)
#         p_qp = cache('state', self, 0, state=state_p, get_vector=self.get_vector)

#         stress = nm.empty_like(B)
#         fun = self.function['stress']
#         fun(stress, p_qp, detF)

#         fun = self.function['element_contribution']
#         for out, chunk in self.char_fun(chunk_size, shape):
#             status = fun(out, stress, detF, vg, chunk, 1)
#             out1 = nm.sum(out,0).reshape((sym,))

#             yield out1, chunk, status

class VolumeULTerm(HyperElasticULBase):
    r"""
    Volume term (weak form) in the updated Lagrangian formulation.

    :Definition:

    .. math::
         \begin{array}{l}
         \int_{\Omega} q J(\ul{u}) \\
         \mbox{volume mode: vector for } K \from \Ical_h: \int_{T_K}
         J(\ul{u}) \\
         \mbox{rel\_volume mode: vector for } K \from \Ical_h:
         \int_{T_K} J(\ul{u}) / \int_{T_K} 1
         \end{array}

    :Arguments:
        - virtual : :math:`q`
        - state   : :math:`\ul{u}`
    """
    name = 'dw_ul_volume'
    arg_types = ('virtual', 'state')
    arg_shapes = {'virtual' : (1, None), 'state' : 'D'}
    family_data_names = ['mtx_f', 'det_f']

    function = staticmethod(terms.dw_ul_volume)
    def get_fargs(self, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vgs, _ = self.get_mapping(virtual)
        vgv, _ = self.get_mapping(state)

        fd = self.get_family_data(state, 'ul_common', self.family_data_names)

        if mode == 'weak':
            if diff_var is None:
                fmode = 0

            else:
                fmode = 1

        elif mode == 'eval':
            if term_mode == 'volume':
                fmode = 2

            elif term_mode == 'rel_volume':
                fmode = 3

            else:
                raise ValueError('unsupported term evaluation mode in %s! (%s)'
                                 % (self.name, term_mode))

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

        return fd.det_f, vgs, vgv, 0, fmode

    def get_eval_shape(self, virtual, state,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)

        return (n_el, 1, 1, 1), state.dtype

class CompressibilityULTerm(HyperElasticULBase):
    r"""
    Compressibility term for the updated Lagrangian formulation

    :Definition:

    .. math::
        \int_{\Omega} 1\over \gamma p \, q

    :Arguments:
        - material : :math:`\gamma`
        - virtual  : :math:`q`
        - state    : :math:`p`
        - parameter_u  : :math:`\ul(u)`
    """
    name = 'dw_ul_compressible'
    arg_types = ('material', 'virtual', 'state', 'parameter_u')
    arg_shapes = {'material' : '1, 1', 'virtual' : (1, 'state'), 'state' : 1,
                  'parameter_u' : 'D'}
    family_data_names = ['mtx_f', 'det_f']

    function = staticmethod(terms.dw_volume_dot_scalar)

    def get_fargs(self, bulk, virtual, state, parameter_u,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vgp, _ = self.get_mapping(virtual)
        vgs, _ = self.get_mapping(state)
        vgu, _ = self.get_mapping(parameter_u)

        fd = self.get_family_data(parameter_u, 'ul_common', self.family_data_names)

        coef = nm.divide(bulk, fd.det_f)

        if mode == 'weak':
            if diff_var is None:
                val_qp = self.get(state, 'val')
                fmode = 0

            else:
                val_qp = nm.array([0], ndmin=4, dtype=nm.float64)
                fmode = 1

            return coef, val_qp, vgp, vgs, fmode

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

########NEW FILE########
__FILENAME__ = terms_membrane
import numpy as nm

from sfepy.base.base import assert_
from sfepy.linalg import dot_sequences
from sfepy.mechanics.tensors import dim2sym, transform_data
import sfepy.mechanics.membranes as membranes
from sfepy.terms.terms import Term

def eval_membrane_mooney_rivlin(a1, a2, mtx_c, c33, mode):
    """
    Evaluate stress or tangent stiffness of the Mooney-Rivlin membrane.

    [1] Baoguo Wu, Xingwen Du and Huifeng Tan: A three-dimensional FE
    nonlinear analysis of membranes, Computers & Structures 59 (1996),
    no. 4, 601--605.
    """
    a12 = 2.0 * a1[..., 0, 0]
    a22 = 2.0 * a2[..., 0, 0]

    sh = mtx_c.shape
    sym = dim2sym(sh[2])

    c11 = mtx_c[..., 0, 0]
    c12 = mtx_c[..., 0, 1]
    c22 = mtx_c[..., 1, 1]
    pressure = c33 * (a12 + a22 * (c11 + c22))

    if mode == 0:
        out = nm.empty((sh[0], sh[1], sym, 1))

        # S_11, S_22, S_12.
        out[..., 0, 0] = -pressure * c22 * c33 + a12 + a22 * (c22 + c33)
        out[..., 1, 0] = -pressure * c11 * c33 + a12 + a22 * (c11 + c33)
        out[..., 2, 0] = +pressure * c12 * c33 - a22 * c12

    else:
        out = nm.empty((sh[0], sh[1], sym, sym))

        dp11 = a22 * c33 - pressure * c22 * c33
        dp22 = a22 * c33 - pressure * c11 * c33
        dp12 = 2.0 * pressure * c12 * c33

        # D_11, D_22, D_33
        out[..., 0, 0] = - 2.0 * ((a22 - pressure * c22) * c22 * c33**2
                                  + c33 * c22 * dp11)
        out[..., 1, 1] = - 2.0 * ((a22 - pressure * c11) * c11 * c33**2
                                  + c33 * c11 * dp22)
        out[..., 2, 2] = - a22 + pressure * (c33 + 2.0 * c12**2 * c33**2) \
                         + c12 * c33 * dp12

        # D_21, D_31, D_32
        out[..., 1, 0] = 2.0 * ((a22 - pressure * c33
                                 - (a22 - pressure * c11) * c22 * c33**2)
                                - c33 * c11 * dp11)
        out[..., 2, 0] = 2.0 * (-pressure * c12 * c22 * c33**2
                                + c12 * c33 * dp11)
        out[..., 2, 1] = 2.0 * (-pressure * c12 * c11 * c33**2
                                + c12 * c33 * dp22)

        out[..., 0, 1] = out[..., 1, 0]
        out[..., 0, 2] = out[..., 2, 0]
        out[..., 1, 2] = out[..., 2, 1]

        # D_12, D_13, D_23
        ## out[..., 0, 1] = 2.0 * ((a22 - pressure * c33
        ##                          - (a22 - pressure * c22) * c11 * c33**2)
        ##                         - c33 * c22 * dp22)
        ## out[..., 0, 2] = 2.0 * (a22 - pressure * c22) * c12 * c33**2 \
        ##                  - c33 * c22 * dp12
        ## out[..., 1, 2] = 2.0 * (a22 - pressure * c11) * c12 * c33**2 \
        ##                  - c33 * c11 * dp12

    return out

class TLMembraneTerm(Term):
    r"""
    Mooney-Rivlin membrane with plain stress assumption.

    The membrane has a uniform initial thickness :math:`h_0` and obeys a
    hyperelastic material law with strain energy by Mooney-Rivlin: :math:`\Psi
    = a_1 (I_1 - 3) + a_2 (I_2 - 3)`.

    :Arguments:
        - material_a1 : :math:`a_1`
        - material_a2 : :math:`a_2`
        - material_h0 : :math:`h_0`
        - virtual     : :math:`\ul{v}`
        - state       : :math:`\ul{u}`
    """
    name = 'dw_tl_membrane'
    arg_types = ('material_a1', 'material_a2', 'material_h0',
                 'virtual', 'state')
    arg_shapes = {'material_a1' : '1, 1', 'material_a2' : '1, 1',
                  'material_h0' : '1, 1',
                  'virtual' : ('D', 'state'), 'state' : 'D'}
    geometries = ['3_4', '3_8']
    integration = 'surface'

    @staticmethod
    def function(out, fun, *args):
        """
        Notes
        -----
        `fun` is either `weak_function` or `eval_function` according to
        evaluation mode.
        """
        return fun(out, *args)

    @staticmethod
    def weak_function(out, a1, a2, h0, mtx_c, c33, mtx_b, mtx_t, bfg, geo,
                      fmode):
        crt = eval_membrane_mooney_rivlin(a1, a2, mtx_c, c33, fmode)

        if fmode == 0:
            bts = dot_sequences(mtx_b, crt, 'ATB')

            status = geo.integrate(out, bts * h0)
            membranes.transform_asm_vectors(out, mtx_t)

        else:
            btd = dot_sequences(mtx_b, crt, 'ATB')
            btdb = dot_sequences(btd, mtx_b)

            stress = eval_membrane_mooney_rivlin(a1, a2, mtx_c, c33, 0)

            kts =  membranes.get_tangent_stress_matrix(stress, bfg)

            mtx_k = kts + btdb

            status = geo.integrate(out, mtx_k * h0)
            membranes.transform_asm_matrices(out, mtx_t)

        return status

    @staticmethod
    def eval_function(out, a1, a2, h0, mtx_c, c33, mtx_b, mtx_t, geo,
                      term_mode, fmode):

        if term_mode == 'strain':
            out_qp = membranes.get_green_strain_sym3d(mtx_c, c33)

        elif term_mode == 'stress':
            n_el, n_qp, dm, _ = mtx_c.shape
            dim = dm + 1
            sym = dim2sym(dim)
            out_qp = nm.zeros((n_el, n_qp, sym, 1), dtype=mtx_c.dtype)

            stress = eval_membrane_mooney_rivlin(a1, a2, mtx_c, c33, 0)
            out_qp[..., 0:2, 0] = stress[..., 0:2, 0]
            out_qp[..., 3, 0] = stress[..., 2, 0]

        status = geo.integrate(out, out_qp, fmode)
        out[:, 0, :, 0] = transform_data(out.squeeze(), mtx=mtx_t)

        return status

    def __init__(self, *args, **kwargs):
        Term.__init__(self, *args, **kwargs)

        igs = self.region.igs
        self.mtx_t = {}.fromkeys(igs, None)
        self.membrane_geo = {}.fromkeys(igs, None)
        self.bfg = {}.fromkeys(igs, None)

    def get_fargs(self, a1, a2, h0, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vv, vu = virtual, state

        ap, sg = self.get_approximation(vv)
        sd = ap.surface_data[self.region.name]

        ig = self.char_fun.ig
        if self.mtx_t[ig] is None:
            aux = membranes.describe_geometry(ig, vu.field,
                                              self.region, self.integral)
            self.mtx_t[ig], self.membrane_geo[ig] = aux
            # Transformed base function gradient w.r.t. material coordinates
            # in quadrature points.
            self.bfg[ig] = self.membrane_geo[ig].bfg

        mtx_t = self.mtx_t[ig]
        bfg = self.bfg[ig]
        geo = self.membrane_geo[ig]

        # Displacements of element nodes.
        vec_u = vu.get_state_in_region(self.region, igs=[self.char_fun.ig])
        el_u = vec_u[sd.leconn]

        # Transform displacements to the local coordinate system.
        # u_new = T^T u
        el_u_loc = dot_sequences(el_u, mtx_t, 'AB')
        ## print el_u_loc

        mtx_c, c33, mtx_b = membranes.describe_deformation(el_u_loc, bfg)

        if mode == 'weak':
            fmode = diff_var is not None

            return (self.weak_function,
                    a1, a2, h0, mtx_c, c33, mtx_b, mtx_t, bfg, geo, fmode)

        else:
            fmode = {'eval' : 0, 'el_avg' : 1, 'qp' : 2}.get(mode, 1)

            assert_(term_mode in ['strain', 'stress'])

            return (self.eval_function,
                    a1, a2, h0, mtx_c, c33, mtx_b, mtx_t, geo, term_mode, fmode)

    def get_eval_shape(self, a1, a2, h0, virtual, state,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)
        sym = dim * (dim + 1) / 2

        return (n_el, 1, sym, 1), state.dtype

########NEW FILE########
__FILENAME__ = terms_navier_stokes
import numpy as nm

from sfepy.linalg import dot_sequences
from sfepy.terms.terms import Term, terms

class DivGradTerm(Term):
    r"""
    Diffusion term.

    :Definition:

    .. math::
        \int_{\Omega} \nu\ \nabla \ul{v} : \nabla \ul{u} \mbox{ , }
        \int_{\Omega} \nu\ \nabla \ul{u} : \nabla \ul{w} \\
        \int_{\Omega} \nabla \ul{v} : \nabla \ul{u} \mbox{ , }
        \int_{\Omega} \nabla \ul{u} : \nabla \ul{w}

    :Arguments 1:
        - material : :math:`\nu` (viscosity, optional)
        - virtual  : :math:`\ul{v}`
        - state    : :math:`\ul{u}`

    :Arguments 2:
        - material    : :math:`\nu` (viscosity, optional)
        - parameter_1 : :math:`\ul{u}`
        - parameter_2 : :math:`\ul{w}`
    """
    name = 'dw_div_grad'
    arg_types = (('opt_material', 'virtual', 'state'),
                 ('opt_material', 'parameter_1', 'parameter_2'))
    arg_shapes = {'opt_material' : '1, 1', 'virtual' : ('D', 'state'),
                  'state' : 'D', 'parameter_1' : 'D', 'parameter_2' : 'D'}
    modes = ('weak', 'eval')

    function = staticmethod(terms.term_ns_asm_div_grad)

    def d_div_grad(self, out, grad1, grad2, mat, vg, fmode):
        sh = grad1.shape
        g1 = grad1.reshape((sh[0], sh[1], sh[2] * sh[3]))
        g2 = grad2.reshape((sh[0], sh[1], sh[2] * sh[3]))
        aux = mat * dot_sequences(g1[..., None], g2, 'ATB')[..., None]

        if fmode == 2:
            out[:] = aux
            status = 0

        else:
            status = vg.integrate(out, aux, fmode)

        return status

    def get_fargs(self, mat, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(state)

        if mat is None:
            n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)
            mat = nm.ones((1, n_qp, 1, 1), dtype=nm.float64)

        if mode == 'weak':
            if diff_var is None:
                grad = self.get(state, 'grad').transpose((0, 1, 3, 2))
                sh = grad.shape
                grad = grad.reshape((sh[0], sh[1], sh[2] * sh[3], 1))
                fmode = 0

            else:
                grad = nm.array([0], ndmin=4, dtype=nm.float64)
                fmode = 1

            return grad, mat, vg, fmode

        elif mode == 'eval':
            grad1 = self.get(virtual, 'grad')
            grad2 = self.get(state, 'grad')
            fmode = {'eval' : 0, 'el_avg' : 1, 'qp' : 2}.get(mode, 1)

            return grad1, grad2, mat, vg, fmode

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

    def get_eval_shape(self, mat, virtual, state,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(state)

        return (n_el, 1, 1, 1), state.dtype

    def set_arg_types(self):
        if self.mode == 'weak':
            self.function = terms.term_ns_asm_div_grad

        else:
            self.function = self.d_div_grad

class ConvectTerm(Term):
    r"""
    Nonlinear convective term.

    :Definition:

    .. math::
        \int_{\Omega} ((\ul{u} \cdot \nabla) \ul{u}) \cdot \ul{v}

    :Arguments:
        - virtual : :math:`\ul{v}`
        - state   : :math:`\ul{u}`
    """
    name = 'dw_convect'
    arg_types = ('virtual', 'state')
    arg_shapes = {'virtual' : ('D', 'state'), 'state' : 'D'}

    function = staticmethod(terms.term_ns_asm_convect)

    def get_fargs(self, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(state)

        grad = self.get(state, 'grad').transpose((0, 1, 3, 2)).copy()
        val_qp = self.get(state, 'val')

        fmode = diff_var is not None

        return grad, val_qp, vg, fmode

class LinearConvectTerm(Term):
    r"""
    Linearized convective term.

    :Definition:

    .. math::
        \int_{\Omega} ((\ul{b} \cdot \nabla) \ul{u}) \cdot \ul{v}

    .. math::
        ((\ul{b} \cdot \nabla) \ul{u})|_{qp}

    :Arguments:
        - virtual   : :math:`\ul{v}`
        - parameter : :math:`\ul{b}`
        - state     : :math:`\ul{u}`
    """
    name = 'dw_lin_convect'
    arg_types = ('virtual', 'parameter', 'state')
    arg_shapes = {'virtual' : ('D', 'state'), 'parameter' : 'D', 'state' : 'D'}

    function = staticmethod(terms.dw_lin_convect)

    def get_fargs(self, virtual, parameter, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(state)

        val_qp = self.get(parameter, 'val')

        if mode == 'weak':
            if diff_var is None:
                grad = self.get(state, 'grad').transpose((0, 1, 3, 2)).copy()
                fmode = 0

            else:
                grad = nm.array([0], ndmin=4, dtype=nm.float64)
                fmode = 1

            return grad, val_qp, vg, fmode

        elif mode == 'qp':
            grad = self.get(state, 'grad').transpose((0, 1, 3, 2)).copy()
            fmode = 2

            return grad, val_qp, vg, fmode

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

class StokesTerm(Term):
    r"""
    Stokes problem coupling term. Corresponds to weak forms of gradient and
    divergence terms. Can be evaluated.

    :Definition:

    .. math::
        \int_{\Omega} p\ \nabla \cdot \ul{v} \mbox{ , }
        \int_{\Omega} q\ \nabla \cdot \ul{u}
        \mbox{ or }
        \int_{\Omega} c\ p\ \nabla \cdot \ul{v} \mbox{ , }
        \int_{\Omega} c\ q\ \nabla \cdot \ul{u}

    :Arguments 1:
        - material : :math:`c` (optional)
        - virtual  : :math:`\ul{v}`
        - state    : :math:`p`

    :Arguments 2:
        - material : :math:`c` (optional)
        - state    : :math:`\ul{u}`
        - virtual  : :math:`q`

    :Arguments 3:
        - material    : :math:`c` (optional)
        - parameter_v : :math:`\ul{u}`
        - parameter_s : :math:`p`
    """
    name = 'dw_stokes'
    arg_types = (('opt_material', 'virtual', 'state'),
                 ('opt_material', 'state', 'virtual'),
                 ('opt_material', 'parameter_v', 'parameter_s'))
    arg_shapes = [{'opt_material' : '1, 1',
                   'virtual/grad' : ('D', None), 'state/grad' : 1,
                   'virtual/div' : (1, None), 'state/div' : 'D',
                   'parameter_v' : 'D', 'parameter_s' : 1},
                  {'opt_material' : None}]
    modes = ('grad', 'div', 'eval')

    @staticmethod
    def d_eval(out, coef, vec_qp, div, vvg):
        out_qp = coef * vec_qp * div

        status = vvg.integrate(out, out_qp)

        return status

    def get_fargs(self, coef, vvar, svar,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        if self.mode == 'grad':
            qp_var, qp_name = svar, 'val'

        else:
            qp_var, qp_name = vvar, 'div'

        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(vvar)
        if coef is None:
            coef = nm.ones((1, n_qp, 1, 1), dtype=nm.float64)

        if mode == 'weak':
            vvg, _ = self.get_mapping(vvar)
            svg, _ = self.get_mapping(svar)

            if diff_var is None:
                val_qp = self.get(qp_var, qp_name)
                fmode = 0

            else:
                val_qp = nm.array([0], ndmin=4, dtype=nm.float64)
                fmode = 1

            return coef, val_qp, svg, vvg, fmode

        elif mode == 'eval':
            vvg, _ = self.get_mapping(vvar)

            div = self.get(vvar, 'div')
            vec_qp = self.get(svar, 'val')

            return coef, vec_qp, div, vvg

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

    def get_eval_shape(self, coef, vvar, svar,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(vvar)

        return (n_el, 1, 1, 1), vvar.dtype

    def set_arg_types(self):
        self.function = {
            'grad' : terms.dw_grad,
            'div' : terms.dw_div,
            'eval' : self.d_eval,
        }[self.mode]

class GradTerm(Term):
    r"""
    Evaluate gradient of a scalar or vector field.

    Supports 'eval', 'el_avg' and 'qp' evaluation modes.

    :Definition:

    .. math::
        \int_{\Omega} \nabla p \mbox{ or } \int_{\Omega} \nabla \ul{w}

    .. math::
        \mbox{vector for } K \from \Ical_h: \int_{T_K} \nabla p /
        \int_{T_K} 1 \mbox{ or } \int_{T_K} \nabla \ul{w} /
        \int_{T_K} 1

    .. math::
        (\nabla p)|_{qp} \mbox{ or } \nabla \ul{w}|_{qp}

    :Arguments:
        - parameter : :math:`p` or :math:`\ul{w}`
    """
    name = 'ev_grad'
    arg_types = ('parameter',)
    arg_shapes = [{'parameter' : 1}, {'parameter' : 'D'}]

    @staticmethod
    def function(out, grad, vg, fmode):
        if fmode == 2:
            out[:] = grad
            status = 0

        else:
            status = vg.integrate(out, grad, fmode)

        return status

    def get_fargs(self, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(parameter)

        grad = self.get(parameter, 'grad')

        fmode = {'eval' : 0, 'el_avg' : 1, 'qp' : 2}.get(mode, 1)

        return grad, vg, fmode

    def get_eval_shape(self, parameter,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(parameter)

        if mode != 'qp':
            n_qp = 1

        return (n_el, n_qp, dim, n_c), parameter.dtype

class DivTerm(Term):
    r"""
    Evaluate divergence of a vector field.

    Supports 'eval', 'el_avg' and 'qp' evaluation modes.

    :Definition:

    .. math::
         \int_{\Omega} \nabla \cdot \ul{u}

    .. math::
         \mbox{vector for } K \from \Ical_h:
         \int_{T_K} \nabla \cdot \ul{u} / \int_{T_K} 1

    .. math::
        (\nabla \cdot \ul{u})|_{qp}

    :Arguments:
        - parameter : :math:`\ul{u}`
    """
    name = 'ev_div'
    arg_types = ('parameter',)
    arg_shapes = {'parameter' : 'D'}

    @staticmethod
    def function(out, div, vg, fmode):
        if fmode == 2:
            out[:] = div
            status = 0

        else:
            status = vg.integrate(out, div, fmode)

        return status

    def get_fargs(self, parameter,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(parameter)

        div = self.get(parameter, 'div')

        fmode = {'eval' : 0, 'el_avg' : 1, 'qp' : 2}.get(mode, 1)

        return div, vg, fmode

    def get_eval_shape(self, parameter,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(parameter)

        if mode != 'qp':
            n_qp = 1

        return (n_el, n_qp, 1, 1), parameter.dtype

class DivOperatorTerm(Term):
    r"""
    Weighted divergence term of a test function.

    :Definition:

    .. math::
        \int_{\Omega} \nabla \cdot \ul{v} \mbox { or } \int_{\Omega} c \nabla
        \cdot \ul{v}

    :Arguments:
        - material : :math:`c` (optional)
        - virtual  : :math:`\ul{v}`
    """
    name = 'dw_div'
    arg_types = ('opt_material', 'virtual')
    arg_shapes = [{'opt_material' : '1, 1', 'virtual' : ('D', None)},
                  {'opt_material' : None}]

    @staticmethod
    def function(out, mat, vg):
        div_bf = vg.bfg

        n_el, n_qp, dim, n_ep = div_bf.shape
        div_bf = div_bf.reshape((n_el, n_qp, dim * n_ep, 1))
        div_bf = nm.ascontiguousarray(div_bf)

        if mat is not None:
            status = vg.integrate(out, mat * div_bf)
        else:
            status = vg.integrate(out, div_bf)

        return status

    def get_fargs(self, mat, virtual,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(virtual)

        return mat, vg

class GradDivStabilizationTerm(Term):
    r"""
    Grad-div stabilization term ( :math:`\gamma` is a global stabilization
    parameter).

    :Definition:

    .. math::
        \gamma \int_{\Omega} (\nabla\cdot\ul{u}) \cdot (\nabla\cdot\ul{v})

    :Arguments:
        - material : :math:`\gamma`
        - virtual  : :math:`\ul{v}`
        - state    : :math:`\ul{u}`
    """
    name = 'dw_st_grad_div'
    arg_types = ('material', 'virtual', 'state')
    arg_shapes = {'material' : '1, 1', 'virtual' : ('D', 'state'),
                  'state' : 'D'}

    function = staticmethod(terms.dw_st_grad_div)

    def get_fargs(self, gamma, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(state)

        if diff_var is None:
            div = self.get(state, 'div')
            fmode = 0

        else:
            div = nm.array([0], ndmin=4, dtype=nm.float64)
            fmode = 1

        return div, gamma, vg, fmode

from sfepy.terms.terms_diffusion import LaplaceTerm
class PSPGPStabilizationTerm(LaplaceTerm):
    r"""
    PSPG stabilization term, pressure part ( :math:`\tau` is a local
    stabilization parameter), alias to Laplace term dw_laplace.

    :Definition:

    .. math::
        \sum_{K \in \Ical_h}\int_{T_K} \tau_K\ \nabla p \cdot \nabla q

    :Arguments:
        - material : :math:`\tau_K`
        - virtual  : :math:`q`
        - state    : :math:`p`
    """
    name = 'dw_st_pspg_p'

class PSPGCStabilizationTerm(Term):
    r"""
    PSPG stabilization term, convective part ( :math:`\tau` is a local
    stabilization parameter).

    :Definition:

    .. math::
        \sum_{K \in \Ical_h}\int_{T_K} \tau_K\ ((\ul{b} \cdot \nabla) \ul{u})
        \cdot \nabla q

    :Arguments:
        - material  : :math:`\tau_K`
        - virtual   : :math:`q`
        - parameter : :math:`\ul{b}`
        - state     : :math:`\ul{u}`
    """
    name = 'dw_st_pspg_c'
    arg_types = ('material', 'virtual', 'parameter', 'state')
    arg_shapes = {'material' : '1, 1', 'virtual' : (1, None),
                  'parameter' : 'D', 'state' : 'D'}

    function = staticmethod(terms.dw_st_pspg_c)

    def get_fargs(self, tau, virtual, parameter, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        sap, svg = self.get_approximation(virtual)
        vap, vvg = self.get_approximation(state)

        val_qp = self.get(parameter, 'val')
        conn = vap.get_connectivity(self.region, self.integration)

        if diff_var is None:
            fmode = 0

        else:
            fmode = 1

        return val_qp, state(), tau, svg, vvg, conn, fmode

class SUPGPStabilizationTerm(Term):
    r"""
    SUPG stabilization term, pressure part ( :math:`\delta` is a local
    stabilization parameter).

    :Definition:

    .. math::
        \sum_{K \in \Ical_h}\int_{T_K} \delta_K\ \nabla p\cdot ((\ul{b} \cdot
        \nabla) \ul{v})

    :Arguments:
        - material  : :math:`\delta_K`
        - virtual   : :math:`\ul{v}`
        - parameter : :math:`\ul{b}`
        - state     : :math:`p`
    """
    name = 'dw_st_supg_p'
    arg_types = ('material', 'virtual', 'parameter', 'state')
    arg_shapes = {'material' : '1, 1', 'virtual' : ('D', None),
                  'parameter' : 'D', 'state' : 1}

    function = staticmethod(terms.dw_st_supg_p)

    def get_fargs(self, delta, virtual, parameter, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vvg, _ = self.get_mapping(virtual)
        svg, _ = self.get_mapping(state)

        val_qp = self.get(parameter, 'val')

        if diff_var is None:
            grad = self.get(state, 'grad')
            fmode = 0

        else:
            grad = nm.array([0], ndmin=4, dtype=nm.float64)
            fmode = 1

        return val_qp, grad, delta, vvg, svg, fmode

class SUPGCStabilizationTerm(Term):
    r"""
    SUPG stabilization term, convective part ( :math:`\delta` is a local
    stabilization parameter).

    :Definition:

    .. math::
        \sum_{K \in \Ical_h}\int_{T_K} \delta_K\ ((\ul{b} \cdot \nabla)
        \ul{u})\cdot ((\ul{b} \cdot \nabla) \ul{v})

    :Arguments:
        - material  : :math:`\delta_K`
        - virtual   : :math:`\ul{v}`
        - parameter : :math:`\ul{b}`
        - state     : :math:`\ul{u}`
    """
    name = 'dw_st_supg_c'
    arg_types = ('material', 'virtual', 'parameter', 'state')
    arg_shapes = {'material' : '1, 1', 'virtual' : ('D', 'state'),
                  'parameter' : 'D', 'state' : 'D'}

    function = staticmethod(terms.dw_st_supg_c)

    def get_fargs(self, delta, virtual, parameter, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        ap, vg = self.get_approximation(virtual)

        val_qp = self.get(parameter, 'val')
        conn = ap.get_connectivity(self.region, self.integration)

        if diff_var is None:
            fmode = 0

        else:
            fmode = 1

        return val_qp, state(), delta, vg, conn, fmode

########NEW FILE########
__FILENAME__ = terms_new
"""
todo:
    - get row variable, col variable (if diff_var)
    - determine out shape
    - set current group to all variable arguments
    - loop over row/col dofs:
        - call term

? how to deal with components of (vector) variables?

(*) for given group, Variable has to be able to:
    - evaluate value in quadrature points
    - evaluate gradient in quadrature points
    - ? evaluate divergence in quadrature points
    - ? lazy evaluation, cache!

?? base function gradients in space elements stored now in terms - in
geometry, shared dict of geometries belongs to Equations
-> where to cache stuff? - in variables!
"""
import numpy as nm

from sfepy.base.base import output
from sfepy.terms.terms import Term, get_shape_kind
from sfepy.terms.utils import get_range_indices
from sfepy.mechanics.tensors import get_full_indices
from sfepy.linalg import dot_sequences as dot

class NewTerm(Term):

    def get_geometry_key(self, variable):
        is_trace = self.arg_traces[variable.name]
        geometry_type = self.geometry_types[variable.name]

        iname, region_name, ig = self.get_current_group()

        if is_trace:
            region, ig_map, ig_map_i = self.region.get_mirror_region()
            region_name = region.name
            ig = ig_map_i[ig]

        ap = variable.get_approximation(ig)
        key = (iname, region_name, geometry_type, ap.name)

        return key, ig

    def get_geometry(self, variable):
        key, ig = self.get_geometry_key(variable)

        geo = self.get_mapping(variable)[0]

        return geo, key, ig

    def set_current_group(self, ig):
        """
        Set current group for the term and all variables in its
        arguments.
        """
        self.char_fun.set_current_group(ig)

        shape_kind = get_shape_kind(self.integration)
        for var in self.get_variables():
            geo, geo_key, geo_ig = self.get_geometry(var)
            var.setup_bases(geo_key, geo_ig, geo, self.integral, shape_kind)
            var.set_current_group(geo_key, geo_ig)

    def integrate(self, val_qp, variable):
        shape_kind = get_shape_kind(self.integration)

        geo, _, _ = self.get_geometry(variable)

        sh = val_qp.shape
        val = nm.zeros((sh[0], 1, sh[2], sh[3]), dtype=val_qp.dtype)
        if shape_kind == 'volume':
            geo.integrate(val, val_qp)

        else:
            geo.integrate(val, val_qp)

        return val

    def evaluate(self, mode='eval', diff_var=None, **kwargs):
        shape_kind = get_shape_kind(self.integration)

        if mode == 'eval':
            var = self.get_variables()[0]

            val = 0.0
            for ig in self.iter_groups():
                args = self.get_args(**kwargs)

                val_qp = self(*args, **kwargs)
                _val = self.integrate(val_qp, var)
                val += self.sign * _val.sum()

        elif mode in ('el_avg', 'qp'):
            raise NotImplementedError()

        elif mode == 'weak':
            varr = self.get_virtual_variable()

            vals = []
            iels = []

            if diff_var is None:
                for ig in self.iter_groups():
                    args = self.get_args(**kwargs)

                    aux = varr.get_data_shape(ig, self.integral,
                                              shape_kind, self.region.name)
                    n_elr, n_qpr, dim, n_enr, n_cr = aux
                    n_row = n_cr * n_enr

                    shape = (n_elr, 1, n_row, 1)
                    val = nm.zeros(shape, dtype=varr.dtype)
                    for ir in varr.iter_dofs():
                        irs = slice(ir, ir + 1)

                        try:
                            val_qp = self(*args, **kwargs)

                        except ValueError:
                            output('%s term evaluation failed!' % self.name)
                            raise

                        _val = self.integrate(val_qp, varr)
                        val[..., irs, :] = _val

                    vals.append(self.sign * val)
                    iels.append((ig, nm.arange(n_elr, dtype=nm.int32)))

            else:
                varc = self.get_variables(as_list=False)[diff_var]

                for ig in self.iter_groups():
                    args = self.get_args(**kwargs)

                    aux = varr.get_data_shape(ig, self.integral,
                                              shape_kind, self.region.name)
                    n_elr, n_qpr, dim, n_enr, n_cr = aux
                    n_row = n_cr * n_enr

                    aux = varc.get_data_shape(ig, self.integral,
                                              shape_kind, self.region.name)
                    n_elc, n_qpc, dim, n_enc, n_cc = aux
                    n_col = n_cc * n_enc

                    shape = (n_elr, 1, n_row, n_col)
                    val = nm.zeros(shape, dtype=varr.dtype)
                    for ir in varr.iter_dofs():
                        irs = slice(ir, ir + 1)
                        for ic in varc.iter_dofs():
                            ics = slice(ic, ic + 1)
                            try:
                                val_qp = self(*args, **kwargs)

                            except ValueError:
                                output('%s term evaluation failed!' % self.name)
                                raise

                            _val = self.integrate(val_qp, varr)
                            val[..., irs, ics] = _val

                    vals.append(self.sign * val)
                    iels.append((ig, nm.arange(n_elr, dtype=nm.int32)))

        # Setup return value.
        if mode == 'eval':
            out = (val,)

        else:
            out = (vals, iels)

        # Hack: add zero status.
        out = out + (0,)

        if len(out) == 1:
            out = out[0]

        return out

class NewDiffusionTerm(NewTerm):
    """
    """
    name = 'dw_new_diffusion'
    arg_types = ('material', 'virtual', 'state')

    def __call__(self, mat, virtual, state, **kwargs):

        val = dot(virtual.grad(), dot(mat, state.grad()), 'ATB')

        return val

class NewMassScalarTerm(NewTerm):
    """
    """
    name = 'dw_new_mass_scalar'
    arg_types = ('virtual', 'state')

    def __call__(self, virtual, state, **kwargs):

        val = virtual.val() * state.val()

        return val

class NewMassTerm(NewTerm):
    """
    Works for both scalar and vector variables.
    """
    name = 'dw_new_mass'
    arg_types = ('virtual', 'state')

    def __call__(self, virtual, state, **kwargs):

        rindx = virtual.get_component_indices()
        cindx = state.get_component_indices()

        val = virtual.get_element_zeros()
        for ir, irs in rindx:
            for ic, ics in cindx:
                if ir == ic:
                    val += virtual.val(ir) * state.val(ic)

        return val

class NewLinearElasticTerm(NewTerm):
    """
    """
    name = 'dw_new_lin_elastic'
    arg_types = ('material', 'virtual', 'state')

    def __call__(self, mat, virtual, state, **kwargs):
        """
        Doubled out-of-diagonal strain entries!
        """
        rindx = virtual.get_component_indices()
        cindx = state.get_component_indices()

        kindx = lindx = get_range_indices(state.dim)
        fi = nm.array(get_full_indices(state.dim))

        val = virtual.get_element_zeros()
        for ir, irs in rindx:
            for ik, iks in kindx:
                irk = fi[ir, ik]
                irks = slice(irk, irk + 1)

                erk = virtual.grad(ir, ik)

                for ic, ics in cindx:
                    for il, ils in lindx:
                        icl = fi[ic, il]
                        icls = slice(icl, icl + 1)

                        ecl = state.grad(ic, il)

                        val += mat[..., irks, icls] * erk * ecl

        return val

########NEW FILE########
__FILENAME__ = terms_piezo
import numpy as nm

from sfepy.terms.terms import Term, terms

class PiezoCouplingTerm(Term):
    r"""
    Piezoelectric coupling term. Can be evaluated.

    :Definition:

    .. math::
        \int_{\Omega} g_{kij}\ e_{ij}(\ul{v}) \nabla_k p \mbox{ , }
        \int_{\Omega} g_{kij}\ e_{ij}(\ul{u}) \nabla_k q

    :Arguments 1:
        - material : :math:`g_{kij}`
        - virtual  : :math:`\ul{v}`
        - state    : :math:`p`

    :Arguments 2:
        - material : :math:`g_{kij}`
        - state    : :math:`\ul{u}`
        - virtual  : :math:`q`

    :Arguments 3:
        - material    : :math:`g_{kij}`
        - parameter_v : :math:`\ul{u}`
        - parameter_s : :math:`p`
    """
    name = 'dw_piezo_coupling'
    arg_types = (('material', 'virtual', 'state'),
                 ('material', 'state', 'virtual'),
                 ('material', 'parameter_v', 'parameter_s'))
    arg_shapes = {'material' : 'D, S',
                  'virtual/grad' : ('D', None), 'state/grad' : 1,
                  'virtual/div' : (1, None), 'state/div' : 'D',
                  'parameter_v' : 'D', 'parameter_s' : 1}
    modes = ('grad', 'div', 'eval')

    def get_fargs(self, mat, vvar, svar,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        if self.mode == 'grad':
            qp_var, qp_name = svar, 'grad'

        else:
            qp_var, qp_name = vvar, 'cauchy_strain'

        vvg, _ = self.get_mapping(vvar)

        if mode == 'weak':
            aux = nm.array([0], ndmin=4, dtype=nm.float64)
            if diff_var is None:
                # grad or strain according to mode.
                val_qp = self.get(qp_var, qp_name)
                fmode = 0

            else:
                val_qp = aux
                fmode = 1

            if self.mode == 'grad':
                strain, grad = aux, val_qp

            else:
                strain, grad = val_qp, aux
                fmode += 2

            return strain, grad, mat, vvg, fmode

        elif mode == 'eval':
            strain = self.get(vvar, 'cauchy_strain')
            grad = self.get(svar, 'grad')

            return strain, grad, mat, vvg

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

    def get_eval_shape(self, mat, vvar, svar,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(vvar)

        return (n_el, 1, 1, 1), vvar.dtype

    def set_arg_types( self ):
        self.function = {
            'grad' : terms.dw_piezo_coupling,
            'div' : terms.dw_piezo_coupling,
            'eval' : terms.d_piezo_coupling,
        }[self.mode]

########NEW FILE########
__FILENAME__ = terms_point
import numpy as nm

from sfepy.base.base import assert_
from sfepy.terms.terms import Term

class LinearPointSpringTerm(Term):
    r"""
    Linear springs constraining movement of FE nodes in a region; to use as a
    relaxed Dirichlet boundary conditions.

    :Definition:

    .. math::
        \ul{f}^i = -k \ul{u}^i \quad \forall \mbox{ FE node } i \mbox{ in
        a region }

    :Arguments:
        - material : :math:`k`
        - virtual  : :math:`\ul{v}`
        - state    : :math:`\ul{u}`
    """
    name = 'dw_point_lspring'
    arg_types = ('material', 'virtual', 'state')
    integration = 'point'

    @staticmethod
    def function(out, stiffness, vec, diff_var):
        if diff_var is None:
            out[:, 0, :, 0] = - stiffness * vec

        else:
            dim = vec.shape[1]
            eye = nm.eye(dim, dim, dtype=nm.float64)
            eye.shape = (1, 1) + eye.shape
            out[...] = - stiffness * eye

        return 0

    def get_fargs(self, mat, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vec = state.get_state_in_region(self.region)

        stiffness = mat['stiffness']

        return stiffness, vec, diff_var

class ConcentratedPointLoadTerm(Term):
    r"""
    Concentrated point load term.

    The load value must be given in form of a special material
    parameter (name prefixed with '.'), e.g. (in 2D)::

        'load' : ({'.val' : [0.0, 1.0]},)

    This term should be used with special care, as it bypasses the usual
    evaluation in quadrature points. It should only be used with nodal
    FE basis. The number of rows of the load must be equal to the number
    of nodes in the region and the number of columns equal to the field
    dimension.

    :Definition:

    .. math::
        \ul{f}^i = \ul{\bar f}^i \quad \forall \mbox{ FE node } i \mbox{ in
        a region }

    :Arguments:
        - material : :math:`\ul{\bar f}^i`
        - virtual  : :math:`\ul{v}`,
    """
    name = 'dw_point_load'
    arg_types = ('material', 'virtual')
    integration = 'point'

    @staticmethod
    def function(out, mat):
        out[:, 0, :, 0] = mat

        return 0

    def check_shapes(self, mat, virtual):
        mat = nm.asarray(mat)
        assert_(mat.shape[-1] == virtual.dim)

    def get_fargs(self, mat, virtual,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        return nm.asarray(mat),

########NEW FILE########
__FILENAME__ = terms_surface
import numpy as nm

from sfepy.base.base import assert_
from sfepy.terms.terms import Term, terms
from sfepy.linalg import dot_sequences
from sfepy.mechanics.contact_bodies import ContactPlane, ContactSphere
from sfepy.discrete.fem.extmods._geommech import geme_mulAVSB3py

##
# 22.08.2006, c
class LinearTractionTerm(Term):
    r"""
    Linear traction forces, where, depending on dimension of
    'material' argument, :math:`\ull{\sigma} \cdot \ul{n}` is
    :math:`\bar{p} \ull{I} \cdot \ul{n}` for a given scalar pressure,
    :math:`\ul{f}` for a traction vector, and itself for a stress tensor.

    :Definition:

    .. math::
        \int_{\Gamma} \ul{v} \cdot \ull{\sigma} \cdot \ul{n},
        \int_{\Gamma} \ul{v} \cdot \ul{n},

    :Arguments:
        - material : :math:`\ull{\sigma}`
        - virtual  : :math:`\ul{v}`
    """
    name = 'dw_surface_ltr'
    arg_types = (('opt_material', 'virtual'),
                 ('opt_material', 'parameter'))
    arg_shapes = [{'opt_material' : 'S, 1', 'virtual' : ('D', None),
                   'parameter' : 'D'},
                  {'opt_material' : 'D, 1'}, {'opt_material' : '1, 1'},
                  {'opt_material' : None}]
    modes = ('weak', 'eval')
    integration = 'surface'

    @staticmethod
    def d_fun(out, traction, val, sg):
        tdim = traction.shape[2]
        dim = val.shape[2]
        sym = (dim + 1) * dim / 2

        if tdim == 0:
            aux = dot_sequences(val, sg.normal, 'ATB')

        elif tdim == 1: # Pressure
            aux = dot_sequences(val, traction * sg.normal, 'ATB')

        elif tdim == dim: # Traction vector
            aux = dot_sequences(val, traction, 'ATB')

        elif tdim == sym: # Traction tensor
            trn, ret = geme_mulAVSB3py(traction, sg.normal)
            aux = dot_sequences(val, trn, 'ATB')

        status = sg.integrate(out, aux)
        return status

    def get_fargs(self, traction, virtual,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        sg, _ = self.get_mapping(virtual)

        if traction is None:
            traction = nm.zeros((0,0,0,0), dtype=nm.float64)

        if mode == 'weak':
            return traction, sg

        elif mode == 'eval':
            val = self.get(virtual, 'val')
            return traction, val, sg

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

    def get_eval_shape(self, traction, virtual,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(virtual)

        return (n_el, 1, 1, 1), virtual.dtype

    def set_arg_types( self ):
        if self.mode == 'weak':
            self.function = terms.dw_surface_ltr

        else:
            self.function = self.d_fun

class ContactPlaneTerm(Term):
    r"""
    Small deformation elastic contact plane term with penetration penalty.

    The plane is given by an anchor point :math:`\ul{A}` and a normal
    :math:`\ul{n}`. The contact occurs in points that orthogonally project onto
    the plane into a polygon given by orthogonal projections of boundary points
    :math:`\{\ul{B}_i\}`, :math:`i = 1, \dots, N_B` on the plane. In such
    points, a penetration distance :math:`d(\ul{u}) = (\ul{X} + \ul{u} -
    \ul{A}, \ul{n})` is computed, and a force :math:`f(d(\ul{u})) \ul{n}` is
    applied. The force depends on the non-negative parameters :math:`k`
    (stiffness) and :math:`f_0` (force at zero penetration):

    - If :math:`f_0 = 0`:

      .. math::

         f(d) = 0 \mbox{ for } d \leq 0 \;, \\
         f(d) = k d \mbox{ for } d > 0 \;.

    - If :math:`f_0 > 0`:

      .. math::

         f(d) = 0 \mbox{ for } d \leq -\frac{2 r_0}{k} \;, \\
         f(d) = \frac{k^2}{4 r_0} d^2 + k d + r_0
         \mbox{ for } -\frac{2 r_0}{k} < d \leq 0 \;, \\
         f(d) = k d + f_0 \mbox{ for } d > 0 \;.

      In this case the dependence :math:`f(d)` is smooth, and a (small) force
      is applied even for (small) negative penetrations: :math:`-\frac{2
      r_0}{k} < d \leq 0`.

    :Definition:

    .. math::
        \int_{\Gamma} \ul{v} \cdot f(d(\ul{u})) \ul{n}

    :Arguments:
        - material_f : :math:`[k, f_0]`
        - material_n : :math:`\ul{n}` (special)
        - material_a : :math:`\ul{A}` (special)
        - material_b : :math:`\{\ul{B}_i\}`, :math:`i = 1, \dots, N_B` (special)
        - virtual    : :math:`\ul{v}`
        - state      : :math:`\ul{u}`
    """
    name = 'dw_contact_plane'
    arg_types = ('material_f', 'material_n', 'material_a', 'material_b',
                 'virtual', 'state')
    arg_shapes = {'material_f' : '1, 2', 'material_n' : '.: D',
                  'material_a' : '.: D', 'material_b' : '.: N, D',
                  'virtual' : ('D', 'state'), 'state' : 'D'}
    geometries = ['3_4', '3_8']
    integration = 'surface'

    def __init__(self, *args, **kwargs):
        Term.__init__(self, *args, **kwargs)

        self.cp = None

    @staticmethod
    def function(out, force, normal, geo, fmode):
        bf = geo.bf[0]
        nbf = bf * normal[None, :, None]
        nbf.shape = (bf.shape[0], bf.shape[2] * normal.shape[0])

        if fmode == 0:
            out_qp = force * nbf[None, :, :, None]

        else:
            nbf2 = nbf[:, :, None] * nbf[:, None, :]
            out_qp = force * nbf2[None, :, :, :]

        status = geo.integrate(out, nm.ascontiguousarray(out_qp))

        return status

    @staticmethod
    def smooth_f(d, k, f0, a, eps, diff):
        ii = nm.where((d > eps) & (d <= 0.0))[0]

        if not diff:
            out = nm.where(d > 0.0, k * d + f0, 0.0)
            if len(ii):
                di = d[ii]
                out[ii] = a[ii] * di**2 + k[ii] * di + f0[ii]

        else:
            out = nm.where(d > 0.0, k, 0.0)
            if len(ii):
                out[ii] = 2 * a[ii] * d[ii] + k[ii]

        return out

    @staticmethod
    def _get_force_pars(force_pars, shape):
        k = force_pars[..., 0]
        f0 = force_pars[..., 1]
        k.shape = f0.shape = shape

        ir = f0 >= 1e-14
        eps = nm.where(ir, - 2.0 * f0 / k, 0.0)
        a = nm.zeros_like(eps)
        a[ir] = k[ir]**2 / (4.0 * f0[ir])

        return k, f0, eps, a

    def get_fargs(self, force_pars, normal, anchor, bounds, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        sg, _ = self.get_mapping(virtual)

        assert_((force_pars >= 0.0).all(),
                'force parameters must be non-negative!')

        if self.cp is None:
            self.cp = ContactPlane(anchor, normal, bounds)

        ig = self.char_fun.ig
        qps = self.get_physical_qps()
        qp_coors = qps.values[ig]
        u_qp = self.get(state, 'val').reshape(qp_coors.shape)

        # Deformed QP coordinates.
        coors = u_qp + qp_coors

        force = nm.zeros(coors.shape[0], dtype=nm.float64)

        k, f0, eps, a = self._get_force_pars(force_pars, force.shape)

        # Active points in contact change with displacements!
        ii = self.cp.mask_points(qp_coors)

        if diff_var is None:
            if ii.any():
                d = self.cp.get_distance(coors[ii])
                # Force in the plane normal direction.
                force[ii] = self.smooth_f(d, k[ii], f0[ii], a[ii], eps[ii], 0)

            fmode = 0

        else:
            if ii.any():
                d = self.cp.get_distance(coors[ii])
                # Force in the plane normal direction derivative.
                force[ii] = self.smooth_f(d, k[ii], f0[ii], a[ii], eps[ii], 1)

            fmode = 1

        force.shape = qps.shape[ig][:2] + (1, 1)

        return force, self.cp.normal, sg, fmode

class ContactSphereTerm(ContactPlaneTerm):
    r"""
    Small deformation elastic contact sphere term with penetration penalty.

    The sphere is given by a centre point :math:`\ul{C}` and a radius
    :math:`R`. The contact occurs in points that are closer to :math:`\ul{C}`
    than :math:`R`. In such points, a penetration distance :math:`d(\ul{u}) =
    R - ||\ul{X} + \ul{u} - \ul{C}||` is computed, and a force
    :math:`f(d(\ul{u})) \ul{n}(\ul{u})` is applied, where :math:`\ul{n}(\ul{u})
    = (\ul{X} + \ul{u} - \ul{C}) / ||\ul{X} + \ul{u} - \ul{C}||`. The force
    depends on the non-negative parameters :math:`k` (stiffness) and
    :math:`f_0` (force at zero penetration):

    - If :math:`f_0 = 0`:

      .. math::

         f(d) = 0 \mbox{ for } d \leq 0 \;, \\
         f(d) = k d \mbox{ for } d > 0 \;.

    - If :math:`f_0 > 0`:

      .. math::

         f(d) = 0 \mbox{ for } d \leq -\frac{2 r_0}{k} \;, \\
         f(d) = \frac{k^2}{4 r_0} d^2 + k d + r_0
         \mbox{ for } -\frac{2 r_0}{k} < d \leq 0 \;, \\
         f(d) = k d + f_0 \mbox{ for } d > 0 \;.

      In this case the dependence :math:`f(d)` is smooth, and a (small) force
      is applied even for (small) negative penetrations: :math:`-\frac{2
      r_0}{k} < d \leq 0`.

    :Definition:

    .. math::
        \int_{\Gamma} \ul{v} \cdot f(d(\ul{u})) \ul{n}(\ul{u})

    :Arguments:
        - material_f : :math:`[k, f_0]`
        - material_c : :math:`\ul{C}` (special)
        - material_r : :math:`R` (special)
        - virtual    : :math:`\ul{v}`
        - state      : :math:`\ul{u}`
    """
    name = 'dw_contact_sphere'
    arg_types = ('material_f', 'material_c', 'material_r', 'virtual', 'state')
    arg_shapes = {'material_f' : '1, 2', 'material_c' : '.: D',
                  'material_r' : '.: 1',
                  'virtual' : ('D', 'state'), 'state' : 'D'}
    geometries = ['3_4', '3_8']
    integration = 'surface'

    def __init__(self, *args, **kwargs):
        Term.__init__(self, *args, **kwargs)

        self.cs = None

    @staticmethod
    def function(out, force, normals, fd, geo, fmode):
        bf = geo.bf[0]
        nbf = bf * normals
        nbf.shape = (normals.shape[0], normals.shape[1],
                     bf.shape[2] * normals.shape[2])

        if fmode == 0:
            out_qp = force * nbf[..., None]

        else:
            nbf2 = nbf[..., None] * nbf[..., None, :]

            dim = normals.shape[2]
            n_ep = bf.shape[2]
            bb = bf[:, 0]
            vb = nm.zeros((bf.shape[0], dim, dim * n_ep))
            for ii in range(dim):
                vb[:, ii, ii*n_ep:(ii+1)*n_ep] = bb
            ee = nm.eye(3)[None, ...]
            eebf2 = dot_sequences(vb, dot_sequences(ee, vb), 'ATB')

            out_qp = force * nbf2
            if fd is not None:
                out_qp -= fd * (eebf2[None, :, :, :] - nbf2)

        status = geo.integrate(out, nm.ascontiguousarray(out_qp))

        return status

    def get_fargs(self, force_pars, centre, radius, virtual, state,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        sg, _ = self.get_mapping(virtual)

        assert_((force_pars >= 0.0).all(),
                'force parameters must be non-negative!')

        if self.cs is None:
            self.cs = ContactSphere(centre, radius)

        ig = self.char_fun.ig
        qps = self.get_physical_qps()
        qp_coors = qps.values[ig]
        u_qp = self.get(state, 'val').reshape(qp_coors.shape)

        # Deformed QP coordinates.
        coors = u_qp + qp_coors

        force = nm.zeros(coors.shape[0], dtype=nm.float64)
        normals = nm.zeros((coors.shape[0], 3), dtype=nm.float64)
        fd = None

        k, f0, eps, a = self._get_force_pars(force_pars, force.shape)

        # Active points in contact change with displacements!
        ii = self.cs.mask_points(coors, nm.abs(eps.min()))

        if diff_var is None:
            if ii.any():
                d, normals[ii] = self.cs.get_distance(coors[ii])
                force[ii] = self.smooth_f(d, k[ii], f0[ii], a[ii], eps[ii], 0)

            fmode = 0

        else:
            if ii.any():
                d, normals[ii] = self.cs.get_distance(coors[ii])
                # Force derivative.
                force[ii] = self.smooth_f(d, k[ii], f0[ii], a[ii], eps[ii], 1)

                # Force / (R - d).
                aux = self.smooth_f(d, k[ii], f0[ii], a[ii], eps[ii], 0)
                fd = nm.zeros_like(force)
                fd[ii] = aux / (self.cs.radius - d)
                fd.shape = qps.shape[ig][:2] + (1, 1)

            fmode = 1

        force.shape = qps.shape[ig][:2] + (1, 1)
        normals.shape = qps.shape[ig][:2] + (3, 1)

        return force, normals, fd, sg, fmode

class SufaceNormalDotTerm(Term):
    r"""
    "Scalar traction" term, (weak form).

    :Definition:

    .. math::
        \int_{\Gamma} q \ul{c} \cdot \ul{n}

    :Arguments:
        - material : :math:`\ul{c}`
        - virtual  : :math:`q`
    """
    name = 'dw_surface_ndot'
    arg_types = (('material', 'virtual'),
                 ('material', 'parameter'))
    arg_shapes = {'material' : 'D, 1', 'virtual' : (1, None), 'parameter' : 1}
    modes = ('weak', 'eval')
    integration = 'surface'

    @staticmethod
    def dw_fun(out, material, bf, sg):
        bf_t = nm.tile(bf.transpose((0, 1, 3, 2)), (out.shape[0], 1, 1, 1))
        bf_t = nm.ascontiguousarray(bf_t)
        aux = dot_sequences(material, sg.normal, 'ATB')
        status = sg.integrate(out, bf_t * aux)
        return status

    @staticmethod
    def d_fun(out, material, val, sg):
        aux = dot_sequences(material, sg.normal, 'ATB')
        status = sg.integrate(out, val * aux)
        return status

    def get_fargs(self, mat, virtual,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        sg, _ = self.get_mapping(virtual)

        if mode == 'weak':
            return mat, sg.bf, sg

        elif mode == 'eval':
            val = self.get(virtual, 'val')
            return mat, val, sg

        else:
            raise ValueError('unsupported evaluation mode in %s! (%s)'
                             % (self.name, mode))

    def get_eval_shape(self, mat, virtual,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(virtual)

        return (n_el, 1, 1, 1), virtual.dtype

    def set_arg_types( self ):
        if self.mode == 'weak':
            self.function = self.dw_fun

        else:
            self.function = self.d_fun

class SDSufaceIntegrateTerm(Term):
    r"""
    Sensitivity of scalar traction.

    :Definition:

    .. math::
        \int_{\Gamma} p \nabla \cdot \ul{\Vcal}

    :Arguments:
        - parameter : :math:`p`
        - parameter_mesh_velocity : :math:`\ul{\Vcal}`
    """
    name = 'd_sd_surface_integrate'
    arg_types = ('parameter', 'parameter_mesh_velocity')
    arg_shapes = {'parameter' : 1, 'parameter_mesh_velocity' : 'D'}
    integration = 'surface'

    @staticmethod
    def function(out, val_p, div_v, sg):
        status = sg.integrate(out, val_p * div_v)
        return status

    def get_fargs(self, par, par_v,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        sg, _ = self.get_mapping(par)

        val_p = self.get(par, 'val')
        div_v = self.get(par_v, 'div', integration='surface_extra')
        return val_p, div_v, sg

    def get_eval_shape(self, par, par_v,
                       mode=None, term_mode=None, diff_var=None, **kwargs):
        n_el, n_qp, dim, n_en, n_c = self.get_data_shape(par_v)

        return (n_el, 1, 1, 1), par.dtype

class SurfaceJumpTerm(Term):
    r"""
    Interface jump condition.

    :Definition:

    .. math::
        \int_{\Gamma} c\, q (p_1 - p_2)

    :Arguments:
        - material : :math:`c`
        - virtual  : :math:`q`
        - state_1  : :math:`p_1`
        - state_2  : :math:`p_2`
    """
    name = 'dw_jump'
    arg_types = ('opt_material', 'virtual', 'state_1', 'state_2')
    arg_shapes = [{'opt_material' : '1, 1', 'virtual' : (1, None),
                   'state_1' : 1, 'state_2' : 1},
                  {'opt_material' : None}]
    integration = 'surface'

    @staticmethod
    def function(out, jump, mul, bf1, bf2, sg, fmode):
        bf_t = nm.tile(sg.bf.transpose((0, 1, 3, 2)), (out.shape[0], 1, 1, 1))

        if fmode == 0:
            vec = bf_t * jump

        elif fmode == 1:
            vec = bf_t * bf1

        else:
            vec = - bf_t * bf2

        if mul is None:
            status = sg.integrate(out, vec)

        else:
            status = sg.integrate(out, mul * vec)

        return status

    def get_fargs(self, coef, virtual, state1, state2,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        sg, _ = self.get_mapping(virtual)
        sg1, _ = self.get_mapping(state1)
        sg2, _ = self.get_mapping(state2)

        if diff_var is None:
            val1 = self.get(state1, 'val')
            val2 = self.get(state2, 'val')
            jump = val1 - val2
            fmode = 0

        else:
            jump = None

            if diff_var == state1.name:
                fmode = 1

            else:
                fmode = 2

        return jump, coef, sg1.bf, sg2.bf, sg, fmode

########NEW FILE########
__FILENAME__ = terms_th
import numpy as nm

from sfepy.base.base import Struct
from sfepy.terms.terms import Term

class THTerm(Term):
    """
    Base class for terms depending on time history (fading memory
    terms).
    """

    def eval_real(self, shape, fargs, mode='eval', term_mode=None,
                  diff_var=None, **kwargs):

        if diff_var is None:
            if mode == 'eval':
                out = 0.0

            else:
                out = nm.zeros(shape, dtype=nm.float64)

            iter_kernel = fargs
            for ii, fargs in iter_kernel():
                out1, status = Term.eval_real(self, shape, fargs, mode=mode,
                                              term_mode=term_mode,
                                              diff_var=diff_var, **kwargs)
                out += out1

        else:
            out, status = Term.eval_real(self, shape, fargs, mode=mode,
                                         term_mode=term_mode,
                                         diff_var=diff_var, **kwargs)

        return out, status

class ETHTerm(Term):
    """
    Base class for terms depending on time history with exponential
    convolution kernel (fading memory terms).
    """

    def get_eth_data(self, key, state, decay, values):
        step_cache = state.evaluate_cache.setdefault('eth', {})
        cache = step_cache.setdefault(None, {})

        data_key = key + (self.arg_derivatives[state.name],)
        if data_key in cache:
            out = cache[data_key]
            out.values = values

        else:
            out = Struct(history=nm.zeros_like(values),
                         values=values,
                         decay=decay,
                         __advance__=self.advance_eth_data)
            cache[data_key] = out

        return out

    def advance_eth_data(self, ts, data):
        data.history[:] = data.decay * (data.history + data.values)

########NEW FILE########
__FILENAME__ = terms_volume
from sfepy.terms.terms import Term, terms

class LinearVolumeForceTerm(Term):
    r"""
    Vector or scalar linear volume forces (weak form) --- a right-hand side
    source term.

    :Definition:

    .. math::
        \int_{\Omega} \ul{f} \cdot \ul{v} \mbox{ or } \int_{\Omega} f q

    :Arguments:
        - material : :math:`\ul{f}` or :math:`f`
        - virtual  : :math:`\ul{v}` or :math:`q`
    """
    name = 'dw_volume_lvf'
    arg_types = ('material', 'virtual')
    arg_shapes = [{'material' : 'D, 1', 'virtual' : ('D', None)},
                  {'material' : '1, 1', 'virtual' : (1, None)}]

    function = staticmethod(terms.dw_volume_lvf)

    def get_fargs(self, mat, virtual,
                  mode=None, term_mode=None, diff_var=None, **kwargs):
        vg, _ = self.get_mapping(virtual)

        return mat, vg

########NEW FILE########
__FILENAME__ = utils
import numpy as nm

def check_finiteness(data, info):
    is_finite = nm.isfinite(data)
    if not is_finite.all():
        ii = nm.where(is_finite == False)
        print ii
        print data[ii]
        msg = 'infinite %s!, see above' % info
        raise ValueError(msg)

def get_range_indices(num):
    """
    Return indices and slices in given range.

    Returns
    -------
    indx : list of tuples
        The list of `(ii, slice(ii, ii + 1))` of the indices. The first
        item is the index itself, the second item is a convenience slice
        to index components of material parameters.
    """
    indx = [(ii, slice(ii, ii + 1)) for ii in range(num)]

    return indx

########NEW FILE########
__FILENAME__ = version
# SfePy version
__version__ = '2014.2'

# "Minimal" supported versions.
NUMPY_MIN_VERSION = '1.3'
SCIPY_MIN_VERSION = '0.7'
MATPLOTLIB_MIN_VERSION = '0.99.0'
PYPARSING_MIN_VERSION = '1.5.0'
PYTABLES_MIN_VERSION = '2.1.2'
IPYTHON_MIN_VERSION = '0.10.0'
MAYAVI_MIN_VERSION = '3.3.0'
SYMPY_MIN_VERSION = '0.6.7'
CYTHON_MIN_VERSION = '0.14.1'

def get_basic_info(version=__version__):
    """
    Return SfePy installation directory information. Append current git
    commit hash to `version`.
    """
    import os.path as op
    from sfepy import Config

    # If installed, up_dir is '.', otherwise (in (git) source directory) '..'.
    for up_dir in ['..', '.']:
        top_dir = op.normpath(op.join(op.dirname(__file__), up_dir))
        aux = op.join(top_dir, 'README')
        if op.isfile(aux):
            break
    else:
        raise RuntimeError('cannot determine SfePy top level directory!')

    config = Config()
    if not config.is_release():
        # Append current git commit hash to __version__.
        master = op.join(top_dir, '.git/refs/heads/master')
        if op.isfile(master):
            fd = open(master, 'r')
            version += '-git-%s' % fd.readline().strip()
            fd.close()

    in_source_tree = up_dir == '..'

    return version, top_dir, in_source_tree

__version__, top_dir, in_source_tree = get_basic_info(__version__)

########NEW FILE########
__FILENAME__ = shaper
#!/usr/bin/env python
# 06.04.2005, c
# 16.06.2005
from optparse import OptionParser

import numpy as nm

import sfepy
from sfepy.base.base import output, remap_dict, Struct, IndexedStruct
from sfepy.base.conf import ProblemConf, get_standard_keywords
from sfepy.discrete.evaluate import BasicEvaluator
import sfepy.base.ioutils as io
import sfepy.optimize.shape_optim as so
from sfepy.discrete.problem import Problem
from sfepy.solvers import Solver

def solve_stokes(dpb, equations_stokes, nls_conf):
    dpb.set_equations(equations_stokes)
    dpb.time_update(None)

    output('solving Stokes problem...')
    state = dpb.solve(nls_conf=nls_conf)
    output('...done')

    return state

def solve_navier_stokes(conf, options):
    opts = conf.options

    dpb = Problem.from_conf(conf, init_equations=False)
    equations = getattr(conf, '_'.join(('equations_direct', opts.problem)))
    dpb.set_equations(equations)

    ls_conf = dpb.get_solver_conf( opts.ls )
    nls_conf = dpb.get_solver_conf(opts.nls_direct)

    method = opts.direct_method
    if method == 'stationary':
        data = {}
        dpb.time_update(None)
        state_dp = dpb.solve(nls_conf=nls_conf)

    elif method == 'transient':
        ls = Solver.any_from_conf( ls_conf )
        ts_conf = dpb.get_solver_conf( opts.ts_direct )

        data = {'ts' : Struct( dt = ts_conf.dt )}

        # Plug in mass term.
        mequations = {}
        for key, eq in equations.iteritems():
            if 'dw_div_grad' in eq:
                eq = '+'.join( (ts_conf.mass_term, eq) ).replace( '++', '+')
            mequations[key] = eq

        if ts_conf.stokes_init:
            state_dp0 = solve_stokes( dpb, conf.equations_direct_stokes, nls_conf )
            dpb.set_equations( mequations )
        else:
            dpb.set_equations( mequations )
            state_dp0 = dpb.create_state()
            dpb.time_update( None )
            state_dp0.apply_ebc()

        from sfepy.base.log import Log

        log = Log.from_conf( Struct( is_plot = True ),
                            ([r'$||u||$'], [r'$||p||$']) )

        output( 'Navier-Stokes...' )
        ev = BasicEvaluator( dpb, ts = Struct( dt = ts_conf.dt ) )
        nls = Solver.any_from_conf( nls_conf, evaluator = ev, lin_solver = ls )

        n_step = ts_conf.n_step
        step = 0
        while 1:
            for ii in xrange( n_step ):
                output( step )

                vec_u = state_dp0('w')
                vec_p = state_dp0('r')
                log( nm.linalg.norm( vec_u ), nm.linalg.norm( vec_p ) )

                dpb.variables.set_data_from_state('w_0', state_dp0(), 'w')
                vec_dp = nls( state_dp0() )

                step += 1
                state_dp = state_dp0.copy()
                state_dp.set_reduced(vec_dp)

                state_dp0 = state_dp

            if ts_conf.interactive:
                try:
                    n_step = int( raw_input( 'continue: ' ) )
                    if n_step <= 0: break
                except:
                    break

        vec_u = state_dp('w')
        vec_p = state_dp('r')
        log( nm.linalg.norm( vec_u ), nm.linalg.norm( vec_p ), finished = True )

    else:
        raise 'unknown Navier-Stokes solution method (%s)!'  % method

    return dpb, state_dp, data

def solve_generic_direct(conf, options):
    opts = conf.options

    dpb = Problem.from_conf(conf, init_equations=False)
    equations = getattr(conf, '_'.join(('equations_direct', opts.problem)))
    dpb.set_equations(equations)

    dpb.time_update(None)

    nls_conf = dpb.get_solver_conf(opts.nls_direct)
    state_dp = dpb.solve(nls_conf=nls_conf)

    return dpb, state_dp, {}

##
# c: 22.11.2006, r: 15.04.2008
def solve_direct( conf, options ):
    """
    Solve the direct (nonlinear) problem.
    """
    opts = conf.options
    if hasattr( opts, 'problem' ):
        if opts.problem == 'navier_stokes':
            dpb, state_dp, data = solve_navier_stokes( conf, options )
        else:
            output( 'unknown problem type (%s), using generic solver.'\
                    % opts.problem )
            dpb, state_dp, data = solve_generic_direct( conf, options )
    else: # Generic direct problem.
        dpb, state_dp, data = solve_generic_direct( conf, options )

    trunk = io.get_trunk( conf.filename_mesh )
    dpb.save_state( trunk + '_direct.vtk', state_dp )

    if options.dump_filename is not None:
        import tables as pt
        import numarray as nar

        fd = pt.openFile( options.dump_filename, mode = 'w',
                          title = "Dump file" )
        out = state_dp.create_output_dict()
        for key, val in out.iteritems():
            fd.createArray( fd.root, key, nar.asarray( val.data ),
                            '%s data' % val.mode )
        fd.close()

    if options.pert_mesh_filename is not None:
        coors0 = dpb.get_mesh_coors()
        # !!!
        # 'u' is here for displacements of le.py!
        vec_u = state_dp('u').copy()
        vec_u = vec_u.reshape( coors0.shape )
        coors = coors0 + vec_u
        dpb.set_mesh_coors( coors )
        dpb.domain.mesh.write( options.pert_mesh_filename, io = 'auto' )

    return dpb, state_dp, data

def solve_adjoint(conf, options, dpb, state_dp, data):
    """
    Solve the adjoint (linear) problem.
    """
    opts = conf.options

    if dpb:
        apb = dpb.copy('adjoint')

    else:
        apb = Problem.from_conf(conf, init_equations=False)

    equations = getattr(conf, '_'.join(('equations_adjoint',
                                        opts.problem,
                                        opts.objective_function)))
    apb.set_equations(equations)
    apb.time_update(None)
    apb.ebcs.zero_dofs()
    apb.update_equations(None, ebcs=apb.ebcs)

    var_data = state_dp.get_parts()
    var_data = remap_dict(var_data, opts.var_map)

    nls_conf = apb.get_solver_conf(opts.nls_adjoint)
    state_ap = apb.solve(nls_conf=nls_conf, var_data=var_data)

    trunk = io.get_trunk(conf.filename_mesh)
    apb.save_state(trunk + '_adjoint.vtk', state_ap)

    shape_opt = so.ShapeOptimFlowCase.from_conf(conf, dpb, apb)

    if options.test is not None:
        ##
        # Test shape sensitivity.
        if shape_opt.test_terms_if_test:
            so.test_terms([options.test], opts.term_delta, shape_opt,
                          var_data, state_ap)

        shape_opt.check_sensitivity([options.test], opts.delta,
                                    var_data, state_ap)
    ##
    # Compute objective function.
    val = shape_opt.obj_fun(state_dp)
    print 'actual obj_fun:', val

    ##
    # Compute shape sensitivity.
    vec_sa = shape_opt.sensitivity(var_data, state_ap)
    print 'actual sensitivity:', vec_sa

##
# c: 22.11.2006, r: 15.04.2008
def solve_optimize( conf, options ):
    opts = conf.options
    trunk = io.get_trunk( conf.filename_mesh )
    data = {}

    dpb = Problem.from_conf(conf, init_equations=False)
    equations = getattr( conf, '_'.join( ('equations_direct', opts.problem) ) )

    dpb.set_equations( equations )

    dpb.name = 'direct'
    dpb.time_update(None)

    apb = dpb.copy('adjoint')
    equations = getattr( conf, '_'.join( ('equations_adjoint',
                                          opts.problem,
                                          opts.objective_function) ) )

    apb.set_equations( equations )
    apb.time_update(None)
    apb.ebcs.zero_dofs()
    apb.update_equations(None, ebcs=apb.ebcs)

    ls_conf = dpb.get_solver_conf(opts.ls)
    dnls_conf = dpb.get_solver_conf(opts.nls_direct)
    anls_conf = dpb.get_solver_conf(opts.nls_adjoint)
    opt_conf = dpb.get_solver_conf(opts.optimizer)

    dpb.init_solvers(ls_conf=ls_conf, nls_conf=dnls_conf)

    apb.init_solvers(ls_conf=ls_conf, nls_conf=anls_conf)

    shape_opt = so.ShapeOptimFlowCase.from_conf(conf, dpb, apb)
    design0 = shape_opt.dsg_vars.val
    shape_opt.cache = Struct(design=design0 + 100,
                             state=None,
                             i_mesh=-1)

    opt_status = IndexedStruct()
    optimizer = Solver.any_from_conf(opt_conf,
                                     obj_fun=so.obj_fun,
                                     obj_fun_grad=so.obj_fun_grad,
                                     status=opt_status,
                                     obj_args=(shape_opt, opts))

    ##
    # State problem solution for the initial design.
    vec_dp0 = so.solve_problem_for_design(dpb, design0, shape_opt, opts)

    dpb.save_state( trunk + '_direct_initial.vtk', vec_dp0 )

    ##
    # Optimize.
    des = optimizer( design0 )
    print opt_status

    ##
    # Save final state (for "optimal" design).
    dpb.domain.mesh.write( trunk + '_opt.mesh', io = 'auto' )
    dpb.save_state(trunk + '_direct_current.vtk', shape_opt.cache.state)

    print des

usage = """%prog [options] filename_in"""

help = {
    'server_mode' :
    "run in server mode [default: %default], N/A",
    'adjoint' :
    "solve adjoint problem [default: %default]",
    'direct' :
    "solve direct problem [default: %default]",
    'test' :
    "test sensitivity by finite difference,"
    " using design variable idsg; switches on -a, -d",
    'dump' :
    "dump direct problem state to filename",
    'pert':
    "save displacement-perturbed mesh to filename",
    'optimize' :
    "full shape optimization problem",
}

##
# created:       13.06.2005
# last revision: 15.04.2008
def main():
    parser = OptionParser(usage = usage, version = "%prog " + sfepy.__version__)
    parser.add_option( "-s", "--server",
                       action = "store_true", dest = "server_mode",
                       default = False, help = help['server_mode'] )
    parser.add_option( "-a", "--adjoint",
                       action = "store_true", dest = "adjoint",
                       default = False, help = help['adjoint'] )
    parser.add_option( "-d", "--direct",
                       action = "store_true", dest = "direct",
                       default = False, help = help['direct'] )
    parser.add_option( "-t", "--test", type = int, metavar = 'idsg',
                       action = "store", dest = "test",
                       default = None, help = help['test'] )
    parser.add_option( "", "--dump", metavar = 'filename',
                       action = "store", dest = "dump_filename",
                       default = None, help = help['dump'] )
    parser.add_option( "", "--pert-mesh", metavar = 'filename',
                       action = "store", dest = "pert_mesh_filename",
                       default = None, help = help['pert'] )
    parser.add_option( "-f", "--full",
                       action = "store_true", dest = "optimize",
                       default = False, help = help['optimize'] )

    options, args = parser.parse_args()

    if options.test is not None:
        options.adjoint = options.direct = True

    if options.optimize:
        options.adjoint = options.direct = False

    if ((len( args ) == 1)
        and (options.direct or options.adjoint or options.optimize)):
        filename_in = args[0];
    else:
        parser.print_help(),
        return

    required, other = get_standard_keywords()
    required.remove('equations')
    if options.adjoint:
        required += ['equations_adjoint_.*', 'filename_vp',
                     'equations_direct_.*']
        options.direct = True
    elif options.direct:
        required += ['equations_direct_.*']
    elif options.optimize:
        required += ['equations_direct_.*', 'equations_adjoint_.*',
                     'equations_sensitivity_.*',
                     'filename_vp']

    conf = ProblemConf.from_file( filename_in, required, other )

    if options.direct:
        dpb, state_dp, data = solve_direct( conf, options )
    else:
        dpb, state_dp, data = None, None, None

    if options.adjoint:
        solve_adjoint( conf, options, dpb, state_dp, data )

    if options.optimize:
        solve_optimize( conf, options )

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = simple
#!/usr/bin/env python
# 12.01.2007, c
"""
Solve partial differential equations given in a SfePy problem definition file.

Example problem definition files can be found in ``examples/`` directory of the
SfePy top-level directory. This script works with all the examples except those
in ``examples/standalone/``.

Both normal and parametric study runs are supported. A parametric study allows
repeated runs for varying some of the simulation parameters - see
``examples/diffusion/poisson_parametric_study.py`` file.
"""
from optparse import OptionParser

import sfepy
from sfepy.base.base import output
from sfepy.base.conf import ProblemConf, get_standard_keywords
from sfepy.applications import PDESolverApp

def print_terms():
    import sfepy.terms as t
    tt = t.term_table
    print 'Terms: %d available:' % len(tt)
    print sorted(tt.keys())

def print_solvers():
    from sfepy.solvers import solver_table
    print 'Solvers: %d available:' % len(solver_table)
    print sorted(solver_table.keys())

usage = """%prog [options] filename_in\n""" + __doc__.rstrip()

help = {
    'conf' :
    'override problem description file items, written as python'
    ' dictionary without surrounding braces',
    'options' : 'override options item of problem description,'
    ' written as python dictionary without surrounding braces',
    'define' : 'pass given arguments written as python dictionary'
    ' without surrounding braces to define() function of problem description'
    ' file',
    'filename' :
    'basename of output file(s) [default: <basename of input file>]',
    'output_format' :
    'output file format, one of: {vtk, h5} [default: vtk]',
    'log' :
    'log all messages to specified file (existing file will be overwritten!)',
    'quiet' :
    'do not print any messages to screen',
    'save_ebc' :
    'save a zero solution with applied EBCs (Dirichlet boundary conditions)',
    'save_ebc_nodes' :
    'save a zero solution with added non-zeros in EBC (Dirichlet boundary'
    ' conditions) nodes - scalar variables are shown using colors,'
    ' vector variables using arrows with non-zero components corresponding'
    ' to constrained components',
    'save_regions' :
    'save problem regions as meshes',
    'save_regions_as_groups' :
    'save problem regions in a single mesh but mark them by using different'
    ' element/node group numbers',
    'save_field_meshes' :
    'save meshes of problem fields (with extra DOF nodes)',
    'solve_not' :
    'do not solve (use in connection with --save-*)',
    'list' :
    'list data, what can be one of: {terms, solvers}',
}

def main():
    parser = OptionParser(usage=usage, version='%prog ' + sfepy.__version__)
    parser.add_option('-c', '--conf', metavar='"key : value, ..."',
                      action='store', dest='conf', type='string',
                      default=None, help= help['conf'])
    parser.add_option('-O', '--options', metavar='"key : value, ..."',
                      action='store', dest='app_options', type='string',
                      default=None, help=help['options'])
    parser.add_option('-d', '--define', metavar='"key : value, ..."',
                      action='store', dest='define_args', type='string',
                      default=None, help=help['define'])
    parser.add_option('-o', '', metavar='filename',
                      action='store', dest='output_filename_trunk',
                      default=None, help=help['filename'])
    parser.add_option('', '--format', metavar='format',
                      action='store', dest='output_format',
                      default=None, help=help['output_format'])
    parser.add_option('', '--log', metavar='file',
                      action='store', dest='log',
                      default=None, help=help['log'])
    parser.add_option('-q', '--quiet',
                      action='store_true', dest='quiet',
                      default=False, help=help['quiet'])
    parser.add_option('', '--save-ebc',
                      action='store_true', dest='save_ebc',
                      default=False, help=help['save_ebc'])
    parser.add_option('', '--save-ebc-nodes',
                      action='store_true', dest='save_ebc_nodes',
                      default=False, help=help['save_ebc_nodes'])
    parser.add_option('', '--save-regions',
                      action='store_true', dest='save_regions',
                      default=False, help=help['save_regions'])
    parser.add_option('', '--save-regions-as-groups',
                      action='store_true', dest='save_regions_as_groups',
                      default=False, help=help['save_regions_as_groups'])
    parser.add_option('', '--save-field-meshes',
                      action='store_true', dest='save_field_meshes',
                      default=False, help=help['save_field_meshes'])
    parser.add_option('', '--solve-not',
                      action='store_true', dest='solve_not',
                      default=False, help=help['solve_not'])
    parser.add_option('', '--list', metavar='what',
                      action='store', dest='_list',
                      default=None, help=help['list'])

    options, args = parser.parse_args()

    if (len(args) == 1):
        filename_in = args[0];
    else:
        if options._list == 'terms':
            print_terms()

        elif options._list == 'solvers':
            print_solvers()

        else:
            parser.print_help(),
        return

    output.set_output(filename=options.log,
                      quiet=options.quiet,
                      combined=options.log is not None)

    required, other = get_standard_keywords()
    if options.solve_not:
        required.remove('equations')
        required.remove('solver_[0-9]+|solvers')
        other.extend(['equations'])

    conf = ProblemConf.from_file_and_options(filename_in, options,
                                             required, other,
                                             define_args=options.define_args)

    opts = conf.options
    output_prefix = opts.get('output_prefix', 'sfepy:')

    app = PDESolverApp(conf, options, output_prefix)
    if hasattr(opts, 'parametric_hook'): # Parametric study.
        parametric_hook = conf.get_function(opts.parametric_hook)
        app.parametrize(parametric_hook)
    app()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = site_cfg_template
##
# Template file for site configuration - copy it to site_cfg.py:
# $ cp site_cfg_template.py site_cfg.py

# Force python version (e.g. '2.4'), or left 'auto' for autodetection.
python_version = 'auto'

# Operating system - one of 'posix', 'windows' or None for automatic
# determination.
system = None

# Extra flags added to the flags supplied by distutils to compile C
# extension modules.
compile_flags = '-g -O2'

# Extra flags added to the flags supplied by distutils to link C
# extension modules.
link_flags = ''

# Can be '' or one or several from '-DDEBUG_FMF', '-DDEBUG_MESH'. For
# developers internal use only.
debug_flags = ''

# Sphinx documentation uses numpydoc extension. Set the path here in case it is
# not installed in a standard location.
numpydoc_path = None

# True for a release, False otherwise. If False, current git commit hash
# is appended to version string, if the sources are in a repository.
is_release = False

# Tetgen executable path.
tetgen_path = '/usr/bin/tetgen'

########NEW FILE########
__FILENAME__ = sympy_operators
import sympy as sp
from sympy import sin, cos, Plot, Basic, Symbol, sympify, lambdify,\
     symbols
from sympy.abc import x, y, z, t

from numpy import arange, zeros
import numpy as nm

dim = 3
def set_dim( dim ):
    globals()['dim'] = dim
    
def default_space_variables( variables ):
    if variables is None:
        variables = [x, y, z][:dim]
    return variables

def grad( f, variables = None ):
    variables = default_space_variables( variables )
    n_var = len( variables )
#    import pdb; pdb.set_trace()
    f = sp.sympify( f )

    out = sp.zeros((n_var, 1))
    for iv, var in enumerate( variables ):
       out[iv,0] = f.diff( var )
    return out

##
# c: 09.06.2008, r: 09.06.2008
def grad_v( f, variables = None ):
    variables = default_space_variables( variables )
    n_var = len( variables )

    f = sympify( f )
    out = zeros( (n_var,) + f.shape )
    for iv, var in enumerate( variables ):
       out[iv,...] = f.diff( var )
    return out

def div( field, variables = None ):
    variables = default_space_variables( variables )
    n_var = len( variables )

    field = list( field )
    assert len( field ) == n_var

    out = 0
    for f_i, x_i in zip( field, variables ):
        out += sp.sympify( f_i ).diff( x_i )
#    import pdb; pdb.set_trace()
    return out

def laplace( f, variables = None ):
    return div( grad( f, variables ), variables)

def boundary(f, variables):
    lap = laplace(f, variables)
    l = lambdify(lap, variables)
    a = 5.
    for x in arange(-a, a, 0.1):
        y = -a
        print x, y, l(x, y)

if __name__ == '__main__':
    f = sin(x)*cos(y)
    boundary(f, [x, y])
    #Plot(f, [x, -10, 10], [y, -10, 10])

    set_dim( 2 )

    f = 'sin( 3.0 * x ) * cos( 4.0 * y )'
    A = sp.Matrix( [[1, 0.1], [0.1, 2]] )

    gf = grad( f )
    agf = A * gf

########NEW FILE########
__FILENAME__ = tests_basic
"""
This module is not a test file. It contains classes grouping some common
functionality, that is used in several test files.
"""
from sfepy.base.base import IndexedStruct
from sfepy.base.testing import TestCommon
import os.path as op

class NLSStatus(IndexedStruct):
    """
    Custom nonlinear solver status storing stopping condition of all
    time steps.
    """
    def __setitem__(self, key, val):
        IndexedStruct.__setitem__(self, key, val)
        if key == 'condition':
            self.conditions.append(val)

class TestInput(TestCommon):
    """Test that an input file works. See test_input_*.py files."""

    @staticmethod
    def from_conf(conf, options, cls=None):
        from sfepy.base.base import Struct
        from sfepy.base.conf import ProblemConf, get_standard_keywords
        from sfepy.applications import assign_standard_hooks

        required, other = get_standard_keywords()
        input_name = op.join(op.dirname(__file__), conf.input_name)
        test_conf = ProblemConf.from_file(input_name, required, other)

        if cls is None:
            cls = TestInput
        test = cls(test_conf=test_conf, conf=conf, options=options)

        assign_standard_hooks(test, test_conf.options.get, test_conf)

        name = test.get_output_name_trunk()
        test.solver_options = Struct(output_filename_trunk=name,
                                     output_format ='vtk',
                                     save_ebc=False, save_ebc_nodes=False,
                                     save_regions=False,
                                     save_regions_as_groups=False,
                                     save_field_meshes=False,
                                     solve_not=False)

        return test

    def get_output_name_trunk(self):
        return op.splitext(op.split(self.conf.output_name)[1])[0]

    def check_conditions(self, conditions):
        ok = (conditions == 0).all()
        if not ok:
            self.report('nls stopping conditions:')
            self.report(conditions)
        return ok

    def test_input(self):
        import numpy as nm
        from sfepy.applications import solve_pde

        self.report('solving %s...' % self.conf.input_name)

        status = NLSStatus(conditions=[])

        solve_pde(self.test_conf,
                  self.solver_options,
                  nls_status=status,
                  output_dir=self.options.out_dir,
                  step_hook=self.step_hook,
                  post_process_hook=self.post_process_hook,
                  post_process_hook_final=self.post_process_hook_final)
        self.report('%s solved' % self.conf.input_name)

        ok = self.check_conditions(nm.array(status.conditions))

        return ok

class TestInputEvolutionary(TestInput):

    @staticmethod
    def from_conf(conf, options, cls=None):
        if cls is None:
            cls = TestInputEvolutionary

        return TestInput.from_conf(conf, options, cls=cls)

    def get_output_name_trunk(self):
        return self.conf.output_name_trunk

##
# 03.10.2007, c
class TestLCBC( TestCommon ):
    """Test linear combination BC. See test_lcbc_*.py files."""

    ##
    # 03.10.2007, c
    def from_conf( conf, options ):
        return TestLCBC( conf = conf, options = options )
    from_conf = staticmethod( from_conf )

    ##
    # c: 03.10.2007, r: 08.02.2008
    def test_linear_rigid_body_bc( self ):
        import scipy
        if scipy.version.version == "0.6.0":
            # This test uses a functionality implemented in scipy svn, which is
            # missing in scipy 0.6.0
            return True
        from sfepy.base.base import Struct
        from sfepy.applications import solve_pde
        from sfepy.base.base import IndexedStruct

        status = IndexedStruct()
        problem, state = solve_pde(self.conf, nls_status=status,
                                   save_results=False)
        ok = status.condition == 0
        self.report( 'converged: %s' % ok )
        out = state.create_output_dict()

        strain = problem.evaluate('ev_cauchy_strain.i.Y( u )', mode='el_avg')
        out['strain'] = Struct(name='output_data',
                               mode='cell', data=strain, dofs=None)

        name = op.join( self.options.out_dir,
                        op.split( self.conf.output_name )[1] )
        problem.domain.mesh.write( name, io = 'auto', out = out )

        ##
        # Check if rigid body displacements are really rigid should go here.

        return ok

########NEW FILE########
__FILENAME__ = test_assembling
import numpy as nm
import scipy.sparse as sps

from sfepy.base.testing import TestCommon

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        conn = nm.array([[0, 1, 2],
                         [2, 3, 4]], dtype=nm.int32)

        num = conn.max() + 1

        iels = nm.array([0, 1], dtype=nm.int32)

        vec_in_els = nm.array([[1, 1, 1], [2, 2, 2]], dtype=nm.float64)
        vec_in_els.shape = (2, 1, 1, 3)

        mtx_in_els = nm.array([nm.ones((3, 3)), 2 * nm.ones((3, 3))],
                              dtype=nm.float64)
        mtx_in_els.shape = (2, 1, 3, 3)

        return Test(conn=conn, num=num, iels=iels,
                    vec_in_els=vec_in_els, mtx_in_els=mtx_in_els,
                    conf=conf, options=options)

    def test_assemble_vector(self):
        from sfepy.discrete.fem.extmods.assemble import assemble_vector

        vec = nm.zeros(self.num, dtype=nm.float64)

        assemble_vector(vec, self.vec_in_els, self.iels, 1, self.conn)

        aux = nm.array([1, 1, 3, 2, 2], dtype=nm.float64)

        self.report('assembled: %s' % vec)
        self.report('expected: %s' % aux)
        ok = self.compare_vectors(vec, aux,
                                  label1='assembled',
                                  label2='expected')
        return ok

    def test_assemble_vector_complex(self):
        from sfepy.discrete.fem.extmods.assemble import assemble_vector_complex

        vec = nm.zeros(self.num, dtype=nm.complex128)
        vec_in_els = self.vec_in_els.astype(nm.complex128) * (2 - 3j)

        assemble_vector_complex(vec, vec_in_els, self.iels, 1, self.conn)

        aux = nm.array([2-3j, 2-3j, 6-9j, 4-6j, 4-6j],
                       dtype=nm.complex128)

        self.report('assembled: %s' % vec)
        self.report('expected: %s' % aux)
        ok = self.compare_vectors(vec, aux,
                                  label1='assembled',
                                  label2='expected')
        return ok

    def test_assemble_matrix(self):
        from sfepy.discrete.fem.extmods.assemble import assemble_matrix

        mtx = sps.csr_matrix(nm.ones((self.num, self.num),
                                     dtype=nm.float64))
        mtx.data[:] = 0.0

        assemble_matrix(mtx.data, mtx.indptr, mtx.indices, self.mtx_in_els,
                        self.iels, 1, self.conn, self.conn)

        aux = nm.array([[1, 1, 1, 0, 0],
                        [1, 1, 1, 0, 0],
                        [1, 1, 3, 2, 2],
                        [0, 0, 2, 2, 2],
                        [0, 0, 2, 2, 2]], dtype=nm.float64)

        self.report('assembled:\n%s' % mtx.toarray())
        self.report('expected:\n%s' % aux)
        ok = self.compare_vectors(mtx, aux,
                                  label1='assembled',
                                  label2='expected')
        return ok

    def test_assemble_matrix_complex(self):
        from sfepy.discrete.fem.extmods.assemble import assemble_matrix_complex

        mtx = sps.csr_matrix(nm.ones((self.num, self.num),
                                     dtype=nm.complex128))
        mtx.data[:] = 0.0
        mtx_in_els = self.mtx_in_els.astype(nm.complex128) * (2 - 3j)

        assemble_matrix_complex(mtx.data, mtx.indptr, mtx.indices, mtx_in_els,
                                self.iels, 1, self.conn, self.conn)

        aux = nm.array([[2-3j, 2-3j, 2-3j, 0+0j, 0+0j],
                        [2-3j, 2-3j, 2-3j, 0+0j, 0+0j],
                        [2-3j, 2-3j, 6-9j, 4-6j, 4-6j],
                        [0+0j, 0+0j, 4-6j, 4-6j, 4-6j],
                        [0+0j, 0+0j, 4-6j, 4-6j, 4-6j]], dtype=nm.complex128)

        self.report('assembled:\n%s' % mtx.toarray())
        self.report('expected:\n%s' % aux)
        ok = self.compare_vectors(mtx, aux,
                                  label1='assembled',
                                  label2='expected')
        return ok

########NEW FILE########
__FILENAME__ = test_base
from sfepy.base.base import assert_
from sfepy.base.testing import TestCommon

##
# 28.08.2007, c
class Test( TestCommon ):

    ##
    # 28.08.2007, c
    def from_conf( conf, options ):
        return Test( conf = conf, options = options )
    from_conf = staticmethod( from_conf )

    ##
    # 28.08.2007, c
    def test_struct_add( self ):
        from sfepy.base.base import Struct
        from copy import deepcopy

        a = Struct( f1 = 0,
                    f2 = [1, 2, 3],
                    f3 = Struct( ff = 'abc' ),
                    f4 = 3.14 )
        a0 = deepcopy( a )
        b = Struct( f1 = 5,
                    f2 = [1],
                    f3 = Struct( ff = '', gg = 123 ),
                    f5 = 'new one' )
        c = a + b

        assert_( c.f1 == 0 )
        assert_( c.f2 == [1, 2, 3] )
        assert_( c.f3.ff == 'abc' )
        assert_( c.f3.gg == 123 )
        assert_( c.f4 == 3.14 )
        assert_( c.f5 == 'new one' )

        assert_( a.f1 == a0.f1 )
        assert_( a.f2 == a0.f2 )
        assert_( a.f3.ff == a0.f3.ff )
        assert_( a.f4 == a0.f4 )

        return True

    ##
    # 28.08.2007, c
    def test_struct_i_add( self ):
        from sfepy.base.base import Struct

        a = Struct( f1 = 0,
                    f2 = [1, 2, 3],
                    f3 = Struct( ff = 'abc' ) )
        b = Struct( f1 = 5,
                    f2 = [1],
                    f3 = Struct( ff = '', gg = 123 ),
                    f4 = 'new one' )
        a += b

        assert_( a.f1 == 0 )
        assert_( a.f2 == [1, 2, 3] )
        assert_( a.f3.ff == 'abc' )
        assert_( a.f3.gg == 123 )
        assert_( a.f4 == 'new one' )

        return True

    def test_verbose_output(self):
        import StringIO
        from sfepy.base.base import Output, goptions

        fd = StringIO.StringIO()

        output = Output('test', filename=fd)

        output('test1')
        goptions['verbose'] = False
        output('test2')
        goptions['verbose'] = 1
        output('test3')

        _ok1 = goptions['verbose'] == True
        _ok2 = fd.getvalue() == 'test test1\ntest test3\n'

        fd.close()

        ok = _ok1 and _ok2

        return ok

    def test_resolve_deps(self):
        from sfepy.base.resolve_deps import resolve

        deps = {
            'a' : ['a', 'b'],
            'b' : ['a', 'b'],
            'c' : ['b', 'c', 'd', 'e'],
            'd' : ['c', 'e'],
            'e' : ['d', 'e'],
            'f' : ['e', 'f', 'g'],
            'g' : ['g'],
        }
        order = resolve(deps)
        ok1 = order == [['g'], ['a', 'b'], ['c', 'd', 'e'], ['f']]

        deps = {
            'a' : ['b'],
            'b' : ['c'],
            'c' : ['a'],
        }
        order = resolve(deps)
        ok2 = order == [['a', 'b', 'c']]

        return ok1 and ok2

########NEW FILE########
__FILENAME__ = test_cmesh
import os

import numpy as nm

from sfepy.base.testing import TestCommon
from sfepy import data_dir

# n_cell, n_face, n_edge, n_vertex
# d1 -> d2 : num, n_incident
expected = {
    '2_3_2.mesh' : ([4, 5, 2, 0], {
        (0, 0) : (4, 10),
        (0, 1) : (4, 10),
        (0, 2) : (4, 6),
        (1, 0) : (5, 10),
        (1, 1) : (5, 16),
        (1, 2) : (5, 6),
        (2, 0) : (2, 6),
        (2, 1) : (2, 6),
        (2, 2) : (2, 2),
        }),
    '2_4_2.mesh' : ([6, 7, 2, 0], {
        (0, 0) : (6, 22),
        (0, 1) : (6, 14),
        (0, 2) : (6, 8),
        (1, 0) : (7, 14),
        (1, 1) : (7, 20),
        (1, 2) : (7, 8),
        (2, 0) : (2, 8),
        (2, 1) : (2, 8),
        (2, 2) : (2, 2),
        }),
    '3_4_2.mesh' : ([5, 9, 7, 2], {
        (0, 0) : (5, 18),
        (0, 1) : (5, 18),
        (0, 2) : (5, 21),
        (0, 3) : (5, 8),
        (1, 0) : (9, 18),
        (1, 1) : (9, 48),
        (1, 2) : (9, 21),
        (1, 3) : (9, 12),
        (2, 0) : (7, 21),
        (2, 1) : (7, 21),
        (2, 2) : (7, 42),
        (2, 3) : (7, 8),
        (3, 0) : (2, 8),
        (3, 1) : (2, 12),
        (3, 2) : (2, 8),
        (3, 3) : (2, 2),
        }),
    '3_8_2.mesh' : ([12, 20, 11, 2], {
        (0, 0) : (12, 100),
        (0, 1) : (12, 40),
        (0, 2) : (12, 44),
        (0, 3) : (12, 16),
        (1, 0) : (20, 40),
        (1, 1) : (20, 96),
        (1, 2) : (20, 44),
        (1, 3) : (20, 24),
        (2, 0) : (11, 44),
        (2, 1) : (11, 44),
        (2, 2) : (11, 72),
        (2, 3) : (11, 12),
        (3, 0) : (2, 16),
        (3, 1) : (2, 24),
        (3, 2) : (2, 12),
        (3, 3) : (2, 2),
        }),
    'square_triquad.mesh' : ([470, 1127, 658, 0], {
        (0, 0) : (470, 3054),
        (0, 1) : (470, 2254),
        (0, 2) : (470, 2174),
        (1, 0) : (1127, 2254),
        (1, 1) : (1127, 9174),
        (1, 2) : (1127, 2174),
        (2, 0) : (658, 2174),
        (2, 1) : (658, 2174),
        (2, 2) : (658, 6686),
        }),
}

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        filename_meshes = [data_dir + '/meshes/elements/%s_2.mesh' % geom
                           for geom in ['2_3', '2_4', '3_4', '3_8']]
        filename_meshes.append(data_dir
                               + '/meshes/2d/special/square_triquad.mesh')

        test = Test(filename_meshes=filename_meshes,
                    conf=conf, options=options)
        return test

    def test_cmesh_counts(self):
        from sfepy.discrete.fem import Mesh
        from sfepy.discrete.fem.geometry_element import create_geometry_elements
        from sfepy.discrete.fem.extmods.cmesh import CMesh, get_cmem_usage

        gels = create_geometry_elements()

        ok = True

        for filename in self.filename_meshes:
            basename = os.path.basename(filename)
            enum, esizes = expected[basename]

            self.report('mesh: %s' % basename)

            mesh = Mesh.from_file(filename)
            cmesh = CMesh.from_mesh(mesh)
            cmesh.set_local_entities(gels)

            cmesh.setup_entities()

            self.report('dim:', cmesh.dim)
            self.report('n_cell: %d, n_face: %d, n_edge: %d, n_vertex: %d' %
                        tuple(cmesh.num))

            _ok = (enum == cmesh.num).all()
            if not _ok:
                self.report('%s == %s failed!' % (enum, cmesh.num))
            ok = ok and _ok

            dim = cmesh.dim
            for ir in range(dim + 1):
                for ic in range(dim + 1):
                    cmesh.setup_connectivity(ir, ic)
                    mem_usage1 = get_cmem_usage()[0]

                    if (ir == dim) and (ic == 0):
                        continue

                    cmesh.free_connectivity(ir, ic)
                    mem_usage2 = get_cmem_usage()[0]

                    cmesh.setup_connectivity(ir, ic)
                    mem_usage3 = get_cmem_usage()[0]

                    conn = cmesh.get_conn(ir, ic)

                    self.report('(%d, %d) : (%d, %d)'
                                % (ir, ic, conn.num, conn.n_incident))
                    sizes = nm.array([conn.num, conn.n_incident])

                    _ok = (esizes[ir, ic] == sizes).all()
                    if not _ok:
                        self.report('%s == %s failed!' % (esizes, sizes))
                    ok = ok and _ok

                    _ok1 = mem_usage3 == mem_usage1
                    _ok2 = mem_usage3 > mem_usage2
                    if not (_ok1 and _ok2):
                        self.report('unexpected memory usage! (%s)'
                                    % (mem_usage1, mem_usage2, mem_usage3))
                    ok = ok and (_ok1 and _ok2)

        return ok

########NEW FILE########
__FILENAME__ = test_domain
import os.path as op

import numpy as nm

from sfepy.base.testing import TestCommon
from sfepy import data_dir
from sfepy.discrete.fem import Mesh, FEDomain

def refine(domain, out_dir, level=3):
    for ii in range(level):
        domain = domain.refine()
        filename = op.join(out_dir, 'refine_' + domain.mesh.name + '.mesh')
        domain.mesh.write(filename, io='auto')

    return domain

expected_coors = {
    '2_3' : nm.array([[0. , 0. ],
                      [1. , 0. ],
                      [0. , 1. ],
                      [0.5, 0. ],
                      [0.5, 0.5],
                      [0. , 0.5]], dtype=nm.float64),
    '2_4' : nm.array([[0. , 0. ],
                      [1. , 0. ],
                      [1. , 1. ],
                      [0. , 1. ],
                      [0.5, 0. ],
                      [1. , 0.5],
                      [0.5, 1. ],
                      [0. , 0.5],
                      [0.5, 0.5]], dtype=nm.float64),
    '3_4' : nm.array([[0. , 0. , 0. ],
                      [1. , 0. , 0. ],
                      [0. , 1. , 0. ],
                      [0. , 0. , 1. ],
                      [0.5, 0. , 0. ],
                      [0.5, 0.5, 0. ],
                      [0. , 0.5, 0. ],
                      [0. , 0. , 0.5],
                      [0.5, 0. , 0.5],
                      [0. , 0.5, 0.5]], dtype=nm.float64),
    '3_8' : nm.array([[0. , 0. , 0. ],
                      [1. , 0. , 0. ],
                      [1. , 1. , 0. ],
                      [0. , 1. , 0. ],
                      [0. , 0. , 1. ],
                      [1. , 0. , 1. ],
                      [1. , 1. , 1. ],
                      [0. , 1. , 1. ],
                      [0.5, 0. , 0. ],
                      [1. , 0.5, 0. ],
                      [0.5, 1. , 0. ],
                      [0. , 0.5, 0. ],
                      [0.5, 0. , 1. ],
                      [1. , 0.5, 1. ],
                      [0.5, 1. , 1. ],
                      [0. , 0.5, 1. ],
                      [0. , 0. , 0.5],
                      [1. , 0. , 0.5],
                      [1. , 1. , 0.5],
                      [0. , 1. , 0.5],
                      [0.5, 0.5, 0. ],
                      [0. , 0.5, 0.5],
                      [0.5, 0. , 0.5],
                      [0.5, 0.5, 1. ],
                      [1. , 0.5, 0.5],
                      [0.5, 1. , 0.5],
                      [0.5, 0.5, 0.5]], dtype=nm.float64),
}

expected_conn = {
    '2_3' : nm.array([[0, 3, 5],
                      [3, 4, 5],
                      [1, 4, 3],
                      [2, 5, 4]], dtype=nm.int32),
    '2_4' : nm.array([[0, 4, 8, 7],
                      [1, 5, 8, 4],
                      [2, 6, 8, 5],
                      [3, 7, 8, 6]], dtype=nm.int32),
    '3_4' : nm.array([[0, 4, 6, 7],
                      [4, 1, 5, 8],
                      [6, 5, 2, 9],
                      [7, 8, 9, 3],
                      [4, 6, 7, 8],
                      [4, 6, 8, 5],
                      [6, 7, 8, 9],
                      [6, 5, 9, 8]], dtype=nm.int32),
    '3_8' : nm.array([[0,  8, 20, 11, 16, 22, 26, 21],
                      [1,  9, 20,  8, 17, 24, 26, 22],
                      [2, 10, 20,  9, 18, 25, 26, 24],
                      [3, 11, 20, 10, 19, 21, 26, 25],
                      [4, 15, 23, 12, 16, 21, 26, 22],
                      [5, 12, 23, 13, 17, 22, 26, 24],
                      [6, 13, 23, 14, 18, 24, 26, 25],
                      [7, 14, 23, 15, 19, 25, 26, 21]], dtype=nm.int32),
}

def compare_mesh(geo_name, coors, conn):
    _coors = expected_coors[geo_name]
    _conn = expected_conn[geo_name]

    print coors.__repr__()
    print conn.__repr__()

    ok = nm.allclose(coors, _coors, rtol=0.0, atol=1e-14)
    ok = ok and (conn == _conn).all()

    return ok

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        mesh = Mesh('mesh_tetra',
                    data_dir + '/meshes/various_formats/small3d.mesh')
        domain = FEDomain('domain', mesh)

        return Test(conf=conf, options=options, domain=domain)

    def test_facets(self):
        ok = True
        cmesh = self.domain.cmesh

        _ok = cmesh.num[1] == 26
        self.report('unique edges: %s' % _ok)
        ok = ok and _ok

        _ok = cmesh.num[2] == 30
        self.report('unique faces: %s' % _ok)
        ok = ok and _ok

        return ok

    def test_refine_tetra(self):
        refine(self.domain, self.options.out_dir)

        return True

    def test_refine_hexa(self):
        mesh = Mesh('mesh_hexa',
                    data_dir + '/meshes/various_formats/abaqus_hex.inp')
        domain = FEDomain('domain', mesh)

        refine(domain, self.options.out_dir)

        return True

    def test_refine_2_3(self):
        mesh = Mesh('2_3', data_dir + '/meshes/elements/2_3_1.mesh')
        domain = refine(FEDomain('domain', mesh), self.options.out_dir, 1)

        ok = compare_mesh('2_3', domain.mesh.coors, domain.mesh.conns[0])

        return ok

    def test_refine_2_4(self):
        mesh = Mesh('2_4', data_dir + '/meshes/elements/2_4_1.mesh')
        domain = refine(FEDomain('domain', mesh), self.options.out_dir, 1)

        ok = compare_mesh('2_4', domain.mesh.coors, domain.mesh.conns[0])

        return ok

    def test_refine_3_4(self):
        mesh = Mesh('3_4', data_dir + '/meshes/elements/3_4_1.mesh')
        domain = refine(FEDomain('domain', mesh), self.options.out_dir, 1)

        ok = compare_mesh('3_4', domain.mesh.coors, domain.mesh.conns[0])

        return ok

    def test_refine_3_8(self):
        mesh = Mesh('3_8', data_dir + '/meshes/elements/3_8_1.mesh')
        domain = refine(FEDomain('domain', mesh), self.options.out_dir, 1)

        ok = compare_mesh('3_8', domain.mesh.coors, domain.mesh.conns[0])

        return ok

########NEW FILE########
__FILENAME__ = test_ebcs
import os.path as op

from sfepy import data_dir
from sfepy.discrete.fem.periodic import match_y_line

filename_mesh = data_dir + '/meshes/2d/square_unit_tri.mesh'

materials = {
    'm' : ({'K' : [[3.0, 0.1], [0.3, 1.0]]},),
}

fields = {
    'pressure' : ('real', 'scalar', 'Omega', 2),
}

variables = {
    'p' : ('unknown field', 'pressure', 0),
    'q' : ('test field', 'pressure', 'p'),
    'r' : ('parameter field', 'pressure', 'p'),
}

regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < -0.499)', 'facet'),
    'LeftStrip' : ('vertices in (x < -0.499) & (y > -0.199) & (y < 0.199)',
                   'facet'),
    'LeftFix' : ('r.Left -v r.LeftStrip', 'facet'),
    'Right' : ('vertices in (x > 0.499)', 'facet'),
    'RightStrip' : ('vertices in (x > 0.499) & (y > -0.199) & (y < 0.199)',
                    'facet'),
    'RightFix' : ('r.Right -v r.RightStrip', 'facet'),
}

ebcs = {
    't_left' : ('LeftFix', {'p.0' : 5.0}),
    't_right' : ('RightFix', {'p.0' : 0.0}),
}

epbcs = {
    'periodic_x' : (['LeftStrip', 'RightStrip'],
                    {'p.0' : 'p.0'}, 'match_y_line'),
}

functions = {
    'match_y_line' : (match_y_line,),
}

equations = {
    'eq' : """dw_diffusion.2.Omega( m.K, q, p ) = 0"""
}

solvers = {
    'ls' : ('ls.scipy_direct', {}),
    'newton' : ('nls.newton',
                {'i_max'      : 1,
                 'eps_a'      : 1e-10,
    }),
}

from sfepy.base.testing import TestCommon

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        from sfepy.applications import solve_pde

        problem, state = solve_pde(conf, output_dir=options.out_dir)

        test = Test(problem=problem, state=state, conf=conf, options=options)
        return test

    def test_save_ebc(self):
        name = op.join(self.options.out_dir,
                       op.splitext(op.basename(__file__))[0])
        self.problem.save_ebc(name + '_ebcs_f.vtk', force=True)
        self.problem.save_ebc(name + '_ebcs.vtk', default=-1, force=False)

        return True

########NEW FILE########
__FILENAME__ = test_elasticity_small_strain
# 10.07.2007, c
# last revision: 25.03.2008
from sfepy import data_dir

filename_meshes = ['/meshes/3d/cube_medium_tetra.mesh',
                   '/meshes/3d/cube_medium_tetra.mesh',
                   '/meshes/3d/cube_medium_hexa.mesh']
filename_meshes = [data_dir + name for name in filename_meshes]

all_your_bases = [1, 2, 1]

filename_mesh = None

field_1 = {
    'name' : '3_displacement',
    'dtype' : 'real',
    'shape' : (3,),
    'region' : 'Omega',
    'approx_order' : None,
}

def get_pars( dim, full = False ):
    import numpy as nm
    sym = (dim + 1) * dim / 2
    lam = 1e1
    mu = 1e0
    o = nm.array( [1.] * dim + [0.] * (sym - dim), dtype = nm.float64 )
    oot = nm.outer( o, o )
    if full:
        return lam * oot + mu * nm.diag( o + 1.0 )
    else:
        return lam, mu

material_1 = {
    'name' : 'solid',
    'values' : {
        'lam' : get_pars( 3 )[0],
        'mu' : get_pars( 3 )[1],
        'Dijkl' : get_pars( 3, True ),
    }
}

material_2 = {
    'name' : 'spring',
    'values' : {
        '.pars' : {'stiffness' : 1e0, 'projection' : None},
    }
}

variable_1 = {
    'name' : 'u',
    'kind' : 'unknown field',
    'field' : '3_displacement',
    'order' : 0,
}
variable_2 = {
    'name' : 'v',
    'kind' : 'test field',
    'field' : '3_displacement',
    'dual' : 'u',
}

region_1000 = {
    'name' : 'Omega',
    'select' : 'all',
}

region_1 = {
    'name' : 'Bottom',
    'select' : 'vertices in (z < -0.499)',
    'kind' : 'facet',
}
region_2 = {
    'name' : 'Top',
    'select' : 'vertices in (z > 0.499)',
    'kind' : 'facet',
}

ebc_1 = {
    'name' : 'Load',
    'region' : 'Top',
    'dofs' : {'u.2' : 0.1},
}

integral_1 = {
    'name' : 'i',
    'order' : 2,
}

equations_iso = {
    'balance_of_forces' :
    """dw_lin_elastic_iso.i.Omega( solid.lam, solid.mu, v, u )
     = dw_point_lspring.i.Bottom( spring.pars, v, u )""",
}
equations_general = {
    'balance_of_forces' :
    """dw_lin_elastic.i.Omega( solid.Dijkl, v, u )
     = dw_point_lspring.i.Bottom( spring.pars, v, u )""",
}

solver_0 = {
    'name' : 'ls',
    'kind' : 'ls.scipy_direct',
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 1,
    'eps_a'      : 1e-10,
}

from sfepy.base.testing import TestCommon

##
# 10.07.2007, c
class Test( TestCommon ):
    tests = ['test_get_solution', 'test_linear_terms']

    ##
    # 10.07.2007, c
    def from_conf( conf, options ):
        return Test( conf = conf, options = options )
    from_conf = staticmethod( from_conf )

    ##
    # c: 25.03.2008, r: 25.03.2008
    def test_linear_terms( self ):
        ok = True
        for sols in self.solutions:
            ok = ok and self.compare_vectors( sols[0], sols[1],
                                             label1 = 'isotropic',
                                             label2 = 'general' )
        return ok
        
    ##
    # c: 10.07.2007, r: 25.03.2008
    def test_get_solution( self ):
        from sfepy.applications import solve_pde
        from sfepy.base.base import IndexedStruct
        import os.path as op

        ok = True
        self.solutions = []
        for ii, approx_order in enumerate(all_your_bases):
            fname = filename_meshes[ii]

            self.conf.filename_mesh = fname
            fields = {'field_1' : {
                          'name' : '3_displacement',
                          'dtype' : 'real',
                          'shape' : (3,),
                          'region' : 'Omega',
                          'approx_order' : approx_order,
                    }
            }
            self.conf.edit('fields', fields)
            self.report( 'mesh: %s, base: %s' % (fname, approx_order) )
            status = IndexedStruct()

            self.report( 'isotropic' )
            self.conf.equations = self.conf.equations_iso
            problem, state1 = solve_pde(self.conf, nls_status=status,
                                        save_results=False)
            converged = status.condition == 0
            ok = ok and converged
            self.report( 'converged: %s' % converged )

            self.report( 'general' )
            self.conf.equations = self.conf.equations_general
            problem, state2 = solve_pde(self.conf, nls_status=status,
                                        save_results=False)
            converged = status.condition == 0
            ok = ok and converged
            self.report( 'converged: %s' % converged )

            self.solutions.append((state1(), state2()))

            name = op.join(self.options.out_dir,
                           '_'.join(('test_elasticity_small_strain',
                                     op.splitext(op.basename(fname))[0],
                                     '%d' % approx_order))
                           + '.vtk')
            problem.save_state(name, state1)

##             trunk = op.join( self.options.out_dir,
##                              op.splitext( op.basename( fname ) )[0] )
##             problem.save_field_meshes( trunk )
##             problem.save_regions( trunk )
            
        return ok

########NEW FILE########
__FILENAME__ = test_fem
import os.path as op
import numpy as nm

import sfepy
from sfepy.base.testing import TestCommon

test_bases = {
    '2_3_P1'
    : nm.array([[[ 1. ,  0. ,  0. ]],

                [[ 0. ,  1. ,  0. ]],

                [[ 0. ,  0. ,  1. ]],

                [[ 0.6,  0.2,  0.2]]]),
    '2_3_P1_grad'
    : nm.array([[[-1.,  1.,  0.],
                 [-1.,  0.,  1.]],

                [[-1.,  1.,  0.],
                 [-1.,  0.,  1.]],

                [[-1.,  1.,  0.],
                 [-1.,  0.,  1.]],

                [[-1.,  1.,  0.],
                 [-1.,  0.,  1.]]]),
    '2_4_Q1'
    : nm.array([[[ 1.  ,  0.  ,  0.  ,  0.  ]],

                [[ 0.  ,  1.  ,  0.  ,  0.  ]],

                [[ 0.  ,  0.  ,  1.  ,  0.  ]],

                [[ 0.  ,  0.  ,  0.  ,  1.  ]],

                [[ 0.64,  0.16,  0.04,  0.16]]]),
    '2_4_Q1_grad'
    : nm.array([[[-1. ,  1. ,  0. , -0. ],      
                 [-1. , -0. ,  0. ,  1. ]],     

                [[-1. ,  1. ,  0. , -0. ],
                 [-0. , -1. ,  1. ,  0. ]],

                [[-0. ,  0. ,  1. , -1. ],
                 [-0. , -1. ,  1. ,  0. ]],

                [[-0. ,  0. ,  1. , -1. ],
                 [-1. , -0. ,  0. ,  1. ]],

                [[-0.8,  0.8,  0.2, -0.2],
                 [-0.8, -0.2,  0.2,  0.8]]]),
    '3_4_P0' : nm.ones((5, 1, 1)),
    '3_4_P0_grad' : nm.zeros((5, 3, 1)),
    '3_8_Q0' : nm.ones((9, 1, 1)),
    '3_8_Q0_grad' : nm.zeros((9, 3, 1)),
    '3_8_Q1'
    : nm.array([[[ 1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,
                   0.   ]],       

                [[ 0.   ,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,
                   0.   ]],

                [[ 0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,
                   0.   ]],

                [[ 0.   ,  0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  0.   ,
                   0.   ]],

                [[ 0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  0.   ,  0.   ,
                   0.   ]],

                [[ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,  0.   ,
                   0.   ]],

                [[ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,
                   0.   ]],

                [[ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,
                   1.   ]],

                [[ 0.512,  0.128,  0.032,  0.128,  0.128,  0.032,  0.008,
                   0.032]]]),
    '3_8_Q1_grad'
    : nm.array([[[-1.  ,  1.  ,  0.  , -0.  , -0.  ,  0.  ,  0.  , -0.  ],
                 [-1.  , -0.  ,  0.  ,  1.  , -0.  , -0.  ,  0.  ,  0.  ],
                 [-1.  , -0.  , -0.  , -0.  ,  1.  ,  0.  ,  0.  ,  0.  ]],

                [[-1.  ,  1.  ,  0.  , -0.  , -0.  ,  0.  ,  0.  , -0.  ],
                 [-0.  , -1.  ,  1.  ,  0.  , -0.  , -0.  ,  0.  ,  0.  ],
                 [-0.  , -1.  , -0.  , -0.  ,  0.  ,  1.  ,  0.  ,  0.  ]],

                [[-0.  ,  0.  ,  1.  , -1.  , -0.  ,  0.  ,  0.  , -0.  ],
                 [-0.  , -1.  ,  1.  ,  0.  , -0.  , -0.  ,  0.  ,  0.  ],
                 [-0.  , -0.  , -1.  , -0.  ,  0.  ,  0.  ,  1.  ,  0.  ]],

                [[-0.  ,  0.  ,  1.  , -1.  , -0.  ,  0.  ,  0.  , -0.  ],
                 [-1.  , -0.  ,  0.  ,  1.  , -0.  , -0.  ,  0.  ,  0.  ],
                 [-0.  , -0.  , -0.  , -1.  ,  0.  ,  0.  ,  0.  ,  1.  ]],

                [[-0.  ,  0.  ,  0.  , -0.  , -1.  ,  1.  ,  0.  , -0.  ],
                 [-0.  , -0.  ,  0.  ,  0.  , -1.  , -0.  ,  0.  ,  1.  ],
                 [-1.  , -0.  , -0.  , -0.  ,  1.  ,  0.  ,  0.  ,  0.  ]],

                [[-0.  ,  0.  ,  0.  , -0.  , -1.  ,  1.  ,  0.  , -0.  ],
                 [-0.  , -0.  ,  0.  ,  0.  , -0.  , -1.  ,  1.  ,  0.  ],
                 [-0.  , -1.  , -0.  , -0.  ,  0.  ,  1.  ,  0.  ,  0.  ]],

                [[-0.  ,  0.  ,  0.  , -0.  , -0.  ,  0.  ,  1.  , -1.  ],
                 [-0.  , -0.  ,  0.  ,  0.  , -0.  , -1.  ,  1.  ,  0.  ],
                 [-0.  , -0.  , -1.  , -0.  ,  0.  ,  0.  ,  1.  ,  0.  ]],

                [[-0.  ,  0.  ,  0.  , -0.  , -0.  ,  0.  ,  1.  , -1.  ],
                 [-0.  , -0.  ,  0.  ,  0.  , -1.  , -0.  ,  0.  ,  1.  ],
                 [-0.  , -0.  , -0.  , -1.  ,  0.  ,  0.  ,  0.  ,  1.  ]],

                [[-0.64,  0.64,  0.16, -0.16, -0.16,  0.16,  0.04, -0.04],
                 [-0.64, -0.16,  0.16,  0.64, -0.16, -0.04,  0.04,  0.16],
                 [-0.64, -0.16, -0.04, -0.16,  0.64,  0.16,  0.04,  0.16]]]),
    '3_4_P2'
    : nm.array([[[ 1.  , -0.  , -0.  , -0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,
                   0.  ]],

                [[-0.  ,  1.  , -0.  , -0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,
                   0.  ]],

                [[-0.  , -0.  ,  1.  , -0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,
                   0.  ]],
                
                [[-0.  , -0.  , -0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,
                   0.  ]],

                [[-0.08, -0.12, -0.12, -0.12,  0.32,  0.16,  0.32,  0.32,  0.16,
                  0.16]]]),
    '3_4_P2_grad'
    : nm.array([[[-3. , -1. ,  0. ,  0. ,  4. ,  0. ,  0. ,  0. ,  0. ,  0. ],
                 [-3. ,  0. , -1. ,  0. ,  0. ,  0. ,  4. ,  0. ,  0. ,  0. ],
                 [-3. ,  0. ,  0. , -1. ,  0. ,  0. ,  0. ,  4. ,  0. ,  0. ]],

                [[ 1. ,  3. ,  0. ,  0. , -4. ,  0. ,  0. ,  0. ,  0. ,  0. ],
                 [ 1. ,  0. , -1. ,  0. , -4. ,  4. ,  0. ,  0. ,  0. ,  0. ],
                 [ 1. ,  0. ,  0. , -1. , -4. ,  0. ,  0. ,  0. ,  4. ,  0. ]],

                [[ 1. , -1. ,  0. ,  0. ,  0. ,  4. , -4. ,  0. ,  0. ,  0. ],
                 [ 1. ,  0. ,  3. ,  0. ,  0. ,  0. , -4. ,  0. ,  0. ,  0. ],
                 [ 1. ,  0. ,  0. , -1. ,  0. ,  0. , -4. ,  0. ,  0. ,  4. ]],

                [[ 1. , -1. ,  0. ,  0. ,  0. ,  0. ,  0. , -4. ,  4. ,  0. ],
                [ 1. ,  0. , -1. ,  0. ,  0. ,  0. ,  0. , -4. ,  0. ,  4. ],
                 [ 1. ,  0. ,  0. ,  3. ,  0. ,  0. ,  0. , -4. ,  0. ,  0. ]],

                [[-0.6, -0.2,  0. ,  0. ,  0.8,  0.8, -0.8, -0.8,  0.8,  0. ],
                 [-0.6,  0. , -0.2,  0. , -0.8,  0.8,  0.8, -0.8,  0. ,  0.8],
                 [-0.6,  0. ,  0. , -0.2, -0.8,  0. , -0.8,  0.8,  0.8,  0.8]]]),

    '3_4_P2B'
    : nm.array([[[ 1.     , -0.     , -0.     , -0.     ,  0.     ,  0.     ,
                   0.     ,  0.     ,  0.     ,  0.     ,  0.     ]],
             
                [[-0.     ,  1.     , -0.     , -0.     ,  0.     ,  0.     ,
                  0.     ,  0.     ,  0.     ,  0.     ,  0.     ]],

                [[-0.     , -0.     ,  1.     , -0.     ,  0.     ,  0.     ,
                  0.     ,  0.     ,  0.     ,  0.     ,  0.     ]],

                [[-0.     , -0.     , -0.     ,  1.     ,  0.     ,  0.     ,
                  0.     ,  0.     ,  0.     ,  0.     ,  0.     ]],

                [[-0.16192, -0.20192, -0.20192, -0.20192,  0.23808,  0.07808,
                  0.23808,  0.23808,  0.07808,  0.07808,  0.8192 ]]]),
    '3_4_P2B_grad'
    : nm.array([[[-3.    , -1.    ,  0.    ,  0.    ,  4.    ,  0.    ,  0.    ,
                  0.    ,  0.    ,  0.    ,  0.    ],                          
                 [-3.    ,  0.    , -1.    ,  0.    ,  0.    ,  0.    ,  4.    ,
                  0.    ,  0.    ,  0.    ,  0.    ],                          
                 [-3.    ,  0.    ,  0.    , -1.    ,  0.    ,  0.    ,  0.    ,
                  4.    ,  0.    ,  0.    ,  0.    ]],                         

                [[ 1.    ,  3.    ,  0.    ,  0.    , -4.    ,  0.    ,  0.    ,
                   0.    ,  0.    ,  0.    ,  0.    ],                          
                 [ 1.    ,  0.    , -1.    ,  0.    , -4.    ,  4.    ,  0.    ,
                   0.    ,  0.    ,  0.    ,  0.    ],
                 [ 1.    ,  0.    ,  0.    , -1.    , -4.    ,  0.    ,  0.    ,
                   0.    ,  4.    ,  0.    ,  0.    ]],

                [[ 1.    , -1.    ,  0.    ,  0.    ,  0.    ,  4.    , -4.    ,
                   0.    ,  0.    ,  0.    ,  0.    ],
                 [ 1.    ,  0.    ,  3.    ,  0.    ,  0.    ,  0.    , -4.    ,
                   0.    ,  0.    ,  0.    ,  0.    ],
                 [ 1.    ,  0.    ,  0.    , -1.    ,  0.    ,  0.    , -4.    ,
                   0.    ,  0.    ,  4.    ,  0.    ]],

                [[ 1.    , -1.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,
                   -4.    ,  4.    ,  0.    ,  0.    ],
                 [ 1.    ,  0.    , -1.    ,  0.    ,  0.    ,  0.    ,  0.    ,
                   -4.    ,  0.    ,  4.    ,  0.    ],
                 [ 1.    ,  0.    ,  0.    ,  3.    ,  0.    ,  0.    ,  0.    ,
                   -4.    ,  0.    ,  0.    ,  0.    ]],

                [[-0.8048, -0.4048, -0.2048, -0.2048,  0.5952,  0.5952, -1.0048,
                  -1.0048,  0.5952, -0.2048,  2.048 ],
                 [-0.8048, -0.2048, -0.4048, -0.2048, -1.0048,  0.5952,  0.5952,
                  -1.0048, -0.2048,  0.5952,  2.048 ],
                 [-0.8048, -0.2048, -0.2048, -0.4048, -1.0048, -0.2048, -1.0048,
                  0.5952,  0.5952,  0.5952,  2.048 ]]]),
}

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        from sfepy.discrete.fem.geometry_element import GeometryElement

        gels = {}
        for key in ['2_3', '2_4', '3_4', '3_8']:
            gel = GeometryElement(key)
            gels[key] = gel

        return Test(conf=conf, options=options, gels=gels)

    def test_base_functions_values(self):
        """
        Compare base function values and their gradients with correct
        data. Also test that sum of values over all element nodes gives one.
        """
        from sfepy.base.base import ordered_iteritems
        from sfepy.discrete.fem.poly_spaces import PolySpace

        ok = True

        for key, val in ordered_iteritems(test_bases):
            gel = self.gels[key[:3]]
            diff = key[-4:] == 'grad'
            order = int(key[5])
            force_bubble = key[6:7] == 'B'
            
            ps = PolySpace.any_from_args('aux', gel, order,
                                         base='lagrange',
                                         force_bubble=force_bubble)
            dim = ps.geometry.dim
            coors = nm.r_[ps.geometry.coors, [[0.2] * dim]]

            bf = ps.eval_base(coors, diff=diff)
            _ok = nm.allclose(val, bf, rtol=0.0, atol=1e-14)
            ## if not _ok:
            ##     nm.set_printoptions(threshold=1000000, linewidth=65)
            ##     print bf.__repr__()

            if not diff:
                _ok = _ok and nm.allclose(bf.sum(axis=2), 1.0,
                                          rtol=0.0, atol=1e-14)

            self.report('%s: %s' % (key, _ok))

            ok = ok and _ok
            
        return ok

    def test_base_functions_delta(self):
        """
        Test :math:`\delta` property of base functions evaluated in the
        reference element nodes.
        """
        from sfepy.base.base import ordered_iteritems
        from sfepy.discrete.fem.poly_spaces import PolySpace

        ok = True

        for key, gel in ordered_iteritems(self.gels):
            for order in range(11):
                ps = PolySpace.any_from_args('aux', gel, order,
                                             base='lagrange',
                                             force_bubble=False)
                bf = ps.eval_base(ps.node_coors)
                _ok = nm.allclose(nm.eye(ps.n_nod),
                                  bf.squeeze(),
                                  rtol=0.0, atol=(order + 1) * 1e-14)

                self.report('%s order %d (n_nod: %d): %s'
                            % (key, order, ps.n_nod, _ok))

                if not _ok:
                    import pdb; pdb.set_trace()

            ok = ok and _ok

        return ok

########NEW FILE########
__FILENAME__ = test_functions
# c: 14.04.2008, r: 14.04.2008
import numpy as nm

from sfepy import data_dir

filename_mesh = data_dir + '/meshes/2d/square_unit_tri.mesh'

def get_pars(ts, coors, mode=None, extra_arg=None,
             equations=None, term=None, problem=None, **kwargs):
    if mode == 'special':
        if extra_arg == 'hello!':
            ic = 0
        else:
            ic = 1
        coors = problem.get_mesh_coors()
        return {('x_%s' % ic) : coors[:,ic]}

def get_p_edge(ts, coors, bc=None, **kwargs):
    if bc.name == 'p_left':
        return nm.sin(nm.pi * coors[:,1])
    else:
        return nm.cos(nm.pi * coors[:,1])

def get_circle(coors, domain=None):
    r = nm.sqrt(coors[:,0]**2.0 + coors[:,1]**2.0)
    return nm.where(r < 0.2)[0]

functions = {
    'get_pars1' : (lambda ts, coors, mode=None, **kwargs:
                   get_pars(ts, coors, mode, extra_arg='hello!', **kwargs),),
    'get_p_edge' : (get_p_edge,),
    'get_circle' : (get_circle,),
}

# Just another way of adding a function, besides 'functions' keyword.
function_1 = {
    'name' : 'get_pars2',
    'function' : lambda ts, coors, mode=None, **kwargs:
        get_pars(ts, coors, mode, extra_arg='hi!', **kwargs),
}

materials = {
    'mf1' : (None, 'get_pars1'),
    'mf2' : 'get_pars2',
    # Dot denotes a special value, that is not propagated to all QP.
    'mf3' : ({'a' : 10.0, 'b' : 2.0, '.c' : 'ahoj'},),
}

fields = {
    'pressure' : (nm.float64, 1, 'Omega', 2),
}

variables = {
    'p'   : ('unknown field', 'pressure', 0),
    'q'   : ('test field',    'pressure', 'p'),
}

wx = 0.499
regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < -%.3f)' % wx, 'facet'),
    'Right' : ('vertices in (x > %.3f)' % wx, 'facet'),
    'Circle' : 'vertices by get_circle',
}

ebcs = {
    'p_left' : ('Left', {'p.all' : 'get_p_edge'}),
    'p_right' : ('Right', {'p.all' : 'get_p_edge'}),
}

equations = {
    'e1' : """dw_laplace.2.Omega( mf3.a, q, p ) = 0""",
}

solver_0 = {
    'name' : 'ls',
    'kind' : 'ls.scipy_direct',
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',
}

from sfepy.base.base import assert_
from sfepy.base.testing import TestCommon

class Test( TestCommon ):

    def from_conf( conf, options ):
        from sfepy.discrete import Problem

        problem = Problem.from_conf(conf)
        test = Test(problem = problem, conf = conf, options = options)
        return test
    from_conf = staticmethod( from_conf )


    def test_material_functions(self):
        from sfepy.discrete import Material

        problem = self.problem
        conf = problem.conf

        ts = problem.get_default_ts(step=0)

        conf_mat1 = conf.get_item_by_name('materials', 'mf1')
        mat1 = Material.from_conf(conf_mat1, problem.functions)
        mat1.time_update(ts, None, mode='normal', problem=problem)

        coors = problem.domain.get_mesh_coors()
        assert_(nm.all(coors[:,0] == mat1.get_data(None, None, 'x_0')))

        conf_mat2 = conf.get_item_by_name('materials', 'mf2')
        mat2 = Material.from_conf(conf_mat2, problem.functions)
        mat2.time_update(ts, None, mode='normal', problem=problem)

        assert_(nm.all(coors[:,1] == mat2.get_data(None, None, 'x_1')))

        materials = problem.get_materials()
        materials.time_update(ts, problem.equations, mode='normal',
                              problem=problem)
        mat3 = materials['mf3']
        key = mat3.get_keys(region_name='Omega')[0]

        assert_(nm.all(mat3.get_data(key, 0, 'a') == 10.0))
        assert_(nm.all(mat3.get_data(key, 0, 'b') == 2.0))
        assert_(mat3.get_data(None, None, 'c') == 'ahoj')

        return True

    def test_ebc_functions(self):
        import os.path as op
        problem = self.problem

        problem.set_equations(self.conf.equations) 

        problem.time_update()
        state = problem.solve()
        name = op.join(self.options.out_dir,
                       op.splitext(op.basename(__file__))[0] + '_ebc.vtk')
        problem.save_state(name, state)

        ok = True
        domain = problem.domain

        vec = state()

        iv = domain.regions['Left'].get_vertices(0)
        coors = domain.get_mesh_coors()[iv]
        ok = ok and self.compare_vectors(vec[iv], nm.sin(nm.pi * coors[:,1]),
                                         label1='state_left', label2='bc_left')

        iv = domain.regions['Right'].get_vertices(0)
        coors = domain.get_mesh_coors()[iv]
        ok = ok and self.compare_vectors(vec[iv], nm.cos(nm.pi * coors[:,1]),
                                         label1='state_right', label2='bc_right')

        return ok

    def test_region_functions(self):
        import os.path as op
        problem = self.problem

        name = op.join(self.options.out_dir,
                       op.splitext(op.basename(__file__))[0])
        problem.save_regions(name, ['Circle'])

        return True

########NEW FILE########
__FILENAME__ = test_high_level
import time
import os.path as op
import numpy as nm

from sfepy.base.testing import TestCommon

def fix_u_fun(ts, coors, bc=None, problem=None, extra_arg=None):
    return nm.zeros_like(coors)

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        import sfepy
        from sfepy.discrete.fem import Mesh, FEDomain, Field
        mesh = Mesh.from_file('meshes/2d/rectangle_tri.mesh',
                              prefix_dir=sfepy.data_dir)
        domain = FEDomain('domain', mesh)
        dim = domain.shape.dim

        min_x, max_x = domain.get_mesh_bounding_box()[:,0]
        eps = 1e-8 * (max_x - min_x)

        omega = domain.create_region('Omega', 'all')
        gamma1 = domain.create_region('Gamma1',
                                      'vertices in x < %.10f' % (min_x + eps),
                                      'facet')
        gamma2 = domain.create_region('Gamma2',
                                      'vertices in x > %.10f' % (max_x - eps),
                                      'facet')

        field = Field.from_args('fu', nm.float64, 'vector', omega,
                                approx_order=2)

        test = Test(conf=conf, options=options, dim=dim,
                    omega=omega, gamma1=gamma1, gamma2=gamma2,
                    field=field)
        return test

    def test_term_evaluation(self):
        from sfepy.discrete import Integral, FieldVariable
        from sfepy.terms.terms import Term

        integral = Integral('i', order=3)

        u = FieldVariable('u', 'parameter', self.field,
                          primary_var_name='(set-to-None)')

        term = Term.new('d_volume(u)', integral, self.omega, u=u)
        term *= 10.0

        term.setup()

        vol = term.evaluate()

        self.report('volume: %.8f == 2000.0' % vol)
        ok = nm.allclose(vol, 2000.0, rtol=1e-15, atol=0)

        ## vec = t1.evaluate() # Returns vector.
        ## vec = t1.evaluate(u=u_vec) # Returns the same vector.
        ## mtx = t1.evaluate(diff_var='u') # Returns matrix.
        ## val = t1.evaluate(v=u_vec, u=u_vec) # Forbidden - virtual variable
        ##                                     # cannot have value.

        return ok

    def test_term_arithmetics(self):
        from sfepy.discrete import FieldVariable, Integral
        from sfepy.terms.terms import Term

        integral = Integral('i', order=3)

        u = FieldVariable('u', 'parameter', self.field,
                          primary_var_name='(set-to-None)')

        t1 = Term.new('d_volume(u)', integral, self.omega, u=u)
        t2 = Term.new('d_surface(u)', integral, self.gamma1, u=u)

        expr = 2.2j * (t1 * 5.5 - 3j * t2) * 0.25

        ok = len(expr) == 2
        if not ok:
            self.report('wrong expression length!')

        _ok = nm.allclose(expr[0].sign, 3.025j, rtol=1e-15, atol=0)
        if not _ok:
            self.report('wrong sign of the first term!')
        ok = ok and _ok

        _ok = nm.allclose(expr[1].sign, 1.65, rtol=1e-15, atol=0)
        if not _ok:
            self.report('wrong sign of the second term!')
        ok = ok and _ok

        return ok

    def test_variables(self):
        from sfepy.discrete import FieldVariable

        u = FieldVariable('u', 'parameter', self.field,
                          primary_var_name='(set-to-None)')

        u.set_constant(1.0)

        vec = u() # Nodal values.

        ok = nm.allclose(vec, 1.0)

        ## print u() 
        ## print u.get_vector() # Coefficient vector w.r.t. the field space basis.
        ## print u(gamma1)
        ## print u.get_vector(gamma2)

        return ok

    def test_solving(self):
        from sfepy.base.base import IndexedStruct
        from sfepy.discrete import (FieldVariable, Material, Problem, Function,
                                    Equation, Equations, Integral)
        from sfepy.discrete.conditions import Conditions, EssentialBC
        from sfepy.terms import Term
        from sfepy.solvers.ls import ScipyDirect
        from sfepy.solvers.nls import Newton

        u = FieldVariable('u', 'unknown', self.field)
        v = FieldVariable('v', 'test', self.field, primary_var_name='u')

        m = Material('m', lam=1.0, mu=1.0)
        f = Material('f', val=[[0.02], [0.01]])

        bc_fun = Function('fix_u_fun', fix_u_fun,
                          extra_args={'extra_arg' : 'hello'})

        fix_u = EssentialBC('fix_u', self.gamma1, {'u.all' : bc_fun})
        shift_u = EssentialBC('shift_u', self.gamma2, {'u.0' : 0.1})

        integral = Integral('i', order=3)

        t1 = Term.new('dw_lin_elastic_iso(m.lam, m.mu, v, u)',
                      integral, self.omega, m=m, v=v, u=u)

        t2 = Term.new('dw_volume_lvf(f.val, v)', integral, self.omega, f=f, v=v)

        eq = Equation('balance', t1 + t2)
        eqs = Equations([eq])

        ls = ScipyDirect({})

        nls_status = IndexedStruct()
        nls = Newton({}, lin_solver=ls, status=nls_status)

        pb = Problem('elasticity', equations=eqs, nls=nls, ls=ls)
        ## pb.save_regions_as_groups('regions')

        pb.time_update(ebcs=Conditions([fix_u, shift_u]))

        state = pb.solve()

        name = op.join(self.options.out_dir, 'test_high_level_solving.vtk')
        pb.save_state(name, state)

        ok = nls_status.condition == 0
        if not ok:
            self.report('solver did not converge!')

        _ok = state.has_ebc()
        if not _ok:
            self.report('EBCs violated!')

        ok = ok and _ok

        return ok

########NEW FILE########
__FILENAME__ = test_homogenization_perfusion
input_name = '../examples/homogenization/perfusion_micro.py'

from sfepy.base.testing import TestCommon

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        return Test(conf = conf, options = options)

    def compare_scalars(s1, s2, l1= 's1', l2 = 's2',
                        allowed_error = 1e-8):

        diff  = abs(s1 - s2)
        TestCommon.report( '|%s - %s|: %e' % (l1, l2, diff))
        if diff > allowed_error:
            return False
        else:
            return True
    compare_scalars = staticmethod(compare_scalars)

    def test_solution(self):

        from sfepy.base.base import Struct
        from sfepy.base.conf import ProblemConf, get_standard_keywords
        from sfepy.homogenization.homogen_app import HomogenizationApp
        #import numpy as nm
        import os.path as op

        ok = True

        required, other = get_standard_keywords()
        required.remove('equations')
        print input_name
        full_name = op.join(op.dirname(__file__), input_name)
        test_conf = ProblemConf.from_file(full_name, required, other)

        options = Struct(output_filename_trunk=None,
                         save_ebc=False,
                         save_ebc_nodes=False,
                         save_regions=False,
                         save_field_meshes=False,
                         save_regions_as_groups=False,
                         solve_not=False)

        test_conf.options['output_dir'] = './output-tests'
        app = HomogenizationApp(test_conf, options, 'homogen:' )
        coefs = app()

        aerr = 1.0e-9
        self.report('allowed error: abs = %e' % (aerr, ))

        # G^A = G^B ?
        ok = ok and self.compare_scalars(coefs.GA, coefs.GB,\
                                         'G^A', 'G^B', aerr)

        # F^{A+} + F^{B+} = -1/h \int_{\partial_+Y_m} ?
        aux = 1.0 / test_conf.param_h * coefs.volume['bYMp']
        ok = ok and self.compare_scalars(coefs.FpA + coefs.FpB, -aux,
                                         'F^{A+} + F^{B+}', '-bYM^+', aerr)

        # F^{A-} + F^{B-} = -1/h \int_{\partial_-Y_m} ?
        aux = 1.0 / test_conf.param_h * coefs.volume['bYMm']
        ok = ok and self.compare_scalars(coefs.FmA + coefs.FmB, -aux,
                                         'F^{A-} + F^{B-}', '-bYM^-', aerr)

        # symmetry of H ?
        ok = ok and self.compare_scalars(coefs.Hpm, coefs.Hmp,
                                         'H^{+-}', 'H^{-+}', aerr)

        # E = -F ?
        ok = ok and self.compare_scalars(coefs.EmA, -coefs.FmA,
                                         'E^{A-}', '-F^{A-}',aerr)
        ok = ok and self.compare_scalars(coefs.EpA, -coefs.FpA,
                                         'E^{A+}', '-F^{A+}',aerr)
        ok = ok and self.compare_scalars(coefs.EmB, -coefs.FmB,
                                         'E^{B-}', '-F^{B-}',aerr)
        ok = ok and self.compare_scalars(coefs.EpB, -coefs.FpB,
                                         'E^{B+}', '-F^{B+}',aerr)

        # S = S_test ?
        coefsd = coefs.to_dict()
        compare = []
        for ii in coefsd.iterkeys():
            if 'S_test' in ii:
                ch = ii[6]
                io = ii[-1]
                compare.append((ii, 'S%s_%s' % (ch, io)))

        for s1, s2 in compare:
            ok = ok and self.compare_vectors(coefsd[s1], -coefsd[s2],
                                             label1='S_test', label2='S',
                                             allowed_error=aerr)

        return ok

########NEW FILE########
__FILENAME__ = test_hyperelastic_tlul
input_names = {'TL': '../examples/large_deformation/hyperelastic.py',
               'UL': '../examples/large_deformation/hyperelastic_ul.py',
               'ULM': '../examples/large_deformation/hyperelastic_ul_up.py'}
output_name_trunk = 'test_hyperelastic_'

from sfepy.base.testing import TestCommon
from tests_basic import NLSStatus

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        return Test(conf = conf, options = options)

    def test_solution(self):

        from sfepy.base.base import Struct
        from sfepy.base.conf import ProblemConf, get_standard_keywords
        from sfepy.applications import solve_pde, assign_standard_hooks
        import numpy as nm
        import os.path as op

        solutions = {}
        ok = True

        for hp, pb_filename in input_names.iteritems():

            required, other = get_standard_keywords()
            input_name = op.join(op.dirname(__file__), pb_filename)
            test_conf = ProblemConf.from_file(input_name, required, other)

            name = output_name_trunk + hp
            solver_options = Struct(output_filename_trunk=name,
                                    output_format='vtk',
                                    save_ebc=False, save_ebc_nodes=False,
                                    save_regions=False,
                                    save_regions_as_groups=False,
                                    save_field_meshes=False,
                                    solve_not=False)
            assign_standard_hooks(self, test_conf.options.get, test_conf)

            self.report( 'hyperelastic formulation: %s' % (hp, ) )

            status = NLSStatus(conditions=[])

            pb, state = solve_pde(test_conf,
                                  solver_options,
                                  nls_status=status,
                                  output_dir=self.options.out_dir,
                                  step_hook=self.step_hook,
                                  post_process_hook=self.post_process_hook,
                                  post_process_hook_final=self.post_process_hook_final)

            converged = status.condition == 0
            ok = ok and converged

            solutions[hp] = state.get_parts()['u']
            self.report('%s solved' % input_name)

        rerr = 1.0e-3
        aerr = nm.linalg.norm(solutions['TL'], ord=None) * rerr

        self.report('allowed error: rel = %e, abs = %e' % (rerr, aerr))
        ok = ok and self.compare_vectors(solutions['TL'], solutions['UL'],
                                         label1='TLF',
                                         label2='ULF',
                                         allowed_error=rerr)

        ok = ok and self.compare_vectors(solutions['UL'], solutions['ULM'],
                                         label1='ULF',
                                         label2='ULF_mixed',
                                         allowed_error=rerr)

        return ok

########NEW FILE########
__FILENAME__ = test_input_acoustics
input_name = '../examples/acoustics/acoustics.py'
output_name = 'test_acoustics.vtk'

from tests_basic import TestInput
class Test( TestInput ):
    pass

########NEW FILE########
__FILENAME__ = test_input_acoustics3d
input_name = '../examples/acoustics/acoustics3d.py'
output_name = 'test_acoustics3d'

from tests_basic import TestInput
class Test(TestInput):
    pass

########NEW FILE########
__FILENAME__ = test_input_active_fibres
input_name = '../examples/large_deformation/active_fibres.py'
output_name_trunk = 'test_active_fibres'

from tests_basic import TestInputEvolutionary

class Test(TestInputEvolutionary):

    @staticmethod
    def from_conf(conf, options):
        return TestInputEvolutionary.from_conf(conf, options, cls=Test)

    def check_conditions(self, conditions):
        """
        Special-case the first iteration, as the solver converges slowly there.
        """
        ok = (conditions[1:] == 0).all()
        ok = ok and (conditions[0] == 1)

        if not ok:
            self.report('nls stopping conditions:')
            self.report(conditions)

        return ok

########NEW FILE########
__FILENAME__ = test_input_biot
input_name = '../examples/biot/biot.py'
output_name = 'test_biot.vtk'

from tests_basic import TestInput

class Test( TestInput ):
    pass

########NEW FILE########
__FILENAME__ = test_input_biot_npbc
input_name = '../examples/biot/biot_npbc.py'
output_name = 'test_biot_npbc.vtk'

from tests_basic import TestInput
class Test( TestInput ):
    pass

########NEW FILE########
__FILENAME__ = test_input_biot_npbc_lagrange
input_name = '../examples/biot/biot_npbc_lagrange.py'
output_name = 'test_biot_npbc_lagrange.vtk'

from tests_basic import TestInput
class Test(TestInput):
    pass

########NEW FILE########
__FILENAME__ = test_input_cube
input_name = '../examples/diffusion/cube.py'
output_name = 'test_cube.vtk'

from tests_basic import TestInput
class Test( TestInput ):
    pass

########NEW FILE########
__FILENAME__ = test_input_elastic_contact_planes
input_name = '../examples/linear_elasticity/elastic_contact_planes.py'
output_name = 'test_elastic_contact_planes.vtk'

from tests_basic import TestInput
class Test(TestInput):
    pass

########NEW FILE########
__FILENAME__ = test_input_elastic_contact_sphere
input_name = '../examples/linear_elasticity/elastic_contact_sphere.py'
output_name = 'test_elastic_contact_sphere.vtk'

from tests_basic import TestInput
class Test(TestInput):
    pass

########NEW FILE########
__FILENAME__ = test_input_its2D_2
input_name = '../examples/linear_elasticity/its2D_2.py'
output_name = 'test_its2D_2.vtk'

from tests_basic import TestInput
class Test(TestInput):
    pass

########NEW FILE########
__FILENAME__ = test_input_laplace_time_ebcs
input_name = '../examples/diffusion/laplace_time_ebcs.py'
output_name_trunk = 'test_laplace_time_ebcs'

from tests_basic import TestInputEvolutionary

class Test(TestInputEvolutionary):
    pass

########NEW FILE########
__FILENAME__ = test_input_linear_elastic
input_name = '../examples/linear_elasticity/linear_elastic.py'
output_name = 'test_linear_elastic.vtk'

from tests_basic import TestInput
class Test( TestInput ):
    pass

########NEW FILE########
__FILENAME__ = test_input_linear_elastic_damping
input_name = '../examples/linear_elasticity/linear_elastic_damping.py'
output_name_trunk = 'test_linear_elastic_damping'

from tests_basic import TestInputEvolutionary

class Test( TestInputEvolutionary ):
    pass

########NEW FILE########
__FILENAME__ = test_input_linear_elastic_mM
input_name = '../examples/homogenization/linear_elastic_mM.py'
output_name = 'test_linear_elastic_mM.vtk'

from tests_basic import TestInput
class Test( TestInput ):
    pass

########NEW FILE########
__FILENAME__ = test_input_linear_elastic_tractions
input_name = '../examples/linear_elasticity/linear_elastic_tractions.py'
output_name = 'test_linear_elastic_tractions.vtk'

from tests_basic import TestInput
class Test(TestInput):
    pass

########NEW FILE########
__FILENAME__ = test_input_linear_elastic_up
input_name = '../examples/linear_elasticity/linear_elastic_up.py'
output_name = 'test_linear_elastic_up.vtk'

from tests_basic import TestInput
class Test( TestInput ):
    pass

########NEW FILE########
__FILENAME__ = test_input_linear_viscoelastic
input_name = '../examples/linear_elasticity/linear_viscoelastic.py'
output_name_trunk = 'test_linear_viscoelastic'

from tests_basic import TestInputEvolutionary

class Test(TestInputEvolutionary):
    pass

########NEW FILE########
__FILENAME__ = test_input_material_nonlinearity
input_name = '../examples/linear_elasticity/material_nonlinearity.py'
output_name = 'test_material_nonlinearity.vtk'

from tests_basic import TestInput
class Test(TestInput):
    pass

########NEW FILE########
__FILENAME__ = test_input_navier_stokes
input_name = '../examples/navier_stokes/navier_stokes.py'
output_name = 'test_navier_stokes.vtk'

from tests_basic import TestInput
class Test( TestInput ):
    pass

########NEW FILE########
__FILENAME__ = test_input_navier_stokes2d
input_name = '../examples/navier_stokes/navier_stokes2d.py'
output_name = 'test_navier_stokes2d.vtk'

from tests_basic import TestInput
class Test(TestInput):
    pass

########NEW FILE########
__FILENAME__ = test_input_octahedron
# c: 15.02.2008, r: 15.02.2008
input_name = '../examples/diffusion/octahedron.py'
output_name = 'test_octahedron.vtk'

from tests_basic import TestInput
class Test( TestInput ):
    pass

########NEW FILE########
__FILENAME__ = test_input_perfusion_tl
input_name = '../examples/large_deformation/perfusion_tl.py'
output_name_trunk = 'test_perfusion_tl'

from tests_basic import TestInputEvolutionary

class Test(TestInputEvolutionary):
    pass

########NEW FILE########
__FILENAME__ = test_input_piezo
input_name = '../examples/piezo_elasticity/piezo.py'
output_name = 'test_piezo.vtk'


from tests_basic import TestInput

class Test( TestInput ):

    def from_conf( conf, options ):
        return TestInput.from_conf( conf, options, cls = Test )
    from_conf = staticmethod( from_conf )

    def test_ebc( self ):
        import numpy as nm
        from sfepy.discrete import Problem

        pb = Problem.from_conf(self.test_conf)
        pb.time_update()

        vvs = pb.get_variables()
        setv = vvs.set_state_part
        make_full = vvs.make_full_vec

        svec_u = nm.ones( (vvs.adi.n_dof['u'],), dtype = nm.float64 )
        svec_phi = nm.empty( (vvs.adi.n_dof['phi'],), dtype = nm.float64 )
        svec_phi.fill( 2.0 )

        svec = vvs.create_stripped_state_vector()
        setv( svec, svec_u, 'u', stripped = True )
        setv( svec, svec_phi, 'phi', stripped = True )

        vec = make_full( svec )

        ii_u = vvs.di.indx['u'].start + vvs['u'].eq_map.eqi
        ii_phi = vvs.di.indx['phi'].start + vvs['phi'].eq_map.eqi

        ok_ebc = vvs.has_ebc( vec )
        ok_u = nm.all( vec[ii_u] == svec_u )
        ok_phi = nm.all( vec[ii_phi] == svec_phi )

        msg = '%s: %s'
        self.report( msg % ('ebc', ok_ebc) )
        self.report( msg % ('u', ok_u) )
        self.report( msg % ('phi', ok_phi) )

        ok = ok_ebc and ok_u and ok_phi

        return ok

########NEW FILE########
__FILENAME__ = test_input_poisson
input_name = '../examples/diffusion/poisson.py'
output_name = 'test_poisson.vtk'

from tests_basic import TestInput
class Test( TestInput ):
    pass

########NEW FILE########
__FILENAME__ = test_input_poisson_field_dependent_material
input_name = '../examples/diffusion/poisson_field_dependent_material.py'
output_name_trunk = 'test_poisson_field_dependent_material'

from tests_basic import TestInputEvolutionary

class Test(TestInputEvolutionary):
    pass

########NEW FILE########
__FILENAME__ = test_input_poisson_functions
input_name = '../examples/diffusion/poisson_functions.py'
output_name = 'test_poisson_functions.vtk'

from tests_basic import TestInput
class Test(TestInput):
    pass

########NEW FILE########
__FILENAME__ = test_input_poisson_neumann
input_name = '../examples/diffusion/poisson_neumann.py'
output_name = 'test_poisson_neumann.vtk'

from tests_basic import TestInput
class Test(TestInput):
    pass

########NEW FILE########
__FILENAME__ = test_input_poisson_periodic_boundary_condition
input_name = '../examples/diffusion/poisson_periodic_boundary_condition.py'
output_name_trunk = 'test_poisson_periodic_boundary_condition'

from tests_basic import TestInputEvolutionary

class Test(TestInputEvolutionary):
    pass

########NEW FILE########
__FILENAME__ = test_input_prestress_fibres
input_name = '../examples/linear_elasticity/prestress_fibres.py'
output_name = 'test_prestress_fibres.vtk'

from tests_basic import TestInput

class Test(TestInput):
    pass

########NEW FILE########
__FILENAME__ = test_input_sinbc
# c: 20.03.2008, r: 20.03.2008
input_name = '../examples/diffusion/sinbc.py'
output_name = 'test_sinbc.vtk'

from tests_basic import TestInput
class Test( TestInput ):
    pass

########NEW FILE########
__FILENAME__ = test_input_stabilized_navier_stokes
input_name = '../examples/navier_stokes/stabilized_navier_stokes.py'
output_name = 'test_stabilized_navier_stokes.vtk'

from tests_basic import TestInput
class Test( TestInput ):
    pass

########NEW FILE########
__FILENAME__ = test_input_stokes
# 05.06.2007, c
# last revision: 05.06.2007

input_name = '../examples/navier_stokes/stokes.py'
output_name = 'test_stokes.vtk'

from tests_basic import TestInput
class Test( TestInput ):
    pass

########NEW FILE########
__FILENAME__ = test_input_stokes_slip_bc
input_name = '../examples/navier_stokes/stokes_slip_bc.py'
output_name = 'test_stokes_slip_bc.vtk'

from tests_basic import TestInput
class Test(TestInput):
    pass

########NEW FILE########
__FILENAME__ = test_input_thermo_elasticity
input_name = '../examples/thermo_elasticity/thermo_elasticity.py'
output_name = 'test_thermo_elasticity.vtk'

from tests_basic import TestInput
class Test(TestInput):
    pass

########NEW FILE########
__FILENAME__ = test_input_thermo_elasticity_ess
input_name = '../examples/thermo_elasticity/thermo_elasticity_ess.py'
output_name = 'test_thermo_elasticity_ess.vtk'

from tests_basic import TestInput
class Test(TestInput):
    pass

########NEW FILE########
__FILENAME__ = test_input_time_poisson
# c: 06.02.2008, r: 06.02.2008
input_name = '../examples/diffusion/time_poisson.py'
output_name_trunk = 'test_time_poisson'

from tests_basic import TestInputEvolutionary

class Test( TestInputEvolutionary ):
    pass

########NEW FILE########
__FILENAME__ = test_input_vibro_acoustic3d
input_name = '../examples/acoustics/vibro_acoustic3d.py'
output_name = 'test_vibro_acoustic3d'

from tests_basic import TestInput
class Test(TestInput):
    pass

########NEW FILE########
__FILENAME__ = test_io
from sfepy.base.base import assert_
from sfepy.base.testing import TestCommon
import numpy as nm
import scipy.sparse as sp
import os.path as op

##
# 02.07.2007, c
class Test( TestCommon ):

    ##
    # 02.07.2007, c
    def from_conf( conf, options ):
        return Test( conf = conf, options = options )
    from_conf = staticmethod( from_conf )

    ##
    # c: 02.07.2007, r: 12.06.2008
    def test_sparse_matrix_hdf5( self ):
        from sfepy.base.ioutils import write_sparse_matrix_hdf5, read_sparse_matrix_hdf5
        from sfepy.base.ioutils import pt
        if pt is None:
            self.report( 'skipped (no pytables)' )
            return True
        filename = op.join( self.options.out_dir, 'mtx.h5' )

        aux = nm.random.rand( 5, 5 )
        aux[1,:] = aux[:,2] = aux[3,:] = 0.0

        mtx = sp.csr_matrix( aux, dtype = nm.float64 )
#        self.report( 'sparse matrix:\n%s' % mtx )
        self.report( 'saving matrix into %s...' % filename )
        write_sparse_matrix_hdf5( filename, mtx )
        self.report( 'reading...' )
        mtx2 = read_sparse_matrix_hdf5( filename )
#        self.report( 'read matrix:\n%s' % mtx2 )
        self.report( 'difference:\n%s' % (mtx2 - mtx).__repr__() )

        assert_( mtx.shape == mtx2.shape )
        assert_( mtx.dtype == mtx2.dtype )
        assert_( mtx.format == mtx2.format )
        assert_( nm.allclose( mtx.data, mtx2.data ) )
        assert_( nm.allclose( mtx.indices, mtx2.indices ) )
        assert_( nm.allclose( mtx.indptr, mtx2.indptr ) )

        return True

    ##
    # c: 09.07.2007, r: 12.06.2008
    def test_recursive_dict_hdf5( self ):
        from sfepy.base.ioutils import write_dict_hdf5, read_dict_hdf5
        from sfepy.base.ioutils import pt
        if pt is None:
            self.report( 'skipped (no pytables)' )
            return True
        filename = op.join( self.options.out_dir, 'dict.h5' )

        test = {'A' : 0, 'B' : {'C' : [0, 1],
                                'D' : {'E' : {'F' : {'G' : 2.0}}}}}

        self.report( '%s' % test )
        self.report( 'saving into %s...' % filename )
        write_dict_hdf5( filename, test )
        self.report( 'reading...' )
        test2 = read_dict_hdf5( filename )
        self.report( '%s' % test2 )

        assert_( test == test2 )

        return True

########NEW FILE########
__FILENAME__ = test_laplace_unit_disk
# 31.05.2007, c
# last revision: 25.02.2008
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/2d/circle_sym.mesh'

material_1 = {
    'name' : 'coef',
    'values' : {
        'val' : 1.0,
    },
}
material_2 = {
    'name' : 'm',
    'values' : {
        'K' : [[1.0, 0.0], [0.0, 1.0]],
    },
}

field_1 = {
    'name' : 'a_harmonic_field',
    'dtype' : 'real',
    'shape' : 'scalar',
    'region' : 'Omega',
    'approx_order' : 2,
}

variable_1 = {
    'name' : 't',
    'kind' : 'unknown field',
    'field' : 'a_harmonic_field',
    'order' : 0,
}
variable_2 = {
    'name' : 's',
    'kind' : 'test field',
    'field' : 'a_harmonic_field',
    'dual' : 't',
}

region_1000 = {
    'name' : 'Omega',
    'select' : 'all',
}

region_1 = {
    'name' : 'Centre',
    'select' : 'vertices in (x < 1e-8) & (x > -1e-8) & (y < 1e-8) & (y > -1e-8)',
}

region_2 = {
    'name' : 'Gamma',
    'select' : 'vertices of surface',
    'kind' : 'facet',
}

ebc_1 = {
    'name' : 't_centre',
    'region' : 'Centre',
    'dofs' : {'t.0' : 1.0},
}
ebc_2 = {
    'name' : 't_gamma',
    'region' : 'Gamma',
    'dofs' : {'t.0' : 0.0},
}

integral_1 = {
    'name' : 'i',
    'order' : 2,
}

equations = {
    'Temperature' : """dw_laplace.i.Omega( coef.val, s, t ) = 0"""
}

solution = {
    't' : '- 5.0 * (x - 0.5)',
}

solver_0 = {
    'name' : 'ls',
    'kind' : 'ls.umfpack',
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 1,
    'eps_a'      : 1e-10,
}

from sfepy.base.testing import TestCommon

##
# 31.05.2007, c
class Test( TestCommon ):

    ##
    # 30.05.2007, c
    def from_conf( conf, options ):
        from sfepy.applications import solve_pde

        problem, state = solve_pde(conf, save_results=False)

        test = Test(problem=problem, state=state, conf=conf, options=options)
        return test
    from_conf = staticmethod( from_conf )

    ##
    # 31.05.2007, c
    # 02.10.2007
    def test_boundary_fluxes( self ):
        from sfepy.discrete.evaluate import BasicEvaluator
        from sfepy.discrete import Material
        problem = self.problem

        region_names = ['Gamma']

        variables = problem.get_variables()
        get_state = variables.get_state_part_view
        state = self.state.copy(deep=True)

        problem.time_update(ebcs={}, epbcs={})
        ## problem.save_ebc( 'aux.vtk' )

        state.apply_ebc()
        ev = BasicEvaluator( problem )
        aux = ev.eval_residual(state())

        field = variables['t'].field

        conf_m = problem.conf.get_item_by_name('materials', 'm')
        m = Material.from_conf(conf_m, problem.functions)

        ok = True
        for ii, region_name in enumerate( region_names ):
            flux_term = 'd_surface_flux.1.%s( m.K, t )' % region_name
            val1 = problem.evaluate(flux_term, t=variables['t'], m=m)

            rvec = get_state( aux, 't', True )
            reg = problem.domain.regions[region_name]
            nods = field.get_dofs_in_region(reg, merge=True)
            val2 = rvec[nods].sum() # Assume 1 dof per node.

            eps = 1e-2
            ok = ok and ((abs( val1 - val2 ) < eps))
            self.report( '%d. %s: |%e - %e| = %e < %.2e'\
                         % (ii, region_name, val1, val2, abs( val1 - val2 ),
                            eps) )

        return ok

########NEW FILE########
__FILENAME__ = test_laplace_unit_square
# 30.05.2007, c
# last revision: 25.02.2008
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/2d/square_unit_tri.mesh'

material_1 = {
    'name' : 'coef',
    'values' : {
        'val' : 1.0,
    },
}
material_2 = {
    'name' : 'm',
    'values' : {
        'K' : [[1.0, 0.0], [0.0, 1.0]],
    },
}

field_1 = {
    'name' : 'a_harmonic_field',
    'dtype' : 'real',
    'shape' : 'scalar',
    'region' : 'Omega',
    'approx_order' : 2,
}

variable_1 = {
    'name' : 't',
    'kind' : 'unknown field',
    'field' : 'a_harmonic_field',
    'order' : 0,
}
variable_2 = {
    'name' : 's',
    'kind' : 'test field',
    'field' : 'a_harmonic_field',
    'dual' : 't',
}

region_1000 = {
    'name' : 'Omega',
    'select' : 'all',
}

region_1 = {
    'name' : 'Left',
    'select' : 'vertices in (x < -0.499)',
    'kind' : 'facet',
}
region_2 = {
    'name' : 'Right',
    'select' : 'vertices in (x > 0.499)',
    'kind' : 'facet',
}
region_3 = {
    'name' : 'Gamma',
    'select' : 'vertices of surface',
    'kind' : 'facet',
}

ebc_1 = {
    'name' : 't_left',
    'region' : 'Left',
    'dofs' : {'t.0' : 5.0},
}
ebc_2 = {
    'name' : 't_right',
    'region' : 'Right',
    'dofs' : {'t.0' : 0.0},
}
#    'Left' : ('T3', (30,), 'linear_y'),

integral_1 = {
    'name' : 'i',
    'order' : 2,
}

equations = {
    'Temperature' : """dw_laplace.i.Omega( coef.val, s, t ) = 0"""
}

solution = {
    't' : '- 5.0 * (x - 0.5)',
}

solver_0 = {
    'name' : 'ls',
    'kind' : 'ls.umfpack',
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 1,
    'eps_a'      : 1e-10,
}

lin_min, lin_max = 0.0, 2.0

##
# 31.05.2007, c
def linear( bc, ts, coor, which ):
    vals = coor[:,which]
    min_val, max_val = vals.min(), vals.max()
    vals = (vals - min_val) / (max_val - min_val) * (lin_max - lin_min) + lin_min
    return vals

##
# 31.05.2007, c
def linear_x( bc, ts, coor ):
    return linear( bc, ts, coor, 0 )
def linear_y( bc, ts, coor ):
    return linear( bc, ts, coor, 1 )
def linear_z( bc, ts, coor ):
    return linear( bc, ts, coor, 2 )

from sfepy.base.testing import TestCommon

##
# 30.05.2007, c
class Test( TestCommon ):

    ##
    # 30.05.2007, c
    def from_conf( conf, options ):
        from sfepy.applications import solve_pde

        problem, state = solve_pde(conf, save_results=False)

        test = Test(problem=problem, state=state, conf=conf, options=options)
        return test
    from_conf = staticmethod( from_conf )

    ##
    # 30.05.2007, c
    def test_solution( self ):
        sol = self.conf.solution
        vec = self.state()
        problem = self.problem

        variables = problem.get_variables()

        ok = True
        for var_name, expression in sol.iteritems():
            coor = variables[var_name].field.get_coor()
            ana_sol = self.eval_coor_expression( expression, coor )
            num_sol = variables.get_state_part_view( vec, var_name )
            ret = self.compare_vectors( ana_sol, num_sol,
                                       label1 = 'analytical %s' % var_name,
                                       label2 = 'numerical %s' % var_name )
            if not ret:
                self.report( 'variable %s: failed' % var_name )

            ok = ok and ret

        return ok

    ##
    # c: 30.05.2007, r: 19.02.2008
    def test_boundary_fluxes( self ):
        import os.path as op
        from sfepy.linalg import rotation_matrix2d
        from sfepy.discrete.evaluate import BasicEvaluator
        from sfepy.discrete import Material
        problem = self.problem

        angles = [0, 30, 45]
        region_names = ['Left', 'Right', 'Gamma']
        values = [5.0, -5.0, 0.0]

        variables = problem.get_variables()
        get_state = variables.get_state_part_view
        state = self.state.copy(deep=True)

        problem.time_update(ebcs={}, epbcs={})
#        problem.save_ebc( 'aux.vtk' )

        state.apply_ebc()
        ev = BasicEvaluator( problem )
        aux = ev.eval_residual(state())

        field = variables['t'].field

        conf_m = problem.conf.get_item_by_name('materials', 'm')
        m = Material.from_conf(conf_m, problem.functions)

        name = op.join( self.options.out_dir,
                        op.split( problem.domain.mesh.name )[1] + '_%02d.mesh' ) 

        orig_coors = problem.get_mesh_coors().copy()
        ok = True
        for ia, angle in enumerate( angles ):
            self.report( '%d: mesh rotation %d degrees' % (ia, angle) )
            problem.domain.mesh.transform_coors( rotation_matrix2d( angle ),
                                                 ref_coors = orig_coors )
            problem.set_mesh_coors(problem.domain.mesh.coors,
                                   update_fields=True)
            problem.domain.mesh.write( name % angle, io = 'auto' )
            for ii, region_name in enumerate( region_names ):
                flux_term = 'd_surface_flux.i.%s( m.K, t )' % region_name
                val1 = problem.evaluate(flux_term, t=variables['t'], m=m)

                rvec = get_state( aux, 't', True )
                reg = problem.domain.regions[region_name]
                nods = field.get_dofs_in_region(reg, merge=True)
                val2 = rvec[nods].sum() # Assume 1 dof per node.

                ok = ok and ((abs( val1 - values[ii] ) < 1e-10) and
                             (abs( val2 - values[ii] ) < 1e-10))
                self.report( '  %d. %s: %e == %e == %e'\
                             % (ii, region_name, val1, val2, values[ii]) )

        # Restore original coordinates.
        problem.domain.mesh.transform_coors(rotation_matrix2d(0),
                                            ref_coors=orig_coors)
        problem.set_mesh_coors(problem.domain.mesh.coors,
                               update_fields=True)

        return ok

########NEW FILE########
__FILENAME__ = test_lcbc_2d
# 03.10.2007, c
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/2d/special/circle_in_square.mesh'

# Whole domain $Y$.
region_1000 = {
    'name' : 'Y',
    'select' : 'all',
}

# Domain $Y_1$.
region_1 = {
    'name' : 'Y1',
    'select' : 'cells of group 1',
}

# Domain $Y_2$.
region_2 = {
    'name' : 'Y2',
    'select' : 'cells of group 2',
}

# Domain $Y_3$.
region_3 = {
    'name' : 'Y3',
    'select' : 'vertices in (x > %f) & (x < %f) & (y > %f) & (y < %f)'\
    % (-0.3, 0.3, -0.48, -0.3),
}

wx = wy = 0.499
region_10 = {
    'name' : 'Bottom',
    'select' : 'vertices in (y < %f)' % -wy,
    'kind' : 'facet',
}

region_11 = {
    'name' : 'Top',
    'select' : 'vertices in (y > %f)' % wy,
    'kind' : 'facet',
}

material_1 = {
    'name' : 'solid',

    'values' : {
        'lam' : 1e1,
        'mu' : 1e0,
        'density' : 1e-1,
    },
}

field_1 = {
    'name' : '2_displacement',
    'dtype' : 'real',
    'shape' : 'vector',
    'region' : 'Y',
    'approx_order' : 2,
}

variable_1 = {
    'name' : 'u',
    'kind' : 'unknown field',
    'field' : '2_displacement',
    'order' : 0,
}
variable_2 = {
    'name' : 'v',
    'kind' : 'test field',
    'field' : '2_displacement',
    'dual' : 'u',
}

ebc_1 = {
    'name' : 'Fix',
    'region' : 'Bottom',
    'dofs' : {'u.all' : 0.0},
}
ebc_2 = {
    'name' : 'Load',
    'region' : 'Top',
    'dofs' : {'u.all' : 0.2},
}

lcbc_1 = {
    'name' : 'rigid1',
    'region' : 'Y2',
    'dofs' : {'u.all' : 'rigid'},
}
lcbc_2 = {
    'name' : 'rigid2',
    'region' : 'Y3',
    'dofs' : {'u.all' : 'rigid'},
}

integral_1 = {
    'name' : 'i',
    'order' : 2,
}

equations = {
    'balance' : """dw_lin_elastic_iso.i.Y( solid.lam, solid.mu, v, u ) = 0""",
}

solver_0 = {
    'name' : 'ls',
    'kind' : 'ls.scipy_direct',
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 1,
    'eps_a'      : 1e-10,
}

from tests_basic import TestLCBC
output_name = 'test_lcbc_2d.vtk'

##
# 03.10.2007, c
class Test( TestLCBC ):
    pass

########NEW FILE########
__FILENAME__ = test_lcbc_3d
# 05.10.2007, c
# last revision: 25.02.2008
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/special/cube_sphere.mesh'

# Whole domain $Y$.
region_1000 = {
    'name' : 'Y',
    'select' : 'all',
}

# Domain $Y_1$.
region_1 = {
    'name' : 'Y1',
    'select' : 'cells of group 1',
}

# Domain $Y_2$.
region_2 = {
    'name' : 'Y2',
    'select' : 'cells of group 2',
}

region_10 = {
    'name' : 'Bottom',
    'select' : 'vertices in (z < %f)' % -0.499,
    'kind' : 'facet',
}

region_11 = {
    'name' : 'Top',
    'select' : 'vertices in (z > %f)' % 0.499,
    'kind' : 'facet',
}

material_1 = {
    'name' : 'solid',
    
    'values' : {
        'lam' : 1e1,
        'mu' : 1e0,
        'density' : 1e-1,
    },
}

field_1 = {
    'name' : '3_displacement',
    'dtype' : 'real',
    'shape' : 'vector',
    'region' : 'Y',
    'approx_order' : 1,
}

variable_1 = {
    'name' : 'u',
    'kind' : 'unknown field',
    'field' : '3_displacement',
    'order' : 0,
}
variable_2 = {
    'name' : 'v',
    'kind' : 'test field',
    'field' : '3_displacement',
    'dual' : 'u',
}

ebc_1 = {
    'name' : 'Fix',
    'region' : 'Bottom',
    'dofs' : {'u.all' : 0.0},
}
ebc_2 = {
    'name' : 'Load',
    'region' : 'Top',
    'dofs' : {'u.[0,1]' : 0.2, 'u.2' : 0.5},
}

lcbc_1 = {
    'name' : 'rigid1',
    'region' : 'Y2',
    'dofs' : {'u.all' : 'rigid'},
}

integral_1 = {
    'name' : 'i',
    'order' : 2,
}

equations = {
    'balance' : """dw_lin_elastic_iso.i.Y( solid.lam, solid.mu, v, u ) = 0""",
}

solver_0 = {
    'name' : 'ls',
    'kind' : 'ls.scipy_direct',
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 1,
    'eps_a'      : 1e-10,
}

from tests_basic import TestLCBC
output_name = 'test_lcbc_3d.vtk'

##
# 03.10.2007, c
class Test( TestLCBC ):
    pass

########NEW FILE########
__FILENAME__ = test_linalg
from sfepy.base.testing import TestCommon

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        return Test(conf=conf, options=options)

    def test_tensors(self):
        import numpy as nm
        from sfepy.linalg import dot_sequences, insert_strided_axis

        ok = True

        a = nm.arange(1, 10).reshape(3, 3)
        b = nm.arange(9, 0, -1).reshape(3, 3)

        dab = nm.dot(a, b)
        dabt = nm.dot(a, b.T)
        datb = nm.dot(a.T, b)
        datbt = nm.dot(a.T, b.T)

        sa = insert_strided_axis(a, 0, 10)
        sb = insert_strided_axis(b, 0, 10)

        dsab = dot_sequences(sa, sb, mode='AB')
        _ok = nm.allclose(dab[None, ...], dsab, rtol=0.0, atol=1e-14)
        self.report('dot_sequences AB: %s' % _ok)
        ok = ok and _ok

        dsabt = dot_sequences(sa, sb, mode='ABT')
        _ok = nm.allclose(dabt[None, ...], dsabt, rtol=0.0, atol=1e-14)
        self.report('dot_sequences ABT: %s' % _ok)
        ok = ok and _ok

        dsatb = dot_sequences(sa, sb, mode='ATB')
        _ok = nm.allclose(datb[None, ...], dsatb, rtol=0.0, atol=1e-14)
        self.report('dot_sequences ATB: %s' % _ok)
        ok = ok and _ok

        dsatbt = dot_sequences(sa, sb, mode='ATBT')
        _ok = nm.allclose(datbt[None, ...], dsatbt, rtol=0.0, atol=1e-14)
        self.report('dot_sequences ATBT: %s' % _ok)
        ok = ok and _ok

        return ok

    def test_unique_rows(self):
        import numpy as nm
        from sfepy.linalg import unique_rows

        a = nm.arange(1, 10).reshape(3, 3)

        b = nm.r_[a, a]
        c = unique_rows(b)

        ok = (a == c).all()

        return ok

    def test_assemble1d(self):
        import numpy as nm
        from sfepy.linalg import assemble1d

        a = nm.arange(5)
        b = nm.arange(2)

        assemble1d(b, [1, 1, 1, 1, 0, 0], a[[0, 2, 3, 4, 1, 1]])

        ok = (b == [2, 10]).all()

        return ok

    def test_geometry(self):
        import numpy as nm
        from sfepy.linalg import get_face_areas

        a1 = get_face_areas([[0, 1, 2, 3]],
                            [[0, 0], [1, 0], [1, 1], [0, 1]])

        a2 = get_face_areas([[0, 1, 2, 3]],
                            [[0, 0, 2], [1, 0, 2], [1, 1, 2], [0, 1, 2]])
        ok = nm.allclose([a1, a2], [1, 1], rtol=0, atol=1e-15)

        return ok

########NEW FILE########
__FILENAME__ = test_linearization
import os
import numpy as nm

from sfepy.base.testing import TestCommon

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        test = Test(conf=conf, options=options)
        test.join = lambda x: os.path.join(test.options.out_dir, x)
        return test

    def test_linearization(self):
        from sfepy.base.base import Struct
        from sfepy.discrete.fem import Mesh, FEDomain, Field
        from sfepy import data_dir

        geometries = ['2_3', '2_4', '3_4', '3_8']
        approx_orders = [1, 2]
        funs = [nm.cos, nm.sin, lambda x: x]

        ok = True
        for geometry in geometries:
            name = os.path.join(data_dir,
                                'meshes/elements/%s_1.mesh' % geometry)
            mesh = Mesh.from_file(name)

            domain = FEDomain('', mesh)
            domain = domain.refine()

            domain.mesh.write(self.join('linearizer-%s-0.mesh' % geometry))

            omega = domain.create_region('Omega', 'all')

            for approx_order in approx_orders:
                for dpn in [1, mesh.dim]:
                    self.report('geometry: %s, approx. order: %d, dpn: %d' %
                                (geometry, approx_order, dpn))

                    field = Field.from_args('fu', nm.float64, dpn, omega,
                                            approx_order=approx_order)

                    cc = field.get_coor()
                    dofs = nm.zeros((field.n_nod, dpn), dtype=nm.float64)

                    for ic in range(dpn):
                        dofs[:, ic] = funs[ic](3 * (cc[:, 0] * cc[:, 1]))

                    vmesh, vdofs, levels = field.linearize(dofs,
                                                           min_level=0,
                                                           max_level=3,
                                                           eps=1e-2)
                    level = levels[0]

                    if approx_order == 1:
                        _ok = level == 0

                    else:
                        _ok = level > 0
                    self.report('max. refinement level: %d: %s' % (level, _ok))

                    ok = ok and _ok

                    rdofs = nm.zeros((vmesh.n_nod, dpn), dtype=nm.float64)
                    cc = vmesh.coors
                    for ic in range(dpn):
                        rdofs[:, ic] = funs[ic](3 * (cc[:, 0] * cc[:, 1]))

                    _ok = nm.allclose(rdofs, vdofs, rtol=0.0, atol=0.03)
                    self.report('interpolation: %s' % _ok)
                    ok = ok and _ok

                    out = {
                        'u' : Struct(name='output_data',
                                     mode='vertex', data=vdofs,
                                     var_name='u', dofs=None)
                    }

                    name = self.join('linearizer-%s-%d-%d'
                                     % (geometry, approx_order, dpn))

                    vmesh.write(name + '.mesh')
                    vmesh.write(name + '.vtk', out=out)

        return ok

########NEW FILE########
__FILENAME__ = test_linear_solvers
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/special/cube_cylinder.mesh'

if 0:
    from sfepy.discrete.fem.utils import refine_mesh
    refinement_level = 1
    filename_mesh = refine_mesh(filename_mesh, refinement_level)

material_2 = {
    'name' : 'coef',
    'values' : {'val' : 1.0},
}

field_1 = {
    'name' : 'temperature',
    'dtype' : 'real',
    'shape' : (1,),
    'region' : 'Omega',
    'approx_order' : 1,
}

variables = {
    't' : ('unknown field', 'temperature', 0),
    's' : ('test field',    'temperature', 't'),
}

regions = {
    'Omega' : 'all',
    'Gamma_Left' : ('vertices in (x < 0.0001)', 'facet'),
    'Gamma_Right' : ('vertices in (x > 0.999)', 'facet'),
}

ebcs = {
    't1' : ('Gamma_Left', {'t.0' : 2.0}),
    't2' : ('Gamma_Right', {'t.0' : -2.0}),
}

integral_1 = {
    'name' : 'i',
    'order' : 1,
}

equations = {
    'Temperature' : """dw_laplace.i.Omega( coef.val, s, t ) = 0"""
}

solvers = {
    'd00' : ('ls.umfpack',
             {'warn' : True,}
    ),
##     'd01' : ('ls.scipy_direct',
##              {'method' : 'superlu',
##               'warn' : True,}
##     ),
    'i00' : ('ls.pyamg',
             {'method' : 'ruge_stuben_solver',
              'accel' : 'cg',
              'eps_r'   : 1e-12,}
    ),
    'i01' : ('ls.pyamg',
             {'method' : 'smoothed_aggregation_solver',
              'accel' : 'cg',
              'eps_r'   : 1e-12,}
    ),
    'i10' : ('ls.petsc',
             {'method' : 'cg', # ksp_type
              'precond' : 'icc', # pc_type
              'eps_a' : 1e-12, # abstol
              'eps_r' : 1e-12, # rtol
              'i_max' : 1000,} # maxits
    ),
    'i20' : ('ls.scipy_iterative',
             {'method' : 'cg',
              'i_max'   : 1000,
              'eps_r'   : 1e-12,}
    ),
    'i21' : ('ls.scipy_iterative',
             {'method' : 'bicgstab',
              'i_max'   : 1000,
              'eps_r'   : 1e-12,}
    ),
    'i22' : ('ls.scipy_iterative',
             {'method' : 'qmr',
              'i_max'   : 1000,
              'eps_r'   : 1e-12,}
    ),

    'newton' : ('nls.newton', {
        'i_max'      : 1,
        'eps_a'      : 1e-10,
    }),
}


options = {
    'nls' : 'newton',
}

from sfepy.base.testing import TestCommon
output_name = 'test_linear_solvers_%s.vtk'

##
# c: 02.05.2008
class Test( TestCommon ):
    can_fail = ['ls.pyamg', 'ls.petsc']

    ##
    # c: 02.05.2008, r: 02.05.2008
    def from_conf( conf, options ):
        from sfepy.discrete import Problem

        problem = Problem.from_conf(conf)
        problem.time_update()

        test = Test( problem = problem, 
                     conf = conf, options = options )
        return test
    from_conf = staticmethod( from_conf )

    ##
    # c: 02.05.2008, r: 02.05.2008
    def _list_linear_solvers( self, confs ):
        d = []
        for key, val in confs.iteritems():
            if val.kind.find( 'ls.' ) == 0:
                d.append( val )
        d.sort( cmp = lambda a, b: cmp( a.name, b.name ) )

        return d

    ##
    # c: 02.05.2008, r: 07.05.2008
    def test_solvers( self ):
        from sfepy.base.base import IndexedStruct
        import os.path as op

        solver_confs = self._list_linear_solvers( self.problem.solver_confs )

        ok = True
        tt = []
        for solver_conf in solver_confs:
            method = solver_conf.get('method', '')
            precond = solver_conf.get('precond', '')
            name = ' '.join( (solver_conf.name, solver_conf.kind,
                              method, precond) ).rstrip()
            self.report( name )
            self.report( 'matrix size:', self.problem.mtx_a.shape )
            self.report( '        nnz:', self.problem.mtx_a.nnz )
            status = IndexedStruct()
            try:
                self.problem.init_solvers( nls_status = status,
                                          ls_conf = solver_conf )
                state = self.problem.solve()
                failed = status.condition != 0
##                 self.problem.mtx_a.save( 'mtx_laplace_cube',
##                                         format='%d %d %.12e\n' )
            except Exception, exc:
                failed = True
                status = None

            ok = ok and ((not failed) or (solver_conf.kind in self.can_fail))

            if status is not None:
                for kv in status.time_stats.iteritems():
                    self.report( '%10s: %7.2f [s]' % kv )
                self.report( 'condition: %d, err0: %.3e, err: %.3e'\
                             % (status.condition, status.err0, status.err) )
                tt.append( [name, status.time_stats['solve'], status.err] )

                aux = name.replace(' ', '_')
                fname = op.join( self.options.out_dir,
                                op.split( self.conf.output_name )[1] ) % aux
                self.problem.save_state( fname, state )
            else:
                self.report( 'solver failed:' )
                self.report( exc )
                tt.append( [name, 1e10, 1e10] )


        tt.sort( cmp = lambda a, b: cmp( a[1], b[1] ) )
        self.report( 'solution times (rezidual norms):' )
        for row in tt:
            self.report( '%.2f [s]' % row[1], '(%.3e)' % row[2], ':', row[0] )

        return ok

########NEW FILE########
__FILENAME__ = test_matcoefs
from sfepy.base.testing import TestCommon

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        return Test(conf=conf, options=options)

    def test_elastic_constants(self):
        import numpy as nm
        from sfepy.mechanics.matcoefs import ElasticConstants

        ok = True

        names = ['bulk', 'lam', 'mu', 'young', 'poisson', 'p_wave']

        ec = ElasticConstants(lam=1.0, mu=1.5)
        vals = ec.get(names)

        self.report('using values:', vals)

        for i1 in range(len(names)):
            for i2 in range(i1+1, len(names)):
                kwargs = {names[i1] : vals[i1], names[i2] : vals[i2]}

                try:
                    ec.init(**kwargs)

                except:
                    _ok = False

                else:
                    _ok = True

                ec_vals = ec.get(names)
                _ok = _ok and nm.allclose(ec_vals, vals)

                self.report(names[i1], names[i2], '->', _ok)
                if not _ok:
                    self.report('correct:', vals)
                    self.report('    got:', ec_vals)

                ok = ok and _ok

        return ok

    def test_conversion_functions(self):
        import numpy as nm

        import sfepy.mechanics.matcoefs as mc

        ok = True

        lam = 1.0
        mu = 1.5

        ec = mc.ElasticConstants(lam=lam, mu=mu)
        young, poisson, bulk = ec.get(['young', 'poisson', 'bulk'])

        lam = nm.array([lam] * 3)
        mu = nm.array([mu] * 3)
        young = nm.array([young] * 3)
        poisson = nm.array([poisson] * 3)

        _lam, _mu = mc.lame_from_youngpoisson(young, poisson)
        _ok = (nm.allclose(lam, _lam, rtol=0.0, atol=1e-14) and
               nm.allclose(mu, _mu, rtol=0.0, atol=1e-14))
        self.report('lame_from_youngpoisson():', _ok)
        if not _ok:
            self.report('correct:', lam, mu)
            self.report('    got:', _lam, _mu)
        ok = ok and _ok

        _bulk = mc.bulk_from_youngpoisson(young, poisson)
        _ok = nm.allclose(bulk, _bulk, rtol=0.0, atol=1e-14)
        self.report('bulk_from_youngpoisson():', _ok)
        if not _ok:
            self.report('correct:', bulk)
            self.report('    got:', _bulk)
        ok = ok and _ok

        _bulk = mc.bulk_from_lame(lam, mu)
        _ok = nm.allclose(bulk, _bulk, rtol=0.0, atol=1e-14)
        self.report('bulk_from_lame():', _ok)
        if not _ok:
            self.report('correct:', bulk)
            self.report('    got:', _bulk)
        ok = ok and _ok

        return ok

    def test_stiffness_tensors(self):
        import numpy as nm

        from sfepy.base.base import assert_
        import sfepy.mechanics.matcoefs as mc

        ok = True

        lam = 1.0
        mu = 4.0

        lam = nm.array([lam] * 3)
        mu = nm.array([mu] * 3)

        d = nm.array([[ 9.,  1.,  1.,  0.,  0.,  0.],
                      [ 1.,  9.,  1.,  0.,  0.,  0.],
                      [ 1.,  1.,  9.,  0.,  0.,  0.],
                      [ 0.,  0.,  0.,  4.,  0.,  0.],
                      [ 0.,  0.,  0.,  0.,  4.,  0.],
                      [ 0.,  0.,  0.,  0.,  0.,  4.]])

        _ds = mc.stiffness_from_lame(3, lam, mu)
        assert_(_ds.shape == (3, 6, 6))

        _ok = True
        for _d in _ds:
            __ok = nm.allclose(_d, d, rtol=0.0, atol=1e-14)
            _ok = _ok and __ok
        self.report('stiffness_from_lame():', _ok)
        ok = ok and _ok

        d = 4.0 / 3.0 * nm.array([[ 4., -2., -2.,  0.,  0.,  0.],
                                  [-2.,  4., -2.,  0.,  0.,  0.],
                                  [-2., -2.,  4.,  0.,  0.,  0.],
                                  [ 0.,  0.,  0.,  3.,  0.,  0.],
                                  [ 0.,  0.,  0.,  0.,  3.,  0.],
                                  [ 0.,  0.,  0.,  0.,  0.,  3.]])

        _ds = mc.stiffness_from_lame_mixed(3, lam, mu)
        assert_(_ds.shape == (3, 6, 6))

        _ok = True
        for _d in _ds:
            __ok = nm.allclose(_d, d, rtol=0.0, atol=1e-14)
            _ok = _ok and __ok
        self.report('stiffness_from_lame_mixed():', _ok)
        ok = ok and _ok

        blam = - mu * 2.0 / 3.0
        _ds = mc.stiffness_from_lame(3, blam, mu)
        assert_(_ds.shape == (3, 6, 6))

        _ok = True
        for _d in _ds:
            __ok = nm.allclose(_d, d, rtol=0.0, atol=1e-14)
            _ok = _ok and __ok
        self.report('stiffness_from_lame() with modified lambda:', _ok)
        ok = ok and _ok

        return ok

########NEW FILE########
__FILENAME__ = test_meshio
from sfepy import data_dir

filename_meshes = ['/meshes/3d/cylinder.mesh',
                   '/meshes/3d/cylinder.vtk',
                   '/meshes/various_formats/small2d.mesh',
                   '/meshes/various_formats/small2d.vtk',
                   '/meshes/various_formats/octahedron.node',
                   '/meshes/various_formats/comsol_tri.txt',
                   '/meshes/various_formats/abaqus_hex.inp',
                   '/meshes/various_formats/abaqus_tet.inp',
                   '/meshes/various_formats/abaqus_quad.inp',
                   '/meshes/various_formats/abaqus_tri.inp',
                   '/meshes/various_formats/abaqus_quad_tri.inp',
                   '/meshes/various_formats/hex4.mesh3d',
                   '/meshes/various_formats/tetra8.mesh3d',
                   '/meshes/various_formats/cube.bdf',
                   '/meshes/various_formats/med_2d_tri_quad.med',
                   '/meshes/various_formats/med_3d_tet_hex.med']
filename_meshes = [data_dir + name for name in filename_meshes]

def mesh_hook(mesh, mode):
    """
    Define a mesh programmatically.
    """
    if mode == 'read':
        nodes = [[0, 0], [1, 0], [1, 1], [0, 1]]
        nod_ids = [0, 0, 1, 1]
        conns = [[[0, 1, 2], [0, 2, 3]]]
        mat_ids = [[0, 1]]
        descs = ['2_3']

        mesh._set_data(nodes, nod_ids, conns, mat_ids, descs)

        ## mesh.write('aux.vtk', io='auto')

    elif mode == 'write':
        pass

from sfepy.discrete.fem.meshio import UserMeshIO

filename_meshes.extend([mesh_hook, UserMeshIO(mesh_hook)])

same = [(0, 1), (2, 3)]

import os.path as op
from sfepy.base.base import assert_
from sfepy.base.testing import TestCommon

##
# c: 05.02.2008
class Test( TestCommon ):
    """Write test names explicitely to impose a given order of evaluation."""
    tests = ['test_read_meshes', 'test_compare_same_meshes',
             'test_read_dimension', 'test_write_read_meshes']

    ##
    # c: 05.02.2008, r: 05.02.2008
    def from_conf( conf, options ):
        return Test( conf = conf, options = options )
    from_conf = staticmethod( from_conf )
    
    ##
    # c: 05.02.2008, r: 05.02.2008
    def test_read_meshes( self ):
        """Try to read all listed meshes."""
        from sfepy.discrete.fem import Mesh

        conf_dir = op.dirname(__file__)
        meshes = {}
        for ii, filename in enumerate( filename_meshes ):
            self.report( '%d. mesh: %s' % (ii + 1, filename) )
            mesh = Mesh.from_file(filename, prefix_dir=conf_dir)

            assert_(mesh.dim == (mesh.coors.shape[1]))
            assert_(mesh.n_nod == (mesh.coors.shape[0]))
            assert_(mesh.n_nod == (mesh.ngroups.shape[0]))
            assert_(mesh.n_el == sum(mesh.n_els))
            for ig, conn in enumerate( mesh.conns ):
                assert_(conn.shape[0] == len(mesh.mat_ids[ig]))
                assert_(conn.shape[0] == mesh.n_els[ig])
                assert_(conn.shape[1] == mesh.n_e_ps[ig])
                
            self.report( 'read ok' )
            meshes[filename] = mesh

        self.meshes = meshes

        return True

    def _compare_meshes(self, mesh0, mesh1):
        import numpy as nm

        oks = []

        ok0 = (mesh0.dim == mesh1.dim)
        if not ok0:
            self.report( 'dimension failed!' )
        oks.append( ok0 )

        ok0 = mesh0.n_nod == mesh1.n_nod
        if not ok0:
            self.report( 'number of nodes failed!' )
        oks.append( ok0 )

        ok0 = mesh0.n_el == mesh1.n_el
        if not ok0:
            self.report( 'number of elements failed!' )
        oks.append( ok0 )

        ok0 = mesh0.n_e_ps == mesh1.n_e_ps
        if not ok0:
            self.report( 'number of element points failed!' )
        oks.append( ok0 )

        ok0 = mesh0.descs == mesh1.descs
        if not ok0:
            self.report( 'element types failed!' )
        oks.append( ok0 )

        ok0 = nm.allclose( mesh0.coors, mesh1.coors )
        if not ok0:
            self.report( 'nodes failed!' )
        oks.append( ok0 )

        ok0 = nm.all( mesh0.ngroups == mesh1.ngroups )
        if not ok0:
            self.report( 'node groups failed!' )
        oks.append( ok0 )

        for ii in range( len( mesh0.mat_ids ) ):
            ok0 = nm.all( mesh0.mat_ids[ii] == mesh1.mat_ids[ii] )
            if not ok0:
                self.report( 'material ids failed!' )
            oks.append( ok0 )

        for ii in range( len( mesh0.mat_ids ) ):
            ok0 = nm.all( mesh0.conns[ii] == mesh1.conns[ii] )
            if not ok0:
                self.report( 'connectivities failed!' )
            oks.append( ok0 )

        return oks

    def test_compare_same_meshes(self):
        """
        Compare same meshes in various formats.
        """
        oks = []
        for i0, i1 in same:
            name0 = filename_meshes[i0]
            name1 = filename_meshes[i1]
            self.report('comparing meshes from "%s" and "%s"' % (name0, name1))

            mesh0 = self.meshes[name0]
            mesh1 = self.meshes[name1]
            oks = self._compare_meshes(mesh0, mesh1)

        return sum(oks) == len(oks)

    ##
    # c: 03.07.2008, r: 03.07.2008
    def test_read_dimension( self ):
        from sfepy.discrete.fem import MeshIO

        meshes = {data_dir + '/meshes/various_formats/small2d.mesh' : 2,
                  data_dir + '/meshes/various_formats/small2d.vtk' : 2,
                  data_dir + '/meshes/various_formats/small3d.mesh' : 3}

        ok = True
        conf_dir = op.dirname(__file__)
        for filename, adim in meshes.iteritems():
            self.report( 'mesh: %s, dimension %d' % (filename, adim) )
            io = MeshIO.any_from_filename(filename, prefix_dir=conf_dir)
            dim = io.read_dimension()
            if dim != adim:
                self.report( 'read dimension %d -> failed' % dim )
                ok = False
            else:
                self.report( 'read dimension %d -> ok' % dim )

        return ok

    def test_write_read_meshes(self):
        """
        Try to write and then read all supported formats.
        """
        from sfepy.discrete.fem import Mesh
        from sfepy.discrete.fem.meshio import (supported_formats,
                                               supported_capabilities)

        conf_dir = op.dirname(__file__)
        mesh0 = Mesh.from_file(data_dir
                               + '/meshes/various_formats/small3d.mesh',
                               prefix_dir=conf_dir)

        oks = []
        for suffix, format_ in supported_formats.iteritems():
            if isinstance(format_, tuple):
                continue
            if 'w' not in supported_capabilities[format_]: continue

            filename = op.join(self.options.out_dir, 'test_mesh_wr' + suffix)
            self.report('%s format: %s' % (suffix, filename))

            mesh0.write(filename, io='auto')
            mesh1 = Mesh.from_file(filename)

            oks.extend(self._compare_meshes(mesh0, mesh1))

        return sum(oks) == len(oks)

########NEW FILE########
__FILENAME__ = test_mesh_generators
import os.path as op

from sfepy.base.testing import TestCommon

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        test = Test(conf=conf, options=options)
        return test

    def test_gen_block_mesh(self):
        from sfepy.mesh.mesh_generators import gen_block_mesh

        mesh = gen_block_mesh([1, 2, 3], [4, 3, 5], [2, 1, 0], verbose=False)
        filename = op.join(self.options.out_dir, 'gen_block.mesh')
        mesh.write(filename)

        self.report('block mesh generated')
        return True

    def test_gen_cylinder_mesh(self):
        from sfepy.mesh.mesh_generators import gen_cylinder_mesh

        mesh = gen_cylinder_mesh([0.5, 1, 2, 1.5, 3], [5, 4, 3], [0, 2, 1],
                                 axis='z', non_uniform=True, verbose=False)
        filename = op.join(self.options.out_dir, 'gen_cylinger.mesh')
        mesh.write(filename)

        self.report('cylinder mesh generated')
        return True

    def test_gen_extended_block_mesh(self):
        from sfepy.mesh.mesh_generators import gen_extended_block_mesh

        mesh = gen_extended_block_mesh([2, 3, 1], [5, 3, 4], [14, 10, 20],
                                       5, [1, 0, 2])
        filename = op.join(self.options.out_dir, 'gen_extended_block.mesh')
        mesh.write(filename)

        self.report('extended block mesh generated')
        return True

########NEW FILE########
__FILENAME__ = test_mesh_interp
import os.path as op

import numpy as nm

from sfepy.base.conf import transform_variables
from sfepy.base.testing import TestCommon

variables = {
    'u'       : ('unknown field', 'f', 0),
    'v'       : ('test field',    'f', 'u'),
}

def in_dir(adir):
    return lambda x: op.join(adir, x)

def gen_datas(meshes):
    datas = {}

    for key, mesh in meshes.iteritems():
        bbox = mesh.get_bounding_box()
        nx = bbox[1,0] - bbox[0,0]
        centre = 0.5 * bbox.sum(axis=0)
        mesh.coors -= centre

        data = nm.sin(4.0 * nm.pi * mesh.coors[:,0:1] / nx)
        datas['scalar_' + key] = data

        data = nm.zeros_like(mesh.coors)
        data[:,0] = 0.05 * nx * nm.sin(4.0 * nm.pi * mesh.coors[:,0] / nx)
        data[:,2] = 0.05 * nx * nm.cos(4.0 * nm.pi * mesh.coors[:,0] / nx)
        datas['vector_' + key] = data

    return datas

def do_interpolation(m2, m1, data, field_name, force=False):
    """Interpolate data from m1 to m2. """
    from sfepy.discrete import Variables
    from sfepy.discrete.fem import FEDomain, Field

    fields = {
        'scalar_si' : ((1,1), 'Omega', 2),
        'vector_si' : ((3,1), 'Omega', 2),
        'scalar_tp' : ((1,1), 'Omega', 1),
        'vector_tp' : ((3,1), 'Omega', 1),
    }

    d1 = FEDomain('d1', m1)

    omega1 = d1.create_region('Omega', 'all')

    f = fields[field_name]

    field1 = Field.from_args('f', nm.float64, f[0], d1.regions[f[1]],
                             approx_order=f[2])
    ff = {field1.name : field1}

    vv = Variables.from_conf(transform_variables(variables), ff)
    u1 = vv['u']
    u1.set_from_mesh_vertices(data)

    d2 = FEDomain('d2', m2)
    omega2 = d2.create_region('Omega', 'all')

    field2 = Field.from_args('f', nm.float64, f[0], d2.regions[f[1]],
                             approx_order=f[2])
    ff2 = {field2.name : field2}

    vv2 = Variables.from_conf(transform_variables(variables), ff2)
    u2 = vv2['u']

    if not force:
        # Performs interpolation, if other field differs from self.field
        # or, in particular, is defined on a different mesh.
        u2.set_from_other(u1, strategy='interpolation', close_limit=0.5)

    else:
        coors = u2.field.get_coor()
        vals = u1.evaluate_at(coors, close_limit=0.5)
        u2.set_data(vals)

    return u1, u2

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        test = Test(conf=conf, options=options)
        return test

    def test_interpolation(self):
        from sfepy import data_dir
        from sfepy.discrete.fem import Mesh
        from sfepy.linalg import make_axis_rotation_matrix

        fname = in_dir(self.options.out_dir)

        meshes = {
            'tp' : Mesh('original mesh', data_dir + '/meshes/3d/block.mesh'),
            'si' : Mesh('original mesh', data_dir + '/meshes/3d/cylinder.mesh'),
        }

        datas = gen_datas(meshes)

        for field_name in ['scalar_si', 'vector_si', 'scalar_tp', 'vector_tp']:
            m1 = meshes[field_name[-2:]]

            for ia, angle in enumerate(nm.linspace(0.0, nm.pi, 11)):
                self.report('%s: %d. angle: %f' % (field_name, ia, angle))
                shift = [0.0, 0.0, 0.0]
                mtx = make_axis_rotation_matrix([0, 1, 0], angle)

                m2 = m1.copy('rotated mesh')
                m2.transform_coors(mtx)

                data = datas[field_name]
                u1, u2 = do_interpolation(m2, m1, data, field_name)

                if ia == 0:
                    u1.save_as_mesh(fname('test_mesh_interp_%s_u1.vtk'
                                          % field_name))

                u2.save_as_mesh(fname('test_mesh_interp_%s_u2.%03d.vtk'
                                      % (field_name, ia)))

        return True

    def test_interpolation_two_meshes(self):
        from sfepy import data_dir
        from sfepy.discrete import Variables
        from sfepy.discrete.fem import Mesh, FEDomain, Field

        m1 = Mesh('source mesh', data_dir + '/meshes/3d/block.mesh')

        m2 = Mesh('target mesh', data_dir + '/meshes/3d/cube_medium_tetra.mesh')
        m2.coors *= 2.0

        bbox = m1.get_bounding_box()
        dd = bbox[1,:] - bbox[0,:]
        data = nm.sin(4.0 * nm.pi * m1.coors[:,0:1] / dd[0]) \
               * nm.cos(4.0 * nm.pi * m1.coors[:,1:2] / dd[1])

        variables1 = {
            'u'       : ('unknown field', 'scalar_tp', 0),
            'v'       : ('test field',    'scalar_tp', 'u'),
        }

        variables2 = {
            'u'       : ('unknown field', 'scalar_si', 0),
            'v'       : ('test field',    'scalar_si', 'u'),
        }

        d1 = FEDomain('d1', m1)
        omega1 = d1.create_region('Omega', 'all')
        field1 = Field.from_args('scalar_tp', nm.float64, (1,1), omega1,
                                 approx_order=1)
        ff1 = {field1.name : field1}

        d2 = FEDomain('d2', m2)
        omega2 = d2.create_region('Omega', 'all')
        field2 = Field.from_args('scalar_si', nm.float64, (1,1), omega2,
                                 approx_order=0)
        ff2 = {field2.name : field2}

        vv1 = Variables.from_conf(transform_variables(variables1), ff1)
        u1 = vv1['u']
        u1.set_from_mesh_vertices(data)

        vv2 = Variables.from_conf(transform_variables(variables2), ff2)
        u2 = vv2['u']

        # Performs interpolation, if other field differs from self.field
        # or, in particular, is defined on a different mesh.
        u2.set_from_other(u1, strategy='interpolation', close_limit=0.1)

        fname = in_dir(self.options.out_dir)
        u1.save_as_mesh(fname('test_mesh_interp_block_scalar.vtk'))
        u2.save_as_mesh(fname('test_mesh_interp_cube_scalar.vtk'))

        return True

    def test_invariance(self):
        from sfepy import data_dir
        from sfepy.discrete.fem import Mesh

        meshes = {
            'tp' : Mesh('original mesh', data_dir + '/meshes/3d/block.mesh'),
            'si' : Mesh('original mesh', data_dir + '/meshes/3d/cylinder.mesh'),
        }
        datas = gen_datas(meshes)

        ok = True
        for field_name in ['scalar_si', 'vector_si', 'scalar_tp', 'vector_tp']:
            m1 = meshes[field_name[-2:]]

            data = datas[field_name]
            u1, u2 = do_interpolation(m1, m1, data, field_name, force=True)

            self.report('max. difference:', nm.abs(u1() - u2()).max())
            _ok = nm.allclose(u1(), u2(), rtol=0.0, atol=1e-12)
            self.report('invariance for %s field: %s' % (field_name, _ok))

            ok = ok and _ok

        return ok

    def test_invariance_qp(self):
        from sfepy import data_dir
        from sfepy.discrete import Variables, Integral
        from sfepy.discrete.fem import Mesh, FEDomain, Field
        from sfepy.terms import Term
        from sfepy.discrete.common.mappings import get_physical_qps

        mesh = Mesh('source mesh', data_dir + '/meshes/3d/block.mesh')

        bbox = mesh.get_bounding_box()
        dd = bbox[1,:] - bbox[0,:]
        data = nm.sin(4.0 * nm.pi * mesh.coors[:,0:1] / dd[0]) \
               * nm.cos(4.0 * nm.pi * mesh.coors[:,1:2] / dd[1])

        variables = {
            'u'       : ('unknown field', 'scalar_tp', 0),
            'v'       : ('test field',    'scalar_tp', 'u'),
        }

        domain = FEDomain('domain', mesh)
        omega = domain.create_region('Omega', 'all')
        field = Field.from_args('scalar_tp', nm.float64, 1, omega,
                                approx_order=1)
        ff = {field.name : field}

        vv = Variables.from_conf(transform_variables(variables), ff)
        u = vv['u']
        u.set_from_mesh_vertices(data)

        integral = Integral('i', order=2)
        term = Term.new('ev_volume_integrate(u)', integral, omega, u=u)
        term.setup()
        val1, _ = term.evaluate(mode='qp')
        val1 = val1.ravel()

        qps = get_physical_qps(omega, integral)
        coors = qps.get_merged_values()

        val2 = u.evaluate_at(coors).ravel()

        self.report('max. difference:', nm.abs(val1 - val2).max())
        ok = nm.allclose(val1, val2, rtol=0.0, atol=1e-12)
        self.report('invariance in qp: %s' % ok)

        return ok

########NEW FILE########
__FILENAME__ = test_mesh_smoothing
import os.path as op

from sfepy.base.testing import TestCommon

def get_volume(el, nd):
    from sfepy.mesh.mesh_tools import elems_q2t
    from sfepy.base.compat import factorial
    import numpy as nm
    from sfepy.linalg.utils import dets_fast

    dim = nd.shape[1]
    nnd = el.shape[1]

    etype = '%d_%d' % (dim, nnd)
    if etype == '2_4' or etype == '3_8':
        el = elems_q2t(el)

    nel = el.shape[0]

    mul = 1.0 / factorial(dim)
    if dim == 3:
        mul *= -1.0

    mtx = nm.ones((nel, dim + 1, dim + 1), dtype=nm.double)
    mtx[:,:,:-1] = nd[el,:]
    vols = mul * dets_fast(mtx.copy())
    vol = vols.sum()

    return vol

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        test = Test(conf=conf, options=options)
        return test

    def test_mesh_smoothing(self):
        from sfepy.mesh.mesh_tools import smooth_mesh
        from sfepy.discrete.fem.mesh import Mesh
        from sfepy import data_dir

        mesh = Mesh.from_file(data_dir + '/meshes/3d/cylinder.vtk')
        vol0 = get_volume(mesh.conns[0], mesh.coors)
        mesh.coors = smooth_mesh(mesh, n_iter=10)
        vol1 = get_volume(mesh.conns[0], mesh.coors)
        filename = op.join(self.options.out_dir, 'smoothed_cylinder.vtk')
        mesh.write(filename)
        frac = vol1 / vol0

        if (frac < 0.967) and (frac > 0.966):
            self.report('mesh smoothed')
            return True

        else:
            self.report('mesh smoothed, volume mismatch!')
            return False

########NEW FILE########
__FILENAME__ = test_msm_laplace
# c: 07.05.2007, r: 25.06.2008
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/2d/special/circle_in_square.mesh'

dim = 2

field_1 = {
    'name' : 'a_harmonic_field',
    'dtype' : 'real',
    'shape' : 'scalar',
    'region' : 'Omega',
    'approx_order' : 1,
}

variables = {
    't': ('unknown field', 'a_harmonic_field', 0),
    's': ('test field',    'a_harmonic_field', 't'),
}

regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < 0.001) & (y < 0.001)', 'facet'),
    'Right' : ('vertices in (x > 0.999)', 'facet'),
    'Gamma' : ('vertices of surface', 'facet'),
}

ebcs = {
    't_left' : ('Gamma', {'t.0' : 'ebc'}),
#    't_right' : ('Right', {'t.0' : 'ebc'}),
}

integral_1 = {
    'name' : 'i',
    'order' : 2,
}

coef = 2.0
materials = {
    'coef' : ({'val' : coef},),
    'rhs' : 'rhs',
}

equations = {
    'Temperature' :
    """dw_laplace.i.Omega( coef.val, s, t )
       = - dw_volume_lvf.i.Omega( rhs.val, s )""",
}

solutions = {
    'sincos' : ('t', 'sin( 3.0 * x ) * cos( 4.0 * y )',
                '-25.0 * %s * sin( 3.0 * x ) * cos( 4.0 * y )' % coef),
    'poly' : ('t', '(x**2) + (y**2)', '4.0 * %s' % coef),
    'polysin' : ('t', '((x - 0.5)**3) * sin( 5.0 * y )',
                 '%s * (6.0 * (x - 0.5) * sin( 5.0 * y ) - 25.0 * ((x - 0.5)**3) * sin( 5.0 * y ))' % coef),
}

solver_0 = {
    'name' : 'ls',
    'kind' : 'ls.scipy_direct',
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 1,
    'eps_a'      : 1e-10,
}

import numpy as nm
from sfepy.base.testing import TestCommon
from sfepy.base.base import debug, pause, assert_
output_name = 'test_msm_laplace_%s.vtk'

##
# c: 07.05.2007, r: 09.05.2008
solution = ['']
def ebc(ts, coor, **kwargs):
    expression = solution[0]
    val = TestCommon.eval_coor_expression( expression, coor )
    return nm.atleast_1d( val )

def rhs(ts, coor, mode=None, expression=None, **kwargs):
    if mode == 'qp':
        if expression is None:
            expression = '0.0 * x'

        val = TestCommon.eval_coor_expression( expression, coor )
        val.shape = (val.shape[0], 1, 1)

        return {'val' : val}

functions = {
    'ebc' : (ebc,),
    'rhs' : (rhs,),
}

##
# c: 07.05.2008
class Test( TestCommon ):

    ##
    # c: 07.05.2007, r: 07.05.2008
    def from_conf( conf, options ):
        from sfepy.discrete import Problem

        problem = Problem.from_conf(conf)
        test = Test( problem = problem,
                     conf = conf, options = options )
        return test
    from_conf = staticmethod( from_conf )

    ##
    # c: 09.05.2007, r: 25.06.2008
    def _build_rhs( self, sols ):
        for sol in sols.itervalues():
            assert_( len( sol ) == 3 )
        return sols

    ##
    # c: 07.05.2007, r: 09.05.2008
    def test_msm_laplace( self ):
        import os.path as op
        problem  = self.problem

        variables = problem.get_variables()
        materials = problem.get_materials()

        sols = self._build_rhs( self.conf.solutions )

        ok = True
        for sol_name, sol in sols.iteritems():
            self.report( 'testing', sol_name )
            var_name, sol_expr, rhs_expr = sol

            self.report( 'sol:', sol_expr )
            self.report( 'rhs:', rhs_expr )
            globals()['solution'][0] = sol_expr
            materials['rhs'].function.set_extra_args(expression=rhs_expr)
            problem.time_update()
            state = problem.solve()
            coor = variables[var_name].field.get_coor()
            ana_sol = self.eval_coor_expression( sol_expr, coor )
            num_sol = state(var_name)

            ana_norm = nm.linalg.norm( ana_sol, nm.inf )
            ret = self.compare_vectors( ana_sol, num_sol,
                                       allowed_error = ana_norm * 1e-2,
                                       label1 = 'analytical %s' % var_name,
                                       label2 = 'numerical %s' % var_name,
                                       norm = nm.inf )
            if not ret:
                self.report( 'variable %s: failed' % var_name )

            fname = op.join( self.options.out_dir, self.conf.output_name )
            out = {}
            astate = state.copy()
            astate.set_full(ana_sol)
            aux = astate.create_output_dict()
            out['ana_t'] = aux['t']
            aux = state.create_output_dict()
            out['num_t'] = aux['t']

            problem.domain.mesh.write( fname % sol_name, io = 'auto', out = out )

            ok = ok and ret

        return ok

########NEW FILE########
__FILENAME__ = test_msm_symbolic
# c: 07.05.2007, r: 08.07.2008
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/2d/special/circle_in_square.mesh'

dim = 2

field_1 = {
    'name' : 'a_harmonic_field',
    'dtype' : 'real',
    'shape' : 'scalar',
    'region' : 'Omega',
    'approx_order' : 1,
}

variables = {
    't': ('unknown field', 'a_harmonic_field', 0),
    's': ('test field',    'a_harmonic_field', 't'),
}

regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < 0.001) & (y < 0.001)', 'facet'),
    'Right' : ('vertices in (x > 0.999)', 'facet'),
    'Gamma' : ('vertices of surface', 'facet'),
}

ebcs = {
    't_left' : ('Gamma', {'t.0' : 'ebc'}),
#    't_right' : ('Right', {'t.0' : 'ebc'}),
}

integral_1 = {
    'name' : 'i',
    'order' : 2,
}

material_1 = {
    'name' : 'coef',
    'values' : {
        'val' : 12.0,
        'K' : [[1.0, 0.3], [0.3, 2.0]],
    }
}

material_2 = {
    'name' : 'rhs',
    'function' : 'rhs',
}

equations = {
    'Laplace' :
    """2 * dw_laplace.i.Omega( coef.val, s, t )
    """,
    'Diffusion' :
    """3 * dw_diffusion.i.Omega( coef.K, s, t )
    """,
}
equations_rhs = {
    'Laplace' :
    """= - dw_volume_lvf.i.Omega( rhs.val, s )""",
    'Diffusion' :
    """= - dw_volume_lvf.i.Omega( rhs.val, s )""",
}

solutions = {
    'sincos' : ('t', 'sin( 3.0 * x ) * cos( 4.0 * y )'),
    'poly' : ('t', '(x**2) + (y**2)'),
    'polysin' : ('t', '((x - 0.5)**3) * sin( 5.0 * y )'),
}

solver_0 = {
    'name' : 'ls',
    'kind' : 'ls.scipy_direct',
}

solver_1 = {
    'name' : 'newton',
    'kind' : 'nls.newton',

    'i_max'      : 1,
    'eps_a'      : 1e-10,
}

import numpy as nm
try:
    import sympy_operators as sops
except ImportError, exc:
    sops = None
    
from sfepy.base.testing import TestCommon
from sfepy.base.base import debug, pause
output_name = 'test_msm_symbolic_%s.vtk'

##
# c: 07.05.2007, r: 09.05.2008
solution = ['']
def ebc(ts, coor, solution=None):
    expression = solution[0]
    val = TestCommon.eval_coor_expression( expression, coor )
    return nm.atleast_1d( val )

def rhs(ts, coor, mode=None, expression=None, **kwargs):
    if mode == 'qp':
        if expression is None:
            expression = '0.0 * x'

        val = TestCommon.eval_coor_expression( expression, coor )
        val.shape = (val.shape[0], 1, 1)
        return {'val' : val}

functions = {
    'ebc' : (lambda ts, coor, **kwargs:
             ebc(ts, coor, solution=solution),),
    'rhs' : (rhs,),
}

##
# c: 07.05.2008
class Test( TestCommon ):

    ##
    # c: 07.05.2007, r: 25.06.2008
    def from_conf( conf, options ):
        from sfepy.discrete import Problem

        problem = Problem.from_conf(conf, init_equations=False)
        test = Test( problem = problem,
                     conf = conf, options = options )
        return test
    from_conf = staticmethod( from_conf )

    ##
    # c: 09.05.2007, r: 08.07.2008
    def _build_rhs( self, equation, sols ):

        problem  = self.problem
        rhss = {}
        self.report( '%s:' % equation.name )
        self.report( 'evaluating terms, "<=" is solution, "=>" is the rhs:' )
        for term in equation.terms:
            if not hasattr( term, 'symbolic' ):
                self.report( 'term %s has no symbolic description!' % term.name )
                raise ValueError
            expr = term.symbolic['expression']
            arg_map = term.symbolic['map']
            self.report( '%s( %s )' %\
                         (term.name, ', '.join( term.ats )) )
            self.report( 'multiplicator: %f' % term.sign )
            self.report( '  symbolic:', expr )
            self.report( '  using argument map:', arg_map )

            for sol_name, sol in sols.iteritems():
                rhs = self._eval_term( sol[1], term, sops )
                srhs = "(%s * (%s))" % (term.sign, rhs)
                rhss.setdefault( sol_name, [] ).append( srhs )

        for key, val in rhss.iteritems():
            rhss[key] = '+'.join( val )

        return rhss

    ##
    # c: 09.05.2007, r: 25.06.2008
    def _eval_term( self, sol, term, sops ):
        """Works for scalar, single unknown terms only!"""
        expr = term.symbolic['expression']
        arg_map = term.symbolic['map']
        env = {'x' : sops.Symbol( 'x' ),
               'y' : sops.Symbol( 'y' ),
               'z' : sops.Symbol( 'z' ),
               'dim' : dim}
        for key, val in arg_map.iteritems():
            if val == 'state':
                env[key] = sol
            else:
                term.set_current_group(0)
                env[key] = term.get_args( [val] )[0]

            if 'material' in val:
                # Take the first value - constant in all QPs.
                aux = env[key][0,0]
                if nm.prod( aux.shape ) == 1:
                    env[key] = aux.squeeze()
                else:
                    import sympy
                    env[key] = sympy.Matrix( aux )

#        print env

        self.report( '  <= ', sol )
        sops.set_dim( dim )
        val = str( eval( expr, sops.__dict__, env ) )
        self.report( '   =>', val )
        return val

    ##
    # c: 07.05.2007, r: 30.06.2008
    def _test_msm_symbolic( self, equations ):
        import os.path as op

        if sops is None:
            self.report( 'cannot import sympy, skipping' )
            return True

        problem  = self.problem
        ok = True
        for eq_name, equation in equations.iteritems():
            problem.set_equations( {eq_name : equation} )
            problem.update_materials()

            rhss = self._build_rhs( problem.equations[eq_name],
                                   self.conf.solutions )
            erhs = problem.conf.equations_rhs[eq_name]  

            problem.set_equations( {eq_name : equation + erhs} )
            variables = problem.get_variables()
            materials = problem.get_materials()
            rhs_mat = materials['rhs']

            for sol_name, sol in problem.conf.solutions.iteritems():
                self.report( 'testing', sol_name )
                var_name, sol_expr = sol
                rhs_expr = rhss[sol_name]

                self.report( 'sol:', sol_expr )
                self.report( 'rhs:', rhs_expr )
                globals()['solution'][0] = sol_expr
                rhs_mat.function.set_extra_args(expression=rhs_expr)
                problem.time_update()
                state = problem.solve()
                coor = variables[var_name].field.get_coor()
                ana_sol = self.eval_coor_expression( sol_expr, coor )
                num_sol = state(var_name)

                ana_norm = nm.linalg.norm( ana_sol, nm.inf )
                ret = self.compare_vectors( ana_sol, num_sol,
                                           allowed_error = ana_norm * 1e-2,
                                           label1 = 'analytical %s' % var_name,
                                           label2 = 'numerical %s' % var_name,
                                           norm = nm.inf )
                if not ret:
                    self.report( 'variable %s: failed' % var_name )

                fname = op.join( self.options.out_dir, self.conf.output_name )
                out = {}
                astate = state.copy()
                astate.set_full(ana_sol)
                aux = astate.create_output_dict()
                out['ana_t'] = aux['t']
                aux = state.create_output_dict()
                out['num_t'] = aux['t']

                problem.domain.mesh.write( fname % '_'.join( (sol_name, eq_name) ),
                                           io = 'auto', out = out )

                ok = ok and ret

        return ok

    ##
    # c: 30.06.2008, r: 30.06.2008
    def _get_equations( self, name ):
        """Choose a sub-problem from all equations."""
        return {name : self.problem.conf.equations[name]}
        
    ##
    # c: 30.06.2008, r: 30.06.2008
    def test_msm_symbolic_laplace( self ):
        return self._test_msm_symbolic( self._get_equations( 'Laplace' ) )

    ##
    # c: 30.06.2008, r: 30.06.2008
    def test_msm_symbolic_diffusion( self ):
        return self._test_msm_symbolic( self._get_equations( 'Diffusion' ) )

########NEW FILE########
__FILENAME__ = test_normals
import numpy as nm

from sfepy.base.testing import TestCommon

expected_normals = { # Need to be normalized!
    '2_3' : nm.array([[ 0, -1],
                      [ 1,  1],
                      [-1,  0]], dtype=nm.float64),
    '2_4' : nm.array([[ 0, -1],
                      [ 1,  0],
                      [ 0,  1],
                      [-1,  0]], dtype=nm.float64),
    '3_4' : nm.array([[ 0,  0, -1],
                      [-1,  0,  0],
                      [ 0, -1,  0],
                      [ 1,  1,  1]], dtype=nm.float64),
    '3_8' : nm.array([[ 0,  0, -1],
                      [-1,  0,  0],
                      [ 0, -1,  0],
                      [ 0,  0,  1],
                      [ 1,  0,  0],
                      [ 0,  1,  0]], dtype=nm.float64),
}

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        return Test(conf=conf, options=options)

    def test_normals(self):
        """
        Check orientations of surface normals on the reference elements.
        """
        import sfepy
        from sfepy.discrete import Integral
        from sfepy.discrete.fem import Mesh, FEDomain
        from sfepy.discrete.fem.poly_spaces import PolySpace
        from sfepy.discrete.fem.mappings import SurfaceMapping
        from sfepy.linalg import normalize_vectors

        ok = True

        for geom in ['2_3', '2_4', '3_4', '3_8']:
            mesh = Mesh.from_file('meshes/elements/%s_1.mesh' % geom,
                                  prefix_dir=sfepy.data_dir)
            domain = FEDomain('domain', mesh)
            surface = domain.create_region('Surface', 'vertices of surface',
                                           'facet')
            domain.create_surface_group(surface)

            sd = domain.surface_groups[0][surface.name]

            coors = domain.get_mesh_coors()
            gel = domain.geom_els[geom].surface_facet
            ps = PolySpace.any_from_args('aux', gel, 1)

            mapping = SurfaceMapping(coors, sd.get_connectivity(), ps)

            integral = Integral('i', order=1)
            vals, weights = integral.get_qp(gel.name)

            # Evaluate just in the first quadrature point...
            geo = mapping.get_mapping(vals[:1], weights[:1])

            expected = expected_normals[geom].copy()
            normalize_vectors(expected)

            _ok = nm.allclose(expected, geo.normal[:, 0, :, 0],
                              rtol=0.0, atol=1e-14)
            self.report('%s: %s' % (geom, _ok))

            if not _ok:
                self.report('expected:')
                self.report(expected)
                self.report('actual:')
                self.report(geo.normal[:, 0, :, 0])

            ok = ok and _ok

        return ok

########NEW FILE########
__FILENAME__ = test_parsing
from sfepy.base.testing import TestCommon

class Test(TestCommon):

    def from_conf(conf, options):
        return Test(conf=conf, options=options)
    from_conf = staticmethod(from_conf)

    def test_parse_equations(self):
        from sfepy.discrete.parse_equations import create_bnf

        test_strs = [
            """- d_volume.i1.Omega(uc)""",
            """- 2 * dw_term.i1.Omega(uc)
             = - 3.0 * dw_term2.i1.Omega2(uc)""",
            """2 * dw_term.i1.Omega(uc)
             = 3.0 * dw_term2.i1.Omega2(uc)""",
            """- (2 + 1j - 3) * dw_term.i1.Omega(uc)
               = - (1j - 3.0 + 2.0j) * dw_term2.i1.Omega2(uc)""",
            """(2 + 1j) * dw_term.i1.Omega(uc)
               = (3.0 + 2.0j) * dw_term2.i1.Omega2(uc)""",
            """d_term1.Y(fluid, u, w, Nu, dcf, mode)
                 + d_term2.Omega(u, w, Nu, dcf, mode)
                 - d_another_term.Elsewhere(w, p, Nu, dcf, mode)
                 = - dw_rhs.a.Y3(u, q, Nu, dcf, mode)""",
            """no_args() = 0""",
            """+ something(a, b, c) = + something_else(c, a, d[-1])""",
            """term_.a.a(u)""",
            """term.i1.Omega(v, du/dt) + term2.i2.Gamma(v, dphi/dt)""",
            """dw_jump.isurf.Gamma12_1(jump1.val, q1, p1, tr(p2))""",
        ]

        n_fail = 0
        term_descs = []
        for test_str in test_strs:
            term_descs[:] = []
            try:
                bnf = create_bnf(term_descs)
                bnf.parseString(test_str)
            except:
                self.report('failed: %s' % test_str)
                if self.options.debug:
                    raise
                n_fail += 1
            for td in term_descs:
                print td
        self.report('%d failure(s)' % n_fail)

        if n_fail:
            raise AssertionError
        return True

    def test_parse_regions(self):
        from sfepy.discrete.parse_regions import create_bnf

        test_strs = ['vertices of surface -v r.Omega',
                     'r.Y_2 +v copy r.Y_1',
                     'vertices in (y <= 0.00001) & (x < 0.11)',
                     'vertices in ((y <= 0.00001) & (x < 0.11))',
                     'vertices in (((y <= 0.00001) & (x < 0.11)))',
                     'vertices in (((0.00001 < y) & (x < 0.11)))',
                     'vertices of group 0',
                     """vertices of group 0 +v vertices of group 1
                     +c cells by afun""",
                     'all -v vertices in (y == 0.00001)',
                     'all -v vertices of surface',
                     'all -c r.DOmega_100',
                     """r.Y_1 -v vertices of surface *c r.Z_8
                        *v vertices in (y > 0)""",
                     'vertices of surface +v vertices by pokus',
                     'cells of group 6 +c vertices by fn2_3c',
                     """r.Y_1 *v (r.Y_2 +c (vertices in (y > 0) *v r.Y_32))
                        -v vertices of surface -c r.Y_5""",
                     'vertices by afun',
                     'vertex in r.Gamma_3',
                     'vertex 10',
                     'vertex 10, 20, 30',
                     'cell 10',
                     'cell 10, 20, 30',
                     'cell (0, 10), (1, 20), (0, 30)',
                     'vertex 10, 20 +v cell 30, 40',
                     '(vertex 10, 20) +v (cell 30, 40)',
                     'cell (0, 10), (1, 20), (0, 30) +v vertex 10',
                     'cells by afun']

        stack = []
        bnf = create_bnf(stack)

        n_fail = 0
        for test_str in test_strs:
            stack[:] = []

            try:
                out = bnf.parseString(test_str)
                print out

            except:
                self.report('failed: %s' % test_str)
                n_fail += 1

        self.report('%d failures' % n_fail)

        if n_fail:
            raise AssertionError
        return True

########NEW FILE########
__FILENAME__ = test_periodic_bc_2d
# 01.06.2007, c
# last revision: 25.02.2008
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/various_formats/small2d.mesh'

material_1 = {
    'name' : 'coef',
    'values' : {'coef' : 1.0},
}

region_1000 = {
    'name' : 'Omega',
    'select' : 'all',
}
region_1 = {
    'name' : 'Left',
    'select' : 'vertices in (x < -0.499)',
    'kind' : 'facet',
}
region_2 = {
    'name' : 'Right',
    'select' : 'vertices in (x > 0.499)',
    'kind' : 'facet',
}
region_22 = {
    'name' : 'Bottom',
    'select' : 'vertices in (y < -0.499)',
    'kind' : 'facet',
}
region_23 = {
    'name' : 'Top',
    'select' : 'vertices in (y > 0.499)',
    'kind' : 'facet',
}

field_1 = {
    'name' : '2_displacement',
    'dtype' : 'real',
    'shape' : (2,),
    'region' : 'Omega',
    'approx_order' : 2,
}

field_2 = {
    'name' : 'pressure',
    'dtype' : 'real',
    'shape' : (1,),
    'region' : 'Omega',
    'approx_order' : 1,
}

variable_1 = {
    'name' : 'u',
    'kind' : 'unknown field',
    'field' : '2_displacement',
    'order' : 0,
}
variable_2 = {
    'name' : 'v',
    'kind' : 'test field',
    'field' : '2_displacement',
    'dual' : 'u',
}
variable_3 = {
    'name' : 'p',
    'kind' : 'unknown field',
    'field' : 'pressure',
    'order' : 1,
}
variable_4 = {
    'name' : 'q',
    'kind' : 'test field',
    'field' : 'pressure',
    'dual' : 'p',
}

ebcs = {}
epbc_10 = {
    'name' : 'rl',
    'region' : ['Left', 'Right'],
    'dofs' : {'u.all' : 'u.all', 'p.0' : 'p.0'},
    'match' : 'match_y_line',
}
epbc_12 = {
    'name' : 'tb',
    'region' : ['Top', 'Bottom'],
    'dofs' : {'u.all' : 'u.all', 'p.0' : 'p.0'},
    'match' : 'match_x_line',
}

from sfepy.discrete.fem.periodic import match_x_line, match_y_line

functions = {
    'match_x_line' : (match_x_line,),
    'match_y_line' : (match_y_line,),
}

from sfepy.base.testing import TestCommon

##
# 01.06.2007, c
class Test( TestCommon ):

    ##
    # 01.06.2007, c
    def from_conf( conf, options ):
        from sfepy.discrete import Problem
        problem = Problem.from_conf(conf, init_equations=False)

        test = Test( problem = problem,
                     conf = conf, options = options )
        return test
    from_conf = staticmethod( from_conf )

    ##
    # c: 01.06.2007, r: 18.02.2008
    def test_pbc( self ):
        from sfepy.discrete import Variables, Conditions

        problem  = self.problem
        conf = self.conf

        ebcs = Conditions.from_conf(conf.ebcs, problem.domain.regions)
        epbcs = Conditions.from_conf(conf.epbcs, problem.domain.regions)

        variables = Variables.from_conf(conf.variables, problem.fields)
        variables.equation_mapping(ebcs, epbcs, None, problem.functions)
        state = variables.create_state_vector()
        variables.apply_ebc(state)
        return variables.has_ebc(state)

########NEW FILE########
__FILENAME__ = test_periodic_bc_3d
# 04.06.2007, c
# last revision: 25.02.2008
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/various_formats/small3d.mesh'

material_1 = {
    'name' : 'coef',
    'values' : {'coef' : 1.0},
}

region_1000 = {
    'name' : 'Omega',
    'select' : 'all',
}
region_1 = {
    'name' : 'Left',
    'select' : 'vertices in (x < -0.499)',
    'kind' : 'facet',
}
region_2 = {
    'name' : 'Right',
    'select' : 'vertices in (x > 0.499)',
    'kind' : 'facet',
}
region_3 = {
    'name' : 'Near',
    'select' : 'vertices in (y < -0.499)',
    'kind' : 'facet',
}
region_4 = {
    'name' : 'Far',
    'select' : 'vertices in (y > 0.499)',
    'kind' : 'facet',
}
region_5 = {
    'name' : 'Bottom',
    'select' : 'vertices in (z < -0.499)',
    'kind' : 'facet',
}
region_6 = {
    'name' : 'Top',
    'select' : 'vertices in (z > 0.499)',
    'kind' : 'facet',
}

field_1 = {
    'name' : '3_displacement',
    'dtype' : 'real',
    'shape' : (3,),
    'region' : 'Omega',
    'approx_order' : 2,
}

field_2 = {
    'name' : 'pressure',
    'dtype' : 'real',
    'shape' : (1,),
    'region' : 'Omega',
    'approx_order' : 1,
}

variable_1 = {
    'name' : 'u',
    'kind' : 'unknown field',
    'field' : '3_displacement',
    'order' : 0,
}
variable_2 = {
    'name' : 'v',
    'kind' : 'test field',
    'field' : '3_displacement',
    'dual' : 'u',
}
variable_3 = {
    'name' : 'p',
    'kind' : 'unknown field',
    'field' : 'pressure',
    'order' : 1,
}
variable_4 = {
    'name' : 'q',
    'kind' : 'test field',
    'field' : 'pressure',
    'dual' : 'p',
}

ebcs = {}
epbc_10 = {
    'name' : 'rl',
    'region' : ['Left', 'Right'],
    'dofs' : {'u.all' : 'u.all', 'p.0' : 'p.0'},
    'match' : 'match_x_plane',
}
epbc_12 = {
    'name' : 'tb',
    'region' : ['Top', 'Bottom'],
    'dofs' : {'u.all' : 'u.all', 'p.0' : 'p.0'},
    'match' : 'match_z_plane',
}
epbc_13 = {
    'name' : 'nf',
    'region' : ['Near', 'Far'],
    'dofs' : {'u.all' : 'u.all', 'p.0' : 'p.0'},
    'match' : 'match_y_plane',
}

from sfepy.discrete.fem.periodic import match_x_plane, match_y_plane, match_z_plane

functions = {
    'match_x_plane' : (match_x_plane,),
    'match_y_plane' : (match_y_plane,),
    'match_z_plane' : (match_z_plane,),
}

from test_periodic_bc_2d import Test

########NEW FILE########
__FILENAME__ = test_permutations
import os.path as op

from sfepy.base.base import assert_
from sfepy.base.testing import TestCommon

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        return Test(conf = conf, options = options)

    def test_rcm(self):
        from sfepy import data_dir
        from sfepy.linalg import rcm, permute_in_place, save_sparse_txt
        from sfepy.discrete.fem import Mesh

        filename = data_dir + '/meshes/2d/special/square_triquad.mesh'

        self.report('testing reversed Cuthill-McKee permutation')

        conf_dir = op.dirname(__file__)
        mesh = Mesh.from_file(filename, prefix_dir=conf_dir)

        graph = mesh.create_conn_graph()
        graph0 = graph.copy()

        save_sparse_txt(op.join(self.options.out_dir, 'test_rcm_graph_orig'),
                        graph, fmt='%d %d %d\n')

        perm = rcm(graph)

        permute_in_place(graph, perm)
        save_sparse_txt(op.join(self.options.out_dir, 'test_rcm_graph_rcm'),
                        graph, fmt='%d %d %d\n')
        
        assert_((graph0.indptr != graph.indptr).any())
        assert_((graph0.indices != graph.indices).any())

        permute_in_place(graph, perm, inverse=True)
        save_sparse_txt(op.join(self.options.out_dir, 'test_rcm_graph_rcm_inv'),
                        graph, fmt='%d %d %d\n')

        assert_((graph0.indptr == graph.indptr).all())
        assert_((graph0.indices == graph.indices).all())

        return True

########NEW FILE########
__FILENAME__ = test_poly_spaces
"""
Test continuity of polynomial basis and its gradients along an edge on
:math:`y` line (2D) or on a face in :math:`x`-:math:`y` plane (3D) between two
elements aligned with the coordinate system, stack one on top of the other. The
evaluation occurs in several points shifted by a very small amount from the
boundary between the elements into the top and the bottom element.

For H1 space, the basis should be continuous. The components of its gradient
parallel to the edge/face should be continuous as well, while the perpendicular
component should have the same absolute value, but different sign in the top
and the bottom element.

All connectivity permutations of the two elements are tested.

WARNING: Lagrange basis on 3_8 elements fails the test for order >= 3 for many
connectivity permutations!
"""
import numpy as nm

from sfepy.base.testing import TestCommon
from sfepy.base.base import assert_

rsels = {
    '2_3' : 'vertices in (y > -0.1) & (y < 0.1)',
    '2_4' : 'vertices in (y > 0.9) & (y < 1.1)',
    '3_4' : 'vertices in (z > -0.1) & (z < 0.1)',
    '3_8' : 'vertices in (z > 0.9) & (z < 1.1)',
}

eps = 1e-5

shifts = {
    '2_3' : nm.array([[0.0, 0.0], [0.0, eps]], dtype=nm.float64),
    '2_4' : nm.array([[0.0, 1.0], [0.0, eps]], dtype=nm.float64),
    '3_4' : nm.array([[0.0, 0.0, 0.0], [0.0, 0.0, eps]], dtype=nm.float64),
    '3_8' : nm.array([[0.0, 0.0, 1.0], [0.0, 0.0, eps]], dtype=nm.float64),
}

def _gen_common_data(orders, gels, report):
    import sfepy
    from sfepy.base.base import Struct
    from sfepy.linalg import combine
    from sfepy.discrete import FieldVariable, Integral
    from sfepy.discrete.fem import Mesh, FEDomain, Field
    from sfepy.discrete.fem.global_interp import get_ref_coors

    bases = ([ii for ii in combine([['2_4', '3_8'],
                                    ['lagrange', 'lobatto']])]
             + [ii for ii in combine([['2_3', '3_4'],
                                      ['lagrange']])])
    for geom, poly_space_base in bases:
        report('geometry: %s, base: %s' % (geom, poly_space_base))

        order = orders[geom]
        integral = Integral('i', order=order)

        aux = '' if geom in ['2_4', '3_8'] else 'z'
        mesh0 = Mesh.from_file('meshes/elements/%s_2%s.mesh' % (geom, aux),
                               prefix_dir=sfepy.data_dir)
        gel = gels[geom]

        perms = gel.get_conn_permutations()

        qps, qp_weights = integral.get_qp(gel.surface_facet.name)
        zz = nm.zeros_like(qps[:, :1])
        qps = nm.hstack(([qps] + [zz]))

        shift = shifts[geom]
        rcoors = nm.ascontiguousarray(qps
                                      + shift[:1, :] - shift[1:, :])
        ccoors = nm.ascontiguousarray(qps
                                      + shift[:1, :] + shift[1:, :])

        for ir, pr in enumerate(perms):
            for ic, pc in enumerate(perms):
                report('ir: %d, ic: %d' % (ir, ic))
                report('pr: %s, pc: %s' % (pr, pc))

                mesh = mesh0.copy()
                conn = mesh.conns[0]
                conn[0, :] = conn[0, pr]
                conn[1, :] = conn[1, pc]

                cache = Struct(mesh=mesh)

                domain = FEDomain('domain', mesh)
                omega = domain.create_region('Omega', 'all')
                region = domain.create_region('Facet', rsels[geom], 'facet')
                field = Field.from_args('f', nm.float64, shape=1,
                                        region=omega, approx_order=order,
                                        poly_space_base=poly_space_base)
                var = FieldVariable('u', 'unknown', field)
                report('# dofs: %d' % var.n_dof)

                vec = nm.empty(var.n_dof, dtype=var.dtype)

                ap = field.aps[0]
                ps = ap.interp.poly_spaces['v']

                dofs = field.get_dofs_in_region_group(region, 0,
                                                      merge=False)
                edofs, fdofs = nm.unique(dofs[1]), nm.unique(dofs[2])

                rrc, rcells, rstatus = get_ref_coors(field, rcoors,
                                                     cache=cache)
                crc, ccells, cstatus = get_ref_coors(field, ccoors,
                                                     cache=cache)
                assert_((rstatus == 0).all() and (cstatus == 0).all())

                yield (geom, poly_space_base, qp_weights, mesh, ir, ic,
                       ap, ps, rrc, rcells[0, 1], crc, ccells[0, 1],
                       vec, edofs, fdofs)

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        from sfepy.discrete.fem.geometry_element import GeometryElement

        gels = {}
        for key in ['2_3', '2_4', '3_4', '3_8']:
            gel = GeometryElement(key)
            gel.create_surface_facet()
            gels[key] = gel

        return Test(conf=conf, options=options, gels=gels)

    def test_continuity(self):
        ok = True
        orders = {'2_3' : 3, '2_4' : 3, '3_4' : 4, '3_8' : 3}

        bads = []
        bad_families = set()
        for (geom, poly_space_base, qp_weights, mesh, ir, ic,
             ap, ps, rrc, rcell, crc, ccell, vec,
             edofs, fdofs) in _gen_common_data(orders, self.gels, self.report):

            if poly_space_base == 'lagrange':
                rbf = ps.eval_base(rrc)
                cbf = ps.eval_base(crc)

            else:
                rbf = ps.eval_base(rrc, ori=ap.ori[:1])
                cbf = ps.eval_base(crc, ori=ap.ori[1:])

            dofs = nm.r_[edofs, fdofs]

            res = nm.zeros((2, dofs.shape[0]), dtype=nm.int32)
            res[0, :] = dofs
            for ii, ip in enumerate(dofs):
                vec.fill(0.0)
                vec[ip] = 1.0

                evec = vec[ap.econn]

                rvals = nm.dot(rbf, evec[rcell])
                cvals = nm.dot(cbf, evec[ccell])

                _ok = nm.allclose(rvals, cvals, atol=1e-14, rtol=0.0)
                res[1, ii] = _ok
                if not _ok:
                    bads.append([geom, poly_space_base, ir, ic, ip])
                    bad_families.add((geom, poly_space_base))

                ok = ok and _ok

            self.report('results (dofs, status: 1 ok, 0 failure):\n%s' % res)

        if not ok:
            self.report('continuity errors:\n', bads)
            self.report('%d in total!' % len(bads))
            self.report('continuity errors occurred in these spaces:\n',
                        bad_families)

        return ok

    def test_gradients(self):
        from sfepy.discrete.fem.mappings import VolumeMapping

        ok = True
        orders = {'2_3' : 3, '2_4' : 3, '3_4' : 4, '3_8' : 3}

        bads = []
        bad_families = set()
        for (geom, poly_space_base, qp_weights, mesh, ir, ic,
             ap, ps, rrc, rcell, crc, ccell, vec,
             edofs, fdofs) in _gen_common_data(orders, self.gels, self.report):
            conn = mesh.conns[0]
            gel = self.gels[geom]

            geo_ps = ap.interp.get_geom_poly_space('v')
            rmapping = VolumeMapping(mesh.coors, conn[rcell:rcell+1],
                                     poly_space=geo_ps)
            rori = ap.ori[:1] if ap.ori is not None else None
            rvg = rmapping.get_mapping(rrc, qp_weights,
                                       poly_space=ps, ori=rori)
            rbfg = rvg.bfg

            cmapping = VolumeMapping(mesh.coors, conn[ccell:ccell+1],
                                     poly_space=geo_ps)
            cori = ap.ori[1:] if ap.ori is not None else None
            cvg = cmapping.get_mapping(crc, qp_weights,
                                       poly_space=ps, ori=cori)
            cbfg = cvg.bfg

            dofs = nm.r_[edofs, fdofs]

            res = nm.zeros((2, dofs.shape[0]), dtype=nm.int32)
            res[0, :] = dofs
            for ii, ip in enumerate(dofs):
                vec.fill(0.0)
                vec[ip] = 1.0

                evec = vec[ap.econn]

                rvals = nm.dot(rbfg, evec[rcell])[0]
                cvals = nm.dot(cbfg, evec[ccell])[0]

                okx = nm.allclose(rvals[:, 0], cvals[:, 0],
                                  atol=1e-12, rtol=0.0)
                if gel.dim == 2:
                    oky = nm.allclose(rvals[:, 1], -cvals[:, 1],
                                      atol=1e-12, rtol=0.0)
                    _ok = okx and oky

                else:
                    oky = nm.allclose(rvals[:, 1], cvals[:, 1],
                                      atol=1e-12, rtol=0.0)
                    okz = nm.allclose(rvals[:, 2], -cvals[:, 2],
                                      atol=1e-12, rtol=0.0)
                    _ok = okx and oky and okz

                res[1, ii] = _ok
                if not _ok:
                    bads.append([geom, poly_space_base, ir, ic, ip])
                    bad_families.add((geom, poly_space_base))

                ok = ok and _ok

            self.report('results (dofs, status: 1 ok, 0 failure):\n%s' % res)

        if not ok:
            self.report('gradient continuity errors:\n', bads)
            self.report('%d in total!' % len(bads))
            self.report('gradient continuity errors occurred in these'
                        ' spaces:\n', bad_families)

        return ok

########NEW FILE########
__FILENAME__ = test_projections
import os.path as op
import numpy as nm

import sfepy
from sfepy.discrete import FieldVariable
from sfepy.discrete.fem import Mesh, FEDomain, Field

from sfepy.base.base import assert_
from sfepy.base.testing import TestCommon

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        mesh = Mesh.from_file('meshes/2d/square_unit_tri.mesh',
                              prefix_dir=sfepy.data_dir)
        domain = FEDomain('domain', mesh)

        omega = domain.create_region('Omega', 'all')

        field = Field.from_args('linear', nm.float64, 'scalar', omega,
                                approx_order=1)

        test = Test(conf=conf, options=options, omega=omega, field=field)
        return test

    def test_mass_matrix(self):
        from sfepy.discrete.projections import create_mass_matrix

        field = self.field

        mtx = create_mass_matrix(field)

        assert_(mtx.shape == (field.n_nod, field.n_nod))
        assert_(abs(mtx.sum() - 1.0) < 1e-14)

        return True

    def test_projection_tri_quad(self):
        from sfepy.discrete.projections import make_l2_projection

        source = FieldVariable('us', 'unknown', self.field)

        coors = self.field.get_coor()
        vals = nm.sin(2.0 * nm.pi * coors[:,0] * coors[:,1])
        source.set_data(vals)

        name = op.join(self.options.out_dir,
                       'test_projection_tri_quad_source.vtk')
        source.save_as_mesh(name)

        mesh = Mesh.from_file('meshes/2d/square_quad.mesh',
                              prefix_dir=sfepy.data_dir)
        domain = FEDomain('domain', mesh)

        omega = domain.create_region('Omega', 'all')


        field = Field.from_args('bilinear', nm.float64, 'scalar', omega,
                                approx_order=1)

        target = FieldVariable('ut', 'unknown', field)

        make_l2_projection(target, source)

        name = op.join(self.options.out_dir,
                       'test_projection_tri_quad_target.vtk')
        target.save_as_mesh(name)

        bbox = self.field.domain.get_mesh_bounding_box()
        x = nm.linspace(bbox[0, 0] + 0.001, bbox[1, 0] - 0.001, 20)
        y = nm.linspace(bbox[0, 1] + 0.001, bbox[1, 1] - 0.001, 20)

        xx, yy = nm.meshgrid(x, y)
        test_coors = nm.c_[xx.ravel(), yy.ravel()].copy()

        vec1 = source.evaluate_at(test_coors)
        vec2 = target.evaluate_at(test_coors)

        ok = (nm.abs(vec1 - vec2) < 0.01).all()

        return ok

########NEW FILE########
__FILENAME__ = test_quadratures
import numpy as nm

try:
    import sympy as sm
except ImportError:
    sm = None

from sfepy.base.testing import TestCommon
from sfepy.base.base import assert_, ordered_iteritems

def symarray(prefix, shape):
    """
    Copied from SymPy so that the tests pass for its different versions.
    """
    arr = nm.empty(shape, dtype=object)
    for index in nm.ndindex(shape):
        arr[index] = sm.Symbol('%s_%s' % (prefix, '_'.join(map(str, index))))
    return arr

def get_poly(order, dim, is_simplex=False):
    """
    Construct a polynomial of given `order` in space dimension `dim`,
    and integrate it symbolically over a rectangular or simplex domain
    for coordinates in [0, 1].
    """
    xs = symarray('x', dim)

    opd = max(1, int((order + 1) / dim))

    poly = 1.0
    oo = 0
    for ii, x in enumerate(xs):
        if ((oo + opd) > order) or (ii == (len(xs) - 1)):
            opd = max(order - oo, 0)

        poly *= (x**opd + 1)
        oo += opd

    assert_(oo == order)

    limits = [[xs[ii], 0, 1] for ii in range(dim)]
    if is_simplex:
        for ii in range(1, dim):
            for ip in range(0, ii):
                limits[ii][2] -= xs[ip]

    integral = sm.integrate(poly, *reversed(limits))

    return xs, poly, limits, integral

class Test(TestCommon):

    @staticmethod
    def from_conf( conf, options ):

        test = Test(conf=conf, options=options)
        return test

    def test_weight_consistency(self):
        """
        Test if integral of 1 (= sum of weights) gives the domain volume.
        """
        from sfepy.discrete.quadratures import quadrature_tables

        ok = True
        for geometry, qps in ordered_iteritems(quadrature_tables):
            self.report('geometry:', geometry)
            for order, qp in ordered_iteritems(qps):
                diff = nm.abs(qp.weights.sum() - qp.volume)
                _ok = diff < 1e-14
                self.report('order %d: %s (%.2e)' % (order, _ok, diff))

                ok = ok and _ok

        return ok

    def test_quadratures(self):
        """
        Test if the quadratures have orders they claim to have, using
        symbolic integration by sympy.
        """
        from sfepy.discrete.quadratures import quadrature_tables, tp_geometries
        from sfepy.discrete.integrals import Integral

        if sm is None:
            self.report('cannot import sympy, skipping')
            return True

        quad = Integral('test_integral')

        ok = True
        failed = []
        for geometry, qps in ordered_iteritems(quadrature_tables):
            self.report('geometry:', geometry)

            if geometry in tp_geometries:
                iter_qp = xrange(1, 11)

            elif geometry == '2_3':
                iter_qp = xrange(1, 25)

            elif geometry == '3_4':
                iter_qp = xrange(1, 12)

            else:
                iter_qp = sorted(qps.keys())

            for order in iter_qp:
                self.report('order:', order)

                aux = self._test_quadratures(quad, geometry, order)
                _ok, integral, val = aux

                if not _ok:
                    failed.append((geometry, order, float(integral), val))

                ok = ok and _ok

        if not ok:
            self.report('failed:')
            for aux in failed:
                self.report(aux, '%.1e' % abs(aux[2] - aux[3]))

        return ok

    def _test_quadratures(self, quad, geometry, order):
        dim = int(geometry[0])
        n_v = int(geometry[2])
        is_simplex = n_v == (dim + 1)

        porder = order if is_simplex else dim * order

        xs, poly, limits, integral = get_poly(porder, dim,
                                              is_simplex=is_simplex)

        self.report('  polynomial:', poly)
        self.report('  limits:', limits)
        self.report('  integral:', integral)

        def fun(coors):
            vals = nm.empty(coors.shape[0], dtype=nm.float64)

            subs = {}
            for ir, cc in enumerate(coors):
                for ic, x in enumerate(xs):
                    subs[x] = coors[ir,ic]
                vals[ir] = float(poly.subs(subs))

            return vals

        val = quad.integrate(fun, order=order, geometry=geometry)
        ok = nm.allclose(val, float(integral), rtol=0.0, atol=1e-11)

        self.report('  sym. == num.: %f == %f -> %s' %
                    (float(integral), val, ok))

        return ok, integral, val

########NEW FILE########
__FILENAME__ = test_regions
import numpy as nm

from sfepy.base.conf import transform_functions
from sfepy.base.testing import TestCommon

def get_vertices(coors, domain=None):
    x, z = coors[:,0], coors[:,2]

    return nm.where((z < 0.1) & (x < 0.1))[0]

def get_cells(coors, domain=None):
    return nm.where(coors[:, 0] < 0)[0]

class Test(TestCommon):

    @staticmethod
    def from_conf( conf, options ):
        from sfepy import data_dir
        from sfepy.discrete.fem import Mesh, FEDomain
        from sfepy.discrete import Functions

        mesh = Mesh('test mesh',
                    data_dir + '/meshes/various_formats/abaqus_tet.inp')
        mesh.nodal_bcs['set0'] = [0, 7]
        domain = FEDomain('test domain', mesh)

        conf_functions = {
            'get_vertices' : (get_vertices,),
            'get_cells' : (get_cells,),
        }
        functions = Functions.from_conf(transform_functions(conf_functions))

        test = Test(conf=conf, options=options,
                    domain=domain, functions=functions)
        return test

    def test_selectors(self):
        """
        Test basic region selectors.
        """
        selectors = [
            'all',
            'vertices of surface',
            'vertices of group 0',
            'vertices of set set0',
            'vertices in (z < 0.1) & (x < 0.1)',
            'vertices by get_vertices',
            'vertex 0, 1, 2',
            'vertex in r.r6',
            'cells of group 0',
            # 'cells of set 0', not implemented...
            'cells by get_cells',
            'cell 1, 4, 5',
            'cell (0, 1), (0, 4), (0, 5)',
            'copy r.r5',
            'r.r5',
        ]

        vertices = [
            [0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],
            [0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],
            [0,  1,  3,  7],
            [0,  7],
            [1,  2,  3,  4,  5,  9, 11],
            [1,  2,  3,  4,  5,  9, 11],
            [0,  1,  2],
            [0],
            [0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],
            [0,  1,  2,  3,  4,  5,  6,  9, 10, 11],
            [0,  1,  2,  3,  4,  5,  6,  8],
            [0,  1,  2,  3,  4,  5,  6,  8],
            [1,  2,  3,  4,  5,  9, 11],
            [1,  2,  3,  4,  5,  9, 11],
        ]

        ok = True
        for ii, sel in enumerate(selectors):
            self.report('select:', sel)
            reg = self.domain.create_region('r%d' % ii, sel,
                                            functions=self.functions)
            _ok = ((len(reg.vertices) == len(vertices[ii]))
                   and (reg.vertices == vertices[ii]).all())
            self.report('  vertices:', _ok)

            ok = ok and _ok

        return ok

    def test_operators(self):
        """
        Test operators in region selectors.
        """
        ok = True

        r1 = self.domain.create_region('r1', 'all')

        sel = 'r.r1 -v vertices of group 0'
        self.report('select:', sel)
        reg = self.domain.create_region('reg', sel)
        av = [2,  4,  5,  6,  8,  9, 10, 11, 12]
        _ok = (reg.vertices == nm.array(av)).all()
        self.report('  vertices:', _ok)
        ok = ok and _ok

        sel = 'vertex 0, 1, 2 +v vertices of group 0'
        self.report('select:', sel)
        reg = self.domain.create_region('reg', sel)
        av = [0,  1,  2,  3,  7]
        _ok = (reg.vertices == nm.array(av)).all()
        self.report('  vertices:', _ok)
        ok = ok and _ok

        sel = 'vertex 0, 1, 2 *v vertices of group 0'
        self.report('select:', sel)
        reg = self.domain.create_region('reg', sel)
        av = [0,  1]
        _ok = (reg.vertices == nm.array(av)).all()
        self.report('  vertices:', _ok)
        ok = ok and _ok

        sel = 'r.r1 -c cell 1, 4, 5'
        self.report('select:', sel)
        reg = self.domain.create_region('reg', sel)
        _ok = (nm.setdiff1d(r1.cells[0], [1, 4, 5]) == reg.cells[0]).all()
        self.report('  cells:', _ok)
        ok = ok and _ok

        sel = 'cell 8, 3 +c cell 1, 4, 5'
        self.report('select:', sel)
        reg = self.domain.create_region('reg', sel)
        cells = [1,  3,  4,  5,  8]
        _ok = (reg.cells == nm.array(cells)).all()
        self.report('  cells:', _ok)
        ok = ok and _ok

        sel = 'cell 8, 3, 2 *c cell 8, 4, 2, 7'
        self.report('select:', sel)
        reg = self.domain.create_region('reg', sel)
        cells = [2,  8]
        _ok = (reg.cells == nm.array(cells)).all()
        self.report('  cells:', _ok)
        ok = ok and _ok

        return ok

########NEW FILE########
__FILENAME__ = test_semismooth_newton
import sympy as sm
import numpy as nm
import scipy.sparse as sps

from sfepy.base.base import dict_to_struct
from sfepy.base.testing import TestCommon

conf = {
    'name' : 'semismooth_newton',
    'kind' : 'nls.semismooth_newton',

    'semismooth' : True,

    'i_max'      : 10,
    'eps_a'      : 1e-8,
    'eps_r'      : 1e-2,
    'macheps'   : 1e-16,
    'lin_red'    : 1e-2, # Linear system error < (eps_a * lin_red).
    'ls_red_reg' : 0.1,
    'ls_red_alt' : 0.01,
    'ls_red_warp' : 0.001,
    'ls_on'      : 2.0,
    'ls_min'     : 1e-10,
    ## 'log'        : {'plot' : 'aux.png'},
}

ls_conf = {
    'name' : 'ls',
    'kind' : 'ls.scipy_direct',
}

alpha = 30.0 * nm.pi / 180.0
ca = nm.cos(alpha)
sa = nm.sin(alpha)

# Coefficient of friction.
fc = 0.01

# Spring (bar) stiffnesses.
ks = nm.ones((7,), dtype=nm.float64)

# Load vector.
fs = nm.array([0, 0, 1, 1, -1, -1], dtype=nm.float64)


def eval_matrix(mtx, **kwargs):

    num_mtx = nm.zeros(mtx.shape, dtype=nm.float64)

    mtx = mtx.subs(kwargs)
    for ir in range(mtx.shape[0]):
        for ic in range(mtx.shape[1]):
            num_mtx[ir,ic] = float(mtx[ir,ic])

    return num_mtx

def convert_to_csr(m_in):
    m_out = {}
    for key, mtx in m_in.iteritems():
        m_out[key] = sps.csr_matrix(mtx)

    return m_out

def define_matrices():
    e = sm.zeros((7, 1))
    e[0] = sm.sympify('u1')
    e[1] = sm.sympify('c * u3 + s * v3')
    e[2] = sm.sympify('u2 - u1')
    e[3] = sm.sympify('v4 + u2n')
    e[4] = sm.sympify('u4 - u3')
    e[5] = sm.sympify('v3 + u1n')
    e[6] = sm.sympify('c * u4 + s * v4 - c * u1 + s * u1n')
    k = sm.Matrix(7, 1, lambda i, j: sm.Symbol('k%d' % i))

    psi = (k.T * e.applyfunc(lambda x: x**2))[0]
    work = sm.sympify('u1 * g1 + u2 * g2')

    w = ['u1', 'u2', 'u3', 'u4', 'v3', 'v4']
    wbar = w[:2]
    g = ['g1', 'g2']

    mtx_a = sm.zeros((6, 6))
    for ir in range(mtx_a.shape[0]):
        for ic in range(mtx_a.shape[1]):
            mtx_a[ir,ic] = sm.diff(sm.diff(psi, w[ir]), w[ic])

    mtx_b_bar = sm.zeros((2, 6))
    for ir in range(mtx_b_bar.shape[0]):
        for ic in range(mtx_b_bar.shape[1]):
            mtx_b_bar[ir,ic] = sm.diff(sm.diff(work, g[ir]), w[ic])

    mtx_b = sm.zeros((2, 2))
    for ir in range(mtx_b.shape[0]):
        for ic in range(mtx_b.shape[1]):
            mtx_b[ir,ic] = sm.diff(sm.diff(work, wbar[ir]), g[ic])

    mtx_c = sm.eye(2)

    sn1 = sm.diff(psi, 'u1n').subs('u1n', 0)
    sn2 = sm.diff(psi, 'u2n').subs('u2n', 0)
    sn = sm.Matrix([sn1, sn2])

    mtx_d = sm.zeros((2, 6))
    for ir in range(mtx_d.shape[0]):
        for ic in range(mtx_d.shape[1]):
            mtx_d[ir,ic] = sm.diff(sn[ir], w[ic])

    subsd = {'c' : ca, 's' : sa}
    for ii, _k in enumerate(k):
        subsd['k%d' % ii] = ks[ii]

    m = {
        'A' : eval_matrix(mtx_a, **subsd),
        'Bb' : eval_matrix(mtx_b_bar),
        'B' : eval_matrix(mtx_b),
        'C' : eval_matrix(mtx_c),
    }

    ms = convert_to_csr(m)

    m['sn'] = sn.subs(subsd)
    m['D'] = mtx_d.subs(subsd)
    m['fs'] = fs

    return m, ms, w

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):

        test = Test(conf=conf, options=options)

        test.m, test.ms, test.w_names = define_matrices()

        return test

    def test_semismooth_newton(self):
        import numpy as nm
        from sfepy.solvers import Solver

        ns = [0, 6, 2, 2]

        offsets = nm.cumsum(ns)
        nx = offsets[-1]

        iw = slice(offsets[0], offsets[1])
        ig = slice(offsets[1], offsets[2])
        il = slice(offsets[2], offsets[3])

        def fun_smooth(vec_x):
            xw = vec_x[iw]
            xg = vec_x[ig]
            xl = vec_x[il]

            m = self.ms
            rw = m['A'] * xw - m['Bb'].T * xg - self.m['fs'] 
            rg = m['Bb'] * xw + xl * xg

            rwg = nm.r_[rw, rg]

            return rwg

        def fun_smooth_grad(vec_x):
            xw = vec_x[iw]
            xl = vec_x[il]
            xg = vec_x[ig]

            m = self.m

            mzl = nm.zeros((6, 2), dtype=nm.float64)

            mw = nm.c_[m['A'], -m['Bb'].T, mzl]
            mg = nm.c_[m['Bb'], nm.diag(xl), nm.diag(xg)]

            mx = nm.r_[mw, mg]

            mx = sps.csr_matrix(mx)
            return mx

        def fun_a(vec_x):
            xw = vec_x[iw]
            xg = vec_x[ig]

            subsd = {}
            for ii, key in enumerate(self.w_names):
                subsd[key] = xw[ii]

            sn = eval_matrix(self.m['sn'], **subsd).squeeze()

            ra = nm.abs(xg) - fc * nm.abs(sn)

            return -ra

        def fun_a_grad(vec_x):
            xw = vec_x[iw]
            xg = vec_x[ig]
            xl = vec_x[il]

            subsd = {}
            for ii, key in enumerate(self.w_names):
                subsd[key] = xw[ii]

            md = eval_matrix(self.m['D'], **subsd)
            sn = eval_matrix(self.m['sn'], **subsd).squeeze()

            ma = nm.zeros((xl.shape[0], nx), dtype=nm.float64)
            ma[:,iw] = - fc * nm.sign(sn)[:,None] * md
            ma[:,ig] = nm.sign(xg)[:,None] * self.m['C']

            return -sps.csr_matrix(ma)

        def fun_b(vec_x):
            xl = vec_x[il]

            return xl

        def fun_b_grad(vec_x):
            xl = vec_x[il]

            mb = nm.zeros((xl.shape[0], nx), dtype=nm.float64)
            mb[:,il] = self.m['C']

            return sps.csr_matrix(mb)

        vec_x0 = 0.1 * nm.ones((nx,), dtype=nm.float64)

        lin_solver = Solver.any_from_conf(dict_to_struct(ls_conf))
        status = {}
        solver = Solver.any_from_conf(dict_to_struct(conf),
                                      fun_smooth=fun_smooth,
                                      fun_smooth_grad=fun_smooth_grad,
                                      fun_a=fun_a,
                                      fun_a_grad=fun_a_grad,
                                      fun_b=fun_b,
                                      fun_b_grad=fun_b_grad,
                                      lin_solver=lin_solver,
                                      status=status)

        vec_x = solver(vec_x0)

        xw = vec_x[iw]
        xg = vec_x[ig]
        xl = vec_x[il]

        self.report('x:', xw)
        self.report('g:', xg)
        self.report('l:', xl)


        subsd = {}
        for ii, key in enumerate(self.w_names):
            subsd[key] = xw[ii]

        sn = eval_matrix(self.m['sn'], **subsd).squeeze()
        self.report('sn:', sn)

        ok = status['condition'] == 0

        return ok

########NEW FILE########
__FILENAME__ = test_sparse
from sfepy.base.testing import TestCommon

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        return Test(conf=conf, options=options)

    def test_compose_sparse(self):
        import numpy as nm
        import scipy.sparse as sps
        from sfepy.linalg import compose_sparse

        ok = True

        # basic
        ma = sps.csr_matrix([[1, 0], [0, 1]])
        mb = sps.coo_matrix([[1, 1]])
        mk = compose_sparse([[ma, mb.T], [mb, 0]])
        expected = nm.array([[1, 0, 1],
                             [0, 1, 1],
                             [1, 1, 0]])

        _ok = nm.alltrue(mk.toarray() == expected)
        self.report('basic: %s' % _ok)
        ok = ok and _ok

        # sizes and slices
        ma = sps.csr_matrix([[2, 3]])
        mb = sps.coo_matrix([[4, 5, 6]])

        mk = compose_sparse([[ma, mb]], col_sizes=[2, 3])
        expected = nm.array([[2, 3, 4, 5, 6]])

        _ok = nm.alltrue(mk.toarray() == expected)
        self.report('sizes: %s' % _ok)
        ok = ok and _ok

        i1 = slice(1, 3)
        i2 = slice(8, 11)
        mk = compose_sparse([[ma, mb]], col_sizes=[i1, i2])
        expected = nm.array([[0, 2, 3, 0, 0, 0, 0, 0, 4, 5, 6]])

        _ok = nm.alltrue(mk.toarray() == expected)
        self.report('slices: %s' % _ok)
        ok = ok and _ok

        # zero block sizes and slices
        mk = compose_sparse([[0, ma, 0, mb, 0]], col_sizes=[1, 2, 5, 3, 1])
        expected = nm.array([[0, 2, 3, 0, 0, 0, 0, 0, 4, 5, 6, 0]])

        _ok = nm.alltrue(mk.toarray() == expected)
        self.report('zero block sizes: %s' % _ok)
        ok = ok and _ok

        expected = nm.array([[0, 2, 3, 0, 4, 5, 6, 0, 0, 0, 0],
                             [0, 0, 0, 0, 0, 0, 0, 0, 4, 5, 6]])

        i0 = slice(0, 1)
        i1 = slice(1, 3)
        i2 = slice(4, 7)
        i3 = slice(8, 11)
        mk = compose_sparse([[0, ma, mb, 0],
                             [0, 0, 0, mb]], col_sizes=[i0, i1, i2, i3])

        _ok = nm.alltrue(mk.toarray() == expected)
        self.report('zero block slices: %s' % _ok)
        ok = ok and _ok

        return ok


########NEW FILE########
__FILENAME__ = test_tensors
from sfepy.base.testing import TestCommon

def get_ortho_d(phi1, phi2):
    import numpy as nm
    import sfepy.mechanics.tensors as tn

    v1 = nm.array([nm.cos(phi1), nm.sin(phi1), 0])
    v2 = nm.array([nm.cos(phi2), nm.sin(phi2), 0])
    om1 = nm.outer(v1, v1)
    om2 = nm.outer(v2, v2)

    ii = tn.get_sym_indices(3)

    o1 = om1.flat[ii]
    o2 = om2.flat[ii]

    dr = nm.outer(o1, o1) + nm.outer(o2, o2)
    return dr, v1, v2, om1, om2

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        return Test(conf=conf, options=options)

    def test_tensors(self):
        import numpy as nm
        import sfepy.mechanics.tensors as tn

        ok = True

        a_full = 2.0 * nm.ones((5,3,3), dtype=nm.float64)
        a_sym = 2.0 * nm.ones((5,6), dtype=nm.float64)

        _tr = nm.array([6.0] * 5, dtype=nm.float64)
        _vt_full = 2.0 * nm.tile(nm.eye(3, dtype=nm.float64), (5,1,1))
        _vt_sym = nm.tile(nm.array([2, 2, 2, 0, 0, 0], dtype=nm.float64),
                          (5,1,1))
        _dev_full = a_full - _vt_full
        _dev_sym = a_sym - _vt_sym
        _vms = 6.0 * nm.ones((5,1), dtype=nm.float64)

        tr = tn.get_trace(a_full, sym_storage=False)
        _ok = nm.allclose(tr, _tr, rtol=0.0, atol=1e-14)
        self.report('trace full: %s' % _ok)
        ok = ok and _ok

        tr = tn.get_trace(a_sym, sym_storage=True)
        ok = ok and nm.allclose(tr, _tr, rtol=0.0, atol=1e-14)
        self.report('trace sym: %s' % _ok)
        ok = ok and _ok

        vt = tn.get_volumetric_tensor(a_full, sym_storage=False)
        _ok = nm.allclose(vt, _vt_full, rtol=0.0, atol=1e-14)
        self.report('volumetric tensor full: %s' % _ok)
        ok = ok and _ok

        vt = tn.get_volumetric_tensor(a_sym, sym_storage=True)
        _ok = nm.allclose(vt, _vt_sym, rtol=0.0, atol=1e-14)
        self.report('volumetric tensor sym: %s' % _ok)
        ok = ok and _ok

        dev = tn.get_deviator(a_full, sym_storage=False)
        _ok = nm.allclose(dev, _dev_full, rtol=0.0, atol=1e-14)
        self.report('deviator full: %s' % _ok)
        ok = ok and _ok

        aux = (dev * nm.transpose(dev, (0, 2, 1))).sum(axis=1).sum(axis=1)
        vms2 = nm.sqrt((3.0/2.0) * aux)[:,None]

        dev = tn.get_deviator(a_sym, sym_storage=True)
        _ok = nm.allclose(dev, _dev_sym, rtol=0.0, atol=1e-14)
        self.report('deviator sym: %s' % _ok)
        ok = ok and _ok

        vms = tn.get_von_mises_stress(a_full, sym_storage=False)
        _ok = nm.allclose(vms, _vms, rtol=0.0, atol=1e-14)
        self.report('von Mises stress full: %s' % _ok)
        ok = ok and _ok

        vms = tn.get_von_mises_stress(a_sym, sym_storage=True)
        _ok = nm.allclose(vms, _vms, rtol=0.0, atol=1e-14)
        self.report('von Mises stress sym: %s' % _ok)
        ok = ok and _ok

        _ok = nm.allclose(vms2, _vms, rtol=0.0, atol=1e-14)
        self.report('von Mises stress via deviator: %s' % _ok)
        ok = ok and _ok

        t2s = nm.arange(9).reshape(3, 3)
        t2s = (t2s + t2s.T) / 2
        t4 = tn.get_t4_from_t2s(t2s)
        expected = nm.array([[[[0, 4], [4, 2]],
                              [[4, 8], [8, 6]]],
                             [[[4, 8], [8, 6]],
                              [[2, 6],  [6, 4]]]])
        _ok = nm.allclose(t4, expected, rtol=0.0, atol=1e-14)
        self.report('full 4D tensor from 2D matrix, 2D space: %s' % _ok)
        ok = ok and _ok

        return ok

    def test_transform_data(self):
        import numpy as nm
        from sfepy.mechanics.tensors import transform_data

        ok = True

        coors = nm.eye(3)

        data = nm.eye(3)
        expected = nm.zeros((3, 3))
        expected[[0, 1, 2], [0, 0, 2]] = 1.0

        out = transform_data(data, coors)

        _ok = nm.allclose(out, expected, rtol=0.0, atol=1e-14)
        self.report('vectors in cylindrical coordinates: %s' % _ok)
        ok = ok and _ok

        data = nm.zeros((3, 6))
        data[:, :3] = [[1, 2, 3]]
        expected = data.copy()
        expected[1, [0, 1]] = expected[1, [1, 0]]

        out = transform_data(data, coors)

        _ok = nm.allclose(out, expected, rtol=0.0, atol=1e-14)
        self.report('sym. tensors in cylindrical coordinates: %s' % _ok)
        ok = ok and _ok

        return ok

    def test_transform_data4(self):
        import numpy as nm
        import sfepy.mechanics.tensors as tn

        ok = True

        if not hasattr(nm, 'einsum'):
            self.report('no numpy.einsum(), skipping!')
            return ok

        expected = nm.zeros((6, 6), dtype=nm.float64)
        expected[0, 0] = expected[1, 1] = 1.0

        phi = nm.deg2rad(30.)
        dr, v1, v2, om1, om2 = get_ortho_d(phi, phi + nm.deg2rad(90.))

        # Rotate coordinate system by phi.
        mtx = tn.make_axis_rotation_matrix([0., 0., 1.], phi)
        do = tn.transform_data(dr[None, ...], mtx=mtx[None, ...])

        _ok = nm.allclose(do, expected, rtol=0.0, atol=1e-14)
        self.report('sym. 4th-th order tensor rotation: %s' % _ok)
        ok = ok and _ok

        dt, vt1, vt2, omt1, omt2 = get_ortho_d(0, nm.deg2rad(90.))

        expected1 = nm.zeros((3, 3), dtype=nm.float64)
        expected1[0, 0] = 1.0

        expected2 = nm.zeros((3, 3), dtype=nm.float64)
        expected2[1, 1] = 1.0

        omr1 = nm.einsum('pq,ip,jq->ij', om1, mtx, mtx)
        omr2 = nm.einsum('pq,ip,jq->ij', om2, mtx, mtx)

        ii = tn.get_sym_indices(3)
        jj = tn.get_full_indices(3)

        o1 = om1.flat[ii]
        o2 = om2.flat[ii]

        omr12 = tn.transform_data(o1[None,...], mtx=mtx[None, ...])[0, jj]
        omr22 = tn.transform_data(o2[None,...], mtx=mtx[None, ...])[0, jj]

        _ok1 = nm.allclose(omr1, expected1, rtol=0.0, atol=1e-14)
        _ok2 = nm.allclose(omr12, expected1, rtol=0.0, atol=1e-14)
        self.report('einsum-transform_data compatibility 1: %s %s'
                    % (_ok1, _ok2))
        ok = ok and _ok1 and _ok2

        _ok1 = nm.allclose(omr2, expected2, rtol=0.0, atol=1e-14)
        _ok2 = nm.allclose(omr22, expected2, rtol=0.0, atol=1e-14)
        self.report('einsum-transform_data compatibility 2: %s %s'
                    % (_ok1, _ok2))
        ok = ok and _ok1 and _ok2

        return ok

    def test_stress_transform(self):
        import numpy as nm
        from sfepy.mechanics.tensors import StressTransform

        stress_2pk = nm.arange(6) + 1

        def_grad = nm.array([[0.5047051 , 0.71142596, 0.10180901],
                             [0.13427707, 0.87156371, 0.42612244],
                             [0.27509466, 0.6262605 , 0.87659051]])
        det = nm.linalg.det(def_grad)

        aux = stress_2pk[[0, 3, 4, 3, 1, 5, 4, 5, 2]].reshape(3, 3)
        expected = nm.dot(nm.dot(def_grad, aux), def_grad.T) / det
        expected = expected.ravel()[[0, 4, 8, 1, 2, 5]][:, None]
        expected = nm.tile(expected, (5, 1, 1, 1))

        transform = StressTransform(nm.tile(def_grad, (5, 1, 1, 1)))

        stress_2pk.shape = (6, 1)
        ts = nm.tile(stress_2pk.reshape((6, 1)), (5, 1, 1, 1))
        stress_cauchy = transform.get_cauchy_from_2pk(ts)

        ok = nm.allclose(stress_cauchy, expected, rtol=0.0, atol=1e-12)
        self.report('stress: Cauchy from second Piola-Kirchhoff: %s' % ok)

        return ok

########NEW FILE########
__FILENAME__ = test_term_call_modes
from copy import copy

import numpy as nm

from sfepy.base.testing import TestCommon
from sfepy.base.base import ordered_iteritems
from sfepy import data_dir

filename_meshes = [data_dir + '/meshes/elements/%s_2.mesh' % geom
                   for geom in ['2_3', '2_4', '3_4', '3_8']]

def make_term_args(arg_shapes, arg_kinds, arg_types, ats_mode, domain):
    from sfepy.base.base import basestr
    from sfepy.discrete import FieldVariable, Material, Variables, Materials
    from sfepy.discrete.fem import Field
    from sfepy.mechanics.tensors import dim2sym

    omega = domain.regions['Omega']
    dim = domain.shape.dim
    sym = dim2sym(dim)

    def _parse_scalar_shape(sh):
        if isinstance(sh, basestr):
            if sh == 'D':
                return dim

            elif sh == 'S':
                return sym

            elif sh == 'N': # General number ;)
                return 5

            else:
                return int(sh)

        else:
            return sh

    def _parse_tuple_shape(sh):
        if isinstance(sh, basestr):
            return [_parse_scalar_shape(ii.strip()) for ii in sh.split(',')]

        else:
            return (int(sh),)

    args = {}
    str_args = []
    materials = []
    variables = []
    for ii, arg_kind in enumerate(arg_kinds):
        if ats_mode is not None:
            extended_ats = arg_types[ii] + ('/%s' % ats_mode)

        else:
            extended_ats = arg_types[ii]

        try:
            sh = arg_shapes[arg_types[ii]]

        except KeyError:
            sh = arg_shapes[extended_ats]

        if arg_kind.endswith('variable'):
            shape = _parse_scalar_shape(sh[0] if isinstance(sh, tuple) else sh)
            field = Field.from_args('f%d' % ii, nm.float64, shape, omega,
                                    approx_order=1)

            if arg_kind == 'virtual_variable':
                if sh[1] is not None:
                    istate = arg_types.index(sh[1])

                else:
                    # Only virtual variable in arguments.
                    istate = -1
                    # -> Make fake variable.
                    var = FieldVariable('u-1', 'unknown', field)
                    var.set_constant(0.0)
                    variables.append(var)

                var = FieldVariable('v', 'test', field,
                                    primary_var_name='u%d' % istate)

            elif arg_kind == 'state_variable':
                var = FieldVariable('u%d' % ii, 'unknown', field)
                var.set_constant(0.0)

            elif arg_kind == 'parameter_variable':
                var = FieldVariable('p%d' % ii, 'parameter', field,
                                    primary_var_name='(set-to-None)')
                var.set_constant(0.0)

            variables.append(var)
            str_args.append(var.name)
            args[var.name] = var

        elif arg_kind.endswith('material'):
            if sh is None: # Switched-off opt_material.
                continue

            prefix = ''
            if isinstance(sh, basestr):
                aux = sh.split(':')
                if len(aux) == 2:
                    prefix, sh = aux

            shape = _parse_tuple_shape(sh)
            if (len(shape) > 1) or (shape[0] > 1):
                # Array.
                values = {'%sc%d' % (prefix, ii)
                          : nm.ones(shape, dtype=nm.float64)}

            elif (len(shape) == 1) and (shape[0] == 1):
                # Single scalar as a special value.
                values = {'.c%d' % ii : 1.0}

            else:
                raise ValueError('wrong material shape! (%s)' % shape)

            mat = Material('m%d' % ii, values=values)

            materials.append(mat)
            str_args.append(mat.name + '.' + 'c%d' % ii)
            args[mat.name] = mat

        else:
            str_args.append('user%d' % ii)
            args[str_args[-1]] = None

    materials = Materials(materials)
    variables = Variables(variables)

    return args, str_args, materials, variables

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        from sfepy.discrete import Integral
        from sfepy.discrete.fem import Mesh, FEDomain

        domains = []
        for filename in filename_meshes:
            mesh = Mesh.from_file(filename)
            domain = FEDomain('domain_%s' % mesh.name.replace(data_dir, ''),
                              mesh)
            domain.create_region('Omega', 'all')
            domain.create_region('Gamma', 'vertices of surface', 'facet')

            domains.append(domain)

        integral = Integral('i', order=3)

        test = Test(domains=domains, integral=integral,
                    conf=conf, options=options)
        return test

    def test_term_call_modes(self):
        from sfepy.terms import term_table
        ok = True

        failed = []
        for domain in self.domains:
            self.report('domain: %s' % domain.name)

            for _, term_cls in ordered_iteritems(term_table):
                if not domain.groups[0].gel.name in term_cls.geometries:
                    continue

                rname = 'Omega' \
                        if term_cls.integration in ('volume', 'point') \
                        else 'Gamma'

                self.report('<-- %s ...' % term_cls.name)

                if term_cls.arg_shapes:
                    try:
                        _ok = self._test_single_term(term_cls, domain, rname)

                    except:
                        _ok = False

                    if not _ok:
                        failed.append((domain.name, term_cls.name))

                    ok = ok and _ok
                    self.report('--> ok: %s' % _ok)

                else:
                    self.report('--> not tested!')

        self.report('failed:', failed)

        return ok

    def _test_single_term(self, term_cls, domain, rname):
        from sfepy.terms import Term
        from sfepy.terms.terms import get_arg_kinds

        ok = True

        term_call = term_cls.name + '(%s)'

        arg_shapes_list = term_cls.arg_shapes
        if not isinstance(arg_shapes_list, list):
            arg_shapes_list = [arg_shapes_list]

        prev_shapes = {}
        for _arg_shapes in arg_shapes_list:
            # Unset shapes are taken from the previous iteration.
            arg_shapes = copy(prev_shapes)
            arg_shapes.update(_arg_shapes)
            prev_shapes = arg_shapes

            self.report('arg_shapes:', arg_shapes)
            arg_types = term_cls.arg_types
            if not isinstance(arg_types[0], tuple):
                arg_types = (arg_types,)

            for iat, ats in enumerate(arg_types):
                self.report('arg_types:', ats)

                arg_kinds = get_arg_kinds(ats)
                modes = getattr(term_cls, 'modes', None)
                mode = modes[iat] if modes is not None else None
                aux = make_term_args(arg_shapes, arg_kinds, ats, mode, domain)
                args, str_args, materials, variables = aux

                self.report('args:', str_args)

                name = term_call % (', '.join(str_args))
                term = Term.new(name, self.integral,
                                domain.regions[rname], **args)
                term.setup()

                call_mode = 'weak' if term.names.virtual else 'eval'
                self.report('call mode:', call_mode)

                out = term.evaluate(mode=call_mode, ret_status=True)

                if call_mode == 'eval':
                    vals, status = out
                    vals = nm.array(vals)

                else:
                    vals, iels, status = out
                    vals = vals[0]

                _ok = nm.isfinite(vals).all()
                ok = ok and _ok
                self.report('values shape: %s' % (vals.shape,))
                if not _ok:
                    self.report('values are not finite!')
                    self.report(vals)

                _ok = status == 0
                if not _ok:
                    self.report('status is %d!' % status)

                ok = ok and _ok

                if term.names.virtual:
                    # Test differentiation w.r.t. state variables in the weak
                    # mode.
                    svars = term.get_state_variables(unknown_only=True)
                    for svar in svars:
                        vals, iels, status = term.evaluate(mode=call_mode,
                                                           diff_var=svar.name,
                                                           ret_status=True)
                        vals = vals[0]

                        _ok = status == 0
                        ok = ok and _ok
                        self.report('diff: %s' % svar.name)
                        if not _ok:
                            self.report('status is %d!' % status)

                        _ok = nm.isfinite(vals).all()
                        ok = ok and _ok
                        self.report('values shape: %s' % (vals.shape,))
                        if not _ok:
                            self.report('values are not finite!')
                            self.report(vals)

        return ok

########NEW FILE########
__FILENAME__ = test_term_consistency
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/2d/special/circle_in_square.mesh'

dim = 2

fields = {
    'scalar_field' : ('real', 'scalar', 'Omega', 1),
    'vector_field' : ('real', 'vector', 'Omega', 1),
}

variables = {
    'us'  : ('unknown field',   'scalar_field', 0),
    'ts'  : ('test field',      'scalar_field', 'us'),
    'ps1' : ('parameter field', 'scalar_field', 'us'),
    'ps2' : ('parameter field', 'scalar_field', 'us'),
    'uv'  : ('unknown field',   'vector_field', 1),
    'tv'  : ('test field',      'vector_field', 'uv'),
    'pv1' : ('parameter field', 'vector_field', 'uv'),
    'pv2' : ('parameter field', 'vector_field', 'uv'),
}

regions = {
    'Omega' : 'all',
    'Left' : ('vertices in (x < -0.499)', 'facet'),
}

integrals = {
    'i' : 2,
}

materials = {
    'm' : 'get_pars',
    'm2' : ({'K' : [[3.0, 0.1], [0.3, 1.0]]},),
}

equations = {
    'eq' : """dw_diffusion.i.Omega( m2.K, ts, us ) = 0"""
}

def get_pars(ts, coor, mode=None, term=None, **kwargs):
    if mode == 'qp':
        n_nod, dim = coor.shape
        sym = (dim + 1) * dim / 2

        if 'biot' in term.name:
            val = nm.zeros((sym, 1), dtype=nm.float64)
            val[:dim] = 0.132
            val[dim:sym] = 0.092
        elif 'volume_dot' in term.name:
            val = 1.0 / nm.array([3.8], dtype=nm.float64)
        elif 'diffusion' in term.name:
            val = nm.eye(dim, dtype=nm.float64)
        else:
            raise ValueError

        return {'val' : nm.tile(val, (coor.shape[0], 1, 1))}

functions = {
    'get_pars' : (get_pars,),
}

# (eval term prefix, parameter corresponding to test variable, 'd' variables,
# 'dw' variables (test must be paired with unknown, which should be at
# index 2!), mat mode)
test_terms = [
    ('%s_biot.i.Omega( m.val, %s, %s )',
     ('dw', 'ps1', ('pv1', 'ps1'), ('pv1', 'ts', 'us', 'uv', 'tv'))),
    ('%s_biot.i.Omega( m.val, %s, %s )',
     ('dw', 'pv1', ('pv1', 'ps1'), ('tv', 'ps1', 'uv', 'us', 'ts'))),
    ('%s_diffusion.i.Omega( m.val, %s, %s )',
     ('dw', 'ps1', ('ps1', 'ps2'), ('ts', 'ps1', 'us'))),
    ('%s_volume_dot.i.Omega( m.val, %s, %s )',
     ('dw', 'ps1', ('ps1', 'ps2'), ('ts', 'ps1', 'us'))),
]

import numpy as nm
from sfepy.base.testing import TestCommon

def _integrate(var, val_qp):
    from sfepy.discrete import Integral
    from sfepy.discrete.common.mappings import get_jacobian

    integral = Integral('i', 2)
    det = get_jacobian(var.field, integral)
    val = (val_qp * det).sum(axis=1) / det.sum(axis=1)

    return val

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        from sfepy.discrete import Problem

        problem = Problem.from_conf(conf, init_equations=False)
        test = Test(problem=problem,
                    conf=conf, options=options)
        return test

    def test_consistency_d_dw(self):
        from sfepy.discrete import Variables

        ok = True
        pb = self.problem
        for aux in test_terms:
            term_template, (prefix, par_name, d_vars, dw_vars) = aux
            print term_template, prefix, par_name, d_vars, dw_vars

            term1 = term_template % ((prefix,) + d_vars)

            variables = Variables.from_conf(self.conf.variables, pb.fields)

            for var_name in d_vars:
                var = variables[var_name]
                n_dof = var.field.n_nod * var.field.shape[0]
                aux = nm.arange(n_dof, dtype=nm.float64)
                var.set_data(aux)

            if prefix == 'd':
                val1 = pb.evaluate(term1, var_dict=variables.as_dict())

            else:
                val1 = pb.evaluate(term1, call_mode='d_eval',
                                   var_dict=variables.as_dict())

            self.report('%s: %s' % (term1, val1))

            term2 = term_template % (('dw',) + dw_vars[:2])

            vec, vv = pb.evaluate(term2, mode='weak',
                                  var_dict=variables.as_dict(),
                                  ret_variables=True)

            pvec = vv.get_state_part_view(vec, dw_vars[2])
            val2 = nm.dot(variables[par_name](), pvec)
            self.report('%s: %s' % (term2, val2))

            err = nm.abs(val1 - val2) / nm.abs(val1)
            _ok = err < 1e-12
            self.report('relative difference: %e -> %s' % (err, _ok))

            ok = ok and _ok

        return ok

    def test_eval_matrix(self):
        problem = self.problem

        problem.set_equations()
        problem.time_update(ebcs={}, epbcs={})

        var = problem.get_variables()['us']

        vec = nm.arange(var.n_dof, dtype=var.dtype)

        var.set_data(vec)

        val1 = problem.evaluate('dw_diffusion.i.Omega( m2.K, us, us )',
                                mode='eval', us=var)

        mtx = problem.evaluate('dw_diffusion.i.Omega( m2.K, ts, us )',
                               mode='weak', dw_mode='matrix')

        val2 = nm.dot(vec, mtx * vec)

        ok = (nm.abs(val1 - val2) / nm.abs(val1)) < 1e-15

        self.report('eval: %s, weak: %s, ok: %s' % (val1, val2, ok))

        return ok

    def test_vector_matrix(self):
        problem = self.problem

        problem.set_equations()
        problem.time_update()

        state = problem.create_state()
        state.apply_ebc()

        aux1 = problem.evaluate("dw_diffusion.i.Omega( m2.K, ts, us )",
                                mode='weak', dw_mode='vector')

        problem.time_update(ebcs={}, epbcs={})

        mtx = problem.evaluate("dw_diffusion.i.Omega( m2.K, ts, us )",
                               mode='weak', dw_mode='matrix')
        aux2g = mtx * state()
        problem.time_update(ebcs=self.conf.ebcs,
                            epbcs=self.conf.epbcs)
        aux2 = problem.equations.strip_state_vector(aux2g, follow_epbc=True)

        ret = self.compare_vectors(aux1, aux2,
                                   label1='vector mode',
                                   label2='matrix mode')
        if not ret:
            self.report('failed')

        return ret

    def test_surface_evaluate(self):
        from sfepy.discrete import FieldVariable
        problem = self.problem

        us = problem.get_variables()['us']
        vec = nm.empty(us.n_dof, dtype=us.dtype)
        vec[:] = 1.0
        us.set_data(vec)

        expr = 'ev_surface_integrate.i.Left( us )'
        val = problem.evaluate(expr, us=us)
        ok1 = nm.abs(val - 1.0) < 1e-15
        self.report('with unknown: %s, value: %s, ok: %s'
                    % (expr, val, ok1))

        ps1 = FieldVariable('ps1', 'parameter', us.get_field(),
                            primary_var_name='(set-to-None)')
        ps1.set_data(vec)

        expr = 'ev_surface_integrate.i.Left( ps1 )'
        val = problem.evaluate(expr, ps1=ps1)
        ok2 = nm.abs(val - 1.0) < 1e-15
        self.report('with parameter: %s, value: %s, ok: %s'
                    % (expr, val, ok2))
        ok2 = True

        return ok1 and ok2

    def test_ev_grad(self):
        problem = self.problem

        var = problem.create_variables(['us'])['us']
        val = nm.arange(var.n_dof, dtype=var.dtype)
        var.set_data(val)

        val1 = problem.evaluate('ev_grad.i.Omega( us )', us=var, mode='el_avg')
        self.report('ev_grad(el_avg): min, max:', val1.min(), val1.max())

        aux = problem.evaluate('ev_grad.i.Omega( us )', us=var, mode='qp')
        val2 = _integrate(var, aux)
        val2.shape = val1.shape
        self.report('ev_grad(qp): min, max:', val2.min(), val2.max())

        ok = self.compare_vectors(val1, val2,
                                  label1='de mode',
                                  label2='dq mode')
        if not ok:
            self.report('failed')

        return ok

    def test_ev_div(self):
        problem = self.problem

        var = problem.create_variables(['uv'])['uv']
        val = nm.arange(var.n_dof, dtype=var.dtype)
        var.set_data(val)

        val1 = problem.evaluate('ev_div.i.Omega( uv )', uv=var, mode='el_avg')
        self.report('ev_div(el_avg): min, max:', val1.min(), val1.max())

        aux = problem.evaluate('ev_div.i.Omega( uv )', uv=var, mode='qp')
        val2 = _integrate(var, aux)
        val2.shape = val1.shape
        self.report('ev_div(qp): min, max:', val2.min(), val2.max())

        ok = self.compare_vectors(val1, val2,
                                  label1='de mode',
                                  label2='dq mode')
        if not ok:
            self.report('failed')

        return ok

########NEW FILE########
__FILENAME__ = test_units
from sfepy.base.base import assert_
from sfepy.base.testing import TestCommon

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        return Test(conf=conf, options=options)

    def test_units(self):
        from sfepy.mechanics.units import Unit, Quantity, sm

        if sm is None:
            self.report('cannot import sympy, skipping')
            return True

        units = ['m', 's', 'kg', 'C']
        self.report('units:', units)
        unit_set = [Unit(key) for key in units]
 
        q1 = Quantity('stress', unit_set)
        self.report(q1.name, ':', q1())
        assert_(q1() == '1.0 Pa')
        assert_(q1('c') == '100.0 cPa')

        q2 = Quantity('force', unit_set)
        self.report(q2.name, ':', q2())
        assert_(q2() == '1.0 Newton')
        assert_(q2('d') == '0.1 dNewton')

        q3 = Quantity('energy', unit_set)
        self.report(q3.name, ':', q3())
        assert_(q3() == '1.0 J')
        assert_(q3('mu') == '1000000.0 muJ')

        units = ['mm', 's', 'g', 'C']
        self.report('units:', units)
        unit_set = [Unit(key) for key in units]
 
        q1 = Quantity('stress', unit_set)
        self.report(q1.name, ':', q1())
        assert_(q1() == '1.0 Pa')

        q2 = Quantity('force', unit_set)
        self.report(q2.name, ':', q2())
        assert_(q2() == '1.0 muNewton')

        q3 = Quantity('energy', unit_set)
        self.report(q3.name, ':', q3())
        assert_(q3() == '1.0 nJ')

        units = ['cm', 'ms', 'kg', 'kC']
        self.report('units:', units)
        unit_set = [Unit(key) for key in units]
 
        q1 = Quantity('stress', unit_set)
        self.report(q1.name, ':', q1())
        assert_(q1() == '0.1 GPa')

        q2 = Quantity('force', unit_set)
        self.report(q2.name, ':', q2())
        assert_(q2() == '10.0 kNewton')

        q3 = Quantity('energy', unit_set)
        self.report(q3.name, ':', q3())
        assert_(q3() == '0.1 kJ')

        q4 = Quantity('thermal_expandability', unit_set)
        self.report(q4.name, ':', q4())
        assert_(q4() == '0.1 MPa / C')

        assert_(q4('G') == '0.0001 GPa / C')
        assert_(q4('M') == '0.1 MPa / C')
        assert_(q4('k') == '100.0 kPa / C')
        assert_(q4('d') == '10000.0 dPa / C')
        assert_(q4('') == '100000.0 Pa / C')

        units = ['m', 's', 'g', 'C']
        self.report('units:', units)
        unit_set = [Unit(key) for key in units]

        q4 = Quantity('thermal_expandability', unit_set)
        self.report(q4.name, ':', q4())
        assert_(q4() == '1.0 mPa / C')

        assert_(q4('k') == str(0.000001) + ' kPa / C')
        assert_(q4('d') == '0.0001 dPa / C')
        assert_(q4('') == '0.001 Pa / C')
        assert_(q4('c') == '0.1 cPa / C')
        assert_(q4('m') == '1.0 mPa / C')
        assert_(q4('mu') == '1000.0 muPa / C')
        assert_(q4('n') == '1000000.0 nPa / C')

        return True

    def test_consistent_sets(self):
        from sfepy.mechanics.units import get_consistent_unit_set, sm

        if sm is None:
            self.report('cannot import sympy, skipping')
            return True

        u_sets = {
            ('m', 's', 'kg', 'C') : {'force' : '1.0 Newton',
                                     'stress' : '1.0 Pa',
                                     'energy' : '1.0 J',
                                     'thermal_expandability' : '1.0 Pa / C'},
            ('mm', 's', 'kg', 'C') : {'force' : '1.0 mNewton',
                                      'stress' : '1.0 kPa',
                                      'energy' : '1.0 muJ',
                                     'thermal_expandability' : '1.0 kPa / C'},
            ('mm', 's', 'g', 'C') : {'force' : '1.0 muNewton',
                                     'stress' : '1.0 Pa',
                                     'energy' : '1.0 nJ',
                                     'thermal_expandability' : '1.0 Pa / C'},
        }

        ok = True
        for unit_set, true_derived_units in u_sets.iteritems():
            self.report('units:', unit_set)
            derived_units = get_consistent_unit_set(*unit_set)

            for key, true_val in true_derived_units.iteritems():
                val = derived_units[key]
                _ok = true_val == val
                self.report('%s: %s == %s -> %s' % (key, true_val, val, _ok))

                ok = ok and _ok

        return ok

########NEW FILE########
__FILENAME__ = test_volume
"""
Test computing volumes by volume or surface integrals.
"""
from sfepy import data_dir

filename_mesh = data_dir + '/meshes/3d/elbow.mesh'

fields = {
    'scalar' : ('real', 'scalar', 'Omega', 1),
    'vector' : ('real', 'vector', 'Omega', 1),
}

integrals = {
    'i' : 2,
}

regions = {
    'Omega' : 'all',
    'Gamma' : ('vertices of surface', 'facet'),
}

expressions = {
    'volume_p' : 'd_volume.i.Omega(p)',
    'volume_u' : 'd_volume.i.Omega(u)',
    'surface_p' : 'd_volume_surface.i.Gamma(p)',
    'surface_u' : 'd_volume_surface.i.Gamma(u)',
}

import numpy as nm
from sfepy.base.testing import TestCommon

class Test(TestCommon):

    @staticmethod
    def from_conf(conf, options):
        from sfepy.discrete import Problem

        problem = Problem.from_conf(conf, init_equations=False)
        test = Test(problem=problem, conf=conf, options=options)
        return test

    def test_volume(self):
        from sfepy.discrete import FieldVariable

        ok = True

        field_map = {'u' : 'vector', 'p' : 'scalar'}

        volumes = {}
        avg = 0.0
        for key, term in expressions.items():
            var_name = key[-1]
            field = self.problem.fields[field_map[var_name]]
            var = FieldVariable(var_name, 'parameter', field,
                                primary_var_name='(set-to-None)')

            val = self.problem.evaluate(term, **{var_name : var})

            volumes[key] = val
            avg += val

        avg /= len(volumes)

        for key, val in volumes.items():
            err = nm.abs(avg - val) / nm.abs(avg)
            _ok = err < 1e-12
            self.report('"'"%s"'" - volume: %e' % (key, val))
            self.report('"'"%s"'" - relative volume difference: %e -> %s'
                        % (key, err, _ok))
            ok = ok and _ok

        return ok

    def test_volume_tl(self):
        from sfepy.discrete import FieldVariable

        fu = self.problem.fields['vector']
        fq = self.problem.fields['scalar']

        var_u = FieldVariable('u', 'parameter', fu,
                              primary_var_name='(set-to-None)')
        var_q = FieldVariable('q', 'test', fq,
                              primary_var_name='(set-to-None)')

        var_u.set_data(nm.linspace(0, 0.004, var_u.n_dof))

        vval = self.problem.evaluate('dw_tl_volume.i.Omega( q, u )',
                                     term_mode='volume', q=var_q, u=var_u)

        sval = self.problem.evaluate('d_tl_volume_surface.i.Gamma( u )',
                                     u=var_u)

        ok = abs(vval - sval) < 1e-14

        self.report('TL: by volume: %e == by surface: %e -> %s' %
                    (vval, sval, ok))

        return ok

########NEW FILE########
__FILENAME__ = test_install
#!/usr/bin/env python
"""
Simple script for testing various SfePy functionality, examples not
covered by tests, and running the tests.

The script just runs the commands specified in its main() using the
`subprocess` module, captures the output and compares one or more key
words to the expected ones.

The output of failed commands is saved to 'test_install.log' file.
"""
import time
from optparse import OptionParser
import shlex
import subprocess

def check_output(cmd):
    """
    Run the specified command and capture its outputs.

    Returns
    -------
    out : tuple
        The (stdout, stderr) output tuple.
    """
    print cmd
    args = shlex.split(cmd)

    p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    out = p.communicate()

    return out

def report(out, name, line, item, value, eps=None, return_item=False):
    """
    Check that `item` at `line` of the output string `out` is equal
    to `value`. If not, print the output.
    """
    try:
        status = out.split('\n')[line].split()

    except IndexError:
        print '  not enough output from command!'
        ok = False

    else:
        try:
            print '  comparing:', status[item], value

            if eps is None:
                ok = (status[item] == value)

            else:
                try:
                    ok = abs(float(status[item]) - float(value)) < eps

                except:
                    ok = False

        except IndexError:
            ok = False

    if ok:
        print '  %s:' % name, ok

    else:
        print '! %s:' % name, ok

        fd = open('test_install.log', 'a')
        fd.write('*' * 55)
        fd.write(out)
        fd.write('*' * 55)

    if return_item:
        return ok, status[item]

    else:
        return ok

usage = '%prog' + '\n' + __doc__

def main():
    parser = OptionParser(usage=usage, version='%prog')
    options, args = parser.parse_args()
    if len(args) > 0:
        parser.print_help()
        return

    fd = open('test_install.log', 'w')
    fd.close()

    eok = 0

    t0 = time.time()

    out, err = check_output('python ./script/blockgen.py')
    eok += report(out, '...', -2, 1, '...done')

    out, err = check_output('python ./script/cylindergen.py')
    eok += report(out, '...', -2, 1, '...done')

    out, err = check_output('python ./script/convert_mesh.py meshes/3d/cylinder.vtk out.mesh')
    eok += report(out, '...', -2, 1, '...done')

    out, err = check_output('python ./script/tile_periodic_mesh.py -r 2,2 meshes/elements/2_4_2.mesh out-per.mesh')
    eok += report(out, '...', -2, 1, 'done.')

    out, err = check_output('python ./script/extract_surface.py meshes/various_formats/octahedron.node -')
    eok += report(out, '...', -2, 0, '1185')

    out, err = check_output('python ./simple.py examples/diffusion/poisson.py')
    eok += report(out, '...', -2, 5, '1.173819e-16', eps=1e-15)

    out, err = check_output("""python ./simple.py -c "ebc_2 : {'name' : 't2', 'region' : 'Gamma_Right', 'dofs' : {'t.0' : -5.0}}" examples/diffusion/poisson.py""")
    eok += report(out, '...', -2, 5, '2.308051e-16', eps=1e-15)

    out, err = check_output('python ./simple.py examples/diffusion/poisson_iga.py')
    eok += report(out, '...', -2, 5, '3.373487e-15', eps=1e-14)

    out, err = check_output('python ./simple.py examples/navier_stokes/stokes.py')
    eok += report(out, '...', -2, 5, '1.210678e-13', eps=1e-11)

    out, err = check_output('python ./simple.py examples/diffusion/poisson_parametric_study.py')
    eok += report(out, '...', -2, 5, '1.606408e-14', eps=1e-13)

    out, err = check_output('python ./simple.py examples/linear_elasticity/its2D_3.py')
    eok += report(out, '...', -23, 5, '3.964886e-12', eps=1e-11)
    eok += report(out, '...', -3, 4, '2.58660e+01', eps=1e-5)

    out, err = check_output('python ./simple.py examples/linear_elasticity/linear_elastic_probes.py')
    eok += report(out, '...', -11, 5, '4.638192e-18', eps=1e-15)

    out, err = check_output('python ./probe.py examples/linear_elasticity/linear_elastic_probes.py cylinder.h5')
    eok += report(out, '...', -2, 3, 'cylinder_2.txt')

    out, err = check_output('python ./extractor.py -d cylinder.h5')
    eok += report(out, '...', -2, 1, '...done')

    out, err = check_output('python ./phonon.py examples/phononic/band_gaps.py')
    eok += report(out, '...', -6, 2, '208.54511594')
    eok += report(out, '...', -5, 1, '116309.22337295]')

    out, err = check_output('python ./phonon.py examples/phononic/band_gaps.py --phase-velocity')
    eok += report(out, '...', -2, 3, '4.1894123')
    eok += report(out, '...', -2, 4, '2.62055608]')

    out, err = check_output('python ./phonon.py examples/phononic/band_gaps.py -d')
    eok += report(out, '...', -6, 1, '[0,')

    out, err = check_output('python ./phonon.py examples/phononic/band_gaps_rigid.py')
    eok += report(out, '...', -6, 2, '4.58709531e+01')
    eok += report(out, '...', -5, 1, '1.13929200e+05]')

    out, err = check_output('python ./schroedinger.py --hydrogen')
    eok += report(out, '...', -4, -2, '-0.01913506', eps=1e-4)

    out, err = check_output('python ./homogen.py examples/homogenization/perfusion_micro.py')
    eok += report(out, '...', -7, -1, 'EpA...')

    out, err = check_output('python examples/standalone/homogenized_elasticity/rs_correctors.py -n')
    eok += report(out, '...', -2, -1, '1.644e-01]]')

    out, err = check_output('python examples/standalone/elastic_materials/compare_elastic_materials.py -n')
    eok += report(out, '...', -2, 5, '1.068759e-14', eps=1e-13)

    out, err = check_output('python examples/standalone/interactive/linear_elasticity.py')
    eok += report(out, '...', -8, 0, '1.62128841139e-14', eps=1e-13)

    out, err = check_output('python examples/standalone/thermal_electric/thermal_electric.py')
    eok += report(out, '...', -3, 5, '2.612933e-14', eps=1e-13)

    t1 = time.time()

    out, err = check_output('python ./run_tests.py')
    tok, failed = report(out, 'tests', -2, 7, '0', return_item=True)
    tok = {True : 'ok', False : 'fail'}[tok]

    t2 = time.time()

    fd = open('test_install_times.log', 'a+')
    fd.write('%s: examples: %.2f [s] (%d), tests: %.2f [s] (%s: %s)\n'
             % (time.ctime(t0), t1 - t0, eok, t2 - t1, tok, failed))
    fd.close()

if __name__ == '__main__':
    main()

########NEW FILE########
